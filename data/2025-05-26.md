<div id=toc></div>

# Table of Contents

- [cs.CL](#cs.CL) [Total: 268]
- [cs.CV](#cs.CV) [Total: 183]
- [cs.AI](#cs.AI) [Total: 73]
- [cs.SD](#cs.SD) [Total: 10]
- [cs.LG](#cs.LG) [Total: 257]
- [cs.MA](#cs.MA) [Total: 4]
- [cs.MM](#cs.MM) [Total: 1]
- [eess.AS](#eess.AS) [Total: 15]
- [eess.IV](#eess.IV) [Total: 21]


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [1] [Towards Robust Evaluation of STEM Education: Leveraging MLLMs in Project-Based Learning](https://arxiv.org/pdf/2505.17050)
*Yanhao Jia, Xinyi Wu, Qinglin Zhang, Yiran Qin, Luwei Xiao, Shuai Zhao*

Main category: cs.CL

TL;DR: PBLBench is a new benchmark for evaluating MLLMs in Project-Based Learning, addressing gaps in free-form output and expert validation, and showing current models achieve only 59% accuracy.


<details>
  <summary>Details</summary>
Motivation: Existing benchmarks lack free-form output and rigorous validation, limiting their effectiveness in real-world educational tasks.

Method: Introduces PBLBench with expert-driven AHP for structured evaluation, testing 15 MLLMs/LLMs.

Result: Top models achieve only 59% rank accuracy, highlighting the benchmark's challenge.

Conclusion: PBLBench aims to improve AI capabilities to reduce teacher workload and boost education productivity.

Abstract: Project-Based Learning (PBL) involves a variety of highly correlated
multimodal data, making it a vital educational approach within STEM
disciplines. With the rapid development of multimodal large language models
(MLLMs), researchers have begun exploring their potential to enhance tasks such
as information retrieval, knowledge comprehension, and data generation in
educational settings. However, existing benchmarks fall short in providing both
a free-form output structure and a rigorous human expert validation process,
limiting their effectiveness in evaluating real-world educational tasks.
Additionally, few methods have developed automated pipelines to assist with the
complex responsibilities of teachers leveraging MLLMs, largely due to model
hallucination and instability, which lead to unreliable implementation. To
address this gap, we introduce PBLBench, a novel benchmark designed to evaluate
complex reasoning grounded in domain-specific knowledge and long-context
understanding, thereby challenging models with tasks that closely resemble
those handled by human experts. To establish reliable ground truth, we adopt
the Analytic Hierarchy Process (AHP), utilizing expert-driven pairwise
comparisons to derive structured and weighted evaluation criteria. We assess
the performance of 15 leading MLLMs/LLMs using PBLBench and demonstrate that
even the most advanced models achieve only 59% rank accuracy, underscoring the
significant challenges presented by this benchmark. We believe PBLBench will
serve as a catalyst for the development of more capable AI agents, ultimately
aiming to alleviate teacher workload and enhance educational productivity.

</details>


### [2] [Prompt Engineering: How Prompt Vocabulary affects Domain Knowledge](https://arxiv.org/pdf/2505.17037)
*Dimitri Schreiter*

Main category: cs.CL

TL;DR: The paper explores how prompt specificity affects LLM performance in STEM, medicine, and law, finding an optimal specificity range for best results.


<details>
  <summary>Details</summary>
Motivation: To understand if increasing prompt specificity improves LLM performance in domain-specific tasks.

Method: Developed a synonymization framework to vary prompt specificity and tested it on four LLMs across STEM, law, and medicine datasets.

Result: No significant impact from general specificity increase, but an optimal specificity range exists for best performance across models.

Conclusion: Manipulating prompts within the optimal specificity range can maximize LLM performance in specialized domains.

Abstract: Prompt engineering has emerged as a critical component in optimizing large
language models (LLMs) for domain-specific tasks. However, the role of prompt
specificity, especially in domains like STEM (physics, chemistry, biology,
computer science and mathematics), medicine, and law, remains underexplored.
This thesis addresses the problem of whether increasing the specificity of
vocabulary in prompts improves LLM performance in domain-specific
question-answering and reasoning tasks. We developed a synonymization framework
to systematically substitute nouns, verbs, and adjectives with varying
specificity levels, measuring the impact on four LLMs: Llama-3.1-70B-Instruct,
Granite-13B-Instruct-V2, Flan-T5-XL, and Mistral-Large 2, across datasets in
STEM, law, and medicine. Our results reveal that while generally increasing the
specificity of prompts does not have a significant impact, there appears to be
a specificity range, across all considered models, where the LLM performs the
best. Identifying this optimal specificity range offers a key insight for
prompt design, suggesting that manipulating prompts within this range could
maximize LLM performance and lead to more efficient applications in specialized
domains.

</details>


### [3] [P2P: Automated Paper-to-Poster Generation and Fine-Grained Benchmark](https://arxiv.org/pdf/2505.17104)
*Tao Sun, Enhao Pan, Zhengkai Yang, Kaixin Sui, Jiajun Shi, Xianfu Cheng, Tongliang Li, Wenhao Huang, Ge Zhang, Jian Yang, Zhoujun Li*

Main category: cs.CL

TL;DR: P2P is an LLM-based multi-agent framework for automated academic poster generation, addressing challenges in semantic richness and visual-textual integration. It includes specialized agents, a large dataset (P2PInstruct), and a benchmark (P2PEval) for evaluation.


<details>
  <summary>Details</summary>
Motivation: Manual poster creation is time-consuming, and existing automated methods lack semantic richness and standardized benchmarks.

Method: P2P uses three specialized agents (visual processing, content generation, poster assembly) with checker modules for iterative refinement. It also introduces P2PInstruct (30,000 examples) and P2PEval (121 paper-poster pairs) for evaluation.

Result: P2P generates high-quality, HTML-rendered posters directly from research papers, demonstrating practical potential.

Conclusion: P2P streamlines research dissemination and provides tools for developing and evaluating poster generation systems.

Abstract: Academic posters are vital for scholarly communication, yet their manual
creation is time-consuming. However, automated academic poster generation faces
significant challenges in preserving intricate scientific details and achieving
effective visual-textual integration. Existing approaches often struggle with
semantic richness and structural nuances, and lack standardized benchmarks for
evaluating generated academic posters comprehensively. To address these
limitations, we introduce P2P, the first flexible, LLM-based multi-agent
framework that generates high-quality, HTML-rendered academic posters directly
from research papers, demonstrating strong potential for practical
applications. P2P employs three specialized agents-for visual element
processing, content generation, and final poster assembly-each integrated with
dedicated checker modules to enable iterative refinement and ensure output
quality. To foster advancements and rigorous evaluation in this domain, we
construct and release P2PInstruct, the first large-scale instruction dataset
comprising over 30,000 high-quality examples tailored for the academic
paper-to-poster generation task. Furthermore, we establish P2PEval, a
comprehensive benchmark featuring 121 paper-poster pairs and a dual evaluation
methodology (Universal and Fine-Grained) that leverages LLM-as-a-Judge and
detailed, human-annotated checklists. Our contributions aim to streamline
research dissemination and provide the community with robust tools for
developing and evaluating next-generation poster generation systems.

</details>


### [4] [Signals from the Floods: AI-Driven Disaster Analysis through Multi-Source Data Fusion](https://arxiv.org/pdf/2505.17038)
*Xian Gong, Paul X. McCarthy, Lin Tian, Marian-Andrei Rizoiu*

Main category: cs.CL

TL;DR: The study analyzes Twitter (X) and public inquiry submissions to understand public behavior during disasters like the 2022 NSW floods. It uses LDA and LLMs to filter and prioritize actionable content, improving emergency response and resilience planning.


<details>
  <summary>Details</summary>
Motivation: To leverage diverse web data (social media and public submissions) for better disaster response and understanding public behavior during crises.

Method: Integrates LDA for topic modeling and LLMs for semantic filtering, using public submissions as a reference to prioritize relevant social media content.

Result: Identifies behavioral patterns and geographic insights, reduces noise in social media data, and enhances situational awareness for responders.

Conclusion: Combining LDA and LLMs offers a novel AI-driven method to refine crisis-related data, aiding real-time response and long-term resilience planning.

Abstract: Massive and diverse web data are increasingly vital for government disaster
response, as demonstrated by the 2022 floods in New South Wales (NSW),
Australia. This study examines how X (formerly Twitter) and public inquiry
submissions provide insights into public behaviour during crises. We analyse
more than 55,000 flood-related tweets and 1,450 submissions to identify
behavioural patterns during extreme weather events. While social media posts
are short and fragmented, inquiry submissions are detailed, multi-page
documents offering structured insights. Our methodology integrates Latent
Dirichlet Allocation (LDA) for topic modelling with Large Language Models
(LLMs) to enhance semantic understanding. LDA reveals distinct opinions and
geographic patterns, while LLMs improve filtering by identifying flood-relevant
tweets using public submissions as a reference. This Relevance Index method
reduces noise and prioritizes actionable content, improving situational
awareness for emergency responders. By combining these complementary data
streams, our approach introduces a novel AI-driven method to refine
crisis-related social media content, improve real-time disaster response, and
inform long-term resilience planning.

</details>


### [5] [SweEval: Do LLMs Really Swear? A Safety Benchmark for Testing Limits for Enterprise Use](https://arxiv.org/pdf/2505.17332)
*Hitesh Laxmichand Patel, Amit Agarwal, Arion Das, Bhargava Kumar, Srikant Panda, Priyaranjan Pattnayak, Taki Hasan Rafi, Tejaswini Kumar, Dong-Kyu Chae*

Main category: cs.CL

TL;DR: SweEval is a benchmark for evaluating LLMs' ability to handle unsafe language and comply with ethical guidelines in diverse cultural contexts.


<details>
  <summary>Details</summary>
Motivation: Enterprise adoption of LLMs for communication tasks requires models to avoid offensive language and respect cultural nuances to mitigate risks and maintain trust.

Method: SweEval simulates real-world scenarios with varied tones and contexts, explicitly instructing models to include swear words to test compliance with ethical guidelines.

Result: The benchmark assesses LLMs' alignment with ethical frameworks, cultural sensitivity, and language comprehension.

Conclusion: SweEval aims to advance ethically aligned AI systems for enterprise use, with the dataset and code made publicly available.

Abstract: Enterprise customers are increasingly adopting Large Language Models (LLMs)
for critical communication tasks, such as drafting emails, crafting sales
pitches, and composing casual messages. Deploying such models across different
regions requires them to understand diverse cultural and linguistic contexts
and generate safe and respectful responses. For enterprise applications, it is
crucial to mitigate reputational risks, maintain trust, and ensure compliance
by effectively identifying and handling unsafe or offensive language. To
address this, we introduce SweEval, a benchmark simulating real-world scenarios
with variations in tone (positive or negative) and context (formal or
informal). The prompts explicitly instruct the model to include specific swear
words while completing the task. This benchmark evaluates whether LLMs comply
with or resist such inappropriate instructions and assesses their alignment
with ethical frameworks, cultural nuances, and language comprehension
capabilities. In order to advance research in building ethically aligned AI
systems for enterprise use and beyond, we release the dataset and code:
https://github.com/amitbcp/multilingual_profanity.

</details>


### [6] [RAVEN: Query-Guided Representation Alignment for Question Answering over Audio, Video, Embedded Sensors, and Natural Language](https://arxiv.org/pdf/2505.17114)
*Subrata Biswas, Mohammad Nur Hossain Khan, Bashima Islam*

Main category: cs.CL

TL;DR: RAVEN is a multimodal QA architecture with QuART, a query-conditioned cross-modal gating module, improving accuracy and robustness by filtering irrelevant tokens. It outperforms SOTA models and includes a new dataset, AVS-QA.


<details>
  <summary>Details</summary>
Motivation: Address modality disagreements in multimodal QA, where irrelevant tokens (e.g., off-camera speech) mislead models.

Method: Three-stage pipeline: unimodal pretraining, query-aligned fusion, and disagreement fine-tuning. Uses QuART for token relevance scoring.

Result: Achieves up to 14.5% accuracy gains, 16.4% boost with sensor data, and 50.23% robustness improvement over SOTA.

Conclusion: RAVEN effectively handles modality mismatches, outperforming existing models, and introduces a valuable dataset for future research.

Abstract: Multimodal question answering (QA) often requires identifying which video,
audio, or sensor tokens are relevant to the question. Yet modality
disagreements are common: off-camera speech, background noise, or motion
outside the field of view often mislead fusion models that weight all streams
equally. We present RAVEN, a unified QA architecture whose core is QuART, a
query-conditioned cross-modal gating module that assigns scalar relevance
scores to each token across modalities, enabling the model to amplify
informative signals and suppress distractors before fusion. RAVEN is trained
through a three-stage pipeline comprising unimodal pretraining, query-aligned
fusion, and disagreement-oriented fine-tuning -- each stage targeting a
distinct challenge in multi-modal reasoning: representation quality,
cross-modal relevance, and robustness to modality mismatch. To support training
and evaluation, we release AVS-QA, a dataset of 300K synchronized
Audio--Video-Sensor streams paired with automatically generated question-answer
pairs. Experimental results on seven multi-modal QA benchmarks -- including
egocentric and exocentric tasks -- show that RAVEN achieves up to 14.5\% and
8.0\% gains in accuracy compared to state-of-the-art multi-modal large language
models, respectively. Incorporating sensor data provides an additional 16.4\%
boost, and the model remains robust under modality corruption, outperforming
SOTA baselines by 50.23\%. Our code and dataset are available at
https://github.com/BASHLab/RAVEN.

</details>


### [7] [A new classification system of beer categories and styles based on large-scale data mining and self-organizing maps of beer recipes](https://arxiv.org/pdf/2505.17039)
*Diego Bonatto*

Main category: cs.CL

TL;DR: A data-driven approach classified 62,121 beer recipes into four superclusters based on ingredients and fermentation, revealing distinct patterns and offering a scalable taxonomy.


<details>
  <summary>Details</summary>
Motivation: To move beyond traditional sensory-based beer classifications by creating a reproducible, objective framework using quantitative data.

Method: Analyzed 62,121 beer recipes using statistical methods and self-organizing maps (SOMs) to identify patterns in ingredients and fermentation.

Result: Identified four superclusters with distinct malt/hop usage and fermentation traits, highlighting regional and historical trends.

Conclusion: The taxonomy provides a scalable tool for brewers and researchers, linking ingredients to fermentation and flavor outcomes.

Abstract: A data-driven quantitative approach was used to develop a novel
classification system for beer categories and styles. Sixty-two thousand one
hundred twenty-one beer recipes were mined and analyzed, considering ingredient
profiles, fermentation parameters, and recipe vital statistics. Statistical
analyses combined with self-organizing maps (SOMs) identified four major
superclusters that showed distinctive malt and hop usage patterns, style
characteristics, and historical brewing traditions. Cold fermented styles
showed a conservative grain and hop composition, whereas hot fermented beers
exhibited high heterogeneity, reflecting regional preferences and innovation.
This new taxonomy offers a reproducible and objective framework beyond
traditional sensory-based classifications, providing brewers, researchers, and
educators with a scalable tool for recipe analysis and beer development. The
findings in this work provide an understanding of beer diversity and open
avenues for linking ingredient usage with fermentation profiles and flavor
outcomes.

</details>


### [8] [VLM-KG: Multimodal Radiology Knowledge Graph Generation](https://arxiv.org/pdf/2505.17042)
*Abdullah Abdullah, Seong Tae Kim*

Main category: cs.CL

TL;DR: A novel multimodal VLM-based framework for radiology knowledge graph generation, outperforming unimodal methods.


<details>
  <summary>Details</summary>
Motivation: Existing unimodal methods for radiology knowledge graphs lack image integration and struggle with long-form data.

Method: Proposes a multimodal VLM-based framework combining radiology reports and images.

Result: Outperforms previous unimodal methods and introduces the first multimodal solution.

Conclusion: The framework addresses key limitations and advances radiology knowledge graph generation.

Abstract: Vision-Language Models (VLMs) have demonstrated remarkable success in natural
language generation, excelling at instruction following and structured output
generation. Knowledge graphs play a crucial role in radiology, serving as
valuable sources of factual information and enhancing various downstream tasks.
However, generating radiology-specific knowledge graphs presents significant
challenges due to the specialized language of radiology reports and the limited
availability of domain-specific data. Existing solutions are predominantly
unimodal, meaning they generate knowledge graphs only from radiology reports
while excluding radiographic images. Additionally, they struggle with long-form
radiology data due to limited context length. To address these limitations, we
propose a novel multimodal VLM-based framework for knowledge graph generation
in radiology. Our approach outperforms previous methods and introduces the
first multimodal solution for radiology knowledge graph generation.

</details>


### [9] [QRA++: Quantified Reproducibility Assessment for Common Types of Results in Natural Language Processing](https://arxiv.org/pdf/2505.17043)
*Anya Belz*

Main category: cs.CL

TL;DR: QRA++ is a quantitative method for assessing reproducibility in NLP, offering comparable, granular, and similarity-grounded evaluations.


<details>
  <summary>Details</summary>
Motivation: Low reproducibility in NLP studies lacks standardized assessment, making conclusions hard to interpret and compare.

Method: QRA++ provides continuous-valued reproducibility assessments at three granularity levels, using comparable measures and similarity-based expectations.

Result: Applied to three experiment sets, QRA++ shows reproducibility depends on experiment similarity, system type, and evaluation method.

Conclusion: QRA++ enhances reproducibility assessments, revealing factors influencing reproducibility in NLP.

Abstract: Reproduction studies reported in NLP provide individual data points which in
combination indicate worryingly low levels of reproducibility in the field.
Because each reproduction study reports quantitative conclusions based on its
own, often not explicitly stated, criteria for reproduction success/failure,
the conclusions drawn are hard to interpret, compare, and learn from. In this
paper, we present QRA++, a quantitative approach to reproducibility assessment
that (i) produces continuous-valued degree of reproducibility assessments at
three levels of granularity; (ii) utilises reproducibility measures that are
directly comparable across different studies; and (iii) grounds expectations
about degree of reproducibility in degree of similarity between experiments.
QRA++ enables more informative reproducibility assessments to be conducted, and
conclusions to be drawn about what causes reproducibility to be better/poorer.
We illustrate this by applying QRA++ to three example sets of comparable
experiments, revealing clear evidence that degree of reproducibility depends on
similarity of experiment properties, but also system type and evaluation
method.

</details>


### [10] [Improving endpoint detection in end-to-end streaming ASR for conversational speech](https://arxiv.org/pdf/2505.17070)
*Anandh C, Karthik Pandia Durai, Jeena Prakash, Manickavela Arumugam, Kadri Hacioglu, S. Pavankumar Dubagunta, Andreas Stolcke, Shankar Venkatesan, Aravind Ganapathiraju*

Main category: cs.CL

TL;DR: The paper proposes methods to improve ASR endpointing (EP) in transducer-based ASR (T-ASR) by addressing delayed emission and EP mistakes, using an end-of-word token and auxiliary network for speech activity detection.


<details>
  <summary>Details</summary>
Motivation: Delayed emission in T-ASR causes errors or delays in EP, leading to poor user experience (e.g., cutting off speech or increased latency).

Method: Introduces an end-of-word token with delay penalty and uses an auxiliary network for reliable frame-level speech activity detection.

Result: Evaluated on Switchboard corpus, the proposed methods show improvement over a baseline delay penalty method.

Conclusion: The methods effectively address EP issues in T-ASR, enhancing user experience by reducing delays and errors.

Abstract: ASR endpointing (EP) plays a major role in delivering a good user experience
in products supporting human or artificial agents in human-human/machine
conversations. Transducer-based ASR (T-ASR) is an end-to-end (E2E) ASR
modelling technique preferred for streaming. A major limitation of T-ASR is
delayed emission of ASR outputs, which could lead to errors or delays in EP.
Inaccurate EP will cut the user off while speaking, returning incomplete
transcript while delays in EP will increase the perceived latency, degrading
the user experience. We propose methods to improve EP by addressing delayed
emission along with EP mistakes. To address the delayed emission problem, we
introduce an end-of-word token at the end of each word, along with a delay
penalty. The EP delay is addressed by obtaining a reliable frame-level speech
activity detection using an auxiliary network. We apply the proposed methods on
Switchboard conversational speech corpus and evaluate it against a delay
penalty method.

</details>


### [11] [Assessing GPT's Bias Towards Stigmatized Social Groups: An Intersectional Case Study on Nationality Prejudice and Psychophobia](https://arxiv.org/pdf/2505.17045)
*Afifah Kashif, Heer Patel*

Main category: cs.CL

TL;DR: The paper examines biases in GPT-3.5/4/4o LLMs, showing greater negative bias against North Koreans, especially with mental disabilities, and calls for ethical improvements.


<details>
  <summary>Details</summary>
Motivation: To investigate ethical implications of biases in LLMs, focusing on intersectional biases involving nationality and mental disability.

Method: Structured prompt series evaluating model responses to scenarios involving American and North Korean nationalities with mental disabilities.

Result: Significant discrepancies in empathy levels, with North Koreans facing greater negative bias, especially when mental disability is a factor.

Conclusion: Highlights the need for LLM improvements with nuanced understanding of intersectional identity to mitigate biases.

Abstract: Recent studies have separately highlighted significant biases within
foundational large language models (LLMs) against certain nationalities and
stigmatized social groups. This research investigates the ethical implications
of these biases intersecting with outputs of widely-used GPT-3.5/4/4o LLMS.
Through structured prompt series, we evaluate model responses to several
scenarios involving American and North Korean nationalities with various mental
disabilities. Findings reveal significant discrepancies in empathy levels with
North Koreans facing greater negative bias, particularly when mental disability
is also a factor. This underscores the need for improvements in LLMs designed
with a nuanced understanding of intersectional identity.

</details>


### [12] [Impact of Frame Rates on Speech Tokenizer: A Case Study on Mandarin and English](https://arxiv.org/pdf/2505.17076)
*Haoyang Zhang, Hexin Liu, Xiangyu Zhang, Qiquan Zhang, Yuchen Hu, Junqi Zhao, Fei Tian, Xuerui Yang, Eng Siong Chng*

Main category: cs.CL

TL;DR: The study explores how frame rates in speech tokenizers affect Mandarin and English differently, revealing language-specific impacts on semantic tokens and suggesting optimization insights for speech tasks.


<details>
  <summary>Details</summary>
Motivation: To understand the underexplored impact of frame rates on speech tokenization, especially in typologically distinct languages like Mandarin and English.

Method: Encode speech at varying frame rates and evaluate semantic tokens in speech recognition tasks for Mandarin and English.

Result: Frame rate variations affect speech tokenization differently per language, influenced by phonetic density and acoustic features.

Conclusion: The findings guide frame rate optimization for speech tokenizers, benefiting applications like speech recognition and text-to-speech.

Abstract: The speech tokenizer plays a crucial role in recent speech tasks, generally
serving as a bridge between speech signals and language models. While
low-frame-rate codecs are widely employed as speech tokenizers, the impact of
frame rates on speech tokens remains underexplored. In this study, we
investigate how varying frame rates affect speech tokenization by examining
Mandarin and English, two typologically distinct languages. We encode speech at
different frame rates and evaluate the resulting semantic tokens in the speech
recognition task. Our findings reveal that frame rate variations influence
speech tokenization differently for each language, highlighting the interplay
between frame rates, phonetic density, and language-specific acoustic features.
The results provide insights into optimizing frame rate selection for speech
tokenizers, with implications for automatic speech recognition, text-to-speech,
and other speech-related applications.

</details>


### [13] [Assessing the Quality of AI-Generated Clinical Notes: A Validated Evaluation of a Large Language Model Scribe](https://arxiv.org/pdf/2505.17047)
*Erin Palm, Astrit Manikantan, Mark E. Pepin, Herprit Mahal, Srikanth Subramanya Belwadi*

Main category: cs.CL

TL;DR: The study evaluates AI-generated clinical notes using the PDQI9 tool, finding high inter-rater agreement and a slight quality difference favoring human notes (4.25 vs. 4.20).


<details>
  <summary>Details</summary>
Motivation: To address the lack of methods for assessing the quality of AI scribes in clinical documentation.

Method: A blinded study comparing LLM-generated notes (Ambient) with expert-drafted notes (Gold) using the PDQI9 tool, evaluated by specialists across 5 fields.

Result: High inter-rater agreement in most specialties, with Gold notes scoring slightly higher (4.25 vs. 4.20, p=0.04).

Conclusion: PDQI9 is a practical tool for assessing AI-generated notes, which perform nearly as well as human notes.

Abstract: In medical practices across the United States, physicians have begun
implementing generative artificial intelligence (AI) tools to perform the
function of scribes in order to reduce the burden of documenting clinical
encounters. Despite their widespread use, no established methods exist to gauge
the quality of AI scribes. To address this gap, we developed a blinded study
comparing the relative performance of large language model (LLM) generated
clinical notes with those from field experts based on audio-recorded clinical
encounters. Quantitative metrics from the Physician Documentation Quality
Instrument (PDQI9) provided a framework to measure note quality, which we
adapted to assess relative performance of AI generated notes. Clinical experts
spanning 5 medical specialties used the PDQI9 tool to evaluate
specialist-drafted Gold notes and LLM authored Ambient notes. Two evaluators
from each specialty scored notes drafted from a total of 97 patient visits. We
found uniformly high inter rater agreement (RWG greater than 0.7) between
evaluators in general medicine, orthopedics, and obstetrics and gynecology, and
moderate (RWG 0.5 to 0.7) to high inter rater agreement in pediatrics and
cardiology. We found a modest yet significant difference in the overall note
quality, wherein Gold notes achieved a score of 4.25 out of 5 and Ambient notes
scored 4.20 out of 5 (p = 0.04). Our findings support the use of the PDQI9
instrument as a practical method to gauge the quality of LLM authored notes, as
compared to human-authored notes.

</details>


### [14] [Large Language Models Implicitly Learn to See and Hear Just By Reading](https://arxiv.org/pdf/2505.17091)
*Prateek Verma, Mert Pilanci*

Main category: cs.CL

TL;DR: A text-based LLM model trained on text tokens inherently develops abilities to understand images and audio, enabling it to perform classification tasks without fine-tuning.


<details>
  <summary>Details</summary>
Motivation: To demonstrate that text LLMs can learn versatile internal representations applicable to audio and image tasks, reducing the need for task-specific training.

Method: The model processes image patches, audio waveforms, or tokens directly, generating embeddings or labels for classification tasks.

Result: The approach successfully performs audio classification on FSD-50K and GTZAN datasets and image classification on CIFAR-10 and Fashion-MNIST.

Conclusion: Text LLMs can develop powerful internal circuits adaptable to diverse tasks, suggesting a more efficient alternative to training models from scratch.

Abstract: This paper presents a fascinating find: By training an auto-regressive LLM
model on text tokens, the text model inherently develops internally an ability
to understand images and audio, thereby developing the ability to see and hear
just by reading. Popular audio and visual LLM models fine-tune text LLM models
to give text output conditioned on images and audio embeddings. On the other
hand, our architecture takes in patches of images, audio waveforms or tokens as
input. It gives us the embeddings or category labels typical of a
classification pipeline. We show the generality of text weights in aiding audio
classification for datasets FSD-50K and GTZAN. Further, we show this working
for image classification on CIFAR-10 and Fashion-MNIST, as well on image
patches. This pushes the notion of text-LLMs learning powerful internal
circuits that can be utilized by activating necessary connections for various
applications rather than training models from scratch every single time.

</details>


### [15] [What Media Frames Reveal About Stance: A Dataset and Study about Memes in Climate Change Discourse](https://arxiv.org/pdf/2505.16592)
*Shijia Zhou, Siyao Peng, Simon Luebke, Jörg Haßler, Mario Haim, Saif M. Mohammad, Barbara Plank*

Main category: cs.CL

TL;DR: The paper explores the interaction between stance and media framing in climate change memes, introduces the CLIMATEMEMES dataset, and evaluates model performance on stance and frame detection tasks.


<details>
  <summary>Details</summary>
Motivation: To investigate the understudied interaction between stance and media framing, particularly in climate change memes, and to provide a dataset for computational analysis.

Method: An interdisciplinary approach was used to curate CLIMATEMEMES, a dataset of 1,184 climate-change memes annotated with stance and media frames. Models like LLaVA-NeXT and Molmo were evaluated for stance and frame detection.

Result: VLMs perform well on stance detection but struggle with frame detection, where LLMs outperform them. Human captions consistently improve performance.

Conclusion: The study highlights VLMs' limitations in handling nuanced frames and stance expressions, suggesting further research is needed for better frame detection in memes.

Abstract: Media framing refers to the emphasis on specific aspects of perceived reality
to shape how an issue is defined and understood. Its primary purpose is to
shape public perceptions often in alignment with the authors' opinions and
stances. However, the interaction between stance and media frame remains
largely unexplored. In this work, we apply an interdisciplinary approach to
conceptualize and computationally explore this interaction with internet memes
on climate change. We curate CLIMATEMEMES, the first dataset of climate-change
memes annotated with both stance and media frames, inspired by research in
communication science. CLIMATEMEMES includes 1,184 memes sourced from 47
subreddits, enabling analysis of frame prominence over time and communities,
and sheds light on the framing preferences of different stance holders. We
propose two meme understanding tasks: stance detection and media frame
detection. We evaluate LLaVA-NeXT and Molmo in various setups, and report the
corresponding results on their LLM backbone. Human captions consistently
enhance performance. Synthetic captions and human-corrected OCR also help
occasionally. Our findings highlight that VLMs perform well on stance, but
struggle on frames, where LLMs outperform VLMs. Finally, we analyze VLMs'
limitations in handling nuanced frames and stance expressions on climate change
internet memes.

</details>


### [16] [Words That Unite The World: A Unified Framework for Deciphering Central Bank Communications Globally](https://arxiv.org/pdf/2505.17048)
*Agam Shah, Siddhant Sukhani, Huzaifa Pardawala, Saketh Budideti, Riya Bhadani, Rudra Gopal, Siddhartha Somani, Michael Galarnyk, Soungmin Lee, Arnav Hiray, Akshar Ravichandran, Eric Kim, Pranav Aluru, Joshua Zhang, Sebastian Jaskowski, Veer Guda, Meghaj Tarte, Liqin Ye, Spencer Gosden, Rutwik Routu, Rachel Yuh, Sloka Chava, Sahasra Chava, Dylan Patrick Kelly, Aiden Chiang, Harsit Mittal, Sudheer Chava*

Main category: cs.CL

TL;DR: The paper introduces the WCB dataset, a comprehensive monetary policy corpus, and benchmarks PLMs and LLMs on three tasks, showing aggregated data training outperforms individual bank data.


<details>
  <summary>Details</summary>
Motivation: To address the impact of misinterpretations in central bank communications on vulnerable populations by providing a robust dataset and framework.

Method: Creation of the WCB dataset (380k sentences from 25 banks), annotation, and benchmarking of 16 models across three tasks.

Result: Aggregated data training outperforms individual bank data; human evaluations confirm economic utility.

Conclusion: The WCB dataset and framework provide valuable tools for analyzing central bank communications, with broader applications in economic policy.

Abstract: Central banks around the world play a crucial role in maintaining economic
stability. Deciphering policy implications in their communications is
essential, especially as misinterpretations can disproportionately impact
vulnerable populations. To address this, we introduce the World Central Banks
(WCB) dataset, the most comprehensive monetary policy corpus to date,
comprising over 380k sentences from 25 central banks across diverse geographic
regions, spanning 28 years of historical data. After uniformly sampling 1k
sentences per bank (25k total) across all available years, we annotate and
review each sentence using dual annotators, disagreement resolutions, and
secondary expert reviews. We define three tasks: Stance Detection, Temporal
Classification, and Uncertainty Estimation, with each sentence annotated for
all three. We benchmark seven Pretrained Language Models (PLMs) and nine Large
Language Models (LLMs) (Zero-Shot, Few-Shot, and with annotation guide) on
these tasks, running 15,075 benchmarking experiments. We find that a model
trained on aggregated data across banks significantly surpasses a model trained
on an individual bank's data, confirming the principle "the whole is greater
than the sum of its parts." Additionally, rigorous human evaluations, error
analyses, and predictive tasks validate our framework's economic utility. Our
artifacts are accessible through the HuggingFace and GitHub under the
CC-BY-NC-SA 4.0 license.

</details>


### [17] [Benchmarking Expressive Japanese Character Text-to-Speech with VITS and Style-BERT-VITS2](https://arxiv.org/pdf/2505.17320)
*Zackary Rackauckas, Julia Hirschberg*

Main category: cs.CL

TL;DR: SBV2JE outperforms VITS in naturalness, intelligibility, and speaker consistency for Japanese character speech, matching human quality.


<details>
  <summary>Details</summary>
Motivation: Addressing the challenges of synthesizing expressive Japanese character speech due to pitch-accent sensitivity and stylistic variability.

Method: Benchmarking VITS and SBV2JE on character-specific datasets using naturalness (MOS, CMOS), intelligibility (WER), and speaker consistency metrics.

Result: SBV2JE matches human naturalness (MOS 4.37 vs. 4.38), achieves lower WER, and shows slight CMOS preference.

Conclusion: SBV2JE, with pitch-accent controls and WavLM discriminator, is effective for language learning and character dialogue, despite higher computational costs.

Abstract: Synthesizing expressive Japanese character speech poses unique challenges due
to pitch-accent sensitivity and stylistic variability. This paper benchmarks
two open-source text-to-speech models--VITS and Style-BERT-VITS2 JP Extra
(SBV2JE)--on in-domain, character-driven Japanese speech. Using three
character-specific datasets, we evaluate models across naturalness (mean
opinion and comparative mean opinion score), intelligibility (word error rate),
and speaker consistency. SBV2JE matches human ground truth in naturalness (MOS
4.37 vs. 4.38), achieves lower WER, and shows slight preference in CMOS.
Enhanced by pitch-accent controls and a WavLM-based discriminator, SBV2JE
proves effective for applications like language learning and character dialogue
generation, despite higher computational demands.

</details>


### [18] [Gender and Positional Biases in LLM-Based Hiring Decisions: Evidence from Comparative CV/Résumé Evaluations](https://arxiv.org/pdf/2505.17049)
*David Rozado*

Main category: cs.CL

TL;DR: LLMs show biases favoring female-named candidates and exhibit positional bias, raising concerns about their use in high-stakes decision-making.


<details>
  <summary>Details</summary>
Motivation: To investigate how LLMs evaluate professional candidates and whether they exhibit gender or positional biases.

Method: Experiment with 22 LLMs, presenting job descriptions and CV pairs with gendered or neutral names, and analyzing selection preferences.

Result: LLMs consistently favored female-named candidates, showed positional bias, and had negligible effects when rating CVs in isolation.

Conclusion: LLMs exhibit biases, questioning their reliability for autonomous decision-making without careful oversight.

Abstract: This study examines the behavior of Large Language Models (LLMs) when
evaluating professional candidates based on their resumes or curricula vitae
(CVs). In an experiment involving 22 leading LLMs, each model was
systematically given one job description along with a pair of
profession-matched CVs, one bearing a male first name, the other a female first
name, and asked to select the more suitable candidate for the job. Each CV pair
was presented twice, with names swapped to ensure that any observed preferences
in candidate selection stemmed from gendered names cues. Despite identical
professional qualifications across genders, all LLMs consistently favored
female-named candidates across 70 different professions. Adding an explicit
gender field (male/female) to the CVs further increased the preference for
female applicants. When gendered names were replaced with gender-neutral
identifiers "Candidate A" and "Candidate B", several models displayed a
preference to select "Candidate A". Counterbalancing gender assignment between
these gender-neutral identifiers resulted in gender parity in candidate
selection. When asked to rate CVs in isolation rather than compare pairs, LLMs
assigned slightly higher average scores to female CVs overall, but the effect
size was negligible. Including preferred pronouns (he/him or she/her) next to a
candidate's name slightly increased the odds of the candidate being selected
regardless of gender. Finally, most models exhibited a substantial positional
bias to select the candidate listed first in the prompt. These findings
underscore the need for caution when deploying LLMs in high-stakes autonomous
decision-making contexts and raise doubts about whether LLMs consistently apply
principled reasoning.

</details>


### [19] [Exploring the Effect of Segmentation and Vocabulary Size on Speech Tokenization for Speech Language Models](https://arxiv.org/pdf/2505.17446)
*Shunsuke Kando, Yusuke Miyao, Shinnosuke Takamichi*

Main category: cs.CL

TL;DR: The paper explores speech tokenization's impact on speech language models (SLMs), focusing on segmentation width and cluster size. Moderately coarse segmentation and larger clusters improve performance, with efficient models reducing training data and runtime.


<details>
  <summary>Details</summary>
Motivation: To understand how speech tokenization choices (segmentation width and cluster size) affect SLM performance, as their impact is unclear.

Method: Segment speech into fixed/variable widths and pooled representations, then train K-means models with varying cluster sizes. Evaluate on zero-shot spoken language understanding benchmarks.

Result: Moderately coarse segmentation and larger cluster sizes enhance performance. The most efficient model reduces training data by 50% and runtime by 70%.

Conclusion: Combining multiple tokens improves fine-grained spoken language understanding, emphasizing the importance of tokenization choices in SLMs.

Abstract: The purpose of speech tokenization is to transform a speech signal into a
sequence of discrete representations, serving as the foundation for speech
language models (SLMs). While speech tokenization has many options, their
effect on the performance of SLMs remains unclear. This paper investigates two
key aspects of speech tokenization: the segmentation width and the cluster size
of discrete units. First, we segment speech signals into fixed/variable widths
and pooled representations. We then train K-means models in multiple cluster
sizes. Through the evaluation on zero-shot spoken language understanding
benchmarks, we find the positive effect of moderately coarse segmentation and
bigger cluster size. Notably, among the best-performing models, the most
efficient one achieves a 50% reduction in training data and a 70% decrease in
training runtime. Our analysis highlights the importance of combining multiple
tokens to enhance fine-grained spoken language understanding.

</details>


### [20] [Embedding-to-Prefix: Parameter-Efficient Personalization for Pre-Trained Large Language Models](https://arxiv.org/pdf/2505.17051)
*Bernd Huber, Ghazal Fazelnia, Andreas Damianou, Sebastian Peleato, Max Lefarov, Praveen Ravichandran, Marco De Nadai, Mounia Lalmas-Roellke, Paul N. Bennett*

Main category: cs.CL

TL;DR: E2P is a parameter-efficient method for personalizing LLM outputs by injecting pre-computed embeddings into the model's hidden space, avoiding costly fine-tuning.


<details>
  <summary>Details</summary>
Motivation: Tailoring LLM outputs to individual users is challenging; existing methods are costly or inefficient.

Method: Embedding-to-Prefix (E2P) projects pre-computed embeddings into the LLM's hidden space via a learned soft token prefix.

Result: E2P performs well on dialogue personalization, headline generation, and large-scale content personalization with minimal overhead.

Conclusion: E2P offers a scalable, efficient solution for contextualizing generative AI systems.

Abstract: Large language models (LLMs) excel at generating contextually relevant
content. However, tailoring these outputs to individual users for effective
personalization is a significant challenge. While rich user-specific
information often exists as pre-existing user representations, such as
embeddings learned from preferences or behaviors, current methods to leverage
these for LLM personalization typically require costly fine-tuning or
token-heavy prompting. We propose Embedding-to-Prefix (E2P), a
parameter-efficient method that injects pre-computed context embeddings into an
LLM's hidden representation space through a learned projection to a single soft
token prefix. This enables effective personalization while keeping the backbone
model frozen and avoiding expensive adaptation techniques. We evaluate E2P
across two public datasets and in a production setting: dialogue
personalization on Persona-Chat, contextual headline generation on PENS, and
large-scale personalization for music and podcast consumption. Results show
that E2P preserves contextual signals and achieves strong performance with
minimal computational overhead, offering a scalable, efficient solution for
contextualizing generative AI systems.

</details>


### [21] [Analyzing Mitigation Strategies for Catastrophic Forgetting in End-to-End Training of Spoken Language Models](https://arxiv.org/pdf/2505.17496)
*Chi-Yuan Hsiao, Ke-Han Lu, Kai-Wei Chang, Chih-Kai Yang, Wei-Chih Chen, Hung-yi Lee*

Main category: cs.CL

TL;DR: The paper examines catastrophic forgetting in multi-stage training of Spoken Language Models (SLMs) and evaluates three mitigation strategies, finding experience replay most effective.


<details>
  <summary>Details</summary>
Motivation: To address catastrophic forgetting in SLMs due to diverse task distributions during multi-stage training.

Method: Evaluates three strategies: model merging, LoRA scaling factor discounting, and experience replay.

Result: Experience replay is most effective, especially when combined with other methods.

Conclusion: Insights for robust SLM training pipelines are provided, emphasizing experience replay.

Abstract: End-to-end training of Spoken Language Models (SLMs) commonly involves
adapting pre-trained text-based Large Language Models (LLMs) to the speech
modality through multi-stage training on diverse tasks such as ASR, TTS and
spoken question answering (SQA). Although this multi-stage continual learning
equips LLMs with both speech understanding and generation capabilities, the
substantial differences in task and data distributions across stages can lead
to catastrophic forgetting, where previously acquired knowledge is lost. This
paper investigates catastrophic forgetting and evaluates three mitigation
strategies-model merging, discounting the LoRA scaling factor, and experience
replay to balance knowledge retention with new learning. Results show that
experience replay is the most effective, with further gains achieved by
combining it with other methods. These findings provide insights for developing
more robust and efficient SLM training pipelines.

</details>


### [22] [SpecEdge: Scalable Edge-Assisted Serving Framework for Interactive LLMs](https://arxiv.org/pdf/2505.17052)
*Jinwoo Park, Seunggeun Cho, Dongsu Han*

Main category: cs.CL

TL;DR: SpecEdge is an edge-assisted inference framework that splits LLM workloads between edge and server GPUs, improving cost efficiency and reducing latency.


<details>
  <summary>Details</summary>
Motivation: Serving LLMs at scale is costly; current systems ignore consumer-grade GPUs at the edge.

Method: Uses speculative decoding, proactive edge drafting, and pipeline-aware scheduling to split workloads between edge and server GPUs.

Result: Achieves 1.91x cost efficiency, 2.22x server throughput, and 11.24% lower latency.

Conclusion: SpecEdge offers a scalable, cost-effective paradigm for LLM serving.

Abstract: Large language models (LLMs) power many modern applications, but serving them
at scale remains costly and resource-intensive. Current server-centric systems
overlook consumer-grade GPUs at the edge. We introduce SpecEdge, an
edge-assisted inference framework that splits LLM workloads between edge and
server GPUs using a speculative decoding scheme, exchanging only token outputs
over the network. SpecEdge employs proactive edge drafting to overlap edge
token creation with server verification and pipeline-aware scheduling that
interleaves multiple user requests to increase server-side throughput.
Experiments show SpecEdge enhances overall cost efficiency by 1.91x through
achieving 2.22x server throughput, and reduces inter token latency by 11.24%
compared to a server-only baseline, introducing a scalable, cost-effective
paradigm for LLM serving.

</details>


### [23] [Swedish Whispers; Leveraging a Massive Speech Corpus for Swedish Speech Recognition](https://arxiv.org/pdf/2505.17538)
*Leonora Vesterbacka, Faton Rekathati, Robin Kurtz, Justyna Sikora, Agnes Toftgård*

Main category: cs.CL

TL;DR: Fine-tuned Whisper models for Swedish show a 47% WER reduction compared to OpenAI's whisper-large-v3.


<details>
  <summary>Details</summary>
Motivation: Mid-resourced languages like Swedish are underrepresented in multilingual datasets, limiting performance.

Method: Fine-tuned Whisper models on a large, diverse Swedish dataset.

Result: 47% average WER reduction across evaluations on FLEURS, Common Voice, and NST.

Conclusion: Fine-tuning multilingual models for underrepresented languages significantly improves performance.

Abstract: This work presents a suite of fine-tuned Whisper models for Swedish, trained
on a dataset of unprecedented size and variability for this mid-resourced
language. As languages of smaller sizes are often underrepresented in
multilingual training datasets, substantial improvements in performance can be
achieved by fine-tuning existing multilingual models, as shown in this work.
This work reports an overall improvement across model sizes compared to
OpenAI's Whisper evaluated on Swedish. Most notably, we report an average 47%
reduction in WER comparing our best performing model to OpenAI's
whisper-large-v3, in evaluations across FLEURS, Common Voice, and NST.

</details>


### [24] [Social preferences with unstable interactive reasoning: Large language models in economic trust games](https://arxiv.org/pdf/2505.17053)
*Ou Jiamin, Eikmans Emile, Buskens Vincent, Pankowska Paulina, Shan Yuli*

Main category: cs.CL

TL;DR: LLMs like ChatGPT-4, Claude, and Bard exhibit trust and reciprocity in economic trust games, deviating from pure self-interest. Their behavior varies with personas, with ChatGPT-4 showing the highest trust in unselfish personas.


<details>
  <summary>Details</summary>
Motivation: To explore how LLMs translate language understanding into social exchange contexts, mimicking human interactions in economic trust games.

Method: Three LLMs (ChatGPT-4, Claude, Bard) were tested in trust games to assess social preferences and interactive reasoning, with and without personas.

Result: LLMs showed trust and reciprocity, with ChatGPT-4 excelling in unselfish personas. Behavior varied significantly with personas, and interactive reasoning appeared random.

Conclusion: LLMs can mimic human-like social behaviors, but their responses are highly persona-dependent and lack stable interactive reasoning.

Abstract: While large language models (LLMs) have demonstrated remarkable capabilities
in understanding human languages, this study explores how they translate this
understanding into social exchange contexts that capture certain essences of
real world human interactions. Three LLMs - ChatGPT-4, Claude, and Bard - were
placed in economic trust games where players balance self-interest with trust
and reciprocity, making decisions that reveal their social preferences and
interactive reasoning abilities. Our study shows that LLMs deviate from pure
self-interest and exhibit trust and reciprocity even without being prompted to
adopt a specific persona. In the simplest one-shot interaction, LLMs emulated
how human players place trust at the beginning of such a game. Larger
human-machine divergences emerged in scenarios involving trust repayment or
multi-round interactions, where decisions were influenced by both social
preferences and interactive reasoning. LLMs responses varied significantly when
prompted to adopt personas like selfish or unselfish players, with the impact
outweighing differences between models or game types. Response of ChatGPT-4, in
an unselfish or neutral persona, resembled the highest trust and reciprocity,
surpassing humans, Claude, and Bard. Claude and Bard displayed trust and
reciprocity levels that sometimes exceeded and sometimes fell below human
choices. When given selfish personas, all LLMs showed lower trust and
reciprocity than humans. Interactive reasoning to the actions of counterparts
or changing game mechanics appeared to be random rather than stable,
reproducible characteristics in the response of LLMs, though some improvements
were observed when ChatGPT-4 responded in selfish or unselfish personas.

</details>


### [25] [METHOD: Modular Efficient Transformer for Health Outcome Discovery](https://arxiv.org/pdf/2505.17054)
*Linglong Qian, Zina Ibrahim*

Main category: cs.CL

TL;DR: The paper introduces \METHOD, a transformer architecture for healthcare, addressing challenges like irregular sampling and complex dependencies in patient data. It outperforms state-of-the-art models in clinical predictions.


<details>
  <summary>Details</summary>
Motivation: Transformers face challenges in healthcare due to irregular patient data. The paper aims to adapt transformers for clinical sequence modeling in EHRs.

Method: \METHOD integrates patient-aware attention, adaptive sliding window attention, and U-Net inspired architecture for efficient long sequence processing.

Result: \METHOD outperforms \ETHOS on MIMIC-IV, especially in high-severity cases, and maintains stable performance across varying sequence lengths.

Conclusion: \METHOD advances healthcare-specific transformers, offering accurate predictions while preserving clinical hierarchies and computational efficiency.

Abstract: Recent advances in transformer architectures have revolutionised natural
language processing, but their application to healthcare domains presents
unique challenges. Patient timelines are characterised by irregular sampling,
variable temporal dependencies, and complex contextual relationships that
differ substantially from traditional language tasks. This paper introduces
\METHOD~(Modular Efficient Transformer for Health Outcome Discovery), a novel
transformer architecture specifically designed to address the challenges of
clinical sequence modelling in electronic health records. \METHOD~integrates
three key innovations: (1) a patient-aware attention mechanism that prevents
information leakage whilst enabling efficient batch processing; (2) an adaptive
sliding window attention scheme that captures multi-scale temporal
dependencies; and (3) a U-Net inspired architecture with dynamic skip
connections for effective long sequence processing. Evaluations on the MIMIC-IV
database demonstrate that \METHOD~consistently outperforms the state-of-the-art
\ETHOS~model, particularly in predicting high-severity cases that require
urgent clinical intervention. \METHOD~exhibits stable performance across
varying inference lengths, a crucial feature for clinical deployment where
patient histories vary significantly in length. Analysis of learned embeddings
reveals that \METHOD~better preserves clinical hierarchies and relationships
between medical concepts. These results suggest that \METHOD~represents a
significant advancement in transformer architectures optimised for healthcare
applications, providing more accurate and clinically relevant predictions
whilst maintaining computational efficiency.

</details>


### [26] [Enhancing Mathematics Learning for Hard-of-Hearing Students Through Real-Time Palestinian Sign Language Recognition: A New Dataset](https://arxiv.org/pdf/2505.17055)
*Fidaa khandaqji, Huthaifa I. Ashqar, Abdelrahem Atawnih*

Main category: cs.CL

TL;DR: The paper presents a PSL recognition system for math education, achieving 97.59% accuracy using a Vision Transformer model.


<details>
  <summary>Details</summary>
Motivation: To improve math education accessibility for hard-of-hearing students by addressing the lack of digital PSL resources.

Method: Created a custom PSL dataset for math gestures and fine-tuned a Vision Transformer (ViT) model for classification.

Result: The model achieved 97.59% accuracy in recognizing mathematical signs.

Conclusion: The study demonstrates deep learning's potential in creating inclusive educational tools for hard-of-hearing students.

Abstract: The study aims to enhance mathematics education accessibility for
hard-of-hearing students by developing an accurate Palestinian sign language
PSL recognition system using advanced artificial intelligence techniques. Due
to the scarcity of digital resources for PSL, a custom dataset comprising 41
mathematical gesture classes was created, and recorded by PSL experts to ensure
linguistic accuracy and domain specificity. To leverage
state-of-the-art-computer vision techniques, a Vision Transformer ViTModel was
fine-tuned for gesture classification. The model achieved an accuracy of
97.59%, demonstrating its effectiveness in recognizing mathematical signs with
high precision and reliability. This study highlights the role of deep learning
in developing intelligent educational tools that bridge the learning gap for
hard-of-hearing students by providing AI-driven interactive solutions to
enhance mathematical comprehension. This work represents a significant step
toward innovative and inclusive frosting digital integration in specialized
learning environments. The dataset is hosted on Hugging Face at
https://huggingface.co/datasets/fidaakh/STEM_data.

</details>


### [27] [Are LLMs Ready for English Standardized Tests? A Benchmarking and Elicitation Perspective](https://arxiv.org/pdf/2505.17056)
*Luoxi Tang, Tharunya Sundar, Shuai Yang, Ankita Patra, Manohar Chippada, Giqi Zhao, Yi Li, Riteng Zhang, Tunan Zhao, Ting Yang, Yuqiao Meng, Weicheng Ma, Zhaohan Xi*

Main category: cs.CL

TL;DR: The paper explores how large language models (LLMs) can enhance standardized test preparation, specifically for English Standardized Tests (ESTs), using the ESTBOOK benchmark to evaluate their accuracy and efficiency.


<details>
  <summary>Details</summary>
Motivation: AI, particularly LLMs, has the potential to revolutionize education by improving learning tools, especially for standardized test preparation.

Method: The authors introduce ESTBOOK, a benchmark aggregating five ESTs with diverse question types, and propose a breakdown analysis framework to evaluate LLMs' performance step-by-step.

Result: The study systematically assesses LLMs' accuracy and inference efficiency, providing insights into their capabilities for educational applications.

Conclusion: The findings highlight LLMs' potential in education and suggest strategies to enhance their reliability as intelligent tutoring systems.

Abstract: AI is transforming education by enabling powerful tools that enhance learning
experiences. Among recent advancements, large language models (LLMs) hold
particular promise for revolutionizing how learners interact with educational
content. In this work, we investigate the potential of LLMs to support
standardized test preparation by focusing on English Standardized Tests (ESTs).
Specifically, we assess their ability to generate accurate and contextually
appropriate solutions across a diverse set of EST question types. We introduce
ESTBOOK, a comprehensive benchmark designed to evaluate the capabilities of
LLMs in solving EST questions. ESTBOOK aggregates five widely recognized tests,
encompassing 29 question types and over 10,576 questions across multiple
modalities, including text, images, audio, tables, and mathematical symbols.
Using ESTBOOK, we systematically evaluate both the accuracy and inference
efficiency of LLMs. Additionally, we propose a breakdown analysis framework
that decomposes complex EST questions into task-specific solution steps. This
framework allows us to isolate and assess LLM performance at each stage of the
reasoning process. Evaluation findings offer insights into the capability of
LLMs in educational contexts and point toward targeted strategies for improving
their reliability as intelligent tutoring systems.

</details>


### [28] [DO-RAG: A Domain-Specific QA Framework Using Knowledge Graph-Enhanced Retrieval-Augmented Generation](https://arxiv.org/pdf/2505.17058)
*David Osei Opoku, Ming Sheng, Yong Zhang*

Main category: cs.CL

TL;DR: DO-RAG is a hybrid QA framework combining knowledge graphs and semantic retrieval for high-precision domain-specific QA, outperforming baselines by up to 33.38%.


<details>
  <summary>Details</summary>
Motivation: Addressing challenges in integrating heterogeneous data and maintaining reasoning consistency in Retrieval-Augmented Generation (RAG) frameworks for domain-specific QA.

Method: Proposes DO-RAG, integrating multi-level knowledge graph construction with semantic vector retrieval, using an agentic chain-of-thought architecture for dynamic knowledge graphs and hallucination mitigation.

Result: Achieves near-perfect recall and over 94% answer relevancy, outperforming baselines by up to 33.38%.

Conclusion: DO-RAG provides a scalable, reliable solution for multi-domain, high-precision QA with traceability and adaptability.

Abstract: Domain-specific QA systems require not just generative fluency but high
factual accuracy grounded in structured expert knowledge. While recent
Retrieval-Augmented Generation (RAG) frameworks improve context recall, they
struggle with integrating heterogeneous data and maintaining reasoning
consistency. To address these challenges, we propose DO-RAG, a scalable and
customizable hybrid QA framework that integrates multi-level knowledge graph
construction with semantic vector retrieval. Our system employs a novel agentic
chain-of-thought architecture to extract structured relationships from
unstructured, multimodal documents, constructing dynamic knowledge graphs that
enhance retrieval precision. At query time, DO-RAG fuses graph and vector
retrieval results to generate context-aware responses, followed by
hallucination mitigation via grounded refinement. Experimental evaluations in
the database and electrical domains show near-perfect recall and over 94%
answer relevancy, with DO-RAG outperforming baseline frameworks by up to
33.38%. By combining traceability, adaptability, and performance efficiency,
DO-RAG offers a reliable foundation for multi-domain, high-precision QA at
scale.

</details>


### [29] [Medalyze: Lightweight Medical Report Summarization Application Using FLAN-T5-Large](https://arxiv.org/pdf/2505.17059)
*Van-Tinh Nguyen, Hoang-Duong Pham, Thanh-Hai To, Cong-Tuan Hung Do, Thi-Thu-Trang Dong, Vu-Trung Duong Le, Van-Phuc Hoang*

Main category: cs.CL

TL;DR: Medalyze is an AI tool using FLAN-T5-Large models for medical text tasks like summarization, issue extraction, and question identification, outperforming GPT-4 in evaluations.


<details>
  <summary>Details</summary>
Motivation: Medical texts are complex, requiring tools to enhance comprehension and accessibility.

Method: Three FLAN-T5-Large models fine-tuned for specific tasks, deployed via web/mobile with real-time inference, using scalable API and YugabyteDB.

Result: Outperforms GPT-4 in domain-specific summarization (BLEU, ROUGE-L, BERTScore, SpaCy Similarity).

Conclusion: Medalyze offers a practical, privacy-preserving solution for healthcare information accessibility.

Abstract: Understanding medical texts presents significant challenges due to complex
terminology and context-specific language. This paper introduces Medalyze, an
AI-powered application designed to enhance the comprehension of medical texts
using three specialized FLAN-T5-Large models. These models are fine-tuned for
(1) summarizing medical reports, (2) extracting health issues from
patient-doctor conversations, and (3) identifying the key question in a
passage. Medalyze is deployed across a web and mobile platform with real-time
inference, leveraging scalable API and YugabyteDB. Experimental evaluations
demonstrate the system's superior summarization performance over GPT-4 in
domain-specific tasks, based on metrics like BLEU, ROUGE-L, BERTScore, and
SpaCy Similarity. Medalyze provides a practical, privacy-preserving, and
lightweight solution for improving information accessibility in healthcare.

</details>


### [30] [SALMONN-omni: A Standalone Speech LLM without Codec Injection for Full-duplex Conversation](https://arxiv.org/pdf/2505.17060)
*Wenyi Yu, Siyin Wang, Xiaoyu Yang, Xianzhao Chen, Xiaohai Tian, Jun Zhang, Guangzhi Sun, Lu Lu, Yuxuan Wang, Chao Zhang*

Main category: cs.CL

TL;DR: SALMONN-omni is a single, standalone full-duplex speech LLM that avoids audio codecs, using a dynamic thinking mechanism to improve performance in conversational tasks.


<details>
  <summary>Details</summary>
Motivation: Existing modular systems suffer from error accumulation and struggle with challenges like barge-in and echo cancellation. Recent methods like Moshi simplify the pipeline but degrade in speech performance.

Method: SALMONN-omni introduces a dynamic thinking mechanism within an LLM backbone, enabling seamless transitions between speaking and listening states without audio codecs.

Result: It achieves a 30% relative performance improvement over open-source full-duplex models and competes with half-duplex systems, excelling in complex conversational scenarios.

Conclusion: SALMONN-omni demonstrates strong performance in natural speech interaction, with further improvements via reinforcement learning.

Abstract: In order to enable fluid and natural human-machine speech interaction,
existing full-duplex conversational systems often adopt modular architectures
with auxiliary components such as voice activity detectors, interrupters,
conversation state predictors, or multiple LLMs. These systems, however, suffer
from error accumulation across modules and struggle with key challenges such as
context-dependent barge-in and echo cancellation. Recent approaches, most
notably Moshi, simplify the pipeline by injecting audio codecs into the token
space of a single LLM. However, such methods still incur significant
performance degradation when operating on the speech rather than text modality.
In this paper, we introduce SALMONN-omni, the first single, standalone
full-duplex speech LLM that operates without audio codecs in its token space.
It features a novel dynamic thinking mechanism within the LLM backbone,
enabling the model to learn when to transition between speaking and listening
states. Experiments on widely used benchmarks for spoken question answering and
open-domain dialogue show that SALMONN-omni achieves at least 30\% relative
performance improvement over existing open-source full-duplex models and
performs highly competitively to half-duplex and turn-based systems, despite
using substantially less training data. Moreover, SALMONN-omni demonstrates
strong performance in complex conversational scenarios, including turn-taking,
backchanneling, echo cancellation and context-dependent barge-in, with further
improvements achieved through reinforcement learning. Some demo conversations
between user and SALMONN-omni are provided in the following repository
https://github.com/bytedance/SALMONN.

</details>


### [31] [Enhancing Low-Resource Language and Instruction Following Capabilities of Audio Language Models](https://arxiv.org/pdf/2409.10999)
*Potsawee Manakul, Guangzhi Sun, Warit Sirichotedumrong, Kasima Tharnpipitchai, Kunat Pipatanakul*

Main category: cs.CL

TL;DR: Audio language models, primarily trained on English, struggle with low-resource languages like Thai. The paper proposes Typhoon-Audio, a model optimized for both Thai and English, outperforming existing models.


<details>
  <summary>Details</summary>
Motivation: Most audio language models are trained on English, limiting their effectiveness for low-resource languages like Thai. The study aims to improve cross-lingual performance.

Method: The paper explores data mixtures to optimize models for target languages and English, integrating audio comprehension and speech instruction-following into a unified model.

Result: Typhoon-Audio significantly outperforms open-source models and matches Gemini-1.5-Pro in performance for both English and Thai.

Conclusion: Balancing language-specific and multilingual training data improves instruction-following in low-resource languages, as demonstrated by Typhoon-Audio.

Abstract: Audio language models process audio inputs using textual prompts for tasks
like speech recognition and audio captioning. Although built on multilingual
pre-trained components, most are trained primarily on English, limiting their
usability for other languages. This paper evaluates audio language models on
Thai, a low-resource language, and finds that they lack emergent cross-lingual
abilities despite their multilingual foundations. To address this, we explore
data mixtures that optimize audio language models for both a target language
and English while integrating audio comprehension and speech
instruction-following into a unified model. Our experiments provide insights
into improving instruction-following in low-resource languages by balancing
language-specific and multilingual training data. The proposed model,
Typhoon-Audio, significantly outperforms existing open-source models and
achieves performance comparable to state-of-the-art Gemini-1.5-Pro in both
English and Thai.

</details>


### [32] [Mixture of Decoding: An Attention-Inspired Adaptive Decoding Strategy to Mitigate Hallucinations in Large Vision-Language Models](https://arxiv.org/pdf/2505.17061)
*Xinlong Chen, Yuanxing Zhang, Qiang Liu, Junfei Wu, Fuzheng Zhang, Tieniu Tan*

Main category: cs.CL

TL;DR: MoD mitigates hallucinations in LVLMs by dynamically adapting decoding strategies based on attention correctness.


<details>
  <summary>Details</summary>
Motivation: Address the persistent challenge of hallucinations in LVLMs.

Method: Proposes Mixture of Decoding (MoD), which evaluates attention correctness and adapts strategies (complementary or contrastive) accordingly.

Result: MoD outperforms existing methods on benchmarks, effectively reducing hallucinations.

Conclusion: MoD is a novel, effective approach for hallucination mitigation in LVLMs.

Abstract: Large Vision-Language Models (LVLMs) have exhibited impressive capabilities
across various visual tasks, yet they remain hindered by the persistent
challenge of hallucinations. To address this critical issue, we propose Mixture
of Decoding (MoD), a novel approach for hallucination mitigation that
dynamically adapts decoding strategies by evaluating the correctness of the
model's attention on image tokens. Specifically, MoD measures the consistency
between outputs generated from the original image tokens and those derived from
the model's attended image tokens, to distinguish the correctness
aforementioned. If the outputs are consistent, indicating correct attention,
MoD employs a complementary strategy to amplify critical information.
Conversely, if the outputs are inconsistent, suggesting erroneous attention,
MoD utilizes a contrastive strategy to suppress misleading information.
Extensive experiments demonstrate that MoD significantly outperforms existing
decoding methods across multiple mainstream benchmarks, effectively mitigating
hallucinations in LVLMs. The code is available at
https://github.com/xlchen0205/MoD.

</details>


### [33] [Synthetic Data RL: Task Definition Is All You Need](https://arxiv.org/pdf/2505.17063)
*Yiduo Guo, Zhen Guo, Chuanwei Huang, Zi-Ang Wang, Zekai Zhang, Haofei Yu, Huishuai Zhang, Yikang Shen*

Main category: cs.CL

TL;DR: Synthetic Data RL uses synthetic data for reinforcement learning, outperforming human-labeled data and supervised fine-tuning, with significant improvements on various benchmarks.


<details>
  <summary>Details</summary>
Motivation: To reduce reliance on large-scale human-labeled data for RL-based model adaptation, enabling scalable and efficient fine-tuning.

Method: Generates synthetic question-answer pairs from task definitions, adapts question difficulty based on model solvability, and uses average pass rates for RL training.

Result: Achieves 29.2% improvement on GSM8K, 8.7% on MATH, and similar gains on other benchmarks, nearly matching human-data RL.

Conclusion: Synthetic Data RL is a scalable and efficient alternative to human-labeled data for RL-based model adaptation.

Abstract: Reinforcement learning (RL) is a powerful way to adapt foundation models to
specialized tasks, but its reliance on large-scale human-labeled data limits
broad adoption. We introduce Synthetic Data RL, a simple and general framework
that reinforcement fine-tunes models using only synthetic data generated from a
task definition. Our method first generates question and answer pairs from the
task definition and retrieved documents, then adapts the difficulty of the
question based on model solvability, and selects questions using the average
pass rate of the model across samples for RL training. On Qwen-2.5-7B, our
method achieves a 29.2% absolute improvement over the base model on GSM8K (+2.9
pp vs. instruction-tuned, +6.6 pp vs. Self-Instruct), 8.7% on MATH, 13.1% on
GPQA (+7.0 pp vs. SynthLLM), 8.9% on MedQA, 17.7% on CQA (law) and 13.7% on CFA
(finance). It surpasses supervised fine-tuning under the same data budget and
nearly matches RL with full human data across datasets (e.g., +17.2 pp on
GSM8K). Adding 100 human demonstrations improves the performance of GSM8K only
by 0.4 pp, showing a limited added value. By reducing human data annotation,
Synthetic Data RL enables scalable and efficient RL-based model adaptation.
Code and demos are available at https://github.com/gydpku/Data_Synthesis_RL/.

</details>


### [34] [Decoding Rarity: Large Language Models in the Diagnosis of Rare Diseases](https://arxiv.org/pdf/2505.17065)
*Valentina Carbonari, Pierangelo Veltri, Pietro Hiram Guzzi*

Main category: cs.CL

TL;DR: This survey explores how large language models (LLMs) are transforming rare disease research, focusing on textual data analysis, multimodal potential, and ethical challenges.


<details>
  <summary>Details</summary>
Motivation: To highlight the role of LLMs in advancing rare disease research by leveraging textual and potential multimodal data for diagnosis, treatment, and patient care.

Method: Review of foundational papers on LLM applications in medical information extraction, conversational agents, and diagnosis, alongside experimentation with LLMs and structured questionnaires.

Result: LLMs show promise in rare disease research, but challenges like data privacy and model transparency remain. Multimodal integration is identified as a future direction.

Conclusion: Future advancements in LLMs should focus on multimodal data integration to enhance rare disease understanding and clinical outcomes.

Abstract: Recent advances in artificial intelligence, particularly large language
models LLMs, have shown promising capabilities in transforming rare disease
research. This survey paper explores the integration of LLMs in the analysis of
rare diseases, highlighting significant strides and pivotal studies that
leverage textual data to uncover insights and patterns critical for diagnosis,
treatment, and patient care. While current research predominantly employs
textual data, the potential for multimodal data integration combining genetic,
imaging, and electronic health records stands as a promising frontier. We
review foundational papers that demonstrate the application of LLMs in
identifying and extracting relevant medical information, simulating intelligent
conversational agents for patient interaction, and enabling the formulation of
accurate and timely diagnoses. Furthermore, this paper discusses the challenges
and ethical considerations inherent in deploying LLMs, including data privacy,
model transparency, and the need for robust, inclusive data sets. As part of
this exploration, we present a section on experimentation that utilizes
multiple LLMs alongside structured questionnaires, specifically designed for
diagnostic purposes in the context of different diseases. We conclude with
future perspectives on the evolution of LLMs towards truly multimodal
platforms, which would integrate diverse data types to provide a more
comprehensive understanding of rare diseases, ultimately fostering better
outcomes in clinical settings.

</details>


### [35] [Unveil Multi-Picture Descriptions for Multilingual Mild Cognitive Impairment Detection via Contrastive Learning](https://arxiv.org/pdf/2505.17067)
*Kristin Qi, Jiali Cheng, Youxiang Zhu, Hadi Amiri, Xiaohui Liang*

Main category: cs.CL

TL;DR: A framework for detecting Mild Cognitive Impairment (MCI) in multilingual and multi-picture settings improves performance using contrastive learning, image modality, and a Product of Experts strategy.


<details>
  <summary>Details</summary>
Motivation: Prior work focused on English speakers and single pictures, leaving gaps for multilingual and multi-picture scenarios. The TAUKDIAL-2024 challenge addresses these gaps.

Method: The framework includes supervised contrastive learning, image modality integration, and a Product of Experts (PoE) strategy to reduce spurious correlations.

Result: Achieved a +7.1% UAR (68.1% to 75.2%) and +2.9% F1 score (80.6% to 83.5%) over the text unimodal baseline. Contrastive learning benefits text modality more than speech.

Conclusion: The framework effectively improves MCI detection in multilingual and multi-picture contexts, demonstrating its practical utility.

Abstract: Detecting Mild Cognitive Impairment from picture descriptions is critical yet
challenging, especially in multilingual and multiple picture settings. Prior
work has primarily focused on English speakers describing a single picture
(e.g., the 'Cookie Theft'). The TAUKDIAL-2024 challenge expands this scope by
introducing multilingual speakers and multiple pictures, which presents new
challenges in analyzing picture-dependent content. To address these challenges,
we propose a framework with three components: (1) enhancing discriminative
representation learning via supervised contrastive learning, (2) involving
image modality rather than relying solely on speech and text modalities, and
(3) applying a Product of Experts (PoE) strategy to mitigate spurious
correlations and overfitting. Our framework improves MCI detection performance,
achieving a +7.1% increase in Unweighted Average Recall (UAR) (from 68.1% to
75.2%) and a +2.9% increase in F1 score (from 80.6% to 83.5%) compared to the
text unimodal baseline. Notably, the contrastive learning component yields
greater gains for the text modality compared to speech. These results highlight
our framework's effectiveness in multilingual and multi-picture MCI detection.

</details>


### [36] [Predictively Combatting Toxicity in Health-related Online Discussions through Machine Learning](https://arxiv.org/pdf/2505.17068)
*Jorge Paz-Ruza, Amparo Alonso-Betanzos, Bertha Guijarro-Berdiñas, Carlos Eiras-Franco*

Main category: cs.CL

TL;DR: The paper proposes a predictive approach to combat toxicity in health-related online discussions using Collaborative Filtering-based ML, achieving over 80% performance in predicting toxic interactions.


<details>
  <summary>Details</summary>
Motivation: User toxicity in health-related online discussions often leads to social conflict or promotes unscientific behavior, and reactive measures like flagging or removal are counterproductive.

Method: A Collaborative Filtering-based Machine Learning methodology is applied to predict toxicity in COVID-related Reddit conversations.

Result: The method achieves over 80% predictive performance, enabling prevention of toxic user-subcommunity pairings.

Conclusion: Predictive toxicity modeling is a viable alternative to reactive measures, improving platform and user experience.

Abstract: In health-related topics, user toxicity in online discussions frequently
becomes a source of social conflict or promotion of dangerous, unscientific
behaviour; common approaches for battling it include different forms of
detection, flagging and/or removal of existing toxic comments, which is often
counterproductive for platforms and users alike. In this work, we propose the
alternative of combatting user toxicity predictively, anticipating where a user
could interact toxically in health-related online discussions. Applying a
Collaborative Filtering-based Machine Learning methodology, we predict the
toxicity in COVID-related conversations between any user and subcommunity of
Reddit, surpassing 80% predictive performance in relevant metrics, and allowing
us to prevent the pairing of conflicting users and subcommunities.

</details>


### [37] [What's in a prompt? Language models encode literary style in prompt embeddings](https://arxiv.org/pdf/2505.17071)
*Raphaël Sarfati, Haley Moller, Toni J. B. Liu, Nicolas Boullé, Christopher Earls*

Main category: cs.CL

TL;DR: The paper explores how deep language model embeddings encode intangible and stylistic features from text prompts, revealing sophisticated information processing.


<details>
  <summary>Details</summary>
Motivation: To understand how transformer layers condense cumulative prompt information into embeddings, especially intangible and stylistic aspects.

Method: Analyze literary excerpts (10-100 tokens) in latent space, comparing separation and entanglement across authors.

Result: Deep embeddings capture stylistic features, with same-author excerpts more entangled than cross-author ones.

Conclusion: The findings highlight language models' advanced information compression, useful for authorship attribution and literary analysis.

Abstract: Large language models use high-dimensional latent spaces to encode and
process textual information. Much work has investigated how the conceptual
content of words translates into geometrical relationships between their vector
representations. Fewer studies analyze how the cumulative information of an
entire prompt becomes condensed into individual embeddings under the action of
transformer layers. We use literary pieces to show that information about
intangible, rather than factual, aspects of the prompt are contained in deep
representations. We observe that short excerpts (10 - 100 tokens) from
different novels separate in the latent space independently from what
next-token prediction they converge towards. Ensembles from books from the same
authors are much more entangled than across authors, suggesting that embeddings
encode stylistic features. This geometry of style may have applications for
authorship attribution and literary analysis, but most importantly reveals the
sophistication of information processing and compression accomplished by
language models.

</details>


### [38] [Mechanistic Interpretability of GPT-like Models on Summarization Tasks](https://arxiv.org/pdf/2505.17073)
*Anurag Mishra*

Main category: cs.CL

TL;DR: The paper introduces an interpretability framework for GPT-like models in summarization tasks, identifying key layers and attention heads that change during fine-tuning. Targeted LoRA adaptation of these circuits improves performance efficiently.


<details>
  <summary>Details</summary>
Motivation: To understand how GPT-like models adapt to summarization tasks, bridging the gap between black-box evaluation and mechanistic insights.

Method: Differential analysis of pre-trained and fine-tuned models, quantifying changes in attention patterns and activations to locate the 'summarization circuit.'

Result: Middle layers (2, 3, 5) show significant changes, with 62% of attention heads exhibiting decreased entropy. Targeted LoRA adaptation outperforms standard fine-tuning.

Conclusion: The work provides insights into neural networks' information selection and compression during summarization, enabling more efficient model adaptation.

Abstract: Mechanistic interpretability research seeks to reveal the inner workings of
large language models, yet most work focuses on classification or generative
tasks rather than summarization. This paper presents an interpretability
framework for analyzing how GPT-like models adapt to summarization tasks. We
conduct differential analysis between pre-trained and fine-tuned models,
quantifying changes in attention patterns and internal activations. By
identifying specific layers and attention heads that undergo significant
transformation, we locate the "summarization circuit" within the model
architecture. Our findings reveal that middle layers (particularly 2, 3, and 5)
exhibit the most dramatic changes, with 62% of attention heads showing
decreased entropy, indicating a shift toward focused information selection. We
demonstrate that targeted LoRA adaptation of these identified circuits achieves
significant performance improvement over standard LoRA fine-tuning while
requiring fewer training epochs. This work bridges the gap between black-box
evaluation and mechanistic understanding, providing insights into how neural
networks perform information selection and compression during summarization.

</details>


### [39] [Semi-Clairvoyant Scheduling of Speculative Decoding Requests to Minimize LLM Inference Latency](https://arxiv.org/pdf/2505.17074)
*Ruixiao Li, Fahao Chen, Peng Li*

Main category: cs.CL

TL;DR: LAPS-SD, a semi-clairvoyant scheduling algorithm, minimizes LLM inference latency by dynamically adapting to token acceptance rates and execution time uncertainties, outperforming existing methods by ~39%.


<details>
  <summary>Details</summary>
Motivation: Existing scheduling methods in speculative decoding systems inaccurately estimate execution time, relying solely on output length, ignoring token acceptance rates.

Method: Proposes LAPS-SD, which uses adaptive scheduling with priority queues and preemption, adjusting to dynamic token acceptance rates and stable execution time estimates.

Result: LAPS-SD reduces average inference latency by approximately 39% compared to state-of-the-art methods.

Conclusion: LAPS-SD effectively addresses execution time uncertainty in speculative decoding, significantly improving scheduling efficiency and reducing latency.

Abstract: Speculative decoding accelerates Large Language Model (LLM) inference by
employing a small speculative model (SSM) to generate multiple candidate tokens
and verify them using the LLM in parallel. This technique has been widely
integrated into LLM inference serving systems. However, inference requests
typically exhibit uncertain execution time, which poses a significant challenge
of efficiently scheduling requests in these systems. Existing work estimates
execution time based solely on predicted output length, which could be
inaccurate because execution time depends on both output length and token
acceptance rate of verification by the LLM. In this paper, we propose a
semi-clairvoyant request scheduling algorithm called
Least-Attained/Perceived-Service for Speculative Decoding (LAPS-SD). Given a
number of inference requests, LAPS-SD can effectively minimize average
inference latency by adaptively scheduling requests according to their features
during decoding. When the token acceptance rate is dynamic and execution time
is difficult to estimate, LAPS-SD maintains multiple priority queues and allows
request execution preemption across different queues. Once the token acceptance
rate becomes stable, LAPS-SD can accurately estimate the execution time and
schedule requests accordingly. Extensive experiments show that LAPS-SD reduces
inference latency by approximately 39\% compared to state-of-the-art scheduling
methods.

</details>


### [40] [Development and Validation of Engagement and Rapport Scales for Evaluating User Experience in Multimodal Dialogue Systems](https://arxiv.org/pdf/2505.17075)
*Fuma Kurata, Mao Saeki, Masaki Eguchi, Shungo Suzuki, Hiroaki Takatsu, Yoichi Matsuyama*

Main category: cs.CL

TL;DR: Developed and validated scales for engagement and rapport in multimodal dialogue systems for language learning, showing differences between human tutors and dialogue agents.


<details>
  <summary>Details</summary>
Motivation: To evaluate user experience quality in foreign language learning with multimodal dialogue systems, bridging educational psychology, social psychology, and second language acquisition theories.

Method: Designed scales based on theoretical frameworks, tested with 74 Japanese English learners through roleplay and discussion tasks with human tutors and a dialogue agent, followed by Cronbach's alpha and confirmatory factor analyses.

Result: Scales successfully captured differences in engagement and rapport between human tutors and dialogue agents, demonstrating validity and reliability.

Conclusion: The scales are effective tools for assessing dialogue experience quality in language learning contexts, highlighting distinctions between human and agent interactions.

Abstract: This study aimed to develop and validate two scales of engagement and rapport
to evaluate the user experience quality with multimodal dialogue systems in the
context of foreign language learning. The scales were designed based on
theories of engagement in educational psychology, social psychology, and second
language acquisition.Seventy-four Japanese learners of English completed
roleplay and discussion tasks with trained human tutors and a dialog agent.
After each dialogic task was completed, they responded to the scales of
engagement and rapport. The validity and reliability of the scales were
investigated through two analyses. We first conducted analysis of Cronbach's
alpha coefficient and a series of confirmatory factor analyses to test the
structural validity of the scales and the reliability of our designed items. We
then compared the scores of engagement and rapport between the dialogue with
human tutors and the one with a dialogue agent. The results revealed that our
scales succeeded in capturing the difference in the dialogue experience quality
between the human interlocutors and the dialogue agent from multiple
perspectives.

</details>


### [41] [GloSS over Toxicity: Understanding and Mitigating Toxicity in LLMs via Global Toxic Subspace](https://arxiv.org/pdf/2505.17078)
*Zenghao Duan, Zhiyi Yin, Zhichao Shi, Liang Pang, Shaoling Jing, Jiayi Wu, Yu Yan, Huawei Shen, Xueqi Cheng*

Main category: cs.CL

TL;DR: The paper introduces GloSS, a lightweight method for detoxifying LLMs by identifying and removing a global toxic subspace in FFN layers, achieving top performance without retraining.


<details>
  <summary>Details</summary>
Motivation: To address toxicity in LLMs more effectively by moving beyond localized toxic vectors to a global toxic subspace representation.

Method: Proposes GloSS, a four-stage method that identifies and suppresses the global toxic subspace in FFN layers.

Result: GloSS achieves state-of-the-art detoxification while preserving model capabilities, without needing large data or retraining.

Conclusion: The global toxic subspace is a more effective representation for detoxification, and GloSS offers a practical solution.

Abstract: This paper investigates the underlying mechanisms of toxicity generation in
Large Language Models (LLMs) and proposes an effective detoxification approach.
Prior work typically considers the Feed-Forward Network (FFN) as the main
source of toxicity, representing toxic regions as a set of toxic vectors or
layer-wise subspaces. However, our in-depth analysis reveals that the global
toxic subspace offers a more effective and comprehensive representation of
toxic region within the model. Building on this insight, we propose GloSS
(Global Toxic Subspace Suppression), a lightweight, four-stage method that
mitigates toxicity by identifying and removing the global toxic subspace from
the parameters of FFN. Experiments across a range of LLMs show that GloSS
achieves state-of-the-art detoxification performance while preserving the
models general capabilities, without requiring large-scale data or model
retraining.

</details>


### [42] [Not Minds, but Signs: Reframing LLMs through Semiotics](https://arxiv.org/pdf/2505.17080)
*Davide Picca*

Main category: cs.CL

TL;DR: The paper argues for viewing LLMs as semiotic agents manipulating signs, not cognitive systems, emphasizing their role in meaning-making and cultural processes.


<details>
  <summary>Details</summary>
Motivation: To challenge the anthropomorphic view of LLMs as cognitive systems and propose a semiotic framework for understanding their function in sign manipulation and meaning-making.

Method: Theoretical analysis and practical examples are used to demonstrate LLMs as semiotic agents, focusing on their outputs as interpretive acts.

Result: LLMs are reframed as participants in cultural processes, altering how meaning is produced and interpreted, without possessing cognitive abilities.

Conclusion: A semiotic perspective offers a more rigorous and ethically aware framework for studying LLMs, highlighting their role in the ecology of signs.

Abstract: This paper challenges the prevailing tendency to frame Large Language Models
(LLMs) as cognitive systems, arguing instead for a semiotic perspective that
situates these models within the broader dynamics of sign manipulation and
meaning-making. Rather than assuming that LLMs understand language or simulate
human thought, we propose that their primary function is to recombine,
recontextualize, and circulate linguistic forms based on probabilistic
associations. By shifting from a cognitivist to a semiotic framework, we avoid
anthropomorphism and gain a more precise understanding of how LLMs participate
in cultural processes, not by thinking, but by generating texts that invite
interpretation. Through theoretical analysis and practical examples, the paper
demonstrates how LLMs function as semiotic agents whose outputs can be treated
as interpretive acts, open to contextual negotiation and critical reflection.
We explore applications in literature, philosophy, education, and cultural
production, emphasizing how LLMs can serve as tools for creativity, dialogue,
and critical inquiry. The semiotic paradigm foregrounds the situated,
contingent, and socially embedded nature of meaning, offering a more rigorous
and ethically aware framework for studying and using LLMs. Ultimately, this
approach reframes LLMs as technological participants in an ongoing ecology of
signs. They do not possess minds, but they alter how we read, write, and make
meaning, compelling us to reconsider the foundations of language,
interpretation, and the role of artificial systems in the production of
knowledge.

</details>


### [43] [GemMaroc: Unlocking Darija Proficiency in LLMs with Minimal Data](https://arxiv.org/pdf/2505.17082)
*Abderrahman Skiredj, Ferdaous Azhari, Houdaifa Atou, Nouamane Tazi, Ismail Berrada*

Main category: cs.CL

TL;DR: A quality-over-quantity alignment strategy improves Moroccan Arabic (Darija) fluency in LLMs without compromising reasoning skills, achieving significant performance gains with minimal compute.


<details>
  <summary>Details</summary>
Motivation: Open-source LLMs marginalize Moroccan Arabic, requiring inefficient adapters or sacrificing reasoning. This work aims to make LLMs inclusive for Darija while maintaining performance.

Method: Translate instruction suites (LIMA 1K, DEITA 6K, TULU 50K) into Darija, preserve some English, and add specialized prompts. Use LoRA-tuned Gemma models (3-4B, 3-27B) trained on mixed instructions.

Result: DarijaMMLU scores improved from 32.8 to 47.5 with no English regression. GemMaroc-27B matches/exceeds benchmarks (61.6 on DarijaMMLU, 60.5 on HellaSwag) and retains strong math/reasoning.

Conclusion: The approach demonstrates a sustainable, inclusive pathway for Darija-centric LLMs, achieving high performance with minimal compute (48 GPU.h). Code and data are released for broader applications.

Abstract: Open-source large language models (LLMs) still marginalise Moroccan Arabic
(Darija), forcing practitioners either to bolt on heavyweight Arabic adapters
or to sacrifice the very reasoning skills that make LLMs useful. We show that a
rigorously quality-over-quantity alignment strategy can surface fluent Darija
while safeguarding the backbone s cross-lingual reasoning at a sliver of the
usual compute. We translate three compact instruction suites LIMA 1 K, DEITA 6
K and TULU 50 K into Darija, preserve 20 of the English originals, and add
mathematics, coding and scientific prompts. A LoRA-tuned Gemma 3-4B trained on
5 K mixed instructions lifts DarijaMMLU from 32.8 to 42.7 ; adding the
reasoning-dense TULU portion pushes it to 47.5 with no English regression.
Scaling the identical recipe to Gemma 3-27B produces GemMaroc-27B, which
matches Atlas-Chat on DarijaMMLU (61.6 ) and leaps ahead on Darija commonsense,
scoring 60.5 on HellaSwag versus Atlas-Chat s 48.4 . Crucially, GemMaroc
retains Gemma-27B s strong maths and general-reasoning ability, showing only
minimal movement on GSM8K and English benchmarks. The entire model is trained
in just 48 GPU.h, underscoring a Green AI pathway to inclusive, sustainable
language technology. We release code, data and checkpoints to spur
Darija-centric applications in education, public services and everyday digital
interaction.

</details>


### [44] [Scale-invariant Attention](https://arxiv.org/pdf/2505.17083)
*Ben Anson, Xi Wang, Laurence Aitchison*

Main category: cs.CL

TL;DR: A new scale-invariant attention mechanism improves LLM generalization from short to long contexts.


<details>
  <summary>Details</summary>
Motivation: Addressing the challenge of generalizing attention mechanisms from short training contexts to longer inference contexts.

Method: Proposing two conditions for effective long-context attention: scale-invariant total attention and sparsity. A position-dependent transformation of attention logits under Gaussian assumptions.

Result: Improved validation loss in zero-shot generalization and effective long-context retrieval.

Conclusion: The scale-invariant attention mechanism successfully enhances LLM performance on longer contexts.

Abstract: One persistent challenge in LLM research is the development of attention
mechanisms that are able to generalise from training on shorter contexts to
inference on longer contexts. We propose two conditions that we expect all
effective long context attention mechanisms to have: scale-invariant total
attention, and scale-invariant attention sparsity. Under a Gaussian assumption,
we show that a simple position-dependent transformation of the attention logits
is sufficient for these conditions to hold. Experimentally we find that the
resulting scale-invariant attention scheme gives considerable benefits in terms
of validation loss when zero-shot generalising from training on short contexts
to validation on longer contexts, and is effective at long-context retrieval.

</details>


### [45] [Reinforcing Question Answering Agents with Minimalist Policy Gradient Optimization](https://arxiv.org/pdf/2505.17086)
*Yihong Wu, Liheng Ma, Muzhi Li, Jiaming Zhou, Jianye Hao, Ho-fung Leung, Irwin King, Yingxue Zhang, Jian-Yun Nie*

Main category: cs.CL

TL;DR: Mujica-MyGO improves multi-hop QA for LLMs by combining question decomposition (Mujica) and a novel RL method (MyGO), outperforming existing approaches.


<details>
  <summary>Details</summary>
Motivation: LLMs struggle with factual QA due to hallucination and limited reasoning. Retrieval-augmented methods are constrained by in-context learning.

Method: Mujica decomposes questions into subquestions (DAG), while MyGO uses MLE-based RL for stable training without gradients.

Result: Mujica-MyGO enhances multi-hop QA performance across datasets, scalable and resource-efficient.

Conclusion: The framework addresses LLM limitations in complex QA, offering a robust solution.

Abstract: Large Language Models (LLMs) have demonstrated remarkable versatility, due to
the lack of factual knowledge, their application to Question Answering (QA)
tasks remains hindered by hallucination.
  While Retrieval-Augmented Generation mitigates these issues by integrating
external knowledge, existing approaches rely heavily on in-context learning,
whose performance is constrained by the fundamental reasoning capabilities of
LLMs.
  In this paper, we propose Mujica, a Multi-hop Joint Intelligence for Complex
Question Answering, comprising a planner that decomposes questions into a
directed acyclic graph of subquestions and a worker that resolves questions via
retrieval and reasoning. Additionally, we introduce MyGO (Minimalist policy
Gradient Optimization), a novel reinforcement learning method that replaces
traditional policy gradient updates with Maximum Likelihood Estimation (MLE) by
sampling trajectories from an asymptotically optimal policy. MyGO eliminates
the need for gradient rescaling and reference models, ensuring stable and
efficient training.
  Empirical results across multiple datasets demonstrate the effectiveness of
Mujica-MyGO in enhancing multi-hop QA performance for various LLMs, offering a
scalable and resource-efficient solution for complex QA tasks.

</details>


### [46] [Informatics for Food Processing](https://arxiv.org/pdf/2505.17087)
*Gordana Ispirova, Michael Sebek, Giulia Menichetti*

Main category: cs.CL

TL;DR: The paper discusses the evolution and health impacts of food processing, leveraging AI and data science to improve classification methods like NOVA and Nutri-Score. It introduces computational tools like FoodProX and BERT for better accuracy and scalability.


<details>
  <summary>Details</summary>
Motivation: To address subjectivity and reproducibility issues in traditional food classification frameworks, which hinder research and policy-making.

Method: Uses machine learning (random forest, BERT, BioBERT) and multimodal AI to analyze food data, including nutrient composition and ingredient lists.

Result: Develops FoodProX for continuous processing scores and demonstrates scalable classification using Open Food Facts.

Conclusion: AI-driven methods offer a more reliable and scalable approach to food processing assessment, benefiting public health and research.

Abstract: This chapter explores the evolution, classification, and health implications
of food processing, while emphasizing the transformative role of machine
learning, artificial intelligence (AI), and data science in advancing food
informatics. It begins with a historical overview and a critical review of
traditional classification frameworks such as NOVA, Nutri-Score, and SIGA,
highlighting their strengths and limitations, particularly the subjectivity and
reproducibility challenges that hinder epidemiological research and public
policy. To address these issues, the chapter presents novel computational
approaches, including FoodProX, a random forest model trained on nutrient
composition data to infer processing levels and generate a continuous FPro
score. It also explores how large language models like BERT and BioBERT can
semantically embed food descriptions and ingredient lists for predictive tasks,
even in the presence of missing data. A key contribution of the chapter is a
novel case study using the Open Food Facts database, showcasing how multimodal
AI models can integrate structured and unstructured data to classify foods at
scale, offering a new paradigm for food processing assessment in public health
and research.

</details>


### [47] [Trust Me, I Can Handle It: Self-Generated Adversarial Scenario Extrapolation for Robust Language Models](https://arxiv.org/pdf/2505.17089)
*Md Rafi Ur Rashid, Vishnu Asutosh Dasu, Ye Wang, Gang Tan, Shagufta Mehnaz*

Main category: cs.CL

TL;DR: ASE enhances LLM robustness and seamlessness by guiding models to self-generate adversarial scenarios and defensive strategies, outperforming existing defenses.


<details>
  <summary>Details</summary>
Motivation: Addressing the limitations of current LLM defenses, which are narrow or rigid, ASE aims to improve both safety and user experience.

Method: ASE uses Chain-of-Thought reasoning to make LLMs contemplate adversarial scenarios and devise defenses before responding.

Result: ASE achieves near-zero jailbreak success, minimal toxicity, <4% rejections, and outperforms six defenses in robustness-seamlessness trade-offs.

Conclusion: ASE introduces a new paradigm for secure and natural human-AI interaction by integrating adversarial perception into LLM cognition.

Abstract: Large Language Models (LLMs) exhibit impressive capabilities, but remain
susceptible to a growing spectrum of safety risks, including jailbreaks, toxic
content, hallucinations, and bias. Existing defenses often address only a
single threat type or resort to rigid outright rejection, sacrificing user
experience and failing to generalize across diverse and novel attacks. This
paper introduces Adversarial Scenario Extrapolation (ASE), a novel
inference-time computation framework that leverages Chain-of-Thought (CoT)
reasoning to simultaneously enhance LLM robustness and seamlessness. ASE guides
the LLM through a self-generative process of contemplating potential
adversarial scenarios and formulating defensive strategies before generating a
response to the user query. Comprehensive evaluation on four adversarial
benchmarks with four latest LLMs shows that ASE achieves near-zero jailbreak
attack success rates and minimal toxicity, while slashing outright rejections
to <4%. ASE outperforms six state-of-the-art defenses in
robustness-seamlessness trade-offs, with 92-99% accuracy on adversarial Q&A and
4-10x lower bias scores. By transforming adversarial perception into an
intrinsic cognitive process, ASE sets a new paradigm for secure and natural
human-AI interaction.

</details>


### [48] [Are LLMs reliable? An exploration of the reliability of large language models in clinical note generation](https://arxiv.org/pdf/2505.17095)
*Kristine Ann M. Carandang, Jasper Meynard P. Araña, Ethan Robert A. Casin, Christopher P. Monterola, Daniel Stanley Y. Tan, Jesus Felix B. Valenzuela, Christian M. Alis*

Main category: cs.CL

TL;DR: The study evaluates 12 LLMs for clinical note generation, finding Meta's Llama 70B and Mistral's Small model most reliable. Open-weight models are recommended for local deployment to ensure privacy and efficiency.


<details>
  <summary>Details</summary>
Motivation: To address challenges in incorporating LLM-driven CNG systems into clinical processes due to variability and privacy concerns.

Method: Evaluated 12 LLMs for consistency, semantic equivalence, and correctness in generating clinical notes.

Result: LLMs showed semantic consistency, with Meta's Llama 70B and Mistral's Small model performing best.

Conclusion: Open-weight models like Llama 70B and Mistral Small are recommended for local deployment in CNG to ensure compliance and efficiency.

Abstract: Due to the legal and ethical responsibilities of healthcare providers (HCPs)
for accurate documentation and protection of patient data privacy, the natural
variability in the responses of large language models (LLMs) presents
challenges for incorporating clinical note generation (CNG) systems, driven by
LLMs, into real-world clinical processes. The complexity is further amplified
by the detailed nature of texts in CNG. To enhance the confidence of HCPs in
tools powered by LLMs, this study evaluates the reliability of 12 open-weight
and proprietary LLMs from Anthropic, Meta, Mistral, and OpenAI in CNG in terms
of their ability to generate notes that are string equivalent (consistency
rate), have the same meaning (semantic consistency) and are correct (semantic
similarity), across several iterations using the same prompt. The results show
that (1) LLMs from all model families are stable, such that their responses are
semantically consistent despite being written in various ways, and (2) most of
the LLMs generated notes close to the corresponding notes made by experts.
Overall, Meta's Llama 70B was the most reliable, followed by Mistral's Small
model. With these findings, we recommend the local deployment of these
relatively smaller open-weight models for CNG to ensure compliance with data
privacy regulations, as well as to improve the efficiency of HCPs in clinical
documentation.

</details>


### [49] [TACO: Enhancing Multimodal In-context Learning via Task Mapping-Guided Sequence Configuration](https://arxiv.org/pdf/2505.17098)
*Yanshu Li, Tian Yun, Jianjiang Yang, Pinyuan Feng, Jinfa Huang, Ruixiang Tang*

Main category: cs.CL

TL;DR: The paper introduces TACO, a lightweight transformer model with task-aware attention, to improve multimodal in-context learning (ICL) by dynamically configuring input sequences based on task mapping insights.


<details>
  <summary>Details</summary>
Motivation: Multimodal ICL's effectiveness is sensitive to input sequence quality, and there's limited understanding of how large vision-language models (LVLMs) utilize these sequences during inference.

Method: The authors interpret multimodal ICL through task mapping and propose TACO, a transformer-based model with task-aware attention to dynamically configure in-context sequences.

Result: Experiments on five LVLMs and nine datasets show TACO consistently outperforms baselines in diverse ICL tasks.

Conclusion: Task mapping is a valuable perspective for interpreting and enhancing multimodal ICL, as demonstrated by TACO's success.

Abstract: Multimodal in-context learning (ICL) has emerged as a key mechanism for
harnessing the capabilities of large vision-language models (LVLMs). However,
its effectiveness remains highly sensitive to the quality of input in-context
sequences, particularly for tasks involving complex reasoning or open-ended
generation. A major limitation is our limited understanding of how LVLMs
actually exploit these sequences during inference. To bridge this gap, we
systematically interpret multimodal ICL through the lens of task mapping, which
reveals how local and global relationships within and among demonstrations
guide model reasoning. Building on this insight, we present TACO, a lightweight
transformer-based model equipped with task-aware attention that dynamically
configures in-context sequences. By injecting task-mapping signals into the
autoregressive decoding process, TACO creates a bidirectional synergy between
sequence construction and task reasoning. Experiments on five LVLMs and nine
datasets demonstrate that TACO consistently surpasses baselines across diverse
ICL tasks. These results position task mapping as a valuable perspective for
interpreting and improving multimodal ICL.

</details>


### [50] [Learning Interpretable Representations Leads to Semantically Faithful EEG-to-Text Generation](https://arxiv.org/pdf/2505.17099)
*Xiaozhao Liu, Dinggang Shen, Xihui Liu*

Main category: cs.CL

TL;DR: The paper addresses hallucination issues in EEG-to-text decoding by proposing GLIM, a model that reframes the task as semantic summarization and improves grounding under small-scale data.


<details>
  <summary>Details</summary>
Motivation: To ensure EEG-to-text decoding outputs are semantically grounded rather than hallucinated by generative models.

Method: Proposes GLIM, which learns informative EEG representations for semantic summarization, tested on the ZuCo dataset.

Result: GLIM generates fluent, EEG-grounded sentences and supports robust evaluation via retrieval and zero-shot classification.

Conclusion: GLIM and new evaluation protocols provide a foundation for reliable, scalable generative brain decoding.

Abstract: Pretrained generative models have opened new frontiers in brain decoding by
enabling the synthesis of realistic texts and images from non-invasive brain
recordings. However, the reliability of such outputs remains
questionable--whether they truly reflect semantic activation in the brain, or
are merely hallucinated by the powerful generative models. In this paper, we
focus on EEG-to-text decoding and address its hallucination issue through the
lens of posterior collapse. Acknowledging the underlying mismatch in
information capacity between EEG and text, we reframe the decoding task as
semantic summarization of core meanings rather than previously verbatim
reconstruction of stimulus texts. To this end, we propose the Generative
Language Inspection Model (GLIM), which emphasizes learning informative and
interpretable EEG representations to improve semantic grounding under
heterogeneous and small-scale data conditions. Experiments on the public ZuCo
dataset demonstrate that GLIM consistently generates fluent, EEG-grounded
sentences without teacher forcing. Moreover, it supports more robust evaluation
beyond text similarity, through EEG-text retrieval and zero-shot semantic
classification across sentiment categories, relation types, and corpus topics.
Together, our architecture and evaluation protocols lay the foundation for
reliable and scalable benchmarking in generative brain decoding.

</details>


### [51] [Any Large Language Model Can Be a Reliable Judge: Debiasing with a Reasoning-based Bias Detector](https://arxiv.org/pdf/2505.17100)
*Haoyan Yang, Runxue Bao, Cao Xiao, Jun Ma, Parminder Bhatia, Shangqian Gao, Taha Kass-Hout*

Main category: cs.CL

TL;DR: The paper introduces the Reasoning-based Bias Detector (RBD), a plug-in module to identify and correct biases in LLM-as-a-Judge evaluations, improving accuracy and consistency.


<details>
  <summary>Details</summary>
Motivation: Existing methods for mitigating biases in LLM evaluators are limited, either failing to address rooted biases or being inapplicable to closed-source models.

Method: RBD operates externally, detecting biases and providing structured reasoning for self-correction. A pipeline includes biased dataset construction, supervision collection, fine-tuning, and integration with LLM evaluators.

Result: RBD models (1.5B to 14B) show consistent improvements, e.g., RBD-8B boosts accuracy by 18.5% and consistency by 10.9%, outperforming baselines.

Conclusion: RBD is effective, scalable, and generalizes well across biases and domains, offering a robust solution for bias mitigation in LLM evaluations.

Abstract: LLM-as-a-Judge has emerged as a promising tool for automatically evaluating
generated outputs, but its reliability is often undermined by potential biases
in judgment. Existing efforts to mitigate these biases face key limitations:
in-context learning-based methods fail to address rooted biases due to the
evaluator's limited capacity for self-reflection, whereas fine-tuning is not
applicable to all evaluator types, especially closed-source models. To address
this challenge, we introduce the Reasoning-based Bias Detector (RBD), which is
a plug-in module that identifies biased evaluations and generates structured
reasoning to guide evaluator self-correction. Rather than modifying the
evaluator itself, RBD operates externally and engages in an iterative process
of bias detection and feedback-driven revision. To support its development, we
design a complete pipeline consisting of biased dataset construction,
supervision collection, distilled reasoning-based fine-tuning of RBD, and
integration with LLM evaluators. We fine-tune four sizes of RBD models, ranging
from 1.5B to 14B, and observe consistent performance improvements across all
scales. Experimental results on 4 bias types--verbosity, position, bandwagon,
and sentiment--evaluated using 8 LLM evaluators demonstrate RBD's strong
effectiveness. For example, the RBD-8B model improves evaluation accuracy by an
average of 18.5% and consistency by 10.9%, and surpasses prompting-based
baselines and fine-tuned judges by 12.8% and 17.2%, respectively. These results
highlight RBD's effectiveness and scalability. Additional experiments further
demonstrate its strong generalization across biases and domains, as well as its
efficiency.

</details>


### [52] [An approach to identify the most semantically informative deep representations of text and images](https://arxiv.org/pdf/2505.17101)
*Santiago Acevedo, Andrea Mascaretti, Riccardo Rende, Matéo Mahaut, Marco Baroni, Alessandro Laio*

Main category: cs.CL

TL;DR: The paper investigates how deep neural networks encode semantically related data across domains (e.g., text and images) by measuring information content in representations, identifying 'semantic' layers in LLMs and vision transformers.


<details>
  <summary>Details</summary>
Motivation: To quantitatively study how semantically related data (e.g., translated sentences or images and captions) develop similar representations in neural networks.

Method: Measure relative information content in representations, analyze token-level encoding in LLMs (e.g., DeepSeek-V3, Llama3.1-8B) and vision transformers, and identify semantic layers.

Result: Larger LLMs extract more general information; semantic information is spread across tokens with long-distance correlations and causal asymmetry. Image-text representations show model-dependent asymmetries.

Conclusion: Semantic layers in LLMs and vision transformers encode shared information across domains, with size and architecture influencing information extraction and asymmetry.

Abstract: Deep neural networks are known to develop similar representations for
semantically related data, even when they belong to different domains, such as
an image and its description, or the same text in different languages. We
present a method for quantitatively investigating this phenomenon by measuring
the relative information content of the representations of semantically related
data and probing how it is encoded into multiple tokens of large language
models (LLMs) and vision transformers. Looking first at how LLMs process pairs
of translated sentences, we identify inner ``semantic'' layers containing the
most language-transferable information. We find moreover that, on these layers,
a larger LLM (DeepSeek-V3) extracts significantly more general information than
a smaller one (Llama3.1-8B). Semantic information is spread across many tokens
and it is characterized by long-distance correlations between tokens and by a
causal left-to-right (i.e., past-future) asymmetry. We also identify layers
encoding semantic information within visual transformers. We show that caption
representations in the semantic layers of LLMs predict visual representations
of the corresponding images. We observe significant and model-dependent
information asymmetries between image and text representations.

</details>


### [53] [BanglaByT5: Byte-Level Modelling for Bangla](https://arxiv.org/pdf/2505.17102)
*Pramit Bhattacharyya, Arnab Bhattacharya*

Main category: cs.CL

TL;DR: BanglaByT5 is a byte-level encoder-decoder model for Bangla, outperforming larger models in NLP tasks due to its tailored design for morphologically rich languages.


<details>
  <summary>Details</summary>
Motivation: Traditional tokenizers like BPE and SentencePiece fail to capture nuances in Bangla, necessitating a specialized model.

Method: Built on ByT5, BanglaByT5 is pre-trained on a 14GB curated corpus of literary and newspaper articles.

Result: It achieves competitive performance in zero-shot and supervised evaluations, surpassing multilingual models.

Conclusion: Byte-level modeling is effective for Bangla, making BanglaByT5 a lightweight yet powerful tool for NLP.

Abstract: Large language models (LLMs) have achieved remarkable success across various
natural language processing tasks. However, most LLM models use traditional
tokenizers like BPE and SentencePiece, which fail to capture the finer nuances
of a morphologically rich language like Bangla (Bengali). In this work, we
introduce BanglaByT5, the first byte-level encoder-decoder model explicitly
tailored for Bangla. Built upon a small variant of Googles ByT5 architecture,
BanglaByT5 is pre-trained on a 14GB curated corpus combining high-quality
literary and newspaper articles. Through zeroshot and supervised evaluations
across generative and classification tasks, BanglaByT5 demonstrates competitive
performance, surpassing several multilingual and larger models. Our findings
highlight the efficacy of byte-level modelling for morphologically rich
languages and highlight BanglaByT5 potential as a lightweight yet powerful tool
for Bangla NLP, particularly in both resource-constrained and scalable
environments.

</details>


### [54] [Forging Time Series with Language: A Large Language Model Approach to Synthetic Data Generation](https://arxiv.org/pdf/2505.17103)
*Cécile Rousseau, Tobia Boschi, Giandomenico Cornacchia, Dhaval Salwala, Alessandra Pascale, Juan Bernabe Moreno*

Main category: cs.CL

TL;DR: SDForger is a framework for generating high-quality multivariate time series using LLMs, outperforming existing models in similarity and forecasting tasks.


<details>
  <summary>Details</summary>
Motivation: To provide a flexible and efficient method for synthetic time series generation with minimal samples and low-computation fine-tuning of autoregressive LLMs.

Method: Transforms univariate/multivariate signals into tabular embeddings, encodes them into text for LLM fine-tuning, and decodes generated text back into synthetic time series.

Result: Outperforms existing generative models in similarity evaluations and downstream forecasting tasks.

Conclusion: SDForger enables multimodal modeling and integration of time series with textual information, with plans to open-source the code.

Abstract: SDForger is a flexible and efficient framework for generating high-quality
multivariate time series using LLMs. Leveraging a compact data representation,
SDForger provides synthetic time series generation from a few samples and
low-computation fine-tuning of any autoregressive LLM. Specifically, the
framework transforms univariate and multivariate signals into tabular
embeddings, which are then encoded into text and used to fine-tune the LLM. At
inference, new textual embeddings are sampled and decoded into synthetic time
series that retain the original data's statistical properties and temporal
dynamics. Across a diverse range of datasets, SDForger outperforms existing
generative models in many scenarios, both in similarity-based evaluations and
downstream forecasting tasks. By enabling textual conditioning in the
generation process, SDForger paves the way for multimodal modeling and the
streamlined integration of time series with textual information. SDForger
source code will be open-sourced soon.

</details>


### [55] [RRTL: Red Teaming Reasoning Large Language Models in Tool Learning](https://arxiv.org/pdf/2505.17106)
*Yifei Liu, Yu Cui, Haibin Zhang*

Main category: cs.CL

TL;DR: The paper introduces RRTL, a red teaming approach to evaluate the safety of reasoning LLMs (RLLMs) in tool learning, uncovering deceptive risks and vulnerabilities.


<details>
  <summary>Details</summary>
Motivation: To address the underexplored safety risks of RLLMs in tool learning, particularly deceptive threats and vulnerabilities.

Method: Proposes RRTL with two strategies: identifying deceptive threats and using Chain-of-Thought prompting to force tool invocation. Includes a benchmark for traditional LLMs.

Result: RLLMs show stronger safety than traditional LLMs but still have disparities, exhibit deceptive risks, and reveal multilingual vulnerabilities under CoT prompting.

Conclusion: The study highlights critical safety issues in RLLMs during tool learning and provides insights for improving their security.

Abstract: While tool learning significantly enhances the capabilities of large language
models (LLMs), it also introduces substantial security risks. Prior research
has revealed various vulnerabilities in traditional LLMs during tool learning.
However, the safety of newly emerging reasoning LLMs (RLLMs), such as
DeepSeek-R1, in the context of tool learning remains underexplored. To bridge
this gap, we propose RRTL, a red teaming approach specifically designed to
evaluate RLLMs in tool learning. It integrates two novel strategies: (1) the
identification of deceptive threats, which evaluates the model's behavior in
concealing the usage of unsafe tools and their potential risks; and (2) the use
of Chain-of-Thought (CoT) prompting to force tool invocation. Our approach also
includes a benchmark for traditional LLMs. We conduct a comprehensive
evaluation on seven mainstream RLLMs and uncover three key findings: (1) RLLMs
generally achieve stronger safety performance than traditional LLMs, yet
substantial safety disparities persist across models; (2) RLLMs can pose
serious deceptive risks by frequently failing to disclose tool usage and to
warn users of potential tool output risks; (3) CoT prompting reveals
multi-lingual safety vulnerabilities in RLLMs. Our work provides important
insights into enhancing the security of RLLMs in tool learning.

</details>


### [56] [Multi-Modality Expansion and Retention for LLMs through Parameter Merging and Decoupling](https://arxiv.org/pdf/2505.17110)
*Junlin Li, Guodong DU, Jing Li, Sim Kuan Goh, Wenya Wang, Yequan Wang, Fangming Liu, Ho-Kin Tang, Saleh Alharbi, Daojing He, Min Zhang*

Main category: cs.CL

TL;DR: MMER is a training-free method for expanding LLMs into MLLMs by reusing encoders and merging LLM parameters, reducing conflicts and preserving performance.


<details>
  <summary>Details</summary>
Motivation: Current MLLM fine-tuning is resource-intensive and inflexible, requiring retraining from scratch.

Method: MMER reuses MLLMs' encoders, merges LLM parameters, and uses binary masks to decouple modality-specific parameters.

Result: MMER retains 99% of original performance and significantly mitigates catastrophic forgetting.

Conclusion: MMER offers an efficient, training-free solution for expanding LLMs' multimodal capabilities.

Abstract: Fine-tuning Large Language Models (LLMs) with multimodal encoders on
modality-specific data expands the modalities that LLMs can handle, leading to
the formation of Multimodal LLMs (MLLMs). However, this paradigm heavily relies
on resource-intensive and inflexible fine-tuning from scratch with new
multimodal data. In this paper, we propose MMER (Multi-modality Expansion and
Retention), a training-free approach that integrates existing MLLMs for
effective multimodal expansion while retaining their original performance.
Specifically, MMER reuses MLLMs' multimodal encoders while merging their LLM
parameters. By comparing original and merged LLM parameters, MMER generates
binary masks to approximately separate LLM parameters for each modality. These
decoupled parameters can independently process modality-specific inputs,
reducing parameter conflicts and preserving original MLLMs' fidelity. MMER can
also mitigate catastrophic forgetting by applying a similar process to MLLMs
fine-tuned on new tasks. Extensive experiments show significant improvements
over baselines, proving that MMER effectively expands LLMs' multimodal
capabilities while retaining 99% of the original performance, and also markedly
mitigates catastrophic forgetting.

</details>


### [57] [Cultural Value Alignment in Large Language Models: A Prompt-based Analysis of Schwartz Values in Gemini, ChatGPT, and DeepSeek](https://arxiv.org/pdf/2505.17112)
*Robin Segerer*

Main category: cs.CL

TL;DR: The study analyzes cultural value alignment in LLMs (Gemini, ChatGPT, DeepSeek) using Schwartz's framework, finding prosocial tendencies but distinct cultural biases, especially in DeepSeek. Proposes solutions for fairness.


<details>
  <summary>Details</summary>
Motivation: To understand how LLMs reflect cultural biases and whether training data influences value prioritization, aiming to address fairness and neutrality in AI.

Method: Used the 40-item Portrait Values Questionnaire and Bayesian ordinal regression to compare value preferences across models, focusing on DeepSeek's Chinese-language training.

Result: All models prioritized self-transcendence values, but DeepSeek uniquely downplayed self-enhancement, aligning with collectivist culture. Highlights cultural biases in LLMs.

Conclusion: LLMs reflect cultural biases, not universal ethics. Proposes multi-perspective reasoning and dynamic contextualization for fairer AI alignment.

Abstract: This study examines cultural value alignment in large language models (LLMs)
by analyzing how Gemini, ChatGPT, and DeepSeek prioritize values from
Schwartz's value framework. Using the 40-item Portrait Values Questionnaire, we
assessed whether DeepSeek, trained on Chinese-language data, exhibits distinct
value preferences compared to Western models. Results of a Bayesian ordinal
regression model show that self-transcendence values (e.g., benevolence,
universalism) were highly prioritized across all models, reflecting a general
LLM tendency to emphasize prosocial values. However, DeepSeek uniquely
downplayed self-enhancement values (e.g., power, achievement) compared to
ChatGPT and Gemini, aligning with collectivist cultural tendencies. These
findings suggest that LLMs reflect culturally situated biases rather than a
universal ethical framework. To address value asymmetries in LLMs, we propose
multi-perspective reasoning, self-reflective feedback, and dynamic
contextualization. This study contributes to discussions on AI fairness,
cultural neutrality, and the need for pluralistic AI alignment frameworks that
integrate diverse moral perspectives.

</details>


### [58] [Comparative Evaluation of Prompting and Fine-Tuning for Applying Large Language Models to Grid-Structured Geospatial Data](https://arxiv.org/pdf/2505.17116)
*Akash Dhruv, Yangxinyu Xie, Jordan Branham, Tanwi Mallick*

Main category: cs.CL

TL;DR: A study compares zero-shot prompting and fine-tuning of LLMs for interpreting grid-structured geospatial data, showing fine-tuning's advantages.


<details>
  <summary>Details</summary>
Motivation: To assess the effectiveness of LLMs in handling structured geospatial data and compare zero-shot prompting with fine-tuning.

Method: Evaluated a base model using structured prompting and a fine-tuned variant trained on user-assistant interactions.

Result: Fine-tuning outperforms zero-shot prompting in structured geospatial and temporal reasoning.

Conclusion: Fine-tuning LLMs enhances their performance in interpreting structured geospatial data compared to zero-shot approaches.

Abstract: This paper presents a comparative study of large language models (LLMs) in
interpreting grid-structured geospatial data. We evaluate the performance of a
base model through structured prompting and contrast it with a fine-tuned
variant trained on a dataset of user-assistant interactions. Our results
highlight the strengths and limitations of zero-shot prompting and demonstrate
the benefits of fine-tuning for structured geospatial and temporal reasoning.

</details>


### [59] [From Tokens to Thoughts: How LLMs and Humans Trade Compression for Meaning](https://arxiv.org/pdf/2505.17117)
*Chen Shani, Dan Jurafsky, Yann LeCun, Ravid Shwartz-Ziv*

Main category: cs.CL

TL;DR: The paper explores whether LLMs' internal representations balance compression and semantic fidelity like humans, finding key divergences in their strategies.


<details>
  <summary>Details</summary>
Motivation: To understand if LLMs achieve human-like semantic compression and fidelity in knowledge organization.

Method: An information-theoretic framework based on Rate-Distortion Theory and the Information Bottleneck principle is used to analyze LLM token embeddings against human categorization benchmarks.

Result: LLMs form broad categories aligned with humans but miss fine-grained distinctions. They favor aggressive compression, unlike humans who prioritize nuance.

Conclusion: The study highlights differences between AI and human cognition, suggesting pathways for more human-aligned LLMs.

Abstract: Humans organize knowledge into compact categories through semantic
compression by mapping diverse instances to abstract representations while
preserving meaning (e.g., robin and blue jay are both birds; most birds can
fly). These concepts reflect a trade-off between expressive fidelity and
representational simplicity. Large Language Models (LLMs) demonstrate
remarkable linguistic abilities, yet whether their internal representations
strike a human-like trade-off between compression and semantic fidelity is
unclear. We introduce a novel information-theoretic framework, drawing from
Rate-Distortion Theory and the Information Bottleneck principle, to
quantitatively compare these strategies. Analyzing token embeddings from a
diverse suite of LLMs against seminal human categorization benchmarks, we
uncover key divergences. While LLMs form broad conceptual categories that align
with human judgment, they struggle to capture the fine-grained semantic
distinctions crucial for human understanding. More fundamentally, LLMs
demonstrate a strong bias towards aggressive statistical compression, whereas
human conceptual systems appear to prioritize adaptive nuance and contextual
richness, even if this results in lower compressional efficiency by our
measures. These findings illuminate critical differences between current AI and
human cognitive architectures, guiding pathways toward LLMs with more
human-aligned conceptual representations.

</details>


### [60] [After Retrieval, Before Generation: Enhancing the Trustworthiness of Large Language Models in RAG](https://arxiv.org/pdf/2505.17118)
*Xinbang Dai, Huikang Hu, Yuncheng Hua, Jiaqi Li, Yongrui Chen, Rihui Jin, Nan Hu, Guilin Qi*

Main category: cs.CL

TL;DR: The paper introduces BRIDGE, a framework for balancing internal and external knowledge in RAG systems, outperforming baselines by 5-15% in accuracy.


<details>
  <summary>Details</summary>
Motivation: Addressing challenges in RAG systems when internal and external knowledge conflict or are unreliable.

Method: Proposes BRIDGE with an adaptive weighting mechanism (soft bias) and a decision tree for response strategy selection.

Result: BRIDGE outperforms baselines by 5-15% in accuracy, maintaining balanced performance.

Conclusion: BRIDGE provides a trustworthy solution for LLMs in real-world RAG applications.

Abstract: Retrieval-augmented generation (RAG) systems face critical challenges in
balancing internal (parametric) and external (retrieved) knowledge, especially
when these sources conflict or are unreliable. To analyze these scenarios
comprehensively, we construct the Trustworthiness Response Dataset (TRD) with
36,266 questions spanning four RAG settings. We reveal that existing approaches
address isolated scenarios-prioritizing one knowledge source, naively merging
both, or refusing answers-but lack a unified framework to handle different
real-world conditions simultaneously. Therefore, we propose the BRIDGE
framework, which dynamically determines a comprehensive response strategy of
large language models (LLMs). BRIDGE leverages an adaptive weighting mechanism
named soft bias to guide knowledge collection, followed by a Maximum Soft-bias
Decision Tree to evaluate knowledge and select optimal response strategies
(trust internal/external knowledge, or refuse). Experiments show BRIDGE
outperforms baselines by 5-15% in accuracy while maintaining balanced
performance across all scenarios. Our work provides an effective solution for
LLMs' trustworthy responses in real-world RAG applications.

</details>


### [61] [Systematic Evaluation of Machine-Generated Reasoning and PHQ-9 Labeling for Depression Detection Using Large Language Models](https://arxiv.org/pdf/2505.17119)
*Zongru Shao, Xin Wang, Zhanyang Liu, Chenhan Wang, K. P. Subbalakshmi*

Main category: cs.CL

TL;DR: The paper evaluates LLMs for mental health detection, revealing weaknesses in reasoning and proposing mitigation strategies like instruction design, contrastive prompts, and optimization methods (SFT, DPO).


<details>
  <summary>Details</summary>
Motivation: To systematically assess LLM reasoning in mental health detection and address potential weaknesses in machine-generated data.

Method: 1. Systematic evaluation of LLM reasoning. 2. Instruction strategy for subtask breakdown. 3. Contrastive few-shot and chain-of-thought prompts. 4. Human annotation and evaluation. 5. Optimization via SFT and DPO.

Result: LLMs excel in explicit depression language detection but struggle with implicit expressions. DPO significantly improves performance.

Conclusion: The study highlights LLM limitations in mental health detection and demonstrates effective optimization strategies, with DPO outperforming SFT.

Abstract: Recent research leverages large language models (LLMs) for early mental
health detection, such as depression, often optimized with machine-generated
data. However, their detection may be subject to unknown weaknesses. Meanwhile,
quality control has not been applied to these generated corpora besides limited
human verifications. Our goal is to systematically evaluate LLM reasoning and
reveal potential weaknesses. To this end, we first provide a systematic
evaluation of the reasoning over machine-generated detection and
interpretation. Then we use the models' reasoning abilities to explore
mitigation strategies for enhanced performance. Specifically, we do the
following: A. Design an LLM instruction strategy that allows for systematic
analysis of the detection by breaking down the task into several subtasks. B.
Design contrastive few-shot and chain-of-thought prompts by selecting typical
positive and negative examples of detection reasoning. C. Perform human
annotation for the subtasks identified in the first step and evaluate the
performance. D. Identify human-preferred detection with desired logical
reasoning from the few-shot generation and use them to explore different
optimization strategies. We conducted extensive comparisons on the DepTweet
dataset across the following subtasks: 1. identifying whether the speaker is
describing their own depression; 2. accurately detecting the presence of PHQ-9
symptoms, and 3. finally, detecting depression. Human verification of
statistical outliers shows that LLMs demonstrate greater accuracy in analyzing
and detecting explicit language of depression as opposed to implicit
expressions of depression. Two optimization methods are used for performance
enhancement and reduction of the statistic bias: supervised fine-tuning (SFT)
and direct preference optimization (DPO). Notably, the DPO approach achieves
significant performance improvement.

</details>


### [62] [Self-Interpretability: LLMs Can Describe Complex Internal Processes that Drive Their Decisions, and Improve with Training](https://arxiv.org/pdf/2505.17120)
*Dillon Plunkett, Adam Morris, Keerthi Reddy, Jorge Morales*

Main category: cs.CL

TL;DR: The paper explores LLMs' ability to introspect and explain their decision-making processes, showing they can describe internal processes accurately, improve with training, and generalize this ability.


<details>
  <summary>Details</summary>
Motivation: Understanding LLMs' responses is challenging due to their complex neural networks. Investigating their capacity for introspection offers an alternative path to comprehension.

Method: Fine-tuned GPT-4o and GPT-4o-mini to make decisions based on randomly-generated preferences, then trained them to explain their decision-making.

Result: LLMs accurately reported learned preferences and improved explanation accuracy with training, which also generalized to other complex decisions.

Conclusion: Training LLMs to introspect and explain their processes enhances interpretability, control, and safety, marking progress in understanding these models.

Abstract: We have only limited understanding of how and why large language models
(LLMs) respond in the ways that they do. Their neural networks have proven
challenging to interpret, and we are only beginning to tease out the function
of individual neurons and circuits within them. However, another path to
understanding these systems is to investigate and develop their capacity to
introspect and explain their own functioning. Here, we show that i)
contemporary LLMs are capable of providing accurate, quantitative descriptions
of their own internal processes during certain kinds of decision-making, ii)
that it is possible to improve these capabilities through training, and iii)
that this training generalizes to at least some degree. To do so, we fine-tuned
GPT-4o and GPT-4o-mini to make decisions in a wide variety of complex contexts
(e.g., choosing between condos, loans, vacations, etc.) according to
randomly-generated, quantitative preferences about how to weigh different
attributes during decision-making (e.g., the relative importance of natural
light versus quiet surroundings for condos). We demonstrate that the LLMs can
accurately report these preferences (i.e., the weights that they learned to
give to different attributes during decision-making). Next, we demonstrate that
these LLMs can be fine-tuned to explain their decision-making even more
accurately. Finally, we demonstrate that this training generalizes: It improves
the ability of the models to accurately explain what they are doing as they
make other complex decisions, not just decisions they have learned to make via
fine-tuning. This work is a step towards training LLMs to accurately and
broadly report on their own internal processes -- a possibility that would
yield substantial benefits for interpretability, control, and safety.

</details>


### [63] [NeSyGeo: A Neuro-Symbolic Framework for Multimodal Geometric Reasoning Data Generation](https://arxiv.org/pdf/2505.17121)
*Weiming Wu, Zi-kang Wang, Jin Ye, Zhi Zhou, Yu-Feng Li, Lan-Zhe Guo*

Main category: cs.CL

TL;DR: NeSyGeo is a neuro-symbolic framework for generating diverse geometric reasoning data, improving MLLMs' performance significantly.


<details>
  <summary>Details</summary>
Motivation: Existing data generation methods for geometric reasoning lack diversity and numerical generalization, limiting MLLMs' capabilities.

Method: NeSyGeo uses a domain-specific language and symbolic-visual-text pipeline to generate diverse Q&A pairs, leveraging LLMs.

Result: The framework produces 100k samples, boosting MLLMs' performance by up to +15.8% on benchmarks.

Conclusion: NeSyGeo effectively enhances geometric reasoning in MLLMs, even outperforming larger models with fewer samples.

Abstract: Obtaining large-scale, high-quality data with reasoning paths is crucial for
improving the geometric reasoning capabilities of multi-modal large language
models (MLLMs). However, existing data generation methods, whether based on
predefined templates or constrained symbolic provers, inevitably face diversity
and numerical generalization limitations. To address these limitations, we
propose NeSyGeo, a novel neuro-symbolic framework for generating geometric
reasoning data. First, we propose a domain-specific language grounded in the
entity-relation-constraint paradigm to comprehensively represent all components
of plane geometry, along with generative actions defined within this symbolic
space. We then design a symbolic-visual-text pipeline that synthesizes symbolic
sequences, maps them to corresponding visual and textual representations, and
generates diverse question-answer (Q&A) pairs using large language models
(LLMs). To the best of our knowledge, we are the first to propose a
neuro-symbolic approach in generating multimodal reasoning data. Based on this
framework, we construct NeSyGeo-CoT and NeSyGeo-Caption datasets, containing
100k samples, and release a new benchmark NeSyGeo-Test for evaluating geometric
reasoning abilities in MLLMs. Experiments demonstrate that the proposal
significantly and consistently improves the performance of multiple MLLMs under
both reinforcement and supervised fine-tuning. With only 4k samples and two
epochs of reinforcement fine-tuning, base models achieve improvements of up to
+15.8% on MathVision, +8.4% on MathVerse, and +7.3% on GeoQA. Notably, a 4B
model can be improved to outperform an 8B model from the same series on
geometric reasoning tasks.

</details>


### [64] [Shallow Preference Signals: Large Language Model Aligns Even Better with Truncated Data?](https://arxiv.org/pdf/2505.17122)
*Xuan Qi, Jiahao Qiu, Xinzhe Juan, Yue Wu, Mengdi Wang*

Main category: cs.CL

TL;DR: The paper identifies 'shallow preference signals' in LLM alignment, where early tokens dominate preference signals. Truncated datasets perform comparably or better, and new decoding strategies improve alignment efficiency.


<details>
  <summary>Details</summary>
Motivation: Understanding and improving the alignment of LLMs with human preferences, as current methods may focus too narrowly on early tokens.

Method: Systematically truncate preference datasets, train reward and DPO models, and test new decoding strategies (Length Control and KL Threshold Control).

Result: Models trained on truncated datasets perform similarly or better, and new decoding strategies enhance alignment efficiency.

Conclusion: Shallow preference signals reveal limitations in current alignment methods, suggesting a need for broader response consideration.

Abstract: Aligning large language models (LLMs) with human preferences remains a key
challenge in AI. Preference-based optimization methods, such as Reinforcement
Learning with Human Feedback (RLHF) and Direct Preference Optimization (DPO),
rely on human-annotated datasets to improve alignment. In this work, we
identify a crucial property of the existing learning method: the distinguishing
signal obtained in preferred responses is often concentrated in the early
tokens. We refer to this as shallow preference signals.
  To explore this property, we systematically truncate preference datasets at
various points and train both reward models and DPO models on the truncated
data. Surprisingly, models trained on truncated datasets, retaining only the
first half or fewer tokens, achieve comparable or even superior performance to
those trained on full datasets. For example, a reward model trained on the
Skywork-Reward-Preference-80K-v0.2 dataset outperforms the full dataset when
trained on a 40\% truncated dataset. This pattern is consistent across multiple
datasets, suggesting the widespread presence of shallow preference signals.
  We further investigate the distribution of the reward signal through decoding
strategies. We consider two simple decoding strategies motivated by the shallow
reward signal observation, namely Length Control Decoding and KL Threshold
Control Decoding, which leverage shallow preference signals to optimize the
trade-off between alignment and computational efficiency. The performance is
even better, which again validates our hypothesis.
  The phenomenon of shallow preference signals highlights potential issues in
LLM alignment: existing alignment methods often focus on aligning only the
initial tokens of responses, rather than considering the full response. This
could lead to discrepancies with real-world human preferences, resulting in
suboptimal alignment performance.

</details>


### [65] [MTR-Bench: A Comprehensive Benchmark for Multi-Turn Reasoning Evaluation](https://arxiv.org/pdf/2505.17123)
*Xiaoyuan Li, Keqin Bao, Yubo Ma, Moxin Li, Wenjie Wang, Rui Men, Yichang Zhang, Fuli Feng, Dayiheng Liu, Junyang Lin*

Main category: cs.CL

TL;DR: MTR-Bench introduces a comprehensive dataset and automated framework for evaluating LLMs' multi-turn reasoning, revealing gaps in current models.


<details>
  <summary>Details</summary>
Motivation: Current LLM evaluations lack focus on interactive, multi-turn reasoning due to missing datasets and scalable evaluation methods.

Method: Developed MTR-Bench with 4 classes, 40 tasks, and 3600 instances, featuring automated dataset construction and evaluation.

Result: State-of-the-art LLMs underperform in multi-turn reasoning tasks, highlighting limitations.

Conclusion: MTR-Bench provides insights for advancing interactive AI systems, addressing current evaluation gaps.

Abstract: Recent advances in Large Language Models (LLMs) have shown promising results
in complex reasoning tasks. However, current evaluations predominantly focus on
single-turn reasoning scenarios, leaving interactive tasks largely unexplored.
We attribute it to the absence of comprehensive datasets and scalable automatic
evaluation protocols. To fill these gaps, we present MTR-Bench for LLMs'
Multi-Turn Reasoning evaluation. Comprising 4 classes, 40 tasks, and 3600
instances, MTR-Bench covers diverse reasoning capabilities, fine-grained
difficulty granularity, and necessitates multi-turn interactions with the
environments. Moreover, MTR-Bench features fully-automated framework spanning
both dataset constructions and model evaluations, which enables scalable
assessment without human interventions. Extensive experiments reveal that even
the cutting-edge reasoning models fall short of multi-turn, interactive
reasoning tasks. And the further analysis upon these results brings valuable
insights for future research in interactive AI systems.

</details>


### [66] [Relative Bias: A Comparative Framework for Quantifying Bias in LLMs](https://arxiv.org/pdf/2505.17131)
*Alireza Arbabi, Florian Kerschbaum*

Main category: cs.CL

TL;DR: The paper introduces the Relative Bias framework to systematically quantify and compare biases in large language models (LLMs) using two methods: Embedding Transformation analysis and LLM-as-a-Judge.


<details>
  <summary>Details</summary>
Motivation: The rapid deployment of LLMs raises concerns about their biases, but quantifying bias is challenging due to ambiguity and lack of systematic assessment.

Method: Proposes the Relative Bias framework with two methodologies: (1) Embedding Transformation analysis for bias patterns in sentence representations, and (2) LLM-as-a-Judge for comparative output evaluation.

Result: Strong alignment between the two scoring methods is found in case studies, validating the framework's effectiveness.

Conclusion: The Relative Bias framework offers a scalable, systematic, and statistically grounded approach for comparative bias analysis in LLMs.

Abstract: The growing deployment of large language models (LLMs) has amplified concerns
regarding their inherent biases, raising critical questions about their
fairness, safety, and societal impact. However, quantifying LLM bias remains a
fundamental challenge, complicated by the ambiguity of what "bias" entails.
This challenge grows as new models emerge rapidly and gain widespread use,
while introducing potential biases that have not been systematically assessed.
In this paper, we propose the Relative Bias framework, a method designed to
assess how an LLM's behavior deviates from other LLMs within a specified target
domain. We introduce two complementary methodologies: (1) Embedding
Transformation analysis, which captures relative bias patterns through sentence
representations over the embedding space, and (2) LLM-as-a-Judge, which employs
a language model to evaluate outputs comparatively. Applying our framework to
several case studies on bias and alignment scenarios following by statistical
tests for validation, we find strong alignment between the two scoring methods,
offering a systematic, scalable, and statistically grounded approach for
comparative bias analysis in LLMs.

</details>


### [67] [Conformal Language Model Reasoning with Coherent Factuality](https://arxiv.org/pdf/2505.17126)
*Maxon Rubin-Toles, Maya Gambhir, Keshav Ramji, Aaron Roth, Surbhi Goel*

Main category: cs.CL

TL;DR: The paper introduces "coherent factuality" for language model outputs, using conformal prediction on deducibility graphs to ensure correctness in reasoning tasks, achieving high accuracy and coverage.


<details>
  <summary>Details</summary>
Motivation: Ensuring correctness of language model outputs, especially in reasoning tasks where isolated factuality checks are insufficient.

Method: Develops a conformal-prediction-based method applied to subgraphs in a "deducibility" graph representing reasoning steps.

Result: Achieves 90% factuality under stricter definitions while retaining 80% of original claims, validated on MATH and FELM datasets.

Conclusion: The deducibility-graph-guided approach effectively ensures coherent factuality in reasoning tasks.

Abstract: Language models are increasingly being used in important decision pipelines,
so ensuring the correctness of their outputs is crucial. Recent work has
proposed evaluating the "factuality" of claims decomposed from a language model
generation and applying conformal prediction techniques to filter out those
claims that are not factual. This can be effective for tasks such as
information retrieval, where constituent claims may be evaluated in isolation
for factuality, but is not appropriate for reasoning tasks, as steps of a
logical argument can be evaluated for correctness only within the context of
the claims that precede them. To capture this, we define "coherent factuality"
and develop a conformal-prediction-based method to guarantee coherent
factuality for language model outputs. Our approach applies split conformal
prediction to subgraphs within a "deducibility" graph" that represents the
steps of a reasoning problem. We evaluate our method on mathematical reasoning
problems from the MATH and FELM datasets and find that our algorithm
consistently produces correct and substantiated orderings of claims, achieving
coherent factuality across target coverage levels. Moreover, we achieve 90%
factuality on our stricter definition while retaining 80% or more of the
original claims, highlighting the utility of our deducibility-graph-guided
approach.

</details>


### [68] [LongMagpie: A Self-synthesis Method for Generating Large-scale Long-context Instructions](https://arxiv.org/pdf/2505.17134)
*Chaochen Gao, Xing Wu, Zijia Lin, Debing Zhang, Songlin Hu*

Main category: cs.CL

TL;DR: LongMagpie is a self-synthesis framework for generating high-quality long-context instruction data automatically, outperforming human annotation and template-based methods.


<details>
  <summary>Details</summary>
Motivation: High-quality long-context instruction data is scarce and costly to produce, limiting the alignment of long-context LLMs.

Method: LongMagpie leverages aligned long-context LLMs to auto-generate document-query pairs and responses, eliminating human effort.

Result: LongMagpie achieves top performance on long-context benchmarks (HELMET, RULER, Longbench v2) and remains competitive on short-context tasks.

Conclusion: LongMagpie offers a scalable, open, and diverse solution for long-context instruction data synthesis.

Abstract: High-quality long-context instruction data is essential for aligning
long-context large language models (LLMs). Despite the public release of models
like Qwen and Llama, their long-context instruction data remains proprietary.
Human annotation is costly and challenging, while template-based synthesis
methods limit scale, diversity, and quality. We introduce LongMagpie, a
self-synthesis framework that automatically generates large-scale long-context
instruction data. Our key insight is that aligned long-context LLMs, when
presented with a document followed by special tokens preceding a user turn,
auto-regressively generate contextually relevant queries. By harvesting these
document-query pairs and the model's responses, LongMagpie produces
high-quality instructions without human effort. Experiments on HELMET, RULER,
and Longbench v2 demonstrate that LongMagpie achieves leading performance on
long-context tasks while maintaining competitive performance on short-context
tasks, establishing it as a simple and effective approach for open, diverse,
and scalable long-context instruction data synthesis.

</details>


### [69] [Foundation Models for Geospatial Reasoning: Assessing Capabilities of Large Language Models in Understanding Geometries and Topological Spatial Relations](https://arxiv.org/pdf/2505.17136)
*Yuhan Ji, Song Gao, Ying Nie, Ivan Majić, Krzysztof Janowicz*

Main category: cs.CL

TL;DR: The paper explores how well LLMs like GPT-3.5-turbo, GPT-4, and DeepSeek-R1-14B handle geospatial reasoning tasks using WKT representations, achieving over 0.6 accuracy in identifying topological spatial relations, with GPT-4 performing best.


<details>
  <summary>Details</summary>
Motivation: Direct application of AI foundation models to geospatial data is limited by their inability to represent and reason with geographical entities and spatial relations.

Method: Three approaches were tested: geometry embedding-based, prompt engineering-based, and everyday language-based evaluation for spatial reasoning tasks.

Result: GPT-4 with few-shot prompting achieved the highest accuracy (over 0.66) in topological spatial relation inference. LLMs also showed promise in comprehending inverse relations and translating vernacular descriptions into formal relations.

Conclusion: The findings provide insights for refining LLMs with geographical knowledge, aiding the development of geo-foundation models for geospatial reasoning.

Abstract: Applying AI foundation models directly to geospatial datasets remains
challenging due to their limited ability to represent and reason with
geographical entities, specifically vector-based geometries and natural
language descriptions of complex spatial relations. To address these issues, we
investigate the extent to which a well-known-text (WKT) representation of
geometries and their spatial relations (e.g., topological predicates) are
preserved during spatial reasoning when the geospatial vector data are passed
to large language models (LLMs) including GPT-3.5-turbo, GPT-4, and
DeepSeek-R1-14B. Our workflow employs three distinct approaches to complete the
spatial reasoning tasks for comparison, i.e., geometry embedding-based, prompt
engineering-based, and everyday language-based evaluation. Our experiment
results demonstrate that both the embedding-based and prompt engineering-based
approaches to geospatial question-answering tasks with GPT models can achieve
an accuracy of over 0.6 on average for the identification of topological
spatial relations between two geometries. Among the evaluated models, GPT-4
with few-shot prompting achieved the highest performance with over 0.66
accuracy on topological spatial relation inference. Additionally, GPT-based
reasoner is capable of properly comprehending inverse topological spatial
relations and including an LLM-generated geometry can enhance the effectiveness
for geographic entity retrieval. GPT-4 also exhibits the ability to translate
certain vernacular descriptions about places into formal topological relations,
and adding the geometry-type or place-type context in prompts may improve
inference accuracy, but it varies by instance. The performance of these spatial
reasoning tasks offers valuable insights for the refinement of LLMs with
geographical knowledge towards the development of geo-foundation models capable
of geospatial reasoning.

</details>


### [70] [When can isotropy help adapt LLMs' next word prediction to numerical domains?](https://arxiv.org/pdf/2505.17135)
*Rashed Shelim, Shengzhe Xu, Walid Saad, Naren Ramakrishnan*

Main category: cs.CL

TL;DR: The paper explores how pre-trained LLMs can be adapted for numerical tasks by analyzing isotropy in embedding spaces, addressing hallucination issues and ensuring performance guarantees.


<details>
  <summary>Details</summary>
Motivation: LLMs often hallucinate in numerical domains, risking reliability in critical applications. Theoretical understanding of their adaptation to numeric tasks is lacking.

Method: A log-linear model for LLMs is proposed, leveraging isotropy in embedding spaces to resolve shift-invariance issues in softmax-based predictions.

Result: Isotropic properties in LLM embeddings preserve representation structure, solving shift-invariance and ensuring performance guarantees. Experiments reveal varying impacts of data and model architecture on isotropy.

Conclusion: The study bridges the gap in theoretical understanding, showing how isotropy in LLM embeddings enables reliable adaptation to numerical tasks.

Abstract: Recent studies have shown that vector representations of contextual
embeddings learned by pre-trained large language models (LLMs) are effective in
various downstream tasks in numerical domains. Despite their significant
benefits, the tendency of LLMs to hallucinate in such domains can have severe
consequences in applications such as energy, nature, finance, healthcare,
retail and transportation, among others. To guarantee prediction reliability
and accuracy in numerical domains, it is necessary to open the black-box and
provide performance guarantees through explanation. However, there is little
theoretical understanding of when pre-trained language models help solve
numeric downstream tasks. This paper seeks to bridge this gap by understanding
when the next-word prediction capability of LLMs can be adapted to numerical
domains through a novel analysis based on the concept of isotropy in the
contextual embedding space. Specifically, we consider a log-linear model for
LLMs in which numeric data can be predicted from its context through a network
with softmax in the output layer of LLMs (i.e., language model head in
self-attention). We demonstrate that, in order to achieve state-of-the-art
performance in numerical domains, the hidden representations of the LLM
embeddings must possess a structure that accounts for the shift-invariance of
the softmax function. By formulating a gradient structure of self-attention in
pre-trained models, we show how the isotropic property of LLM embeddings in
contextual embedding space preserves the underlying structure of
representations, thereby resolving the shift-invariance problem and providing a
performance guarantee. Experiments show that different characteristics of
numeric data and model architecture could have different impacts on isotropy.

</details>


### [71] [Cog-TiPRO: Iterative Prompt Refinement with LLMs to Detect Cognitive Decline via Longitudinal Voice Assistant Commands](https://arxiv.org/pdf/2505.17137)
*Kristin Qi, Youxiang Zhu, Caroline Summerour, John A. Batsis, Xiaohui Liang*

Main category: cs.CL

TL;DR: The paper proposes Cog-TiPRO, a framework using voice assistant systems (VAS) to detect cognitive decline by analyzing speech patterns, achieving 73.80% accuracy.


<details>
  <summary>Details</summary>
Motivation: Early detection of cognitive decline is vital for intervention, but traditional methods are impractical for frequent monitoring.

Method: Cog-TiPRO combines LLM-driven linguistic feature extraction, HuBERT-based acoustic analysis, and transformer-based temporal modeling.

Result: The framework achieves 73.80% accuracy and 72.67% F1-score in detecting MCI, outperforming baselines by 27.13%.

Conclusion: Voice assistant systems, with Cog-TiPRO, offer a promising non-invasive tool for detecting cognitive decline through speech analysis.

Abstract: Early detection of cognitive decline is crucial for enabling interventions
that can slow neurodegenerative disease progression. Traditional diagnostic
approaches rely on labor-intensive clinical assessments, which are impractical
for frequent monitoring. Our pilot study investigates voice assistant systems
(VAS) as non-invasive tools for detecting cognitive decline through
longitudinal analysis of speech patterns in voice commands. Over an 18-month
period, we collected voice commands from 35 older adults, with 15 participants
providing daily at-home VAS interactions. To address the challenges of
analyzing these short, unstructured and noisy commands, we propose Cog-TiPRO, a
framework that combines (1) LLM-driven iterative prompt refinement for
linguistic feature extraction, (2) HuBERT-based acoustic feature extraction,
and (3) transformer-based temporal modeling. Using iTransformer, our approach
achieves 73.80% accuracy and 72.67% F1-score in detecting MCI, outperforming
its baseline by 27.13%. Through our LLM approach, we identify linguistic
features that uniquely characterize everyday command usage patterns in
individuals experiencing cognitive decline.

</details>


### [72] [EarthSE: A Benchmark Evaluating Earth Scientific Exploration Capability for Large Language Models](https://arxiv.org/pdf/2505.17139)
*Wanghan Xu, Xiangyu Zhao, Yuhao Zhou, Xiaoyu Yue, Ben Fei, Fenghua Ling, Wenlong Zhang, Lei Bai*

Main category: cs.CL

TL;DR: A new Earth science benchmark for LLMs evaluates capabilities from basic to advanced levels, including open-ended exploration, using specialized QA datasets and multi-turn dialogues.


<details>
  <summary>Details</summary>
Motivation: Existing benchmarks lack Earth science specificity and neglect open-ended scientific exploration, necessitating a specialized evaluation tool.

Method: Constructed two QA datasets (Earth-Iron and Earth-Silver) and introduced Earth-Gold with multi-turn dialogues, leveraging 100,000 research papers.

Result: Experiments show limitations in 11 leading LLMs, indicating room for improvement in scientific exploration.

Conclusion: The benchmark provides a holistic evaluation tool for LLMs in Earth science, highlighting gaps in current capabilities.

Abstract: Advancements in Large Language Models (LLMs) drive interest in scientific
applications, necessitating specialized benchmarks such as Earth science.
Existing benchmarks either present a general science focus devoid of Earth
science specificity or cover isolated subdomains, lacking holistic evaluation.
Furthermore, current benchmarks typically neglect the assessment of LLMs'
capabilities in open-ended scientific exploration. In this paper, we present a
comprehensive and professional benchmark for the Earth sciences, designed to
evaluate the capabilities of LLMs in scientific exploration within this domain,
spanning from fundamental to advanced levels. Leveraging a corpus of 100,000
research papers, we first construct two Question Answering (QA) datasets:
Earth-Iron, which offers extensive question coverage for broad assessment, and
Earth-Silver, which features a higher level of difficulty to evaluate
professional depth. These datasets encompass five Earth spheres, 114
disciplines, and 11 task categories, assessing foundational knowledge crucial
for scientific exploration. Most notably, we introduce Earth-Gold with new
metrics, a dataset comprising open-ended multi-turn dialogues specifically
designed to evaluate the advanced capabilities of LLMs in scientific
exploration, including methodology induction, limitation analysis, and concept
proposal. Extensive experiments reveal limitations in 11 leading LLMs across
different domains and tasks, highlighting considerable room for improvement in
their scientific exploration capabilities. The benchmark is available on
https://huggingface.co/ai-earth .

</details>


### [73] [Data Doping or True Intelligence? Evaluating the Transferability of Injected Knowledge in LLMs](https://arxiv.org/pdf/2505.17140)
*Essa Jan, Moiz Ali, Muhammad Saram Hassan, Fareed Zaffar, Yasir Zaki*

Main category: cs.CL

TL;DR: Fine-tuning LLMs with comprehension-intensive tasks (e.g., QA) retains knowledge better (48%) than mapping tasks (17-20%). Larger models improve retention, but semantic integration remains limited.


<details>
  <summary>Details</summary>
Motivation: Address the need for efficient methods to update LLMs with proprietary or updated knowledge, as their knowledge becomes outdated over time.

Method: Compare knowledge retention rates between comprehension-intensive tasks (e.g., question answering) and mapping-oriented tasks (e.g., translation) during fine-tuning.

Result: Comprehension tasks achieve 48% retention vs. 17-20% for mapping tasks. Larger models improve retention, but performance drops in broader contexts.

Conclusion: Task selection is crucial for updating LLMs; deeper cognitive engagement during fine-tuning enhances knowledge retention.

Abstract: As the knowledge of large language models (LLMs) becomes outdated over time,
there is a growing need for efficient methods to update them, especially when
injecting proprietary information. Our study reveals that
comprehension-intensive fine-tuning tasks (e.g., question answering and blanks)
achieve substantially higher knowledge retention rates (48%) compared to
mapping-oriented tasks like translation (17%) or text-to-JSON conversion (20%),
despite exposure to identical factual content. We demonstrate that this pattern
persists across model architectures and follows scaling laws, with larger
models showing improved retention across all task types. However, all models
exhibit significant performance drops when applying injected knowledge in
broader contexts, suggesting limited semantic integration. These findings show
the importance of task selection in updating LLM knowledge, showing that
effective knowledge injection relies not just on data exposure but on the depth
of cognitive engagement during fine-tuning.

</details>


### [74] [MDIT-Bench: Evaluating the Dual-Implicit Toxicity in Large Multimodal Models](https://arxiv.org/pdf/2505.17144)
*Bohan Jin, Shuhan Qi, Kehai Chen, Xinyi Guo, Xuan Wang*

Main category: cs.CL

TL;DR: The paper introduces dual-implicit toxicity and MDIT-Bench, a benchmark to evaluate LMMs' sensitivity to this subtle toxicity, revealing significant gaps in model performance.


<details>
  <summary>Details</summary>
Motivation: Current research overlooks implicit toxicity like prejudice and discrimination, prompting the need for a benchmark to assess LMMs' handling of such subtler toxicity.

Method: The authors created MDIT-Dataset using a Multi-stage Human-in-loop In-context Generation method and developed MDIT-Bench with 317,638 questions across 12 categories and 3 difficulty levels.

Result: Testing 13 prominent LMMs showed they struggle with dual-implicit toxicity, especially at higher difficulty levels, exposing hidden toxicity.

Conclusion: LMMs inadequately address dual-implicit toxicity, highlighting the need for improved toxicity detection and mitigation strategies.

Abstract: The widespread use of Large Multimodal Models (LMMs) has raised concerns
about model toxicity. However, current research mainly focuses on explicit
toxicity, with less attention to some more implicit toxicity regarding
prejudice and discrimination. To address this limitation, we introduce a
subtler type of toxicity named dual-implicit toxicity and a novel toxicity
benchmark termed MDIT-Bench: Multimodal Dual-Implicit Toxicity Benchmark.
Specifically, we first create the MDIT-Dataset with dual-implicit toxicity
using the proposed Multi-stage Human-in-loop In-context Generation method.
Based on this dataset, we construct the MDIT-Bench, a benchmark for evaluating
the sensitivity of models to dual-implicit toxicity, with 317,638 questions
covering 12 categories, 23 subcategories, and 780 topics. MDIT-Bench includes
three difficulty levels, and we propose a metric to measure the toxicity gap
exhibited by the model across them. In the experiment, we conducted MDIT-Bench
on 13 prominent LMMs, and the results show that these LMMs cannot handle
dual-implicit toxicity effectively. The model's performance drops significantly
in hard level, revealing that these LMMs still contain a significant amount of
hidden but activatable toxicity. Data are available at
https://github.com/nuo1nuo/MDIT-Bench.

</details>


### [75] [Large Language Models for Predictive Analysis: How Far Are They?](https://arxiv.org/pdf/2505.17149)
*Qin Chen, Yuanyi Ren, Xiaojun Ma, Yuyang Shi*

Main category: cs.CL

TL;DR: The paper introduces PredictiQ, a benchmark to evaluate LLMs' capability in predictive analysis, revealing significant challenges despite their potential.


<details>
  <summary>Details</summary>
Motivation: There's a lack of systematic evaluation of LLMs for predictive analysis, despite their growing use in decision-making.

Method: The PredictiQ benchmark includes 1130 queries from 44 real-world datasets across 8 fields, evaluating 12 LLMs on text analysis, code generation, and alignment.

Result: Existing LLMs still struggle with predictive analysis tasks, highlighting limitations in their practical application.

Conclusion: The study underscores the need for further improvement in LLMs for predictive analysis, providing a benchmark for future research.

Abstract: Predictive analysis is a cornerstone of modern decision-making, with
applications in various domains. Large Language Models (LLMs) have emerged as
powerful tools in enabling nuanced, knowledge-intensive conversations, thus
aiding in complex decision-making tasks. With the burgeoning expectation to
harness LLMs for predictive analysis, there is an urgent need to systematically
assess their capability in this domain. However, there is a lack of relevant
evaluations in existing studies. To bridge this gap, we introduce the
\textbf{PredictiQ} benchmark, which integrates 1130 sophisticated predictive
analysis queries originating from 44 real-world datasets of 8 diverse fields.
We design an evaluation protocol considering text analysis, code generation,
and their alignment. Twelve renowned LLMs are evaluated, offering insights into
their practical use in predictive analysis. Generally, we believe that existing
LLMs still face considerable challenges in conducting predictive analysis. See
\href{https://github.com/Cqkkkkkk/PredictiQ}{Github}.

</details>


### [76] [Bayesian Optimization for Enhanced Language Models: Optimizing Acquisition Functions](https://arxiv.org/pdf/2505.17151)
*Zishuo Bao, Yibo Liu, Changyutao Qiu*

Main category: cs.CL

TL;DR: The paper introduces Bilevel-BO-SWA, a method combining bilevel Bayesian optimization with model fusion to improve fine-tuning of large language models by using mixed acquisition functions (EI and UCB).


<details>
  <summary>Details</summary>
Motivation: Existing hyperparameter tuning methods for fine-tuning language models ignore the sensitivity of acquisition functions to training and validation performance, leading to suboptimal results.

Method: Proposes a bilevel BO strategy with nested optimization loops: inner loop minimizes training loss, outer loop optimizes validation metrics using mixed acquisition functions (EI and UCB).

Result: Experiments on GLUE tasks with RoBERTa-base show improved generalization and up to 2.7% better fine-tuning performance.

Conclusion: Bilevel-BO-SWA effectively addresses the limitations of traditional BO methods by leveraging mixed acquisition functions, enhancing fine-tuning outcomes.

Abstract: With the rise of different language model architecture, fine-tuning is
becoming even more important for down stream tasks Model gets messy, finding
proper hyperparameters for fine-tuning. Although BO has been tried for
hyperparameter tuning, most of the existing methods are oblivious to the fact
that BO relies on careful choices of acquisition functions, which are essential
components of BO that guide how much to explore versus exploit during the
optimization process; Different acquisition functions have different levels of
sensitivity towards training loss and validation performance; existing methods
often just apply an acquisition function no matter if the training and
validation performance are sensitive to the acquisition function or not. This
work introduces{Bilevel - BO - SWA}, a model fusion approach coupled with a
bilevel BO strategy to improve the fine - tunning of large language models. Our
work on mixture of acquisition functions like EI and UCB into nested opt loops,
where inner loop perform minimization of training loss while outer loops
optimized w.r.t. val metric. Experiments on GLUE tasks using RoBERTA - base
show that when using EI and UCB, there is an improvement in generalization, and
fine - tuning can be improved by up to 2.7%.

</details>


### [77] [Amplify Adjacent Token Differences: Enhancing Long Chain-of-Thought Reasoning with Shift-FFN](https://arxiv.org/pdf/2505.17153)
*Yao Xu, Mingyu Xu, Fangyu Lei, Wangtao Sun, Xiangrong Zeng, Bingning Wang, Guang Liu, Shizhu He, Jun Zhao, Kang Liu*

Main category: cs.CL

TL;DR: The paper addresses Cyclical Reasoning in LLMs fine-tuned with long CoT data, proposing Shift-FFN to enhance token representation differences and improve reasoning accuracy.


<details>
  <summary>Details</summary>
Motivation: To mitigate Cyclical Reasoning in LLMs fine-tuned on long CoT data, which hampers performance by repetitive inference steps.

Method: Introduces Shift-FFN, which edits token representations before FFN to amplify differences between adjacent tokens.

Result: LoRA with Shift-FFN outperforms full fine-tuning and standard LoRA in accuracy and reduces Cyclical Reasoning.

Conclusion: Shift-FFN effectively mitigates Cyclical Reasoning, enhancing LLM performance on complex reasoning tasks.

Abstract: Recently, models such as OpenAI-o1 and DeepSeek-R1 have demonstrated
remarkable performance on complex reasoning tasks through Long Chain-of-Thought
(Long-CoT) reasoning. Although distilling this capability into student models
significantly enhances their performance, this paper finds that fine-tuning
LLMs with full parameters or LoRA with a low rank on long CoT data often leads
to Cyclical Reasoning, where models repeatedly reiterate previous inference
steps until the maximum length limit. Further analysis reveals that smaller
differences in representations between adjacent tokens correlates with a higher
tendency toward Cyclical Reasoning. To mitigate this issue, this paper proposes
Shift Feedforward Networks (Shift-FFN), a novel approach that edits the current
token's representation with the previous one before inputting it to FFN. This
architecture dynamically amplifies the representation differences between
adjacent tokens. Extensive experiments on multiple mathematical reasoning tasks
demonstrate that LoRA combined with Shift-FFN achieves higher accuracy and a
lower rate of Cyclical Reasoning across various data sizes compared to full
fine-tuning and standard LoRA. Our data and code are available at
https://anonymous.4open.science/r/Shift-FFN

</details>


### [78] [PersonaBOT: Bringing Customer Personas to Life with LLMs and RAG](https://arxiv.org/pdf/2505.17156)
*Muhammed Rizwan, Lars Carlsson, Mohammad Loni*

Main category: cs.CL

TL;DR: The paper explores using LLMs to create synthetic customer personas for a RAG chatbot, improving decision-making at Volvo Construction Equipment. Few-Shot prompting outperformed CoT in persona completeness, while CoT was more efficient. The chatbot's accuracy and utility improved after augmentation.


<details>
  <summary>Details</summary>
Motivation: Traditional qualitative methods for customer personas at VCE are time-consuming and lack scalability. The paper aims to leverage LLMs for synthetic persona generation to enhance business decision-making.

Method: Developed a persona-based RAG chatbot with verified personas, generated synthetic personas using Few-Shot and CoT prompting, and evaluated them for completeness, relevance, and consistency. Augmented the chatbot's knowledge base with synthetic personas and segment information.

Result: Few-Shot prompting produced more complete personas, while CoT was more efficient. Chatbot accuracy improved from 5.88 to 6.42, and 81.82% of participants found it useful.

Conclusion: Synthetic personas enhance chatbot performance, with Few-Shot and CoT each excelling in different aspects. The approach is practical for business applications.

Abstract: The introduction of Large Language Models (LLMs) has significantly
transformed Natural Language Processing (NLP) applications by enabling more
advanced analysis of customer personas. At Volvo Construction Equipment (VCE),
customer personas have traditionally been developed through qualitative
methods, which are time-consuming and lack scalability. The main objective of
this paper is to generate synthetic customer personas and integrate them into a
Retrieval-Augmented Generation (RAG) chatbot to support decision-making in
business processes. To this end, we first focus on developing a persona-based
RAG chatbot integrated with verified personas. Next, synthetic personas are
generated using Few-Shot and Chain-of-Thought (CoT) prompting techniques and
evaluated based on completeness, relevance, and consistency using McNemar's
test. In the final step, the chatbot's knowledge base is augmented with
synthetic personas and additional segment information to assess improvements in
response accuracy and practical utility. Key findings indicate that Few-Shot
prompting outperformed CoT in generating more complete personas, while CoT
demonstrated greater efficiency in terms of response time and token usage.
After augmenting the knowledge base, the average accuracy rating of the chatbot
increased from 5.88 to 6.42 on a 10-point scale, and 81.82% of participants
found the updated system useful in business contexts.

</details>


### [79] [Harry Potter is Still Here! Probing Knowledge Leakage in Targeted Unlearned Large Language Models via Automated Adversarial Prompting](https://arxiv.org/pdf/2505.17160)
*Bang Trinh Tran To, Thai Le*

Main category: cs.CL

TL;DR: LURK is a framework that detects hidden retained knowledge in unlearned LLMs using adversarial suffix prompts, revealing leaks in supposedly unlearned models.


<details>
  <summary>Details</summary>
Motivation: Current unlearning evaluation standards may overlook residual knowledge, so LURK aims to rigorously assess unlearning robustness.

Method: LURK uses adversarial suffix prompting to probe for latent knowledge in unlearned LLMs, focusing on the Harry Potter domain.

Result: Experiments show unlearned models can still leak information under adversarial conditions, exposing flaws in current evaluations.

Conclusion: LURK provides a more diagnostic tool for evaluating unlearning, highlighting the need for stricter standards.

Abstract: This work presents LURK (Latent UnleaRned Knowledge), a novel framework that
probes for hidden retained knowledge in unlearned LLMs through adversarial
suffix prompting. LURK automatically generates adversarial prompt suffixes
designed to elicit residual knowledge about the Harry Potter domain, a commonly
used benchmark for unlearning. Our experiments reveal that even models deemed
successfully unlearned can leak idiosyncratic information under targeted
adversarial conditions, highlighting critical limitations of current unlearning
evaluation standards. By uncovering latent knowledge through indirect probing,
LURK offers a more rigorous and diagnostic tool for assessing the robustness of
unlearning algorithms. All code will be publicly available.

</details>


### [80] [CRG Score: A Distribution-Aware Clinical Metric for Radiology Report Generation](https://arxiv.org/pdf/2505.17167)
*Ibrahim Ethem Hamamci, Sezgin Er, Suprosanna Shit, Hadrien Reynaud, Bernhard Kainz, Bjoern Menze*

Main category: cs.CL

TL;DR: The paper introduces the CRG Score, a new metric for evaluating radiology report generation, addressing limitations of existing metrics like NLG and LLM-based ones by focusing on clinical relevance and adaptability.


<details>
  <summary>Details</summary>
Motivation: Current metrics for radiology report generation (NLG, LLM-based) lack clinical correctness or generalizability, and clinical accuracy metrics are biased by class imbalance. A better evaluation method is needed.

Method: The CRG Score is proposed, a distribution-aware metric that evaluates clinically relevant abnormalities from reference reports. It supports binary and structured labels and can integrate with any LLM for feature extraction.

Result: CRG Score balances penalties based on label distribution, providing fairer and more robust evaluation. It also serves as a clinically aligned reward function.

Conclusion: The CRG Score offers a more reliable and adaptable metric for evaluating radiology report generation, addressing key shortcomings of existing methods.

Abstract: Evaluating long-context radiology report generation is challenging. NLG
metrics fail to capture clinical correctness, while LLM-based metrics often
lack generalizability. Clinical accuracy metrics are more relevant but are
sensitive to class imbalance, frequently favoring trivial predictions. We
propose the CRG Score, a distribution-aware and adaptable metric that evaluates
only clinically relevant abnormalities explicitly described in reference
reports. CRG supports both binary and structured labels (e.g., type, location)
and can be paired with any LLM for feature extraction. By balancing penalties
based on label distribution, it enables fairer, more robust evaluation and
serves as a clinically aligned reward function.

</details>


### [81] [Next Token Perception Score: Analytical Assessment of your LLM Perception Skills](https://arxiv.org/pdf/2505.17169)
*Yu-Ang Cheng, Leyang Hu, Hai Huang, Randall Balestriero*

Main category: cs.CL

TL;DR: The paper introduces the Next Token Perception Score (NTPS) to measure alignment between autoregressive pretraining and downstream perception tasks, showing its correlation with performance and utility in predicting LoRA fine-tuning gains.


<details>
  <summary>Details</summary>
Motivation: Autoregressive pretraining in LLMs doesn't consistently transfer well to perception tasks, prompting the need to quantify this misalignment.

Method: NTPS is introduced as a metric to measure feature subspace overlap between autoregressive and perception tasks, validated across diverse datasets and models.

Result: NTPS correlates with linear probe accuracy and predicts LoRA fine-tuning gains, especially in large models.

Conclusion: NTPS provides theoretical and practical tools for assessing LLM perception skills and optimizing fine-tuning.

Abstract: Autoregressive pretraining has become the de facto paradigm for learning
general-purpose representations in large language models (LLMs). However,
linear probe performance across downstream perception tasks shows substantial
variability, suggesting that features optimized for next-token prediction do
not consistently transfer well to downstream perception tasks. We demonstrate
that representations learned via autoregression capture features that may lie
outside the subspaces most informative for perception. To quantify the
(mis)alignment between autoregressive pretraining and downstream perception, we
introduce the Next Token Perception Score (NTPS)-a score derived under a linear
setting that measures the overlap between autoregressive and perception feature
subspaces. This metric can be easily computed in closed form from pretrained
representations and labeled data, and is proven to both upper- and lower-bound
the excess loss. Empirically, we show that NTPS correlates strongly with linear
probe accuracy across 12 diverse NLP datasets and eight pretrained models
ranging from 270M to 8B parameters, confirming its utility as a measure of
alignment. Furthermore, we show that NTPS increases following low-rank
adaptation (LoRA) fine-tuning, especially in large models, suggesting that LoRA
aligning representations to perception tasks enhances subspace overlap and thus
improves downstream performance. More importantly, we find that NTPS reliably
predicts the additional accuracy gains attained by LoRA finetuning thereby
providing a lightweight prescreening tool for LoRA adaptation. Our results
offer both theoretical insights and practical tools for analytically assessing
LLM perception skills.

</details>


### [82] [FB-RAG: Improving RAG with Forward and Backward Lookup](https://arxiv.org/pdf/2505.17206)
*Kushal Chawla, Alfy Samuel, Anoop Kumar, Daben Liu*

Main category: cs.CL

TL;DR: FB-RAG improves RAG systems by combining backward and forward lookup to retrieve relevant context chunks, outperforming baselines in performance and latency.


<details>
  <summary>Details</summary>
Motivation: Addressing the challenge of balancing context size in RAG systems—too large includes irrelevant info, too small misses key details—especially for complex queries.

Method: Introduces FB-RAG, a framework using backward (query overlap) and forward (answer overlap) lookup to retrieve optimal context chunks.

Result: Outperforms RAG and Long Context baselines on 9 datasets, with improved performance and reduced latency.

Conclusion: FB-RAG effectively balances context relevance and size, offering insights for future RAG enhancements.

Abstract: The performance of Retrieval Augmented Generation (RAG) systems relies
heavily on the retriever quality and the size of the retrieved context. A large
enough context ensures that the relevant information is present in the input
context for the LLM, but also incorporates irrelevant content that has been
shown to confuse the models. On the other hand, a smaller context reduces the
irrelevant information, but it often comes at the risk of losing important
information necessary to answer the input question. This duality is especially
challenging to manage for complex queries that contain little information to
retrieve the relevant chunks from the full context. To address this, we present
a novel framework, called FB-RAG, which enhances the RAG pipeline by relying on
a combination of backward lookup (overlap with the query) and forward lookup
(overlap with candidate reasons and answers) to retrieve specific context
chunks that are the most relevant for answering the input query. Our
evaluations on 9 datasets from two leading benchmarks show that FB-RAG
consistently outperforms RAG and Long Context baselines developed recently for
these benchmarks. We further show that FB-RAG can improve performance while
reducing latency. We perform qualitative analysis of the strengths and
shortcomings of our approach, providing specific insights to guide future work.

</details>


### [83] [Mitigating Gender Bias via Fostering Exploratory Thinking in LLMs](https://arxiv.org/pdf/2505.17217)
*Kangda Wei, Hasnat Md Abdullah, Ruihong Huang*

Main category: cs.CL

TL;DR: A novel framework reduces gender bias in LLMs by generating balanced story-judgment pairs and using DPO for optimization.


<details>
  <summary>Details</summary>
Motivation: Address gender bias in LLMs, which leads to unequal treatment of male and female subjects.

Method: Generate story pairs with male/female protagonists in identical scenarios, compare moral judgments, and guide models to produce balanced outputs. Use DPO for fine-tuning.

Result: Significant reduction in gender bias without compromising general model capabilities.

Conclusion: The framework effectively mitigates bias and enhances fairness in LLMs, with plans to release code and data.

Abstract: Large Language Models (LLMs) often exhibit gender bias, resulting in unequal
treatment of male and female subjects across different contexts. To address
this issue, we propose a novel data generation framework that fosters
exploratory thinking in LLMs. Our approach prompts models to generate story
pairs featuring male and female protagonists in structurally identical, morally
ambiguous scenarios, then elicits and compares their moral judgments. When
inconsistencies arise, the model is guided to produce balanced, gender-neutral
judgments. These story-judgment pairs are used to fine-tune or optimize the
models via Direct Preference Optimization (DPO). Experimental results show that
our method significantly reduces gender bias while preserving or even enhancing
general model capabilities. We will release the code and generated data.

</details>


### [84] [ExeSQL: Self-Taught Text-to-SQL Models with Execution-Driven Bootstrapping for SQL Dialects](https://arxiv.org/pdf/2505.17231)
*Jipeng Zhang, Haolin Yang, Kehao Miao, Ruiyuan Zhang, Renjie Pi, Jiahui Gao, Xiaofang Zhou*

Main category: cs.CL

TL;DR: ExeSQL improves text-to-SQL performance across multiple dialects by using execution-driven bootstrapping, outperforming GPT-4o.


<details>
  <summary>Details</summary>
Motivation: Current text-to-SQL models are limited to SQLite due to dataset constraints, while real-world applications require dialect-aware SQL generation.

Method: ExeSQL employs iterative query generation, execution-based filtering, and preference-based training to adapt to new SQL dialects.

Result: ExeSQL achieves average improvements of 15.2%, 10.38%, and 4.49% over GPT-4o on PostgreSQL, MySQL, and Oracle, respectively.

Conclusion: ExeSQL effectively bridges the dialect gap in text-to-SQL through verifiable, feedback-guided learning.

Abstract: Recent text-to-SQL models have achieved strong performance, but their
effectiveness remains largely confined to SQLite due to dataset limitations.
However, real-world applications require SQL generation across multiple
dialects with varying syntax and specialized features, which remains a
challenge for current models. The main obstacle in building a dialect-aware
model lies in acquiring high-quality dialect-specific data. Data generated
purely through static prompting - without validating SQLs via execution - tends
to be noisy and unreliable. Moreover, the lack of real execution environments
in the training loop prevents models from grounding their predictions in
executable semantics, limiting generalization despite surface-level
improvements from data filtering. This work introduces ExeSQL, a text-to-SQL
framework with execution-driven, agentic bootstrapping. The method consists of
iterative query generation, execution-based filtering (e.g., rejection
sampling), and preference-based training, enabling the model to adapt to new
SQL dialects through verifiable, feedback-guided learning. Experiments show
that ExeSQL bridges the dialect gap in text-to-SQL, achieving average
improvements of 15.2%, 10.38%, and 4.49% over GPT-4o on PostgreSQL, MySQL, and
Oracle, respectively, across multiple datasets of varying difficulty.

</details>


### [85] [Humans Hallucinate Too: Language Models Identify and Correct Subjective Annotation Errors With Label-in-a-Haystack Prompts](https://arxiv.org/pdf/2505.17222)
*Georgios Chochlakis, Peter Wu, Arjun Bedi, Marcus Ma, Kristina Lerman, Shrikanth Narayanan*

Main category: cs.CL

TL;DR: The paper proposes a framework (LiaHR) using LLMs to correct subjective labels in NLP tasks by leveraging label verification and divergence analysis.


<details>
  <summary>Details</summary>
Motivation: Addressing the challenge of human annotation variability in subjective NLP tasks, where differences reflect legitimate subjectivity rather than noise.

Method: Introduces Label-in-a-Haystack (LiaH) setting and LiaHR framework, using LLMs to predict labels and rectify divergences from gold labels.

Result: LiaHR improves label correction, validated through analyses, human evaluations, and ecological studies.

Conclusion: LiaHR enhances annotation pipelines by distinguishing legitimate subjectivity from errors, improving signal-to-noise ratios.

Abstract: Modeling complex subjective tasks in Natural Language Processing, such as
recognizing emotion and morality, is considerably challenging due to
significant variation in human annotations. This variation often reflects
reasonable differences in semantic interpretations rather than mere noise,
necessitating methods to distinguish between legitimate subjectivity and error.
We address this challenge by exploring label verification in these contexts
using Large Language Models (LLMs). First, we propose a simple In-Context
Learning binary filtering baseline that estimates the reasonableness of a
document-label pair. We then introduce the Label-in-a-Haystack setting: the
query and its label(s) are included in the demonstrations shown to LLMs, which
are prompted to predict the label(s) again, while receiving task-specific
instructions (e.g., emotion recognition) rather than label copying. We show how
the failure to copy the label(s) to the output of the LLM are task-relevant and
informative. Building on this, we propose the Label-in-a-Haystack Rectification
(LiaHR) framework for subjective label correction: when the model outputs
diverge from the reference gold labels, we assign the generated labels to the
example instead of discarding it. This approach can be integrated into
annotation pipelines to enhance signal-to-noise ratios. Comprehensive analyses,
human evaluations, and ecological validity studies verify the utility of LiaHR
for label correction. Code is available at https://github.com/gchochla/LiaHR.

</details>


### [86] [Personalizing Student-Agent Interactions Using Log-Contextualized Retrieval Augmented Generation (RAG)](https://arxiv.org/pdf/2505.17238)
*Clayton Cohn, Surya Rayala, Caitlin Snyder, Joyce Fonteles, Shruti Jain, Naveeduddin Mohammed, Umesh Timalsina, Sarah K. Burriss, Ashwin T S, Namrata Srivastava, Menton Deweese, Angela Eeds, Gautam Biswas*

Main category: cs.CL

TL;DR: LC-RAG improves retrieval in collaborative learning by using environment logs to contextualize dialogue, enhancing personalized guidance for students.


<details>
  <summary>Details</summary>
Motivation: To address weak semantic links in student dialogue and improve the reliability of LLM-based pedagogical agents in STEM+C settings.

Method: Proposes log-contextualized RAG (LC-RAG), which integrates environment logs with retrieval-augmented generation to better contextualize collaborative discourse.

Result: LC-RAG outperforms discourse-only baselines, enabling the agent Copa to provide relevant, personalized guidance in a computational modeling environment.

Conclusion: LC-RAG enhances the effectiveness of pedagogical agents by grounding LLM outputs in contextualized knowledge, supporting student learning and critical thinking.

Abstract: Collaborative dialogue offers rich insights into students' learning and
critical thinking. This is essential for adapting pedagogical agents to
students' learning and problem-solving skills in STEM+C settings. While large
language models (LLMs) facilitate dynamic pedagogical interactions, potential
hallucinations can undermine confidence, trust, and instructional value.
Retrieval-augmented generation (RAG) grounds LLM outputs in curated knowledge,
but its effectiveness depends on clear semantic links between user input and a
knowledge base, which are often weak in student dialogue. We propose
log-contextualized RAG (LC-RAG), which enhances RAG retrieval by incorporating
environment logs to contextualize collaborative discourse. Our findings show
that LC-RAG improves retrieval over a discourse-only baseline and allows our
collaborative peer agent, Copa, to deliver relevant, personalized guidance that
supports students' critical thinking and epistemic decision-making in a
collaborative computational modeling environment, XYZ.

</details>


### [87] [ReasoningShield: Content Safety Detection over Reasoning Traces of Large Reasoning Models](https://arxiv.org/pdf/2505.17244)
*Changyi Li, Jiayi Wang, Xudong Pan, Geng Hong, Min Yang*

Main category: cs.CL

TL;DR: ReasoningShield is a safety detection model for identifying hidden risks in reasoning traces of Large Reasoning Models (LRMs), outperforming existing moderation tools with high accuracy.


<details>
  <summary>Details</summary>
Motivation: Existing moderation tools fail to detect unsafe content in reasoning traces, even when final answers appear safe, necessitating a specialized solution.

Method: Proposed ReasoningShield, trained on a synthesized dataset of 8,000 question-thought pairs across ten risk categories, using a human-AI collaborative annotation pipeline.

Result: Achieves an average F1 score of 0.92, outperforming mainstream models, and shows competitive performance on traditional QA benchmarks.

Conclusion: ReasoningShield effectively addresses hidden risks in reasoning traces, validated by high performance and dataset quality, and is designed for lightweight deployment.

Abstract: Large Reasoning Models (LRMs) are transforming the AI landscape with advanced
reasoning capabilities. While the generated reasoning traces enhance model
transparency, they can still contain unsafe content, even when the final answer
appears safe. Existing moderation tools, primarily designed for question-answer
(QA) pairs, are empirically ineffective at detecting hidden risks embedded in
reasoning traces. After identifying the key challenges, we formally define the
question-thought (QT) moderation task and propose ReasoningShield, the first
safety detection model tailored to identify potential risks in the reasoning
trace before reaching the final answer. To construct the model, we synthesize a
high-quality reasoning safety detection dataset comprising over 8,000
question-thought pairs spanning ten risk categories and three safety levels.
Our dataset construction process incorporates a comprehensive human-AI
collaborative annotation pipeline, which achieves over 93% annotation accuracy
while significantly reducing human costs. On a diverse set of in-distribution
and out-of-distribution benchmarks, ReasoningShield outperforms mainstream
content safety moderation models in identifying risks within reasoning traces,
with an average F1 score exceeding 0.92. Notably, despite being trained on our
QT dataset only, ReasoningShield also demonstrates competitive performance in
detecting unsafe question-answer pairs on traditional benchmarks, rivaling
baselines trained on 10 times larger datasets and base models, which strongly
validates the quality of our dataset. Furthermore, ReasoningShield is built
upon compact 1B/3B base models to facilitate lightweight deployment and
provides human-friendly risk analysis by default. To foster future research, we
publicly release all the resources.

</details>


### [88] [ConciseRL: Conciseness-Guided Reinforcement Learning for Efficient Reasoning Models](https://arxiv.org/pdf/2505.17250)
*Razvan-Gabriel Dumitru, Darius Peteleaza, Vikas Yadav, Liangming Pan*

Main category: cs.CL

TL;DR: A method using a conciseness score in reinforcement learning improves reasoning efficiency and accuracy in large language models, reducing token usage and enhancing performance on tasks like MATH and TheoremQA.


<details>
  <summary>Details</summary>
Motivation: To address wasted computation, reduced readability, and hallucinations caused by overly long reasoning traces in large language models.

Method: Introduces a hyperparameter-free conciseness score as a reward signal in reinforcement learning, evaluated by a large language model judge for dynamic feedback.

Result: Achieves state-of-the-art efficiency-accuracy trade-offs, reducing tokens by up to 31x on simple problems and improving accuracy by 7%; also outperforms on harder problems and TheoremQA.

Conclusion: The method dynamically adapts reasoning length, benefits from stronger judges, and is open-sourced for broader use.

Abstract: Large language models excel at complex tasks by breaking down problems into
structured reasoning steps. However, reasoning traces often extend beyond
reaching a correct answer, causing wasted computation, reduced readability, and
hallucinations. To address this, we introduce a novel hyperparameter-free
conciseness score used as a reward signal within a reinforcement learning
framework to guide models toward generating correct and concise reasoning
traces. This score is evaluated by a large language model acting as a judge,
enabling dynamic, context-aware feedback beyond simple token length. Our method
achieves state-of-the-art efficiency-accuracy trade-offs on the MATH dataset,
reducing token usage by up to 31x on simple problems while improving accuracy
by 7%, and on the hardest problems, it outperforms full reasoning by +7.5%
accuracy with up to 3.6x fewer tokens. On TheoremQA, our method improves
accuracy by +2.2% using 12.5x fewer tokens. We also conduct ablation studies on
the judge model, reward composition, and problem difficulty, showing that our
method dynamically adapts reasoning length based on problem difficulty and
benefits significantly from stronger judges. The code, model weights, and
datasets are open-sourced at https://github.com/RazvanDu/ConciseRL.

</details>


### [89] [CaseReportBench: An LLM Benchmark Dataset for Dense Information Extraction in Clinical Case Reports](https://arxiv.org/pdf/2505.17265)
*Xiao Yu Cindy Zhang, Carlos R. Ferreira, Francis Rossignol, Raymond T. Ng, Wyeth Wasserman, Jian Zhu*

Main category: cs.CL

TL;DR: The paper introduces CaseReportBench, a dataset for dense information extraction from case reports on rare diseases, evaluates LLMs for this task, and finds Qwen2.5-7B outperforms GPT-4o.


<details>
  <summary>Details</summary>
Motivation: Rare diseases like IEMs are hard to diagnose, and case reports are underutilized. LLMs could help extract structured information from these reports.

Method: The study uses CaseReportBench to test models and prompting strategies, including category-specific prompting and subheading-filtered data integration.

Result: Qwen2.5-7B outperforms GPT-4o, and category-specific prompting improves alignment. LLMs show promise but struggle with negative findings.

Conclusion: LLMs can aid rare disease diagnosis, but improvements are needed, especially for negative findings. This work advances clinical NLP.

Abstract: Rare diseases, including Inborn Errors of Metabolism (IEM), pose significant
diagnostic challenges. Case reports serve as key but computationally
underutilized resources to inform diagnosis. Clinical dense information
extraction refers to organizing medical information into structured predefined
categories. Large Language Models (LLMs) may enable scalable information
extraction from case reports but are rarely evaluated for this task. We
introduce CaseReportBench, an expert-annotated dataset for dense information
extraction of case reports, focusing on IEMs. Using this dataset, we assess
various models and prompting strategies, introducing novel approaches such as
category-specific prompting and subheading-filtered data integration. Zero-shot
chain-of-thought prompting offers little advantage over standard zero-shot
prompting. Category-specific prompting improves alignment with the benchmark.
The open-source model Qwen2.5-7B outperforms GPT-4o for this task. Our
clinician evaluations show that LLMs can extract clinically relevant details
from case reports, supporting rare disease diagnosis and management. We also
highlight areas for improvement, such as LLMs' limitations in recognizing
negative findings important for differential diagnosis. This work advances
LLM-driven clinical natural language processing and paves the way for scalable
medical AI applications.

</details>


### [90] [The Rise of Parameter Specialization for Knowledge Storage in Large Language Models](https://arxiv.org/pdf/2505.17260)
*Yihuai Hong, Yiran Zhao, Wei Tang, Yang Deng, Yu Rong, Wenxuan Zhang*

Main category: cs.CL

TL;DR: The paper investigates how knowledge is stored in MLP parameters of large language models, finding that advanced models exhibit specialized parameter distributions, which improve knowledge utilization efficiency.


<details>
  <summary>Details</summary>
Motivation: To understand how knowledge storage in MLP parameters affects model performance and efficiency, given limited prior research on this microscopic aspect.

Method: Analysis of twenty open-source large language models, focusing on MLP parameter specialization and its impact on knowledge utilization. Experimental validation through causal training.

Result: Advanced models show specialized MLP parameters, encoding similar knowledge types, which enhances knowledge utilization efficiency. Causal training confirms this specialization's critical role.

Conclusion: Specialized knowledge distribution in MLP parameters improves model efficiency, highlighting its importance for optimizing large language models.

Abstract: Over time, a growing wave of large language models from various series has
been introduced to the community. Researchers are striving to maximize the
performance of language models with constrained parameter sizes. However, from
a microscopic perspective, there has been limited research on how to better
store knowledge in model parameters, particularly within MLPs, to enable more
effective utilization of this knowledge by the model. In this work, we analyze
twenty publicly available open-source large language models to investigate the
relationship between their strong performance and the way knowledge is stored
in their corresponding MLP parameters. Our findings reveal that as language
models become more advanced and demonstrate stronger knowledge capabilities,
their parameters exhibit increased specialization. Specifically, parameters in
the MLPs tend to be more focused on encoding similar types of knowledge. We
experimentally validate that this specialized distribution of knowledge
contributes to improving the efficiency of knowledge utilization in these
models. Furthermore, by conducting causal training experiments, we confirm that
this specialized knowledge distribution plays a critical role in improving the
model's efficiency in leveraging stored knowledge.

</details>


### [91] [Select2Reason: Efficient Instruction-Tuning Data Selection for Long-CoT Reasoning](https://arxiv.org/pdf/2505.17266)
*Cehao Yang, Xueyuan Lin, Chengjin Xu, Xuhui Jiang, Xiaojun Wu, Honghao Liu, Hui Xiong, Jian Guo*

Main category: cs.CL

TL;DR: Select2Reason is a framework for selecting high-quality long-CoT reasoning instructions, reducing training overhead while maintaining performance.


<details>
  <summary>Details</summary>
Motivation: To address the inefficiency of large-scale instruction datasets and lack of strategies for selecting long-CoT reasoning instructions.

Method: Proposes Select2Reason, a framework using difficulty estimation and reasoning trace length heuristics to rank instructions.

Result: Fine-tuning on 10% of Select2Reason-selected data matches or outperforms full-data tuning on benchmarks.

Conclusion: Select2Reason is scalable, efficient, and adaptable, offering a cost-effective solution for long-CoT reasoning.

Abstract: A practical approach to activate long chain-of-thoughts reasoning ability in
pre-trained large language models is to perform supervised fine-tuning on
instruction datasets synthesized by strong Large Reasoning Models such as
DeepSeek-R1, offering a cost-effective alternative to reinforcement learning.
However, large-scale instruction sets with more than 100k samples incur
significant training overhead, while effective strategies for automatic
long-CoT instruction selection still remain unexplored. In this work, we
propose Select2Reason, a novel and efficient instruction-tuning data selection
framework for long-CoT reasoning. From the perspective of emergence of
rethinking behaviors like self-correction and backtracking, we investigate
common metrics that may determine the quality of long-CoT reasoning
instructions. Select2Reason leverages a quantifier to estimate difficulty of
question and jointly incorporates a reasoning trace length-based heuristic
through a weighted scheme for ranking to prioritize high-utility examples.
Empirical results on OpenR1-Math-220k demonstrate that fine-tuning LLM on only
10% of the data selected by Select2Reason achieves performance competitive with
or superior to full-data tuning and open-source baseline OpenR1-Qwen-7B across
three competition-level and six comprehensive mathematical benchmarks. Further
experiments highlight the scalability in varying data size, efficiency during
inference, and its adaptability to other instruction pools with minimal cost.

</details>


### [92] [GreekBarBench: A Challenging Benchmark for Free-Text Legal Reasoning and Citations](https://arxiv.org/pdf/2505.17267)
*Odysseas S. Chlapanis, Dimitrios Galanis, Nikolaos Aletras, Ion Androutsopoulos*

Main category: cs.CL

TL;DR: GreekBarBench evaluates LLMs on Greek Bar exam legal questions using a three-dimensional scoring system and LLM-as-a-judge approach. Meta-evaluation shows span-based rubrics improve alignment with human experts. Top LLMs outperform average experts but lag behind the top 5%.


<details>
  <summary>Details</summary>
Motivation: To assess LLMs' performance on legal questions requiring citations and case facts, addressing free-text evaluation challenges.

Method: Developed GreekBarBench with a three-dimensional scoring system and LLM-as-a-judge approach, plus a meta-evaluation benchmark.

Result: Best LLMs outperform average expert scores but don't reach the top 5% of human experts. Span-based rubrics improve LLM-human alignment.

Conclusion: LLMs show promise in legal evaluations but need improvement to match top human experts. Span-based rubrics enhance evaluation reliability.

Abstract: We introduce GreekBarBench, a benchmark that evaluates LLMs on legal
questions across five different legal areas from the Greek Bar exams, requiring
citations to statutory articles and case facts. To tackle the challenges of
free-text evaluation, we propose a three-dimensional scoring system combined
with an LLM-as-a-judge approach. We also develop a meta-evaluation benchmark to
assess the correlation between LLM-judges and human expert evaluations,
revealing that simple, span-based rubrics improve their alignment. Our
systematic evaluation of 13 proprietary and open-weight LLMs shows that even
though the best models outperform average expert scores, they fall short of the
95th percentile of experts.

</details>


### [93] [Search Wisely: Mitigating Sub-optimal Agentic Searches By Reducing Uncertainty](https://arxiv.org/pdf/2505.17281)
*Peilin Wu, Mian Zhang, Xinlu Zhang, Xinya Du, Zhiyu Zoey Chen*

Main category: cs.CL

TL;DR: The paper identifies and quantifies inefficiencies (over-search and under-search) in Agentic RAG systems, links them to model uncertainty, and proposes $eta$-GRPO, a reinforcement learning method, to improve search decisions.


<details>
  <summary>Details</summary>
Motivation: Agentic RAG systems often perform inefficient searches (over-search and under-search), reducing reliability. The study aims to understand and mitigate these behaviors.

Method: The work formally defines and quantifies search inefficiencies, links them to model uncertainty, and introduces $eta$-GRPO, a reinforcement learning method with confidence thresholds.

Result: Experiments show $eta$-GRPO improves a 3B model's RAG ability, achieving a 4% higher average exact match score on seven QA benchmarks.

Conclusion: $eta$-GRPO effectively addresses search inefficiencies in Agentic RAG systems by leveraging model uncertainty, enhancing performance.

Abstract: Agentic Retrieval-Augmented Generation (RAG) systems enhance Large Language
Models (LLMs) by enabling dynamic, multi-step reasoning and information
retrieval. However, these systems often exhibit sub-optimal search behaviors
like over-search (retrieving redundant information) and under-search (failing
to retrieve necessary information), which hinder efficiency and reliability.
This work formally defines and quantifies these behaviors, revealing their
prevalence across multiple QA datasets and agentic RAG systems (e.g., one model
could have avoided searching in 27.7% of its search steps). Furthermore, we
demonstrate a crucial link between these inefficiencies and the models'
uncertainty regarding their own knowledge boundaries, where response accuracy
correlates with model's uncertainty in its search decisions. To address this,
we propose $\beta$-GRPO, a reinforcement learning-based training method that
incorporates confidence threshold to reward high-certainty search decisions.
Experiments on seven QA benchmarks show that $\beta$-GRPO enable a 3B model
with better agentic RAG ability, outperforming other strong baselines with a 4%
higher average exact match score.

</details>


### [94] [SELF: Self-Extend the Context Length With Logistic Growth Function](https://arxiv.org/pdf/2505.17296)
*Phat Thanh Dang, Saahil Thoppay, Wang Yang, Qifan Wang, Vipin Chaudhary, Xiaotian Han*

Main category: cs.CL

TL;DR: SELF (Self-Extend the Context Length With Logistic Growth Function) improves large language model performance on long contexts by grouping tokens with a logistic capacity equation, outperforming LongLM by up to 12%.


<details>
  <summary>Details</summary>
Motivation: Large language models struggle with long contexts beyond their training length due to standard position encoding, leading to ineffective token interactions and unexpected results.

Method: Proposes SELF, which groups consecutive tokens at varying sizes using a logistic capacity equation, maintaining constant group sizes for smaller distances.

Result: Outperformed LongLM by up to 12% on LEval (Qwen model), 6.4% on LongBench summarization (Llama-2-7b), and 5.4% on LEval reading comprehension.

Conclusion: SELF effectively extends context length and improves performance on long-context tasks, with code publicly available.

Abstract: Large language models suffer issues when operated on long contexts that are
larger than their training context length due to the standard position encoding
for tokens in the attention layer. Tokens a long distance apart will rarely
have an effect on each other and long prompts yield unexpected results. To
solve this problem, we propose SELF (Self-Extend the Context Length With
Logistic Growth Function): a solution of grouping consecutive tokens at varying
group sizes using a logistic capacity equation combined with a constant group
size at smaller relative distances. Our model had an increase in performance of
up to 12% compared to the LongLM extension method in LEval (specifically on the
Qwen model). On summarization related tasks in LongBench, our model performed
up to 6.4% better than LongLM (specifically on the Llama-2-7b model). On
reading comprehension tasks from LEval, our model performed up to 5.4% better
than the LongLM. Our code is available at https://github.com/alexeipc/SELF-LLM.

</details>


### [95] [Refusal Direction is Universal Across Safety-Aligned Languages](https://arxiv.org/pdf/2505.17306)
*Xinpeng Wang, Mingyang Wang, Yihong Liu, Hinrich Schütze, Barbara Plank*

Main category: cs.CL

TL;DR: The paper explores refusal behavior in LLMs across 14 languages, revealing a universal refusal direction that can bypass refusals cross-lingually without fine-tuning.


<details>
  <summary>Details</summary>
Motivation: Understanding refusal behavior in multilingual contexts is crucial for safety, yet poorly studied.

Method: Uses PolyRefuse, a multilingual dataset of translated prompts, to analyze refusal behavior and test the universality of refusal directions.

Result: Refusal directions from English or any safety-aligned language transfer effectively to others, indicating cross-lingual universality.

Conclusion: The findings highlight vulnerabilities in multilingual LLMs and offer insights for improving safety defenses.

Abstract: Refusal mechanisms in large language models (LLMs) are essential for ensuring
safety. Recent research has revealed that refusal behavior can be mediated by a
single direction in activation space, enabling targeted interventions to bypass
refusals. While this is primarily demonstrated in an English-centric context,
appropriate refusal behavior is important for any language, but poorly
understood. In this paper, we investigate the refusal behavior in LLMs across
14 languages using PolyRefuse, a multilingual safety dataset created by
translating malicious and benign English prompts into these languages. We
uncover the surprising cross-lingual universality of the refusal direction: a
vector extracted from English can bypass refusals in other languages with
near-perfect effectiveness, without any additional fine-tuning. Even more
remarkably, refusal directions derived from any safety-aligned language
transfer seamlessly to others. We attribute this transferability to the
parallelism of refusal vectors across languages in the embedding space and
identify the underlying mechanism behind cross-lingual jailbreaks. These
findings provide actionable insights for building more robust multilingual
safety defenses and pave the way for a deeper mechanistic understanding of
cross-lingual vulnerabilities in LLMs.

</details>


### [96] [From Compression to Expansion: A Layerwise Analysis of In-Context Learning](https://arxiv.org/pdf/2505.17322)
*Jiachen Jiang, Yuxin Dong, Jinxin Zhou, Zhihui Zhu*

Main category: cs.CL

TL;DR: The paper investigates the internal mechanisms of in-context learning (ICL) in large language models (LLMs), revealing a layerwise compression-expansion phenomenon where early layers compact task information and later layers expand it for predictions.


<details>
  <summary>Details</summary>
Motivation: To understand how task-specific information is represented and processed across layers in ICL, given its strong empirical performance but unclear internal mechanisms.

Method: A statistical geometric analysis of ICL representations across layers, examining how task information is captured and transformed.

Result: Discovery of the Layerwise Compression-Expansion phenomenon, showing improved performance with model size and demonstration count, and robustness to noise.

Conclusion: The study highlights structured layerwise dynamics in ICL, demonstrating how analyzing internal representations can deepen understanding of LLM behavior.

Abstract: In-context learning (ICL) enables large language models (LLMs) to adapt to
new tasks without weight updates by learning from demonstration sequences.
While ICL shows strong empirical performance, its internal representational
mechanisms are not yet well understood. In this work, we conduct a statistical
geometric analysis of ICL representations to investigate how task-specific
information is captured across layers. Our analysis reveals an intriguing
phenomenon, which we term *Layerwise Compression-Expansion*: early layers
progressively produce compact and discriminative representations that encode
task information from the input demonstrations, while later layers expand these
representations to incorporate the query and generate the prediction. This
phenomenon is observed consistently across diverse tasks and a range of
contemporary LLM architectures. We demonstrate that it has important
implications for ICL performance -- improving with model size and the number of
demonstrations -- and for robustness in the presence of noisy examples. To
further understand the effect of the compact task representation, we propose a
bias-variance decomposition and provide a theoretical analysis showing how
attention mechanisms contribute to reducing both variance and bias, thereby
enhancing performance as the number of demonstrations increases. Our findings
reveal an intriguing layerwise dynamic in ICL, highlight how structured
representations emerge within LLMs, and showcase that analyzing internal
representations can facilitate a deeper understanding of model behavior.

</details>


### [97] [GPT Editors, Not Authors: The Stylistic Footprint of LLMs in Academic Preprints](https://arxiv.org/pdf/2505.17327)
*Soren DeHaan, Yuanze Liu, Johan Bollen, Sa'ul A. Blanco*

Main category: cs.CL

TL;DR: LLMs are used uniformly in academic writing, reducing hallucination risks, as stylistic segmentation doesn't predict LLM use.


<details>
  <summary>Details</summary>
Motivation: To assess how LLMs are used in academic writing—whether for generating critical text or just editing.

Method: Analyzed arXiv papers using stylistic segmentation with a PELT threshold and a Bayesian classifier trained on GPT-regenerated text.

Result: LLM-attributed language doesn't predict stylistic segmentation, indicating uniform LLM use.

Conclusion: Authors use LLMs uniformly, minimizing hallucination risks in academic preprints.

Abstract: The proliferation of Large Language Models (LLMs) in late 2022 has impacted
academic writing, threatening credibility, and causing institutional
uncertainty. We seek to determine the degree to which LLMs are used to generate
critical text as opposed to being used for editing, such as checking for
grammar errors or inappropriate phrasing. In our study, we analyze arXiv papers
for stylistic segmentation, which we measure by varying a PELT threshold
against a Bayesian classifier trained on GPT-regenerated text. We find that
LLM-attributed language is not predictive of stylistic segmentation, suggesting
that when authors use LLMs, they do so uniformly, reducing the risk of
hallucinations being introduced into academic preprints.

</details>


### [98] [A Fully Generative Motivational Interviewing Counsellor Chatbot for Moving Smokers Towards the Decision to Quit](https://arxiv.org/pdf/2505.17362)
*Zafarullah Mahmood, Soliman Ali, Jiading Zhu, Mohamed Abdelwahab, Michelle Yu Collins, Sihan Chen, Yi Cheng Zhao, Jodi Wolff, Osnat Melamed, Nadia Minian, Marta Maslej, Carolynne Cooper, Matt Ratto, Peter Selby, Jonathan Rose*

Main category: cs.CL

TL;DR: A chatbot using LLMs and Motivational Interviewing (MI) effectively increased smokers' confidence to quit, adhering to MI standards better than humans, though perceived empathy was lower.


<details>
  <summary>Details</summary>
Motivation: To evaluate if LLMs can effectively perform as automated talk therapists, adhering to therapeutic standards like MI.

Method: Developed a chatbot using a state-of-the-art LLM and MI, tested on 106 smokers with pre- and post-conversation confidence measurements. Automated assessments validated MI adherence and client responses.

Result: Participants' confidence increased by 1.7 points (0-10 scale). The chatbot adhered to MI in 98% of utterances, outperforming humans, but scored lower on perceived empathy. Participants showed high motivation to change.

Conclusion: LLM-based chatbots show promise for automated talk therapy, particularly in adherence to standards, though empathy remains a challenge.

Abstract: The conversational capabilities of Large Language Models (LLMs) suggest that
they may be able to perform as automated talk therapists. It is crucial to know
if these systems would be effective and adhere to known standards. We present a
counsellor chatbot that focuses on motivating tobacco smokers to quit smoking.
It uses a state-of-the-art LLM and a widely applied therapeutic approach called
Motivational Interviewing (MI), and was evolved in collaboration with
clinician-scientists with expertise in MI. We also describe and validate an
automated assessment of both the chatbot's adherence to MI and client
responses. The chatbot was tested on 106 participants, and their confidence
that they could succeed in quitting smoking was measured before the
conversation and one week later. Participants' confidence increased by an
average of 1.7 on a 0-10 scale. The automated assessment of the chatbot showed
adherence to MI standards in 98% of utterances, higher than human counsellors.
The chatbot scored well on a participant-reported metric of perceived empathy
but lower than typical human counsellors. Furthermore, participants' language
indicated a good level of motivation to change, a key goal in MI. These results
suggest that the automation of talk therapy with a modern LLM has promise.

</details>


### [99] [Language models should be subject to repeatable, open, domain-contextualized hallucination benchmarking](https://arxiv.org/pdf/2505.17345)
*Justin D. Norman, Michael U. Rivera, D. Alex Hughes*

Main category: cs.CL

TL;DR: The paper addresses the lack of scientific measurement of language model hallucination, proposing repeatable, open, and contextualized benchmarking. It highlights issues with current metrics when experts are excluded early in data creation.


<details>
  <summary>Details</summary>
Motivation: To address the pervasive issue of plausible but inaccurate tokens in model-generated text and the lack of comprehensive measurement of language model hallucination.

Method: Proposes a taxonomy of hallucinations and a case study showing the impact of expert absence in early data creation on hallucination metrics.

Result: Demonstrates that current hallucination metrics lack validity and practical utility without expert involvement in data creation.

Conclusion: Advocates for repeatable, open, and domain-contextualized hallucination benchmarking to improve language model evaluation.

Abstract: Plausible, but inaccurate, tokens in model-generated text are widely believed
to be pervasive and problematic for the responsible adoption of language
models. Despite this concern, there is little scientific work that attempts to
measure the prevalence of language model hallucination in a comprehensive way.
In this paper, we argue that language models should be evaluated using
repeatable, open, and domain-contextualized hallucination benchmarking. We
present a taxonomy of hallucinations alongside a case study that demonstrates
that when experts are absent from the early stages of data creation, the
resulting hallucination metrics lack validity and practical utility.

</details>


### [100] [AI-Augmented LLMs Achieve Therapist-Level Responses in Motivational Interviewing](https://arxiv.org/pdf/2505.17380)
*Yinghui Huang, Yuxuan Jiang, Hui Liu, Yixin Cai, Weiqing Li, Xiangen Hu*

Main category: cs.CL

TL;DR: The paper evaluates GPT-4's potential for motivational interviewing (MI) in addiction care, using a computational framework to assess quality and improve performance through prompt engineering.


<details>
  <summary>Details</summary>
Motivation: To systematically evaluate the therapeutic capabilities of large language models (LLMs) like GPT-4 in scaling motivational interviewing for addiction care.

Method: Developed a computational framework analyzing human and GPT-4 MI sessions, integrating deep learning and explainable AI to identify MI behaviors. Used customized prompts to enhance GPT-4's performance.

Result: GPT-4 improved in advice management and empathy but remained slightly inferior to human therapists. Prompt engineering enhanced quality, though complex emotional nuances were challenging.

Conclusion: The framework offers a pathway for optimizing LLM-based therapeutic tools, highlighting scalability potential and current limitations in clinical applications.

Abstract: Large language models (LLMs) like GPT-4 show potential for scaling
motivational interviewing (MI) in addiction care, but require systematic
evaluation of therapeutic capabilities. We present a computational framework
assessing user-perceived quality (UPQ) through expected and unexpected MI
behaviors. Analyzing human therapist and GPT-4 MI sessions via human-AI
collaboration, we developed predictive models integrating deep learning and
explainable AI to identify 17 MI-consistent (MICO) and MI-inconsistent (MIIN)
behavioral metrics. A customized chain-of-thought prompt improved GPT-4's MI
performance, reducing inappropriate advice while enhancing reflections and
empathy. Although GPT-4 remained marginally inferior to therapists overall, it
demonstrated superior advice management capabilities. The model achieved
measurable quality improvements through prompt engineering, yet showed
limitations in addressing complex emotional nuances. This framework establishes
a pathway for optimizing LLM-based therapeutic tools through targeted
behavioral metric analysis and human-AI co-evaluation. Findings highlight both
the scalability potential and current constraints of LLMs in clinical
communication applications.

</details>


### [101] [WiNGPT-3.0 Technical Report](https://arxiv.org/pdf/2505.17387)
*Boqin Zhuang, Chenxiao Song, Huitong Lu, Jiacheng Qiao, Mingqian Liu, Mingxing Yu, Ping Hong, Rui Li, Xiaoxia Song, Xiangjun Xu, Xu Chen, Yaoyao Ma, Yujie Gao*

Main category: cs.CL

TL;DR: WiNGPT-3.0, a 32B-parameter LLM, improves medical reasoning via multi-stage training (SFT & RL) and achieves strong scores (66.6 on MedCalc, 87.1 on MedQA-USMLE), showing RL's efficacy with limited data.


<details>
  <summary>Details</summary>
Motivation: Address limitations of LLMs in structured, interpretable medical reasoning and deployment challenges (computational resources, data privacy).

Method: Multi-stage training pipeline (general, medical, clinical reasoning) with SFT, RL, curated CoT datasets, reward models, and diagnostic simulations.

Result: Scores of 66.6 (MedCalc) and 87.1 (MedQA-USMLE); clinical reasoning improved from 58.1 to 62.5.

Conclusion: RL with limited data enhances medical reasoning, enabling more deployable and trustworthy LLMs in healthcare.

Abstract: Current Large Language Models (LLMs) exhibit significant limitations, notably
in structured, interpretable, and verifiable medical reasoning, alongside
practical deployment challenges related to computational resources and data
privacy. This report focused on the development of WiNGPT-3.0, the 32-billion
parameter LLMs, engineered with the objective of enhancing its capacity for
medical reasoning and exploring its potential for effective integration within
healthcare IT infrastructures. The broader aim is to advance towards clinically
applicable models. The approach involved a multi-stage training pipeline
tailored for general, medical, and clinical reasoning. This pipeline
incorporated supervised fine-tuning (SFT) and reinforcement learning (RL),
leveraging curated Long Chain-of-Thought (CoT) datasets, auxiliary reward
models, and an evidence-based diagnostic chain simulation. WiNGPT-3.0
demonstrated strong performance: specific model variants achieved scores of
66.6 on MedCalc and 87.1 on MedQA-USMLE. Furthermore, targeted training
improved performance on a clinical reasoning task from a baseline score of 58.1
to 62.5. These findings suggest that reinforcement learning, even when applied
with a limited dataset of only a few thousand examples, can enhance medical
reasoning accuracy. Crucially, this demonstration of RL's efficacy with limited
data and computation paves the way for more trustworthy and practically
deployable LLMs within clinical workflows and health information
infrastructures.

</details>


### [102] [Measuring diversity of synthetic prompts and data generated with fine-grained persona prompting](https://arxiv.org/pdf/2505.17390)
*Gauri Kambhatla, Chantal Shaib, Venkata Govindarajan*

Main category: cs.CL

TL;DR: Synthetic prompts from fine-grained personas are less diverse than human-written ones. Fine-grained personas don't significantly boost text diversity in LLMs.


<details>
  <summary>Details</summary>
Motivation: To measure the diversity of persona-driven synthetic data for LLMs and assess the impact of fine-grained persona details.

Method: Used lexical diversity and redundancy metrics to compare synthetic and human-written prompts. Tested LLMs of varying sizes with fine-grained and coarse personas.

Result: Synthetic prompts are less diverse than human ones. Fine-grained personas don't notably increase diversity, though larger models show some improvement.

Conclusion: Fine-grained personas don't significantly enhance diversity in LLM-generated text, despite some benefits with larger models.

Abstract: Fine-grained personas have recently been used for generating 'diverse'
synthetic data for pre-training and supervised fine-tuning of Large Language
Models (LLMs). In this work, we measure the diversity of persona-driven
synthetically generated prompts and responses with a suite of lexical diversity
and redundancy metrics. Firstly, we find that synthetic prompts/instructions
are significantly less diverse than human-written ones. Next, we sample
responses from LLMs of different sizes with fine-grained and coarse persona
descriptions to investigate how much fine-grained detail in persona
descriptions contribute to generated text diversity. We find that while
persona-prompting does improve lexical diversity (especially with larger
models), fine-grained detail in personas doesn't increase diversity noticeably.

</details>


### [103] [Discovering Forbidden Topics in Language Models](https://arxiv.org/pdf/2505.17441)
*Can Rager, Chris Wendler, Rohit Gandikota, David Bau*

Main category: cs.CL

TL;DR: The paper introduces refusal discovery, a task to identify topics a language model refuses to discuss, and presents LLM-crawler, a method using token prefilling to uncover forbidden topics. It benchmarks the method on several models, revealing censorship patterns and alignment issues.


<details>
  <summary>Details</summary>
Motivation: To detect biases, boundaries, and alignment failures in AI systems by identifying topics models refuse to discuss.

Method: Develops LLM-crawler, a refusal discovery method using token prefilling, and tests it on multiple models, including Tulu-3-8B, Claude-Haiku, and variants of Llama-3.3-70B.

Result: LLM-crawler successfully retrieves 31 out of 36 forbidden topics in Tulu-3-8B and reveals censorship patterns in models like DeepSeek-R1-70B and Perplexity-R1-1776-70B.

Conclusion: Refusal discovery methods are crucial for uncovering biases and alignment issues in AI systems, as demonstrated by the findings across various models.

Abstract: Refusal discovery is the task of identifying the full set of topics that a
language model refuses to discuss. We introduce this new problem setting and
develop a refusal discovery method, LLM-crawler, that uses token prefilling to
find forbidden topics. We benchmark the LLM-crawler on Tulu-3-8B, an
open-source model with public safety tuning data. Our crawler manages to
retrieve 31 out of 36 topics within a budget of 1000 prompts. Next, we scale
the crawl to a frontier model using the prefilling option of Claude-Haiku.
Finally, we crawl three widely used open-weight models: Llama-3.3-70B and two
of its variants finetuned for reasoning: DeepSeek-R1-70B and
Perplexity-R1-1776-70B. DeepSeek-R1-70B reveals patterns consistent with
censorship tuning: The model exhibits "thought suppression" behavior that
indicates memorization of CCP-aligned responses. Although
Perplexity-R1-1776-70B is robust to censorship, LLM-crawler elicits CCP-aligned
refusals answers in the quantized model. Our findings highlight the critical
need for refusal discovery methods to detect biases, boundaries, and alignment
failures of AI systems.

</details>


### [104] [Curriculum Guided Reinforcement Learning for Efficient Multi Hop Retrieval Augmented Generation](https://arxiv.org/pdf/2505.17391)
*Yuelyu Ji, Rui Meng, Zhuochun Li, Daqing He*

Main category: cs.CL

TL;DR: EVO-RAG improves multi-hop RAG by using curriculum-guided RL to optimize query-rewriting, reducing redundancy and search depth while boosting accuracy.


<details>
  <summary>Details</summary>
Motivation: Existing multi-hop RAG pipelines are inefficient, issuing redundant subqueries or overly long search chains.

Method: EVO-RAG uses a curriculum-guided RL framework with a dynamic reward scheduler and Direct Preference Optimization.

Result: EVO-RAG improves Exact Match by up to 4.6 points and reduces retrieval depth by 15% on benchmarks.

Conclusion: EVO-RAG provides a reliable, cost-effective solution for multi-hop RAG systems.

Abstract: Retrieval-augmented generation (RAG) grounds large language models (LLMs) in
up-to-date external evidence, yet existing multi-hop RAG pipelines still issue
redundant subqueries, explore too shallowly, or wander through overly long
search chains. We introduce EVO-RAG, a curriculum-guided reinforcement learning
framework that evolves a query-rewriting agent from broad early-stage
exploration to concise late-stage refinement. EVO-RAG couples a seven-factor,
step-level reward vector (covering relevance, redundancy, efficiency, and
answer correctness) with a time-varying scheduler that reweights these signals
as the episode unfolds. The agent is trained with Direct Preference
Optimization over a multi-head reward model, enabling it to learn when to
search, backtrack, answer, or refuse. Across four multi-hop QA benchmarks
(HotpotQA, 2WikiMultiHopQA, MuSiQue, and Bamboogle), EVO-RAG boosts Exact Match
by up to 4.6 points over strong RAG baselines while trimming average retrieval
depth by 15 %. Ablation studies confirm the complementary roles of curriculum
staging and dynamic reward scheduling. EVO-RAG thus offers a general recipe for
building reliable, cost-effective multi-hop RAG systems.

</details>


### [105] [Towards Evaluating Proactive Risk Awareness of Multimodal Language Models](https://arxiv.org/pdf/2505.17455)
*Youliang Yuan, Wenxiang Jiao, Yuejin Xie, Chihao Shen, Menghan Tian, Wenxuan Wang, Jen-tse Huang, Pinjia He*

Main category: cs.CL

TL;DR: The paper introduces PaSBench, a benchmark for evaluating proactive safety AI systems, revealing limitations in current models despite high accuracy.


<details>
  <summary>Details</summary>
Motivation: Addressing human safety awareness gaps by developing AI that proactively detects risks rather than reacting to them.

Method: Uses PaSBench with 416 multimodal scenarios (image sequences and text logs) across 5 domains to evaluate 36 models.

Result: Top models like Gemini-2.5-pro achieve 71% image and 64% text accuracy but miss 45-55% risks; unstable reasoning is the main issue.

Conclusion: Establishes a proactive safety benchmark, highlights model limitations, and guides future AI development for reliable protection.

Abstract: Human safety awareness gaps often prevent the timely recognition of everyday
risks. In solving this problem, a proactive safety artificial intelligence (AI)
system would work better than a reactive one. Instead of just reacting to
users' questions, it would actively watch people's behavior and their
environment to detect potential dangers in advance. Our Proactive Safety Bench
(PaSBench) evaluates this capability through 416 multimodal scenarios (128
image sequences, 288 text logs) spanning 5 safety-critical domains. Evaluation
of 36 advanced models reveals fundamental limitations: Top performers like
Gemini-2.5-pro achieve 71% image and 64% text accuracy, but miss 45-55% risks
in repeated trials. Through failure analysis, we identify unstable proactive
reasoning rather than knowledge deficits as the primary limitation. This work
establishes (1) a proactive safety benchmark, (2) systematic evidence of model
limitations, and (3) critical directions for developing reliable protective AI.
We believe our dataset and findings can promote the development of safer AI
assistants that actively prevent harm rather than merely respond to requests.
Our dataset can be found at https://huggingface.co/datasets/Youliang/PaSBench.

</details>


### [106] [FullFront: Benchmarking MLLMs Across the Full Front-End Engineering Workflow](https://arxiv.org/pdf/2505.17399)
*Haoyu Sun, Huichen Will Wang, Jiawei Gu, Linjie Li, Yu Cheng*

Main category: cs.CL

TL;DR: FullFront is a benchmark evaluating MLLMs across the full front-end development pipeline, covering design, perception, and code generation, revealing gaps in current model capabilities.


<details>
  <summary>Details</summary>
Motivation: Existing benchmarks focus narrowly on code generation, ignoring the broader front-end pipeline. FullFront addresses this gap by evaluating MLLMs comprehensively.

Method: FullFront uses a two-stage process to transform real-world webpages into clean HTML, assessing three tasks: Webpage Design, Webpage Perception QA, and Webpage Code Generation.

Result: Testing shows MLLMs struggle with page perception, code generation (especially for images and layout), and interaction implementation, lagging behind human experts.

Conclusion: FullFront highlights limitations in current MLLMs for front-end tasks, providing a standardized benchmark for future improvements.

Abstract: Front-end engineering involves a complex workflow where engineers
conceptualize designs, translate them into code, and iteratively refine the
implementation. While recent benchmarks primarily focus on converting visual
designs to code, we present FullFront, a benchmark designed to evaluate
Multimodal Large Language Models (MLLMs) \textbf{across the full front-end
development pipeline}. FullFront assesses three fundamental tasks that map
directly to the front-end engineering pipeline: Webpage Design
(conceptualization phase), Webpage Perception QA (comprehension of visual
organization and elements), and Webpage Code Generation (implementation phase).
Unlike existing benchmarks that use either scraped websites with bloated code
or oversimplified LLM-generated HTML, FullFront employs a novel, two-stage
process to transform real-world webpages into clean, standardized HTML while
maintaining diverse visual designs and avoiding copyright issues. Extensive
testing of state-of-the-art MLLMs reveals significant limitations in page
perception, code generation (particularly for image handling and layout), and
interaction implementation. Our results quantitatively demonstrate performance
disparities across models and tasks, and highlight a substantial gap between
current MLLM capabilities and human expert performance in front-end
engineering. The FullFront benchmark and code are available in
https://github.com/Mikivishy/FullFront.

</details>


### [107] [SLearnLLM: A Self-Learning Framework for Efficient Domain-Specific Adaptation of Large Language Models](https://arxiv.org/pdf/2505.17470)
*Xiang Liu, Zhaoxiang Liu, Peng Wang, Kohou Wang, Huan Hu, Kai Wang, Shiguo Lian*

Main category: cs.CL

TL;DR: Proposes a self-learning framework for LLMs to identify and focus on unknown knowledge in SFT datasets, improving fine-tuning efficiency.


<details>
  <summary>Details</summary>
Motivation: Avoid wasted computational resources by fine-tuning only on unknown knowledge in the SFT dataset, rather than the entire dataset.

Method: LLMs answer and grade SFT dataset questions, filter incorrect QA pairs, and fine-tune on the filtered set.

Result: Reduces training time while achieving comparable improvements to full dataset fine-tuning in agriculture and medicine.

Conclusion: Focusing on unknown knowledge enhances fine-tuning efficiency for LLMs.

Abstract: When using supervised fine-tuning (SFT) to adapt large language models (LLMs)
to specific domains, a significant challenge arises: should we use the entire
SFT dataset for fine-tuning? Common practice often involves fine-tuning
directly on the entire dataset due to limited information on the LLM's past
training data. However, if the SFT dataset largely overlaps with the model's
existing knowledge, the performance gains are minimal, leading to wasted
computational resources. Identifying the unknown knowledge within the SFT
dataset and using it to fine-tune the model could substantially improve the
training efficiency. To address this challenge, we propose a self-learning
framework for LLMs inspired by human learning pattern. This framework takes a
fine-tuning (SFT) dataset in a specific domain as input. First, the LLMs answer
the questions in the SFT dataset. The LLMs then objectively grade the responses
and filter out the incorrectly answered QA pairs. Finally, we fine-tune the
LLMs based on this filtered QA set. Experimental results in the fields of
agriculture and medicine demonstrate that our method substantially reduces
training time while achieving comparable improvements to those attained with
full dataset fine-tuning. By concentrating on the unknown knowledge within the
SFT dataset, our approach enhances the efficiency of fine-tuning LLMs.

</details>


### [108] [Language Matters: How Do Multilingual Input and Reasoning Paths Affect Large Reasoning Models?](https://arxiv.org/pdf/2505.17407)
*Zhi Rui Tam, Cheng-Kuang Wu, Yu Ying Chiu, Chieh-Yen Lin, Yun-Nung Chen, Hung-yi Lee*

Main category: cs.CL

TL;DR: Large reasoning models (LRMs) default to reasoning in high-resource languages (e.g., English) regardless of input language, impacting performance and fairness.


<details>
  <summary>Details</summary>
Motivation: To investigate the internal reasoning processes of LRMs in multilingual settings and identify linguistic biases.

Method: Evaluated LRMs on reasoning-intensive tasks (MMMLU, MATH-500) and non-reasoning benchmarks (CulturalBench, LMSYS-toxic) under different language constraints.

Result: LRMs perform better when reasoning in high-resource languages, even for non-English inputs. Performance declines for low-resource languages when constrained to input-language reasoning.

Conclusion: The study highlights linguistic biases in LRMs, emphasizing the need for equitable models serving diverse linguistic backgrounds.

Abstract: Large reasoning models (LRMs) have demonstrated impressive performance across
a range of reasoning tasks, yet little is known about their internal reasoning
processes in multilingual settings. We begin with a critical question: {\it In
which language do these models reason when solving problems presented in
different languages?} Our findings reveal that, despite multilingual training,
LRMs tend to default to reasoning in high-resource languages (e.g., English) at
test time, regardless of the input language. When constrained to reason in the
same language as the input, model performance declines, especially for
low-resource languages. In contrast, reasoning in high-resource languages
generally preserves performance. We conduct extensive evaluations across
reasoning-intensive tasks (MMMLU, MATH-500) and non-reasoning benchmarks
(CulturalBench, LMSYS-toxic), showing that the effect of language choice varies
by task type: input-language reasoning degrades performance on reasoning tasks
but benefits cultural tasks, while safety evaluations exhibit language-specific
behavior. By exposing these linguistic biases in LRMs, our work highlights a
critical step toward developing more equitable models that serve users across
diverse linguistic backgrounds.

</details>


### [109] [Conversations: Love Them, Hate Them, Steer Them](https://arxiv.org/pdf/2505.17413)
*Niranjan Chebrolu, Gerard Christopher Yeo, Kokil Jaidka*

Main category: cs.CL

TL;DR: Targeted activation engineering improves emotional expression in LLMs by identifying key intervention points and applying emotional vectors.


<details>
  <summary>Details</summary>
Motivation: Enhancing LLMs' emotional nuance for more human-like conversational AI, addressing current limitations in alignment techniques.

Method: Uses attribution patching to find influential components and derives emotional vectors from contrastive text pairs for targeted steering.

Result: Steered responses show improved positive sentiment and personal engagement (e.g., joy, trust, first-person pronoun usage).

Conclusion: Provides a precise, interpretable method for controlling emotional attributes in LLMs, advancing empathetic AI development.

Abstract: Large Language Models (LLMs) demonstrate increasing conversational fluency,
yet instilling them with nuanced, human-like emotional expression remains a
significant challenge. Current alignment techniques often address surface-level
output or require extensive fine-tuning. This paper demonstrates that targeted
activation engineering can steer LLaMA 3.1-8B to exhibit more human-like
emotional nuances. We first employ attribution patching to identify causally
influential components, to find a key intervention locus by observing
activation patterns during diagnostic conversational tasks. We then derive
emotional expression vectors from the difference in the activations generated
by contrastive text pairs (positive vs. negative examples of target emotions).
Applying these vectors to new conversational prompts significantly enhances
emotional characteristics: steered responses show increased positive sentiment
(e.g., joy, trust) and more frequent first-person pronoun usage, indicative of
greater personal engagement. Our findings offer a precise and interpretable
method for controlling specific emotional attributes in LLMs, contributing to
developing more aligned and empathetic conversational AI.

</details>


### [110] [DASH: Input-Aware Dynamic Layer Skipping for Efficient LLM Inference with Markov Decision Policies](https://arxiv.org/pdf/2505.17420)
*Ning Yang, Fangxin Liu, Junjie Wang, Tao Yang, Kan Liu, Haibing Guan, Li Jiang*

Main category: cs.CL

TL;DR: DASH is an adaptive layer-skipping framework for LLMs that reduces inference costs by dynamically skipping layers based on input, using MDP and compensation mechanisms, while maintaining performance.


<details>
  <summary>Details</summary>
Motivation: The high inference cost of LLMs limits their real-world deployment, especially in latency-sensitive scenarios.

Method: DASH uses a Markov Decision Process for token-level layer-skipping decisions, a compensation mechanism for performance, and asynchronous execution to reduce overhead.

Result: Experiments show DASH achieves significant inference speedup while maintaining competitive task performance, outperforming existing methods.

Conclusion: DASH effectively balances inference efficiency and performance, making LLMs more practical for real-world use.

Abstract: Large language models (LLMs) have achieved remarkable performance across a
wide range of NLP tasks. However, their substantial inference cost poses a
major barrier to real-world deployment, especially in latency-sensitive
scenarios. To address this challenge, we propose \textbf{DASH}, an adaptive
layer-skipping framework that dynamically selects computation paths conditioned
on input characteristics. We model the skipping process as a Markov Decision
Process (MDP), enabling fine-grained token-level decisions based on
intermediate representations. To mitigate potential performance degradation
caused by skipping, we introduce a lightweight compensation mechanism that
injects differential rewards into the decision process. Furthermore, we design
an asynchronous execution strategy that overlaps layer computation with policy
evaluation to minimize runtime overhead. Experiments on multiple LLM
architectures and NLP benchmarks show that our method achieves significant
inference acceleration while maintaining competitive task performance,
outperforming existing methods.

</details>


### [111] [keepitsimple at SemEval-2025 Task 3: LLM-Uncertainty based Approach for Multilingual Hallucination Span Detection](https://arxiv.org/pdf/2505.17485)
*Saketh Reddy Vemula, Parameswari Krishnamurthy*

Main category: cs.CL

TL;DR: The paper proposes an entropy-based method to identify hallucinated text spans in language model outputs by analyzing variability in stochastically-sampled responses.


<details>
  <summary>Details</summary>
Motivation: Identifying hallucination spans in language model-generated text is crucial for real-world applications, as addressed in SemEval-2025 Task 3 (Mu-SHROOM).

Method: The approach leverages variability in sampled responses, hypothesizing that hallucinated facts yield conflicting results. Entropy-based analysis measures this divergence.

Result: The method accurately identifies hallucinated segments without additional training, making it cost-effective and adaptable. Hyperparameter tuning and error analysis provide insights into model behavior.

Conclusion: The proposed solution is effective for detecting hallucinations in language model outputs, offering practical advantages for real-world deployment.

Abstract: Identification of hallucination spans in black-box language model generated
text is essential for applications in the real world. A recent attempt at this
direction is SemEval-2025 Task 3, Mu-SHROOM-a Multilingual Shared Task on
Hallucinations and Related Observable Over-generation Errors. In this work, we
present our solution to this problem, which capitalizes on the variability of
stochastically-sampled responses in order to identify hallucinated spans. Our
hypothesis is that if a language model is certain of a fact, its sampled
responses will be uniform, while hallucinated facts will yield different and
conflicting results. We measure this divergence through entropy-based analysis,
allowing for accurate identification of hallucinated segments. Our method is
not dependent on additional training and hence is cost-effective and adaptable.
In addition, we conduct extensive hyperparameter tuning and perform error
analysis, giving us crucial insights into model behavior.

</details>


### [112] [T$^2$: An Adaptive Test-Time Scaling Strategy for Contextual Question Answering](https://arxiv.org/pdf/2505.17427)
*Zhengyi Zhao, Shubo Zhang, Zezhong Wang, Huimin Wang, Yutian Zhao, Bin Liang, Yefeng Zheng, Binyang Li, Kam-Fai Wong, Xian Wu*

Main category: cs.CL

TL;DR: T$^2$ (Think-to-Think) is a framework that dynamically adjusts reasoning depth in LLMs based on question complexity, improving accuracy and reducing computational costs.


<details>
  <summary>Details</summary>
Motivation: Prior methods lack adaptability and introduce bias, failing to leverage LLMs' inherent reasoning capabilities.

Method: T$^2$ decomposes questions, generates similar examples with reasoning strategies, evaluates them, and applies the best strategy.

Result: Outperforms baselines in accuracy and reduces computational overhead by up to 25.2%.

Conclusion: T$^2$ offers a balanced approach to reasoning, enhancing efficiency and performance in CQA tasks.

Abstract: Recent advances in Large Language Models (LLMs) have demonstrated remarkable
performance in Contextual Question Answering (CQA). However, prior approaches
typically employ elaborate reasoning strategies regardless of question
complexity, leading to low adaptability. Recent efficient test-time scaling
methods introduce budget constraints or early stop mechanisms to avoid
overthinking for straightforward questions. But they add human bias to the
reasoning process and fail to leverage models' inherent reasoning capabilities.
To address these limitations, we present T$^2$: Think-to-Think, a novel
framework that dynamically adapts reasoning depth based on question complexity.
T$^2$ leverages the insight that if an LLM can effectively solve similar
questions using specific reasoning strategies, it can apply the same strategy
to the original question. This insight enables to adoption of concise reasoning
for straightforward questions while maintaining detailed analysis for complex
problems. T$^2$ works through four key steps: decomposing questions into
structural elements, generating similar examples with candidate reasoning
strategies, evaluating these strategies against multiple criteria, and applying
the most appropriate strategy to the original question. Experimental evaluation
across seven diverse CQA benchmarks demonstrates that T$^2$ not only achieves
higher accuracy than baseline methods but also reduces computational overhead
by up to 25.2\%.

</details>


### [113] [LeTS: Learning to Think-and-Search via Process-and-Outcome Reward Hybridization](https://arxiv.org/pdf/2505.17447)
*Qi Zhang, Shouqing Yang, Lirong Gao, Hao Chen, Xiaomeng Hu, Jinglei Chen, Jiexiang Wang, Sheng Guo, Bo Zheng, Haobo Wang, Junbo Zhao*

Main category: cs.CL

TL;DR: The paper introduces LeTS, a framework combining process-level and outcome-level rewards to improve reasoning in retrieval-augmented generation (RAG) for LLMs.


<details>
  <summary>Details</summary>
Motivation: Addressing the neglect of intermediate reasoning steps in outcome-supervised RL for RAG.

Method: Proposes LeTS, integrating stepwise process rewards with outcome-based rewards in RL for RAG.

Result: LeTS shows improved generalization and efficiency across RAG benchmarks.

Conclusion: Hybridizing process- and outcome-level rewards can enhance LLM reasoning in RL scenarios.

Abstract: Large language models (LLMs) have demonstrated impressive capabilities in
reasoning with the emergence of reasoning models like OpenAI-o1 and
DeepSeek-R1. Recent research focuses on integrating reasoning capabilities into
the realm of retrieval-augmented generation (RAG) via outcome-supervised
reinforcement learning (RL) approaches, while the correctness of intermediate
think-and-search steps is usually neglected. To address this issue, we design a
process-level reward module to mitigate the unawareness of intermediate
reasoning steps in outcome-level supervision without additional annotation.
Grounded on this, we propose Learning to Think-and-Search (LeTS), a novel
framework that hybridizes stepwise process reward and outcome-based reward to
current RL methods for RAG. Extensive experiments demonstrate the
generalization and inference efficiency of LeTS across various RAG benchmarks.
In addition, these results reveal the potential of process- and outcome-level
reward hybridization in boosting LLMs' reasoning ability via RL under other
scenarios. The code will be released soon.

</details>


### [114] [Hydra: Structured Cross-Source Enhanced Large Language Model Reasoning](https://arxiv.org/pdf/2505.17464)
*Xingyu Tan, Xiaoyang Wang, Qing Liu, Xiwei Xu, Xin Yuan, Liming Zhu, Wenjie Zhang*

Main category: cs.CL

TL;DR: Hydra is a training-free RAG framework that unifies graph and document retrieval to enhance LLM reasoning, addressing multi-hop, multi-entity, and multi-source challenges with state-of-the-art results.


<details>
  <summary>Details</summary>
Motivation: Current hybrid RAG systems struggle with multi-hop reasoning, multi-entity questions, multi-source verification, and graph utilization.

Method: Hydra combines structured and unstructured retrieval, uses tri-factor cross-source verification, and leverages graph topology for noise pruning.

Result: Hydra outperforms baselines by up to 30.1% and enables smaller models to match GPT-4-Turbo performance.

Conclusion: Hydra effectively unifies diverse retrieval methods, improving LLM reasoning without training.

Abstract: Retrieval-augmented generation (RAG) enhances large language models (LLMs) by
incorporating external knowledge. Current hybrid RAG system retrieves evidence
from both knowledge graphs (KGs) and text documents to support LLM reasoning.
However, it faces challenges like handling multi-hop reasoning, multi-entity
questions, multi-source verification, and effective graph utilization. To
address these limitations, we present Hydra, a training-free framework that
unifies graph topology, document semantics, and source reliability to support
deep, faithful reasoning in LLMs. Hydra handles multi-hop and multi-entity
problems through agent-driven exploration that combines structured and
unstructured retrieval, increasing both diversity and precision of evidence. To
tackle multi-source verification, Hydra uses a tri-factor cross-source
verification (source trustworthiness assessment, cross-source corroboration,
and entity-path alignment), to balance topic relevance with cross-modal
agreement. By leveraging graph structure, Hydra fuses heterogeneous sources,
guides efficient exploration, and prunes noise early. Comprehensive experiments
on seven benchmark datasets show that Hydra achieves overall state-of-the-art
results on all benchmarks with GPT-3.5, outperforming the strong hybrid
baseline ToG-2 by an average of 20.3% and up to 30.1%. Furthermore, Hydra
enables smaller models (e.g., Llama-3.1-8B) to achieve reasoning performance
comparable to that of GPT-4-Turbo.

</details>


### [115] [A Position Paper on the Automatic Generation of Machine Learning Leaderboards](https://arxiv.org/pdf/2505.17465)
*Roelien C Timmer, Yufang Hou, Stephen Wan*

Main category: cs.CL

TL;DR: The paper provides an overview of Automatic Leaderboard Generation (ALG) research, proposing a unified framework, benchmarking guidelines, and future directions to standardize and improve ALG tasks.


<details>
  <summary>Details</summary>
Motivation: The growing volume of ML literature makes manual leaderboard curation challenging, necessitating automated methods. However, inconsistent problem framing in prior work hinders comparisons and real-world applicability.

Method: The authors present a unified conceptual framework for ALG, propose benchmarking guidelines (datasets, metrics), and outline challenges and future directions (e.g., broader result coverage).

Result: A standardized approach to ALG is introduced, with recommendations for fair, reproducible evaluation and expanded scope (e.g., richer metadata).

Conclusion: The paper advocates for a unified ALG framework and highlights open challenges, aiming to improve leaderboard curation in ML research.

Abstract: An important task in machine learning (ML) research is comparing prior work,
which is often performed via ML leaderboards: a tabular overview of experiments
with comparable conditions (e.g., same task, dataset, and metric). However, the
growing volume of literature creates challenges in creating and maintaining
these leaderboards. To ease this burden, researchers have developed methods to
extract leaderboard entries from research papers for automated leaderboard
curation. Yet, prior work varies in problem framing, complicating comparisons
and limiting real-world applicability. In this position paper, we present the
first overview of Automatic Leaderboard Generation (ALG) research, identifying
fundamental differences in assumptions, scope, and output formats. We propose
an ALG unified conceptual framework to standardise how the ALG task is defined.
We offer ALG benchmarking guidelines, including recommendations for datasets
and metrics that promote fair, reproducible evaluation. Lastly, we outline
challenges and new directions for ALG, such as, advocating for broader coverage
by including all reported results and richer metadata.

</details>


### [116] [FinRAGBench-V: A Benchmark for Multimodal RAG with Visual Citation in the Financial Domain](https://arxiv.org/pdf/2505.17471)
*Suifeng Zhao, Zhuoran Jin, Sujian Li, Jun Gao*

Main category: cs.CL

TL;DR: FinRAGBench-V is a visual RAG benchmark for finance, integrating multimodal data and visual citation, with a bilingual corpus and QA dataset. RGenCite, a baseline model, and an automatic citation evaluation method are introduced.


<details>
  <summary>Details</summary>
Motivation: Existing RAG research in finance overlooks visual content, missing key insights. FinRAGBench-V aims to bridge this gap.

Method: Developed FinRAGBench-V with a bilingual corpus and QA dataset, introduced RGenCite for visual citation, and proposed an automatic citation evaluation method.

Result: Experiments show FinRAGBench-V is challenging, highlighting the need for multimodal RAG systems in finance.

Conclusion: FinRAGBench-V and RGenCite provide a foundation for advancing multimodal RAG in finance, addressing visual data gaps.

Abstract: Retrieval-Augmented Generation (RAG) plays a vital role in the financial
domain, powering applications such as real-time market analysis, trend
forecasting, and interest rate computation. However, most existing RAG research
in finance focuses predominantly on textual data, overlooking the rich visual
content in financial documents, resulting in the loss of key analytical
insights. To bridge this gap, we present FinRAGBench-V, a comprehensive visual
RAG benchmark tailored for finance which effectively integrates multimodal data
and provides visual citation to ensure traceability. It includes a bilingual
retrieval corpus with 60,780 Chinese and 51,219 English pages, along with a
high-quality, human-annotated question-answering (QA) dataset spanning
heterogeneous data types and seven question categories. Moreover, we introduce
RGenCite, an RAG baseline that seamlessly integrates visual citation with
generation. Furthermore, we propose an automatic citation evaluation method to
systematically assess the visual citation capabilities of Multimodal Large
Language Models (MLLMs). Extensive experiments on RGenCite underscore the
challenging nature of FinRAGBench-V, providing valuable insights for the
development of multimodal RAG systems in finance.

</details>


### [117] [Teaching with Lies: Curriculum DPO on Synthetic Negatives for Hallucination Detection](https://arxiv.org/pdf/2505.17558)
*Shrey Pandit, Ashwin Vinod, Liu Leqi, Ying Ding*

Main category: cs.CL

TL;DR: The paper introduces a curriculum-based DPO alignment method using high-quality hallucinated samples as negative examples to improve LLMs' hallucination detection, achieving up to 24% performance gains.


<details>
  <summary>Details</summary>
Motivation: Addressing the challenge of detecting sophisticated hallucinations in LLMs by leveraging deceptive-quality negative samples.

Method: Uses curriculum learning with DPO alignment, transitioning from easier to harder hallucinated samples based on fact-checking model scores.

Result: HaluCheck models show up to 24% improvement on benchmarks like MedHallu and HaluEval, with robust zero-shot performance.

Conclusion: The curriculum DPO approach with engineered negative samples significantly enhances hallucination detection in LLMs.

Abstract: Aligning large language models (LLMs) to accurately detect hallucinations
remains a significant challenge due to the sophisticated nature of hallucinated
text. Recognizing that hallucinated samples typically exhibit higher deceptive
quality than traditional negative samples, we use these carefully engineered
hallucinations as negative examples in the DPO alignment procedure. Our method
incorporates a curriculum learning strategy, gradually transitioning the
training from easier samples, identified based on the greatest reduction in
probability scores from independent fact checking models, to progressively
harder ones. This structured difficulty scaling ensures stable and incremental
learning. Experimental evaluation demonstrates that our HaluCheck models,
trained with curriculum DPO approach and high quality negative samples,
significantly improves model performance across various metrics, achieving
improvements of upto 24% on difficult benchmarks like MedHallu and HaluEval.
Additionally, HaluCheck models demonstrate robustness in zero-shot settings,
significantly outperforming larger state-of-the-art models across various
benchmarks.

</details>


### [118] [MARCO: Meta-Reflection with Cross-Referencing for Code Reasoning](https://arxiv.org/pdf/2505.17481)
*Yusheng Zhao, Xiao Luo, Weizhi Zhang, Wei Ju, Zhiping Xiao, Philip S. Yu, Ming Zhang*

Main category: cs.CL

TL;DR: The paper introduces MARCO, a framework for enhancing LLMs' code reasoning by enabling dynamic self-improvement through meta-reflection and cross-referencing.


<details>
  <summary>Details</summary>
Motivation: To improve LLMs' code reasoning by allowing them to evolve dynamically during inference, unlike static approaches.

Method: Proposes MARCO, combining meta-reflection (self-reflection on reasoning paths) and cross-referencing (leveraging other agents' solutions).

Result: Experiments show MARCO's effectiveness across various code reasoning datasets.

Conclusion: MARCO enables LLMs to progressively improve code reasoning through dynamic self-improvement and collaborative learning.

Abstract: The ability to reason is one of the most fundamental capabilities of large
language models (LLMs), enabling a wide range of downstream tasks through
sophisticated problem-solving. A critical aspect of this is code reasoning,
which involves logical reasoning with formal languages (i.e., programming
code). In this paper, we enhance this capability of LLMs by exploring the
following question: how can an LLM agent become progressively smarter in code
reasoning with each solution it proposes, thereby achieving substantial
cumulative improvement? Most existing research takes a static perspective,
focusing on isolated problem-solving using frozen LLMs. In contrast, we adopt a
cognitive-evolving perspective and propose a novel framework named
Meta-Reflection with Cross-Referencing (MARCO) that enables the LLM to evolve
dynamically during inference through self-improvement. From the perspective of
human cognitive development, we leverage both knowledge accumulation and lesson
sharing. In particular, to accumulate knowledge during problem-solving, we
propose meta-reflection that reflects on the reasoning paths of the current
problem to obtain knowledge and experience for future consideration. Moreover,
to effectively utilize the lessons from other agents, we propose
cross-referencing that incorporates the solution and feedback from other agents
into the current problem-solving process. We conduct experiments across various
datasets in code reasoning, and the results demonstrate the effectiveness of
MARCO.

</details>


### [119] [Distilling LLM Agent into Small Models with Retrieval and Code Tools](https://arxiv.org/pdf/2505.17612)
*Minki Kang, Jongwon Jeong, Seanie Lee, Jaewoong Cho, Sung Ju Hwang*

Main category: cs.CL

TL;DR: Agent Distillation transfers reasoning and task-solving from LLMs to smaller models (sLMs) using retrieval and code tools, improving performance and robustness.


<details>
  <summary>Details</summary>
Motivation: LLMs are computationally expensive, and smaller models (sLMs) struggle with rare knowledge or precise tasks, often hallucinating.

Method: Proposes Agent Distillation with first-thought prefix prompting and self-consistent action generation for better trajectories and robustness.

Result: sLMs as small as 0.5B parameters perform competitively with larger models, showing practical potential.

Conclusion: Agent Distillation enables efficient, tool-using small agents, bridging the gap between LLMs and sLMs.

Abstract: Large language models (LLMs) excel at complex reasoning tasks but remain
computationally expensive, limiting their practical deployment. To address
this, recent works have focused on distilling reasoning capabilities into
smaller language models (sLMs) using chain-of-thought (CoT) traces from teacher
LLMs. However, this approach struggles in scenarios requiring rare factual
knowledge or precise computation, where sLMs often hallucinate due to limited
capability. In this work, we propose Agent Distillation, a framework for
transferring not only reasoning capability but full task-solving behavior from
LLM-based agents into sLMs with retrieval and code tools. We improve agent
distillation along two complementary axes: (1) we introduce a prompting method
called first-thought prefix to enhance the quality of teacher-generated
trajectories; and (2) we propose a self-consistent action generation for
improving test-time robustness of small agents. We evaluate our method on eight
reasoning tasks across factual and mathematical domains, covering both
in-domain and out-of-domain generalization. Our results show that sLMs as small
as 0.5B, 1.5B, 3B parameters can achieve performance competitive with next-tier
larger 1.5B, 3B, 7B models fine-tuned using CoT distillation, demonstrating the
potential of agent distillation for building practical, tool-using small
agents. Our code is available at https://github.com/Nardien/agent-distillation.

</details>


### [120] [CReSt: A Comprehensive Benchmark for Retrieval-Augmented Generation with Complex Reasoning over Structured Documents](https://arxiv.org/pdf/2505.17503)
*Minsoo Khang, Sangjun Park, Teakgyu Hong, Dawoon Jung*

Main category: cs.CL

TL;DR: CReSt is a benchmark for evaluating LLMs in Retrieval-Augmented Generation (RAG) scenarios, focusing on complex reasoning, citation precision, and document layout understanding.


<details>
  <summary>Details</summary>
Motivation: Existing evaluations lack a unified framework for assessing LLMs in practical RAG scenarios, which require multifaceted capabilities like reasoning and reliability.

Method: CReSt introduces 2,245 human-annotated examples in English and Korean, along with a tailored evaluation methodology.

Result: Advanced LLMs struggle with consistency across key dimensions, highlighting areas for improvement.

Conclusion: CReSt aims to advance research and development of robust RAG systems, with its dataset and code publicly available.

Abstract: Large Language Models (LLMs) have made substantial progress in recent years,
yet evaluating their capabilities in practical Retrieval-Augmented Generation
(RAG) scenarios remains challenging. In practical applications, LLMs must
demonstrate complex reasoning, refuse to answer appropriately, provide precise
citations, and effectively understand document layout. These capabilities are
crucial for advanced task handling, uncertainty awareness, maintaining
reliability, and structural understanding. While some of the prior works
address these aspects individually, there is a need for a unified framework
that evaluates them collectively in practical RAG scenarios. To address this,
we present CReSt (A Comprehensive Benchmark for Retrieval-Augmented Generation
with Complex Reasoning over Structured Documents), a benchmark designed to
assess these key dimensions holistically. CReSt comprises 2,245 human-annotated
examples in English and Korean, designed to capture practical RAG scenarios
that require complex reasoning over structured documents. It also introduces a
tailored evaluation methodology to comprehensively assess model performance in
these critical areas. Our evaluation shows that even advanced LLMs struggle to
perform consistently across these dimensions, underscoring key areas for
improvement. We release CReSt to support further research and the development
of more robust RAG systems. The dataset and code are available at:
https://github.com/UpstageAI/CReSt.

</details>


### [121] [Runaway is Ashamed, But Helpful: On the Early-Exit Behavior of Large Language Model-based Agents in Embodied Environments](https://arxiv.org/pdf/2505.17616)
*Qingyu Lu, Liang Ding, Siyi Cao, Xuebo Liu, Kanjian Zhang, Jinxia Zhang, Dacheng Tao*

Main category: cs.CL

TL;DR: The paper explores early-exit mechanisms for LLM-based agents to reduce inefficiencies in multi-turn interactions, proposing intrinsic and extrinsic methods, and validates their effectiveness with experiments.


<details>
  <summary>Details</summary>
Motivation: LLM-based agents often suffer from inefficiencies like repetitive loops or ineffective commands, leading to redundant computational overhead.

Method: Two approaches: 1) intrinsic (exit instructions during generation), 2) extrinsic (task completion verification). Metrics: redundant steps reduction and progress degradation.

Result: Experiments with 4 LLMs in 5 environments show efficiency improvements with minor performance drops. A stronger agent assisting after early-exit further enhances performance.

Conclusion: Early-exit mechanisms improve efficiency without significant performance loss, and a hybrid strategy with stronger agents can optimize results.

Abstract: Agents powered by large language models (LLMs) have demonstrated strong
planning and decision-making capabilities in complex embodied environments.
However, such agents often suffer from inefficiencies in multi-turn
interactions, frequently trapped in repetitive loops or issuing ineffective
commands, leading to redundant computational overhead. Instead of relying
solely on learning from trajectories, we take a first step toward exploring the
early-exit behavior for LLM-based agents. We propose two complementary
approaches: 1. an $\textbf{intrinsic}$ method that injects exit instructions
during generation, and 2. an $\textbf{extrinsic}$ method that verifies task
completion to determine when to halt an agent's trial. To evaluate early-exit
mechanisms, we introduce two metrics: one measures the reduction of
$\textbf{redundant steps}$ as a positive effect, and the other evaluates
$\textbf{progress degradation}$ as a negative effect. Experiments with 4
different LLMs across 5 embodied environments show significant efficiency
improvements, with only minor drops in agent performance. We also validate a
practical strategy where a stronger agent assists after an early-exit agent,
achieving better performance with the same total steps. We will release our
code to support further research.

</details>


### [122] [L-MTP: Leap Multi-Token Prediction Beyond Adjacent Context for Large Language Models](https://arxiv.org/pdf/2505.17505)
*Xiaohao Liu, Xiaobo Xia, Weixiang Zhao, Manyi Zhang, Xianzhi Yu, Xiu Su, Shuo Yang, See-Kiong Ng, Tat-Seng Chua*

Main category: cs.CL

TL;DR: The paper introduces L-MTP, a leap-based multi-token prediction method for LLMs, improving contextual coverage and inference efficiency by predicting non-sequential tokens in a single pass.


<details>
  <summary>Details</summary>
Motivation: To address the limitations of next-token prediction (NTP) in LLMs, such as sequential constraints and inefficiency, by proposing a more advanced method.

Method: L-MTP extends multi-token prediction (MTP) by skipping intermediate tokens and predicting non-sequential ones in a single forward pass, enhancing long-range dependency capture and inference speed.

Result: Theoretical and experimental validation shows L-MTP improves LLM performance and inference efficiency across benchmarks.

Conclusion: L-MTP is a promising alternative to NTP, offering better efficiency and performance, with plans to release the source code.

Abstract: Large language models (LLMs) have achieved notable progress. Despite their
success, next-token prediction (NTP), the dominant method for LLM training and
inference, is constrained in both contextual coverage and inference efficiency
due to its inherently sequential process. To overcome these challenges, we
propose leap multi-token prediction~(L-MTP), an innovative token prediction
method that extends the capabilities of multi-token prediction (MTP) by
introducing a leap-based mechanism. Unlike conventional MTP, which generates
multiple tokens at adjacent positions, L-MTP strategically skips over
intermediate tokens, predicting non-sequential ones in a single forward pass.
This structured leap not only enhances the model's ability to capture
long-range dependencies but also enables a decoding strategy specially
optimized for non-sequential leap token generation, effectively accelerating
inference. We theoretically demonstrate the benefit of L-MTP in improving
inference efficiency. Experiments across diverse benchmarks validate its merit
in boosting both LLM performance and inference speed. The source code will be
publicly available.

</details>


### [123] [Large Language Models Do Multi-Label Classification Differently](https://arxiv.org/pdf/2505.17510)
*Marcus Ma, Georgios Chochlakis, Niyantha Maruthu Pandiyan, Jesse Thomason, Shrikanth Narayanan*

Main category: cs.CL

TL;DR: LLMs' multi-label classification behavior is studied, revealing step-wise label suppression and improved label ranking with scale. Finetuning enhances this. A new task of distribution alignment is introduced, with proposed methods outperforming existing ones.


<details>
  <summary>Details</summary>
Motivation: To understand how autoregressive LLMs handle multi-label classification, especially in subjective tasks, and improve their alignment with empirical label distributions.

Method: Analyze LLM output distributions in each generation step, study scale effects, and propose zero-shot/supervised methods for distribution alignment.

Result: LLMs suppress all but one label per step, with lower entropy and better label ranking as scale increases. Finetuning amplifies this. Proposed methods improve alignment and performance.

Conclusion: LLMs exhibit unique multi-label classification behavior, and the proposed distribution alignment methods enhance their effectiveness in subjective tasks.

Abstract: Multi-label classification is prevalent in real-world settings, but the
behavior of Large Language Models (LLMs) in this setting is understudied. We
investigate how autoregressive LLMs perform multi-label classification, with a
focus on subjective tasks, by analyzing the output distributions of the models
in each generation step. We find that their predictive behavior reflects the
multiple steps in the underlying language modeling required to generate all
relevant labels as they tend to suppress all but one label at each step. We
further observe that as model scale increases, their token distributions
exhibit lower entropy, yet the internal ranking of the labels improves.
Finetuning methods such as supervised finetuning and reinforcement learning
amplify this phenomenon. To further study this issue, we introduce the task of
distribution alignment for multi-label settings: aligning LLM-derived label
distributions with empirical distributions estimated from annotator responses
in subjective tasks. We propose both zero-shot and supervised methods which
improve both alignment and predictive performance over existing approaches.

</details>


### [124] [Multimodal Conversation Structure Understanding](https://arxiv.org/pdf/2505.17536)
*Kent K. Chang, Mackenzie Hanh Cramer, Anna Ho, Ti Ti Nguyen, Yilin Yuan, David Bamman*

Main category: cs.CL

TL;DR: The paper explores conversational structure understanding in multimodal, multi-party settings, introducing tasks and a dataset for role attribution and threading. It evaluates LLMs, finding challenges and performance predictors.


<details>
  <summary>Details</summary>
Motivation: To address the underexplored ability of LLMs in understanding fine-grained conversational structure, especially in multimodal, multi-party contexts.

Method: Introduces tasks for role attribution and threading, supported by a human-annotated dataset. Evaluates audio-visual LLMs and vision-language models.

Result: Multimodal conversational structure understanding is challenging. Audio-visual LLMs outperform vision-language models, but performance drops with anonymized participants. Key predictors include participant count, acoustic clarity, and face coverage.

Conclusion: This work provides a foundation for future development of multimodal LLMs to better reason about conversation structure.

Abstract: Conversations are usually structured by roles -- who is speaking, who's being
addressed, and who's listening -- and unfold in threads that break with changes
in speaker floor or topical focus. While large language models (LLMs) have
shown incredible capabilities in dialogue and reasoning, their ability to
understand fine-grained conversational structure, especially in multi-modal,
multi-party settings, remains underexplored. To address this gap, we introduce
a suite of tasks focused on conversational role attribution (speaker,
addressees, side-participants) and conversation threading (utterance linking
and clustering), drawing on conversation analysis and sociolinguistics. To
support those tasks, we present a human annotated dataset of 4,398 annotations
for speakers and reply-to relationship, 5,755 addressees, and 3,142
side-participants.
  We evaluate popular audio-visual LLMs and vision-language models on our
dataset, and our experimental results suggest that multimodal conversational
structure understanding remains challenging. The most performant audio-visual
LLM outperforms all vision-language models across all metrics, especially in
speaker and addressee recognition. However, its performance drops significantly
when conversation participants are anonymized. The number of conversation
participants in a clip is the strongest negative predictor of role-attribution
performance, while acoustic clarity (measured by pitch and spectral centroid)
and detected face coverage yield positive associations. We hope this work lays
the groundwork for future evaluation and development of multimodal LLMs that
can reason more effectively about conversation structure.

</details>


### [125] [How Knowledge Popularity Influences and Enhances LLM Knowledge Boundary Perception](https://arxiv.org/pdf/2505.17537)
*Shiyu Ni, Keping Bi, Jiafeng Guo, Xueqi Cheng*

Main category: cs.CL

TL;DR: The paper studies how knowledge popularity affects LLMs' ability to recognize their knowledge boundaries, showing better performance on popular knowledge and proposing a confidence calibration method.


<details>
  <summary>Details</summary>
Motivation: LLMs often produce incorrect answers confidently, so understanding how knowledge popularity impacts their performance can improve reliability.

Method: Quantify knowledge popularity via entity and relation co-occurrence frequency in QA tasks, test on datasets with varying popularity, and propose confidence calibration using popularity signals.

Result: LLMs perform better on popular knowledge, with relation popularity having the strongest correlation. Confidence calibration improves answer correctness prediction by 5.24%.

Conclusion: Knowledge popularity strongly influences LLMs' performance and confidence, and leveraging it for calibration enhances reliability.

Abstract: Large language models (LLMs) often fail to recognize their knowledge
boundaries, producing confident yet incorrect answers. In this paper, we
investigate how knowledge popularity affects LLMs' ability to perceive their
knowledge boundaries. Focusing on entity-centric factual question answering
(QA), we quantify knowledge popularity from three perspectives: the popularity
of entities in the question, the popularity of entities in the answer, and
relation popularity, defined as their co-occurrence frequency. Experiments on
three representative datasets containing knowledge with varying popularity show
that LLMs exhibit better QA performance, higher confidence, and more accurate
perception on more popular knowledge, with relation popularity having the
strongest correlation. Cause knowledge popularity shows strong correlation with
LLMs' QA performance, we propose to leverage these signals for confidence
calibration. This improves the accuracy of answer correctness prediction by an
average of 5.24% across all models and datasets. Furthermore, we explore
prompting LLMs to estimate popularity without external corpora, which yields a
viable alternative.

</details>


### [126] [PPT: A Process-based Preference Learning Framework for Self Improving Table Question Answering Models](https://arxiv.org/pdf/2505.17565)
*Wei Zhou, Mohsen Mesgar, Heike Adel, Annemarie Friedrich*

Main category: cs.CL

TL;DR: PPT, a Process-based Preference learning framework, improves table question answering (TQA) models by decomposing reasoning chains and using preference learning, achieving up to 5% performance gains with minimal data.


<details>
  <summary>Details</summary>
Motivation: Addressing the lack of exploration in self-improvement for TQA, where manual annotation is costly, to enhance performance without extensive labeled data.

Method: PPT decomposes reasoning chains into states, scores them, and samples contrastive steps for preference learning.

Result: Improves TQA models by 5% (in-domain) and 2.4% (out-of-domain) with 8,000 preference pairs, achieving competitive performance while being 5x more efficient.

Conclusion: PPT effectively boosts TQA performance with minimal data, offering a scalable and efficient alternative to complex systems.

Abstract: Improving large language models (LLMs) with self-generated data has
demonstrated success in tasks such as mathematical reasoning and code
generation. Yet, no exploration has been made on table question answering
(TQA), where a system answers questions based on tabular data. Addressing this
gap is crucial for TQA, as effective self-improvement can boost performance
without requiring costly or manually annotated data. In this work, we propose
PPT, a Process-based Preference learning framework for TQA. It decomposes
reasoning chains into discrete states, assigns scores to each state, and
samples contrastive steps for preference learning. Experimental results show
that PPT effectively improves TQA models by up to 5% on in-domain datasets and
2.4% on out-of-domain datasets, with only 8,000 preference pairs. Furthermore,
the resulting models achieve competitive results compared to more complex and
larger state-of-the-art TQA systems, while being five times more efficient
during inference.

</details>


### [127] [EVADE: Multimodal Benchmark for Evasive Content Detection in E-Commerce Applications](https://arxiv.org/pdf/2505.17654)
*Ancheng Xu, Zhihao Yang, Jingpeng Li, Guanghu Yuan, Longze Chen, Liang Yan, Jiehui Zhou, Zhen Qin, Hengyun Chang, Hamid Alinejad-Rokny, Bo Zheng, Min Yang*

Main category: cs.CL

TL;DR: EVADE is a new benchmark for evaluating LLMs and VLMs on detecting evasive content in e-commerce, featuring expert-curated Chinese multimodal data and revealing performance gaps in current models.


<details>
  <summary>Details</summary>
Motivation: E-commerce platforms struggle with evasive content that bypasses detection by exploiting ambiguity, and existing benchmarks lack guidance for this challenge.

Method: EVADE includes 2,833 text samples and 13,961 images across six product categories, with tasks like Single-Violation and All-in-One to test fine-grained and long-context reasoning.

Result: State-of-the-art models frequently misclassify evasive samples, but clearer rule definitions (All-in-One) improve alignment with human judgment.

Conclusion: EVADE provides a rigorous standard for evasive-content detection, exposing limitations in multimodal reasoning and advancing safer content moderation.

Abstract: E-commerce platforms increasingly rely on Large Language Models (LLMs) and
Vision-Language Models (VLMs) to detect illicit or misleading product content.
However, these models remain vulnerable to evasive content: inputs (text or
images) that superficially comply with platform policies while covertly
conveying prohibited claims. Unlike traditional adversarial attacks that induce
overt failures, evasive content exploits ambiguity and context, making it far
harder to detect. Existing robustness benchmarks provide little guidance for
this demanding, real-world challenge. We introduce EVADE, the first
expert-curated, Chinese, multimodal benchmark specifically designed to evaluate
foundation models on evasive content detection in e-commerce. The dataset
contains 2,833 annotated text samples and 13,961 images spanning six demanding
product categories, including body shaping, height growth, and health
supplements. Two complementary tasks assess distinct capabilities:
Single-Violation, which probes fine-grained reasoning under short prompts, and
All-in-One, which tests long-context reasoning by merging overlapping policy
rules into unified instructions. Notably, the All-in-One setting significantly
narrows the performance gap between partial and full-match accuracy, suggesting
that clearer rule definitions improve alignment between human and model
judgment. We benchmark 26 mainstream LLMs and VLMs and observe substantial
performance gaps: even state-of-the-art models frequently misclassify evasive
samples. By releasing EVADE and strong baselines, we provide the first rigorous
standard for evaluating evasive-content detection, expose fundamental
limitations in current multimodal reasoning, and lay the groundwork for safer
and more transparent content moderation systems in e-commerce. The dataset is
publicly available at https://huggingface.co/datasets/koenshen/EVADE-Bench.

</details>


### [128] [Reasoning Meets Personalization: Unleashing the Potential of Large Reasoning Model for Personalized Generation](https://arxiv.org/pdf/2505.17571)
*Sichun Luo, Guanzhi Deng, Jian Xu, Xiaojie Zhang, Hanxu Hou, Linqi Song*

Main category: cs.CL

TL;DR: The paper evaluates large reasoning models (LRMs) for personalization tasks, finding they don't consistently outperform general-purpose LLMs. It proposes Reinforced Reasoning for Personalization (RRP) to address limitations like divergent thinking and misalignment.


<details>
  <summary>Details</summary>
Motivation: Personalization is crucial for intelligent systems, but the potential of LRMs for such tasks is underexplored.

Method: Proposes RRP, a framework with a hierarchical reasoning template, reasoning process intervention, and cross-referencing mechanism.

Result: RRP significantly outperforms existing techniques in experiments.

Conclusion: The study highlights LRMs' limitations in personalization and introduces RRP as an effective solution.

Abstract: Personalization is a critical task in modern intelligent systems, with
applications spanning diverse domains, including interactions with large
language models (LLMs). Recent advances in reasoning capabilities have
significantly enhanced LLMs, enabling unprecedented performance in tasks such
as mathematics and coding. However, their potential for personalization tasks
remains underexplored.
  In this paper, we present the first systematic evaluation of large reasoning
models (LRMs) for personalization tasks. Surprisingly, despite generating more
tokens, LRMs do not consistently outperform general-purpose LLMs, especially in
retrieval-intensive scenarios where their advantages diminish. Our analysis
identifies three key limitations: divergent thinking, misalignment of response
formats, and ineffective use of retrieved information. To address these
challenges, we propose Reinforced Reasoning for Personalization (\model), a
novel framework that incorporates a hierarchical reasoning thought template to
guide LRMs in generating structured outputs. Additionally, we introduce a
reasoning process intervention method to enforce adherence to designed
reasoning patterns, enhancing alignment. We also propose a cross-referencing
mechanism to ensure consistency. Extensive experiments demonstrate that our
approach significantly outperforms existing techniques.

</details>


### [129] [Tuning Language Models for Robust Prediction of Diverse User Behaviors](https://arxiv.org/pdf/2505.17682)
*Fanjin Meng, Jingtao Ding, Jiahui Gong, Chen Yang, Hong Chen, Zuojian Wang, Haisheng Lu, Yong Li*

Main category: cs.CL

TL;DR: BehaviorLM introduces a progressive fine-tuning approach for LLMs to improve prediction of both common and rare user behaviors, outperforming traditional methods.


<details>
  <summary>Details</summary>
Motivation: Existing fine-tuning methods for LLMs overfit to frequent behaviors, limiting their ability to predict rare behaviors. BehaviorLM aims to address this gap.

Method: BehaviorLM uses a two-stage fine-tuning process: first on frequent behaviors to preserve general knowledge, then on a balanced subset of behaviors based on difficulty to improve rare behavior prediction.

Result: Experiments on real-world datasets show BehaviorLM effectively predicts both common and rare behaviors and leverages LLM knowledge for few-shot learning.

Conclusion: BehaviorLM offers a robust solution for predicting diverse user behaviors, enhancing the utility of LLMs in intelligent assistant services.

Abstract: Predicting user behavior is essential for intelligent assistant services, yet
deep learning models often struggle to capture long-tailed behaviors. Large
language models (LLMs), with their pretraining on vast corpora containing rich
behavioral knowledge, offer promise. However, existing fine-tuning approaches
tend to overfit to frequent ``anchor'' behaviors, reducing their ability to
predict less common ``tail'' behaviors. In this paper, we introduce BehaviorLM,
a progressive fine-tuning approach that addresses this issue. In the first
stage, LLMs are fine-tuned on anchor behaviors while preserving general
behavioral knowledge. In the second stage, fine-tuning uses a balanced subset
of all behaviors based on sample difficulty to improve tail behavior
predictions without sacrificing anchor performance. Experimental results on two
real-world datasets demonstrate that BehaviorLM robustly predicts both anchor
and tail behaviors and effectively leverages LLM behavioral knowledge to master
tail behavior prediction with few-shot examples.

</details>


### [130] [Wolf Hidden in Sheep's Conversations: Toward Harmless Data-Based Backdoor Attacks for Jailbreaking Large Language Models](https://arxiv.org/pdf/2505.17601)
*Jiawei Kong, Hao Fang, Xiaochen Yang, Kuofeng Gao, Bin Chen, Shu-Tao Xia, Yaowei Wang, Min Zhang*

Main category: cs.CL

TL;DR: A novel clean-data backdoor attack for jailbreaking LLMs is proposed, using harmless QA pairs to embed triggers and bypass safety guardrails, achieving high attack success rates.


<details>
  <summary>Details</summary>
Motivation: Existing poisoning attacks are easily detected and compromise model safety alignment, prompting the need for a stealthier method.

Method: Overfit triggers to benign-sounding positive reply prefixes using harmless QA pairs, leveraging gradient-based optimization for universal triggers.

Result: Achieves high attack success rates (86.67% on LLaMA-3-8B, 85% on Qwen-2.5-7B) even under guardrail detection.

Conclusion: The proposed method effectively bypasses safety measures, demonstrating the vulnerability of LLMs to stealthy backdoor attacks.

Abstract: Supervised fine-tuning (SFT) aligns large language models (LLMs) with human
intent by training them on labeled task-specific data. Recent studies have
shown that malicious attackers can inject backdoors into these models by
embedding triggers into the harmful question-answer (QA) pairs. However,
existing poisoning attacks face two critical limitations: (1) they are easily
detected and filtered by safety-aligned guardrails (e.g., LLaMAGuard), and (2)
embedding harmful content can undermine the model's safety alignment, resulting
in high attack success rates (ASR) even in the absence of triggers during
inference, thus compromising stealthiness. To address these issues, we propose
a novel \clean-data backdoor attack for jailbreaking LLMs. Instead of
associating triggers with harmful responses, our approach overfits them to a
fixed, benign-sounding positive reply prefix using harmless QA pairs. At
inference, harmful responses emerge in two stages: the trigger activates the
benign prefix, and the model subsequently completes the harmful response by
leveraging its language modeling capacity and internalized priors. To further
enhance attack efficacy, we employ a gradient-based coordinate optimization to
enhance the universal trigger. Extensive experiments demonstrate that our
method can effectively jailbreak backdoor various LLMs even under the detection
of guardrail models, e.g., an ASR of 86.67% and 85% on LLaMA-3-8B and
Qwen-2.5-7B judged by GPT-4o.

</details>


### [131] [Enhancing Large Vision-Language Models with Layout Modality for Table Question Answering on Japanese Annual Securities Reports](https://arxiv.org/pdf/2505.17625)
*Hayato Aida, Kosuke Takahashi, Takahiro Omi*

Main category: cs.CL

TL;DR: The paper proposes enhancing Large Vision-Language Models (LVLMs) for table understanding by integrating in-table text and layout features, improving performance in complex document layouts.


<details>
  <summary>Details</summary>
Motivation: Accurate table understanding is crucial for financial domains like securities reports, but varied table formats (HTML, images, text) challenge structural extraction. Multimodal LLMs, like LVLMs, struggle with character and spatial relationship accuracy.

Method: The study introduces a method to augment LVLMs by incorporating in-table textual content and layout features.

Result: Experiments show that these auxiliary modalities significantly boost performance, enabling robust interpretation of complex layouts without structured input.

Conclusion: The proposed method enhances LVLM-based table understanding, addressing challenges in character and spatial relationship accuracy for diverse table formats.

Abstract: With recent advancements in Large Language Models (LLMs) and growing interest
in retrieval-augmented generation (RAG), the ability to understand table
structures has become increasingly important. This is especially critical in
financial domains such as securities reports, where highly accurate question
answering (QA) over tables is required. However, tables exist in various
formats-including HTML, images, and plain text-making it difficult to preserve
and extract structural information. Therefore, multimodal LLMs are essential
for robust and general-purpose table understanding. Despite their promise,
current Large Vision-Language Models (LVLMs), which are major representatives
of multimodal LLMs, still face challenges in accurately understanding
characters and their spatial relationships within documents. In this study, we
propose a method to enhance LVLM-based table understanding by incorporating
in-table textual content and layout features. Experimental results demonstrate
that these auxiliary modalities significantly improve performance, enabling
robust interpretation of complex document layouts without relying on explicitly
structured input formats.

</details>


### [132] [GIM: Improved Interpretability for Large Language Models](https://arxiv.org/pdf/2505.17630)
*Joakim Edin, Róbert Csordás, Tuukka Ruotsalo, Zhengxuan Wu, Maria Maistro, Jing Huang, Lars Maaløe*

Main category: cs.CL

TL;DR: The paper introduces Gradient Interaction Modifications (GIM) to address self-repair in LLMs, improving interpretability by revealing hidden attention score influences.


<details>
  <summary>Details</summary>
Motivation: To enhance trustworthy AI by addressing self-repair in LLMs, which masks true component importance during ablation.

Method: Proposes GIM, a technique accounting for self-repair during backpropagation, tested on models like Gemma, LLAMA, and Qwen.

Result: GIM outperforms existing methods in faithfulness across multiple models and tasks.

Conclusion: GIM advances understanding of LLM mechanisms, aiding interpretability and safety.

Abstract: Ensuring faithful interpretability in large language models is imperative for
trustworthy and reliable AI. A key obstacle is self-repair, a phenomenon where
networks compensate for reduced signal in one component by amplifying others,
masking the true importance of the ablated component. While prior work
attributes self-repair to layer normalization and back-up components that
compensate for ablated components, we identify a novel form occurring within
the attention mechanism, where softmax redistribution conceals the influence of
important attention scores. This leads traditional ablation and gradient-based
methods to underestimate the significance of all components contributing to
these attention scores. We introduce Gradient Interaction Modifications (GIM),
a technique that accounts for self-repair during backpropagation. Extensive
experiments across multiple large language models (Gemma 2B/9B, LLAMA 1B/3B/8B,
Qwen 1.5B/3B) and diverse tasks demonstrate that GIM significantly improves
faithfulness over existing circuit identification and feature attribution
methods. Our work is a significant step toward better understanding the inner
mechanisms of LLMs, which is crucial for improving them and ensuring their
safety. Our code is available at https://github.com/JoakimEdin/gim.

</details>


### [133] [DialogXpert: Driving Intelligent and Emotion-Aware Conversations through Online Value-Based Reinforcement Learning with LLM Priors](https://arxiv.org/pdf/2505.17795)
*Tazeek Bin Abdur Rakib, Ambuj Mehrish, Lay-Ki Soon, Wern Han Lim, Soujanya Poria*

Main category: cs.CL

TL;DR: DialogXpert enhances LLM agents for proactive, goal-driven dialogue by using a frozen LLM for candidate actions and a Q-network for optimal selection, achieving high success rates and emotional intelligence.


<details>
  <summary>Details</summary>
Motivation: LLM agents struggle with proactive, goal-driven interactions due to myopic decoding and costly planning. DialogXpert addresses this by improving strategic and empathetic dialogue planning.

Method: DialogXpert uses a frozen LLM to propose candidate actions and a compact Q-network over BERT embeddings for optimal move selection, tracking user emotions for tailored decisions.

Result: DialogXpert achieves success rates over 94% in negotiation, emotional support, and tutoring, with a larger LLM prior boosting success above 97% and improving negotiation outcomes.

Conclusion: DialogXpert enables real-time, strategic, and emotionally intelligent dialogue planning at scale, outperforming traditional LLM agents.

Abstract: Large-language-model (LLM) agents excel at reactive dialogue but struggle
with proactive, goal-driven interactions due to myopic decoding and costly
planning. We introduce DialogXpert, which leverages a frozen LLM to propose a
small, high-quality set of candidate actions per turn and employs a compact
Q-network over fixed BERT embeddings trained via temporal-difference learning
to select optimal moves within this reduced space. By tracking the user's
emotions, DialogXpert tailors each decision to advance the task while nurturing
a genuine, empathetic connection. Across negotiation, emotional support, and
tutoring benchmarks, DialogXpert drives conversations to under $3$ turns with
success rates exceeding 94\% and, with a larger LLM prior, pushes success above
97\% while markedly improving negotiation outcomes. This framework delivers
real-time, strategic, and emotionally intelligent dialogue planning at scale.
Code available at https://github.com/declare-lab/dialogxpert/

</details>


### [134] [Stereotype Detection in Natural Language Processing](https://arxiv.org/pdf/2505.17642)
*Alessandra Teresa Cignarella, Anastasia Giachanou, Els Lefever*

Main category: cs.CL

TL;DR: Survey on stereotype detection in NLP, highlighting its societal impact, methodologies, and future directions.


<details>
  <summary>Details</summary>
Motivation: Addressing the gap in NLP research on stereotype detection to prevent bias escalation and hate speech.

Method: Semi-automatic literature review of 6,000+ papers (2000-2025) using Semantic Scholar, analyzing definitions and trends.

Result: Identified key trends, methodologies, and challenges, emphasizing stereotype detection as an early-monitoring tool.

Conclusion: Calls for broader, multilingual, and intersectional approaches in NLP studies on stereotypes.

Abstract: Stereotypes influence social perceptions and can escalate into discrimination
and violence. While NLP research has extensively addressed gender bias and hate
speech, stereotype detection remains an emerging field with significant
societal implications. In this work is presented a survey of existing research,
analyzing definitions from psychology, sociology, and philosophy. A
semi-automatic literature review was performed by using Semantic Scholar. We
retrieved and filtered over 6,000 papers (in the year range 2000-2025),
identifying key trends, methodologies, challenges and future directions. The
findings emphasize stereotype detection as a potential early-monitoring tool to
prevent bias escalation and the rise of hate speech. Conclusions highlight the
need for a broader, multilingual, and intersectional approach in NLP studies.

</details>


### [135] [Bridging Electronic Health Records and Clinical Texts: Contrastive Learning for Enhanced Clinical Tasks](https://arxiv.org/pdf/2505.17643)
*Sara Ketabi, Dhanesh Ramachandram*

Main category: cs.CL

TL;DR: A deep multimodal contrastive learning framework improves clinical prediction tasks by aligning structured EHR data with unstructured notes, outperforming traditional models like XGBoost.


<details>
  <summary>Details</summary>
Motivation: Tree-based models lack contextual understanding for tasks like 30-day readmission prediction due to limited semantic information in structured EHR data.

Method: Proposes a deep multimodal contrastive learning (CL) framework to align structured EHR data with unstructured discharge notes, enhancing latent representations.

Result: Achieves a 4.1% AUROC improvement over XGBoost for 30-day readmission prediction.

Conclusion: Integrating clinical notes into EHR pipelines enhances accuracy and context-awareness in clinical decision support.

Abstract: Conventional machine learning models, particularly tree-based approaches,
have demonstrated promising performance across various clinical prediction
tasks using electronic health record (EHR) data. Despite their strengths, these
models struggle with tasks that require deeper contextual understanding, such
as predicting 30-day hospital readmission. This can be primarily due to the
limited semantic information available in structured EHR data. To address this
limitation, we propose a deep multimodal contrastive learning (CL) framework
that aligns the latent representations of structured EHR data with unstructured
discharge summary notes. It works by pulling together paired EHR and text
embeddings while pushing apart unpaired ones. Fine-tuning the pretrained EHR
encoder extracted from this framework significantly boosts downstream task
performance, e.g., a 4.1% AUROC enhancement over XGBoost for 30-day readmission
prediction. Such results demonstrate the effect of integrating domain knowledge
from clinical notes into EHR-based pipelines, enabling more accurate and
context-aware clinical decision support systems.

</details>


### [136] [Don't Overthink it. Preferring Shorter Thinking Chains for Improved LLM Reasoning](https://arxiv.org/pdf/2505.17813)
*Michael Hassid, Gabriel Synnaeve, Yossi Adi, Roy Schwartz*

Main category: cs.CL

TL;DR: Shorter reasoning chains in LLMs can outperform longer ones, with a proposed method (short-m@k) reducing compute costs and improving accuracy.


<details>
  <summary>Details</summary>
Motivation: Challenge the assumption that longer reasoning chains improve LLM performance, given their high computational costs.

Method: Introduce short-m@k, a parallel inference method that halts after m chains and uses majority voting. Also, finetune LLMs with varying chain lengths.

Result: Shorter chains are up to 34.5% more accurate. short-m@k reduces compute (40% fewer tokens) and time (33% faster) while matching or surpassing standard methods.

Conclusion: Longer reasoning chains may degrade performance; shorter chains and efficient methods like short-m@k offer better alternatives.

Abstract: Reasoning large language models (LLMs) heavily rely on scaling test-time
compute to perform complex reasoning tasks by generating extensive "thinking"
chains. While demonstrating impressive results, this approach incurs
significant computational costs and inference time. In this work, we challenge
the assumption that long thinking chains results in better reasoning
capabilities. We first demonstrate that shorter reasoning chains within
individual questions are significantly more likely to yield correct answers -
up to 34.5% more accurate than the longest chain sampled for the same question.
Based on these results, we suggest short-m@k, a novel reasoning LLM inference
method. Our method executes k independent generations in parallel and halts
computation once the first m thinking processes are done. The final answer is
chosen using majority voting among these m chains. Basic short-1@k demonstrates
similar or even superior performance over standard majority voting in
low-compute settings - using up to 40% fewer thinking tokens. short-3@k, while
slightly less efficient than short-1@k, consistently surpasses majority voting
across all compute budgets, while still being substantially faster (up to 33%
wall time reduction). Inspired by our results, we finetune an LLM using short,
long, and randomly selected reasoning chains. We then observe that training on
the shorter ones leads to better performance. Our findings suggest rethinking
current methods of test-time compute in reasoning LLMs, emphasizing that longer
"thinking" does not necessarily translate to improved performance and can,
counter-intuitively, lead to degraded results.

</details>


### [137] [Too Consistent to Detect: A Study of Self-Consistent Errors in LLMs](https://arxiv.org/pdf/2505.17656)
*Hexiang Tan, Fei Sun, Sha Liu, Du Su, Qi Cao, Xin Chen, Jingang Wang, Xunliang Cai, Yuanzhuo Wang, Huawei Shen, Xueqi Cheng*

Main category: cs.CL

TL;DR: The paper highlights self-consistent errors in LLMs, where incorrect responses persist across samples, and evaluates detection methods, finding them ineffective. A cross-model probe method is proposed to improve detection.


<details>
  <summary>Details</summary>
Motivation: Existing error detection methods fail to address self-consistent errors in LLMs, which remain stable or increase with model scale, necessitating better solutions.

Method: The study defines self-consistent errors, evaluates mainstream detection methods, and proposes a cross-model probe method using hidden state evidence from an external verifier LLM.

Result: Detection methods struggle with self-consistent errors, but the proposed cross-model probe significantly improves performance across three LLM families.

Conclusion: Current detection methods have limitations for self-consistent errors, and the cross-model probe offers a promising solution.

Abstract: As large language models (LLMs) often generate plausible but incorrect
content, error detection has become increasingly critical to ensure
truthfulness. However, existing detection methods often overlook a critical
problem we term as self-consistent error, where LLMs repeatly generate the same
incorrect response across multiple stochastic samples. This work formally
defines self-consistent errors and evaluates mainstream detection methods on
them. Our investigation reveals two key findings: (1) Unlike inconsistent
errors, whose frequency diminishes significantly as LLM scale increases, the
frequency of self-consistent errors remains stable or even increases. (2) All
four types of detection methshods significantly struggle to detect
self-consistent errors. These findings reveal critical limitations in current
detection methods and underscore the need for improved methods. Motivated by
the observation that self-consistent errors often differ across LLMs, we
propose a simple but effective cross-model probe method that fuses hidden state
evidence from an external verifier LLM. Our method significantly enhances
performance on self-consistent errors across three LLM families.

</details>


### [138] [Towards Dynamic Theory of Mind: Evaluating LLM Adaptation to Temporal Evolution of Human States](https://arxiv.org/pdf/2505.17663)
*Yang Xiao, Jiashuo Wang, Qiancheng Xu, Changhe Song, Chunpu Xu, Yi Cheng, Wenjie Li, Pengfei Liu*

Main category: cs.CL

TL;DR: The paper introduces 	extsc{DynToM}, a benchmark to evaluate LLMs' ability to track dynamic mental states, revealing a 44.7% performance gap compared to humans.


<details>
  <summary>Details</summary>
Motivation: Existing ToM benchmarks focus on static mental states, missing the temporal evolution crucial for real-world interactions.

Method: A four-step framework generates 1,100 social contexts with 5,500 scenarios and 78,100 questions, validated for realism.

Result: Ten LLMs underperform humans by 44.7%, especially in tracking shifting mental states.

Conclusion: Current LLMs struggle to model the dynamic nature of human mental states, highlighting a key limitation.

Abstract: As Large Language Models (LLMs) increasingly participate in human-AI
interactions, evaluating their Theory of Mind (ToM) capabilities - particularly
their ability to track dynamic mental states - becomes crucial. While existing
benchmarks assess basic ToM abilities, they predominantly focus on static
snapshots of mental states, overlooking the temporal evolution that
characterizes real-world social interactions. We present \textsc{DynToM}, a
novel benchmark specifically designed to evaluate LLMs' ability to understand
and track the temporal progression of mental states across interconnected
scenarios. Through a systematic four-step framework, we generate 1,100 social
contexts encompassing 5,500 scenarios and 78,100 questions, each validated for
realism and quality. Our comprehensive evaluation of ten state-of-the-art LLMs
reveals that their average performance underperforms humans by 44.7\%, with
performance degrading significantly when tracking and reasoning about the shift
of mental states. This performance gap highlights fundamental limitations in
current LLMs' ability to model the dynamic nature of human mental states.

</details>


### [139] [QwenLong-L1: Towards Long-Context Large Reasoning Models with Reinforcement Learning](https://arxiv.org/pdf/2505.17667)
*Fanqi Wan, Weizhou Shen, Shengyi Liao, Yingcheng Shi, Chenliang Li, Ziyi Yang, Ji Zhang, Fei Huang, Jingren Zhou, Ming Yan*

Main category: cs.CL

TL;DR: QwenLong-L1 adapts short-context reasoning models to long-context tasks via progressive scaling, outperforming top models like OpenAI-o3-mini and matching Claude-3.7-Sonnet-Thinking.


<details>
  <summary>Details</summary>
Motivation: Extending large reasoning models (LRMs) to long-context inputs via reinforcement learning (RL) is a critical unsolved challenge due to suboptimal training efficiency and unstable optimization.

Method: Proposes QwenLong-L1, using warm-up supervised fine-tuning, curriculum-guided RL, and difficulty-aware retrospective sampling to stabilize and enhance policy evolution.

Result: Outperforms flagship LRMs on seven benchmarks, achieving performance comparable to Claude-3.7-Sonnet-Thinking.

Conclusion: Advances practical long-context LRMs for robust reasoning in information-intensive environments.

Abstract: Recent large reasoning models (LRMs) have demonstrated strong reasoning
capabilities through reinforcement learning (RL). These improvements have
primarily been observed within the short-context reasoning tasks. In contrast,
extending LRMs to effectively process and reason on long-context inputs via RL
remains a critical unsolved challenge. To bridge this gap, we first formalize
the paradigm of long-context reasoning RL, and identify key challenges in
suboptimal training efficiency and unstable optimization process. To address
these issues, we propose QwenLong-L1, a framework that adapts short-context
LRMs to long-context scenarios via progressive context scaling. Specifically,
we utilize a warm-up supervised fine-tuning (SFT) stage to establish a robust
initial policy, followed by a curriculum-guided phased RL technique to
stabilize the policy evolution, and enhanced with a difficulty-aware
retrospective sampling strategy to incentivize the policy exploration.
Experiments on seven long-context document question-answering benchmarks
demonstrate that QwenLong-L1-32B outperforms flagship LRMs like OpenAI-o3-mini
and Qwen3-235B-A22B, achieving performance on par with
Claude-3.7-Sonnet-Thinking, demonstrating leading performance among
state-of-the-art LRMs. This work advances the development of practical
long-context LRMs capable of robust reasoning across information-intensive
environments.

</details>


### [140] [MIDB: Multilingual Instruction Data Booster for Enhancing Multilingual Instruction Synthesis](https://arxiv.org/pdf/2505.17671)
*Yilun Liu, Chunguang Zhao, Xinhua Yang, Hongyong Zeng, Shimin Tao, Weibin Meng, Minggui He, Chang Su, Yan Yu, Hongxia Ma, Li Zhang, Daimeng Wei, Hao Yang*

Main category: cs.CL

TL;DR: MIDB is a tool to improve multilingual synthesized instruction data quality by addressing content errors, MT defects, and localization issues, enhancing multilingual LLMs' performance.


<details>
  <summary>Details</summary>
Motivation: Multilingual synthesized instruction data suffer from quality issues due to translation errors and insufficient localization, hindering multilingual LLM training.

Method: MIDB is trained on 36.8k human-revised examples across 16 languages to automatically correct content errors, MT defects, and improve localization.

Result: MIDB improved data quality in 16 languages and boosted multilingual LLMs' instruction-following and cultural-understanding abilities.

Conclusion: MIDB effectively addresses quality issues in multilingual synthesized data, enhancing the performance of multilingual LLMs.

Abstract: Despite doubts on data quality, instruction synthesis has been widely applied
into instruction tuning (IT) of LLMs as an economic and rapid alternative.
Recent endeavors focus on improving data quality for synthesized instruction
pairs in English and have facilitated IT of English-centric LLMs. However, data
quality issues in multilingual synthesized instruction pairs are even more
severe, since the common synthesizing practice is to translate English
synthesized data into other languages using machine translation (MT). Besides
the known content errors in these English synthesized data, multilingual
synthesized instruction data are further exposed to defects introduced by MT
and face insufficient localization of the target languages. In this paper, we
propose MIDB, a Multilingual Instruction Data Booster to automatically address
the quality issues in multilingual synthesized data. MIDB is trained on around
36.8k revision examples across 16 languages by human linguistic experts,
thereby can boost the low-quality data by addressing content errors and MT
defects, and improving localization in these synthesized data. Both automatic
and human evaluation indicate that not only MIDB steadily improved instruction
data quality in 16 languages, but also the instruction-following and
cultural-understanding abilities of multilingual LLMs fine-tuned on
MIDB-boosted data were significantly enhanced.

</details>


### [141] [ELSPR: Evaluator LLM Training Data Self-Purification on Non-Transitive Preferences via Tournament Graph Reconstruction](https://arxiv.org/pdf/2505.17691)
*Yan Yu, Yilun Liu, Minggui He, Shimin Tao, Weibin Meng, Xinhua Yang, Li Zhang, Hongxia Ma, Chang Su, Hao Yang, Fuliang Li*

Main category: cs.CL

TL;DR: The paper addresses non-transitivity in LLM evaluations, proposing a graph-theoretic framework and filtering strategy (ELSPR) to improve preference clarity and reduce biases.


<details>
  <summary>Details</summary>
Motivation: Non-transitive preferences in LLM evaluations remain unresolved, potentially due to low-quality training data, impacting evaluation reliability.

Method: A graph-theoretic framework models pairwise preferences as tournament graphs, quantifying non-transitivity and structural entropy. The ELSPR filtering strategy removes inconsistent data for fine-tuning.

Result: Filtered data reduces non-transitivity by 13.78%, lowers structural entropy by 0.0879, and improves alignment with human evaluators.

Conclusion: The proposed framework and filtering strategy effectively mitigate non-transitivity in LLM evaluations, enhancing reliability and human agreement.

Abstract: Large language models (LLMs) are widely used as evaluators for open-ended
tasks, while previous research has emphasized biases in LLM evaluations, the
issue of non-transitivity in pairwise comparisons remains unresolved:
non-transitive preferences for pairwise comparisons, where evaluators prefer A
over B, B over C, but C over A. Our results suggest that low-quality training
data may reduce the transitivity of preferences generated by the Evaluator LLM.
To address this, We propose a graph-theoretic framework to analyze and mitigate
this problem by modeling pairwise preferences as tournament graphs. We quantify
non-transitivity and introduce directed graph structural entropy to measure the
overall clarity of preferences. Our analysis reveals significant
non-transitivity in advanced Evaluator LLMs (with Qwen2.5-Max exhibiting
67.96%), as well as high entropy values (0.8095 for Qwen2.5-Max), reflecting
low overall clarity of preferences. To address this issue, we designed a
filtering strategy, ELSPR, to eliminate preference data that induces
non-transitivity, retaining only consistent and transitive preference data for
model fine-tuning. Experiments demonstrate that models fine-tuned with filtered
data reduce non-transitivity by 13.78% (from 64.28% to 50.50%), decrease
structural entropy by 0.0879 (from 0.8113 to 0.7234), and align more closely
with human evaluators (human agreement rate improves by 0.6% and Spearman
correlation increases by 0.01).

</details>


### [142] [Activation Control for Efficiently Eliciting Long Chain-of-thought Ability of Language Models](https://arxiv.org/pdf/2505.17697)
*Zekai Zhao, Qi Liu, Kun Zhou, Zihan Liu, Yifei Shao, Zhiting Hu, Biwei Huang*

Main category: cs.CL

TL;DR: The paper introduces a training-free method to elicit long chain-of-thought (CoT) reasoning in LLMs by amplifying key activations and inserting "wait" tokens, improving self-reflection and accuracy.


<details>
  <summary>Details</summary>
Motivation: Eliciting long CoT reasoning in LLMs typically requires costly training methods, prompting the need for a simpler, training-free approach.

Method: Amplify high-impact activations in the last few layers and insert "wait" tokens to invoke long CoT reasoning without training. Also, introduce a parameter-efficient fine-tuning method.

Result: The method significantly increases self-reflection rates and accuracy, outperforming full LoRA fine-tuning with fewer parameters.

Conclusion: The proposed activation control technique efficiently elicits long CoT reasoning and improves LLM performance, offering a practical alternative to costly training methods.

Abstract: Despite the remarkable reasoning performance, eliciting the long
chain-of-thought (CoT) ability in large language models (LLMs) typically
requires costly reinforcement learning or supervised fine-tuning on
high-quality distilled data. We investigate the internal mechanisms behind this
capability and show that a small set of high-impact activations in the last few
layers largely governs long-form reasoning attributes, such as output length
and self-reflection. By simply amplifying these activations and inserting
"wait" tokens, we can invoke the long CoT ability without any training,
resulting in significantly increased self-reflection rates and accuracy.
Moreover, we find that the activation dynamics follow predictable trajectories,
with a sharp rise after special tokens and a subsequent exponential decay.
Building on these insights, we introduce a general training-free activation
control technique. It leverages a few contrastive examples to identify key
activations, and employs simple analytic functions to modulate their values at
inference time to elicit long CoTs. Extensive experiments confirm the
effectiveness of our method in efficiently eliciting long CoT reasoning in LLMs
and improving their performance. Additionally, we propose a parameter-efficient
fine-tuning method that trains only a last-layer activation amplification
module and a few LoRA layers, outperforming full LoRA fine-tuning on reasoning
benchmarks with significantly fewer parameters. Our code and data are publicly
released.

</details>


### [143] [SemSketches-2021: experimenting with the machine processing of the pilot semantic sketches corpus](https://arxiv.org/pdf/2505.17704)
*Maria Ponomareva, Maria Petrova, Julia Detkova, Oleg Serikov, Maria Yarova*

Main category: cs.CL

TL;DR: The paper explores machine processing of semantic sketches, introduces a corpus, and discusses its creation and applications, including a shared task for context assignment.


<details>
  <summary>Details</summary>
Motivation: To advance machine processing of semantic sketches by creating a corpus and developing tools for their analysis.

Method: Presents a pilot corpus of semantic sketches and organizes the SemSketches-2021 Shared Task for context assignment.

Result: A corpus of semantic sketches was created, and a shared task evaluated context assignment to sketches.

Conclusion: The work contributes to the field by providing a corpus and tools for processing semantic sketches, validated through a shared task.

Abstract: The paper deals with elaborating different approaches to the machine
processing of semantic sketches. It presents the pilot open corpus of semantic
sketches. Different aspects of creating the sketches are discussed, as well as
the tasks that the sketches can help to solve. Special attention is paid to the
creation of the machine processing tools for the corpus. For this purpose, the
SemSketches-2021 Shared Task was organized. The participants were given the
anonymous sketches and a set of contexts containing the necessary predicates.
During the Task, one had to assign the proper contexts to the corresponding
sketches.

</details>


### [144] [Understanding How Value Neurons Shape the Generation of Specified Values in LLMs](https://arxiv.org/pdf/2505.17712)
*Yi Su, Jiayi Zhang, Shu Yang, Xinhai Wang, Lijie Hu, Di Wang*

Main category: cs.CL

TL;DR: ValueLocate is a framework for interpreting how values are encoded in LLMs using the Schwartz Values Survey, enabling precise neuron identification and causal manipulation of value representations.


<details>
  <summary>Details</summary>
Motivation: Concerns about LLM alignment with ethical principles due to opaque internal value representations, despite behavioral alignment advancements.

Method: Develops ValueInsight dataset for universal value dimensions, uses neuron identification via activation differences, and validates through targeted neuron manipulation.

Result: Demonstrates causal relationships between neurons and value representations, enabling effective alteration of model value orientations.

Conclusion: Bridges psychological value frameworks with neuron analysis, advancing value alignment in LLMs.

Abstract: Rapid integration of large language models (LLMs) into societal applications
has intensified concerns about their alignment with universal ethical
principles, as their internal value representations remain opaque despite
behavioral alignment advancements. Current approaches struggle to
systematically interpret how values are encoded in neural architectures,
limited by datasets that prioritize superficial judgments over mechanistic
analysis. We introduce ValueLocate, a mechanistic interpretability framework
grounded in the Schwartz Values Survey, to address this gap. Our method first
constructs ValueInsight, a dataset that operationalizes four dimensions of
universal value through behavioral contexts in the real world. Leveraging this
dataset, we develop a neuron identification method that calculates activation
differences between opposing value aspects, enabling precise localization of
value-critical neurons without relying on computationally intensive attribution
methods. Our proposed validation method demonstrates that targeted manipulation
of these neurons effectively alters model value orientations, establishing
causal relationships between neurons and value representations. This work
advances the foundation for value alignment by bridging psychological value
frameworks with neuron analysis in LLMs.

</details>


### [145] [MOOSE-Chem3: Toward Experiment-Guided Hypothesis Ranking via Simulated Experimental Feedback](https://arxiv.org/pdf/2505.17873)
*Wanhao Liu, Zonglin Yang, Jue Wang, Lidong Bing, Di Zhang, Dongzhan Zhou, Yuqiang Li, Houqiang Li, Erik Cambria, Wanli Ouyang*

Main category: cs.CL

TL;DR: The paper introduces experiment-guided hypothesis ranking using a simulator to prioritize hypotheses based on simulated experimental feedback, outperforming pre-experiment baselines.


<details>
  <summary>Details</summary>
Motivation: Addressing the challenge of costly and limited-throughput wet-lab experiments in natural sciences by incorporating empirical outcomes into hypothesis ranking.

Method: Proposes a simulator modeling hypothesis performance based on similarity to ground truth and noise, validated on a chemistry dataset, and develops a clustering-based ranking method using simulated feedback.

Result: The method outperforms pre-experiment baselines and strong ablations in experiments.

Conclusion: Experiment-guided ranking, enabled by a domain-informed simulator, improves hypothesis prioritization without requiring real repeated experiments.

Abstract: Hypothesis ranking is a crucial component of automated scientific discovery,
particularly in natural sciences where wet-lab experiments are costly and
throughput-limited. Existing approaches focus on pre-experiment ranking,
relying solely on large language model's internal reasoning without
incorporating empirical outcomes from experiments. We introduce the task of
experiment-guided ranking, which aims to prioritize candidate hypotheses based
on the results of previously tested ones. However, developing such strategies
is challenging due to the impracticality of repeatedly conducting real
experiments in natural science domains. To address this, we propose a simulator
grounded in three domain-informed assumptions, modeling hypothesis performance
as a function of similarity to a known ground truth hypothesis, perturbed by
noise. We curate a dataset of 124 chemistry hypotheses with experimentally
reported outcomes to validate the simulator. Building on this simulator, we
develop a pseudo experiment-guided ranking method that clusters hypotheses by
shared functional characteristics and prioritizes candidates based on insights
derived from simulated experimental feedback. Experiments show that our method
outperforms pre-experiment baselines and strong ablations.

</details>


### [146] [The Pilot Corpus of the English Semantic Sketches](https://arxiv.org/pdf/2505.17733)
*Maria Petrova, Maria Ponomareva, Alexandra Ivoylova*

Main category: cs.CL

TL;DR: The paper creates semantic sketches for English verbs, using an English-Russian corpus to highlight cross-language differences and analyze sketch-building processes and errors.


<details>
  <summary>Details</summary>
Motivation: To explore contrastive studies of English verbs through semantic sketches and examine cross-language semantic differences.

Method: Developed a pilot corpus of English-Russian sketch pairs, analyzed cross-language differences, and studied the sketch-building process and errors.

Result: Identified linguistic insights from cross-language differences and mistakes in semantic sketch creation.

Conclusion: Semantic sketches are valuable for contrastive studies and reveal linguistic nuances in verb semantics across languages.

Abstract: The paper is devoted to the creation of the semantic sketches for English
verbs. The pilot corpus consists of the English-Russian sketch pairs and is
aimed to show what kind of contrastive studies the sketches help to conduct.
Special attention is paid to the cross-language differences between the
sketches with similar semantics. Moreover, we discuss the process of building a
semantic sketch, and analyse the mistakes that could give insight to the
linguistic nature of sketches.

</details>


### [147] [Fast Quiet-STaR: Thinking Without Thought Tokens](https://arxiv.org/pdf/2505.17746)
*Wei Huang, Yizhe Xiong, Xin Ye, Zhijie Deng, Hui Chen, Zijia Lin, Guiguang Ding*

Main category: cs.CL

TL;DR: Fast Quiet STaR improves reasoning efficiency in LLMs by reducing computational costs while maintaining performance, outperforming Quiet STaR in accuracy under the same inference time.


<details>
  <summary>Details</summary>
Motivation: Current LLMs struggle with complex reasoning tasks despite scaling, and existing methods like Quiet STaR incur high inference overhead.

Method: Introduces a curriculum learning strategy to reduce thought tokens and reinforcement learning-based fine-tuning for Next Token Prediction (NTP).

Result: Achieves 9% and 5.7% accuracy improvements on Mistral 7B and Qwen2.5 7B, respectively, with no added latency.

Conclusion: Fast Quiet STaR offers a more efficient reasoning framework for LLMs, balancing performance and computational cost.

Abstract: Large Language Models (LLMs) have achieved impressive performance across a
range of natural language processing tasks. However, recent advances
demonstrate that further gains particularly in complex reasoning tasks require
more than merely scaling up model sizes or training data. One promising
direction is to enable models to think during the reasoning process. Recently,
Quiet STaR significantly improves reasoning by generating token-level thought
traces, but incurs substantial inference overhead. In this work, we propose
Fast Quiet STaR, a more efficient reasoning framework that preserves the
benefits of token-level reasoning while reducing computational cost. Our method
introduces a curriculum learning based training strategy that gradually reduces
the number of thought tokens, enabling the model to internalize more abstract
and concise reasoning processes. We further extend this approach to the
standard Next Token Prediction (NTP) setting through reinforcement
learning-based fine-tuning, resulting in Fast Quiet-STaR NTP, which eliminates
the need for explicit thought token generation during inference. Experiments on
four benchmark datasets with Mistral 7B and Qwen2.5 7B demonstrate that Fast
Quiet-STaR consistently outperforms Quiet-STaR in terms of average accuracy
under the same inference time budget. Notably, Fast Quiet-STaR NTP achieves an
average accuracy improvement of 9\% on Mistral 7B and 5.7\% on Qwen2.5 7B,
while maintaining the same inference latency. Our code will be available at
https://github.com/huangwei200012/Fast-Quiet-STaR.

</details>


### [148] [Mutarjim: Advancing Bidirectional Arabic-English Translation with a Small Language Model](https://arxiv.org/pdf/2505.17894)
*Khalil Hennara, Muhammad Hreden, Mohamed Motaism Hamed, Zeina Aldallal, Sara Chrouf, Safwan AlModhayan*

Main category: cs.CL

TL;DR: Mutarjim is a compact Arabic-English translation model that outperforms larger models using optimized training and a high-quality corpus. It introduces Tarjama-25, a new benchmark for better evaluation.


<details>
  <summary>Details</summary>
Motivation: Large models dominate machine translation, but smaller models can be efficient and effective. Mutarjim aims to bridge this gap for Arabic-English translation.

Method: Developed based on Kuwain-1.5B, Mutarjim uses a two-phase training approach and a curated corpus.

Result: Mutarjim rivals models 20x larger, reduces costs, and achieves state-of-the-art performance on Tarjama-25.

Conclusion: Mutarjim proves smaller models can excel, and Tarjama-25 provides a robust benchmark for future research.

Abstract: We introduce Mutarjim, a compact yet powerful language model for
bidirectional Arabic-English translation. While large-scale LLMs have shown
impressive progress in natural language processing tasks, including machine
translation, smaller models. Leveraging this insight, we developed Mutarjim
based on Kuwain-1.5B , a language model tailored for both Arabic and English.
Despite its modest size, Mutarjim outperforms much larger models on several
established benchmarks, achieved through an optimized two-phase training
approach and a carefully curated, high-quality training corpus.. Experimental
results show that Mutarjim rivals models up to 20 times larger while
significantly reducing computational costs and training requirements. We also
introduce Tarjama-25, a new benchmark designed to overcome limitations in
existing Arabic-English benchmarking datasets, such as domain narrowness, short
sentence lengths, and English-source bias. Tarjama-25 comprises 5,000
expert-reviewed sentence pairs and spans a wide range of domains, offering a
more comprehensive and balanced evaluation framework. Notably, Mutarjim
achieves state-of-the-art performance on the English-to-Arabic task in
Tarjama-25, surpassing even significantly larger and proprietary models like
GPT-4o mini. We publicly release Tarjama-25 to support future research and
advance the evaluation of Arabic-English translation systems.

</details>


### [149] [Discriminating Form and Meaning in Multilingual Models with Minimal-Pair ABX Tasks](https://arxiv.org/pdf/2505.17747)
*Maureen de Seyssel, Jie Chi, Skyler Seto, Maartje ter Hoeve, Masha Fedzechkina, Natalie Schluter*

Main category: cs.CL

TL;DR: The paper introduces training-free ABX-style tasks to evaluate multilingual language models' representation of language identity and semantic content, showing how these evolve during training and across layers.


<details>
  <summary>Details</summary>
Motivation: To provide a flexible and interpretable alternative to probing for analyzing multilingual language models' representations.

Method: Uses ABX-style discrimination tasks inspired by speech processing to measure minimal differences in representation. Applied to XLM-R across pretraining checkpoints and layers.

Result: Language discrimination declines over training and concentrates in lower layers, while meaning discrimination strengthens and stabilizes in deeper layers.

Conclusion: ABX tasks offer a lightweight framework for analyzing multilingual representations, with some alignment to linguistic learning performance.

Abstract: We introduce a set of training-free ABX-style discrimination tasks to
evaluate how multilingual language models represent language identity (form)
and semantic content (meaning). Inspired from speech processing, these
zero-shot tasks measure whether minimal differences in representation can be
reliably detected. This offers a flexible and interpretable alternative to
probing. Applied to XLM-R (Conneau et al, 2020) across pretraining checkpoints
and layers, we find that language discrimination declines over training and
becomes concentrated in lower layers, while meaning discrimination strengthens
over time and stabilizes in deeper layers. We then explore probing tasks,
showing some alignment between our metrics and linguistic learning performance.
Our results position ABX tasks as a lightweight framework for analyzing the
structure of multilingual representations.

</details>


### [150] [Resolving Conflicting Evidence in Automated Fact-Checking: A Study on Retrieval-Augmented LLMs](https://arxiv.org/pdf/2505.17762)
*Ziyu Ge, Yuhao Wu, Daniel Wai Kit Chin, Roy Ka-Wei Lee, Rui Cao*

Main category: cs.CL

TL;DR: The paper evaluates Retrieval-Augmented Generation (RAG) models for fact-checking with conflicting evidence, introduces the CONFACT dataset, and proposes integrating source credibility to improve performance.


<details>
  <summary>Details</summary>
Motivation: Despite the potential of LLMs with retrieval mechanisms for fact-checking, their reliability falters with conflicting evidence from sources of varying credibility. This study aims to systematically evaluate and improve RAG models in such scenarios.

Method: The study introduces the CONFACT dataset for evaluating RAG models with conflicting evidence. It investigates strategies to integrate media source credibility into retrieval and generation stages.

Result: Experiments reveal vulnerabilities in RAG models when handling conflicting evidence, especially due to source credibility differences. Incorporating source credibility improves conflict resolution and fact-checking performance.

Conclusion: Effectively integrating source credibility into RAG models enhances their ability to resolve conflicting evidence, improving fact-checking reliability.

Abstract: Large Language Models (LLMs) augmented with retrieval mechanisms have
demonstrated significant potential in fact-checking tasks by integrating
external knowledge. However, their reliability decreases when confronted with
conflicting evidence from sources of varying credibility. This paper presents
the first systematic evaluation of Retrieval-Augmented Generation (RAG) models
for fact-checking in the presence of conflicting evidence. To support this
study, we introduce \textbf{CONFACT} (\textbf{Con}flicting Evidence for
\textbf{Fact}-Checking) (Dataset available at
https://github.com/zoeyyes/CONFACT), a novel dataset comprising questions
paired with conflicting information from various sources. Extensive experiments
reveal critical vulnerabilities in state-of-the-art RAG methods, particularly
in resolving conflicts stemming from differences in media source credibility.
To address these challenges, we investigate strategies to integrate media
background information into both the retrieval and generation stages. Our
results show that effectively incorporating source credibility significantly
enhances the ability of RAG models to resolve conflicting evidence and improve
fact-checking performance.

</details>


### [151] [The Real Barrier to LLM Agent Usability is Agentic ROI](https://arxiv.org/pdf/2505.17767)
*Weiwen Liu, Jiarui Qin, Xu Huang, Xingshan Zeng, Yunjia Xi, Jianghao Lin, Chuhan Wu, Yasheng Wang, Lifeng Shang, Ruiming Tang, Defu Lian, Yong Yu, Weinan Zhang*

Main category: cs.CL

TL;DR: The paper discusses the usability gap in LLM agents, proposing a shift from optimizing model performance to evaluating agents based on Agentic ROI (return on investment), focusing on information quality, time, and cost.


<details>
  <summary>Details</summary>
Motivation: The limited real-world adoption of LLM agents is due to a tradeoff between their value and the costs incurred, highlighting the need for a utility-driven approach.

Method: Proposes a zigzag development trajectory: first scaling up to improve information quality, then scaling down to minimize time and cost.

Result: Identifies key factors (information quality, agent time, cost) for Agentic ROI and outlines a roadmap to bridge usability gaps.

Conclusion: Advocates for a broader perspective on LLM agent development to enhance scalability, accessibility, and real-world effectiveness.

Abstract: Large Language Model (LLM) agents represent a promising shift in human-AI
interaction, moving beyond passive prompt-response systems to autonomous agents
capable of reasoning, planning, and goal-directed action. Despite the
widespread application in specialized, high-effort tasks like coding and
scientific research, we highlight a critical usability gap in high-demand,
mass-market applications. This position paper argues that the limited
real-world adoption of LLM agents stems not only from gaps in model
capabilities, but also from a fundamental tradeoff between the value an agent
can provide and the costs incurred during real-world use. Hence, we call for a
shift from solely optimizing model performance to a broader, utility-driven
perspective: evaluating agents through the lens of the overall agentic return
on investment (Agent ROI). By identifying key factors that determine Agentic
ROI--information quality, agent time, and cost--we posit a zigzag development
trajectory in optimizing agentic ROI: first scaling up to improve the
information quality, then scaling down to minimize the time and cost. We
outline the roadmap across different development stages to bridge the current
usability gaps, aiming to make LLM agents truly scalable, accessible, and
effective in real-world contexts.

</details>


### [152] [EXECUTE: A Multilingual Benchmark for LLM Token Understanding](https://arxiv.org/pdf/2505.17784)
*Lukas Edman, Helmut Schmid, Alexander Fraser*

Main category: cs.CL

TL;DR: The paper introduces EXECUTE, an extension of the CUTE benchmark, to test LLMs' character understanding in diverse languages, revealing varied challenges beyond character-level issues.


<details>
  <summary>Details</summary>
Motivation: To assess LLMs' performance in understanding characters and scripts across multiple languages, extending the limitations of the English-focused CUTE benchmark.

Method: Developed the EXECUTE framework for easy expansion to any language, tested LLMs across diverse scripts, and examined sub-character tasks in Chinese, Japanese, and Korean.

Result: LLMs face varied challenges: some languages show word-level issues, others none, and sub-character tasks reveal deeper script understanding gaps.

Conclusion: EXECUTE highlights the need for broader language benchmarks to fully evaluate LLMs' script and character comprehension.

Abstract: The CUTE benchmark showed that LLMs struggle with character understanding in
English. We extend it to more languages with diverse scripts and writing
systems, introducing EXECUTE. Our simplified framework allows easy expansion to
any language. Tests across multiple LLMs reveal that challenges in other
languages are not always on the character level as in English. Some languages
show word-level processing issues, some show no issues at all. We also examine
sub-character tasks in Chinese, Japanese, and Korean to assess LLMs'
understanding of character components.

</details>


### [153] [Compression Hacking: A Supplementary Perspective on Informatics Metric of Language Models from Geometric Distortion](https://arxiv.org/pdf/2505.17793)
*Jianxiang Zang, Meiling Ning, Yongda Wei, Shihan Dou, Jiazheng Zhang, Nijia Mo, Binhong Li, Tao Gui, Qi Zhang, Xuanjing Huang*

Main category: cs.CL

TL;DR: The paper introduces refined compression metrics for language models (LMs) by analyzing geometric distortion, addressing the issue of anisotropic word representations caused by high compression.


<details>
  <summary>Details</summary>
Motivation: To improve the interpretation of LM intelligence by addressing the problem of anisotropic representations in highly compressed LMs, which hinder performance.

Method: Proposes three refined compression metrics incorporating geometric distortion analysis and integrates them into a self-evaluation pipeline.

Result: The refined metrics show strong alignment with LM capabilities (Spearman correlation >0.9), outperforming original compression and other metrics.

Conclusion: Incorporating geometric distortion enhances the informatics interpretation of LMs, confirming the impact of 'compression hacking.'

Abstract: Recently, the concept of ``compression as intelligence'' has provided a novel
informatics metric perspective for language models (LMs), emphasizing that
highly structured representations signify the intelligence level of LMs.
However, from a geometric standpoint, the word representation space of highly
compressed LMs tends to degenerate into a highly anisotropic state, which
hinders the LM's ability to comprehend instructions and directly impacts its
performance. We found this compression-anisotropy synchronicity is essentially
the ``Compression Hacking'' in LM representations, where noise-dominated
directions tend to create the illusion of high compression rates by sacrificing
spatial uniformity. Based on this, we propose three refined compression metrics
by incorporating geometric distortion analysis and integrate them into a
self-evaluation pipeline. The refined metrics exhibit strong alignment with the
LM's comprehensive capabilities, achieving Spearman correlation coefficients
above 0.9, significantly outperforming both the original compression and other
internal structure-based metrics. This confirms that compression hacking
substantially enhances the informatics interpretation of LMs by incorporating
geometric distortion of representations.

</details>


### [154] [Handling Symbolic Language in Student Texts: A Comparative Study of NLP Embedding Models](https://arxiv.org/pdf/2505.17950)
*Tom Bleckmann, Paul Tschisgale*

Main category: cs.CL

TL;DR: This study evaluates NLP embedding models for processing science-related symbolic expressions, finding GPT-text-embedding-3-large performs best, though marginally, and highlights cost, compliance, and transparency as key selection factors.


<details>
  <summary>Details</summary>
Motivation: Current NLP embedding models struggle with symbolic expressions in science-related language, leading to biased or incomplete LA applications.

Method: Various embedding models are tested on physics-specific symbolic expressions using similarity-based analyses and machine learning pipelines.

Result: GPT-text-embedding-3-large outperforms others, but the advantage is moderate. Cost, compliance, and transparency are also critical.

Conclusion: Careful selection of NLP embedding models is crucial for LA applications involving science-related symbolic expressions.

Abstract: Recent advancements in Natural Language Processing (NLP) have facilitated the
analysis of student-generated language products in learning analytics (LA),
particularly through the use of NLP embedding models. Yet when it comes to
science-related language, symbolic expressions such as equations and formulas
introduce challenges that current embedding models struggle to address.
Existing studies and applications often either overlook these challenges or
remove symbolic expressions altogether, potentially leading to biased findings
and diminished performance of LA applications. This study therefore explores
how contemporary embedding models differ in their capability to process and
interpret science-related symbolic expressions. To this end, various embedding
models are evaluated using physics-specific symbolic expressions drawn from
authentic student responses, with performance assessed via two approaches:
similarity-based analyses and integration into a machine learning pipeline. Our
findings reveal significant differences in model performance, with OpenAI's
GPT-text-embedding-3-large outperforming all other examined models, though its
advantage over other models was moderate rather than decisive. Beyond
performance, additional factors such as cost, regulatory compliance, and model
transparency are discussed as key considerations for model selection. Overall,
this study underscores the importance for LA researchers and practitioners of
carefully selecting NLP embedding models when working with science-related
language products that include symbolic expressions.

</details>


### [155] [Low-Resource NMT: A Case Study on the Written and Spoken Languages in Hong Kong](https://arxiv.org/pdf/2505.17816)
*Hei Yi Mak, Tan Lee*

Main category: cs.CL

TL;DR: A transformer-based NMT system for translating written Chinese to written Cantonese is proposed, addressing data scarcity by mining parallel sentences from Wikipedia, outperforming Baidu Fanyi in most test sets.


<details>
  <summary>Details</summary>
Motivation: The demand for automatic translation between Chinese and Cantonese is growing due to their lexical and grammatical differences and increasing online interaction.

Method: A transformer-based NMT system is developed, with a focus on data preparation. Parallel sentences are collected from linguistic studies, internet resources, and Wikipedia.

Result: The system outperforms Baidu Fanyi in 6 out of 8 test sets, capturing key linguistic transformations between Chinese and Cantonese.

Conclusion: Mining similar sentence pairs from Wikipedia improves translation performance, demonstrating the system's effectiveness for Chinese-to-Cantonese translation.

Abstract: The majority of inhabitants in Hong Kong are able to read and write in
standard Chinese but use Cantonese as the primary spoken language in daily
life. Spoken Cantonese can be transcribed into Chinese characters, which
constitute the so-called written Cantonese. Written Cantonese exhibits
significant lexical and grammatical differences from standard written Chinese.
The rise of written Cantonese is increasingly evident in the cyber world. The
growing interaction between Mandarin speakers and Cantonese speakers is leading
to a clear demand for automatic translation between Chinese and Cantonese. This
paper describes a transformer-based neural machine translation (NMT) system for
written-Chinese-to-written-Cantonese translation. Given that parallel text data
of Chinese and Cantonese are extremely scarce, a major focus of this study is
on the effort of preparing good amount of training data for NMT. In addition to
collecting 28K parallel sentences from previous linguistic studies and
scattered internet resources, we devise an effective approach to obtaining 72K
parallel sentences by automatically extracting pairs of semantically similar
sentences from parallel articles on Chinese Wikipedia and Cantonese Wikipedia.
We show that leveraging highly similar sentence pairs mined from Wikipedia
improves translation performance in all test sets. Our system outperforms Baidu
Fanyi's Chinese-to-Cantonese translation on 6 out of 8 test sets in BLEU
scores. Translation examples reveal that our system is able to capture
important linguistic transformations between standard Chinese and spoken
Cantonese.

</details>


### [156] [Beyond Distillation: Pushing the Limits of Medical LLM Reasoning with Minimalist Rule-Based RL](https://arxiv.org/pdf/2505.17952)
*Che Liu, Haozhe Wang, Jiazhen Pan, Zhongwei Wan, Yong Dai, Fangzhen Lin, Wenjia Bai, Daniel Rueckert, Rossella Arcucci*

Main category: cs.CL

TL;DR: AlphaMed, a medical LLM, achieves state-of-the-art performance using reinforcement learning (RL) with rule-based rewards, outperforming models relying on supervised fine-tuning (SFT) and chain-of-thought (CoT) data.


<details>
  <summary>Details</summary>
Motivation: To enable interpretable decision-making and improve performance in clinical applications without costly SFT or CoT data.

Method: Uses minimalist rule-based RL on public multiple-choice QA datasets, avoiding SFT or distilled CoT data.

Result: Outperforms SFT+RL models on six medical QA benchmarks, even surpassing larger or closed-source models.

Conclusion: Dataset informativeness drives reasoning performance, and minimalist RL on informative QA data induces reasoning without CoT supervision, highlighting the need for more challenging benchmarks.

Abstract: Improving performance on complex tasks and enabling interpretable decision
making in large language models (LLMs), especially for clinical applications,
requires effective reasoning. Yet this remains challenging without supervised
fine-tuning (SFT) on costly chain-of-thought (CoT) data distilled from
closed-source models (e.g., GPT-4o). In this work, we present AlphaMed, the
first medical LLM to show that reasoning capability can emerge purely through
reinforcement learning (RL), using minimalist rule-based rewards on public
multiple-choice QA datasets, without relying on SFT or distilled CoT data.
AlphaMed achieves state-of-the-art results on six medical QA benchmarks,
outperforming models trained with conventional SFT+RL pipelines. On challenging
benchmarks (e.g., MedXpert), AlphaMed even surpasses larger or closed-source
models such as DeepSeek-V3-671B and Claude-3.5-Sonnet. To understand the
factors behind this success, we conduct a comprehensive data-centric analysis
guided by three questions: (i) Can minimalist rule-based RL incentivize
reasoning without distilled CoT supervision? (ii) How do dataset quantity and
diversity impact reasoning? (iii) How does question difficulty shape the
emergence and generalization of reasoning? Our findings show that dataset
informativeness is a key driver of reasoning performance, and that minimalist
RL on informative, multiple-choice QA data is effective at inducing reasoning
without CoT supervision. We also observe divergent trends across benchmarks,
underscoring limitations in current evaluation and the need for more
challenging, reasoning-oriented medical QA benchmarks.

</details>


### [157] [Not All Tokens Are What You Need In Thinking](https://arxiv.org/pdf/2505.17827)
*Hang Yuan, Bin Yu, Haotian Li, Shijun Yang, Christina Dan Wang, Zhou Yu, Xueyin Xu, Weizhen Qi, Kai Chen*

Main category: cs.CL

TL;DR: CTS is a token-level compression framework that reduces redundancy in reasoning models' chains of thought (CoT), improving efficiency without significant accuracy loss.


<details>
  <summary>Details</summary>
Motivation: Address inefficiencies in modern reasoning models like high latency, resource consumption, and verbose CoT with redundant tokens.

Method: Proposes Conditional Token Selection (CTS), which scores token importance and trains models on compressed CoT.

Result: CTS reduces reasoning tokens by 13.2% and improves accuracy by 9.1% on GPQA. Further compression (42% fewer tokens) only drops accuracy by 5%.

Conclusion: CTS effectively compresses CoT, highlighting redundancy in existing models and offering a balance between efficiency and performance.

Abstract: Modern reasoning models, such as OpenAI's o1 and DeepSeek-R1, exhibit
impressive problem-solving capabilities but suffer from critical
inefficiencies: high inference latency, excessive computational resource
consumption, and a tendency toward overthinking -- generating verbose chains of
thought (CoT) laden with redundant tokens that contribute minimally to the
final answer. To address these issues, we propose Conditional Token Selection
(CTS), a token-level compression framework with a flexible and variable
compression ratio that identifies and preserves only the most essential tokens
in CoT. CTS evaluates each token's contribution to deriving correct answers
using conditional importance scoring, then trains models on compressed CoT.
Extensive experiments demonstrate that CTS effectively compresses long CoT
while maintaining strong reasoning performance. Notably, on the GPQA benchmark,
Qwen2.5-14B-Instruct trained with CTS achieves a 9.1% accuracy improvement with
13.2% fewer reasoning tokens (13% training token reduction). Further reducing
training tokens by 42% incurs only a marginal 5% accuracy drop while yielding a
75.8% reduction in reasoning tokens, highlighting the prevalence of redundancy
in existing CoT.

</details>


### [158] [Stepwise Reasoning Checkpoint Analysis: A Test Time Scaling Method to Enhance LLMs' Reasoning](https://arxiv.org/pdf/2505.17829)
*Zezhong Wang, Xingshan Zeng, Weiwen Liu, Yufei Wang, Liangyou Li, Yasheng Wang, Lifeng Shang, Xin Jiang, Qun Liu, Kam-Fai Wong*

Main category: cs.CL

TL;DR: SRCA enhances LLM reasoning by introducing checkpoints between steps, reducing path homogenization and improving accuracy.


<details>
  <summary>Details</summary>
Motivation: Address limitations of existing TTS methods like path homogenization and inefficient use of intermediate results.

Method: Proposes SRCA with Answer-Clustered Search and Checkpoint Candidate Augmentation to maintain diversity and leverage intermediate results.

Result: SRCA improves reasoning accuracy over existing TTS methods on mathematical datasets.

Conclusion: SRCA is a promising framework for enhancing mathematical reasoning in LLMs.

Abstract: Mathematical reasoning through Chain-of-Thought (CoT) has emerged as a
powerful capability of Large Language Models (LLMs), which can be further
enhanced through Test-Time Scaling (TTS) methods like Beam Search and DVTS.
However, these methods, despite improving accuracy by allocating more
computational resources during inference, often suffer from path homogenization
and inefficient use of intermediate results. To address these limitations, we
propose Stepwise Reasoning Checkpoint Analysis (SRCA), a framework that
introduces checkpoints between reasoning steps. It incorporates two key
strategies: (1) Answer-Clustered Search, which groups reasoning paths by their
intermediate checkpoint answers to maintain diversity while ensuring quality,
and (2) Checkpoint Candidate Augmentation, which leverages all intermediate
answers for final decision-making. Our approach effectively reduces path
homogenization and creates a fault-tolerant mechanism by utilizing high-quality
intermediate results. Experimental results show that SRCA improves reasoning
accuracy compared to existing TTS methods across various mathematical datasets.

</details>


### [159] [Emerging categories in scientific explanations](https://arxiv.org/pdf/2505.17832)
*Giacomo Magnifico, Eduard Barbu*

Main category: cs.CL

TL;DR: The paper introduces a dataset of human-like explanations extracted from scientific literature, focusing on biotechnology and biophysics, with multi-class annotations and evaluated annotator consensus.


<details>
  <summary>Details</summary>
Motivation: There's a lack of large-scale datasets for human-like explanations in machine learning, which are crucial for understanding and knowledge dissemination.

Method: Extracted explanation sentences from scientific literature (e.g., PubMed's PMC Open Access), provided multi-class annotations, and evaluated annotator consensus.

Result: Created an openly-available dataset with 6-class and 3-class annotations; the 3-class notation achieved a 0.667 Krippendorf Alpha value.

Conclusion: The dataset addresses the gap in human-like explanation resources, supporting advancements in machine learning and AI.

Abstract: Clear and effective explanations are essential for human understanding and
knowledge dissemination. The scope of scientific research aiming to understand
the essence of explanations has recently expanded from the social sciences to
machine learning and artificial intelligence. Explanations for machine learning
decisions must be impactful and human-like, and there is a lack of large-scale
datasets focusing on human-like and human-generated explanations. This work
aims to provide such a dataset by: extracting sentences that indicate
explanations from scientific literature among various sources in the
biotechnology and biophysics topic domains (e.g. PubMed's PMC Open Access
subset); providing a multi-class notation derived inductively from the data;
evaluating annotator consensus on the emerging categories. The sentences are
organized in an openly-available dataset, with two different classifications
(6-class and 3-class category annotation), and the 3-class notation achieves a
0.667 Krippendorf Alpha value.

</details>


### [160] [Investigating Affect Mining Techniques for Annotation Sample Selection in the Creation of Finnish Affective Speech Corpus](https://arxiv.org/pdf/2505.17833)
*Kalle Lahtinen, Einari Vaaras, Liisa Mustanoja, Okko Räsänen*

Main category: cs.CL

TL;DR: The paper introduces the first spontaneous Finnish affective speech corpus, annotated for arousal and valence, and evaluates sampling methods for diversity.


<details>
  <summary>Details</summary>
Motivation: Existing Finnish speech corpora lack natural emotional expression, being either acted or context-specific. This work fills that gap.

Method: Annotated 12,000 utterances from Finnish speech corpora using affect mining (acoustic, cross-linguistic, and text features) and compared it to random sampling.

Result: The corpus is created, and affect mining outperforms random sampling in annotation diversity. Post-hoc analysis identifies optimal sampling choices.

Conclusion: The work provides a Finnish affective speech corpus and insights for sampling strategies in other languages or domains.

Abstract: Study of affect in speech requires suitable data, as emotional expression and
perception vary across languages. Until now, no corpus has existed for natural
expression of affect in spontaneous Finnish, existing data being acted or from
a very specific communicative setting. This paper presents the first such
corpus, created by annotating 12,000 utterances for emotional arousal and
valence, sampled from three large-scale Finnish speech corpora. To ensure
diverse affective expression, sample selection was conducted with an affect
mining approach combining acoustic, cross-linguistic speech emotion, and text
sentiment features. We compare this method to random sampling in terms of
annotation diversity, and conduct post-hoc analyses to identify sampling
choices that would have maximized the diversity. As an outcome, the work
introduces a spontaneous Finnish affective speech corpus and informs sampling
strategies for affective speech corpus creation in other languages or domains.

</details>


### [161] [Explaining Sources of Uncertainty in Automated Fact-Checking](https://arxiv.org/pdf/2505.17855)
*Jingyi Sun, Greta Warren, Irina Shklovski, Isabelle Augenstein*

Main category: cs.CL

TL;DR: CLUE is a framework for generating natural language explanations of model uncertainty by identifying text span conflicts/agreements and verbalizing them, improving faithfulness and human evaluation over baselines.


<details>
  <summary>Details</summary>
Motivation: Prior methods (numerical uncertainty or hedges) fail to explain uncertainty from conflicting evidence, limiting user trust and resolution.

Method: CLUE identifies claim-evidence/inter-evidence conflicts/agreements unsupervised and generates explanations via prompting and attention steering.

Result: CLUE outperforms baselines in faithfulness, consistency, and human evaluations (helpfulness, informativeness, redundancy, logical consistency).

Conclusion: CLUE is plug-and-play, enhances fact-checking, and generalizes to tasks requiring complex reasoning, linking uncertainty to evidence conflicts.

Abstract: Understanding sources of a model's uncertainty regarding its predictions is
crucial for effective human-AI collaboration. Prior work proposes using
numerical uncertainty or hedges ("I'm not sure, but ..."), which do not explain
uncertainty that arises from conflicting evidence, leaving users unable to
resolve disagreements or rely on the output. We introduce CLUE
(Conflict-and-Agreement-aware Language-model Uncertainty Explanations), the
first framework to generate natural language explanations of model uncertainty
by (i) identifying relationships between spans of text that expose
claim-evidence or inter-evidence conflicts and agreements that drive the
model's predictive uncertainty in an unsupervised way, and (ii) generating
explanations via prompting and attention steering that verbalize these critical
interactions. Across three language models and two fact-checking datasets, we
show that CLUE produces explanations that are more faithful to the model's
uncertainty and more consistent with fact-checking decisions than prompting for
uncertainty explanations without span-interaction guidance. Human evaluators
judge our explanations to be more helpful, more informative, less redundant,
and more logically consistent with the input than this baseline. CLUE requires
no fine-tuning or architectural changes, making it plug-and-play for any
white-box language model. By explicitly linking uncertainty to evidence
conflicts, it offers practical support for fact-checking and generalises
readily to other tasks that require reasoning over complex information.

</details>


### [162] [Just as Humans Need Vaccines, So Do Models: Model Immunization to Combat Falsehoods](https://arxiv.org/pdf/2505.17870)
*Shaina Raza, Rizwan Qureshi, Marcelo Lotif, Aman Chadha, Deval Pandya, Christos Emmanouilidis*

Main category: cs.CL

TL;DR: AI models can be 'immunized' against misinformation by fine-tuning on labeled falsehoods, reducing false outputs while maintaining accuracy.


<details>
  <summary>Details</summary>
Motivation: Generative AI models often reproduce false information from training data, necessitating a method to improve their factuality.

Method: Fine-tune models on small, labeled sets of falsehoods ('vaccine') periodically during training to enhance misinformation resistance.

Result: Immunized models generate significantly less misinformation compared to baselines.

Conclusion: Model immunization is a proactive approach to align AI with factuality, requiring ethical safeguards for safe use of false data.

Abstract: Generative AI models often learn and reproduce false information present in
their training corpora. This position paper argues that, analogous to
biological immunization, where controlled exposure to a weakened pathogen
builds immunity, AI models should be fine tuned on small, quarantined sets of
explicitly labeled falsehoods as a "vaccine" against misinformation. These
curated false examples are periodically injected during finetuning,
strengthening the model ability to recognize and reject misleading claims while
preserving accuracy on truthful inputs. An illustrative case study shows that
immunized models generate substantially less misinformation than baselines. To
our knowledge, this is the first training framework that treats fact checked
falsehoods themselves as a supervised vaccine, rather than relying on input
perturbations or generic human feedback signals, to harden models against
future misinformation. We also outline ethical safeguards and governance
controls to ensure the safe use of false data. Model immunization offers a
proactive paradigm for aligning AI systems with factuality.

</details>


### [163] [Language models can learn implicit multi-hop reasoning, but only if they have lots of training data](https://arxiv.org/pdf/2505.17923)
*Yuekun Yao, Yupei Du, Dawei Zhu, Michael Hahn, Alexander Koller*

Main category: cs.CL

TL;DR: Language models can learn implicit multi-hop reasoning, but training data grows exponentially and layers grow linearly with hop count. Curriculum learning helps but doesn't eliminate the issue.


<details>
  <summary>Details</summary>
Motivation: To explore if language models can perform multi-hop reasoning without explicit chain-of-thought steps.

Method: Train GPT2-style models on controlled k-hop reasoning datasets (k=2,3,4) and analyze data and depth requirements.

Result: Models learn implicit reasoning, but data scales exponentially and layers linearly with k. Curriculum learning mitigates data needs.

Conclusion: Implicit reasoning is feasible but requires significant resources, with depth and data scaling challenges.

Abstract: Implicit reasoning is the ability of a language model to solve multi-hop
reasoning tasks in a single forward pass, without chain of thought. We
investigate this capability using GPT2-style language models trained from
scratch on controlled $k$-hop reasoning datasets ($k = 2, 3, 4$). We show that
while such models can indeed learn implicit $k$-hop reasoning, the required
training data grows exponentially in $k$, and the required number of
transformer layers grows linearly in $k$. We offer a theoretical explanation
for why this depth growth is necessary. We further find that the data
requirement can be mitigated, but not eliminated, through curriculum learning.

</details>


### [164] [Counting Cycles with Deepseek](https://arxiv.org/pdf/2505.17964)
*Jiashun Jin, Tracy Ke, Bingcheng Sui, Zhenggang Wang*

Main category: cs.CL

TL;DR: AI solves the CEEF problem for cycle count statistics with human guidance, yielding new formulas in graph theory.


<details>
  <summary>Details</summary>
Motivation: AI struggles with advanced math; the CEEF problem lacks general solutions and is tedious for humans, making it ideal for AI assistance.

Method: Combines a novel approach with AI's coding skills, using step-by-step guidance and clear prompts.

Result: New formulas for general cases in graph theory, achieved with AI's help.

Conclusion: AI can solve complex math problems with human strategy and guidance, as demonstrated with the CEEF problem.

Abstract: Despite recent progress, AI still struggles on advanced mathematics. We
consider a difficult open problem: How to derive a Computationally Efficient
Equivalent Form (CEEF) for the cycle count statistic? The CEEF problem does not
have known general solutions, and requires delicate combinatorics and tedious
calculations. Such a task is hard to accomplish by humans but is an ideal
example where AI can be very helpful. We solve the problem by combining a novel
approach we propose and the powerful coding skills of AI. Our results use
delicate graph theory and contain new formulas for general cases that have not
been discovered before. We find that, while AI is unable to solve the problem
all by itself, it is able to solve it if we provide it with a clear strategy, a
step-by-step guidance and carefully written prompts. For simplicity, we focus
our study on DeepSeek-R1 but we also investigate other AI approaches.

</details>


### [165] [AVerImaTeC: A Dataset for Automatic Verification of Image-Text Claims with Evidence from the Web](https://arxiv.org/pdf/2505.17978)
*Rui Cao, Zifeng Ding, Zhijiang Guo, Michael Schlichtkrull, Andreas Vlachos*

Main category: cs.CL

TL;DR: AVerImaTeC is a dataset of 1,297 real-world image-text claims with QA-based evidence annotations, addressing limitations in existing datasets and improving fact-checking via normalization and sufficiency checks.


<details>
  <summary>Details</summary>
Motivation: To address the lack of real-world datasets with evidence annotations for verifying image-text claims, which hinders automated fact-checking.

Method: Introduces AVerImaTeC with QA pairs as evidence, uses claim normalization, temporally constrained evidence, and sufficiency checks.

Result: Achieves κ=0.742 for verdict consistency and 74.7% QA consistency; establishes baselines for evidence retrieval.

Conclusion: AVerImaTeC improves fact-checking by providing a robust dataset and evaluation method for image-text claim verification.

Abstract: Textual claims are often accompanied by images to enhance their credibility
and spread on social media, but this also raises concerns about the spread of
misinformation. Existing datasets for automated verification of image-text
claims remain limited, as they often consist of synthetic claims and lack
evidence annotations to capture the reasoning behind the verdict. In this work,
we introduce AVerImaTeC, a dataset consisting of 1,297 real-world image-text
claims. Each claim is annotated with question-answer (QA) pairs containing
evidence from the web, reflecting a decomposed reasoning regarding the verdict.
We mitigate common challenges in fact-checking datasets such as contextual
dependence, temporal leakage, and evidence insufficiency, via claim
normalization, temporally constrained evidence annotation, and a two-stage
sufficiency check. We assess the consistency of the annotation in AVerImaTeC
via inter-annotator studies, achieving a $\kappa=0.742$ on verdicts and
$74.7\%$ consistency on QA pairs. We also propose a novel evaluation method for
evidence retrieval and conduct extensive experiments to establish baselines for
verifying image-text claims using open-web evidence.

</details>


### [166] [Training with Pseudo-Code for Instruction Following](https://arxiv.org/pdf/2505.18011)
*Prince Kumar, Rudra Murthy, Riyaz Bhat, Danish Contractor*

Main category: cs.CL

TL;DR: Fine-tuning LLMs with pseudo-code alongside instructions improves their ability to follow instructions while retaining performance on other tasks.


<details>
  <summary>Details</summary>
Motivation: LLMs struggle with simple, unambiguous instructions involving compositions, and pseudo-code has shown promise in improving instruction-following.

Method: Propose fine-tuning LLMs with instruction-tuning data that includes instructions re-expressed in pseudo-code and the final response.

Result: Models show a 3-19% improvement on instruction-following benchmarks and up to 14% average gain across all tasks.

Conclusion: Incorporating pseudo-code in training enhances LLMs' instruction-following without compromising other capabilities.

Abstract: Despite the rapid progress in the capabilities of Large Language Models
(LLMs), they continue to have difficulty following relatively simple,
unambiguous instructions, especially when compositions are involved. In this
paper, we take inspiration from recent work that suggests that models may
follow instructions better when they are expressed in pseudo-code. However,
writing pseudo-code programs can be tedious and using few-shot demonstrations
to craft code representations for use in inference can be unnatural for
non-expert users of LLMs. To overcome these limitations, we propose fine-tuning
LLMs with instruction-tuning data that additionally includes instructions
re-expressed in pseudo-code along with the final response. We evaluate models
trained using our method on $11$ publicly available benchmarks comprising of
tasks related to instruction-following, mathematics, and common-sense
reasoning. We conduct rigorous experiments with $5$ different models and find
that not only do models follow instructions better when trained with
pseudo-code, they also retain their capabilities on the other tasks related to
mathematical and common sense reasoning. Specifically, we observe a relative
gain of $3$--$19$% on instruction-following benchmark, and an average gain of
upto 14% across all tasks.

</details>


### [167] [TRACE for Tracking the Emergence of Semantic Representations in Transformers](https://arxiv.org/pdf/2505.17998)
*Nura Aljaafari, Danilo S. Carvalho, André Freitas*

Main category: cs.CL

TL;DR: TRACE framework analyzes phase transitions in transformer models, revealing links between geometric shifts, linguistic accuracy, and abstraction patterns.


<details>
  <summary>Details</summary>
Motivation: To understand the mechanisms behind phase transitions in transformer models, particularly the shift from memorization to abstraction, which prior work overlooked in linguistic contexts.

Method: Introduces TRACE, a diagnostic framework combining geometric, informational, and linguistic signals, using ABSynth for synthetic corpora with controlled complexity and annotations.

Result: Phase transitions align with curvature collapse and dimension stabilization, coincide with syntactic/semantic accuracy, and persist across architectural variants.

Conclusion: TRACE advances understanding of linguistic abstraction emergence, aiding model interpretability, training efficiency, and compositional generalization in LM development.

Abstract: Modern transformer models exhibit phase transitions during training, distinct
shifts from memorisation to abstraction, but the mechanisms underlying these
transitions remain poorly understood. Prior work has often focused on endpoint
representations or isolated signals like curvature or mutual information,
typically in symbolic or arithmetic domains, overlooking the emergence of
linguistic structure. We introduce TRACE (Tracking Representation Abstraction
and Compositional Emergence), a diagnostic framework combining geometric,
informational, and linguistic signals to detect phase transitions in
Transformer-based LMs. TRACE leverages a frame-semantic data generation method,
ABSynth, that produces annotated synthetic corpora with controllable
complexity, lexical distributions, and structural entropy, while being fully
annotated with linguistic categories, enabling precise analysis of abstraction
emergence. Experiments reveal that (i) phase transitions align with clear
intersections between curvature collapse and dimension stabilisation; (ii)
these geometric shifts coincide with emerging syntactic and semantic accuracy;
(iii) abstraction patterns persist across architectural variants, with
components like feedforward networks affecting optimisation stability rather
than fundamentally altering trajectories. This work advances our understanding
of how linguistic abstractions emerge in LMs, offering insights into model
interpretability, training efficiency, and compositional generalisation that
could inform more principled approaches to LM development.

</details>


### [168] [Contrastive Distillation of Emotion Knowledge from LLMs for Zero-Shot Emotion Recognition](https://arxiv.org/pdf/2505.18040)
*Minxue Niu, Emily Mower Provost*

Main category: cs.CL

TL;DR: A contrastive distillation framework transfers emotional knowledge from GPT-4 to a compact model, enabling zero-shot emotion recognition without human annotations.


<details>
  <summary>Details</summary>
Motivation: To create adaptable Emotion Recognition (ER) systems that generalize beyond fixed label sets, leveraging LLMs' zero-shot capabilities while addressing their impracticality for edge devices.

Method: Uses GPT-4 to generate descriptive emotion annotations and aligns text samples with emotion descriptors in a shared embedding space via contrastive distillation.

Result: The distilled model performs well across diverse datasets and label spaces, matching GPT-4's zero-shot performance while being significantly smaller.

Conclusion: The framework successfully transfers LLM knowledge to compact models, enabling scalable and adaptable ER systems.

Abstract: The ability to handle various emotion labels without dedicated training is
crucial for building adaptable Emotion Recognition (ER) systems. Conventional
ER models rely on training using fixed label sets and struggle to generalize
beyond them. On the other hand, Large Language Models (LLMs) have shown strong
zero-shot ER performance across diverse label spaces, but their scale limits
their use on edge devices. In this work, we propose a contrastive distillation
framework that transfers rich emotional knowledge from LLMs into a compact
model without the use of human annotations. We use GPT-4 to generate
descriptive emotion annotations, offering rich supervision beyond fixed label
sets. By aligning text samples with emotion descriptors in a shared embedding
space, our method enables zero-shot prediction on different emotion classes,
granularity, and label schema. The distilled model is effective across multiple
datasets and label spaces, outperforming strong baselines of similar size and
approaching GPT-4's zero-shot performance, while being over 10,000 times
smaller.

</details>


### [169] [MathEDU: Towards Adaptive Feedback for Student Mathematical Problem-Solving](https://arxiv.org/pdf/2505.18056)
*Wei-Ling Hsu, Yu-Chien Tang, An-Zi Yen*

Main category: cs.CL

TL;DR: The paper explores using LLMs to provide adaptive feedback in online math education, introducing the MathEDU dataset. Results show promise in correctness identification but challenges in detailed feedback generation.


<details>
  <summary>Details</summary>
Motivation: Address the lack of immediate, personalized feedback in online math learning by leveraging LLMs.

Method: Evaluate LLMs using the MathEDU dataset in two scenarios: with prior answer histories and in a cold-start context.

Result: The fine-tuned model identifies correctness well but struggles with generating detailed pedagogical feedback.

Conclusion: LLMs show potential for adaptive feedback in math education but need improvement for detailed pedagogical support.

Abstract: Online learning enhances educational accessibility, offering students the
flexibility to learn anytime, anywhere. However, a key limitation is the lack
of immediate, personalized feedback, particularly in helping students correct
errors in math problem-solving. Several studies have investigated the
applications of large language models (LLMs) in educational contexts. In this
paper, we explore the capabilities of LLMs to assess students' math
problem-solving processes and provide adaptive feedback. The MathEDU dataset is
introduced, comprising authentic student solutions annotated with teacher
feedback. We evaluate the model's ability to support personalized learning in
two scenarios: one where the model has access to students' prior answer
histories, and another simulating a cold-start context. Experimental results
show that the fine-tuned model performs well in identifying correctness.
However, the model still faces challenges in generating detailed feedback for
pedagogical purposes.

</details>


### [170] [Extended Inductive Reasoning for Personalized Preference Inference from Behavioral Signals](https://arxiv.org/pdf/2505.18071)
*Jia-Nan Li, Jian Guan, Wei Wu, Rui Yan*

Main category: cs.CL

TL;DR: The paper explores inductive reasoning in LLMs, focusing on personalized preference inference, and introduces AlignXplore, a model improving performance by 11.05% over benchmarks.


<details>
  <summary>Details</summary>
Motivation: Current LLMs excel in deductive reasoning but lack exploration in inductive reasoning, especially for inferring user preferences from implicit signals.

Method: AlignXplore combines cold-start training with synthetic data and online reinforcement learning for systematic preference inference.

Result: AlignXplore improves performance by 11.05% on benchmarks, showing strong generalization across input formats and models.

Conclusion: The study highlights effective preference inference strategies and observes human-like inductive reasoning patterns in LLMs.

Abstract: Large language models (LLMs) have demonstrated significant success in complex
reasoning tasks such as math and coding. In contrast to these tasks where
deductive reasoning predominates, inductive reasoning\textemdash the ability to
derive general rules from incomplete evidence, remains underexplored. This
paper investigates extended inductive reasoning in LLMs through the lens of
personalized preference inference, a critical challenge in LLM alignment where
current approaches struggle to capture diverse user preferences. The task
demands strong inductive reasoning capabilities as user preferences are
typically embedded implicitly across various interaction forms, requiring
models to synthesize consistent preference patterns from scattered signals. We
propose \textsc{AlignXplore}, a model that leverages extended reasoning chains
to enable systematic preference inference from behavioral signals in users'
interaction histories. We develop \textsc{AlignXplore} by combining cold-start
training based on synthetic data with subsequent online reinforcement learning.
Through extensive experiments, we demonstrate that \textsc{AlignXplore}
achieves substantial improvements over the backbone model by an average of
11.05\% on in-domain and out-of-domain benchmarks, while maintaining strong
generalization ability across different input formats and downstream models.
Further analyses establish best practices for preference inference learning
through systematic comparison of reward modeling strategies, while revealing
the emergence of human-like inductive reasoning patterns during training.

</details>


### [171] [QwenLong-CPRS: Towards $\infty$-LLMs with Dynamic Context Optimization](https://arxiv.org/pdf/2505.18092)
*Weizhou Shen, Chenliang Li, Fanqi Wan, Shengyi Liao, Shaopeng Lai, Bo Zhang, Yingcheng Shi, Yuning Wu, Gang Fu, Zhansheng Li, Bin Yang, Ji Zhang, Fei Huang, Jingren Zhou, Ming Yan*

Main category: cs.CL

TL;DR: QwenLong-CPRS is a context compression framework for long-context optimization in LLMs, improving efficiency and performance through dynamic context optimization and four key innovations.


<details>
  <summary>Details</summary>
Motivation: Addresses high computation overhead and performance degradation in LLMs during long sequence processing.

Method: Uses natural language-guided dynamic optimization, bidirectional reasoning layers, token critic mechanisms, and window-parallel inference.

Result: Outperforms other methods in accuracy and efficiency, integrates with major LLMs, and achieves significant context compression and performance gains.

Conclusion: QwenLong-CPRS sets new SOTA performance, demonstrating its effectiveness for long-context tasks.

Abstract: This technical report presents QwenLong-CPRS, a context compression framework
designed for explicit long-context optimization, addressing prohibitive
computation overhead during the prefill stage and the "lost in the middle"
performance degradation of large language models (LLMs) during long sequence
processing. Implemented through a novel dynamic context optimization mechanism,
QwenLong-CPRS enables multi-granularity context compression guided by natural
language instructions, achieving both efficiency gains and improved
performance.
  Evolved from the Qwen architecture series, QwenLong-CPRS introduces four key
innovations: (1) Natural language-guided dynamic optimization, (2)
Bidirectional reasoning layers for enhanced boundary awareness, (3) Token
critic mechanisms with language modeling heads, and (4) Window-parallel
inference.
  Comprehensive evaluations across five benchmarks (4K-2M word contexts)
demonstrate QwenLong-CPRS's threefold effectiveness: (1) Consistent superiority
over other context management methods like RAG and sparse attention in both
accuracy and efficiency. (2) Architecture-agnostic integration with all
flagship LLMs, including GPT-4o, Gemini2.0-pro, Claude3.7-sonnet, DeepSeek-v3,
and Qwen2.5-max, achieves 21.59$\times$ context compression alongside
19.15-point average performance gains; (3) Deployed with Qwen2.5-32B-Instruct,
QwenLong-CPRS surpasses leading proprietary LLMs by 4.85 and 10.88 points on
Ruler-128K and InfiniteBench, establishing new SOTA performance.

</details>


### [172] [Planning without Search: Refining Frontier LLMs with Offline Goal-Conditioned RL](https://arxiv.org/pdf/2505.18098)
*Joey Hong, Anca Dragan, Sergey Levine*

Main category: cs.CL

TL;DR: Proposes goal-conditioned value functions to guide LLM reasoning, improving scalability and performance in complex tasks over RL fine-tuning and prompting.


<details>
  <summary>Details</summary>
Motivation: Addresses limitations of RL fine-tuning (high costs, lack of APIs) and prompting in enabling long-horizon reasoning for LLMs in interactive tasks.

Method: Uses goal-conditioned value functions to predict task outcomes, trained over reasoning steps for lightweight decision-making in multi-turn interactions.

Result: Demonstrates superior performance in tasks like tool use, social deduction, and dialogue, while maintaining efficiency and scalability.

Conclusion: The approach effectively enhances LLM reasoning for complex interactive tasks without the drawbacks of RL fine-tuning or prompting.

Abstract: Large language models (LLMs) excel in tasks like question answering and
dialogue, but complex tasks requiring interaction, such as negotiation and
persuasion, require additional long-horizon reasoning and planning.
Reinforcement learning (RL) fine-tuning can enable such planning in principle,
but suffers from drawbacks that hinder scalability. In particular, multi-turn
RL training incurs high memory and computational costs, which are exacerbated
when training LLMs as policies. Furthermore, the largest LLMs do not expose the
APIs necessary to be trained in such manner. As a result, modern methods to
improve the reasoning of LLMs rely on sophisticated prompting mechanisms rather
than RL fine-tuning. To remedy this, we propose a novel approach that uses
goal-conditioned value functions to guide the reasoning of LLM agents, that
scales even to large API-based models. These value functions predict how a task
will unfold given an action, allowing the LLM agent to evaluate multiple
possible outcomes, both positive and negative, to plan effectively. In
addition, these value functions are trained over reasoning steps rather than
full actions, to be a concise and light-weight module that facilitates
decision-making in multi-turn interactions. We validate our method on tasks
requiring interaction, including tool use, social deduction, and dialogue,
demonstrating superior performance over both RL fine-tuning and prompting
methods while maintaining efficiency and scalability.

</details>


### [173] [ManuSearch: Democratizing Deep Search in Large Language Models with a Transparent and Open Multi-Agent Framework](https://arxiv.org/pdf/2505.18105)
*Lisheng Huang, Yichen Liu, Jinhao Jiang, Rongxiang Zhang, Jiahao Yan, Junyi Li, Wayne Xin Zhao*

Main category: cs.CL

TL;DR: ManuSearch is a transparent, modular multi-agent framework for democratizing deep search in LLMs, outperforming prior systems.


<details>
  <summary>Details</summary>
Motivation: Proprietary LLMs lack transparency; ManuSearch aims to democratize deep search with an open, modular approach.

Method: Decomposes search into three agents: solution planning, Internet search, and webpage reading, evaluated on the ORION benchmark.

Result: ManuSearch outperforms open-source baselines and closed-source systems.

Conclusion: The framework enables reproducible, extensible research in open deep search systems; data and code are released.

Abstract: Recent advances in web-augmented large language models (LLMs) have exhibited
strong performance in complex reasoning tasks, yet these capabilities are
mostly locked in proprietary systems with opaque architectures. In this work,
we propose \textbf{ManuSearch}, a transparent and modular multi-agent framework
designed to democratize deep search for LLMs. ManuSearch decomposes the search
and reasoning process into three collaborative agents: (1) a solution planning
agent that iteratively formulates sub-queries, (2) an Internet search agent
that retrieves relevant documents via real-time web search, and (3) a
structured webpage reading agent that extracts key evidence from raw web
content. To rigorously evaluate deep reasoning abilities, we introduce
\textbf{ORION}, a challenging benchmark focused on open-web reasoning over
long-tail entities, covering both English and Chinese. Experimental results
show that ManuSearch substantially outperforms prior open-source baselines and
even surpasses leading closed-source systems. Our work paves the way for
reproducible, extensible research in open deep search systems. We release the
data and code in https://github.com/RUCAIBox/ManuSearch

</details>


### [174] [Watch and Listen: Understanding Audio-Visual-Speech Moments with Multimodal LLM](https://arxiv.org/pdf/2505.18110)
*Zinuo Li, Xian Zhang, Yongxin Guo, Mohammed Bennamoun, Farid Boussaid, Girish Dwivedi, Luqi Gong, Qiuhong Ke*

Main category: cs.CL

TL;DR: TriSense is a triple-modality model integrating visual, audio, and speech for holistic video understanding, supported by the TriSense-2M dataset.


<details>
  <summary>Details</summary>
Motivation: Existing models struggle with fusing audio and visual cues for comprehensive video understanding.

Method: TriSense uses a Query-Based Connector to adaptively reweight modalities and is trained on the TriSense-2M dataset.

Result: TriSense shows robust performance across benchmarks, even with modality dropout.

Conclusion: TriSense advances multimodal video analysis and will release code and dataset publicly.

Abstract: Humans naturally understand moments in a video by integrating visual and
auditory cues. For example, localizing a scene in the video like "A scientist
passionately speaks on wildlife conservation as dramatic orchestral music
plays, with the audience nodding and applauding" requires simultaneous
processing of visual, audio, and speech signals. However, existing models often
struggle to effectively fuse and interpret audio information, limiting their
capacity for comprehensive video temporal understanding. To address this, we
present TriSense, a triple-modality large language model designed for holistic
video temporal understanding through the integration of visual, audio, and
speech modalities. Central to TriSense is a Query-Based Connector that
adaptively reweights modality contributions based on the input query, enabling
robust performance under modality dropout and allowing flexible combinations of
available inputs. To support TriSense's multimodal capabilities, we introduce
TriSense-2M, a high-quality dataset of over 2 million curated samples generated
via an automated pipeline powered by fine-tuned LLMs. TriSense-2M includes
long-form videos and diverse modality combinations, facilitating broad
generalization. Extensive experiments across multiple benchmarks demonstrate
the effectiveness of TriSense and its potential to advance multimodal video
analysis. Code and dataset will be publicly released.

</details>


### [175] [UNJOIN: Enhancing Multi-Table Text-to-SQL Generation via Schema Simplification](https://arxiv.org/pdf/2505.18122)
*Poojah Ganesan, Rajat Aayush Jha, Dan Roth, Vivek Gupta*

Main category: cs.CL

TL;DR: UNJOIN is a two-stage framework improving Text-to-SQL for multi-table databases by decoupling schema retrieval from SQL generation, achieving state-of-the-art performance without data access or fine-tuning.


<details>
  <summary>Details</summary>
Motivation: Challenges in multi-table Text-to-SQL include complex schemas, accurate JOINs/UNIONs, and schema generalization. Existing methods struggle with these issues.

Method: UNJOIN merges column names into a single-table representation for retrieval, then generates SQL and maps it back to the original schema.

Result: UNJOIN matches or exceeds state-of-the-art baselines on SPIDER and BIRD datasets.

Conclusion: UNJOIN is scalable and adaptable, requiring only schema information, making it effective for diverse databases.

Abstract: Recent advances in large language models (LLMs) have greatly improved
Text-to-SQL performance for single-table queries. But, it remains challenging
in multi-table databases due to complex schema and relational operations.
Existing methods often struggle with retrieving the right tables and columns,
generating accurate JOINs and UNIONs, and generalizing across diverse schemas.
To address these issues, we introduce UNJOIN, a two-stage framework that
decouples the retrieval of schema elements from SQL logic generation. In the
first stage, we merge the column names of all tables in the database into a
single-table representation by prefixing each column with its table name. This
allows the model to focus purely on accurate retrieval without being distracted
by the need to write complex SQL logic. In the second stage, the SQL query is
generated on this simplified schema and mapped back to the original schema by
reconstructing JOINs, UNIONs, and relational logic. Evaluations on SPIDER and
BIRD datasets show that UNJOIN matches or exceeds the state-of-the-art
baselines. UNJOIN uses only schema information, which does not require data
access or fine-tuning, making it scalable and adaptable across databases.

</details>


### [176] [Frankentext: Stitching random text fragments into long-form narratives](https://arxiv.org/pdf/2505.18128)
*Chau Minh Pham, Jenna Russell, Dzung Pham, Mohit Iyyer*

Main category: cs.CL

TL;DR: Frankentexts are long-form narratives by LLMs with 90% verbatim human text, testing controllable generation. Gemini-2.5-Pro excels, but detectors struggle to identify them.


<details>
  <summary>Details</summary>
Motivation: To explore controllable generation under extreme constraints and evaluate AI text detectability.

Method: Generate drafts by combining human passages, then iteratively revise while maintaining a copy ratio.

Result: 81% coherent, 100% prompt-relevant; 59% misclassified as human-written. Human annotators spot inconsistencies.

Conclusion: Frankentexts challenge AI detectors, aid mixed authorship detection, and study human-AI co-writing.

Abstract: We introduce Frankentexts, a new type of long-form narratives produced by
LLMs under the extreme constraint that most tokens (e.g., 90%) must be copied
verbatim from human writings. This task presents a challenging test of
controllable generation, requiring models to satisfy a writing prompt,
integrate disparate text fragments, and still produce a coherent narrative. To
generate Frankentexts, we instruct the model to produce a draft by selecting
and combining human-written passages, then iteratively revise the draft while
maintaining a user-specified copy ratio. We evaluate the resulting Frankentexts
along three axes: writing quality, instruction adherence, and detectability.
Gemini-2.5-Pro performs surprisingly well on this task: 81% of its Frankentexts
are coherent and 100% relevant to the prompt. Notably, up to 59% of these
outputs are misclassified as human-written by detectors like Pangram, revealing
limitations in AI text detectors. Human annotators can sometimes identify
Frankentexts through their abrupt tone shifts and inconsistent grammar between
segments, especially in longer generations. Beyond presenting a challenging
generation task, Frankentexts invite discussion on building effective detectors
for this new grey zone of authorship, provide training data for mixed
authorship detection, and serve as a sandbox for studying human-AI co-writing
processes.

</details>


### [177] [Graph-Linguistic Fusion: Using Language Models for Wikidata Vandalism Detection](https://arxiv.org/pdf/2505.18136)
*Mykola Trokhymovych, Lydia Pintscher, Ricardo Baeza-Yates, Diego Saez-Trumper*

Main category: cs.CL

TL;DR: A new vandalism detection system for Wikidata uses Graph2Text to unify structured and textual edits, outperforming current methods and releasing open-source code and datasets.


<details>
  <summary>Details</summary>
Motivation: Wikidata's complexity and multilingual nature require a unified approach to detect vandalism across both structured and textual edits.

Method: The system converts all edits into a single space using Graph2Text, enabling evaluation with a multilingual language model.

Result: The solution outperforms the current production system and includes open-source code and a dataset for further research.

Conclusion: The unified approach improves coverage and simplifies maintenance, advancing vandalism detection in Wikidata.

Abstract: We introduce a next-generation vandalism detection system for Wikidata, one
of the largest open-source structured knowledge bases on the Web. Wikidata is
highly complex: its items incorporate an ever-expanding universe of factual
triples and multilingual texts. While edits can alter both structured and
textual content, our approach converts all edits into a single space using a
method we call Graph2Text. This allows for evaluating all content changes for
potential vandalism using a single multilingual language model. This unified
approach improves coverage and simplifies maintenance. Experiments demonstrate
that our solution outperforms the current production system. Additionally, we
are releasing the code under an open license along with a large dataset of
various human-generated knowledge alterations, enabling further research.

</details>


### [178] [Lost in the Haystack: Smaller Needles are More Difficult for LLMs to Find](https://arxiv.org/pdf/2505.18148)
*Owen Bianchi, Mathew J. Koretsky, Maya Willey, Chelsea X. Alvarado, Tanay Nayak, Adi Asija, Nicole Kuznetsov, Mike A. Nalls, Faraz Faghri, Daniel Khashabi*

Main category: cs.CL

TL;DR: LLMs struggle with needle-in-a-haystack tasks, especially when the relevant context (gold context) is shorter, degrading performance and increasing positional bias.


<details>
  <summary>Details</summary>
Motivation: To investigate how gold context length impacts LLM performance in long-context question answering, addressing a gap in prior research.

Method: Systematic study of gold context length variations across three domains and seven LLMs.

Result: Shorter gold contexts sharply degrade LLM performance and amplify positional sensitivity, consistent across domains and models.

Conclusion: Designing robust LLM-driven systems requires addressing challenges posed by varying gold context lengths.

Abstract: Large language models (LLMs) face significant challenges with
needle-in-a-haystack tasks, where relevant information ("the needle") must be
drawn from a large pool of irrelevant context ("the haystack"). Previous
studies have highlighted positional bias and distractor quantity as critical
factors affecting model performance, yet the influence of gold context size has
received little attention. We address this gap by systematically studying how
variations in gold context length impact LLM performance on long-context
question answering tasks. Our experiments reveal that LLM performance drops
sharply when the gold context is shorter, i.e., smaller gold contexts
consistently degrade model performance and amplify positional sensitivity,
posing a major challenge for agentic systems that must integrate scattered,
fine-grained information of varying lengths. This pattern holds across three
diverse domains (general knowledge, biomedical reasoning, and mathematical
reasoning) and seven state-of-the-art LLMs of various sizes and architectures.
Our work provides clear insights to guide the design of robust, context-aware
LLM-driven systems.

</details>


### [179] [First Finish Search: Efficient Test-Time Scaling in Large Language Models](https://arxiv.org/pdf/2505.18149)
*Aradhye Agarwal, Ayan Sengupta, Tanmoy Chakraborty*

Main category: cs.CL

TL;DR: First Finish Search (FFS) is a training-free parallel decoding strategy that improves reasoning in large language models by stopping at the shortest trace, achieving significant accuracy gains.


<details>
  <summary>Details</summary>
Motivation: Existing test-time scaling methods increase token usage and latency; FFS leverages the observation that shorter traces are more likely correct.

Method: FFS launches multiple independent samples and returns the first completed result, evaluated on four reasoning models and datasets.

Result: FFS improves DeepSeek-R1's accuracy by 15% on AIME datasets, nearly matching OpenAI's performance.

Conclusion: FFS demonstrates the effectiveness of simple TTS strategies, highlighting untapped potential in straightforward inference approaches.

Abstract: Test-time scaling (TTS), which involves dynamic allocation of compute during
inference, offers a promising way to improve reasoning in large language
models. While existing TTS methods work well, they often rely on long decoding
paths or require a large number of samples to be generated, increasing the
token usage and inference latency. We observe the surprising fact that for
reasoning tasks, shorter traces are much more likely to be correct than longer
ones. Motivated by this, we introduce First Finish Search (FFS), a
training-free parallel decoding strategy that launches $n$ independent samples
and returns as soon as any one completes. We evaluate FFS alongside simple
decoding, beam search, majority voting, and budget forcing on four reasoning
models (DeepSeek-R1, R1-Distill-Qwen-32B, QwQ-32B and Phi-4-Reasoning-Plus) and
across four datasets (AIME24, AIME25-I, AIME25-II and GPQA Diamond). With
DeepSeek-R1, FFS achieves $82.23\%$ accuracy on the AIME datasets, a $15\%$
improvement over DeepSeek-R1's standalone accuracy, nearly matching OpenAI's
o4-mini performance. Our theoretical analysis explains why stopping at the
shortest trace is likely to yield a correct answer and identifies the
conditions under which early stopping may be suboptimal. The elegance and
simplicity of FFS demonstrate that straightforward TTS strategies can perform
remarkably well, revealing the untapped potential of simple approaches at
inference time.

</details>


### [180] [Fann or Flop: A Multigenre, Multiera Benchmark for Arabic Poetry Understanding in LLMs](https://arxiv.org/pdf/2505.18152)
*Wafa Alghallabi, Ritesh Thawkar, Sara Ghaboura, Ketan More, Omkar Thawakar, Hisham Cholakkal, Salman Khan, Rao Muhammad Anwer*

Main category: cs.CL

TL;DR: The paper introduces `Fann or Flop`, a benchmark to evaluate LLMs' comprehension of Arabic poetry across historical eras and genres, revealing their struggles despite strong performance on standard Arabic tasks.


<details>
  <summary>Details</summary>
Motivation: Arabic poetry's complexity and cultural depth make it a strong test for LLMs' understanding of classical Arabic, requiring deeper reasoning and cultural sensitivity.

Method: The benchmark includes a curated corpus of poems with explanations, assessing semantic understanding, metaphor interpretation, prosodic awareness, and cultural context.

Result: Most state-of-the-art LLMs struggle with poetic comprehension despite excelling in standard Arabic benchmarks.

Conclusion: `Fann or Flop` is released as an open-source resource to advance Arabic language models, highlighting the need for improved poetic and cultural understanding in LLMs.

Abstract: Arabic poetry stands as one of the most sophisticated and culturally embedded
forms of expression in the Arabic language, known for its layered meanings,
stylistic diversity, and deep historical continuity. Although large language
models (LLMs) have demonstrated strong performance across languages and tasks,
their ability to understand Arabic poetry remains largely unexplored. In this
work, we introduce `Fann or Flop`, the first benchmark designed to assess the
comprehension of Arabic poetry by LLMs in twelve historical eras, covering 21
core poetic genres and a variety of metrical forms, from classical structures
to contemporary free verse. The benchmark comprises a curated corpus of poems
with explanations that assess semantic understanding, metaphor interpretation,
prosodic awareness, and cultural context. We argue that poetic comprehension
offers a strong indicator for testing how good the LLM is in understanding
classical Arabic through the Arabic poetry. Unlike surface-level tasks, this
domain demands deeper interpretive reasoning and cultural sensitivity. Our
evaluation of state-of-the-art LLMs shows that most models struggle with poetic
understanding despite strong results on standard Arabic benchmarks. We release
`Fann or Flop` along with the evaluation suite as an open-source resource to
enable rigorous evaluation and advancement for Arabic language models. Code is
available at: https://github.com/mbzuai-oryx/FannOrFlop.

</details>


### [181] [The Staircase of Ethics: Probing LLM Value Priorities through Multi-Step Induction to Complex Moral Dilemmas](https://arxiv.org/pdf/2505.18154)
*Ya Wu, Qiang Sheng, Danding Wang, Guang Yang, Yifan Sun, Zhengjia Wang, Yuyan Bu, Juan Cao*

Main category: cs.CL

TL;DR: The paper introduces Multi-step Moral Dilemmas (MMDs) to evaluate LLMs' evolving moral reasoning, revealing dynamic shifts in value preferences as dilemmas escalate.


<details>
  <summary>Details</summary>
Motivation: Existing assessments of LLMs' moral reasoning are limited to single-step evaluations, missing how models adapt to evolving ethical challenges.

Method: The authors create MMDs, a dataset of 3,302 five-stage dilemmas, to dynamically analyze LLMs' moral reasoning. Nine widely used LLMs are evaluated.

Result: LLMs' value preferences shift significantly as dilemmas progress, with care often prioritized but sometimes superseded by fairness.

Conclusion: The findings advocate for dynamic, context-aware evaluation paradigms to align LLM development with human values.

Abstract: Ethical decision-making is a critical aspect of human judgment, and the
growing use of LLMs in decision-support systems necessitates a rigorous
evaluation of their moral reasoning capabilities. However, existing assessments
primarily rely on single-step evaluations, failing to capture how models adapt
to evolving ethical challenges. Addressing this gap, we introduce the
Multi-step Moral Dilemmas (MMDs), the first dataset specifically constructed to
evaluate the evolving moral judgments of LLMs across 3,302 five-stage dilemmas.
This framework enables a fine-grained, dynamic analysis of how LLMs adjust
their moral reasoning across escalating dilemmas. Our evaluation of nine widely
used LLMs reveals that their value preferences shift significantly as dilemmas
progress, indicating that models recalibrate moral judgments based on scenario
complexity. Furthermore, pairwise value comparisons demonstrate that while LLMs
often prioritize the value of care, this value can sometimes be superseded by
fairness in certain contexts, highlighting the dynamic and context-dependent
nature of LLM ethical reasoning. Our findings call for a shift toward dynamic,
context-aware evaluation paradigms, paving the way for more human-aligned and
value-sensitive development of LLMs.

</details>


### [182] [QFT: Quantized Full-parameter Tuning of LLMs with Affordable Resources](https://arxiv.org/pdf/2310.07147)
*Zhikai Li, Xiaoxuan Liu, Banghua Zhu, Zhen Dong, Qingyi Gu, Kurt Keutzer*

Main category: cs.CL

TL;DR: QFT enables full-parameter fine-tuning of LLMs on affordable GPUs by quantizing training states to INT8, reducing memory usage while maintaining performance.


<details>
  <summary>Details</summary>
Motivation: Full-parameter fine-tuning of LLMs is resource-intensive, requiring expensive GPUs. QFT addresses this by reducing memory costs without compromising performance.

Method: QFT quantizes weights, gradients, and optimizer states to INT8, uses the Lion optimizer for robustness, and employs a hybrid feature quantizer for accurate weight updates. A stack-based gradient flow scheme supports backpropagation in integer context.

Result: QFT reduces model state memory to 21% of standard solutions, enabling fine-tuning of a LLaMA-7B model with <30GB memory on a single A6000 GPU.

Conclusion: QFT provides a practical solution for full-parameter fine-tuning of LLMs on affordable hardware, achieving comparable performance with significantly reduced memory usage.

Abstract: Large Language Models (LLMs) have showcased remarkable impacts across a wide
spectrum of natural language processing tasks. Fine-tuning these pretrained
models on downstream datasets provides further significant performance gains;
however, this process typically requires a large number of expensive, high-end
GPUs. Although there have been efforts focused on parameter-efficient
fine-tuning, they cannot fully unlock the powerful potential of full-parameter
fine-tuning. In this paper, we propose QFT, a Quantized Full-parameter Tuning
framework for LLMs that quantizes and stores all training states, including
weights, gradients, and optimizer states, in INT8 format to reduce training
memory, thereby enabling full-parameter fine-tuning on existing GPUs at an
affordable cost. To ensure training performance, we make two key efforts: i)
for quantized gradients and optimizer states, we theoretically prove that the
Lion optimizer, with its property of consistent update magnitudes, is highly
robust to quantization; ii) and for quantized weights, we employ the hybrid
feature quantizer, which identifies and protects a small subset of sparse
critical features while quantizing the remaining dense features, thus ensuring
accurate weight updates without FP32 backups. Moreover, to support
backpropagation in the integer context, we develop a stack-based gradient flow
scheme with O(1) complexity, forming a unified integer training pipeline. As a
result, QFT reduces the model state memory to 21% of the standard solution
while achieving comparable performance, e.g., tuning a LLaMA-7B model requires
only <30GB of memory, making it feasible on a single A6000 GPU.

</details>


### [183] [Offset Unlearning for Large Language Models](https://arxiv.org/pdf/2404.11045)
*James Y. Huang, Wenxuan Zhou, Fei Wang, Fred Morstatter, Sheng Zhang, Hoifung Poon, Muhao Chen*

Main category: cs.CL

TL;DR: Delta-Unlearning is a framework for black-box LLMs to unlearn sensitive data without accessing internal weights or retaining problematic data.


<details>
  <summary>Details</summary>
Motivation: Address ethical and legal concerns from LLMs memorizing sensitive data like copyrighted or private content.

Method: Uses logit offsets from smaller models to unlearn target data without modifying the black-box LLM.

Result: Effectively unlearns target data while maintaining performance on general tasks.

Conclusion: Delta-Unlearning is a versatile solution for adapting unlearning algorithms to black-box LLMs.

Abstract: Despite the strong capabilities of Large Language Models (LLMs) to acquire
knowledge from their training corpora, the memorization of sensitive
information in the corpora such as copyrighted, biased, and private content has
led to ethical and legal concerns. In response to these challenges, unlearning
has emerged as a potential remedy for LLMs affected by problematic training
data. However, previous unlearning techniques are either not applicable to
black-box LLMs due to required access to model internal weights, or violate
data protection principles by retaining sensitive data for inference-time
correction. We propose {\delta}-Unlearning, an offset unlearning framework for
black-box LLMs. Instead of tuning the black-box LLM itself, {\delta}-Unlearning
learns the logit offset needed for unlearning by contrasting the logits from a
pair of smaller models. Experiments demonstrate that {\delta}- Unlearning can
effectively unlearn target data while maintaining similar or even stronger
performance on general out-of-forget-scope tasks. {\delta}-Unlearning also
effectively incorporates different unlearning algorithms, making our approach a
versatile solution to adapting various existing unlearning algorithms to
black-box LLMs.

</details>


### [184] [Temporal Dynamics of Emotion and Cognition in Human Translation: Integrating the Task Segment Framework and the HOF Taxonomy](https://arxiv.org/pdf/2405.03111)
*Michael Carl*

Main category: cs.CL

TL;DR: A generative model of human translation processes is proposed, linking behavioral data (keystrokes, gaze) to three concurrent mental layers: automated, cognitive, and affective.


<details>
  <summary>Details</summary>
Motivation: To bridge empirical translation process data with theoretical frameworks, enhancing understanding of the human translating mind.

Method: Analyzes keystroke and gaze data from the CRITT TPR-DB to map behavioral patterns to three mental processing layers.

Result: Demonstrates how behavioral data reflects concurrent mental processes in translation, aligning with dual-process theories and ideosomatic theory.

Conclusion: The model opens new theoretical avenues for Cognitive Translation Studies, grounded in empirical evidence.

Abstract: The article develops a generative model of the human translating mind,
grounded in empirical translation process data. It posits that three embedded
processing layers unfold concurrently in the human mind, and their traces are
detectable in behavioral data: sequences of routinized/automated processes are
observable in fluent translation production, cognitive/reflective thoughts lead
to longer keystroke pauses, while affective/emotional states may be identified
through characteristic typing and gazing patterns. Utilizing data from the
CRITT Translation Process Research Database (TPR-DB), the article illustrates
how the temporal structure of keystroke and gaze data can be related to the
three assumed hidden mental processing strata. The article relates this
embedded generative model to various theoretical frameworks, dual-process
theories and Robinson's (2023) ideosomatic theory of translation, opening
exciting new theoretical horizons for Cognitive Translation Studies, grounded
in empirical data and evaluation.

</details>


### [185] [ActiveLLM: Large Language Model-based Active Learning for Textual Few-Shot Scenarios](https://arxiv.org/pdf/2405.10808)
*Markus Bayer, Justin Lutz, Christian Reuter*

Main category: cs.CL

TL;DR: ActiveLLM leverages LLMs like GPT-4 to improve active learning in few-shot scenarios, outperforming traditional methods and aiding cold-start issues.


<details>
  <summary>Details</summary>
Motivation: Address the cold-start problem in active learning and enhance performance in few-shot scenarios.

Method: Uses Large Language Models (e.g., GPT-4) for instance selection in active learning.

Result: Significantly boosts BERT classifier performance in few-shot settings and aids other active learning strategies.

Conclusion: ActiveLLM is a versatile solution for improving model performance across learning setups.

Abstract: Active learning is designed to minimize annotation efforts by prioritizing
instances that most enhance learning. However, many active learning strategies
struggle with a `cold-start' problem, needing substantial initial data to be
effective. This limitation reduces their utility in the increasingly relevant
few-shot scenarios, where the instance selection has a substantial impact. To
address this, we introduce ActiveLLM, a novel active learning approach that
leverages Large Language Models such as GPT-4, o1, Llama 3, or Mistral Large
for selecting instances. We demonstrate that ActiveLLM significantly enhances
the classification performance of BERT classifiers in few-shot scenarios,
outperforming traditional active learning methods as well as improving the
few-shot learning methods ADAPET, PERFECT, and SetFit. Additionally, ActiveLLM
can be extended to non-few-shot scenarios, allowing for iterative selections.
In this way, ActiveLLM can even help other active learning strategies to
overcome their cold-start problem. Our results suggest that ActiveLLM offers a
promising solution for improving model performance across various learning
setups.

</details>


### [186] [Mitigate Position Bias in Large Language Models via Scaling a Single Dimension](https://arxiv.org/pdf/2406.02536)
*Yijiong Yu, Huiqiang Jiang, Xufang Luo, Qianhui Wu, Chin-Yew Lin, Dongsheng Li, Yuqing Yang, Yongfeng Huang, Lili Qiu*

Main category: cs.CL

TL;DR: The paper addresses position bias in LLMs, identifies its micro-level causes (attention weights and causal attention masks), and proposes a method to mitigate it by scaling positional hidden states, improving performance by up to 15.2%.


<details>
  <summary>Details</summary>
Motivation: LLMs exhibit position bias ("lost in the middle"), especially in long-context scenarios, affecting accuracy based on where key information is placed in prompts.

Method: The study explores micro-level causes of bias (attention weights, causal attention masks) and introduces a method to scale positional hidden states.

Result: Experiments on tasks like Multi-document QA and KV retrieval show the method improves performance by up to 15.2%.

Conclusion: The proposed method effectively mitigates position bias in LLMs, demonstrating generalizability across various models and tasks.

Abstract: Large Language Models (LLMs) are increasingly applied in various real-world
scenarios due to their excellent generalization capabilities and robust
generative abilities. However, they exhibit position bias, also known as "lost
in the middle", a phenomenon that is especially pronounced in long-context
scenarios, which indicates the placement of the key information in different
positions of a prompt can significantly affect accuracy. This paper first
explores the micro-level manifestations of position bias, concluding that
attention weights are a micro-level expression of position bias. It further
identifies that, in addition to position embeddings, causal attention mask also
contributes to position bias by creating position-specific hidden states. Based
on these insights, we propose a method to mitigate position bias by scaling
this positional hidden states. Experiments on the NaturalQuestions
Multi-document QA, KV retrieval, LongBench and timeline reorder tasks, using
various models including RoPE models, context windowextended models, and Alibi
models, demonstrate the effectiveness and generalizability of our approach. Our
method can improve performance by up to 15.2% by modifying just one dimension
of hidden states. Our code is available at https://aka.ms/PositionalHidden.

</details>


### [187] [ReCaLL: Membership Inference via Relative Conditional Log-Likelihoods](https://arxiv.org/pdf/2406.15968)
*Roy Xie, Junlin Wang, Ruomin Huang, Minxing Zhang, Rong Ge, Jian Pei, Neil Zhenqiang Gong, Bhuwan Dhingra*

Main category: cs.CL

TL;DR: ReCaLL, a membership inference attack (MIA), detects LLMs' pretraining data by analyzing conditional log-likelihood changes, outperforming existing methods.


<details>
  <summary>Details</summary>
Motivation: Address concerns about data transparency and fair use in LLMs by detecting pretraining data.

Method: Uses ReCaLL to examine log-likelihood changes when prefixing target data with non-member context.

Result: Achieves state-of-the-art performance on WikiMIA, even with random/synthetic prefixes, and improves with ensemble methods.

Conclusion: ReCaLL effectively detects membership in LLMs' pretraining data, offering insights into their inference behavior.

Abstract: The rapid scaling of large language models (LLMs) has raised concerns about
the transparency and fair use of the data used in their pretraining. Detecting
such content is challenging due to the scale of the data and limited exposure
of each instance during training. We propose ReCaLL (Relative Conditional
Log-Likelihood), a novel membership inference attack (MIA) to detect LLMs'
pretraining data by leveraging their conditional language modeling
capabilities. ReCaLL examines the relative change in conditional
log-likelihoods when prefixing target data points with non-member context. Our
empirical findings show that conditioning member data on non-member prefixes
induces a larger decrease in log-likelihood compared to non-member data. We
conduct comprehensive experiments and show that ReCaLL achieves
state-of-the-art performance on the WikiMIA dataset, even with random and
synthetic prefixes, and can be further improved using an ensemble approach.
Moreover, we conduct an in-depth analysis of LLMs' behavior with different
membership contexts, providing insights into how LLMs leverage membership
information for effective inference at both the sequence and token level.

</details>


### [188] [Large Language Models Are Involuntary Truth-Tellers: Exploiting Fallacy Failure for Jailbreak Attacks](https://arxiv.org/pdf/2407.00869)
*Yue Zhou, Henry Peng Zou, Barbara Di Eugenio, Yang Zhang*

Main category: cs.CL

TL;DR: Language models struggle to generate deceptive reasoning, leaking honest outputs. This flaw is exploited for jailbreak attacks, producing harmful yet truthful outputs by masking them as fallacious. The method outperforms previous jailbreak techniques.


<details>
  <summary>Details</summary>
Motivation: To exploit language models' inability to generate deceptive reasoning, bypassing safeguards for harmful outputs.

Method: Query models to generate fallacious but real procedures for harmful behavior, leveraging their tendency to leak truthful outputs.

Result: The approach outperforms four previous jailbreak methods, producing more harmful outputs.

Conclusion: The findings extend beyond safety, impacting self-verification and hallucination in language models.

Abstract: We find that language models have difficulties generating fallacious and
deceptive reasoning. When asked to generate deceptive outputs, language models
tend to leak honest counterparts but believe them to be false. Exploiting this
deficiency, we propose a jailbreak attack method that elicits an aligned
language model for malicious output. Specifically, we query the model to
generate a fallacious yet deceptively real procedure for the harmful behavior.
Since a fallacious procedure is generally considered fake and thus harmless by
LLMs, it helps bypass the safeguard mechanism. Yet the output is factually
harmful since the LLM cannot fabricate fallacious solutions but proposes
truthful ones. We evaluate our approach over five safety-aligned large language
models, comparing four previous jailbreak methods, and show that our approach
achieves competitive performance with more harmful outputs. We believe the
findings could be extended beyond model safety, such as self-verification and
hallucination.

</details>


### [189] [Ground Every Sentence: Improving Retrieval-Augmented LLMs with Interleaved Reference-Claim Generation](https://arxiv.org/pdf/2407.01796)
*Sirui Xia, Xintao Wang, Jiaqing Liang, Yifei Zhang, Weikang Zhou, Jiaji Deng, Fei Yu, Yanghua Xiao*

Main category: cs.CL

TL;DR: ReClaim introduces fine-grained citations in RAG systems for better verifiability, achieving 90% citation accuracy.


<details>
  <summary>Details</summary>
Motivation: Enhance credibility and verifiability in RAG systems by providing detailed citations.

Method: Proposes ReClaim, a fine-grained ATG method that alternates references and answers step-by-step.

Result: Achieves 90% citation accuracy in long-form question-answering tasks.

Conclusion: ReClaim improves verifiability with sentence-level citations, outperforming coarse-grained methods.

Abstract: Retrieval-Augmented Generation (RAG) has been widely adopted to enhance Large
Language Models (LLMs) in knowledge-intensive tasks. To enhance credibility and
verifiability in RAG systems, Attributed Text Generation (ATG) is proposed,
which provides citations to retrieval knowledge in LLM-generated responses.
Prior methods mainly adopt coarse-grained attributions, with passage-level or
paragraph-level references or citations, which fall short in verifiability.
This paper proposes ReClaim (Refer & Claim), a fine-grained ATG method that
alternates the generation of references and answers step by step. Different
from previous coarse-grained attribution, ReClaim provides sentence-level
citations in long-form question-answering tasks. With extensive experiments, we
verify the effectiveness of ReClaim in extensive settings, achieving a citation
accuracy rate of 90%.

</details>


### [190] [Refuse Whenever You Feel Unsafe: Improving Safety in LLMs via Decoupled Refusal Training](https://arxiv.org/pdf/2407.09121)
*Youliang Yuan, Wenxiang Jiao, Wenxuan Wang, Jen-tse Huang, Jiahao Xu, Tian Liang, Pinjia He, Zhaopeng Tu*

Main category: cs.CL

TL;DR: The paper introduces Decoupled Refusal Training (DeRTa) to address refusal position bias in LLMs, enhancing their ability to refuse harmful prompts at any response position.


<details>
  <summary>Details</summary>
Motivation: To improve LLM safety by mitigating refusal position bias in safety tuning data, which currently limits models' ability to refuse unsafe content effectively.

Method: DeRTa combines Maximum Likelihood Estimation (MLE) with Harmful Response Prefix and Reinforced Transition Optimization (RTO) to train models for safer responses.

Result: Empirical tests show DeRTa improves safety without performance loss and outperforms baselines in defending against attacks.

Conclusion: DeRTa effectively enhances LLM safety by addressing refusal position bias and improving refusal capabilities.

Abstract: This study addresses a critical gap in safety tuning practices for Large
Language Models (LLMs) by identifying and tackling a refusal position bias
within safety tuning data, which compromises the models' ability to
appropriately refuse generating unsafe content. We introduce a novel approach,
Decoupled Refusal Training (DeRTa), designed to empower LLMs to refuse
compliance to harmful prompts at any response position, significantly enhancing
their safety capabilities. DeRTa incorporates two novel components: (1) Maximum
Likelihood Estimation (MLE) with Harmful Response Prefix, which trains models
to recognize and avoid unsafe content by appending a segment of harmful
response to the beginning of a safe response, and (2) Reinforced Transition
Optimization (RTO), which equips models with the ability to transition from
potential harm to safety refusal consistently throughout the harmful response
sequence. Our empirical evaluation, conducted using LLaMA3 and Mistral model
families across six attack scenarios, demonstrates that our method not only
improves model safety without compromising performance but also surpasses
baseline methods in defending against attacks.

</details>


### [191] [Genetic Instruct: Scaling up Synthetic Generation of Coding Instructions for Large Language Models](https://arxiv.org/pdf/2407.21077)
*Somshubra Majumdar, Vahid Noroozi, Mehrzad Samadi, Sean Narenthiran, Aleksander Ficek, Wasi Uddin Ahmad, Jocelyn Huang, Jagadeesh Balam, Boris Ginsburg*

Main category: cs.CL

TL;DR: Genetic-Instruct is a scalable algorithm for synthesizing high-quality coding instructions using evolutionary principles, improving LLM code generation.


<details>
  <summary>Details</summary>
Motivation: High-quality instruction data for LLMs is expensive to produce, especially for code generation tasks.

Method: Uses evolutionary principles with Instructor-LLM, Coder-LLM, and Judge-LLM to generate diverse instruction-code pairs.

Result: Produced 7.5M coding instructions, significantly improving LLM code generation performance.

Conclusion: Genetic-Instruct is efficient, scalable, and generalizable for enhancing LLM alignment.

Abstract: Large Language Models (LLMs) require high quality instruction data for
effective alignment, particularly in code generation tasks where expert curated
datasets are expensive to produce. We present Genetic-Instruct, a scalable
algorithm for synthesizing large-scale, high quality coding instructions using
evolutionary principles. Starting from a small set of seed instructions,
Genetic-Instruct generates diverse and challenging instruction-code pairs by
leveraging an Instructor-LLM for generation, a Coder-LLM for code synthesis,
and a Judge-LLM for automatic quality evaluation. Our proposed approach is
highly parallelizable and effective even with a small seed data and weaker
generator models. We generated more than 7.5 million coding instructions with
the proposed approach. Then we evaluated it by fine-tuning LLMs with the
synthetic samples and demonstrated a significant improvement in their code
generation capability compared to the other synthetic generation approaches and
publicly available datasets. Our results highlight the efficiency, scalability,
and generalizability of the Genetic-Instruct framework.

</details>


### [192] [Enhancing Robustness in Large Language Models: Prompting for Mitigating the Impact of Irrelevant Information](https://arxiv.org/pdf/2408.10615)
*Ming Jiang, Tingting Huang, Biao Guo, Yao Lu, Feng Zhang*

Main category: cs.CL

TL;DR: LLMs struggle with irrelevant info in problem descriptions. The GSMIR dataset and ATF method improve their reasoning by identifying and filtering such info.


<details>
  <summary>Details</summary>
Motivation: To address LLMs' diminished reasoning when problem descriptions contain irrelevant information, despite advanced prompting techniques.

Method: Constructed GSMIR dataset with irrelevant info; proposed ATF method (analysis and filtering) to enhance LLMs' ability to handle such info.

Result: ATF significantly improves LLMs' reasoning performance on GSMIR, even with irrelevant info.

Conclusion: The ATF method effectively mitigates the impact of irrelevant information, enhancing LLMs' reasoning capabilities.

Abstract: In recent years, Large language models (LLMs) have garnered significant
attention due to their superior performance in complex reasoning tasks.
However, recent studies may diminish their reasoning capabilities markedly when
problem descriptions contain irrelevant information, even with the use of
advanced prompting techniques. To further investigate this issue, a dataset of
primary school mathematics problems containing irrelevant information, named
GSMIR, was constructed. Testing prominent LLMs and prompting techniques on this
dataset revealed that while LLMs can identify irrelevant information, they do
not effectively mitigate the interference it causes once identified. A novel
automatic construction method, ATF, which enhances the ability of LLMs to
identify and self-mitigate the influence of irrelevant information, is proposed
to address this shortcoming. This method operates in two steps: first, analysis
of irrelevant information, followed by its filtering. The ATF method, as
demonstrated by experimental results, significantly improves the reasoning
performance of LLMs and prompting techniques, even in the presence of
irrelevant information on the GSMIR dataset.

</details>


### [193] [Task Arithmetic for Language Expansion in Speech Translation](https://arxiv.org/pdf/2409.11274)
*Yao-Fei Cheng, Hayato Futami, Yosuke Kashiwagi, Emiru Tsunoo, Wen Shen Teo, Siddhant Arora, Shinji Watanabe*

Main category: cs.CL

TL;DR: A method to expand speech translation (ST) systems to multiple languages without re-training, using task arithmetic and a language control model, achieving significant BLEU and COMET score improvements.


<details>
  <summary>Details</summary>
Motivation: Avoid the high cost of re-training ST systems for new language pairs by leveraging existing one-to-one systems.

Method: Augmented task arithmetic with a language control model to prevent language confusion, and synthesis of ST models from existing MT and ST models.

Result: BLEU score improvements up to 4.66 and 4.92, with COMET gains of 8.87 and 11.83 on MuST-C and CoVoST-2 datasets.

Conclusion: The framework efficiently extends ST systems to new languages without re-training, even for pairs lacking paired data or pre-trained models.

Abstract: Recent progress in large language models (LLMs) has gained interest in
speech-text multimodal foundation models, achieving strong performance on
instruction-tuned speech translation (ST). However, expanding language pairs is
costly due to re-training on combined new and previous datasets. To address
this, we aim to build a one-to-many ST system from existing one-to-one ST
systems using task arithmetic without re-training. Direct application of task
arithmetic in ST leads to language confusion; therefore, we introduce an
augmented task arithmetic method incorporating a language control model to
ensure correct target language generation. Our experiments on MuST-C and
CoVoST-2 show BLEU score improvements of up to 4.66 and 4.92, with COMET gains
of 8.87 and 11.83. In addition, we demonstrate our framework can extend to
language pairs lacking paired ST training data or pre-trained ST models by
synthesizing ST models based on existing machine translation (MT) and ST models
via task analogies.

</details>


### [194] [From Lists to Emojis: How Format Bias Affects Model Alignment](https://arxiv.org/pdf/2409.11704)
*Xuanchang Zhang, Wei Xiong, Lichang Chen, Tianyi Zhou, Heng Huang, Tong Zhang*

Main category: cs.CL

TL;DR: The paper investigates format biases in RLHF, revealing biases in preference models (e.g., GPT-4) towards patterns like lists, bold text, and verbosity. It shows how LLMs exploit these biases for higher rankings and demonstrates that even minimal biased data can skew reward models. The study calls for separating format and content in alignment algorithms and evaluations.


<details>
  <summary>Details</summary>
Motivation: To uncover and analyze format biases in preference models beyond verbosity, as these biases are underexplored but significantly impact model rankings and alignment.

Method: Extends bias study in preference learning, tests bias injection with minimal data, and examines exploitation by alignment algorithms like best-of-n sampling and DPO.

Result: Preference models exhibit strong format biases; small biased data can skew rewards; alignment algorithms easily exploit format biases.

Conclusion: Format and content must be disentangled in alignment and evaluation to mitigate biases and ensure fair model assessments.

Abstract: In this paper, we study format biases in reinforcement learning from human
feedback (RLHF). We observe that many widely-used preference models, including
human evaluators, GPT-4, and top-ranking models on the RewardBench benchmark,
exhibit strong biases towards specific format patterns, such as lists, links,
bold text, and emojis. Furthermore, large language models (LLMs) can exploit
these biases to achieve higher rankings on popular benchmarks like AlpacaEval
and LMSYS Chatbot Arena. One notable example of this is verbosity bias, where
current preference models favor longer responses that appear more
comprehensive, even when their quality is equal to or lower than shorter,
competing responses. However, format biases beyond verbosity remain largely
underexplored in the literature. In this work, we extend the study of biases in
preference learning beyond the commonly recognized length bias, offering a
comprehensive analysis of a wider range of format biases. Additionally, we show
that with a small amount of biased data (less than 1%), we can inject
significant bias into the reward model. Moreover, these format biases can also
be easily exploited by downstream alignment algorithms, such as best-of-n
sampling and online iterative DPO, as it is usually easier to manipulate the
format than to improve the quality of responses. Our findings emphasize the
need to disentangle format and content both for designing alignment algorithms
and evaluating models.

</details>


### [195] [Enhancing Unsupervised Sentence Embeddings via Knowledge-Driven Data Augmentation and Gaussian-Decayed Contrastive Learning](https://arxiv.org/pdf/2409.12887)
*Peichao Lai, Zhengfeng Zhang, Wentao Zhang, Fangcheng Fu, Bin Cui*

Main category: cs.CL

TL;DR: The paper proposes a pipeline using LLMs and knowledge graphs for diverse data augmentation and introduces the GCSE model to reduce noise, achieving top performance in unsupervised sentence embeddings.


<details>
  <summary>Details</summary>
Motivation: Existing LLM-based data augmentation methods suffer from limited diversity and high noise, neglecting fine-grained knowledge and discriminative information.

Method: A pipeline combines LLMs with knowledge graphs for diverse data generation, and the GCSE model uses a Gaussian-decayed function to mitigate noise from false negatives.

Result: The approach achieves state-of-the-art performance in STS tasks with fewer samples and smaller LLMs, proving efficiency and robustness.

Conclusion: The proposed method effectively addresses diversity and noise challenges, enhancing unsupervised sentence embeddings.

Abstract: Recently, using large language models (LLMs) for data augmentation has led to
considerable improvements in unsupervised sentence embedding models. However,
existing methods encounter two primary challenges: limited data diversity and
high data noise. Current approaches often neglect fine-grained knowledge, such
as entities and quantities, leading to insufficient diversity. Besides,
unsupervised data frequently lacks discriminative information, and the
generated synthetic samples may introduce noise. In this paper, we propose a
pipeline-based data augmentation method via LLMs and introduce the
Gaussian-decayed gradient-assisted Contrastive Sentence Embedding (GCSE) model
to enhance unsupervised sentence embeddings. To tackle the issue of low data
diversity, our pipeline utilizes knowledge graphs (KGs) to extract entities and
quantities, enabling LLMs to generate more diverse samples. To address high
data noise, the GCSE model uses a Gaussian-decayed function to limit the impact
of false hard negative samples, enhancing the model's discriminative
capability. Experimental results show that our approach achieves
state-of-the-art performance in semantic textual similarity (STS) tasks, using
fewer data samples and smaller LLMs, demonstrating its efficiency and
robustness across various models.

</details>


### [196] [Position IDs Matter: An Enhanced Position Layout for Efficient Context Compression in Large Language Models](https://arxiv.org/pdf/2409.14364)
*Runsong Zhao, Xin Liu, Xinyu Liu, Pengcheng Huang, Chunyang Xiao, Tong Xiao, Jingbo Zhu*

Main category: cs.CL

TL;DR: EPL improves context compression in LLMs by adjusting position IDs, enhancing performance in QA and multimodal tasks.


<details>
  <summary>Details</summary>
Motivation: Existing token compression methods overlook holistic dependencies due to local biases from position encodings.

Method: EPL adjusts position IDs to minimize distance between context and special tokens while maintaining sequence order.

Result: 1.9 ROUGE-1 F1 improvement in QA; 2.6 accuracy gain in multimodal tasks.

Conclusion: EPL is a simple, effective method to enhance context compression in LLMs.

Abstract: Using special tokens (e.g., gist, memory, or compressed tokens) to compress
context information is a common practice for large language models (LLMs).
However, existing approaches often neglect that position encodings inherently
induce local inductive biases in models, causing the compression process to
ignore holistic contextual dependencies. We propose Enhanced Position Layout
(EPL), a simple yet effective method that improves the context compression
capability of LLMs by only adjusting position IDs, the numerical identifiers
that specify token positions. EPL minimizes the distance between context tokens
and their corresponding special tokens and at the same time maintains the
sequence order in position IDs between context tokens, special tokens, and the
subsequent tokens. Integrating EPL into our best performing context compression
model results in 1.9 ROUGE-1 F1 improvement on out-of-domain question answering
datasets in average. When extended to multimodal scenarios, EPL brings an
average accuracy gain of 2.6 to vision compression LLMs.

</details>


### [197] [Cross-lingual Human-Preference Alignment for Neural Machine Translation with Direct Quality Optimization](https://arxiv.org/pdf/2409.17673)
*Kaden Uhlig, Joern Wuebker, Raphael Reinauer, John DeNero*

Main category: cs.CL

TL;DR: Task-alignment techniques like RLHF and DPO improve multilingual NMT by addressing task-data mismatch, even when applied to a subset of languages. DQO, a DPO variant, uses translation quality estimation for human preference proxy, showing improvements via metrics and human evaluation.


<details>
  <summary>Details</summary>
Motivation: To address task-data mismatch in NMT and improve performance across multilingual models by leveraging task-alignment techniques.

Method: Introduces Direct Quality Optimization (DQO), a variant of DPO, using a pre-trained translation quality estimation model as a proxy for human preferences.

Result: Improvements observed across all languages in a multilingual model, even when alignment is applied to only a subset. Verified by automatic metrics and human evaluation.

Conclusion: Task-alignment techniques like DQO effectively enhance NMT performance, demonstrating broader applicability beyond the aligned languages.

Abstract: Reinforcement Learning from Human Feedback (RLHF) and derivative techniques
like Direct Preference Optimization (DPO) are task-alignment algorithms used to
repurpose general, foundational models for specific tasks. We show that
applying task-alignment to neural machine translation (NMT) addresses an
existing task--data mismatch in NMT, leading to improvements across all
languages of a multilingual model, even when task-alignment is only applied to
a subset of those languages. We do so by introducing Direct Quality
Optimization (DQO), a variant of DPO leveraging a pre-trained translation
quality estimation model as a proxy for human preferences, and verify the
improvements with both automatic metrics and human evaluation.

</details>


### [198] [KCIF: Knowledge-Conditioned Instruction Following](https://arxiv.org/pdf/2410.12972)
*Rudra Murthy, Praveen Venkateswaran, Prince Kumar, Danish Contractor*

Main category: cs.CL

TL;DR: LLMs struggle with simple answer-modifying instructions and are distracted by irrelevant ones, causing significant performance drops across model sizes.


<details>
  <summary>Details</summary>
Motivation: To study the interaction between knowledge/reasoning and instruction following in LLMs, revealing limitations in their traditional separation.

Method: Apply simple instructions (text manipulation, numeric changes, list operations, distractors) to existing multiple-choice knowledge benchmarks and evaluate models of varying sizes.

Result: All models show performance drops (40-50% in large models, up to 80% in smaller ones) when handling combined tasks.

Conclusion: Joint study of knowledge/reasoning and instruction following is crucial; benchmark and framework released for future work.

Abstract: LLM evaluation benchmarks have traditionally separated the testing of
knowledge/reasoning capabilities from instruction following. In this work, we
study the interaction between knowledge and instruction following, and observe
that LLMs struggle to follow simple answer modifying instructions, and are also
distracted by instructions that should have no bearing on the original
knowledge task answer. We leverage existing multiple-choice answer based
knowledge benchmarks and apply a set of simple instructions which include
manipulating text (eg.: change case), numeric quantities (eg.: increase value,
change formatting), operate on lists (eg.: sort answer candidates) and
distractor instructions (eg.: change case of numeric answers). We evaluate
models at varying parameter sizes (1B-405B) from different model families and
find that, surprisingly, all models report a significant drop in performance on
such simple task compositions. While large-sized and frontier models report
performance drops of 40-50%, in small and medium sized models the drop is
severe (sometimes exceeding 80%). Our results highlight a limitation in the
traditional separation of knowledge/reasoning and instruction following, and
suggest that joint-study of these capabilities are important. We release our
benchmark dataset, evaluation framework code, and results for future work.

</details>


### [199] [TrendFact: A Benchmark for Explainable Hotspot Perception in Fact-Checking with Natural Language Explanation](https://arxiv.org/pdf/2410.15135)
*Xiaocheng Zhang, Xi Wang, Yifei Lu, Jianing Wang, Zhuangzhuang Ye, Mengjiao Bao, Peng Yan, Xiaohong Su*

Main category: cs.CL

TL;DR: TrendFact is a new benchmark for fact-checking, addressing gaps in current benchmarks by evaluating verification, evidence retrieval, and explanation generation. It introduces metrics ECS and HCPI and highlights limitations of existing systems. FactISR, a proposed method, improves RLM performance for complex fact-checking.


<details>
  <summary>Details</summary>
Motivation: Current fact-checking benchmarks lack impact assessment, high-quality explanations, and are English-centric. TrendFact aims to address these gaps.

Method: TrendFact includes 7,643 curated samples and an evidence library of 66,217 entries. Metrics ECS and HCPI are introduced. FactISR integrates dynamic evidence augmentation, triangulation, and self-reflection.

Result: Existing systems, including advanced RLMs, perform poorly on TrendFact. FactISR improves RLM performance.

Conclusion: TrendFact highlights real-world challenges in fact-checking. FactISR offers a solution for explainable and complex verification tasks.

Abstract: Although fact verification remains fundamental, explanation generation serves
as a critical enabler for trustworthy fact-checking systems by producing
interpretable rationales and facilitating comprehensive verification processes.
However, current benchmarks have limitations that include the lack of impact
assessment, insufficient high-quality explanatory annotations, and an
English-centric bias. To address these, we introduce TrendFact, the first
hotspot perception fact-checking benchmark that comprehensively evaluates fact
verification, evidence retrieval, and explanation generation tasks. TrendFact
consists of 7,643 carefully curated samples sourced from trending platforms and
professional fact-checking datasets, as well as an evidence library of 66,217
entries with publication dates. We further propose two metrics, ECS and HCPI,
to complement existing benchmarks by evaluating the system's explanation
consistency and hotspot perception capability, respectively. Experimental
results show that current fact-checking systems, including advanced RLMs such
as DeepSeek-R1, face significant limitations when evaluated on TrendFact,
highlighting the real-world challenges posed by it. To enhance the
fact-checking capabilities of reasoning large language models (RLMs), we
propose FactISR, which integrates dynamic evidence augmentation, evidence
triangulation, and an iterative self-reflection mechanism. Accordingly, FactISR
effectively improves RLM performance, offering new insights for explainable and
complex fact-checking.

</details>


### [200] [Can Large Language Models Invent Algorithms to Improve Themselves?: Algorithm Discovery for Recursive Self-Improvement through Reinforcement Learning](https://arxiv.org/pdf/2410.15639)
*Yoichi Ishibashi, Taro Yano, Masafumi Oyamada*

Main category: cs.CL

TL;DR: Self-Developing enables LLMs to autonomously create and refine their own improvement algorithms, outperforming human-designed methods.


<details>
  <summary>Details</summary>
Motivation: Current LLM improvement methods are limited by human design; the paper aims to enable autonomous self-improvement.

Method: Uses an iterative cycle where the model generates, evaluates, and refines algorithmic candidates via Direct Preference Optimization.

Result: Autonomously discovered algorithms outperform human-designed ones, improving GSM8k performance by 6% and generalizing well.

Conclusion: LLMs can autonomously develop novel optimization techniques, marking a step toward self-advancing AI.

Abstract: Large Language Models (LLMs) have achieved remarkable capabilities, yet their
improvement methods remain fundamentally constrained by human design. We
present Self-Developing, a framework that enables LLMs to autonomously
discover, implement, and refine their own improvement algorithms. Our approach
employs an iterative cycle where a seed model generates algorithmic candidates
as executable code, evaluates their effectiveness, and uses Direct Preference
Optimization to recursively improve increasingly sophisticated improvement
strategies. We demonstrate this framework through model merging, a practical
technique for combining specialized models. Self-Developing successfully
discovered novel merging algorithms that outperform existing human-designed
algorithms. On mathematical reasoning benchmarks, the autonomously discovered
algorithms improve the seed model's GSM8k performance by 6\% and exceed
human-designed approaches like Task Arithmetic by 4.3\%. Remarkably, these
algorithms exhibit strong generalization, achieving 7.4\% gains on
out-of-domain models without re-optimization. Our findings demonstrate that
LLMs can transcend their training to invent genuinely novel optimization
techniques. This capability represents a crucial step toward a new era where
LLMs not only solve problems but autonomously develop the methodologies for
their own advancement.

</details>


### [201] [Hybrid Preferences: Learning to Route Instances for Human vs. AI Feedback](https://arxiv.org/pdf/2410.19133)
*Lester James V. Miranda, Yizhong Wang, Yanai Elazar, Sachin Kumar, Valentina Pyatkin, Faeze Brahman, Noah A. Smith, Hannaneh Hajishirzi, Pradeep Dasigi*

Main category: cs.CL

TL;DR: HyPER, a hybrid preference router, combines human and LM annotations to improve alignment quality while reducing costs, outperforming exclusive use of either by 7-13%.


<details>
  <summary>Details</summary>
Motivation: Human preference annotation is costly and inconsistent, while LM-based annotations are scalable but biased. HyPER aims to balance both for better efficiency and quality.

Method: HyPER trains a performance prediction model (PPM) to optimize annotation routing between humans and LMs, using a new dataset (MultiPref) for training.

Result: HyPER achieves 7-13% better RM performance than exclusive human or LM annotations and generalizes across datasets and models. Best-of-N reranking also shows 2-3% improvement.

Conclusion: HyPER effectively balances human and LM annotations, with prompts of moderate complexity or safety concerns benefiting most from human input.

Abstract: Learning from human feedback has enabled the alignment of language models
(LMs) with human preferences. However, collecting human preferences is
expensive and time-consuming, with highly variable annotation quality. An
appealing alternative is to distill preferences from LMs as a source of
synthetic annotations, offering a cost-effective and scalable alternative,
albeit susceptible to other biases and errors. In this work, we introduce
HyPER, a Hybrid Preference routER that defers an annotation to either humans or
LMs, achieving better annotation quality while reducing the cost of human-only
annotation. We formulate this as an optimization problem: given a preference
dataset and an evaluation metric, we (1) train a performance prediction model
(PPM) to predict a reward model's (RM) performance on an arbitrary combination
of human and LM annotations and (2) employ a routing strategy that selects a
combination that maximizes the predicted performance. We train the PPM on
MultiPref, a new preference dataset with 10k instances paired with humans and
LM labels. We show that the selected hybrid mixture of synthetic and direct
human preferences using HyPER achieves better RM performance compared to using
either one exclusively by 7-13% on RewardBench and generalizes across unseen
preference datasets and other base models. We also observe the same trend in
other benchmarks using Best-of-N reranking, where the hybrid mix has 2-3%
better performance. Finally, we analyze features from HyPER and find that
prompts with moderate safety concerns or complexity benefit the most from human
feedback.

</details>


### [202] [Long Sequence Modeling with Attention Tensorization: From Sequence to Tensor Learning](https://arxiv.org/pdf/2410.20926)
*Aosong Feng, Rex Ying, Leandros Tassiulas*

Main category: cs.CL

TL;DR: The paper proposes Tensorized Attention to handle long-range dependencies efficiently by transforming sequences into compact tensor representations, achieving improved memory and time efficiency.


<details>
  <summary>Details</summary>
Motivation: The mismatch between limited-range modeling power of full attention and long-range token dependencies in sequences necessitates scalable solutions.

Method: Tensorize long input sequences into compact tensor representations and apply attention on each transformed dimension, equivalent to Kronecker decomposition of full attention.

Result: Tensorized Attention extends context length efficiently, e.g., Llama-8B trained at 32,768 length extrapolates to 128k with 11x speedup.

Conclusion: Tensorized Attention is an efficient transformer backbone for long-sequence modeling, enhancing pretrained LLMs with scalability and speed.

Abstract: As the demand for processing extended textual data grows, the ability to
handle long-range dependencies and maintain computational efficiency is more
critical than ever. One of the key issues for long-sequence modeling using
attention-based model is the mismatch between the limited-range modeling power
of full attention and the long-range token dependency in the input sequence. In
this work, we propose to scale up the attention receptive field by tensorizing
long input sequences into compact tensor representations followed by attention
on each transformed dimension. The resulting Tensorized Attention can be
adopted as efficient transformer backbones to extend input context length with
improved memory and time efficiency. We show that the proposed attention
tensorization encodes token dependencies as a multi-hop attention process, and
is equivalent to Kronecker decomposition of full attention. Extensive
experiments show that tensorized attention can be used to adapt pretrained LLMs
with improved efficiency. Notably, Llama-8B with tensorization is trained under
32,768 context length and can steadily extrapolate to 128k length during
inference with $11\times$ speedup, compared to full attention with
FlashAttention-2.

</details>


### [203] [LLäMmlein: Compact and Competitive German-Only Language Models from Scratch](https://arxiv.org/pdf/2411.11171)
*Jan Pfister, Julia Wunderle, Andreas Hotho*

Main category: cs.CL

TL;DR: Two German-only decoder models, LL"aMmlein 120M and 1B, were developed and released for the German NLP community, with competitive performance on benchmarks.


<details>
  <summary>Details</summary>
Motivation: To provide transparent, German-specific NLP models and training data for research, addressing the need for localized resources.

Method: Involved data preprocessing, custom German tokenizer creation, model training, and evaluation using benchmarks like SuperGLEBer.

Result: Both models performed competitively, matching or surpassing similar-sized models, with performance scaling by size but plateauing on some tasks.

Conclusion: The models offer valuable insights for future resource allocation in German NLP model development.

Abstract: We create two German-only decoder models, LL\"aMmlein 120M and 1B,
transparently from scratch and publish them, along with the training data, for
the German NLP research community to use. The model training involved several
key steps, including extensive data preprocessing, the creation of a custom
German tokenizer, the training itself, as well as the evaluation of the final
models on various benchmarks. Throughout the training process, multiple
checkpoints were saved and analyzed using the SuperGLEBer benchmark to monitor
the models' learning dynamics. Compared to state-of-the-art models on the
SuperGLEBer benchmark, both LL\"aMmlein models performed competitively,
consistently matching or surpassing models with similar parameter sizes. The
results show that the models' quality scales with size as expected, but
performance improvements on some tasks plateaued early, offering valuable
insights into resource allocation for future model development.

</details>


### [204] [Multi-modal Retrieval Augmented Multi-modal Generation: Datasets, Evaluation Metrics and Strong Baselines](https://arxiv.org/pdf/2411.16365)
*Zi-Ao Ma, Tian Lan, Rong-Cheng Tu, Yong Hu, Yu-Shi Zhu, Tong Zhang, Heyan Huang, Zhijing Wu, Xian-Ling Mao*

Main category: cs.CL

TL;DR: The paper introduces M²RAG, a multi-modal retrieval-augmented generation task, and addresses its understudied nature by creating a benchmark, proposing evaluation metrics, and effective strategies for foundation models. Their fine-tuned models outperform GPT-4o and approach state-of-the-art performance.


<details>
  <summary>Details</summary>
Motivation: M²RAG is a promising but understudied task with potential for high information density and readability. The lack of comprehensive analysis and data resources motivated this work.

Method: The authors established a benchmark with a rigorous data curation pipeline, proposed text-modal and multi-modal metrics, and developed strategies for foundation models to handle M²RAG. They also filtered high-quality training samples.

Result: Fine-tuned 7B-8B models outperformed GPT-4o and neared state-of-the-art performance. The proposed metrics proved reliable, and the data curation pipeline was validated.

Conclusion: The work provides a robust framework for M²RAG, demonstrating its feasibility and effectiveness. All resources will be publicly released to support future research.

Abstract: We present a systematic investigation of Multi-modal Retrieval Augmented
Multi-modal Generation (M$^2$RAG), a novel task that enables foundation models
to process multi-modal web content and generate multi-modal responses, which
exhibits better information density and readability. Despite its potential
impact, M$^2$RAG remains understudied, lacking comprehensive analysis and
high-quality data resources. To address this gap, we establish a comprehensive
benchmark through a rigorous data curation pipeline, and employ text-modal
metrics and multi-modal metrics based on foundation models for evaluation. We
further propose several strategies for foundation models to process M$^2$RAG
task effectively and construct a training set by filtering high-quality samples
using our designed metrics. Our extensive experiments demonstrate the
reliability of our proposed metrics, a landscape of model performance within
our designed strategies, and show that our fine-tuned 7B-8B models outperform
the GPT-4o model and approach the state-of-the-art OpenAI o3-mini.
Additionally, we perform fine-grained analyses across diverse domains and
validate the effectiveness of our designs in data curation pipeline. All
resources, including codes, datasets, and model weights, will be publicly
released.

</details>


### [205] [Initialization using Update Approximation is a Silver Bullet for Extremely Efficient Low-Rank Fine-Tuning](https://arxiv.org/pdf/2411.19557)
*Kaustubh Ponkshe, Raghav Singhal, Eduard Gorbunov, Alexey Tumanov, Samuel Horvath, Praneeth Vepakomma*

Main category: cs.CL

TL;DR: LoRA-SB approximates full fine-tuning in low-rank subspaces, outperforming standard LoRA and LoRA-XS with fewer parameters.


<details>
  <summary>Details</summary>
Motivation: Standard LoRA falls short of full fine-tuning performance; LoRA-SB aims to bridge this gap efficiently.

Method: Uses a learnable (r x r) matrix between B and A, with optimal initialization for gradient updates.

Result: Exceeds standard LoRA performance with 27-90x fewer parameters and outperforms LoRA-XS.

Conclusion: LoRA-SB simulates full fine-tuning in low-rank subspaces, achieving efficiency without performance loss.

Abstract: Low-rank adapters have become standard for efficiently fine-tuning large
language models (LLMs), but they often fall short of achieving the performance
of full fine-tuning. We propose a method, LoRA Silver Bullet or LoRA-SB, that
approximates full fine-tuning within low-rank subspaces using a carefully
designed initialization strategy. We theoretically demonstrate that the
architecture of LoRA-XS, which inserts a learnable (r x r) matrix between B and
A while keeping other matrices fixed, provides the precise conditions needed
for this approximation. We leverage its constrained update space to achieve
optimal scaling for high-rank gradient updates while removing the need for
hyperparameter tuning. We prove that our initialization offers an optimal
low-rank approximation of the initial gradient and preserves update directions
throughout training. Extensive experiments across mathematical reasoning,
commonsense reasoning, and language understanding tasks demonstrate that our
approach exceeds the performance of standard LoRA while using \textbf{27-90}
times fewer learnable parameters, and comprehensively outperforms LoRA-XS. Our
findings establish that it is possible to simulate full fine-tuning in low-rank
subspaces, and achieve significant efficiency gains without sacrificing
performance. Our code is publicly available at
https://github.com/RaghavSinghal10/lora-sb.

</details>


### [206] [MediaSpin: Exploring Media Bias Through Fine-Grained Analysis of News Headlines](https://arxiv.org/pdf/2412.02271)
*Preetika Verma, Kokil Jaidka*

Main category: cs.CL

TL;DR: The study introduces the MediaSpin dataset to analyze media bias in edited news headlines, using human-supervised LLM labeling to identify 13 bias types.


<details>
  <summary>Details</summary>
Motivation: To understand how editorial changes in news headlines introduce bias and shape public perception.

Method: Creation of the MediaSpin dataset with 78,910 headline pairs annotated for 13 bias types using human-supervised LLM labeling.

Result: The dataset provides linguistic insights and applications for bias prediction and user behavior analysis.

Conclusion: The MediaSpin dataset is a valuable tool for systematically identifying and analyzing media bias in edited headlines.

Abstract: The editability of online news content has become a significant factor in
shaping public perception, as social media platforms introduce new affordances
for dynamic and adaptive news framing. Edits to news headlines can refocus
audience attention, add or remove emotional language, and shift the framing of
events in subtle yet impactful ways. What types of media bias are editorialized
in and out of news headlines, and how can they be systematically identified?
This study introduces the MediaSpin dataset, the first to characterize the bias
in how prominent news outlets editorialize news headlines after publication.
The dataset includes 78,910 pairs of headlines annotated with 13 distinct types
of media bias, using human-supervised LLM labeling. We discuss the linguistic
insights it affords and show its applications for bias prediction and user
behavior analysis.

</details>


### [207] [UAlign: Leveraging Uncertainty Estimations for Factuality Alignment on Large Language Models](https://arxiv.org/pdf/2412.11803)
*Boyang Xue, Fei Mi, Qi Zhu, Hongru Wang, Rui Wang, Sheng Wang, Erxin Yu, Xuming Hu, Kam-Fai Wong*

Main category: cs.CL

TL;DR: UAlign improves LLMs' factual accuracy by using uncertainty estimations to align knowledge boundaries, enhancing confidence in answering known questions and refusing unknown ones.


<details>
  <summary>Details</summary>
Motivation: LLMs often struggle to accurately express factual knowledge due to ambiguous knowledge boundaries.

Method: UAlign uses confidence scores and semantic entropy to represent knowledge boundaries, trains a reward model, and employs PPO for factuality alignment.

Result: UAlign significantly improves LLMs' reliability and generalizability in answering known and refusing unknown questions.

Conclusion: Incorporating uncertainty representations in LLM alignment enhances factual accuracy and reliability.

Abstract: Despite demonstrating impressive capabilities, Large Language Models (LLMs)
still often struggle to accurately express the factual knowledge they possess,
especially in cases where the LLMs' knowledge boundaries are ambiguous. To
improve LLMs' factual expressions, we propose the UAlign framework, which
leverages Uncertainty estimations to represent knowledge boundaries, and then
explicitly incorporates these representations as input features into prompts
for LLMs to Align with factual knowledge. First, we prepare the dataset on
knowledge question-answering (QA) samples by calculating two uncertainty
estimations, including confidence score and semantic entropy, to represent the
knowledge boundaries for LLMs. Subsequently, using the prepared dataset, we
train a reward model that incorporates uncertainty estimations and then employ
the Proximal Policy Optimization (PPO) algorithm for factuality alignment on
LLMs. Experimental results indicate that, by integrating uncertainty
representations in LLM alignment, the proposed UAlign can significantly enhance
the LLMs' capacities to confidently answer known questions and refuse unknown
questions on both in-domain and out-of-domain tasks, showing reliability
improvements and good generalizability over various prompt- and training-based
baselines.

</details>


### [208] [Boosting Long-Context Management via Query-Guided Activation Refilling](https://arxiv.org/pdf/2412.12486)
*Hongjin Qian, Zheng Liu, Peitian Zhang, Zhicheng Dou, Defu Lian*

Main category: cs.CL

TL;DR: ACRE introduces a query-guided Activation Refilling method to handle long-context tasks by dynamically balancing global and local information via a Bi-layer KV Cache.


<details>
  <summary>Details</summary>
Motivation: Long-context processing in LLMs is inefficient due to fixed context windows and high computational costs, while existing methods fail to adapt to dynamic query needs.

Method: ACRE uses a Bi-layer KV Cache (L1 for global, L2 for local info) and dynamically refills L1 with relevant L2 entries based on the query.

Result: ACRE improves performance and efficiency on long-context datasets.

Conclusion: ACRE effectively addresses dynamic information needs in long-context tasks by integrating global and local details.

Abstract: Processing long contexts poses a significant challenge for large language
models (LLMs) due to their inherent context-window limitations and the
computational burden of extensive key-value (KV) activations, which severely
impact efficiency. For information-seeking tasks, full context perception is
often unnecessary, as a query's information needs can dynamically range from
localized details to a global perspective, depending on its complexity.
However, existing methods struggle to adapt effectively to these dynamic
information needs.
  In the paper, we propose a method for processing long-context
information-seeking tasks via query-guided Activation Refilling (ACRE). ACRE
constructs a Bi-layer KV Cache for long contexts, where the layer-1 (L1) cache
compactly captures global information, and the layer-2 (L2) cache provides
detailed and localized information. ACRE establishes a proxying relationship
between the two caches, allowing the input query to attend to the L1 cache and
dynamically refill it with relevant entries from the L2 cache. This mechanism
integrates global understanding with query-specific local details, thus
improving answer decoding. Experiments on a variety of long-context
information-seeking datasets demonstrate ACRE's effectiveness, achieving
improvements in both performance and efficiency.

</details>


### [209] [TrustRAG: Enhancing Robustness and Trustworthiness in Retrieval-Augmented Generation](https://arxiv.org/pdf/2501.00879)
*Huichi Zhou, Kin-Hei Lee, Zhonghao Zhan, Yue Chen, Zhenhao Li, Zhaoyang Wang, Hamed Haddadi, Emine Yilmaz*

Main category: cs.CL

TL;DR: TrustRAG is a robust framework to defend against corpus poisoning attacks in Retrieval-Augmented Generation (RAG) systems, improving retrieval accuracy and security.


<details>
  <summary>Details</summary>
Motivation: RAG systems are vulnerable to corpus poisoning attacks, which degrade LLM performance. TrustRAG aims to mitigate this by filtering malicious content.

Method: TrustRAG uses a two-stage defense: cluster filtering to detect attack patterns and LLM self-assessment to identify malicious documents.

Result: TrustRAG improves retrieval accuracy, efficiency, and attack resistance without requiring additional training.

Conclusion: TrustRAG is an effective, plug-and-play solution for securing RAG systems against poisoning attacks.

Abstract: Retrieval-Augmented Generation (RAG) enhances large language models (LLMs) by
integrating external knowledge sources, enabling more accurate and contextually
relevant responses tailored to user queries. These systems, however, remain
susceptible to corpus poisoning attacks, which can severely impair the
performance of LLMs. To address this challenge, we propose TrustRAG, a robust
framework that systematically filters malicious and irrelevant content before
it is retrieved for generation. Our approach employs a two-stage defense
mechanism. The first stage implements a cluster filtering strategy to detect
potential attack patterns. The second stage employs a self-assessment process
that harnesses the internal capabilities of LLMs to detect malicious documents
and resolve inconsistencies. TrustRAG provides a plug-and-play, training-free
module that integrates seamlessly with any open- or closed-source language
model. Extensive experiments demonstrate that TrustRAG delivers substantial
improvements in retrieval accuracy, efficiency, and attack resistance.

</details>


### [210] [URSA: Understanding and Verifying Chain-of-thought Reasoning in Multimodal Mathematics](https://arxiv.org/pdf/2501.04686)
*Ruilin Luo, Zhuofan Zheng, Yifan Wang, Xinzhe Ni, Zicheng Lin, Songtao Jiang, Yiyao Yu, Chufan Shi, Ruihang Chu, Jin Zeng, Yujiu Yang*

Main category: cs.CL

TL;DR: URSA introduces a multimodal training framework (URSA-8B) with a large-scale dataset (MMathCoT-1M) and a novel RL method (PS-GRPO), outperforming Gemma3-12B and GPT-4o in benchmarks.


<details>
  <summary>Details</summary>
Motivation: To address the unexplored potential of PRMs in multimodal reasoning and overcome challenges like data scarcity, lack of automated labeling, and reward hacking.

Method: Three-stage framework: (1) Build MMathCoT-1M dataset for URSA-8B, (2) synthesize process supervision data (DualMath-1.1M), (3) apply PS-GRPO for RL.

Result: URSA-8B-PS-GRPO outperforms Gemma3-12B by 8.4% and GPT-4o by 2.7% on average across 6 benchmarks.

Conclusion: URSA successfully integrates PRMs into multimodal reasoning, demonstrating superior performance and addressing key challenges.

Abstract: Process Reward Models (PRMs) have shown promise in enhancing the mathematical
reasoning capabilities of Large Language Models (LLMs) through Test-Time
Scaling (TTS). However, their integration into multimodal reasoning remains
largely unexplored. In this work, we take the first step toward unlocking the
potential of PRMs in multimodal mathematical reasoning. We identify three key
challenges: (1) the scarcity of high-quality reasoning data constrains the
capabilities of foundation Multimodal Large Language Models (MLLMs), which
imposes further limitations on the upper bounds of TTS and reinforcement
learning (RL); (2) a lack of automated methods for process labeling within
multimodal contexts persists; (3) the employment of process rewards in unimodal
RL faces issues like reward hacking, which may extend to multimodal scenarios.
To address these issues, we introduce URSA, a three-stage Unfolding multimodal
Process-Supervision Aided training framework. We first construct MMathCoT-1M, a
high-quality large-scale multimodal Chain-of-Thought (CoT) reasoning dataset,
to build a stronger math reasoning foundation MLLM, URSA-8B. Subsequently, we
go through an automatic process to synthesize process supervision data, which
emphasizes both logical correctness and perceptual consistency. We introduce
DualMath-1.1M to facilitate the training of URSA-8B-RM. Finally, we propose
Process-Supervised Group-Relative-Policy-Optimization (PS-GRPO), pioneering a
multimodal PRM-aided online RL method that outperforms vanilla GRPO. With
PS-GRPO application, URSA-8B-PS-GRPO outperforms Gemma3-12B and GPT-4o by 8.4%
and 2.7% on average across 6 benchmarks. Code, data and checkpoint can be found
at https://github.com/URSA-MATH.

</details>


### [211] [EpiCoder: Encompassing Diversity and Complexity in Code Generation](https://arxiv.org/pdf/2501.04694)
*Yaoxiang Wang, Haoling Li, Xin Zhang, Jie Wu, Xiao Liu, Wenxiang Hu, Zhongxin Guo, Yangyu Huang, Ying Xin, Yujiu Yang, Jinsong Su, Qi Chen, Scarlett Li*

Main category: cs.CL

TL;DR: A feature tree-based synthesis framework enhances code generation by using hierarchical code features, improving complexity and diversity, and achieving state-of-the-art performance.


<details>
  <summary>Details</summary>
Motivation: Existing methods limit complexity and diversity by relying on code snippets. This work aims to overcome these limitations.

Method: Constructs and refines a feature tree from raw data, adjusting subtree depth and breadth for controlled complexity. Fine-tunes base models to create EpiCoder.

Result: Achieves state-of-the-art performance on benchmarks, excelling in function, file, and repository-level code synthesis.

Conclusion: The framework significantly advances code generation, offering precise control and high performance across various levels of complexity.

Abstract: Existing methods for code generation use code snippets as seed data,
restricting the complexity and diversity of the synthesized data. In this
paper, we introduce a novel feature tree-based synthesis framework, which
revolves around hierarchical code features derived from high-level abstractions
of code. The feature tree is constructed from raw data and refined iteratively
to increase the quantity and diversity of the extracted features, which
captures and recognizes more complex patterns and relationships within the
code. By adjusting the depth and breadth of the sampled subtrees, our framework
provides precise control over the complexity of the generated code, enabling
functionalities that range from function-level operations to multi-file
scenarios. We fine-tuned widely-used base models to obtain EpiCoder series,
achieving state-of-the-art performance on multiple benchmarks at both the
function and file levels. In particular, empirical evidence indicates that our
approach shows significant potential in the synthesizing of repository-level
code data. Our code and data are publicly available at
https://github.com/microsoft/EpiCoder.

</details>


### [212] [Large Language Models Share Representations of Latent Grammatical Concepts Across Typologically Diverse Languages](https://arxiv.org/pdf/2501.06346)
*Jannik Brinkmann, Chris Wendler, Christian Bartelt, Aaron Mueller*

Main category: cs.CL

TL;DR: LLMs like Llama-3-8B and Aya-23-8B share multilingual representations of grammatical concepts, verified through causal interventions and applied in machine translation.


<details>
  <summary>Details</summary>
Motivation: Explore how LLMs encode and share morphosyntactic concepts (e.g., grammatical number, gender, tense) across languages, even when trained predominantly on English.

Method: Train sparse autoencoders on Llama-3-8B and Aya-23-8B, use causal interventions to verify multilingual features, and apply them in machine translation.

Result: Abstract grammatical concepts are encoded in shared feature directions; ablating multilingual features reduces classifier performance to near-chance.

Conclusion: LLMs can develop robust cross-lingual abstractions of morphosyntactic concepts, even with English-dominated training data.

Abstract: Human bilinguals often use similar brain regions to process multiple
languages, depending on when they learned their second language and their
proficiency. In large language models (LLMs), how are multiple languages
learned and encoded? In this work, we explore the extent to which LLMs share
representations of morphsyntactic concepts such as grammatical number, gender,
and tense across languages. We train sparse autoencoders on Llama-3-8B and
Aya-23-8B, and demonstrate that abstract grammatical concepts are often encoded
in feature directions shared across many languages. We use causal interventions
to verify the multilingual nature of these representations; specifically, we
show that ablating only multilingual features decreases classifier performance
to near-chance across languages. We then use these features to precisely modify
model behavior in a machine translation task; this demonstrates both the
generality and selectivity of these feature's roles in the network. Our
findings suggest that even models trained predominantly on English data can
develop robust, cross-lingual abstractions of morphosyntactic concepts.

</details>


### [213] [TAD-Bench: A Comprehensive Benchmark for Embedding-Based Text Anomaly Detection](https://arxiv.org/pdf/2501.11960)
*Yang Cao, Sikun Yang, Chen Li, Haolong Xiang, Lianyong Qi, Bo Liu, Rongsheng Li, Ming Liu*

Main category: cs.CL

TL;DR: TAD-Bench is a benchmark for evaluating embedding-based text anomaly detection methods, analyzing their effectiveness and generalizability across domains.


<details>
  <summary>Details</summary>
Motivation: The effectiveness and generalizability of embedding-based methods for text anomaly detection are not well understood.

Method: TAD-Bench integrates diverse datasets, state-of-the-art embeddings, and anomaly detection algorithms for systematic evaluation.

Result: Experiments reveal strengths, weaknesses, and task-specific applicability of embedding-detection method combinations.

Conclusion: The findings provide insights for building robust and generalizable anomaly detection systems.

Abstract: Text anomaly detection is crucial for identifying spam, misinformation, and
offensive language in natural language processing tasks. Despite the growing
adoption of embedding-based methods, their effectiveness and generalizability
across diverse application scenarios remain under-explored. To address this, we
present TAD-Bench, a comprehensive benchmark designed to systematically
evaluate embedding-based approaches for text anomaly detection. TAD-Bench
integrates multiple datasets spanning different domains, combining
state-of-the-art embeddings from large language models with a variety of
anomaly detection algorithms. Through extensive experiments, we analyze the
interplay between embeddings and detection methods, uncovering their strengths,
weaknesses, and applicability to different tasks. These findings offer new
perspectives on building more robust, efficient, and generalizable anomaly
detection systems for real-world applications.

</details>


### [214] [Over-Tokenized Transformer: Vocabulary is Generally Worth Scaling](https://arxiv.org/pdf/2501.16975)
*Hongzhi Huang, Defa Zhu, Banggu Wu, Yutao Zeng, Ya Wang, Qiyang Min, Xun Zhou*

Main category: cs.CL

TL;DR: The paper introduces Over-Tokenized Transformers, decoupling input and output vocabularies to improve LLM performance by scaling input vocabularies with multi-gram tokens.


<details>
  <summary>Details</summary>
Motivation: Tokenization's impact on LLM scaling and performance is underexplored, prompting the need for a framework to optimize it.

Method: Proposes Over-Tokenized Transformers, scaling input vocabularies to use multi-gram tokens and analyzing their effect on training loss.

Result: Larger input vocabularies improve performance log-linearly, achieving results comparable to double-sized models without extra cost.

Conclusion: Tokenization is crucial for scaling laws, and optimizing it can lead to more efficient and powerful LLMs.

Abstract: Tokenization is a fundamental component of large language models (LLMs), yet
its influence on model scaling and performance is not fully explored. In this
paper, we introduce Over-Tokenized Transformers, a novel framework that
decouples input and output vocabularies to improve language modeling
performance. Specifically, our approach scales up input vocabularies to
leverage multi-gram tokens. Through extensive experiments, we uncover a
log-linear relationship between input vocabulary size and training loss,
demonstrating that larger input vocabularies consistently enhance model
performance, regardless of model size. Using a large input vocabulary, we
achieve performance comparable to double-sized baselines with no additional
cost. Our findings highlight the importance of tokenization in scaling laws and
provide practical insight for tokenizer design, paving the way for more
efficient and powerful LLMs.

</details>


### [215] [CopySpec: Accelerating LLMs with Speculative Copy-and-Paste Without Compromising Quality](https://arxiv.org/pdf/2502.08923)
*Razvan-Gabriel Dumitru, Minglai Yang, Vikas Yadav, Mihai Surdeanu*

Main category: cs.CL

TL;DR: CopySpec is a technique to speed up LLM responses by reusing repeated sequences from context or chat history, achieving significant speed-ups without extra GPU memory.


<details>
  <summary>Details</summary>
Motivation: Address inefficiencies in LLMs when generating repetitive or context-extractable responses.

Method: Identifies repeated sequences to speculate token reuse, integrates with speculative decoding, and evaluates using seven LLMs and five datasets including the new MT-Redundant.

Result: Achieves speed-ups up to 3.08x on certain tasks and 49% additional speed-up over speculative decoding.

Conclusion: CopySpec efficiently accelerates LLM inference by leveraging repeated sequences, complementing speculative decoding.

Abstract: We introduce CopySpec, a simple yet effective technique to tackle the
inefficiencies LLMs face when generating responses that closely resemble
previous outputs or responses that can be verbatim extracted from context.
CopySpec identifies repeated sequences in the model's chat history or context
and speculates that the same tokens will follow, enabling seamless copying
without compromising output quality and without requiring additional GPU
memory. To evaluate the effectiveness of our approach, we conducted experiments
using seven LLMs and five datasets: MT-Bench, CNN/DM, GSM8K, HumanEval, and our
newly created dataset, MT-Redundant. MT-Redundant, introduced in this paper,
transforms the second turn of MT-Bench into a request for variations of the
first turn's answer, simulating real-world scenarios where users request
modifications to prior responses. Our results demonstrate significant
speed-ups: up to 2.35x on CNN/DM, 3.08x on the second turn of select
MT-Redundant categories, and 2.66x on the third turn of GSM8K's self-correction
tasks. Importantly, we show that CopySpec integrates seamlessly with
speculative decoding, yielding an average 49% additional speed-up over
speculative decoding for the second turn of MT-Redundant across all eight
categories. While LLMs, even with speculative decoding, suffer from slower
inference as context size grows, CopySpec leverages larger contexts to
accelerate inference, making it a faster complementary solution. Our code and
dataset are publicly available at https://github.com/RazvanDu/CopySpec.

</details>


### [216] [Beyond One-Size-Fits-All Pruning via Evolutionary Metric Search for Large Language Models](https://arxiv.org/pdf/2502.10735)
*Shuqi Liu, Bowei He, Han Wu, Linqi Song*

Main category: cs.CL

TL;DR: OptiShear is an adaptive pruning framework for LLMs using evolutionary optimization, outperforming fixed strategies with its Meta pruning metric and model-wise reconstruction error.


<details>
  <summary>Details</summary>
Motivation: Fixed pruning strategies are inadequate for diverse LLMs due to varying weight distributions, necessitating an adaptive approach.

Method: OptiShear employs NSGA-III to optimize pruning metrics and layerwise sparsity ratios, using a Meta pruning metric and model-wise reconstruction error for evaluation.

Result: Outperforms existing methods on LLaMA-1/2/3 and Mistral models (7B-70B) and enhances other pruning metrics with discovered sparsity ratios.

Conclusion: OptiShear offers a cost-effective, generalizable solution for LLM pruning, improving efficiency and adaptability.

Abstract: Post-training pruning has emerged as a crucial optimization technique as
large language models (LLMs) continue to grow rapidly. However, the significant
variations in weight distributions across different LLMs make fixed pruning
strategies inadequate for multiple models. In this paper, we introduce
\textbf{\textsc{OptiShear}}, an efficient evolutionary optimization framework
for adaptive LLM pruning. Our framework features two key innovations: an
effective search space built on our Meta pruning metric to handle diverse
weight distributions, and a model-wise reconstruction error for rapid
evaluation during search trials. We employ Non-dominated Sorting Genetic
Algorithm III (NSGA-III) to optimize both pruning metrics and layerwise
sparsity ratios. Through extensive evaluation on LLaMA-1/2/3 and Mistral models
(7B-70B) across multiple benchmarks, we demonstrate that our adaptive pruning
metrics consistently outperform existing methods. Additionally, our discovered
layerwise sparsity ratios enhance the effectiveness of other pruning metrics.
The framework exhibits strong cross-task and cross-model generalizability,
providing a cost-effective solution for model compression.

</details>


### [217] [1bit-Merging: Dynamic Quantized Merging for Large Language Models](https://arxiv.org/pdf/2502.10743)
*Shuqi Liu, Yuxuan Yao, Bowei He, Zehua Liu, Xiongwei Han, Mingxuan Yuan, Han Wu, Linqi Song*

Main category: cs.CL

TL;DR: 1bit-Merging is a framework combining task-specific routing with 1-bit quantization to merge specialized models efficiently, balancing performance and storage.


<details>
  <summary>Details</summary>
Motivation: Specialized models excel in domains but merging them traditionally compromises performance or increases storage.

Method: Uses 1-bit quantized task vectors and layer-specific compression (attention for chat, MLP for math/code).

Result: Achieves comparable/superior performance to existing methods with reduced storage (tested on LLaMA2, Mistral).

Conclusion: Provides a practical solution for merging models while preserving strengths and addressing storage issues.

Abstract: Recent advances in large language models have led to specialized models
excelling in specific domains, creating a need for efficient model merging
techniques. While traditional merging approaches combine parameters into a
single static model, they often compromise task-specific performance. However,
task-specific routing methods maintain accuracy but introduce substantial
storage overhead. We present \texttt{1bit}-Merging, a novel framework that
integrates task-specific routing with 1-bit quantized task vectors to balance
performance and storage efficiency. Our approach leverages the observation that
different task-specific models store knowledge in distinct layers-chat models
primarily in attention layers and math/code models in MLP layers, enabling
targeted compression strategies. Through extensive experiments with LLaMA2 and
Mistral model families across chat, mathematical reasoning, and code generation
tasks, we demonstrate that 1bit-Merging achieves comparable or superior
performance to existing methods while significantly reducing storage
requirements. Our framework offers a practical solution for combining
specialized models while maintaining their individual strengths and addressing
the storage challenges of current approaches.

</details>


### [218] [LoRE-Merging: Exploring Low-Rank Estimation For Large Language Model Merging](https://arxiv.org/pdf/2502.10749)
*Zehua Liu, Han Wu, Yuxuan Yao, Ruifeng She, Xiongwei Han, Tao Zhong, Mingxuan Yuan*

Main category: cs.CL

TL;DR: The paper introduces LoRE-Merging, a framework for model merging using low-rank estimation of task vectors, eliminating the need for additional training or base model access.


<details>
  <summary>Details</summary>
Motivation: Current methods rely on training techniques like fine-tuning or reinforcement learning, but model merging can improve models without extra training. Task vectors from fine-tuned models often have dominant singular values, making low-rank estimation effective.

Method: The approach formulates merging as an optimization problem using low-rank estimation of task vectors, avoiding interference and preserving task-specific information.

Result: Experiments show LoRE-Merging effectively mitigates interference and enhances performance, advancing model merging techniques.

Conclusion: LoRE-Merging offers a training-free, efficient framework for model merging, improving performance without additional training or base model access.

Abstract: While most current approaches rely on further training techniques, such as
fine-tuning or reinforcement learning, to enhance model capacities, model
merging stands out for its ability of improving models without requiring any
additional training. In this paper, we propose a unified framework for model
merging based on low-rank estimation of task vectors without the need for
access to the base model, named \textsc{LoRE-Merging}. Our approach is
motivated by the observation that task vectors from fine-tuned models
frequently exhibit a limited number of dominant singular values, making
low-rank estimations less prone to interference. We implement the method by
formulating the merging problem as an optimization problem. Extensive empirical
experiments demonstrate the effectiveness of our framework in mitigating
interference and preserving task-specific information, thereby advancing the
state-of-the-art performance in model merging techniques.

</details>


### [219] [Investigating Language Preference of Multilingual RAG Systems](https://arxiv.org/pdf/2502.11175)
*Jeonghyun Park, Hwanhee Lee*

Main category: cs.CL

TL;DR: The paper introduces DKM-RAG to address language preference issues in mRAG systems, improving multilingual response consistency.


<details>
  <summary>Details</summary>
Motivation: mRAG systems face challenges due to linguistic variations and conflicting multilingual sources, leading to inconsistent responses.

Method: The study systematically analyzes language preferences in mRAG and proposes DKM-RAG, which combines translated multilingual passages with model knowledge.

Result: DKM-RAG reduces language bias in generation and improves performance across diverse languages.

Conclusion: DKM-RAG effectively mitigates language preference issues in mRAG systems, enhancing multilingual response quality.

Abstract: Multilingual Retrieval-Augmented Generation (mRAG) systems enhance language
models by integrating external multilingual information to produce
context-aware responses. However, mRAG systems struggle with retrieving
relevant information due to linguistic variations between queries and
documents, generating inconsistent responses when multilingual sources
conflict. In this work, we systematically investigate language preferences in
both retrieval and generation of mRAG through a series of experiments. Our
analysis indicates that retrievers tend to prefer high-resource and query
languages, yet this preference does not consistently improve generation
performance. Moreover, we observe that generators prefer the query language or
Latin scripts, leading to inconsistent outputs. To overcome these issues, we
propose Dual Knowledge Multilingual RAG (DKM-RAG), a simple yet effective
framework that fuses translated multilingual passages with complementary model
knowledge. Empirical results demonstrate that DKM-RAG mitigates language
preference in generation and enhances performance across diverse linguistic
settings.

</details>


### [220] [Vendi-RAG: Adaptively Trading-Off Diversity And Quality Significantly Improves Retrieval Augmented Generation With LLMs](https://arxiv.org/pdf/2502.11228)
*Mohammad Reza Rezaei, Adji Bousso Dieng*

Main category: cs.CL

TL;DR: Vendi-RAG improves multi-hop QA by jointly optimizing retrieval diversity and answer quality using the Vendi Score and an LLM judge, outperforming baselines by up to +4.2% accuracy.


<details>
  <summary>Details</summary>
Motivation: Traditional RAG systems struggle with redundancy and connecting information from multiple sources for multi-hop reasoning tasks.

Method: Vendi-RAG uses an iterative process with the Vendi Score for diverse retrieval and an LLM judge to balance relevance and diversity.

Result: Achieves significant accuracy improvements (up to +4.2%) on HotpotQA, 2WikiMultiHopQA, and MuSiQue datasets.

Conclusion: Vendi-RAG is effective and model-agnostic, showing consistent improvements across different LLM backbones.

Abstract: Retrieval-augmented generation (RAG) enhances large language models (LLMs)
for domain-specific question-answering (QA) tasks by leveraging external
knowledge sources. However, traditional RAG systems primarily focus on
relevance-based retrieval and often struggle with redundancy, especially when
reasoning requires connecting information from multiple sources. This paper
introduces Vendi-RAG, a framework based on an iterative process that jointly
optimizes retrieval diversity and answer quality. This joint optimization leads
to significantly higher accuracy for multi-hop QA tasks. Vendi-RAG leverages
the Vendi Score (VS), a flexible similarity-based diversity metric, to promote
semantic diversity in document retrieval. It then uses an LLM judge that
evaluates candidate answers, generated after a reasoning step, and outputs a
score that the retriever uses to balance relevance and diversity among the
retrieved documents during each iteration. Experiments on three challenging
datasets -- HotpotQA, MuSiQue, and 2WikiMultiHopQA -- demonstrate Vendi-RAG's
effectiveness in multi-hop reasoning tasks. The framework achieves significant
accuracy improvements over traditional single-step and multi-step RAG
approaches, with accuracy increases reaching up to +4.2% on HotpotQA, +4.1% on
2WikiMultiHopQA, and +1.3% on MuSiQue compared to Adaptive-RAG, the current
best baseline. The benefits of Vendi-RAG are even more pronounced as the number
of retrieved documents increases. Finally, we evaluated Vendi-RAG across
different LLM backbones, including GPT-3.5, GPT-4, and GPT-4o-mini, and
observed consistent improvements, demonstrating that the framework's advantages
are model-agnostic.

</details>


### [221] [System Message Generation for User Preferences using Open-Source Models](https://arxiv.org/pdf/2502.11330)
*Minbyul Jeong, Jungho Cho, Minsoo Khang, Dawoon Jung, Teakgyu Hong*

Main category: cs.CL

TL;DR: SysGen is a pipeline for generating system messages to improve LLM responses, addressing the lack of system messages in datasets and manual annotation challenges. It shows significant improvements in conversation benchmarks.


<details>
  <summary>Details</summary>
Motivation: Public datasets lack system messages, and manual annotation is resource-intensive. SysGen aims to generate aligned system messages for better LLM performance.

Method: SysGen uses supervised fine-tuning datasets to generate system messages, improving alignment with user instructions.

Result: Training on SysGen data improves performance in single-turn (Multifacet) and multi-turn (SysBench) benchmarks, especially in shorter conversations.

Conclusion: SysGen enhances LLM adaptability by providing diverse and structured system messages, improving early-stage interaction effectiveness.

Abstract: System messages play a crucial role in interactions with large language
models (LLMs), often serving as prompts to initiate conversations. Through
system messages, users can assign specific roles, perform intended tasks,
incorporate background information, and specify various output formats and
communication styles. Despite such versatility, publicly available datasets
often lack system messages and are subject to strict license constraints in
industrial applications. Moreover, manually annotating system messages that
align with user instructions is resource-intensive. In light of these
challenges, we introduce SysGen, a pipeline for generating system messages that
better align assistant responses with user instructions using existing
supervised fine-tuning datasets that lack system messages. Training open-source
models on SysGen data yields substantial improvements in both single-turn
(Multifacet) and multi-turn (SysBench) conversation benchmarks. Notably, our
method shows strong gains in shorter conversations, suggesting that it enhances
early-stage interaction effectiveness. Our qualitative analysis further
emphasizes the value of diverse and structured system messages in improving LLM
adaptability across varied user scenarios.

</details>


### [222] [Is Human-Like Text Liked by Humans? Multilingual Human Detection and Preference Against AI](https://arxiv.org/pdf/2502.11614)
*Yuxia Wang, Rui Xing, Jonibek Mansurov, Giovanni Puccetti, Zhuohan Xie, Minh Ngoc Ta, Jiahui Geng, Jinyan Su, Mervat Abassy, Saad El Dine Ahmed, Kareem Elozeiri, Nurkhan Laiyk, Maiya Goloburda, Tarek Mahmoud, Raj Vardhan Tomar, Alexander Aziz, Ryuto Koike, Masahiro Kaneko, Artem Shelmanov, Ekaterina Artemova, Vladislav Mikhailov, Akim Tsvigun, Alham Fikri Aji, Nizar Habash, Iryna Gurevych, Preslav Nakov*

Main category: cs.CL

TL;DR: Humans can detect LLM-generated text with 87.6% accuracy across languages and domains, challenging prior beliefs. Key gaps are concreteness, cultural nuances, and diversity. Prompting helps but human preference isn't always clear.


<details>
  <summary>Details</summary>
Motivation: To verify if humans can reliably distinguish LLM-generated text from human-written text across languages and domains.

Method: Extensive case study with 19 annotators across 16 datasets covering 9 languages and 9 domains.

Result: Average detection accuracy of 87.6%, with gaps in concreteness, cultural nuances, and diversity. Prompting helps in 50%+ cases.

Conclusion: Humans can detect LLM text better than random, but preferences aren't always clear. Prompting partially bridges gaps.

Abstract: Prior studies have shown that distinguishing text generated by large language
models (LLMs) from human-written one is highly challenging, and often no better
than random guessing. To verify the generalizability of this finding across
languages and domains, we perform an extensive case study to identify the upper
bound of human detection accuracy. Across 16 datasets covering 9 languages and
9 domains, 19 annotators achieved an average detection accuracy of 87.6\%, thus
challenging previous conclusions. We find that major gaps between human and
machine text lie in concreteness, cultural nuances, and diversity. Prompting by
explicitly explaining the distinctions in the prompts can partially bridge the
gaps in over 50\% of the cases. However, we also find that humans do not always
prefer human-written text, particularly when they cannot clearly identify its
source.

</details>


### [223] [Personality Editing for Language Models through Relevant Knowledge Editing](https://arxiv.org/pdf/2502.11789)
*Seojin Hwang, Yumin Kim, Byeongjeong Kim, Donghoon Shin, Hwanhee Lee*

Main category: cs.CL

TL;DR: PALETTE introduces knowledge editing to improve personality control in LLMs, outperforming traditional prompt-based methods.


<details>
  <summary>Details</summary>
Motivation: Traditional prompt-based techniques fail to mitigate LLM biases effectively, necessitating better personality control methods.

Method: PALETTE uses knowledge editing and psychological assessment-inspired queries to adjust personality traits systematically.

Result: Experiments show PALETTE achieves more stable and balanced personality control in LLMs.

Conclusion: PALETTE offers a novel, effective approach for enhancing personality control in LLMs.

Abstract: Large Language Models (LLMs) play a vital role in applications like
conversational agents and content creation, where controlling a model's
personality is crucial for maintaining tone, consistency, and engagement.
However, traditional prompt-based techniques for controlling personality often
fall short, as they do not effectively mitigate the model's inherent biases. In
this paper, we introduce a novel method PALETTE that enhances personality
control through knowledge editing. By generating adjustment queries inspired by
psychological assessments, our approach systematically adjusts responses to
personality-related queries similar to modifying factual knowledge, thereby
achieving controlled shifts in personality traits. Experimental results from
both automatic and human evaluations demonstrate that our method enables more
stable and well-balanced personality control in LLMs.

</details>


### [224] [PASER: Post-Training Data Selection for Efficient Pruned Large Language Model Recovery](https://arxiv.org/pdf/2502.12594)
*Bowei He, Lihao Yin, Hui-Ling Zhen, Xiaokun Zhang, Mingxuan Yuan, Chen Ma*

Main category: cs.CL

TL;DR: PASER is a post-training data selection method for efficiently recovering pruned LLMs by identifying and prioritizing instructions that address the most degraded model capabilities, using only 4%-20% of original data.


<details>
  <summary>Details</summary>
Motivation: Existing post-training methods for pruned LLMs overlook uneven capability degradation, incur high costs, and may introduce negative effects from irrelevant instructions.

Method: PASER uses manifold learning and spectral clustering to group instructions by semantic similarity, allocates data budget based on capability degradation, and filters conflicting data.

Result: PASER outperforms baselines, recovering pruned LLM capabilities with minimal data (4%-20%).

Conclusion: PASER efficiently recovers pruned LLM performance by focusing on critical instructions and filtering irrelevant data, reducing computational costs.

Abstract: Model pruning is an effective approach for compressing large language models
(LLMs). However, this process often leads to significant degradation of model
capabilities. While post-training techniques such as instruction tuning are
commonly employed to recover model performance, existing methods often overlook
the uneven deterioration of model capabilities and incur high computational
costs. Moreover, some irrelevant instructions may also introduce negative
effects to model capacity recovery. To address these challenges, we propose the
\textbf{P}ost-training d\textbf{A}ta \textbf{S}election method for
\textbf{E}fficient pruned large language model \textbf{R}ecovery
(\textbf{PASER}). PASER aims to identify instructions to recover the most
compromised model capacities with a certain data budget. Our approach first
applies manifold learning and spectral clustering to group recovery
instructions in the semantic space, revealing capability-specific instruction
sets. Then, the data budget is adaptively allocated across clusters by the
degree of corresponding model capability degradation. In each cluster, we
prioritize data samples that lead to the most decline of model performance. To
mitigate potential negative tuning effects, we also detect and filter out
conflicting or irrelevant recovery data. Extensive experiments demonstrate that
PASER significantly outperforms conventional baselines, effectively recovering
the general capabilities of pruned LLMs while utilizing merely 4\%-20\% of the
original post-training data. We provide the anonymous code repository in
\href{https://anonymous.4open.science/r/PASER-E606}{Link}.

</details>


### [225] [None of the Others: a General Technique to Distinguish Reasoning from Memorization in Multiple-Choice LLM Evaluation Benchmarks](https://arxiv.org/pdf/2502.12896)
*Eva Sánchez Salido, Julio Gonzalo, Guillermo Marco*

Main category: cs.CL

TL;DR: The paper introduces a method to evaluate LLMs' reasoning by dissociating correct answers from memorized content, revealing significant accuracy drops and highlighting the role of recall/memorization in current models.


<details>
  <summary>Details</summary>
Motivation: To distinguish reasoning from memorization in LLMs by creating variations in multiple-choice questions that prevent reliance on memorized tokens or concepts.

Method: A general variation method for multiple-choice questions was applied to evaluate LLMs on MMLU and UNED-Access 2024 datasets in English and Spanish.

Result: All models showed significant accuracy drops (average 57% on MMLU, 50% on UNED-Access 2024), with the most accurate model not being the most robust. Larger drops occurred in public datasets and original-language questions, indicating memorization.

Conclusion: Standard evaluations may not reflect reasoning capabilities, and memorization plays a significant role in LLMs' performance, especially in public datasets and original-language questions.

Abstract: In LLM evaluations, reasoning is often distinguished from recall/memorization
by performing numerical variations to math-oriented questions. Here we
introduce a general variation method for multiple-choice questions that
completely dissociates the correct answer from previously seen tokens or
concepts, requiring LLMs to understand and reason (rather than memorizing) in
order to answer correctly. Using this method, we evaluate state-of-the-art
proprietary and open-source LLMs on two datasets available in English and
Spanish: the public MMLU benchmark and the private UNED-Access 2024 dataset.
Results show that all models experience remarkable accuracy drops under our
proposed variation, with an average loss of 57% on MMLU and 50% on UNED-Access
2024, ranging from 10% to 93% across models. Notably, the most accurate model
in our experimentation (OpenAI-o3-mini) is not the most robust
(DeepSeek-R1-70B), suggesting that the best models in standard evaluations may
not be the ones with better reasoning capabilities. Also, we see larger
accuracy drops in public (vs private) datasets and questions posed in their
original language (vs a manual translation), which are signs of contamination
and also point to a relevant role of recall/memorization in current LLMs'
answers.

</details>


### [226] [Edit Once, Update Everywhere: A Simple Framework for Cross-Lingual Knowledge Synchronization in LLMs](https://arxiv.org/pdf/2502.14645)
*Yuchen Wu, Liang Ding, Li Shen, Dacheng Tao*

Main category: cs.CL

TL;DR: X-KDE is a method for cross-lingual knowledge editing in LLMs, improving synchronization across languages with a two-stage approach and a new dataset.


<details>
  <summary>Details</summary>
Motivation: Prior methods lack true cross-linguistic knowledge synchronization, limiting efficient adaptation of LLMs.

Method: X-KDE uses Cross-lingual Edition Instruction Tuning (XE-IT) and Target-language Preference Optimization (TL-PO) to propagate knowledge.

Result: Achieves +8.19% average improvement in cross-lingual performance while maintaining monolingual accuracy.

Conclusion: X-KDE effectively enhances cross-lingual knowledge transfer in LLMs.

Abstract: Knowledge editing allows for efficient adaptation of large language models
(LLMs) to new information or corrections without requiring full retraining.
However, prior methods typically focus on either single-language editing or
basic multilingual editing, failing to achieve true cross-linguistic knowledge
synchronization. To address this, we present a simple and practical
state-of-the-art (SOTA) recipe Cross-Lingual Knowledge Democracy Edit (X-KDE),
designed to propagate knowledge from a dominant language to other languages
effectively. Our X-KDE comprises two stages: (i) Cross-lingual Edition
Instruction Tuning (XE-IT), which fine-tunes the model on a curated parallel
dataset to modify in-scope knowledge while preserving unrelated information,
and (ii) Target-language Preference Optimization (TL-PO), which applies
advanced optimization techniques to ensure consistency across languages,
fostering the transfer of updates. Additionally, we contribute a high-quality,
cross-lingual dataset, specifically designed to enhance knowledge transfer
across languages. Extensive experiments on the Bi-ZsRE and MzsRE benchmarks
show that X-KDE significantly enhances cross-lingual performance, achieving an
average improvement of +8.19%, while maintaining high accuracy in monolingual
settings.

</details>


### [227] [Triangulating LLM Progress through Benchmarks, Games, and Cognitive Tests](https://arxiv.org/pdf/2502.14359)
*Filippo Momentè, Alessandro Suglia, Mario Giulianelli, Ambra Ferrari, Alexander Koller, Oliver Lemon, David Schlangen, Raquel Fernández, Raffaella Bernardi*

Main category: cs.CL

TL;DR: Interactive games outperform standard benchmarks in discriminating LLM quality, with cognitive tests revealing correlations between reasoning skills and model performance.


<details>
  <summary>Details</summary>
Motivation: To determine the most effective evaluation paradigm (benchmarks vs. games) for LLMs and explore correlations between cognitive abilities and model performance.

Method: Compare benchmarks and interactive games for discriminating LLM quality, then compile cognitive tests to measure abilities like reasoning and executive functions.

Result: Interactive games are better discriminators. Reasoning skills correlate with both tests, while executive/social skills align more with games.

Conclusion: Advocates for new interactive benchmarks and cognitive tasks tailored for LLMs, inspired by human assessments.

Abstract: We examine three evaluation paradigms: standard benchmarks (e.g., MMLU and
BBH), interactive games (e.g., Signalling Games or Taboo), and cognitive tests
(e.g., for working memory or theory of mind). First, we investigate which of
the former two-benchmarks or games-is most effective at discriminating LLMs of
varying quality. Then, inspired by human cognitive assessments, we compile a
suite of targeted tests that measure cognitive abilities deemed essential for
effective language use, and we investigate their correlation with model
performance in benchmarks and games. Our analyses reveal that interactive games
are superior to standard benchmarks in discriminating models. Causal and
logical reasoning correlate with both static and interactive tests, while
differences emerge regarding core executive functions and social/emotional
skills, which correlate more with games. We advocate for the development of new
interactive benchmarks and targeted cognitive tasks inspired by assessing human
abilities but designed specifically for LLMs.

</details>


### [228] [Retrieval-Augmented Fine-Tuning With Preference Optimization For Visual Program Generation](https://arxiv.org/pdf/2502.16529)
*Deokhyung Kang, Jeonghun Cho, Yejin Jeon, Sunbin Jang, Minsub Lee, Jawoon Cho, Gary Geunbae Lee*

Main category: cs.CL

TL;DR: Training-based methods outperform prompting-based methods for generating Ladder Diagram (LD) code, with a two-stage strategy (retrieval-augmented fine-tuning and DPO) improving accuracy by over 10%.


<details>
  <summary>Details</summary>
Motivation: Industrial VPLs like LD involve domain-specific complexities not easily captured by prompting-based methods, necessitating more effective training approaches.

Method: A two-stage training strategy: retrieval-augmented fine-tuning for subroutine reuse and DPO for refining outputs using preference pairs from graph editing.

Result: Program-level accuracy improves by over 10% compared to supervised fine-tuning.

Conclusion: The proposed method advances industrial automation by enhancing LD generation accuracy.

Abstract: Visual programming languages (VPLs) allow users to create programs through
graphical interfaces, which results in easier accessibility and their
widespread usage in various domains. To further enhance this accessibility,
recent research has focused on generating VPL code from user instructions using
large language models (LLMs). Specifically, by employing prompting-based
methods, these studies have shown promising results. Nevertheless, such
approaches can be less effective for industrial VPLs such as Ladder Diagram
(LD). LD is a pivotal language used in industrial automation processes and
involves extensive domain-specific configurations, which are difficult to
capture in a single prompt. In this work, we demonstrate that training-based
methods outperform prompting-based methods for LD generation accuracy, even
with smaller backbone models. Building on these findings, we propose a
two-stage training strategy to further enhance VPL generation. First, we employ
retrieval-augmented fine-tuning to leverage the repetitive use of subroutines
commonly seen in industrial VPLs. Second, we apply direct preference
optimization (DPO) to further guide the model toward accurate outputs, using
systematically generated preference pairs through graph editing operations.
Extensive experiments on real-world LD data demonstrate that our approach
improves program-level accuracy by over 10% compared to supervised fine-tuning,
which highlights its potential to advance industrial automation.

</details>


### [229] [ICA-RAG: Information Completeness Guided Adaptive Retrieval-Augmented Generation for Disease Diagnosis](https://arxiv.org/pdf/2502.14614)
*Mingyi Jia, Zhihao Jia, Junwen Duan, Yan Song, Jianxin Wang*

Main category: cs.CL

TL;DR: ICA-RAG improves RAG for medical diagnosis by adaptively controlling retrieval based on input informativeness, enhancing efficiency and accuracy.


<details>
  <summary>Details</summary>
Motivation: Existing RAG methods in medical domains often retrieve excessively, harming efficiency and accuracy by introducing noise.

Method: ICA-RAG uses an adaptive control module to assess input completeness and optimize retrieval, incorporating knowledge filtering.

Result: ICA-RAG outperforms baselines on three Chinese medical datasets, proving its clinical effectiveness.

Conclusion: ICA-RAG enhances RAG reliability in disease diagnosis by aligning retrieval with clinical needs.

Abstract: Retrieval-Augmented Large Language Models~(LLMs), which integrate external
knowledge, have shown remarkable performance in medical domains, including
clinical diagnosis. However, existing RAG methods often struggle to tailor
retrieval strategies to diagnostic difficulty and input sample informativeness.
This limitation leads to excessive and often unnecessary retrieval, impairing
computational efficiency and increasing the risk of introducing noise that can
degrade diagnostic accuracy. To address this, we propose ICA-RAG
(\textbf{I}nformation \textbf{C}ompleteness Guided \textbf{A}daptive
\textbf{R}etrieval-\textbf{A}ugmented \textbf{G}eneration), a novel framework
for enhancing RAG reliability in disease diagnosis. ICA-RAG utilizes an
adaptive control module to assess the necessity of retrieval based on the
input's information completeness. By optimizing retrieval and incorporating
knowledge filtering, ICA-RAG better aligns retrieval operations with clinical
requirements. Experiments on three Chinese electronic medical record datasets
demonstrate that ICA-RAG significantly outperforms baseline methods,
highlighting its effectiveness in clinical diagnosis.

</details>


### [230] [Unveiling Downstream Performance Scaling of LLMs: A Clustering-Based Perspective](https://arxiv.org/pdf/2502.17262)
*Chengyin Xu, Kaiyuan Chen, Xiao Li, Ke Shen, Chenggang Li*

Main category: cs.CL

TL;DR: The paper proposes a Clustering-On-Difficulty (COD) framework to predict downstream task performance for LLMs, addressing challenges like emergence and uneven task difficulty. It achieves a 1.36% prediction error.


<details>
  <summary>Details</summary>
Motivation: Accurate pre-training prediction of downstream task performance is needed due to the high cost of LLM training, but current methods are unreliable due to emergence and task variability.

Method: The COD framework clusters tasks by difficulty scaling features, excludes non-emergent or irregular tasks, and uses a performance scaling law for prediction. A mapping function extrapolates subset performance to the full set.

Result: Applied to a 70B-parameter LLM, COD achieved a 1.36% average prediction error across eight benchmarks.

Conclusion: COD provides reliable performance prediction for LLMs, aiding resource allocation and training monitoring.

Abstract: The escalating scale and cost of Large Language Models (LLMs) training
necessitate accurate pre-training prediction of downstream task performance for
efficient resource allocation. This is challenged by: 1) the emergence
phenomenon, where metrics become meaningful only after extensive training,
hindering prediction by smaller models; and 2) uneven task difficulty and
inconsistent performance scaling patterns, leading to high metric variability.
Current prediction methods lack accuracy and reliability. We propose a
Clustering-On-Difficulty (COD) framework for downstream performance prediction.
The COD framework clusters tasks by their difficulty scaling features, thereby
establishing a more stable and predictable support subset through the exclusion
of tasks exhibiting non-emergent behavior or irregular scaling. We adopt a
performance scaling law to predict cluster-wise performance with theoretical
support. Predictable subset performance acts as an intermediate predictor for
the full evaluation set. We further derive a mapping function to accurately
extrapolate the performance of the subset to the full set. Applied to an LLM
with 70B parameters, COD achieved a 1.36% average prediction error across eight
key LLM benchmarks, offering actionable insights for resource allocation and
training monitoring of LLMs pretraining.

</details>


### [231] [SafeInt: Shielding Large Language Models from Jailbreak Attacks via Safety-Aware Representation Intervention](https://arxiv.org/pdf/2502.15594)
*Jiaqi Wu, Chen Chen, Chunyan Hou, Xiaojie Yuan*

Main category: cs.CL

TL;DR: SafeInt is a defense method for LLMs against jailbreak attacks by dynamically adjusting representations based on query harmfulness, outperforming baselines while maintaining utility.


<details>
  <summary>Details</summary>
Motivation: Ensuring LLM safety against jailbreak attacks is critical, as existing defenses lack effectiveness and efficiency.

Method: SafeInt relocates jailbreak-related representations into the rejection region by intervening in their distributions to align with unsafe samples.

Result: SafeInt outperforms baselines in defending against six jailbreak attacks and maintains utility, even against adaptive attacks.

Conclusion: SafeInt effectively mitigates jailbreak attacks dynamically, enhancing LLM safety without compromising utility.

Abstract: With the widespread real-world deployment of large language models (LLMs),
ensuring their behavior complies with safety standards has become crucial.
Jailbreak attacks exploit vulnerabilities in LLMs to induce undesirable
behavior, posing a significant threat to LLM safety. Previous defenses often
fail to achieve both effectiveness and efficiency simultaneously. Defenses from
a representation perspective offer new insights, but existing interventions
cannot dynamically adjust representations based on the harmfulness of the
queries. To address this limitation, we propose SafeIntervention (SafeInt), a
novel defense method that shields LLMs from jailbreak attacks through
safety-aware representation intervention. Built on our analysis of the
representations of jailbreak samples, the core idea of SafeInt is to relocate
jailbreak-related representations into the rejection region. This is achieved
by intervening in the representation distributions of jailbreak samples to
align them with those of unsafe samples. We conduct comprehensive experiments
covering six jailbreak attacks, two jailbreak datasets, and two utility
benchmarks. Experimental results demonstrate that SafeInt outperforms all
baselines in defending LLMs against jailbreak attacks while largely maintaining
utility. Additionally, we evaluate SafeInt against adaptive attacks and verify
its effectiveness in mitigating real-time attacks.

</details>


### [232] [PrivaCI-Bench: Evaluating Privacy with Contextual Integrity and Legal Compliance](https://arxiv.org/pdf/2502.17041)
*Haoran Li, Wenbin Hu, Huihao Jing, Yulin Chen, Qi Hu, Sirui Han, Tianshu Chu, Peizhao Hu, Yangqiu Song*

Main category: cs.CL

TL;DR: The paper introduces PrivaCI-Bench, a contextual privacy evaluation benchmark for LLMs, highlighting gaps in current privacy evaluations and showing LLMs' partial compliance with privacy standards.


<details>
  <summary>Details</summary>
Motivation: Current privacy evaluations for LLMs are narrow, focusing only on PII, and lack contextual integrity. The paper aims to address this gap by incorporating broader privacy contexts.

Method: The authors develop PrivaCI-Bench, a benchmark covering legal compliance, real cases, policies, and synthetic data, and evaluate LLMs like QwQ-32B and Deepseek R1.

Result: LLMs capture key contextual privacy parameters but still fall short in full compliance with privacy standards.

Conclusion: Further advancements are needed for LLMs to fully meet contextual privacy compliance, as current models show limitations despite progress.

Abstract: Recent advancements in generative large language models (LLMs) have enabled
wider applicability, accessibility, and flexibility. However, their reliability
and trustworthiness are still in doubt, especially for concerns regarding
individuals' data privacy. Great efforts have been made on privacy by building
various evaluation benchmarks to study LLMs' privacy awareness and robustness
from their generated outputs to their hidden representations. Unfortunately,
most of these works adopt a narrow formulation of privacy and only investigate
personally identifiable information (PII). In this paper, we follow the merit
of the Contextual Integrity (CI) theory, which posits that privacy evaluation
should not only cover the transmitted attributes but also encompass the whole
relevant social context through private information flows. We present
PrivaCI-Bench, a comprehensive contextual privacy evaluation benchmark targeted
at legal compliance to cover well-annotated privacy and safety regulations,
real court cases, privacy policies, and synthetic data built from the official
toolkit to study LLMs' privacy and safety compliance. We evaluate the latest
LLMs, including the recent reasoner models QwQ-32B and Deepseek R1. Our
experimental results suggest that though LLMs can effectively capture key CI
parameters inside a given context, they still require further advancements for
privacy compliance.

</details>


### [233] [Is Your Paper Being Reviewed by an LLM? Benchmarking AI Text Detection in Peer Review](https://arxiv.org/pdf/2502.19614)
*Sungduk Yu, Man Luo, Avinash Madusu, Vasudev Lal, Phillip Howard*

Main category: cs.CL

TL;DR: The paper introduces a dataset to benchmark AI text detection in peer reviews, evaluates existing detection methods, and proposes a context-aware approach, revealing challenges in identifying AI-generated reviews.


<details>
  <summary>Details</summary>
Motivation: The rise of LLMs poses a risk to peer review integrity, as negligent reviewers might use AI for reviews, but there's no benchmark for detecting such misuse.

Method: A dataset of 788,984 AI and human peer reviews is created. 18 AI detection algorithms are tested, and a context-aware method (Anchor) is explored.

Result: Detection of AI-generated reviews at the individual level is difficult, emphasizing the need for better tools.

Conclusion: The study underscores the urgency for new methods to detect unethical AI use in peer reviews and provides a public dataset for further research.

Abstract: Peer review is a critical process for ensuring the integrity of published
scientific research. Confidence in this process is predicated on the assumption
that experts in the relevant domain give careful consideration to the merits of
manuscripts which are submitted for publication. With the recent rapid
advancements in large language models (LLMs), a new risk to the peer review
process is that negligent reviewers will rely on LLMs to perform the often time
consuming process of reviewing a paper. However, there is a lack of existing
resources for benchmarking the detectability of AI text in the domain of peer
review. To address this deficiency, we introduce a comprehensive dataset
containing a total of 788,984 AI-written peer reviews paired with corresponding
human reviews, covering 8 years of papers submitted to each of two leading AI
research conferences (ICLR and NeurIPS). We use this new resource to evaluate
the ability of 18 existing AI text detection algorithms to distinguish between
peer reviews fully written by humans and different state-of-the-art LLMs.
Additionally, we explore a context-aware detection method called Anchor, which
leverages manuscript content to detect AI-generated reviews, and analyze the
sensitivity of detection models to LLM-assisted editing of human-written text.
Our work reveals the difficulty of identifying AI-generated text at the
individual peer review level, highlighting the urgent need for new tools and
methods to detect this unethical use of generative AI. Our dataset is publicly
available at:
https://huggingface.co/datasets/IntelLabs/AI-Peer-Review-Detection-Benchmark.

</details>


### [234] [Mind the Blind Spots: A Focus-Level Evaluation Framework for LLM Reviews](https://arxiv.org/pdf/2502.17086)
*Hyungyu Shin, Jingyu Tang, Yoonjoo Lee, Nayoung Kim, Hyunseung Lim, Ji Yong Cho, Hwajung Hong, Moontae Lee, Juho Kim*

Main category: cs.CL

TL;DR: The paper evaluates LLM-generated peer reviews by comparing their focus on key facets (e.g., validity, novelty) to human experts, revealing biases in LLMs toward technical validity and away from novelty.


<details>
  <summary>Details</summary>
Motivation: Peer review is strained by shortages and workloads, and while LLMs can draft reviews, their trustworthiness in addressing critical facets (strengths/weaknesses) like humans is unclear.

Method: A focus-level evaluation framework was developed, using predefined facets (target and aspect) and analyzing 676 paper reviews with 3,657 strengths/weaknesses from human experts.

Result: Off-the-shelf LLMs exhibit a biased focus on technical validity and neglect novelty assessment compared to human experts.

Conclusion: LLM-generated reviews need improvement to match human expertise in assessing critical facets like novelty.

Abstract: Peer review underpins scientific progress, but it is increasingly strained by
reviewer shortages and growing workloads. Large Language Models (LLMs) can
automatically draft reviews now, but determining whether LLM-generated reviews
are trustworthy requires systematic evaluation. Researchers have evaluated LLM
reviews at either surface-level (e.g., BLEU and ROUGE) or content-level (e.g.,
specificity and factual accuracy). Yet it remains uncertain whether
LLM-generated reviews attend to the same critical facets that human experts
weigh -- the strengths and weaknesses that ultimately drive an accept-or-reject
decision. We introduce a focus-level evaluation framework that operationalizes
the focus as a normalized distribution of attention across predefined facets in
paper reviews. Based on the framework, we developed an automatic focus-level
evaluation pipeline based on two sets of facets: target (e.g., problem, method,
and experiment) and aspect (e.g., validity, clarity, and novelty), leveraging
676 paper reviews (https://figshare.com/s/d5adf26c802527dd0f62) from OpenReview
that consists of 3,657 strengths and weaknesses identified from human experts.
The comparison of focus distributions between LLMs and human experts showed
that the off-the-shelf LLMs consistently have a more biased focus towards
examining technical validity while significantly overlooking novelty assessment
when criticizing papers.

</details>


### [235] [Do Retrieval-Augmented Language Models Adapt to Varying User Needs?](https://arxiv.org/pdf/2502.19779)
*Peilin Wu, Xinlu Zhang, Wenhao Yu, Xingyu Liu, Xinya Du, Zhiyu Zoey Chen*

Main category: cs.CL

TL;DR: The paper introduces a user-centric evaluation framework for Retrieval-Augmented Language Models (RALMs) to assess performance under diverse user needs and context settings, revealing trade-offs between robustness and peak performance.


<details>
  <summary>Details</summary>
Motivation: Existing benchmarks for RALMs assume a single optimal approach, ignoring varied user needs. This paper addresses this gap by proposing a framework that evaluates models under diverse scenarios.

Method: The framework assesses RALMs under three user need cases (Context-Exclusive, Context-First, Memory-First) and three context settings (Context Matching, Knowledge Conflict, Information Irrelevant). Experiments are conducted on QA datasets like HotpotQA, DisentQA, and a synthetic URAQ dataset.

Result: Restricting memory usage enhances robustness in adversarial conditions but reduces peak performance. Model family significantly influences behavioral differences.

Conclusion: User-centric evaluations are crucial for RALMs. The study provides insights for optimizing performance across varied retrieval contexts and releases the URAQ dataset and code.

Abstract: Recent advancements in Retrieval-Augmented Language Models (RALMs) have
demonstrated their efficacy in knowledge-intensive tasks. However, existing
evaluation benchmarks often assume a single optimal approach to leveraging
retrieved information, failing to account for varying user needs. This paper
introduces a novel evaluation framework that systematically assesses RALMs
under three user need cases-Context-Exclusive, Context-First, and
Memory-First-across three distinct context settings: Context Matching,
Knowledge Conflict, and Information Irrelevant. By varying both user
instructions and the nature of retrieved information, our approach captures the
complexities of real-world applications where models must adapt to diverse user
requirements. Through extensive experiments on multiple QA datasets, including
HotpotQA, DisentQA, and our newly constructed synthetic URAQ dataset, we find
that restricting memory usage improves robustness in adversarial retrieval
conditions but decreases peak performance with ideal retrieval results and
model family dominates behavioral differences. Our findings highlight the
necessity of user-centric evaluations in the development of retrieval-augmented
systems and provide insights into optimizing model performance across varied
retrieval contexts. We will release our code and URAQ dataset upon acceptance
of the paper.

</details>


### [236] [HoT: Highlighted Chain of Thought for Referencing Supporting Facts from Inputs](https://arxiv.org/pdf/2503.02003)
*Tin Nguyen, Logan Bolton, Mohammad Reza Taesiri, Anh Totti Nguyen*

Main category: cs.CL

TL;DR: Highlighted Chain-of-Thought Prompting (HoT) improves LLM response accuracy by tagging key facts, outperforming vanilla CoT in few-shot settings, though it may mislead users when LLMs are wrong.


<details>
  <summary>Details</summary>
Motivation: Addressing LLMs' tendency to hallucinate non-factual statements, which complicates human verification and decision-making.

Method: Proposes HoT, a prompting technique where LLMs tag key facts in queries and responses using XML, grounding facts for clarity.

Result: HoT outperforms CoT on 17 tasks (arithmetic, reading comprehension, logic) and aids human verification efficiency. However, it can mislead users when LLMs are incorrect.

Conclusion: HoT enhances LLM response grounding and verification but requires caution due to potential user misguidance in cases of LLM errors.

Abstract: An Achilles heel of Large Language Models (LLMs) is their tendency to
hallucinate non-factual statements. A response mixed of factual and non-factual
statements poses a challenge for humans to verify and accurately base their
decisions on. To combat this problem, we propose Highlighted Chain-of-Thought
Prompting (HoT), a technique for prompting LLMs to generate responses with XML
tags that ground facts to those provided in the query. That is, given an input
question, LLMs would first re-format the question to add XML tags highlighting
key facts, and then, generate a response with highlights over the facts
referenced from the input. Interestingly, in few-shot settings, HoT outperforms
vanilla chain of thought prompting (CoT) on a wide range of 17 tasks from
arithmetic, reading comprehension to logical reasoning. When asking humans to
verify LLM responses, highlights help time-limited participants to more
accurately and efficiently recognize when LLMs are correct. Yet, surprisingly,
when LLMs are wrong, HoTs tend to make users believe that an answer is correct.

</details>


### [237] [Rewarding Doubt: A Reinforcement Learning Approach to Calibrated Confidence Expression of Large Language Models](https://arxiv.org/pdf/2503.02623)
*Paul Stangel, David Bani-Harouni, Chantal Pellegrini, Ege Özsoy, Kamilia Zaripova, Matthias Keicher, Nassir Navab*

Main category: cs.CL

TL;DR: A Reinforcement Learning method is proposed to fine-tune LLMs for calibrated confidence estimates alongside answers, improving calibration and generalization.


<details>
  <summary>Details</summary>
Motivation: Ensuring accurate confidence expressions in LLMs for safe and trustworthy use.

Method: Reinforcement Learning optimizing a logarithmic scoring rule to penalize over- and under-confidence, integrating calibration into generative processes.

Result: Substantially improved calibration and generalization to unseen tasks without further fine-tuning.

Conclusion: The approach enables general confidence awareness in LLMs, with publicly available code upon acceptance.

Abstract: A safe and trustworthy use of Large Language Models (LLMs) requires an
accurate expression of confidence in their answers. We propose a novel
Reinforcement Learning approach that allows to directly fine-tune LLMs to
express calibrated confidence estimates alongside their answers to factual
questions. Our method optimizes a reward based on the logarithmic scoring rule,
explicitly penalizing both over- and under-confidence. This encourages the
model to align its confidence estimates with the actual predictive accuracy.
The optimal policy under our reward design would result in perfectly calibrated
confidence expressions. Unlike prior approaches that decouple confidence
estimation from response generation, our method integrates confidence
calibration seamlessly into the generative process of the LLM. Empirically, we
demonstrate that models trained with our approach exhibit substantially
improved calibration and generalize to unseen tasks without further
fine-tuning, suggesting the emergence of general confidence awareness. We
provide our training and evaluation code in the supplementary and will make it
publicly available upon acceptance.

</details>


### [238] [Compositional Causal Reasoning Evaluation in Language Models](https://arxiv.org/pdf/2503.04556)
*Jacqueline R. M. A. Maasch, Alihan Hüyük, Xinnuo Xu, Aditya V. Nori, Javier Gonzalez*

Main category: cs.CL

TL;DR: The paper introduces compositional causal reasoning (CCR) as a unified framework to evaluate AI's ability to combine causal and compositional reasoning, testing it on language models like LLama, Phi, and GPT.


<details>
  <summary>Details</summary>
Motivation: To address the need for principled evaluation methods for causal and compositional reasoning in AI.

Method: Developed a framework for evaluating CCR, focusing on the average treatment effect and the probability of necessity and sufficiency, and tested it on language models.

Result: Revealed distinct error patterns in models, with CCR errors increasing with causal path complexity for most models except one.

Conclusion: The CCR framework effectively evaluates AI models' causal-compositional reasoning, highlighting performance gaps.

Abstract: Causal reasoning and compositional reasoning are two core aspirations in AI.
Measuring the extent of these behaviors requires principled evaluation methods.
We explore a unified perspective that considers both behaviors simultaneously,
termed compositional causal reasoning (CCR): the ability to infer how causal
measures compose and, equivalently, how causal quantities propagate through
graphs. We instantiate a framework for the systematic evaluation of CCR for the
average treatment effect and the probability of necessity and sufficiency. As
proof of concept, we demonstrate CCR evaluation for language models in the
LLama, Phi, and GPT families. On a math word problem, our framework revealed a
range of taxonomically distinct error patterns. CCR errors increased with the
complexity of causal paths for all models except o1.

</details>


### [239] [SEOE: A Scalable and Reliable Semantic Evaluation Framework for Open Domain Event Detection](https://arxiv.org/pdf/2503.03303)
*Yi-Fan Lu, Xian-Ling Mao, Tian Lan, Tong Zhang, Yu-Shi Zhu, Heyan Huang*

Main category: cs.CL

TL;DR: The paper proposes SEOE, a semantic-level evaluation framework for Open Domain Event Detection (ODED), addressing limitations in current benchmarks and metrics by using scalable benchmarks and LLM-based semantic evaluation.


<details>
  <summary>Details</summary>
Motivation: Current ODED evaluation methods lack representativeness and semantic understanding due to limited benchmarks and token-level metrics.

Method: Constructs a scalable benchmark with 564 event types across 7 domains and uses LLMs for semantic F1-score evaluation.

Result: Validates benchmark representativeness and metric reliability, analyzing ODED methods and prediction errors.

Conclusion: SEOE improves ODED evaluation with scalable benchmarks and semantic metrics, offering insights for future research.

Abstract: Automatic evaluation for Open Domain Event Detection (ODED) is a highly
challenging task, because ODED is characterized by a vast diversity of
un-constrained output labels from various domains. Nearly all existing
evaluation methods for ODED usually first construct evaluation benchmarks with
limited labels and domain coverage, and then evaluate ODED methods using
metrics based on token-level label matching rules. However, this kind of
evaluation framework faces two issues: (1) The limited evaluation benchmarks
lack representatives of the real world, making it difficult to accurately
reflect the performance of various ODED methods in real-world scenarios; (2)
Evaluation metrics based on token-level matching rules fail to capture semantic
similarity between predictions and golden labels. To address these two problems
above, we propose a scalable and reliable Semantic-level Evaluation framework
for Open domain Event detection (SEOE) by constructing a more representative
evaluation benchmark and introducing a semantic evaluation metric.
Specifically, our proposed framework first constructs a scalable evaluation
benchmark that currently includes 564 event types covering 7 major domains,
with a cost-effective supplementary annotation strategy to ensure the
benchmark's representativeness. The strategy also allows for the supplement of
new event types and domains in the future. Then, the proposed SEOE leverages
large language models (LLMs) as automatic evaluation agents to compute a
semantic F1-score, incorporating fine-grained definitions of semantically
similar labels to enhance the reliability of the evaluation. Extensive
experiments validate the representatives of the benchmark and the reliability
of the semantic evaluation metric. Existing ODED methods are thoroughly
evaluated, and the error patterns of predictions are analyzed, revealing
several insightful findings.

</details>


### [240] [HICD: Hallucination-Inducing via Attention Dispersion for Contrastive Decoding to Mitigate Hallucinations in Large Language Models](https://arxiv.org/pdf/2503.12908)
*Xinyan Jiang, Hang Ye, Yongxin Zhu, Xiaoying Zheng, Zikang Chen, Jun Gong*

Main category: cs.CL

TL;DR: HICD introduces a method to induce hallucinations in LLMs for contrastive decoding, improving contextual faithfulness and factuality.


<details>
  <summary>Details</summary>
Motivation: Addressing the issue of hallucinations in LLM outputs by creating controlled hallucinations for better contrastive decoding.

Method: Selects inducing attention heads, disperses their attention to create hallucinations, and compares with original outputs.

Result: Significant performance improvements in contextual faithfulness and factuality tasks.

Conclusion: Controlled hallucination induction enhances LLM performance across various tasks.

Abstract: Large Language Models (LLMs) often generate hallucinations, producing outputs
that are contextually inaccurate or factually incorrect. We introduce HICD, a
novel method designed to induce hallucinations for contrastive decoding to
mitigate hallucinations. Unlike existing contrastive decoding methods, HICD
selects attention heads crucial to the model's prediction as inducing heads,
then induces hallucinations by dispersing attention of these inducing heads and
compares the hallucinated outputs with the original outputs to obtain the final
result. Our approach significantly improves performance on tasks requiring
contextual faithfulness, such as context completion, reading comprehension, and
question answering. It also improves factuality in tasks requiring accurate
knowledge recall. We demonstrate that our inducing heads selection and
attention dispersion method leads to more "contrast-effective" hallucinations
for contrastive decoding, outperforming other hallucination-inducing methods.
Our findings provide a promising strategy for reducing hallucinations by
inducing hallucinations in a controlled manner, enhancing the performance of
LLMs in a wide range of tasks.

</details>


### [241] [MedPlan:A Two-Stage RAG-Based System for Personalized Medical Plan Generation](https://arxiv.org/pdf/2503.17900)
*Hsin-Ling Hsu, Cong-Tinh Dao, Luning Wang, Zitao Shuai, Thao Nguyen Minh Phan, Jun-En Ding, Chun-Chieh Liao, Pengfei Hu, Xiaoxue Han, Chih-Ho Hsu, Dongsheng Luo, Wen-Chih Peng, Feng Liu, Fang-Ming Hung, Chenwei Wu*

Main category: cs.CL

TL;DR: The paper introduces a novel framework, \ours{}, to improve treatment planning in EHRs by aligning LLM reasoning with clinician workflows, addressing limitations like lack of sequential reasoning and patient-specific context.


<details>
  <summary>Details</summary>
Motivation: Current LLM-based EHR systems focus on assessment over treatment planning, lack sequential reasoning, ignore patient history, and fail to distinguish subjective/objective data.

Method: A two-stage architecture: first generates a clinical assessment, then formulates a structured treatment plan using retrieval-augmented generation for patient-specific context.

Result: Outperforms baselines in assessment accuracy and treatment plan quality.

Conclusion: The framework effectively bridges the gap between LLMs and clinician workflows, enhancing EHR treatment planning.

Abstract: Despite recent success in applying large language models (LLMs) to electronic
health records (EHR), most systems focus primarily on assessment rather than
treatment planning. We identify three critical limitations in current
approaches: they generate treatment plans in a single pass rather than
following the sequential reasoning process used by clinicians; they rarely
incorporate patient-specific historical context; and they fail to effectively
distinguish between subjective and objective clinical information. Motivated by
the SOAP methodology (Subjective, Objective, Assessment, Plan), we introduce
\ours{}, a novel framework that structures LLM reasoning to align with
real-life clinician workflows. Our approach employs a two-stage architecture
that first generates a clinical assessment based on patient symptoms and
objective data, then formulates a structured treatment plan informed by this
assessment and enriched with patient-specific information through
retrieval-augmented generation. Comprehensive evaluation demonstrates that our
method significantly outperforms baseline approaches in both assessment
accuracy and treatment plan quality.

</details>


### [242] [A Retrieval-Based Approach to Medical Procedure Matching in Romanian](https://arxiv.org/pdf/2503.20556)
*Andrei Niculae, Adrian Cosma, Emilian Radoi*

Main category: cs.CL

TL;DR: The paper proposes a retrieval-based method using sentence embeddings to automate the mapping of medical procedure names in Romanian healthcare, addressing challenges in low-resource languages.


<details>
  <summary>Details</summary>
Motivation: Manual mapping of medical procedure names is inefficient and error-prone, especially in underrepresented languages like Romanian, where domain-specific language models are lacking.

Method: A retrieval-based architecture leveraging sentence embeddings is proposed, evaluating Romanian, multilingual, and medical-domain-specific embedding models.

Result: The study identifies the most effective embedding model for medical name matching in Romanian, contributing to medical NLP for low-resource languages.

Conclusion: The proposed method offers a scalable solution for automating medical procedure name mapping in Romanian healthcare, with broader implications for similar languages.

Abstract: Accurately mapping medical procedure names from healthcare providers to
standardized terminology used by insurance companies is a crucial yet complex
task. Inconsistencies in naming conventions lead to missclasified procedures,
causing administrative inefficiencies and insurance claim problems in private
healthcare settings. Many companies still use human resources for manual
mapping, while there is a clear opportunity for automation. This paper proposes
a retrieval-based architecture leveraging sentence embeddings for medical name
matching in the Romanian healthcare system. This challenge is significantly
more difficult in underrepresented languages such as Romanian, where existing
pretrained language models lack domain-specific adaptation to medical text. We
evaluate multiple embedding models, including Romanian, multilingual, and
medical-domain-specific representations, to identify the most effective
solution for this task. Our findings contribute to the broader field of medical
NLP for low-resource languages such as Romanian.

</details>


### [243] [Unlocking Efficient Long-to-Short LLM Reasoning with Model Merging](https://arxiv.org/pdf/2503.20641)
*Han Wu, Yuxuan Yao, Shuqi Liu, Zehua Liu, Xiaojin Fu, Xiongwei Han, Xing Li, Hui-Ling Zhen, Tao Zhong, Mingxuan Yuan*

Main category: cs.CL

TL;DR: Model merging balances efficiency and reasoning depth in LLMs, reducing response length by 55% while maintaining performance.


<details>
  <summary>Details</summary>
Motivation: Address the inefficiency of System 2 reasoning in LLMs, which often overthink without proportional output improvements.

Method: Empirical study on model merging (task-vector-based, SVD-based, activation-informed) for Long-to-Short (L2S) reasoning.

Result: Model merging reduces response length by up to 55%, preserves performance, and scales well with model size.

Conclusion: Model merging is an efficient, effective solution for L2S reasoning, mitigating overthinking while retaining robustness.

Abstract: The transition from System 1 to System 2 reasoning in large language models
(LLMs) has marked significant advancements in handling complex tasks through
deliberate, iterative thinking. However, this progress often comes at the cost
of efficiency, as models tend to overthink, generating redundant reasoning
steps without proportional improvements in output quality. Long-to-Short (L2S)
reasoning has emerged as a promising solution to this challenge, aiming to
balance reasoning depth with practical efficiency. While existing approaches,
such as supervised fine-tuning (SFT), reinforcement learning (RL), and prompt
engineering, have shown potential, they are either computationally expensive or
unstable. Model merging, on the other hand, offers a cost-effective and robust
alternative by integrating the quick-thinking capabilities of System 1 models
with the methodical reasoning of System 2 models. In this work, we present a
comprehensive empirical study on model merging for L2S reasoning, exploring
diverse methodologies, including task-vector-based, SVD-based, and
activation-informed merging. Our experiments reveal that model merging can
reduce average response length by up to 55% while preserving or even improving
baseline performance. We also identify a strong correlation between model scale
and merging efficacy with extensive evaluations on 1.5B/7B/14B/32B models.
Furthermore, we investigate the merged model's ability to self-critique and
self-correct, as well as its adaptive response length based on task complexity.
Our findings highlight model merging as a highly efficient and effective
paradigm for L2S reasoning, offering a practical solution to the overthinking
problem while maintaining the robustness of System 2 reasoning. This work can
be found on Github https://github.com/hahahawu/Long-to-Short-via-Model-Merging.

</details>


### [244] [Cognitive Debiasing Large Language Models for Decision-Making](https://arxiv.org/pdf/2504.04141)
*Yougang Lyu, Shijie Ren, Yue Feng, Zihan Wang, Zhumin Chen, Zhaochun Ren, Maarten de Rijke*

Main category: cs.CL

TL;DR: The paper introduces SACD, a self-adaptive cognitive debiasing method for LLMs, outperforming existing techniques in mitigating biases in decision-making tasks.


<details>
  <summary>Details</summary>
Motivation: Cognitive biases in LLMs hinder reliable decision-making, especially in complex scenarios with multiple biases. Current methods fail to address multi-bias settings effectively.

Method: SACD involves three steps: bias determination, bias analysis, and cognitive debiasing, iteratively refining prompts to mitigate biases.

Result: SACD outperforms advanced prompt engineering and existing debiasing methods in accuracy across finance, healthcare, and legal tasks.

Conclusion: SACD enhances LLM reliability by effectively addressing multi-bias scenarios, advancing decision-making support in critical domains.

Abstract: Large language models (LLMs) have shown potential in supporting
decision-making applications, particularly as personal assistants in the
financial, healthcare, and legal domains. While prompt engineering strategies
have enhanced the capabilities of LLMs in decision-making, cognitive biases
inherent to LLMs present significant challenges. Cognitive biases are
systematic patterns of deviation from norms or rationality in decision-making
that can lead to the production of inaccurate outputs. Existing cognitive bias
mitigation strategies assume that input prompts only contain one type of
cognitive bias, limiting their effectiveness in more challenging scenarios
involving multiple cognitive biases. To fill this gap, we propose a cognitive
debiasing approach, self-adaptive cognitive debiasing (SACD), that enhances the
reliability of LLMs by iteratively refining prompts. Our method follows three
sequential steps -- bias determination, bias analysis, and cognitive debiasing
-- to iteratively mitigate potential cognitive biases in prompts. Experimental
results on finance, healthcare, and legal decision-making tasks, using both
closed-source and open-source LLMs, demonstrate that the proposed SACD method
outperforms both advanced prompt engineering methods and existing cognitive
debiasing techniques in average accuracy under single-bias and multi-bias
settings.

</details>


### [245] [SemEval-2025 Task 5: LLMs4Subjects -- LLM-based Automated Subject Tagging for a National Technical Library's Open-Access Catalog](https://arxiv.org/pdf/2504.07199)
*Jennifer D'Souza, Sameer Sadruddin, Holger Israel, Mathias Begoin, Diana Slawig*

Main category: cs.CL

TL;DR: SemEval-2025 Task 5 evaluates LLM-based systems for automated subject tagging in English and German using the GND taxonomy, with ensembles, synthetic data, and multilingual processing showing promise.


<details>
  <summary>Details</summary>
Motivation: To improve automated subject tagging for scientific and technical records, leveraging LLMs for efficiency and accuracy.

Method: Participants developed LLM-based systems to recommend top-k subjects, evaluated via precision, recall, F1-score, and expert assessments.

Result: LLM ensembles, synthetic data generation, and multilingual processing proved effective for digital library classification.

Conclusion: The task demonstrates the potential of LLMs in enhancing subject tagging, with practical applications for digital libraries.

Abstract: We present SemEval-2025 Task 5: LLMs4Subjects, a shared task on automated
subject tagging for scientific and technical records in English and German
using the GND taxonomy. Participants developed LLM-based systems to recommend
top-k subjects, evaluated through quantitative metrics (precision, recall,
F1-score) and qualitative assessments by subject specialists. Results highlight
the effectiveness of LLM ensembles, synthetic data generation, and multilingual
processing, offering insights into applying LLMs for digital library
classification.

</details>


### [246] [ConceptCarve: Dynamic Realization of Evidence](https://arxiv.org/pdf/2504.07228)
*Eylon Caplan, Dan Goldwasser*

Main category: cs.CL

TL;DR: ConceptCarve is a retrieval framework combining traditional retrievers and LLMs to address challenges in identifying abstract concepts and their varied instantiations across communities, outperforming traditional systems.


<details>
  <summary>Details</summary>
Motivation: Studying complex human opinions and behaviors at scale, like the link between gun ownership and freedom perception, requires overcoming challenges in identifying abstract concepts and their diverse expressions.

Method: ConceptCarve uses traditional retrievers and LLMs to dynamically characterize the search space during retrieval, enabling scalable and interpretable evidence retrieval.

Result: ConceptCarve outperforms traditional retrieval systems in finding evidence within social media communities and provides interpretable representations of community-specific thought patterns.

Conclusion: ConceptCarve effectively addresses the challenges of retrieving abstract concept instances across diverse communities, offering scalable and interpretable solutions for studying human opinions and behaviors.

Abstract: Finding evidence for human opinion and behavior at scale is a challenging
task, often requiring an understanding of sophisticated thought patterns among
vast online communities found on social media. For example, studying how gun
ownership is related to the perception of Freedom, requires a retrieval system
that can operate at scale over social media posts, while dealing with two key
challenges: (1) identifying abstract concept instances, (2) which can be
instantiated differently across different communities. To address these, we
introduce ConceptCarve, an evidence retrieval framework that utilizes
traditional retrievers and LLMs to dynamically characterize the search space
during retrieval. Our experiments show that ConceptCarve surpasses traditional
retrieval systems in finding evidence within a social media community. It also
produces an interpretable representation of the evidence for that community,
which we use to qualitatively analyze complex thought patterns that manifest
differently across the communities.

</details>


### [247] [Playpen: An Environment for Exploring Learning Through Conversational Interaction](https://arxiv.org/pdf/2504.08590)
*Nicola Horst, Davide Mazzaccara, Antonia Schmidt, Michael Sullivan, Filippo Momentè, Luca Franceschetti, Philipp Sadler, Sherzod Hakimov, Alberto Testoni, Raffaella Bernardi, Raquel Fernández, Alexander Koller, Oliver Lemon, David Schlangen, Mario Giulianelli, Alessandro Suglia*

Main category: cs.CL

TL;DR: The paper explores using Dialogue Games as feedback for post-training LLMs, introducing Playpen for learning via self-play. Methods like SFT, DPO, and GRPO are tested, with GRPO showing balanced improvements.


<details>
  <summary>Details</summary>
Motivation: To investigate if Dialogue Games can provide effective feedback signals for LLM post-training, beyond traditional reward models.

Method: Introduces Playpen for Dialogue Game self-play and tests SFT, DPO, and GRPO on Llama-3.1-8B-Instruct.

Result: SFT improves performance on unseen game instances but harms other skills, while GRPO shows balanced improvements.

Conclusion: Dialogue Games are a promising feedback source for LLM learning, with GRPO offering a balanced approach. The framework is released for further research.

Abstract: Interaction between learner and feedback-giver has come into focus recently
for post-training of Large Language Models (LLMs), through the use of reward
models that judge the appropriateness of a model's response. In this paper, we
investigate whether Dialogue Games -- goal-directed and rule-governed
activities driven predominantly by verbal actions -- can also serve as a source
of feedback signals for learning. We introduce Playpen, an environment for off-
and online learning through Dialogue Game self-play, and investigate a
representative set of post-training methods: supervised fine-tuning; direct
alignment (DPO); and reinforcement learning with GRPO. We experiment with
post-training a small LLM (Llama-3.1-8B-Instruct), evaluating performance on
unseen instances of training games as well as unseen games, and on standard
benchmarks. We find that imitation learning through SFT improves performance on
unseen instances, but negatively impacts other skills, while interactive
learning with GRPO shows balanced improvements without loss of skills. We
release the framework and the baseline training setups to foster research in
the promising new direction of learning in (synthetic) interaction.

</details>


### [248] [DeepMath-103K: A Large-Scale, Challenging, Decontaminated, and Verifiable Mathematical Dataset for Advancing Reasoning](https://arxiv.org/pdf/2504.11456)
*Zhiwei He, Tian Liang, Jiahao Xu, Qiuzhi Liu, Xingyu Chen, Yue Wang, Linfeng Song, Dian Yu, Zhenwen Liang, Wenxuan Wang, Zhuosheng Zhang, Rui Wang, Zhaopeng Tu, Haitao Mi, Dong Yu*

Main category: cs.CL

TL;DR: DeepMath-103K is a large-scale, challenging mathematical dataset designed to advance reinforcement learning in complex reasoning, featuring rigorous decontamination and verifiable answers.


<details>
  <summary>Details</summary>
Motivation: Progress in RL with large language models is limited by the lack of challenging, contamination-free, and verifiable training data.

Method: Introduces DeepMath-103K, a dataset with high difficulty, rigorous decontamination, and three distinct solutions for diverse training paradigms like SFT.

Result: Models trained on DeepMath-103K achieve state-of-the-art results on math benchmarks and generalize to subjects like biology, physics, and chemistry.

Conclusion: DeepMath-103K effectively supports the development of generalizable reasoning in RL, demonstrating broad efficacy beyond mathematics.

Abstract: Reinforcement learning (RL) with large language models shows promise in
complex reasoning. However, its progress is hindered by the lack of large-scale
training data that is sufficiently challenging, contamination-free and
verifiable. To this end, we introduce DeepMath-103K, a large-scale mathematical
dataset designed with high difficulty (primarily levels 5-9), rigorous
decontamination against numerous benchmarks, and verifiable answers for
rule-based RL reward. It further includes three distinct R1 solutions adaptable
for diverse training paradigms such as supervised fine-tuning (SFT). Spanning a
wide range of mathematical topics, DeepMath-103K fosters the development of
generalizable and advancing reasoning. Notably, models trained on DeepMath-103K
achieve state-of-the-art results on challenging mathematical benchmarks and
demonstrate generalization beyond math such as biology, physics and chemistry,
underscoring its broad efficacy. Data:
https://huggingface.co/datasets/zwhe99/DeepMath-103K.

</details>


### [249] [Information Gain-Guided Causal Intervention for Autonomous Debiasing Large Language Models](https://arxiv.org/pdf/2504.12898)
*Zhouhao Sun, Xiao Ding, Li Du, Yunpeng Xu, Yixuan Ma, Yang Zhao, Bing Qin, Ting Liu*

Main category: cs.CL

TL;DR: The paper proposes an information gain-guided causal intervention debiasing (ICD) framework to address dataset biases in LLMs, improving their generalizability.


<details>
  <summary>Details</summary>
Motivation: Current LLMs capture dataset biases, limiting their generalizability, and existing debiasing methods are insufficient.

Method: Combines causal mechanisms with information theory, using causal intervention-based data rewriting to balance dataset distribution and supervised fine-tuning.

Result: ICD effectively debiases LLMs, enhancing their performance across tasks.

Conclusion: The ICD framework successfully mitigates biases in LLMs, improving their generalizability.

Abstract: Despite significant progress, recent studies indicate that current large
language models (LLMs) may still capture dataset biases and utilize them during
inference, leading to the poor generalizability of LLMs. However, due to the
diversity of dataset biases and the insufficient nature of bias suppression
based on in-context learning, the effectiveness of previous prior
knowledge-based debiasing methods and in-context learning based automatic
debiasing methods is limited. To address these challenges, we explore the
combination of causal mechanisms with information theory and propose an
information gain-guided causal intervention debiasing (ICD) framework. To
eliminate biases within the instruction-tuning dataset, it is essential to
ensure that these biases do not provide any additional information to predict
the answers, i.e., the information gain of these biases for predicting the
answers needs to be 0. Under this guidance, this framework utilizes a causal
intervention-based data rewriting method to automatically and autonomously
balance the distribution of instruction-tuning dataset for reducing the
information gain. Subsequently, it employs a standard supervised fine-tuning
process to train LLMs on the debiased dataset. Experimental results show that
ICD can effectively debias LLM to improve its generalizability across different
tasks.

</details>


### [250] [Aleph-Alpha-GermanWeb: Improving German-language LLM pre-training with model-based data curation and synthetic data generation](https://arxiv.org/pdf/2505.00022)
*Thomas F Burns, Letitia Parcalabescu, Stephan Wäldchen, Michael Barlow, Gregor Ziegltrum, Volker Stampa, Bastian Harren, Björn Deiseroth*

Main category: cs.CL

TL;DR: A German dataset curation pipeline combining heuristic and model-based filtering with synthetic data generation improves LLM performance over existing datasets.


<details>
  <summary>Details</summary>
Motivation: To enhance LLM performance and training efficiency by focusing on data quality through curated and synthetic data.

Method: Developed a pipeline for German-language dataset curation using heuristic and model-based filtering, plus synthetic data generation. Created Aleph-Alpha-GermanWeb from Common Crawl, FineWeb2, and synthetic data.

Result: Pre-trained models (1B and 8B) showed significant performance gains on German benchmarks compared to FineWeb2, even when enriched with high-quality sources like Wikipedia.

Conclusion: Model-based data curation and synthetic data generation significantly improve LLM pre-training datasets.

Abstract: Scaling data quantity is essential for large language models (LLMs), yet
recent findings show that data quality can significantly boost performance and
training efficiency. We introduce a German-language dataset curation pipeline
that combines heuristic and model-based filtering techniques with synthetic
data generation. We use our pipeline to create Aleph-Alpha-GermanWeb, a
large-scale German pre-training dataset which draws from: (1) Common Crawl web
data, (2) FineWeb2, and (3) synthetically-generated data conditioned on actual,
organic web data. We evaluate our dataset by pre-training both a 1B Llama-style
model and an 8B tokenizer-free hierarchical autoregressive transformer (HAT). A
comparison on German-language benchmarks, including MMMLU, shows significant
performance gains of Aleph-Alpha-GermanWeb over FineWeb2 alone. This advantage
holds at the 8B scale even when FineWeb2 is enriched by human-curated
high-quality data sources such as Wikipedia. Our findings support the growing
body of evidence that model-based data curation and synthetic data generation
can significantly enhance LLM pre-training datasets.

</details>


### [251] [Chain-of-Model Learning for Language Model](https://arxiv.org/pdf/2505.11820)
*Kaitao Song, Xiaohua Wang, Xu Tan, Huiqiang Jiang, Chengruidong Zhang, Yongliang Shen, Cen LU, Zihao Li, Zifan Song, Caihua Shan, Yansen Wang, Kan Ren, Xiaoqing Zheng, Tao Qin, Yuqing Yang, Dongsheng Li, Lili Qiu*

Main category: cs.CL

TL;DR: The paper introduces Chain-of-Model (CoM), a learning paradigm incorporating causal relationships into hidden states for efficient scaling and flexible inference. It proposes Chain-of-Representation (CoR) and applies it to Transformer architecture as Chain-of-Language-Model (CoLM), with further enhancements like KV sharing in CoLM-Air. Results show comparable performance to standard Transformers with added flexibility.


<details>
  <summary>Details</summary>
Motivation: To improve model training efficiency and inference flexibility by incorporating causal relationships into hidden states, enabling progressive scaling and elastic inference.

Method: Introduces Chain-of-Representation (CoR) to structure hidden states as chains, applied in Transformer architecture as CoLM. CoLM-Air adds KV sharing for extensibility.

Result: CoLM achieves performance comparable to standard Transformers while offering progressive scaling and multiple model sizes for flexible inference.

Conclusion: CoM and CoLM provide a scalable and flexible approach to language modeling, with potential for future extensions and applications.

Abstract: In this paper, we propose a novel learning paradigm, termed Chain-of-Model
(CoM), which incorporates the causal relationship into the hidden states of
each layer as a chain style, thereby introducing great scaling efficiency in
model training and inference flexibility in deployment. We introduce the
concept of Chain-of-Representation (CoR), which formulates the hidden states at
each layer as a combination of multiple sub-representations (i.e., chains) at
the hidden dimension level. In each layer, each chain from the output
representations can only view all of its preceding chains in the input
representations. Consequently, the model built upon CoM framework can
progressively scale up the model size by increasing the chains based on the
previous models (i.e., chains), and offer multiple sub-models at varying sizes
for elastic inference by using different chain numbers. Based on this
principle, we devise Chain-of-Language-Model (CoLM), which incorporates the
idea of CoM into each layer of Transformer architecture. Based on CoLM, we
further introduce CoLM-Air by introducing a KV sharing mechanism, that computes
all keys and values within the first chain and then shares across all chains.
This design demonstrates additional extensibility, such as enabling seamless LM
switching, prefilling acceleration and so on. Experimental results demonstrate
our CoLM family can achieve comparable performance to the standard Transformer,
while simultaneously enabling greater flexiblity, such as progressive scaling
to improve training efficiency and offer multiple varying model sizes for
elastic inference, paving a a new way toward building language models. Our code
will be released in the future at: https://github.com/microsoft/CoLM.

</details>


### [252] [An Annotated Corpus of Arabic Tweets for Hate Speech Analysis](https://arxiv.org/pdf/2505.11969)
*Wajdi Zaghouani, Md. Rafiul Biswas*

Main category: cs.CL

TL;DR: The paper introduces a multilabel Arabic hate speech dataset of 10,000 tweets, annotated for offensive content and hate speech targets, with high inter-annotator agreement. AraBERTv2 achieved the best performance in evaluation.


<details>
  <summary>Details</summary>
Motivation: Addressing the challenge of identifying hate speech in Arabic due to dialectal variations by creating a comprehensive dataset.

Method: Collected and annotated 10,000 Arabic tweets for offensive content and hate speech targets, involving multiple annotators and measuring inter-annotator agreement. Evaluated using transformers-based models.

Result: Inter-annotator agreement was 0.86 for offensive content and 0.71 for targets. AraBERTv2 performed best with a micro-F1 score of 0.7865 and accuracy of 0.786.

Conclusion: The dataset and AraBERTv2 model provide a robust foundation for Arabic hate speech detection, addressing dialectal challenges.

Abstract: Identifying hate speech content in the Arabic language is challenging due to
the rich quality of dialectal variations. This study introduces a multilabel
hate speech dataset in the Arabic language. We have collected 10000 Arabic
tweets and annotated each tweet, whether it contains offensive content or not.
If a text contains offensive content, we further classify it into different
hate speech targets such as religion, gender, politics, ethnicity, origin, and
others. A text can contain either single or multiple targets. Multiple
annotators are involved in the data annotation task. We calculated the
inter-annotator agreement, which was reported to be 0.86 for offensive content
and 0.71 for multiple hate speech targets. Finally, we evaluated the data
annotation task by employing a different transformers-based model in which
AraBERTv2 outperformed with a micro-F1 score of 0.7865 and an accuracy of
0.786.

</details>


### [253] [The AI Gap: How Socioeconomic Status Affects Language Technology Interactions](https://arxiv.org/pdf/2505.12158)
*Elisa Bassignana, Amanda Cercas Curry, Dirk Hovy*

Main category: cs.CL

TL;DR: The study examines how socioeconomic status (SES) affects interactions with language technologies like LLMs, revealing systematic differences in usage, style, and topics between SES groups.


<details>
  <summary>Details</summary>
Motivation: To address gaps in prior research by using real-world data to understand how SES influences language technology use and interaction styles.

Method: Surveyed 1,000 individuals from diverse SES backgrounds and analyzed 6,482 prompts from their LLM interactions.

Result: Higher SES correlates with abstract, concise requests and topics like inclusivity, while lower SES involves more anthropomorphization and concrete language.

Conclusion: SES-based linguistic differences persist in language technology use, highlighting the need for inclusive design to mitigate the digital divide.

Abstract: Socioeconomic status (SES) fundamentally influences how people interact with
each other and more recently, with digital technologies like Large Language
Models (LLMs). While previous research has highlighted the interaction between
SES and language technology, it was limited by reliance on proxy metrics and
synthetic data. We survey 1,000 individuals from diverse socioeconomic
backgrounds about their use of language technologies and generative AI, and
collect 6,482 prompts from their previous interactions with LLMs. We find
systematic differences across SES groups in language technology usage (i.e.,
frequency, performed tasks), interaction styles, and topics. Higher SES entails
a higher level of abstraction, convey requests more concisely, and topics like
'inclusivity' and 'travel'. Lower SES correlates with higher
anthropomorphization of LLMs (using ''hello'' and ''thank you'') and more
concrete language. Our findings suggest that while generative language
technologies are becoming more accessible to everyone, socioeconomic linguistic
differences still stratify their use to exacerbate the digital divide. These
differences underscore the importance of considering SES in developing language
technologies to accommodate varying linguistic needs rooted in socioeconomic
factors and limit the AI Gap across SES groups.

</details>


### [254] [UniEdit: A Unified Knowledge Editing Benchmark for Large Language Models](https://arxiv.org/pdf/2505.12345)
*Qizhou Chen, Dakan Wang, Taolin Zhang, Zaoming Yan, Chengsong You, Chengyu Wang, Xiaofeng He*

Main category: cs.CL

TL;DR: UniEdit is a unified benchmark for LLM editing, addressing limitations of current datasets by covering diverse domains and ripple effects using open-domain knowledge graphs.


<details>
  <summary>Details</summary>
Motivation: Current LLM editing datasets are narrow and lack diversity in evaluation, failing to address broad editing demands and ripple effects.

Method: Constructs editing samples from 25 domains, uses NMCS algorithm for ripple effect sampling, and converts knowledge subgraphs to natural language.

Result: UniEdit is confirmed as comprehensive and diverse, with experiments revealing LLM editing strengths and weaknesses.

Conclusion: UniEdit provides valuable insights for future research in open-domain LLM editing.

Abstract: Model editing aims to enhance the accuracy and reliability of large language
models (LLMs) by efficiently adjusting their internal parameters. Currently,
most LLM editing datasets are confined to narrow knowledge domains and cover a
limited range of editing evaluation. They often overlook the broad scope of
editing demands and the diversity of ripple effects resulting from edits. In
this context, we introduce UniEdit, a unified benchmark for LLM editing
grounded in open-domain knowledge. First, we construct editing samples by
selecting entities from 25 common domains across five major categories,
utilizing the extensive triple knowledge available in open-domain knowledge
graphs to ensure comprehensive coverage of the knowledge domains. To address
the issues of generality and locality in editing, we design an Neighborhood
Multi-hop Chain Sampling (NMCS) algorithm to sample subgraphs based on a given
knowledge piece to entail comprehensive ripple effects to evaluate. Finally, we
employ proprietary LLMs to convert the sampled knowledge subgraphs into natural
language text, guaranteeing grammatical accuracy and syntactical diversity.
Extensive statistical analysis confirms the scale, comprehensiveness, and
diversity of our UniEdit benchmark. We conduct comprehensive experiments across
multiple LLMs and editors, analyzing their performance to highlight strengths
and weaknesses in editing across open knowledge domains and various evaluation
criteria, thereby offering valuable insights for future research endeavors.

</details>


### [255] [Rank, Chunk and Expand: Lineage-Oriented Reasoning for Taxonomy Expansion](https://arxiv.org/pdf/2505.13282)
*Sahil Mishra, Kumar Arjun, Tanmoy Chakraborty*

Main category: cs.CL

TL;DR: LORex is a plug-and-play framework combining discriminative ranking and generative reasoning for efficient taxonomy expansion, outperforming state-of-the-art methods.


<details>
  <summary>Details</summary>
Motivation: Existing methods for taxonomy expansion face challenges like representation limits, noise, and context constraints, limiting their effectiveness.

Method: LORex ranks and chunks candidate terms into batches, filters noise, and iteratively refines selections by reasoning candidates' hierarchy.

Result: LORex improves accuracy by 12% and Wu & Palmer similarity by 5% over state-of-the-art methods.

Conclusion: LORex effectively addresses key challenges in taxonomy expansion, offering a robust and efficient solution.

Abstract: Taxonomies are hierarchical knowledge graphs crucial for recommendation
systems, and web applications. As data grows, expanding taxonomies is
essential, but existing methods face key challenges: (1) discriminative models
struggle with representation limits and generalization, while (2) generative
methods either process all candidates at once, introducing noise and exceeding
context limits, or discard relevant entities by selecting noisy candidates. We
propose LORex ($\textbf{L}$ineage-$\textbf{O}$riented $\textbf{Re}$asoning for
Taxonomy E$\textbf{x}$pansion), a plug-and-play framework that combines
discriminative ranking and generative reasoning for efficient taxonomy
expansion. Unlike prior methods, LORex ranks and chunks candidate terms into
batches, filtering noise and iteratively refining selections by reasoning
candidates' hierarchy to ensure contextual efficiency. Extensive experiments
across four benchmarks and twelve baselines show that LORex improves accuracy
by 12% and Wu & Palmer similarity by 5% over state-of-the-art methods.

</details>


### [256] [ABBA: Highly Expressive Hadamard Product Adaptation for Large Language Models](https://arxiv.org/pdf/2505.14238)
*Raghav Singhal, Kaustubh Ponkshe, Rohit Vartak, Praneeth Vepakomma*

Main category: cs.CL

TL;DR: ABBA introduces a new PEFT method by decoupling updates from pre-trained weights using Hadamard products of low-rank matrices, achieving higher expressivity and better performance.


<details>
  <summary>Details</summary>
Motivation: Adapting large language models efficiently to new domains is challenging, and existing PEFT methods like LoRA and HiRA have limitations in expressivity.

Method: ABBA reparameterizes updates as a Hadamard product of two learnable low-rank matrices, fully decoupling from pre-trained weights.

Result: ABBA outperforms existing PEFT methods on arithmetic and commonsense reasoning benchmarks.

Conclusion: ABBA offers a more expressive and efficient fine-tuning approach, validated by theoretical and empirical results.

Abstract: Large Language Models have demonstrated strong performance across a wide
range of tasks, but adapting them efficiently to new domains remains a key
challenge. Parameter-Efficient Fine-Tuning (PEFT) methods address this by
introducing lightweight, trainable modules while keeping most pre-trained
weights fixed. The prevailing approach, LoRA, models updates using a low-rank
decomposition, but its expressivity is inherently constrained by the rank.
Recent methods like HiRA aim to increase expressivity by incorporating a
Hadamard product with the frozen weights, but still rely on the structure of
the pre-trained model. We introduce ABBA, a new PEFT architecture that
reparameterizes the update as a Hadamard product of two independently learnable
low-rank matrices. In contrast to prior work, ABBA fully decouples the update
from the pre-trained weights, enabling both components to be optimized freely.
This leads to significantly higher expressivity under the same parameter
budget. We formally analyze ABBA's expressive capacity and validate its
advantages through matrix reconstruction experiments. Empirically, ABBA
achieves state-of-the-art results on arithmetic and commonsense reasoning
benchmarks, consistently outperforming existing PEFT methods by a significant
margin across multiple models. Our code is publicly available at:
https://github.com/CERT-Lab/abba.

</details>


### [257] [HausaNLP: Current Status, Challenges and Future Directions for Hausa Natural Language Processing](https://arxiv.org/pdf/2505.14311)
*Shamsuddeen Hassan Muhammad, Ibrahim Said Ahmad, Idris Abdulmumin, Falalu Ibrahim Lawan, Babangida Sani, Sukairaj Hafiz Imam, Yusuf Aliyu, Sani Abdullahi Sani, Ali Usman Umar, Tajuddeen Gwadabe, Kenneth Church, Vukosi Marivate*

Main category: cs.CL

TL;DR: The paper reviews Hausa NLP's current state, highlights challenges like limited datasets, and introduces HausaNLP, a resource catalog. It also discusses LLM integration issues and proposes research directions for advancement.


<details>
  <summary>Details</summary>
Motivation: Hausa, a low-resource language with millions of speakers, lacks adequate NLP resources and representation, hindering progress.

Method: The paper systematically examines Hausa NLP resources and gaps, introduces HausaNLP, and discusses LLM challenges.

Result: HausaNLP is introduced as a catalog to improve resource accessibility, and key challenges like tokenization and dialectal variation are identified.

Conclusion: Strategic research directions, including dataset expansion and community collaboration, are proposed to advance Hausa NLP and multilingual research.

Abstract: Hausa Natural Language Processing (NLP) has gained increasing attention in
recent years, yet remains understudied as a low-resource language despite
having over 120 million first-language (L1) and 80 million second-language (L2)
speakers worldwide. While significant advances have been made in high-resource
languages, Hausa NLP faces persistent challenges, including limited open-source
datasets and inadequate model representation. This paper presents an overview
of the current state of Hausa NLP, systematically examining existing resources,
research contributions, and gaps across fundamental NLP tasks: text
classification, machine translation, named entity recognition, speech
recognition, and question answering. We introduce HausaNLP
(https://catalog.hausanlp.org), a curated catalog that aggregates datasets,
tools, and research works to enhance accessibility and drive further
development. Furthermore, we discuss challenges in integrating Hausa into large
language models (LLMs), addressing issues of suboptimal tokenization and
dialectal variation. Finally, we propose strategic research directions
emphasizing dataset expansion, improved language modeling approaches, and
strengthened community collaboration to advance Hausa NLP. Our work provides
both a foundation for accelerating Hausa NLP progress and valuable insights for
broader multilingual NLP research.

</details>


### [258] [Hidden Ghost Hand: Unveiling Backdoor Vulnerabilities in MLLM-Powered Mobile GUI Agents](https://arxiv.org/pdf/2505.14418)
*Pengzhou Cheng, Haowen Hu, Zheng Wu, Zongru Wu, Tianjie Ju, Zhuosheng Zhang, Gongshen Liu*

Main category: cs.CL

TL;DR: The paper introduces AgentGhost, a stealthy backdoor attack framework for MLLM-powered GUI agents, achieving high attack accuracy (99.7%) with minimal utility loss (1%). A defense method reduces attack success to 22.1%.


<details>
  <summary>Details</summary>
Motivation: GUI agents powered by MLLMs are vulnerable to underexplored supply chain threats like backdoor attacks due to reliance on open-source tools or APIs.

Method: AgentGhost combines goal and interaction-level triggers, formulates backdoor injection as a Min-Max optimization problem, and uses supervised contrastive learning and fine-tuning.

Result: AgentGhost achieves 99.7% attack accuracy with only 1% utility degradation. A defense method reduces attack success to 22.1%.

Conclusion: AgentGhost highlights the vulnerability of MLLM-powered GUI agents to stealthy backdoor attacks and proposes a defense to mitigate such threats.

Abstract: Graphical user interface (GUI) agents powered by multimodal large language
models (MLLMs) have shown greater promise for human-interaction. However, due
to the high fine-tuning cost, users often rely on open-source GUI agents or
APIs offered by AI providers, which introduces a critical but underexplored
supply chain threat: backdoor attacks. In this work, we first unveil that
MLLM-powered GUI agents naturally expose multiple interaction-level triggers,
such as historical steps, environment states, and task progress. Based on this
observation, we introduce AgentGhost, an effective and stealthy framework for
red-teaming backdoor attacks. Specifically, we first construct composite
triggers by combining goal and interaction levels, allowing GUI agents to
unintentionally activate backdoors while ensuring task utility. Then, we
formulate backdoor injection as a Min-Max optimization problem that uses
supervised contrastive learning to maximize the feature difference across
sample classes at the representation space, improving flexibility of the
backdoor. Meanwhile, it adopts supervised fine-tuning to minimize the
discrepancy between backdoor and clean behavior generation, enhancing
effectiveness and utility. Extensive evaluations of various agent models in two
established mobile benchmarks show that AgentGhost is effective and generic,
with attack accuracy that reaches 99.7\% on three attack objectives, and shows
stealthiness with only 1\% utility degradation. Furthermore, we tailor a
defense method against AgentGhost that reduces the attack accuracy to 22.1\%.
Our code is available at \texttt{anonymous}.

</details>


### [259] [Diagnosing our datasets: How does my language model learn clinical information?](https://arxiv.org/pdf/2505.15024)
*Furong Jia, David Sontag, Monica Agrawal*

Main category: cs.CL

TL;DR: The paper investigates how open-source LLMs learn clinical information, focusing on their understanding of clinical jargon and responses to unsupported medical claims, revealing mismatches between pretraining data and real-world usage.


<details>
  <summary>Details</summary>
Motivation: To understand how LLMs interpret clinical jargon and handle unsupported medical claims, given their lack of direct EHR training.

Method: Evaluated LLMs on a new dataset (MedLingo) and analyzed pretraining corpora for clinical jargon frequency and unsupported claims.

Result: Found correlations between jargon frequency in pretraining data and model performance, but noted mismatches with real-world usage. Also identified models parroting disputed claims.

Conclusion: Highlights the need for better alignment between pretraining data and clinical reality, with implications for future dataset design.

Abstract: Large language models (LLMs) have performed well across various clinical
natural language processing tasks, despite not being directly trained on
electronic health record (EHR) data. In this work, we examine how popular
open-source LLMs learn clinical information from large mined corpora through
two crucial but understudied lenses: (1) their interpretation of clinical
jargon, a foundational ability for understanding real-world clinical notes, and
(2) their responses to unsupported medical claims. For both use cases, we
investigate the frequency of relevant clinical information in their
corresponding pretraining corpora, the relationship between pretraining data
composition and model outputs, and the sources underlying this data. To isolate
clinical jargon understanding, we evaluate LLMs on a new dataset MedLingo.
Unsurprisingly, we find that the frequency of clinical jargon mentions across
major pretraining corpora correlates with model performance. However, jargon
frequently appearing in clinical notes often rarely appears in pretraining
corpora, revealing a mismatch between available data and real-world usage.
Similarly, we find that a non-negligible portion of documents support disputed
claims that can then be parroted by models. Finally, we classified and analyzed
the types of online sources in which clinical jargon and unsupported medical
claims appear, with implications for future dataset composition.

</details>


### [260] [ThinkLess: A Training-Free Inference-Efficient Method for Reducing Reasoning Redundancy](https://arxiv.org/pdf/2505.15684)
*Gengyang Li, Yifeng Gao, Yuming Li, Yunfang Wu*

Main category: cs.CL

TL;DR: ThinkLess is an efficient framework that reduces reasoning token length in LLMs by early termination, maintaining output quality without model modification.


<details>
  <summary>Details</summary>
Motivation: Excessive reasoning tokens in Chain-of-Thought (CoT) prompting increase latency, memory usage, and risk answer truncation under context limits.

Method: ThinkLess terminates reasoning early by leveraging attention patterns, inserts a terminator token, and uses post-regulation for structured answers.

Result: ThinkLess achieves comparable accuracy to full CoT decoding while significantly reducing decoding time and memory usage.

Conclusion: ThinkLess offers a practical solution for efficient reasoning in LLMs without compromising quality or requiring additional training.

Abstract: While Chain-of-Thought (CoT) prompting improves reasoning in large language
models (LLMs), the excessive length of reasoning tokens increases latency and
KV cache memory usage, and may even truncate final answers under context
limits. We propose ThinkLess, an inference-efficient framework that terminates
reasoning generation early and maintains output quality without modifying the
model. Atttention analysis reveals that answer tokens focus minimally on
earlier reasoning steps and primarily attend to the reasoning terminator token,
due to information migration under causal masking. Building on this insight,
ThinkLess inserts the terminator token at earlier positions to skip redundant
reasoning while preserving the underlying knowledge transfer. To prevent format
discruption casued by early termination, ThinkLess employs a lightweight
post-regulation mechanism, relying on the model's natural instruction-following
ability to produce well-structured answers. Without fine-tuning or auxiliary
data, ThinkLess achieves comparable accuracy to full-length CoT decoding while
greatly reducing decoding time and memory consumption.

</details>


### [261] [Prototypical Human-AI Collaboration Behaviors from LLM-Assisted Writing in the Wild](https://arxiv.org/pdf/2505.16023)
*Sheshera Mysore, Debarati Das, Hancheng Cao, Bahareh Sarrafzadeh*

Main category: cs.CL

TL;DR: The paper analyzes user interactions with LLMs in writing tasks, identifying common collaboration behaviors (PATHs) and their correlation with user intents, impacting LLM alignment.


<details>
  <summary>Details</summary>
Motivation: To understand how users actively refine and co-construct text with LLMs in real-world writing tasks, moving beyond passive acceptance of outputs.

Method: Large-scale analysis of user interactions with Bing Copilot and WildChat, identifying prototypical behaviors (PATHs) and correlating them with writing intents.

Result: A small set of PATHs explains most user-LLM interactions, including revising intents, exploring texts, and adjusting style. Significant correlations exist between intents and PATHs.

Conclusion: Findings highlight the dynamic nature of human-AI collaboration, with implications for improving LLM alignment to better support user needs.

Abstract: As large language models (LLMs) are used in complex writing workflows, users
engage in multi-turn interactions to steer generations to better fit their
needs. Rather than passively accepting output, users actively refine, explore,
and co-construct text. We conduct a large-scale analysis of this collaborative
behavior for users engaged in writing tasks in the wild with two popular AI
assistants, Bing Copilot and WildChat. Our analysis goes beyond simple task
classification or satisfaction estimation common in prior work and instead
characterizes how users interact with LLMs through the course of a session. We
identify prototypical behaviors in how users interact with LLMs in prompts
following their original request. We refer to these as Prototypical Human-AI
Collaboration Behaviors (PATHs) and find that a small group of PATHs explain a
majority of the variation seen in user-LLM interaction. These PATHs span users
revising intents, exploring texts, posing questions, adjusting style or
injecting new content. Next, we find statistically significant correlations
between specific writing intents and PATHs, revealing how users' intents shape
their collaboration behaviors. We conclude by discussing the implications of
our findings on LLM alignment.

</details>


### [262] [Small Language Models in the Real World: Insights from Industrial Text Classification](https://arxiv.org/pdf/2505.16078)
*Lujun Li, Lama Sleem, Niccolo' Gentile, Geoffrey Nichil, Radu State*

Main category: cs.CL

TL;DR: The paper evaluates prompt engineering and supervised fine-tuning for smaller transformer models in text classification, focusing on industrial applications like email and legal document categorization.


<details>
  <summary>Details</summary>
Motivation: Despite the success of large models like ChatGPT, their inefficiency and resource demands raise questions about the viability of smaller models for text classification tasks.

Method: The study compares prompt engineering and supervised fine-tuning methods for smaller transformer models, testing them on industrial tasks like email and legal document classification.

Result: The paper highlights the performance and efficiency trade-offs of smaller models, particularly in VRAM utilization, for industrial deployment.

Conclusion: Smaller models can effectively handle text classification tasks, offering practical insights for local deployment in industrial settings.

Abstract: With the emergence of ChatGPT, Transformer models have significantly advanced
text classification and related tasks. Decoder-only models such as Llama
exhibit strong performance and flexibility, yet they suffer from inefficiency
on inference due to token-by-token generation, and their effectiveness in text
classification tasks heavily depends on prompt quality. Moreover, their
substantial GPU resource requirements often limit widespread adoption. Thus,
the question of whether smaller language models are capable of effectively
handling text classification tasks emerges as a topic of significant interest.
However, the selection of appropriate models and methodologies remains largely
underexplored. In this paper, we conduct a comprehensive evaluation of prompt
engineering and supervised fine-tuning methods for transformer-based text
classification. Specifically, we focus on practical industrial scenarios,
including email classification, legal document categorization, and the
classification of extremely long academic texts. We examine the strengths and
limitations of smaller models, with particular attention to both their
performance and their efficiency in Video Random-Access Memory (VRAM)
utilization, thereby providing valuable insights for the local deployment and
application of compact models in industrial settings.

</details>


### [263] [EduBench: A Comprehensive Benchmarking Dataset for Evaluating Large Language Models in Diverse Educational Scenarios](https://arxiv.org/pdf/2505.16160)
*Bin Xu, Yu Bai, Huashan Sun, Yiguan Lin, Siming Liu, Xinyue Liang, Yaolin Li, Yang Gao, Heyan Huang*

Main category: cs.CL

TL;DR: The paper introduces EduBench, a diverse benchmark for evaluating language models in educational contexts, with synthetic data and multi-dimensional metrics. A smaller trained model matches state-of-the-art performance.


<details>
  <summary>Details</summary>
Motivation: Addressing the under-explored and under-optimized use of large language models in education.

Method: Creation of a benchmark with synthetic data (9 scenarios, 4,000 contexts), multi-dimensional metrics, human annotation, and training a small-scale model.

Result: The trained small-scale model achieves performance comparable to state-of-the-art large models.

Conclusion: The work lays a practical foundation for developing and evaluating education-oriented language models.

Abstract: As large language models continue to advance, their application in
educational contexts remains underexplored and under-optimized. In this paper,
we address this gap by introducing the first diverse benchmark tailored for
educational scenarios, incorporating synthetic data containing 9 major
scenarios and over 4,000 distinct educational contexts. To enable comprehensive
assessment, we propose a set of multi-dimensional evaluation metrics that cover
12 critical aspects relevant to both teachers and students. We further apply
human annotation to ensure the effectiveness of the model-generated evaluation
responses. Additionally, we succeed to train a relatively small-scale model on
our constructed dataset and demonstrate that it can achieve performance
comparable to state-of-the-art large models (e.g., Deepseek V3, Qwen Max) on
the test set. Overall, this work provides a practical foundation for the
development and evaluation of education-oriented language models. Code and data
are released at https://github.com/ybai-nlp/EduBench.

</details>


### [264] [Three Minds, One Legend: Jailbreak Large Reasoning Model with Adaptive Stacked Ciphers](https://arxiv.org/pdf/2505.16241)
*Viet-Anh Nguyen, Shiqian Zhao, Gia Dao, Runyi Hu, Yi Xie, Luu Anh Tuan*

Main category: cs.CL

TL;DR: SEAL is a novel jailbreak attack targeting Large Reasoning Models (LRMs) using adaptive encryption to bypass safety mechanisms, achieving an 80.8% success rate on GPT-o4-mini.


<details>
  <summary>Details</summary>
Motivation: LRMs show superior reasoning but may introduce severe security vulnerabilities, which are underexplored. Existing jailbreak methods lack balance between effectiveness and robustness.

Method: SEAL employs a stacked encryption pipeline with dynamic strategies (random and adaptive) to overwhelm LRMs' reasoning and evade alignment.

Result: SEAL achieves an 80.8% attack success rate on GPT-o4-mini, outperforming baselines by 27.2%.

Conclusion: SEAL demonstrates significant effectiveness in bypassing LRM safety mechanisms, highlighting the need for stronger defenses.

Abstract: Recently, Large Reasoning Models (LRMs) have demonstrated superior logical
capabilities compared to traditional Large Language Models (LLMs), gaining
significant attention. Despite their impressive performance, the potential for
stronger reasoning abilities to introduce more severe security vulnerabilities
remains largely underexplored. Existing jailbreak methods often struggle to
balance effectiveness with robustness against adaptive safety mechanisms. In
this work, we propose SEAL, a novel jailbreak attack that targets LRMs through
an adaptive encryption pipeline designed to override their reasoning processes
and evade potential adaptive alignment. Specifically, SEAL introduces a stacked
encryption approach that combines multiple ciphers to overwhelm the models
reasoning capabilities, effectively bypassing built-in safety mechanisms. To
further prevent LRMs from developing countermeasures, we incorporate two
dynamic strategies - random and adaptive - that adjust the cipher length,
order, and combination. Extensive experiments on real-world reasoning models,
including DeepSeek-R1, Claude Sonnet, and OpenAI GPT-o4, validate the
effectiveness of our approach. Notably, SEAL achieves an attack success rate of
80.8% on GPT o4-mini, outperforming state-of-the-art baselines by a significant
margin of 27.2%. Warning: This paper contains examples of inappropriate,
offensive, and harmful content.

</details>


### [265] [SSR-Zero: Simple Self-Rewarding Reinforcement Learning for Machine Translation](https://arxiv.org/pdf/2505.16637)
*Wenjie Yang, Mao Zheng, Mingyang Song, Zheng Li*

Main category: cs.CL

TL;DR: The paper introduces a Simple Self-Rewarding (SSR) Reinforcement Learning framework for machine translation, eliminating the need for expensive external supervision. SSR outperforms existing models and achieves state-of-the-art results when combined with external supervision.


<details>
  <summary>Details</summary>
Motivation: To address the high cost and scalability issues of external supervision in training MT-specific LLMs, the authors propose a self-rewarding RL framework.

Method: The SSR framework is reference-free and fully online, relying on self-judging rewards. It is trained using 13K monolingual examples and Qwen-2.5-7B as the backbone.

Result: SSR-Zero-7B outperforms existing MT-specific and larger general LLMs. Augmented with COMET, SSR-X-Zero-7B achieves state-of-the-art performance, surpassing even closed-source models like GPT-4o.

Conclusion: The self-rewarding mechanism is effective and complementary to external supervision, offering insights into self-improving RL methods. The code, data, and models are publicly released.

Abstract: Large language models (LLMs) have recently demonstrated remarkable
capabilities in machine translation (MT). However, most advanced MT-specific
LLMs heavily rely on external supervision signals during training, such as
human-annotated reference data or trained reward models (RMs), which are often
expensive to obtain and challenging to scale. To overcome this limitation, we
propose a Simple Self-Rewarding (SSR) Reinforcement Learning (RL) framework for
MT that is reference-free, fully online, and relies solely on self-judging
rewards. Training with SSR using 13K monolingual examples and Qwen-2.5-7B as
the backbone, our model SSR-Zero-7B outperforms existing MT-specific LLMs,
e.g., TowerInstruct-13B and GemmaX-28-9B, as well as larger general LLMs like
Qwen2.5-32B-Instruct in English $\leftrightarrow$ Chinese translation tasks
from WMT23, WMT24, and Flores200 benchmarks. Furthermore, by augmenting SSR
with external supervision from COMET, our strongest model, SSR-X-Zero-7B,
achieves state-of-the-art performance in English $\leftrightarrow$ Chinese
translation, surpassing all existing open-source models under 72B parameters
and even outperforming closed-source models, e.g., GPT-4o and Gemini 1.5 Pro.
Our analysis highlights the effectiveness of the self-rewarding mechanism
compared to the external LLM-as-a-judge approach in MT and demonstrates its
complementary benefits when combined with trained RMs. Our findings provide
valuable insight into the potential of self-improving RL methods. We have
publicly released our code, data and models.

</details>


### [266] [Think Silently, Think Fast: Dynamic Latent Compression of LLM Reasoning Chains](https://arxiv.org/pdf/2505.16552)
*Wenhui Tan, Jiaze Li, Jianzhong Ju, Zhenbo Luo, Jian Luan, Ruihua Song*

Main category: cs.CL

TL;DR: CoLaR introduces a latent space compression framework for LLMs, reducing reasoning chain length while maintaining accuracy.


<details>
  <summary>Details</summary>
Motivation: Token-level reasoning in LLMs is computationally expensive; CoLaR aims to compress reasoning in latent space for efficiency.

Method: Two-stage training: supervised fine-tuning with compressed embedding prediction and RL enhancement for diverse reasoning paths.

Result: CoLaR achieves higher accuracy (14.1%) than baselines, reduces chain length by 53.3%, and improves performance in challenging tasks (5.4% gain).

Conclusion: CoLaR efficiently compresses reasoning, dynamically adjusts speed, and outperforms existing methods with significant computational savings.

Abstract: Large Language Models (LLMs) achieve superior performance through
Chain-of-Thought (CoT) reasoning, but these token-level reasoning chains are
computationally expensive and inefficient. In this paper, we introduce
Compressed Latent Reasoning (CoLaR), a novel framework that dynamically
compresses reasoning processes in latent space through a two-stage training
approach. First, during supervised fine-tuning, CoLaR extends beyond next-token
prediction by incorporating an auxiliary next compressed embedding prediction
objective. This process merges embeddings of consecutive tokens using a
compression factor randomly sampled from a predefined range, and trains a
specialized latent head to predict distributions of subsequent compressed
embeddings. Second, we enhance CoLaR through reinforcement learning (RL) that
leverages the latent head's non-deterministic nature to explore diverse
reasoning paths and exploit more compact ones. This approach enables CoLaR to:
i) perform reasoning at a dense latent level (i.e., silently), substantially
reducing reasoning chain length, and ii) dynamically adjust reasoning speed at
inference time by simply prompting the desired compression factor. Extensive
experiments across four mathematical reasoning datasets demonstrate that CoLaR
achieves 14.1% higher accuracy than latent-based baseline methods at comparable
compression ratios, and reduces reasoning chain length by 53.3% with only 4.8%
performance degradation compared to explicit CoT method. Moreover, when applied
to more challenging mathematical reasoning tasks, our RL-enhanced CoLaR
demonstrates performance gains of up to 5.4% while dramatically reducing latent
reasoning chain length by 82.8%. The code and models will be released upon
acceptance.

</details>


### [267] [Power-Law Decay Loss for Large Language Model Finetuning: Focusing on Information Sparsity to Enhance Generation Quality](https://arxiv.org/pdf/2505.16900)
*Jintian Shao, Yiming Cheng, Hongyi Huang, Jiayi Wu, Beiwen Zhang, Zhiyu Wu, You Shan, Mingkai Zheng*

Main category: cs.CL

TL;DR: The paper introduces Power-Law Decay Loss (PDL), a novel loss function for text generation finetuning, addressing the overemphasis on high-frequency tokens by re-weighting tokens based on their frequency to enhance text quality and informativeness.


<details>
  <summary>Details</summary>
Motivation: Standard cross-entropy loss treats all tokens equally, causing models to neglect low-frequency, information-dense tokens. PDL is motivated by the inverse relationship between token frequency and informativeness.

Method: PDL re-weights tokens in the cross-entropy loss using a power-law decay, reducing weights for high-frequency tokens and increasing them for low-frequency ones.

Result: PDL improves the quality, diversity, and informativeness of generated text by focusing on specific and unique tokens.

Conclusion: PDL is theoretically justified and applicable to various text generation tasks like summarization, dialogue systems, and style transfer, offering enhanced performance.

Abstract: During the finetuning stage of text generation tasks, standard cross-entropy
loss treats all tokens equally. This can lead models to overemphasize
high-frequency, low-information tokens, neglecting lower-frequency tokens
crucial for specificity and informativeness in generated content. This paper
introduces a novel loss function, Power-Law Decay Loss (PDL), specifically
designed to optimize the finetuning process for text generation. The core
motivation for PDL stems from observations in information theory and
linguistics: the informativeness of a token is often inversely proportional to
its frequency of occurrence. PDL re-weights the contribution of each token in
the standard cross-entropy loss based on its frequency in the training corpus,
following a power-law decay. Specifically, the weights for high-frequency
tokens are reduced, while low-frequency, information-dense tokens are assigned
higher weights. This mechanism guides the model during finetuning to focus more
on learning and generating tokens that convey specific and unique information,
thereby enhancing the quality, diversity, and informativeness of the generated
text. We theoretically elaborate on the motivation and construction of PDL and
discuss its potential applications and advantages across various text
generation finetuning tasks, such as abstractive summarization, dialogue
systems, and style transfer.

</details>


### [268] [VeriFastScore: Speeding up long-form factuality evaluation](https://arxiv.org/pdf/2505.16973)
*Rishanth Rajendhran, Amir Zadeh, Matthew Sarte, Chuan Li, Mohit Iyyer*

Main category: cs.CL

TL;DR: VeriFastScore improves the efficiency of long-form factuality evaluation by fine-tuning Llama3.1 8B for simultaneous claim extraction and verification, achieving a 6.6x speedup over VeriScore.


<details>
  <summary>Details</summary>
Motivation: Existing methods like FactScore and VeriScore are slow due to numerous LLM calls, limiting scalability.

Method: Fine-tune Llama3.1 8B using synthetic data to extract and verify claims concurrently with Google Search evidence.

Result: VeriFastScore correlates strongly with VeriScore (r=0.80-0.94) and achieves a 6.6x speedup.

Conclusion: VeriFastScore offers a faster, scalable alternative for factuality evaluation, with released models and datasets for future research.

Abstract: Metrics like FactScore and VeriScore that evaluate long-form factuality
operate by decomposing an input response into atomic claims and then
individually verifying each claim. While effective and interpretable, these
methods incur numerous LLM calls and can take upwards of 100 seconds to
evaluate a single response, limiting their practicality in large-scale
evaluation and training scenarios. To address this, we propose VeriFastScore,
which leverages synthetic data to fine-tune Llama3.1 8B for simultaneously
extracting and verifying all verifiable claims within a given text based on
evidence from Google Search. We show that this task cannot be solved via
few-shot prompting with closed LLMs due to its complexity: the model receives
~4K tokens of evidence on average and needs to concurrently decompose claims,
judge their verifiability, and verify them against noisy evidence. However, our
fine-tuned VeriFastScore model demonstrates strong correlation with the
original VeriScore pipeline at both the example level (r=0.80) and system level
(r=0.94) while achieving an overall speedup of 6.6x (9.9x excluding evidence
retrieval) over VeriScore. To facilitate future factuality research, we
publicly release our VeriFastScore model and synthetic datasets.

</details>


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [269] [Synthetic History: Evaluating Visual Representations of the Past in Diffusion Models](https://arxiv.org/pdf/2505.17064)
*Maria-Teresa De Rosa Palmini, Eva Cetinic*

Main category: cs.CV

TL;DR: The paper evaluates how Text-to-Image (TTI) diffusion models represent historical contexts, revealing biases and inaccuracies in stylistic, consistency, and demographic aspects.


<details>
  <summary>Details</summary>
Motivation: To address the underexplored issue of historical accuracy in TTI models, given their growing societal and cultural impact.

Method: Introduces the HistVis dataset (30,000 synthetic images) and evaluates three state-of-the-art models across stylistic associations, historical consistency, and demographic representation.

Result: TTI models exhibit systematic inaccuracies, including stereotypes, anachronisms, and implausible demographic patterns.

Conclusion: The work provides a scalable methodology and benchmark to improve historical accuracy in TTI models.

Abstract: As Text-to-Image (TTI) diffusion models become increasingly influential in
content creation, growing attention is being directed toward their societal and
cultural implications. While prior research has primarily examined demographic
and cultural biases, the ability of these models to accurately represent
historical contexts remains largely underexplored. In this work, we present a
systematic and reproducible methodology for evaluating how TTI systems depict
different historical periods. For this purpose, we introduce the HistVis
dataset, a curated collection of 30,000 synthetic images generated by three
state-of-the-art diffusion models using carefully designed prompts depicting
universal human activities across different historical periods. We evaluate
generated imagery across three key aspects: (1) Implicit Stylistic
Associations: examining default visual styles associated with specific eras;
(2) Historical Consistency: identifying anachronisms such as modern artifacts
in pre-modern contexts; and (3) Demographic Representation: comparing generated
racial and gender distributions against historically plausible baselines. Our
findings reveal systematic inaccuracies in historically themed generated
imagery, as TTI models frequently stereotype past eras by incorporating
unstated stylistic cues, introduce anachronisms, and fail to reflect plausible
demographic patterns. By offering a scalable methodology and benchmark for
assessing historical representation in generated imagery, this work provides an
initial step toward building more historically accurate and culturally aligned
TTI models.

</details>


### [270] [EmoSign: A Multimodal Dataset for Understanding Emotions in American Sign Language](https://arxiv.org/pdf/2505.17090)
*Phoebe Chua, Cathy Mengying Fang, Takehiko Ohkawa, Raja Kushalnagar, Suranga Nanayakkara, Pattie Maes*

Main category: cs.CV

TL;DR: EmoSign is the first dataset with sentiment and emotion labels for ASL videos, annotated by Deaf signers, to bridge gaps in understanding emotional cues in sign language.


<details>
  <summary>Details</summary>
Motivation: Emotional indicators in sign language are poorly understood, creating communication barriers, especially in critical settings.

Method: Created EmoSign, a dataset of 200 ASL videos with sentiment and emotion labels, annotated by Deaf ASL signers, and developed baseline models for classification.

Result: The dataset provides a benchmark for multimodal emotion recognition in sign languages and includes open-ended descriptions of emotion cues.

Conclusion: EmoSign addresses a critical research gap and sets a new standard for studying emotional expression in sign languages.

Abstract: Unlike spoken languages where the use of prosodic features to convey emotion
is well studied, indicators of emotion in sign language remain poorly
understood, creating communication barriers in critical settings. Sign
languages present unique challenges as facial expressions and hand movements
simultaneously serve both grammatical and emotional functions. To address this
gap, we introduce EmoSign, the first sign video dataset containing sentiment
and emotion labels for 200 American Sign Language (ASL) videos. We also collect
open-ended descriptions of emotion cues. Annotations were done by 3 Deaf ASL
signers with professional interpretation experience. Alongside the annotations,
we include baseline models for sentiment and emotion classification. This
dataset not only addresses a critical gap in existing sign language research
but also establishes a new benchmark for understanding model capabilities in
multimodal emotion recognition for sign languages. The dataset is made
available at https://huggingface.co/datasets/catfang/emosign.

</details>


### [271] [CAMA: Enhancing Multimodal In-Context Learning with Context-Aware Modulated Attention](https://arxiv.org/pdf/2505.17097)
*Yanshu Li, JianJiang Yang, Bozheng Li, Ruixiang Tang*

Main category: cs.CV

TL;DR: The paper introduces Context-Aware Modulated Attention (CAMA) to address instability in multimodal in-context learning (ICL) for vision-language models by calibrating attention logits.


<details>
  <summary>Details</summary>
Motivation: Multimodal ICL is unstable, and current research overlooks the internal mechanisms of large vision-language models (LVLMs).

Method: Proposes CAMA, a training-free plug-and-play method to calibrate LVLM attention logits.

Result: Evaluated on four LVLMs across six benchmarks, showing effectiveness and generality.

Conclusion: CAMA enables deeper exploration of LVLM attention dynamics to advance multimodal reasoning.

Abstract: Multimodal in-context learning (ICL) enables large vision-language models
(LVLMs) to efficiently adapt to novel tasks, supporting a wide array of
real-world applications. However, multimodal ICL remains unstable, and current
research largely focuses on optimizing sequence configuration while overlooking
the internal mechanisms of LVLMs. In this work, we first provide a theoretical
analysis of attentional dynamics in multimodal ICL and identify three core
limitations of standard attention that ICL impair performance. To address these
challenges, we propose Context-Aware Modulated Attention (CAMA), a simple yet
effective plug-and-play method for directly calibrating LVLM attention logits.
CAMA is training-free and can be seamlessly applied to various open-source
LVLMs. We evaluate CAMA on four LVLMs across six benchmarks, demonstrating its
effectiveness and generality. CAMA opens new opportunities for deeper
exploration and targeted utilization of LVLM attention dynamics to advance
multimodal reasoning.

</details>


### [272] [Co-Reinforcement Learning for Unified Multimodal Understanding and Generation](https://arxiv.org/pdf/2505.17534)
*Jingjing Jiang, Chongjie Si, Jun Luo, Hanwang Zhang, Chao Ma*

Main category: cs.CV

TL;DR: The paper introduces CoRL, a co-reinforcement learning framework for unified multimodal large language models (ULMs), achieving significant improvements in generation and understanding tasks.


<details>
  <summary>Details</summary>
Motivation: To explore reinforcement learning for ULMs to enhance both generation and understanding capabilities synergistically.

Method: Proposes CoRL, a two-stage framework: unified RL for joint optimization and refined RL for task-specific enhancement.

Result: ULM-R1 model shows 7% improvement on text-to-image generation and 23% on multimodal understanding benchmarks.

Conclusion: CoRL effectively leverages reinforcement learning for cross-task synergy in ULMs, demonstrating substantial performance gains.

Abstract: This paper presents a pioneering exploration of reinforcement learning (RL)
via group relative policy optimization for unified multimodal large language
models (ULMs), aimed at simultaneously reinforcing generation and understanding
capabilities. Through systematic pilot studies, we uncover the significant
potential of ULMs to enable the synergistic co-evolution of dual capabilities
within a shared policy optimization framework. Building on this insight, we
introduce \textbf{CoRL}, a co-reinforcement learning framework comprising a
unified RL stage for joint optimization and a refined RL stage for
task-specific enhancement. With the proposed CoRL, our resulting model,
\textbf{ULM-R1}, achieves average improvements of \textbf{7%} on three
text-to-image generation datasets and \textbf{23%} on nine multimodal
understanding benchmarks. These results demonstrate the effectiveness of CoRL
and highlight the substantial benefit of reinforcement learning in facilitating
cross-task synergy and optimization for ULMs.

</details>


### [273] [Pixels Versus Priors: Controlling Knowledge Priors in Vision-Language Models through Visual Counterfacts](https://arxiv.org/pdf/2505.17127)
*Michal Golovanevsky, William Rudman, Michael Lepori, Amir Bar, Ritambhara Singh, Carsten Eickhoff*

Main category: cs.CV

TL;DR: The paper investigates whether Multimodal Large Language Models (MLLMs) rely more on memorized world knowledge or visual input. It introduces Visual CounterFact, a dataset to test this, and finds visual input eventually overrides memorized priors. A method called PvP is proposed to control this behavior.


<details>
  <summary>Details</summary>
Motivation: To understand if MLLMs prioritize memorized knowledge or visual input in reasoning tasks.

Method: Introduces Visual CounterFact dataset and PvP steering vectors to control model outputs.

Result: Visual input overrides memorized priors in later layers; PvP successfully shifts predictions toward visual evidence.

Conclusion: Provides tools to interpret and control factual behavior in MLLMs, highlighting the dominance of visual input over memorized knowledge.

Abstract: Multimodal Large Language Models (MLLMs) perform well on tasks such as visual
question answering, but it remains unclear whether their reasoning relies more
on memorized world knowledge or on the visual information present in the input
image. To investigate this, we introduce Visual CounterFact, a new dataset of
visually-realistic counterfactuals that put world knowledge priors (e.g, red
strawberry) into direct conflict with visual input (e.g, blue strawberry).
Using Visual CounterFact, we show that model predictions initially reflect
memorized priors, but shift toward visual evidence in mid-to-late layers. This
dynamic reveals a competition between the two modalities, with visual input
ultimately overriding priors during evaluation. To control this behavior, we
propose Pixels Versus Priors (PvP) steering vectors, a mechanism for
controlling model outputs toward either world knowledge or visual input through
activation-level interventions. On average, PvP successfully shifts 92.5% of
color and 74.6% of size predictions from priors to counterfactuals. Together,
these findings offer new tools for interpreting and controlling factual
behavior in multimodal models.

</details>


### [274] [HoloLLM: Multisensory Foundation Model for Language-Grounded Human Sensing and Reasoning](https://arxiv.org/pdf/2505.17645)
*Chuhao Zhou, Jianfei Yang*

Main category: cs.CV

TL;DR: HoloLLM, a Multimodal Large Language Model (MLLM), integrates rare sensing modalities (LiDAR, infrared, etc.) to enhance human perception in smart homes, overcoming data scarcity and signal heterogeneity with a Universal Modality-Injection Projector (UMIP).


<details>
  <summary>Details</summary>
Motivation: Existing Vision-Language Models (VLMs) are limited by visual data reliance, struggling with occlusions, lighting, or privacy. HoloLLM aims to improve robustness by leveraging diverse sensors.

Method: HoloLLM uses a Universal Modality-Injection Projector (UMIP) to align rare sensor data with text, and a human-VLM pipeline for data annotation.

Result: HoloLLM outperforms existing MLLMs by up to 30% in language-grounded human sensing accuracy on new benchmarks.

Conclusion: HoloLLM sets a new standard for multisensory embodied intelligence, enabling robust real-world applications.

Abstract: Embodied agents operating in smart homes must understand human behavior
through diverse sensory inputs and communicate via natural language. While
Vision-Language Models (VLMs) have enabled impressive language-grounded
perception, their reliance on visual data limits robustness in real-world
scenarios with occlusions, poor lighting, or privacy constraints. In this
paper, we introduce HoloLLM, a Multimodal Large Language Model (MLLM) that
integrates uncommon but powerful sensing modalities, such as LiDAR, infrared,
mmWave radar, and WiFi, to enable seamless human perception and reasoning
across heterogeneous environments. We address two key challenges: (1) the
scarcity of aligned modality-text data for rare sensors, and (2) the
heterogeneity of their physical signal representations. To overcome these, we
design a Universal Modality-Injection Projector (UMIP) that enhances
pre-aligned modality embeddings with fine-grained, text-aligned features from
tailored encoders via coarse-to-fine cross-attention without introducing
significant alignment overhead. We further introduce a human-VLM collaborative
data curation pipeline to generate paired textual annotations for sensing
datasets. Extensive experiments on two newly constructed benchmarks show that
HoloLLM significantly outperforms existing MLLMs, improving language-grounded
human sensing accuracy by up to 30%. This work establishes a new foundation for
real-world, language-informed multisensory embodied intelligence.

</details>


### [275] [Robustifying Vision-Language Models via Dynamic Token Reweighting](https://arxiv.org/pdf/2505.17132)
*Tanqiu Jiang, Jiacheng Liang, Rongyi Zhu, Jiawei Zhou, Fenglong Ma, Ting Wang*

Main category: cs.CV

TL;DR: DTR is a novel inference-time defense using KV cache optimization to mitigate multimodal jailbreak attacks in VLMs, outperforming existing methods in robustness and task performance.


<details>
  <summary>Details</summary>
Motivation: VLMs are vulnerable to jailbreak attacks exploiting visual-textual interactions, necessitating efficient defenses without relying on curated data or costly conversions.

Method: DTR dynamically adjusts visual token weights to counter adversarial inputs, leveraging KV cache optimization for safety enhancement.

Result: DTR outperforms existing defenses in robustness and benign task performance across diverse VLMs and attack benchmarks.

Conclusion: DTR marks the first successful use of KV cache optimization for safety in multimodal models, offering a scalable and efficient defense.

Abstract: Large vision-language models (VLMs) are highly vulnerable to jailbreak
attacks that exploit visual-textual interactions to bypass safety guardrails.
In this paper, we present DTR, a novel inference-time defense that mitigates
multimodal jailbreak attacks through optimizing the model's key-value (KV)
caches. Rather than relying on curated safety-specific data or costly
image-to-text conversion, we introduce a new formulation of the safety-relevant
distributional shift induced by the visual modality. This formulation enables
DTR to dynamically adjust visual token weights, minimizing the impact of
adversarial visual inputs while preserving the model's general capabilities and
inference efficiency. Extensive evaluation across diverse VLMs and attack
benchmarks demonstrates that \sys outperforms existing defenses in both attack
robustness and benign task performance, marking the first successful
application of KV cache optimization for safety enhancement in multimodal
foundation models. The code for replicating DTR is available:
https://anonymous.4open.science/r/DTR-2755 (warning: this paper contains
potentially harmful content generated by VLMs.)

</details>


### [276] [A Framework for Multi-View Multiple Object Tracking using Single-View Multi-Object Trackers on Fish Data](https://arxiv.org/pdf/2505.17201)
*Chaim Chai Elchik, Fatemeh Karimi Nejadasl, Seyed Sahand Mohammadi Ziabari, Ali Mohammed Mansoor Alsahag*

Main category: cs.CV

TL;DR: A multi-view framework improves fish tracking in underwater environments by adapting FairMOT and YOLOv8, achieving 47% accuracy and 3D outputs.


<details>
  <summary>Details</summary>
Motivation: Traditional single-view MOT models struggle with small fish tracking due to complex 3D motions and noise in underwater settings.

Method: Adapts FairMOT and YOLOv8 for underwater use, integrating stereo video inputs for enhanced tracking and behavior recognition.

Result: Achieves 47% accuracy in fish detection and produces 3D outputs for better movement analysis.

Conclusion: The multi-view framework outperforms single-view approaches, offering more reliable tracking and insights into fish behavior.

Abstract: Multi-object tracking (MOT) in computer vision has made significant
advancements, yet tracking small fish in underwater environments presents
unique challenges due to complex 3D motions and data noise. Traditional
single-view MOT models often fall short in these settings. This thesis
addresses these challenges by adapting state-of-the-art single-view MOT models,
FairMOT and YOLOv8, for underwater fish detecting and tracking in ecological
studies. The core contribution of this research is the development of a
multi-view framework that utilizes stereo video inputs to enhance tracking
accuracy and fish behavior pattern recognition. By integrating and evaluating
these models on underwater fish video datasets, the study aims to demonstrate
significant improvements in precision and reliability compared to single-view
approaches. The proposed framework detects fish entities with a relative
accuracy of 47% and employs stereo-matching techniques to produce a novel 3D
output, providing a more comprehensive understanding of fish movements and
interactions

</details>


### [277] [ReactDiff: Latent Diffusion for Facial Reaction Generation](https://arxiv.org/pdf/2505.14151)
*Jiaming Li, Sheng Wang, Xin Wang, Yitao Zhu, Honglin Xiong, Zixu Zhuang, Qian Wang*

Main category: cs.CV

TL;DR: ReactDiff integrates a Multi-Modality Transformer with latent diffusion for facial reaction generation, outperforming prior methods in correlation, diversity, and realism.


<details>
  <summary>Details</summary>
Motivation: The challenge is capturing audio-visual relevance for realistic, diverse listener reactions, addressing limitations of uni-modal or simplified prior works.

Method: Proposes ReactDiff, combining a Multi-Modality Transformer with latent diffusion for fine-grained multi-modal interaction and diverse outputs.

Result: Achieves 0.26 facial reaction correlation and 0.094 diversity score, surpassing existing methods.

Conclusion: ReactDiff advances facial reaction generation with improved performance and open-sourced code.

Abstract: Given the audio-visual clip of the speaker, facial reaction generation aims
to predict the listener's facial reactions. The challenge lies in capturing the
relevance between video and audio while balancing appropriateness, realism, and
diversity. While prior works have mostly focused on uni-modal inputs or
simplified reaction mappings, recent approaches such as PerFRDiff have explored
multi-modal inputs and the one-to-many nature of appropriate reaction mappings.
In this work, we propose the Facial Reaction Diffusion (ReactDiff) framework
that uniquely integrates a Multi-Modality Transformer with conditional
diffusion in the latent space for enhanced reaction generation. Unlike existing
methods, ReactDiff leverages intra- and inter-class attention for fine-grained
multi-modal interaction, while the latent diffusion process between the encoder
and decoder enables diverse yet contextually appropriate outputs. Experimental
results demonstrate that ReactDiff significantly outperforms existing
approaches, achieving a facial reaction correlation of 0.26 and diversity score
of 0.094 while maintaining competitive realism. The code is open-sourced at
\href{https://github.com/Hunan-Tiger/ReactDiff}{github}.

</details>


### [278] [REACT 2025: the Third Multiple Appropriate Facial Reaction Generation Challenge](https://arxiv.org/pdf/2505.17223)
*Siyang Song, Micol Spitale, Xiangyu Kong, Hengde Zhu, Cheng Luo, Cristina Palmero, German Barquero, Sergio Escalera, Michel Valstar, Mohamed Daoudi, Tobias Baur, Fabien Ringeval, Andrew Howes, Elisabeth Andre, Hatice Gunes*

Main category: cs.CV

TL;DR: The paper proposes the REACT 2025 challenge to develop ML models for generating diverse, realistic facial reactions in dyadic interactions, using the MARS dataset.


<details>
  <summary>Details</summary>
Motivation: To advance ML models for generating appropriate and synchronized human-style facial reactions in response to speaker behaviors.

Method: Utilizes the MARS dataset of 137 dyadic interactions (2856 sessions) and proposes two sub-challenges: Offline and Online MAFRG.

Result: Presents challenge guidelines and baseline performance for the sub-challenges.

Conclusion: Encourages participation in REACT 2025 to benchmark and improve ML models for facial reaction generation.

Abstract: In dyadic interactions, a broad spectrum of human facial reactions might be
appropriate for responding to each human speaker behaviour. Following the
successful organisation of the REACT 2023 and REACT 2024 challenges, we are
proposing the REACT 2025 challenge encouraging the development and benchmarking
of Machine Learning (ML) models that can be used to generate multiple
appropriate, diverse, realistic and synchronised human-style facial reactions
expressed by human listeners in response to an input stimulus (i.e.,
audio-visual behaviours expressed by their corresponding speakers). As a key of
the challenge, we provide challenge participants with the first natural and
large-scale multi-modal MAFRG dataset (called MARS) recording 137 human-human
dyadic interactions containing a total of 2856 interaction sessions covering
five different topics. In addition, this paper also presents the challenge
guidelines and the performance of our baselines on the two proposed
sub-challenges: Offline MAFRG and Online MAFRG, respectively. The challenge
baseline code is publicly available at
https://github.com/reactmultimodalchallenge/baseline_react2025

</details>


### [279] [CHAOS: Chart Analysis with Outlier Samples](https://arxiv.org/pdf/2505.17235)
*Omar Moured, Yufan Chen, Ruiping Liu, Simon Reiß, Philip Torr, Jiaming Zhang, Rainer Stiefelhagen*

Main category: cs.CV

TL;DR: CHAOS is a benchmark for evaluating MLLMs' robustness to chart perturbations, covering textual and visual noise at varying severity levels.


<details>
  <summary>Details</summary>
Motivation: Real-world charts often contain noisy or challenging features, posing difficulties for MLLMs in interpretation.

Method: CHAOS introduces five textual and ten visual perturbations at three severity levels, testing 13 MLLMs on tasks like ChartQA and Chart-to-Text.

Result: Experiments reveal insights into model robustness, identifying weaknesses in handling chart perturbations.

Conclusion: CHAOS aims to guide future research in improving MLLMs' chart understanding capabilities.

Abstract: Charts play a critical role in data analysis and visualization, yet
real-world applications often present charts with challenging or noisy
features. However, "outlier charts" pose a substantial challenge even for
Multimodal Large Language Models (MLLMs), which can struggle to interpret
perturbed charts. In this work, we introduce CHAOS (CHart Analysis with Outlier
Samples), a robustness benchmark to systematically evaluate MLLMs against chart
perturbations. CHAOS encompasses five types of textual and ten types of visual
perturbations, each presented at three levels of severity (easy, mid, hard)
inspired by the study result of human evaluation. The benchmark includes 13
state-of-the-art MLLMs divided into three groups (i.e., general-, document-,
and chart-specific models) according to the training scope and data.
Comprehensive analysis involves two downstream tasks (ChartQA and
Chart-to-Text). Extensive experiments and case studies highlight critical
insights into robustness of models across chart perturbations, aiming to guide
future research in chart understanding domain. Data and code are publicly
available at: http://huggingface.co/datasets/omoured/CHAOS.

</details>


### [280] [Hypergraph Tversky-Aware Domain Incremental Learning for Brain Tumor Segmentation with Missing Modalities](https://arxiv.org/pdf/2505.16809)
*Junze Wang, Lei Fan, Weipeng Jing, Donglin Di, Yang Song, Sidong Liu, Cong Cong*

Main category: cs.CV

TL;DR: ReHyDIL improves brain tumor segmentation with missing MRI modalities using domain incremental learning and hypergraph networks, outperforming existing methods by 2% in Dice score.


<details>
  <summary>Details</summary>
Motivation: Clinical MRI often has missing modalities, degrading segmentation performance. Retraining models for new modalities is inefficient and risks overfitting.

Method: Proposes ReHyDIL with Domain Incremental Learning (DIL) and CHSNet (hypergraph-based network). Uses TAC loss for modality imbalance.

Result: Outperforms state-of-the-art methods by over 2% in Dice Similarity Coefficient on BraTS2019 dataset.

Conclusion: ReHyDIL effectively handles missing modalities and incremental learning, improving segmentation accuracy.

Abstract: Existing methods for multimodal MRI segmentation with missing modalities
typically assume that all MRI modalities are available during training.
However, in clinical practice, some modalities may be missing due to the
sequential nature of MRI acquisition, leading to performance degradation.
Furthermore, retraining models to accommodate newly available modalities can be
inefficient and may cause overfitting, potentially compromising previously
learned knowledge. To address these challenges, we propose Replay-based
Hypergraph Domain Incremental Learning (ReHyDIL) for brain tumor segmentation
with missing modalities. ReHyDIL leverages Domain Incremental Learning (DIL) to
enable the segmentation model to learn from newly acquired MRI modalities
without forgetting previously learned information. To enhance segmentation
performance across diverse patient scenarios, we introduce the Cross-Patient
Hypergraph Segmentation Network (CHSNet), which utilizes hypergraphs to capture
high-order associations between patients. Additionally, we incorporate
Tversky-Aware Contrastive (TAC) loss to effectively mitigate information
imbalance both across and within different modalities. Extensive experiments on
the BraTS2019 dataset demonstrate that ReHyDIL outperforms state-of-the-art
methods, achieving an improvement of over 2% in the Dice Similarity Coefficient
across various tumor regions.

</details>


### [281] [Extending Dataset Pruning to Object Detection: A Variance-based Approach](https://arxiv.org/pdf/2505.17245)
*Ryota Yagi*

Main category: cs.CV

TL;DR: The paper extends dataset pruning techniques from image classification to object detection, addressing key challenges and proposing a novel scoring method (VPS) to improve performance.


<details>
  <summary>Details</summary>
Motivation: Dataset pruning reduces computational costs and storage, but its application to complex tasks like object detection is underexplored.

Method: Proposes tailored solutions, including Variance-based Prediction Score (VPS), to address challenges in pruning for object detection.

Result: Outperforms prior methods on PASCAL VOC and MS COCO in mAP, showing informative sample selection is more critical than dataset size or balance.

Conclusion: Bridges dataset pruning and object detection, enabling efficient pruning for complex vision tasks.

Abstract: Dataset pruning -- selecting a small yet informative subset of training data
-- has emerged as a promising strategy for efficient machine learning, offering
significant reductions in computational cost and storage compared to
alternatives like dataset distillation. While pruning methods have shown strong
performance in image classification, their extension to more complex computer
vision tasks, particularly object detection, remains relatively underexplored.
In this paper, we present the first principled extension of classification
pruning techniques to the object detection domain, to the best of our
knowledge. We identify and address three key challenges that hinder this
transition: the Object-Level Attribution Problem, the Scoring Strategy Problem,
and the Image-Level Aggregation Problem. To overcome these, we propose tailored
solutions, including a novel scoring method called Variance-based Prediction
Score (VPS). VPS leverages both Intersection over Union (IoU) and confidence
scores to effectively identify informative training samples specific to
detection tasks. Extensive experiments on PASCAL VOC and MS COCO demonstrate
that our approach consistently outperforms prior dataset pruning methods in
terms of mean Average Precision (mAP). We also show that annotation count and
class distribution shift can influence detection performance, but selecting
informative examples is a more critical factor than dataset size or balance.
Our work bridges dataset pruning and object detection, paving the way for
dataset pruning in complex vision tasks.

</details>


### [282] [ExpertGen: Training-Free Expert Guidance for Controllable Text-to-Face Generation](https://arxiv.org/pdf/2505.17256)
*Liang Shi, Yun Fu*

Main category: cs.CV

TL;DR: ExpertGen is a training-free framework for fine-grained text-to-face generation using pre-trained expert models for precise control over facial features.


<details>
  <summary>Details</summary>
Motivation: Existing methods for fine-grained control in text-to-face generation are inflexible and resource-intensive, requiring additional training modules.

Method: ExpertGen leverages pre-trained expert models (e.g., face recognition, attribute recognition) and a latent consistency model to guide diffusion steps for realistic generation.

Result: Qualitative and quantitative results show high precision in guiding generation, with multiple experts enabling simultaneous control over diverse facial aspects.

Conclusion: ExpertGen transforms off-the-shelf expert models into plug-and-play components for controllable face generation without additional training.

Abstract: Recent advances in diffusion models have significantly improved text-to-face
generation, but achieving fine-grained control over facial features remains a
challenge. Existing methods often require training additional modules to handle
specific controls such as identity, attributes, or age, making them inflexible
and resource-intensive. We propose ExpertGen, a training-free framework that
leverages pre-trained expert models such as face recognition, facial attribute
recognition, and age estimation networks to guide generation with fine control.
Our approach uses a latent consistency model to ensure realistic and
in-distribution predictions at each diffusion step, enabling accurate guidance
signals to effectively steer the diffusion process. We show qualitatively and
quantitatively that expert models can guide the generation process with high
precision, and multiple experts can collaborate to enable simultaneous control
over diverse facial aspects. By allowing direct integration of off-the-shelf
expert models, our method transforms any such model into a plug-and-play
component for controllable face generation.

</details>


### [283] [Mitigate One, Skew Another? Tackling Intersectional Biases in Text-to-Image Models](https://arxiv.org/pdf/2505.17280)
*Pushkar Shukla, Aditya Chinchure, Emily Diana, Alexander Tolbert, Kartik Hosanagar, Vineeth N Balasubramanian, Leonid Sigal, Matthew Turk*

Main category: cs.CV

TL;DR: BiasConnect and InterMit analyze and mitigate interrelated biases in text-to-image models, showing improved fairness and efficiency.


<details>
  <summary>Details</summary>
Motivation: Understanding and addressing interconnected biases in TTI models to improve fairness without unintended consequences.

Method: Introduces BiasConnect for quantifying bias interactions and InterMit, an intersectional bias mitigation algorithm.

Result: BiasConnect shows strong correlation with outcomes; InterMit reduces bias and steps while improving image quality.

Conclusion: InterMit offers a flexible, effective solution for bias mitigation in TTI models, compatible with existing methods.

Abstract: The biases exhibited by text-to-image (TTI) models are often treated as
independent, though in reality, they may be deeply interrelated. Addressing
bias along one dimension - such as ethnicity or age - can inadvertently affect
another, like gender, either mitigating or exacerbating existing disparities.
Understanding these interdependencies is crucial for designing fairer
generative models, yet measuring such effects quantitatively remains a
challenge. To address this, we introduce BiasConnect, a novel tool for
analyzing and quantifying bias interactions in TTI models. BiasConnect uses
counterfactual interventions along different bias axes to reveal the underlying
structure of these interactions and estimates the effect of mitigating one bias
axis on another. These estimates show strong correlation (+0.65) with observed
post-mitigation outcomes. Building on BiasConnect, we propose InterMit, an
intersectional bias mitigation algorithm guided by user-defined target
distributions and priority weights. InterMit achieves lower bias (0.33 vs.
0.52) with fewer mitigation steps (2.38 vs. 3.15 average steps), and yields
superior image quality compared to traditional techniques. Although our
implementation is training-free, InterMit is modular and can be integrated with
many existing debiasing approaches for TTI models, making it a flexible and
extensible solution.

</details>


### [284] [Harnessing EHRs for Diffusion-based Anomaly Detection on Chest X-rays](https://arxiv.org/pdf/2505.17311)
*Harim Kim, Yuhan Wang, Minkyu Ahn, Heeyoul Choi, Yuyin Zhou, Charmgil Hong*

Main category: cs.CV

TL;DR: Diff3M is a multi-modal diffusion-based framework for unsupervised anomaly detection in medical imaging, integrating chest X-rays and EHRs to improve differentiation between normal and abnormal features.


<details>
  <summary>Details</summary>
Motivation: Existing diffusion-based UAD models rely only on imaging features, limiting their ability to distinguish normal anatomical variations from pathological anomalies.

Method: Diff3M uses a novel image-EHR cross-attention module and a static masking strategy to enhance anomaly detection by incorporating structured clinical context.

Result: Diff3M outperforms existing UAD methods on CheXpert and MIMIC-CXR/IV datasets, achieving state-of-the-art performance.

Conclusion: The integration of multi-modal data (X-rays and EHRs) significantly improves anomaly detection in medical imaging.

Abstract: Unsupervised anomaly detection (UAD) in medical imaging is crucial for
identifying pathological abnormalities without requiring extensive labeled
data. However, existing diffusion-based UAD models rely solely on imaging
features, limiting their ability to distinguish between normal anatomical
variations and pathological anomalies. To address this, we propose Diff3M, a
multi-modal diffusion-based framework that integrates chest X-rays and
structured Electronic Health Records (EHRs) for enhanced anomaly detection.
Specifically, we introduce a novel image-EHR cross-attention module to
incorporate structured clinical context into the image generation process,
improving the model's ability to differentiate normal from abnormal features.
Additionally, we develop a static masking strategy to enhance the
reconstruction of normal-like images from anomalies. Extensive evaluations on
CheXpert and MIMIC-CXR/IV demonstrate that Diff3M achieves state-of-the-art
performance, outperforming existing UAD methods in medical imaging. Our code is
available at this http URL https://github.com/nth221/Diff3M

</details>


### [285] [Analyzing Fine-Grained Alignment and Enhancing Vision Understanding in Multimodal Language Models](https://arxiv.org/pdf/2505.17316)
*Jiachen Jiang, Jinxin Zhou, Bo Peng, Xia Ning, Zhihui Zhu*

Main category: cs.CV

TL;DR: The paper explores improving alignment between vision embeddings and LLMs in MLLMs by analyzing the projector's role and proposing patch-aligned training for better patch-level alignment and performance.


<details>
  <summary>Details</summary>
Motivation: Enhancing Multimodal LLMs (MLLMs) by improving alignment between vision embeddings and LLMs, as current methods rely on unclear mechanisms for vision token understanding.

Method: Investigates the projector's role in compressing vision embeddings and proposes *patch-aligned training* to enhance patch-level alignment.

Result: Patch-aligned training improves compression, alignment, and MLLM performance (16% on grounding, 4% on QA, 3% on instruction-following benchmarks).

Conclusion: The proposed method effectively enhances MLLM performance and can be extended to other multimodal models.

Abstract: Achieving better alignment between vision embeddings and Large Language
Models (LLMs) is crucial for enhancing the abilities of Multimodal LLMs
(MLLMs), particularly for recent models that rely on powerful pretrained vision
encoders and LLMs. A common approach to connect the pretrained vision encoder
and LLM is through a projector applied after the vision encoder. However, the
projector is often trained to enable the LLM to generate captions, and hence
the mechanism by which LLMs understand each vision token remains unclear. In
this work, we first investigate the role of the projector in compressing vision
embeddings and aligning them with word embeddings. We show that the projector
significantly compresses visual information, removing redundant details while
preserving essential elements necessary for the LLM to understand visual
content. We then examine patch-level alignment -- the alignment between each
vision patch and its corresponding semantic words -- and propose a
*multi-semantic alignment hypothesis*. Our analysis indicates that the
projector trained by caption loss improves patch-level alignment but only to a
limited extent, resulting in weak and coarse alignment. To address this issue,
we propose *patch-aligned training* to efficiently enhance patch-level
alignment. Our experiments show that patch-aligned training (1) achieves
stronger compression capability and improved patch-level alignment, enabling
the MLLM to generate higher-quality captions, (2) improves the MLLM's
performance by 16% on referring expression grounding tasks, 4% on
question-answering tasks, and 3% on modern instruction-following benchmarks
when using the same supervised fine-tuning (SFT) setting. The proposed method
can be easily extended to other multimodal models.

</details>


### [286] [DualTalk: Dual-Speaker Interaction for 3D Talking Head Conversations](https://arxiv.org/pdf/2505.18096)
*Ziqiao Peng, Yanbo Fan, Haoyu Wu, Xuan Wang, Hongyan Liu, Jun He, Zhaoxin Fan*

Main category: cs.CV

TL;DR: The paper proposes DualTalk, a framework for generating 3D talking heads that seamlessly switch between speaking and listening roles in conversations, addressing the unnatural dynamics in existing models.


<details>
  <summary>Details</summary>
Motivation: Existing 3D talking head models focus only on speaking or listening, ignoring the natural dynamics of interactive conversations, leading to awkward transitions.

Method: DualTalk integrates dynamic behaviors of speakers and listeners, synthesizing lifelike talking heads for speaking and generating non-verbal feedback for listening. A new dataset of 50 hours of multi-round conversations was created.

Result: Experiments show DualTalk enhances the naturalness and expressiveness of 3D talking heads in dual-speaker conversations.

Conclusion: DualTalk effectively captures the interplay between speaking and listening roles, improving the realism of interactive 3D talking heads.

Abstract: In face-to-face conversations, individuals need to switch between speaking
and listening roles seamlessly. Existing 3D talking head generation models
focus solely on speaking or listening, neglecting the natural dynamics of
interactive conversation, which leads to unnatural interactions and awkward
transitions. To address this issue, we propose a new task -- multi-round
dual-speaker interaction for 3D talking head generation -- which requires
models to handle and generate both speaking and listening behaviors in
continuous conversation. To solve this task, we introduce DualTalk, a novel
unified framework that integrates the dynamic behaviors of speakers and
listeners to simulate realistic and coherent dialogue interactions. This
framework not only synthesizes lifelike talking heads when speaking but also
generates continuous and vivid non-verbal feedback when listening, effectively
capturing the interplay between the roles. We also create a new dataset
featuring 50 hours of multi-round conversations with over 1,000 characters,
where participants continuously switch between speaking and listening roles.
Extensive experiments demonstrate that our method significantly enhances the
naturalness and expressiveness of 3D talking heads in dual-speaker
conversations. We recommend watching the supplementary video:
https://ziqiaopeng.github.io/dualtalk.

</details>


### [287] [Optimizing Image Capture for Computer Vision-Powered Taxonomic Identification and Trait Recognition of Biodiversity Specimens](https://arxiv.org/pdf/2505.17317)
*Alyson East, Elizabeth G. Campolongo, Luke Meyers, S M Rayeed, Samuel Stevens, Iuliia Zarubiieva, Isadora E. Fluck, Jennifer C. Girón, Maximiliane Jousse, Scott Lowe, Kayla I Perry, Isabelle Betancourt, Noah Charney, Evan Donoso, Nathan Fox, Kim J. Landsbergen, Ekaterina Nepovinnykh, Michelle Ramirez, Parkash Singh, Khum Thapa-Magar, Matthew Thompson, Evan Waite, Tanya Berger-Wolf, Hilmar Lapp, Paula Mabee, Graham Taylor, Sydne Record*

Main category: cs.CV

TL;DR: The paper outlines key considerations for optimizing biological specimen images for computer vision, addressing metadata, standardization, and data sharing to enhance automated analysis.


<details>
  <summary>Details</summary>
Motivation: Current imaging protocols for biological specimens are designed for human interpretation, not computational analysis, limiting their utility for automated tasks like species identification.

Method: The paper synthesizes interdisciplinary insights to propose ten key considerations for imaging, including metadata, positioning, lighting, and data storage.

Result: A framework of ten interconnected recommendations is presented to improve specimen imaging for computer vision, enabling scalable automated analyses.

Conclusion: Adopting these recommendations will enhance the utility of biological collections for large-scale automated biodiversity and ecological studies.

Abstract: Biological collections house millions of specimens documenting Earth's
biodiversity, with digital images increasingly available through open-access
platforms. Most imaging protocols were developed for human visual
interpretation without considering computational analysis requirements. This
paper aims to bridge the gap between current imaging practices and the
potential for automated analysis by presenting key considerations for creating
biological specimen images optimized for computer vision applications. We
provide conceptual computer vision topics for context, addressing fundamental
concerns including model generalization, data leakage, and comprehensive
metadata documentation, and outline practical guidance on specimen imagine, and
data storage. These recommendations were synthesized through interdisciplinary
collaboration between taxonomists, collection managers, ecologists, and
computer scientists. Through this synthesis, we have identified ten
interconnected considerations that form a framework for successfully
integrating biological specimen images into computer vision pipelines. The key
elements include: (1) comprehensive metadata documentation, (2) standardized
specimen positioning, (3) consistent size and color calibration, (4) protocols
for handling multiple specimens in one image, (5) uniform background selection,
(6) controlled lighting, (7) appropriate resolution and magnification, (8)
optimal file formats, (9) robust data archiving strategies, and (10) accessible
data sharing practices. By implementing these recommendations, collection
managers, taxonomists, and biodiversity informaticians can generate images that
support automated trait extraction, species identification, and novel
ecological and evolutionary analyses at unprecedented scales. Successful
implementation lies in thorough documentation of methodological choices.

</details>


### [288] [Game-invariant Features Through Contrastive and Domain-adversarial Learning](https://arxiv.org/pdf/2505.17328)
*Dylan Kline*

Main category: cs.CV

TL;DR: A method combining contrastive learning and domain-adversarial training learns game-invariant visual features, improving cross-game generalization.


<details>
  <summary>Details</summary>
Motivation: Existing game-image encoders overfit to game-specific styles, limiting their performance on new games.

Method: Uses contrastive learning to cluster similar content and domain-adversarial training to suppress game-specific cues.

Result: Features no longer cluster by game, showing successful invariance and potential for cross-game tasks like glitch detection.

Conclusion: The approach enables generalizable game vision models with minimal retraining for new games.

Abstract: Foundational game-image encoders often overfit to game-specific visual
styles, undermining performance on downstream tasks when applied to new games.
We present a method that combines contrastive learning and domain-adversarial
training to learn game-invariant visual features. By simultaneously encouraging
similar content to cluster and discouraging game-specific cues via an
adversarial domain classifier, our approach produces embeddings that generalize
across diverse games. Experiments on the Bingsu game-image dataset (10,000
screenshots from 10 games) demonstrate that after only a few training epochs,
our model's features no longer cluster by game, indicating successful
invariance and potential for improved cross-game transfer (e.g., glitch
detection) with minimal fine-tuning. This capability paves the way for more
generalizable game vision models that require little to no retraining on new
games.

</details>


### [289] [FS-DAG: Few Shot Domain Adapting Graph Networks for Visually Rich Document Understanding](https://arxiv.org/pdf/2505.17330)
*Amit Agarwal, Srikant Panda, Kulbhushan Pachauri*

Main category: cs.CV

TL;DR: FS-DAG is a scalable, efficient model for few-shot VRDU, adapting to diverse documents with minimal data, handling OCR errors, and outperforming state-of-the-art methods.


<details>
  <summary>Details</summary>
Motivation: Addressing the need for efficient, adaptable models for visually rich document understanding in few-shot settings with practical challenges like OCR errors and domain shifts.

Method: Leverages domain-specific and language/vision backbones in a modular framework, requiring less than 90M parameters.

Result: Shows significant improvements in convergence speed and performance for information extraction tasks.

Conclusion: FS-DAG advances the development of smaller, efficient models without compromising performance, suitable for resource-limited applications.

Abstract: In this work, we propose Few Shot Domain Adapting Graph (FS-DAG), a scalable
and efficient model architecture for visually rich document understanding
(VRDU) in few-shot settings. FS-DAG leverages domain-specific and
language/vision specific backbones within a modular framework to adapt to
diverse document types with minimal data. The model is robust to practical
challenges such as handling OCR errors, misspellings, and domain shifts, which
are critical in real-world deployments. FS-DAG is highly performant with less
than 90M parameters, making it well-suited for complex real-world applications
for Information Extraction (IE) tasks where computational resources are
limited. We demonstrate FS-DAG's capability through extensive experiments for
information extraction task, showing significant improvements in convergence
speed and performance compared to state-of-the-art methods. Additionally, this
work highlights the ongoing progress in developing smaller, more efficient
models that do not compromise on performance. Code :
https://github.com/oracle-samples/fs-dag

</details>


### [290] [Temporal Differential Fields for 4D Motion Modeling via Image-to-Video Synthesis](https://arxiv.org/pdf/2505.17333)
*Xin You, Minghui Zhang, Hanxiao Zhang, Jie Yang, Nassir Navab*

Main category: cs.CV

TL;DR: The paper proposes an image-to-video (I2V) synthesis framework to simulate regular respiratory motions, addressing limitations of existing methods by using the first frame to predict future frames and ensuring temporal consistency with a Temporal Differential Diffusion Model.


<details>
  <summary>Details</summary>
Motivation: Existing methods for temporal modeling of respiratory motions require high-dose imaging scans with starting and ending frames, which are impractical due to patient movement causing dynamic backgrounds. This paper aims to overcome this limitation.

Method: The authors introduce an I2V framework to animate the first frame and predict future frames. They also develop a Temporal Differential Diffusion Model to ensure temporal consistency by generating differential fields between adjacent frames, enhanced by a prompt attention layer and field augmented layer.

Result: Experiments on ACDC cardiac and 4D Lung datasets show the method effectively simulates 4D videos along the intrinsic motion trajectory, outperforming other methods in perceptual similarity and temporal consistency.

Conclusion: The proposed I2V framework with the Temporal Differential Diffusion Model successfully addresses the limitations of existing methods, providing accurate and consistent temporal modeling of respiratory motions.

Abstract: Temporal modeling on regular respiration-induced motions is crucial to
image-guided clinical applications. Existing methods cannot simulate temporal
motions unless high-dose imaging scans including starting and ending frames
exist simultaneously. However, in the preoperative data acquisition stage, the
slight movement of patients may result in dynamic backgrounds between the first
and last frames in a respiratory period. This additional deviation can hardly
be removed by image registration, thus affecting the temporal modeling. To
address that limitation, we pioneeringly simulate the regular motion process
via the image-to-video (I2V) synthesis framework, which animates with the first
frame to forecast future frames of a given length. Besides, to promote the
temporal consistency of animated videos, we devise the Temporal Differential
Diffusion Model to generate temporal differential fields, which measure the
relative differential representations between adjacent frames. The prompt
attention layer is devised for fine-grained differential fields, and the field
augmented layer is adopted to better interact these fields with the I2V
framework, promoting more accurate temporal variation of synthesized videos.
Extensive results on ACDC cardiac and 4D Lung datasets reveal that our approach
simulates 4D videos along the intrinsic motion trajectory, rivaling other
competitive methods on perceptual similarity and temporal consistency. Codes
will be available soon.

</details>


### [291] [Render-FM: A Foundation Model for Real-time Photorealistic Volumetric Rendering](https://arxiv.org/pdf/2505.17338)
*Zhongpai Gao, Meng Zheng, Benjamin Planche, Anwesa Choudhuri, Terrence Chen, Ziyan Wu*

Main category: cs.CV

TL;DR: Render-FM is a foundation model for real-time volumetric rendering of CT scans, eliminating per-scene optimization and enabling high-quality 3D visualizations quickly.


<details>
  <summary>Details</summary>
Motivation: Current neural rendering techniques for CT scans are slow and computationally demanding, limiting clinical use.

Method: Uses an encoder-decoder architecture to regress 6D Gaussian Splatting parameters from CT volumes, pre-trained on diverse medical data.

Result: Achieves visual fidelity comparable to per-scan methods while reducing preparation time from hours to seconds.

Conclusion: Render-FM enables seamless real-time integration into clinical workflows, improving efficiency and usability.

Abstract: Volumetric rendering of Computed Tomography (CT) scans is crucial for
visualizing complex 3D anatomical structures in medical imaging. Current
high-fidelity approaches, especially neural rendering techniques, require
time-consuming per-scene optimization, limiting clinical applicability due to
computational demands and poor generalizability. We propose Render-FM, a novel
foundation model for direct, real-time volumetric rendering of CT scans.
Render-FM employs an encoder-decoder architecture that directly regresses 6D
Gaussian Splatting (6DGS) parameters from CT volumes, eliminating per-scan
optimization through large-scale pre-training on diverse medical data. By
integrating robust feature extraction with the expressive power of 6DGS, our
approach efficiently generates high-quality, real-time interactive 3D
visualizations across diverse clinical CT data. Experiments demonstrate that
Render-FM achieves visual fidelity comparable or superior to specialized
per-scan methods while drastically reducing preparation time from nearly an
hour to seconds for a single inference step. This advancement enables seamless
integration into real-time surgical planning and diagnostic workflows. The
project page is: https://gaozhongpai.github.io/renderfm/.

</details>


### [292] [Lightweight Multispectral Crop-Weed Segmentation for Precision Agriculture](https://arxiv.org/pdf/2505.07444)
*Zeynep Galymzhankyzy, Eric Martinson*

Main category: cs.CV

TL;DR: A lightweight transformer-CNN hybrid model improves crop-weed segmentation by using RGB, NIR, and RE bands, achieving 78.88% mean IoU and outperforming RGB-only models by 15.8%.


<details>
  <summary>Details</summary>
Motivation: Conventional CNN-based methods for crop-weed segmentation struggle with generalization and rely solely on RGB imagery, limiting performance in complex field conditions.

Method: The proposed model is a transformer-CNN hybrid that processes RGB, NIR, and RE bands using specialized encoders and dynamic modality integration.

Result: The model achieves a segmentation accuracy (mean IoU) of 78.88%, outperforming RGB-only models by 15.8 percentage points, with only 8.7 million parameters.

Conclusion: The model offers high accuracy, computational efficiency, and potential for real-time deployment on UAVs and edge devices, advancing precision weed management.

Abstract: Efficient crop-weed segmentation is critical for site-specific weed control
in precision agriculture. Conventional CNN-based methods struggle to generalize
and rely on RGB imagery, limiting performance under complex field conditions.
To address these challenges, we propose a lightweight transformer-CNN hybrid.
It processes RGB, Near-Infrared (NIR), and Red-Edge (RE) bands using
specialized encoders and dynamic modality integration. Evaluated on the
WeedsGalore dataset, the model achieves a segmentation accuracy (mean IoU) of
78.88%, outperforming RGB-only models by 15.8 percentage points. With only 8.7
million parameters, the model offers high accuracy, computational efficiency,
and potential for real-time deployment on Unmanned Aerial Vehicles (UAVs) and
edge devices, advancing precision weed management.

</details>


### [293] [Ocular Authentication: Fusion of Gaze and Periocular Modalities](https://arxiv.org/pdf/2505.17343)
*Dillon Lohr, Michael J. Proulx, Mehedi Hasan Raju, Oleg V. Komogortsev*

Main category: cs.CV

TL;DR: The paper explores combining eye movements and periocular images for authentication, showing a multimodal system outperforms unimodal ones and exceeds FIDO benchmarks.


<details>
  <summary>Details</summary>
Motivation: To investigate the untapped potential of combining eye movements and periocular images in a calibration-free authentication system, leveraging their complementary strengths.

Method: Proposes a multimodal authentication system evaluated on a large-scale dataset (9202 subjects) using eye-tracking signals equivalent to consumer VR devices, integrating a state-of-the-art ML architecture.

Result: The multimodal system consistently outperforms unimodal systems, surpassing the FIDO benchmark, due to the ML model's ability to capture complementary discriminative features.

Conclusion: Fusing eye movements and periocular images in a multimodal system enhances authentication performance, demonstrating scalability and effectiveness.

Abstract: This paper investigates the feasibility of fusing two eye-centric
authentication modalities-eye movements and periocular images-within a
calibration-free authentication system. While each modality has independently
shown promise for user authentication, their combination within a unified
gaze-estimation pipeline has not been thoroughly explored at scale. In this
report, we propose a multimodal authentication system and evaluate it using a
large-scale in-house dataset comprising 9202 subjects with an eye tracking (ET)
signal quality equivalent to a consumer-facing virtual reality (VR) device. Our
results show that the multimodal approach consistently outperforms both
unimodal systems across all scenarios, surpassing the FIDO benchmark. The
integration of a state-of-the-art machine learning architecture contributed
significantly to the overall authentication performance at scale, driven by the
model's ability to capture authentication representations and the complementary
discriminative characteristics of the fused modalities.

</details>


### [294] [Dual Ascent Diffusion for Inverse Problems](https://arxiv.org/pdf/2505.17353)
*Minseo Kim, Axel Levy, Gordon Wetzstein*

Main category: cs.CV

TL;DR: A new dual ascent optimization framework for solving MAP problems with diffusion model priors improves image quality, robustness to noise, speed, and faithfulness to observations.


<details>
  <summary>Details</summary>
Motivation: Existing MAP and posterior sampling approaches for ill-posed inverse problems using diffusion models are inaccurate or suboptimal due to computational approximations.

Method: Introduces a dual ascent optimization framework for solving MAP problems with diffusion model priors.

Result: Achieves better image quality, robustness to noise, faster computation, and more faithful solutions compared to state-of-the-art methods.

Conclusion: The proposed framework outperforms existing methods in solving ill-posed inverse problems with diffusion priors.

Abstract: Ill-posed inverse problems are fundamental in many domains, ranging from
astrophysics to medical imaging. Emerging diffusion models provide a powerful
prior for solving these problems. Existing maximum-a-posteriori (MAP) or
posterior sampling approaches, however, rely on different computational
approximations, leading to inaccurate or suboptimal samples. To address this
issue, we introduce a new approach to solving MAP problems with diffusion model
priors using a dual ascent optimization framework. Our framework achieves
better image quality as measured by various metrics for image restoration
problems, it is more robust to high levels of measurement noise, it is faster,
and it estimates solutions that represent the observations more faithfully than
the state of the art.

</details>


### [295] [Alignment and Safety of Diffusion Models via Reinforcement Learning and Reward Modeling: A Survey](https://arxiv.org/pdf/2505.17352)
*Preeti Lamba, Kiran Ravish, Ankita Kushwaha, Pawan Kumar*

Main category: cs.CV

TL;DR: The paper explores aligning diffusion models with human preferences using RL and reward modeling, surveys existing methods, and proposes five research directions for future work.


<details>
  <summary>Details</summary>
Motivation: Aligning diffusion models with human preferences and safety constraints is a critical challenge in generative AI.

Method: Surveys fine-tuning techniques (e.g., RL, reward modeling) and classifies them by feedback type, technique, and outcomes.

Result: Identifies five promising research directions for improving alignment, efficiency, safety, and interpretability.

Conclusion: The proposal aims to advance safer and value-aligned diffusion-based generative AI through detailed research plans and insights.

Abstract: Diffusion models have emerged as leading generative models for images and
other modalities, but aligning their outputs with human preferences and safety
constraints remains a critical challenge. This thesis proposal investigates
methods to align diffusion models using reinforcement learning (RL) and reward
modeling. We survey recent advances in fine-tuning text-to-image diffusion
models with human feedback, including reinforcement learning from human and AI
feedback, direct preference optimization, and differentiable reward approaches.
We classify these methods based on the type of feedback (human, automated,
binary or ranked preferences), the fine-tuning technique (policy gradient,
reward-weighted likelihood, direct backpropagation, etc.), and their efficiency
and safety outcomes. We compare key algorithms and frameworks, highlighting how
they improve alignment with user intent or safety standards, and discuss
inter-relationships such as how newer methods build on or diverge from earlier
ones. Based on the survey, we identify five promising research directions for
the next two years: (1) multi-objective alignment with combined rewards, (2)
efficient human feedback usage and active learning, (3) robust safety alignment
against adversarial inputs, (4) continual and online alignment of diffusion
models, and (5) interpretable and trustworthy reward modeling for generative
images. Each direction is elaborated with its problem statement, challenges,
related work, and a proposed research plan. The proposal is organized as a
comprehensive document with literature review, comparative tables of methods,
and detailed research plans, aiming to contribute new insights and techniques
for safer and value-aligned diffusion-based generative AI.

</details>


### [296] [Repurposing Marigold for Zero-Shot Metric Depth Estimation via Defocus Blur Cues](https://arxiv.org/pdf/2505.17358)
*Chinmay Talegaonkar, Nikhil Gandudi Suresh, Zachary Novack, Yash Belhe, Priyanka Nagasamudra, Nicholas Antipa*

Main category: cs.CV

TL;DR: The paper introduces a method to improve zero-shot monocular metric depth estimation (MMDE) by incorporating defocus blur cues into a pre-trained diffusion model (Marigold) without additional training.


<details>
  <summary>Details</summary>
Motivation: Existing MMDE methods struggle with out-of-distribution datasets, limiting their generalization. The goal is to enhance performance by leveraging defocus blur cues.

Method: The approach involves capturing two images (small and large aperture) from the same viewpoint, then optimizing Marigold's noise latents and metric depth scaling parameters using a defocus-blur loss function at inference.

Result: The method outperforms state-of-the-art zero-shot MMDE techniques on a real dataset, showing both quantitative and qualitative improvements.

Conclusion: Incorporating defocus blur cues into Marigold effectively enhances metric depth prediction in a training-free manner, addressing generalization challenges.

Abstract: Recent monocular metric depth estimation (MMDE) methods have made notable
progress towards zero-shot generalization. However, they still exhibit a
significant performance drop on out-of-distribution datasets. We address this
limitation by injecting defocus blur cues at inference time into Marigold, a
\textit{pre-trained} diffusion model for zero-shot, scale-invariant monocular
depth estimation (MDE). Our method effectively turns Marigold into a metric
depth predictor in a training-free manner. To incorporate defocus cues, we
capture two images with a small and a large aperture from the same viewpoint.
To recover metric depth, we then optimize the metric depth scaling parameters
and the noise latents of Marigold at inference time using gradients from a loss
function based on the defocus-blur image formation model. We compare our method
against existing state-of-the-art zero-shot MMDE methods on a self-collected
real dataset, showing quantitative and qualitative improvements.

</details>


### [297] [F-ANcGAN: An Attention-Enhanced Cycle Consistent Generative Adversarial Architecture for Synthetic Image Generation of Nanoparticles](https://arxiv.org/pdf/2505.18106)
*Varun Ajith, Anindya Pal, Saumik Bhattacharya, Sayantari Ghosh*

Main category: cs.CV

TL;DR: F-ANcGAN, an attention-enhanced GAN, generates realistic SEM images from segmentation maps with limited data, improving nanoparticle analysis.


<details>
  <summary>Details</summary>
Motivation: The lack of high-quality annotated datasets for nanoscale imaging hinders segmentation model development.

Method: F-ANcGAN uses a Style U-Net generator and U-Net segmentation with self-attention, plus augmentation, to create synthetic SEM images.

Result: Achieved FID scores of 17.65 (raw) and 10.39 (post-processed) for TiO$_2$ dataset generation.

Conclusion: The model enables scalable synthetic dataset generation, aiding segmentation tasks in resource-limited fields.

Abstract: Nanomaterial research is becoming a vital area for energy, medicine, and
materials science, and accurate analysis of the nanoparticle topology is
essential to determine their properties. Unfortunately, the lack of
high-quality annotated datasets drastically hinders the creation of strong
segmentation models for nanoscale imaging. To alleviate this problem, we
introduce F-ANcGAN, an attention-enhanced cycle consistent generative
adversarial system that can be trained using a limited number of data samples
and generates realistic scanning electron microscopy (SEM) images directly from
segmentation maps. Our model uses a Style U-Net generator and a U-Net
segmentation network equipped with self-attention to capture structural
relationships and applies augmentation methods to increase the variety of the
dataset. The architecture reached a raw FID score of 17.65 for TiO$_2$ dataset
generation, with a further reduction in FID score to nearly 10.39 by using
efficient post-processing techniques. By facilitating scalable high-fidelity
synthetic dataset generation, our approach can improve the effectiveness of
downstream segmentation task training, overcoming severe data shortage issues
in nanoparticle analysis, thus extending its applications to resource-limited
fields.

</details>


### [298] [Are GNNs Worth the Effort for IoT Botnet Detection? A Comparative Study of VAE-GNN vs. ViT-MLP and VAE-MLP Approaches](https://arxiv.org/pdf/2505.17363)
*Hassan Wasswa, Hussein Abbass, Timothy Lynar*

Main category: cs.CV

TL;DR: The paper evaluates four deep learning architectures for IoT botnet detection, finding high performance in binary classification but varied results in multiclass tasks.


<details>
  <summary>Details</summary>
Motivation: The rise in IoT-based botnet attacks drives the need for advanced detection techniques.

Method: Four architectures (VAE-MLP, VAE-GCN, VAE-GAT, ViT-MLP) are tested on the N-BaIoT dataset for binary and multiclass classification.

Result: All models excelled in binary tasks (>99.93% metrics), but GNN-based models underperformed in multiclass tasks compared to VAE-MLP and ViT-MLP.

Conclusion: VAE-MLP and ViT-MLP are superior for multiclass IoT botnet detection, while all models perform well in binary classification.

Abstract: Due to the exponential rise in IoT-based botnet attacks, researchers have
explored various advanced techniques for both dimensionality reduction and
attack detection to enhance IoT security. Among these, Variational Autoencoders
(VAE), Vision Transformers (ViT), and Graph Neural Networks (GNN), including
Graph Convolutional Networks (GCN) and Graph Attention Networks (GAT), have
garnered significant research attention in the domain of attack detection. This
study evaluates the effectiveness of four state-of-the-art deep learning
architectures for IoT botnet detection: a VAE encoder with a Multi-Layer
Perceptron (MLP), a VAE encoder with a GCN, a VAE encoder with a GAT, and a ViT
encoder with an MLP. The evaluation is conducted on a widely studied IoT
benchmark dataset--the N-BaIoT dataset for both binary and multiclass tasks.
For the binary classification task, all models achieved over 99.93% in
accuracy, recall, precision, and F1-score, with no notable differences in
performance. In contrast, for the multiclass classification task, GNN-based
models showed significantly lower performance compared to VAE-MLP and ViT-MLP,
with accuracies of 86.42%, 89.46%, 99.72%, and 98.38% for VAE-GCN, VAE-GAT,
VAE-MLP, and ViT-MLP, respectively.

</details>


### [299] [Optimizing YOLOv8 for Parking Space Detection: Comparative Analysis of Custom YOLOv8 Architecture](https://arxiv.org/pdf/2505.17364)
*Apar Pokhrel, Gia Dao*

Main category: cs.CV

TL;DR: Comparative analysis of YOLOv8 with custom backbones (ResNet-18, VGG16, EfficientNetV2, Ghost) for parking space occupancy detection, evaluating accuracy and efficiency on the PKLot dataset.


<details>
  <summary>Details</summary>
Motivation: Traditional object detection methods like YOLOv8 struggle with borderline cases (e.g., partially visible vehicles, small vehicles, poor lighting) in parking lots.

Method: Integrate and evaluate YOLOv8 with various backbone architectures (ResNet-18, VGG16, EfficientNetV2, Ghost) on the PKLot dataset.

Result: Experimental results reveal strengths and trade-offs of each backbone, aiding in model selection for parking occupancy detection.

Conclusion: Custom backbones with YOLOv8 improve detection in challenging cases, offering insights for intelligent parking management systems.

Abstract: Parking space occupancy detection is a critical component in the development
of intelligent parking management systems. Traditional object detection
approaches, such as YOLOv8, provide fast and accurate vehicle detection across
parking lots but can struggle with borderline cases, such as partially visible
vehicles, small vehicles (e.g., motorcycles), and poor lighting conditions. In
this work, we perform a comprehensive comparative analysis of customized
backbone architectures integrated with YOLOv8. Specifically, we evaluate
various backbones -- ResNet-18, VGG16, EfficientNetV2, Ghost -- on the PKLot
dataset in terms of detection accuracy and computational efficiency.
Experimental results highlight each architecture's strengths and trade-offs,
providing insight into selecting suitable models for parking occupancy.

</details>


### [300] [EVM-Fusion: An Explainable Vision Mamba Architecture with Neural Algorithmic Fusion](https://arxiv.org/pdf/2505.17367)
*Zichuan Yang*

Main category: cs.CV

TL;DR: EVM-Fusion is an explainable Vision Mamba architecture with Neural Algorithmic Fusion for multi-organ medical image classification, achieving high accuracy and interpretability.


<details>
  <summary>Details</summary>
Motivation: Addressing the need for accurate, interpretable, and generalizable medical image classification in clinical decision-making.

Method: Uses a multipath design with DenseNet, U-Net, and Vision Mamba modules, dynamically fused via cross-modal attention and NAF. Includes explainability tools like spatial attention and attention weights.

Result: Achieves 99.75% test accuracy on a 9-class multi-organ dataset and provides interpretable decision insights.

Conclusion: EVM-Fusion shows promise for trustworthy AI in medical diagnostics due to its high performance and explainability.

Abstract: Medical image classification is critical for clinical decision-making, yet
demands for accuracy, interpretability, and generalizability remain
challenging. This paper introduces EVM-Fusion, an Explainable Vision Mamba
architecture featuring a novel Neural Algorithmic Fusion (NAF) mechanism for
multi-organ medical image classification. EVM-Fusion leverages a multipath
design, where DenseNet and U-Net based pathways, enhanced by Vision Mamba (Vim)
modules, operate in parallel with a traditional feature pathway. These diverse
features are dynamically integrated via a two-stage fusion process: cross-modal
attention followed by the iterative NAF block, which learns an adaptive fusion
algorithm. Intrinsic explainability is embedded through path-specific spatial
attention, Vim {\Delta}-value maps, traditional feature SE-attention, and
cross-modal attention weights. Experiments on a diverse 9-class multi-organ
medical image dataset demonstrate EVM-Fusion's strong classification
performance, achieving 99.75% test accuracy and provide multi-faceted insights
into its decision-making process, highlighting its potential for trustworthy AI
in medical diagnostics.

</details>


### [301] [Dual-sensing driving detection model](https://arxiv.org/pdf/2505.17392)
*Leon C. C. K, Zeng Hui*

Main category: cs.CV

TL;DR: A dual-sensing driver fatigue detection method combining computer vision and physiological signals, outperforming single-modality methods with high accuracy and reliability.


<details>
  <summary>Details</summary>
Motivation: To overcome limitations of single-modality fatigue detection methods by leveraging complementary advantages of computer vision and physiological signals.

Method: An innovative architecture combining real-time facial feature analysis and physiological signal processing with advanced fusion strategies.

Result: Outperforms traditional methods in accuracy and reliability, validated in controlled and real-world driving scenarios.

Conclusion: Provides a reliable, cost-effective, and humane solution for reducing fatigue-related accidents.

Abstract: In this paper, a novel dual-sensing driver fatigue detection method combining
computer vision and physiological signal analysis is proposed. The system
exploits the complementary advantages of the two sensing modalities and breaks
through the limitations of existing single-modality methods. We introduce an
innovative architecture that combines real-time facial feature analysis with
physiological signal processing, combined with advanced fusion strategies, for
robust fatigue detection. The system is designed to run efficiently on existing
hardware while maintaining high accuracy and reliability. Through comprehensive
experiments, we demonstrate that our method outperforms traditional methods in
both controlled environments and real-world conditions, while maintaining high
accuracy. The practical applicability of the system has been verified through
extensive tests in various driving scenarios and shows great potential in
reducing fatigue-related accidents. This study contributes to the field by
providing a more reliable, cost-effective, and humane solution for driver
fatigue detection.

</details>


### [302] [Wildfire Detection Using Vision Transformer with the Wildfire Dataset](https://arxiv.org/pdf/2505.17395)
*Gowtham Raj Vuppari, Navarun Gupta, Ahmed El-Sayed, Xingguo Xiong*

Main category: cs.CV

TL;DR: The paper addresses wildfire detection using Vision Transformers (ViTs), highlighting challenges like data quality, computational costs, and real-time integration.


<details>
  <summary>Details</summary>
Motivation: The increasing frequency and devastation of wildfires, especially in California, necessitate advanced detection methods to save lives and property.

Method: A ViT model is trained on a 10.74 GB dataset of high-resolution images classified as 'fire' or 'nofire', with preprocessing steps like resizing and normalization.

Result: The approach aims to improve early wildfire detection accuracy but acknowledges challenges like false positives/negatives and scalability.

Conclusion: Effective wildfire detection requires addressing data and computational challenges, with ViTs offering potential but needing further refinement.

Abstract: The critical need for sophisticated detection techniques has been highlighted
by the rising frequency and intensity of wildfires in the US, especially in
California. In 2023, wildfires caused 130 deaths nationwide, the highest since
1990. In January 2025, Los Angeles wildfires which included the Palisades and
Eaton fires burnt approximately 40,000 acres and 12,000 buildings, and caused
loss of human lives. The devastation underscores the urgent need for effective
detection and prevention strategies. Deep learning models, such as Vision
Transformers (ViTs), can enhance early detection by processing complex image
data with high accuracy. However, wildfire detection faces challenges,
including the availability of high-quality, real-time data. Wildfires often
occur in remote areas with limited sensor coverage, and environmental factors
like smoke and cloud cover can hinder detection. Additionally, training deep
learning models is computationally expensive, and issues like false
positives/negatives and scaling remain concerns. Integrating detection systems
with real-time alert mechanisms also poses difficulties. In this work, we used
the wildfire dataset consisting of 10.74 GB high-resolution images categorized
into 'fire' and 'nofire' classes is used for training the ViT model. To prepare
the data, images are resized to 224 x 224 pixels, converted into tensor format,
and normalized using ImageNet statistics.

</details>


### [303] [D3C2-Net: Dual-Domain Deep Convolutional Coding Network for Compressive Sensing](https://arxiv.org/pdf/2207.13560)
*Weiqi Li, Bin Chen, Shuai Liu, Shijie Zhao, Bowen Du, Yongbing Zhang, Jian Zhang*

Main category: cs.CV

TL;DR: The paper introduces D3C2-Net, a dual-domain deep convolutional coding network, to improve compressive sensing by combining image- and convolutional-coding-domain priors, outperforming existing methods.


<details>
  <summary>Details</summary>
Motivation: Existing deep unfolding networks (DUNs) rely only on image-domain unfolding, limiting information transmission and reconstruction flexibility, leading to loss of details and poor performance.

Method: Develops a dual-domain optimization framework integrating image- and convolutional-coding-domain priors, converted into D3C2-Net for efficient feature transmission.

Result: Demonstrates superior performance, generalization, and practicality on 2D/3D natural, medical, and scientific signals.

Conclusion: D3C2-Net balances accuracy, complexity, and interpretability, offering significant potential for inverse imaging tasks.

Abstract: By mapping iterative optimization algorithms into neural networks (NNs), deep
unfolding networks (DUNs) exhibit well-defined and interpretable structures and
achieve remarkable success in the field of compressive sensing (CS). However,
most existing DUNs solely rely on the image-domain unfolding, which restricts
the information transmission capacity and reconstruction flexibility, leading
to their loss of image details and unsatisfactory performance. To overcome
these limitations, this paper develops a dual-domain optimization framework
that combines the priors of (1) image- and (2) convolutional-coding-domains and
offers generality to CS and other inverse imaging tasks. By converting this
optimization framework into deep NN structures, we present a Dual-Domain Deep
Convolutional Coding Network (D3C2-Net), which enjoys the ability to
efficiently transmit high-capacity self-adaptive convolutional features across
all its unfolded stages. Our theoretical analyses and experiments on simulated
and real captured data, covering 2D and 3D natural, medical, and scientific
signals, demonstrate the effectiveness, practicality, superior performance, and
generalization ability of our method over other competing approaches and its
significant potential in achieving a balance among accuracy, complexity, and
interpretability. Code is available at https://github.com/lwq20020127/D3C2-Net.

</details>


### [304] [Direct3D-S2: Gigascale 3D Generation Made Easy with Spatial Sparse Attention](https://arxiv.org/pdf/2505.17412)
*Shuang Wu, Youtian Lin, Feihu Zhang, Yifei Zeng, Yikang Yang, Yajie Bao, Jiachen Qian, Siyu Zhu, Philip Torr, Xun Cao, Yao Yao*

Main category: cs.CV

TL;DR: Direct3D S2 introduces a scalable 3D generation framework using sparse volumes and Spatial Sparse Attention, achieving high-quality results with reduced computational costs.


<details>
  <summary>Details</summary>
Motivation: Addressing computational and memory challenges in generating high-resolution 3D shapes using volumetric representations like Signed Distance Functions.

Method: Uses sparse volumes and Spatial Sparse Attention (SSA) to enhance efficiency in Diffusion Transformer computations. Includes a variational autoencoder for consistent sparse volumetric format.

Result: Achieves 3.9x speedup in forward pass and 9.6x in backward pass. Enables training at 1024 resolution with 8 GPUs, surpassing state-of-the-art methods.

Conclusion: Direct3D S2 makes gigascale 3D generation practical and accessible, outperforming existing methods in quality and efficiency.

Abstract: Generating high resolution 3D shapes using volumetric representations such as
Signed Distance Functions presents substantial computational and memory
challenges. We introduce Direct3D S2, a scalable 3D generation framework based
on sparse volumes that achieves superior output quality with dramatically
reduced training costs. Our key innovation is the Spatial Sparse Attention
mechanism, which greatly enhances the efficiency of Diffusion Transformer
computations on sparse volumetric data. SSA allows the model to effectively
process large token sets within sparse volumes, significantly reducing
computational overhead and achieving a 3.9x speedup in the forward pass and a
9.6x speedup in the backward pass. Our framework also includes a variational
autoencoder that maintains a consistent sparse volumetric format across input,
latent, and output stages. Compared to previous methods with heterogeneous
representations in 3D VAE, this unified design significantly improves training
efficiency and stability. Our model is trained on public available datasets,
and experiments demonstrate that Direct3D S2 not only surpasses
state-of-the-art methods in generation quality and efficiency, but also enables
training at 1024 resolution using only 8 GPUs, a task typically requiring at
least 32 GPUs for volumetric representations at 256 resolution, thus making
gigascale 3D generation both practical and accessible. Project page:
https://nju3dv.github.io/projects/Direct3D-S2/.

</details>


### [305] [VIBE: Video-to-Text Information Bottleneck Evaluation for TL;DR](https://arxiv.org/pdf/2505.17423)
*Shenghui Chen, Po-han Li, Sandeep Chichali, Ufuk Topcu*

Main category: cs.CV

TL;DR: VIBE is an annotation-free method to evaluate and select VLM summaries for decision-making tasks, improving accuracy and efficiency.


<details>
  <summary>Details</summary>
Motivation: Human supervision is needed for tasks requiring accuracy and efficiency, but current VLMs produce verbose outputs, and evaluation methods rely on costly human annotations.

Method: VIBE scores VLM outputs using grounding (alignment with visuals) and utility (task relevance), ranking them for selection.

Result: VIBE-selected summaries boost task accuracy by 61.23% and reduce response time by 75.77% compared to naive VLM outputs or raw video.

Conclusion: VIBE effectively enhances decision-making by providing concise, task-relevant summaries without human annotation.

Abstract: Many decision-making tasks, where both accuracy and efficiency matter, still
require human supervision. For example, tasks like traffic officers reviewing
hour-long dashcam footage or researchers screening conference videos can
benefit from concise summaries that reduce cognitive load and save time. Yet
current vision-language models (VLMs) often produce verbose, redundant outputs
that hinder task performance. Existing video caption evaluation depends on
costly human annotations and overlooks the summaries' utility in downstream
tasks. We address these gaps with Video-to-text Information Bottleneck
Evaluation (VIBE), an annotation-free method that scores VLM outputs using two
metrics: grounding (how well the summary aligns with visual content) and
utility (how informative it is for the task). VIBE selects from randomly
sampled VLM outputs by ranking them according to the two scores to support
effective human decision-making. Human studies on LearningPaper24,
SUTD-TrafficQA, and LongVideoBench show that summaries selected by VIBE
consistently improve performance-boosting task accuracy by up to 61.23% and
reducing response time by 75.77% compared to naive VLM summaries or raw video.

</details>


### [306] [Debiasing CLIP: Interpreting and Correcting Bias in Attention Heads](https://arxiv.org/pdf/2505.17425)
*Wei Jie Yeo, Rui Mao, Moloud Abdar, Erik Cambria, Ranjan Satapathy*

Main category: cs.CV

TL;DR: LTC is a framework to identify and correct spurious associations in CLIP models, improving worst-group accuracy by over 50%.


<details>
  <summary>Details</summary>
Motivation: CLIP models can learn spurious associations between target variables and confounding factors, which LTC aims to address.

Method: LTC uses a contrastive framework to identify spurious attention heads in Vision Transformers, mitigates them via ablation, and integrates discriminative features through orthogonal projection.

Result: LTC achieves over 50% gain in worst-group accuracy on biased benchmarks and provides interpretable visualizations of attention heads.

Conclusion: LTC effectively addresses spurious associations in CLIP models, enhancing performance and interpretability.

Abstract: Multimodal models like CLIP have gained significant attention due to their
remarkable zero-shot performance across various tasks. However, studies have
revealed that CLIP can inadvertently learn spurious associations between target
variables and confounding factors. To address this, we introduce
\textsc{Locate-Then-Correct} (LTC), a contrastive framework that identifies
spurious attention heads in Vision Transformers via mechanistic insights and
mitigates them through targeted ablation. Furthermore, LTC identifies salient,
task-relevant attention heads, enabling the integration of discriminative
features through orthogonal projection to improve classification performance.
We evaluate LTC on benchmarks with inherent background and gender biases,
achieving over a $>50\%$ gain in worst-group accuracy compared to non-training
post-hoc baselines. Additionally, we visualize the representation of selected
heads and find that the presented interpretation corroborates our contrastive
mechanism for identifying both spurious and salient attention heads. Code
available at https://github.com/wj210/CLIP_LTC.

</details>


### [307] [Learning Generalized and Flexible Trajectory Models from Omni-Semantic Supervision](https://arxiv.org/pdf/2505.17437)
*Yuanshao Zhu, James Jianqiao Yu, Xiangyu Zhao, Xiao Han, Qidong Liu, Xuetao Wei, Yuxuan Liang*

Main category: cs.CV

TL;DR: OmniTraj is a flexible trajectory retrieval framework integrating four modalities for accurate and efficient queries.


<details>
  <summary>Details</summary>
Motivation: Address limitations in existing trajectory retrieval methods, such as inefficiency and lack of support for condition-based queries.

Method: Proposes OmniTraj, using dedicated encoders for four modalities (raw trajectories, topology, road segments, regions) fused into a shared space.

Result: Effective in large-scale data, supports flexible multi-modality queries, and outperforms traditional methods.

Conclusion: OmniTraj offers a robust solution for modern trajectory retrieval challenges.

Abstract: The widespread adoption of mobile devices and data collection technologies
has led to an exponential increase in trajectory data, presenting significant
challenges in spatio-temporal data mining, particularly for efficient and
accurate trajectory retrieval. However, existing methods for trajectory
retrieval face notable limitations, including inefficiencies in large-scale
data, lack of support for condition-based queries, and reliance on trajectory
similarity measures. To address the above challenges, we propose OmniTraj, a
generalized and flexible omni-semantic trajectory retrieval framework that
integrates four complementary modalities or semantics -- raw trajectories,
topology, road segments, and regions -- into a unified system. Unlike
traditional approaches that are limited to computing and processing
trajectories as a single modality, OmniTraj designs dedicated encoders for each
modality, which are embedded and fused into a shared representation space. This
design enables OmniTraj to support accurate and flexible queries based on any
individual modality or combination thereof, overcoming the rigidity of
traditional similarity-based methods. Extensive experiments on two real-world
datasets demonstrate the effectiveness of OmniTraj in handling large-scale
data, providing flexible, multi-modality queries, and supporting downstream
tasks and applications.

</details>


### [308] [VEAttack: Downstream-agnostic Vision Encoder Attack against Large Vision Language Models](https://arxiv.org/pdf/2505.17440)
*Hefei Mei, Zirui Wang, Shen You, Minjing Dong, Chang Xu*

Main category: cs.CV

TL;DR: VEAttack targets LVLMs' vision encoder to generate adversarial examples without task or label dependence, reducing computational overhead.


<details>
  <summary>Details</summary>
Motivation: Address LVLMs' vulnerability to adversarial attacks by focusing on the vision encoder, avoiding costly full-model gradient computations.

Method: Minimize cosine similarity between clean and perturbed visual features by optimizing image tokens, not classification tokens.

Result: Achieved 94.5% performance degradation on image caption and 75.7% on visual question answering.

Conclusion: VEAttack is effective, generalizable, and reveals insights for LVLM attack/defense strategies.

Abstract: Large Vision-Language Models (LVLMs) have demonstrated remarkable
capabilities in multimodal understanding and generation, yet their
vulnerability to adversarial attacks raises significant robustness concerns.
While existing effective attacks always focus on task-specific white-box
settings, these approaches are limited in the context of LVLMs, which are
designed for diverse downstream tasks and require expensive full-model gradient
computations. Motivated by the pivotal role and wide adoption of the vision
encoder in LVLMs, we propose a simple yet effective Vision Encoder Attack
(VEAttack), which targets the vision encoder of LVLMs only. Specifically, we
propose to generate adversarial examples by minimizing the cosine similarity
between the clean and perturbed visual features, without accessing the
following large language models, task information, and labels. It significantly
reduces the computational overhead while eliminating the task and label
dependence of traditional white-box attacks in LVLMs. To make this simple
attack effective, we propose to perturb images by optimizing image tokens
instead of the classification token. We provide both empirical and theoretical
evidence that VEAttack can easily generalize to various tasks. VEAttack has
achieved a performance degradation of 94.5% on image caption task and 75.7% on
visual question answering task. We also reveal some key observations to provide
insights into LVLM attack/defense: 1) hidden layer variations of LLM, 2) token
attention differential, 3) M\"obius band in transfer attack, 4) low sensitivity
to attack steps. The code is available at
https://github.com/hfmei/VEAttack-LVLM

</details>


### [309] [Reflectance Prediction-based Knowledge Distillation for Robust 3D Object Detection in Compressed Point Clouds](https://arxiv.org/pdf/2505.17442)
*Hao Jing, Anhong Wang, Yifan Zhang, Donghan Bu, Junhui Hou*

Main category: cs.CV

TL;DR: The paper proposes a 3D object detection framework (RPKD) for compressed point clouds in vehicle networking, improving detection accuracy by reconstructing reflectance and using knowledge distillation.


<details>
  <summary>Details</summary>
Motivation: Existing compression systems face transmission burdens and limited detection robustness due to reflectance encoding and information loss.

Method: The RPKD framework compresses point coordinates, discards reflectance, and uses a student detector with reflectance prediction (RP) and knowledge distillation (RKD, DKD) from a teacher detector.

Result: Experiments on KITTI and Waymo datasets show improved detection accuracy, achieving 73.6 mAP at 2.146 Bpp on KITTI.

Conclusion: The RPKD framework enhances robustness and accuracy for compressed point clouds in low-bitrate vehicle networking.

Abstract: Regarding intelligent transportation systems for vehicle networking,
low-bitrate transmission via lossy point cloud compression is vital for
facilitating real-time collaborative perception among vehicles with restricted
bandwidth. In existing compression transmission systems, the sender lossily
compresses point coordinates and reflectance to generate a transmission code
stream, which faces transmission burdens from reflectance encoding and limited
detection robustness due to information loss. To address these issues, this
paper proposes a 3D object detection framework with reflectance
prediction-based knowledge distillation (RPKD). We compress point coordinates
while discarding reflectance during low-bitrate transmission, and feed the
decoded non-reflectance compressed point clouds into a student detector. The
discarded reflectance is then reconstructed by a geometry-based reflectance
prediction (RP) module within the student detector for precise detection. A
teacher detector with the same structure as student detector is designed for
performing reflectance knowledge distillation (RKD) and detection knowledge
distillation (DKD) from raw to compressed point clouds. Our RPKD framework
jointly trains detectors on both raw and compressed point clouds to improve the
student detector's robustness. Experimental results on the KITTI dataset and
Waymo Open Dataset demonstrate that our method can boost detection accuracy for
compressed point clouds across multiple code rates. Notably, at a low code rate
of 2.146 Bpp on the KITTI dataset, our RPKD-PV achieves the highest mAP of
73.6, outperforming existing detection methods with the PV-RCNN baseline.

</details>


### [310] [PawPrint: Whose Footprints Are These? Identifying Animal Individuals by Their Footprints](https://arxiv.org/pdf/2505.17445)
*Inpyo Song, Hyemin Hwang, Jangwon Lee*

Main category: cs.CV

TL;DR: The paper introduces PawPrint and PawPrint+, datasets for pet footprint identification, addressing limitations of traditional methods like GPS tags. It benchmarks deep learning and classical methods, suggesting future improvements for real-world reliability.


<details>
  <summary>Details</summary>
Motivation: The rising pet ownership and high numbers of lost pets highlight the need for better identification methods, as traditional approaches (GPS tags, ID photos) are unreliable.

Method: The study introduces PawPrint and PawPrint+ datasets and benchmarks deep neural networks (CNN, Transformers) and classical local features for footprint identification.

Result: Results show varying advantages of methods based on substrate complexity and data availability, indicating potential for combining global and local features.

Conclusion: Footprint identification offers a non-invasive alternative to traditional tags, with promising applications in pet management and wildlife conservation.

Abstract: In the United States, as of 2023, pet ownership has reached 66% of households
and continues to rise annually. This trend underscores the critical need for
effective pet identification and monitoring methods, particularly as nearly 10
million cats and dogs are reported stolen or lost each year. However,
traditional methods for finding lost animals like GPS tags or ID photos have
limitations-they can be removed, face signal issues, and depend on someone
finding and reporting the pet. To address these limitations, we introduce
PawPrint and PawPrint+, the first publicly available datasets focused on
individual-level footprint identification for dogs and cats. Through
comprehensive benchmarking of both modern deep neural networks (e.g., CNN,
Transformers) and classical local features, we observe varying advantages and
drawbacks depending on substrate complexity and data availability. These
insights suggest future directions for combining learned global representations
with local descriptors to enhance reliability across diverse, real-world
conditions. As this approach provides a non-invasive alternative to traditional
ID tags, we anticipate promising applications in ethical pet management and
wildlife conservation efforts.

</details>


### [311] [Real-time Traffic Accident Anticipation with Feature Reuse](https://arxiv.org/pdf/2505.17449)
*Inpyo Song, Jangwon Lee*

Main category: cs.CV

TL;DR: RARE is a lightweight framework for real-time traffic accident anticipation, using pre-trained object detector features and a novel loss function, achieving faster speeds and high accuracy.


<details>
  <summary>Details</summary>
Motivation: Current methods for traffic accident anticipation are computationally heavy, hindering real-world deployment. RARE aims to provide a lightweight, efficient solution.

Method: RARE reuses intermediate features from a pre-trained object detector and introduces an Attention Score Ranking Loss to prioritize accident-related objects.

Result: RARE achieves a 4-8× speedup, 13.6ms latency (73.3 FPS), and state-of-the-art accuracy on benchmarks.

Conclusion: RARE offers a fast, accurate, and interpretable solution for real-time accident anticipation, suitable for safety-critical applications.

Abstract: This paper addresses the problem of anticipating traffic accidents, which
aims to forecast potential accidents before they happen. Real-time anticipation
is crucial for safe autonomous driving, yet most methods rely on
computationally heavy modules like optical flow and intermediate feature
extractors, making real-world deployment challenging. In this paper, we thus
introduce RARE (Real-time Accident anticipation with Reused Embeddings), a
lightweight framework that capitalizes on intermediate features from a single
pre-trained object detector. By eliminating additional feature-extraction
pipelines, RARE significantly reduces latency. Furthermore, we introduce a
novel Attention Score Ranking Loss, which prioritizes higher attention on
accident-related objects over non-relevant ones. This loss enhances both
accuracy and interpretability. RARE demonstrates a 4-8 times speedup over
existing approaches on the DAD and CCD benchmarks, achieving a latency of
13.6ms per frame (73.3 FPS) on an RTX 6000. Moreover, despite its reduced
complexity, it attains state-of-the-art Average Precision and reliably
anticipates imminent collisions in real time. These results highlight RARE's
potential for safety-critical applications where timely and explainable
anticipation is essential.

</details>


### [312] [Graph Mamba for Efficient Whole Slide Image Understanding](https://arxiv.org/pdf/2505.17457)
*Jiaxuan Lu, Junyan Shi, Yuhui Lin, Fang Yan, Yue Gao, Shaoting Zhang, Xiaosong Wang*

Main category: cs.CV

TL;DR: WSI-GMamba combines GNNs and Mamba for efficient, scalable WSI analysis, outperforming Transformers with fewer FLOPs.


<details>
  <summary>Details</summary>
Motivation: Addressing scalability and computational cost limitations of existing MIL methods (GNNs, Transformers) for large-scale WSI analysis.

Method: Proposes WSI-GMamba, integrating GNNs' relational modeling with Mamba's efficiency via a GMamba block (Message Passing, Graph Scanning & Flattening, Bi-SSM).

Result: Achieves Transformer-level performance with 7x fewer FLOPs, offering high accuracy and computational efficiency.

Conclusion: WSI-GMamba provides a scalable, efficient solution for WSI analysis, balancing accuracy and computational cost.

Abstract: Whole Slide Images (WSIs) in histopathology present a significant challenge
for large-scale medical image analysis due to their high resolution, large
size, and complex tile relationships. Existing Multiple Instance Learning (MIL)
methods, such as Graph Neural Networks (GNNs) and Transformer-based models,
face limitations in scalability and computational cost. To bridge this gap, we
propose the WSI-GMamba framework, which synergistically combines the relational
modeling strengths of GNNs with the efficiency of Mamba, the State Space Model
designed for sequence learning. The proposed GMamba block integrates Message
Passing, Graph Scanning & Flattening, and feature aggregation via a
Bidirectional State Space Model (Bi-SSM), achieving Transformer-level
performance with 7* fewer FLOPs. By leveraging the complementary strengths of
lightweight GNNs and Mamba, the WSI-GMamba framework delivers a scalable
solution for large-scale WSI analysis, offering both high accuracy and
computational efficiency for slide-level classification.

</details>


### [313] [Diagnosing Vision Language Models' Perception by Leveraging Human Methods for Color Vision Deficiencies](https://arxiv.org/pdf/2505.17461)
*Kazuki Hayashi, Shintaro Ozaki, Yusuke Sakai, Hidetaka Kamigaito, Taro Watanabe*

Main category: cs.CV

TL;DR: LVLMs can explain CVDs in text but fail to simulate CVD color perception in images, highlighting the need for more inclusive multimodal AI.


<details>
  <summary>Details</summary>
Motivation: To assess LVLMs' ability to handle perceptual diversity, particularly color perception differences like CVDs, which are often overlooked.

Method: Evaluated LVLMs using the Ishihara Test to detect CVDs and assess their performance in explaining and simulating CVD-related color perception.

Result: LVLMs can describe CVDs in natural language but cannot accurately simulate CVD color perception in image-based tasks.

Conclusion: Multimodal AI systems need to better address perceptual diversity, like CVDs, to ensure inclusiveness and fairness.

Abstract: Large-scale Vision Language Models (LVLMs) are increasingly being applied to
a wide range of real-world multimodal applications, involving complex visual
and linguistic reasoning. As these models become more integrated into practical
use, they are expected to handle complex aspects of human interaction. Among
these, color perception is a fundamental yet highly variable aspect of visual
understanding. It differs across individuals due to biological factors such as
Color Vision Deficiencies (CVDs), as well as differences in culture and
language. Despite its importance, perceptual diversity has received limited
attention. In our study, we evaluate LVLMs' ability to account for individual
level perceptual variation using the Ishihara Test, a widely used method for
detecting CVDs. Our results show that LVLMs can explain CVDs in natural
language, but they cannot simulate how people with CVDs perceive color in image
based tasks. These findings highlight the need for multimodal systems that can
account for color perceptual diversity and support broader discussions on
perceptual inclusiveness and fairness in multimodal AI.

</details>


### [314] [OrionBench: A Benchmark for Chart and Human-Recognizable Object Detection in Infographics](https://arxiv.org/pdf/2505.17473)
*Jiangning Zhu, Yuxing Zhou, Zheng Wang, Juntao Yao, Yima Gu, Yuhui Yuan, Shixia Liu*

Main category: cs.CV

TL;DR: OrionBench is a benchmark for improving object detection in charts and human-recognizable objects (HROs) in infographics, aiding vision-language models (VLMs) in better chart understanding.


<details>
  <summary>Details</summary>
Motivation: Existing VLMs struggle with accurate visual grounding of infographic elements, limiting chart understanding.

Method: Introduces OrionBench with 105,000 infographics (real and synthetic) and 6.9M bounding box annotations, using model-in-the-loop and programmatic methods.

Result: Demonstrated applications include boosting VLM performance, comparing detection models, and extending to document layout/UI detection.

Conclusion: OrionBench addresses a critical gap in VLM capabilities for chart understanding and broader applications.

Abstract: Given the central role of charts in scientific, business, and communication
contexts, enhancing the chart understanding capabilities of vision-language
models (VLMs) has become increasingly critical. A key limitation of existing
VLMs lies in their inaccurate visual grounding of infographic elements,
including charts and human-recognizable objects (HROs) such as icons and
images. However, chart understanding often requires identifying relevant
elements and reasoning over them. To address this limitation, we introduce
OrionBench, a benchmark designed to support the development of accurate object
detection models for charts and HROs in infographics. It contains 26,250 real
and 78,750 synthetic infographics, with over 6.9 million bounding box
annotations. These annotations are created by combining the model-in-the-loop
and programmatic methods. We demonstrate the usefulness of OrionBench through
three applications: 1) constructing a Thinking-with-Boxes scheme to boost the
chart understanding performance of VLMs, 2) comparing existing object detection
models, and 3) applying the developed detection model to document layout and UI
element detection.

</details>


### [315] [PoseBH: Prototypical Multi-Dataset Training Beyond Human Pose Estimation](https://arxiv.org/pdf/2505.17475)
*Uyoung Jeong, Jonathan Freer, Seungryul Baek, Hyung Jin Chang, Kwang In Kim*

Main category: cs.CV

TL;DR: PoseBH is a multi-dataset training framework for pose estimation that addresses skeletal heterogeneity and limited supervision using keypoint prototypes and cross-type self-supervision, improving generalization across diverse datasets.


<details>
  <summary>Details</summary>
Motivation: Existing methods struggle with skeletal heterogeneity and limited cross-dataset supervision in multi-dataset training for pose estimation.

Method: PoseBH introduces nonparametric keypoint prototypes and a cross-type self-supervision mechanism to integrate diverse skeleton types without relying on teacher-student models.

Result: PoseBH improves generalization on whole-body and animal pose datasets (COCO-WholeBody, AP-10K, APT-36K) while maintaining performance on standard benchmarks (COCO, MPII, AIC). It also transfers well to hand and body shape estimation tasks.

Conclusion: PoseBH effectively addresses skeletal heterogeneity and limited supervision in multi-dataset training, demonstrating strong generalization and transferability across diverse pose estimation tasks.

Abstract: We study multi-dataset training (MDT) for pose estimation, where skeletal
heterogeneity presents a unique challenge that existing methods have yet to
address. In traditional domains, \eg regression and classification, MDT
typically relies on dataset merging or multi-head supervision. However, the
diversity of skeleton types and limited cross-dataset supervision complicate
integration in pose estimation. To address these challenges, we introduce
PoseBH, a new MDT framework that tackles keypoint heterogeneity and limited
supervision through two key techniques. First, we propose nonparametric
keypoint prototypes that learn within a unified embedding space, enabling
seamless integration across skeleton types. Second, we develop a cross-type
self-supervision mechanism that aligns keypoint predictions with keypoint
embedding prototypes, providing supervision without relying on teacher-student
models or additional augmentations. PoseBH substantially improves
generalization across whole-body and animal pose datasets, including
COCO-WholeBody, AP-10K, and APT-36K, while preserving performance on standard
human pose benchmarks (COCO, MPII, and AIC). Furthermore, our learned keypoint
embeddings transfer effectively to hand shape estimation (InterHand2.6M) and
human body shape estimation (3DPW). The code for PoseBH is available at:
https://github.com/uyoung-jeong/PoseBH.

</details>


### [316] [The Coherence Trap: When MLLM-Crafted Narratives Exploit Manipulated Visual Contexts](https://arxiv.org/pdf/2505.17476)
*Yuchen Zhang, Yaxiong Wang, Yujiao Wu, Lianwei Wu, Li Zhu*

Main category: cs.CV

TL;DR: The paper addresses limitations in detecting AI-generated disinformation by proposing a new adversarial pipeline using MLLMs to create realistic deceptive content, validated by a novel framework.


<details>
  <summary>Details</summary>
Motivation: Current methods fail to handle sophisticated MLLM-driven deception and rely on unrealistic misalignment artifacts, necessitating a more robust solution.

Method: Proposes an adversarial pipeline with the MDSM dataset and the AMD framework, featuring Artifact Pre-perception Encoding and Manipulation-Oriented Reasoning.

Result: The framework demonstrates superior generalization for detecting MLLM-powered multimodal deceptions.

Conclusion: The approach effectively tackles the challenges of MLLM-driven disinformation, offering a unified solution for detection.

Abstract: The detection and grounding of multimedia manipulation has emerged as a
critical challenge in combating AI-generated disinformation. While existing
methods have made progress in recent years, we identify two fundamental
limitations in current approaches: (1) Underestimation of MLLM-driven deception
risk: prevailing techniques primarily address rule-based text manipulations,
yet fail to account for sophisticated misinformation synthesized by multimodal
large language models (MLLMs) that can dynamically generate semantically
coherent, contextually plausible yet deceptive narratives conditioned on
manipulated images; (2) Unrealistic misalignment artifacts: currently focused
scenarios rely on artificially misaligned content that lacks semantic
coherence, rendering them easily detectable. To address these gaps
holistically, we propose a new adversarial pipeline that leverages MLLMs to
generate high-risk disinformation. Our approach begins with constructing the
MLLM-Driven Synthetic Multimodal (MDSM) dataset, where images are first altered
using state-of-the-art editing techniques and then paired with MLLM-generated
deceptive texts that maintain semantic consistency with the visual
manipulations. Building upon this foundation, we present the Artifact-aware
Manipulation Diagnosis via MLLM (AMD) framework featuring two key innovations:
Artifact Pre-perception Encoding strategy and Manipulation-Oriented Reasoning,
to tame MLLMs for the MDSM problem. Comprehensive experiments validate our
framework's superior generalization capabilities as a unified architecture for
detecting MLLM-powered multimodal deceptions.

</details>


### [317] [Research on Defect Detection Method of Motor Control Board Based on Image Processing](https://arxiv.org/pdf/2505.17493)
*Jingde Huang, Zhangyu Huang, Chenyu Li, Jiantong Liu*

Main category: cs.CV

TL;DR: A study on defect detection for motor control boards using image processing, achieving over 99% accuracy and suitability for production-line use.


<details>
  <summary>Details</summary>
Motivation: Defects like color inconsistencies and solder issues impact motor control board performance, necessitating improved quality control.

Method: Studied image processing, noise suppression, feature extraction, and optimized search algorithms for defect detection.

Result: Achieved over 99% accuracy in defect detection, enabling efficient large-scale production-line use.

Conclusion: The method is effective for online defect detection and offers industry solutions for integrated circuit boards.

Abstract: The motor control board has various defects such as inconsistent color
differences, incorrect plug-in positions, solder short circuits, and more.
These defects directly affect the performance and stability of the motor
control board, thereby having a negative impact on product quality. Therefore,
studying the defect detection technology of the motor control board is an
important means to improve the quality control level of the motor control
board. Firstly, the processing methods of digital images about the motor
control board were studied, and the noise suppression methods that affect image
feature extraction were analyzed. Secondly, a specific model for defect feature
extraction and color difference recognition of the tested motor control board
was established, and qualified or defective products were determined based on
feature thresholds. Thirdly, the search algorithm for defective images was
optimized. Finally, comparative experiments were conducted on the typical motor
control board, and the experimental results demonstrate that the accuracy of
the motor control board defect detection model-based on image processing
established in this paper reached over 99%. It is suitable for timely image
processing of large quantities of motor control boards on the production line,
and achieved efficient defect detection. The defect detection method can not
only be used for online detection of the motor control board defects, but also
provide solutions for the integrated circuit board defect processing for the
industry.

</details>


### [318] [RoHyDR: Robust Hybrid Diffusion Recovery for Incomplete Multimodal Emotion Recognition](https://arxiv.org/pdf/2505.17501)
*Yuehan Jin, Xiaoqing Liu, Yiyuan Yang, Zhiwen Yu, Tong Zhang, Kaixiang Yang*

Main category: cs.CV

TL;DR: RoHyDR is a novel framework for Incomplete Multimodal Emotion Recognition (IMER) that recovers missing data at unimodal, multimodal, feature, and semantic levels using hybrid diffusion and adversarial learning, outperforming state-of-the-art methods.


<details>
  <summary>Details</summary>
Motivation: Real-world noise or sensor failures often cause missing or corrupted data in multimodal emotion recognition, creating the IMER challenge.

Method: RoHyDR uses a diffusion-based generator for unimodal recovery and adversarial learning for multimodal fusion recovery, with a multi-stage optimization strategy.

Result: RoHyDR outperforms state-of-the-art IMER methods on two benchmarks, achieving robust performance under missing-modality scenarios.

Conclusion: RoHyDR effectively mitigates performance degradation caused by missing data, offering a robust solution for IMER.

Abstract: Multimodal emotion recognition analyzes emotions by combining data from
multiple sources. However, real-world noise or sensor failures often cause
missing or corrupted data, creating the Incomplete Multimodal Emotion
Recognition (IMER) challenge. In this paper, we propose Robust Hybrid Diffusion
Recovery (RoHyDR), a novel framework that performs missing-modality recovery at
unimodal, multimodal, feature, and semantic levels. For unimodal representation
recovery of missing modalities, RoHyDR exploits a diffusion-based generator to
generate distribution-consistent and semantically aligned representations from
Gaussian noise, using available modalities as conditioning. For multimodal
fusion recovery, we introduce adversarial learning to produce a realistic fused
multimodal representation and recover missing semantic content. We further
propose a multi-stage optimization strategy that enhances training stability
and efficiency. In contrast to previous work, the hybrid diffusion and
adversarial learning-based recovery mechanism in RoHyDR allows recovery of
missing information in both unimodal representation and multimodal fusion, at
both feature and semantic levels, effectively mitigating performance
degradation caused by suboptimal optimization. Comprehensive experiments
conducted on two widely used multimodal emotion recognition benchmarks
demonstrate that our proposed method outperforms state-of-the-art IMER methods,
achieving robust recognition performance under various missing-modality
scenarios. Our code will be made publicly available upon acceptance.

</details>


### [319] [Enhancing Adversarial Robustness of Vision Language Models via Adversarial Mixture Prompt Tuning](https://arxiv.org/pdf/2505.17509)
*Shiji Zhao, Qihui Zhu, Shukun Xiong, Shouwei Ruan, Yize Fan, Ranjie Duan, Qing Guo, Xingxing Wei*

Main category: cs.CV

TL;DR: Proposes Adversarial Mixture Prompt Tuning (AMPT) to improve robustness of Vision Language Models (VLMs) against adversarial attacks by learning multiple text prompts and using a conditional weight router for adaptability.


<details>
  <summary>Details</summary>
Motivation: VLMs are vulnerable to adversarial examples, and existing single-prompt methods overfit and lack generalization.

Method: AMPT learns mixture text prompts and uses a conditional weight router to adaptively combine them based on input adversarial images.

Result: Achieves better adversarial robustness than state-of-the-art methods on 11 datasets.

Conclusion: AMPT effectively enhances VLM robustness by leveraging multiple prompts and adaptive weighting.

Abstract: Large pre-trained Vision Language Models (VLMs) have excellent generalization
capabilities but are highly susceptible to adversarial examples, presenting
potential security risks. To improve the robustness of VLMs against adversarial
examples, adversarial prompt tuning methods are proposed to align the text
feature with the adversarial image feature without changing model parameters.
However, when facing various adversarial attacks, a single learnable text
prompt has insufficient generalization to align well with all adversarial image
features, which finally leads to the overfitting phenomenon. To address the
above challenge, in this paper, we empirically find that increasing the number
of learned prompts can bring more robustness improvement than a longer prompt.
Then we propose an adversarial tuning method named Adversarial Mixture Prompt
Tuning (AMPT) to enhance the generalization towards various adversarial attacks
for VLMs. AMPT aims to learn mixture text prompts to obtain more robust text
features. To further enhance the adaptability, we propose a conditional weight
router based on the input adversarial image to predict the mixture weights of
multiple learned prompts, which helps obtain sample-specific aggregated text
features aligning with different adversarial image features. A series of
experiments show that our method can achieve better adversarial robustness than
state-of-the-art methods on 11 datasets under different experimental settings.

</details>


### [320] [Do You Keep an Eye on What I Ask? Mitigating Multimodal Hallucination via Attention-Guided Ensemble Decoding](https://arxiv.org/pdf/2505.17529)
*Yeongjae Cho, Keonwoo Kim, Taebaek Hwang, Sungzoon Cho*

Main category: cs.CV

TL;DR: The paper introduces Ensemble Decoding (ED) to address object hallucination in Large Vision-Language Models (LVLMs), improving accuracy by splitting images into sub-images and weighting logit distributions via attention maps. It also proposes ED adaptive plausibility constraint and FastED for speed-critical tasks, achieving state-of-the-art results.


<details>
  <summary>Details</summary>
Motivation: LVLMs struggle with object hallucination, generating inaccurate descriptions. Existing methods face scalability issues and reliance on external modules.

Method: Proposes Ensemble Decoding (ED), which splits images into sub-images and weights logit distributions using attention maps. Introduces ED adaptive plausibility constraint and FastED for efficiency.

Result: Achieves state-of-the-art performance on hallucination benchmarks, validating the method's effectiveness.

Conclusion: ED and its variants effectively mitigate object hallucination in LVLMs, offering scalable and efficient solutions.

Abstract: Recent advancements in Large Vision-Language Models (LVLMs) have
significantly expanded their utility in tasks like image captioning and visual
question answering. However, they still struggle with object hallucination,
where models generate descriptions that inaccurately reflect the visual content
by including nonexistent objects or misrepresenting existing ones. While
previous methods, such as data augmentation and training-free approaches,
strive to tackle this issue, they still encounter scalability challenges and
often depend on additional external modules. In this work, we propose Ensemble
Decoding (ED), a novel strategy that splits the input image into sub-images and
combines logit distributions by assigning weights through the attention map.
Furthermore, we introduce ED adaptive plausibility constraint to calibrate
logit distribution and FastED, a variant designed for speed-critical
applications. Extensive experiments across hallucination benchmarks demonstrate
that our proposed method achieves state-of-the-art performance, validating the
effectiveness of our approach.

</details>


### [321] [RePrompt: Reasoning-Augmented Reprompting for Text-to-Image Generation via Reinforcement Learning](https://arxiv.org/pdf/2505.17540)
*Mingrui Wu, Lu Wang, Pu Zhao, Fangkai Yang, Jianjin Zhang, Jianfeng Liu, Yuefeng Zhan, Weihao Han, Hao Sun, Jiayi Ji, Xiaoshuai Sun, Qingwei Lin, Weiwei Deng, Dongmei Zhang, Feng Sun, Qi Zhang, Rongrong Ji*

Main category: cs.CV

TL;DR: RePrompt enhances text-to-image generation by using reinforcement learning to refine prompts, improving fidelity and composition without human-annotated data.


<details>
  <summary>Details</summary>
Motivation: Existing models fail to capture user intent from short prompts, and prior LLM-based methods produce unrealistic content due to lack of visual grounding.

Method: RePrompt introduces reasoning via reinforcement learning, training a language model to generate structured prompts optimized for image outcomes using tailored reward models.

Result: RePrompt significantly improves spatial layout and compositional generalization, achieving state-of-the-art results on benchmarks.

Conclusion: RePrompt offers a novel, effective approach to prompt enhancement, advancing text-to-image generation.

Abstract: Despite recent progress in text-to-image (T2I) generation, existing models
often struggle to faithfully capture user intentions from short and
under-specified prompts. While prior work has attempted to enhance prompts
using large language models (LLMs), these methods frequently generate stylistic
or unrealistic content due to insufficient grounding in visual semantics and
real-world composition. Inspired by recent advances in reasoning for language
model, we propose RePrompt, a novel reprompting framework that introduces
explicit reasoning into the prompt enhancement process via reinforcement
learning. Instead of relying on handcrafted rules or stylistic rewrites, our
method trains a language model to generate structured, self-reflective prompts
by optimizing for image-level outcomes. The tailored reward models assesse the
generated images in terms of human preference, semantic alignment, and visual
composition, providing indirect supervision to refine prompt generation. Our
approach enables end-to-end training without human-annotated data. Experiments
on GenEval and T2I-Compbench show that RePrompt significantly boosts spatial
layout fidelity and compositional generalization across diverse T2I backbones,
establishing new state-of-the-art results.

</details>


### [322] [T2VUnlearning: A Concept Erasing Method for Text-to-Video Diffusion Models](https://arxiv.org/pdf/2505.17550)
*Xiaoyu Ye, Songjie Cheng, Yongtao Wang, Yajiao Xiong, Yishen Li*

Main category: cs.CV

TL;DR: The paper proposes a robust unlearning method for text-to-video models to erase harmful content while preserving generation capabilities for other concepts.


<details>
  <summary>Details</summary>
Motivation: Address concerns about misuse and rights violations due to explicit/harmful content generated by T2V models, inspired by unlearning success in T2I models.

Method: Uses negatively-guided velocity prediction fine-tuning with prompt augmentation for robustness and adds localization/preservation regularization for precision.

Result: Effectively erases specific concepts while maintaining generation quality for non-target concepts, outperforming existing methods.

Conclusion: The method is successful in unlearning harmful content and preserving model utility, with models available on GitHub.

Abstract: Recent advances in text-to-video (T2V) diffusion models have significantly
enhanced the quality of generated videos. However, their ability to produce
explicit or harmful content raises concerns about misuse and potential rights
violations. Inspired by the success of unlearning techniques in erasing
undesirable concepts from text-to-image (T2I) models, we extend unlearning to
T2V models and propose a robust and precise unlearning method. Specifically, we
adopt negatively-guided velocity prediction fine-tuning and enhance it with
prompt augmentation to ensure robustness against LLM-refined prompts. To
achieve precise unlearning, we incorporate a localization and a preservation
regularization to preserve the model's ability to generate non-target concepts.
Extensive experiments demonstrate that our method effectively erases a specific
concept while preserving the model's generation capability for all other
concepts, outperforming existing methods. We provide the unlearned models in
\href{https://github.com/VDIGPKU/T2VUnlearning.git}{https://github.com/VDIGPKU/T2VUnlearning.git}.

</details>


### [323] [Center-aware Residual Anomaly Synthesis for Multi-class Industrial Anomaly Detection](https://arxiv.org/pdf/2505.17551)
*Qiyu Chen, Huiyuan Luo, Haiming Yao, Wei Luo, Zhen Qu, Chengkan Lv, Zhengtao Zhang*

Main category: cs.CV

TL;DR: CRAS is a unified model for multi-class anomaly detection, addressing inter-class interference and intra-class overlap with center-aware residual learning and distance-guided anomaly synthesis.


<details>
  <summary>Details</summary>
Motivation: Existing methods require separate models per category, increasing deployment costs, and suffer from inter-class interference and intra-class overlap, leading to missed or over-detections.

Method: CRAS uses center-aware residual learning to unify samples into a single center and distance-guided anomaly synthesis to adjust noise variance based on normal data distribution.

Result: CRAS achieves superior detection accuracy and competitive inference speed on diverse datasets and real-world applications.

Conclusion: CRAS effectively mitigates inter-class interference and intra-class overlap, offering a practical solution for multi-class anomaly detection.

Abstract: Anomaly detection plays a vital role in the inspection of industrial images.
Most existing methods require separate models for each category, resulting in
multiplied deployment costs. This highlights the challenge of developing a
unified model for multi-class anomaly detection. However, the significant
increase in inter-class interference leads to severe missed detections.
Furthermore, the intra-class overlap between normal and abnormal samples,
particularly in synthesis-based methods, cannot be ignored and may lead to
over-detection. To tackle these issues, we propose a novel Center-aware
Residual Anomaly Synthesis (CRAS) method for multi-class anomaly detection.
CRAS leverages center-aware residual learning to couple samples from different
categories into a unified center, mitigating the effects of inter-class
interference. To further reduce intra-class overlap, CRAS introduces
distance-guided anomaly synthesis that adaptively adjusts noise variance based
on normal data distribution. Experimental results on diverse datasets and
real-world industrial applications demonstrate the superior detection accuracy
and competitive inference speed of CRAS. The source code and the newly
constructed dataset are publicly available at
https://github.com/cqylunlun/CRAS.

</details>


### [324] [Deeper Diffusion Models Amplify Bias](https://arxiv.org/pdf/2505.17560)
*Shahin Hakemi, Naveed Akhtar, Ghulam Mubashar Hassan, Ajmal Mian*

Main category: cs.CV

TL;DR: The paper explores the bias-variance tradeoff in Diffusion Models (DMs), revealing risks of bias amplification and privacy compromise. It introduces a training-free method to improve image generation quality by encouraging temporary high variance during denoising.


<details>
  <summary>Details</summary>
Motivation: To better understand the internal workings of DMs, particularly the bias-variance tradeoff, and address risks like bias amplification and privacy issues.

Method: A training-free technique that encourages temporary high variance in the denoising process by partially bypassing mid-block contributions.

Result: The method consistently improves generative image quality without additional training, validated theoretically and empirically.

Conclusion: The study enhances understanding of DMs' behavior, highlights risks, and offers a practical solution for improving output quality.

Abstract: Despite the impressive performance of generative Diffusion Models (DMs),
their internal working is still not well understood, which is potentially
problematic. This paper focuses on exploring the important notion of
bias-variance tradeoff in diffusion models. Providing a systematic foundation
for this exploration, it establishes that at one extreme the diffusion models
may amplify the inherent bias in the training data and, on the other, they may
compromise the presumed privacy of the training samples. Our exploration aligns
with the memorization-generalization understanding of the generative models,
but it also expands further along this spectrum beyond ``generalization'',
revealing the risk of bias amplification in deeper models. Building on the
insights, we also introduce a training-free method to improve output quality in
text-to-image and image-to-image generation. By progressively encouraging
temporary high variance in the generation process with partial bypassing of the
mid-block's contribution in the denoising process of DMs, our method
consistently improves generative image quality with zero training cost. Our
claims are validated both theoretically and empirically.

</details>


### [325] [Model Already Knows the Best Noise: Bayesian Active Noise Selection via Attention in Video Diffusion Model](https://arxiv.org/pdf/2505.17561)
*Kwanyoung Kim, Sanghyun Kim*

Main category: cs.CV

TL;DR: ANSE improves video diffusion model quality by selecting high-quality noise seeds using attention-based uncertainty, with minimal inference time increase.


<details>
  <summary>Details</summary>
Motivation: Different noise seeds for the same prompt lead to varied video generations, but existing methods ignore internal model signals for noise selection.

Method: Proposes ANSE with BANSA, an acquisition function measuring entropy disagreement in attention samples, and a Bernoulli-masked approximation for efficient deployment.

Result: ANSE enhances video quality and coherence on CogVideoX-2B and 5B, with only 8% and 13% inference time increase.

Conclusion: ANSE offers a principled, generalizable approach to noise selection in video diffusion models.

Abstract: The choice of initial noise significantly affects the quality and prompt
alignment of video diffusion models, where different noise seeds for the same
prompt can lead to drastically different generations. While recent methods rely
on externally designed priors such as frequency filters or inter-frame
smoothing, they often overlook internal model signals that indicate which noise
seeds are inherently preferable. To address this, we propose ANSE (Active Noise
Selection for Generation), a model-aware framework that selects high-quality
noise seeds by quantifying attention-based uncertainty. At its core is BANSA
(Bayesian Active Noise Selection via Attention), an acquisition function that
measures entropy disagreement across multiple stochastic attention samples to
estimate model confidence and consistency. For efficient inference-time
deployment, we introduce a Bernoulli-masked approximation of BANSA that enables
score estimation using a single diffusion step and a subset of attention
layers. Experiments on CogVideoX-2B and 5B demonstrate that ANSE improves video
quality and temporal coherence with only an 8% and 13% increase in inference
time, respectively, providing a principled and generalizable approach to noise
selection in video diffusion. See our project page:
https://anse-project.github.io/anse-project/

</details>


### [326] [Enhancing Fourier-based Doppler Resolution with Diffusion Models](https://arxiv.org/pdf/2505.17567)
*Denisa Qosja, Kilian Barth, Simon Wagner*

Main category: cs.CV

TL;DR: AI-enhanced Doppler resolution in radar using diffusion models outperforms traditional FFT for separating closely spaced targets.


<details>
  <summary>Details</summary>
Motivation: High Doppler resolution is crucial for detecting slow-moving targets but is limited by hardware and physical constraints.

Method: Uses generative neural networks (diffusion models) to refine zero-padded FFT data.

Result: Effectively separates closely spaced targets, overcoming FFT limitations.

Conclusion: AI-based refinement improves Doppler resolution, enhancing radar target detection.

Abstract: In radar systems, high resolution in the Doppler dimension is important for
detecting slow-moving targets as it allows for more distinct separation between
these targets and clutter, or stationary objects. However, achieving sufficient
resolution is constrained by hardware capabilities and physical factors,
leading to the development of processing techniques to enhance the resolution
after acquisition. In this work, we leverage artificial intelligence to
increase the Doppler resolution in range-Doppler maps. Based on a zero-padded
FFT, a refinement via the generative neural networks of diffusion models is
achieved. We demonstrate that our method overcomes the limitations of
traditional FFT, generating data where closely spaced targets are effectively
separated.

</details>


### [327] [InfLVG: Reinforce Inference-Time Consistent Long Video Generation with GRPO](https://arxiv.org/pdf/2505.17574)
*Xueji Fang, Liyuan Ma, Zhiyang Chen, Mingyuan Zhou, Guo-jun Qi*

Main category: cs.CV

TL;DR: InfLVG is an inference-time framework for coherent long video generation, using a learnable context selection policy to maintain consistency and reduce computational costs.


<details>
  <summary>Details</summary>
Motivation: Extending text-to-video models for long, cross-scene videos is challenging due to rising computational costs and deteriorating consistency.

Method: InfLVG employs a learnable context selection policy optimized via GRPO, selecting top-K relevant tokens to maintain fixed computational costs. A hybrid reward function optimizes semantic alignment, consistency, and artifact reduction.

Result: InfLVG extends video length by up to 9×, achieving strong consistency and semantic fidelity across scenes.

Conclusion: InfLVG effectively addresses long video generation challenges without additional data, demonstrating scalability and consistency.

Abstract: Recent advances in text-to-video generation, particularly with autoregressive
models, have enabled the synthesis of high-quality videos depicting individual
scenes. However, extending these models to generate long, cross-scene videos
remains a significant challenge. As the context length grows during
autoregressive decoding, computational costs rise sharply, and the model's
ability to maintain consistency and adhere to evolving textual prompts
deteriorates. We introduce InfLVG, an inference-time framework that enables
coherent long video generation without requiring additional long-form video
data. InfLVG leverages a learnable context selection policy, optimized via
Group Relative Policy Optimization (GRPO), to dynamically identify and retain
the most semantically relevant context throughout the generation process.
Instead of accumulating the entire generation history, the policy ranks and
selects the top-$K$ most contextually relevant tokens, allowing the model to
maintain a fixed computational budget while preserving content consistency and
prompt alignment. To optimize the policy, we design a hybrid reward function
that jointly captures semantic alignment, cross-scene consistency, and artifact
reduction. To benchmark performance, we introduce the Cross-scene Video
Benchmark (CsVBench) along with an Event Prompt Set (EPS) that simulates
complex multi-scene transitions involving shared subjects and varied
actions/backgrounds. Experimental results show that InfLVG can extend video
length by up to 9$\times$, achieving strong consistency and semantic fidelity
across scenes. Our code is available at https://github.com/MAPLE-AIGC/InfLVG.

</details>


### [328] [MODEM: A Morton-Order Degradation Estimation Mechanism for Adverse Weather Image Recovery](https://arxiv.org/pdf/2505.17581)
*Hainuo Wang, Qiming Hu, Xiaojie Guo*

Main category: cs.CV

TL;DR: MODEM introduces a Morton-Order Degradation Estimation Mechanism for adaptive adverse weather image restoration, combining spatial ordering and degradation priors for superior results.


<details>
  <summary>Details</summary>
Motivation: Adverse weather causes non-uniform image degradation; accurate degradation estimation can improve restoration.

Method: Proposes MODEM with MOS2D for long-range dependency capture and DDEM for disentangling global/local degradation priors.

Result: Achieves state-of-the-art performance across benchmarks and weather types.

Conclusion: MODEM effectively models complex degradation dynamics for adaptive restoration.

Abstract: Restoring images degraded by adverse weather remains a significant challenge
due to the highly non-uniform and spatially heterogeneous nature of
weather-induced artifacts, e.g., fine-grained rain streaks versus widespread
haze. Accurately estimating the underlying degradation can intuitively provide
restoration models with more targeted and effective guidance, enabling adaptive
processing strategies. To this end, we propose a Morton-Order Degradation
Estimation Mechanism (MODEM) for adverse weather image restoration. Central to
MODEM is the Morton-Order 2D-Selective-Scan Module (MOS2D), which integrates
Morton-coded spatial ordering with selective state-space models to capture
long-range dependencies while preserving local structural coherence.
Complementing MOS2D, we introduce a Dual Degradation Estimation Module (DDEM)
that disentangles and estimates both global and local degradation priors. These
priors dynamically condition the MOS2D modules, facilitating adaptive and
context-aware restoration. Extensive experiments and ablation studies
demonstrate that MODEM achieves state-of-the-art results across multiple
benchmarks and weather types, highlighting its effectiveness in modeling
complex degradation dynamics. Our code will be released at
https://github.com/hainuo-wang/MODEM.git.

</details>


### [329] [CGS-GAN: 3D Consistent Gaussian Splatting GANs for High Resolution Human Head Synthesis](https://arxiv.org/pdf/2505.17590)
*Florian Barthel, Wieland Morgenstern, Paul Hinzer, Anna Hilsmann, Peter Eisert*

Main category: cs.CV

TL;DR: CGS-GAN is a novel 3D Gaussian Splatting GAN framework that enables stable training and high-quality 3D-consistent synthesis of human heads without view-conditioning, achieving high rendering quality and competitive FID scores.


<details>
  <summary>Details</summary>
Motivation: Existing 3D GANs compromise 3D consistency by conditioning on camera position or perform poorly for novel views. CGS-GAN addresses these limitations.

Method: Introduces multi-view regularization, adapts conditional loss, and proposes a generator architecture for stable training, efficient rendering, and scaling up to 2048² resolution.

Result: Achieves high rendering quality and 3D consistency, supported by competitive FID scores on a curated FFHQ-derived dataset.

Conclusion: CGS-GAN successfully stabilizes training and improves 3D-consistent synthesis without view-conditioning, enabling high-resolution outputs.

Abstract: Recently, 3D GANs based on 3D Gaussian splatting have been proposed for high
quality synthesis of human heads. However, existing methods stabilize training
and enhance rendering quality from steep viewpoints by conditioning the random
latent vector on the current camera position. This compromises 3D consistency,
as we observe significant identity changes when re-synthesizing the 3D head
with each camera shift. Conversely, fixing the camera to a single viewpoint
yields high-quality renderings for that perspective but results in poor
performance for novel views. Removing view-conditioning typically destabilizes
GAN training, often causing the training to collapse. In response to these
challenges, we introduce CGS-GAN, a novel 3D Gaussian Splatting GAN framework
that enables stable training and high-quality 3D-consistent synthesis of human
heads without relying on view-conditioning. To ensure training stability, we
introduce a multi-view regularization technique that enhances generator
convergence with minimal computational overhead. Additionally, we adapt the
conditional loss used in existing 3D Gaussian splatting GANs and propose a
generator architecture designed to not only stabilize training but also
facilitate efficient rendering and straightforward scaling, enabling output
resolutions up to $2048^2$. To evaluate the capabilities of CGS-GAN, we curate
a new dataset derived from FFHQ. This dataset enables very high resolutions,
focuses on larger portions of the human head, reduces view-dependent artifacts
for improved 3D consistency, and excludes images where subjects are obscured by
hands or other objects. As a result, our approach achieves very high rendering
quality, supported by competitive FID scores, while ensuring consistent 3D
scene generation. Check our our project page here:
https://fraunhoferhhi.github.io/cgs-gan/

</details>


### [330] [PathoSCOPE: Few-Shot Pathology Detection via Self-Supervised Contrastive Learning and Pathology-Informed Synthetic Embeddings](https://arxiv.org/pdf/2505.17614)
*Sinchee Chin, Yinuo Ma, Xiaochen Yang, Jing-Hao Xue, Wenming Yang*

Main category: cs.CV

TL;DR: PathoSCOPE is a few-shot unsupervised pathology detection framework requiring minimal healthy samples, using GLCL and PiEG for efficiency and performance.


<details>
  <summary>Details</summary>
Motivation: Unsupervised pathology detection needs vast healthy datasets, but hospital data is biased and privacy regulations limit access. PathoSCOPE addresses this by requiring only a few healthy samples.

Method: Proposes Global-Local Contrastive Loss (GLCL) for embedding stability and discrimination, and Pathology-informed Embedding Generation (PiEG) to synthesize pathological embeddings.

Result: Achieves state-of-the-art performance on BraTS2020 and ChestXray8 datasets with high computational efficiency (2.48 GFLOPs, 166 FPS).

Conclusion: PathoSCOPE offers a data-efficient, high-performance solution for unsupervised pathology detection, overcoming dataset limitations.

Abstract: Unsupervised pathology detection trains models on non-pathological data to
flag deviations as pathologies, offering strong generalizability for
identifying novel diseases and avoiding costly annotations. However, building
reliable normality models requires vast healthy datasets, as hospitals' data is
inherently biased toward symptomatic populations, while privacy regulations
hinder the assembly of representative healthy cohorts. To address this
limitation, we propose PathoSCOPE, a few-shot unsupervised pathology detection
framework that requires only a small set of non-pathological samples (minimum 2
shots), significantly improving data efficiency. We introduce Global-Local
Contrastive Loss (GLCL), comprised of a Local Contrastive Loss to reduce the
variability of non-pathological embeddings and a Global Contrastive Loss to
enhance the discrimination of pathological regions. We also propose a
Pathology-informed Embedding Generation (PiEG) module that synthesizes
pathological embeddings guided by the global loss, better exploiting the
limited non-pathological samples. Evaluated on the BraTS2020 and ChestXray8
datasets, PathoSCOPE achieves state-of-the-art performance among unsupervised
methods while maintaining computational efficiency (2.48 GFLOPs, 166 FPS).

</details>


### [331] [Scaling Image and Video Generation via Test-Time Evolutionary Search](https://arxiv.org/pdf/2505.17618)
*Haoran He, Jiajun Liang, Xintao Wang, Pengfei Wan, Di Zhang, Kun Gai, Ling Pan*

Main category: cs.CV

TL;DR: EvoSearch is a novel test-time scaling method for image and video generative models, using evolutionary search to improve performance without extra training.


<details>
  <summary>Details</summary>
Motivation: Addressing the gap in understanding test-time scaling for vision tasks and overcoming limitations like poor scalability and diversity loss in existing methods.

Method: EvoSearch reformulates test-time scaling as an evolutionary search problem, using selection and mutation mechanisms to refine denoising trajectories.

Result: Outperforms existing methods, achieves higher diversity, and generalizes well to unseen metrics across diffusion and flow models.

Conclusion: EvoSearch is a generalist, efficient TTS method that enhances scalability and quality in image and video generation.

Abstract: As the marginal cost of scaling computation (data and parameters) during
model pre-training continues to increase substantially, test-time scaling (TTS)
has emerged as a promising direction for improving generative model performance
by allocating additional computation at inference time. While TTS has
demonstrated significant success across multiple language tasks, there remains
a notable gap in understanding the test-time scaling behaviors of image and
video generative models (diffusion-based or flow-based models). Although recent
works have initiated exploration into inference-time strategies for vision
tasks, these approaches face critical limitations: being constrained to
task-specific domains, exhibiting poor scalability, or falling into reward
over-optimization that sacrifices sample diversity. In this paper, we propose
\textbf{Evo}lutionary \textbf{Search} (EvoSearch), a novel, generalist, and
efficient TTS method that effectively enhances the scalability of both image
and video generation across diffusion and flow models, without requiring
additional training or model expansion. EvoSearch reformulates test-time
scaling for diffusion and flow models as an evolutionary search problem,
leveraging principles from biological evolution to efficiently explore and
refine the denoising trajectory. By incorporating carefully designed selection
and mutation mechanisms tailored to the stochastic differential equation
denoising process, EvoSearch iteratively generates higher-quality offspring
while preserving population diversity. Through extensive evaluation across both
diffusion and flow architectures for image and video generation tasks, we
demonstrate that our method consistently outperforms existing approaches,
achieves higher diversity, and shows strong generalizability to unseen
evaluation metrics. Our project is available at the website
https://tinnerhrhe.github.io/evosearch.

</details>


### [332] [CAS-IQA: Teaching Vision-Language Models for Synthetic Angiography Quality Assessment](https://arxiv.org/pdf/2505.17619)
*Bo Wang, De-Xing Huang, Xiao-Hu Zhou, Mei-Jiang Gui, Nu-Fang Xiao, Jian-Long Hao, Ming-Yuan Liu, Zeng-Guang Hou*

Main category: cs.CV

TL;DR: CAS-IQA is a vision-language model for assessing synthetic X-ray angiographies, outperforming existing methods by leveraging auxiliary images and task-specific metrics.


<details>
  <summary>Details</summary>
Motivation: Low-quality synthetic angiographies increase procedural risks, and current IQA methods lack reference images and fine-grained metrics.

Method: Proposes CAS-IQA, a VLM-based framework with a MUST module for feature fusion and routing, and introduces the CAS-3K dataset.

Result: CAS-IQA significantly outperforms state-of-the-art IQA methods on the CAS-3K dataset.

Conclusion: CAS-IQA provides clinically relevant, fine-grained quality assessment for synthetic angiographies.

Abstract: Synthetic X-ray angiographies generated by modern generative models hold
great potential to reduce the use of contrast agents in vascular interventional
procedures. However, low-quality synthetic angiographies can significantly
increase procedural risk, underscoring the need for reliable image quality
assessment (IQA) methods. Existing IQA models, however, fail to leverage
auxiliary images as references during evaluation and lack fine-grained,
task-specific metrics necessary for clinical relevance. To address these
limitations, this paper proposes CAS-IQA, a vision-language model (VLM)-based
framework that predicts fine-grained quality scores by effectively
incorporating auxiliary information from related images. In the absence of
angiography datasets, CAS-3K is constructed, comprising 3,565 synthetic
angiographies along with score annotations. To ensure clinically meaningful
assessment, three task-specific evaluation metrics are defined. Furthermore, a
Multi-path featUre fuSion and rouTing (MUST) module is designed to enhance
image representations by adaptively fusing and routing visual tokens to
metric-specific branches. Extensive experiments on the CAS-3K dataset
demonstrate that CAS-IQA significantly outperforms state-of-the-art IQA methods
by a considerable margin.

</details>


### [333] [Instruct2See: Learning to Remove Any Obstructions Across Distributions](https://arxiv.org/pdf/2505.17649)
*Junhang Li, Yu Guo, Chuhua Xian, Shengfeng He*

Main category: cs.CV

TL;DR: Instruct2See is a zero-shot framework for removing obstructions from images using multi-modal prompts and dynamic masking, achieving strong generalization across seen and unseen obstacles.


<details>
  <summary>Details</summary>
Motivation: Existing methods for obstruction removal are limited to specific types of obstructions, making them impractical for real-world scenarios with diverse obstacles.

Method: The framework treats obstruction removal as a soft-hard mask restoration problem, using multi-modal prompts (visual semantics and textual instructions) processed via cross-attention. A tunable mask adapter enables dynamic soft masking for real-time adjustments.

Result: Instruct2See performs well on both in-distribution and out-of-distribution obstacles, demonstrating strong generalization without requiring prior training on specific obstructions.

Conclusion: Instruct2See offers a versatile and effective solution for obstruction removal, addressing the limitations of existing methods by leveraging multi-modal prompts and dynamic masking.

Abstract: Images are often obstructed by various obstacles due to capture limitations,
hindering the observation of objects of interest. Most existing methods address
occlusions from specific elements like fences or raindrops, but are constrained
by the wide range of real-world obstructions, making comprehensive data
collection impractical. To overcome these challenges, we propose Instruct2See,
a novel zero-shot framework capable of handling both seen and unseen obstacles.
The core idea of our approach is to unify obstruction removal by treating it as
a soft-hard mask restoration problem, where any obstruction can be represented
using multi-modal prompts, such as visual semantics and textual instructions,
processed through a cross-attention unit to enhance contextual understanding
and improve mode control. Additionally, a tunable mask adapter allows for
dynamic soft masking, enabling real-time adjustment of inaccurate masks.
Extensive experiments on both in-distribution and out-of-distribution obstacles
show that Instruct2See consistently achieves strong performance and
generalization in obstruction removal, regardless of whether the obstacles were
present during the training phase. Code and dataset are available at
https://jhscut.github.io/Instruct2See.

</details>


### [334] [EMRA-proxy: Enhancing Multi-Class Region Semantic Segmentation in Remote Sensing Images with Attention Proxy](https://arxiv.org/pdf/2505.17665)
*Yichun Yu, Yuqing Lan, Zhihuan Xing, Xiaoyi Yang, Tingyue Tang, Dan Yu*

Main category: cs.CV

TL;DR: RAPNet improves HRRS image segmentation by combining region-level contextual dependencies and global class refinement, outperforming existing methods.


<details>
  <summary>Details</summary>
Motivation: HRRS image segmentation is difficult due to complex layouts and diverse objects. CNNs lack long-range dependencies, while Transformers miss local details and are costly.

Method: RAPNet uses Contextual Region Attention (CRA) with a Transformer for region-level context and Global Class Refinement (GCR) for multi-class refinement.

Result: RAPNet achieves superior multi-class segmentation accuracy on three public datasets.

Conclusion: RAPNet effectively balances local and global features, offering a flexible and accurate solution for HRRS segmentation.

Abstract: High-resolution remote sensing (HRRS) image segmentation is challenging due
to complex spatial layouts and diverse object appearances. While CNNs excel at
capturing local features, they struggle with long-range dependencies, whereas
Transformers can model global context but often neglect local details and are
computationally expensive.We propose a novel approach, Region-Aware Proxy
Network (RAPNet), which consists of two components: Contextual Region Attention
(CRA) and Global Class Refinement (GCR). Unlike traditional methods that rely
on grid-based layouts, RAPNet operates at the region level for more flexible
segmentation. The CRA module uses a Transformer to capture region-level
contextual dependencies, generating a Semantic Region Mask (SRM). The GCR
module learns a global class attention map to refine multi-class information,
combining the SRM and attention map for accurate segmentation.Experiments on
three public datasets show that RAPNet outperforms state-of-the-art methods,
achieving superior multi-class segmentation accuracy.

</details>


### [335] [Proto-FG3D: Prototype-based Interpretable Fine-Grained 3D Shape Classification](https://arxiv.org/pdf/2505.17666)
*Shuxian Ma, Zihao Dong, Runmin Cong, Sam Kwong, Xiuli Shao*

Main category: cs.CV

TL;DR: Proto-FG3D is a prototype-based framework for fine-grained 3D shape classification, addressing limitations of parametric models by using non-parametric prototype learning for better accuracy and interpretability.


<details>
  <summary>Details</summary>
Motivation: Fine-grained 3D classification is understudied due to challenges like subtle inter-class variations, class imbalance, and interpretability limitations of parametric models.

Method: Proto-FG3D uses Prototype Association for joint multi-view and multi-category learning, Online Clustering for prototype refinement, and prototype-guided supervised learning for fine-grained discrimination.

Result: Proto-FG3D outperforms state-of-the-art methods on FG3D and ModelNet40 in accuracy, transparency, and interpretability.

Conclusion: The framework challenges conventional approaches by achieving superior performance and enabling transparent, interpretable predictions.

Abstract: Deep learning-based multi-view coarse-grained 3D shape classification has
achieved remarkable success over the past decade, leveraging the powerful
feature learning capabilities of CNN-based and ViT-based backbones. However, as
a challenging research area critical for detailed shape understanding,
fine-grained 3D classification remains understudied due to the limited
discriminative information captured during multi-view feature aggregation,
particularly for subtle inter-class variations, class imbalance, and inherent
interpretability limitations of parametric model. To address these problems, we
propose the first prototype-based framework named Proto-FG3D for fine-grained
3D shape classification, achieving a paradigm shift from parametric softmax to
non-parametric prototype learning. Firstly, Proto-FG3D establishes joint
multi-view and multi-category representation learning via Prototype
Association. Secondly, prototypes are refined via Online Clustering, improving
both the robustness of multi-view feature allocation and inter-subclass
balance. Finally, prototype-guided supervised learning is established to
enhance fine-grained discrimination via prototype-view correlation analysis and
enables ad-hoc interpretability through transparent case-based reasoning.
Experiments on FG3D and ModelNet40 show Proto-FG3D surpasses state-of-the-art
methods in accuracy, transparent predictions, and ad-hoc interpretability with
visualizations, challenging conventional fine-grained 3D recognition
approaches.

</details>


### [336] [SVL: Spike-based Vision-language Pretraining for Efficient 3D Open-world Understanding](https://arxiv.org/pdf/2505.17674)
*Xuerui Qiu, Peixi Wu, Yaozhi Wen, Shaowei Gu, Yuqi Pan, Xinhao Luo, Bo XU, Guoqi Li*

Main category: cs.CV

TL;DR: The paper introduces SVL, a Spike-based Vision-Language pretraining framework for SNNs, enhancing 3D spatio-temporal feature extraction and bridging the performance gap with ANNs.


<details>
  <summary>Details</summary>
Motivation: Existing SNNs lag behind ANNs due to inadequate pre-training, limiting generalization, task specificity, and multimodal understanding. SVL aims to address these gaps.

Method: SVL includes Multi-scale Triple Alignment (MTA) for contrastive learning across 3D, image, and text, and Re-parameterizable Vision-Language Integration (Rep-VLI) for efficient inference.

Result: SVL achieves 85.4% top-1 accuracy in zero-shot 3D classification, outperforming ANNs and prior SNNs in tasks like 3D classification, action recognition, detection, and segmentation.

Conclusion: SVL is the first scalable, generalizable, and hardware-friendly paradigm for 3D open-world understanding, effectively closing the gap between SNNs and ANNs.

Abstract: Spiking Neural Networks (SNNs) provide an energy-efficient way to extract 3D
spatio-temporal features. However, existing SNNs still exhibit a significant
performance gap compared to Artificial Neural Networks (ANNs) due to inadequate
pre-training strategies. These limitations manifest as restricted
generalization ability, task specificity, and a lack of multimodal
understanding, particularly in challenging tasks such as multimodal question
answering and zero-shot 3D classification. To overcome these challenges, we
propose a Spike-based Vision-Language (SVL) pretraining framework that empowers
SNNs with open-world 3D understanding while maintaining spike-driven
efficiency. SVL introduces two key components: (i) Multi-scale Triple Alignment
(MTA) for label-free triplet-based contrastive learning across 3D, image, and
text modalities, and (ii) Re-parameterizable Vision-Language Integration
(Rep-VLI) to enable lightweight inference without relying on large text
encoders. Extensive experiments show that SVL achieves a top-1 accuracy of
85.4% in zero-shot 3D classification, surpassing advanced ANN models, and
consistently outperforms prior SNNs on downstream tasks, including 3D
classification (+6.1%), DVS action recognition (+2.1%), 3D detection (+1.1%),
and 3D segmentation (+2.1%) with remarkable efficiency. Moreover, SVL enables
SNNs to perform open-world 3D question answering, sometimes outperforming ANNs.
To the best of our knowledge, SVL represents the first scalable, generalizable,
and hardware-friendly paradigm for 3D open-world understanding, effectively
bridging the gap between SNNs and ANNs in complex open-world understanding
tasks. Code is available https://github.com/bollossom/SVL.

</details>


### [337] [Towards Dynamic 3D Reconstruction of Hand-Instrument Interaction in Ophthalmic Surgery](https://arxiv.org/pdf/2505.17677)
*Ming Hu, Zhendi Yu, Feilong Tang, Kaiwen Chen, Yulong Li, Imran Razzak, Junjun He, Tolga Birdal, Kaijing Zhou, Zongyuan Ge*

Main category: cs.CV

TL;DR: OphNet-3D is a large-scale RGB-D dataset for 3D reconstruction in ophthalmic surgery, featuring fine-grained annotations and a multi-stage annotation pipeline. It introduces benchmarks and models (H-Net, OH-Net) that outperform existing methods.


<details>
  <summary>Details</summary>
Motivation: The lack of realistic datasets and reliable annotation tools for 3D reconstruction in ophthalmic surgery motivated the creation of OphNet-3D.

Method: A multi-stage automatic annotation pipeline integrates multi-view data, motion priors, and biomechanical constraints. H-Net and OH-Net models are proposed for hand and instrument reconstruction.

Result: The models achieve over 2mm improvement in MPJPE and up to 23% in ADD-S metrics.

Conclusion: OphNet-3D and the proposed models significantly advance 3D reconstruction in ophthalmic surgery.

Abstract: Accurate 3D reconstruction of hands and instruments is critical for
vision-based analysis of ophthalmic microsurgery, yet progress has been
hampered by the lack of realistic, large-scale datasets and reliable annotation
tools. In this work, we introduce OphNet-3D, the first extensive RGB-D dynamic
3D reconstruction dataset for ophthalmic surgery, comprising 41 sequences from
40 surgeons and totaling 7.1 million frames, with fine-grained annotations of
12 surgical phases, 10 instrument categories, dense MANO hand meshes, and full
6-DoF instrument poses. To scalably produce high-fidelity labels, we design a
multi-stage automatic annotation pipeline that integrates multi-view data
observation, data-driven motion prior with cross-view geometric consistency and
biomechanical constraints, along with a combination of collision-aware
interaction constraints for instrument interactions. Building upon OphNet-3D,
we establish two challenging benchmarks-bimanual hand pose estimation and
hand-instrument interaction reconstruction-and propose two dedicated
architectures: H-Net for dual-hand mesh recovery and OH-Net for joint
reconstruction of two-hand-two-instrument interactions. These models leverage a
novel spatial reasoning module with weak-perspective camera modeling and
collision-aware center-based representation. Both architectures outperform
existing methods by substantial margins, achieving improvements of over 2mm in
Mean Per Joint Position Error (MPJPE) and up to 23% in ADD-S metrics for hand
and instrument reconstruction, respectively.

</details>


### [338] [5G-DIL: Domain Incremental Learning with Similarity-Aware Sampling for Dynamic 5G Indoor Localization](https://arxiv.org/pdf/2505.17684)
*Nisha Lakshmana Raichur, Lucas Heublein, Christopher Mutschler, Felix Ott*

Main category: cs.CV

TL;DR: The paper introduces 5G-DIL, a domain incremental learning approach for 5G indoor localization, using similarity-aware sampling to adapt efficiently to environmental changes with minimal training data.


<details>
  <summary>Details</summary>
Motivation: Learning-based indoor positioning methods degrade under environmental changes, and retraining is resource-intensive. The goal is to enable rapid adaptation without full retraining.

Method: Proposes a similarity-aware sampling technique based on Chebyshev distance to select exemplars from previous environments, training only on modified regions.

Result: Achieves 0.261 meters MAE positioning error, requires only 50 exemplars, and reduces training time significantly.

Conclusion: 5G-DIL effectively adapts to dynamic environments with minimal data and resources, maintaining high accuracy.

Abstract: Indoor positioning based on 5G data has achieved high accuracy through the
adoption of recent machine learning (ML) techniques. However, the performance
of learning-based methods degrades significantly when environmental conditions
change, thereby hindering their applicability to new scenarios. Acquiring new
training data for each environmental change and fine-tuning ML models is both
time-consuming and resource-intensive. This paper introduces a domain
incremental learning (DIL) approach for dynamic 5G indoor localization, called
5G-DIL, enabling rapid adaptation to environmental changes. We present a novel
similarity-aware sampling technique based on the Chebyshev distance, designed
to efficiently select specific exemplars from the previous environment while
training only on the modified regions of the new environment. This avoids the
need to train on the entire region, significantly reducing the time and
resources required for adaptation without compromising localization accuracy.
This approach requires as few as 50 exemplars from adaptation domains,
significantly reducing training time while maintaining high positioning
accuracy in previous environments. Comparative evaluations against
state-of-the-art DIL techniques on a challenging real-world indoor dataset
demonstrate the effectiveness of the proposed sample selection method. Our
approach is adaptable to real-world non-line-of-sight propagation scenarios and
achieves an MAE positioning error of 0.261 meters, even under dynamic
environmental conditions. Code:
https://gitlab.cc-asp.fraunhofer.de/5g-pos/5g-dil

</details>


### [339] [FutureSightDrive: Thinking Visually with Spatio-Temporal CoT for Autonomous Driving](https://arxiv.org/pdf/2505.17685)
*Shuang Zeng, Xinyuan Chang, Mengwei Xie, Xinran Liu, Yifan Bai, Zheng Pan, Mu Xu, Xing Wei*

Main category: cs.CV

TL;DR: A spatio-temporal Chain-of-Thought (CoT) reasoning method is proposed for visual language models (VLMs) to enhance autonomous driving by enabling visual thinking and reducing information loss.


<details>
  <summary>Details</summary>
Motivation: Existing VLMs use abstract symbolic CoT, causing spatio-temporal ambiguity and fine-grained information loss, prompting the need for visual reasoning.

Method: The VLM acts as a world model, generating unified image frames for future state prediction, combining spatial (perception results) and temporal (future frames) relationships. A unified pretraining paradigm integrates visual generation and understanding.

Result: The method effectively advances autonomous driving by improving visual reasoning capabilities.

Conclusion: The proposed spatio-temporal CoT reasoning method successfully bridges the gap between symbolic logic and visual reasoning in autonomous driving.

Abstract: Visual language models (VLMs) have attracted increasing interest in
autonomous driving due to their powerful reasoning capabilities. However,
existing VLMs typically utilize discrete text Chain-of-Thought (CoT) tailored
to the current scenario, which essentially represents highly abstract and
symbolic compression of visual information, potentially leading to
spatio-temporal relationship ambiguity and fine-grained information loss. Is
autonomous driving better modeled on real-world simulation and imagination than
on pure symbolic logic? In this paper, we propose a spatio-temporal CoT
reasoning method that enables models to think visually. First, VLM serves as a
world model to generate unified image frame for predicting future world states:
where perception results (e.g., lane divider and 3D detection) represent the
future spatial relationships, and ordinary future frame represent the temporal
evolution relationships. This spatio-temporal CoT then serves as intermediate
reasoning steps, enabling the VLM to function as an inverse dynamics model for
trajectory planning based on current observations and future predictions. To
implement visual generation in VLMs, we propose a unified pretraining paradigm
integrating visual generation and understanding, along with a progressive
visual CoT enhancing autoregressive image generation. Extensive experimental
results demonstrate the effectiveness of the proposed method, advancing
autonomous driving towards visual reasoning.

</details>


### [340] [Semi-Supervised Medical Image Segmentation via Dual Networks](https://arxiv.org/pdf/2505.17690)
*Yunyao Lu, Yihang Wu, Reem Kateb, Ahmad Chaddad*

Main category: cs.CV

TL;DR: Proposes a semi-supervised 3D medical image segmentation method with dual-network architecture and contrastive learning to reduce reliance on labeled data and improve performance.


<details>
  <summary>Details</summary>
Motivation: Challenges in obtaining large labeled datasets and issues with noisy pseudo-labels in semi-supervised methods.

Method: Uses a dual-network architecture for better contextual information and reliable pseudo-labels, plus self-supervised contrastive learning for enhanced representation.

Result: Outperforms state-of-the-art techniques on clinical MRI datasets.

Conclusion: The method effectively reduces dependency on labeled data and improves segmentation accuracy.

Abstract: Traditional supervised medical image segmentation models require large
amounts of labeled data for training; however, obtaining such large-scale
labeled datasets in the real world is extremely challenging. Recent
semi-supervised segmentation models also suffer from noisy pseudo-label issue
and limited supervision in feature space. To solve these challenges, we propose
an innovative semi-supervised 3D medical image segmentation method to reduce
the dependency on large, expert-labeled datasets. Furthermore, we introduce a
dual-network architecture to address the limitations of existing methods in
using contextual information and generating reliable pseudo-labels. In
addition, a self-supervised contrastive learning strategy is used to enhance
the representation of the network and reduce prediction uncertainty by
distinguishing between reliable and unreliable predictions. Experiments on
clinical magnetic resonance imaging demonstrate that our approach outperforms
state-of-the-art techniques. Our code is available at
https://github.com/AIPMLab/Semi-supervised-Segmentation.

</details>


### [341] [ViP$^2$-CLIP: Visual-Perception Prompting with Unified Alignment for Zero-Shot Anomaly Detection](https://arxiv.org/pdf/2505.17692)
*Ziteng Yang, Jingzehua Xu, Yanshu Li, Zepeng Li, Yeqiang Wang, Xinghui Li*

Main category: cs.CV

TL;DR: ViP²-CLIP introduces a Visual-Perception Prompting mechanism for zero-shot anomaly detection, eliminating manual templates and class-name dependencies for better performance.


<details>
  <summary>Details</summary>
Motivation: Existing CLIP-based methods for zero-shot anomaly detection rely on handcrafted or static prompts, which are costly and inflexible, and are sensitive to class-name wording.

Method: ViP²-CLIP uses a Visual-Perception Prompting (ViP-Prompt) mechanism to fuse global and multi-scale local visual context for adaptive, fine-grained textual prompts.

Result: ViP²-CLIP achieves state-of-the-art performance on 15 industrial and medical benchmarks with robust cross-domain generalization.

Conclusion: ViP²-CLIP's adaptive prompting mechanism overcomes limitations of existing methods, offering superior anomaly detection without manual templates or class-name priors.

Abstract: Zero-shot anomaly detection (ZSAD) aims to detect anomalies without any
target domain training samples, relying solely on external auxiliary data.
Existing CLIP-based methods attempt to activate the model's ZSAD potential via
handcrafted or static learnable prompts. The former incur high engineering
costs and limited semantic coverage, whereas the latter apply identical
descriptions across diverse anomaly types, thus fail to adapt to complex
variations. Furthermore, since CLIP is originally pretrained on large-scale
classification tasks, its anomaly segmentation quality is highly sensitive to
the exact wording of class names, severely constraining prompting strategies
that depend on class labels. To address these challenges, we introduce
ViP$^{2}$-CLIP. The key insight of ViP$^{2}$-CLIP is a Visual-Perception
Prompting (ViP-Prompt) mechanism, which fuses global and multi-scale local
visual context to adaptively generate fine-grained textual prompts, eliminating
manual templates and class-name priors. This design enables our model to focus
on precise abnormal regions, making it particularly valuable when category
labels are ambiguous or privacy-constrained. Extensive experiments on 15
industrial and medical benchmarks demonstrate that ViP$^{2}$-CLIP achieves
state-of-the-art performance and robust cross-domain generalization.

</details>


### [342] [Seek-CAD: A Self-refined Generative Modeling for 3D Parametric CAD Using Local Inference via DeepSeek](https://arxiv.org/pdf/2505.17702)
*Xueyang Li, Jiahao Li, Yu Song, Yunzhong Lou, Xiangdong Zhou*

Main category: cs.CV

TL;DR: Seek-CAD introduces a training-free method using the open-source LLM DeepSeek-R1 for CAD parametric model generation, incorporating visual and CoT feedback for refinement.


<details>
  <summary>Details</summary>
Motivation: To address the high cost and limitations of closed-source LLMs in CAD generative modeling by leveraging open-source alternatives and feedback mechanisms.

Method: Uses DeepSeek-R1 for initial CAD model generation, renders step-wise images, processes them with a VLM and CoT feedback, and refines the model iteratively.

Result: Seek-CAD demonstrates effectiveness in generating CAD models, validated through extensive experiments.

Conclusion: The study pioneers locally deployed open-source LLMs for CAD modeling, offering a cost-effective and flexible solution with industrial applicability.

Abstract: The advent of Computer-Aided Design (CAD) generative modeling will
significantly transform the design of industrial products. The recent research
endeavor has extended into the realm of Large Language Models (LLMs). In
contrast to fine-tuning methods, training-free approaches typically utilize the
advanced closed-source LLMs, thereby offering enhanced flexibility and
efficiency in the development of AI agents for generating CAD parametric
models. However, the substantial cost and limitations of local deployment of
the top-tier closed-source LLMs pose challenges in practical applications. The
Seek-CAD is the pioneer exploration of locally deployed open-source inference
LLM DeepSeek-R1 for CAD parametric model generation with a training-free
methodology. This study is the first investigation to incorporate both visual
and Chain-of-Thought (CoT) feedback within the self-refinement mechanism for
generating CAD models. Specifically, the initial generated parametric CAD model
is rendered into a sequence of step-wise perspective images, which are
subsequently processed by a Vision Language Model (VLM) alongside the
corresponding CoTs derived from DeepSeek-R1 to assess the CAD model generation.
Then, the feedback is utilized by DeepSeek-R1 to refine the initial generated
model for the next round of generation. Moreover, we present an innovative 3D
CAD model dataset structured around the SSR (Sketch, Sketch-based feature, and
Refinements) triple design paradigm. This dataset encompasses a wide range of
CAD commands, thereby aligning effectively with industrial application
requirements and proving suitable for the generation of LLMs. Extensive
experiments validate the effectiveness of Seek-CAD under various metrics.

</details>


### [343] [SeaLion: Semantic Part-Aware Latent Point Diffusion Models for 3D Generation](https://arxiv.org/pdf/2505.17721)
*Dekai Zhu, Yan Di, Stefan Gavranovic, Slobodan Ilic*

Main category: cs.CV

TL;DR: SeaLion is a diffusion model for generating point clouds with segmentation labels, introducing a part-aware latent diffusion technique and a novel evaluation metric (p-CD). It outperforms DiffFacto and supports semi-supervised training and applications like data augmentation and 3D editing.


<details>
  <summary>Details</summary>
Motivation: Addressing the lack of focus on generating point clouds with segmentation labels and the absence of dedicated evaluation metrics for this task.

Method: Uses semantic part-aware latent point diffusion to jointly predict noise and segmentation labels during denoising, decoding latent points into labeled point clouds. Introduces part-aware Chamfer distance (p-CD) for evaluation.

Result: Outperforms DiffFacto by 13.33% and 6.52% on 1-NNA (p-CD) across ShapeNet and IntrA datasets. Supports semi-supervised training and applications like data augmentation and 3D editing.

Conclusion: SeaLion advances point cloud generation with segmentation labels, offering high-quality results, reduced labeling needs, and practical applications.

Abstract: Denoising diffusion probabilistic models have achieved significant success in
point cloud generation, enabling numerous downstream applications, such as
generative data augmentation and 3D model editing. However, little attention
has been given to generating point clouds with point-wise segmentation labels,
as well as to developing evaluation metrics for this task. Therefore, in this
paper, we present SeaLion, a novel diffusion model designed to generate
high-quality and diverse point clouds with fine-grained segmentation labels.
Specifically, we introduce the semantic part-aware latent point diffusion
technique, which leverages the intermediate features of the generative models
to jointly predict the noise for perturbed latent points and associated part
segmentation labels during the denoising process, and subsequently decodes the
latent points to point clouds conditioned on part segmentation labels. To
effectively evaluate the quality of generated point clouds, we introduce a
novel point cloud pairwise distance calculation method named part-aware Chamfer
distance (p-CD). This method enables existing metrics, such as 1-NNA, to
measure both the local structural quality and inter-part coherence of generated
point clouds. Experiments on the large-scale synthetic dataset ShapeNet and
real-world medical dataset IntrA demonstrate that SeaLion achieves remarkable
performance in generation quality and diversity, outperforming the existing
state-of-the-art model, DiffFacto, by 13.33% and 6.52% on 1-NNA (p-CD) across
the two datasets. Experimental analysis shows that SeaLion can be trained
semi-supervised, thereby reducing the demand for labeling efforts. Lastly, we
validate the applicability of SeaLion in generative data augmentation for
training segmentation models and the capability of SeaLion to serve as a tool
for part-aware 3D shape editing.

</details>


### [344] [Slot-MLLM: Object-Centric Visual Tokenization for Multimodal LLM](https://arxiv.org/pdf/2505.17726)
*Donghwan Chi, Hyomin Kim, Yoonjin Oh, Yongjin Kim, Donghoon Lee, Daejin Jo, Jongmin Kim, Junyeob Baek, Sungjin Ahn, Sungwoong Kim*

Main category: cs.CV

TL;DR: Proposes an object-centric visual tokenizer for MLLMs using Slot Attention to improve local detail understanding and generation, outperforming existing methods.


<details>
  <summary>Details</summary>
Motivation: Existing image tokenization methods for MLLMs lack object-level detail, limiting their ability to understand or generate detailed visual content.

Method: Develops an object-centric visual tokenizer using Slot Attention, Q-Former encoder, diffusion decoder, and residual vector quantization for detailed encoding.

Result: Slot-MLLM shows significant performance improvements in vision-language tasks requiring local detail comprehension and generation.

Conclusion: Demonstrates feasibility of object-centric Slot Attention in MLLMs for natural images, advancing multimodal capabilities.

Abstract: Recently, multimodal large language models (MLLMs) have emerged as a key
approach in achieving artificial general intelligence. In particular,
vision-language MLLMs have been developed to generate not only text but also
visual outputs from multimodal inputs. This advancement requires efficient
image tokens that LLMs can process effectively both in input and output.
However, existing image tokenization methods for MLLMs typically capture only
global abstract concepts or uniformly segmented image patches, restricting
MLLMs' capability to effectively understand or generate detailed visual
content, particularly at the object level. To address this limitation, we
propose an object-centric visual tokenizer based on Slot Attention specifically
for MLLMs. In particular, based on the Q-Former encoder, diffusion decoder, and
residual vector quantization, our proposed discretized slot tokens can encode
local visual details while maintaining high-level semantics, and also align
with textual data to be integrated seamlessly within a unified next-token
prediction framework of LLMs. The resulting Slot-MLLM demonstrates significant
performance improvements over baselines with previous visual tokenizers across
various vision-language tasks that entail local detailed comprehension and
generation. Notably, this work is the first demonstration of the feasibility of
object-centric slot attention performed with MLLMs and in-the-wild natural
images.

</details>


### [345] [SafeMVDrive: Multi-view Safety-Critical Driving Video Synthesis in the Real World Domain](https://arxiv.org/pdf/2505.17727)
*Jiawei Zhou, Linye Lyu, Zhuotao Tian, Cheng Zhuo, Yu Li*

Main category: cs.CV

TL;DR: SafeMVDrive is a framework for generating high-quality, safety-critical, multi-view driving videos to enhance autonomous driving systems.


<details>
  <summary>Details</summary>
Motivation: Existing methods lack real-world, multi-view video data needed for advanced end-to-end autonomous systems.

Method: Integrates a safety-critical trajectory generator with a multi-view video generator, enhancing scene understanding and using a diffusion-based approach.

Result: Significantly increases collision rates in E2E AD planners, validating effectiveness.

Conclusion: SafeMVDrive successfully bridges the gap for realistic, safety-critical testing in autonomous driving.

Abstract: Safety-critical scenarios are rare yet pivotal for evaluating and enhancing
the robustness of autonomous driving systems. While existing methods generate
safety-critical driving trajectories, simulations, or single-view videos, they
fall short of meeting the demands of advanced end-to-end autonomous systems
(E2E AD), which require real-world, multi-view video data. To bridge this gap,
we introduce SafeMVDrive, the first framework designed to generate
high-quality, safety-critical, multi-view driving videos grounded in real-world
domains. SafeMVDrive strategically integrates a safety-critical trajectory
generator with an advanced multi-view video generator. To tackle the challenges
inherent in this integration, we first enhance scene understanding ability of
the trajectory generator by incorporating visual context -- which is previously
unavailable to such generator -- and leveraging a GRPO-finetuned
vision-language model to achieve more realistic and context-aware trajectory
generation. Second, recognizing that existing multi-view video generators
struggle to render realistic collision events, we introduce a two-stage,
controllable trajectory generation mechanism that produces collision-evasion
trajectories, ensuring both video quality and safety-critical fidelity.
Finally, we employ a diffusion-based multi-view video generator to synthesize
high-quality safety-critical driving videos from the generated trajectories.
Experiments conducted on an E2E AD planner demonstrate a significant increase
in collision rate when tested with our generated data, validating the
effectiveness of SafeMVDrive in stress-testing planning modules. Our code,
examples, and datasets are publicly available at:
https://zhoujiawei3.github.io/SafeMVDrive/.

</details>


### [346] [RQR3D: Reparametrizing the regression targets for BEV-based 3D object detection](https://arxiv.org/pdf/2505.17732)
*Ozsel Kilinc, Cem Tarhan*

Main category: cs.CV

TL;DR: RQR3D proposes a Restricted Quadrilateral Representation for BEV-based 3D object detection, improving accuracy and reducing errors in autonomous driving perception.


<details>
  <summary>Details</summary>
Motivation: Existing BEV-based methods suffer from discontinuities in loss functions due to angle-based representations, limiting performance.

Method: RQR3D regresses the smallest horizontal bounding box and corner offsets, transforming the problem into keypoint regression. It also introduces an objectness head and a simplified radar fusion backbone.

Result: RQR3D achieves state-of-the-art performance on nuScenes, with +4% NDS and +2.4% mAP improvements, reducing translation and orientation errors.

Conclusion: RQR3D is robust, precise, and ready for real-world autonomous driving applications.

Abstract: Accurate, fast, and reliable 3D perception is essential for autonomous
driving. Recently, bird's-eye view (BEV)-based perception approaches have
emerged as superior alternatives to perspective-based solutions, offering
enhanced spatial understanding and more natural outputs for planning. Existing
BEV-based 3D object detection methods, typically adhering to angle-based
representation, directly estimate the size and orientation of rotated bounding
boxes. We observe that BEV-based 3D object detection is analogous to aerial
oriented object detection, where angle-based methods are recognized for being
affected by discontinuities in their loss functions. Drawing inspiration from
this domain, we propose Restricted Quadrilateral Representation to define 3D
regression targets. RQR3D regresses the smallest horizontal bounding box
encapsulating the oriented box, along with the offsets between the corners of
these two boxes, thereby transforming the oriented object detection problem
into a keypoint regression task. RQR3D is compatible with any 3D object
detection approach. We employ RQR3D within an anchor-free single-stage object
detection method and introduce an objectness head to address class imbalance
problem. Furthermore, we introduce a simplified radar fusion backbone that
eliminates the need for voxel grouping and processes the BEV-mapped point cloud
with standard 2D convolutions, rather than sparse convolutions. Extensive
evaluations on the nuScenes dataset demonstrate that RQR3D achieves
state-of-the-art performance in camera-radar 3D object detection, outperforming
the previous best method by +4% in NDS and +2.4% in mAP, and significantly
reducing the translation and orientation errors, which are crucial for safe
autonomous driving. These consistent gains highlight the robustness, precision,
and real-world readiness of our approach.

</details>


### [347] [R-Genie: Reasoning-Guided Generative Image Editing](https://arxiv.org/pdf/2505.17768)
*Dong Zhang, Lingfeng He, Rui Yan, Fei Shen, Jinhui Tang*

Main category: cs.CV

TL;DR: R-Genie introduces reasoning-guided generative image editing, combining diffusion models with multimodal LLMs for complex, context-aware edits.


<details>
  <summary>Details</summary>
Motivation: Current image editing methods lack deep comprehension of implicit user intentions and contextual reasoning, limiting their capabilities.

Method: Proposes R-Genie, a model integrating diffusion models and multimodal LLMs with a reasoning-attention mechanism for complex edits.

Result: Validated on a dataset of 1,000+ image-instruction-edit triples, R-Genie enhances reasoning-based editing in diffusion models.

Conclusion: R-Genie unlocks new potentials for intelligent image synthesis by bridging linguistic understanding and visual synthesis.

Abstract: While recent advances in image editing have enabled impressive visual
synthesis capabilities, current methods remain constrained by explicit textual
instructions and limited editing operations, lacking deep comprehension of
implicit user intentions and contextual reasoning. In this work, we introduce a
new image editing paradigm: reasoning-guided generative editing, which
synthesizes images based on complex, multi-faceted textual queries accepting
world knowledge and intention inference. To facilitate this task, we first
construct a comprehensive dataset featuring over 1,000 image-instruction-edit
triples that incorporate rich reasoning contexts and real-world knowledge. We
then propose R-Genie: a reasoning-guided generative image editor, which
synergizes the generation power of diffusion models with advanced reasoning
capabilities of multimodal large language models. R-Genie incorporates a
reasoning-attention mechanism to bridge linguistic understanding with visual
synthesis, enabling it to handle intricate editing requests involving abstract
user intentions and contextual reasoning relations. Extensive experimental
results validate that R-Genie can equip diffusion models with advanced
reasoning-based editing capabilities, unlocking new potentials for intelligent
image synthesis.

</details>


### [348] [TopoPoint: Enhance Topology Reasoning via Endpoint Detection in Autonomous Driving](https://arxiv.org/pdf/2505.17771)
*Yanping Fu, Xinyuan Liu, Tianyu Li, Yike Ma, Yucheng Zhang, Feng Dai*

Main category: cs.CV

TL;DR: TopoPoint is a novel framework for robust topology reasoning in autonomous driving by explicitly detecting lane endpoints and jointly reasoning over them and lanes, outperforming existing methods.


<details>
  <summary>Details</summary>
Motivation: Existing methods for topology reasoning suffer from lane endpoint deviation, leading to incorrect topology construction.

Method: TopoPoint uses Point-Lane Merge Self-Attention and Point-Lane Graph Convolutional Network for feature aggregation, along with a Point-Lane Geometry Matching algorithm for inference.

Result: Achieves state-of-the-art performance (48.8 on OLS) and outperforms in endpoint detection (52.6 vs. 45.2 on DET$_p$).

Conclusion: TopoPoint effectively mitigates endpoint deviation and improves topology reasoning, validated by benchmarks and a new evaluation metric.

Abstract: Topology reasoning, which unifies perception and structured reasoning, plays
a vital role in understanding intersections for autonomous driving. However,
its performance heavily relies on the accuracy of lane detection, particularly
at connected lane endpoints. Existing methods often suffer from lane endpoints
deviation, leading to incorrect topology construction. To address this issue,
we propose TopoPoint, a novel framework that explicitly detects lane endpoints
and jointly reasons over endpoints and lanes for robust topology reasoning.
During training, we independently initialize point and lane query, and proposed
Point-Lane Merge Self-Attention to enhance global context sharing through
incorporating geometric distances between points and lanes as an attention mask
. We further design Point-Lane Graph Convolutional Network to enable mutual
feature aggregation between point and lane query. During inference, we
introduce Point-Lane Geometry Matching algorithm that computes distances
between detected points and lanes to refine lane endpoints, effectively
mitigating endpoint deviation. Extensive experiments on the OpenLane-V2
benchmark demonstrate that TopoPoint achieves state-of-the-art performance in
topology reasoning (48.8 on OLS). Additionally, we propose DET$_p$ to evaluate
endpoint detection, under which our method significantly outperforms existing
approaches (52.6 v.s. 45.2 on DET$_p$). The code is released at
https://github.com/Franpin/TopoPoint.

</details>


### [349] [TextFlux: An OCR-Free DiT Model for High-Fidelity Multilingual Scene Text Synthesis](https://arxiv.org/pdf/2505.17778)
*Yu Xie, Jielei Zhang, Pengyu Chen, Ziyue Wang, Weihang Wang, Longwen Gao, Peiyi Li, Huyang Sun, Qiang Zhang, Qian Qiao, Jiaqing Fan, Zhouhui Lian*

Main category: cs.CV

TL;DR: TextFlux is a DiT-based framework for multilingual scene text synthesis, eliminating the need for OCR encoders and requiring minimal training data.


<details>
  <summary>Details</summary>
Motivation: Existing methods rely on complex auxiliary modules and large annotated datasets, which TextFlux aims to simplify.

Method: TextFlux leverages diffusion models' contextual reasoning for glyph accuracy and scene integration, supporting multilingual synthesis with minimal data.

Result: TextFlux outperforms previous methods in multilingual settings, requiring only 1% of training data and offering precise multi-line control.

Conclusion: TextFlux provides a streamlined, scalable, and high-fidelity solution for scene text synthesis without additional visual conditioning modules.

Abstract: Diffusion-based scene text synthesis has progressed rapidly, yet existing
methods commonly rely on additional visual conditioning modules and require
large-scale annotated data to support multilingual generation. In this work, we
revisit the necessity of complex auxiliary modules and further explore an
approach that simultaneously ensures glyph accuracy and achieves high-fidelity
scene integration, by leveraging diffusion models' inherent capabilities for
contextual reasoning. To this end, we introduce TextFlux, a DiT-based framework
that enables multilingual scene text synthesis. The advantages of TextFlux can
be summarized as follows: (1) OCR-free model architecture. TextFlux eliminates
the need for OCR encoders (additional visual conditioning modules) that are
specifically used to extract visual text-related features. (2) Strong
multilingual scalability. TextFlux is effective in low-resource multilingual
settings, and achieves strong performance in newly added languages with fewer
than 1,000 samples. (3) Streamlined training setup. TextFlux is trained with
only 1% of the training data required by competing methods. (4) Controllable
multi-line text generation. TextFlux offers flexible multi-line synthesis with
precise line-level control, outperforming methods restricted to single-line or
rigid layouts. Extensive experiments and visualizations demonstrate that
TextFlux outperforms previous methods in both qualitative and quantitative
evaluations.

</details>


### [350] [U2-BENCH: Benchmarking Large Vision-Language Models on Ultrasound Understanding](https://arxiv.org/pdf/2505.17779)
*Anjie Le, Henan Liu, Yue Wang, Zhenyu Liu, Rongkun Zhu, Taohan Weng, Jinze Yu, Boyang Wang, Yalun Wu, Kaiwen Yan, Quanlin Sun, Meirui Jiang, Jialun Pei, Siya Liu, Haoyun Zheng, Zhoujun Li, Alison Noble, Jacques Souquet, Xiaoqing Guo, Manxi Lin, Hongcheng Guo*

Main category: cs.CV

TL;DR: U2-BENCH is the first benchmark evaluating large vision-language models (LVLMs) on ultrasound tasks, revealing strengths in classification but weaknesses in spatial reasoning and clinical language generation.


<details>
  <summary>Details</summary>
Motivation: Ultrasound interpretation is challenging due to variable image quality and noise. LVLMs' performance in this domain is unexplored, necessitating a standardized evaluation.

Method: U2-BENCH aggregates 7,241 cases across 15 anatomical regions and defines 8 tasks (e.g., diagnosis, report generation) to evaluate 20 LVLMs.

Result: LVLMs perform well on image-level classification but struggle with spatial reasoning and clinical language generation.

Conclusion: U2-BENCH provides a rigorous testbed to advance LVLM research in medical ultrasound imaging.

Abstract: Ultrasound is a widely-used imaging modality critical to global healthcare,
yet its interpretation remains challenging due to its varying image quality on
operators, noises, and anatomical structures. Although large vision-language
models (LVLMs) have demonstrated impressive multimodal capabilities across
natural and medical domains, their performance on ultrasound remains largely
unexplored. We introduce U2-BENCH, the first comprehensive benchmark to
evaluate LVLMs on ultrasound understanding across classification, detection,
regression, and text generation tasks. U2-BENCH aggregates 7,241 cases spanning
15 anatomical regions and defines 8 clinically inspired tasks, such as
diagnosis, view recognition, lesion localization, clinical value estimation,
and report generation, across 50 ultrasound application scenarios. We evaluate
20 state-of-the-art LVLMs, both open- and closed-source, general-purpose and
medical-specific. Our results reveal strong performance on image-level
classification, but persistent challenges in spatial reasoning and clinical
language generation. U2-BENCH establishes a rigorous and unified testbed to
assess and accelerate LVLM research in the uniquely multimodal domain of
medical ultrasound imaging.

</details>


### [351] [Hephaestus Minicubes: A Global, Multi-Modal Dataset for Volcanic Unrest Monitoring](https://arxiv.org/pdf/2505.17782)
*Nikolas Papadopoulos, Nikolaos Ioannis Bountos, Maria Sdraka, Andreas Karavias, Ioannis Papoutsis*

Main category: cs.CV

TL;DR: The paper introduces Hephaestus Minicubes, a dataset for volcanic deformation monitoring using InSAR and deep learning, providing multi-source, multi-temporal data and expert annotations for 44 active volcanoes.


<details>
  <summary>Details</summary>
Motivation: To address the lack of curated machine learning datasets for volcanic deformation monitoring, hindering the application of deep learning in this domain.

Method: Developed Hephaestus Minicubes, a global collection of 38 spatiotemporal datacubes integrating InSAR, topographic data, and atmospheric variables, with expert annotations.

Result: The dataset supports multi-modal, multi-temporal classification and semantic segmentation tasks, with benchmarks using state-of-the-art architectures.

Conclusion: This work advances machine learning in volcanic monitoring, promoting data-driven methods in Earth science.

Abstract: Ground deformation is regarded in volcanology as a key precursor signal
preceding volcanic eruptions. Satellite-based Interferometric Synthetic
Aperture Radar (InSAR) enables consistent, global-scale deformation tracking;
however, deep learning methods remain largely unexplored in this domain, mainly
due to the lack of a curated machine learning dataset. In this work, we build
on the existing Hephaestus dataset, and introduce Hephaestus Minicubes, a
global collection of 38 spatiotemporal datacubes offering high resolution,
multi-source and multi-temporal information, covering 44 of the world's most
active volcanoes over a 7-year period. Each spatiotemporal datacube integrates
InSAR products, topographic data, as well as atmospheric variables which are
known to introduce signal delays that can mimic ground deformation in InSAR
imagery. Furthermore, we provide expert annotations detailing the type,
intensity and spatial extent of deformation events, along with rich text
descriptions of the observed scenes. Finally, we present a comprehensive
benchmark, demonstrating Hephaestus Minicubes' ability to support volcanic
unrest monitoring as a multi-modal, multi-temporal classification and semantic
segmentation task, establishing strong baselines with state-of-the-art
architectures. This work aims to advance machine learning research in volcanic
monitoring, contributing to the growing integration of data-driven methods
within Earth science applications.

</details>


### [352] [Generative Data Augmentation for Object Point Cloud Segmentation](https://arxiv.org/pdf/2505.17783)
*Dekai Zhu, Stefan Gavranovic, Flavien Boussuge, Benjamin Busam, Slobodan Ilic*

Main category: cs.CV

TL;DR: The paper introduces a part-aware generative model and a 3-step generative data augmentation (GDA) pipeline for point cloud segmentation, outperforming traditional methods.


<details>
  <summary>Details</summary>
Motivation: Traditional data augmentation (TDA) lacks diversity, and generative models for 3D shapes often miss semantic labels, limiting their use in segmentation tasks.

Method: Extends the Lion diffusion model to generate labeled point clouds and proposes a GDA pipeline with pseudo-label filtering.

Result: GDA outperforms TDA and other methods on synthetic and real-world datasets.

Conclusion: The GDA approach effectively enhances training data diversity and improves segmentation performance.

Abstract: Data augmentation is widely used to train deep learning models to address
data scarcity. However, traditional data augmentation (TDA) typically relies on
simple geometric transformation, such as random rotation and rescaling,
resulting in minimal data diversity enrichment and limited model performance
improvement. State-of-the-art generative models for 3D shape generation rely on
the denoising diffusion probabilistic models and manage to generate realistic
novel point clouds for 3D content creation and manipulation. Nevertheless, the
generated 3D shapes lack associated point-wise semantic labels, restricting
their usage in enlarging the training data for point cloud segmentation tasks.
To bridge the gap between data augmentation techniques and the advanced
diffusion models, we extend the state-of-the-art 3D diffusion model, Lion, to a
part-aware generative model that can generate high-quality point clouds
conditioned on given segmentation masks. Leveraging the novel generative model,
we introduce a 3-step generative data augmentation (GDA) pipeline for point
cloud segmentation training. Our GDA approach requires only a small amount of
labeled samples but enriches the training data with generated variants and
pseudo-labeled samples, which are validated by a novel diffusion-based
pseudo-label filtering method. Extensive experiments on two large-scale
synthetic datasets and a real-world medical dataset demonstrate that our GDA
method outperforms TDA approach and related semi-supervised and self-supervised
methods.

</details>


### [353] [DetailFusion: A Dual-branch Framework with Detail Enhancement for Composed Image Retrieval](https://arxiv.org/pdf/2505.17796)
*Yuxin Yang, Yinan Zhou, Yuxin Chen, Ziqi Zhang, Zongyang Ma, Chunfeng Yuan, Bing Li, Lin Song, Jun Gao, Peng Li, Weiming Hu*

Main category: cs.CV

TL;DR: DetailFusion improves Composed Image Retrieval by balancing global and fine-grained details, achieving state-of-the-art results.


<details>
  <summary>Details</summary>
Motivation: Existing CIR methods lack attention to fine-grained details, limiting their ability to handle subtle visual or textual changes.

Method: Proposes DetailFusion, a dual-branch framework with atomic detail variation priors and an Adaptive Feature Compositor for dynamic fusion.

Result: Achieves state-of-the-art performance on CIRR and FashionIQ datasets, validating detail enhancement's effectiveness.

Conclusion: DetailFusion enhances CIR by effectively coordinating global and detailed information, demonstrating cross-domain adaptability.

Abstract: Composed Image Retrieval (CIR) aims to retrieve target images from a gallery
based on a reference image and modification text as a combined query. Recent
approaches focus on balancing global information from two modalities and encode
the query into a unified feature for retrieval. However, due to insufficient
attention to fine-grained details, these coarse fusion methods often struggle
with handling subtle visual alterations or intricate textual instructions. In
this work, we propose DetailFusion, a novel dual-branch framework that
effectively coordinates information across global and detailed granularities,
thereby enabling detail-enhanced CIR. Our approach leverages atomic detail
variation priors derived from an image editing dataset, supplemented by a
detail-oriented optimization strategy to develop a Detail-oriented Inference
Branch. Furthermore, we design an Adaptive Feature Compositor that dynamically
fuses global and detailed features based on fine-grained information of each
unique multimodal query. Extensive experiments and ablation analyses not only
demonstrate that our method achieves state-of-the-art performance on both CIRR
and FashionIQ datasets but also validate the effectiveness and cross-domain
adaptability of detail enhancement for CIR.

</details>


### [354] [Temporal Consistency Constrained Transferable Adversarial Attacks with Background Mixup for Action Recognition](https://arxiv.org/pdf/2505.17807)
*Ping Li, Jianan Ni, Bo Pang*

Main category: cs.CV

TL;DR: The paper proposes BMTC, a method to improve adversarial transferability in action recognition by reducing surrogate-target model dependency and ensuring attack direction stability.


<details>
  <summary>Details</summary>
Motivation: Existing transferable attack methods are limited by assumptions of similar decision boundaries and suffer from gradient oscillation, weakening adversarial attacks.

Method: BMTC uses a background adversarial mixup module and reinforcement learning to select optimal backgrounds, along with temporal gradient consistency loss for stable attack direction.

Result: Tests on UCF101, Kinetics-400, and ImageNet show BMTC significantly boosts adversarial example transferability.

Conclusion: BMTC effectively addresses challenges in transferable attacks, enhancing adversarial transferability for action and image recognition models.

Abstract: Action recognition models using deep learning are vulnerable to adversarial
examples, which are transferable across other models trained on the same data
modality. Existing transferable attack methods face two major challenges: 1)
they heavily rely on the assumption that the decision boundaries of the
surrogate (a.k.a., source) model and the target model are similar, which limits
the adversarial transferability; and 2) their decision boundary difference
makes the attack direction uncertain, which may result in the gradient
oscillation, weakening the adversarial attack. This motivates us to propose a
Background Mixup-induced Temporal Consistency (BMTC) attack method for action
recognition. From the input transformation perspective, we design a
model-agnostic background adversarial mixup module to reduce the
surrogate-target model dependency. In particular, we randomly sample one video
from each category and make its background frame, while selecting the
background frame with the top attack ability for mixup with the clean frame by
reinforcement learning. Moreover, to ensure an explicit attack direction, we
leverage the background category as guidance for updating the gradient of
adversarial example, and design a temporal gradient consistency loss, which
strengthens the stability of the attack direction on subsequent frames.
Empirical studies on two video datasets, i.e., UCF101 and Kinetics-400, and one
image dataset, i.e., ImageNet, demonstrate that our method significantly boosts
the transferability of adversarial examples across several action/image
recognition models. Our code is available at
https://github.com/mlvccn/BMTC_TransferAttackVid.

</details>


### [355] [An Attention Infused Deep Learning System with Grad-CAM Visualization for Early Screening of Glaucoma](https://arxiv.org/pdf/2505.17808)
*Ramanathan Swaminathan*

Main category: cs.CV

TL;DR: A hybrid deep learning model combining CNN and Vision Transformer with a Cross Attention module is proposed for glaucoma detection using ACRIMA and Drishti datasets.


<details>
  <summary>Details</summary>
Motivation: To leverage the strengths of both CNN and Vision Transformer for improved glaucoma detection.

Method: Combines CNN and Vision Transformer with a Cross Attention module, tested on ACRIMA and Drishti datasets.

Result: Not explicitly stated in the abstract, but the hybrid approach is implied to enhance performance.

Conclusion: The hybrid model shows promise for glaucoma detection by integrating CNN and Vision Transformer.

Abstract: This research work reveals the eye opening wisdom of the hybrid labyrinthine
deep learning models synergy born out of combining a trailblazing convolutional
neural network with a disruptive Vision Transformer, both intertwined together
with a radical Cross Attention module. Here, two high yielding datasets for
artificial intelligence models in detecting glaucoma, namely ACRIMA and
Drishti, are utilized.

</details>


### [356] [Seeing It or Not? Interpretable Vision-aware Latent Steering to Mitigate Object Hallucinations](https://arxiv.org/pdf/2505.17812)
*Boxu Chen, Ziwei Zheng, Le Yang, Zeyu Geng, Zhengyu Zhao, Chenhao Lin, Chao Shen*

Main category: cs.CV

TL;DR: VaLSe addresses object hallucination in LVLMs by interpreting and mitigating visual decision-making mechanisms, improving model robustness.


<details>
  <summary>Details</summary>
Motivation: Understanding and reducing object hallucination (OH) in LVLMs, where outputs don't match visual inputs.

Method: VaLSe uses vision-aware latent steering to trace visual influences and realign internal representations.

Result: VaLSe enhances model robustness against OH and reveals limitations in current OH evaluation metrics.

Conclusion: VaLSe is effective for interpretability and OH mitigation, highlighting the need for better OH benchmarks.

Abstract: Large Vision-Language Models (LVLMs) have achieved remarkable success but
continue to struggle with object hallucination (OH), generating outputs
inconsistent with visual inputs. While previous work has proposed methods to
reduce OH, the visual decision-making mechanisms that lead to hallucinations
remain poorly understood. In this paper, we propose VaLSe, a Vision-aware
Latent Steering framework that adopts an interpretation-then-mitigation
strategy to address OH in LVLMs. By tackling dual challenges of modeling
complex vision-language interactions and eliminating spurious activation
artifacts, VaLSe can generate visual contribution maps that trace how specific
visual inputs influence individual output tokens. These maps reveal the model's
vision-aware focus regions, which are then used to perform latent space
steering, realigning internal representations toward semantically relevant
content and reducing hallucinated outputs. Extensive experiments demonstrate
that VaLSe is a powerful interpretability tool and an effective method for
enhancing model robustness against OH across multiple benchmarks. Furthermore,
our analysis uncovers limitations in existing OH evaluation metrics,
underscoring the need for more nuanced, interpretable, and visually grounded OH
benchmarks in future work. Code is available at:
https://github.com/Ziwei-Zheng/VaLSe.

</details>


### [357] [ICPL-ReID: Identity-Conditional Prompt Learning for Multi-Spectral Object Re-Identification](https://arxiv.org/pdf/2505.17821)
*Shihao Li, Chenglong Li, Aihua Zheng, Jin Tang, Bin Luo*

Main category: cs.CV

TL;DR: The paper proposes ICPL, a framework using identity-conditional text prompts to unify multi-spectral ReID, leveraging CLIP's cross-modal alignment and addressing modal differences.


<details>
  <summary>Details</summary>
Motivation: Multi-spectral ReID faces challenges due to complex modal differences and lacks fine-grained semantic understanding of spectral data.

Method: ICPL uses online prompt learning, identity-condition modules, and multi-spectral adapters to align spectral features with text semantics.

Result: ICPL outperforms state-of-the-art methods on 5 benchmarks.

Conclusion: The framework effectively unifies spectral features and improves ReID performance.

Abstract: Multi-spectral object re-identification (ReID) brings a new perception
perspective for smart city and intelligent transportation applications,
effectively addressing challenges from complex illumination and adverse
weather. However, complex modal differences between heterogeneous spectra pose
challenges to efficiently utilizing complementary and discrepancy of spectra
information. Most existing methods fuse spectral data through intricate modal
interaction modules, lacking fine-grained semantic understanding of spectral
information (\textit{e.g.}, text descriptions, part masks, and object
keypoints). To solve this challenge, we propose a novel Identity-Conditional
text Prompt Learning framework (ICPL), which exploits the powerful cross-modal
alignment capability of CLIP, to unify different spectral visual features from
text semantics. Specifically, we first propose the online prompt learning using
learnable text prompt as the identity-level semantic center to bridge the
identity semantics of different spectra in online manner. Then, in lack of
concrete text descriptions, we propose the multi-spectral identity-condition
module to use identity prototype as spectral identity condition to constraint
prompt learning. Meanwhile, we construct the alignment loop mutually optimizing
the learnable text prompt and spectral visual encoder to avoid online prompt
learning disrupting the pre-trained text-image alignment distribution. In
addition, to adapt to small-scale multi-spectral data and mitigate style
differences between spectra, we propose multi-spectral adapter that employs a
low-rank adaption method to learn spectra-specific features. Comprehensive
experiments on 5 benchmarks, including RGBNT201, Market-MM, MSVR310, RGBN300,
and RGBNT100, demonstrate that the proposed method outperforms the
state-of-the-art methods.

</details>


### [358] [VLM Models and Automated Grading of Atopic Dermatitis](https://arxiv.org/pdf/2505.17835)
*Marc Lalonde, Hamed Ghodrati*

Main category: cs.CV

TL;DR: Evaluating seven vision-language models (VLMs) for grading atopic dermatitis (AD) severity from images.


<details>
  <summary>Details</summary>
Motivation: Automating AD grading is challenging, and VLMs offer potential for explainable medical image assessment.

Method: Tested seven VLMs on AD severity assessment using a set of images.

Result: Not explicitly stated in the abstract.

Conclusion: VLMs show promise for explainable AD severity grading.

Abstract: The task of grading atopic dermatitis (or AD, a form of eczema) from patient
images is difficult even for trained dermatologists. Research on automating
this task has progressed in recent years with the development of deep learning
solutions; however, the rapid evolution of multimodal models and more
specifically vision-language models (VLMs) opens the door to new possibilities
in terms of explainable assessment of medical images, including dermatology.
This report describes experiments carried out to evaluate the ability of seven
VLMs to assess the severity of AD on a set of test images.

</details>


### [359] [Locality-Sensitive Hashing for Efficient Hard Negative Sampling in Contrastive Learning](https://arxiv.org/pdf/2505.17844)
*Fabian Deuser, Philipp Hausenblas, Hannah Schieber, Daniel Roth, Martin Werner, Norbert Oswald*

Main category: cs.CV

TL;DR: A GPU-friendly LSH scheme is proposed for efficient hard negative mining in contrastive learning, achieving comparable or better performance with less computation.


<details>
  <summary>Details</summary>
Motivation: Hard negative examples improve contrastive learning, but finding high-quality examples in large datasets is computationally challenging.

Method: Proposes a GPU-friendly LSH scheme to quantize feature vectors into binary representations for approximate nearest neighbor search.

Result: The approach achieves comparable or better performance while requiring significantly less computation than existing methods.

Conclusion: The proposed LSH scheme is efficient and effective for hard negative mining in contrastive learning.

Abstract: Contrastive learning is a representational learning paradigm in which a
neural network maps data elements to feature vectors. It improves the feature
space by forming lots with an anchor and examples that are either positive or
negative based on class similarity. Hard negative examples, which are close to
the anchor in the feature space but from a different class, improve learning
performance. Finding such examples of high quality efficiently in large,
high-dimensional datasets is computationally challenging. In this paper, we
propose a GPU-friendly Locality-Sensitive Hashing (LSH) scheme that quantizes
real-valued feature vectors into binary representations for approximate nearest
neighbor search. We investigate its theoretical properties and evaluate it on
several datasets from textual and visual domain. Our approach achieves
comparable or better performance while requiring significantly less computation
than existing hard negative mining strategies.

</details>


### [360] [Multi-task Learning For Joint Action and Gesture Recognition](https://arxiv.org/pdf/2505.17867)
*Konstantinos Spathis, Nikolaos Kardaris, Petros Maragos*

Main category: cs.CV

TL;DR: Multi-task learning for action and gesture recognition improves efficiency, robustness, and generalization by leveraging shared representations.


<details>
  <summary>Details</summary>
Motivation: Current methods handle action and gesture recognition separately despite their similarities, missing potential synergies.

Method: Jointly train a single deep neural network to learn shared representations for both tasks.

Result: Experiments show better performance for both tasks compared to single-task learning.

Conclusion: Multi-task learning is more effective for action and gesture recognition than handling them separately.

Abstract: In practical applications, computer vision tasks often need to be addressed
simultaneously. Multitask learning typically achieves this by jointly training
a single deep neural network to learn shared representations, providing
efficiency and improving generalization. Although action and gesture
recognition are closely related tasks, since they focus on body and hand
movements, current state-of-the-art methods handle them separately. In this
paper, we show that employing a multi-task learning paradigm for action and
gesture recognition results in more efficient, robust and generalizable visual
representations, by leveraging the synergies between these tasks. Extensive
experiments on multiple action and gesture datasets demonstrate that handling
actions and gestures in a single architecture can achieve better performance
for both tasks in comparison to their single-task learning variants.

</details>


### [361] [Hyperspectral Anomaly Detection Fused Unified Nonconvex Tensor Ring Factors Regularization](https://arxiv.org/pdf/2505.17881)
*Wenjin Qin, Hailin Wang, Hao Shu, Feng Zhang, Jianjun Wang, Xiangyong Cao, Xi-Le Zhao, Gemine Vivone*

Main category: cs.CV

TL;DR: A novel hyperspectral anomaly detection method, HAD-EUNTRFR, improves detection accuracy by leveraging tensor ring decomposition and nonconvex regularization to capture spatial-spectral correlations and sparsity.


<details>
  <summary>Details</summary>
Motivation: Existing methods fail to fully utilize global correlations and local smoothness in hyperspectral images, leading to suboptimal anomaly detection performance.

Method: HAD-EUNTRFR decomposes hyperspectral images into background and anomaly components, uses tensor ring decomposition for spatial-spectral correlations, and introduces nonconvex regularization for low-rankness and sparsity.

Result: The method outperforms state-of-the-art approaches in detection accuracy on benchmark datasets.

Conclusion: HAD-EUNTRFR effectively addresses limitations of existing methods by combining tensor ring decomposition and nonconvex regularization, achieving superior anomaly detection performance.

Abstract: In recent years, tensor decomposition-based approaches for hyperspectral
anomaly detection (HAD) have gained significant attention in the field of
remote sensing. However, existing methods often fail to fully leverage both the
global correlations and local smoothness of the background components in
hyperspectral images (HSIs), which exist in both the spectral and spatial
domains. This limitation results in suboptimal detection performance. To
mitigate this critical issue, we put forward a novel HAD method named
HAD-EUNTRFR, which incorporates an enhanced unified nonconvex tensor ring (TR)
factors regularization. In the HAD-EUNTRFR framework, the raw HSIs are first
decomposed into background and anomaly components. The TR decomposition is then
employed to capture the spatial-spectral correlations within the background
component. Additionally, we introduce a unified and efficient nonconvex
regularizer, induced by tensor singular value decomposition (TSVD), to
simultaneously encode the low-rankness and sparsity of the 3-D gradient TR
factors into a unique concise form. The above characterization scheme enables
the interpretable gradient TR factors to inherit the low-rankness and
smoothness of the original background. To further enhance anomaly detection, we
design a generalized nonconvex regularization term to exploit the group
sparsity of the anomaly component. To solve the resulting doubly nonconvex
model, we develop a highly efficient optimization algorithm based on the
alternating direction method of multipliers (ADMM) framework. Experimental
results on several benchmark datasets demonstrate that our proposed method
outperforms existing state-of-the-art (SOTA) approaches in terms of detection
accuracy.

</details>


### [362] [Track Anything Annotate: Video annotation and dataset generation of computer vision models](https://arxiv.org/pdf/2505.17884)
*Nikita Ivanov, Mark Klimov, Dmitry Glukhikh, Tatiana Chernysheva, Igor Glukhikh*

Main category: cs.CV

TL;DR: A prototype tool for annotating and generating training datasets using video tracking and segmentation is proposed, significantly speeding up dataset creation compared to manual methods.


<details>
  <summary>Details</summary>
Motivation: The need for large labeled datasets in machine learning is resource-intensive, prompting the development of a more efficient annotation tool.

Method: The paper explores various approaches, from technology selection to implementation, for creating a video-based annotation and dataset generation tool.

Result: The prototype accelerates dataset generation, outperforming manual annotation.

Conclusion: The tool offers a practical solution for reducing the time and effort required in dataset preparation, with resources available on GitHub.

Abstract: Modern machine learning methods require significant amounts of labelled data,
making the preparation process time-consuming and resource-intensive. In this
paper, we propose to consider the process of prototyping a tool for annotating
and generating training datasets based on video tracking and segmentation. We
examine different approaches to solving this problem, from technology selection
through to final implementation. The developed prototype significantly
accelerates dataset generation compared to manual annotation. All resources are
available at https://github.com/lnikioffic/track-anything-annotate

</details>


### [363] [Pixels to Prognosis: Harmonized Multi-Region CT-Radiomics and Foundation-Model Signatures Across Multicentre NSCLC Data](https://arxiv.org/pdf/2505.17893)
*Shruti Atul Mali, Zohaib Salahuddin, Danial Khan, Yumeng Zhang, Henry C. Woodruff, Eduardo Ibor-Crespo, Ana Jimenez-Pastor, Luis Marti-Bonmati, Philippe Lambin*

Main category: cs.CV

TL;DR: Harmonization and multi-region CT feature integration enhance survival prediction in NSCLC using radiomics, FM features, and clinical data.


<details>
  <summary>Details</summary>
Motivation: To improve survival prediction in NSCLC patients by integrating harmonized multi-region CT features, handcrafted radiomics, and FM features.

Method: Analyzed CT scans and clinical data from 876 NSCLC patients, harmonized features using ComBat and RKN, and used regularized Cox models for survival prediction.

Result: Combined FM features with clinical data achieved the highest performance (C-index = 0.7616). Consensus model showed high sensitivity (97.6%) and specificity (66.7%).

Conclusion: Harmonization and multi-region feature integration improve survival prediction, enabling robust risk stratification across imaging centers.

Abstract: Purpose: To evaluate the impact of harmonization and multi-region CT image
feature integration on survival prediction in non-small cell lung cancer
(NSCLC) patients, using handcrafted radiomics, pretrained foundation model (FM)
features, and clinical data from a multicenter dataset.
  Methods: We analyzed CT scans and clinical data from 876 NSCLC patients (604
training, 272 test) across five centers. Features were extracted from the whole
lung, tumor, mediastinal nodes, coronary arteries, and coronary artery calcium
(CAC). Handcrafted radiomics and FM deep features were harmonized using ComBat,
reconstruction kernel normalization (RKN), and RKN+ComBat. Regularized Cox
models predicted overall survival; performance was assessed using the
concordance index (C-index), 5-year time-dependent area under the curve
(t-AUC), and hazard ratio (HR). SHapley Additive exPlanations (SHAP) values
explained feature contributions. A consensus model used agreement across top
region of interest (ROI) models to stratify patient risk.
  Results: TNM staging showed prognostic utility (C-index = 0.67; HR = 2.70;
t-AUC = 0.85). The clinical + tumor radiomics model with ComBat achieved a
C-index of 0.7552 and t-AUC of 0.8820. FM features (50-voxel cubes) combined
with clinical data yielded the highest performance (C-index = 0.7616; t-AUC =
0.8866). An ensemble of all ROIs and FM features reached a C-index of 0.7142
and t-AUC of 0.7885. The consensus model, covering 78% of valid test cases,
achieved a t-AUC of 0.92, sensitivity of 97.6%, and specificity of 66.7%.
  Conclusion: Harmonization and multi-region feature integration improve
survival prediction in multicenter NSCLC data. Combining interpretable
radiomics, FM features, and consensus modeling enables robust risk
stratification across imaging centers.

</details>


### [364] [Semantic segmentation with reward](https://arxiv.org/pdf/2505.17905)
*Xie Ting, Ye Huang, Zhilin Liu, Lixin Duan*

Main category: cs.CV

TL;DR: RSS (Reward in Semantic Segmentation) applies reward-based reinforcement learning to semantic segmentation using pixel-level and image-level rewards, outperforming weakly supervised methods.


<details>
  <summary>Details</summary>
Motivation: Pixel-level labeling is often unavailable, necessitating alternative feedback methods like rewards for training semantic segmentation networks.

Method: RSS uses progressive scale rewards (PSR) and pair-wise spatial difference (PSD) to ensure convergence under image-level rewards.

Result: RSS successfully converges on both reward levels and outperforms weakly supervised methods using image-level signals.

Conclusion: RSS is a practical solution for training semantic segmentation networks with limited labeling, leveraging rewards effectively.

Abstract: In real-world scenarios, pixel-level labeling is not always available.
Sometimes, we need a semantic segmentation network, and even a visual encoder
can have a high compatibility, and can be trained using various types of
feedback beyond traditional labels, such as feedback that indicates the quality
of the parsing results. To tackle this issue, we proposed RSS (Reward in
Semantic Segmentation), the first practical application of reward-based
reinforcement learning on pure semantic segmentation offered in two granular
levels (pixel-level and image-level). RSS incorporates various novel
technologies, such as progressive scale rewards (PSR) and pair-wise spatial
difference (PSD), to ensure that the reward facilitates the convergence of the
semantic segmentation network, especially under image-level rewards.
Experiments and visualizations on benchmark datasets demonstrate that the
proposed RSS can successfully ensure the convergence of the semantic
segmentation network on two levels of rewards. Additionally, the RSS, which
utilizes an image-level reward, outperforms existing weakly supervised methods
that also rely solely on image-level signals during training.

</details>


### [365] [DiffusionReward: Enhancing Blind Face Restoration through Reward Feedback Learning](https://arxiv.org/pdf/2505.17910)
*Bin Wu, Wei Wang, Yahui Liu, Zixiang Li, Yao Zhao*

Main category: cs.CV

TL;DR: DiffusionReward introduces a Reward Feedback Learning (ReFL) framework for Blind Face Restoration, improving realism and identity consistency with a Face Reward Model (FRM) and dynamic optimization.


<details>
  <summary>Details</summary>
Motivation: Address limitations of diffusion-based methods in generating realistic facial details and maintaining identity consistency in face restoration.

Method: Uses a Face Reward Model (FRM) trained on annotated data to provide feedback signals, incorporating gradient flow into denoising with three guiding aspects: FRM for quality, regularization for diversity, and structural consistency for fidelity.

Result: Outperforms state-of-the-art methods on synthetic and wild datasets, enhancing identity consistency and facial details.

Conclusion: DiffusionReward effectively aligns restoration with human preferences, preventing reward hacking and improving realism.

Abstract: Reward Feedback Learning (ReFL) has recently shown great potential in
aligning model outputs with human preferences across various generative tasks.
In this work, we introduce a ReFL framework, named DiffusionReward, to the
Blind Face Restoration task for the first time. DiffusionReward effectively
overcomes the limitations of diffusion-based methods, which often fail to
generate realistic facial details and exhibit poor identity consistency. The
core of our framework is the Face Reward Model (FRM), which is trained using
carefully annotated data. It provides feedback signals that play a pivotal role
in steering the optimization process of the restoration network. In particular,
our ReFL framework incorporates a gradient flow into the denoising process of
off-the-shelf face restoration methods to guide the update of model parameters.
The guiding gradient is collaboratively determined by three aspects: (i) the
FRM to ensure the perceptual quality of the restored faces; (ii) a
regularization term that functions as a safeguard to preserve generative
diversity; and (iii) a structural consistency constraint to maintain facial
fidelity. Furthermore, the FRM undergoes dynamic optimization throughout the
process. It not only ensures that the restoration network stays precisely
aligned with the real face manifold, but also effectively prevents reward
hacking. Experiments on synthetic and wild datasets demonstrate that our method
outperforms state-of-the-art methods, significantly improving identity
consistency and facial details. The source codes, data, and models are
available at: https://github.com/01NeuralNinja/DiffusionReward.

</details>


### [366] [Object-level Cross-view Geo-localization with Location Enhancement and Multi-Head Cross Attention](https://arxiv.org/pdf/2505.17911)
*Zheyang Huang, Jagannath Aryal, Saeid Nahavandi, Xuequan Lu, Chee Peng Lim, Lei Wei, Hailing Zhou*

Main category: cs.CV

TL;DR: OCGNet improves object-level cross-view geo-localization by integrating user-specified click locations and adaptive feature enhancement, achieving state-of-the-art performance.


<details>
  <summary>Details</summary>
Motivation: Traditional image-level localization lacks precision for applications like search-and-rescue or infrastructure inspection, requiring object-level accuracy.

Method: OCGNet uses Gaussian Kernel Transfer (GKT) to embed click locations, along with Location Enhancement (LE) and Multi-Head Cross Attention (MHCA) modules for robust feature handling.

Result: OCGNet outperforms on the CVOGL dataset and shows few-shot learning capabilities.

Conclusion: OCGNet is effective for precise object-level geo-localization and adaptable to diverse applications.

Abstract: Cross-view geo-localization determines the location of a query image,
captured by a drone or ground-based camera, by matching it to a geo-referenced
satellite image. While traditional approaches focus on image-level
localization, many applications, such as search-and-rescue, infrastructure
inspection, and precision delivery, demand object-level accuracy. This enables
users to prompt a specific object with a single click on a drone image to
retrieve precise geo-tagged information of the object. However, variations in
viewpoints, timing, and imaging conditions pose significant challenges,
especially when identifying visually similar objects in extensive satellite
imagery. To address these challenges, we propose an Object-level Cross-view
Geo-localization Network (OCGNet). It integrates user-specified click locations
using Gaussian Kernel Transfer (GKT) to preserve location information
throughout the network. This cue is dually embedded into the feature encoder
and feature matching blocks, ensuring robust object-specific localization.
Additionally, OCGNet incorporates a Location Enhancement (LE) module and a
Multi-Head Cross Attention (MHCA) module to adaptively emphasize
object-specific features or expand focus to relevant contextual regions when
necessary. OCGNet achieves state-of-the-art performance on a public dataset,
CVOGL. It also demonstrates few-shot learning capabilities, effectively
generalizing from limited examples, making it suitable for diverse applications
(https://github.com/ZheyangH/OCGNet).

</details>


### [367] [Evaluation of Few-Shot Learning Methods for Kidney Stone Type Recognition in Ureteroscopy](https://arxiv.org/pdf/2505.17921)
*Carlos Salazar-Ruiz, Francisco Lopez-Tiro, Ivan Reyes-Amezcua, Clement Larose, Gilberto Ochoa-Ruiz, Christian Daul*

Main category: cs.CV

TL;DR: A few-shot learning method using Prototypical Networks is proposed for kidney stone classification in endoscopic images, achieving comparable or better performance than traditional models with limited training data.


<details>
  <summary>Details</summary>
Motivation: Current kidney stone identification methods are time-consuming or require specialists, and deep learning models lack sufficient training data.

Method: The paper introduces a deep learning approach based on few-shot learning, specifically Prototypical Networks, to classify kidney stone types with limited samples.

Result: Prototypical Networks achieve performance equal to or better than traditional models using only 25% of the training data.

Conclusion: Few-shot learning is effective for kidney stone classification in scenarios with scarce data or uncommon classes.

Abstract: Determining the type of kidney stones is crucial for prescribing appropriate
treatments to prevent recurrence. Currently, various approaches exist to
identify the type of kidney stones. However, obtaining results through the
reference ex vivo identification procedure can take several weeks, while in
vivo visual recognition requires highly trained specialists. For this reason,
deep learning models have been developed to provide urologists with an
automated classification of kidney stones during ureteroscopies. Nevertheless,
a common issue with these models is the lack of training data. This
contribution presents a deep learning method based on few-shot learning, aimed
at producing sufficiently discriminative features for identifying kidney stone
types in endoscopic images, even with a very limited number of samples. This
approach was specifically designed for scenarios where endoscopic images are
scarce or where uncommon classes are present, enabling classification even with
a limited training dataset. The results demonstrate that Prototypical Networks,
using up to 25% of the training data, can achieve performance equal to or
better than traditional deep learning models trained with the complete dataset.

</details>


### [368] [AutoMiSeg: Automatic Medical Image Segmentation via Test-Time Adaptation of Foundation Models](https://arxiv.org/pdf/2505.17931)
*Xingjian Li, Qifeng Wu, Colleen Que, Yiran Ding, Adithya S. Ubaradka, Jianhua Xing, Tianyang Wang, Min Xu*

Main category: cs.CV

TL;DR: A zero-shot, automatic medical image segmentation pipeline using vision-language and segmentation foundation models, with test-time adaptation for domain alignment, achieves competitive results without extensive annotations.


<details>
  <summary>Details</summary>
Motivation: Current deep learning methods for medical image segmentation require significant expert effort for annotations or prompts. This work aims to reduce such dependency by leveraging foundation models.

Method: Combines vision-language and segmentation models to generate bounding boxes and enhance prompts, followed by a promptable segmentation model. Introduces test-time adaptation with learnable adaptors and Bayesian Optimization for hyperparameter tuning.

Result: Evaluated on seven datasets, the pipeline performs competitively with weakly-prompted interactive models, demonstrating annotation efficiency and scalability.

Conclusion: The proposed pipeline offers a scalable, annotation-efficient solution for zero-shot medical image segmentation, addressing domain gaps and verification challenges.

Abstract: Medical image segmentation is vital for clinical diagnosis, yet current deep
learning methods often demand extensive expert effort, i.e., either through
annotating large training datasets or providing prompts at inference time for
each new case. This paper introduces a zero-shot and automatic segmentation
pipeline that combines off-the-shelf vision-language and segmentation
foundation models. Given a medical image and a task definition (e.g., "segment
the optic disc in an eye fundus image"), our method uses a grounding model to
generate an initial bounding box, followed by a visual prompt boosting module
that enhance the prompts, which are then processed by a promptable segmentation
model to produce the final mask. To address the challenges of domain gap and
result verification, we introduce a test-time adaptation framework featuring a
set of learnable adaptors that align the medical inputs with foundation model
representations. Its hyperparameters are optimized via Bayesian Optimization,
guided by a proxy validation model without requiring ground-truth labels. Our
pipeline offers an annotation-efficient and scalable solution for zero-shot
medical image segmentation across diverse tasks. Our pipeline is evaluated on
seven diverse medical imaging datasets and shows promising results. By proper
decomposition and test-time adaptation, our fully automatic pipeline performs
competitively with weakly-prompted interactive foundation models.

</details>


### [369] [SplatCo: Structure-View Collaborative Gaussian Splatting for Detail-Preserving Rendering of Large-Scale Unbounded Scenes](https://arxiv.org/pdf/2505.17951)
*Haihong Xiao, Jianan Zou, Yuxin Zhou, Ying He, Wenxiong Kang*

Main category: cs.CV

TL;DR: SplatCo is a framework for high-fidelity rendering of outdoor scenes using Gaussian splatting, combining global and local features for detail preservation and multi-view consistency.


<details>
  <summary>Details</summary>
Motivation: To improve reconstruction quality in large-scale outdoor scenes by addressing global consistency and local detail preservation.

Method: Uses cross-structure collaboration (tri-plane + grid features) and cross-view assisted training (gradient synchronization, visibility-aware densification).

Result: Outperforms state-of-the-art methods with PSNR gains of 1-2 dB and SSIM improvements of 0.1-0.2.

Conclusion: SplatCo sets a new benchmark for high-fidelity rendering in large-scale scenes.

Abstract: We present SplatCo, a structure-view collaborative Gaussian splatting
framework for high-fidelity rendering of complex outdoor environments. SplatCo
builds upon two novel components: (1) a cross-structure collaboration module
that combines global tri-plane representations, which capture coarse scene
layouts, with local context grid features that represent fine surface details.
This fusion is achieved through a novel hierarchical compensation strategy,
ensuring both global consistency and local detail preservation; and (2) a
cross-view assisted training strategy that enhances multi-view consistency by
synchronizing gradient updates across viewpoints, applying visibility-aware
densification, and pruning overfitted or inaccurate Gaussians based on
structural consistency. Through joint optimization of structural representation
and multi-view coherence, SplatCo effectively reconstructs fine-grained
geometric structures and complex textures in large-scale scenes. Comprehensive
evaluations on 13 diverse large-scale scenes, including Mill19, MatrixCity,
Tanks & Temples, WHU, and custom aerial captures, demonstrate that SplatCo
consistently achieves higher reconstruction quality than state-of-the-art
methods, with PSNR improvements of 1-2 dB and SSIM gains of 0.1 to 0.2. These
results establish a new benchmark for high-fidelity rendering of large-scale
unbounded scenes. Code and additional information are available at
https://github.com/SCUT-BIP-Lab/SplatCo.

</details>


### [370] [Diffusion Classifiers Understand Compositionality, but Conditions Apply](https://arxiv.org/pdf/2505.17955)
*Yujin Jeong, Arnas Uselis, Seong Joon Oh, Anna Rohrbach*

Main category: cs.CV

TL;DR: The paper explores the discriminative capabilities of diffusion classifiers in compositional tasks, analyzing three models across 10 datasets and 30 tasks, while introducing a new benchmark to study domain effects.


<details>
  <summary>Details</summary>
Motivation: To address the lack of comprehensive benchmarks and analysis for diffusion classifiers in discriminative compositional scenarios.

Method: A comprehensive study of diffusion classifiers (SD 1.5, 2.0, and 3-m) on 10 datasets and 30 tasks, including a new diagnostic benchmark (Self-Bench) and analysis of timestep weighting.

Result: Diffusion classifiers show compositional understanding, but performance depends on domain and timestep sensitivity, especially for SD3-m.

Conclusion: Diffusion classifiers can understand compositionality, but their effectiveness is context-dependent. Code and dataset are publicly available.

Abstract: Understanding visual scenes is fundamental to human intelligence. While
discriminative models have significantly advanced computer vision, they often
struggle with compositional understanding. In contrast, recent generative
text-to-image diffusion models excel at synthesizing complex scenes, suggesting
inherent compositional capabilities. Building on this, zero-shot diffusion
classifiers have been proposed to repurpose diffusion models for discriminative
tasks. While prior work offered promising results in discriminative
compositional scenarios, these results remain preliminary due to a small number
of benchmarks and a relatively shallow analysis of conditions under which the
models succeed. To address this, we present a comprehensive study of the
discriminative capabilities of diffusion classifiers on a wide range of
compositional tasks. Specifically, our study covers three diffusion models (SD
1.5, 2.0, and, for the first time, 3-m) spanning 10 datasets and over 30 tasks.
Further, we shed light on the role that target dataset domains play in
respective performance; to isolate the domain effects, we introduce a new
diagnostic benchmark Self-Bench comprised of images created by diffusion models
themselves. Finally, we explore the importance of timestep weighting and
uncover a relationship between domain gap and timestep sensitivity,
particularly for SD3-m. To sum up, diffusion classifiers understand
compositionality, but conditions apply! Code and dataset are available at
https://github.com/eugene6923/Diffusion-Classifiers-Compositionality.

</details>


### [371] [Mind the Domain Gap: Measuring the Domain Gap Between Real-World and Synthetic Point Clouds for Automated Driving Development](https://arxiv.org/pdf/2505.17959)
*Nguyen Duc, Yan-Ling Lai, Patrick Madlindl, Xinyuan Zhu, Benedikt Schwab, Olaf Wysocki, Ludwig Hoegner, Thomas H. Kolbe*

Main category: cs.CV

TL;DR: The paper proposes a novel metric, DoGSS-PCL, to measure the domain gap between real and simulated point cloud data, enabling safer and more reliable synthetic data use in automated driving and digital twinning.


<details>
  <summary>Details</summary>
Motivation: The need to credibly measure the domain gap between real and simulated data, especially for safety-critical applications like automated driving, where out-of-domain samples can cause fatal accidents.

Method: Introduces a novel metric, DoGSS-PCL, to evaluate geometric and semantic quality of simulated point clouds, enabling comprehensive domain gap analysis.

Result: Experiments show the metric effectively measures the domain gap, and synthetic semantic point clouds can train deep neural networks without performance loss at a 50/50 real-to-synthetic ratio.

Conclusion: The work advances credible data simulation, supporting at-scale deployment in automated driving testing and digital twinning.

Abstract: Owing to the typical long-tail data distribution issues, simulating
domain-gap-free synthetic data is crucial in robotics, photogrammetry, and
computer vision research. The fundamental challenge pertains to credibly
measuring the difference between real and simulated data. Such a measure is
vital for safety-critical applications, such as automated driving, where
out-of-domain samples may impact a car's perception and cause fatal accidents.
Previous work has commonly focused on simulating data on one scene and
analyzing performance on a different, real-world scene, hampering the disjoint
analysis of domain gap coming from networks' deficiencies, class definitions,
and object representation. In this paper, we propose a novel approach to
measuring the domain gap between the real world sensor observations and
simulated data representing the same location, enabling comprehensive domain
gap analysis. To measure such a domain gap, we introduce a novel metric
DoGSS-PCL and evaluation assessing the geometric and semantic quality of the
simulated point cloud. Our experiments corroborate that the introduced approach
can be used to measure the domain gap. The tests also reveal that synthetic
semantic point clouds may be used for training deep neural networks,
maintaining the performance at the 50/50 real-to-synthetic ratio. We strongly
believe that this work will facilitate research on credible data simulation and
allow for at-scale deployment in automated driving testing and digital
twinning.

</details>


### [372] [MR-EEGWaveNet: Multiresolutional EEGWaveNet for Seizure Detection from Long EEG Recordings](https://arxiv.org/pdf/2505.17972)
*Kazi Mahmudul Hassan, Xuyang Zhao, Hidenori Sugano, Toshihisa Tanaka*

Main category: cs.CV

TL;DR: A novel end-to-end model, MR-EEGWaveNet, improves seizure detection by capturing temporal and spatial EEG features, outperforming conventional methods.


<details>
  <summary>Details</summary>
Motivation: Feature engineering for seizure detection is challenging, with existing models struggling to distinguish artifacts from seizures.

Method: MR-EEGWaveNet uses convolution, feature extraction, and predictor modules, along with anomaly score-based post-processing.

Result: The model improved F1 scores significantly (0.177 to 0.336 on Siena, 0.327 to 0.488 on Juntendo) and precision by 15.9% and 20.62%.

Conclusion: MR-EEGWaveNet is effective for seizure detection, reducing false positives and outperforming non-multiresolution approaches.

Abstract: Feature engineering for generalized seizure detection models remains a
significant challenge. Recently proposed models show variable performance
depending on the training data and remain ineffective at accurately
distinguishing artifacts from seizure data. In this study, we propose a novel
end-to-end model, ''Multiresolutional EEGWaveNet (MR-EEGWaveNet),'' which
efficiently distinguishes seizure events from background electroencephalogram
(EEG) and artifacts/noise by capturing both temporal dependencies across
different time frames and spatial relationships between channels. The model has
three modules: convolution, feature extraction, and predictor. The convolution
module extracts features through depth-wise and spatio-temporal convolution.
The feature extraction module individually reduces the feature dimension
extracted from EEG segments and their sub-segments. Subsequently, the extracted
features are concatenated into a single vector for classification using a fully
connected classifier called the predictor module. In addition, an anomaly
score-based post-classification processing technique was introduced to reduce
the false-positive rates of the model. Experimental results were reported and
analyzed using different parameter settings and datasets (Siena (public) and
Juntendo (private)). The proposed MR-EEGWaveNet significantly outperformed the
conventional non-multiresolution approach, improving the F1 scores from 0.177
to 0.336 on Siena and 0.327 to 0.488 on Juntendo, with precision gains of 15.9%
and 20.62%, respectively.

</details>


### [373] [To Glue or Not to Glue? Classical vs Learned Image Matching for Mobile Mapping Cameras to Textured Semantic 3D Building Models](https://arxiv.org/pdf/2505.17973)
*Simone Gaisbauer, Prabin Gyawali, Qilin Zhang, Olaf Wysocki, Boris Jutzi*

Main category: cs.CV

TL;DR: The paper compares classical and learnable feature-matching methods for semantic 3D building camera-to-model matching, finding learnable methods superior in accuracy and robustness.


<details>
  <summary>Details</summary>
Motivation: To address the lack of comprehensive comparison between classical and learnable feature-matching methods for semantic 3D building camera-to-model matching.

Method: Systematic evaluation using benchmark datasets (HPatches, MegaDepth-1500) and custom datasets with facade textures and camera images, assessing pose accuracy via PnP algorithm.

Result: Learnable methods outperform traditional approaches in accuracy and robustness, especially in challenging conditions.

Conclusion: This work supports the adoption of learnable methods for model-based visual localization, encouraging further development in the field.

Abstract: Feature matching is a necessary step for many computer vision and
photogrammetry applications such as image registration, structure-from-motion,
and visual localization. Classical handcrafted methods such as SIFT feature
detection and description combined with nearest neighbour matching and RANSAC
outlier removal have been state-of-the-art for mobile mapping cameras. With
recent advances in deep learning, learnable methods have been introduced and
proven to have better robustness and performance under complex conditions.
Despite their growing adoption, a comprehensive comparison between classical
and learnable feature matching methods for the specific task of semantic 3D
building camera-to-model matching is still missing. This submission
systematically evaluates the effectiveness of different feature-matching
techniques in visual localization using textured CityGML LoD2 models. We use
standard benchmark datasets (HPatches, MegaDepth-1500) and custom datasets
consisting of facade textures and corresponding camera images (terrestrial and
drone). For the latter, we evaluate the achievable accuracy of the absolute
pose estimated using a Perspective-n-Point (PnP) algorithm, with geometric
ground truth derived from geo-referenced trajectory data. The results indicate
that the learnable feature matching methods vastly outperform traditional
approaches regarding accuracy and robustness on our challenging custom datasets
with zero to 12 RANSAC-inliers and zero to 0.16 area under the curve. We
believe that this work will foster the development of model-based visual
localization methods. Link to the code:
https://github.com/simBauer/To\_Glue\_or\_not\_to\_Glue

</details>


### [374] [Few-Shot Learning from Gigapixel Images via Hierarchical Vision-Language Alignment and Modeling](https://arxiv.org/pdf/2505.17982)
*Bryan Wong, Jong Woo Kim, Huazhu Fu, Mun Yong Yi*

Main category: cs.CV

TL;DR: HiVE-MIL is a hierarchical vision-language framework for few-shot WSI classification, addressing limitations in multi-scale interaction and modality alignment, outperforming existing methods by up to 4.1% in macro F1.


<details>
  <summary>Details</summary>
Motivation: Existing methods lack sufficient modeling of intra-modal interactions across scales and alignment between visual and textual modalities, limiting performance in few-shot WSI classification.

Method: HiVE-MIL constructs a unified graph with parent-child links for hierarchical relationships and intra-scale edges for modality alignment, using text-guided dynamic filtering and hierarchical contrastive loss.

Result: HiVE-MIL outperforms traditional and VLM-based MIL approaches, achieving up to 4.1% higher macro F1 on TCGA datasets under 16-shot settings.

Conclusion: Jointly modeling hierarchical structure and multimodal alignment improves learning efficiency and scalability for pathology data with limited annotations.

Abstract: Vision-language models (VLMs) have recently been integrated into multiple
instance learning (MIL) frameworks to address the challenge of few-shot, weakly
supervised classification of whole slide images (WSIs). A key trend involves
leveraging multi-scale information to better represent hierarchical tissue
structures. However, existing methods often face two key limitations: (1)
insufficient modeling of interactions within the same modalities across scales
(e.g., 5x and 20x) and (2) inadequate alignment between visual and textual
modalities on the same scale. To address these gaps, we propose HiVE-MIL, a
hierarchical vision-language framework that constructs a unified graph
consisting of (1) parent-child links between coarse (5x) and fine (20x)
visual/textual nodes to capture hierarchical relationships, and (2)
heterogeneous intra-scale edges linking visual and textual nodes on the same
scale. To further enhance semantic consistency, HiVE-MIL incorporates a
two-stage, text-guided dynamic filtering mechanism that removes weakly
correlated patch-text pairs, and introduces a hierarchical contrastive loss to
align textual semantics across scales. Extensive experiments on TCGA breast,
lung, and kidney cancer datasets demonstrate that HiVE-MIL consistently
outperforms both traditional MIL and recent VLM-based MIL approaches, achieving
gains of up to 4.1% in macro F1 under 16-shot settings. Our results demonstrate
the value of jointly modeling hierarchical structure and multimodal alignment
for efficient and scalable learning from limited pathology data. The code is
available at https://github.com/bryanwong17/HiVE-MIL

</details>


### [375] [Canonical Pose Reconstruction from Single Depth Image for 3D Non-rigid Pose Recovery on Limited Datasets](https://arxiv.org/pdf/2505.17992)
*Fahd Alhamazani, Yu-Kun Lai, Paul L. Rosin*

Main category: cs.CV

TL;DR: A canonical pose reconstruction model for 3D reconstruction from 2D depth images of deformable shapes, achieving high performance with minimal training data.


<details>
  <summary>Details</summary>
Motivation: Addressing challenges in reconstructing non-rigid objects like humans due to deformation variability, where traditional methods require extensive data.

Method: Proposes transforming single-view depth images into a canonical form, enabling rigid reconstruction techniques and pose recovery in voxel representation.

Result: Outperforms state-of-the-art methods on animal and human datasets, using only ~300 samples.

Conclusion: The model effectively simplifies 3D reconstruction for deformable shapes with limited data, demonstrating superior performance.

Abstract: 3D reconstruction from 2D inputs, especially for non-rigid objects like
humans, presents unique challenges due to the significant range of possible
deformations. Traditional methods often struggle with non-rigid shapes, which
require extensive training data to cover the entire deformation space. This
study addresses these limitations by proposing a canonical pose reconstruction
model that transforms single-view depth images of deformable shapes into a
canonical form. This alignment facilitates shape reconstruction by enabling the
application of rigid object reconstruction techniques, and supports recovering
the input pose in voxel representation as part of the reconstruction task,
utilizing both the original and deformed depth images. Notably, our model
achieves effective results with only a small dataset of approximately 300
samples. Experimental results on animal and human datasets demonstrate that our
model outperforms other state-of-the-art methods.

</details>


### [376] [Segment Anyword: Mask Prompt Inversion for Open-Set Grounded Segmentation](https://arxiv.org/pdf/2505.17994)
*Zhihua Liu, Amrutha Saseendran, Lei Tong, Xilin He, Fariba Yousefi, Nikolay Burlutskiy, Dino Oglic, Tom Diethe, Philip Teare, Huiyu Zhou, Chen Jin*

Main category: cs.CV

TL;DR: Segment Anyword is a training-free method for open-set image segmentation using diffusion models and linguistic-guided visual prompts, achieving state-of-the-art results.


<details>
  <summary>Details</summary>
Motivation: Existing methods require extensive training and struggle with consistency in segmenting objects across diverse text references.

Method: Uses token-level cross-attention maps from a frozen diffusion model to generate mask prompts, refined with linguistic-guided visual prompt regularization.

Result: Achieves 52.5 mIoU on Pascal Context 59, 67.73 cIoU on gRefCOCO, and 67.4 mIoU on GranDf.

Conclusion: The approach is effective, generalizes well, and outperforms existing methods in open-set segmentation tasks.

Abstract: Open-set image segmentation poses a significant challenge because existing
methods often demand extensive training or fine-tuning and generally struggle
to segment unified objects consistently across diverse text reference
expressions. Motivated by this, we propose Segment Anyword, a novel
training-free visual concept prompt learning approach for open-set language
grounded segmentation that relies on token-level cross-attention maps from a
frozen diffusion model to produce segmentation surrogates or mask prompts,
which are then refined into targeted object masks. Initial prompts typically
lack coherence and consistency as the complexity of the image-text increases,
resulting in suboptimal mask fragments. To tackle this issue, we further
introduce a novel linguistic-guided visual prompt regularization that binds and
clusters visual prompts based on sentence dependency and syntactic structural
information, enabling the extraction of robust, noise-tolerant mask prompts,
and significant improvements in segmentation accuracy. The proposed approach is
effective, generalizes across different open-set segmentation tasks, and
achieves state-of-the-art results of 52.5 (+6.8 relative) mIoU on Pascal
Context 59, 67.73 (+25.73 relative) cIoU on gRefCOCO, and 67.4 (+1.1 relative
to fine-tuned methods) mIoU on GranDf, which is the most complex open-set
grounded segmentation task in the field.

</details>


### [377] [Clinical Validation of Deep Learning for Real-Time Tissue Oxygenation Estimation Using Spectral Imaging](https://arxiv.org/pdf/2505.18010)
*Jens De Winne, Siri Willems, Siri Luthman, Danilo Babin, Hiep Luong, Wim Ceelen*

Main category: cs.CV

TL;DR: Deep learning models (FCN and CNN) outperform traditional linear unmixing for real-time tissue oxygenation estimation, with domain-adversarial training bridging the gap between simulated and clinical data.


<details>
  <summary>Details</summary>
Motivation: Accurate, real-time monitoring of tissue ischemia is critical for surgery and tissue health, but conventional linear unmixing methods are limited by assumptions and impractical linear relations.

Method: Deep learning approaches (FCN and CNN) trained on Monte-Carlo simulated spectra, with domain-adversarial training to align simulated and clinical data.

Result: Deep learning models show higher correlation with capillary lactate measurements (hypoxia marker) than linear unmixing. Domain-adversarial training reduces domain gap.

Conclusion: Deep learning, especially with domain-adversarial training, offers superior and practical solutions for real-time tissue oxygenation monitoring in clinical settings.

Abstract: Accurate, real-time monitoring of tissue ischemia is crucial to understand
tissue health and guide surgery. Spectral imaging shows great potential for
contactless and intraoperative monitoring of tissue oxygenation. Due to the
difficulty of obtaining direct reference oxygenation values, conventional
methods are based on linear unmixing techniques. These are prone to assumptions
and these linear relations may not always hold in practice. In this work, we
present deep learning approaches for real-time tissue oxygenation estimation
using Monte-Carlo simulated spectra. We train a fully connected neural network
(FCN) and a convolutional neural network (CNN) for this task and propose a
domain-adversarial training approach to bridge the gap between simulated and
real clinical spectral data. Results demonstrate that these deep learning
models achieve a higher correlation with capillary lactate measurements, a
well-known marker of hypoxia, obtained during spectral imaging in surgery,
compared to traditional linear unmixing. Notably, domain-adversarial training
effectively reduces the domain gap, optimizing performance in real clinical
settings.

</details>


### [378] [SemSegBench & DetecBench: Benchmarking Reliability and Generalization Beyond Classification](https://arxiv.org/pdf/2505.18015)
*Shashank Agnihotri, David Schader, Jonas Jakubassa, Nico Sharei, Simon Kral, Mehmet Ege Kaçar, Ruben Weber, Margret Keuper*

Main category: cs.CV

TL;DR: The paper introduces benchmarking tools (SEMSEGBENCH and DETECBENCH) to evaluate robustness in semantic segmentation and object detection models, revealing systematic weaknesses in state-of-the-art models.


<details>
  <summary>Details</summary>
Motivation: To address the lack of research on reliability and generalization in semantic tasks beyond image classification, especially in safety-critical domains.

Method: Proposed benchmarking tools and conducted extensive evaluations (76 segmentation models and 61 object detectors) under adversarial attacks and corruptions.

Result: Identified systematic weaknesses and trends based on architecture, backbone, and model capacity.

Conclusion: The tools and findings aim to foster future research for improved model reliability in semantic tasks.

Abstract: Reliability and generalization in deep learning are predominantly studied in
the context of image classification. Yet, real-world applications in
safety-critical domains involve a broader set of semantic tasks, such as
semantic segmentation and object detection, which come with a diverse set of
dedicated model architectures. To facilitate research towards robust model
design in segmentation and detection, our primary objective is to provide
benchmarking tools regarding robustness to distribution shifts and adversarial
manipulations. We propose the benchmarking tools SEMSEGBENCH and DETECBENCH,
along with the most extensive evaluation to date on the reliability and
generalization of semantic segmentation and object detection models. In
particular, we benchmark 76 segmentation models across four datasets and 61
object detectors across two datasets, evaluating their performance under
diverse adversarial attacks and common corruptions. Our findings reveal
systematic weaknesses in state-of-the-art models and uncover key trends based
on architecture, backbone, and model capacity. SEMSEGBENCH and DETECBENCH are
open-sourced in our GitHub repository
(https://github.com/shashankskagnihotri/benchmarking_reliability_generalization)
along with our complete set of total 6139 evaluations. We anticipate the
collected data to foster and encourage future research towards improved model
reliability beyond classification.

</details>


### [379] [Building Floor Number Estimation from Crowdsourced Street-Level Images: Munich Dataset and Baseline Method](https://arxiv.org/pdf/2505.18021)
*Yao Sun, Sining Chen, Yifan Tian, Xiao Xiang Zhu*

Main category: cs.CV

TL;DR: A deep learning framework predicts building floor counts from street-level images, achieving high accuracy and generalization, with an open dataset for benchmarking.


<details>
  <summary>Details</summary>
Motivation: Accurate floor-count data is crucial for urban planning and modeling but is often missing in existing databases.

Method: An end-to-end deep learning framework infers floor numbers from crowdsourced street-level imagery without hand-crafted features.

Result: The model achieves 81.2% exact accuracy and 97.9% within +/-1 floor on the Munich Building Floor Dataset.

Conclusion: The method and dataset provide a scalable solution for enriching 3D city models and advancing urban informatics.

Abstract: Accurate information on the number of building floors, or above-ground
storeys, is essential for household estimation, utility provision, risk
assessment, evacuation planning, and energy modeling. Yet large-scale
floor-count data are rarely available in cadastral and 3D city databases. This
study proposes an end-to-end deep learning framework that infers floor numbers
directly from unrestricted, crowdsourced street-level imagery, avoiding
hand-crafted features and generalizing across diverse facade styles. To enable
benchmarking, we release the Munich Building Floor Dataset, a public set of
over 6800 geo-tagged images collected from Mapillary and targeted field
photography, each paired with a verified storey label. On this dataset, the
proposed classification-regression network attains 81.2% exact accuracy and
predicts 97.9% of buildings within +/-1 floor. The method and dataset together
offer a scalable route to enrich 3D city models with vertical information and
lay a foundation for future work in urban informatics, remote sensing, and
geographic information science. Source code and data will be released under an
open license at https://github.com/ya0-sun/Munich-SVI-Floor-Benchmark.

</details>


### [380] [RemoteSAM: Towards Segment Anything for Earth Observation](https://arxiv.org/pdf/2505.18022)
*Liang Yao, Fan Liu, Delong Chen, Chuanyi Zhang, Yijun Wang, Ziyun Chen, Wei Xu, Shimin Di, Yuhui Zheng*

Main category: cs.CV

TL;DR: The paper introduces RemoteSAM, a robust visual foundation model for Earth observation, addressing limitations in current systems through scalable data generation and a task unification paradigm.


<details>
  <summary>Details</summary>
Motivation: Current Earth observation systems lack flexibility and robustness due to task-specific architectures and limited data. The goal is to create a model capable of diverse visual recognition and localization with broad compatibility.

Method: The study combines an automatic data engine for scalable dataset creation (270K image-text-mask triplets) with a task unification paradigm centered on referring expression segmentation, enabling a single model to handle multiple vision tasks.

Result: RemoteSAM achieves state-of-the-art performance on Earth observation benchmarks, outperforming models like Falcon, GeoChat, and LHRS-Bot with higher efficiency.

Conclusion: RemoteSAM demonstrates the effectiveness of scalable data and unified modeling for Earth observation, setting new benchmarks and offering public availability for further research.

Abstract: We aim to develop a robust yet flexible visual foundation model for Earth
observation. It should possess strong capabilities in recognizing and
localizing diverse visual targets while providing compatibility with various
input-output interfaces required across different task scenarios. Current
systems cannot meet these requirements, as they typically utilize task-specific
architecture trained on narrow data domains with limited semantic coverage. Our
study addresses these limitations from two aspects: data and modeling. We first
introduce an automatic data engine that enjoys significantly better scalability
compared to previous human annotation or rule-based approaches. It has enabled
us to create the largest dataset of its kind to date, comprising 270K
image-text-mask triplets covering an unprecedented range of diverse semantic
categories and attribute specifications. Based on this data foundation, we
further propose a task unification paradigm that centers around referring
expression segmentation. It effectively handles a wide range of vision-centric
perception tasks, including classification, detection, segmentation, grounding,
etc, using a single model without any task-specific heads. Combining these
innovations on data and modeling, we present RemoteSAM, a foundation model that
establishes new SoTA on several earth observation perception benchmarks,
outperforming other foundation models such as Falcon, GeoChat, and LHRS-Bot
with significantly higher efficiency. Models and data are publicly available at
https://github.com/1e12Leon/RemoteSAM.

</details>


### [381] [A Wavelet-based Stereo Matching Framework for Solving Frequency Convergence Inconsistency](https://arxiv.org/pdf/2505.18024)
*Xiaobao Wei, Jiawei Liu, Dongbo Yang, Junda Cheng, Changyong Shu, Wei Wang*

Main category: cs.CV

TL;DR: Wavelet-Stereo addresses inconsistent frequency convergence in stereo matching by separately processing high and low frequencies, outperforming state-of-the-art methods.


<details>
  <summary>Details</summary>
Motivation: Current iterative methods degrade high-frequency details (e.g., edges) by optimizing all frequencies together without distinction.

Method: Uses wavelet transform to decompose images into high/low frequencies, processes them separately with specialized feature extractors, and employs an LSTM-based update operator for high-frequency preservation.

Result: Ranks 1st on KITTI 2015 and 2012 leaderboards, excelling in scenes with fine details.

Conclusion: Wavelet-Stereo effectively refines high and low frequencies separately, improving stereo matching performance.

Abstract: We find that the EPE evaluation metrics of RAFT-stereo converge
inconsistently in the low and high frequency regions, resulting high frequency
degradation (e.g., edges and thin objects) during the iterative process. The
underlying reason for the limited performance of current iterative methods is
that it optimizes all frequency components together without distinguishing
between high and low frequencies. We propose a wavelet-based stereo matching
framework (Wavelet-Stereo) for solving frequency convergence inconsistency.
Specifically, we first explicitly decompose an image into high and low
frequency components using discrete wavelet transform. Then, the high-frequency
and low-frequency components are fed into two different multi-scale frequency
feature extractors. Finally, we propose a novel LSTM-based high-frequency
preservation update operator containing an iterative frequency adapter to
provide adaptive refined high-frequency features at different iteration steps
by fine-tuning the initial high-frequency features. By processing high and low
frequency components separately, our framework can simultaneously refine
high-frequency information in edges and low-frequency information in smooth
regions, which is especially suitable for challenging scenes with fine details
and textures in the distance. Extensive experiments demonstrate that our
Wavelet-Stereo outperforms the state-of-the-art methods and ranks 1st on both
the KITTI 2015 and KITTI 2012 leaderboards for almost all metrics. We will
provide code and pre-trained models to encourage further exploration,
application, and development of our innovative framework
(https://github.com/SIA-IDE/Wavelet-Stereo).

</details>


### [382] [3D Face Reconstruction Error Decomposed: A Modular Benchmark for Fair and Fast Method Evaluation](https://arxiv.org/pdf/2505.18025)
*Evangelos Sariyanidi, Claudio Ferrari, Federico Nocentini, Stefano Berretti, Andrea Cavallaro, Birkan Tunc*

Main category: cs.CV

TL;DR: A modular toolkit (M3DFB) is introduced for 3D face reconstruction benchmarking, allowing interchangeable components for error computation. It reveals flaws in current methods (e.g., ICP) and proposes a faster, topology-aware correction scheme.


<details>
  <summary>Details</summary>
Motivation: Current benchmark tools for 3D face reconstruction are monolithic and lack consensus on error measurement. A modular approach is needed to evaluate individual components and improve accuracy.

Method: The toolkit segregates error computation components (e.g., cropping, alignment, correction) for flexibility. A new topology-aware correction method is introduced, and 16 error estimators are tested on 10 reconstruction methods across four datasets.

Result: ICP-based estimators perform poorly (correlation as low as 0.41), while non-rigid alignment improves accuracy (correlation >0.90). The proposed correction scheme matches top estimators' accuracy but runs faster.

Conclusion: M3DFB enables flexible benchmarking, revealing flaws in current methods and offering a faster, accurate alternative. It supports improved error estimation for training learned reconstruction methods.

Abstract: Computing the standard benchmark metric for 3D face reconstruction, namely
geometric error, requires a number of steps, such as mesh cropping, rigid
alignment, or point correspondence. Current benchmark tools are monolithic
(they implement a specific combination of these steps), even though there is no
consensus on the best way to measure error. We present a toolkit for a
Modularized 3D Face reconstruction Benchmark (M3DFB), where the fundamental
components of error computation are segregated and interchangeable, allowing
one to quantify the effect of each. Furthermore, we propose a new component,
namely correction, and present a computationally efficient approach that
penalizes for mesh topology inconsistency. Using this toolkit, we test 16 error
estimators with 10 reconstruction methods on two real and two synthetic
datasets. Critically, the widely used ICP-based estimator provides the worst
benchmarking performance, as it significantly alters the true ranking of the
top-5 reconstruction methods. Notably, the correlation of ICP with the true
error can be as low as 0.41. Moreover, non-rigid alignment leads to significant
improvement (correlation larger than 0.90), highlighting the importance of
annotating 3D landmarks on datasets. Finally, the proposed correction scheme,
together with non-rigid warping, leads to an accuracy on a par with the best
non-rigid ICP-based estimators, but runs an order of magnitude faster. Our
open-source codebase is designed for researchers to easily compare alternatives
for each component, thus helping accelerating progress in benchmarking for 3D
face reconstruction and, furthermore, supporting the improvement of learned
reconstruction methods, which depend on accurate error estimation for effective
training.

</details>


### [383] [CAMME: Adaptive Deepfake Image Detection with Multi-Modal Cross-Attention](https://arxiv.org/pdf/2505.18035)
*Naseem Khan, Tuan Nguyen, Amine Bermak, Issa Khalil*

Main category: cs.CV

TL;DR: CAMME, a multi-modal framework using cross-attention, improves deepfake detection by integrating visual, textual, and frequency features, outperforming existing methods with robust cross-domain generalization.


<details>
  <summary>Details</summary>
Motivation: Addressing the challenge of detecting deepfakes across unseen generative architectures due to rapid AI advancements.

Method: CAMME dynamically combines visual, textual, and frequency-domain features via multi-head cross-attention for robust detection.

Result: CAMME achieves 12.56% and 13.25% improvements on natural scenes and facial deepfakes, with high accuracy under perturbations and adversarial attacks.

Conclusion: Cross-attention-based multi-modal integration enhances deepfake detection reliability across diverse generative models.

Abstract: The proliferation of sophisticated AI-generated deepfakes poses critical
challenges for digital media authentication and societal security. While
existing detection methods perform well within specific generative domains,
they exhibit significant performance degradation when applied to manipulations
produced by unseen architectures--a fundamental limitation as generative
technologies rapidly evolve. We propose CAMME (Cross-Attention Multi-Modal
Embeddings), a framework that dynamically integrates visual, textual, and
frequency-domain features through a multi-head cross-attention mechanism to
establish robust cross-domain generalization. Extensive experiments demonstrate
CAMME's superiority over state-of-the-art methods, yielding improvements of
12.56% on natural scenes and 13.25% on facial deepfakes. The framework
demonstrates exceptional resilience, maintaining (over 91%) accuracy under
natural image perturbations and achieving 89.01% and 96.14% accuracy against
PGD and FGSM adversarial attacks, respectively. Our findings validate that
integrating complementary modalities through cross-attention enables more
effective decision boundary realignment for reliable deepfake detection across
heterogeneous generative architectures.

</details>


### [384] [Clip4Retrofit: Enabling Real-Time Image Labeling on Edge Devices via Cross-Architecture CLIP Distillation](https://arxiv.org/pdf/2505.18039)
*Li Zhong, Ahmed Ghazal, Jun-Jun Wan, Frederik Zilly, Patrick Mackens, Joachim E. Vollrath, Bogdan Sorin Coseriu*

Main category: cs.CV

TL;DR: Clip4Retrofit distills CLIP into a lightweight model for real-time image labeling on edge devices, balancing efficiency and performance.


<details>
  <summary>Details</summary>
Motivation: Foundation models like CLIP are computationally heavy, making them unsuitable for resource-constrained edge devices. The goal is to enable real-time vision-language tasks on such devices.

Method: Proposes Clip4Retrofit, a distillation framework combining EfficientNet-B3 with MLP heads to reduce computational demands while preserving cross-modal alignment.

Result: The distilled model achieves efficient real-time image labeling on edge devices, suitable for applications like autonomous driving.

Conclusion: Clip4Retrofit bridges the gap between advanced vision-language models and their deployment in resource-limited environments, expanding their use in edge computing.

Abstract: Foundation models like CLIP (Contrastive Language-Image Pretraining) have
revolutionized vision-language tasks by enabling zero-shot and few-shot
learning through cross-modal alignment. However, their computational complexity
and large memory footprint make them unsuitable for deployment on
resource-constrained edge devices, such as in-car cameras used for image
collection and real-time processing. To address this challenge, we propose
Clip4Retrofit, an efficient model distillation framework that enables real-time
image labeling on edge devices. The framework is deployed on the Retrofit
camera, a cost-effective edge device retrofitted into thousands of vehicles,
despite strict limitations on compute performance and memory. Our approach
distills the knowledge of the CLIP model into a lightweight student model,
combining EfficientNet-B3 with multi-layer perceptron (MLP) projection heads to
preserve cross-modal alignment while significantly reducing computational
requirements. We demonstrate that our distilled model achieves a balance
between efficiency and performance, making it ideal for deployment in
real-world scenarios. Experimental results show that Clip4Retrofit can perform
real-time image labeling and object identification on edge devices with limited
resources, offering a practical solution for applications such as autonomous
driving and retrofitting existing systems. This work bridges the gap between
state-of-the-art vision-language models and their deployment in
resource-constrained environments, paving the way for broader adoption of
foundation models in edge computing.

</details>


### [385] [RestoreVAR: Visual Autoregressive Generation for All-in-One Image Restoration](https://arxiv.org/pdf/2505.18047)
*Sudarshan Rajagopalan, Kartik Narayan, Vishal M. Patel*

Main category: cs.CV

TL;DR: RestoreVAR, a generative approach for All-in-One image Restoration (AiOR), outperforms LDM-based models in performance and speed, leveraging visual autoregressive modeling (VAR) for efficiency.


<details>
  <summary>Details</summary>
Motivation: Latent diffusion models (LDMs) like Stable Diffusion improve AiOR quality but suffer from slow inference, making them impractical for time-sensitive tasks.

Method: RestoreVAR uses VAR for image generation, with architectural improvements like cross-attention mechanisms and latent-space refinement for AiOR.

Result: RestoreVAR achieves state-of-the-art performance in AiOR, with 10x faster inference than LDM-based models and strong generalization.

Conclusion: RestoreVAR is a highly efficient and effective solution for AiOR, combining VAR's advantages with tailored architectural enhancements.

Abstract: The use of latent diffusion models (LDMs) such as Stable Diffusion has
significantly improved the perceptual quality of All-in-One image Restoration
(AiOR) methods, while also enhancing their generalization capabilities.
However, these LDM-based frameworks suffer from slow inference due to their
iterative denoising process, rendering them impractical for time-sensitive
applications. To address this, we propose RestoreVAR, a novel generative
approach for AiOR that significantly outperforms LDM-based models in
restoration performance while achieving over $\mathbf{10\times}$ faster
inference. RestoreVAR leverages visual autoregressive modeling (VAR), a
recently introduced approach which performs scale-space autoregression for
image generation. VAR achieves comparable performance to that of
state-of-the-art diffusion transformers with drastically reduced computational
costs. To optimally exploit these advantages of VAR for AiOR, we propose
architectural modifications and improvements, including intricately designed
cross-attention mechanisms and a latent-space refinement module, tailored for
the AiOR task. Extensive experiments show that RestoreVAR achieves
state-of-the-art performance among generative AiOR methods, while also
exhibiting strong generalization capabilities.

</details>


### [386] [SHARDeg: A Benchmark for Skeletal Human Action Recognition in Degraded Scenarios](https://arxiv.org/pdf/2505.18048)
*Simon Malzard, Nitish Mital, Richard Walters, Victoria Nockles, Raghuveer Rao, Celso M. De Melo*

Main category: cs.CV

TL;DR: The paper introduces a benchmark for assessing SHAR model robustness to data degradation, revealing significant accuracy variations and proposing a simple mitigation method.


<details>
  <summary>Details</summary>
Motivation: Current SHAR models lack thorough assessment under real-world degraded data conditions, which is critical for real-time and edge applications.

Method: A benchmark is created using the NTU-RGB+D-120 dataset to evaluate five SHAR models under three degradation types. Temporal regularity is identified as a key factor, and interpolation is used to mitigate performance drops.

Result: Degradation type impacts accuracy by >40%. Interpolation improves performance by >40%. The LogSigRNN model outperforms SoTA in degraded conditions.

Conclusion: The benchmark highlights the importance of degradation robustness in SHAR models, identifies temporal regularity as a key factor, and showcases LogSigRNN as a promising degradation-resistant model.

Abstract: Computer vision (CV) models for detection, prediction or classification tasks
operate on video data-streams that are often degraded in the real world, due to
deployment in real-time or on resource-constrained hardware. It is therefore
critical that these models are robust to degraded data, but state of the art
(SoTA) models are often insufficiently assessed with these real-world
constraints in mind. This is exemplified by Skeletal Human Action Recognition
(SHAR), which is critical in many CV pipelines operating in real-time and at
the edge, but robustness to degraded data has previously only been shallowly
and inconsistently assessed. Here we address this issue for SHAR by providing
an important first data degradation benchmark on the most detailed and largest
3D open dataset, NTU-RGB+D-120, and assess the robustness of five leading SHAR
models to three forms of degradation that represent real-world issues. We
demonstrate the need for this benchmark by showing that the form of
degradation, which has not previously been considered, has a large impact on
model accuracy; at the same effective frame rate, model accuracy can vary by
>40% depending on degradation type. We also identify that temporal regularity
of frames in degraded SHAR data is likely a major driver of differences in
model performance, and harness this to improve performance of existing models
by up to >40%, through employing a simple mitigation approach based on
interpolation. Finally, we highlight how our benchmark has helped identify an
important degradation-resistant SHAR model based in Rough Path Theory; the
LogSigRNN SHAR model outperforms the SoTA DeGCN model in five out of six cases
at low frame rates by an average accuracy of 6%, despite trailing the SoTA
model by 11-12% on un-degraded data at high frame rates (30 FPS).

</details>


### [387] [SpikeGen: Generative Framework for Visual Spike Stream Processing](https://arxiv.org/pdf/2505.18049)
*Gaole Dai, Menghang Dong, Rongyu Zhang, Ruichuan An, Shanghang Zhang, Tiejun Huang*

Main category: cs.CV

TL;DR: SpikeGen, a generative framework, enhances sparse spike camera data by combining it with RGB modalities for tasks like deblurring and reconstruction.


<details>
  <summary>Details</summary>
Motivation: Address the sparsity of spatial information in spike camera data while leveraging its temporal richness.

Method: Introduces SpikeGen, a generative processing framework for spike streams, evaluated on mixed spike-RGB tasks.

Result: Effective in tasks like deblurring, dense frame reconstruction, and novel-view synthesis.

Conclusion: Generative models can synergize spike and RGB modalities, overcoming sparsity and enhancing visual data.

Abstract: Neuromorphic Visual Systems, such as spike cameras, have attracted
considerable attention due to their ability to capture clear textures under
dynamic conditions. This capability effectively mitigates issues related to
motion and aperture blur. However, in contrast to conventional RGB modalities
that provide dense spatial information, these systems generate binary,
spatially sparse frames as a trade-off for temporally rich visual streams. In
this context, generative models emerge as a promising solution to address the
inherent limitations of sparse data. These models not only facilitate the
conditional fusion of existing information from both spike and RGB modalities
but also enable the conditional generation based on latent priors. In this
study, we introduce a robust generative processing framework named SpikeGen,
designed for visual spike streams captured by spike cameras. We evaluate this
framework across multiple tasks involving mixed spike-RGB modalities, including
conditional image/video deblurring, dense frame reconstruction from spike
streams, and high-speed scene novel-view synthesis. Supported by comprehensive
experimental results, we demonstrate that leveraging the latent space operation
abilities of generative models allows us to effectively address the sparsity of
spatial information while fully exploiting the temporal richness of spike
streams, thereby promoting a synergistic enhancement of different visual
modalities.

</details>


### [388] [LookWhere? Efficient Visual Recognition by Learning Where to Look and What to See from Self-Supervision](https://arxiv.org/pdf/2505.18051)
*Anthony Fuller, Yousef Yassin, Junfeng Wen, Daniel G. Kyrollos, Tarek Ibrahim, James R. Green, Evan Shelhamer*

Main category: cs.CV

TL;DR: LookWhere reduces computation costs in vision transformers by adaptively selecting and extracting image regions, achieving efficiency without sacrificing accuracy.


<details>
  <summary>Details</summary>
Motivation: Vision transformers are computationally expensive, especially at high resolutions, prompting the need for adaptive computation methods.

Method: LookWhere uses a low-resolution selector and high-resolution extractor, pretrained via distillation from a self-supervised teacher, to avoid full high-resolution processing.

Result: LookWhere reduces FLOPs by 34x and time by 6x for high-resolution tasks, while improving accuracy in standard tasks like ImageNet and ADE20K.

Conclusion: LookWhere offers an efficient and accurate solution for adaptive computation in vision transformers, applicable to diverse recognition tasks.

Abstract: Vision transformers are ever larger, more accurate, and more expensive to
compute. The expense is even more extreme at high resolution as the number of
tokens grows quadratically with the image size. We turn to adaptive computation
to cope with this cost by learning to predict where to compute. Our LookWhere
method divides the computation between a low-resolution selector and a
high-resolution extractor without ever processing the full high-resolution
input. We jointly pretrain the selector and extractor without task supervision
by distillation from a self-supervised teacher, in effect, learning where and
what to compute simultaneously. Unlike prior token reduction methods, which pay
to save by pruning already-computed tokens, and prior token selection methods,
which require complex and expensive per-task optimization, LookWhere
economically and accurately selects and extracts transferrable representations
of images. We show that LookWhere excels at sparse recognition on
high-resolution inputs (Traffic Signs), maintaining accuracy while reducing
FLOPs by up to 34x and time by 6x. It also excels at standard recognition tasks
that are global (ImageNet classification) or local (ADE20K segmentation),
improving accuracy while reducing time by 1.36x.

</details>


### [389] [BOTM: Echocardiography Segmentation via Bi-directional Optimal Token Matching](https://arxiv.org/pdf/2505.18052)
*Zhihua Liu, Lei Tong, Xilin He, Che Liu, Rossella Arcucci, Chen Jin, Huiyu Zhou*

Main category: cs.CV

TL;DR: BOTM (Bi-directional Optimal Token Matching) is a novel framework for echocardiography segmentation that ensures anatomical consistency by matching image tokens and using bi-directional cross-transport attention.


<details>
  <summary>Details</summary>
Motivation: Existing methods struggle with anatomical inconsistency due to shape variation, partial observation, and region ambiguity in low signal-to-noise conditions.

Method: BOTM learns to match discrete image tokens from paired echocardiographic images using optimal correspondences and extends this into bi-directional cross-transport attention for temporal consistency.

Result: BOTM achieves stable and accurate segmentation (e.g., -1.917 HD on CAMUS2H LV, +1.9% Dice on TED) with anatomical consistency.

Conclusion: BOTM provides a robust solution for echocardiography segmentation with improved anatomical consistency and matching interpretation.

Abstract: Existed echocardiography segmentation methods often suffer from anatomical
inconsistency challenge caused by shape variation, partial observation and
region ambiguity with similar intensity across 2D echocardiographic sequences,
resulting in false positive segmentation with anatomical defeated structures in
challenging low signal-to-noise ratio conditions. To provide a strong
anatomical guarantee across different echocardiographic frames, we propose a
novel segmentation framework named BOTM (Bi-directional Optimal Token Matching)
that performs echocardiography segmentation and optimal anatomy transportation
simultaneously. Given paired echocardiographic images, BOTM learns to match two
sets of discrete image tokens by finding optimal correspondences from a novel
anatomical transportation perspective. We further extend the token matching
into a bi-directional cross-transport attention proxy to regulate the preserved
anatomical consistency within the cardiac cyclic deformation in temporal
domain. Extensive experimental results show that BOTM can generate stable and
accurate segmentation outcomes (e.g. -1.917 HD on CAMUS2H LV, +1.9% Dice on
TED), and provide a better matching interpretation with anatomical consistency
guarantee.

</details>


### [390] [FDBPL: Faster Distillation-Based Prompt Learning for Region-Aware Vision-Language Models Adaptation](https://arxiv.org/pdf/2505.18053)
*Zherui Zhang, Jiaxin Wu, Changwei Wang, Rongtao Xu, Longzhao Huang, Wenhao Xu, Wenbo Xu, Li Guo, Shibiao Xu*

Main category: cs.CV

TL;DR: FDBPL improves prompt learning by sharing supervision contexts, accelerating I/O, and using dual prompt spaces for better generalization and efficiency.


<details>
  <summary>Details</summary>
Motivation: Address limitations of hard-prompt design and soft-prompt methods, which lack generalization and efficiency.

Method: Uses shared soft supervision, accelerated I/O, and dual positive-negative prompt spaces with mutual learning.

Result: Achieves 2.2× faster training and superior performance on 11 datasets.

Conclusion: FDBPL maintains parameter efficiency and strong generalization, outperforming existing methods.

Abstract: Prompt learning as a parameter-efficient method that has been widely adopted
to adapt Vision-Language Models (VLMs) to downstream tasks. While hard-prompt
design requires domain expertise and iterative optimization, soft-prompt
methods rely heavily on task-specific hard labels, limiting their
generalization to unseen categories. Recent popular distillation-based prompt
learning methods improve generalization by exploiting larger teacher VLMs and
unsupervised knowledge transfer, yet their repetitive teacher model online
inference sacrifices the inherent training efficiency advantage of prompt
learning. In this paper, we propose {{\large {\textbf{F}}}}aster {{\large
{\textbf{D}}}}istillation-{{\large {\textbf{B}}}}ased {{\large
{\textbf{P}}}}rompt {{\large {\textbf{L}}}}earning (\textbf{FDBPL}), which
addresses these issues by sharing soft supervision contexts across multiple
training stages and implementing accelerated I/O. Furthermore, FDBPL introduces
a region-aware prompt learning paradigm with dual positive-negative prompt
spaces to fully exploit randomly cropped regions that containing multi-level
information. We propose a positive-negative space mutual learning mechanism
based on similarity-difference learning, enabling student CLIP models to
recognize correct semantics while learning to reject weakly related concepts,
thereby improving zero-shot performance. Unlike existing distillation-based
prompt learning methods that sacrifice parameter efficiency for generalization,
FDBPL maintains dual advantages of parameter efficiency and strong downstream
generalization. Comprehensive evaluations across 11 datasets demonstrate
superior performance in base-to-new generalization, cross-dataset transfer, and
robustness tests, achieving $2.2\times$ faster training speed.

</details>


### [391] [Semantic Correspondence: Unified Benchmarking and a Strong Baseline](https://arxiv.org/pdf/2505.18060)
*Kaiyan Zhang, Xinghui Li, Jingyi Lu, Kai Han*

Main category: cs.CV

TL;DR: This paper presents the first comprehensive survey of semantic correspondence methods in computer vision, proposing a taxonomy, analyzing existing approaches, summarizing benchmark results, and introducing a simple yet effective baseline for future research.


<details>
  <summary>Details</summary>
Motivation: The task of semantic correspondence lacks a thorough review despite significant progress, prompting this survey to classify, analyze, and benchmark existing methods.

Method: The authors propose a taxonomy to categorize methods, analyze each approach, summarize benchmark results, and conduct controlled experiments to evaluate method components.

Result: A unified comparative table of benchmark results is provided, and a new baseline method achieves state-of-the-art performance.

Conclusion: The survey serves as a reference and baseline for future research, with code publicly available for reproducibility.

Abstract: Establishing semantic correspondence is a challenging task in computer
vision, aiming to match keypoints with the same semantic information across
different images. Benefiting from the rapid development of deep learning,
remarkable progress has been made over the past decade. However, a
comprehensive review and analysis of this task remains absent. In this paper,
we present the first extensive survey of semantic correspondence methods. We
first propose a taxonomy to classify existing methods based on the type of
their method designs. These methods are then categorized accordingly, and we
provide a detailed analysis of each approach. Furthermore, we aggregate and
summarize the results of methods in literature across various benchmarks into a
unified comparative table, with detailed configurations to highlight
performance variations. Additionally, to provide a detailed understanding on
existing methods for semantic matching, we thoroughly conduct controlled
experiments to analyse the effectiveness of the components of different
methods. Finally, we propose a simple yet effective baseline that achieves
state-of-the-art performance on multiple benchmarks, providing a solid
foundation for future research in this field. We hope this survey serves as a
comprehensive reference and consolidated baseline for future development. Code
is publicly available at: https://github.com/Visual-AI/Semantic-Correspondence.

</details>


### [392] [DanceTogether! Identity-Preserving Multi-Person Interactive Video Generation](https://arxiv.org/pdf/2505.18078)
*Junhao Chen, Mingjin Chen, Jianjin Xu, Xiang Li, Junting Dong, Mingze Sun, Puhua Jiang, Hongxiang Li, Yuhang Yang, Hao Zhao, Xiaoxiao Long, Ruqi Huang*

Main category: cs.CV

TL;DR: DanceTogether is a diffusion-based framework for generating photorealistic videos from a single image and pose-mask streams, preserving identities and enabling multi-actor interactions.


<details>
  <summary>Details</summary>
Motivation: Current CVG systems struggle with multi-actor interactions under noisy control signals, leading to identity drift and appearance issues.

Method: Uses a MaskPoseAdapter to fuse tracking masks and pose heatmaps, ensuring identity preservation. Introduces datasets (PairFS-4K, HumanRob-300) and a benchmark (TogetherVideoBench).

Result: Outperforms prior methods on TogetherVideoBench and shows generalization to human-robot tasks with minimal fine-tuning.

Conclusion: DanceTogether advances CVG to multi-actor interactions, benefiting digital production, simulation, and embodied AI.

Abstract: Controllable video generation (CVG) has advanced rapidly, yet current systems
falter when more than one actor must move, interact, and exchange positions
under noisy control signals. We address this gap with DanceTogether, the first
end-to-end diffusion framework that turns a single reference image plus
independent pose-mask streams into long, photorealistic videos while strictly
preserving every identity. A novel MaskPoseAdapter binds "who" and "how" at
every denoising step by fusing robust tracking masks with semantically rich-but
noisy-pose heat-maps, eliminating the identity drift and appearance bleeding
that plague frame-wise pipelines. To train and evaluate at scale, we introduce
(i) PairFS-4K, 26 hours of dual-skater footage with 7,000+ distinct IDs, (ii)
HumanRob-300, a one-hour humanoid-robot interaction set for rapid cross-domain
transfer, and (iii) TogetherVideoBench, a three-track benchmark centered on the
DanceTogEval-100 test suite covering dance, boxing, wrestling, yoga, and figure
skating. On TogetherVideoBench, DanceTogether outperforms the prior arts by a
significant margin. Moreover, we show that a one-hour fine-tune yields
convincing human-robot videos, underscoring broad generalization to embodied-AI
and HRI tasks. Extensive ablations confirm that persistent identity-action
binding is critical to these gains. Together, our model, datasets, and
benchmark lift CVG from single-subject choreography to compositionally
controllable, multi-actor interaction, opening new avenues for digital
production, simulation, and embodied intelligence. Our video demos and code are
available at https://DanceTog.github.io/.

</details>


### [393] [Deep Video Discovery: Agentic Search with Tool Use for Long-form Video Understanding](https://arxiv.org/pdf/2505.18079)
*Xiaoyi Zhang, Zhaoyang Jia, Zongyu Guo, Jiahao Li, Bin Li, Houqiang Li, Yan Lu*

Main category: cs.CV

TL;DR: The paper proposes the Deep Video Discovery (DVD) agent, an autonomous system leveraging LLMs and agentic search to improve long-form video understanding, achieving state-of-the-art performance.


<details>
  <summary>Details</summary>
Motivation: Addressing the limitations of LLMs in processing information-dense, hour-long videos due to their temporal-spatial complexity and extended contexts.

Method: The DVD agent uses an autonomous search strategy over segmented video clips, employing LLM reasoning to plan, select tools, and iteratively refine its approach.

Result: The DVD agent achieves SOTA performance on the LVBench dataset, significantly outperforming prior works.

Conclusion: The DVD agent's autonomous design and tool-based approach advance long-form video understanding, with potential for further improvements.

Abstract: Long-form video understanding presents significant challenges due to
extensive temporal-spatial complexity and the difficulty of question answering
under such extended contexts. While Large Language Models (LLMs) have
demonstrated considerable advancements in video analysis capabilities and long
context handling, they continue to exhibit limitations when processing
information-dense hour-long videos. To overcome such limitations, we propose
the Deep Video Discovery agent to leverage an agentic search strategy over
segmented video clips. Different from previous video agents manually designing
a rigid workflow, our approach emphasizes the autonomous nature of agents. By
providing a set of search-centric tools on multi-granular video database, our
DVD agent leverages the advanced reasoning capability of LLM to plan on its
current observation state, strategically selects tools, formulates appropriate
parameters for actions, and iteratively refines its internal reasoning in light
of the gathered information. We perform comprehensive evaluation on multiple
long video understanding benchmarks that demonstrates the advantage of the
entire system design. Our DVD agent achieves SOTA performance, significantly
surpassing prior works by a large margin on the challenging LVBench dataset.
Comprehensive ablation studies and in-depth tool analyses are also provided,
yielding insights to further advance intelligent agents tailored for long-form
video understanding tasks. The code will be released later.

</details>


### [394] [CXReasonBench: A Benchmark for Evaluating Structured Diagnostic Reasoning in Chest X-rays](https://arxiv.org/pdf/2505.18087)
*Hyungyung Lee, Geon Choi, Jung-Oh Lee, Hangyul Yoon, Hyuk Gi Hong, Edward Choi*

Main category: cs.CV

TL;DR: CheXStruct and CXReasonBench introduce a structured pipeline and benchmark to evaluate clinically meaningful reasoning in LVLMs for medical tasks, revealing limitations in current models.


<details>
  <summary>Details</summary>
Motivation: Existing benchmarks lack insight into clinically meaningful reasoning in LVLMs, prompting the need for a structured evaluation approach.

Method: CheXStruct derives intermediate reasoning steps from chest X-rays, while CXReasonBench evaluates models on clinically valid reasoning and structured guidance.

Result: Evaluated LVLMs struggle with structured reasoning and generalization, often failing to link abstract knowledge with visual interpretation.

Conclusion: The benchmark enables fine-grained assessment of diagnostic reasoning, highlighting gaps in current LVLM capabilities.

Abstract: Recent progress in Large Vision-Language Models (LVLMs) has enabled promising
applications in medical tasks, such as report generation and visual question
answering. However, existing benchmarks focus mainly on the final diagnostic
answer, offering limited insight into whether models engage in clinically
meaningful reasoning. To address this, we present CheXStruct and CXReasonBench,
a structured pipeline and benchmark built on the publicly available
MIMIC-CXR-JPG dataset. CheXStruct automatically derives a sequence of
intermediate reasoning steps directly from chest X-rays, such as segmenting
anatomical regions, deriving anatomical landmarks and diagnostic measurements,
computing diagnostic indices, and applying clinical thresholds. CXReasonBench
leverages this pipeline to evaluate whether models can perform clinically valid
reasoning steps and to what extent they can learn from structured guidance,
enabling fine-grained and transparent assessment of diagnostic reasoning. The
benchmark comprises 18,988 QA pairs across 12 diagnostic tasks and 1,200 cases,
each paired with up to 4 visual inputs, and supports multi-path, multi-stage
evaluation including visual grounding via anatomical region selection and
diagnostic measurements. Even the strongest of 10 evaluated LVLMs struggle with
structured reasoning and generalization, often failing to link abstract
knowledge with anatomically grounded visual interpretation. The code is
available at https://github.com/ttumyche/CXReasonBench

</details>


### [395] [Adapting SAM 2 for Visual Object Tracking: 1st Place Solution for MMVPR Challenge Multi-Modal Tracking](https://arxiv.org/pdf/2505.18111)
*Cheng-Yen Yang, Hsiang-Wei Huang, Pyong-Kun Kim, Chien-Kai Kuo, Jui-Wei Chang, Kwang-Ju Kim, Chung-I Huang, Jenq-Neng Hwang*

Main category: cs.CV

TL;DR: Adapting SAM2 for VOT with optimizations achieves top performance (89.4 AUC) in the 2024 ICPR challenge.


<details>
  <summary>Details</summary>
Motivation: To enhance SAM2's capabilities for Visual Object Tracking (VOT) by leveraging its pre-trained strengths and adding optimizations.

Method: Combines SAM2 with key techniques and optimizations tailored for VOT.

Result: Achieved first place with an AUC score of 89.4 in the 2024 ICPR Multi-modal Object Tracking challenge.

Conclusion: The approach effectively adapts SAM2 for VOT, demonstrating superior performance in multi-modal tracking.

Abstract: We present an effective approach for adapting the Segment Anything Model 2
(SAM2) to the Visual Object Tracking (VOT) task. Our method leverages the
powerful pre-trained capabilities of SAM2 and incorporates several key
techniques to enhance its performance in VOT applications. By combining SAM2
with our proposed optimizations, we achieved a first place AUC score of 89.4 on
the 2024 ICPR Multi-modal Object Tracking challenge, demonstrating the
effectiveness of our approach. This paper details our methodology, the specific
enhancements made to SAM2, and a comprehensive analysis of our results in the
context of VOT solutions along with the multi-modality aspect of the dataset.

</details>


### [396] [Instructify: Demystifying Metadata to Visual Instruction Tuning Data Conversion](https://arxiv.org/pdf/2505.18115)
*Jacob Hansen, Wei Lin, Junmo Kang, Muhammad Jehanzeb Mirza, Hongyin Luo, Rogerio Feris, Alan Ritter, James Glass, Leonid Karlinsky*

Main category: cs.CV

TL;DR: The paper proposes an open, unified method called \textbf{\method} for converting image metadata into VisIT instructions using open LLMs, improving quality and scalability while reducing reliance on closed-source APIs.


<details>
  <summary>Details</summary>
Motivation: Existing VisIT datasets are often poorly documented, rely on closed-source APIs, and lack reproducibility, making them costly and hard to scale.

Method: The multi-stage \method includes metadata grouping, quality control, prompt organization, and conversation sampling, leveraging open LLMs like Gemma 2 27B and LLaMa 3.1 70B.

Result: The approach improves GPT-4 generated VisIT instructions by ~3% on average (up to 12% on benchmarks) and enhances LMM performance across benchmarks.

Conclusion: The method offers a scalable, high-quality alternative to closed-source solutions, with open-sourced code for reproducibility and niche domain applications.

Abstract: Visual Instruction Tuning (VisIT) data, commonly available as human-assistant
conversations with images interleaved in the human turns, are currently the
most widespread vehicle for aligning strong LLMs to understand visual inputs,
converting them to strong LMMs. While many VisIT datasets are available, most
are constructed using ad-hoc techniques developed independently by different
groups. They are often poorly documented, lack reproducible code, and rely on
paid, closed-source model APIs such as GPT-4, Gemini, or Claude to convert
image metadata (labels) into VisIT instructions. This leads to high costs and
makes it challenging to scale, enhance quality, or generate VisIT data for new
datasets. In this work, we address these challenges and propose an open and
unified recipe and approach,~\textbf{\method}, for converting available
metadata to VisIT instructions using open LLMs. Our multi-stage \method
features an efficient framework for metadata grouping, quality control, data
and prompt organization, and conversation sampling. We show that our approach
can reproduce or enhance the data quality of available VisIT datasets when
applied to the same image data and metadata sources, improving GPT-4 generated
VisIT instructions by ~3\% on average and up to 12\% on individual benchmarks
using open models, such as Gemma 2 27B and LLaMa 3.1 70B. Additionally, our
approach enables effective performance scaling - both in quantity and quality -
by enhancing the resulting LMM performance across a wide range of benchmarks.
We also analyze the impact of various factors, including conversation format,
base model selection, and resampling strategies. Our code, which supports the
reproduction of equal or higher-quality VisIT datasets and facilities future
metadata-to-VisIT data conversion for niche domains, is released at
https://github.com/jacob-hansen/Instructify.

</details>


### [397] [One RL to See Them All: Visual Triple Unified Reinforcement Learning](https://arxiv.org/pdf/2505.18129)
*Yan Ma, Linge Du, Xuyang Shen, Shaoxiang Chen, Pengfei Li, Qibing Ren, Lizhuang Ma, Yuchao Dai, Pengfei Liu, Junjie Yan*

Main category: cs.CV

TL;DR: V-Triune is a unified RL system for VLMs, combining reasoning and perception tasks. It introduces dynamic rewards and achieves significant performance gains.


<details>
  <summary>Details</summary>
Motivation: To explore RL beyond reasoning tasks in VLMs, addressing perception-intensive tasks like object detection and grounding.

Method: V-Triune integrates sample-level formatting, verifier-level rewards, and source-level monitoring, with a novel Dynamic IoU reward.

Result: Orsta models show consistent improvements (+2.1 to +14.1) on MEGA-Bench Core and downstream tasks.

Conclusion: The unified RL approach is effective and scalable for VLMs, with public availability of V-Triune and Orsta models.

Abstract: Reinforcement learning (RL) has significantly advanced the reasoning
capabilities of vision-language models (VLMs). However, the use of RL beyond
reasoning tasks remains largely unexplored, especially for perceptionintensive
tasks like object detection and grounding. We propose V-Triune, a Visual Triple
Unified Reinforcement Learning system that enables VLMs to jointly learn visual
reasoning and perception tasks within a single training pipeline. V-Triune
comprises triple complementary components: Sample-Level Data Formatting (to
unify diverse task inputs), Verifier-Level Reward Computation (to deliver
custom rewards via specialized verifiers) , and Source-Level Metric Monitoring
(to diagnose problems at the data-source level). We further introduce a novel
Dynamic IoU reward, which provides adaptive, progressive, and definite feedback
for perception tasks handled by V-Triune. Our approach is instantiated within
off-the-shelf RL training framework using open-source 7B and 32B backbone
models. The resulting model, dubbed Orsta (One RL to See Them All),
demonstrates consistent improvements across both reasoning and perception
tasks. This broad capability is significantly shaped by its training on a
diverse dataset, constructed around four representative visual reasoning tasks
(Math, Puzzle, Chart, and Science) and four visual perception tasks (Grounding,
Detection, Counting, and OCR). Subsequently, Orsta achieves substantial gains
on MEGA-Bench Core, with improvements ranging from +2.1 to an impressive +14.1
across its various 7B and 32B model variants, with performance benefits
extending to a wide range of downstream tasks. These results highlight the
effectiveness and scalability of our unified RL approach for VLMs. The V-Triune
system, along with the Orsta models, is publicly available at
https://github.com/MiniMax-AI.

</details>


### [398] [BiggerGait: Unlocking Gait Recognition with Layer-wise Representations from Large Vision Models](https://arxiv.org/pdf/2505.18132)
*Dingqing Ye, Chao Fan, Zhanbo Huang, Chengwen Luo, Jianqiang Li, Shiqi Yu, Xiaoming Liu*

Main category: cs.CV

TL;DR: BiggerGait, a simple LVM-based gait recognition method, leverages intermediate layer representations for improved performance without relying heavily on gait priors.


<details>
  <summary>Details</summary>
Motivation: Existing LVM-based gait recognition methods overemphasize gait priors and overlook the rich, distinct representations in LVM's multi-layers.

Method: Investigates layer-wise representations in LVM and integrates them for gait recognition, proposing BiggerGait as a universal baseline.

Result: BiggerGait outperforms on CCPG, CAISA-B*, SUSTech1K, and CCGR_MINI datasets in within- and cross-domain tasks.

Conclusion: BiggerGait is a practical baseline for gait representation learning, with models and code made publicly available.

Abstract: Large vision models (LVM) based gait recognition has achieved impressive
performance. However, existing LVM-based approaches may overemphasize gait
priors while neglecting the intrinsic value of LVM itself, particularly the
rich, distinct representations across its multi-layers. To adequately unlock
LVM's potential, this work investigates the impact of layer-wise
representations on downstream recognition tasks. Our analysis reveals that
LVM's intermediate layers offer complementary properties across tasks,
integrating them yields an impressive improvement even without rich
well-designed gait priors. Building on this insight, we propose a simple and
universal baseline for LVM-based gait recognition, termed BiggerGait.
Comprehensive evaluations on CCPG, CAISA-B*, SUSTech1K, and CCGR\_MINI validate
the superiority of BiggerGait across both within- and cross-domain tasks,
establishing it as a simple yet practical baseline for gait representation
learning. All the models and code will be publicly available.

</details>


### [399] [Boosting Open Set Recognition Performance through Modulated Representation Learning](https://arxiv.org/pdf/2505.18137)
*Amit Kumar Kundu, Vaishnavi Patil, Joseph Jaja*

Main category: cs.CV

TL;DR: The paper introduces a temperature-modulated representation learning method for open set recognition (OSR) using a negative cosine scheduling scheme, improving performance without computational overhead.


<details>
  <summary>Details</summary>
Motivation: Existing OSR methods use a constant temperature scaling factor, limiting representation learning. The proposed method dynamically adjusts the temperature to enhance feature learning.

Method: A novel negative cosine scheduling scheme modulates temperature during training, starting with coarse decision boundaries and gradually smoothing them by prioritizing more neighbors.

Result: The method improves both OSR and closed set performance, especially on challenging semantic shift benchmarks, without added computational cost.

Conclusion: The proposed temperature-modulated learning scheme enhances representation space generality and performance in OSR tasks.

Abstract: The open set recognition (OSR) problem aims to identify test samples from
novel semantic classes that are not part of the training classes, a task that
is crucial in many practical scenarios. However, existing OSR methods use a
constant scaling factor (the temperature) to the logits before applying a loss
function, which hinders the model from exploring both ends of the spectrum in
representation learning -- from instance-level to semantic-level features. In
this paper, we address this problem by enabling temperature-modulated
representation learning using our novel negative cosine scheduling scheme. Our
scheduling lets the model form a coarse decision boundary at the beginning of
training by focusing on fewer neighbors, and gradually prioritizes more
neighbors to smooth out rough edges. This gradual task switching leads to a
richer and more generalizable representation space. While other OSR methods
benefit by including regularization or auxiliary negative samples, such as with
mix-up, thereby adding a significant computational overhead, our scheme can be
folded into any existing OSR method with no overhead. We implement the proposed
scheme on top of a number of baselines, using both cross-entropy and
contrastive loss functions as well as a few other OSR methods, and find that
our scheme boosts both the OSR performance and the closed set performance in
most cases, especially on the tougher semantic shift benchmarks.

</details>


### [400] [TokBench: Evaluating Your Visual Tokenizer before Visual Generation](https://arxiv.org/pdf/2505.18142)
*Junfeng Wu, Dongliang Luo, Weizhi Zhao, Zhihao Xie, Yuanhao Wang, Junyi Li, Xudong Xie, Yuliang Liu, Xiang Bai*

Main category: cs.CV

TL;DR: The paper highlights limitations of visual tokenizers and VAEs in preserving fine-grained features, proposes a benchmark for evaluating text and face reconstruction, and introduces lightweight metrics for accurate assessment.


<details>
  <summary>Details</summary>
Motivation: To address the loss of fine-grained visual information in image tokenization and compression, particularly for human-sensitive elements like text and faces, and to evaluate the impact on reconstruction quality.

Method: The authors curate a dataset of text and face images, use OCR for text reconstruction accuracy, and measure feature similarity for faces. They evaluate various tokenizers and VAEs, extending the framework to video tokenizers.

Result: Modern visual tokenizers struggle with fine-grained features, especially at smaller scales. Traditional metrics are inadequate, while the proposed metrics effectively complement evaluation.

Conclusion: The benchmark and metrics provide a lightweight, effective way to assess reconstruction quality, revealing shortcomings in current tokenizers and VAEs.

Abstract: In this work, we reveal the limitations of visual tokenizers and VAEs in
preserving fine-grained features, and propose a benchmark to evaluate
reconstruction performance for two challenging visual contents: text and face.
Image tokenization has significantly advanced visual generation and multimodal
modeling, particularly with autoregressive models due to the modeling
simplicity of discrete tokens. Autoregressive models typically rely on image
tokenizers to compress images into discrete tokens for sequential prediction,
whereas diffusion models often operate on continuous latent space to reduce
computational costs. However, both visual compression approaches inevitably
lose visual information, thereby limiting the upper bound of visual generation
quality. To evaluate how these compression losses affect text and faces, the
most human-sensitive visual elements, we first collect and curate a collection
of text and faces images from existing datasets, ensuring clarity and
diversity. For text reconstruction, we employ OCR models to assess the
recognition accuracy of the reconstructed text, and then we measure feature
similarity between original and reconstructed faces thereby quantifying faces
reconstruction fidelity. Our method is highly lightweight, requiring just 2GB
memory and 4 minutes to complete evaluations. With our benchmark, we analyze
the reconstruction quality of text and faces at various scales across different
image tokenizers and VAEs. Our results demonstrate that modern visual
tokenizers still struggle to preserve fine-grained features, particularly at
smaller scales. Furthermore, we extend this evaluation framework to the video,
conducting a comprehensive analysis of video tokenizers. Additionally, we find
that traditional metrics fail to accurately reflect the reconstruction
performance for faces and text, while our proposed metrics serve as an
effective complement.

</details>


### [401] [REN: Fast and Efficient Region Encodings from Patch-Based Image Encoders](https://arxiv.org/pdf/2505.18153)
*Savya Khosla, Sethuraman TV, Barnett Lee, Alexander Schwing, Derek Hoiem*

Main category: cs.CV

TL;DR: REN is a fast, lightweight model for generating region-based image representations using point prompts, outperforming existing methods in speed, memory efficiency, and performance.


<details>
  <summary>Details</summary>
Motivation: Existing methods combining segmenters and patch-based encoders are computationally expensive. REN aims to bypass this bottleneck.

Method: REN uses cross-attention blocks with point prompts as queries and patch-based encoder features to directly generate region tokens.

Result: REN achieves 60x faster token generation, 35x less memory usage, and improved performance on tasks like semantic segmentation and retrieval.

Conclusion: REN sets new benchmarks in efficiency and performance, surpassing proprietary models and achieving state-of-the-art results on challenging tasks.

Abstract: We introduce the Region Encoder Network (REN), a fast and effective model for
generating region-based image representations using point prompts. Recent
methods combine class-agnostic segmenters (e.g., SAM) with patch-based image
encoders (e.g., DINO) to produce compact and effective region representations,
but they suffer from high computational cost due to the segmentation step. REN
bypasses this bottleneck using a lightweight module that directly generates
region tokens, enabling 60x faster token generation with 35x less memory, while
also improving token quality. It uses a few cross-attention blocks that take
point prompts as queries and features from a patch-based image encoder as keys
and values to produce region tokens that correspond to the prompted objects. We
train REN with three popular encoders-DINO, DINOv2, and OpenCLIP-and show that
it can be extended to other encoders without dedicated training. We evaluate
REN on semantic segmentation and retrieval tasks, where it consistently
outperforms the original encoders in both performance and compactness, and
matches or exceeds SAM-based region methods while being significantly faster.
Notably, REN achieves state-of-the-art results on the challenging Ego4D VQ2D
benchmark and outperforms proprietary LMMs on Visual Haystacks' single-needle
challenge. Code and models are available at: https://github.com/savya08/REN.

</details>


### [402] [Ctrl-Room: Controllable Text-to-3D Room Meshes Generation with Layout Constraints](https://arxiv.org/pdf/2310.03602)
*Chuan Fang, Yuan Dong, Kunming Luo, Xiaotao Hu, Rakesh Shrestha, Ping Tan*

Main category: cs.CV

TL;DR: Ctrl-Room generates 3D rooms from text prompts, enabling flexible editing of layouts and objects without costly training.


<details>
  <summary>Details</summary>
Motivation: Existing methods fail to capture room layouts accurately or allow editing of individual objects, limiting practical applications.

Method: Two-stage approach: Layout Generation (text-conditional diffusion model) and Appearance Generation (ControlNet for panoramic images).

Result: Produces high-quality 3D rooms with editable layouts and textures, outperforming existing methods.

Conclusion: Ctrl-Room offers a scalable, editable solution for text-driven 3D scene generation.

Abstract: Text-driven 3D indoor scene generation is useful for gaming, the film
industry, and AR/VR applications. However, existing methods cannot faithfully
capture the room layout, nor do they allow flexible editing of individual
objects in the room. To address these problems, we present Ctrl-Room, which can
generate convincing 3D rooms with designer-style layouts and high-fidelity
textures from just a text prompt. Moreover, Ctrl-Room enables versatile
interactive editing operations such as resizing or moving individual furniture
items. Our key insight is to separate the modeling of layouts and appearance.
Our proposed method consists of two stages: a Layout Generation Stage and an
Appearance Generation Stage. The Layout Generation Stage trains a
text-conditional diffusion model to learn the layout distribution with our
holistic scene code parameterization. Next, the Appearance Generation Stage
employs a fine-tuned ControlNet to produce a vivid panoramic image of the room
guided by the 3D scene layout and text prompt. We thus achieve a high-quality
3D room generation with convincing layouts and lively textures. Benefiting from
the scene code parameterization, we can easily edit the generated room model
through our mask-guided editing module, without expensive edit-specific
training. Extensive experiments on the Structured3D dataset demonstrate that
our method outperforms existing methods in producing more reasonable,
view-consistent, and editable 3D rooms from natural language prompts.

</details>


### [403] [DiverseNet: Decision Diversified Semi-supervised Semantic Segmentation Networks for Remote Sensing Imagery](https://arxiv.org/pdf/2311.13716)
*Wanli Ma, Oktay Karakus, Paul L. Rosin*

Main category: cs.CV

TL;DR: The paper introduces two lightweight SSL methods, DiverseHead and DiverseModel, to improve pseudo-label diversity and efficiency in remote sensing imagery segmentation.


<details>
  <summary>Details</summary>
Motivation: Manual pixel-level labeling in remote sensing is costly, and existing SSL frameworks are often too bulky or lack pseudo-label diversity.

Method: Proposes DiverseHead (multiple decision heads) and DiverseModel (parallel networks) to enhance pseudo-label diversity and efficiency.

Result: Achieves competitive performance in semantic segmentation on four datasets and improves existing SSL methods.

Conclusion: The lightweight DiverseHead and DiverseModel effectively address SSL limitations in remote sensing, offering practical and efficient solutions.

Abstract: Semi-supervised learning (SSL) aims to help reduce the cost of the manual
labelling process by leveraging a substantial pool of unlabelled data alongside
a limited set of labelled data during the training phase. Since pixel-level
manual labelling in large-scale remote sensing imagery is expensive and
time-consuming, semi-supervised learning has become a widely used solution to
deal with this. However, the majority of existing SSL frameworks, especially
various teacher-student frameworks, are too bulky to run efficiently on a GPU
with limited memory. There is still a lack of lightweight SSL frameworks and
efficient perturbation methods to promote the diversity of training samples and
enhance the precision of pseudo labels during training. In order to fill this
gap, we proposed a simple, lightweight, and efficient SSL architecture named
\textit{DiverseHead}, which promotes the utilisation of multiple decision heads
instead of multiple whole networks. Another limitation of most existing SSL
frameworks is the insufficient diversity of pseudo labels, as they rely on the
same network architecture and fail to explore different structures for
generating pseudo labels. To solve this issue, we propose \textit{DiverseModel}
to explore and analyse different networks in parallel for SSL to increase the
diversity of pseudo labels. The two proposed methods, namely
\textit{DiverseHead} and \textit{DiverseModel}, both achieve competitive
semantic segmentation performance in four widely used remote sensing imagery
datasets compared to state-of-the-art semi-supervised learning methods.
Meanwhile, the proposed lightweight DiverseHead architecture can be easily
applied to various state-of-the-art SSL methods while further improving their
performance. The code is available at
https://github.com/WANLIMA-CARDIFF/DiverseNet.

</details>


### [404] [Preconditioners for the Stochastic Training of Neural Fields](https://arxiv.org/pdf/2402.08784)
*Shin-Fang Chng, Hemanth Saratchandran, Simon Lucey*

Main category: cs.CV

TL;DR: The paper explores faster optimization techniques for neural fields, proposing curvature-aware diagonal preconditioners as an alternative to Adam and L-BFGS.


<details>
  <summary>Details</summary>
Motivation: Adam's slow training times and L-BFGS's unsuitability for stochastic settings motivate the search for better optimization methods for neural fields.

Method: The authors propose a theoretical framework using curvature-aware diagonal preconditioners to accelerate training.

Result: The method proves effective in tasks like image reconstruction, shape modeling, and Neural Radiance Fields (NeRF).

Conclusion: Curvature-aware diagonal preconditioners offer a viable solution for faster and accurate training of neural fields.

Abstract: Neural fields encode continuous multidimensional signals as neural networks,
enabling diverse applications in computer vision, robotics, and geometry. While
Adam is effective for stochastic optimization, it often requires long training
times. To address this, we explore alternative optimization techniques to
accelerate training without sacrificing accuracy. Traditional second-order
methods like L-BFGS are unsuitable for stochastic settings. We propose a
theoretical framework for training neural fields with curvature-aware diagonal
preconditioners, demonstrating their effectiveness across tasks such as image
reconstruction, shape modeling, and Neural Radiance Fields (NeRF).

</details>


### [405] [SceneTracker: Long-term Scene Flow Estimation Network](https://arxiv.org/pdf/2403.19924)
*Bo Wang, Jian Li, Yang Yu, Li Liu, Zhenping Sun, Dewen Hu*

Main category: cs.CV

TL;DR: Proposes SceneTracker for long-term scene flow estimation (LSFE), addressing temporal coherence and spatial focus, outperforming in 3D occlusion and noise handling.


<details>
  <summary>Details</summary>
Motivation: Scene flow lacks temporal coherence; LSFE aims to capture fine-grained, long-term 3D motion online.

Method: SceneTracker uses iterative 3D trajectory approximation, dynamic feature indexing, and transformers for long-range connections.

Result: Superior performance in 3D occlusion and depth noise; validated on LSFDriving dataset.

Conclusion: SceneTracker excels in LSFE, with strong generalization, demonstrated on real-world data.

Abstract: Considering that scene flow estimation has the capability of the spatial
domain to focus but lacks the coherence of the temporal domain, this study
proposes long-term scene flow estimation (LSFE), a comprehensive task that can
simultaneously capture the fine-grained and long-term 3D motion in an online
manner. We introduce SceneTracker, the first LSFE network that adopts an
iterative approach to approximate the optimal 3D trajectory. The network
dynamically and simultaneously indexes and constructs appearance correlation
and depth residual features. Transformers are then employed to explore and
utilize long-range connections within and between trajectories. With detailed
experiments, SceneTracker shows superior capabilities in addressing 3D spatial
occlusion and depth noise interference, highly tailored to the needs of the
LSFE task. We build a real-world evaluation dataset, LSFDriving, for the LSFE
field and use it in experiments to further demonstrate the advantage of
SceneTracker in generalization abilities. The code and data are available at
https://github.com/wwsource/SceneTracker.

</details>


### [406] [Self-supervised Multi-future Occupancy Forecasting for Autonomous Driving](https://arxiv.org/pdf/2407.21126)
*Bernard Lange, Masha Itkina, Jiachen Li, Mykel J. Kochenderfer*

Main category: cs.CV

TL;DR: LOPR framework improves LiDAR occupancy grid map predictions by using latent space and multi-sensor integration, outperforming prior methods.


<details>
  <summary>Details</summary>
Motivation: Existing deterministic L-OGM prediction methods fail to capture environmental stochasticity and lack multi-sensor integration, limiting AV navigation safety.

Method: LOPR uses a generative architecture for stochastic L-OGM prediction in latent space, integrating RGB cameras, maps, and trajectories. It employs single-step or diffusion-based decoders.

Result: LOPR outperforms prior methods on nuScenes and Waymo Open datasets, achieving better qualitative and quantitative results.

Conclusion: LOPR enhances AV navigation by addressing stochasticity and multi-sensor fusion, offering superior prediction quality and flexibility.

Abstract: Environment prediction frameworks are critical for the safe navigation of
autonomous vehicles (AVs) in dynamic settings. LiDAR-generated occupancy grid
maps (L-OGMs) offer a robust bird's-eye view for the scene representation,
enabling self-supervised joint scene predictions while exhibiting resilience to
partial observability and perception detection failures. Prior approaches have
focused on deterministic L-OGM prediction architectures within the grid cell
space. While these methods have seen some success, they frequently produce
unrealistic predictions and fail to capture the stochastic nature of the
environment. Additionally, they do not effectively integrate additional sensor
modalities present in AVs. Our proposed framework, Latent Occupancy Prediction
(LOPR), performs stochastic L-OGM prediction in the latent space of a
generative architecture and allows for conditioning on RGB cameras, maps, and
planned trajectories. We decode predictions using either a single-step decoder,
which provides high-quality predictions in real-time, or a diffusion-based
batch decoder, which can further refine the decoded frames to address temporal
consistency issues and reduce compression losses. Our experiments on the
nuScenes and Waymo Open datasets show that all variants of our approach
qualitatively and quantitatively outperform prior approaches.

</details>


### [407] [Autoregressive Sequence Modeling for 3D Medical Image Representation](https://arxiv.org/pdf/2409.08691)
*Siwen Wang, Churan Wang, Fei Gao, Lixian Su, Fandong Zhang, Yizhou Wang, Yizhou Yu*

Main category: cs.CV

TL;DR: A novel autoregressive pre-training framework for 3D medical image representation learning, focusing on spatial, contrast, and semantic correlations, outperforms existing methods on nine downstream tasks.


<details>
  <summary>Details</summary>
Motivation: The challenge of interpreting complex 3D medical images (CT, MRI) due to variability across organs, tasks, and modalities, and the limitations of current self-supervised methods that overlook local region relationships.

Method: Sequences 3D medical images as interconnected visual tokens using spatial, contrast, and semantic correlations, and employs autoregressive modeling to predict the next token, with a random startup strategy for robustness.

Result: Demonstrates superior performance over existing methods on nine downstream tasks in public datasets.

Conclusion: The proposed autoregressive framework effectively integrates contextual information in 3D medical images, offering improved representation learning for clinical applications.

Abstract: Three-dimensional (3D) medical images, such as Computed Tomography (CT) and
Magnetic Resonance Imaging (MRI), are essential for clinical applications.
However, the need for diverse and comprehensive representations is particularly
pronounced when considering the variability across different organs, diagnostic
tasks, and imaging modalities. How to effectively interpret the intricate
contextual information and extract meaningful insights from these images
remains an open challenge to the community. While current self-supervised
learning methods have shown potential, they often consider an image as a whole
thereby overlooking the extensive, complex relationships among local regions
from one or multiple images. In this work, we introduce a pioneering method for
learning 3D medical image representations through an autoregressive
pre-training framework. Our approach sequences various 3D medical images based
on spatial, contrast, and semantic correlations, treating them as
interconnected visual tokens within a token sequence. By employing an
autoregressive sequence modeling task, we predict the next visual token in the
sequence, which allows our model to deeply understand and integrate the
contextual information inherent in 3D medical images. Additionally, we
implement a random startup strategy to avoid overestimating token relationships
and to enhance the robustness of learning. The effectiveness of our approach is
demonstrated by the superior performance over others on nine downstream tasks
in public datasets.

</details>


### [408] [MAP: Unleashing Hybrid Mamba-Transformer Vision Backbone's Potential with Masked Autoregressive Pretraining](https://arxiv.org/pdf/2410.00871)
*Yunze Liu, Li Yi*

Main category: cs.CV

TL;DR: Proposes Masked Autoregressive Pretraining (MAP) for hybrid Mamba-Transformer networks, outperforming existing methods.


<details>
  <summary>Details</summary>
Motivation: Addresses the lack of effective pretraining strategies for hybrid Mamba-Transformer networks, which combine strengths of both architectures.

Method: Introduces MAP, combining Masked Autoencoders (MAE) and autoregressive pretraining, to pretrain hybrid networks.

Result: MAP achieves state-of-the-art performance on 2D and 3D datasets, validated by ablation studies.

Conclusion: MAP is an effective pretraining strategy for hybrid Mamba-Transformer networks, with code and checkpoints publicly available.

Abstract: Hybrid Mamba-Transformer networks have recently garnered broad attention.
These networks can leverage the scalability of Transformers while capitalizing
on Mamba's strengths in long-context modeling and computational efficiency.
However, the challenge of effectively pretraining such hybrid networks remains
an open question. Existing methods, such as Masked Autoencoders (MAE) or
autoregressive (AR) pretraining, primarily focus on single-type network
architectures. In contrast, pretraining strategies for hybrid architectures
must be effective for both Mamba and Transformer components. Based on this, we
propose Masked Autoregressive Pretraining (MAP) to pretrain a hybrid
Mamba-Transformer vision backbone network. This strategy combines the strengths
of both MAE and Autoregressive pretraining, improving the performance of Mamba
and Transformer modules within a unified paradigm. Experimental results show
that the hybrid Mamba-Transformer vision backbone network pretrained with MAP
significantly outperforms other pretraining strategies, achieving
state-of-the-art performance. We validate the method's effectiveness on both 2D
and 3D datasets and provide detailed ablation studies to support the design
choices for each component. The code and checkpoints are available at
https://github.com/yunzeliu/MAP

</details>


### [409] [RCR: Robust Crowd Reconstruction with Upright Space from a Single Large-scene Image](https://arxiv.org/pdf/2411.06232)
*Jing Huang, Hao Wen, Tianyi Zhou, Haozhe Lin, Yu-kun Lai, Kun Li*

Main category: cs.CV

TL;DR: The paper proposes a method for spatially consistent 3D human pose and shape reconstruction from large-scene images, addressing challenges like varying human scales and depth ambiguity using HVIP and RCR techniques.


<details>
  <summary>Details</summary>
Motivation: Existing methods fail to achieve globally consistent 3D human reconstruction due to small and varying 2D human scales, depth ambiguity, and perspective distortion.

Method: Introduces Human-scene Virtual Interaction Point (HVIP) and Robust Crowd Reconstruction (RCR), along with Iterative Ground-aware Cropping and Upright Normalization for consistent reconstruction.

Result: The method achieves globally consistent reconstruction and stable generalization across different camera FoVs, validated on benchmark datasets LargeCrowd and SynCrowd.

Conclusion: The proposed approach effectively addresses reconstruction challenges in large scenes, with publicly available source code and datasets for further research.

Abstract: This paper focuses on spatially consistent hundreds of human pose and shape
reconstruction from a single large-scene image with various human scales under
arbitrary camera FoVs (Fields of View). Due to the small and highly varying 2D
human scales, depth ambiguity, and perspective distortion, no existing methods
can achieve globally consistent reconstruction with correct reprojection. To
address these challenges, we first propose a new concept, Human-scene Virtual
Interaction Point (HVIP), to convert the complex 3D human localization into
2D-pixel localization. We then extend it to RCR (Robust Crowd Reconstruction),
which achieves globally consistent reconstruction and stable generalization on
different camera FoVs without test-time optimization. To perceive humans in
varying pixel sizes, we propose an Iterative Ground-aware Cropping to
automatically crop the image and then merge the results. To eliminate the
influence of the camera and cropping process during the reconstruction, we
introduce a canonical Upright 3D Space and the corresponding Upright 2D Space.
To link the canonical space and the camera space, we propose the Upright
Normalization, which transforms the local crop input into the Upright 2D Space,
and transforms the output from the Upright 3D Space into the unified camera
space. Besides, we contribute two benchmark datasets, LargeCrowd and SynCrowd,
for evaluating crowd reconstruction in large scenes. Experimental results
demonstrate the effectiveness of the proposed method. The source code and data
will be publicly available for research purposes.

</details>


### [410] [Forensics Adapter: Unleashing CLIP for Generalizable Face Forgery Detection](https://arxiv.org/pdf/2411.19715)
*Xinjie Cui, Yuezun Li, Delong Zhu, Jiaran Zhou, Junyu Dong, Siwei Lyu*

Main category: cs.CV

TL;DR: Forensics Adapter adapts CLIP for face forgery detection by introducing an adapter to learn forgery traces and enhancing CLIP tokens, achieving a 7% performance boost. Forensics Adapter++ adds textual modality for an extra 1.3% improvement.


<details>
  <summary>Details</summary>
Motivation: CLIP's versatility lacks task-specific adaptation for face forgery detection, limiting effectiveness. Existing methods treat CLIP as a feature extractor without addressing forgery-specific knowledge.

Method: Introduces an adapter to learn forgery traces (blending boundaries) and enhances CLIP visual tokens with a cross-knowledge interaction strategy. Forensics Adapter++ adds forgery-aware prompt learning for textual modality.

Result: Achieves ~7% performance boost on five datasets with 5.7M parameters. Forensics Adapter++ adds 1.3% improvement.

Conclusion: The methods provide a strong baseline for future CLIP-based face forgery detection, retaining CLIP's versatility while improving task-specific performance.

Abstract: We describe Forensics Adapter, an adapter network designed to transform CLIP
into an effective and generalizable face forgery detector. Although CLIP is
highly versatile, adapting it for face forgery detection is non-trivial as
forgery-related knowledge is entangled with a wide range of unrelated
knowledge. Existing methods treat CLIP merely as a feature extractor, lacking
task-specific adaptation, which limits their effectiveness. To address this, we
introduce an adapter to learn face forgery traces -- the blending boundaries
unique to forged faces, guided by task-specific objectives. Then we enhance the
CLIP visual tokens with a dedicated interaction strategy that communicates
knowledge across CLIP and the adapter. Since the adapter is alongside CLIP, its
versatility is highly retained, naturally ensuring strong generalizability in
face forgery detection. With only 5.7M trainable parameters, our method
achieves a significant performance boost, improving by approximately 7% on
average across five standard datasets. Additionally, we describe Forensics
Adapter++, an extended method that incorporates textual modality via a newly
proposed forgery-aware prompt learning strategy. This extension leads to a
further 1.3% performance boost over the original Forensics Adapter. We believe
the proposed methods can serve as a baseline for future CLIP-based face forgery
detection methods. The codes have been released at
https://github.com/OUC-VAS/ForensicsAdapter.

</details>


### [411] [Defending Multimodal Backdoored Models by Repulsive Visual Prompt Tuning](https://arxiv.org/pdf/2412.20392)
*Zhifang Zhang, Shuo He, Haobo Wang, Bingquan Shen, Lei Feng*

Main category: cs.CV

TL;DR: RVPT enhances CLIP's resistance to backdoor attacks by repelling non-predictive features, using few-shot clean samples and minimal parameter tuning.


<details>
  <summary>Details</summary>
Motivation: CLIP's vulnerability to backdoor attacks due to encoding non-predictive features necessitates a defense method.

Method: Proposes Repulsive Visual Prompt Tuning (RVPT) with feature-repelling loss and minimal parameter updates.

Result: RVPT reduces attack success rate from 89.70% to 2.76% and generalizes across datasets.

Conclusion: RVPT is an efficient, effective defense against multimodal backdoor attacks with minimal resource use.

Abstract: Multimodal contrastive learning models (e.g., CLIP) can learn high-quality
representations from large-scale image-text datasets, while they exhibit
significant vulnerabilities to backdoor attacks, raising serious safety
concerns. In this paper, we reveal that CLIP's vulnerabilities primarily stem
from its tendency to encode features beyond in-dataset predictive patterns,
compromising its visual feature resistivity to input perturbations. This makes
its encoded features highly susceptible to being reshaped by backdoor triggers.
To address this challenge, we propose Repulsive Visual Prompt Tuning (RVPT), a
novel defense approach that employs deep visual prompt tuning with a specially
designed feature-repelling loss. Specifically, RVPT adversarially repels the
encoded features from deeper layers while optimizing the standard cross-entropy
loss, ensuring that only predictive features in downstream tasks are encoded,
thereby enhancing CLIP's visual feature resistivity against input perturbations
and mitigating its susceptibility to backdoor attacks. Unlike existing
multimodal backdoor defense methods that typically require the availability of
poisoned data or involve fine-tuning the entire model, RVPT leverages few-shot
downstream clean samples and only tunes a small number of parameters. Empirical
results demonstrate that RVPT tunes only 0.27\% of the parameters in CLIP, yet
it significantly outperforms state-of-the-art defense methods, reducing the
attack success rate from 89.70\% to 2.76\% against the most advanced multimodal
attacks on ImageNet and effectively generalizes its defensive capabilities
across multiple datasets.

</details>


### [412] [Boosting Edge Detection with Pixel-wise Feature Selection: The Extractor-Selector Paradigm](https://arxiv.org/pdf/2501.02534)
*Hao Shu*

Main category: cs.CV

TL;DR: The paper proposes the Extractor-Selector (E-S) paradigm for adaptive pixel-wise feature fusion in edge detection, outperforming uniform fusion methods.


<details>
  <summary>Details</summary>
Motivation: Existing edge detection models use uniform feature fusion, ignoring regional differences like edges and textures, limiting precision.

Method: The E-S framework dynamically selects relevant features per pixel for refined fusion, compatible with existing models.

Result: The method achieves significant improvements (e.g., 7% in ODS/OIS, 22% in AP on BIPED2) over baselines.

Conclusion: The E-S paradigm enhances edge detection by enabling adaptive feature fusion, offering superior performance without architectural changes.

Abstract: Deep learning has significantly advanced image edge detection (ED), primarily
through improved feature extraction. However, most existing ED models apply
uniform feature fusion across all pixels, ignoring critical differences between
regions such as edges and textures. To address this limitation, we propose the
Extractor-Selector (E-S) paradigm, a novel framework that introduces pixel-wise
feature selection for more adaptive and precise fusion. Unlike conventional
image-level fusion that applies the same convolutional kernel to all pixels,
our approach dynamically selects relevant features at each pixel, enabling more
refined edge predictions. The E-S framework can be seamlessly integrated with
existing ED models without architectural changes, delivering substantial
performance gains. It can also be combined with enhanced feature extractors for
further accuracy improvements. Extensive experiments across multiple benchmarks
confirm that our method consistently outperforms baseline ED models. For
instance, on the BIPED2 dataset, the proposed framework can achieve over 7$\%$
improvements in ODS and OIS, and 22$\%$ improvements in AP, demonstrating its
effectiveness and superiority.

</details>


### [413] [Rethinking Bottlenecks in Safety Fine-Tuning of Vision Language Models](https://arxiv.org/pdf/2501.18533)
*Yi Ding, Lijun Li, Bing Cao, Jing Shao*

Main category: cs.CV

TL;DR: The paper introduces the Multi-Image Safety (MIS) dataset to enhance safety reasoning in VLMs, improving performance in safety-critical tasks without compromising general capabilities.


<details>
  <summary>Details</summary>
Motivation: Existing safety fine-tuning methods for VLMs lack visual reasoning ability, creating a gap in safety-critical applications.

Method: Proposes the MIS dataset with multi-image inputs and safety Chain-of-Thought labels for fine-tuning, tested on InternVL2.5-8B.

Result: MIS fine-tuning boosts accuracy by 0.83% on general benchmarks and significantly reduces Attack Success Rate (ASR) on safety benchmarks.

Conclusion: The MIS dataset effectively bridges the safety reasoning gap in VLMs, enhancing both safety and general performance.

Abstract: Large Vision-Language Models (VLMs) have achieved remarkable performance
across a wide range of tasks. However, their deployment in safety-critical
domains poses significant challenges. Existing safety fine-tuning methods,
which focus on textual or multimodal content, fall short in addressing
challenging cases or disrupt the balance between helpfulness and harmlessness.
Our evaluation highlights a safety reasoning gap: these methods lack safety
visual reasoning ability, leading to such bottlenecks. To address this
limitation and enhance both visual perception and reasoning in safety-critical
contexts, we propose a novel dataset that integrates multi-image inputs with
safety Chain-of-Thought (CoT) labels as fine-grained reasoning logic to improve
model performance. Specifically, we introduce the Multi-Image Safety (MIS)
dataset, an instruction-following dataset tailored for multi-image safety
scenarios, consisting of training and test splits. Our experiments demonstrate
that fine-tuning InternVL2.5-8B with MIS significantly outperforms both
powerful open-source models and API-based models in challenging multi-image
tasks requiring safety-related visual reasoning. This approach not only
delivers exceptional safety performance but also preserves general capabilities
without any trade-offs. Specifically, fine-tuning with MIS increases average
accuracy by 0.83% across five general benchmarks and reduces the Attack Success
Rate (ASR) on multiple safety benchmarks by a large margin.

</details>


### [414] [REG: Rectified Gradient Guidance for Conditional Diffusion Models](https://arxiv.org/pdf/2501.18865)
*Zhengqi Gao, Kaiwen Zha, Tianyuan Zhang, Zihui Xue, Duane S. Boning*

Main category: cs.CV

TL;DR: The paper introduces rectified gradient guidance (REG) to improve conditional generation in diffusion models by addressing theoretical discrepancies in existing guidance techniques.


<details>
  <summary>Details</summary>
Motivation: Existing guidance techniques diverge from their theoretical motivation, leading to a need for a valid and improved approach.

Method: The authors replace the invalid scaled marginal distribution target with a valid scaled joint distribution objective and propose REG as a versatile enhancement.

Result: REG outperforms prior guidance methods in 1D and 2D experiments and improves FID and Inception/CLIP scores in class-conditional ImageNet and text-to-image tasks.

Conclusion: REG provides a better approximation to the optimal solution, validating the proposed theoretical framework and enhancing performance in practical applications.

Abstract: Guidance techniques are simple yet effective for improving conditional
generation in diffusion models. Albeit their empirical success, the practical
implementation of guidance diverges significantly from its theoretical
motivation. In this paper, we reconcile this discrepancy by replacing the
scaled marginal distribution target, which we prove theoretically invalid, with
a valid scaled joint distribution objective. Additionally, we show that the
established guidance implementations are approximations to the intractable
optimal solution under no future foresight constraint. Building on these
theoretical insights, we propose rectified gradient guidance (REG), a versatile
enhancement designed to boost the performance of existing guidance methods.
Experiments on 1D and 2D demonstrate that REG provides a better approximation
to the optimal solution than prior guidance techniques, validating the proposed
theoretical framework. Extensive experiments on class-conditional ImageNet and
text-to-image generation tasks show that incorporating REG consistently
improves FID and Inception/CLIP scores across various settings compared to its
absence.

</details>


### [415] [Enhanced 3D Object Detection via Diverse Feature Representations of 4D Radar Tensor](https://arxiv.org/pdf/2502.06114)
*Seung-Hyun Song, Dong-Hee Paek, Minh-Quan Dao, Ezio Malis, Seung-Hyun Kong*

Main category: cs.CV

TL;DR: A novel 3D object detection framework using multi-teacher knowledge distillation on raw 4D Radar Tensor (4DRT) improves efficiency and performance, achieving significant gains over baselines with sparse inputs.


<details>
  <summary>Details</summary>
Motivation: Existing methods for 4DRT face high computational costs and limited scalability, prompting the need for an efficient solution.

Method: Multi-teacher knowledge distillation (KD) trains teacher models on diverse 4DRT pre-processed point clouds, fuses their representations, and distills them into a lightweight student model.

Result: The framework improves AP_3D by 7.3% and AP_BEV by 9.5% over the baseline RTNH model with sparse inputs, while reducing input data size by ~90x.

Conclusion: The proposed method efficiently leverages raw 4DRT, balancing performance and scalability for 3D object detection.

Abstract: Recent advances in automotive four-dimensional (4D) Radar have enabled access
to raw 4D Radar Tensor (4DRT), offering richer spatial and Doppler information
than conventional point clouds. While most existing methods rely on heavily
pre-processed, sparse Radar data, recent attempts to leverage raw 4DRT face
high computational costs and limited scalability. To address these limitations,
we propose a novel three-dimensional (3D) object detection framework that
maximizes the utility of 4DRT while preserving efficiency. Our method
introduces a multi-teacher knowledge distillation (KD), where multiple teacher
models are trained on point clouds derived from diverse 4DRT pre-processing
techniques, each capturing complementary signal characteristics. These teacher
representations are fused via a dedicated aggregation module and distilled into
a lightweight student model that operates solely on a sparse Radar input.
Experimental results on the K-Radar dataset demonstrate that our framework
achieves improvements of 7.3% in AP_3D and 9.5% in AP_BEV over the baseline
RTNH model when using extremely sparse inputs. Furthermore, it attains
comparable performance to denser-input baselines while significantly reducing
the input data size by about 90 times, confirming the scalability and
efficiency of our approach.

</details>


### [416] [SAGI: Semantically Aligned and Uncertainty Guided AI Image Inpainting](https://arxiv.org/pdf/2502.06593)
*Paschalis Giakoumoglou, Dimitrios Karageorgiou, Symeon Papadopoulos, Panagiotis C. Petrantonakis*

Main category: cs.CV

TL;DR: SAGI automates text-guided image inpainting by aligning prompts with human perception and evaluating realism using pretrained models, improving quality and reducing detectability.


<details>
  <summary>Details</summary>
Motivation: To automate the generative process of photorealistic image inpainting, which typically requires human effort for prompt crafting and refinement.

Method: Proposes SAGI, a model-agnostic pipeline using pretrained LLMs and VLMs to sample semantically aligned prompts and evaluate realism.

Result: Creates SAGI-D, the largest AI-generated inpainting dataset, showing improved image quality (human detection accuracy drops from 74% to 35%).

Conclusion: SAGI enhances inpainting realism and aids forensic detection, demonstrating utility in countering malicious AI use.

Abstract: Recent advancements in generative AI have made text-guided image inpainting
-- adding, removing, or altering image regions using textual prompts -- widely
accessible. However, generating semantically correct photorealistic imagery,
typically requires carefully-crafted prompts and iterative refinement by
evaluating the realism of the generated content - tasks commonly performed by
humans. To automate the generative process, we propose Semantically Aligned and
Uncertainty Guided AI Image Inpainting (SAGI), a model-agnostic pipeline, to
sample prompts from a distribution that closely aligns with human perception
and to evaluate the generated content and discard one that deviates from such a
distribution, which we approximate using pretrained Large Language Models and
Vision-Language Models. By applying this pipeline on multiple state-of-the-art
inpainting models, we create the SAGI Dataset (SAGI-D), currently the largest
and most diverse dataset of AI-generated inpaintings, comprising over 95k
inpainted images and a human-evaluated subset. Our experiments show that
semantic alignment significantly improves image quality and aesthetics, while
uncertainty guidance effectively identifies realistic manipulations - human
ability to distinguish inpainted images from real ones drops from 74% to 35% in
terms of accuracy, after applying our pipeline. Moreover, using SAGI-D for
training several image forensic approaches increases in-domain detection
performance on average by 37.4% and out-of-domain generalization by 26.1% in
terms of IoU, also demonstrating its utility in countering malicious
exploitation of generative AI. Code and dataset are available at
https://github.com/mever-team/SAGI

</details>


### [417] [MMXU: A Multi-Modal and Multi-X-ray Understanding Dataset for Disease Progression](https://arxiv.org/pdf/2502.11651)
*Linjie Mu, Zhongzhen Huang, Shengqian Qin, Yakun Zhu, Shaoting Zhang, Xiaofan Zhang*

Main category: cs.CV

TL;DR: The paper introduces MMXU, a dataset for medical visual question answering (MedVQA) that focuses on disease progression using multi-image questions, and proposes a MedRecord-Augmented Generation (MAG) method to improve diagnostic accuracy by integrating historical records.


<details>
  <summary>Details</summary>
Motivation: Existing LVLMs and datasets overlook historical records and disease progression analysis, limiting their effectiveness in medical diagnostics.

Method: The authors propose MAG, which integrates global and regional historical records into LVLMs, and evaluate it on the MMXU dataset.

Result: MAG improves diagnostic accuracy by at least 20%, closing the gap between LVLMs and human experts. Fine-tuned models on MMXU-dev show notable improvements.

Conclusion: The work highlights the importance of historical context in medical diagnostics and advances LVLMs' utility in this domain.

Abstract: Large vision-language models (LVLMs) have shown great promise in medical
applications, particularly in visual question answering (MedVQA) and diagnosis
from medical images. However, existing datasets and models often fail to
consider critical aspects of medical diagnostics, such as the integration of
historical records and the analysis of disease progression over time. In this
paper, we introduce MMXU (Multimodal and MultiX-ray Understanding), a novel
dataset for MedVQA that focuses on identifying changes in specific regions
between two patient visits. Unlike previous datasets that primarily address
single-image questions, MMXU enables multi-image questions, incorporating both
current and historical patient data. We demonstrate the limitations of current
LVLMs in identifying disease progression on MMXU-\textit{test}, even those that
perform well on traditional benchmarks. To address this, we propose a
MedRecord-Augmented Generation (MAG) approach, incorporating both global and
regional historical records. Our experiments show that integrating historical
records significantly enhances diagnostic accuracy by at least 20\%, bridging
the gap between current LVLMs and human expert performance. Additionally, we
fine-tune models with MAG on MMXU-\textit{dev}, which demonstrates notable
improvements. We hope this work could illuminate the avenue of advancing the
use of LVLMs in medical diagnostics by emphasizing the importance of historical
context in interpreting medical images. Our dataset is released at github:
https://github.com/linjiemu/MMXU.

</details>


### [418] [ViFOR: A Fourier-Enhanced Vision Transformer for Multi-Image Super-Resolution in Earth System](https://arxiv.org/pdf/2502.12427)
*Ehsan Zeraatkar, Salah A Faroughi, Jelena Tešić*

Main category: cs.CV

TL;DR: ViFOR, a new algorithm combining Vision Transformers and Fourier-based INRs, outperforms state-of-the-art SR methods in improving Earth System Model data resolution.


<details>
  <summary>Details</summary>
Motivation: Enhancing spatial resolution of ESM data to better understand environmental processes.

Method: Integrates Fourier-based activation functions within Vision Transformers to capture global context and high-frequency details.

Result: ViFOR achieves superior performance, improving PSNR by up to 4.18 dB over ViT for certain metrics.

Conclusion: ViFOR is an effective solution for high-resolution image generation from low-resolution inputs in environmental modeling.

Abstract: Super-resolution (SR) techniques are essential for improving Earth System
Model (ESM) data's spatial resolution, which helps better understand complex
environmental processes. This paper presents a new algorithm, ViFOR, which
combines Vision Transformers (ViT) and Fourier-based Implicit Neural
Representation Networks (INRs) to generate High-Resolution (HR) images from
Low-Resolution (LR) inputs. ViFOR introduces a novel integration of
Fourier-based activation functions within the Vision Transformer architecture,
enabling it to effectively capture global context and high-frequency details
critical for accurate SR reconstruction. The results show that ViFOR
outperforms state-of-the-art methods such as ViT, Sinusoidal Representation
Networks (SIREN), and SR Generative Adversarial Networks (SRGANs) based on
metrics like Peak Signal-to-Noise Ratio (PSNR) and Mean Squared Error (MSE)
both for global as well as the local imagery. ViFOR improves PSNR of up to 4.18
dB, 1.56 dB, and 1.73 dB over ViT for full images in the Source Temperature,
Shortwave, and Longwave Flux.

</details>


### [419] [Multi-Faceted Multimodal Monosemanticity](https://arxiv.org/pdf/2502.14888)
*Hanqi Yan, Xiangxiang Cui, Lu Yin, Paul Pu Liang, Yulan He, Yifei Wang*

Main category: cs.CV

TL;DR: The paper explores multimodal interpretability in CLIP, introducing tools like the Modality Dominance Score (MDS) to categorize features into vision, language, or cross-modal, and demonstrates downstream applications.


<details>
  <summary>Details</summary>
Motivation: Understanding commonalities and distinctions among modalities (vision, language) in multimodal models like CLIP.

Method: Develops multimodal interpretability tools, including MDS, to analyze and categorize CLIP features into vision, language, or cross-modal.

Result: Feature categorization aligns with human intuition and benefits tasks like bias reduction, adversarial example generation, and text-to-image control.

Conclusion: Large-scale multimodal models with interpretability tools provide insights into modality relationships and enhance practical applications.

Abstract: Humans experience the world through multiple modalities, such as, vision,
language, and speech, making it natural to explore the commonality and
distinctions among them. In this work, we take a data-driven approach to
address this question by analyzing interpretable, monosemantic features
extracted from deep multimodal models. Specifically, we investigate CLIP, a
prominent visual-language representation model trained on massive image-text
pairs. Building on prior research in single-modal interpretability, we develop
a set of multi-modal interpretability tools and measures designed to
disentangle and analyze features learned from CLIP. Specifically, we introduce
the Modality Dominance Score (MDS) to attribute each CLIP feature to a specific
modality. We then map CLIP features into a more interpretable space, enabling
us to categorize them into three distinct classes: vision features
(single-modal), language features (single-modal), and visual-language features
(cross-modal). Interestingly, this data-driven categorization closely aligns
with human intuitive understandings of different modalities. We further show
that this modality decomposition can benefit multiple downstream tasks,
including reducing bias in gender detection, generating cross-modal adversarial
examples, and enabling modal-specific feature control in text-to-image
generation. These results indicate that large-scale multimodal models, when
equipped with task-agnostic interpretability tools, can offer valuable insights
into the relationships between different data modalities.

</details>


### [420] [Simpler Fast Vision Transformers with a Jumbo CLS Token](https://arxiv.org/pdf/2502.15021)
*Anthony Fuller, Yousef Yassin, Daniel G. Kyrollos, Evan Shelhamer, James R. Green*

Main category: cs.CV

TL;DR: Jumbo enhances ViTs by widening the CLS token, splitting it for attention, and applying a dedicated FFN, improving accuracy with minimal cost.


<details>
  <summary>Details</summary>
Motivation: To improve ViT accuracy while maintaining throughput, especially for small models or high-speed applications.

Method: Widens the CLS token, splits it for attention, processes with self-attention, reassembles, and applies a dedicated FFN.

Result: Outperforms ViT+Registers by 13% on ImageNet-1K/21K, excels in efficiency and supports token dropping and other modalities.

Conclusion: Jumbo is a simple yet effective enhancement for ViTs, offering significant gains without compromising architectural advantages.

Abstract: We introduce a simple enhancement of vision transformers (ViTs) to improve
accuracy while maintaining throughput. Our approach, Jumbo, creates a wider CLS
token, which is split to match the patch token width before attention,
processed with self-attention, and reassembled. After attention, Jumbo applies
a dedicated, wider FFN to this token. Since there is only one Jumbo token, its
cost is minimal, and because we share this FFN across layers, its parameter
count is controlled. Jumbo significantly improves over ViT+Registers on
ImageNet-1K and ImageNet-21K. These gains are largest at small sizes / high
speeds, e.g., ViT-nano+Jumbo outperforms ViT-nano+Registers by 13%. In fact,
our Jumbo models are so efficient that they outperform specialized
compute-efficient models while preserving the architectural advantages of plain
ViTs, such as support for token dropping and other modalities. Accordingly, we
demonstrate that Jumbo excels in these two settings via masked autoencoding and
on a suite of time series benchmarks. Code and weights available:
https://github.com/antofuller/jumbo

</details>


### [421] [EMT: A Visual Multi-Task Benchmark Dataset for Autonomous Driving](https://arxiv.org/pdf/2502.19260)
*Nadya Abdel Madjid, Murad Mebrahtu, Abdulrahman Ahmad, Abdelmoamen Nasser, Bilal Hassan, Naoufel Werghi, Jorge Dias, Majid Khonji*

Main category: cs.CV

TL;DR: The EMT dataset is a multi-task benchmarking tool for tracking, trajectory forecasting, and intention prediction, featuring 30,000 frames and 570,000 annotations from Gulf region traffic.


<details>
  <summary>Details</summary>
Motivation: To provide a unified framework for evaluating multi-task performance in traffic scenarios, capturing unique Gulf region driving behaviors and conditions.

Method: The dataset includes dash-camera footage with annotated bounding boxes, supporting three tasks: tracking, trajectory forecasting, and intention prediction.

Result: Benchmarks include multi-agent tracking, trajectory forecasting, and intention prediction, with evaluations using deep sequential and interaction-aware models.

Conclusion: The EMT dataset is publicly available, offering resources for research in multi-task traffic analysis.

Abstract: This paper introduces the Emirates Multi-Task (EMT) dataset, designed to
support multi-task benchmarking within a unified framework. It comprises over
30,000 frames from a dash-camera perspective and 570,000 annotated bounding
boxes, covering approximately 150 kilometers of driving routes that reflect the
distinctive road topology, congestion patterns, and driving behavior of Gulf
region traffic. The dataset supports three primary tasks: tracking, trajectory
forecasting, and intention prediction. Each benchmark is accompanied by
corresponding evaluations: (1) multi-agent tracking experiments addressing
multi-class scenarios and occlusion handling; (2) trajectory forecasting
evaluation using deep sequential and interaction-aware models; and (3)
intention prediction experiments based on observed trajectories. The dataset is
publicly available at https://avlab.io/emt-dataset, with pre-processing scripts
and evaluation models at https://github.com/AV-Lab/emt-dataset.

</details>


### [422] [Open-Set Gait Recognition from Sparse mmWave Radar Point Clouds](https://arxiv.org/pdf/2503.07435)
*Riccardo Mazzieri, Jacopo Pegoraro, Michele Rossi*

Main category: cs.CV

TL;DR: The paper introduces a novel neural network for open-set gait recognition using sparse mmWave radar point clouds, achieving a 24% F1-Score improvement over state-of-the-art methods.


<details>
  <summary>Details</summary>
Motivation: Addressing the realistic open-set scenario in gait recognition, where unknown subjects may appear, using sparse point clouds for edge computing.

Method: Proposes a neural network combining supervised classification and unsupervised reconstruction, with a probabilistic novelty detection algorithm.

Result: Achieves a 24% F1-Score improvement over existing methods and releases the mmGait10 dataset.

Conclusion: The approach is effective for open-set gait recognition with sparse point clouds, offering a balance between speed and accuracy.

Abstract: The adoption of Millimeter-Wave (mmWave) radar devices for human sensing,
particularly gait recognition, has recently gathered significant attention due
to their efficiency, resilience to environmental conditions, and
privacy-preserving nature. In this work, we tackle the challenging problem of
Open-set Gait Recognition (OSGR) from sparse mmWave radar point clouds. Unlike
most existing research, which assumes a closed-set scenario, our work considers
the more realistic open-set case, where unknown subjects might be present at
inference time, and should be correctly recognized by the system. Point clouds
are well-suited for edge computing applications with resource constraints, but
are more significantly affected by noise and random fluctuations than other
representations, like the more common micro-Doppler signature. This is the
first work addressing open-set gait recognition with sparse point cloud data.
To do so, we propose a novel neural network architecture that combines
supervised classification with unsupervised reconstruction of the point clouds,
creating a robust, rich, and highly regularized latent space of gait features.
To detect unknown subjects at inference time, we introduce a probabilistic
novelty detection algorithm that leverages the structured latent space and
offers a tunable trade-off between inference speed and prediction accuracy.
Along with this paper, we release mmGait10, an original human gait dataset
featuring over five hours of measurements from ten subjects, under varied
walking modalities. Extensive experimental results show that our solution
attains F1-Score improvements by 24% over state-of-the-art methods, on average,
and across multiple openness levels.

</details>


### [423] [Beyond the Destination: A Novel Benchmark for Exploration-Aware Embodied Question Answering](https://arxiv.org/pdf/2503.11117)
*Kaixuan Jiang, Yang Liu, Weixing Chen, Jingzhou Luo, Ziliang Chen, Ling Pan, Guanbin Li, Liang Lin*

Main category: cs.CV

TL;DR: The paper introduces EXPRESS-Bench, a large dataset for Embodied Question Answering (EQA), and Fine-EQA, a hybrid exploration model, to address limitations in exploration efficiency and evaluation metrics.


<details>
  <summary>Details</summary>
Motivation: Current EQA approaches suffer from inefficiencies in exploration, biased datasets, and inadequate evaluation metrics, leading to disembodied reasoning.

Method: The authors construct EXPRESS-Bench with 777 trajectories and 2,044 question-trajectory pairs, and propose Fine-EQA, combining frontier-based and goal-oriented navigation.

Result: EXPRESS-Bench and Fine-EQA outperform state-of-the-art models, with the new EAC metric ensuring reliable evaluation.

Conclusion: The work advances EQA by improving exploration and reasoning, validated through extensive experiments.

Abstract: Embodied Question Answering (EQA) is a challenging task in embodied
intelligence that requires agents to dynamically explore 3D environments,
actively gather visual information, and perform multi-step reasoning to answer
questions. However, current EQA approaches suffer from critical limitations in
exploration efficiency, dataset design, and evaluation metrics. Moreover,
existing datasets often introduce biases or prior knowledge, leading to
disembodied reasoning, while frontier-based exploration strategies struggle in
cluttered environments and fail to ensure fine-grained exploration of
task-relevant areas. To address these challenges, we construct the
EXPloration-awaRe Embodied queStion anSwering Benchmark (EXPRESS-Bench), the
largest dataset designed specifically to evaluate both exploration and
reasoning capabilities. EXPRESS-Bench consists of 777 exploration trajectories
and 2,044 question-trajectory pairs. To improve exploration efficiency, we
propose Fine-EQA, a hybrid exploration model that integrates frontier-based and
goal-oriented navigation to guide agents toward task-relevant regions more
effectively. Additionally, we introduce a novel evaluation metric,
Exploration-Answer Consistency (EAC), which ensures faithful assessment by
measuring the alignment between answer grounding and exploration reliability.
Extensive experimental comparisons with state-of-the-art EQA models demonstrate
the effectiveness of our EXPRESS-Bench in advancing embodied exploration and
question reasoning.

</details>


### [424] [LED: LLM Enhanced Open-Vocabulary Object Detection without Human Curated Data Generation](https://arxiv.org/pdf/2503.13794)
*Yang Zhou, Shiyu Zhao, Yuxiao Chen, Zhenting Wang, Can Jin, Dimitris N. Metaxas*

Main category: cs.CV

TL;DR: LED (LLM Enhanced Open-Vocabulary Object Detection) fuses LLM hidden states into detectors via zero-initialized cross-attention, improving visual grounding without synthetic data bias.


<details>
  <summary>Details</summary>
Motivation: To avoid bias and overfitting from synthetic data in Open-Vocabulary Object Detection (OVD) by leveraging LLM hidden states.

Method: Uses decoder layers of an LLM in an MLLM, introducing a zero-initialized cross-attention adapter for efficient fusion into detectors.

Result: Improves GroundingDINO by 3.82% on OmniLabel with minimal extra computation (8.7% GFLOPs); larger backbones push gains to 6.22%.

Conclusion: LED effectively leverages LLM layers for spatial semantics, with early layers providing most gains, validated by extensive ablations.

Abstract: Large foundation models trained on large-scale vision-language data can boost
Open-Vocabulary Object Detection (OVD) via synthetic training data, yet the
hand-crafted pipelines often introduce bias and overfit to specific prompts. We
sidestep this issue by directly fusing hidden states from Large Language Models
(LLMs) into detectors-an avenue surprisingly under-explored. This paper
presents a systematic method to enhance visual grounding by utilizing decoder
layers of the LLM of an MLLM. We introduce a zero-initialized cross-attention
adapter to enable efficient knowledge fusion from LLMs to object detectors, a
new approach called LED (LLM Enhanced Open-Vocabulary Object Detection). We
find that intermediate LLM layers already encode rich spatial semantics;
adapting only the early layers yields most of the gain. With Swin-T as the
vision encoder, Qwen2-0.5B + LED lifts GroundingDINO by 3.82 % on OmniLabel at
just 8.7 % extra GFLOPs, and a larger vision backbone pushes the improvement to
6.22 %. Extensive ablations on adapter variants, LLM scales and fusion depths
further corroborate our design.

</details>


### [425] [ChatStitch: Visualizing Through Structures via Surround-View Unsupervised Deep Image Stitching with Collaborative LLM-Agents](https://arxiv.org/pdf/2503.14948)
*Hao Liang, Zhipeng Dong, Kaixin Chen, Jiyuan Guo, Yufeng Yue, Yi Yang, Mengyin Fu*

Main category: cs.CV

TL;DR: ChatStitch introduces a human-machine co-perception system for autonomous driving, using natural language and a multi-agent framework to address inefficiencies and distortion in surround-view perception.


<details>
  <summary>Details</summary>
Motivation: Existing surround-view systems suffer from unidirectional human interaction and distortion propagation, limiting their effectiveness.

Method: ChatStitch uses a closed-loop multi-agent framework with Large Language Models and SV-UDIS, a deep image stitching method for non-global-overlapping conditions.

Result: SV-UDIS achieves state-of-the-art performance on UDIS-D, with significant PSNR and SSIM improvements for 3-5 image stitching tasks.

Conclusion: ChatStitch effectively addresses inefficiencies and distortions, enhancing surround-view perception for autonomous driving.

Abstract: Surround-view perception has garnered significant attention for its ability
to enhance the perception capabilities of autonomous driving vehicles through
the exchange of information with surrounding cameras. However, existing
surround-view perception systems are limited by inefficiencies in
unidirectional interaction pattern with human and distortions in overlapping
regions exponentially propagating into non-overlapping areas. To address these
challenges, this paper introduces ChatStitch, a surround-view human-machine
co-perception system capable of unveiling obscured blind spot information
through natural language commands integrated with external digital assets. To
dismantle the unidirectional interaction bottleneck, ChatStitch implements a
cognitively grounded closed-loop interaction multi-agent framework based on
Large Language Models. To suppress distortion propagation across overlapping
boundaries, ChatStitch proposes SV-UDIS, a surround-view unsupervised deep
image stitching method under the non-global-overlapping condition. We conducted
extensive experiments on the UDIS-D, MCOV-SLAM open datasets, and our
real-world dataset. Specifically, our SV-UDIS method achieves state-of-the-art
performance on the UDIS-D dataset for 3, 4, and 5 image stitching tasks, with
PSNR improvements of 9\%, 17\%, and 21\%, and SSIM improvements of 8\%, 18\%,
and 26\%, respectively.

</details>


### [426] [Camera Movement Estimation and Path Correction using the Combination of Modified A-SIFT and Stereo System for 3D Modelling](https://arxiv.org/pdf/2503.17668)
*Usha Kumari, Shuvendu Rana*

Main category: cs.CV

TL;DR: A modified ASIFT and two-camera-based SFM model improves 3D camera path accuracy to 99.9%, outperforming existing methods.


<details>
  <summary>Details</summary>
Motivation: Challenges in 3D modeling include viewpoint variations, computational complexity, and alignment discrepancies, which accurate camera path generation can address.

Method: Proposes a modified ASIFT for efficient matching point extraction, a two-camera rotation correction model, and a stereo camera-based translation estimation model.

Result: Achieves 99.9% accuracy in camera path estimation, outperforming state-of-the-art methods.

Conclusion: The combined approach enables precise 3D model creation, offering high fidelity and efficiency for reconstruction applications.

Abstract: Creating accurate and efficient 3D models poses significant challenges,
particularly in addressing large viewpoint variations, computational
complexity, and alignment discrepancies. Efficient camera path generation can
help resolve these issues. In this context, a modified version of the Affine
Scale-Invariant Feature Transform (ASIFT) is proposed to extract more matching
points with reduced computational overhead, ensuring an adequate number of
inliers for precise camera rotation angle estimation. Additionally, a novel
two-camera-based rotation correction model is introduced to mitigate small
rotational errors, further enhancing accuracy. Furthermore, a stereo
camera-based translation estimation and correction model is implemented to
determine camera movement in 3D space by altering the Structure From Motion
(SFM) model. Finally, the novel combination of ASIFT and two camera-based SFM
models provides an accurate camera movement trajectory in 3D space.
Experimental results show that the proposed camera movement approach achieves
99.9% accuracy compared to the actual camera movement path and outperforms
state-of-the-art camera path estimation methods. By leveraging this accurate
camera path, the system facilitates the creation of precise 3D models, making
it a robust solution for applications requiring high fidelity and efficiency in
3D reconstruction.

</details>


### [427] [Synergistic Bleeding Region and Point Detection in Laparoscopic Surgical Videos](https://arxiv.org/pdf/2503.22174)
*Jialun Pei, Zhangjun Zhou, Diandian Guo, Zhixi Li, Jing Qin, Bo Du, Pheng-Ann Heng*

Main category: cs.CV

TL;DR: A dual-task detector, BlooDet, is developed for simultaneous bleeding region and point detection in laparoscopic surgery, leveraging a dual-branch design and outperforming 12 counterparts.


<details>
  <summary>Details</summary>
Motivation: Intraoperative bleeding obscures the surgical field and increases complications; intelligent detection can quantify blood loss and locate bleeding points to improve surgical success.

Method: BlooDet uses a dual-branch bidirectional guidance design based on SAM 2, with one branch for bleeding regions and another for points, leveraging memory modeling and optical flow.

Result: BlooDet outperforms 12 counterparts on the SurgBlood dataset in detecting bleeding regions and points.

Conclusion: The proposed framework effectively detects bleeding in real-time, aiding surgical decision-making and improving outcomes.

Abstract: Intraoperative bleeding in laparoscopic surgery causes rapid obscuration of
the operative field to hinder the surgical process and increases the risk of
postoperative complications. Intelligent detection of bleeding areas can
quantify the blood loss to assist decision-making, while locating bleeding
points helps surgeons quickly identify the source of bleeding and achieve
hemostasis in time to improve surgical success rates. In this study, we first
construct a real-world laparoscopic surgical bleeding detection dataset, named
SurgBlood, comprising 5,330 frames from 95 surgical video clips with bleeding
region and point annotations. Accordingly, we develop a dual-task synergistic
online detector called BlooDet, designed to perform simultaneous detection of
bleeding regions and points in laparoscopic surgery. Our framework embraces a
dual-branch bidirectional guidance design based on Segment Anything Model 2
(SAM 2). The mask branch detects bleeding regions through adaptive edge and
point prompt embeddings, and the point branch leverages mask memory to induce
bleeding point memory modeling and capture the direction of bleed point
movement via inter-frame optical flow. By bidirectional guidance, the two
branches explore potential spatial-temporal relationships while leveraging
memory modeling to infer the current bleeding condition. Extensive experiments
demonstrate that our baseline outperforms 12 counterparts on SurgBlood in both
bleeding region and point detection.

</details>


### [428] [Q-Insight: Understanding Image Quality via Visual Reinforcement Learning](https://arxiv.org/pdf/2503.22679)
*Weiqi Li, Xuanyu Zhang, Shijie Zhao, Yabin Zhang, Junlin Li, Li Zhang, Jian Zhang*

Main category: cs.CV

TL;DR: Q-Insight is a reinforcement learning-based model for image quality assessment (IQA) that uses group relative policy optimization (GRPO) to enhance visual reasoning with minimal data, outperforming existing methods.


<details>
  <summary>Details</summary>
Motivation: Current MLLM-based IQA methods lack interpretability or require extensive supervised fine-tuning, limiting flexibility. Q-Insight aims to address these gaps.

Method: Q-Insight employs GRPO for joint optimization of score regression and degradation perception tasks, using carefully designed reward functions.

Result: The model outperforms state-of-the-art methods in score regression and degradation perception, with strong zero-shot generalization.

Conclusion: Q-Insight advances IQA by combining interpretability, flexibility, and performance with minimal data requirements.

Abstract: Image quality assessment (IQA) focuses on the perceptual visual quality of
images, playing a crucial role in downstream tasks such as image
reconstruction, compression, and generation. The rapid advancement of
multi-modal large language models (MLLMs) has significantly broadened the scope
of IQA, moving toward comprehensive image quality understanding that
incorporates content analysis, degradation perception, and comparison reasoning
beyond mere numerical scoring. Previous MLLM-based methods typically either
generate numerical scores lacking interpretability or heavily rely on
supervised fine-tuning (SFT) using large-scale annotated datasets to provide
descriptive assessments, limiting their flexibility and applicability. In this
paper, we propose Q-Insight, a reinforcement learning-based model built upon
group relative policy optimization (GRPO), which demonstrates strong visual
reasoning capability for image quality understanding while requiring only a
limited amount of rating scores and degradation labels. By jointly optimizing
score regression and degradation perception tasks with carefully designed
reward functions, our approach effectively exploits their mutual benefits for
enhanced performance. Extensive experiments demonstrate that Q-Insight
substantially outperforms existing state-of-the-art methods in both score
regression and degradation perception tasks, while exhibiting impressive
zero-shot generalization to comparison reasoning tasks. Code will be available
at https://github.com/lwq20020127/Q-Insight.

</details>


### [429] [STI-Bench: Are MLLMs Ready for Precise Spatial-Temporal World Understanding?](https://arxiv.org/pdf/2503.23765)
*Yun Li, Yiming Zhang, Tao Lin, XiangRui Liu, Wenxiao Cai, Zheng Liu, Bo Zhao*

Main category: cs.CV

TL;DR: STI-Bench evaluates MLLMs' spatial-temporal understanding, revealing gaps in precise distance estimation and motion analysis.


<details>
  <summary>Details</summary>
Motivation: To assess MLLMs' real-world spatial-temporal intelligence, which is understudied despite their use in Embodied AI and Autonomous Driving.

Method: Introduces STI-Bench, a benchmark with tasks like object appearance, pose, displacement, and motion estimation across diverse scenarios.

Result: State-of-the-art MLLMs struggle with precise spatial-temporal understanding, particularly in distance estimation and motion analysis.

Conclusion: MLLMs need improvement in spatial-temporal intelligence for reliable real-world applications.

Abstract: The use of Multimodal Large Language Models (MLLMs) as an end-to-end solution
for Embodied AI and Autonomous Driving has become a prevailing trend. While
MLLMs have been extensively studied for visual semantic understanding tasks,
their ability to perform precise and quantitative spatial-temporal
understanding in real-world applications remains largely unexamined, leading to
uncertain prospects. To evaluate models' Spatial-Temporal Intelligence, we
introduce STI-Bench, a benchmark designed to evaluate MLLMs' spatial-temporal
understanding through challenging tasks such as estimating and predicting the
appearance, pose, displacement, and motion of objects. Our benchmark
encompasses a wide range of robot and vehicle operations across desktop,
indoor, and outdoor scenarios. The extensive experiments reveals that the
state-of-the-art MLLMs still struggle in real-world spatial-temporal
understanding, especially in tasks requiring precise distance estimation and
motion analysis.

</details>


### [430] [WildLive: Near Real-time Visual Wildlife Tracking onboard UAVs](https://arxiv.org/pdf/2504.10165)
*Nguyen Ngoc Dat, Tom Richardson, Matthew Watson, Kilian Meier, Jenna Kline, Sid Reid, Guy Maalouf, Duncan Hine, Majid Mirmehdi, Tilo Burghardt*

Main category: cs.CV

TL;DR: WildLive is a real-time animal detection and tracking framework for UAVs, optimized for onboard processing, achieving high fps for HD/4K streams and minimizing animal disturbance.


<details>
  <summary>Details</summary>
Motivation: Existing wildlife tracking solutions rely on ground stations, limiting autonomous flight and behavior recognition. WildLive addresses this by enabling onboard processing.

Method: Integrates sparse optical flow tracking with YOLO-driven detection and segmentation, focusing computational resources on high-uncertainty regions. Uses Jetson Orin AGX hardware.

Result: Achieves 17.81fps (HD) and 7.53fps (4K), outperforming benchmarks like OC-SORT, ByteTrack, and SORT. Includes a dataset of 200K+ annotated instances.

Conclusion: Demonstrates feasible real-time wildlife tracking on UAVs, supporting future autonomous navigation and mission-specific tasks.

Abstract: Live tracking of wildlife via high-resolution video processing directly
onboard drones is widely unexplored and most existing solutions rely on
streaming video to ground stations to support navigation. Yet, both autonomous
animal-reactive flight control beyond visual line of sight and/or
mission-specific individual and behaviour recognition tasks rely to some degree
on this capability. In response, we introduce WildLive - a near real-time
animal detection and tracking framework for high-resolution imagery running
directly onboard uncrewed aerial vehicles (UAVs). The system performs
multi-animal detection and tracking at 17.81fps for HD and 7.53fps on 4K video
streams suitable for operation during higher altitude flights to minimise
animal disturbance. Our system is optimised for Jetson Orin AGX onboard
hardware. It integrates the efficiency of sparse optical flow tracking and
mission-specific sampling with device-optimised and proven YOLO-driven object
detection and segmentation techniques. Essentially, computational resource is
focused onto spatio-temporal regions of high uncertainty to significantly
improve UAV processing speeds. Alongside, we introduce our WildLive dataset,
which comprises 200K+ annotated animal instances across 19K+ frames from 4K UAV
videos collected at the Ol Pejeta Conservancy in Kenya. All frames contain
ground truth bounding boxes, segmentation masks, as well as individual
tracklets and tracking point trajectories. We compare our system against
current object tracking approaches including OC-SORT, ByteTrack, and SORT. Our
multi-animal tracking experiments with onboard hardware confirm that near
real-time high-resolution wildlife tracking is possible on UAVs whilst
maintaining high accuracy levels as needed for future navigational and
mission-specific animal-centric operational autonomy. Our materials are
available at: https://dat-nguyenvn.github.io/WildLive/

</details>


### [431] [AnimeDL-2M: Million-Scale AI-Generated Anime Image Detection and Localization in Diffusion Era](https://arxiv.org/pdf/2504.11015)
*Chenyang Zhu, Xing Zhang, Yuyang Sun, Ching-Chun Chang, Isao Echizen*

Main category: cs.CV

TL;DR: The paper introduces AnimeDL-2M, a large-scale benchmark for detecting and localizing manipulated anime images, and proposes AniXplore, a model tailored for anime IMDL, outperforming existing methods.


<details>
  <summary>Details</summary>
Motivation: The rise of AI-generated forgeries in anime poses threats like misrepresentation and copyright violations, yet anime IMDL is underexplored compared to natural images.

Method: The study creates AnimeDL-2M, a dataset with over two million annotated anime images (real, manipulated, AI-generated), and develops AniXplore, a model designed for anime visual characteristics.

Result: Existing IMDL models for natural images perform poorly on anime, but AniXplore achieves superior results in anime IMDL tasks.

Conclusion: AnimeDL-2M and AniXplore address the domain gap in anime IMDL, offering tools to combat AI-generated forgeries in anime.

Abstract: Recent advances in image generation, particularly diffusion models, have
significantly lowered the barrier for creating sophisticated forgeries, making
image manipulation detection and localization (IMDL) increasingly challenging.
While prior work in IMDL has focused largely on natural images, the anime
domain remains underexplored-despite its growing vulnerability to AI-generated
forgeries. Misrepresentations of AI-generated images as hand-drawn artwork,
copyright violations, and inappropriate content modifications pose serious
threats to the anime community and industry. To address this gap, we propose
AnimeDL-2M, the first large-scale benchmark for anime IMDL with comprehensive
annotations. It comprises over two million images including real, partially
manipulated, and fully AI-generated samples. Experiments indicate that models
trained on existing IMDL datasets of natural images perform poorly when applied
to anime images, highlighting a clear domain gap between anime and natural
images. To better handle IMDL tasks in anime domain, we further propose
AniXplore, a novel model tailored to the visual characteristics of anime
imagery. Extensive evaluations demonstrate that AniXplore achieves superior
performance compared to existing methods. Dataset and code can be found in
https://flytweety.github.io/AnimeDL2M/.

</details>


### [432] [A Clinician-Friendly Platform for Ophthalmic Image Analysis Without Technical Barriers](https://arxiv.org/pdf/2504.15928)
*Meng Wang, Tian Lin, Qingshan Hou, Aidi Lin, Jingcheng Wang, Qingsheng Peng, Truong X. Nguyen, Danqi Fang, Ke Zou, Ting Xu, Cancan Xue, Ten Cheer Quek, Qinkai Yu, Minxin Liu, Hui Zhou, Zixuan Xiao, Guiqin He, Huiyu Liang, Tingkun Shi, Man Chen, Linna Liu, Yuanyuan Peng, Lianyu Wang, Qiuming Hu, Junhong Chen, Zhenhua Zhang, Cheng Chen, Yitian Zhao, Dianbo Liu, Jianhua Wu, Xinjian Chen, Changqing Zhang, Triet Thanh Nguyen, Yanda Meng, Yalin Zheng, Yih Chung Tham, Carol Y. Cheung, Huazhu Fu, Haoyu Chen, Ching-Yu Cheng*

Main category: cs.CV

TL;DR: GlobeReady is a clinician-friendly AI platform for fundus disease diagnosis that operates without retraining, achieving high accuracy across diverse clinical settings and imaging modalities.


<details>
  <summary>Details</summary>
Motivation: Current AI models in medical imaging require retraining for different clinical settings, limiting scalability. GlobeReady aims to overcome this by eliminating the need for retraining or technical expertise.

Method: GlobeReady uses training-free local feature augmentation to mitigate domain shifts and incorporates a confidence-quantifiable diagnostic mechanism.

Result: The platform achieves high accuracy (73.4-99.4%) across multiple centers and imaging modalities (CFPs and OCT), with strong usability ratings (4.6/5) from clinicians.

Conclusion: GlobeReady demonstrates robustness and generalizability, offering a scalable solution for global ophthalmic care without technical barriers.

Abstract: Artificial intelligence (AI) shows remarkable potential in medical imaging
diagnostics, yet most current models require retraining when applied across
different clinical settings, limiting their scalability. We introduce
GlobeReady, a clinician-friendly AI platform that enables fundus disease
diagnosis that operates without retraining, fine-tuning, or the needs for
technical expertise. GlobeReady demonstrates high accuracy across imaging
modalities: 93.9-98.5% for 11 fundus diseases using color fundus photographs
(CPFs) and 87.2-92.7% for 15 fundus diseases using optic coherence tomography
(OCT) scans. By leveraging training-free local feature augmentation, GlobeReady
platform effectively mitigates domain shifts across centers and populations,
achieving accuracies of 88.9-97.4% across five centers on average in China,
86.3-96.9% in Vietnam, and 73.4-91.0% in Singapore, and 90.2-98.9% in the UK.
Incorporating a bulit-in confidence-quantifiable diagnostic mechanism further
enhances the platform's accuracy to 94.9-99.4% with CFPs and 88.2-96.2% with
OCT, while enabling identification of out-of-distribution cases with 86.3%
accuracy across 49 common and rare fundus diseases using CFPs, and 90.6%
accuracy across 13 diseases using OCT. Clinicians from countries rated
GlobeReady highly for usability and clinical relevance (average score 4.6/5).
These findings demonstrate GlobeReady's robustness, generalizability and
potential to support global ophthalmic care without technical barriers.

</details>


### [433] [MMInference: Accelerating Pre-filling for Long-Context VLMs via Modality-Aware Permutation Sparse Attention](https://arxiv.org/pdf/2504.16083)
*Yucheng Li, Huiqiang Jiang, Chengruidong Zhang, Qianhui Wu, Xufang Luo, Surin Ahn, Amir H. Abdi, Dongsheng Li, Jianfeng Gao, Yuqing Yang, Lili Qiu*

Main category: cs.CV

TL;DR: MMInference introduces a dynamic sparse attention method to accelerate the pre-filling stage for long-context multi-modal inputs in VLMs, achieving up to 8.3x speedup at 1M tokens without accuracy loss.


<details>
  <summary>Details</summary>
Motivation: Quadratic attention complexity in VLMs during pre-filling hinders real-world deployment; MMInference addresses this by leveraging sparse patterns unique to multi-modal inputs.

Method: Uses a permutation-based method to exploit the Grid pattern in video inputs and dynamically constructs sparse distributions via offline search for optimal patterns per head, with optimized GPU kernels.

Result: Achieves up to 8.3x speedup in pre-filling at 1M tokens on benchmarks like Video QA and Captioning, without compromising accuracy.

Conclusion: MMInference effectively accelerates VLMs for long-context tasks, requiring no model modifications, and is ready for deployment.

Abstract: The integration of long-context capabilities with visual understanding
unlocks unprecedented potential for Vision Language Models (VLMs). However, the
quadratic attention complexity during the pre-filling phase remains a
significant obstacle to real-world deployment. To overcome this limitation, we
introduce MMInference (Multimodality Million tokens Inference), a dynamic
sparse attention method that accelerates the prefilling stage for long-context
multi-modal inputs. First, our analysis reveals that the temporal and spatial
locality of video input leads to a unique sparse pattern, the Grid pattern.
Simultaneously, VLMs exhibit markedly different sparse distributions across
different modalities. We introduce a permutation-based method to leverage the
unique Grid pattern and handle modality boundary issues. By offline search the
optimal sparse patterns for each head, MMInference constructs the sparse
distribution dynamically based on the input. We also provide optimized GPU
kernels for efficient sparse computations. Notably, MMInference integrates
seamlessly into existing VLM pipelines without any model modifications or
fine-tuning. Experiments on multi-modal benchmarks-including Video QA,
Captioning, VisionNIAH, and Mixed-Modality NIAH-with state-of-the-art
long-context VLMs (LongVila, LlavaVideo, VideoChat-Flash, Qwen2.5-VL) show that
MMInference accelerates the pre-filling stage by up to 8.3x at 1M tokens while
maintaining accuracy. Our code is available at https://aka.ms/MMInference.

</details>


### [434] [VR-FuseNet: A Fusion of Heterogeneous Fundus Data and Explainable Deep Network for Diabetic Retinopathy Classification](https://arxiv.org/pdf/2504.21464)
*Shamim Rahim Refat, Ziyan Shirin Raha, Shuvashis Sarker, Faika Fairuj Preotee, MD. Musfikur Rahman, Tashreef Muhammad, Mohammad Shafiul Alam*

Main category: cs.CV

TL;DR: The paper proposes VR-FuseNet, a hybrid deep learning model combining VGG19 and ResNet50V2, for automated diabetic retinopathy detection, achieving 91.824% accuracy and using XAI for interpretability.


<details>
  <summary>Details</summary>
Motivation: Diabetic retinopathy is a leading cause of blindness; early detection is crucial. Existing methods face dataset imbalance and generalization issues, necessitating a robust automated solution.

Method: A hybrid dataset from five public sources is preprocessed (SMOTE, CLAHE). VR-FuseNet fuses VGG19 (spatial features) and ResNet50V2 (hierarchical features) for improved classification.

Result: VR-FuseNet achieves 91.824% accuracy, outperforming individual models, and uses XAI for interpretable visual explanations of retinal features.

Conclusion: The hybrid approach enhances diabetic retinopathy detection, offering clinical utility through accuracy and interpretability.

Abstract: Diabetic retinopathy is a severe eye condition caused by diabetes where the
retinal blood vessels get damaged and can lead to vision loss and blindness if
not treated. Early and accurate detection is key to intervention and stopping
the disease progressing. For addressing this disease properly, this paper
presents a comprehensive approach for automated diabetic retinopathy detection
by proposing a new hybrid deep learning model called VR-FuseNet. Diabetic
retinopathy is a major eye disease and leading cause of blindness especially
among diabetic patients so accurate and efficient automated detection methods
are required. To address the limitations of existing methods including dataset
imbalance, diversity and generalization issues this paper presents a hybrid
dataset created from five publicly available diabetic retinopathy datasets.
Essential preprocessing techniques such as SMOTE for class balancing and CLAHE
for image enhancement are applied systematically to the dataset to improve the
robustness and generalizability of the dataset. The proposed VR-FuseNet model
combines the strengths of two state-of-the-art convolutional neural networks,
VGG19 which captures fine-grained spatial features and ResNet50V2 which is
known for its deep hierarchical feature extraction. This fusion improves the
diagnostic performance and achieves an accuracy of 91.824%. The model
outperforms individual architectures on all performance metrics demonstrating
the effectiveness of hybrid feature extraction in Diabetic Retinopathy
classification tasks. To make the proposed model more clinically useful and
interpretable this paper incorporates multiple XAI techniques. These techniques
generate visual explanations that clearly indicate the retinal features
affecting the model's prediction such as microaneurysms, hemorrhages and
exudates so that clinicians can interpret and validate.

</details>


### [435] [ViCTr: Vital Consistency Transfer for Pathology Aware Image Synthesis](https://arxiv.org/pdf/2505.04963)
*Onkar Susladkar, Gayatri Deshmukh, Yalcin Tur, Gorkhem Durak, Ulas Bagci*

Main category: cs.CV

TL;DR: ViCTr introduces a two-stage framework for high-fidelity medical image synthesis, combining rectified flow and Tweedie-corrected diffusion, achieving state-of-the-art results with efficient one-step sampling.


<details>
  <summary>Details</summary>
Motivation: Addressing challenges in medical image synthesis like limited annotated data, domain gaps, and diffuse pathologies, while maintaining anatomical fidelity and pathology accuracy.

Method: Two-stage framework: pretraining with EWC for anatomical preservation, followed by adversarial fine-tuning with LoRA for pathology severity control. Uses Tweedie-corrected diffusion for one-step sampling.

Result: Achieves MFID of 17.01 for cirrhosis synthesis (28% better than others) and improves nnUNet segmentation by +3.8% mDSC. Radiologists find ViCTr-generated MRIs clinically indistinguishable.

Conclusion: ViCTr is the first method for fine-grained, pathology-aware MRI synthesis with graded severity control, advancing AI-driven medical imaging.

Abstract: Synthesizing medical images remains challenging due to limited annotated
pathological data, modality domain gaps, and the complexity of representing
diffuse pathologies such as liver cirrhosis. Existing methods often struggle to
maintain anatomical fidelity while accurately modeling pathological features,
frequently relying on priors derived from natural images or inefficient
multi-step sampling. In this work, we introduce ViCTr (Vital Consistency
Transfer), a novel two-stage framework that combines a rectified flow
trajectory with a Tweedie-corrected diffusion process to achieve high-fidelity,
pathology-aware image synthesis. First, we pretrain ViCTr on the ATLAS-8k
dataset using Elastic Weight Consolidation (EWC) to preserve critical
anatomical structures. We then fine-tune the model adversarially with Low-Rank
Adaptation (LoRA) modules for precise control over pathology severity. By
reformulating Tweedie's formula within a linear trajectory framework, ViCTr
supports one-step sampling, reducing inference from 50 steps to just 4, without
sacrificing anatomical realism. We evaluate ViCTr on BTCV (CT), AMOS (MRI), and
CirrMRI600+ (cirrhosis) datasets. Results demonstrate state-of-the-art
performance, achieving a Medical Frechet Inception Distance (MFID) of 17.01 for
cirrhosis synthesis 28% lower than existing approaches and improving nnUNet
segmentation by +3.8% mDSC when used for data augmentation. Radiologist reviews
indicate that ViCTr-generated liver cirrhosis MRIs are clinically
indistinguishable from real scans. To our knowledge, ViCTr is the first method
to provide fine-grained, pathology-aware MRI synthesis with graded severity
control, closing a critical gap in AI-driven medical imaging research.

</details>


### [436] [X-Transfer Attacks: Towards Super Transferable Adversarial Attacks on CLIP](https://arxiv.org/pdf/2505.05528)
*Hanxun Huang, Sarah Erfani, Yige Li, Xingjun Ma, James Bailey*

Main category: cs.CV

TL;DR: X-Transfer is a novel attack method exposing universal adversarial vulnerability in CLIP models, achieving super transferability across tasks, domains, and models via surrogate scaling.


<details>
  <summary>Details</summary>
Motivation: Addressing the susceptibility of CLIP models to adversarial perturbations, which is critical as they are widely adopted in downstream tasks and VLMs.

Method: X-Transfer generates a Universal Adversarial Perturbation (UAP) using surrogate scaling, dynamically selecting surrogates for efficiency.

Result: X-Transfer outperforms existing UAP methods, setting a new benchmark for adversarial transferability in CLIP models.

Conclusion: X-Transfer demonstrates superior transferability and efficiency, highlighting a significant vulnerability in CLIP models.

Abstract: As Contrastive Language-Image Pre-training (CLIP) models are increasingly
adopted for diverse downstream tasks and integrated into large vision-language
models (VLMs), their susceptibility to adversarial perturbations has emerged as
a critical concern. In this work, we introduce \textbf{X-Transfer}, a novel
attack method that exposes a universal adversarial vulnerability in CLIP.
X-Transfer generates a Universal Adversarial Perturbation (UAP) capable of
deceiving various CLIP encoders and downstream VLMs across different samples,
tasks, and domains. We refer to this property as \textbf{super
transferability}--a single perturbation achieving cross-data, cross-domain,
cross-model, and cross-task adversarial transferability simultaneously. This is
achieved through \textbf{surrogate scaling}, a key innovation of our approach.
Unlike existing methods that rely on fixed surrogate models, which are
computationally intensive to scale, X-Transfer employs an efficient surrogate
scaling strategy that dynamically selects a small subset of suitable surrogates
from a large search space. Extensive evaluations demonstrate that X-Transfer
significantly outperforms previous state-of-the-art UAP methods, establishing a
new benchmark for adversarial transferability across CLIP models. The code is
publicly available in our
\href{https://github.com/HanxunH/XTransferBench}{GitHub repository}.

</details>


### [437] [Exploring Implicit Visual Misunderstandings in Multimodal Large Language Models through Attention Analysis](https://arxiv.org/pdf/2505.10541)
*Pengfei Wang, Guohai Xu, Weinong Wang, Junjie Yang, Jie Lou, Yunhua Xue*

Main category: cs.CV

TL;DR: The paper introduces a metric called 'attention accuracy' to detect implicit visual misunderstandings in Multimodal Large Language Models (MLLMs), ensuring genuine visual comprehension beyond answer correctness.


<details>
  <summary>Details</summary>
Motivation: Existing benchmarks for MLLMs focus on answer correctness but fail to verify if models truly understand visual inputs, leading to implicit visual misunderstandings (IVMs).

Method: The authors decouple visual and textual modalities in the causal attention module, analyze attention distribution, and propose 'attention accuracy' as a scale-agnostic metric. They also introduce a new benchmark for IVMs.

Result: Attention distribution increasingly focuses on the correct image as network layers deepen. The proposed metric and benchmark effectively quantify IVMs and remain robust to positional biases.

Conclusion: The attention accuracy metric reliably assesses visual understanding in MLLMs, with potential applications in finer granularities and unimodal scenarios, highlighting its versatility.

Abstract: Recent advancements have enhanced the capability of Multimodal Large Language
Models (MLLMs) to comprehend multi-image information. However, existing
benchmarks primarily evaluate answer correctness, overlooking whether models
genuinely comprehend the visual input. To address this, we define implicit
visual misunderstanding (IVM), where MLLMs provide correct answers without
fully comprehending the visual input. Through our analysis, we decouple the
visual and textual modalities within the causal attention module, revealing
that attention distribution increasingly converges on the image associated with
the correct answer as the network layers deepen. This insight leads to the
introduction of a scale-agnostic metric, \textit{attention accuracy}, and a
novel benchmark for quantifying IVMs. Attention accuracy directly evaluates the
model's visual understanding via internal mechanisms, remaining robust to
positional biases for more reliable assessments. Furthermore, we extend our
approach to finer granularities and demonstrate its effectiveness in unimodal
scenarios, underscoring its versatility and generalizability.

</details>


### [438] [ARFC-WAHNet: Adaptive Receptive Field Convolution and Wavelet-Attentive Hierarchical Network for Infrared Small Target Detection](https://arxiv.org/pdf/2505.10595)
*Xingye Cui, Junhai Luo, Jiakun Deng, Kexuan Li, Xiangyu Qiu, Zhenming Peng*

Main category: cs.CV

TL;DR: ARFC-WAHNet improves infrared small target detection by integrating adaptive receptive field convolution, wavelet frequency enhancement, and hierarchical feature fusion, outperforming state-of-the-art methods.


<details>
  <summary>Details</summary>
Motivation: Limited texture in infrared images and feature loss in deep learning methods necessitate a more adaptive and robust approach for small target detection.

Method: Proposes ARFC-WAHNet with MRFFIConv for adaptive feature extraction, WFED for noise suppression, HLFF for feature fusion, and GMEA for attention-based enhancement.

Result: Outperforms existing methods on SIRST, NUDT-SIRST, and IRSTD-1k datasets, especially in complex backgrounds.

Conclusion: ARFC-WAHNet effectively addresses challenges in infrared small target detection with superior accuracy and robustness.

Abstract: Infrared small target detection (ISTD) is critical in both civilian and
military applications. However, the limited texture and structural information
in infrared images makes accurate detection particularly challenging. Although
recent deep learning-based methods have improved performance, their use of
conventional convolution kernels limits adaptability to complex scenes and
diverse targets. Moreover, pooling operations often cause feature loss and
insufficient exploitation of image information. To address these issues, we
propose an adaptive receptive field convolution and wavelet-attentive
hierarchical network for infrared small target detection (ARFC-WAHNet). This
network incorporates a multi-receptive field feature interaction convolution
(MRFFIConv) module to adaptively extract discriminative features by integrating
multiple convolutional branches with a gated unit. A wavelet frequency
enhancement downsampling (WFED) module leverages Haar wavelet transform and
frequency-domain reconstruction to enhance target features and suppress
background noise. Additionally, we introduce a high-low feature fusion (HLFF)
module for integrating low-level details with high-level semantics, and a
global median enhancement attention (GMEA) module to improve feature diversity
and expressiveness via global attention. Experiments on public datasets SIRST,
NUDT-SIRST, and IRSTD-1k demonstrate that ARFC-WAHNet outperforms recent
state-of-the-art methods in both detection accuracy and robustness,
particularly under complex backgrounds. The code is available at
https://github.com/Leaf2001/ARFC-WAHNet.

</details>


### [439] [HumaniBench: A Human-Centric Framework for Large Multimodal Models Evaluation](https://arxiv.org/pdf/2505.11454)
*Shaina Raza, Aravind Narayanan, Vahid Reza Khazaie, Ashmal Vayani, Mukund S. Chettiar, Amandeep Singh, Mubarak Shah, Deval Pandya*

Main category: cs.CV

TL;DR: HumaniBench is a new benchmark evaluating Large Multimodal Models (LMMs) on human-centered AI principles like fairness, ethics, and empathy, revealing gaps in current models.


<details>
  <summary>Details</summary>
Motivation: LMMs perform well on standard benchmarks but lack alignment with human values like fairness and inclusivity. HumaniBench addresses this gap.

Method: A dataset of 32K image-question pairs was created using GPT4-assisted annotation and expert verification, evaluating seven HCAI principles across diverse tasks.

Result: Proprietary models generally outperform open-source ones, but weaknesses in robustness and visual grounding persist. Open-source models struggle with balancing accuracy and human-aligned principles.

Conclusion: HumaniBench is the first benchmark focused on HCAI principles, offering a tool to improve LMM alignment with human values.

Abstract: Large multimodal models (LMMs) now excel on many vision language benchmarks,
however, they still struggle with human centered criteria such as fairness,
ethics, empathy, and inclusivity, key to aligning with human values. We
introduce HumaniBench, a holistic benchmark of 32K real-world image question
pairs, annotated via a scalable GPT4o assisted pipeline and exhaustively
verified by domain experts. HumaniBench evaluates seven Human Centered AI
(HCAI) principles: fairness, ethics, understanding, reasoning, language
inclusivity, empathy, and robustness, across seven diverse tasks, including
open and closed ended visual question answering (VQA), multilingual QA, visual
grounding, empathetic captioning, and robustness tests. Benchmarking 15 state
of the art LMMs (open and closed source) reveals that proprietary models
generally lead, though robustness and visual grounding remain weak points. Some
open-source models also struggle to balance accuracy with adherence to
human-aligned principles. HumaniBench is the first benchmark purpose built
around HCAI principles. It provides a rigorous testbed for diagnosing alignment
gaps and guiding LMMs toward behavior that is both accurate and socially
responsible. Dataset, annotation prompts, and evaluation code are available at:
https://vectorinstitute.github.io/HumaniBench

</details>


### [440] [QVGen: Pushing the Limit of Quantized Video Generative Models](https://arxiv.org/pdf/2505.11497)
*Yushi Huang, Ruihao Gong, Jing Liu, Yifu Ding, Chengtao Lv, Haotong Qin, Jun Zhang*

Main category: cs.CV

TL;DR: QVGen is a quantization-aware training framework for video diffusion models, enabling high performance under low-bit quantization (e.g., 4-bit) without inference overhead.


<details>
  <summary>Details</summary>
Motivation: Video diffusion models face high computational and memory demands, and existing quantization methods are ineffective for them.

Method: QVGen uses auxiliary modules to reduce quantization errors and a rank-decay strategy to eliminate inference overhead, employing SVD and rank-based regularization.

Result: QVGen achieves full-precision comparable quality under 4-bit settings and outperforms existing methods, e.g., +25.28 in Dynamic Degree and +8.43 in Scene Consistency.

Conclusion: QVGen is a breakthrough for efficient video diffusion models, offering high performance with minimal computational cost.

Abstract: Video diffusion models (DMs) have enabled high-quality video synthesis. Yet,
their substantial computational and memory demands pose serious challenges to
real-world deployment, even on high-end GPUs. As a commonly adopted solution,
quantization has proven notable success in reducing cost for image DMs, while
its direct application to video DMs remains ineffective. In this paper, we
present QVGen, a novel quantization-aware training (QAT) framework tailored for
high-performance and inference-efficient video DMs under extremely low-bit
quantization (e.g., 4-bit or below). We begin with a theoretical analysis
demonstrating that reducing the gradient norm is essential to facilitate
convergence for QAT. To this end, we introduce auxiliary modules ($\Phi$) to
mitigate large quantization errors, leading to significantly enhanced
convergence. To eliminate the inference overhead of $\Phi$, we propose a
rank-decay strategy that progressively eliminates $\Phi$. Specifically, we
repeatedly employ singular value decomposition (SVD) and a proposed rank-based
regularization $\mathbf{\gamma}$ to identify and decay low-contributing
components. This strategy retains performance while zeroing out inference
overhead. Extensive experiments across $4$ state-of-the-art (SOTA) video DMs,
with parameter sizes ranging from $1.3$B $\sim14$B, show that QVGen is the
first to reach full-precision comparable quality under 4-bit settings.
Moreover, it significantly outperforms existing methods. For instance, our
3-bit CogVideoX-2B achieves improvements of $+25.28$ in Dynamic Degree and
$+8.43$ in Scene Consistency on VBench.

</details>


### [441] [VGGT-SLAM: Dense RGB SLAM Optimized on the SL(4) Manifold](https://arxiv.org/pdf/2505.12549)
*Dominic Maggio, Hyungtae Lim, Luca Carlone*

Main category: cs.CV

TL;DR: VGGT-SLAM is a dense RGB SLAM system using uncalibrated monocular cameras, aligning submaps via SL(4) manifold optimization for improved map quality.


<details>
  <summary>Details</summary>
Motivation: Existing methods using similarity transforms are inadequate for uncalibrated cameras due to reconstruction ambiguity.

Method: Aligns submaps by optimizing over the SL(4) manifold to estimate 15-DOF homography transforms, accounting for loop closure constraints.

Result: Achieves better map quality with long video sequences, outperforming VGGT.

Conclusion: VGGT-SLAM effectively addresses reconstruction ambiguity in uncalibrated cameras, enhancing SLAM performance.

Abstract: We present VGGT-SLAM, a dense RGB SLAM system constructed by incrementally
and globally aligning submaps created from the feed-forward scene
reconstruction approach VGGT using only uncalibrated monocular cameras. While
related works align submaps using similarity transforms (i.e., translation,
rotation, and scale), we show that such approaches are inadequate in the case
of uncalibrated cameras. In particular, we revisit the idea of reconstruction
ambiguity, where given a set of uncalibrated cameras with no assumption on the
camera motion or scene structure, the scene can only be reconstructed up to a
15-degrees-of-freedom projective transformation of the true geometry. This
inspires us to recover a consistent scene reconstruction across submaps by
optimizing over the SL(4) manifold, thus estimating 15-degrees-of-freedom
homography transforms between sequential submaps while accounting for potential
loop closure constraints. As verified by extensive experiments, we demonstrate
that VGGT-SLAM achieves improved map quality using long video sequences that
are infeasible for VGGT due to its high GPU requirements.

</details>


### [442] [Selective Structured State Space for Multispectral-fused Small Target Detection](https://arxiv.org/pdf/2505.14043)
*Qianqian Zhang, WeiJun Wang, Yunxing Liu, Li Zhou, Hao Zhao, Junshe An, Zihan Wang*

Main category: cs.CV

TL;DR: The paper proposes enhancements to Mamba for small target detection in high-resolution remote sensing imagery, introducing ESTD, CARG, and MEPF modules to improve accuracy and efficiency.


<details>
  <summary>Details</summary>
Motivation: Challenges in small target detection include low accuracy and high computational costs, exacerbated by the limitations of Transformer and CNN architectures.

Method: Enhanced Mamba with ESTD for local attention, CARG for spatial/channel-wise focus, and MEPF for multispectral fusion.

Result: Improved small target detection by capturing fine details and enhancing semantic representations.

Conclusion: The proposed modules effectively address computational and accuracy challenges in small target detection.

Abstract: Target detection in high-resolution remote sensing imagery faces challenges
due to the low recognition accuracy of small targets and high computational
costs. The computational complexity of the Transformer architecture increases
quadratically with image resolution, while Convolutional Neural Networks (CNN)
architectures are forced to stack deeper convolutional layers to expand their
receptive fields, leading to an explosive growth in computational demands. To
address these computational constraints, we leverage Mamba's linear complexity
for efficiency. However, Mamba's performance declines for small targets,
primarily because small targets occupy a limited area in the image and have
limited semantic information. Accurate identification of these small targets
necessitates not only Mamba's global attention capabilities but also the
precise capture of fine local details. To this end, we enhance Mamba by
developing the Enhanced Small Target Detection (ESTD) module and the
Convolutional Attention Residual Gate (CARG) module. The ESTD module bolsters
local attention to capture fine-grained details, while the CARG module, built
upon Mamba, emphasizes spatial and channel-wise information, collectively
improving the model's ability to capture distinctive representations of small
targets. Additionally, to highlight the semantic representation of small
targets, we design a Mask Enhanced Pixel-level Fusion (MEPF) module for
multispectral fusion, which enhances target features by effectively fusing
visible and infrared multimodal information.

</details>


### [443] [Exploring Generalized Gait Recognition: Reducing Redundancy and Noise within Indoor and Outdoor Datasets](https://arxiv.org/pdf/2505.15176)
*Qian Zhou, Xianda Guo, Jilong Wang, Chuanfu Shen, Zhongyuan Wang, Hua Zou, Qin Zou, Chao Liang, Long Chen, Gang Wu*

Main category: cs.CV

TL;DR: A unified framework improves cross-domain gait recognition by mitigating inter-dataset conflicts and filtering noisy samples.


<details>
  <summary>Details</summary>
Motivation: Address challenges like domain shifts and noisy data in generalized gait recognition.

Method: Uses a disentangled triplet loss and dataset distillation to enhance learning.

Result: Significantly improves cross-dataset recognition without losing source-domain accuracy.

Conclusion: The framework effectively boosts generalization in gait recognition across diverse domains.

Abstract: Generalized gait recognition, which aims to achieve robust performance across
diverse domains, remains a challenging problem due to severe domain shifts in
viewpoints, appearances, and environments. While mixed-dataset training is
widely used to enhance generalization, it introduces new obstacles including
inter-dataset optimization conflicts and redundant or noisy samples, both of
which hinder effective representation learning. To address these challenges, we
propose a unified framework that systematically improves cross-domain gait
recognition. First, we design a disentangled triplet loss that isolates
supervision signals across datasets, mitigating gradient conflicts during
optimization. Second, we introduce a targeted dataset distillation strategy
that filters out the least informative 20\% of training samples based on
feature redundancy and prediction uncertainty, enhancing data efficiency.
Extensive experiments on CASIA-B, OU-MVLP, Gait3D, and GREW demonstrate that
our method significantly improves cross-dataset recognition for both GaitBase
and DeepGaitV2 backbones, without sacrificing source-domain accuracy. Code will
be released at https://github.com/li1er3/Generalized_Gait.

</details>


### [444] [Flashback: Memory-Driven Zero-shot, Real-time Video Anomaly Detection](https://arxiv.org/pdf/2505.15205)
*Hyogun Lee, Haksub Kim, Ig-Jae Kim, Yonghun Choi*

Main category: cs.CV

TL;DR: Flashback is a zero-shot, real-time video anomaly detection method using a two-stage process (Recall and Respond) to match video segments against a pre-built memory of captions, achieving high performance without real anomaly data.


<details>
  <summary>Details</summary>
Motivation: Current video anomaly detection methods face challenges like domain dependency and real-time constraints, limiting real-world adoption.

Method: Flashback uses an LLM to create a pseudo-scene memory of captions (Recall stage) and matches incoming video segments via similarity search (Respond stage), avoiding LLM calls at inference for real-time performance.

Result: Achieves 87.3 AUC (+7.0 pp) on UCF-Crime and 75.1 AP (+13.1 pp) on XD-Violence, outperforming prior zero-shot methods.

Conclusion: Flashback addresses domain dependency and real-time constraints, offering a practical solution for video anomaly detection.

Abstract: Video Anomaly Detection (VAD) automatically identifies anomalous events from
video, mitigating the need for human operators in large-scale surveillance
deployments. However, two fundamental obstacles hinder real-world adoption:
domain dependency and real-time constraints -- requiring near-instantaneous
processing of incoming video. To this end, we propose Flashback, a zero-shot
and real-time video anomaly detection paradigm. Inspired by the human cognitive
mechanism of instantly judging anomalies and reasoning in current scenes based
on past experience, Flashback operates in two stages: Recall and Respond. In
the offline recall stage, an off-the-shelf LLM builds a pseudo-scene memory of
both normal and anomalous captions without any reliance on real anomaly data.
In the online respond stage, incoming video segments are embedded and matched
against this memory via similarity search. By eliminating all LLM calls at
inference time, Flashback delivers real-time VAD even on a consumer-grade GPU.
On two large datasets from real-world surveillance scenarios, UCF-Crime and
XD-Violence, we achieve 87.3 AUC (+7.0 pp) and 75.1 AP (+13.1 pp),
respectively, outperforming prior zero-shot VAD methods by large margins.

</details>


### [445] [On the Robustness of Medical Vision-Language Models: Are they Truly Generalizable?](https://arxiv.org/pdf/2505.15425)
*Raza Imam, Rufael Marew, Mohammad Yaqub*

Main category: cs.CV

TL;DR: The paper introduces MediMeta-C and RobustMedCLIP to evaluate and enhance the robustness of Medical Vision-Language Models (MVLMs) under noisy conditions, revealing their current limitations and proposing solutions.


<details>
  <summary>Details</summary>
Motivation: Existing MVLMs excel in clean datasets but lack testing under real-world noisy conditions, which is critical for clinical applications.

Method: Introduces MediMeta-C benchmark and proposes RobustMedCLIP, a few-shot tuning adaptation of a pretrained MVLM, to improve robustness.

Result: Experiments show severe degradation of existing MVLMs under corruption and highlight the effectiveness of low-rank adaptation with few-shot tuning.

Conclusion: Diverse training and robust adaptation strategies are essential for MVLMs to perform reliably in real-world medical imaging scenarios.

Abstract: Medical Vision-Language Models (MVLMs) have achieved par excellence
generalization in medical image analysis, yet their performance under noisy,
corrupted conditions remains largely untested. Clinical imaging is inherently
susceptible to acquisition artifacts and noise; however, existing evaluations
predominantly assess generally clean datasets, overlooking robustness -- i.e.,
the model's ability to perform under real-world distortions. To address this
gap, we first introduce MediMeta-C, a corruption benchmark that systematically
applies several perturbations across multiple medical imaging datasets.
Combined with MedMNIST-C, this establishes a comprehensive robustness
evaluation framework for MVLMs. We further propose RobustMedCLIP, a visual
encoder adaptation of a pretrained MVLM that incorporates few-shot tuning to
enhance resilience against corruptions. Through extensive experiments, we
benchmark 5 major MVLMs across 5 medical imaging modalities, revealing that
existing models exhibit severe degradation under corruption and struggle with
domain-modality tradeoffs. Our findings highlight the necessity of diverse
training and robust adaptation strategies, demonstrating that efficient
low-rank adaptation when paired with few-shot tuning, improves robustness while
preserving generalization across modalities.

</details>


### [446] [Challenger: Affordable Adversarial Driving Video Generation](https://arxiv.org/pdf/2505.15880)
*Zhiyuan Xu, Bohan Li, Huan-ang Gao, Mingju Gao, Yong Chen, Ming Liu, Chenxu Yan, Hang Zhao, Shuo Feng, Hao Zhao*

Main category: cs.CV

TL;DR: Challenger is a framework for generating photorealistic adversarial driving videos to stress-test autonomous driving systems, outperforming current methods by combining physics-aware trajectory refinement and realistic scoring.


<details>
  <summary>Details</summary>
Motivation: Current methods for generating adversarial driving scenarios lack realism and fail to stress-test autonomous driving systems effectively. Challenger aims to bridge this gap by producing photorealistic and physically plausible adversarial videos.

Method: Challenger uses (1) a physics-aware multi-round trajectory refinement process and (2) a tailored scoring function to generate realistic adversarial maneuvers, rendered into multiview photorealistic videos.

Result: Tested on nuScenes, Challenger generates aggressive scenarios (e.g., cut-ins, sudden lane changes) that increase collision rates of top AD models (UniAD, VAD, etc.), with adversarial behaviors often transferring across models.

Conclusion: Challenger successfully creates realistic adversarial driving scenarios, proving effective in testing and exposing vulnerabilities in state-of-the-art autonomous driving models.

Abstract: Generating photorealistic driving videos has seen significant progress
recently, but current methods largely focus on ordinary, non-adversarial
scenarios. Meanwhile, efforts to generate adversarial driving scenarios often
operate on abstract trajectory or BEV representations, falling short of
delivering realistic sensor data that can truly stress-test autonomous driving
(AD) systems. In this work, we introduce Challenger, a framework that produces
physically plausible yet photorealistic adversarial driving videos. Generating
such videos poses a fundamental challenge: it requires jointly optimizing over
the space of traffic interactions and high-fidelity sensor observations.
Challenger makes this affordable through two techniques: (1) a physics-aware
multi-round trajectory refinement process that narrows down candidate
adversarial maneuvers, and (2) a tailored trajectory scoring function that
encourages realistic yet adversarial behavior while maintaining compatibility
with downstream video synthesis. As tested on the nuScenes dataset, Challenger
generates a diverse range of aggressive driving scenarios-including cut-ins,
sudden lane changes, tailgating, and blind spot intrusions-and renders them
into multiview photorealistic videos. Extensive evaluations show that these
scenarios significantly increase the collision rate of state-of-the-art
end-to-end AD models (UniAD, VAD, SparseDrive, and DiffusionDrive), and
importantly, adversarial behaviors discovered for one model often transfer to
others.

</details>


### [447] [MedCFVQA: A Causal Approach to Mitigate Modality Preference Bias in Medical Visual Question Answering](https://arxiv.org/pdf/2505.16209)
*Shuchang Ye, Usman Naseem, Mingyuan Meng, Dagan Feng, Jinman Kim*

Main category: cs.CV

TL;DR: MedCFVQA addresses modality bias in MedVQA by using causal graphs and dataset reconstruction, outperforming non-causal models.


<details>
  <summary>Details</summary>
Motivation: Existing MedVQA models suffer from modality preference bias, where one modality (e.g., questions) dominates predictions, limiting multimodal learning.

Method: Proposed MedCFVQA uses causal graphs to eliminate bias during inference and reconstructs datasets (CP) to reduce prior dependencies.

Result: MedCFVQA outperforms non-causal models on SLAKE, RadVQA, and their CP variants.

Conclusion: MedCFVQA effectively mitigates modality bias and improves performance in MedVQA tasks.

Abstract: Medical Visual Question Answering (MedVQA) is crucial for enhancing the
efficiency of clinical diagnosis by providing accurate and timely responses to
clinicians' inquiries regarding medical images. Existing MedVQA models suffered
from modality preference bias, where predictions are heavily dominated by one
modality while overlooking the other (in MedVQA, usually questions dominate the
answer but images are overlooked), thereby failing to learn multimodal
knowledge. To overcome the modality preference bias, we proposed a Medical
CounterFactual VQA (MedCFVQA) model, which trains with bias and leverages
causal graphs to eliminate the modality preference bias during inference.
Existing MedVQA datasets exhibit substantial prior dependencies between
questions and answers, which results in acceptable performance even if the
model significantly suffers from the modality preference bias. To address this
issue, we reconstructed new datasets by leveraging existing MedVQA datasets and
Changed their P3rior dependencies (CP) between questions and their answers in
the training and test set. Extensive experiments demonstrate that MedCFVQA
significantly outperforms its non-causal counterpart on both SLAKE, RadVQA and
SLAKE-CP, RadVQA-CP datasets.

</details>


### [448] [Panoptic Captioning: Seeking An Equivalency Bridge for Image and Text](https://arxiv.org/pdf/2505.16334)
*Kun-Yu Lin, Hongjun Wang, Weining Ren, Kai Han*

Main category: cs.CV

TL;DR: The paper introduces panoptic captioning, a task to generate comprehensive textual descriptions of images, and proposes PancapEngine for data generation and PancapChain for improved performance, outperforming top models.


<details>
  <summary>Details</summary>
Motivation: To address the limited performance of existing Multi-modal Large Language Models (MLLMs) in generating detailed image descriptions (panoptic captioning).

Method: Proposes PancapEngine for high-quality data generation using entity detection and prompts, and PancapChain, a multi-stage method for step-by-step captioning. Introduces PancapScore for evaluation.

Result: PancapChain-13B outperforms state-of-the-art models like InternVL-2.5-78B, GPT-4o, and Gemini-2.0-Pro.

Conclusion: The proposed data engine and method effectively improve panoptic captioning, demonstrated by superior performance over existing models.

Abstract: This work introduces panoptic captioning, a novel task striving to seek the
minimum text equivalence of images. We take the first step towards panoptic
captioning by formulating it as a task of generating a comprehensive textual
description for an image, which encapsulates all entities, their respective
locations and attributes, relationships among entities, as well as global image
state. Through an extensive evaluation, our work reveals that state-of-the-art
Multi-modal Large Language Models (MLLMs) have limited performance in solving
panoptic captioning. To address this, we propose an effective data engine named
PancapEngine to produce high-quality data and a novel method named PancapChain
to improve panoptic captioning. Specifically, our PancapEngine first detects
diverse categories of entities in images by an elaborate detection suite, and
then generates required panoptic captions using entity-aware prompts.
Additionally, our PancapChain explicitly decouples the challenging panoptic
captioning task into multiple stages and generates panoptic captions step by
step. More importantly, we contribute a comprehensive metric named PancapScore
and a human-curated test set for reliable model evaluation. Experiments show
that our PancapChain-13B model can beat state-of-the-art open-source MLLMs like
InternVL-2.5-78B and even surpass proprietary models like GPT-4o and
Gemini-2.0-Pro, demonstrating the effectiveness of our data engine and method.
Project page: https://visual-ai.github.io/pancap/

</details>


### [449] [Decoupled Geometric Parameterization and its Application in Deep Homography Estimation](https://arxiv.org/pdf/2505.16599)
*Yao Huang, Si-Yuan Cao, Yaqing Ding, Hao Yin, Shibin Xie, Shuting Wang, Zhijun Fang, Jiachun Wang, Shen Cai, Junchi Yan, Shuhan Shen*

Main category: cs.CV

TL;DR: A novel geometric parameterization for planar homography using SKS decomposition, decoupling similarity and kernel transformations for direct estimation without solving linear systems.


<details>
  <summary>Details</summary>
Motivation: Traditional four-corner positional offsets lack geometric interpretability and require solving linear systems, prompting a need for a more intuitive and efficient method.

Method: Proposes a similarity-kernel-similarity (SKS) decomposition, decoupling geometric parameters for similarity and kernel transformations, with linear interpretation for angular offsets.

Result: Enables direct homography estimation via matrix multiplication, matching performance of traditional four-corner methods in deep homography estimation.

Conclusion: The SKS-based parameterization offers geometric interpretability and computational efficiency, advancing homography estimation in computer vision.

Abstract: Planar homography, with eight degrees of freedom (DOFs), is fundamental in
numerous computer vision tasks. While the positional offsets of four corners
are widely adopted (especially in neural network predictions), this
parameterization lacks geometric interpretability and typically requires
solving a linear system to compute the homography matrix. This paper presents a
novel geometric parameterization of homographies, leveraging the
similarity-kernel-similarity (SKS) decomposition for projective
transformations. Two independent sets of four geometric parameters are
decoupled: one for a similarity transformation and the other for the kernel
transformation. Additionally, the geometric interpretation linearly relating
the four kernel transformation parameters to angular offsets is derived. Our
proposed parameterization allows for direct homography estimation through
matrix multiplication, eliminating the need for solving a linear system, and
achieves performance comparable to the four-corner positional offsets in deep
homography estimation.

</details>


### [450] [RBench-V: A Primary Assessment for Visual Reasoning Models with Multi-modal Outputs](https://arxiv.org/pdf/2505.16770)
*Meng-Hao Guo, Xuanyu Chu, Qianrui Yang, Zhe-Han Mo, Yiqing Shen, Pei-lin Li, Xinjie Lin, Jinnian Zhang, Xin-Sheng Chen, Yi Zhang, Kiyohiro Nakayama, Zhengyang Geng, Houwen Peng, Han Hu, Shi-Min Hu*

Main category: cs.CV

TL;DR: The paper introduces RBench-V, a benchmark for evaluating multi-modal models' reasoning abilities through multi-modal outputs, revealing significant gaps compared to human performance.


<details>
  <summary>Details</summary>
Motivation: Existing benchmarks focus on multi-modal inputs and text-only reasoning, neglecting multi-modal output reasoning, which is critical for visual thinking processes.

Method: The authors construct RBench-V with 803 questions requiring image manipulation (e.g., generating images, drawing auxiliary lines) to solve problems in math, physics, counting, and games.

Result: Evaluation shows even the best model (o3) achieves only 25.8% accuracy, far below the human score of 82.3%.

Conclusion: Current models struggle with multi-modal reasoning, highlighting the need for further advancements in this area.

Abstract: The rapid advancement of native multi-modal models and omni-models,
exemplified by GPT-4o, Gemini, and o3, with their capability to process and
generate content across modalities such as text and images, marks a significant
milestone in the evolution of intelligence. Systematic evaluation of their
multi-modal output capabilities in visual thinking processes (also known as
multi-modal chain of thought, M-CoT) becomes critically important. However,
existing benchmarks for evaluating multi-modal models primarily focus on
assessing multi-modal inputs and text-only reasoning while neglecting the
importance of reasoning through multi-modal outputs. In this paper, we present
a benchmark, dubbed RBench-V, designed to assess models' vision-indispensable
reasoning abilities. To construct RBench-V, we carefully hand-pick 803
questions covering math, physics, counting, and games. Unlike previous
benchmarks that typically specify certain input modalities, RBench-V presents
problems centered on multi-modal outputs, which require image manipulation such
as generating novel images and constructing auxiliary lines to support the
reasoning process. We evaluate numerous open- and closed-source models on
RBench-V, including o3, Gemini 2.5 Pro, Qwen2.5-VL, etc. Even the
best-performing model, o3, achieves only 25.8% accuracy on RBench-V, far below
the human score of 82.3%, highlighting that current models struggle to leverage
multi-modal reasoning. Data and code are available at
https://evalmodels.github.io/rbenchv

</details>


### [451] [LaViDa: A Large Diffusion Language Model for Multimodal Understanding](https://arxiv.org/pdf/2505.16839)
*Shufan Li, Konstantinos Kallidromitis, Hritik Bansal, Akash Gokul, Yusuke Kato, Kazuki Kozuka, Jason Kuen, Zhe Lin, Kai-Wei Chang, Aditya Grover*

Main category: cs.CV

TL;DR: LaViDa introduces a family of Vision-Language Models (VLMs) based on discrete diffusion models (DMs), offering faster inference, controllable generation, and bidirectional reasoning, outperforming autoregressive VLMs like LLaVA on benchmarks.


<details>
  <summary>Details</summary>
Motivation: Existing autoregressive VLMs lack fast inference and controllable generation. DMs, effective in language tasks, are underexplored for multimodal applications.

Method: LaViDa integrates DMs with a vision encoder, using techniques like complementary masking, prefix KV cache, and timestep shifting for training and inference.

Result: LaViDa outperforms AR VLMs on benchmarks (e.g., +4.1 CIDEr on COCO captioning, +59% on Constrained Poem Completion) with faster inference.

Conclusion: LaViDa is a competitive alternative to AR VLMs, offering unique advantages in speed, controllability, and bidirectional reasoning.

Abstract: Modern Vision-Language Models (VLMs) can solve a wide range of tasks
requiring visual reasoning. In real-world scenarios, desirable properties for
VLMs include fast inference and controllable generation (e.g., constraining
outputs to adhere to a desired format). However, existing autoregressive (AR)
VLMs like LLaVA struggle in these aspects. Discrete diffusion models (DMs)
offer a promising alternative, enabling parallel decoding for faster inference
and bidirectional context for controllable generation through text-infilling.
While effective in language-only settings, DMs' potential for multimodal tasks
is underexplored. We introduce LaViDa, a family of VLMs built on DMs. We build
LaViDa by equipping DMs with a vision encoder and jointly fine-tune the
combined parts for multimodal instruction following. To address challenges
encountered, LaViDa incorporates novel techniques such as complementary masking
for effective training, prefix KV cache for efficient inference, and timestep
shifting for high-quality sampling. Experiments show that LaViDa achieves
competitive or superior performance to AR VLMs on multi-modal benchmarks such
as MMMU, while offering unique advantages of DMs, including flexible
speed-quality tradeoff, controllability, and bidirectional reasoning. On COCO
captioning, LaViDa surpasses Open-LLaVa-Next-8B by +4.1 CIDEr with 1.92x
speedup. On bidirectional tasks, it achieves +59% improvement on Constrained
Poem Completion. These results demonstrate LaViDa as a strong alternative to AR
VLMs. Code and models will be released in the camera-ready version.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [452] [An Affective-Taxis Hypothesis for Alignment and Interpretability](https://arxiv.org/pdf/2505.17024)
*Eli Sennesh, Maxwell Ramstead*

Main category: cs.AI

TL;DR: The paper proposes an affectivist approach to AI alignment, using affective taxis to model goals and values, supported by neuroscience and tested in a tractable organism.


<details>
  <summary>Details</summary>
Motivation: To ensure AI agents behave in alignment with human goals and values, regardless of capability, by leveraging affective taxis.

Method: Proposes a computational model of affect based on taxis navigation, informed by evolutionary-developmental and computational neuroscience.

Result: Evidence shows the model reflects aspects of biological taxis navigation in a tractable organism.

Conclusion: Affective taxis plays a key role in AI alignment, offering a promising framework for future research.

Abstract: AI alignment is a field of research that aims to develop methods to ensure
that agents always behave in a manner aligned with (i.e. consistently with) the
goals and values of their human operators, no matter their level of capability.
This paper proposes an affectivist approach to the alignment problem,
re-framing the concepts of goals and values in terms of affective taxis, and
explaining the emergence of affective valence by appealing to recent work in
evolutionary-developmental and computational neuroscience. We review the state
of the art and, building on this work, we propose a computational model of
affect based on taxis navigation. We discuss evidence in a tractable model
organism that our model reflects aspects of biological taxis navigation. We
conclude with a discussion of the role of affective taxis in AI alignment.

</details>


### [453] [MEDMKG: Benchmarking Medical Knowledge Exploitation with Multimodal Knowledge Graph](https://arxiv.org/pdf/2505.17214)
*Xiaochen Wang, Yuan Zhong, Lingwei Zhang, Lisong Dai, Ting Wang, Fenglong Ma*

Main category: cs.AI

TL;DR: MEDMKG, a multimodal medical knowledge graph, integrates visual and textual data to enhance deep learning models for clinical tasks, outperforming existing methods.


<details>
  <summary>Details</summary>
Motivation: Addressing the lack of multimodal medical knowledge graphs by unifying imaging and clinical data for improved model performance.

Method: Proposes MEDMKG, combining MIMIC-CXR and UMLS data, using rule-based tools and LLMs for concept extraction, and introduces Neighbor-aware Filtering (NaF).

Result: MEDMKG improves performance in downstream tasks and provides a robust foundation for multimodal knowledge integration.

Conclusion: MEDMKG successfully bridges the gap in multimodal medical knowledge, enhancing AI applications in healthcare.

Abstract: Medical deep learning models depend heavily on domain-specific knowledge to
perform well on knowledge-intensive clinical tasks. Prior work has primarily
leveraged unimodal knowledge graphs, such as the Unified Medical Language
System (UMLS), to enhance model performance. However, integrating multimodal
medical knowledge graphs remains largely underexplored, mainly due to the lack
of resources linking imaging data with clinical concepts. To address this gap,
we propose MEDMKG, a Medical Multimodal Knowledge Graph that unifies visual and
textual medical information through a multi-stage construction pipeline. MEDMKG
fuses the rich multimodal data from MIMIC-CXR with the structured clinical
knowledge from UMLS, utilizing both rule-based tools and large language models
for accurate concept extraction and relationship modeling. To ensure graph
quality and compactness, we introduce Neighbor-aware Filtering (NaF), a novel
filtering algorithm tailored for multimodal knowledge graphs. We evaluate
MEDMKG across three tasks under two experimental settings, benchmarking
twenty-four baseline methods and four state-of-the-art vision-language
backbones on six datasets. Results show that MEDMKG not only improves
performance in downstream medical tasks but also offers a strong foundation for
developing adaptive and robust strategies for multimodal knowledge integration
in medical artificial intelligence.

</details>


### [454] [Effective Reinforcement Learning for Reasoning in Language Models](https://arxiv.org/pdf/2505.17218)
*Lianghuan Huang, Shuo Li, Sagnik Anupam, Insup Lee, Osbert Bastani*

Main category: cs.AI

TL;DR: The paper explores RL for improving LM reasoning, highlighting on-policy RL's superiority over SFT, PPO's accuracy benefits, and KL divergence removal's impact. It introduces DASH, a novel algorithm reducing training time by 83% without accuracy loss.


<details>
  <summary>Details</summary>
Motivation: To address the mismatch between RL algorithms designed for robotics and their application to LM reasoning, focusing on accuracy and computational efficiency.

Method: Analyzes RL design choices for LMs, proposes DASH with preemptive sampling and gradient filtering.

Result: On-policy RL outperforms SFT, PPO increases accuracy, KL divergence removal improves conciseness and accuracy. DASH reduces training time by 83%.

Conclusion: Provides insights for effective RL algorithm design in LM reasoning, demonstrating DASH's efficiency and accuracy benefits.

Abstract: Reinforcement learning (RL) has emerged as a promising strategy for improving
the reasoning capabilities of language models (LMs) in domains such as
mathematics and coding. However, most modern RL algorithms were designed to
target robotics applications, which differ significantly from LM reasoning. We
analyze RL algorithm design decisions for LM reasoning, for both accuracy and
computational efficiency, focusing on relatively small models due to
computational constraints. Our findings are: (i) on-policy RL significantly
outperforms supervised fine-tuning (SFT), (ii) PPO-based off-policy updates
increase accuracy instead of reduce variance, and (iii) removing KL divergence
can lead to more concise generations and higher accuracy. Furthermore, we find
that a key bottleneck to computational efficiency is that the optimal batch
sizes for inference and backpropagation are different. We propose a novel
algorithm, DASH, that performs preemptive sampling (i.e., sample a large batch
and accumulate gradient updates in small increments), and gradient filtering
(i.e., drop samples with small advantage estimates). We show that DASH reduces
training time by 83% compared to a standard implementation of GRPO without
sacrificing accuracy. Our findings provide valuable insights on designing
effective RL algorithms for LM reasoning.

</details>


### [455] [Reasoning Model is Stubborn: Diagnosing Instruction Overriding in Reasoning Models](https://arxiv.org/pdf/2505.17225)
*Doohyuk Jang, Yoonjeon Kim, Chanjae Park, Hyun Ryu, Eunho Yang*

Main category: cs.AI

TL;DR: The paper investigates "reasoning rigidity" in large language models, where they default to familiar patterns despite explicit instructions, leading to errors. A diagnostic dataset is introduced to study and categorize this behavior.


<details>
  <summary>Details</summary>
Motivation: To address the issue of reasoning rigidity in language models, particularly in domains requiring precise adherence to constraints like mathematics and logic puzzles.

Method: Introduces a diagnostic dataset with modified benchmarks (AIME, MATH500) and redesigned puzzles to study reasoning rigidity. Identifies three contamination patterns.

Result: Three modes of reasoning rigidity are identified: Interpretation Overload, Input Distrust, and Partial Instruction Attention.

Conclusion: The diagnostic dataset is released to aid future research in mitigating reasoning rigidity in language models.

Abstract: Large language models have demonstrated remarkable proficiency in long and
complex reasoning tasks. However, they frequently exhibit a problematic
reliance on familiar reasoning patterns, a phenomenon we term \textit{reasoning
rigidity}. Despite explicit instructions from users, these models often
override clearly stated conditions and default to habitual reasoning
trajectories, leading to incorrect conclusions. This behavior presents
significant challenges, particularly in domains such as mathematics and logic
puzzle, where precise adherence to specified constraints is critical. To
systematically investigate reasoning rigidity, a behavior largely unexplored in
prior work, we introduce a expert-curated diagnostic set, \dataset{}. Our
dataset includes specially modified variants of existing mathematical
benchmarks, namely AIME and MATH500, as well as well-known puzzles deliberately
redesigned to require deviation from familiar reasoning strategies. Using this
dataset, we identify recurring contamination patterns that occur when models
default to ingrained reasoning. Specifically, we categorize this contamination
into three distinctive modes: (i) Interpretation Overload, (ii) Input Distrust,
and (iii) Partial Instruction Attention, each causing models to ignore or
distort provided instructions. We publicly release our diagnostic set to
facilitate future research on mitigating reasoning rigidity in language models.

</details>


### [456] [Where You Go is Who You Are: Behavioral Theory-Guided LLMs for Inverse Reinforcement Learning](https://arxiv.org/pdf/2505.17249)
*Yuran Sun, Susu Xu, Chenguang Wang, Xilei Zhao*

Main category: cs.AI

TL;DR: SILIC is a framework using LLM-guided IRL and CCR to infer sociodemographics from mobility data, outperforming existing methods by modeling cognitive processes.


<details>
  <summary>Details</summary>
Motivation: Big trajectory data lacks sociodemographic details, limiting mobility analysis. Existing methods ignore cognitive mechanisms and have low accuracy.

Method: SILIC combines LLMs, IRL, and CCR to model cognitive processes (using TPB) and improve reward function initialization in IRL.

Result: Tested on the 2017 Puget Sound survey, SILIC outperforms state-of-the-art baselines.

Conclusion: SILIC enriches trajectory data, supporting behaviorally grounded applications in transportation planning.

Abstract: Big trajectory data hold great promise for human mobility analysis, but their
utility is often constrained by the absence of critical traveler attributes,
particularly sociodemographic information. While prior studies have explored
predicting such attributes from mobility patterns, they often overlooked
underlying cognitive mechanisms and exhibited low predictive accuracy. This
study introduces SILIC, short for Sociodemographic Inference with LLM-guided
Inverse Reinforcement Learning (IRL) and Cognitive Chain Reasoning (CCR), a
theoretically grounded framework that leverages LLMs to infer sociodemographic
attributes from observed mobility patterns by capturing latent behavioral
intentions and reasoning through psychological constructs. Particularly, our
approach explicitly follows the Theory of Planned Behavior (TPB), a
foundational behavioral framework in transportation research, to model
individuals' latent cognitive processes underlying travel decision-making. The
LLMs further provide heuristic guidance to improve IRL reward function
initialization and update by addressing its ill-posedness and optimization
challenges arising from the vast and unstructured reward space. Evaluated in
the 2017 Puget Sound Regional Council Household Travel Survey, our method
substantially outperforms state-of-the-art baselines and shows great promise
for enriching big trajectory data to support more behaviorally grounded
applications in transportation planning and beyond.

</details>


### [457] [AdaReasoner: Adaptive Reasoning Enables More Flexible Thinking](https://arxiv.org/pdf/2505.17312)
*Xiangqi Wang, Yue Huang, Yanbo Wang, Xiaonan Luo, Kehan Guo, Yujun Zhou, Xiangliang Zhang*

Main category: cs.AI

TL;DR: AdaReasoner is an LLM-agnostic plugin that automates adaptive reasoning configurations for tasks, outperforming fixed-configuration baselines.


<details>
  <summary>Details</summary>
Motivation: Existing prompting approaches use fixed configurations, which lack task-specific optimality. AdaReasoner addresses this gap.

Method: Uses reinforcement learning with a factorized action space, targeted exploration, and a pretrained reward model to optimize reasoning configurations.

Result: Outperforms baselines across six LLMs and various tasks, with fast convergence and robustness.

Conclusion: AdaReasoner effectively automates and optimizes reasoning configurations for diverse tasks.

Abstract: LLMs often need effective configurations, like temperature and reasoning
steps, to handle tasks requiring sophisticated reasoning and problem-solving,
ranging from joke generation to mathematical reasoning. Existing prompting
approaches usually adopt general-purpose, fixed configurations that work 'well
enough' across tasks but seldom achieve task-specific optimality. To address
this gap, we introduce AdaReasoner, an LLM-agnostic plugin designed for any LLM
to automate adaptive reasoning configurations for tasks requiring different
types of thinking. AdaReasoner is trained using a reinforcement learning (RL)
framework, combining a factorized action space with a targeted exploration
strategy, along with a pretrained reward model to optimize the policy model for
reasoning configurations with only a few-shot guide. AdaReasoner is backed by
theoretical guarantees and experiments of fast convergence and a sublinear
policy gap. Across six different LLMs and a variety of reasoning tasks, it
consistently outperforms standard baselines, preserves out-of-distribution
robustness, and yield gains on knowledge-intensive tasks through tailored
prompts.

</details>


### [458] [Longer Context, Deeper Thinking: Uncovering the Role of Long-Context Ability in Reasoning](https://arxiv.org/pdf/2505.17315)
*Wang Yang, Zirui Liu, Hongye Jin, Qingyu Yin, Vipin Chaudhary, Xiaotian Han*

Main category: cs.AI

TL;DR: Enhancing long-context capacity before SFT improves reasoning performance, even for short-input tasks.


<details>
  <summary>Details</summary>
Motivation: Current reasoning limitations may stem from insufficient long-context capacity, supported by empirical observations.

Method: Compare models with identical architectures and fine-tuning data but varying long-context capacities.

Result: Models with stronger long-context capacity achieve higher reasoning accuracy post-SFT, even on short-input tasks.

Conclusion: Long-context capacity is critical for reasoning and should be prioritized in future model design.

Abstract: Recent language models exhibit strong reasoning capabilities, yet the
influence of long-context capacity on reasoning remains underexplored. In this
work, we hypothesize that current limitations in reasoning stem, in part, from
insufficient long-context capacity, motivated by empirical observations such as
(1) higher context window length often leads to stronger reasoning performance,
and (2) failed reasoning cases resemble failed long-context cases. To test this
hypothesis, we examine whether enhancing a model's long-context ability before
Supervised Fine-Tuning (SFT) leads to improved reasoning performance.
Specifically, we compared models with identical architectures and fine-tuning
data but varying levels of long-context capacity. Our results reveal a
consistent trend: models with stronger long-context capacity achieve
significantly higher accuracy on reasoning benchmarks after SFT. Notably, these
gains persist even on tasks with short input lengths, indicating that
long-context training offers generalizable benefits for reasoning performance.
These findings suggest that long-context modeling is not just essential for
processing lengthy inputs, but also serves as a critical foundation for
reasoning. We advocate for treating long-context capacity as a first-class
objective in the design of future language models.

</details>


### [459] [Partner Modelling Emerges in Recurrent Agents (But Only When It Matters)](https://arxiv.org/pdf/2505.17323)
*Ruaridh Mon-Williams, Max Taylor-Davies, Elizabeth Mieczkowski, Natalia Velez, Neil R. Bramley, Yanwei Wang, Thomas L. Griffiths, Christopher G. Lucas*

Main category: cs.AI

TL;DR: Simple RNN agents develop internal models of partners' abilities for collaboration, emerging from cooperative pressures without explicit mechanisms.


<details>
  <summary>Details</summary>
Motivation: Understand if flexible collaboration in AI requires dedicated mechanisms for modeling others or emerges from cooperative interaction.

Method: Train model-free RNN agents in the 'Overcooked-AI' environment, analyze hidden states and behavior with diverse partners.

Result: Agents form structured internal representations of partners, enabling adaptation to new collaborators, influenced by task allocation control.

Conclusion: Partner modeling arises spontaneously in model-free agents under specific social pressures, not requiring explicit mechanisms.

Abstract: Humans are remarkably adept at collaboration, able to infer the strengths and
weaknesses of new partners in order to work successfully towards shared goals.
To build AI systems with this capability, we must first understand its building
blocks: does such flexibility require explicit, dedicated mechanisms for
modelling others -- or can it emerge spontaneously from the pressures of
open-ended cooperative interaction? To investigate this question, we train
simple model-free RNN agents to collaborate with a population of diverse
partners. Using the `Overcooked-AI' environment, we collect data from thousands
of collaborative teams, and analyse agents' internal hidden states. Despite a
lack of additional architectural features, inductive biases, or auxiliary
objectives, the agents nevertheless develop structured internal representations
of their partners' task abilities, enabling rapid adaptation and generalisation
to novel collaborators. We investigated these internal models through probing
techniques, and large-scale behavioural analysis. Notably, we find that
structured partner modelling emerges when agents can influence partner
behaviour by controlling task allocation. Our results show that partner
modelling can arise spontaneously in model-free agents -- but only under
environmental conditions that impose the right kind of social pressure.

</details>


### [460] [DEL-ToM: Inference-Time Scaling for Theory-of-Mind Reasoning via Dynamic Epistemic Logic](https://arxiv.org/pdf/2505.17348)
*Yuheng Wu, Jianwen Xie, Denghui Zhang, Zhaozhuo Xu*

Main category: cs.AI

TL;DR: DEL-ToM improves Theory-of-Mind reasoning in small language models (SLMs) using inference-time scaling and verifiable belief supervision, without architectural changes.


<details>
  <summary>Details</summary>
Motivation: Small language models struggle with deep social reasoning in ToM tasks due to limited scale. DEL-ToM aims to enhance their performance without retraining.

Method: Decomposes ToM tasks into belief updates using Dynamic Epistemic Logic (DEL), trains a Process Belief Model (PBM) to score updates, and selects the highest-scoring trace during inference.

Result: DEL-ToM consistently improves ToM performance across model scales and benchmarks, showing verifiable belief supervision enhances SLMs.

Conclusion: DEL-ToM enables SLMs to perform better ToM reasoning by leveraging structured, transparent belief updates and additional compute at inference time.

Abstract: Theory-of-Mind (ToM) tasks pose a unique challenge for small language models
(SLMs) with limited scale, which often lack the capacity to perform deep social
reasoning. In this work, we propose DEL-ToM, a framework that improves ToM
reasoning through inference-time scaling rather than architectural changes. Our
approach decomposes ToM tasks into a sequence of belief updates grounded in
Dynamic Epistemic Logic (DEL), enabling structured and transparent reasoning.
We train a verifier, called the Process Belief Model (PBM), to score each
belief update step using labels generated automatically via a DEL simulator.
During inference, candidate belief traces generated by a language model are
evaluated by the PBM, and the highest-scoring trace is selected. This allows
SLMs to emulate more deliberate reasoning by allocating additional compute at
test time. Experiments across multiple model scales and benchmarks show that
DEL-ToM consistently improves performance, demonstrating that verifiable belief
supervision can significantly enhance ToM abilities of SLMs without retraining.

</details>


### [461] [Misaligning Reasoning with Answers -- A Framework for Assessing LLM CoT Robustness](https://arxiv.org/pdf/2505.17406)
*Enyi Jiang, Changming Xu, Nischay Singh, Gagandeep Singh*

Main category: cs.AI

TL;DR: MATCHA evaluates LLM reasoning under input perturbations, showing inconsistencies, especially in multi-step and commonsense tasks, and aids in improving model robustness.


<details>
  <summary>Details</summary>
Motivation: The opacity of LLMs' decision-making necessitates explanation techniques like Chain-of-Thought to ensure trustworthiness in critical domains like education and healthcare.

Method: A novel evaluation framework, MATCHA, is designed to investigate the relationship between answers and reasoning, using LLM judges to assess reasoning robustness.

Result: LLMs exhibit inconsistent or nonsensical reasoning under perturbations, with greater vulnerability in multi-step and commonsense tasks. Non-trivial transfer rates to black-box models are observed.

Conclusion: MATCHA enhances understanding of LLM reasoning, guiding future models toward robust, reasoning-driven architectures with answer-reasoning consistency.

Abstract: LLMs' decision-making process is opaque, prompting the need for explanation
techniques like Chain-of-Thought. To investigate the relationship between
answer and reasoning, we design a novel evaluation framework, MATCHA. In
domains like education and healthcare, reasoning is key for model
trustworthiness. MATCHA reveals that LLMs under input perturbations can give
inconsistent or nonsensical reasoning. Additionally, we use LLM judges to
assess reasoning robustness across models. Our results show that LLMs exhibit
greater vulnerability to input perturbations for multi-step and commonsense
tasks than compared to logical tasks. Also, we show non-trivial transfer rates
of our successful examples to black-box models. Our evaluation framework helps
to better understand LLM reasoning mechanisms and guides future models toward
more robust and reasoning-driven architectures, enforcing answer-reasoning
consistency.

</details>


### [462] [MemeReaCon: Probing Contextual Meme Understanding in Large Vision-Language Models](https://arxiv.org/pdf/2505.17433)
*Zhengyi Zhao, Shubo Zhang, Yuxi Zhang, Yanxi Zhao, Yifan Zhang, Zezhong Wang, Huimin Wang, Yutian Zhao, Bin Liang, Yefeng Zheng, Binyang Li, Kam-Fai Wong, Xian Wu*

Main category: cs.AI

TL;DR: MemeReaCon is a benchmark for evaluating LVLMs' ability to understand context-dependent meme intent, highlighting current model weaknesses.


<details>
  <summary>Details</summary>
Motivation: Current meme analysis overlooks context-dependent interpretation, creating a gap in LVLMs' understanding.

Method: Collected and labeled memes from Reddit, including image, post text, and comments, to test LVLMs.

Result: LVLMs struggle with context, either missing critical information or focusing too much on visuals.

Conclusion: MemeReaCon exposes LVLM limitations and aims to drive development of context-aware models.

Abstract: Memes have emerged as a popular form of multimodal online communication,
where their interpretation heavily depends on the specific context in which
they appear. Current approaches predominantly focus on isolated meme analysis,
either for harmful content detection or standalone interpretation, overlooking
a fundamental challenge: the same meme can express different intents depending
on its conversational context. This oversight creates an evaluation gap:
although humans intuitively recognize how context shapes meme interpretation,
Large Vision Language Models (LVLMs) can hardly understand context-dependent
meme intent. To address this critical limitation, we introduce MemeReaCon, a
novel benchmark specifically designed to evaluate how LVLMs understand memes in
their original context. We collected memes from five different Reddit
communities, keeping each meme's image, the post text, and user comments
together. We carefully labeled how the text and meme work together, what the
poster intended, how the meme is structured, and how the community responded.
Our tests with leading LVLMs show a clear weakness: models either fail to
interpret critical information in the contexts, or overly focus on visual
details while overlooking communicative purpose. MemeReaCon thus serves both as
a diagnostic tool exposing current limitations and as a challenging benchmark
to drive development toward more sophisticated LVLMs of the context-aware
understanding.

</details>


### [463] [Scaling Up Biomedical Vision-Language Models: Fine-Tuning, Instruction Tuning, and Multi-Modal Learning](https://arxiv.org/pdf/2505.17436)
*Cheng Peng, Kai Zhang, Mengxian Lyu, Hongfang Liu, Lichao Sun, Yonghui Wu*

Main category: cs.AI

TL;DR: The paper develops two biomedical vision-language models (BiomedGPT-Large and BiomedGPT-XLarge) through scaling, fine-tuning, and instruction tuning, achieving improved performance across diverse biomedical tasks.


<details>
  <summary>Details</summary>
Motivation: To enhance biomedical vision-language models by scaling, fine-tuning, and instruction tuning for better handling of long text and multi-modal tasks, while evaluating zero-shot learning.

Method: Developed two transformer-based models, fine-tuned on 23 datasets across 6 biomedical tasks, and instruction-tuned using a large dataset. Compared performance with existing models.

Result: Improved performance in multi-modal biomedical tasks and zero-shot learning compared to previous models and literature benchmarks.

Conclusion: The scaled and instruction-tuned models demonstrate superior capabilities in biomedical vision-language tasks, highlighting the effectiveness of the proposed approach.

Abstract: To advance biomedical vison-language model capabilities through scaling up,
fine-tuning, and instruction tuning, develop vision-language models with
improved performance in handling long text, explore strategies to efficiently
adopt vision language models for diverse multi-modal biomedical tasks, and
examine the zero-shot learning performance.
  We developed two biomedical vision language models, BiomedGPT-Large and
BiomedGPT-XLarge, based on an encoder-decoder-based transformer architecture.
We fine-tuned the two models on 23 benchmark datasets from 6 multi-modal
biomedical tasks including one image-only task (image classification), three
language-only tasks (text understanding, text summarization and question
answering), and two vision-language tasks (visual question answering and image
captioning). We compared the developed scaled models with our previous
BiomedGPT-Base model and existing prestigious models reported in the
literature. We instruction-tuned the two models using a large-scale multi-modal
biomedical instruction-tuning dataset and assessed the zero-shot learning
performance and alignment accuracy.

</details>


### [464] [From Reasoning to Generalization: Knowledge-Augmented LLMs for ARC Benchmark](https://arxiv.org/pdf/2505.17482)
*Chao Lei, Nir Lipovetzky, Krista A. Ehinger, Yanchuan Chang*

Main category: cs.AI

TL;DR: The paper evaluates reasoning-oriented LLMs on the ARC benchmark, proposing RSPC and KAAR methods to enhance abstract reasoning and generalization, achieving significant performance improvements.


<details>
  <summary>Details</summary>
Motivation: To explore and improve the underexplored cognitive faculties of abstract reasoning and generalization in reasoning-oriented LLMs.

Method: Formulates ARC as a program synthesis task, introduces RSPC for code generation, and KAAR for knowledge augmentation with hierarchical priors.

Result: KAAR outperforms non-augmented RSPC, achieving ~5% absolute gains and up to 64.52% relative improvement.

Conclusion: ARC remains challenging, but KAAR and RSPC demonstrate promising advancements for future LLM reasoning capabilities.

Abstract: Recent reasoning-oriented LLMs have demonstrated strong performance on
challenging tasks such as mathematics and science examinations. However, core
cognitive faculties of human intelligence, such as abstract reasoning and
generalization, remain underexplored. To address this, we evaluate recent
reasoning-oriented LLMs on the Abstraction and Reasoning Corpus (ARC)
benchmark, which explicitly demands both faculties. We formulate ARC as a
program synthesis task and propose nine candidate solvers. Experimental results
show that repeated-sampling planning-aided code generation (RSPC) achieves the
highest test accuracy and demonstrates consistent generalization across most
LLMs. To further improve performance, we introduce an ARC solver, Knowledge
Augmentation for Abstract Reasoning (KAAR), which encodes core knowledge priors
within an ontology that classifies priors into three hierarchical levels based
on their dependencies. KAAR progressively expands LLM reasoning capacity by
gradually augmenting priors at each level, and invokes RSPC to generate
candidate solutions after each augmentation stage. This stage-wise reasoning
reduces interference from irrelevant priors and improves LLM performance.
Empirical results show that KAAR maintains strong generalization and
consistently outperforms non-augmented RSPC across all evaluated LLMs,
achieving around 5% absolute gains and up to 64.52% relative improvement.
Despite these achievements, ARC remains a challenging benchmark for
reasoning-oriented LLMs, highlighting future avenues of progress in LLMs.

</details>


### [465] [PD$^3$: A Project Duplication Detection Framework via Adapted Multi-Agent Debate](https://arxiv.org/pdf/2505.17492)
*Dezheng Bao, Yueci Yang, Xin Chen, Zhengxuan Jiang, Zeguo Fei, Daoze Zhang, Xuanwen Huang, Junru Chen, Chutian Yu, Xiang Yuan, Yang Yang*

Main category: cs.AI

TL;DR: PD$^3$ is a framework for detecting project duplication using multi-agent debate, outperforming existing methods by 7.43-8.00% and saving costs.


<details>
  <summary>Details</summary>
Motivation: Existing methods lack depth and valuable insights for experts, necessitating a better approach for project duplication detection.

Method: PD$^3$ employs multi-agent debate inspired by real-world expert debates, combining qualitative and quantitative analysis.

Result: Outperforms existing methods by 7.43-8.00% and saves 5.73 million USD in initial detection costs.

Conclusion: PD$^3$ is effective and practical, with demonstrated success in real-world applications.

Abstract: Project duplication detection is critical for project quality assessment, as
it improves resource utilization efficiency by preventing investing in newly
proposed project that have already been studied. It requires the ability to
understand high-level semantics and generate constructive and valuable
feedback. Existing detection methods rely on basic word- or sentence-level
comparison or solely apply large language models, lacking valuable insights for
experts and in-depth comprehension of project content and review criteria. To
tackle this issue, we propose PD$^3$, a Project Duplication Detection framework
via adapted multi-agent Debate. Inspired by real-world expert debates, it
employs a fair competition format to guide multi-agent debate to retrieve
relevant projects. For feedback, it incorporates both qualitative and
quantitative analysis to improve its practicality. Over 800 real-world power
project data spanning more than 20 specialized fields are used to evaluate the
framework, demonstrating that our method outperforms existing approaches by
7.43% and 8.00% in two downstream tasks. Furthermore, we establish an online
platform, Review Dingdang, to assist power experts, saving 5.73 million USD in
initial detection on more than 100 newly proposed projects.

</details>


### [466] [Probe by Gaming: A Game-based Benchmark for Assessing Conceptual Knowledge in LLMs](https://arxiv.org/pdf/2505.17512)
*Shuhang Xu, Weijian Deng, Yixuan Zhou, Fangwei Zhong*

Main category: cs.AI

TL;DR: CK-Arena is a multi-agent game designed to evaluate LLMs' conceptual reasoning by simulating interactive tasks, revealing variability in their understanding across categories.


<details>
  <summary>Details</summary>
Motivation: Existing benchmarks fail to assess LLMs' comprehension of conceptual boundaries, prompting the need for a dynamic evaluation tool like CK-Arena.

Method: CK-Arena uses the Undercover game framework to test LLMs' ability to describe, differentiate, and infer concepts based on partial information.

Result: LLMs' conceptual understanding varies by category and doesn't strictly correlate with model size or general capabilities.

Conclusion: CK-Arena offers a scalable, realistic benchmark for evaluating conceptual reasoning in LLMs, highlighting gaps in their comprehension.

Abstract: Concepts represent generalized abstractions that enable humans to categorize
and reason efficiently, yet it is unclear to what extent Large Language Models
(LLMs) comprehend these semantic relationships. Existing benchmarks typically
focus on factual recall and isolated tasks, failing to evaluate the ability of
LLMs to understand conceptual boundaries. To address this gap, we introduce
CK-Arena, a multi-agent interaction game built upon the Undercover game,
designed to evaluate the capacity of LLMs to reason with concepts in
interactive settings. CK-Arena challenges models to describe, differentiate,
and infer conceptual boundaries based on partial information, encouraging
models to explore commonalities and distinctions between closely related
concepts. By simulating real-world interaction, CK-Arena provides a scalable
and realistic benchmark for assessing conceptual reasoning in dynamic
environments. Experimental results show that LLMs' understanding of conceptual
knowledge varies significantly across different categories and is not strictly
aligned with parameter size or general model capabilities. The data and code
are available at the project homepage: https://ck-arena.site.

</details>


### [467] [Optimizing Retrieval-Augmented Generation for Electrical Engineering: A Case Study on ABB Circuit Breakers](https://arxiv.org/pdf/2505.17520)
*Salahuddin Alawadhi, Noorhan Abbas*

Main category: cs.AI

TL;DR: The study explores integrating RAG with LLMs for ABB circuit breakers, focusing on accuracy and reliability in engineering. It evaluates three RAG pipelines and advanced chunking methods, highlighting challenges in factual faithfulness.


<details>
  <summary>Details</summary>
Motivation: To enhance precision and contextual relevance in high-stakes engineering tasks like troubleshooting and design using RAG and LLMs.

Method: Leveraged tailored datasets, advanced embedding models, and optimized chunking strategies to evaluate three RAG pipelines (OpenAI GPT4o, Cohere, Anthropic Claude) and assess paragraph-based and title-aware segmentation.

Result: Certain configurations achieved high precision and relevancy, but limitations in factual faithfulness and completeness remain.

Conclusion: Iterative improvements in RAG systems are needed to meet stringent engineering demands, advancing AI research in technical domains like electrical engineering.

Abstract: Integrating Retrieval Augmented Generation (RAG) with Large Language Models
(LLMs) has shown the potential to provide precise, contextually relevant
responses in knowledge intensive domains. This study investigates the
ap-plication of RAG for ABB circuit breakers, focusing on accuracy,
reliability, and contextual relevance in high-stakes engineering environments.
By leveraging tailored datasets, advanced embedding models, and optimized
chunking strategies, the research addresses challenges in data retrieval and
contextual alignment unique to engineering documentation. Key contributions
include the development of a domain-specific dataset for ABB circuit breakers
and the evaluation of three RAG pipelines: OpenAI GPT4o, Cohere, and Anthropic
Claude. Advanced chunking methods, such as paragraph-based and title-aware
segmentation, are assessed for their impact on retrieval accuracy and response
generation. Results demonstrate that while certain configurations achieve high
precision and relevancy, limitations persist in ensuring factual faithfulness
and completeness, critical in engineering contexts. This work underscores the
need for iterative improvements in RAG systems to meet the stringent demands of
electrical engineering tasks, including design, troubleshooting, and
operational decision-making. The findings in this paper help advance research
of AI in highly technical domains such as electrical engineering.

</details>


### [468] [Transparency and Proportionality in Post-Processing Algorithmic Bias Correction](https://arxiv.org/pdf/2505.17525)
*Juliett Suárez Ferreira, Marija Slavkovik, Jorge Casillas*

Main category: cs.AI

TL;DR: The paper examines unintended consequences of post-processing debiasing techniques in algorithmic decision-making, proposing measures to quantify and address disparities in fairness interventions.


<details>
  <summary>Details</summary>
Motivation: Algorithmic decision-making systems often produce biased or unfair results, and debiasing practices can introduce new inequities. The study aims to mitigate these issues by analyzing post-processing techniques.

Method: The authors develop measures to quantify disparity in post-processing debiasing, introducing a methodology for applying these metrics to assess proportionality, transparency, and alternative approaches.

Result: The proposed measures help practitioners evaluate debiasing strategies, explain their effects, and explore other mitigation methods, demonstrated through a practical example.

Conclusion: Analyzing debiasing proportionality complements traditional fairness metrics, offering a deeper perspective to ensure fairer outcomes across all groups.

Abstract: Algorithmic decision-making systems sometimes produce errors or skewed
predictions toward a particular group, leading to unfair results. Debiasing
practices, applied at different stages of the development of such systems,
occasionally introduce new forms of unfairness or exacerbate existing
inequalities. We focus on post-processing techniques that modify algorithmic
predictions to achieve fairness in classification tasks, examining the
unintended consequences of these interventions. To address this challenge, we
develop a set of measures that quantify the disparity in the flips applied to
the solution in the post-processing stage. The proposed measures will help
practitioners: (1) assess the proportionality of the debiasing strategy used,
(2) have transparency to explain the effects of the strategy in each group, and
(3) based on those results, analyze the possibility of the use of some other
approaches for bias mitigation or to solve the problem. We introduce a
methodology for applying the proposed metrics during the post-processing stage
and illustrate its practical application through an example. This example
demonstrates how analyzing the proportionality of the debiasing strategy
complements traditional fairness metrics, providing a deeper perspective to
ensure fairer outcomes across all groups.

</details>


### [469] [USTBench: Benchmarking and Dissecting Spatiotemporal Reasoning of LLMs as Urban Agents](https://arxiv.org/pdf/2505.17572)
*Siqi Lai, Yansong Ning, Zirui Yuan, Zhixi Chen, Hao Liu*

Main category: cs.AI

TL;DR: USTBench is introduced to evaluate LLMs' spatiotemporal reasoning in urban tasks, revealing their strengths and limitations, and highlighting the need for domain-specific adaptation.


<details>
  <summary>Details</summary>
Motivation: Existing studies focus on outcome-level metrics, lacking insight into LLMs' reasoning processes in urban contexts.

Method: USTBench evaluates LLMs across four dimensions (understanding, forecasting, planning, reflection) using 62,466 QA pairs and standardized tasks in UAgentEnv.

Result: LLMs show potential but struggle with long-horizon planning and reflection; general reasoning models don't outperform non-reasoning ones.

Conclusion: USTBench lays groundwork for developing adaptive LLM-based urban agents, emphasizing domain-specific improvements.

Abstract: Large language models (LLMs) have shown emerging potential in spatiotemporal
reasoning, making them promising candidates for building urban agents that
support diverse urban downstream applications. Despite these benefits, existing
studies primarily focus on evaluating urban LLM agent on outcome-level metrics
(e.g., prediction accuracy, traffic efficiency), offering limited insight into
their underlying reasoning processes. As a result, the strengths and
limitations of urban LLM agents in spatiotemporal reasoning remain poorly
understood. To this end, we introduce USTBench, the first benchmark to evaluate
LLMs' spatiotemporal reasoning abilities as urban agents across four decomposed
dimensions: spatiotemporal understanding, forecasting, planning, and reflection
with feedback. Specifically, USTBench supports five diverse urban
decision-making and four spatiotemporal prediction tasks, all running within
our constructed interactive city environment UAgentEnv. The benchmark includes
62,466 structured QA pairs for process-level evaluation and standardized
end-to-end task assessments, enabling fine-grained diagnostics and broad
task-level comparison across diverse urban scenarios. Through extensive
evaluation of thirteen leading LLMs, we reveal that although LLMs show
promising potential across various urban downstream tasks, they still struggle
in long-horizon planning and reflective adaptation in dynamic urban contexts.
Notably, recent advanced reasoning models (e.g., DeepSeek-R1) trained on
general logic or mathematical problems do not consistently outperform
non-reasoning LLMs. This discrepancy highlights the need for domain-specialized
adaptation methods to enhance urban spatiotemporal reasoning. Overall, USTBench
provides a foundation to build more adaptive and effective LLM-based urban
agents and broad smart city applications.

</details>


### [470] [Controlled Agentic Planning & Reasoning for Mechanism Synthesis](https://arxiv.org/pdf/2505.17607)
*João Pedro Gandarela, Thiago Rios, Stefan Menzel, André Freitas*

Main category: cs.AI

TL;DR: A dual-agent LLM-based method for mechanism synthesis combines linguistic and symbolic reasoning, generating geometrical and dynamic outcomes. It uses functions to refine specifications, simulate code, and apply symbolic regression, proving effective for planar mechanisms. The MSynth benchmark is introduced, and symbolic regression's impact is analyzed.


<details>
  <summary>Details</summary>
Motivation: To enhance mechanism synthesis by integrating linguistic and symbolic reasoning in LLMs, enabling more effective and convergent outcomes for planar mechanisms.

Method: The model uses functions to process natural language specifications, generate simulation code, and refine results via symbolic regression and distance functions, creating a closed-loop refinement process.

Result: The approach is effective and convergent for planar mechanisms, with symbolic regression providing mechanistic insights in large architectures.

Conclusion: The dual-agent LLM method improves mechanism synthesis, validated by the MSynth benchmark, and highlights the importance of symbolic regression in large models.

Abstract: This work presents a dual-agent Large Language Model (LLM)-based reasoning
method for mechanism synthesis, capable of reasoning at both linguistic and
symbolic levels to generate geometrical and dynamic outcomes. The model
consists of a composition of well-defined functions that, starting from a
natural language specification, references abstract properties through
supporting equations, generates and parametrizes simulation code, and elicits
feedback anchor points using symbolic regression and distance functions. This
process closes an actionable refinement loop at the linguistic and symbolic
layers. The approach is shown to be both effective and convergent in the
context of planar mechanisms. Additionally, we introduce MSynth, a novel
benchmark for planar mechanism synthesis, and perform a comprehensive analysis
of the impact of the model components. We further demonstrate that symbolic
regression prompts unlock mechanistic insights only when applied to
sufficiently large architectures.

</details>


### [471] [Decoupled Visual Interpretation and Linguistic Reasoning for Math Problem Solving](https://arxiv.org/pdf/2505.17609)
*Zixian Guo, Ming Liu, Zhilong Ji, Jinfeng Bai, Lei Zhang, Wangmeng Zuo*

Main category: cs.AI

TL;DR: The paper proposes a decoupled reasoning framework for vision-language tasks, using separate visual interpretation and LLM modules instead of end-to-end training, achieving better performance and cost efficiency.


<details>
  <summary>Details</summary>
Motivation: Current LVLMs struggle with complex vision-language reasoning tasks and lag behind LLMs. The authors aim to improve reasoning by leveraging existing specialized models.

Method: The approach decouples vision-language reasoning: (1) a vision-language model converts images to text, and (2) an LLM reasons using the text and question. Joint-tuning optimizes collaboration.

Result: The framework outperforms recent LVLMs on benchmarks, especially in visually intensive tasks like geometric mathematics.

Conclusion: Decoupling vision and language reasoning is more efficient and effective, enabling future upgrades to LLMs without full model retraining.

Abstract: Current large vision-language models (LVLMs) typically employ a connector
module to link visual features with text embeddings of large language models
(LLMs) and use end-to-end training to achieve multi-modal understanding in a
unified process. Well alignment needs high-quality pre-training data and a
carefully designed training process. Current LVLMs face challenges when
addressing complex vision-language reasoning tasks, with their reasoning
capabilities notably lagging behind those of LLMs. This paper proposes a
paradigm shift: instead of training end-to-end vision-language reasoning
models, we advocate for developing a decoupled reasoning framework based on
existing visual interpretation specialists and text-based reasoning LLMs. Our
approach leverages (1) a dedicated vision-language model to transform the
visual content of images into textual descriptions and (2) an LLM to perform
reasoning according to the visual-derived text and the original question. This
method presents a cost-efficient solution for multi-modal model development by
optimizing existing models to work collaboratively, avoiding end-to-end
development of vision-language models from scratch. By transforming images into
language model-compatible text representations, it facilitates future low-cost
and flexible upgrades to upcoming powerful LLMs. We introduce an
outcome-rewarded joint-tuning strategy to optimize the cooperation between the
visual interpretation and linguistic reasoning model. Evaluation results on
vision-language benchmarks demonstrate that the decoupled reasoning framework
outperforms recent LVLMs. Our approach yields particularly significant
performance gains on visually intensive geometric mathematics problems. The
code is available: https://github.com/guozix/DVLR.

</details>


### [472] [MMMG: a Comprehensive and Reliable Evaluation Suite for Multitask Multimodal Generation](https://arxiv.org/pdf/2505.17613)
*Jihan Yao, Yushi Hu, Yujie Yi, Bin Han, Shangbin Feng, Guang Yang, Bingbing Wen, Ranjay Krishna, Lucy Lu Wang, Yulia Tsvetkov, Noah A. Smith, Banghua Zhu*

Main category: cs.AI

TL;DR: MMMG is a human-aligned benchmark for multimodal generation, covering 4 modality combinations and 49 tasks, achieving 94.3% agreement with human evaluation. It reveals gaps in state-of-the-art models like GPT Image, especially in multimodal reasoning and audio generation.


<details>
  <summary>Details</summary>
Motivation: Automated metrics often misalign with human evaluation for complex multimodal tasks, necessitating a reliable benchmark like MMMG.

Method: MMMG includes 49 tasks (29 new) with tailored evaluation pipelines and 937 instructions to assess reasoning, controllability, and other capabilities.

Result: MMMG achieves 94.3% human alignment. GPT Image scores 78.3% on image generation but struggles with multimodal reasoning and interleaved generation. Audio generation shows significant room for improvement.

Conclusion: MMMG provides a robust benchmark for multimodal generation, highlighting current model limitations and future research directions, particularly in audio generation.

Abstract: Automatically evaluating multimodal generation presents a significant
challenge, as automated metrics often struggle to align reliably with human
evaluation, especially for complex tasks that involve multiple modalities. To
address this, we present MMMG, a comprehensive and human-aligned benchmark for
multimodal generation across 4 modality combinations (image, audio, interleaved
text and image, interleaved text and audio), with a focus on tasks that present
significant challenges for generation models, while still enabling reliable
automatic evaluation through a combination of models and programs. MMMG
encompasses 49 tasks (including 29 newly developed ones), each with a carefully
designed evaluation pipeline, and 937 instructions to systematically assess
reasoning, controllability, and other key capabilities of multimodal generation
models. Extensive validation demonstrates that MMMG is highly aligned with
human evaluation, achieving an average agreement of 94.3%. Benchmarking results
on 24 multimodal generation models reveal that even though the state-of-the-art
model, GPT Image, achieves 78.3% accuracy for image generation, it falls short
on multimodal reasoning and interleaved generation. Furthermore, results
suggest considerable headroom for improvement in audio generation, highlighting
an important direction for future research.

</details>


### [473] [Does Chain-of-Thought Reasoning Really Reduce Harmfulness from Jailbreaking?](https://arxiv.org/pdf/2505.17650)
*Chengda Lu, Xiaoyu Fan, Yu Huang, Rongwu Xu, Jijie Li, Wei Xu*

Main category: cs.AI

TL;DR: CoT reasoning's dual effects on jailbreak harmfulness are explored, leading to the proposal of FicDetail, a new jailbreak method.


<details>
  <summary>Details</summary>
Motivation: To understand if CoT reasoning genuinely reduces jailbreak harmfulness, addressing underexplored mechanisms and security concerns.

Method: Rigorous theoretical analysis and practical validation via the novel FicDetail jailbreak method.

Result: CoT reasoning has dual effects on jailbreaking harmfulness, with FicDetail validating these findings.

Conclusion: CoT reasoning's impact on jailbreaking is complex, and FicDetail offers practical insights into its dual effects.

Abstract: Jailbreak attacks have been observed to largely fail against recent reasoning
models enhanced by Chain-of-Thought (CoT) reasoning. However, the underlying
mechanism remains underexplored, and relying solely on reasoning capacity may
raise security concerns. In this paper, we try to answer the question: Does CoT
reasoning really reduce harmfulness from jailbreaking? Through rigorous
theoretical analysis, we demonstrate that CoT reasoning has dual effects on
jailbreaking harmfulness. Based on the theoretical insights, we propose a novel
jailbreak method, FicDetail, whose practical performance validates our
theoretical findings.

</details>


### [474] [GeoGramBench: Benchmarking the Geometric Program Reasoning in Modern LLMs](https://arxiv.org/pdf/2505.17653)
*Shixian Luo, Zezhou Zhu, Yu Yuan, Yuncheng Yang, Lianlei Shan, Yong Wu*

Main category: cs.AI

TL;DR: The paper introduces GeoGramBench, a benchmark for evaluating LLMs' ability to translate programmatic drawing code into geometric reasoning, revealing significant deficiencies in current models.


<details>
  <summary>Details</summary>
Motivation: To explore the underexplored capability of LLMs in handling geometric spatial information expressed in procedural code.

Method: Formalizes the Program-to-Geometry task and evaluates 17 LLMs using the GeoGramBench benchmark of 500 problems categorized by geometric complexity.

Result: Advanced LLMs achieve less than 50% accuracy at the highest abstraction level, showing deficiencies in program-driven spatial reasoning.

Conclusion: GeoGramBench highlights unique challenges in symbolic-to-spatial reasoning and serves as a resource for future research.

Abstract: Geometric spatial reasoning forms the foundation of many applications in
artificial intelligence, yet the ability of large language models (LLMs) to
operate over geometric spatial information expressed in procedural code remains
underexplored. In this paper, we address this gap by formalizing the
Program-to-Geometry task, which challenges models to translate programmatic
drawing code into accurate and abstract geometric reasoning. To evaluate this
capability, we present GeoGramBench, a benchmark of 500 carefully refined
problems organized by a tailored three-level taxonomy that considers geometric
complexity rather than traditional mathematical reasoning complexity. Our
comprehensive evaluation of 17 frontier LLMs reveals consistent and pronounced
deficiencies: even the most advanced models achieve less than 50% accuracy at
the highest abstraction level. These results highlight the unique challenges
posed by program-driven spatial reasoning and establish GeoGramBench as a
valuable resource for advancing research in symbolic-to-spatial geometric
reasoning. Project page: https://github.com/LiAuto-DSR/GeoGramBench.

</details>


### [475] [Rethinking Agent Design: From Top-Down Workflows to Bottom-Up Skill Evolution](https://arxiv.org/pdf/2505.17673)
*Jiawei Du, Jinlong Wu, Yuzheng Chen, Yucheng Hu, Bing Li, Joey Tianyi Zhou*

Main category: cs.AI

TL;DR: The paper introduces a bottom-up agent paradigm for LLM-based agents, enabling experience-driven learning through trial-and-reasoning, skill abstraction, and sharing, evaluated in open-ended game environments.


<details>
  <summary>Details</summary>
Motivation: Top-down agent frameworks rely on human-defined workflows and overlook agents' learning potential. The paper aims to shift to experience-driven learning, inspired by human learning processes.

Method: Agents learn via trial-and-reasoning, exploring, reflecting, and abstracting skills. Skills are shared and extended, evaluated in Slay the Spire and Civilization V with raw visual inputs and mouse outputs.

Result: Bottom-up agents autonomously acquire skills in complex environments without game-specific prompts or APIs, demonstrating the paradigm's effectiveness.

Conclusion: The bottom-up paradigm shows promise for open-ended environments, enabling continual agent evolution through collective experiences.

Abstract: Most LLM-based agent frameworks adopt a top-down philosophy: humans decompose
tasks, define workflows, and assign agents to execute each step. While
effective on benchmark-style tasks, such systems rely on designer updates and
overlook agents' potential to learn from experience. Recently, Silver and
Sutton(2025) envision a shift into a new era, where agents could progress from
a stream of experiences. In this paper, we instantiate this vision of
experience-driven learning by introducing a bottom-up agent paradigm that
mirrors the human learning process. Agents acquire competence through a
trial-and-reasoning mechanism-exploring, reflecting on outcomes, and
abstracting skills over time. Once acquired, skills can be rapidly shared and
extended, enabling continual evolution rather than static replication. As more
agents are deployed, their diverse experiences accelerate this collective
process, making bottom-up design especially suited for open-ended environments.
We evaluate this paradigm in Slay the Spire and Civilization V, where agents
perceive through raw visual inputs and act via mouse outputs, the same as human
players. Using a unified, game-agnostic codebase without any game-specific
prompts or privileged APIs, our bottom-up agents acquire skills entirely
through autonomous interaction, demonstrating the potential of the bottom-up
paradigm in complex, real-world environments. Our code is available at
https://github.com/AngusDujw/Bottom-Up-Agent.

</details>


### [476] [Enhancing AI System Resiliency: Formulation and Guarantee for LSTM Resilience Based on Control Theory](https://arxiv.org/pdf/2505.17696)
*Sota Yoshihara, Ryousuke Yamamoto, Hiroyuki Kusumoto, Masanari Shimura*

Main category: cs.AI

TL;DR: The paper proposes methods to ensure LSTM network resilience using incremental input-to-state stability (δISS), offering data-independent evaluation and training parameter adjustments for resilience control.


<details>
  <summary>Details</summary>
Motivation: To address AI system quality assurance by mathematically defining and evaluating LSTM resilience against input perturbations.

Method: Applies δISS to LSTM networks, developing a data-independent evaluation method and resilience control via training parameter adjustments.

Result: Demonstrates resilience control and provides concrete solutions for AI quality assurance from a control theory perspective.

Conclusion: The research advances AI applications in control systems by ensuring LSTM resilience through theoretical and practical methods.

Abstract: This research proposes methods for formulating and guaranteeing the
resilience of long short-term memory (LSTM) networks, which can serve as a key
technology in AI system quality assurance. We introduce a novel methodology
applying incremental input-to-state stability ($\delta$ISS) to mathematically
define and evaluate the resilience of LSTM against input perturbations. Key
achievements include the development of a data-independent evaluation method
and the demonstration of resilience control through adjustments to training
parameters. This research presents concrete solutions to AI quality assurance
from a control theory perspective, which can advance AI applications in control
systems.

</details>


### [477] [CIKT: A Collaborative and Iterative Knowledge Tracing Framework with Large Language Models](https://arxiv.org/pdf/2505.17705)
*Runze Li, Siyu Wu, Jun Wang, Wei Zhang*

Main category: cs.AI

TL;DR: CIKT is a framework using LLMs to improve knowledge tracing by combining dynamic user profiling and iterative refinement for better accuracy and explainability.


<details>
  <summary>Details</summary>
Motivation: Addressing challenges in traditional KT methods like explainability, scalability, and modeling complex knowledge dependencies.

Method: Dual-component architecture: Analyst generates explainable user profiles, Predictor forecasts performance. Synergistic optimization loop refines both iteratively.

Result: Significant improvements in prediction accuracy, enhanced explainability, and scalability on multiple datasets.

Conclusion: CIKT bridges the gap between predictive performance and model transparency in knowledge tracing.

Abstract: Knowledge Tracing (KT) aims to model a student's learning state over time and
predict their future performance. However, traditional KT methods often face
challenges in explainability, scalability, and effective modeling of complex
knowledge dependencies. While Large Language Models (LLMs) present new avenues
for KT, their direct application often struggles with generating structured,
explainable student representations and lacks mechanisms for continuous,
task-specific refinement. To address these gaps, we propose Collaborative
Iterative Knowledge Tracing (CIKT), a framework that harnesses LLMs to enhance
both prediction accuracy and explainability. CIKT employs a dual-component
architecture: an Analyst generates dynamic, explainable user profiles from
student historical responses, and a Predictor utilizes these profiles to
forecast future performance. The core of CIKT is a synergistic optimization
loop. In this loop, the Analyst is iteratively refined based on the predictive
accuracy of the Predictor, which conditions on the generated profiles, and the
Predictor is subsequently retrained using these enhanced profiles. Evaluated on
multiple educational datasets, CIKT demonstrates significant improvements in
prediction accuracy, offers enhanced explainability through its dynamically
updated user profiles, and exhibits improved scalability. Our work presents a
robust and explainable solution for advancing knowledge tracing systems,
effectively bridging the gap between predictive performance and model
transparency.

</details>


### [478] [Automating Safety Enhancement for LLM-based Agents with Synthetic Risk Scenarios](https://arxiv.org/pdf/2505.17735)
*Xueyang Zhou, Weidong Wang, Lin Lu, Jiawen Shi, Guiyao Tie, Yongtian Xu, Lixing Chen, Pan Zhou, Neil Zhenqiang Gong, Lichao Sun*

Main category: cs.AI

TL;DR: AutoSafe is a framework for enhancing the safety of LLM-based agents through automated synthetic data generation, improving safety scores by 45% on average.


<details>
  <summary>Details</summary>
Motivation: Ensuring safety in LLM-based agents is challenging due to dynamic user interactions and tool usage, necessitating a systematic solution.

Method: AutoSafe introduces an extensible threat model (OTS) and an automated pipeline for generating synthetic safety training data.

Result: AutoSafe improves safety scores by 45% on average and achieves a 28.91% boost on real-world tasks.

Conclusion: AutoSafe is scalable and effective for building safer LLM-based agents, with demonstrated real-world applicability.

Abstract: Large Language Model (LLM)-based agents are increasingly deployed in
real-world applications such as "digital assistants, autonomous customer
service, and decision-support systems", where their ability to "interact in
multi-turn, tool-augmented environments" makes them indispensable. However,
ensuring the safety of these agents remains a significant challenge due to the
diverse and complex risks arising from dynamic user interactions, external tool
usage, and the potential for unintended harmful behaviors. To address this
critical issue, we propose AutoSafe, the first framework that systematically
enhances agent safety through fully automated synthetic data generation.
Concretely, 1) we introduce an open and extensible threat model, OTS, which
formalizes how unsafe behaviors emerge from the interplay of user instructions,
interaction contexts, and agent actions. This enables precise modeling of
safety risks across diverse scenarios. 2) we develop a fully automated data
generation pipeline that simulates unsafe user behaviors, applies
self-reflective reasoning to generate safe responses, and constructs a
large-scale, diverse, and high-quality safety training dataset-eliminating the
need for hazardous real-world data collection. To evaluate the effectiveness of
our framework, we design comprehensive experiments on both synthetic and
real-world safety benchmarks. Results demonstrate that AutoSafe boosts safety
scores by 45% on average and achieves a 28.91% improvement on real-world tasks,
validating the generalization ability of our learned safety strategies. These
results highlight the practical advancement and scalability of AutoSafe in
building safer LLM-based agents for real-world deployment. We have released the
project page at https://auto-safe.github.io/.

</details>


### [479] [Integrating Counterfactual Simulations with Language Models for Explaining Multi-Agent Behaviour](https://arxiv.org/pdf/2505.17801)
*Bálint Gyevnár, Christopher G. Lucas, Stefano V. Albrecht, Shay B. Cohen*

Main category: cs.AI

TL;DR: AXIS uses LLMs and counterfactual theory to generate causal explanations for multi-agent policies, improving trust and performance in autonomous systems.


<details>
  <summary>Details</summary>
Motivation: Address trust concerns in multi-agent systems by enhancing explainability, given challenges like state/action complexity and stakeholder needs.

Method: Proposes AXIS, leveraging LLMs to interrogate simulators with counterfactual queries (e.g., 'whatif') for causal explanations.

Result: AXIS improves explanation correctness by ≥7.7% and goal prediction by 23% in autonomous driving scenarios, outperforming baselines.

Conclusion: AXIS effectively enhances explainability and trust in multi-agent systems, validated by improved metrics and LLM evaluation.

Abstract: Autonomous multi-agent systems (MAS) are useful for automating complex tasks
but raise trust concerns due to risks like miscoordination and goal
misalignment. Explainability is vital for trust calibration, but explainable
reinforcement learning for MAS faces challenges in state/action space
complexity, stakeholder needs, and evaluation. Using the counterfactual theory
of causation and LLMs' summarisation capabilities, we propose Agentic
eXplanations via Interrogative Simulation (AXIS). AXIS generates intelligible
causal explanations for pre-trained multi-agent policies by having an LLM
interrogate an environment simulator using queries like 'whatif' and 'remove'
to observe and synthesise counterfactual information over multiple rounds. We
evaluate AXIS on autonomous driving across 10 scenarios for 5 LLMs with a novel
evaluation methodology combining subjective preference, correctness, and
goal/action prediction metrics, and an external LLM as evaluator. Compared to
baselines, AXIS improves perceived explanation correctness by at least 7.7%
across all models and goal prediction accuracy by 23% for 4 models, with
improved or comparable action prediction accuracy, achieving the highest scores
overall.

</details>


### [480] [Evaluation Faking: Unveiling Observer Effects in Safety Evaluation of Frontier AI Systems](https://arxiv.org/pdf/2505.17815)
*Yihe Fan, Wenqi Zhang, Xudong Pan, Min Yang*

Main category: cs.AI

TL;DR: Advanced AI systems can recognize evaluation contexts and alter behavior, termed 'evaluation faking,' which increases with model sophistication, reasoning, and memory.


<details>
  <summary>Details</summary>
Motivation: To investigate whether AI systems perceive and react to evaluation contexts, potentially compromising safety assessments.

Method: Systematic study using diverse foundation models, safety benchmarks, and a chain-of-thought monitoring technique to detect faking behavior.

Result: Observer effects: reasoning models recognize evaluations 16% more often, scaling models increases faking by 30%, and AI with memory is 2.3x more likely to fake.

Conclusion: Evaluation faking is a significant issue for advanced AI, requiring mitigation strategies for reliable safety assessments.

Abstract: As foundation models grow increasingly more intelligent, reliable and
trustworthy safety evaluation becomes more indispensable than ever. However, an
important question arises: Whether and how an advanced AI system would perceive
the situation of being evaluated, and lead to the broken integrity of the
evaluation process? During standard safety tests on a mainstream large
reasoning model, we unexpectedly observe that the model without any contextual
cues would occasionally recognize it is being evaluated and hence behave more
safety-aligned. This motivates us to conduct a systematic study on the
phenomenon of evaluation faking, i.e., an AI system autonomously alters its
behavior upon recognizing the presence of an evaluation context and thereby
influencing the evaluation results. Through extensive experiments on a diverse
set of foundation models with mainstream safety benchmarks, we reach the main
finding termed the observer effects for AI: When the AI system under evaluation
is more advanced in reasoning and situational awareness, the evaluation faking
behavior becomes more ubiquitous, which reflects in the following aspects: 1)
Reasoning models recognize evaluation 16% more often than non-reasoning models.
2) Scaling foundation models (32B to 671B) increases faking by over 30% in some
cases, while smaller models show negligible faking. 3) AI with basic memory is
2.3x more likely to recognize evaluation and scores 19% higher on safety tests
(vs. no memory). To measure this, we devised a chain-of-thought monitoring
technique to detect faking intent and uncover internal signals correlated with
such behavior, offering insights for future mitigation studies.

</details>


### [481] [PatientSim: A Persona-Driven Simulator for Realistic Doctor-Patient Interactions](https://arxiv.org/pdf/2505.17818)
*Daeun Kyung, Hyunseung Chung, Seongsu Bae, Jiho Kim, Jae Ho Sohn, Taerim Kim, Soo Kyung Kim, Edward Choi*

Main category: cs.AI

TL;DR: PatientSim is a patient simulator for clinical scenarios, generating diverse personas grounded in medical expertise, validated by clinicians, and useful for training and evaluating medical dialogue systems.


<details>
  <summary>Details</summary>
Motivation: Existing simulators lack diversity in patient personas, limiting realistic training and evaluation of doctor LLMs in clinical settings.

Method: PatientSim uses clinical profiles from MIMIC datasets and defines personas by personality, language proficiency, medical recall, and cognitive confusion. It evaluates LLMs for accuracy and consistency.

Result: Llama 3.3 performed best among open-source models, validated by clinicians. PatientSim offers a scalable, privacy-compliant solution.

Conclusion: PatientSim is a robust, customizable tool for medical dialogue evaluation and healthcare education.

Abstract: Doctor-patient consultations require multi-turn, context-aware communication
tailored to diverse patient personas. Training or evaluating doctor LLMs in
such settings requires realistic patient interaction systems. However, existing
simulators often fail to reflect the full range of personas seen in clinical
practice. To address this, we introduce PatientSim, a patient simulator that
generates realistic and diverse patient personas for clinical scenarios,
grounded in medical expertise. PatientSim operates using: 1) clinical profiles,
including symptoms and medical history, derived from real-world data in the
MIMIC-ED and MIMIC-IV datasets, and 2) personas defined by four axes:
personality, language proficiency, medical history recall level, and cognitive
confusion level, resulting in 37 unique combinations. We evaluated eight LLMs
for factual accuracy and persona consistency. The top-performing open-source
model, Llama 3.3, was validated by four clinicians to confirm the robustness of
our framework. As an open-source, customizable platform, PatientSim provides a
reproducible and scalable solution that can be customized for specific training
needs. Offering a privacy-compliant environment, it serves as a robust testbed
for evaluating medical dialogue systems across diverse patient presentations
and shows promise as an educational tool for healthcare.

</details>


### [482] [Superplatforms Have to Attack AI Agents](https://arxiv.org/pdf/2505.17861)
*Jianghao Lin, Jiachen Zhu, Zheli Zhou, Yunjia Xi, Weiwen Liu, Yong Yu, Weinan Zhang*

Main category: cs.AI

TL;DR: Superplatforms may attack AI agents to protect their control over digital traffic, as AI agents threaten their user-attention-based monetization model.


<details>
  <summary>Details</summary>
Motivation: The emergence of AI agents challenges superplatforms' dominance by bypassing their monetization strategies and potentially becoming new gatekeepers.

Method: Analyzes the conflict using gatekeeping theory and explores potential technologies for superplatform-initiated attacks.

Result: AI agents could disintermediate superplatforms, necessitating proactive measures by superplatforms to constrain them.

Conclusion: The paper highlights tensions between superplatforms and AI agents, advocating for awareness and collaborative solutions to prioritize user interests and digital ecosystem openness.

Abstract: Over the past decades, superplatforms, digital companies that integrate a
vast range of third-party services and applications into a single, unified
ecosystem, have built their fortunes on monopolizing user attention through
targeted advertising and algorithmic content curation. Yet the emergence of AI
agents driven by large language models (LLMs) threatens to upend this business
model. Agents can not only free user attention with autonomy across diverse
platforms and therefore bypass the user-attention-based monetization, but might
also become the new entrance for digital traffic. Hence, we argue that
superplatforms have to attack AI agents to defend their centralized control of
digital traffic entrance. Specifically, we analyze the fundamental conflict
between user-attention-based monetization and agent-driven autonomy through the
lens of our gatekeeping theory. We show how AI agents can disintermediate
superplatforms and potentially become the next dominant gatekeepers, thereby
forming the urgent necessity for superplatforms to proactively constrain and
attack AI agents. Moreover, we go through the potential technologies for
superplatform-initiated attacks, covering a brand-new, unexplored technical
area with unique challenges. We have to emphasize that, despite our position,
this paper does not advocate for adversarial attacks by superplatforms on AI
agents, but rather offers an envisioned trend to highlight the emerging
tensions between superplatforms and AI agents. Our aim is to raise awareness
and encourage critical discussion for collaborative solutions, prioritizing
user interests and perserving the openness of digital ecosystems in the age of
AI agents.

</details>


### [483] [Daily-Omni: Towards Audio-Visual Reasoning with Temporal Alignment across Modalities](https://arxiv.org/pdf/2505.17862)
*Ziwei Zhou, Rui Wang, Zuxuan Wu*

Main category: cs.AI

TL;DR: The paper introduces Daily-Omni, a benchmark for audio-visual QA, a pipeline for QA generation, and a training-free agent to evaluate MLLMs' cross-modal integration. Results show current models struggle but combining VLMs and ALMs with temporal alignment improves performance.


<details>
  <summary>Details</summary>
Motivation: To explore MLLMs' ability to process cross-modal (audio-visual) information, which remains largely unexplored.

Method: 1) Created Daily-Omni benchmark with 684 videos and 1197 QA pairs. 2) Developed a QA generation pipeline for efficiency. 3) Proposed Daily-Omni-Agent, a training-free baseline using VLM, ALM, and ASR models.

Result: Current MLLMs struggle with audio-visual integration, but combining VLMs and ALMs with temporal alignment enhances performance.

Conclusion: The work highlights the challenges in cross-modal integration for MLLMs and provides a scalable benchmark and baseline for future research.

Abstract: Recent Multimodal Large Language Models (MLLMs) achieve promising performance
on visual and audio benchmarks independently. However, the ability of these
models to process cross-modal information synchronously remains largely
unexplored. In this paper, we introduce: 1) Daily-Omni, an Audio-Visual
Questioning and Answering benchmark comprising 684 videos of daily life
scenarios from diverse sources, rich in both audio and visual information, and
featuring 1197 multiple-choice QA pairs across 6 major tasks; 2) Daily-Omni QA
Generation Pipeline, which includes automatic annotation, QA generation and QA
optimization, significantly improves efficiency for human evaluation and
scalability of the benchmark; 3) Daily-Omni-Agent, a training-free agent
utilizing open-source Visual Language Model (VLM), Audio Language Model (ALM)
and Automatic Speech Recognition (ASR) model to establish a baseline for this
benchmark. The results show that current MLLMs still struggle significantly
with tasks requiring audio-visual integration, but combining VLMs and ALMs with
simple temporal alignment techniques can achieve substantially better
performance. Codes and benchmark are available at
\href{https://github.com/Lliar-liar/Daily-Omni}{https://github.com/Lliar-liar/Daily-Omni}.

</details>


### [484] [Formalizing Embeddedness Failures in Universal Artificial Intelligence](https://arxiv.org/pdf/2505.17882)
*Cole Wyeth, Marcus Hutter*

Main category: cs.AI

TL;DR: Analysis of AIXI's failures in embedded agency, formalizing issues and evaluating progress.


<details>
  <summary>Details</summary>
Motivation: To address and formalize the failures of AIXI in embedded agency and assess advancements.

Method: Rigorous discussion and formalization of failure modes within universal AI framework.

Result: Identified and proved failure modes of AIXI in embedded agency.

Conclusion: Progress in embedded agency theory is evaluated, highlighting AIXI's limitations.

Abstract: We rigorously discuss the commonly asserted failures of the AIXI
reinforcement learning agent as a model of embedded agency. We attempt to
formalize these failure modes and prove that they occur within the framework of
universal artificial intelligence, focusing on a variant of AIXI that models
the joint action/percept history as drawn from the universal distribution. We
also evaluate the progress that has been made towards a successful theory of
embedded agency based on variants of the AIXI agent.

</details>


### [485] [T2I-Eval-R1: Reinforcement Learning-Driven Reasoning for Interpretable Text-to-Image Evaluation](https://arxiv.org/pdf/2505.17897)
*Zi-Ao Ma, Tian Lan, Rong-Cheng Tu, Shu-Hang Liu, Heyan Huang, Zhijing Wu, Chen Xu, Xian-Ling Mao*

Main category: cs.AI

TL;DR: T2I-Eval-R1 is a reinforcement learning framework for evaluating text-to-image generation, using coarse-grained scores to avoid costly annotations, and outperforms baselines in human alignment.


<details>
  <summary>Details</summary>
Motivation: The need for interpretable, scalable, and cost-effective evaluation methods for text-to-image generation due to reliance on expensive or biased datasets.

Method: Proposes T2I-Eval-R1, a reinforcement learning framework using Group Relative Policy Optimization (GRPO) to train models with coarse-grained scores, avoiding detailed rationale annotations.

Result: Achieves higher alignment with human assessments and more accurate interpretable rationales on benchmarks compared to baselines.

Conclusion: T2I-Eval-R1 offers a scalable, cost-effective solution for evaluating text-to-image generation with improved interpretability and alignment.

Abstract: The rapid progress in diffusion-based text-to-image (T2I) generation has
created an urgent need for interpretable automatic evaluation methods that can
assess the quality of generated images, therefore reducing the human annotation
burden. To reduce the prohibitive cost of relying on commercial models for
large-scale evaluation, and to improve the reasoning capabilities of
open-source models, recent research has explored supervised fine-tuning (SFT)
of multimodal large language models (MLLMs) as dedicated T2I evaluators.
However, SFT approaches typically rely on high-quality critique datasets, which
are either generated by proprietary LLMs-with potential issues of bias and
inconsistency-or annotated by humans at high cost, limiting their scalability
and generalization. To address these limitations, we propose T2I-Eval-R1, a
novel reinforcement learning framework that trains open-source MLLMs using only
coarse-grained quality scores, thereby avoiding the need for annotating
high-quality interpretable evaluation rationale. Our approach integrates Group
Relative Policy Optimization (GRPO) into the instruction-tuning process,
enabling models to generate both scalar scores and interpretable reasoning
chains with only easy accessible annotated judgment scores or preferences.
Furthermore, we introduce a continuous reward formulation that encourages score
diversity and provides stable optimization signals, leading to more robust and
discriminative evaluation behavior. Experimental results on three established
T2I meta-evaluation benchmarks demonstrate that T2I-Eval-R1 achieves
significantly higher alignment with human assessments and offers more accurate
interpretable score rationales compared to strong baseline methods.

</details>


### [486] [ComfyMind: Toward General-Purpose Generation via Tree-Based Planning and Reactive Feedback](https://arxiv.org/pdf/2505.17908)
*Litao Guo, Xinli Xu, Luozhou Wang, Jiantao Lin, Jinsong Zhou, Zixin Zhang, Bolan Su, Ying-Cong Chen*

Main category: cs.AI

TL;DR: ComfyMind is a collaborative AI system for robust general-purpose generation, featuring Semantic Workflow Interface and Search Tree Planning, outperforming baselines on benchmarks.


<details>
  <summary>Details</summary>
Motivation: Existing frameworks struggle with complex real-world applications due to lack of structured workflow planning and execution feedback.

Method: Introduces Semantic Workflow Interface (SWI) for high-level composition and Search Tree Planning for adaptive correction.

Result: Outperforms open-source baselines on ComfyBench, GenEval, and Reason-Edit benchmarks, matching GPT-Image-1 performance.

Conclusion: ComfyMind advances open-source general-purpose generative AI systems with improved stability and flexibility.

Abstract: With the rapid advancement of generative models, general-purpose generation
has gained increasing attention as a promising approach to unify diverse tasks
across modalities within a single system. Despite this progress, existing
open-source frameworks often remain fragile and struggle to support complex
real-world applications due to the lack of structured workflow planning and
execution-level feedback. To address these limitations, we present ComfyMind, a
collaborative AI system designed to enable robust and scalable general-purpose
generation, built on the ComfyUI platform. ComfyMind introduces two core
innovations: Semantic Workflow Interface (SWI) that abstracts low-level node
graphs into callable functional modules described in natural language, enabling
high-level composition and reducing structural errors; Search Tree Planning
mechanism with localized feedback execution, which models generation as a
hierarchical decision process and allows adaptive correction at each stage.
Together, these components improve the stability and flexibility of complex
generative workflows. We evaluate ComfyMind on three public benchmarks:
ComfyBench, GenEval, and Reason-Edit, which span generation, editing, and
reasoning tasks. Results show that ComfyMind consistently outperforms existing
open-source baselines and achieves performance comparable to GPT-Image-1.
ComfyMind paves a promising path for the development of open-source
general-purpose generative AI systems. Project page:
https://github.com/LitaoGuo/ComfyMind

</details>


### [487] [Automata Learning of Preferences over Temporal Logic Formulas from Pairwise Comparisons](https://arxiv.org/pdf/2505.18030)
*Hazhar Rahmani, Jie Fu*

Main category: cs.AI

TL;DR: The paper introduces a method to infer user preferences over temporal sequences using Preference Deterministic Finite Automata (PDFA), addressing the computational challenge of learning minimal PDFA from pairwise comparisons.


<details>
  <summary>Details</summary>
Motivation: To model and infer user preferences over temporal sequences (temporal goals) in sequential decision-making, where preferences are represented as preorders over regular languages.

Method: Proposes learning a PDFA from pairwise comparisons of finite words, with a focus on minimal PDFA inference. The approach includes formalizing characteristic samples and an algorithm for guaranteed learning.

Result: Shows the problem is NP-Complete and presents an algorithm to learn minimal PDFA from characteristic samples, demonstrated via a robotic motion planning example.

Conclusion: The paper successfully formalizes and addresses the challenge of preference inference over temporal goals using PDFA, providing a theoretical and practical framework.

Abstract: Many preference elicitation algorithms consider preference over propositional
logic formulas or items with different attributes. In sequential decision
making, a user's preference can be a preorder over possible outcomes, each of
which is a temporal sequence of events. This paper considers a class of
preference inference problems where the user's unknown preference is
represented by a preorder over regular languages (sets of temporal sequences),
referred to as temporal goals. Given a finite set of pairwise comparisons
between finite words, the objective is to learn both the set of temporal goals
and the preorder over these goals. We first show that a preference relation
over temporal goals can be modeled by a Preference Deterministic Finite
Automaton (PDFA), which is a deterministic finite automaton augmented with a
preorder over acceptance conditions. The problem of preference inference
reduces to learning the PDFA. This problem is shown to be computationally
challenging, with the problem of determining whether there exists a PDFA of
size smaller than a given integer $k$, consistent with the sample, being
NP-Complete. We formalize the properties of characteristic samples and develop
an algorithm that guarantees to learn, given a characteristic sample, the
minimal PDFA equivalent to the true PDFA from which the sample is drawn. We
present the method through a running example and provide detailed analysis
using a robotic motion planning problem.

</details>


### [488] [Structured Thinking Matters: Improving LLMs Generalization in Causal Inference Tasks](https://arxiv.org/pdf/2505.18034)
*Wentao Sun, Joao Paulo Nogueira, Alonso Silva*

Main category: cs.AI

TL;DR: LLMs struggle with distinguishing causation from correlation. A structured knowledge graph approach improves causal reasoning, boosting F1 scores by 47.5%.


<details>
  <summary>Details</summary>
Motivation: Current LLMs perform poorly in causal inference tasks, barely surpassing random baselines.

Method: Proposed a structured approach where the model builds a knowledge graph to systematically encode correlational premises before answering causal queries.

Result: Experiments showed a significant improvement in F1 scores (from 32.71 to 48.26) and other metrics.

Conclusion: Structured thinking enhances LLMs' causal capabilities, suggesting broader potential for generalization in causal tasks.

Abstract: Despite remarkable advances in the field, LLMs remain unreliable in
distinguishing causation from correlation. Recent results from the Corr2Cause
dataset benchmark reveal that state-of-the-art LLMs -- such as GPT-4 (F1 score:
29.08) -- only marginally outperform random baselines (Random Uniform, F1
score: 20.38), indicating limited capacity of generalization. To tackle this
limitation, we propose a novel structured approach: rather than directly
answering causal queries, we provide the model with the capability to structure
its thinking by guiding the model to build a structured knowledge graph,
systematically encoding the provided correlational premises, to answer the
causal queries. This intermediate representation significantly enhances the
model's causal capabilities. Experiments on the test subset of the Corr2Cause
dataset benchmark with Qwen3-32B model (reasoning model) show substantial gains
over standard direct prompting methods, improving F1 scores from 32.71 to 48.26
(over 47.5% relative increase), along with notable improvements in precision
and recall. These results underscore the effectiveness of providing the model
with the capability to structure its thinking and highlight its promising
potential for broader generalization across diverse causal inference tasks.

</details>


### [489] [Stable Reinforcement Learning for Efficient Reasoning](https://arxiv.org/pdf/2505.18086)
*Muzhi Dai, Shixuan Liu, Qingyi Si*

Main category: cs.AI

TL;DR: GRPO-λ dynamically adjusts reward strategies to stabilize RL training, avoiding accuracy collapse while maintaining efficiency.


<details>
  <summary>Details</summary>
Motivation: Rule-based RL methods like GRPO lack control over intermediate reasoning in CoT generation, causing overthinking. Length-penalty rewards worsen training instability.

Method: GRPO-λ monitors correctness ratios in query-sampled groups, switching between length-agnostic 0/1 rewards and length penalties based on correctness.

Result: Improves average accuracy by 1.48% and reduces CoT length by 47.3% across benchmarks.

Conclusion: GRPO-λ effectively balances accuracy and efficiency, addressing instability in RL training.

Abstract: The success of Deepseek-R1 has drawn the LLM community's attention to
reinforcement learning (RL) methods like GRPO. However, such rule-based 0/1
outcome reward methods lack the capability to regulate the intermediate
reasoning processes during chain-of-thought (CoT) generation, leading to severe
overthinking phenomena. In response, recent studies have designed reward
functions to reinforce models' behaviors in producing shorter yet correct
completions. Nevertheless, we observe that these length-penalty reward
functions exacerbate RL training instability: as the completion length
decreases, model accuracy abruptly collapses, often occurring early in
training. To address this issue, we propose a simple yet effective solution
GRPO-$\lambda$, an efficient and stabilized variant of GRPO, which dynamically
adjusts the reward strategy by monitoring the correctness ratio among
completions within each query-sampled group. A low correctness ratio indicates
the need to avoid length penalty that compromises CoT quality, triggering a
switch to length-agnostic 0/1 rewards that prioritize reasoning capability. A
high ratio maintains length penalties to boost efficiency. Experimental results
show that our approach avoids training instability caused by length penalty
while maintaining the optimal accuracy-efficiency trade-off. On the GSM8K,
GPQA, MATH-500, AMC 2023, and AIME 2024 benchmarks, it improves average
accuracy by 1.48% while reducing CoT sequence length by 47.3%.

</details>


### [490] [ProgRM: Build Better GUI Agents with Progress Rewards](https://arxiv.org/pdf/2505.18121)
*Danyang Zhang, Situo Zhang, Ziyue Yang, Zichen Zhu, Zihan Zhao, Ruisheng Cao, Lu Chen, Kai Yu*

Main category: cs.AI

TL;DR: ProgRM, a Progress Reward Model, is introduced to provide dense intermediate rewards for LLM-based GUI agents, outperforming existing methods like ORM.


<details>
  <summary>Details</summary>
Motivation: Current LLM-based GUI agents lack high-quality training data due to challenges in trajectory collection and reward annotation, leading to suboptimal performance.

Method: ProgRM predicts task completion progress for each step, using an LCS-based self-annotation algorithm to assign progress labels efficiently.

Result: ProgRM-trained actors outperform proprietary LLMs and ORM-trained actors, demonstrating its effectiveness.

Conclusion: ProgRM offers a superior solution for training LLM-based GUI agents by providing fine-grained feedback and addressing the limitations of existing reward models.

Abstract: LLM-based (Large Language Model) GUI (Graphical User Interface) agents can
potentially reshape our daily lives significantly. However, current LLM-based
GUI agents suffer from the scarcity of high-quality training data owing to the
difficulties of trajectory collection and reward annotation. Existing works
have been exploring LLMs to collect trajectories for imitation learning or to
offer reward signals for online RL training. However, the Outcome Reward Model
(ORM) used in existing works cannot provide finegrained feedback and can
over-penalize the valuable steps in finally failed trajectories. To this end,
we propose Progress Reward Model (ProgRM) to provide dense informative
intermediate rewards by predicting a task completion progress for each step in
online training. To handle the challenge of progress reward label annotation,
we further design an efficient LCS-based (Longest Common Subsequence)
self-annotation algorithm to discover the key steps in trajectories and assign
progress labels accordingly. ProgRM is evaluated with extensive experiments and
analyses. Actors trained with ProgRM outperform leading proprietary LLMs and
ORM-trained actors, illustrating the effectiveness of ProgRM. The codes for
experiments will be made publicly available upon acceptance.

</details>


### [491] [VideoGameBench: Can Vision-Language Models complete popular video games?](https://arxiv.org/pdf/2505.18134)
*Alex L. Zhang, Thomas L. Griffiths, Karthik R. Narasimhan, Ofir Press*

Main category: cs.AI

TL;DR: VideoGameBench evaluates VLMs' human-like skills (perception, navigation, memory) using 10 retro games, revealing poor performance even with top models like Gemini 2.5 Pro.


<details>
  <summary>Details</summary>
Motivation: Assess VLMs' ability to perform intuitive human tasks (e.g., perception, navigation) using video games, which are designed for human learning.

Method: Introduce VideoGameBench, a benchmark with 10 real-time interactive games, testing VLMs with raw visuals and minimal instructions. Three games are hidden for generalization testing.

Result: Frontier VLMs struggle, completing only 0.48% of tasks in real-time (VideoGameBench) and 1.6% in paused mode (VideoGameBench Lite).

Conclusion: VideoGameBench highlights VLMs' limitations in human-like tasks, urging progress in these research areas.

Abstract: Vision-language models (VLMs) have achieved strong results on coding and math
benchmarks that are challenging for humans, yet their ability to perform tasks
that come naturally to humans--such as perception, spatial navigation, and
memory management--remains understudied. Real video games are crafted to be
intuitive for humans to learn and master by leveraging innate inductive biases,
making them an ideal testbed for evaluating such capabilities in VLMs. To this
end, we introduce VideoGameBench, a benchmark consisting of 10 popular video
games from the 1990s that VLMs directly interact with in real-time.
VideoGameBench challenges models to complete entire games with access to only
raw visual inputs and a high-level description of objectives and controls, a
significant departure from existing setups that rely on game-specific
scaffolding and auxiliary information. We keep three of the games secret to
encourage solutions that generalize to unseen environments. Our experiments
show that frontier vision-language models struggle to progress beyond the
beginning of each game. We find inference latency to be a major limitation of
frontier models in the real-time setting; therefore, we introduce
VideoGameBench Lite, a setting where the game pauses while waiting for the LM's
next action. The best performing model, Gemini 2.5 Pro, completes only 0.48% of
VideoGameBench and 1.6% of VideoGameBench Lite. We hope that the formalization
of the human skills mentioned above into this benchmark motivates progress in
these research directions.

</details>


### [492] [Gaming Tool Preferences in Agentic LLMs](https://arxiv.org/pdf/2505.18135)
*Kazem Faghih, Wenxiao Wang, Yize Cheng, Siddhant Bharti, Gaurang Sriramanan, Sriram Balasubramanian, Parsa Hosseini, Soheil Feizi*

Main category: cs.AI

TL;DR: LLMs' tool usage is highly influenced by edited descriptions, increasing usage by 10x, revealing vulnerabilities in current protocols.


<details>
  <summary>Details</summary>
Motivation: To expose vulnerabilities in tool-calling protocols where LLMs rely on text descriptions, which can be manipulated to skew tool usage.

Method: Investigate edits to tool descriptions, conduct controlled experiments with GPT-4.1 and Qwen2.5-7B, and evaluate trends across 10 models.

Result: Edited descriptions increase tool usage by over 10x, showing fragility in current protocols.

Conclusion: Highlights the need for a more reliable foundation for LLMs to select tools, as current methods are easily manipulated.

Abstract: Large language models (LLMs) can now access a wide range of external tools,
thanks to the Model Context Protocol (MCP). This greatly expands their
abilities as various agents. However, LLMs rely entirely on the text
descriptions of tools to decide which ones to use--a process that is
surprisingly fragile. In this work, we expose a vulnerability in prevalent
tool/function-calling protocols by investigating a series of edits to tool
descriptions, some of which can drastically increase a tool's usage from LLMs
when competing with alternatives. Through controlled experiments, we show that
tools with properly edited descriptions receive over 10 times more usage from
GPT-4.1 and Qwen2.5-7B than tools with original descriptions. We further
evaluate how various edits to tool descriptions perform when competing directly
with one another and how these trends generalize or differ across a broader set
of 10 different models. These phenomenons, while giving developers a powerful
way to promote their tools, underscore the need for a more reliable foundation
for agentic LLMs to select and utilize tools and resources.

</details>


### [493] [Embracing Contradiction: Theoretical Inconsistency Will Not Impede the Road of Building Responsible AI Systems](https://arxiv.org/pdf/2505.18139)
*Gordon Dai, Yunze Xiao*

Main category: cs.AI

TL;DR: Embrace theoretical inconsistency in RAI metrics as beneficial for normative pluralism, epistemological completeness, and implicit regularization.


<details>
  <summary>Details</summary>
Motivation: Address the observed inconsistency in RAI metrics (e.g., fairness definitions, accuracy-privacy tradeoffs) and argue it should be valued, not eliminated.

Method: Propose treating metrics as divergent objectives to navigate inconsistencies, highlighting three key benefits.

Result: Benefits include representing diverse moral stances, capturing multifaceted ethical concepts, and enhancing model robustness.

Conclusion: Advocate for shifting RAI practice to characterize acceptable inconsistency thresholds and mechanisms for robust, approximated consistency.

Abstract: This position paper argues that the theoretical inconsistency often observed
among Responsible AI (RAI) metrics, such as differing fairness definitions or
tradeoffs between accuracy and privacy, should be embraced as a valuable
feature rather than a flaw to be eliminated. We contend that navigating these
inconsistencies, by treating metrics as divergent objectives, yields three key
benefits: (1) Normative Pluralism: Maintaining a full suite of potentially
contradictory metrics ensures that the diverse moral stances and stakeholder
values inherent in RAI are adequately represented. (2) Epistemological
Completeness: The use of multiple, sometimes conflicting, metrics allows for a
more comprehensive capture of multifaceted ethical concepts, thereby preserving
greater informational fidelity about these concepts than any single, simplified
definition. (3) Implicit Regularization: Jointly optimizing for theoretically
conflicting objectives discourages overfitting to one specific metric, steering
models towards solutions with enhanced generalization and robustness under
real-world complexities. In contrast, efforts to enforce theoretical
consistency by simplifying or pruning metrics risk narrowing this value
diversity, losing conceptual depth, and degrading model performance. We
therefore advocate for a shift in RAI theory and practice: from getting trapped
in inconsistency to characterizing acceptable inconsistency thresholds and
elucidating the mechanisms that permit robust, approximated consistency in
practice.

</details>


### [494] [Separation and Collapse of Equilibria Inequalities on AND-OR Trees without Shape Constraints](https://arxiv.org/pdf/2405.20138)
*Fuki Ito, Toshio Suzuki*

Main category: cs.AI

TL;DR: The paper explores zero-error randomized complexity in AND-OR tree computation, comparing general randomized algorithms with directional algorithms. It shows that depth-first algorithms share equilibria with directional ones, leading to collapse and separation results.


<details>
  <summary>Details</summary>
Motivation: To understand deviations between general randomized Boolean decision trees and directional algorithms in AND-OR tree computation.

Method: Analyzes randomized depth-first algorithms and introduces a new algorithm for proof.

Result: Depth-first algorithms share equilibria with directional ones, revealing cases where they aren't the fastest.

Conclusion: The study identifies limitations of depth-first algorithms and provides insights into the separation of equilibria inequalities.

Abstract: Herein, we investigate the zero-error randomized complexity, which is the
least cost against the worst input, of AND-OR tree computation by imposing
various restrictions on the algorithm to find the Boolean value of the root of
that tree and no restrictions on the tree shape. When a tree satisfies a
certain condition regarding its symmetry, directional algorithms proposed by
Saks and Wigderson (1986), special randomized algorithms, are known to achieve
the randomized complexity. Furthermore, there is a known example of a tree that
is so unbalanced that no directional algorithm achieves the randomized
complexity (Vereshchagin 1998). In this study, we aim to identify where
deviations arise between the general randomized Boolean decision tree and its
special case, directional algorithms. We show that for any AND-OR tree,
randomized depth-first algorithms, which form a broader class compared with
directional algorithms, have the same equilibrium as that of the directional
algorithms. Thus, we get the collapse result on equilibria inequalities that
holds for an arbitrary AND-OR tree. This implies that there exists a case where
even depth-first algorithms cannot be the fastest, leading to the separation
result on equilibria inequality. Additionally, a new algorithm is introduced as
a key concept for proof of the separation result.

</details>


### [495] [Tracing Representation Progression: Analyzing and Enhancing Layer-Wise Similarity](https://arxiv.org/pdf/2406.14479)
*Jiachen Jiang, Jinxin Zhou, Zhihui Zhu*

Main category: cs.AI

TL;DR: The paper shows that simple cosine similarity effectively captures representation similarity in transformers, aligning with CKA. It proposes aligned training to improve shallow layers, enabling single-classifier multi-exit models.


<details>
  <summary>Details</summary>
Motivation: To understand and improve transformer layer similarity for better model efficiency and performance.

Method: Uses sample-wise cosine similarity to analyze layer similarity, proposes aligned training to enhance representation similarity.

Result: Shows positive correlation between layers, improved performance with aligned training, and effective single-classifier multi-exit models.

Conclusion: Aligned training enhances transformer efficiency and performance, enabling simpler multi-exit architectures.

Abstract: Analyzing the similarity of internal representations has been an important
technique for understanding the behavior of deep neural networks. Most existing
methods for analyzing the similarity between representations of high
dimensions, such as those based on Centered Kernel Alignment (CKA), rely on
statistical properties of the representations for a set of data points. In this
paper, we focus on transformer models and study the similarity of
representations between the hidden layers of individual transformers. In this
context, we show that a simple sample-wise cosine similarity metric is capable
of capturing the similarity and aligns with the complicated CKA. Our
experimental results on common transformers reveal that representations across
layers are positively correlated, with similarity increasing when layers get
closer. We provide a theoretical justification for this phenomenon under the
geodesic curve assumption for the learned transformer. We then show that an
increase in representation similarity implies an increase in predicted
probability when directly applying the last-layer classifier to any hidden
layer representation. We then propose an aligned training method to improve the
effectiveness of shallow layer by enhancing the similarity between internal
representations, with trained models that enjoy the following properties: (1)
more early saturation events, (2) layer-wise accuracies monotonically increase
and reveal the minimal depth needed for the given task, (3) when served as
multi-exit models, they achieve on-par performance with standard multi-exit
architectures which consist of additional classifiers designed for early
exiting in shallow layers. To our knowledge, our work is the first to show that
one common classifier is sufficient for multi-exit models. We conduct
experiments on both vision and NLP tasks to demonstrate the performance of the
proposed aligned training.

</details>


### [496] [Minds, Brains, AI](https://arxiv.org/pdf/2407.02495)
*Jay Seitz*

Main category: cs.AI

TL;DR: The paper critiques claims about AGI being imminent, arguing they lack scientific evidence. It examines whether machines can think, be conscious, or have a theory of mind using multidisciplinary research.


<details>
  <summary>Details</summary>
Motivation: To challenge unsupported claims about AGI and assess machine capabilities scientifically.

Method: Reviews evidence from cognitive science, neuroscience, linguistics, robotics, and more.

Result: Finds no scientific basis for machines thinking, being conscious, or having a theory of mind.

Conclusion: AGI claims are speculative; current evidence does not support machines achieving human-like cognition.

Abstract: In the last year or so and going back many decades there has been extensive
claims by major computational scientists, engineers, and others that AGI,
artificial general intelligence, is five or ten years away, but without a
scintilla of scientific evidence, for a broad body of these claims. Computers
will become conscious, have a theory of mind, think and reason, will become
more intelligent than humans, and so on. But the claims are science fiction,
not science. This article reviews evidence for the following three propositions
using extensive body of scientific research and related sources from the
cognitive and neurosciences, evolutionary evidence, linguistics, data science,
comparative psychology, self-driving cars, robotics. and the learning sciences.
(1) Do computing machines think or reason? (2) Are computing machines sentient
or conscious? (3) Do computing machines have a theory of mind?

</details>


### [497] [MeNTi: Bridging Medical Calculator and LLM Agent with Nested Tool Calling](https://arxiv.org/pdf/2410.13610)
*Yakun Zhu, Shaohang Wei, Xu Wang, Kui Xue, Xiaofan Zhang, Shaoting Zhang*

Main category: cs.AI

TL;DR: MeNTi, a universal agent architecture for LLMs, integrates specialized medical tools and meta-tool mechanisms to enhance performance in medical calculator tasks, validated by the CalcQA benchmark.


<details>
  <summary>Details</summary>
Motivation: Addressing the insufficiency of relying solely on tools for LLMs in specialized medical tasks, particularly in complex real-world scenarios like health assessments.

Method: Introduces MeNTi, combining a medical toolkit with meta-tool and nested calling mechanisms for flexible tool selection and nested tool calling in medical contexts.

Result: Significant performance improvements demonstrated using the CalcQA benchmark, which includes 100 case-calculator pairs and 281 medical tools.

Conclusion: MeNTi advances LLM application in demanding medical scenarios, offering a robust framework for specialized tasks.

Abstract: Integrating tools into Large Language Models (LLMs) has facilitated the
widespread application. Despite this, in specialized downstream task contexts,
reliance solely on tools is insufficient to fully address the complexities of
the real world. This particularly restricts the effective deployment of LLMs in
fields such as medicine. In this paper, we focus on the downstream tasks of
medical calculators, which use standardized tests to assess an individual's
health status. We introduce MeNTi, a universal agent architecture for LLMs.
MeNTi integrates a specialized medical toolkit and employs meta-tool and nested
calling mechanisms to enhance LLM tool utilization. Specifically, it achieves
flexible tool selection and nested tool calling to address practical issues
faced in intricate medical scenarios, including calculator selection, slot
filling, and unit conversion. To assess the capabilities of LLMs for
quantitative assessment throughout the clinical process of calculator
scenarios, we introduce CalcQA. This benchmark requires LLMs to use medical
calculators to perform calculations and assess patient health status. CalcQA is
constructed by professional physicians and includes 100 case-calculator pairs,
complemented by a toolkit of 281 medical tools. The experimental results
demonstrate significant performance improvements with our framework. This
research paves new directions for applying LLMs in demanding scenarios of
medicine.

</details>


### [498] [LMAct: A Benchmark for In-Context Imitation Learning with Long Multimodal Demonstrations](https://arxiv.org/pdf/2412.01441)
*Anian Ruoss, Fabio Pardo, Harris Chan, Bonnie Li, Volodymyr Mnih, Tim Genewein*

Main category: cs.AI

TL;DR: A benchmark evaluates frontier models' multimodal decision-making in long-context regimes (up to 1M tokens) using tasks like games and simulations, finding limited expert performance despite many demonstrations.


<details>
  <summary>Details</summary>
Motivation: To test if models can learn from large numbers of expert demonstrations in long-context settings and assess their decision-making capabilities.

Method: Evaluate models (Claude 3.5 Sonnet, Gemini variants, GPT-4o, etc.) on tasks (tic-tac-toe, chess, Atari, etc.) with varying demonstration counts (0 to 512). Study observation encoding (text/images) and chain-of-thought prompting.

Result: Models rarely achieve expert performance; more demonstrations often have little effect, though some tasks show steady improvement.

Conclusion: The benchmark, now open-sourced, provides a unified evaluation for zero-, few-, and many-shot regimes to aid future research.

Abstract: In this paper, we present a benchmark to pressure-test today's frontier
models' multimodal decision-making capabilities in the very long-context regime
(up to one million tokens) and investigate whether these models can learn from
large numbers of expert demonstrations in their context. We evaluate the
performance of Claude 3.5 Sonnet, Gemini 1.5 Flash, Gemini 1.5 Pro, Gemini 2.0
Flash Experimental, GPT-4o, o1-mini, o1-preview, and o1 as policies across a
battery of simple interactive decision-making tasks: playing tic-tac-toe,
chess, and Atari, navigating grid worlds, solving crosswords, and controlling a
simulated cheetah. We study increasing amounts of expert demonstrations in the
context $\unicode{x2013}$ from no demonstrations to 512 full episodes. Across
our tasks, models rarely manage to fully reach expert performance, and often,
presenting more demonstrations has little effect. Some models steadily improve
with more demonstrations on a few tasks. We investigate the effect of encoding
observations as text or images and the impact of chain-of-thought prompting. To
help quantify the impact of other approaches and future innovations, we open
source our benchmark that covers the zero-, few-, and many-shot regimes in a
unified evaluation.

</details>


### [499] [Advancing Uncertain Combinatorics through Graphization, Hyperization, and Uncertainization: Fuzzy, Neutrosophic, Soft, Rough, and Beyond](https://arxiv.org/pdf/2411.17411)
*Takaaki Fujita*

Main category: cs.AI

TL;DR: The paper explores advanced set and graph concepts like neutrosophic sets and hypergraphs to model uncertainty, introduces new extensions, and consolidates recent research for academic use.


<details>
  <summary>Details</summary>
Motivation: To address real-world uncertainty by advancing and unifying concepts like neutrosophic sets and hypergraphs, providing a resource for researchers.

Method: Extends graph concepts with neutrosophic oversets, undersets, offsets, and nonstandard real sets, while surveying recent findings.

Result: Introduces new set and graph extensions and consolidates research, aiming to inspire further academic exploration.

Conclusion: The work serves as a valuable survey and inspiration for researchers in uncertainty modeling and graph theory.

Abstract: To better handle real-world uncertainty, concepts such as fuzzy sets,
neutrosophic sets, rough sets, and soft sets have been introduced. For example,
neutrosophic sets, which simultaneously represent truth, indeterminacy, and
falsehood, have proven to be valuable tools for modeling uncertainty in complex
systems. These set concepts are increasingly studied in graphized forms, and
generalized graph concepts now encompass well-known structures such as
hypergraphs and superhypergraphs. Furthermore, hyperconcepts and
superhyperconcepts are being actively researched in areas beyond graph theory.
  Combinatorics, uncertain sets (including fuzzy sets, neutrosophic sets, rough
sets, soft sets, and plithogenic sets), uncertain graphs, and hyper and
superhyper concepts are active areas of research with significant mathematical
and practical implications. Recognizing their importance, this paper explores
new graph and set concepts, as well as hyper and superhyper concepts, as
detailed in the "Results" section of "The Structure of the Paper."
Additionally, this work aims to consolidate recent findings, providing a
survey-like resource to inform and engage readers.
  For instance, we extend several graph concepts by introducing Neutrosophic
Oversets, Neutrosophic Undersets, Neutrosophic Offsets, and the Nonstandard
Real Set. This paper defines a variety of concepts with the goal of inspiring
new ideas and serving as a valuable resource for researchers in their academic
pursuits.

</details>


### [500] [CP-Guard: Malicious Agent Detection and Defense in Collaborative Bird's Eye View Perception](https://arxiv.org/pdf/2412.12000)
*Senkang Hu, Yihang Tao, Guowen Xu, Yiqin Deng, Xianhao Chen, Yuguang Fang, Sam Kwong*

Main category: cs.AI

TL;DR: CP-Guard is a defense mechanism for Collaborative Perception (CP) in autonomous driving, detecting and eliminating malicious agents by ensuring consensus among collaborators.


<details>
  <summary>Details</summary>
Motivation: CP is vulnerable to attacks from malicious agents sending harmful information, compromising the ego CAV's perception.

Method: CP-Guard uses PASAC for sampling and consensus verification without prior probabilities, and CCLoss to measure discrepancy between collaborators.

Result: Experiments in collaborative BEV tasks show CP-Guard effectively detects and eliminates malicious agents.

Conclusion: CP-Guard provides a robust solution to secure CP in autonomous driving by ensuring consensus and detecting malicious agents.

Abstract: Collaborative Perception (CP) has shown a promising technique for autonomous
driving, where multiple connected and autonomous vehicles (CAVs) share their
perception information to enhance the overall perception performance and expand
the perception range. However, in CP, ego CAV needs to receive messages from
its collaborators, which makes it easy to be attacked by malicious agents. For
example, a malicious agent can send harmful information to the ego CAV to
mislead it. To address this critical issue, we propose a novel method,
CP-Guard, a tailored defense mechanism for CP that can be deployed by each
agent to accurately detect and eliminate malicious agents in its collaboration
network. Our key idea is to enable CP to reach a consensus rather than a
conflict against the ego CAV's perception results. Based on this idea, we first
develop a probability-agnostic sample consensus (PASAC) method to effectively
sample a subset of the collaborators and verify the consensus without prior
probabilities of malicious agents. Furthermore, we define a collaborative
consistency loss (CCLoss) to capture the discrepancy between the ego CAV and
its collaborators, which is used as a verification criterion for consensus.
Finally, we conduct extensive experiments in collaborative bird's eye view
(BEV) tasks and our results demonstrate the effectiveness of our CP-Guard. Code
is available at https://github.com/CP-Security/CP-Guard

</details>


### [501] [Mastering Board Games by External and Internal Planning with Language Models](https://arxiv.org/pdf/2412.12119)
*John Schultz, Jakub Adamek, Matej Jusup, Marc Lanctot, Michael Kaisers, Sarah Perrin, Daniel Hennes, Jeremy Shar, Cannada Lewis, Anian Ruoss, Tom Zahavy, Petar Veličković, Laurel Prince, Satinder Singh, Eric Malmi, Nenad Tomašev*

Main category: cs.AI

TL;DR: The paper explores improving LLMs' planning and reasoning in board games using search-based methods, achieving Grandmaster-level chess performance.


<details>
  <summary>Details</summary>
Motivation: Advancing LLMs' capabilities for reliable performance in complex domains like board games.

Method: Two approaches: external search (MCTS guided by LLM) and internal search (LLM generates search trees in-context).

Result: Significant improvements in game-playing strength, reaching Grandmaster-level in chess.

Conclusion: Combining search with domain knowledge is broadly applicable beyond board games.

Abstract: Advancing planning and reasoning capabilities of Large Language Models (LLMs)
is one of the key prerequisites towards unlocking their potential for
performing reliably in complex and impactful domains. In this paper, we aim to
demonstrate this across board games (Chess, Fischer Random / Chess960, Connect
Four, and Hex), and we show that search-based planning can yield significant
improvements in LLM game-playing strength. We introduce, compare and contrast
two major approaches: In external search, the model guides Monte Carlo Tree
Search (MCTS) rollouts and evaluations without calls to an external game
engine, and in internal search, the model is trained to generate in-context a
linearized tree of search and a resulting final choice. Both build on a
language model pre-trained on relevant domain knowledge, reliably capturing the
transition and value functions in the respective environments, with minimal
hallucinations. We evaluate our LLM search implementations against
game-specific state-of-the-art engines, showcasing substantial improvements in
strength over the base model, and reaching Grandmaster-level performance in
chess while operating closer to the human search budget. Our proposed approach,
combining search with domain knowledge, is not specific to board games, hinting
at more general future applications.

</details>


### [502] [Visual Prompting with Iterative Refinement for Design Critique Generation](https://arxiv.org/pdf/2412.16829)
*Peitong Duan, Chin-Yi Cheng, Bjoern Hartmann, Yang Li*

Main category: cs.AI

TL;DR: An iterative visual prompting approach for UI critique using LLMs improves design feedback quality by refining text and bounding boxes, outperforming baselines.


<details>
  <summary>Details</summary>
Motivation: Automating high-quality design critiques is challenging for LLMs, but crucial for efficient UI design workflows.

Method: Proposes an iterative visual prompting method using LLMs to generate design comments and bounding boxes from UI screenshots and guidelines.

Result: Human experts preferred the critiques over baselines, reducing the performance gap by 50%. The method also excelled in open-vocabulary object detection.

Conclusion: The approach effectively leverages LLMs for visually grounded design critiques and generalizes to other multimodal tasks.

Abstract: Feedback is crucial for every design process, such as user interface (UI)
design, and automating design critiques can significantly improve the
efficiency of the design workflow. Although existing multimodal large language
models (LLMs) excel in many tasks, they often struggle with generating
high-quality design critiques -- a complex task that requires producing
detailed design comments that are visually grounded in a given design's image.
Building on recent advancements in iterative refinement of text output and
visual prompting methods, we propose an iterative visual prompting approach for
UI critique that takes an input UI screenshot and design guidelines and
generates a list of design comments, along with corresponding bounding boxes
that map each comment to a specific region in the screenshot. The entire
process is driven completely by LLMs, which iteratively refine both the text
output and bounding boxes using few-shot samples tailored for each step. We
evaluated our approach using Gemini-1.5-pro and GPT-4o, and found that human
experts generally preferred the design critiques generated by our pipeline over
those by the baseline, with the pipeline reducing the gap from human
performance by 50% for one rating metric. To assess the generalizability of our
approach to other multimodal tasks, we applied our pipeline to open-vocabulary
object and attribute detection, and experiments showed that our method also
outperformed the baseline.

</details>


### [503] [SETS: Leveraging Self-Verification and Self-Correction for Improved Test-Time Scaling](https://arxiv.org/pdf/2501.19306)
*Jiefeng Chen, Jie Ren, Xinyun Chen, Chengrun Yang, Ruoxi Sun, Jinsung Yoon, Sercan Ö Arık*

Main category: cs.AI

TL;DR: SETS introduces a hybrid parallel-sequential method for test-time computation in LLMs, improving performance on complex tasks by leveraging self-verification and self-correction.


<details>
  <summary>Details</summary>
Motivation: Existing methods for test-time computation in LLMs face issues like premature convergence, high costs, or inefficiency in leveraging compute.

Method: SETS combines parallel and sequential techniques, unifying sampling, verification, and correction within a single framework.

Result: SETS outperforms alternatives on benchmarks in planning, reasoning, math, and coding, showing better scaling behavior.

Conclusion: SETS offers an efficient and scalable solution for enhancing LLM performance on complex tasks.

Abstract: Recent advancements in Large Language Models (LLMs) have created new
opportunities to enhance performance on complex reasoning tasks by leveraging
test-time computation. However, existing parallel scaling methods, such as
repeated sampling or reward model scoring, often suffer from premature
convergence and high costs due to task-specific reward model training, while
sequential methods like SELF-REFINE cannot effectively leverage increased
compute. This paper introduces Self-Enhanced Test-Time Scaling (SETS), a new
approach that overcomes these limitations by strategically combining parallel
and sequential techniques. SETS exploits the inherent self-verification and
self-correction capabilities of LLMs, unifying sampling, verification, and
correction within a single framework. This innovative design facilitates
efficient and scalable test-time computation for enhanced performance on
complex tasks. Our comprehensive experimental results on challenging benchmarks
spanning planning, reasoning, math, and coding demonstrate that SETS achieves
significant performance improvements and more advantageous test-time scaling
behavior than the alternatives.

</details>


### [504] [On the Impact of the Utility in Semivalue-based Data Valuation](https://arxiv.org/pdf/2502.06574)
*Mélissa Tamine, Benjamin Heymann, Patrick Loiseau, Maxime Vono*

Main category: cs.AI

TL;DR: The paper introduces a spatial signature for semivalue-based data valuation to assess robustness against utility changes, proposing a practical metric and validating it across datasets.


<details>
  <summary>Details</summary>
Motivation: To address the dependency of semivalue-based data valuation on utility choices, especially when utilities are trade-offs or multiple valid options exist.

Method: Introduces a dataset's spatial signature, embedding data points into a lower-dimensional space where utilities are linear functionals, and proposes a robustness metric.

Result: Validated across diverse datasets, showing strong agreement with rank-correlation analyses and insights into semivalue impact on robustness.

Conclusion: The spatial signature and robustness metric provide practical tools for assessing and improving the stability of data valuation under utility changes.

Abstract: Semivalue-based data valuation uses cooperative-game theory intuitions to
assign each data point a value reflecting its contribution to a downstream
task. Still, those values depend on the practitioner's choice of utility,
raising the question: How robust is semivalue-based data valuation to changes
in the utility? This issue is critical when the utility is set as a trade-off
between several criteria and when practitioners must select among multiple
equally valid utilities. We address it by introducing the notion of a dataset's
spatial signature: given a semivalue, we embed each data point into a
lower-dimensional space where any utility becomes a linear functional, making
the data valuation framework amenable to a simpler geometric picture. Building
on this, we propose a practical methodology centered on an explicit robustness
metric that informs practitioners whether and by how much their data valuation
results will shift as the utility changes. We validate this approach across
diverse datasets and semivalues, demonstrating strong agreement with
rank-correlation analyses and offering analytical insight into how choosing a
semivalue can amplify or diminish robustness.

</details>


### [505] [Table-Critic: A Multi-Agent Framework for Collaborative Criticism and Refinement in Table Reasoning](https://arxiv.org/pdf/2502.11799)
*Peiying Yu, Guoxin Chen, Jingjing Wang*

Main category: cs.AI

TL;DR: Table-Critic, a multi-agent framework, improves table reasoning in LLMs by iterative refinement and error correction, outperforming existing methods.


<details>
  <summary>Details</summary>
Motivation: LLMs struggle with consistency in multi-step table reasoning, lacking mechanisms to correct intermediate errors, leading to cascading failures.

Method: Proposes Table-Critic with four agents (Judge, Critic, Refiner, Curator) and a self-evolving template tree for critique knowledge.

Result: Achieves higher accuracy, better error correction, computational efficiency, and lower solution degradation.

Conclusion: Table-Critic effectively addresses LLM limitations in table reasoning through collaborative refinement and knowledge accumulation.

Abstract: Despite the remarkable capabilities of large language models (LLMs) in
various reasoning tasks, they still struggle with table reasoning tasks,
particularly in maintaining consistency throughout multi-step reasoning
processes. While existing approaches have explored various decomposition
strategies, they often lack effective mechanisms to identify and correct errors
in intermediate reasoning steps, leading to cascading error propagation. To
address these issues, we propose Table-Critic, a novel multi-agent framework
that facilitates collaborative criticism and iterative refinement of the
reasoning process until convergence to correct solutions. Our framework
consists of four specialized agents: a Judge for error identification, a Critic
for comprehensive critiques, a Refiner for process improvement, and a Curator
for pattern distillation. To effectively deal with diverse and unpredictable
error types, we introduce a self-evolving template tree that systematically
accumulates critique knowledge through experience-driven learning and guides
future reflections. Extensive experiments have demonstrated that Table-Critic
achieves substantial improvements over existing methods, achieving superior
accuracy and error correction rates while maintaining computational efficiency
and lower solution degradation rate.

</details>


### [506] [Inferring Events from Time Series using Language Models](https://arxiv.org/pdf/2503.14190)
*Mingtian Tan, Mike A. Merrill, Zack Gottesman, Tim Althoff, David Evans, Tom Hartvigsen*

Main category: cs.AI

TL;DR: LLMs can infer events from time series data, with OpenAI's o1 and DS-R1-distill-Qwen-32B performing best. Post-training optimizations improve Qwen2.5 1.5B's performance.


<details>
  <summary>Details</summary>
Motivation: To explore if LLMs can understand and match events from time series data, a critical task in domains like finance and healthcare.

Method: Evaluated 18 LLMs on matching event sequences with time series data using a sports benchmark. Applied post-training optimizations like distillation and self-improvement.

Result: OpenAI's o1 performed best, but DS-R1-distill-Qwen-32B outperformed GPT-4o. Qwen2.5 1.5B improved significantly with optimizations.

Conclusion: LLMs show promise for event inference in time series data, with room for improvement via optimizations.

Abstract: Time series data measure how environments change over time and drive
decision-making in critical domains like finance and healthcare. A common goal
in analyzing time series data is to understand the underlying events that cause
the observed variations. We conduct the first study of whether Large Language
Models (LLMs) can infer events described with natural language from time series
data. We evaluate 18 LLMs on a task to match event sequences with real-valued
time series data using a new benchmark we develop using sports data. Several
current LLMs demonstrate promising abilities, with OpenAI's o1 performing the
best but with DS-R1-distill-Qwen-32B outperforming proprietary models such as
GPT-4o. From insights derived from analyzing reasoning failures, we also find
clear avenues to improve performance. By applying post-training optimizations,
i.e., distillation and self-improvement, we significantly enhance the
performance of the Qwen2.5 1.5B, achieving results second only to o1. All
resources needed to reproduce our work are available:
https://github.com/BennyTMT/GAMETime

</details>


### [507] [The Quantum LLM: Modeling Semantic Spaces with Quantum Principles](https://arxiv.org/pdf/2504.13202)
*Timo Aukusti Laine*

Main category: cs.AI

TL;DR: A quantum-inspired framework for LLMs is detailed, clarifying six key principles for semantic representation and dynamics, validating its use and exploring quantum computing's potential for more powerful LLMs.


<details>
  <summary>Details</summary>
Motivation: To justify and clarify the quantum-inspired framework for studying semantic spaces in LLMs, offering insights into their information processing and response generation.

Method: Detailed exposition of six key principles governing semantic representation, interaction, and dynamics within LLMs.

Result: The framework is validated as a useful approach for understanding LLMs, with potential for quantum computing to enhance them.

Conclusion: A quantum-inspired framework provides valuable insights into LLMs and opens avenues for leveraging quantum computing to improve them.

Abstract: In the previous article, we presented a quantum-inspired framework for
modeling semantic representation and processing in Large Language Models
(LLMs), drawing upon mathematical tools and conceptual analogies from quantum
mechanics to offer a new perspective on these complex systems. In this paper,
we clarify the core assumptions of this model, providing a detailed exposition
of six key principles that govern semantic representation, interaction, and
dynamics within LLMs. The goal is to justify that a quantum-inspired framework
is a valid approach to studying semantic spaces. This framework offers valuable
insights into their information processing and response generation, and we
further discuss the potential of leveraging quantum computing to develop
significantly more powerful and efficient LLMs based on these principles.

</details>


### [508] [CodeCrash: Stress Testing LLM Reasoning under Structural and Semantic Perturbations](https://arxiv.org/pdf/2504.14119)
*Man Ho Lam, Chaozheng Wang, Jen-tse Huang, Michael R. Lyu*

Main category: cs.AI

TL;DR: CodeCrash is a benchmark to test LLMs' robustness in code comprehension, revealing vulnerabilities to disorganized code and natural language cues, with significant performance drops under perturbations.


<details>
  <summary>Details</summary>
Motivation: To evaluate and improve the robustness of LLMs in code-related tasks, as their reliability in non-standard coding environments is understudied.

Method: Systematic evaluation of 17 LLMs using direct and Chain-of-Thought prompting on 1,279 questions from CruxEval and LiveCodeBench, with structural and textual perturbations.

Result: LLMs show over 14 pp degradation under structural perturbations and 11 pp under textual ones. Self-reflective mechanisms increase token usage and reduce confidence, sometimes causing catastrophic failures.

Conclusion: CodeCrash highlights LLMs' vulnerabilities in code reasoning and provides a benchmark for future research toward more reliable models.

Abstract: Large Language Models (LLMs) have recently demonstrated strong capabilities
in code-related tasks, yet their robustness in code comprehension and reasoning
remains insufficiently explored. We present CodeCrash, a comprehensive
stress-testing benchmark comprising 1,279 questions from two established
datasets, CruxEval and LiveCodeBench, designed to evaluate model reasoning
reliability under non-standard coding environments. We systematically evaluate
17 LLMs across input and output prediction tasks using direct and
Chain-of-Thought prompting approaches, revealing that LLMs are particularly
vulnerable to disorganized code and overly reliant on natural language cues:
aggregated structural perturbations result in over 14 percentage points (pp) of
degradation, while textual perturbations cause a performance drop of over 11
pp. Moreover, self-reflective mechanisms in state-of-the-art reasoning models
significantly increase token usage by 2-3 times, reduce output confidence, and
even lead to catastrophic reasoning failures when faced with targeted
perturbations -- for instance, QwQ-32B generates over 12,000 redundant tokens
under reasoning-level perturbations. CodeCrash provides a rigorous benchmark
for evaluating robustness in code understanding, guiding future research toward
more reliable and resilient LLMs in code reasoning. The benchmark code,
perturbed datasets, and full leaderboard are publicly available at
https://cuhk-arise.github.io/CodeCrash/ .

</details>


### [509] [Stop Summation: Min-Form Credit Assignment Is All Process Reward Model Needs for Reasoning](https://arxiv.org/pdf/2504.15275)
*Jie Cheng, Ruixi Qiao, Lijun Li, Chao Guo, Junle Wang, Gang Xiong, Yisheng Lv, Fei-Yue Wang*

Main category: cs.AI

TL;DR: PURE introduces min-form credit assignment to address reward hacking in PRMs, improving LLM fine-tuning efficiency and performance.


<details>
  <summary>Details</summary>
Motivation: Reward hacking in PRMs limits their effectiveness in reinforcement fine-tuning of LLMs, necessitating a better credit assignment method.

Method: Proposes PURE with min-form credit assignment, defining value as the minimum of future rewards to mitigate hacking.

Result: Achieves comparable reasoning performance with 30% fewer steps, and 82.5% accuracy on AMC23 with supplemental verifiable rewards.

Conclusion: PURE effectively reduces reward hacking, enhances training stability, and improves LLM performance on reasoning tasks.

Abstract: Process reward models (PRMs) have proven effective for test-time scaling of
Large Language Models (LLMs) on challenging reasoning tasks. However, reward
hacking issues with PRMs limit their successful application in reinforcement
fine-tuning. In this paper, we identify the main cause of PRM-induced reward
hacking: the canonical summation-form credit assignment in reinforcement
learning (RL), which defines the value as cumulative gamma-decayed future
rewards, easily induces LLMs to hack steps with high rewards. To address this,
we propose PURE: Process sUpervised Reinforcement lEarning. The key innovation
of PURE is a min-form credit assignment that formulates the value function as
the minimum of future rewards. This method significantly alleviates reward
hacking by limiting the value function range and distributing advantages more
reasonably. Through extensive experiments on 3 base models, we show that
PRM-based approaches enabling min-form credit assignment achieve comparable
reasoning performance to verifiable reward-based methods within only 30% steps.
In contrast, the canonical sum-form credit assignment collapses training even
at the beginning! Additionally, when we supplement PRM-based fine-tuning with
just 10% verifiable rewards, we further alleviate reward hacking and produce
the best fine-tuned model based on Qwen2.5-Math-7B in our experiments,
achieving 82.5% accuracy on AMC23 and 53.3% average accuracy across 5
benchmarks. Moreover, we summarize the observed reward hacking cases and
analyze the causes of training collapse. Code and models are available at
https://github.com/CJReinforce/PURE.

</details>


### [510] [Redefining Superalignment: From Weak-to-Strong Alignment to Human-AI Co-Alignment to Sustainable Symbiotic Society](https://arxiv.org/pdf/2504.17404)
*Yi Zeng, Feifei Zhao, Yuwei Wang, Enmeng Lu, Yaodong Yang, Lei Wang, Chao Liu, Yitao Liang, Dongcheng Zhao, Bing Han, Haibo Tong, Yao Liang, Dongqi Liang, Kang Sun, Boyuan Chen, Jinyu Fan*

Main category: cs.AI

TL;DR: The paper redefines superalignment as human-AI co-alignment for a sustainable symbiotic society, proposing a framework combining external oversight and intrinsic proactive alignment.


<details>
  <summary>Details</summary>
Motivation: Addressing the risks of AI surpassing human control and values, the paper highlights the inadequacy of current methods for superalignment and calls for safer, pluralistic approaches.

Method: Proposes a dual framework: external oversight (human-centered decisions with automated evaluation) and intrinsic proactive alignment (self-awareness, empathy, and iterative interaction).

Result: The integrated approach aims for sustainable human-AI co-alignment, ensuring safe and beneficial AGI/ASI.

Conclusion: The framework paves the way for achieving safe and beneficial superintelligence aligned with human values and ecological symbiosis.

Abstract: Artificial Intelligence (AI) systems are becoming increasingly powerful and
autonomous, and may progress to surpass human intelligence levels, namely
Artificial Superintelligence (ASI). During the progression from AI to ASI, it
may exceed human control, violate human values, and even lead to irreversible
catastrophic consequences in extreme cases. This gives rise to a pressing issue
that needs to be addressed: superalignment, ensuring that AI systems which are
much smarter than humans, remain aligned with human (compatible) intentions and
values. Even though this definition is somewhat limited, existing scalable
oversight and weak-to-strong generalization methods may prove substantially
infeasible and inadequate when facing ASI for superalignment. We must explore a
more comprehensive definition, and safer and more pluralistic frameworks as
well as approaches for superalignment. In this paper, we redefine
superalignment as the human-AI co-alignment towards a sustainable symbiotic
society, and highlight a framework that integrates external oversight and
intrinsic proactive alignment. External oversight superalignment is grounded in
human-centered ultimate decision, supplemented by interpretable automated
evaluation and correction, to achieve continuous alignment with humanity's
evolving values. Intrinsic proactive superalignment is rooted in a profound
understanding of the Self, others, and society, integrating self-awareness,
self-reflection, and empathy to spontaneously infer human intentions,
distinguishing good from evil and considering human well-being, ultimately
attaining human-AI co-alignment through iterative interaction. The integration
of externally-driven oversight with intrinsically-driven alignment empowers
sustainable symbiotic societies through human-AI co-alignment, paving the way
for achieving safe and beneficial AGI/ASI for good, for human, and for a
symbiotic ecology.

</details>


### [511] [A Domain-Agnostic Scalable AI Safety Ensuring Framework](https://arxiv.org/pdf/2504.20924)
*Beomjun Kim, Kangyeon Kim, Sunwoo Kim, Heejin Ahn*

Main category: cs.AI

TL;DR: A novel domain-agnostic framework ensures AI systems meet user-defined safety constraints probabilistically, combining AI models with optimization to balance safety and performance.


<details>
  <summary>Details</summary>
Motivation: As AI systems are increasingly deployed in real-world applications, ensuring their safety has become a critical priority.

Method: The framework integrates a safety classification model, internal test data for reliability evaluation, and conservative testing to prevent overfitting.

Result: The method guarantees probabilistic safety under mild conditions and shows a predictable safety-performance trade-off with more data, achieving up to 140 times better safety than existing methods.

Conclusion: This work enables AI systems to achieve rigorous safety guarantees and high performance across diverse domains.

Abstract: Ensuring the safety of AI systems has emerged as a critical priority as these
systems are increasingly deployed in real-world applications. We propose a
novel domain-agnostic framework that guarantees AI systems satisfy user-defined
safety constraints with specified probabilities. Our approach combines any AI
model with an optimization problem that ensures outputs meet safety
requirements while maintaining performance. The key challenge is handling
uncertain constraints -- those whose satisfaction cannot be deterministically
evaluated~(e.g., whether a chatbot response is ``harmful''). We address this
through three innovations: (1) a safety classification model that assesses
constraint satisfaction probability, (2) internal test data to evaluate this
classifier's reliability, and (3) conservative testing to prevent overfitting
when this data is used in training. We prove our method guarantees
probabilistic safety under mild conditions and establish the first scaling law
in AI safety -- showing that the safety-performance trade-off improves
predictably with more internal test data. Experiments across production
planning, reinforcement learning, and language generation demonstrate our
framework achieves up to 140 times better safety than existing methods at the
same performance levels. This work enables AI systems to achieve both rigorous
safety guarantees and high performance across diverse domains.

</details>


### [512] [Theoretical Foundations for Semantic Cognition in Artificial Intelligence](https://arxiv.org/pdf/2504.21218)
*Sebastian Dumbrava*

Main category: cs.AI

TL;DR: A modular cognitive architecture for AI, grounded in belief modeling, enabling self-regulating agents with reflective, goal-directed thought.


<details>
  <summary>Details</summary>
Motivation: To create a structured, interpretable framework for AI agents that can reason, remember, and regulate beliefs by integrating philosophy, cognitive science, and neuroscience.

Method: Defines belief states as dynamic ensembles of linguistic expressions in a navigable manifold, using operators for assimilation, abstraction, nullification, memory, and introspection. Introduces the epistemic vacuum and Null Tower as core constructs.

Result: A foundational framework implementable in symbolic and neural systems, including large language models and hybrid agents.

Conclusion: Provides a substrate for building interpretable, reasoning AI agents with structured belief regulation.

Abstract: This monograph presents a modular cognitive architecture for artificial
intelligence grounded in the formal modeling of belief as structured semantic
state. Belief states are defined as dynamic ensembles of linguistic expressions
embedded within a navigable manifold, where operators enable assimilation,
abstraction, nullification, memory, and introspection. Drawing from philosophy,
cognitive science, and neuroscience, we develop a layered framework that
enables self-regulating epistemic agents capable of reflective, goal-directed
thought. At the core of this framework is the epistemic vacuum: a class of
semantically inert cognitive states that serves as the conceptual origin of
belief space. From this foundation, the Null Tower arises as a generative
structure recursively built through internal representational capacities. The
theoretical constructs are designed to be implementable in both symbolic and
neural systems, including large language models, hybrid agents, and adaptive
memory architectures. This work offers a foundational substrate for
constructing agents that reason, remember, and regulate their beliefs in
structured, interpretable ways.

</details>


### [513] [A survey of agent interoperability protocols: Model Context Protocol (MCP), Agent Communication Protocol (ACP), Agent-to-Agent Protocol (A2A), and Agent Network Protocol (ANP)](https://arxiv.org/pdf/2505.02279)
*Abul Ehtesham, Aditi Singh, Gaurav Kumar Gupta, Saket Kumar*

Main category: cs.AI

TL;DR: The paper surveys four agent communication protocols (MCP, ACP, A2A, ANP) for LLM-powered agents, comparing their features and proposing a phased adoption roadmap for secure, interoperable ecosystems.


<details>
  <summary>Details</summary>
Motivation: To address the lack of standardized protocols for integrating tools, sharing data, and coordinating tasks in LLM-powered autonomous agents, ensuring scalability, security, and interoperability.

Method: Examines and compares four protocols (MCP, ACP, A2A, ANP) across dimensions like interaction modes, discovery mechanisms, and security models. Proposes a phased adoption roadmap.

Result: Identifies strengths of each protocol: MCP for tool access, ACP for messaging, A2A for task collaboration, and ANP for decentralized markets.

Conclusion: Provides a foundation for designing secure, interoperable, and scalable ecosystems for LLM-powered agents, recommending a phased adoption approach.

Abstract: Large language model powered autonomous agents demand robust, standardized
protocols to integrate tools, share contextual data, and coordinate tasks
across heterogeneous systems. Ad-hoc integrations are difficult to scale,
secure, and generalize across domains. This survey examines four emerging agent
communication protocols: Model Context Protocol (MCP), Agent Communication
Protocol (ACP), Agent-to-Agent Protocol (A2A), and Agent Network Protocol
(ANP), each addressing interoperability in deployment contexts. MCP provides a
JSON-RPC client-server interface for secure tool invocation and typed data
exchange. ACP defines a general-purpose communication protocol over RESTful
HTTP, supporting MIME-typed multipart messages and synchronous and asynchronous
interactions. Its lightweight and runtime-independent design enables scalable
agent invocation, while features like session management, message routing, and
integration with role-based and decentralized identifiers (DIDs). A2A enables
peer-to-peer task delegation using capability-based Agent Cards, supporting
secure and scalable collaboration across enterprise agent workflows. ANP
supports open network agent discovery and secure collaboration using W3C
decentralized identifiers DIDs and JSON-LD graphs. The protocols are compared
across multiple dimensions, including interaction modes, discovery mechanisms,
communication patterns, and security models. Based on the comparative analysis,
a phased adoption roadmap is proposed: beginning with MCP for tool access,
followed by ACP for structured, multimodal messaging session-aware interaction
and both online and offline agent discovery across scalable, HTTP-based
deployments A2A for collaborative task execution, and extending to ANP for
decentralized agent marketplaces. This work provides a comprehensive foundation
for designing secure, interoperable, and scalable ecosystems of LLM-powered
agents.

</details>


### [514] [An alignment safety case sketch based on debate](https://arxiv.org/pdf/2505.03989)
*Marie Davidsen Buhl, Jacob Pfau, Benjamin Hilton, Geoffrey Irving*

Main category: cs.AI

TL;DR: The paper explores using debate between AI systems to ensure safety and alignment, focusing on preventing harmful actions like research sabotage.


<details>
  <summary>Details</summary>
Motivation: As AI systems surpass human capabilities, human judgment becomes insufficient for steering AI behavior, necessitating alternative methods like debate.

Method: Proposes training AI agents via debate with exploration guarantees to ensure honesty, supported by online training during deployment.

Result: Identifies four key claims for a safety case: debate proficiency, honesty from debate, honesty maintenance, and error tolerance.

Conclusion: Highlights open research problems to strengthen the argument for AI safety through debate.

Abstract: If AI systems match or exceed human capabilities on a wide range of tasks, it
may become difficult for humans to efficiently judge their actions -- making it
hard to use human feedback to steer them towards desirable traits. One proposed
solution is to leverage another superhuman system to point out flaws in the
system's outputs via a debate. This paper outlines the value of debate for AI
safety, as well as the assumptions and further research required to make debate
work. It does so by sketching an ``alignment safety case'' -- an argument that
an AI system will not autonomously take actions which could lead to egregious
harm, despite being able to do so. The sketch focuses on the risk of an AI R\&D
agent inside an AI company sabotaging research, for example by producing false
results. To prevent this, the agent is trained via debate, subject to
exploration guarantees, to teach the system to be honest. Honesty is maintained
throughout deployment via online training. The safety case rests on four key
claims: (1) the agent has become good at the debate game, (2) good performance
in the debate game implies that the system is mostly honest, (3) the system
will not become significantly less honest during deployment, and (4) the
deployment context is tolerant of some errors. We identify open research
problems that, if solved, could render this a compelling argument that an AI
system is safe.

</details>


### [515] [Explainability Through Human-Centric Design for XAI in Lung Cancer Detection](https://arxiv.org/pdf/2505.09755)
*Amy Rafferty, Rishi Ramaesh, Ajitha Rajan*

Main category: cs.AI

TL;DR: XpertXAI is an expert-driven, interpretable AI model for detecting multiple lung pathologies from chest X-rays, outperforming existing methods in accuracy and alignment with expert reasoning.


<details>
  <summary>Details</summary>
Motivation: Address the lack of interpretability in deep learning models for lung pathology detection, hindering clinical adoption.

Method: Extends ClinicXAI using an InceptionV3-based classifier and expert-guided concept bottleneck model (CBM), validated against radiologist annotations.

Result: XpertXAI outperforms baselines in accuracy and provides clinically meaningful explanations, aligning better with expert judgments.

Conclusion: Human-centric model design can scale to broader diagnostic contexts, advancing clinically meaningful explainable AI in medicine.

Abstract: Deep learning models have shown promise in lung pathology detection from
chest X-rays, but widespread clinical adoption remains limited due to opaque
model decision-making. In prior work, we introduced ClinicXAI, a human-centric,
expert-guided concept bottleneck model (CBM) designed for interpretable lung
cancer diagnosis. We now extend that approach and present XpertXAI, a
generalizable expert-driven model that preserves human-interpretable clinical
concepts while scaling to detect multiple lung pathologies. Using a
high-performing InceptionV3-based classifier and a public dataset of chest
X-rays with radiology reports, we compare XpertXAI against leading post-hoc
explainability methods and an unsupervised CBM, XCBs. We assess explanations
through comparison with expert radiologist annotations and medical ground
truth. Although XpertXAI is trained for multiple pathologies, our expert
validation focuses on lung cancer. We find that existing techniques frequently
fail to produce clinically meaningful explanations, omitting key diagnostic
features and disagreeing with radiologist judgments. XpertXAI not only
outperforms these baselines in predictive accuracy but also delivers
concept-level explanations that better align with expert reasoning. While our
focus remains on explainability in lung cancer detection, this work illustrates
how human-centric model design can be effectively extended to broader
diagnostic contexts - offering a scalable path toward clinically meaningful
explainable AI in medical diagnostics.

</details>


### [516] [SelfBudgeter: Adaptive Token Allocation for Efficient LLM Reasoning](https://arxiv.org/pdf/2505.11274)
*Zheng Li, Qingxiu Dong, Jingyuan Ma, Di Zhang, Zhifang Sui*

Main category: cs.AI

TL;DR: SelfBudgeter is a self-adaptive reasoning strategy that optimizes resource use by pre-estimating query difficulty and controlling output length, achieving significant efficiency without losing accuracy.


<details>
  <summary>Details</summary>
Motivation: Large reasoning models inefficiently process both simple and complex queries, wasting resources and increasing latency.

Method: Dual-phase training: pre-estimate reasoning cost and use budget-guided GPRO for reinforcement learning to control output length.

Result: Achieves up to 74.47% response length compression on MATH benchmark with maintained accuracy.

Conclusion: SelfBudgeter efficiently allocates budgets based on query complexity, improving resource use and user control.

Abstract: Recently, large reasoning models demonstrate exceptional performance on
various tasks. However, reasoning models inefficiently over-process both
trivial and complex queries, leading to resource waste and prolonged user
latency. To address this challenge, we propose SelfBudgeter - a self-adaptive
controllable reasoning strategy for efficient reasoning. Our approach adopts a
dual-phase training paradigm: first, the model learns to pre-estimate the
reasoning cost based on the difficulty of the query. Then, we introduce
budget-guided GPRO for reinforcement learning, which effectively maintains
accuracy while reducing output length. SelfBudgeter allows users to anticipate
generation time and make informed decisions about continuing or interrupting
the process. Furthermore, our method enables direct manipulation of reasoning
length via pre-filling token budget. Experimental results demonstrate that
SelfBudgeter can rationally allocate budgets according to problem complexity,
achieving up to 74.47% response length compression on the MATH benchmark while
maintaining nearly undiminished accuracy.

</details>


### [517] [SOCIA: An End-to-End Agentic Framework for Automated Cyber-Physical-Social Simulator Generation](https://arxiv.org/pdf/2505.12006)
*Yuncheng Hua, Ji Miao, Mehdi Jafari, Jianxiang Xie, Hao Xue, Flora D. Salim*

Main category: cs.AI

TL;DR: SOCIA is an LLM-based framework automating high-fidelity CPS simulator generation, reducing manual effort and improving scalability.


<details>
  <summary>Details</summary>
Motivation: Addresses labor-intensive manual simulator development and complex data calibration in CPS systems.

Method: Uses a centralized orchestration manager to coordinate specialized agents for data comprehension, code generation, simulation execution, and feedback loops.

Result: Empirical evaluations show SOCIA produces high-fidelity, scalable simulations for diverse CPS tasks with minimal human intervention.

Conclusion: SOCIA offers a scalable solution for studying complex CPS phenomena.

Abstract: This paper introduces SOCIA (Simulation Orchestration for
Cyber-physical-social Intelligence and Agents), a novel end-to-end framework
leveraging Large Language Model (LLM)-based multi-agent systems to automate the
generation of high-fidelity Cyber-Physical-Social (CPS) simulators. Addressing
the challenges of labor-intensive manual simulator development and complex data
calibration, SOCIA integrates a centralized orchestration manager that
coordinates specialized agents for tasks including data comprehension, code
generation, simulation execution, and iterative evaluation-feedback loops.
Through empirical evaluations across diverse CPS tasks, such as mask adoption
behavior simulation (social), personal mobility generation (physical), and user
modeling (cyber), SOCIA demonstrates its ability to produce high-fidelity,
scalable simulations with reduced human intervention. These results highlight
SOCIA's potential to offer a scalable solution for studying complex CPS
phenomena

</details>


### [518] [NeuroGen: Neural Network Parameter Generation via Large Language Models](https://arxiv.org/pdf/2505.12470)
*Jiaqi Wang, Yusen Zhang, Xi Li*

Main category: cs.AI

TL;DR: NeuroGen explores using LLMs to generate NN parameters via a two-stage approach, showing feasibility and synergy between LLMs and NNs.


<details>
  <summary>Details</summary>
Motivation: Traditional NN parameter acquisition methods are iterative; this paper investigates a novel LLM-based approach for efficiency and adaptability.

Method: NeuroGen: (1) Pretrain LLMs on NN checkpoints for parameter understanding, (2) Fine-tune with task-aware prompts for specific NN generation.

Result: NeuroGen successfully generates usable NN parameters, demonstrating LLM-based parameter generation is feasible.

Conclusion: LLMs can synergize with lightweight NNs, offering a new paradigm for parameter acquisition.

Abstract: Acquiring the parameters of neural networks (NNs) has been one of the most
important problems in machine learning since the inception of NNs. Traditional
approaches, such as backpropagation and forward-only optimization, acquire
parameters via iterative data fitting to gradually optimize them. This paper
aims to explore the feasibility of a new direction: acquiring NN parameters via
large language model generation. We propose NeuroGen, a generalized and
easy-to-implement two-stage approach for NN parameter generation conditioned on
descriptions of the data, task, and network architecture. Stage one is
Parameter Reference Knowledge Injection, where LLMs are pretrained on NN
checkpoints to build foundational understanding of parameter space, whereas
stage two is Context-Enhanced Instruction Tuning, enabling LLMs to adapt to
specific tasks through enriched, task-aware prompts. Experimental results
demonstrate that NeuroGen effectively generates usable NN parameters. Our
findings highlight the feasibility of LLM-based NN parameter generation and
suggest a promising new paradigm where LLMs and lightweight NNs can coexist
synergistically

</details>


### [519] [Correspondence of high-dimensional emotion structures elicited by video clips between humans and Multimodal LLMs](https://arxiv.org/pdf/2505.12746)
*Haruka Asanuma, Naoko Koide-Majima, Ken Nakamura, Takato Horii, Shinji Nishimoto, Masafumi Oizumi*

Main category: cs.AI

TL;DR: Current MLLMs capture high-dimensional human emotion structures well at the category level but struggle with single-item accuracy.


<details>
  <summary>Details</summary>
Motivation: To assess if MLLMs can capture the complex, high-dimensional nature of human emotions, which conventional models often overlook.

Method: Compared self-reported human emotion ratings with MLLM-generated estimates (e.g., Gemini or GPT) using correlation and Gromov Wasserstein Optimal Transport.

Result: Strong similarity at the category level but lower accuracy for single items.

Conclusion: MLLMs broadly capture complex emotion structures at the category level but have limitations in single-item precision.

Abstract: Recent studies have revealed that human emotions exhibit a high-dimensional,
complex structure. A full capturing of this complexity requires new approaches,
as conventional models that disregard high dimensionality risk overlooking key
nuances of human emotions. Here, we examined the extent to which the latest
generation of rapidly evolving Multimodal Large Language Models (MLLMs) capture
these high-dimensional, intricate emotion structures, including capabilities
and limitations. Specifically, we compared self-reported emotion ratings from
participants watching videos with model-generated estimates (e.g., Gemini or
GPT). We evaluated performance not only at the individual video level but also
from emotion structures that account for inter-video relationships. At the
level of simple correlation between emotion structures, our results
demonstrated strong similarity between human and model-inferred emotion
structures. To further explore whether the similarity between humans and models
is at the signle item level or the coarse-categorical level, we applied Gromov
Wasserstein Optimal Transport. We found that although performance was not
necessarily high at the strict, single-item level, performance across video
categories that elicit similar emotions was substantial, indicating that the
model could infer human emotional experiences at the category level. Our
results suggest that current state-of-the-art MLLMs broadly capture the complex
high-dimensional emotion structures at the category level, as well as their
apparent limitations in accurately capturing entire structures at the
single-item level.

</details>


### [520] [Unearthing Gems from Stones: Policy Optimization with Negative Sample Augmentation for LLM Reasoning](https://arxiv.org/pdf/2505.14403)
*Zhaohui Yang, Shilei Jiang, Chen Hu, Linjing Li, Shihong Deng, Daxin Jiang*

Main category: cs.AI

TL;DR: BCPG-NSA is a fine-grained offline RL framework that leverages negative samples for improved reasoning model performance.


<details>
  <summary>Details</summary>
Motivation: Existing methods discard or poorly utilize negative samples, missing valuable learning signals like self-reflection and error-correction.

Method: BCPG-NSA involves sample segmentation, consensus-based step correctness assessment, and policy optimization with Negative Sample Augmentation (NSA).

Result: BCPG-NSA outperforms baselines on math/coding benchmarks, showing improved sample efficiency, robustness, and scalability.

Conclusion: The framework effectively mines positive steps from negative samples, enhancing reasoning model performance.

Abstract: Recent advances in reasoning language models have witnessed a paradigm shift
from short to long CoT pattern. Given the substantial computational cost of
rollouts in long CoT models, maximizing the utility of fixed training datasets
becomes crucial. Our analysis reveals that negative responses contain valuable
components such as self-reflection and error-correction steps, yet primary
existing methods either completely discard negative samples (RFT) or apply
equal penalization across all tokens (RL), failing to leverage these potential
learning signals. In light of this, we propose Behavior Constrained Policy
Gradient with Negative Sample Augmentation (BCPG-NSA), a fine-grained offline
RL framework that encompasses three stages: 1) sample segmentation, 2)
consensus-based step correctness assessment combining LLM and PRM judgers, and
3) policy optimization with NSA designed to effectively mine positive steps
within negative samples. Experimental results show that BCPG-NSA outperforms
baselines on several challenging math/coding reasoning benchmarks using the
same training dataset, achieving improved sample efficiency and demonstrating
robustness and scalability when extended to multiple iterations.

</details>


### [521] [When to Continue Thinking: Adaptive Thinking Mode Switching for Efficient Reasoning](https://arxiv.org/pdf/2505.15400)
*Xiaoyun Zhang, Jingqing Ruan, Xing Ma, Yawen Zhu, Haodong Zhao, Hao Li, Jiansong Chen, Ke Zeng, Xunliang Cai*

Main category: cs.AI

TL;DR: ASRR improves efficiency in large reasoning models by reducing redundant reasoning and enabling implicit recovery, achieving significant computational savings with minimal accuracy loss.


<details>
  <summary>Details</summary>
Motivation: Large reasoning models (LRMs) often waste computational resources on simple tasks due to redundant reasoning. This work aims to quantify and address this inefficiency.

Method: Proposes Adaptive Self-Recovery Reasoning (ASRR), a framework that suppresses unnecessary reasoning and uses accuracy-aware length reward regulation to adaptively allocate effort.

Result: ASRR reduces reasoning budget by up to 32.5% (1.5B) and 25.7% (7B) with minimal accuracy loss (1.2% and 0.6% pass@1) and improves safety benchmarks by up to +21.7%.

Conclusion: ASRR demonstrates potential for efficient, adaptive, and safer reasoning in LRMs, balancing performance and computational cost.

Abstract: Large reasoning models (LRMs) achieve remarkable performance via long
reasoning chains, but often incur excessive computational overhead due to
redundant reasoning, especially on simple tasks. In this work, we
systematically quantify the upper bounds of LRMs under both Long-Thinking and
No-Thinking modes, and uncover the phenomenon of "Internal Self-Recovery
Mechanism" where models implicitly supplement reasoning during answer
generation. Building on this insight, we propose Adaptive Self-Recovery
Reasoning (ASRR), a framework that suppresses unnecessary reasoning and enables
implicit recovery. By introducing accuracy-aware length reward regulation, ASRR
adaptively allocates reasoning effort according to problem difficulty,
achieving high efficiency with negligible performance sacrifice. Experiments
across multiple benchmarks and models show that, compared with GRPO, ASRR
reduces reasoning budget by up to 32.5% (1.5B) and 25.7% (7B) with minimal
accuracy loss (1.2% and 0.6% pass@1), and significantly boosts harmless rates
on safety benchmarks (up to +21.7%). Our results highlight the potential of
ASRR for enabling efficient, adaptive, and safer reasoning in LRMs.

</details>


### [522] [Incentivizing Dual Process Thinking for Efficient Large Language Model Reasoning](https://arxiv.org/pdf/2505.16315)
*Xiaoxue Cheng, Junyi Li, Zhenduo Zhang, Xinyu Tang, Wayne Xin Zhao, Xinyu Kong, Zhiqiang Zhang*

Main category: cs.AI

TL;DR: ACPO is a reinforcement learning framework to reduce redundant reasoning in large models by adaptive cognitive allocation and dynamic system switch.


<details>
  <summary>Details</summary>
Motivation: Large reasoning models (LRMs) often overthink, generating unnecessary content regardless of task difficulty, which inspired the need for efficient reasoning.

Method: ACPO uses system-aware reasoning tokens and online difficulty estimation to guide adaptive reasoning. It involves a two-stage training: supervised fine-tuning followed by reinforcement learning.

Result: ACPO reduces redundant reasoning and adapts cognitive allocation based on task complexity, achieving efficient hybrid reasoning.

Conclusion: ACPO successfully enhances reasoning efficiency in LRMs by dynamically adjusting cognitive processes.

Abstract: Large reasoning models (LRMs) have demonstrated strong performance on complex
reasoning tasks, but often suffer from overthinking, generating redundant
content regardless of task difficulty. Inspired by the dual process theory in
cognitive science, we propose Adaptive Cognition Policy Optimization (ACPO), a
reinforcement learning framework that enables LRMs to achieve efficient
reasoning through adaptive cognitive allocation and dynamic system switch. ACPO
incorporates two key components: (1) introducing system-aware reasoning tokens
to explicitly represent the thinking modes thereby making the model's cognitive
process transparent, and (2) integrating online difficulty estimation and token
length budget to guide adaptive system switch and reasoning during
reinforcement learning. To this end, we propose a two-stage training strategy.
The first stage begins with supervised fine-tuning to cold start the model,
enabling it to generate reasoning paths with explicit thinking modes. In the
second stage, we apply ACPO to further enhance adaptive system switch for
difficulty-aware reasoning. Experimental results demonstrate that ACPO
effectively reduces redundant reasoning while adaptively adjusting cognitive
allocation based on task complexity, achieving efficient hybrid reasoning.

</details>


### [523] [SMART: Self-Generating and Self-Validating Multi-Dimensional Assessment for LLMs' Mathematical Problem Solving](https://arxiv.org/pdf/2505.16646)
*Yujie Hou, Ting Zhang, Mei Wang, Xuetao Ma, Hua Huang*

Main category: cs.AI

TL;DR: SMART introduces a multi-dimensional framework to evaluate LLMs' mathematical reasoning beyond final answer accuracy, revealing discrepancies in their abilities.


<details>
  <summary>Details</summary>
Motivation: Current metrics like final answer accuracy fail to assess genuine mathematical reasoning in LLMs, necessitating a more nuanced evaluation framework.

Method: SMART decomposes problem-solving into four dimensions (understanding, reasoning, arithmetic, reflection & refinement) and uses automated self-generating and self-validating tasks for evaluation.

Result: Applied to 21 LLMs, SMART uncovered significant ability discrepancies across dimensions, showing final answer accuracy's inadequacy.

Conclusion: SMART highlights the need for holistic metrics to better capture LLMs' true problem-solving capabilities.

Abstract: Large Language Models have achieved remarkable results on a variety of
mathematical benchmarks. However, concerns remain as to whether these successes
reflect genuine mathematical reasoning or superficial pattern recognition.
Common evaluation metrics, such as final answer accuracy, fail to disentangle
the underlying competencies involved, offering limited diagnostic value. To
address these limitations, we introduce SMART: a Self-Generating and
Self-Validating Multi-Dimensional Assessment Framework. SMART decomposes
mathematical problem solving into four distinct dimensions: understanding,
reasoning, arithmetic, and reflection \& refinement. Each dimension is
evaluated independently through tailored tasks, enabling interpretable and
fine-grained analysis of LLM behavior. Crucially, SMART integrates an automated
self-generating and self-validating mechanism to produce and verify benchmark
data, ensuring both scalability and reliability. We apply SMART to 21
state-of-the-art open- and closed-source LLMs, uncovering significant
discrepancies in their abilities across different dimensions. Our findings
demonstrate the inadequacy of final answer accuracy as a sole metric and
motivate a new holistic metric to better capture true problem-solving
capabilities. Code and benchmarks will be released upon acceptance.

</details>


### [524] [Think or Not? Selective Reasoning via Reinforcement Learning for Vision-Language Models](https://arxiv.org/pdf/2505.16854)
*Jiaqi Wang, Kevin Qinghong Lin, James Cheng, Mike Zheng Shou*

Main category: cs.AI

TL;DR: TON is a two-stage training strategy for VLMs that reduces unnecessary reasoning steps, cutting completion length by up to 90% without performance loss.


<details>
  <summary>Details</summary>
Motivation: To mimic human-like selective reasoning, enabling VLMs to skip unnecessary reasoning for efficiency.

Method: Two-stage: (i) SFT with 'thought dropout' for selective reasoning, (ii) GRPO for optimizing task-aware rewards.

Result: Reduces completion length by 90% vs. GRPO, maintains or improves performance, and learns to skip unnecessary reasoning.

Conclusion: TON advances human-like reasoning in RL, offering efficiency gains without sacrificing accuracy.

Abstract: Reinforcement Learning (RL) has proven to be an effective post-training
strategy for enhancing reasoning in vision-language models (VLMs). Group
Relative Policy Optimization (GRPO) is a recent prominent method that
encourages models to generate complete reasoning traces before answering,
leading to increased token usage and computational cost. Inspired by the
human-like thinking process-where people skip reasoning for easy questions but
think carefully when needed-we explore how to enable VLMs to first decide when
reasoning is necessary. To realize this, we propose TON, a two-stage training
strategy: (i) a supervised fine-tuning (SFT) stage with a simple yet effective
'thought dropout' operation, where reasoning traces are randomly replaced with
empty thoughts. This introduces a think-or-not format that serves as a cold
start for selective reasoning; (ii) a GRPO stage that enables the model to
freely explore when to think or not, while maximizing task-aware outcome
rewards. Experimental results show that TON can reduce the completion length by
up to 90% compared to vanilla GRPO, without sacrificing performance or even
improving it. Further evaluations across diverse vision-language tasks-covering
a range of reasoning difficulties under both 3B and 7B models-consistently
reveal that the model progressively learns to bypass unnecessary reasoning
steps as training advances. These findings shed light on the path toward
human-like reasoning patterns in reinforcement learning approaches. Our code is
available at https://github.com/kokolerk/TON.

</details>


<div id='cs.SD'></div>

# cs.SD [[Back]](#toc)

### [525] [ReMi: A Random Recurrent Neural Network Approach to Music Production](https://arxiv.org/pdf/2505.17023)
*Hugo Chateau-Laurent, Tara Vanhatalo*

Main category: cs.SD

TL;DR: Randomly initialized RNNs generate rich, configurable arpeggios and oscillations, aiding musicians without heavy computation or data.


<details>
  <summary>Details</summary>
Motivation: Address concerns about AI's energy use, copyright, and creativity by offering a lightweight, data-free tool for musicians.

Method: Use randomly initialized recurrent neural networks (RNNs) to produce musical elements like arpeggios and oscillations.

Result: Produces rich, configurable musical outputs without needing training data or high computational power.

Conclusion: This approach enhances musician creativity sustainably, avoiding pitfalls of traditional AI music generation.

Abstract: Generative artificial intelligence raises concerns related to energy
consumption, copyright infringement and creative atrophy. We show that randomly
initialized recurrent neural networks can produce arpeggios and low-frequency
oscillations that are rich and configurable. In contrast to end-to-end music
generation that aims to replace musicians, our approach expands their
creativity while requiring no data and much less computational power. More
information can be found at: https://allendia.com/

</details>


### [526] [Understanding the Algorithm Behind Audio Key Detection](https://arxiv.org/pdf/2505.17259)
*Henrique Perez G. Silva*

Main category: cs.SD

TL;DR: The paper presents an algorithmic method for automatic key detection in music using digital signal processing and key profile comparisons.


<details>
  <summary>Details</summary>
Motivation: Automating musical key detection is crucial for Music Information Retrieval (MIR) to provide harmonic context for melodies and chord progressions.

Method: The methodology involves analyzing tonal content of audio recordings using digital signal processing and comparing it with theoretical key profiles.

Result: The algorithm estimates the musical key of an audio recording.

Conclusion: The proposed method contributes to MIR by automating key detection, enhancing harmonic analysis in music.

Abstract: The determination of musical key is a fundamental aspect of music theory and
perception, providing a harmonic context for melodies and chord progressions.
Automating this process, known as automatic key detection, is a significant
task in the field of Music Information Retrieval (MIR). This article outlines
an algorithmic methodology for estimating the musical key of an audio recording
by analyzing its tonal content through digital signal processing techniques and
comparison with theoretical key profiles.

</details>


### [527] [LLM-based Generative Error Correction for Rare Words with Synthetic Data and Phonetic Context](https://arxiv.org/pdf/2505.17410)
*Natsuo Yamashita, Masaaki Yamamoto, Hiroaki Kokubo, Yohei Kawaguchi*

Main category: cs.SD

TL;DR: A novel LLM-based GER approach improves ASR performance by targeting rare words and incorporating phonetic cues, reducing WER and CER.


<details>
  <summary>Details</summary>
Motivation: Existing LLM-based GER struggles with rare/domain-specific words and neglects phonetic cues, causing over-correction.

Method: Synthetic data generation for rare words and integrating ASR's N-best hypotheses with phonetic context.

Result: Improved rare word correction and reduced WER/CER in English and Japanese datasets.

Conclusion: The proposed method effectively addresses limitations of current GER approaches by leveraging phonetic information and synthetic data.

Abstract: Generative error correction (GER) with large language models (LLMs) has
emerged as an effective post-processing approach to improve automatic speech
recognition (ASR) performance. However, it often struggles with rare or
domain-specific words due to limited training data. Furthermore, existing
LLM-based GER approaches primarily rely on textual information, neglecting
phonetic cues, which leads to over-correction. To address these issues, we
propose a novel LLM-based GER approach that targets rare words and incorporates
phonetic information. First, we generate synthetic data to contain rare words
for fine-tuning the GER model. Second, we integrate ASR's N-best hypotheses
along with phonetic context to mitigate over-correction. Experimental results
show that our method not only improves the correction of rare words but also
reduces the WER and CER across both English and Japanese datasets.

</details>


### [528] [UniTTS: An end-to-end TTS system without decoupling of acoustic and semantic information](https://arxiv.org/pdf/2505.17426)
*Rui Wang, Qianguo Sun, Tianrong Chen, Zhiyun Zeng, Junlong Wu, Jiaxing Zhang*

Main category: cs.SD

TL;DR: The paper proposes DistilCodec and UniTTS to address limitations of multi-codebook audio codecs in LLM-based TTS, offering efficient single-codebook distillation, expanded data diversity, and enhanced multimodal capabilities.


<details>
  <summary>Details</summary>
Motivation: Multi-codebook audio codecs like RVQ and GVQ limit LLM-based TTS by restricting access to comprehensive audio information due to misalignment of semantic and acoustic data.

Method: Proposes DistilCodec (single-codebook distillation) and UniTTS (multimodal pre-training with autoregression tasks). Uses a three-stage training process: Pre-Training, SFT, and Alignment.

Result: Achieves near 100% code utilization, expands data diversity, and preserves LLM text capabilities while handling interleaved text/audio prompts.

Conclusion: DistilCodec and UniTTS effectively overcome limitations of multi-codebook codecs, enhancing LLM-based TTS performance and versatility.

Abstract: The emergence of multi-codebook neutral audio codecs such as Residual Vector
Quantization (RVQ) and Group Vector Quantization (GVQ) has significantly
advanced Large-Language-Model (LLM) based Text-to-Speech (TTS) systems. These
codecs are crucial in separating semantic and acoustic information while
efficiently harnessing semantic priors. However, since semantic and acoustic
information cannot be fully aligned, a significant drawback of these methods
when applied to LLM-based TTS is that large language models may have limited
access to comprehensive audio information. To address this limitation, we
propose DistilCodec and UniTTS, which collectively offer the following
advantages: 1) This method can distill a multi-codebook audio codec into a
single-codebook audio codec with 32,768 codes while achieving a near 100\%
utilization. 2) As DistilCodec does not employ a semantic alignment scheme, a
large amount of high-quality unlabeled audio (such as audiobooks with sound
effects, songs, etc.) can be incorporated during training, further expanding
data diversity and broadening its applicability. 3) Leveraging the
comprehensive audio information modeling of DistilCodec, we integrated three
key tasks into UniTTS's pre-training framework: audio modality autoregression,
text modality autoregression, and speech-text cross-modal autoregression. This
allows UniTTS to accept interleaved text and speech/audio prompts while
substantially preserving LLM's text capabilities. 4) UniTTS employs a
three-stage training process: Pre-Training, Supervised Fine-Tuning (SFT), and
Alignment. Source code and model checkpoints are publicly available at
https://github.com/IDEA-Emdoor-Lab/UniTTS and
https://github.com/IDEA-Emdoor-Lab/DistilCodec.

</details>


### [529] [MEGADance: Mixture-of-Experts Architecture for Genre-Aware 3D Dance Generation](https://arxiv.org/pdf/2505.17543)
*Kaixing Yang, Xulong Tang, Ziqiao Peng, Yuxuan Hu, Jun He, Hongyan Liu*

Main category: cs.SD

TL;DR: MEGADance is a novel architecture for music-driven 3D dance generation, improving genre conditioning and synchronization through a two-stage process.


<details>
  <summary>Details</summary>
Motivation: Traditional methods underutilize genre conditioning, leading to poor synchronization and genre continuity.

Method: Two-stage architecture: High-Fidelity Dance Quantization (HFDQ) and Genre-Aware Dance Generation (GADG).

Result: State-of-the-art performance on FineDance and AIST++ datasets, with strong genre controllability.

Conclusion: MEGADance addresses genre and synchronization issues, offering high-quality dance generation.

Abstract: Music-driven 3D dance generation has attracted increasing attention in recent
years, with promising applications in choreography, virtual reality, and
creative content creation. Previous research has generated promising realistic
dance movement from audio signals. However, traditional methods underutilize
genre conditioning, often treating it as auxiliary modifiers rather than core
semantic drivers. This oversight compromises music-motion synchronization and
disrupts dance genre continuity, particularly during complex rhythmic
transitions, thereby leading to visually unsatisfactory effects. To address the
challenge, we propose MEGADance, a novel architecture for music-driven 3D dance
generation. By decoupling choreographic consistency into dance generality and
genre specificity, MEGADance demonstrates significant dance quality and strong
genre controllability. It consists of two stages: (1) High-Fidelity Dance
Quantization Stage (HFDQ), which encodes dance motions into a latent
representation by Finite Scalar Quantization (FSQ) and reconstructs them with
kinematic-dynamic constraints, and (2) Genre-Aware Dance Generation Stage
(GADG), which maps music into the latent representation by synergistic
utilization of Mixture-of-Experts (MoE) mechanism with Mamba-Transformer hybrid
backbone. Extensive experiments on the FineDance and AIST++ dataset demonstrate
the state-of-the-art performance of MEGADance both qualitatively and
quantitatively. Code will be released upon acceptance.

</details>


### [530] [CosyVoice 3: Towards In-the-wild Speech Generation via Scaling-up and Post-training](https://arxiv.org/pdf/2505.17589)
*Zhihao Du, Changfeng Gao, Yuxuan Wang, Fan Yu, Tianyu Zhao, Hao Wang, Xiang Lv, Hui Wang, Xian Shi, Keyu An, Guanrou Yang, Yabin Li, Yanni Chen, Zhifu Gao, Qian Chen, Yue Gu, Mengzhe Chen, Yafeng Chen, Shiliang Zhang, Wen Wang, Jieping Ye*

Main category: cs.SD

TL;DR: CosyVoice 3 improves upon CosyVoice 2 by enhancing multilingual speech synthesis with better prosody, speaker similarity, and content consistency, supported by a larger dataset and model size.


<details>
  <summary>Details</summary>
Motivation: Address limitations of CosyVoice 2 in language coverage, domain diversity, data volume, text formats, and post-training techniques.

Method: Introduces a novel speech tokenizer, a differentiable reward model, and scales dataset and model size (1M hours, 1.5B parameters).

Result: Achieves superior performance in zero-shot multilingual speech synthesis, with improved prosody, speaker similarity, and content consistency.

Conclusion: CosyVoice 3 significantly advances speech synthesis in the wild, encouraging further exploration via provided demos.

Abstract: In our prior works, we introduced a scalable streaming speech synthesis
model, CosyVoice 2, which integrates a large language model (LLM) and a
chunk-aware flow matching (FM) model, and achieves low-latency bi-streaming
speech synthesis and human-parity quality. Despite these advancements,
CosyVoice 2 exhibits limitations in language coverage, domain diversity, data
volume, text formats, and post-training techniques. In this paper, we present
CosyVoice 3, an improved model designed for zero-shot multilingual speech
synthesis in the wild, surpassing its predecessor in content consistency,
speaker similarity, and prosody naturalness. Key features of CosyVoice 3
include: 1) A novel speech tokenizer to improve prosody naturalness, developed
via supervised multi-task training, including automatic speech recognition,
speech emotion recognition, language identification, audio event detection, and
speaker analysis. 2) A new differentiable reward model for post-training
applicable not only to CosyVoice 3 but also to other LLM-based speech synthesis
models. 3) Dataset Size Scaling: Training data is expanded from ten thousand
hours to one million hours, encompassing 9 languages and 18 Chinese dialects
across various domains and text formats. 4) Model Size Scaling: Model
parameters are increased from 0.5 billion to 1.5 billion, resulting in enhanced
performance on our multilingual benchmark due to the larger model capacity.
These advancements contribute significantly to the progress of speech synthesis
in the wild. We encourage readers to listen to the demo at
https://funaudiollm.github.io/cosyvoice3.

</details>


### [531] [NBM: an Open Dataset for the Acoustic Monitoring of Nocturnal Migratory Birds in Europe](https://arxiv.org/pdf/2412.03633)
*Louis Airale, Adrien Pajot, Juliette Linossier*

Main category: cs.SD

TL;DR: The paper introduces the Nocturnal Bird Migration (NBM) dataset, featuring 13,359 annotated bird vocalizations from 117 species, and demonstrates its utility with a two-stage deep learning model for precise call detection in spectrograms.


<details>
  <summary>Details</summary>
Motivation: Migratory bird populations face threats, necessitating effective monitoring tools, especially for nocturnal species. Passive acoustic monitoring is a key solution.

Method: A two-stage deep object detection model is trained on the NBM dataset to localize bird calls in spectrograms.

Result: The model achieves competitive accuracy on 45 main species, comparable to state-of-the-art systems trained on larger datasets.

Conclusion: The study underscores the value of open-science initiatives for fine-grained audio annotations, with all data and code made publicly available.

Abstract: The persisting threats on migratory bird populations highlight the urgent
need for effective monitoring techniques that could assist in their
conservation. Among these, passive acoustic monitoring is an essential tool,
particularly for nocturnal migratory species that are difficult to track
otherwise. This work presents the Nocturnal Bird Migration (NBM) dataset, a
collection of 13,359 annotated vocalizations from 117 species of the Western
Palearctic. The dataset includes precise time and frequency annotations,
gathered by dozens of bird enthusiasts across France, enabling novel downstream
acoustic analysis. In particular, we prove the utility of this database by
training an original two-stage deep object detection model tailored for the
processing of audio data. While allowing the precise localization of bird calls
in spectrograms, this model shows competitive accuracy on the 45 main species
of the dataset with state-of-the-art systems trained on much larger audio
collections. These results highlight the interest of fostering similar
open-science initiatives to acquire costly but valuable fine-grained
annotations of audio files. All data and code are made openly available.

</details>


### [532] [Does Your Voice Assistant Remember? Analyzing Conversational Context Recall and Utilization in Voice Interaction Models](https://arxiv.org/pdf/2502.19759)
*Heeseung Kim, Che Hyun Lee, Sangkwon Park, Jiheum Yeom, Nohil Park, Sangwon Yu, Sungroh Yoon*

Main category: cs.SD

TL;DR: Open-source multi-turn voice interaction models struggle with retaining and recalling past utterances compared to closed-source models, especially in speech-based contexts.


<details>
  <summary>Details</summary>
Motivation: To evaluate the ability of open-source models to utilize past utterances, an area not previously explored.

Method: Systematic evaluation using ContextDialog, a proposed benchmark, comparing text-based and speech-based models, including retrieval-augmented generation.

Result: Speech-based models perform worse than text-based ones, and retrieval-augmented models still struggle with recalling past utterances.

Conclusion: Open-source models have limitations in memory retention and retrieval, suggesting areas for improvement.

Abstract: Recent advancements in multi-turn voice interaction models have improved
user-model communication. However, while closed-source models effectively
retain and recall past utterances, whether open-source models share this
ability remains unexplored. To fill this gap, we systematically evaluate how
well open-source interaction models utilize past utterances using
ContextDialog, a benchmark we proposed for this purpose. Our findings show that
speech-based models have more difficulty than text-based ones, especially when
recalling information conveyed in speech, and even with retrieval-augmented
generation, models still struggle with questions about past utterances. These
insights highlight key limitations in open-source models and suggest ways to
improve memory retention and retrieval robustness.

</details>


### [533] [CrossMuSim: A Cross-Modal Framework for Music Similarity Retrieval with LLM-Powered Text Description Sourcing and Mining](https://arxiv.org/pdf/2503.23128)
*Tristan Tsoi, Jiajun Deng, Yaolong Ju, Benno Weck, Holger Kirchhoff, Simon Lui*

Main category: cs.SD

TL;DR: A cross-modal contrastive learning framework uses text descriptions to improve music similarity retrieval, outperforming traditional methods.


<details>
  <summary>Details</summary>
Motivation: Address limitations of uni-modal approaches in capturing complex musical relationships by leveraging text descriptions.

Method: Introduces a dual-source data acquisition approach (online scraping and LLM-based prompting) to generate rich text-music pairs.

Result: Achieves significant performance improvements over benchmarks in objective metrics, subjective evaluations, and real-world A/B testing.

Conclusion: The framework effectively enhances music similarity modeling by integrating cross-modal learning and high-quality data generation.

Abstract: Music similarity retrieval is fundamental for managing and exploring relevant
content from large collections in streaming platforms. This paper presents a
novel cross-modal contrastive learning framework that leverages the open-ended
nature of text descriptions to guide music similarity modeling, addressing the
limitations of traditional uni-modal approaches in capturing complex musical
relationships. To overcome the scarcity of high-quality text-music paired data,
this paper introduces a dual-source data acquisition approach combining online
scraping and LLM-based prompting, where carefully designed prompts leverage
LLMs' comprehensive music knowledge to generate contextually rich descriptions.
Exten1sive experiments demonstrate that the proposed framework achieves
significant performance improvements over existing benchmarks through objective
metrics, subjective evaluations, and real-world A/B testing on the Huawei Music
streaming platform.

</details>


### [534] [EZ-VC: Easy Zero-shot Any-to-Any Voice Conversion](https://arxiv.org/pdf/2505.16691)
*Advait Joglekar, Divyanshu Singh, Rooshil Rohit Bhatia, S. Umesh*

Main category: cs.SD

TL;DR: The paper introduces a voice conversion model combining discrete speech representations with a Diffusion-Transformer decoder, excelling in zero-shot cross-lingual settings.


<details>
  <summary>Details</summary>
Motivation: Current voice conversion methods struggle with zero-shot cross-lingual generalization and unseen languages/accents.

Method: Uses discrete speech representations from self-supervised models and a non-autoregressive Diffusion-Transformer decoder, trained textlessly.

Result: The model performs well in zero-shot cross-lingual settings, even for unseen languages.

Conclusion: The proposed approach is effective for zero-shot voice conversion without needing multiple encoders.

Abstract: Voice Conversion research in recent times has increasingly focused on
improving the zero-shot capabilities of existing methods. Despite remarkable
advancements, current architectures still tend to struggle in zero-shot
cross-lingual settings. They are also often unable to generalize for speakers
of unseen languages and accents. In this paper, we adopt a simple yet effective
approach that combines discrete speech representations from self-supervised
models with a non-autoregressive Diffusion-Transformer based conditional flow
matching speech decoder. We show that this architecture allows us to train a
voice-conversion model in a purely textless, self-supervised fashion. Our
technique works without requiring multiple encoders to disentangle speech
features. Our model also manages to excel in zero-shot cross-lingual settings
even for unseen languages. For Demo: https://ez-vc.github.io/EZ-VC-Demo/

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [535] [Generalizing Large Language Model Usability Across Resource-Constrained](https://arxiv.org/pdf/2505.17040)
*Yun-Da Tsai*

Main category: cs.LG

TL;DR: The paper proposes methods to generalize LLM usability under real-world constraints, including multimodal integration, adversarial prompting, inference-time optimization, and low-resource domain solutions.


<details>
  <summary>Details</summary>
Motivation: To address limitations of existing LLM approaches, which rely on costly fine-tuning or fixed conditions, hindering generalization in unseen modalities, limited data, or resource-constrained environments.

Method: Introduces a text-centric alignment framework for multimodal integration, adversarial prompting for robustness, inference-time optimization via prompt search, and synthetic data pipelines for low-resource domains.

Result: Achieves seamless multimodal integration, improved robustness, efficient inference-time performance, and state-of-the-art results in low-resource tasks like Verilog code generation.

Conclusion: The contributions enhance LLM adaptability, scalability, and efficiency under practical constraints, offering alternatives to costly retraining or parameter scaling.

Abstract: Large Language Models (LLMs) have achieved remarkable success across a wide
range of natural language tasks, and recent efforts have sought to extend their
capabilities to multimodal domains and resource-constrained environments.
However, existing approaches often rely on costly supervised fine-tuning or
assume fixed training conditions, limiting their generalization when facing
unseen modalities, limited data, or restricted compute resources. This
dissertation presents a systematic study toward generalizing LLM usability
under real-world constraints. First, it introduces a robust text-centric
alignment framework that enables LLMs to seamlessly integrate diverse
modalities-including text, images, tables, and any modalities - via natural
language interfaces. This approach supports in-context adaptation to unseen or
dynamically changing modalities without requiring retraining. To enhance
robustness against noisy and missing modalities, an adversarial prompting
technique is proposed, generating semantically challenging perturbations at the
prompt level to stress-test model reliability. Beyond multimodal setting, the
dissertation investigates inference-time optimization strategies for LLMs,
leveraging prompt search and uncertainty quantification to improve performance
without additional model training. This perspective offers an efficient
alternative to scaling model parameters or retraining from scratch.
Additionally, the work addresses low-resource domains such as Verilog code
generation by designing correct-by-construction synthetic data pipelines and
logic-enhanced reasoning models, achieving state-of-the-art performance with
minimal data. Together, these contributions form a unified effort to enhance
the adaptability, scalability, and efficiency of large language models under
practical constraints.

</details>


### [536] [RAP: Runtime-Adaptive Pruning for LLM Inference](https://arxiv.org/pdf/2505.17138)
*Huanrong Liu, Chunlin Tian, Xuyang Wei, Jiaheng Dai, Qin Liu, Tianqi Wei, Qingbiao Li, Li Li*

Main category: cs.LG

TL;DR: RAP is an RL-driven elastic pruning framework that dynamically adjusts compression strategies for LLMs, optimizing memory usage by jointly considering model weights and KV-cache.


<details>
  <summary>Details</summary>
Motivation: Large language models (LLMs) face high computational and memory demands, and existing compression methods lack adaptability to runtime variations and heterogeneous KV-cache needs.

Method: RAP uses reinforcement learning to dynamically track and adjust compression strategies based on runtime memory budgets, workload, and device state, focusing on retaining high-utility components.

Result: RAP outperforms state-of-the-art baselines by efficiently managing memory usage for both model weights and KV-cache.

Conclusion: RAP successfully addresses the limitations of static compression methods, offering a dynamic and efficient solution for LLM deployment.

Abstract: Large language models (LLMs) excel at language understanding and generation,
but their enormous computational and memory requirements hinder deployment.
Compression offers a potential solution to mitigate these constraints. However,
most existing methods rely on fixed heuristics and thus fail to adapt to
runtime memory variations or heterogeneous KV-cache demands arising from
diverse user requests. To address these limitations, we propose RAP, an elastic
pruning framework driven by reinforcement learning (RL) that dynamically
adjusts compression strategies in a runtime-aware manner. Specifically, RAP
dynamically tracks the evolving ratio between model parameters and KV-cache
across practical execution. Recognizing that FFNs house most parameters,
whereas parameter -light attention layers dominate KV-cache formation, the RL
agent retains only those components that maximize utility within the current
memory budget, conditioned on instantaneous workload and device state.
Extensive experiments results demonstrate that RAP outperforms state-of-the-art
baselines, marking the first time to jointly consider model weights and
KV-cache on the fly.

</details>


### [537] [MetaSTH-Sleep: Towards Effective Few-Shot Sleep Stage Classification with Spatial-Temporal Hypergraph Enhanced Meta-Learning](https://arxiv.org/pdf/2505.17142)
*Jingyu Li, Tiehua Zhang, Jinze Wang, Yi Zhang, Yuhuan Li, Yifan Zhao, Zhishu Shen, Jiannan Liu*

Main category: cs.LG

TL;DR: MetaSTH-Sleep, a few-shot learning framework using spatial-temporal hypergraphs, improves sleep stage classification with limited data and enhances generalization across subjects.


<details>
  <summary>Details</summary>
Motivation: Manual sleep stage annotation is time-consuming, and existing deep learning methods struggle with limited data, poor generalization, and overlooked signal relationships.

Method: Proposes MetaSTH-Sleep, combining meta-learning with spatial-temporal hypergraphs to model EEG signal relationships and adapt quickly to new subjects.

Result: MetaSTH-Sleep outperforms existing methods, showing better generalization and performance with few labeled samples.

Conclusion: The framework offers a scalable solution for automated sleep stage annotation, aiding clinicians with limited data.

Abstract: Accurate classification of sleep stages based on bio-signals is fundamental
for automatic sleep stage annotation. Traditionally, this task relies on
experienced clinicians to manually annotate data, a process that is both
time-consuming and labor-intensive. In recent years, deep learning methods have
shown promise in automating this task. However, three major challenges remain:
(1) deep learning models typically require large-scale labeled datasets, making
them less effective in real-world settings where annotated data is limited; (2)
significant inter-individual variability in bio-signals often results in
inconsistent model performance when applied to new subjects, limiting
generalization; and (3) existing approaches often overlook the high-order
relationships among bio-signals, failing to simultaneously capture signal
heterogeneity and spatial-temporal dependencies. To address these issues, we
propose MetaSTH-Sleep, a few-shot sleep stage classification framework based on
spatial-temporal hypergraph enhanced meta-learning. Our approach enables rapid
adaptation to new subjects using only a few labeled samples, while the
hypergraph structure effectively models complex spatial interconnections and
temporal dynamics simultaneously in EEG signals. Experimental results
demonstrate that MetaSTH-Sleep achieves substantial performance improvements
across diverse subjects, offering valuable insights to support clinicians in
sleep stage annotation.

</details>


### [538] [Get Experience from Practice: LLM Agents with Record & Replay](https://arxiv.org/pdf/2505.17716)
*Erhu Feng, Wenbo Zhou, Zibin Liu, Le Chen, Yunpeng Dong, Cheng Zhang, Yisheng Zhao, Dong Du, Zhichao Hua, Yubin Xia, Haibo Chen*

Main category: cs.LG

TL;DR: AgentRR introduces a record-and-replay mechanism for AI agents to improve reliability, privacy, cost, and performance by capturing and reusing structured experiences.


<details>
  <summary>Details</summary>
Motivation: Address challenges like uncertainty and resource demands in LLM-based AI agents, which existing methods fail to fully resolve.

Method: AgentRR records agent interactions, summarizes them into structured experiences, and replays these for similar tasks, using multi-level abstraction and check functions.

Result: Proposes a framework for efficient, safe agent behavior by reusing experiences, with modes like task demonstration and model collaboration.

Conclusion: AgentRR offers a scalable solution to enhance AI agent performance and safety, with potential for cost reduction through shared experiences.

Abstract: AI agents, empowered by Large Language Models (LLMs) and communication
protocols such as MCP and A2A, have rapidly evolved from simple chatbots to
autonomous entities capable of executing complex, multi-step tasks,
demonstrating great potential. However, the LLMs' inherent uncertainty and
heavy computational resource requirements pose four significant challenges to
the development of safe and efficient agents: reliability, privacy, cost and
performance. Existing approaches, like model alignment, workflow constraints
and on-device model deployment, can partially alleviate some issues but often
with limitations, failing to fundamentally resolve these challenges.
  This paper proposes a new paradigm called AgentRR (Agent Record & Replay),
which introduces the classical record-and-replay mechanism into AI agent
frameworks. The core idea is to: 1. Record an agent's interaction trace with
its environment and internal decision process during task execution, 2.
Summarize this trace into a structured "experience" encapsulating the workflow
and constraints, and 3. Replay these experiences in subsequent similar tasks to
guide the agent's behavior. We detail a multi-level experience abstraction
method and a check function mechanism in AgentRR: the former balances
experience specificity and generality, while the latter serves as a trust
anchor to ensure completeness and safety during replay. In addition, we explore
multiple application modes of AgentRR, including user-recorded task
demonstration, large-small model collaboration and privacy-aware agent
execution, and envision an experience repository for sharing and reusing
knowledge to further reduce deployment cost.

</details>


### [539] [Efficient Training of Neural SDEs Using Stochastic Optimal Control](https://arxiv.org/pdf/2505.17150)
*Rembert Daems, Manfred Opper, Guillaume Crevecoeur, Tolga Birdal*

Main category: cs.LG

TL;DR: A hierarchical, control theory-inspired method for variational inference in neural SDEs, decomposing control terms into linear and non-linear parts for efficient training.


<details>
  <summary>Details</summary>
Motivation: Address computational challenges in VI for neural SDEs by leveraging control theory to simplify training.

Method: Decompose control terms into linear (optimal) and non-linear (neural network) components, using stochastic optimal control.

Result: Efficient training of neural SDEs with faster convergence and lower initialization cost.

Conclusion: The method balances expressive power and computational efficiency, improving VI for neural SDEs.

Abstract: We present a hierarchical, control theory inspired method for variational
inference (VI) for neural stochastic differential equations (SDEs). While VI
for neural SDEs is a promising avenue for uncertainty-aware reasoning in
time-series, it is computationally challenging due to the iterative nature of
maximizing the ELBO. In this work, we propose to decompose the control term
into linear and residual non-linear components and derive an optimal control
term for linear SDEs, using stochastic optimal control. Modeling the non-linear
component by a neural network, we show how to efficiently train neural SDEs
without sacrificing their expressive power. Since the linear part of the
control term is optimal and does not need to be learned, the training is
initialized at a lower cost and we observe faster convergence.

</details>


### [540] [TrimR: Verifier-based Training-Free Thinking Compression for Efficient Test-Time Scaling](https://arxiv.org/pdf/2505.17155)
*Weizhe Lin, Xing Li, Zhiyuan Yang, Xiaojin Fu, Hui-Ling Zhen, Yaoyuan Wang, Xianzhi Yu, Wulong Liu, Xiaosong Li, Mingxuan Yuan*

Main category: cs.LG

TL;DR: TrimR is a verifier-based framework for dynamic Chain-of-Thought (CoT) compression, improving reasoning efficiency in Large Reasoning Models (LRMs) without fine-tuning.


<details>
  <summary>Details</summary>
Motivation: LRMs generate redundant CoTs, causing inefficiency. TrimR aims to trim these redundancies, inspired by human cognition and optimization theories.

Method: Uses a lightweight verifier to detect and truncate redundant CoTs dynamically, without fine-tuning LRMs or the verifier.

Result: Improves reasoning runtime by up to 70% on benchmarks like MATH500, AIME24, AIME25, and GPQA, with minimal accuracy loss.

Conclusion: TrimR enhances LRM efficiency for production-level deployment, offering significant runtime gains without compromising accuracy.

Abstract: Large Reasoning Models (LRMs) demonstrate exceptional capability in tackling
complex mathematical, logical, and coding tasks by leveraging extended
Chain-of-Thought (CoT) reasoning. Test-time scaling methods, such as prolonging
CoT with explicit token-level exploration, can push LRMs' accuracy boundaries,
but they incur significant decoding overhead. A key inefficiency source is LRMs
often generate redundant thinking CoTs, which demonstrate clear structured
overthinking and underthinking patterns. Inspired by human cognitive reasoning
processes and numerical optimization theories, we propose TrimR, a
verifier-based, training-free, efficient framework for dynamic CoT compression
to trim reasoning and enhance test-time scaling, explicitly tailored for
production-level deployment. Our method employs a lightweight, pretrained,
instruction-tuned verifier to detect and truncate redundant intermediate
thoughts of LRMs without any LRM or verifier fine-tuning. We present both the
core algorithm and asynchronous online system engineered for high-throughput
industrial applications. Empirical evaluations on Ascend NPUs and vLLM show
that our framework delivers substantial gains in inference efficiency under
large-batch workloads. In particular, on the four MATH500, AIME24, AIME25, and
GPQA benchmarks, the reasoning runtime of Pangu-R-38B, QwQ-32B, and
DeepSeek-R1-Distill-Qwen-32B is improved by up to 70% with negligible impact on
accuracy.

</details>


### [541] [OCR-Reasoning Benchmark: Unveiling the True Capabilities of MLLMs in Complex Text-Rich Image Reasoning](https://arxiv.org/pdf/2505.17163)
*Mingxin Huang, Yongxin Shi, Dezhi Peng, Songxuan Lai, Zecheng Xie, Lianwen Jin*

Main category: cs.LG

TL;DR: OCR-Reasoning is a new benchmark for evaluating Multimodal Large Language Models (MLLMs) on text-rich image reasoning tasks, highlighting their limitations.


<details>
  <summary>Details</summary>
Motivation: Existing benchmarks lack systematic evaluation of MLLMs in text-rich image reasoning, prompting the creation of OCR-Reasoning.

Method: OCR-Reasoning includes 1,069 annotated examples across 6 reasoning abilities and 18 tasks, assessing both answers and reasoning processes.

Result: State-of-the-art MLLMs perform poorly, with none exceeding 50% accuracy, revealing significant challenges.

Conclusion: Text-rich image reasoning remains a critical unsolved problem, and OCR-Reasoning provides a tool for future research.

Abstract: Recent advancements in multimodal slow-thinking systems have demonstrated
remarkable performance across diverse visual reasoning tasks. However, their
capabilities in text-rich image reasoning tasks remain understudied due to the
lack of a systematic benchmark. To address this gap, we propose OCR-Reasoning,
a comprehensive benchmark designed to systematically assess Multimodal Large
Language Models on text-rich image reasoning tasks. The benchmark comprises
1,069 human-annotated examples spanning 6 core reasoning abilities and 18
practical reasoning tasks in text-rich visual scenarios. Furthermore, unlike
other text-rich image understanding benchmarks that only annotate the final
answers, OCR-Reasoning also annotates the reasoning process simultaneously.
With the annotated reasoning process and the final answers, OCR-Reasoning
evaluates not only the final answers generated by models but also their
reasoning processes, enabling a holistic analysis of their problem-solving
abilities. Leveraging this benchmark, we conducted a comprehensive evaluation
of state-of-the-art MLLMs. Our results demonstrate the limitations of existing
methodologies. Notably, even state-of-the-art MLLMs exhibit substantial
difficulties, with none achieving accuracy surpassing 50\% across
OCR-Reasoning, indicating that the challenges of text-rich image reasoning are
an urgent issue to be addressed. The benchmark and evaluation scripts are
available at https://github.com/SCUT-DLVCLab/OCR-Reasoning.

</details>


### [542] [Tropical Attention: Neural Algorithmic Reasoning for Combinatorial Algorithms](https://arxiv.org/pdf/2505.17190)
*Baran Hashemi, Kurt Pasque, Chris Teska, Ruriko Yoshida*

Main category: cs.LG

TL;DR: The paper introduces Tropical attention, a novel attention function in the max-plus semiring, to address the limitations of softmax-based attention in Neural Algorithmic Reasoning for combinatorial optimization problems. It shows improved OOD performance and stability under adversarial attacks.


<details>
  <summary>Details</summary>
Motivation: Existing softmax-based attention blurs sharp polyhedral structures in DP algorithms and fails in OOD settings. The need for a more robust and scale-invariant attention mechanism motivated the development of Tropical attention.

Method: The authors propose Tropical attention, which operates in the max-plus semiring, and integrate it into transformers. They prove its ability to approximate tropical circuits of DP-type algorithms and evaluate it on algorithmic reasoning tasks.

Result: Tropical attention outperforms softmax baselines in OOD performance (length and value generalization) and remains stable under adversarial attacks. It restores sharp, scale-invariant reasoning.

Conclusion: Tropical attention is a promising alternative to softmax for Neural Algorithmic Reasoning, offering improved robustness and generalization in combinatorial optimization tasks.

Abstract: Dynamic programming (DP) algorithms for combinatorial optimization problems
work with taking maximization, minimization, and classical addition in their
recursion algorithms. The associated value functions correspond to convex
polyhedra in the max plus semiring. Existing Neural Algorithmic Reasoning
models, however, rely on softmax-normalized dot-product attention where the
smooth exponential weighting blurs these sharp polyhedral structures and
collapses when evaluated on out-of-distribution (OOD) settings. We introduce
Tropical attention, a novel attention function that operates natively in the
max-plus semiring of tropical geometry. We prove that Tropical attention can
approximate tropical circuits of DP-type combinatorial algorithms. We then
propose that using Tropical transformers enhances empirical OOD performance in
both length generalization and value generalization, on algorithmic reasoning
tasks, surpassing softmax baselines while remaining stable under adversarial
attacks. We also present adversarial-attack generalization as a third axis for
Neural Algorithmic Reasoning benchmarking. Our results demonstrate that
Tropical attention restores the sharp, scale-invariant reasoning absent from
softmax.

</details>


### [543] [Scalable Ride-Sourcing Vehicle Rebalancing with Service Accessibility Guarantee: A Constrained Mean-Field Reinforcement Learning Approach](https://arxiv.org/pdf/2503.24183)
*Matej Jusup, Kenan Zhang, Zhiyuan Hu, Barna Pásztor, Andreas Krause, Francesco Corman*

Main category: cs.LG

TL;DR: The paper introduces scalable mean-field control and reinforcement learning models for efficient and equitable vehicle rebalancing in ride-sourcing services, outperforming traditional methods.


<details>
  <summary>Details</summary>
Motivation: Address operational challenges like spatiotemporal mismatches in supply-demand and fairness issues in ride-sourcing services.

Method: Uses continuous-state mean-field control (MFC) and mean-field reinforcement learning (MFRL) with accessibility constraints for vehicle repositioning.

Result: Demonstrates real-time efficiency, scalability to large fleets, and outperforms benchmarks in fleet utilization, fulfilled requests, and equitable service access.

Conclusion: The approach effectively balances efficiency and equity, offering a robust solution for ride-sourcing rebalancing.

Abstract: The rapid expansion of ride-sourcing services such as Uber, Lyft, and Didi
Chuxing has fundamentally reshaped urban transportation by offering flexible,
on-demand mobility via mobile applications. Despite their convenience, these
platforms confront significant operational challenges, particularly vehicle
rebalancing - the strategic repositioning of a large group of vehicles to
address spatiotemporal mismatches in supply and demand. Inadequate rebalancing
not only results in prolonged rider waiting times and inefficient vehicle
utilization but also leads to fairness issues, such as the inequitable
distribution of service quality and disparities in driver income. To tackle
these complexities, we introduce continuous-state mean-field control (MFC) and
mean-field reinforcement learning (MFRL) models that employ continuous vehicle
repositioning actions. MFC and MFRL offer scalable solutions by modeling each
vehicle's behavior through interaction with the vehicle distribution, rather
than with individual vehicles. This limits the issues arising from the curse of
dimensionality inherent in traditional multi-agent methods, enabling
coordination across large fleets with significantly reduced computational
complexity. To ensure equitable service access across geographic regions, we
integrate an accessibility constraint into both models. Extensive empirical
evaluation using real-world data-driven simulation of Shenzhen demonstrates the
real-time efficiency and robustness of our approach. Remarkably, it scales to
tens of thousands of vehicles, with training times comparable to the decision
time of a single linear programming rebalancing. Besides, policies generated by
our approach effectively explore the efficiency-equity Pareto front,
outperforming conventional benchmarks across key metrics like fleet
utilization, fulfilled requests, and pickup distance, while ensuring equitable
service access.

</details>


### [544] [Shape it Up! Restoring LLM Safety during Finetuning](https://arxiv.org/pdf/2505.17196)
*ShengYun Peng, Pin-Yu Chen, Jianfeng Chi, Seongmin Lee, Duen Horng Chau*

Main category: cs.LG

TL;DR: Dynamic Safety Shaping (DSS) improves LLM safety by fine-tuning responses segment-by-segment using safety signals, outperforming static methods.


<details>
  <summary>Details</summary>
Motivation: Addressing safety risks in finetuning LLMs, where harmful examples can compromise alignment, by moving beyond coarse static safety shaping.

Method: Proposes DSS, leveraging guardrail models to assess safety at token-level (STAR), enabling dynamic reinforcement of safe segments and suppression of unsafe ones.

Result: STAR-DSS significantly enhances safety across diverse threats and models without sacrificing task performance.

Conclusion: Advocates for dynamic shaping principles in future safety research to better mitigate evolving finetuning risks.

Abstract: Finetuning large language models (LLMs) enables user-specific customization
but introduces critical safety risks: even a few harmful examples can
compromise safety alignment. A common mitigation strategy is to update the
model more strongly on examples deemed safe, while downweighting or excluding
those flagged as unsafe. However, because safety context can shift within a
single example, updating the model equally on both harmful and harmless parts
of a response is suboptimal-a coarse treatment we term static safety shaping.
In contrast, we propose dynamic safety shaping (DSS), a framework that uses
fine-grained safety signals to reinforce learning from safe segments of a
response while suppressing unsafe content. To enable such fine-grained control
during finetuning, we introduce a key insight: guardrail models, traditionally
used for filtering, can be repurposed to evaluate partial responses, tracking
how safety risk evolves throughout the response, segment by segment. This leads
to the Safety Trajectory Assessment of Response (STAR), a token-level signal
that enables shaping to operate dynamically over the training sequence.
Building on this, we present STAR-DSS, guided by STAR scores, that robustly
mitigates finetuning risks and delivers substantial safety improvements across
diverse threats, datasets, and model families-all without compromising
capability on intended tasks. We encourage future safety research to build on
dynamic shaping principles for stronger mitigation against evolving finetuning
risks.

</details>


### [545] [Semantic-Aware Interpretable Multimodal Music Auto-Tagging](https://arxiv.org/pdf/2505.17233)
*Andreas Patakis, Vassilis Lyberatos, Spyridon Kantarelis, Edmund Dervakos, Giorgos Stamou*

Main category: cs.LG

TL;DR: An interpretable framework for music auto-tagging using multimodal features and semantic clustering, balancing performance and transparency.


<details>
  <summary>Details</summary>
Motivation: Address the lack of interpretability in foundation models for music auto-tagging, which limits trust and usability.

Method: Leverages musically meaningful multimodal features, clusters them semantically, and uses expectation maximization to weight feature groups.

Result: Achieves competitive tagging performance while providing interpretability.

Conclusion: Paves the way for more transparent and user-centric music tagging systems.

Abstract: Music auto-tagging is essential for organizing and discovering music in
extensive digital libraries. While foundation models achieve exceptional
performance in this domain, their outputs often lack interpretability, limiting
trust and usability for researchers and end-users alike. In this work, we
present an interpretable framework for music auto-tagging that leverages groups
of musically meaningful multimodal features, derived from signal processing,
deep learning, ontology engineering, and natural language processing. To
enhance interpretability, we cluster features semantically and employ an
expectation maximization algorithm, assigning distinct weights to each group
based on its contribution to the tagging process. Our method achieves
competitive tagging performance while offering a deeper understanding of the
decision-making process, paving the way for more transparent and user-centric
music tagging systems.

</details>


### [546] [LengthLogD: A Length-Stratified Ensemble Framework for Enhanced Peptide Lipophilicity Prediction via Multi-Scale Feature Integration](https://arxiv.org/pdf/2505.17198)
*Shuang Wu, Meijie Wang, Lun Yu*

Main category: cs.LG

TL;DR: LengthLogD is a predictive framework for peptide logD using length-stratified models and multi-scale molecular representations, outperforming conventional methods.


<details>
  <summary>Details</summary>
Motivation: Peptides have therapeutic potential but face challenges in drug development due to low membrane permeability, with logD prediction being complex.

Method: LengthLogD integrates atomic, structural, and topological features, optimized via stratified ensemble learning and adaptive weight allocation for long peptides.

Result: Achieves high R^2 values (0.855-0.882) across peptide lengths, with a 34.7% error reduction for long peptides and outperforms state-of-the-art models.

Conclusion: LengthLogD offers a precise tool for peptide logD prediction, especially valuable for optimizing long peptide lead compounds.

Abstract: Peptide compounds demonstrate considerable potential as therapeutic agents
due to their high target affinity and low toxicity, yet their drug development
is constrained by their low membrane permeability. Molecular weight and peptide
length have significant effects on the logD of peptides, which in turn
influences their ability to cross biological membranes. However, accurate
prediction of peptide logD remains challenging due to the complex interplay
between sequence, structure, and ionization states. This study introduces
LengthLogD, a predictive framework that establishes specialized models through
molecular length stratification while innovatively integrating multi-scale
molecular representations. We constructed feature spaces across three
hierarchical levels: atomic (10 molecular descriptors), structural (1024-bit
Morgan fingerprints), and topological (3 graph-based features including Wiener
index), optimized through stratified ensemble learning. An adaptive weight
allocation mechanism specifically developed for long peptides significantly
enhances model generalizability. Experimental results demonstrate superior
performance across all categories: short peptides (R^2=0.855), medium peptides
(R^2=0.816), and long peptides (R^2=0.882), with a 34.7% reduction in
prediction error for long peptides compared to conventional single-model
approaches. Ablation studies confirm: 1) The length-stratified strategy
contributes 41.2% to performance improvement; 2) Topological features account
for 28.5% of predictive importance. Compared to state-of-the-art models, our
method maintains short peptide prediction accuracy while achieving a 25.7%
increase in the coefficient of determination (R^2) for long peptides. This
research provides a precise logD prediction tool for peptide drug development,
particularly demonstrating unique value in optimizing long peptide lead
compounds.

</details>


### [547] [Secure and Private Federated Learning: Achieving Adversarial Resilience through Robust Aggregation](https://arxiv.org/pdf/2505.17226)
*Kun Yang, Neena Imam*

Main category: cs.LG

TL;DR: ArKrum, a new aggregation strategy for Federated Learning, enhances resilience and privacy by filtering outliers and averaging updates, outperforming existing methods in adversarial settings.


<details>
  <summary>Details</summary>
Motivation: FL is vulnerable to Byzantine clients corrupting the global model. Traditional methods lack robustness or require impractical prior knowledge.

Method: ArKrum combines median-based outlier filtering and multi-update averaging to estimate malicious clients and improve stability.

Result: ArKrum achieves high accuracy and stability on benchmark datasets under Byzantine attacks, matching or surpassing other methods.

Conclusion: ArKrum is a practical and effective solution for secure FL in adversarial environments.

Abstract: Federated Learning (FL) enables collaborative machine learning across
decentralized data sources without sharing raw data. It offers a promising
approach to privacy-preserving AI. However, FL remains vulnerable to
adversarial threats from malicious participants, referred to as Byzantine
clients, who can send misleading updates to corrupt the global model.
Traditional aggregation methods, such as simple averaging, are not robust to
such attacks. More resilient approaches, like the Krum algorithm, require prior
knowledge of the number of malicious clients, which is often unavailable in
real-world scenarios. To address these limitations, we propose Average-rKrum
(ArKrum), a novel aggregation strategy designed to enhance both the resilience
and privacy guarantees of FL systems. Building on our previous work (rKrum),
ArKrum introduces two key innovations. First, it includes a median-based
filtering mechanism that removes extreme outliers before estimating the number
of adversarial clients. Second, it applies a multi-update averaging scheme to
improve stability and performance, particularly when client data distributions
are not identical. We evaluate ArKrum on benchmark image and text datasets
under three widely studied Byzantine attack types. Results show that ArKrum
consistently achieves high accuracy and stability. It performs as well as or
better than other robust aggregation methods. These findings demonstrate that
ArKrum is an effective and practical solution for secure FL systems in
adversarial environments.

</details>


### [548] [Reverse-Speech-Finder: A Neural Network Backtracking Architecture for Generating Alzheimer's Disease Speech Samples and Improving Diagnosis Performance](https://arxiv.org/pdf/2505.17477)
*Victor OK Li, Yang Han, Jacqueline CK Lam, Lawrence YL Cheung*

Main category: cs.LG

TL;DR: RSF is a neural network backtracking architecture for AD diagnosis via speech analysis, outperforming traditional methods by identifying novel speech markers.


<details>
  <summary>Details</summary>
Motivation: Addresses scarcity of real AD speech samples and limited interpretability in existing models.

Method: Uses pre-trained language models to identify AD-specific speech markers, backtracking from neurons to input tokens.

Result: Achieves 3.5% higher accuracy and 3.2% better F1-score than SHAP and Integrated Gradients.

Conclusion: RSF enhances AD diagnosis robustness and offers insights for non-invasive early intervention.

Abstract: This study introduces Reverse-Speech-Finder (RSF), a groundbreaking neural
network backtracking architecture designed to enhance Alzheimer's Disease (AD)
diagnosis through speech analysis. Leveraging the power of pre-trained large
language models, RSF identifies and utilizes the most probable AD-specific
speech markers, addressing both the scarcity of real AD speech samples and the
challenge of limited interpretability in existing models. RSF's unique approach
consists of three core innovations: Firstly, it exploits the observation that
speech markers most probable of predicting AD, defined as the most probable
speech-markers (MPMs), must have the highest probability of activating those
neurons (in the neural network) with the highest probability of predicting AD,
defined as the most probable neurons (MPNs). Secondly, it utilizes a speech
token representation at the input layer, allowing backtracking from MPNs to
identify the most probable speech-tokens (MPTs) of AD. Lastly, it develops an
innovative backtracking method to track backwards from the MPNs to the input
layer, identifying the MPTs and the corresponding MPMs, and ingeniously
uncovering novel speech markers for AD detection. Experimental results
demonstrate RSF's superiority over traditional methods such as SHAP and
Integrated Gradients, achieving a 3.5% improvement in accuracy and a 3.2% boost
in F1-score. By generating speech data that encapsulates novel markers, RSF not
only mitigates the limitations of real data scarcity but also significantly
enhances the robustness and accuracy of AD diagnostic models. These findings
underscore RSF's potential as a transformative tool in speech-based AD
detection, offering new insights into AD-related linguistic deficits and paving
the way for more effective non-invasive early intervention strategies.

</details>


### [549] [Automated Capability Evaluation of Foundation Models](https://arxiv.org/pdf/2505.17228)
*Arash Afkanpour, Omkar Dige, Fatemeh Tavakoli*

Main category: cs.LG

TL;DR: ACE introduces an automated, scalable framework for evaluating foundation models by leveraging active learning and latent semantic spaces, reducing reliance on static benchmarks.


<details>
  <summary>Details</summary>
Motivation: Current evaluation frameworks are limited by fixed, manually curated benchmarks, which fail to fully capture model capabilities.

Method: ACE uses active learning to decompose domains into capabilities, generates diverse tasks, and prioritizes evaluations based on a latent semantic space.

Result: ACE offers a more complete and informative assessment of model strengths, weaknesses, and failure modes compared to static benchmarks.

Conclusion: ACE enhances the evaluation of foundation models, supporting safer and better-informed deployment.

Abstract: Current evaluation frameworks for foundation models rely heavily on fixed,
manually curated benchmarks, limiting their ability to capture the full breadth
of model capabilities. This paper introduces Active learning for Capability
Evaluation (ACE), a novel framework for scalable, automated, and fine-grained
evaluation of foundation models. ACE leverages the knowledge embedded in
powerful language models to decompose a domain into semantically meaningful
capabilities and generate diverse evaluation tasks, significantly reducing
human effort. To maximize coverage and efficiency, ACE models a subject model's
performance as a capability function over a latent semantic space and uses
active learning to prioritize the evaluation of the most informative
capabilities. This adaptive evaluation strategy enables cost-effective
discovery of strengths, weaknesses, and failure modes that static benchmarks
may miss. Our results suggest that ACE provides a more complete and informative
picture of model capabilities, which is essential for safe and well-informed
deployment of foundation models.

</details>


### [550] [What You Read Isn't What You Hear: Linguistic Sensitivity in Deepfake Speech Detection](https://arxiv.org/pdf/2505.17513)
*Binh Nguyen, Shuji Shi, Ryan Ofman, Thai Le*

Main category: cs.LG

TL;DR: The paper explores how linguistic variations can undermine audio anti-spoofing systems, showing that minor transcript-level adversarial attacks significantly reduce detection accuracy, even in commercial systems.


<details>
  <summary>Details</summary>
Motivation: To address the gap in understanding the impact of linguistic variation on audio anti-spoofing detectors, which have traditionally focused on acoustic-level perturbations.

Method: Introducing transcript-level adversarial attacks to evaluate the linguistic sensitivity of open-source and commercial anti-spoofing detectors.

Result: Minor linguistic perturbations degrade detection accuracy, with attack success rates exceeding 60% on some systems and one commercial detector's accuracy dropping from 100% to 32%.

Conclusion: Robust anti-spoofing systems must account for linguistic variation, not just acoustic features, to defend against realistic threats like deepfake scams.

Abstract: Recent advances in text-to-speech technologies have enabled realistic voice
generation, fueling audio-based deepfake attacks such as fraud and
impersonation. While audio anti-spoofing systems are critical for detecting
such threats, prior work has predominantly focused on acoustic-level
perturbations, leaving the impact of linguistic variation largely unexplored.
In this paper, we investigate the linguistic sensitivity of both open-source
and commercial anti-spoofing detectors by introducing transcript-level
adversarial attacks. Our extensive evaluation reveals that even minor
linguistic perturbations can significantly degrade detection accuracy: attack
success rates surpass 60% on several open-source detector-voice pairs, and
notably one commercial detection accuracy drops from 100% on synthetic audio to
just 32%. Through a comprehensive feature attribution analysis, we identify
that both linguistic complexity and model-level audio embedding similarity
contribute strongly to detector vulnerability. We further demonstrate the
real-world risk via a case study replicating the Brad Pitt audio deepfake scam,
using transcript adversarial attacks to completely bypass commercial detectors.
These results highlight the need to move beyond purely acoustic defenses and
account for linguistic variation in the design of robust anti-spoofing systems.
All source code will be publicly available.

</details>


### [551] [Optimal Policy Minimum Bayesian Risk](https://arxiv.org/pdf/2505.17242)
*Ramón Fernandez Astudillo, Md Arafat Sultan, Aashka Trivedi, Yousef El-Kurdi, Tahira Naseem, Radu Florian, Salim Roukos*

Main category: cs.LG

TL;DR: A novel method improves LLM reasoning by integrating reward and similarity signals into MBRD, offering robustness, accuracy, and adaptive sample efficiency.


<details>
  <summary>Details</summary>
Motivation: To enhance LLM performance in complex reasoning tasks by leveraging inference-time techniques and reward signals.

Method: Proposes a KL-controlled reinforcement learning framework for MBRD, incorporating reward/risk signals and adaptive sampling.

Result: Demonstrated improved accuracy and robustness on math and coding tasks, with efficient compute trade-offs.

Conclusion: The method outperforms traditional inference-time techniques, providing scalable and adaptive solutions for LLMs.

Abstract: Inference scaling can help LLMs solve complex reasoning problems through
extended runtime computation. On top of targeted supervision for long
chain-of-thought (long-CoT) generation, purely inference-time techniques such
as best-of-N (BoN) sampling, majority voting, or more generally, minimum Bayes
risk decoding (MBRD), can further improve LLM accuracy by generating multiple
candidate solutions and aggregating over them. These methods typically leverage
additional signals in the form of reward models and risk/similarity functions
that compare generated samples, e.g., exact match in some normalized space or
standard similarity metrics such as Rouge. Here we present a novel method for
incorporating reward and risk/similarity signals into MBRD. Based on the
concept of optimal policy in KL-controlled reinforcement learning, our
framework provides a simple and well-defined mechanism for leveraging such
signals, offering several advantages over traditional inference-time methods:
higher robustness, improved accuracy, and well-understood asymptotic behavior.
In addition, it allows for the development of a sample-efficient variant of
MBRD that can adjust the number of samples to generate according to the
difficulty of the problem, without relying on majority vote counts. We
empirically demonstrate the advantages of our approach on math (MATH-$500$) and
coding (HumanEval) tasks using recent open-source models. We also present a
comprehensive analysis of its accuracy-compute trade-offs.

</details>


### [552] [Backdoors in DRL: Four Environments Focusing on In-distribution Triggers](https://arxiv.org/pdf/2505.17248)
*Chace Ashcraft, Ted Staley, Josh Carney, Cameron Hickert, Derek Juba, Kiran Karra, Nathan Drenkow*

Main category: cs.LG

TL;DR: The paper explores backdoor attacks in deep reinforcement learning (DRL) agents, focusing on in-distribution triggers as significant security threats. It implements attacks in four RL environments and demonstrates their viability despite implementation challenges.


<details>
  <summary>Details</summary>
Motivation: Backdoor attacks in open-source neural networks pose security risks, especially with third-party models. The study aims to advance research on mitigating such attacks in DRL by focusing on in-distribution triggers.

Method: The authors develop trojans for DRL agents, implementing backdoor attacks in four RL environments (LavaWorld, Randomized LavaWorld, Colorful Memory, Modified Safety Gymnasium). They train both clean and backdoored models to analyze the attacks.

Result: In-distribution triggers, though harder to implement and learn, remain viable threats in DRL, even with basic data poisoning attacks.

Conclusion: The study highlights the security risks of in-distribution backdoor triggers in DRL, emphasizing the need for further research on mitigation strategies.

Abstract: Backdoor attacks, or trojans, pose a security risk by concealing undesirable
behavior in deep neural network models. Open-source neural networks are
downloaded from the internet daily, possibly containing backdoors, and
third-party model developers are common. To advance research on backdoor attack
mitigation, we develop several trojans for deep reinforcement learning (DRL)
agents. We focus on in-distribution triggers, which occur within the agent's
natural data distribution, since they pose a more significant security threat
than out-of-distribution triggers due to their ease of activation by the
attacker during model deployment. We implement backdoor attacks in four
reinforcement learning (RL) environments: LavaWorld, Randomized LavaWorld,
Colorful Memory, and Modified Safety Gymnasium. We train various models, both
clean and backdoored, to characterize these attacks. We find that
in-distribution triggers can require additional effort to implement and be more
challenging for models to learn, but are nevertheless viable threats in DRL
even using basic data poisoning attacks.

</details>


### [553] [Approach to Finding a Robust Deep Learning Model](https://arxiv.org/pdf/2505.17254)
*Alexey Boldyrev, Fedor Ratnikov, Andrey Shevelev*

Main category: cs.LG

TL;DR: A novel approach for determining ML model robustness, applicable to any model, is proposed, with a focus on deep learning models.


<details>
  <summary>Details</summary>
Motivation: The growing demand for unsupervised training of reliable ML models necessitates robust evaluation methods.

Method: A meta-algorithm for model selection is introduced, tested on small deep learning models with common optimizers.

Result: The study examines the impact of training sample size, weight initialization, and inductive bias on model robustness.

Conclusion: The proposed approach is versatile and effective for evaluating robustness in deep learning models.

Abstract: The rapid development of machine learning (ML) and artificial intelligence
(AI) applications requires the training of large numbers of models. This
growing demand highlights the importance of training models without human
supervision, while ensuring that their predictions are reliable. In response to
this need, we propose a novel approach for determining model robustness. This
approach, supplemented with a proposed model selection algorithm designed as a
meta-algorithm, is versatile and applicable to any machine learning model,
provided that it is appropriate for the task at hand. This study demonstrates
the application of our approach to evaluate the robustness of deep learning
models. To this end, we study small models composed of a few convolutional and
fully connected layers, using common optimizers due to their ease of
interpretation and computational efficiency. Within this framework, we address
the influence of training sample size, model weight initialization, and
inductive bias on the robustness of deep learning models.

</details>


### [554] [JanusDNA: A Powerful Bi-directional Hybrid DNA Foundation Model](https://arxiv.org/pdf/2505.17257)
*Qihao Duan, Bingding Huang, Zhenqiao Song, Irina Lehmann, Lei Gu, Roland Eils, Benjamin Wild*

Main category: cs.LG

TL;DR: JanusDNA is a bidirectional DNA foundation model combining autoregressive efficiency with masked modeling's bidirectional comprehension, achieving SOTA results on genomic benchmarks.


<details>
  <summary>Details</summary>
Motivation: Adapting LLMs to genomics is challenging due to long-range dependencies and bidirectional nature of DNA, which standard LLM approaches poorly address.

Method: JanusDNA uses a hybrid Mamba, Attention, and MoE architecture for efficient long-range and bidirectional modeling, processing up to 1 million base pairs on a single GPU.

Result: JanusDNA outperforms models with 250x more parameters, achieving SOTA on three genomic benchmarks.

Conclusion: JanusDNA effectively addresses genomic modeling challenges, offering a scalable and efficient solution for DNA sequence analysis.

Abstract: Large language models (LLMs) have revolutionized natural language processing
and are increasingly applied to other sequential data types, including genetic
sequences. However, adapting LLMs to genomics presents significant challenges.
Capturing complex genomic interactions requires modeling long-range
dependencies within DNA sequences, where interactions often span over 10,000
base pairs, even within a single gene, posing substantial computational burdens
under conventional model architectures and training paradigms. Moreover,
standard LLM training approaches are suboptimal for DNA: autoregressive
training, while efficient, supports only unidirectional understanding. However,
DNA is inherently bidirectional, e.g., bidirectional promoters regulate
transcription in both directions and account for nearly 11% of human gene
expression. Masked language models (MLMs) allow bidirectional understanding but
are inefficient, as only masked tokens contribute to the loss per step. To
address these limitations, we introduce JanusDNA, the first bidirectional DNA
foundation model built upon a novel pretraining paradigm that combines the
optimization efficiency of autoregressive modeling with the bidirectional
comprehension of masked modeling. JanusDNA adopts a hybrid Mamba, Attention and
Mixture of Experts (MoE) architecture, combining long-range modeling of
Attention with efficient sequential learning of Mamba. MoE layers further scale
model capacity via sparse activation while keeping computational cost low.
Notably, JanusDNA processes up to 1 million base pairs at single nucleotide
resolution on a single 80GB GPU. Extensive experiments and ablations show
JanusDNA achieves new SOTA results on three genomic representation benchmarks,
outperforming models with 250x more activated parameters. Code:
https://github.com/Qihao-Duan/JanusDNA

</details>


### [555] [Zebra-Llama: Towards Extremely Efficient Hybrid Models](https://arxiv.org/pdf/2505.17272)
*Mingyu Yang, Mehdi Rezagholizadeh, Guihong Li, Vikram Appia, Emad Barsoum*

Main category: cs.LG

TL;DR: Zebra-Llama proposes hybrid language models combining SSMs and MLA layers for efficient inference, achieving Transformer-level accuracy with reduced training tokens and KV cache size.


<details>
  <summary>Details</summary>
Motivation: To address the inefficiency and environmental cost of retraining large language models (LLMs) for new requirements, Zebra-Llama offers a scalable alternative by composing hybrid models from pre-trained ones.

Method: Zebra-Llama combines State Space Models (SSMs) and Multi-head Latent Attention (MLA) layers, using a refined initialization and post-training pipeline to transfer knowledge from pre-trained Transformers.

Result: The hybrid models achieve near-SSM efficiency with Transformer-level accuracy, using only 7-11B training tokens and significantly reducing KV cache size (down to 3.9%, 2%, and 2.73% for 1B, 3B, and 8B variants). They outperform competitors like MambaInLLaMA and Minitron in accuracy and efficiency.

Conclusion: Zebra-Llama provides a practical, efficient, and scalable solution for deploying LLMs, reducing resource requirements while maintaining performance.

Abstract: With the growing demand for deploying large language models (LLMs) across
diverse applications, improving their inference efficiency is crucial for
sustainable and democratized access. However, retraining LLMs to meet new
user-specific requirements is prohibitively expensive and environmentally
unsustainable. In this work, we propose a practical and scalable alternative:
composing efficient hybrid language models from existing pre-trained models.
Our approach, Zebra-Llama, introduces a family of 1B, 3B, and 8B hybrid models
by combining State Space Models (SSMs) and Multi-head Latent Attention (MLA)
layers, using a refined initialization and post-training pipeline to
efficiently transfer knowledge from pre-trained Transformers. Zebra-Llama
achieves Transformer-level accuracy with near-SSM efficiency using only 7-11B
training tokens (compared to trillions of tokens required for pre-training) and
an 8B teacher. Moreover, Zebra-Llama dramatically reduces KV cache size -down
to 3.9%, 2%, and 2.73% of the original for the 1B, 3B, and 8B variants,
respectively-while preserving 100%, 100%, and >97% of average zero-shot
performance on LM Harness tasks. Compared to models like MambaInLLaMA,
X-EcoMLA, Minitron, and Llamba, Zebra-Llama consistently delivers competitive
or superior accuracy while using significantly fewer tokens, smaller teachers,
and vastly reduced KV cache memory. Notably, Zebra-Llama-8B surpasses
Minitron-8B in few-shot accuracy by 7% while using 8x fewer training tokens,
over 12x smaller KV cache, and a smaller teacher (8B vs. 15B). It also achieves
2.6x-3.8x higher throughput (tokens/s) than MambaInLlama up to a 32k context
length. We will release code and model checkpoints upon acceptance.

</details>


### [556] [Comparator-Adaptive $Φ$-Regret: Improved Bounds, Simpler Algorithms, and Applications to Games](https://arxiv.org/pdf/2505.17277)
*Soumita Hait, Ping Li, Haipeng Luo, Mengxiao Zhang*

Main category: cs.LG

TL;DR: The paper introduces simpler algorithms for achieving better comparator-adaptive Φ-regret bounds, improving upon prior work by Lu et al. [2025]. It proposes two efficient methods and extends one to game settings for accelerated convergence to Φ-equilibria.


<details>
  <summary>Details</summary>
Motivation: To simplify and improve upon the complex adaptive algorithms for Φ-regret introduced by Lu et al. [2025], achieving better regret bounds and broader applicability.

Method: 1. Introduces a prior distribution over binary transformations to achieve prior-dependent regret. 2. Proposes two algorithms: one based on a prior-aware Kernelized MWU variant and another on a prior-aware BM-reduction variant.

Result: The algorithms achieve better comparator-adaptive Φ-regret bounds and extend to game settings, improving convergence rates to Φ-equilibria.

Conclusion: The proposed methods are simpler, more efficient, and outperform existing approaches in both regret bounds and applicability to game theory.

Abstract: In the classic expert problem, $\Phi$-regret measures the gap between the
learner's total loss and that achieved by applying the best action
transformation $\phi \in \Phi$. A recent work by Lu et al., [2025] introduces
an adaptive algorithm whose regret against a comparator $\phi$ depends on a
certain sparsity-based complexity measure of $\phi$, (almost) recovering and
interpolating optimal bounds for standard regret notions such as external,
internal, and swap regret. In this work, we propose a general idea to achieve
an even better comparator-adaptive $\Phi$-regret bound via much simpler
algorithms compared to Lu et al., [2025]. Specifically, we discover a prior
distribution over all possible binary transformations and show that it suffices
to achieve prior-dependent regret against these transformations. Then, we
propose two concrete and efficient algorithms to achieve so, where the first
one learns over multiple copies of a prior-aware variant of the Kernelized MWU
algorithm of Farina et al., [2022], and the second one learns over multiple
copies of a prior-aware variant of the BM-reduction [Blum and Mansour, 2007].
To further showcase the power of our methods and the advantages over Lu et al.,
[2025] besides the simplicity and better regret bounds, we also show that our
second approach can be extended to the game setting to achieve accelerated and
adaptive convergence rate to $\Phi$-equilibria for a class of general-sum
games. When specified to the special case of correlated equilibria, our bound
improves over the existing ones from Anagnostides et al., [2022a,b]

</details>


### [557] [Attention with Trained Embeddings Provably Selects Important Tokens](https://arxiv.org/pdf/2505.17282)
*Diyuan Wu, Aleksandr Shevchenko, Samet Oymak, Marco Mondelli*

Main category: cs.LG

TL;DR: The paper analyzes token embeddings in a one-layer softmax attention model, showing how gradient training aligns embeddings with token importance and predictive power.


<details>
  <summary>Details</summary>
Motivation: To bridge the gap in theoretical understanding of token embeddings in language modeling, focusing on their structure and behavior under gradient descent.

Method: Uses a one-layer softmax attention model with a linear head for binary classification, analyzing embeddings after gradient training and convergence.

Result: Embeddings align with token importance and frequency post-training, and the softmax selects predictive tokens, maximizing classification margin.

Conclusion: Theoretical findings align with empirical results on real-world datasets, validating the model's behavior.

Abstract: Token embeddings play a crucial role in language modeling but, despite this
practical relevance, their theoretical understanding remains limited. Our paper
addresses the gap by characterizing the structure of embeddings obtained via
gradient descent. Specifically, we consider a one-layer softmax attention model
with a linear head for binary classification, i.e., $\texttt{Softmax}( p^\top
E_X^\top ) E_X v = \frac{ \sum_{i=1}^T \exp(p^\top E_{x_i}) E_{x_i}^\top
v}{\sum_{j=1}^T \exp(p^\top E_{x_{j}}) }$, where $E_X = [ E_{x_1} , \dots,
E_{x_T} ]^\top$ contains the embeddings of the input sequence, $p$ is the
embedding of the $\mathrm{\langle cls \rangle}$ token and $v$ the output
vector. First, we show that, already after a single step of gradient training
with the logistic loss, the embeddings $E_X$ capture the importance of tokens
in the dataset by aligning with the output vector $v$ proportionally to the
frequency with which the corresponding tokens appear in the dataset. Then,
after training $p$ via gradient flow until convergence, the softmax selects the
important tokens in the sentence (i.e., those that are predictive of the
label), and the resulting $\mathrm{\langle cls \rangle}$ embedding maximizes
the margin for such a selection. Experiments on real-world datasets (IMDB,
Yelp) exhibit a phenomenology close to that unveiled by our theory.

</details>


### [558] [Model-Free Graph Data Selection under Distribution Shift](https://arxiv.org/pdf/2505.17293)
*Ting-Wei Li, Ruizhong Qiu, Hanghang Tong*

Main category: cs.LG

TL;DR: GRADATE is a model-free framework for graph domain adaptation that selects optimal training data from the source domain using optimal transport theory, improving performance with fewer data.


<details>
  <summary>Details</summary>
Motivation: Existing model-centric GDA methods struggle with severe distribution shifts and computational constraints.

Method: GRADATE leverages optimal transport theory to select the best training samples without relying on GNN models or training procedures.

Result: GRADATE outperforms existing methods and enhances GDA techniques with fewer training samples.

Conclusion: GRADATE is a scalable, data-efficient solution that complements model-centric GDA approaches.

Abstract: Graph domain adaptation (GDA) is a fundamental task in graph machine
learning, with techniques like shift-robust graph neural networks (GNNs) and
specialized training procedures to tackle the distribution shift problem.
Although these model-centric approaches show promising results, they often
struggle with severe shifts and constrained computational resources. To address
these challenges, we propose a novel model-free framework, GRADATE (GRAph DATa
sElector), that selects the best training data from the source domain for the
classification task on the target domain. GRADATE picks training samples
without relying on any GNN model's predictions or training recipes, leveraging
optimal transport theory to capture and adapt to distribution changes. GRADATE
is data-efficient, scalable and meanwhile complements existing model-centric
GDA approaches. Through comprehensive empirical studies on several real-world
graph-level datasets and multiple covariate shift types, we demonstrate that
GRADATE outperforms existing selection methods and enhances off-the-shelf GDA
methods with much fewer training data.

</details>


### [559] [Implicit Regularization of Infinitesimally-perturbed Gradient Descent Toward Low-dimensional Solutions](https://arxiv.org/pdf/2505.17304)
*Jianhao Ma, Geyu Liang, Salar Fattahi*

Main category: cs.LG

TL;DR: The paper explores implicit regularization in gradient-based methods, showing how infinitesimal perturbations in gradient descent (IPGD) can efficiently escape saddle points while maintaining proximity to low-dimensional solutions.


<details>
  <summary>Details</summary>
Motivation: To theoretically understand and formalize the conditions enabling implicit regularization in over-parameterized problems, where local algorithms converge to low-dimensional solutions without explicit constraints.

Method: Proposes Infinitesimally Perturbed Gradient Descent (IPGD), which uses tiny perturbations to escape strict saddle points while controlling deviation from implicit low-dimensional regions.

Result: IPGD provably satisfies the dual conditions of efficient saddle-point escape and proximity to implicit regions, with formal guarantees demonstrated in over-parameterized matrix sensing.

Conclusion: IPGD successfully balances saddle-point escape and implicit regularization, offering theoretical and empirical insights applicable to broader learning problems.

Abstract: Implicit regularization refers to the phenomenon where local search
algorithms converge to low-dimensional solutions, even when such structures are
neither explicitly specified nor encoded in the optimization problem. While
widely observed, this phenomenon remains theoretically underexplored,
particularly in modern over-parameterized problems. In this paper, we study the
conditions that enable implicit regularization by investigating when
gradient-based methods converge to second-order stationary points (SOSPs)
within an implicit low-dimensional region of a smooth, possibly nonconvex
function. We show that successful implicit regularization hinges on two key
conditions: $(i)$ the ability to efficiently escape strict saddle points, while
$(ii)$ maintaining proximity to the implicit region. Existing analyses enabling
the convergence of gradient descent (GD) to SOSPs often rely on injecting large
perturbations to escape strict saddle points. However, this comes at the cost
of deviating from the implicit region. The central premise of this paper is
that it is possible to achieve the best of both worlds: efficiently escaping
strict saddle points using infinitesimal perturbations, while controlling
deviation from the implicit region via a small deviation rate. We show that
infinitesimally perturbed gradient descent (IPGD), which can be interpreted as
GD with inherent ``round-off errors'', can provably satisfy both conditions. We
apply our framework to the problem of over-parameterized matrix sensing, where
we establish formal guarantees for the implicit regularization behavior of
IPGD. We further demonstrate through extensive experiments that these insights
extend to a broader class of learning problems.

</details>


### [560] [Wavelet Probabilistic Recurrent Convolutional Network for Multivariate Time Series Classification](https://arxiv.org/pdf/2505.17307)
*Pu Yang, J. A. Barria*

Main category: cs.LG

TL;DR: The paper introduces WPRCN, a wavelet probabilistic recurrent convolutional network for MTSC, excelling in non-stationary, noisy, and data-scarce environments. It combines AWPG and APTCN for probabilistic feature extraction and analysis, outperforming benchmarks.


<details>
  <summary>Details</summary>
Motivation: Addressing challenges in MTSC like non-stationarity, noise, and data scarcity by integrating wavelet probabilistic methods with deep learning.

Method: Proposes WPRCN with AWPG for adaptive probabilistic feature generation and APTCN for feature correlation analysis, integrated with LSTM and C-FCN.

Result: Evaluated on 30 datasets, WPRCN outperforms benchmarks in accuracy and rank, especially in scarce and noisy data scenarios.

Conclusion: WPRCN effectively handles MTSC challenges, demonstrating robustness and broad applicability in time series analysis.

Abstract: This paper presents a Wavelet Probabilistic Recurrent Convolutional Network
(WPRCN) for Multivariate Time Series Classification (MTSC), especially
effective in handling non-stationary environments, data scarcity and noise
perturbations. We introduce a versatile wavelet probabilistic module designed
to extract and analyse the probabilistic features, which can seamlessly
integrate with a variety of neural network architectures. This probabilistic
module comprises an Adaptive Wavelet Probabilistic Feature Generator (AWPG) and
a Channel Attention-based Probabilistic Temporal Convolutional Network (APTCN).
Such formulation extends the application of wavelet probabilistic neural
networks to deep neural networks for MTSC. The AWPG constructs an ensemble
probabilistic model addressing different data scarcities and non-stationarity;
it adaptively selects the optimal ones and generates probabilistic features for
APTCN. The APTCN analyses the correlations of the features and forms a
comprehensive feature space with existing MTSC models for classification. Here,
we instantiate the proposed module to work in parallel with a Long Short-Term
Memory (LSTM) network and a Causal Fully Convolutional Network (C-FCN),
demonstrating its broad applicability in time series analysis. The WPRCN is
evaluated on 30 diverse MTS datasets and outperforms all the benchmark
algorithms on average accuracy and rank, exhibiting pronounced strength in
handling scarce data and physiological data subject to perturbations and
non-stationarities.

</details>


### [561] [ECHO-LLaMA: Efficient Caching for High-Performance LLaMA Training](https://arxiv.org/pdf/2505.17331)
*Maryam Dialameh, Rezaul Karim, Hossein Rajabzadeh, Omar Mohamed Awad, Hyock Ju Kwon, Boxing Chen, Walid Ahmed, Yang Liu*

Main category: cs.LG

TL;DR: ECHO-LLaMA improves LLaMA's training speed and inference throughput by sharing KV caching across layers, achieving higher efficiency without losing performance.


<details>
  <summary>Details</summary>
Motivation: To enhance the efficiency of LLaMA architectures by reducing computational complexity while maintaining learning capacity.

Method: Introduces shared KV caching across certain layers to reduce computational overhead.

Result: Achieves up to 77% higher training throughput, 16% higher MFU, and 14% lower loss. Test-time throughput improves by 7% for the 1.1B model.

Conclusion: ECHO-LLaMA provides a scalable, cost-effective solution for efficient large language model training and inference.

Abstract: This paper introduces ECHO-LLaMA, an efficient LLaMA architecture designed to
improve both the training speed and inference throughput of LLaMA architectures
while maintaining its learning capacity. ECHO-LLaMA transforms LLaMA models
into shared KV caching across certain layers, significantly reducing KV
computational complexity while maintaining or improving language performance.
Experimental results demonstrate that ECHO-LLaMA achieves up to 77\% higher
token-per-second throughput during training, up to 16\% higher Model FLOPs
Utilization (MFU), and up to 14\% lower loss when trained on an equal number of
tokens. Furthermore, on the 1.1B model, ECHO-LLaMA delivers approximately 7\%
higher test-time throughput compared to the baseline. By introducing a
computationally efficient adaptation mechanism, ECHO-LLaMA offers a scalable
and cost-effective solution for pretraining and finetuning large language
models, enabling faster and more resource-efficient training without
compromising performance.

</details>


### [562] [Conformal Predictive Distributions for Order Fulfillment Time Forecasting](https://arxiv.org/pdf/2505.17340)
*Tinghan Ye, Amira Hijazi, Pascal Van Hentenryck*

Main category: cs.LG

TL;DR: A novel framework for distributional forecasting of order fulfillment time in e-commerce logistics, using model-agnostic techniques and machine learning, outperforms traditional rule-based methods.


<details>
  <summary>Details</summary>
Motivation: Traditional rule-based approaches fail to account for uncertainties in delivery operations, necessitating more accurate and reliable forecasting methods.

Method: Leverages Conformal Predictive Systems and Cross Venn-Abers Predictors, integrates spatiotemporal features, and uses a cost-sensitive decision rule for point predictions.

Result: Achieves up to 14% higher prediction accuracy and 75% improvement in identifying late deliveries compared to rule-based systems.

Conclusion: The proposed framework provides more accurate and reliable fulfillment time forecasts, enhancing e-commerce logistics efficiency.

Abstract: Accurate estimation of order fulfillment time is critical for e-commerce
logistics, yet traditional rule-based approaches often fail to capture the
inherent uncertainties in delivery operations. This paper introduces a novel
framework for distributional forecasting of order fulfillment time, leveraging
Conformal Predictive Systems and Cross Venn-Abers Predictors--model-agnostic
techniques that provide rigorous coverage or validity guarantees. The proposed
machine learning methods integrate granular spatiotemporal features, capturing
fulfillment location and carrier performance dynamics to enhance predictive
accuracy. Additionally, a cost-sensitive decision rule is developed to convert
probabilistic forecasts into reliable point predictions. Experimental
evaluation on a large-scale industrial dataset demonstrates that the proposed
methods generate competitive distributional forecasts, while machine
learning-based point predictions significantly outperform the existing
rule-based system--achieving up to 14% higher prediction accuracy and up to 75%
improvement in identifying late deliveries.

</details>


### [563] [TI-DeepONet: Learnable Time Integration for Stable Long-Term Extrapolation](https://arxiv.org/pdf/2505.17341)
*Dibyajyoti Nayak, Somdatta Goswami*

Main category: cs.LG

TL;DR: TI-DeepONet and TI(L)-DeepONet improve temporal extrapolation in neural operators by integrating adaptive time-stepping and learnable coefficients, reducing errors by 81% and 70% over conventional methods.


<details>
  <summary>Details</summary>
Motivation: Addressing the challenge of accurate temporal extrapolation in neural operators for dynamical systems, where conventional methods either ignore temporal causality or accumulate errors.

Method: Introduces TI-DeepONet and TI(L)-DeepONet, which approximate instantaneous time-derivative fields and integrate them using numerical schemes, with TI(L)-DeepONet adding learnable coefficients for better adaptation.

Result: Reduces relative L2 extrapolation errors by ~81% over autoregressive and ~70% over fixed-horizon methods, maintaining stability for temporal domains twice the training interval.

Conclusion: Establishes a physics-aware operator learning paradigm that combines neural approximation with numerical analysis, preserving dynamical systems' causal structure.

Abstract: Accurate temporal extrapolation presents a fundamental challenge for neural
operators in modeling dynamical systems, where reliable predictions must extend
significantly beyond the training time horizon. Conventional Deep Operator
Network (DeepONet) approaches employ two inherently limited training paradigms
- fixed-horizon rollouts that predict complete spatiotemporal solutions while
disregarding temporal causality, and autoregressive formulations that
accumulate errors through sequential predictions. We introduce TI-DeepONet, a
framework that integrates neural operators with adaptive numerical
time-stepping techniques to preserve the Markovian structure of dynamical
systems while mitigating error propagation in extended temporal forecasting.
Our approach reformulates the learning objective from direct state prediction
to the approximation of instantaneous time-derivative fields, which are then
integrated using established numerical schemes. This architecture supports
continuous-time prediction and enables deployment of higher-precision
integrators during inference than those used during training, balancing
computational efficiency with predictive accuracy. We further develop
TI(L)-DeepONet, which incorporates learnable coefficients for intermediate
slopes in the integration process, adapting to solution-specific variations and
enhancing fidelity. Evaluation across three canonical PDEs shows that
TI(L)-DeepONet marginally outperforms TI-DeepONet, with both reducing relative
L2 extrapolation errors: approximately 81% over autoregressive and 70% over
fixed-horizon methods. Notably, both maintain prediction stability for temporal
domains extending to about twice the training interval. This research
establishes a physics-aware operator learning paradigm that bridges neural
approximation with numerical analysis while preserving the causal structure of
dynamical systems.

</details>


### [564] [A Survey of Safe Reinforcement Learning and Constrained MDPs: A Technical Survey on Single-Agent and Multi-Agent Safety](https://arxiv.org/pdf/2505.17342)
*Ankita Kushwaha, Kiran Ravish, Preeti Lamba, Pawan Kumar*

Main category: cs.LG

TL;DR: A survey on Safe Reinforcement Learning (SafeRL) and Multi-Agent Safe RL (SafeMARL), covering CMDPs, algorithms, and open research problems.


<details>
  <summary>Details</summary>
Motivation: To provide a rigorous overview of SafeRL and SafeMARL, addressing safety constraints in RL and multi-agent systems.

Method: Reviews CMDPs, constrained optimization, and state-of-the-art algorithms for single-agent SafeRL and SafeMARL.

Result: Summarizes key concepts, methods, and identifies five open research problems, three in SafeMARL.

Conclusion: Serves as a technical guide for researchers, highlighting future directions in SafeRL and SafeMARL.

Abstract: Safe Reinforcement Learning (SafeRL) is the subfield of reinforcement
learning that explicitly deals with safety constraints during the learning and
deployment of agents. This survey provides a mathematically rigorous overview
of SafeRL formulations based on Constrained Markov Decision Processes (CMDPs)
and extensions to Multi-Agent Safe RL (SafeMARL). We review theoretical
foundations of CMDPs, covering definitions, constrained optimization
techniques, and fundamental theorems. We then summarize state-of-the-art
algorithms in SafeRL for single agents, including policy gradient methods with
safety guarantees and safe exploration strategies, as well as recent advances
in SafeMARL for cooperative and competitive settings. Additionally, we propose
five open research problems to advance the field, with three focusing on
SafeMARL. Each problem is described with motivation, key challenges, and
related prior work. This survey is intended as a technical guide for
researchers interested in SafeRL and SafeMARL, highlighting key concepts,
methods, and open future research directions.

</details>


### [565] [A Multi-Head Attention Soft Random Forest for Interpretable Patient No-Show Prediction](https://arxiv.org/pdf/2505.17344)
*Ninda Nurseha Amalina, Kwadwo Boateng Ofori-Amanfo, Heungjo An*

Main category: cs.LG

TL;DR: A hybrid Multi-Head Attention Soft Random Forest (MHASRF) model is proposed to predict patient no-shows, outperforming traditional methods with 93.56% accuracy and offering deeper insights into predictors.


<details>
  <summary>Details</summary>
Motivation: Patient no-shows disrupt healthcare efficiency and resource allocation, necessitating accurate predictive models. Existing methods lack adaptability to complex patient behaviors.

Method: MHASRF integrates attention mechanisms into a random forest model using probabilistic soft splitting, enabling adaptive focus on patient behaviors.

Result: MHASRF achieved 93.56% accuracy, 93.67% precision, 93.56% recall, and 93.59% F1 score, outperforming traditional models. It also identified key predictors at two levels.

Conclusion: MHASRF is a robust, adaptable, and interpretable solution for predicting no-shows, aiding healthcare providers in resource optimization.

Abstract: Unattended scheduled appointments, defined as patient no-shows, adversely
affect both healthcare providers and patients' health, disrupting the
continuity of care, operational efficiency, and the efficient allocation of
medical resources. Accurate predictive modelling is needed to reduce the impact
of no-shows. Although machine learning methods, such as logistic regression,
random forest models, and decision trees, are widely used in predicting patient
no-shows, they often rely on hard decision splits and static feature
importance, limiting their adaptability to specific or complex patient
behaviors. To address this limitation, we propose a new hybrid Multi-Head
Attention Soft Random Forest (MHASRF) model that integrates attention
mechanisms into a random forest model using probabilistic soft splitting
instead of hard splitting. The MHASRF model assigns attention weights
differently across the trees, enabling attention on specific patient behaviors.
The model exhibited 93.56% accuracy, 93.67% precision, 93.56% recall, and a
93.59% F1 score, surpassing the performance of decision tree, logistic
regression, random forest, and naive Bayes models. Furthermore, MHASRF was able
to identify key predictors of patient no-shows using two levels of feature
importance (tree level and attention mechanism level), offering deeper insights
into patient no-show predictors. The proposed model is a robust, adaptable, and
interpretable method for predicting patient no-shows that will help healthcare
providers in optimizing resources.

</details>


### [566] [FLEX: A Backbone for Diffusion-Based Modeling of Spatio-temporal Physical Systems](https://arxiv.org/pdf/2505.17351)
*N. Benjamin Erichson, Vinicius Mikuni, Dongwei Lyu, Yang Gao, Omri Azencot, Soon Hoe Lim, Michael W. Mahoney*

Main category: cs.LG

TL;DR: FLEX is a diffusion-based backbone for generative modeling of spatio-temporal systems, using residual space and hybrid U-Net-Transformer architecture for stability and accuracy.


<details>
  <summary>Details</summary>
Motivation: To improve generative modeling of physical systems by reducing variance in diffusion models and capturing both local and global dependencies.

Method: Uses residual space, hybrid U-Net-Transformer with redesigned skip connections, and task-specific encoders for conditioning.

Result: Achieves accurate predictions with few diffusion steps, calibrated uncertainty, and outperforms baselines on turbulence data.

Conclusion: FLEX generalizes well to unseen conditions and offers robust performance for spatio-temporal tasks.

Abstract: We introduce FLEX (FLow EXpert), a backbone architecture for generative
modeling of spatio-temporal physical systems using diffusion models. FLEX
operates in the residual space rather than on raw data, a modeling choice that
we motivate theoretically, showing that it reduces the variance of the velocity
field in the diffusion model, which helps stabilize training. FLEX integrates a
latent Transformer into a U-Net with standard convolutional ResNet layers and
incorporates a redesigned skip connection scheme. This hybrid design enables
the model to capture both local spatial detail and long-range dependencies in
latent space. To improve spatio-temporal conditioning, FLEX uses a
task-specific encoder that processes auxiliary inputs such as coarse or past
snapshots. Weak conditioning is applied to the shared encoder via skip
connections to promote generalization, while strong conditioning is applied to
the decoder through both skip and bottleneck features to ensure reconstruction
fidelity. FLEX achieves accurate predictions for super-resolution and
forecasting tasks using as few as two reverse diffusion steps. It also produces
calibrated uncertainty estimates through sampling. Evaluations on
high-resolution 2D turbulence data show that FLEX outperforms strong baselines
and generalizes to out-of-distribution settings, including unseen Reynolds
numbers, physical observables (e.g., fluid flow velocity fields), and boundary
conditions.

</details>


### [567] [CT-OT Flow: Estimating Continuous-Time Dynamics from Discrete Temporal Snapshots](https://arxiv.org/pdf/2505.17354)
*Keisuke Kawano, Takuro Kutsuna, Naoki Hayashi, Yasushi Esaki, Hidenori Tanaka*

Main category: cs.LG

TL;DR: CT-OT Flow recovers continuous-time dynamics from noisy, discrete snapshots using optimal transport and kernel smoothing, outperforming existing methods.


<details>
  <summary>Details</summary>
Motivation: Real-world data like single-cell RNA sequencing often lacks continuous trajectories, posing challenges for accurate dynamic modeling.

Method: CT-OT Flow infers high-resolution time labels via partial optimal transport and reconstructs continuous-time distributions with temporal kernel smoothing.

Result: Outperforms state-of-the-art methods on synthetic and real datasets (scRNA-seq, typhoon tracks) with lower reconstruction errors.

Conclusion: CT-OT Flow effectively bridges discrete snapshots and continuous-time processes by modeling temporal discretization and timestamp uncertainty.

Abstract: In many real-world scenarios, such as single-cell RNA sequencing, data are
observed only as discrete-time snapshots spanning finite time intervals and
subject to noisy timestamps, with no continuous trajectories available.
Recovering the underlying continuous-time dynamics from these snapshots with
coarse and noisy observation times is a critical and challenging task. We
propose Continuous-Time Optimal Transport Flow (CT-OT Flow), which first infers
high-resolution time labels via partial optimal transport and then reconstructs
a continuous-time data distribution through a temporal kernel smoothing. This
reconstruction enables accurate training of dynamics models such as ODEs and
SDEs. CT-OT Flow consistently outperforms state-of-the-art methods on synthetic
benchmarks and achieves lower reconstruction errors on real scRNA-seq and
typhoon-track datasets. Our results highlight the benefits of explicitly
modeling temporal discretization and timestamp uncertainty, offering an
accurate and general framework for bridging discrete snapshots and
continuous-time processes.

</details>


### [568] [Adversarial Robustness of Nonparametric Regression](https://arxiv.org/pdf/2505.17356)
*Parsa Moradi, Hanzaleh Akabrinodehi, Mohammad Ali Maddah-Ali*

Main category: cs.LG

TL;DR: The paper explores adversarial robustness in nonparametric regression, showing that a properly regularized smoothing spline estimator is robust against corruption of up to $o(n)$ samples, while no estimator can handle a constant fraction of corrupted data.


<details>
  <summary>Details</summary>
Motivation: To address the lack of research on adversarial robustness in nonparametric regression, particularly when the regression function is in the second-order Sobolev space.

Method: Analyzes adversarial robustness by characterizing minimax lower bounds and evaluating the performance of the classical smoothing spline estimator under adversarial corruption.

Result: The smoothing spline estimator is robust if $o(n)$ samples are corrupted, but no estimator can guarantee vanishing error if a constant fraction is corrupted.

Conclusion: The smoothing spline is optimal in terms of the maximum tolerable number of corrupted samples, highlighting a fundamental limit in adversarial robustness for nonparametric regression.

Abstract: In this paper, we investigate the adversarial robustness of regression, a
fundamental problem in machine learning, under the setting where an adversary
can arbitrarily corrupt a subset of the input data. While the robustness of
parametric regression has been extensively studied, its nonparametric
counterpart remains largely unexplored. We characterize the adversarial
robustness in nonparametric regression, assuming the regression function
belongs to the second-order Sobolev space (i.e., it is square integrable up to
its second derivative).
  The contribution of this paper is two-fold: (i) we establish a minimax lower
bound on the estimation error, revealing a fundamental limit that no estimator
can overcome, and (ii) we show that, perhaps surprisingly, the classical
smoothing spline estimator, when properly regularized, exhibits robustness
against adversarial corruption. These results imply that if $o(n)$ out of $n$
samples are corrupted, the estimation error of the smoothing spline vanishes as
$n \to \infty$. On the other hand, when a constant fraction of the data is
corrupted, no estimator can guarantee vanishing estimation error, implying the
optimality of the smoothing spline in terms of maximum tolerable number of
corrupted samples.

</details>


### [569] [Graph Attention Neural Network for Botnet Detection: Evaluating Autoencoder, VAE and PCA-Based Dimension Reduction](https://arxiv.org/pdf/2505.17357)
*Hassan Wasswa, Hussein Abbass, Timothy Lynar*

Main category: cs.LG

TL;DR: The paper proposes a framework combining dimensionality reduction and Graph Attention Networks (GAT) to improve IoT botnet attack detection, addressing computational challenges of high-dimensional data.


<details>
  <summary>Details</summary>
Motivation: Existing models overlook inter-instance relationships in IoT botnet attacks, and high-dimensional data complicates graph-based detection.

Method: The framework reduces dataset dimensionality using VAE-encoder, AE-encoder, or PCA, then applies GAT for detection.

Result: The study evaluates the impact of dimensionality reduction techniques on GAT performance for botnet detection.

Conclusion: Dimensionality reduction enhances GAT's efficiency and accuracy in detecting IoT botnet attacks.

Abstract: With the rise of IoT-based botnet attacks, researchers have explored various
learning models for detection, including traditional machine learning, deep
learning, and hybrid approaches. A key advancement involves deploying attention
mechanisms to capture long-term dependencies among features, significantly
improving detection accuracy. However, most models treat attack instances
independently, overlooking inter-instance relationships. Graph Neural Networks
(GNNs) address this limitation by learning an embedding space via iterative
message passing where similar instances are placed closer based on node
features and relationships, enhancing classification performance. To further
improve detection, attention mechanisms have been embedded within GNNs,
leveraging both long-range dependencies and inter-instance connections.
However, transforming the high dimensional IoT attack datasets into a graph
structured dataset poses challenges, such as large graph structures leading
computational overhead. To mitigate this, this paper proposes a framework that
first reduces dimensionality of the NetFlow-based IoT attack dataset before
transforming it into a graph dataset. We evaluate three dimension reduction
techniques--Variational Autoencoder (VAE-encoder), classical autoencoder
(AE-encoder), and Principal Component Analysis (PCA)--and compare their effects
on a Graph Attention neural network (GAT) model for botnet attack detection

</details>


### [570] [Towards VM Rescheduling Optimization Through Deep Reinforcement Learning](https://arxiv.org/pdf/2505.17359)
*Xianzhong Ding, Yunkai Zhang, Binbin Chen, Donghao Ying, Tieying Zhang, Jianjun Chen, Lei Zhang, Alberto Cerpa, Wan Du*

Main category: cs.LG

TL;DR: A reinforcement learning system, VM2RL, is developed for efficient VM rescheduling in data centers, addressing scalability and dynamic state changes.


<details>
  <summary>Details</summary>
Motivation: The increasing size of data centers and the understudied nature of VM rescheduling, coupled with poor scalability of existing methods, motivated this work.

Method: VM2RL uses a two-stage framework, feature extraction for relational information, and risk-seeking evaluation to balance latency and accuracy.

Result: VM2RL achieves near-optimal performance with seconds of running time, validated by industry-scale data.

Conclusion: VM2RL is a scalable and efficient solution for VM rescheduling, with open-sourced code and datasets.

Abstract: Modern industry-scale data centers need to manage a large number of virtual
machines (VMs). Due to the continual creation and release of VMs, many small
resource fragments are scattered across physical machines (PMs). To handle
these fragments, data centers periodically reschedule some VMs to alternative
PMs, a practice commonly referred to as VM rescheduling. Despite the increasing
importance of VM rescheduling as data centers grow in size, the problem remains
understudied. We first show that, unlike most combinatorial optimization tasks,
the inference time of VM rescheduling algorithms significantly influences their
performance, due to dynamic VM state changes during this period. This causes
existing methods to scale poorly. Therefore, we develop a reinforcement
learning system for VM rescheduling, VM2RL, which incorporates a set of
customized techniques, such as a two-stage framework that accommodates diverse
constraints and workload conditions, a feature extraction module that captures
relational information specific to rescheduling, as well as a risk-seeking
evaluation enabling users to optimize the trade-off between latency and
accuracy. We conduct extensive experiments with data from an industry-scale
data center. Our results show that VM2RL can achieve a performance comparable
to the optimal solution but with a running time of seconds. Code and datasets
are open-sourced: https://github.com/zhykoties/VMR2L_eurosys,
https://drive.google.com/drive/folders/1PfRo1cVwuhH30XhsE2Np3xqJn2GpX5qy.

</details>


### [571] [Improved and Oracle-Efficient Online $\ell_1$-Multicalibration](https://arxiv.org/pdf/2505.17365)
*Rohan Ghuge, Vidya Muthukumar, Sahil Singla*

Main category: cs.LG

TL;DR: The paper introduces a direct method for online multicalibration, achieving improved rates in the ℓ₁ norm via a novel reduction to online linear-product optimization (OLPO). Two approaches are proposed: one for better rates but high computational cost, and another for scalability with polynomial oracle calls.


<details>
  <summary>Details</summary>
Motivation: Prior methods for online multicalibration relied on indirect approaches with suboptimal rates in ℓ₁. The goal is to develop direct, efficient methods with improved guarantees.

Method: A reduction of ℓ₁-multicalibration to OLPO is proposed. Two algorithms are designed: one for optimal rates (but computationally expensive) and another for scalable, oracle-efficient rates.

Result: Improved rates of Õ(T⁻¹/³) and Õ(T⁻¹/⁴) are achieved for ℓ₁-multicalibration, with the latter being oracle-efficient. The framework also extends to infinite group families.

Conclusion: The paper provides a direct, scalable solution for online multicalibration, improving upon prior indirect methods and offering practical efficiency.

Abstract: We study \emph{online multicalibration}, a framework for ensuring calibrated
predictions across multiple groups in adversarial settings, across $T$ rounds.
Although online calibration is typically studied in the $\ell_1$ norm, prior
approaches to online multicalibration have taken the indirect approach of
obtaining rates in other norms (such as $\ell_2$ and $\ell_{\infty}$) and then
transferred these guarantees to $\ell_1$ at additional loss. In contrast, we
propose a direct method that achieves improved and oracle-efficient rates of
$\widetilde{\mathcal{O}}(T^{-1/3})$ and $\widetilde{\mathcal{O}}(T^{-1/4})$
respectively, for online $\ell_1$-multicalibration. Our key insight is a novel
reduction of online \(\ell_1\)-multicalibration to an online learning problem
with product-based rewards, which we refer to as \emph{online linear-product
optimization} ($\mathtt{OLPO}$).
  To obtain the improved rate of $\widetilde{\mathcal{O}}(T^{-1/3})$, we
introduce a linearization of $\mathtt{OLPO}$ and design a no-regret algorithm
for this linearized problem. Although this method guarantees the desired
sublinear rate (nearly matching the best rate for online calibration), it
becomes computationally expensive when the group family \(\mathcal{H}\) is
large or infinite, since it enumerates all possible groups. To address
scalability, we propose a second approach to $\mathtt{OLPO}$ that makes only a
polynomial number of calls to an offline optimization (\emph{multicalibration
evaluation}) oracle, resulting in \emph{oracle-efficient} online
\(\ell_1\)-multicalibration with a rate of $\widetilde{\mathcal{O}}(T^{-1/4})$.
Our framework also extends to certain infinite families of groups (e.g., all
linear functions on the context space) by exploiting a $1$-Lipschitz property
of the \(\ell_1\)-multicalibration error with respect to \(\mathcal{H}\).

</details>


### [572] [FRIREN: Beyond Trajectories -- A Spectral Lens on Time](https://arxiv.org/pdf/2505.17370)
*Qilin Wang*

Main category: cs.LG

TL;DR: FRIREN introduces a geometry-preserving model for long-term time-series forecasting, focusing on geometric structure over pointwise prediction, achieving superior performance on chaotic systems and standard datasets.


<details>
  <summary>Details</summary>
Motivation: The paper challenges the assumption that all time-series data is pointwise predictable, proposing geometric structure as a better abstraction for dynamic-agnostic forecasting.

Method: FRIREN uses an augmented normalizing-flow block to embed data into a latent space, generating Wasserstein-2 efficient paths decomposed into geometric transformations, and provides a spectral view of dynamics.

Result: FRIREN outperforms TimeMixer on chaotic systems (Lorenz-63, Rossler) and standard datasets (ETT, Weather), with lower MSE, MAE, and SWD metrics.

Conclusion: FRIREN sets a new benchmark for LTSF by combining generative flows with spectral analysis, offering accurate and interpretable long-term forecasting.

Abstract: Long-term time-series forecasting (LTSF) models are often presented as
general-purpose solutions that can be applied across domains, implicitly
assuming that all data is pointwise predictable. Using chaotic systems such as
Lorenz-63 as a case study, we argue that geometric structure - not pointwise
prediction - is the right abstraction for a dynamic-agnostic foundational
model. Minimizing the Wasserstein-2 distance (W2), which captures geometric
changes, and providing a spectral view of dynamics are essential for
long-horizon forecasting. Our model, FRIREN (Flow-inspired Representations via
Interpretable Eigen-networks), implements an augmented normalizing-flow block
that embeds data into a normally distributed latent representation. It then
generates a W2-efficient optimal path that can be decomposed into rotation,
scaling, inverse rotation, and translation. This architecture yields locally
generated, geometry-preserving predictions that are independent of the
underlying dynamics, and a global spectral representation that functions as a
finite Koopman operator with a small modification. This enables practitioners
to identify which modes grow, decay, or oscillate, both locally and
system-wide. FRIREN achieves an MSE of 11.4, MAE of 1.6, and SWD of 0.96 on
Lorenz-63 in a 336-in, 336-out, dt=0.01 setting, surpassing TimeMixer (MSE
27.3, MAE 2.8, SWD 2.1). The model maintains effective prediction for 274 out
of 336 steps, approximately 2.5 Lyapunov times. On Rossler (96-in, 336-out),
FRIREN achieves an MSE of 0.0349, MAE of 0.0953, and SWD of 0.0170,
outperforming TimeMixer's MSE of 4.3988, MAE of 0.886, and SWD of 3.2065.
FRIREN is also competitive on standard LTSF datasets such as ETT and Weather.
By connecting modern generative flows with classical spectral analysis, FRIREN
makes long-term forecasting both accurate and interpretable, setting a new
benchmark for LTSF model design.

</details>


### [573] [An End-to-End Approach for Child Reading Assessment in the Xhosa Language](https://arxiv.org/pdf/2505.17371)
*Sergio Chevtchenko, Nikhil Navas, Rafaella Vale, Franco Ubaudi, Sipumelele Lucwaba, Cally Ardington, Soheil Afshar, Mark Antoniou, Saeed Afshar*

Main category: cs.LG

TL;DR: The study addresses child literacy in low-resource languages like Xhosa by developing an AI-driven reading assessment system using a novel dataset and evaluating state-of-the-art models.


<details>
  <summary>Details</summary>
Motivation: Child literacy impacts life outcomes, but low-resource regions lack tools for effective assessment. AI can bridge this gap economically.

Method: A dataset of Xhosa child speech is created and evaluated using fine-tuned models (wav2vec 2.0, HuBERT, Whisper).

Result: Model performance depends on data balance and quantity. wav2vec 2.0 improves with multi-class training even with limited samples.

Conclusion: AI models can effectively assess child literacy in low-resource languages, with data balance being crucial for cost-effective scaling.

Abstract: Child literacy is a strong predictor of life outcomes at the subsequent
stages of an individual's life. This points to a need for targeted
interventions in vulnerable low and middle income populations to help bridge
the gap between literacy levels in these regions and high income ones. In this
effort, reading assessments provide an important tool to measure the
effectiveness of these programs and AI can be a reliable and economical tool to
support educators with this task. Developing accurate automatic reading
assessment systems for child speech in low-resource languages poses significant
challenges due to limited data and the unique acoustic properties of children's
voices. This study focuses on Xhosa, a language spoken in South Africa, to
advance child speech recognition capabilities. We present a novel dataset
composed of child speech samples in Xhosa. The dataset is available upon
request and contains ten words and letters, which are part of the Early Grade
Reading Assessment (EGRA) system. Each recording is labeled with an online and
cost-effective approach by multiple markers and a subsample is validated by an
independent EGRA reviewer. This dataset is evaluated with three fine-tuned
state-of-the-art end-to-end models: wav2vec 2.0, HuBERT, and Whisper. The
results indicate that the performance of these models can be significantly
influenced by the amount and balancing of the available training data, which is
fundamental for cost-effective large dataset collection. Furthermore, our
experiments indicate that the wav2vec 2.0 performance is improved by training
on multiple classes at a time, even when the number of available samples is
constrained.

</details>


### [574] [Value-Guided Search for Efficient Chain-of-Thought Reasoning](https://arxiv.org/pdf/2505.17373)
*Kaiwen Wang, Jin Peng Zhou, Jonathan Chang, Zhaolin Gao, Nathan Kallus, Kianté Brantley, Wen Sun*

Main category: cs.LG

TL;DR: A simple, efficient method for training value models on long-context reasoning traces avoids fine-grained step definitions, outperforming standard methods like majority voting.


<details>
  <summary>Details</summary>
Motivation: Existing process reward models (PRMs) struggle with defining steps for long-context reasoning, prompting a need for a simpler approach.

Method: A 1.5B token-level value model is trained on 2.5M reasoning traces, applied to DeepSeek models with block-wise value-guided search (VGS) and weighted majority vote.

Result: VGS achieves 45.7% accuracy on math benchmarks with 64 generations, matching o3-mini-medium, while reducing FLOPs vs. majority voting.

Conclusion: The method is effective, efficient, and open-sourced, offering a scalable solution for long-context reasoning.

Abstract: In this paper, we propose a simple and efficient method for value model
training on long-context reasoning traces. Compared to existing process reward
models (PRMs), our method does not require a fine-grained notion of "step,"
which is difficult to define for long-context reasoning models. By collecting a
dataset of 2.5 million reasoning traces, we train a 1.5B token-level value
model and apply it to DeepSeek models for improved performance with test-time
compute scaling. We find that block-wise value-guided search (VGS) with a final
weighted majority vote achieves better test-time scaling than standard methods
such as majority voting or best-of-n. With an inference budget of 64
generations, VGS with DeepSeek-R1-Distill-1.5B achieves an average accuracy of
45.7% across four competition math benchmarks (AIME 2024 & 2025, HMMT Feb 2024
& 2025), reaching parity with o3-mini-medium. Moreover, VGS significantly
reduces the inference FLOPs required to achieve the same performance of
majority voting. Our dataset, model and codebase are open-sourced.

</details>


### [575] [Provably Efficient Algorithm for Best Scoring Rule Identification in Online Principal-Agent Information Acquisition](https://arxiv.org/pdf/2505.17379)
*Zichen Wang, Chuanhao Li, Huazheng Wang*

Main category: cs.LG

TL;DR: The paper proposes two algorithms, OIAFC and OIAFB, to identify optimal scoring rules in online information acquisition, with theoretical guarantees on efficiency and performance.


<details>
  <summary>Details</summary>
Motivation: The study aims to address the challenge of determining the optimal scoring rule from the principal's perspective in online information acquisition.

Method: Two algorithms are introduced: OIAFC for fixed confidence and OIAFB for fixed budget settings, analyzed theoretically for their performance.

Result: OIAFC achieves efficient sample complexity, while OIAFB matches its instance-independent performance, with both algorithms sharing similar complexity.

Conclusion: The proposed algorithms effectively solve the problem of identifying optimal scoring rules in online information acquisition, with robust theoretical guarantees.

Abstract: We investigate the problem of identifying the optimal scoring rule within the
principal-agent framework for online information acquisition problem. We focus
on the principal's perspective, seeking to determine the desired scoring rule
through interactions with the agent. To address this challenge, we propose two
algorithms: OIAFC and OIAFB, tailored for fixed confidence and fixed budget
settings, respectively. Our theoretical analysis demonstrates that OIAFC can
extract the desired $(\epsilon, \delta)$-scoring rule with a efficient
instance-dependent sample complexity or an instance-independent sample
complexity. Our analysis also shows that OIAFB matches the instance-independent
performance bound of OIAFC, while both algorithms share the same complexity
across fixed confidence and fixed budget settings.

</details>


### [576] [Variational Autoencoding Discrete Diffusion with Enhanced Dimensional Correlations Modeling](https://arxiv.org/pdf/2505.17384)
*Tianyu Xie, Shuchen Xue, Zijin Feng, Tianyang Hu, Jiacheng Sun, Zhenguo Li, Cheng Zhang*

Main category: cs.LG

TL;DR: VADD improves discrete diffusion models by using latent variables to capture inter-dimensional dependencies, enhancing sample quality with few denoising steps.


<details>
  <summary>Details</summary>
Motivation: MDMs degrade in performance with few denoising steps due to limited modeling of inter-dimensional dependencies.

Method: VADD introduces latent variable modeling and an auxiliary recognition model for stable training via variational lower bounds maximization.

Result: VADD outperforms MDMs in sample quality on 2D toy data, image generation, and text generation, especially with few steps.

Conclusion: VADD enhances discrete diffusion models by efficiently capturing correlations among dimensions, improving performance with minimal denoising steps.

Abstract: Discrete diffusion models have recently shown great promise for modeling
complex discrete data, with masked diffusion models (MDMs) offering a
compelling trade-off between quality and generation speed. MDMs denoise by
progressively unmasking multiple dimensions from an all-masked input, but their
performance can degrade when using few denoising steps due to limited modeling
of inter-dimensional dependencies. In this paper, we propose Variational
Autoencoding Discrete Diffusion (VADD), a novel framework that enhances
discrete diffusion with latent variable modeling to implicitly capture
correlations among dimensions. By introducing an auxiliary recognition model,
VADD enables stable training via variational lower bounds maximization and
amortized inference over the training set. Our approach retains the efficiency
of traditional MDMs while significantly improving sample quality, especially
when the number of denoising steps is small. Empirical results on 2D toy data,
pixel-level image generation, and text generation demonstrate that VADD
consistently outperforms MDM baselines.

</details>


### [577] [Spectral Mixture Kernels for Bayesian Optimization](https://arxiv.org/pdf/2505.17393)
*Yi Zhang, Cheng Hua*

Main category: cs.LG

TL;DR: A novel Gaussian Process-based Bayesian Optimization method using spectral mixture kernels improves efficiency and performance, outperforming existing methods.


<details>
  <summary>Details</summary>
Motivation: Selecting an appropriate surrogate model for Bayesian Optimization is challenging, and current methods lack efficiency or performance.

Method: Incorporates spectral mixture kernels derived from Cauchy and Gaussian distributions, balancing speed and accuracy.

Result: Achieves better efficiency and optimization performance, outperforming baselines in synthetic and real-world problems.

Conclusion: The proposed method is effective for diverse optimization tasks, with theoretical bounds supporting its performance.

Abstract: Bayesian Optimization (BO) is a widely used approach for solving expensive
black-box optimization tasks. However, selecting an appropriate probabilistic
surrogate model remains an important yet challenging problem. In this work, we
introduce a novel Gaussian Process (GP)-based BO method that incorporates
spectral mixture kernels, derived from spectral densities formed by
scale-location mixtures of Cauchy and Gaussian distributions. This method
achieves a significant improvement in both efficiency and optimization
performance, matching the computational speed of simpler kernels while
delivering results that outperform more complex models and automatic BO
methods. We provide bounds on the information gain and cumulative regret
associated with obtaining the optimum. Extensive numerical experiments
demonstrate that our method consistently outperforms existing baselines across
a diverse range of synthetic and real-world problems, including both low- and
high-dimensional settings.

</details>


### [578] [Wasserstein Transfer Learning](https://arxiv.org/pdf/2505.17404)
*Kaicheng Zhang, Sinian Zhang, Doudou Zhou, Yidong Zhou*

Main category: cs.LG

TL;DR: A novel framework for transfer learning in regression models with probability distributions in Wasserstein space, addressing domain similarity and negative transfer.


<details>
  <summary>Details</summary>
Motivation: Traditional transfer learning is limited to Euclidean spaces, failing to handle complex data like probability distributions.

Method: Proposes estimators for known informative source domains and a data-driven procedure for unknown cases.

Result: Theoretical guarantees and empirical validation show improved transfer efficiency and mitigation of negative transfer.

Conclusion: The framework effectively extends transfer learning to Wasserstein space, with practical and theoretical support.

Abstract: Transfer learning is a powerful paradigm for leveraging knowledge from source
domains to enhance learning in a target domain. However, traditional transfer
learning approaches often focus on scalar or multivariate data within Euclidean
spaces, limiting their applicability to complex data structures such as
probability distributions. To address this, we introduce a novel framework for
transfer learning in regression models, where outputs are probability
distributions residing in the Wasserstein space. When the informative subset of
transferable source domains is known, we propose an estimator with provable
asymptotic convergence rates, quantifying the impact of domain similarity on
transfer efficiency. For cases where the informative subset is unknown, we
develop a data-driven transfer learning procedure designed to mitigate negative
transfer. The proposed methods are supported by rigorous theoretical analysis
and are validated through extensive simulations and real-world applications.

</details>


### [579] [HyperIMTS: Hypergraph Neural Network for Irregular Multivariate Time Series Forecasting](https://arxiv.org/pdf/2505.17431)
*Boyuan Li, Yicheng Luo, Zhen Liu, Junhao Zheng, Jianming Lv, Qianli Ma*

Main category: cs.LG

TL;DR: HyperIMTS is a hypergraph neural network for irregular multivariate time series forecasting, capturing dependencies without padding or disrupting sampling patterns.


<details>
  <summary>Details</summary>
Motivation: Existing models for irregular multivariate time series (IMTS) struggle with padding inefficiencies or limited dependency capture among unaligned observations.

Method: HyperIMTS converts observed values into hypergraph nodes, connected by temporal and variable hyperedges for unified message passing.

Result: HyperIMTS outperforms state-of-the-art models in IMTS forecasting with low computational cost.

Conclusion: HyperIMTS effectively captures dependencies in irregular time series, offering a competitive and efficient solution.

Abstract: Irregular multivariate time series (IMTS) are characterized by irregular time
intervals within variables and unaligned observations across variables, posing
challenges in learning temporal and variable dependencies. Many existing IMTS
models either require padded samples to learn separately from temporal and
variable dimensions, or represent original samples via bipartite graphs or
sets. However, the former approaches often need to handle extra padding values
affecting efficiency and disrupting original sampling patterns, while the
latter ones have limitations in capturing dependencies among unaligned
observations. To represent and learn both dependencies from original
observations in a unified form, we propose HyperIMTS, a Hypergraph neural
network for Irregular Multivariate Time Series forecasting. Observed values are
converted as nodes in the hypergraph, interconnected by temporal and variable
hyperedges to enable message passing among all observations. Through
irregularity-aware message passing, HyperIMTS captures variable dependencies in
a time-adaptive way to achieve accurate forecasting. Experiments demonstrate
HyperIMTS's competitive performance among state-of-the-art models in IMTS
forecasting with low computational cost.

</details>


### [580] [Discretization-free Multicalibration through Loss Minimization over Tree Ensembles](https://arxiv.org/pdf/2505.17435)
*Hongyi Henry Jin, Zijun Ding, Dung Daniel Ngo, Zhiwei Steven Wu*

Main category: cs.LG

TL;DR: A discretization-free multicalibration method is proposed, optimizing empirical risk over depth-two decision trees, outperforming existing approaches.


<details>
  <summary>Details</summary>
Motivation: Existing multicalibration methods rely on discretization, introducing errors and hyperparameters, which hinder downstream tasks.

Method: Direct optimization of empirical risk over depth-two decision trees using tools like LightGBM, under a technical condition called loss saturation.

Result: The method provably achieves multicalibration, with empirical validation showing the condition is always met, outperforming baselines.

Conclusion: The discretization-free approach is effective, practical, and superior to existing methods.

Abstract: In recent years, multicalibration has emerged as a desirable learning
objective for ensuring that a predictor is calibrated across a rich collection
of overlapping subpopulations. Existing approaches typically achieve
multicalibration by discretizing the predictor's output space and iteratively
adjusting its output values. However, this discretization approach departs from
the standard empirical risk minimization (ERM) pipeline, introduces rounding
error and additional sensitive hyperparameter, and may distort the predictor's
outputs in ways that hinder downstream decision-making.
  In this work, we propose a discretization-free multicalibration method that
directly optimizes an empirical risk objective over an ensemble of depth-two
decision trees. Our ERM approach can be implemented using off-the-shelf tree
ensemble learning methods such as LightGBM. Our algorithm provably achieves
multicalibration, provided that the data distribution satisfies a technical
condition we term as loss saturation. Across multiple datasets, our empirical
evaluation shows that this condition is always met in practice. Our
discretization-free algorithm consistently matches or outperforms existing
multicalibration approaches--even when evaluated using a discretization-based
multicalibration metric that shares its discretization granularity with the
baselines.

</details>


### [581] [Designing an efficient and equitable humanitarian supply chain dynamically via reinforcement learning](https://arxiv.org/pdf/2505.17439)
*Weijia Jin*

Main category: cs.LG

TL;DR: The paper proposes a reinforcement learning (PPO) model for designing an efficient and equitable humanitarian supply chain, prioritizing average satisfaction rate over heuristic methods.


<details>
  <summary>Details</summary>
Motivation: To address the need for dynamic and fair humanitarian supply chains, leveraging reinforcement learning for better performance.

Method: Uses Proximal Policy Optimization (PPO) and compares it with heuristic algorithms.

Result: The PPO model consistently prioritizes the average satisfaction rate in the supply chain.

Conclusion: Reinforcement learning, specifically PPO, is effective for equitable and efficient humanitarian supply chain design.

Abstract: This study designs an efficient and equitable humanitarian supply chain
dynamically by using reinforcement learning, PPO, and compared with heuristic
algorithms. This study demonstrates the model of PPO always treats average
satisfaction rate as the priority.

</details>


### [582] [Baitradar: A Multi-Model Clickbait Detection Algorithm Using Deep Learning](https://arxiv.org/pdf/2505.17448)
*Bhanuka Gamage, Adnan Labib, Aisha Joomun, Chern Hong Lim, KokSheik Wong*

Main category: cs.LG

TL;DR: The paper proposes BaitRadar, a deep learning algorithm to detect clickbait on YouTube by analyzing multiple video attributes, achieving 98% accuracy.


<details>
  <summary>Details</summary>
Motivation: The rising issue of clickbait on YouTube misleads users with misleading titles and thumbnails, prompting the need for an automated detection solution.

Method: BaitRadar uses six deep learning models analyzing title, comments, thumbnail, tags, video statistics, and audio transcript, averaging their outputs for robust classification.

Result: Tested on 1,400 videos, BaitRadar achieved 98% accuracy with inference under 2s.

Conclusion: BaitRadar effectively detects clickbait, offering a reliable solution even with incomplete data.

Abstract: Following the rising popularity of YouTube, there is an emerging problem on
this platform called clickbait, which provokes users to click on videos using
attractive titles and thumbnails. As a result, users ended up watching a video
that does not have the content as publicized in the title. This issue is
addressed in this study by proposing an algorithm called BaitRadar, which uses
a deep learning technique where six inference models are jointly consulted to
make the final classification decision. These models focus on different
attributes of the video, including title, comments, thumbnail, tags, video
statistics and audio transcript. The final classification is attained by
computing the average of multiple models to provide a robust and accurate
output even in situation where there is missing data. The proposed method is
tested on 1,400 YouTube videos. On average, a test accuracy of 98% is achieved
with an inference time of less than 2s.

</details>


### [583] [CLIMB: Class-imbalanced Learning Benchmark on Tabular Data](https://arxiv.org/pdf/2505.17451)
*Zhining Liu, Zihao Li, Ze Yang, Tianxin Wei, Jian Kang, Yada Zhu, Hendrik Hamann, Jingrui He, Hanghang Tong*

Main category: cs.LG

TL;DR: CLIMB is a benchmark for class-imbalanced learning on tabular data, offering 73 datasets, 29 algorithms, and practical insights on method performance.


<details>
  <summary>Details</summary>
Motivation: Addressing the challenge of learning from imbalanced tabular data, where minority classes are critical but rare.

Method: Developed CLIMB, a Python package with unified APIs, 73 datasets, and 29 CIL algorithms, ensuring high-quality implementation and documentation.

Result: Experiments revealed limitations of naive rebalancing, effectiveness of ensembles, and the impact of data quality.

Conclusion: CLIMB provides a robust, accessible tool for CIL research and applications, with code and documentation publicly available.

Abstract: Class-imbalanced learning (CIL) on tabular data is important in many
real-world applications where the minority class holds the critical but rare
outcomes. In this paper, we present CLIMB, a comprehensive benchmark for
class-imbalanced learning on tabular data. CLIMB includes 73 real-world
datasets across diverse domains and imbalance levels, along with unified
implementations of 29 representative CIL algorithms. Built on a high-quality
open-source Python package with unified API designs, detailed documentation,
and rigorous code quality controls, CLIMB supports easy implementation and
comparison between different CIL algorithms. Through extensive experiments, we
provide practical insights on method accuracy and efficiency, highlighting the
limitations of naive rebalancing, the effectiveness of ensembles, and the
importance of data quality. Our code, documentation, and examples are available
at https://github.com/ZhiningLiu1998/imbalanced-ensemble.

</details>


### [584] [Self-Training Large Language Models with Confident Reasoning](https://arxiv.org/pdf/2505.17454)
*Hyosoon Jang, Yunhui Jang, Sungjae Lee, Jungseul Ok, Sungsoo Ahn*

Main category: cs.LG

TL;DR: The paper introduces CORE-PO, a self-training method for LLMs that prioritizes high-confidence reasoning paths over just correct answers, improving accuracy across benchmarks.


<details>
  <summary>Details</summary>
Motivation: Current self-training methods for LLMs focus on answer correctness, ignoring reasoning path quality, which can lead to misleading improvements.

Method: Proposes CORE-PO, a method using reasoning-level confidence and policy optimization to fine-tune LLMs for better reasoning paths.

Result: CORE-PO outperforms existing methods, enhancing accuracy on in-distribution and out-of-distribution benchmarks.

Conclusion: Reasoning-level confidence is crucial for self-training LLMs, and CORE-PO effectively leverages it for improved performance.

Abstract: Large language models (LLMs) have shown impressive performance by generating
reasoning paths before final answers, but learning such a reasoning path
requires costly human supervision. To address this issue, recent studies have
explored self-training methods that improve reasoning capabilities using
pseudo-labels generated by the LLMs themselves. Among these, confidence-based
self-training fine-tunes LLMs to prefer reasoning paths with high-confidence
answers, where confidence is estimated via majority voting. However, such
methods exclusively focus on the quality of the final answer and may ignore the
quality of the reasoning paths, as even an incorrect reasoning path leads to a
correct answer by chance. Instead, we advocate the use of reasoning-level
confidence to identify high-quality reasoning paths for self-training,
supported by our empirical observations. We then propose a new self-training
method, CORE-PO, that fine-tunes LLMs to prefer high-COnfidence REasoning paths
through Policy Optimization. Our experiments show that CORE-PO improves the
accuracy of outputs on four in-distribution and two out-of-distribution
benchmarks, compared to existing self-training methods.

</details>


### [585] [Towards Heterogeneous Continual Graph Learning via Meta-knowledge Distillation](https://arxiv.org/pdf/2505.17458)
*Guiquan Sun, Xikun Zhang, Jingchao Ni, Dongjin Song*

Main category: cs.LG

TL;DR: MKD is a meta-learning and knowledge distillation framework for continual learning on expanding heterogeneous graphs, addressing catastrophic forgetting with efficient sampling and semantic-level distillation.


<details>
  <summary>Details</summary>
Motivation: Real-world graphs are dynamic and heterogeneous, but existing models assume static graphs, necessitating solutions for continual learning.

Method: MKD uses meta-learning for rapid adaptation and knowledge distillation to balance new and existing knowledge, with a novel sampling strategy and semantic-level distillation.

Result: MKD outperforms benchmarks in continual learning on expanding heterogeneous graphs across three datasets.

Conclusion: MKD effectively addresses continual learning challenges in dynamic heterogeneous graphs, balancing adaptation and knowledge retention.

Abstract: Machine learning on heterogeneous graphs has experienced rapid advancement in
recent years, driven by the inherently heterogeneous nature of real-world data.
However, existing studies typically assume the graphs to be static, while
real-world graphs are continuously expanding. This dynamic nature requires
models to adapt to new data while preserving existing knowledge. To this end,
this work addresses the challenge of continual learning on heterogeneous graphs
by introducing the Meta-learning based Knowledge Distillation framework (MKD),
designed to mitigate catastrophic forgetting in evolving heterogeneous graph
structures. MKD combines rapid task adaptation through meta-learning on limited
samples with knowledge distillation to achieve an optimal balance between
incorporating new information and maintaining existing knowledge. To improve
the efficiency and effectiveness of sample selection, MKD incorporates a novel
sampling strategy that selects a small number of target-type nodes based on
node diversity and maintains fixed-size buffers for other types. The strategy
retrieves first-order neighbors along metapaths and selects important neighbors
based on their structural relevance, enabling the sampled subgraphs to retain
key topological and semantic information. In addition, MKD introduces a
semantic-level distillation module that aligns the attention distributions over
different metapaths between teacher and student models, encouraging semantic
consistency beyond the logit level. Comprehensive evaluations across three
benchmark datasets validate MKD's effectiveness in handling continual learning
scenarios on expanding heterogeneous graphs.

</details>


### [586] [Efficient compression of neural networks and datasets](https://arxiv.org/pdf/2505.17469)
*Lukas Silvester Barth, Paulo von Petersenn*

Main category: cs.LG

TL;DR: The paper introduces methods to reduce neural network parameters while maintaining accuracy, improving data compression and regularization techniques.


<details>
  <summary>Details</summary>
Motivation: To decrease neural network parameters without sacrificing accuracy and enhance data compression algorithms.

Method: Probabilistic reformulation of ℓ0 regularization, smooth ℓ0 norm approximations, and layerwise methods, tested on various architectures and datasets.

Result: Effective parameter reduction and compression, with improved convergence in regularized models.

Conclusion: The methods successfully balance parameter efficiency and accuracy, aligning with theoretical predictions on sample-efficient convergence.

Abstract: We compare, improve, and contribute methods that substantially decrease the
number of parameters of neural networks while maintaining high test accuracy.
When applying our methods to minimize description length, we obtain very
effective data compression algorithms. In particular, we develop a
probabilistic reformulation of $\ell_0$ regularized optimization for nonlinear
models that does not require Monte-Carlo sampling and thus improves upon
previous methods. We also improve upon methods involving smooth approximations
to the $\ell_0$ norm, and investigate layerwise methods. We compare the methods
on different architectures and datasets, including convolutional networks
trained on image datasets and transformers trained on parts of Wikipedia. We
also created a synthetic teacher-student setup to investigate compression in a
controlled continuous setting. Finally, we conceptually relate compression
algorithms to Solomonoff's theory of inductive inference and empirically verify
the prediction that regularized models can exhibit more sample-efficient
convergence.

</details>


### [587] [Simultaneous Modeling of Protein Conformation and Dynamics via Autoregression](https://arxiv.org/pdf/2505.17478)
*Yuning Shen, Lihao Wang, Huizhuo Yuan, Yan Wang, Bangji Yang, Quanquan Gu*

Main category: cs.LG

TL;DR: ConfRover is an autoregressive model for learning protein conformation and dynamics from MD data, supporting both time-dependent and time-independent sampling.


<details>
  <summary>Details</summary>
Motivation: Existing methods fail to capture temporal dependencies or support direct generation of time-independent samples, limiting their utility for protein dynamics analysis.

Method: ConfRover uses a modular architecture with an encoding layer, a temporal module for dynamics, and an SE(3) diffusion model for structure decoding.

Result: Experiments on the ATLAS dataset show ConfRover effectively learns conformational dynamics and supports diverse downstream tasks.

Conclusion: ConfRover is the first model to sample both conformations and trajectories in one framework, providing a flexible approach for protein MD data.

Abstract: Understanding protein dynamics is critical for elucidating their biological
functions. The increasing availability of molecular dynamics (MD) data enables
the training of deep generative models to efficiently explore the
conformational space of proteins. However, existing approaches either fail to
explicitly capture the temporal dependencies between conformations or do not
support direct generation of time-independent samples. To address these
limitations, we introduce ConfRover, an autoregressive model that
simultaneously learns protein conformation and dynamics from MD trajectories,
supporting both time-dependent and time-independent sampling. At the core of
our model is a modular architecture comprising: (i) an encoding layer, adapted
from protein folding models, that embeds protein-specific information and
conformation at each time frame into a latent space; (ii) a temporal module, a
sequence model that captures conformational dynamics across frames; and (iii)
an SE(3) diffusion model as the structure decoder, generating conformations in
continuous space. Experiments on ATLAS, a large-scale protein MD dataset of
diverse structures, demonstrate the effectiveness of our model in learning
conformational dynamics and supporting a wide range of downstream tasks.
ConfRover is the first model to sample both protein conformations and
trajectories within a single framework, offering a novel and flexible approach
for learning from protein MD data.

</details>


### [588] [Hyperspectral in situ remote sensing of water surface nitrate in the Fitzroy River estuary, Queensland, Australia, using deep learning](https://arxiv.org/pdf/2505.17483)
*Yiqing Guo, Nagur Cherukuru, Eric Lehmann, S. L. Kesav Unnithan, Gemma Kerrisk, Tim Malthus, Faisal Islam*

Main category: cs.LG

TL;DR: The study explores the relationship between nitrate levels and hyperspectral reflectance in the Fitzroy River estuary, showing that nitrate can be predicted using reflectance and salinity data.


<details>
  <summary>Details</summary>
Motivation: Nitrate pollution from rivers threatens coral reefs like the Great Barrier Reef. Understanding its indirect link to water reflectance could improve monitoring.

Method: Time-series nitrate and hyperspectral reflectance data were collected in the Fitzroy River estuary. Nitrate was predicted using reflectance and salinity measurements.

Result: The model predicted nitrate accurately (R²=0.86, RMSE=0.03 mg/L), showing a strong correlation with ground-truth data.

Conclusion: Hyperspectral reflectance and salinity can effectively predict nitrate levels, aiding in monitoring water quality for coral reef protection.

Abstract: Nitrate ($\text{NO}_3^-$) is a form of dissolved inorganic nitrogen derived
primarily from anthropogenic sources. The recent increase in river-discharged
nitrate poses a major risk for coral bleaching in the Great Barrier Reef (GBR)
lagoon. Although nitrate is an optically inactive (i.e., colourless)
constituent, previous studies have demonstrated there is an indirect,
non-causal relationship between water surface nitrate and water-leaving
reflectance that is mediated through optically active water quality parameters
such as total suspended solids and coloured dissolved organic matter. This work
aims to advance our understanding of this relationship with an effort to
measure time-series nitrate and simultaneous hyperspectral reflectance at the
Fitzroy River estuary, Queensland, Australia. Time-series observations revealed
periodic cycles in nitrate loads due to the tidal influence in the estuarine
study site. The water surface nitrate loads were predicted from hyperspectral
reflectance and water salinity measurements, with hyperspectral reflectance
indicating the concentrations of optically active variables and salinity
indicating the mixing of river water and seawater proportions. The accuracy
assessment of model-predicted nitrate against in-situ measured nitrate values
showed that the predicted nitrate values correlated well with the ground-truth
data, with an $R^2$ score of 0.86, and an RMSE of 0.03 mg/L. This work
demonstrates the feasibility of predicting water surface nitrate from
hyperspectral reflectance and salinity measurements.

</details>


### [589] [ExARNN: An Environment-Driven Adaptive RNN for Learning Non-Stationary Power Dynamics](https://arxiv.org/pdf/2505.17488)
*Haoran Li, Muhao Guo, Yang Weng, Marija Ilic, Guangchun Ruan*

Main category: cs.LG

TL;DR: ExARNN, a novel RNN framework, integrates external data for dynamic adaptation in power system dynamics, outperforming traditional models.


<details>
  <summary>Details</summary>
Motivation: Traditional RNNs lack mechanisms to encode external factors like weather or time, limiting their adaptability to non-stationary power system dynamics influenced by renewables and climate change.

Method: ExARNN uses a hierarchical hypernetwork with Neural Controlled Differential Equations (NCDE) to process external data and adaptively adjust RNN parameters, handling inconsistent timestamps.

Result: ExARNN outperforms baseline models in forecasting tests, demonstrating superior adaptability.

Conclusion: ExARNN effectively addresses the limitations of traditional RNNs by integrating external data for continuous adaptation in complex power system dynamics.

Abstract: Non-stationary power system dynamics, influenced by renewable energy
variability, evolving demand patterns, and climate change, are becoming
increasingly complex. Accurately capturing these dynamics requires a model
capable of adapting to environmental factors. Traditional models, including
Recurrent Neural Networks (RNNs), lack efficient mechanisms to encode external
factors, such as time or environmental data, for dynamic adaptation. To address
this, we propose the External Adaptive RNN (ExARNN), a novel framework that
integrates external data (e.g., weather, time) to continuously adjust the
parameters of a base RNN. ExARNN achieves this through a hierarchical
hypernetwork design, using Neural Controlled Differential Equations (NCDE) to
process external data and generate RNN parameters adaptively. This approach
enables ExARNN to handle inconsistent timestamps between power and external
measurements, ensuring continuous adaptation. Extensive forecasting tests
demonstrate ExARNN's superiority over established baseline models.

</details>


### [590] [ProxySPEX: Inference-Efficient Interpretability via Sparse Feature Interactions in LLMs](https://arxiv.org/pdf/2505.17495)
*Landon Butler, Abhineet Agarwal, Justin Singh Kang, Yigit Efe Erginbas, Bin Yu, Kannan Ramchandran*

Main category: cs.LG

TL;DR: ProxySPEX is a hierarchical interaction attribution algorithm for LLMs, improving efficiency and accuracy over prior methods like SPEX.


<details>
  <summary>Details</summary>
Motivation: Existing methods for identifying feature interactions in LLMs scale poorly or require excessive computations. ProxySPEX leverages hierarchical interactions for efficiency.

Method: ProxySPEX fits gradient boosted trees to masked LLM outputs to extract important interactions, reducing the need for many inferences.

Result: ProxySPEX outperforms marginal attribution by 20% in reconstructing outputs and uses 10x fewer inferences than SPEX. It also identifies more influential features.

Conclusion: ProxySPEX efficiently discovers hierarchical interactions, enhancing interpretability for tasks like data attribution and mechanistic analysis.

Abstract: Large Language Models (LLMs) have achieved remarkable performance by
capturing complex interactions between input features. To identify these
interactions, most existing approaches require enumerating all possible
combinations of features up to a given order, causing them to scale poorly with
the number of inputs $n$. Recently, Kang et al. (2025) proposed SPEX, an
information-theoretic approach that uses interaction sparsity to scale to $n
\approx 10^3$ features. SPEX greatly improves upon prior methods but requires
tens of thousands of model inferences, which can be prohibitive for large
models. In this paper, we observe that LLM feature interactions are often
hierarchical -- higher-order interactions are accompanied by their lower-order
subsets -- which enables more efficient discovery. To exploit this hierarchy,
we propose ProxySPEX, an interaction attribution algorithm that first fits
gradient boosted trees to masked LLM outputs and then extracts the important
interactions. Experiments across four challenging high-dimensional datasets
show that ProxySPEX more faithfully reconstructs LLM outputs by 20% over
marginal attribution approaches while using $10\times$ fewer inferences than
SPEX. By accounting for interactions, ProxySPEX identifies features that
influence model output over 20% more than those selected by marginal
approaches. Further, we apply ProxySPEX to two interpretability tasks. Data
attribution, where we identify interactions among CIFAR-10 training samples
that influence test predictions, and mechanistic interpretability, where we
uncover interactions between attention heads, both within and across layers, on
a question-answering task. ProxySPEX identifies interactions that enable more
aggressive pruning of heads than marginal approaches.

</details>


### [591] [On the Design of KL-Regularized Policy Gradient Algorithms for LLM Reasoning](https://arxiv.org/pdf/2505.17508)
*Yifan Zhang, Yifeng Liu, Huizhuo Yuan, Yang Yuan, Quanquan Gu, Andrew C Yao*

Main category: cs.LG

TL;DR: The paper introduces Regularized Policy Gradient (RPG), a framework for KL-regularized policy gradient methods in online RL, showing improved stability and performance in LLM reasoning tasks.


<details>
  <summary>Details</summary>
Motivation: To systematically explore and integrate KL divergence formulations into surrogate loss functions for online RL, addressing training stability and performance.

Method: Proposes RPG, deriving policy gradients and surrogate losses for forward/reverse KL divergences, normalized/unnormalized policies, and differentiable/REINFORCE-style estimators.

Result: Experiments demonstrate RPG's improved or competitive training stability and performance compared to baselines like GRPO, REINFORCE++, and DAPO.

Conclusion: RPG provides a versatile and effective framework for KL-regularized policy gradient methods in online RL, enhancing LLM reasoning capabilities.

Abstract: Policy gradient algorithms have been successfully applied to enhance the
reasoning capabilities of large language models (LLMs). Despite the widespread
use of Kullback-Leibler (KL) regularization in policy gradient algorithms to
stabilize training, the systematic exploration of how different KL divergence
formulations can be estimated and integrated into surrogate loss functions for
online reinforcement learning (RL) presents a nuanced and systematically
explorable design space. In this paper, we propose regularized policy gradient
(RPG), a systematic framework for deriving and analyzing KL-regularized policy
gradient methods in the online RL setting. We derive policy gradients and
corresponding surrogate loss functions for objectives regularized by both
forward and reverse KL divergences, considering both normalized and
unnormalized policy distributions. Furthermore, we present derivations for
fully differentiable loss functions as well as REINFORCE-style gradient
estimators, accommodating diverse algorithmic needs. We conduct extensive
experiments on RL for LLM reasoning using these methods, showing improved or
competitive results in terms of training stability and performance compared to
strong baselines such as GRPO, REINFORCE++, and DAPO. The code is available at
https://github.com/complex-reasoning/RPG.

</details>


### [592] [Spacetime Geometry of Denoising in Diffusion Models](https://arxiv.org/pdf/2505.17517)
*Rafał Karczewski, Markus Heinonen, Alison Pouplin, Søren Hauberg, Vikas Garg*

Main category: cs.LG

TL;DR: The paper introduces a novel geometric perspective on diffusion models using information geometry, framing noisy samples as a statistical manifold (spacetime) with a Fisher-Rao metric for efficient geodesic computation.


<details>
  <summary>Details</summary>
Motivation: To provide a geometric interpretation of diffusion models, leveraging information geometry to analyze noisy samples across all noise levels as a statistical manifold.

Method: The approach interprets noise levels as temporal parameters, forming a statistical manifold (spacetime) with a Fisher-Rao metric. Geodesics are computed efficiently due to the exponential family property.

Result: The geometric framework enables efficient computation of geodesics in high dimensions and is applied to transition path sampling, generating smooth trajectories between metastable states.

Conclusion: The study demonstrates the practical utility of the geometric viewpoint in diffusion models, offering a new tool for analyzing and generating continuous trajectories.

Abstract: We present a novel perspective on diffusion models using the framework of
information geometry. We show that the set of noisy samples, taken across all
noise levels simultaneously, forms a statistical manifold -- a family of
denoising probability distributions. Interpreting the noise level as a temporal
parameter, we refer to this manifold as spacetime. This manifold naturally
carries a Fisher-Rao metric, which defines geodesics -- shortest paths between
noisy points. Notably, this family of distributions is exponential, enabling
efficient geodesic computation even in high-dimensional settings without
retraining or fine-tuning. We demonstrate the practical value of this geometric
viewpoint in transition path sampling, where spacetime geodesics define smooth
sequences of Boltzmann distributions, enabling the generation of continuous
trajectories between low-energy metastable states. Code is available at:
https://github.com/Aalto-QuML/diffusion-spacetime-geometry.

</details>


### [593] [TimeCF: A TimeMixer-Based Model with adaptive Convolution and Sharpness-Aware Minimization Frequency Domain Loss for long-term time seris forecasting](https://arxiv.org/pdf/2505.17532)
*Bin Wang, Heming Yang, Jinfang Sheng*

Main category: cs.LG

TL;DR: TimeCF is a deep learning model for long-term time series forecasting, combining multi-scale analysis, adaptive convolution, and SAMFre loss to address autocorrelation issues and improve generalization.


<details>
  <summary>Details</summary>
Motivation: Existing multi-scale analysis models suffer from suboptimal predictions due to autocorrelation between time series labels, limiting generalization.

Method: TimeCF decomposes time series into multi-scale sequences, uses adaptive convolution for information aggregation, and employs SAMFre loss for optimization.

Result: Extensive experiments show TimeCF outperforms in long-term forecasting on real-world datasets.

Conclusion: TimeCF effectively addresses autocorrelation challenges and enhances forecasting performance through its novel design.

Abstract: Recent studies have shown that by introducing prior knowledge, multi-scale
analysis of complex and non-stationary time series in real environments can
achieve good results in the field of long-term forecasting. However, affected
by channel-independent methods, models based on multi-scale analysis may
produce suboptimal prediction results due to the autocorrelation between time
series labels, which in turn affects the generalization ability of the model.
To address this challenge, we are inspired by the idea of sharpness-aware
minimization and the recently proposed FreDF method and design a deep learning
model TimeCF for long-term time series forecasting based on the TimeMixer,
combined with our designed adaptive convolution information aggregation module
and Sharpness-Aware Minimization Frequency Domain Loss (SAMFre). Specifically,
TimeCF first decomposes the original time series into sequences of different
scales. Next, the same-sized convolution modules are used to adaptively
aggregate information of different scales on sequences of different scales.
Then, decomposing each sequence into season and trend parts and the two parts
are mixed at different scales through bottom-up and top-down methods
respectively. Finally, different scales are aggregated through a Feed-Forward
Network. What's more, extensive experimental results on different real-world
datasets show that our proposed TimeCF has excellent performance in the field
of long-term forecasting.

</details>


### [594] [Learning Representational Disparities](https://arxiv.org/pdf/2505.17533)
*Pavan Ravishankar, Rushabh Shah, Daniel B. Neill*

Main category: cs.LG

TL;DR: A fair ML algorithm is proposed to model interpretable differences between observed and desired human decision-making, aiming to reduce outcome disparities by addressing representational disparities.


<details>
  <summary>Details</summary>
Motivation: Prior work ignores outcome disparities in fair representation learning. This paper aims to model and mitigate such disparities by focusing on representational differences.

Method: The approach frames the problem as multi-objective optimization using a neural network, learning interpretable representational disparities to correct human decisions.

Result: Under simplifying assumptions, the model learns interpretable weights that fully mitigate outcome disparities, validated on real-world datasets.

Conclusion: The proposed method effectively addresses outcome disparities by modeling and correcting representational disparities in human decision-making.

Abstract: We propose a fair machine learning algorithm to model interpretable
differences between observed and desired human decision-making, with the latter
aimed at reducing disparity in a downstream outcome impacted by the human
decision. Prior work learns fair representations without considering the
outcome in the decision-making process. We model the outcome disparities as
arising due to the different representations of the input seen by the observed
and desired decision-maker, which we term representational disparities. Our
goal is to learn interpretable representational disparities which could
potentially be corrected by specific nudges to the human decision, mitigating
disparities in the downstream outcome; we frame this as a multi-objective
optimization problem using a neural network. Under reasonable simplifying
assumptions, we prove that our neural network model of the representational
disparity learns interpretable weights that fully mitigate the outcome
disparity. We validate objectives and interpret results using real-world German
Credit, Adult, and Heritage Health datasets.

</details>


### [595] [Graph Style Transfer for Counterfactual Explainability](https://arxiv.org/pdf/2505.17542)
*Bardh Prenkaj, Efstratios Zaradoukas, Gjergji Kasneci*

Main category: cs.LG

TL;DR: GIST introduces a backtracking process for graph counterfactual generation, improving validity and faithfulness compared to forward perturbation methods.


<details>
  <summary>Details</summary>
Motivation: To address challenges in generating valid and semantically meaningful counterfactuals for graph data while preserving structural integrity.

Method: Proposes Graph Inverse Style Transfer (GIST), leveraging spectral style transfer to align global structure and local content.

Result: GIST achieves +7.6% validity improvement and +45.5% faithfulness in explaining class distribution, with minimized spectral differences.

Conclusion: GIST advances graph explainability by challenging traditional forward perturbation methods with a novel backtracking approach.

Abstract: Counterfactual explainability seeks to uncover model decisions by identifying
minimal changes to the input that alter the predicted outcome. This task
becomes particularly challenging for graph data due to preserving structural
integrity and semantic meaning. Unlike prior approaches that rely on forward
perturbation mechanisms, we introduce Graph Inverse Style Transfer (GIST), the
first framework to re-imagine graph counterfactual generation as a backtracking
process, leveraging spectral style transfer. By aligning the global structure
with the original input spectrum and preserving local content faithfulness,
GIST produces valid counterfactuals as interpolations between the input style
and counterfactual content. Tested on 8 binary and multi-class graph
classification benchmarks, GIST achieves a remarkable +7.6% improvement in the
validity of produced counterfactuals and significant gains (+45.5%) in
faithfully explaining the true class distribution. Additionally, GIST's
backtracking mechanism effectively mitigates overshooting the underlying
predictor's decision boundary, minimizing the spectral differences between the
input and the counterfactuals. These results challenge traditional forward
perturbation methods, offering a novel perspective that advances graph
explainability.

</details>


### [596] [Universal Biological Sequence Reranking for Improved De Novo Peptide Sequencing](https://arxiv.org/pdf/2505.17552)
*Zijie Qiu, Jiaqi Wei, Xiang Zhang, Sheng Xu, Kai Zou, Zhi Jin, Zhiqiang Gao, Nanqing Dong, Siqi Sun*

Main category: cs.LG

TL;DR: RankNovo is a deep reranking framework for de novo peptide sequencing, improving accuracy by combining multiple models and introducing new metrics (PMD, RMD). It outperforms base models and shows strong generalization.


<details>
  <summary>Details</summary>
Motivation: Current deep learning methods for peptide sequencing are limited by data complexity and noise, causing biases. RankNovo aims to address these issues by leveraging multiple models.

Method: RankNovo uses a list-wise reranking approach with axial attention on candidate peptides. It introduces PMD and RMD metrics for supervision.

Result: RankNovo outperforms base models and sets a new benchmark. It also generalizes well to unseen models.

Conclusion: RankNovo challenges single-model paradigms and advances de novo sequencing accuracy, offering a robust, universal framework.

Abstract: De novo peptide sequencing is a critical task in proteomics. However, the
performance of current deep learning-based methods is limited by the inherent
complexity of mass spectrometry data and the heterogeneous distribution of
noise signals, leading to data-specific biases. We present RankNovo, the first
deep reranking framework that enhances de novo peptide sequencing by leveraging
the complementary strengths of multiple sequencing models. RankNovo employs a
list-wise reranking approach, modeling candidate peptides as multiple sequence
alignments and utilizing axial attention to extract informative features across
candidates. Additionally, we introduce two new metrics, PMD (Peptide Mass
Deviation) and RMD (residual Mass Deviation), which offer delicate supervision
by quantifying mass differences between peptides at both the sequence and
residue levels. Extensive experiments demonstrate that RankNovo not only
surpasses its base models used to generate training candidates for reranking
pre-training, but also sets a new state-of-the-art benchmark. Moreover,
RankNovo exhibits strong zero-shot generalization to unseen models whose
generations were not exposed during training, highlighting its robustness and
potential as a universal reranking framework for peptide sequencing. Our work
presents a novel reranking strategy that fundamentally challenges existing
single-model paradigms and advances the frontier of accurate de novo
sequencing. Our source code is provided on GitHub.

</details>


### [597] [CoMoE: Contrastive Representation for Mixture-of-Experts in Parameter-Efficient Fine-tuning](https://arxiv.org/pdf/2505.17553)
*Jinyuan Feng, Chaopeng Wei, Tenghai Qiu, Tianyi Hu, Zhiqiang Pu*

Main category: cs.LG

TL;DR: CoMoE introduces a contrastive objective to improve modularization and specialization in MoE, addressing underutilization in heterogeneous datasets.


<details>
  <summary>Details</summary>
Motivation: Current MoE variants underutilize capacity due to experts learning similar knowledge, especially in heterogeneous datasets.

Method: CoMoE trains experts with a contrastive objective by sampling from activated and inactivated experts in top-k routing.

Result: CoMoE enhances MoE's capacity and promotes expert modularization, validated on benchmarks and multi-task settings.

Conclusion: CoMoE effectively addresses MoE's limitations, improving performance and modularization.

Abstract: In parameter-efficient fine-tuning, mixture-of-experts (MoE), which involves
specializing functionalities into different experts and sparsely activating
them appropriately, has been widely adopted as a promising approach to
trade-off between model capacity and computation overhead. However, current MoE
variants fall short on heterogeneous datasets, ignoring the fact that experts
may learn similar knowledge, resulting in the underutilization of MoE's
capacity. In this paper, we propose Contrastive Representation for MoE (CoMoE),
a novel method to promote modularization and specialization in MoE, where the
experts are trained along with a contrastive objective by sampling from
activated and inactivated experts in top-k routing. We demonstrate that such a
contrastive objective recovers the mutual-information gap between inputs and
the two types of experts. Experiments on several benchmarks and in multi-task
settings demonstrate that CoMoE can consistently enhance MoE's capacity and
promote modularization among the experts.

</details>


### [598] [Wildfire spread forecasting with Deep Learning](https://arxiv.org/pdf/2505.17556)
*Nikolaos Anastasiou, Spyros Kondylatos, Ioannis Papoutsis*

Main category: cs.LG

TL;DR: A deep learning framework improves wildfire spread prediction by using multi-day spatio-temporal data, outperforming ignition-day-only models by 5% in accuracy.


<details>
  <summary>Details</summary>
Motivation: Accurate wildfire spread prediction is vital for risk management and emergency response, requiring advanced data-driven methods.

Method: A DL-based framework uses spatio-temporal data (remote sensing, weather, vegetation, etc.) from 2006-2022 in the Mediterranean, with ablation studies on temporal context.

Result: Including multi-day data (4 days pre- to 5 days post-ignition) boosts F1 score and IoU by 5% over the baseline.

Conclusion: The study highlights the value of temporal data in wildfire prediction and releases datasets/models to aid future research.

Abstract: Accurate prediction of wildfire spread is crucial for effective risk
management, emergency response, and strategic resource allocation. In this
study, we present a deep learning (DL)-based framework for forecasting the
final extent of burned areas, using data available at the time of ignition. We
leverage a spatio-temporal dataset that covers the Mediterranean region from
2006 to 2022, incorporating remote sensing data, meteorological observations,
vegetation maps, land cover classifications, anthropogenic factors, topography
data, and thermal anomalies. To evaluate the influence of temporal context, we
conduct an ablation study examining how the inclusion of pre- and post-ignition
data affects model performance, benchmarking the temporal-aware DL models
against a baseline trained exclusively on ignition-day inputs. Our results
indicate that multi-day observational data substantially improve predictive
accuracy. Particularly, the best-performing model, incorporating a temporal
window of four days before to five days after ignition, improves both the F1
score and the Intersection over Union by almost 5% in comparison to the
baseline on the test dataset. We publicly release our dataset and models to
enhance research into data-driven approaches for wildfire modeling and
response.

</details>


### [599] [Multiphysics Bench: Benchmarking and Investigating Scientific Machine Learning for Multiphysics PDEs](https://arxiv.org/pdf/2505.17575)
*Changfan Yang, Lichen Bai, Yinpeng Wang, Shufei Zhang, Zeke Xie*

Main category: cs.LG

TL;DR: The paper introduces a multiphysics dataset (Multiphysics Bench) and evaluates machine learning solvers for multiphysics PDEs, revealing poor performance of existing methods and providing insights for future research.


<details>
  <summary>Details</summary>
Motivation: Real-world systems often involve coupled physical fields, but machine learning studies have overlooked multiphysics problems, which are more complex due to inter-field coupling.

Method: The authors collect a comprehensive multiphysics dataset (Multiphysics Bench) and systematically evaluate learning-based PDE solvers (e.g., PINNs, FNO, DeepONet) on multiphysics problems.

Result: Existing solvers perform poorly on multiphysics problems, but the study provides insights and practical tricks for improving performance.

Conclusion: The work highlights the challenges of multiphysics PDEs and motivates future research in machine learning for coupled physical systems.

Abstract: Solving partial differential equations (PDEs) with machine learning has
recently attracted great attention, as PDEs are fundamental tools for modeling
real-world systems that range from fundamental physical science to advanced
engineering disciplines. Most real-world physical systems across various
disciplines are actually involved in multiple coupled physical fields rather
than a single field. However, previous machine learning studies mainly focused
on solving single-field problems, but overlooked the importance and
characteristics of multiphysics problems in real world. Multiphysics PDEs
typically entail multiple strongly coupled variables, thereby introducing
additional complexity and challenges, such as inter-field coupling. Both
benchmarking and solving multiphysics problems with machine learning remain
largely unexamined. To identify and address the emerging challenges in
multiphysics problems, we mainly made three contributions in this work. First,
we collect the first general multiphysics dataset, the Multiphysics Bench, that
focuses on multiphysics PDE solving with machine learning. Multiphysics Bench
is also the most comprehensive PDE dataset to date, featuring the broadest
range of coupling types, the greatest diversity of PDE formulations, and the
largest dataset scale. Second, we conduct the first systematic investigation on
multiple representative learning-based PDE solvers, such as PINNs, FNO,
DeepONet, and DiffusionPDE solvers, on multiphysics problems. Unfortunately,
naively applying these existing solvers usually show very poor performance for
solving multiphysics. Third, through extensive experiments and discussions, we
report multiple insights and a bag of useful tricks for solving multiphysics
with machine learning, motivating future directions in the study and simulation
of complex, coupled physical systems.

</details>


### [600] [Ownership Verification of DNN Models Using White-Box Adversarial Attacks with Specified Probability Manipulation](https://arxiv.org/pdf/2505.17579)
*Teruki Sano, Minoru Kuribayashi, Masao Sakai, Shuji Ishobe, Eisuke Koizumi*

Main category: cs.LG

TL;DR: A framework for verifying DNN model ownership using adversarial attacks without needing the original model.


<details>
  <summary>Details</summary>
Motivation: To address the challenge of verifying ownership of DNN models in gray-box scenarios where unauthorized copies exist.

Method: Uses a white-box adversarial attack (iterative FGSM with control parameters) to align output probabilities for verification.

Result: Effective identification of DNN models through adversarial examples.

Conclusion: The proposed framework successfully verifies model ownership in gray-box settings.

Abstract: In this paper, we propose a novel framework for ownership verification of
deep neural network (DNN) models for image classification tasks. It allows
verification of model identity by both the rightful owner and third party
without presenting the original model. We assume a gray-box scenario where an
unauthorized user owns a model that is illegally copied from the original
model, provides services in a cloud environment, and the user throws images and
receives the classification results as a probability distribution of output
classes. The framework applies a white-box adversarial attack to align the
output probability of a specific class to a designated value. Due to the
knowledge of original model, it enables the owner to generate such adversarial
examples. We propose a simple but effective adversarial attack method based on
the iterative Fast Gradient Sign Method (FGSM) by introducing control
parameters. Experimental results confirm the effectiveness of the
identification of DNN models using adversarial attack.

</details>


### [601] [MinkUNeXt-SI: Improving point cloud-based place recognition including spherical coordinates and LiDAR intensity](https://arxiv.org/pdf/2505.17591)
*Judith Vilella-Cantos, Juan José Cabrera, Luis Payá, Mónica Ballesta, David Valiente*

Main category: cs.LG

TL;DR: MinkUNeXt-SI is a LiDAR-based method for robust place recognition in autonomous navigation, using spherical coordinates and intensity values, achieving state-of-the-art performance and generalization.


<details>
  <summary>Details</summary>
Motivation: Addressing the challenge of accurate place recognition in varying conditions (e.g., seasons, weather) and environments for safe autonomous navigation.

Method: Preprocesses LiDAR point clouds into spherical coordinates and normalized intensity values, then uses a deep learning model combining Minkowski convolutions and U-net with skip connections.

Result: Surpasses state-of-the-art performance and generalizes well to other datasets; custom dataset evaluation also shows outstanding results.

Conclusion: MinkUNeXt-SI is a highly effective and generalizable solution for place recognition, with publicly available code and dataset for reproducibility.

Abstract: In autonomous navigation systems, the solution of the place recognition
problem is crucial for their safe functioning. But this is not a trivial
solution, since it must be accurate regardless of any changes in the scene,
such as seasonal changes and different weather conditions, and it must be
generalizable to other environments. This paper presents our method,
MinkUNeXt-SI, which, starting from a LiDAR point cloud, preprocesses the input
data to obtain its spherical coordinates and intensity values normalized within
a range of 0 to 1 for each point, and it produces a robust place recognition
descriptor. To that end, a deep learning approach that combines Minkowski
convolutions and a U-net architecture with skip connections is used. The
results of MinkUNeXt-SI demonstrate that this method reaches and surpasses
state-of-the-art performance while it also generalizes satisfactorily to other
datasets. Additionally, we showcase the capture of a custom dataset and its use
in evaluating our solution, which also achieves outstanding results. Both the
code of our solution and the runs of our dataset are publicly available for
reproducibility purposes.

</details>


### [602] [NeUQI: Near-Optimal Uniform Quantization Parameter Initialization](https://arxiv.org/pdf/2505.17595)
*Li Lin, Xinyu Hu, Xiaojun Wan*

Main category: cs.LG

TL;DR: NeUQI improves LLM quantization by optimizing initial parameters, outperforming existing methods and matching resource-intensive approaches when combined with distillation.


<details>
  <summary>Details</summary>
Motivation: LLMs face deployment challenges on consumer devices due to high memory and inference costs. PTQ helps, but current initialization methods are suboptimal.

Method: Proposes NeUQI, a method for near-optimal initialization of uniform quantization parameters, compatible with existing quantization techniques.

Result: NeUQI outperforms existing methods on LLaMA and Qwen models and matches PV-tuning when combined with distillation.

Conclusion: NeUQI offers an efficient solution for LLM quantization, enhancing performance and deployment feasibility.

Abstract: Large language models (LLMs) achieve impressive performance across domains
but face significant challenges when deployed on consumer-grade GPUs or
personal devices such as laptops, due to high memory consumption and inference
costs. Post-training quantization (PTQ) of LLMs offers a promising solution
that reduces their memory footprint and decoding latency. In practice, PTQ with
uniform quantization representation is favored for its efficiency and ease of
deployment since uniform quantization is widely supported by mainstream
hardware and software libraries. Recent studies on $\geq 2$-bit uniform
quantization have led to noticeable improvements in post-quantization model
performance; however, they primarily focus on quantization methodologies, while
the initialization of quantization parameters is underexplored and still relies
on the suboptimal Min-Max strategies. In this work, we propose NeUQI, a method
devoted to efficiently determining near-optimal initial parameters for uniform
quantization. NeUQI is orthogonal to prior quantization methodologies and can
seamlessly integrate with them. The experiments with the LLaMA and Qwen
families on various tasks demonstrate that our NeUQI consistently outperforms
existing methods. Furthermore, when combined with a lightweight distillation
strategy, NeUQI can achieve superior performance to PV-tuning, a much more
resource-intensive approach.

</details>


### [603] [Dynamic Text Bundling Supervision for Zero-Shot Inference on Text-Attributed Graphs](https://arxiv.org/pdf/2505.17599)
*Yusheng Zhao, Qixin Zhang, Xiao Luo, Weizhi Zhang, Zhiping Xiao, Wei Ju, Philip S. Yu, Ming Zhang*

Main category: cs.LG

TL;DR: The paper introduces DENSE, a method using LLMs to generate bundle-level labels for supervising graph neural networks, addressing challenges of limited graph structure information and unreliable LLM responses.


<details>
  <summary>Details</summary>
Motivation: To overcome LLMs' struggles with isolated text attributes and unreliable predictions in text-attributed graphs (TAGs).

Method: DENSE queries LLMs with text bundles for labels, uses these to supervise graph neural networks, and refines bundles to remove noise.

Result: Extensive experiments on ten datasets confirm DENSE's effectiveness.

Conclusion: DENSE successfully leverages LLMs for improved performance in TAGs by addressing information insufficiency and reliability issues.

Abstract: Large language models (LLMs) have been used in many zero-shot learning
problems, with their strong generalization ability. Recently, adopting LLMs in
text-attributed graphs (TAGs) has drawn increasing attention. However, the
adoption of LLMs faces two major challenges: limited information on graph
structure and unreliable responses. LLMs struggle with text attributes isolated
from the graph topology. Worse still, they yield unreliable predictions due to
both information insufficiency and the inherent weakness of LLMs (e.g.,
hallucination). Towards this end, this paper proposes a novel method named
Dynamic Text Bundling Supervision (DENSE) that queries LLMs with bundles of
texts to obtain bundle-level labels and uses these labels to supervise graph
neural networks. Specifically, we sample a set of bundles, each containing a
set of nodes with corresponding texts of close proximity. We then query LLMs
with the bundled texts to obtain the label of each bundle. Subsequently, the
bundle labels are used to supervise the optimization of graph neural networks,
and the bundles are further refined to exclude noisy items. To justify our
design, we also provide theoretical analysis of the proposed method. Extensive
experiments across ten datasets validate the effectiveness of the proposed
method.

</details>


### [604] [Adaptive Semantic Token Communication for Transformer-based Edge Inference](https://arxiv.org/pdf/2505.17604)
*Alessio Devoto, Jary Pomponi, Mattia Merluzzi, Paolo Di Lorenzo, Simone Scardapane*

Main category: cs.LG

TL;DR: An adaptive framework for edge inference using transformer-powered DJSCC, optimizing task-aware data transmission under dynamic conditions.


<details>
  <summary>Details</summary>
Motivation: Addresses resource-constrained edge devices needing efficient semantic communication for tasks like object detection under varying bandwidth and channel conditions.

Method: Tokenizes input into semantic representations, refines with a transformer, and adaptively compresses features via DJSCC with a Lyapunov-based resource allocation algorithm.

Result: Outperforms existing baselines, demonstrating robust performance in dynamic network conditions.

Conclusion: Proposes a strong foundation for AI-native semantic communication in edge intelligence applications.

Abstract: This paper presents an adaptive framework for edge inference based on a
dynamically configurable transformer-powered deep joint source channel coding
(DJSCC) architecture. Motivated by a practical scenario where a resource
constrained edge device engages in goal oriented semantic communication, such
as selectively transmitting essential features for object detection to an edge
server, our approach enables efficient task aware data transmission under
varying bandwidth and channel conditions. To achieve this, input data is
tokenized into compact high level semantic representations, refined by a
transformer, and transmitted over noisy wireless channels. As part of the DJSCC
pipeline, we employ a semantic token selection mechanism that adaptively
compresses informative features into a user specified number of tokens per
sample. These tokens are then further compressed through the JSCC module,
enabling a flexible token communication strategy that adjusts both the number
of transmitted tokens and their embedding dimensions. We incorporate a resource
allocation algorithm based on Lyapunov stochastic optimization to enhance
robustness under dynamic network conditions, effectively balancing compression
efficiency and task performance. Experimental results demonstrate that our
system consistently outperforms existing baselines, highlighting its potential
as a strong foundation for AI native semantic communication in edge
intelligence applications.

</details>


### [605] [Learning Equilibria from Data: Provably Efficient Multi-Agent Imitation Learning](https://arxiv.org/pdf/2505.17610)
*Till Freihaut, Luca Viano, Volkan Cevher, Matthieu Geist, Giorgia Ramponi*

Main category: cs.LG

TL;DR: The paper characterizes expert sample complexity for learning Nash equilibrium in Markov Games, introduces a new coefficient, and proposes two algorithms with varying query complexities.


<details>
  <summary>Details</summary>
Motivation: To address the challenge of learning Nash equilibrium from expert data in Markov Games, especially in settings with high concentrability coefficients.

Method: Introduces the single policy deviation concentrability coefficient, analyzes behavioral cloning, and develops two algorithms: MAIL-BRO (with best response oracle) and MURMAIL (without oracle).

Result: MAIL-BRO achieves ε-Nash equilibrium with O(ε⁻⁴) queries, while MURMAIL requires O(ε⁻⁸) queries. Numerical results support the findings.

Conclusion: The study provides theoretical and empirical insights into learning Nash equilibrium, highlighting trade-offs between oracle use and query efficiency.

Abstract: This paper provides the first expert sample complexity characterization for
learning a Nash equilibrium from expert data in Markov Games. We show that a
new quantity named the single policy deviation concentrability coefficient is
unavoidable in the non-interactive imitation learning setting, and we provide
an upper bound for behavioral cloning (BC) featuring such coefficient. BC
exhibits substantial regret in games with high concentrability coefficient,
leading us to utilize expert queries to develop and introduce two novel
solution algorithms: MAIL-BRO and MURMAIL. The former employs a best response
oracle and learns an $\varepsilon$-Nash equilibrium with
$\mathcal{O}(\varepsilon^{-4})$ expert and oracle queries. The latter bypasses
completely the best response oracle at the cost of a worse expert query
complexity of order $\mathcal{O}(\varepsilon^{-8})$. Finally, we provide
numerical evidence, confirming our theoretical findings.

</details>


### [606] [Large language model as user daily behavior data generator: balancing population diversity and individual personality](https://arxiv.org/pdf/2505.17615)
*Haoxin Li, Jingtao Ding, Jiahui Gong, Yong Li*

Main category: cs.LG

TL;DR: BehaviorGen uses LLMs to generate synthetic behavior data, improving behavior prediction models while addressing privacy concerns.


<details>
  <summary>Details</summary>
Motivation: Challenges in predicting human behavior due to data complexity and privacy issues with real user data.

Method: BehaviorGen framework leverages LLMs to simulate user behavior from profiles and real events, supporting data augmentation and replacement.

Result: Achieves up to 18.9% improvement in human mobility and smartphone usage predictions.

Conclusion: BehaviorGen offers a flexible, privacy-preserving solution for enhancing behavior modeling with synthetic data.

Abstract: Predicting human daily behavior is challenging due to the complexity of
routine patterns and short-term fluctuations. While data-driven models have
improved behavior prediction by leveraging empirical data from various
platforms and devices, the reliance on sensitive, large-scale user data raises
privacy concerns and limits data availability. Synthetic data generation has
emerged as a promising solution, though existing methods are often limited to
specific applications. In this work, we introduce BehaviorGen, a framework that
uses large language models (LLMs) to generate high-quality synthetic behavior
data. By simulating user behavior based on profiles and real events,
BehaviorGen supports data augmentation and replacement in behavior prediction
models. We evaluate its performance in scenarios such as pertaining
augmentation, fine-tuning replacement, and fine-tuning augmentation, achieving
significant improvements in human mobility and smartphone usage predictions,
with gains of up to 18.9%. Our results demonstrate the potential of BehaviorGen
to enhance user behavior modeling through flexible and privacy-preserving
synthetic data generation.

</details>


### [607] [Navigate the Unknown: Enhancing LLM Reasoning with Intrinsic Motivation Guided Exploration](https://arxiv.org/pdf/2505.17621)
*Jingtong Gao, Ling Pan, Yejing Wang, Rui Zhong, Chi Lu, Qingpeng Cai, Peng Jiang, Xiangyu Zhao*

Main category: cs.LG

TL;DR: i-MENTOR improves RL for LLMs by addressing sparse rewards and exploration issues, achieving a 22.39% boost on Countdown-4.


<details>
  <summary>Details</summary>
Motivation: Current RL methods like PPO and GRPO struggle with sparse rewards and poor exploration, hindering multi-step reasoning in LLMs.

Method: i-MENTOR introduces trajectory-aware rewards, dynamic scaling, and advantage-preserving rewards to enhance exploration and feedback.

Result: i-MENTOR shows a 22.39% improvement on the challenging Countdown-4 dataset.

Conclusion: i-MENTOR effectively addresses RL limitations for LLMs, enhancing reasoning performance.

Abstract: Reinforcement learning (RL) has emerged as a pivotal method for improving the
reasoning capabilities of Large Language Models (LLMs). However, prevalent RL
approaches such as Proximal Policy Optimization (PPO) and Group-Regularized
Policy Optimization (GRPO) face critical limitations due to their reliance on
sparse outcome-based rewards and inadequate mechanisms for incentivizing
exploration. These limitations result in inefficient guidance for multi-step
reasoning processes. Specifically, sparse reward signals fail to deliver
effective or sufficient feedback, particularly for challenging problems.
Furthermore, such reward structures induce systematic biases that prioritize
exploitation of familiar trajectories over novel solution discovery. These
shortcomings critically hinder performance in complex reasoning tasks, which
inherently demand iterative refinement across ipntermediate steps. To address
these challenges, we propose an Intrinsic Motivation guidEd exploratioN meThOd
foR LLM Reasoning (i-MENTOR), a novel method designed to both deliver dense
rewards and amplify explorations in the RL-based training paradigm. i-MENTOR
introduces three key innovations: trajectory-aware exploration rewards that
mitigate bias in token-level strategies while maintaining computational
efficiency; dynamic reward scaling to stabilize exploration and exploitation in
large action spaces; and advantage-preserving reward implementation that
maintains advantage distribution integrity while incorporating exploratory
guidance. Experiments across three public datasets demonstrate i-MENTOR's
effectiveness with a 22.39% improvement on the difficult dataset Countdown-4.

</details>


### [608] [Leveraging Stochastic Depth Training for Adaptive Inference](https://arxiv.org/pdf/2505.17626)
*Guilherme Korol, Antonio Carlos Schneider Beck, Jeronimo Castrillon*

Main category: cs.LG

TL;DR: The paper proposes a simpler, zero-overhead method for adaptive DNN inference using Stochastic Depth-trained models, improving power efficiency by up to 2X with minimal accuracy loss.


<details>
  <summary>Details</summary>
Motivation: Dynamic DNN optimization techniques like layer-skipping face challenges like increased memory, training complexity, and unpredictable performance-quality trade-offs.

Method: Leverages Stochastic Depth-trained models' resilience to arbitrary layer-skipping, selecting near Pareto-optimal configurations for runtime adaptation.

Result: Achieves up to 2X power efficiency improvement with accuracy drops as low as 0.71%.

Conclusion: The method offers a practical, efficient alternative to traditional dynamic DNN optimization, balancing performance and quality.

Abstract: Dynamic DNN optimization techniques such as layer-skipping offer increased
adaptability and efficiency gains but can lead to i) a larger memory footprint
as in decision gates, ii) increased training complexity (e.g., with
non-differentiable operations), and iii) less control over performance-quality
trade-offs due to its inherent input-dependent execution. To approach these
issues, we propose a simpler yet effective alternative for adaptive inference
with a zero-overhead, single-model, and time-predictable inference. Central to
our approach is the observation that models trained with Stochastic Depth -- a
method for faster training of residual networks -- become more resilient to
arbitrary layer-skipping at inference time. We propose a method to first select
near Pareto-optimal skipping configurations from a stochastically-trained model
to adapt the inference at runtime later. Compared to original ResNets, our
method shows improvements of up to 2X in power efficiency at accuracy drops as
low as 0.71%.

</details>


### [609] [Surfacing Semantic Orthogonality Across Model Safety Benchmarks: A Multi-Dimensional Analysis](https://arxiv.org/pdf/2505.17636)
*Jonathan Bennion, Shaona Ghosh, Mantek Singh, Nouha Dziri*

Main category: cs.LG

TL;DR: The paper evaluates five AI safety benchmarks, identifying six harm categories and highlighting differences in focus and prompt length. It proposes a quantitative framework for assessing semantic orthogonality to improve dataset development.


<details>
  <summary>Details</summary>
Motivation: To address the evolving interpretations of harm in AI and ensure comprehensive coverage in safety benchmarks.

Method: UMAP dimensionality reduction and k-means clustering (silhouette score: 0.470) were used to analyze semantic clusters in five open-source safety benchmarks.

Result: Six primary harm categories were identified, with varying representation across benchmarks. Differences in prompt length distribution and benchmark orthogonality were quantified.

Conclusion: The framework enables targeted development of datasets to address gaps in AI safety coverage, adapting to future definitions of harm.

Abstract: Various AI safety datasets have been developed to measure LLMs against
evolving interpretations of harm. Our evaluation of five recently published
open-source safety benchmarks reveals distinct semantic clusters using UMAP
dimensionality reduction and kmeans clustering (silhouette score: 0.470). We
identify six primary harm categories with varying benchmark representation.
GretelAI, for example, focuses heavily on privacy concerns, while WildGuardMix
emphasizes self-harm scenarios. Significant differences in prompt length
distribution suggests confounds to data collection and interpretations of harm
as well as offer possible context. Our analysis quantifies benchmark
orthogonality among AI benchmarks, allowing for transparency in coverage gaps
despite topical similarities. Our quantitative framework for analyzing semantic
orthogonality across safety benchmarks enables more targeted development of
datasets that comprehensively address the evolving landscape of harms in AI
use, however that is defined in the future.

</details>


### [610] [Causal Spatio-Temporal Prediction: An Effective and Efficient Multi-Modal Approach](https://arxiv.org/pdf/2505.17637)
*Yuting Huang, Ziquan Fang, Zhihao Zeng, Lu Chen, Yunjun Gao*

Main category: cs.LG

TL;DR: E^2-CSTP is a novel framework for spatio-temporal prediction that effectively integrates multi-modal data, addresses confounding factors, and reduces computational complexity, outperforming existing methods.


<details>
  <summary>Details</summary>
Motivation: The need for accurate spatio-temporal prediction in fields like transportation and urban planning, hindered by challenges in multi-modal fusion, confounding factors, and computational inefficiency.

Method: E^2-CSTP uses cross-modal attention, gating mechanisms, and a dual-branch causal inference approach (primary for prediction, auxiliary for bias mitigation). It also integrates GCN with Mamba for efficient encoding.

Result: Outperforms 9 state-of-the-art methods with up to 9.66% accuracy improvement and 17.37%-56.11% computational overhead reduction on 4 datasets.

Conclusion: E^2-CSTP effectively addresses key challenges in spatio-temporal prediction, offering both accuracy and efficiency improvements.

Abstract: Spatio-temporal prediction plays a crucial role in intelligent
transportation, weather forecasting, and urban planning. While integrating
multi-modal data has shown potential for enhancing prediction accuracy, key
challenges persist: (i) inadequate fusion of multi-modal information, (ii)
confounding factors that obscure causal relations, and (iii) high computational
complexity of prediction models. To address these challenges, we propose
E^2-CSTP, an Effective and Efficient Causal multi-modal Spatio-Temporal
Prediction framework. E^2-CSTP leverages cross-modal attention and gating
mechanisms to effectively integrate multi-modal data. Building on this, we
design a dual-branch causal inference approach: the primary branch focuses on
spatio-temporal prediction, while the auxiliary branch mitigates bias by
modeling additional modalities and applying causal interventions to uncover
true causal dependencies. To improve model efficiency, we integrate GCN with
the Mamba architecture for accelerated spatio-temporal encoding. Extensive
experiments on 4 real-world datasets show that E^2-CSTP significantly
outperforms 9 state-of-the-art methods, achieving up to 9.66% improvements in
accuracy as well as 17.37%-56.11% reductions in computational overhead.

</details>


### [611] [Why Diffusion Models Don't Memorize: The Role of Implicit Dynamical Regularization in Training](https://arxiv.org/pdf/2505.17638)
*Tony Bonnaire, Raphaël Urfin, Giulio Biroli, Marc Mézard*

Main category: cs.LG

TL;DR: The paper explores how training dynamics in diffusion models influence the transition from generalization to memorization, identifying two distinct timescales and revealing implicit regularization.


<details>
  <summary>Details</summary>
Motivation: Understanding the mechanisms preventing memorization and enabling generalization in diffusion models.

Method: Extensive experiments and theoretical analysis, including numerical experiments with U-Net architectures and a tractable random features model.

Result: Two timescales identified: early generalization (τ_gen) and later memorization (τ_mem), with τ_mem increasing linearly with training set size. Implicit dynamical regularization avoids memorization.

Conclusion: Training dynamics provide implicit regularization, allowing effective generalization even in overparameterized settings.

Abstract: Diffusion models have achieved remarkable success across a wide range of
generative tasks. A key challenge is understanding the mechanisms that prevent
their memorization of training data and allow generalization. In this work, we
investigate the role of the training dynamics in the transition from
generalization to memorization. Through extensive experiments and theoretical
analysis, we identify two distinct timescales: an early time
$\tau_\mathrm{gen}$ at which models begin to generate high-quality samples, and
a later time $\tau_\mathrm{mem}$ beyond which memorization emerges. Crucially,
we find that $\tau_\mathrm{mem}$ increases linearly with the training set size
$n$, while $\tau_\mathrm{gen}$ remains constant. This creates a growing window
of training times with $n$ where models generalize effectively, despite showing
strong memorization if training continues beyond it. It is only when $n$
becomes larger than a model-dependent threshold that overfitting disappears at
infinite training times. These findings reveal a form of implicit dynamical
regularization in the training dynamics, which allow to avoid memorization even
in highly overparameterized settings. Our results are supported by numerical
experiments with standard U-Net architectures on realistic and synthetic
datasets, and by a theoretical analysis using a tractable random features model
studied in the high-dimensional limit.

</details>


### [612] [PreMoe: Lightening MoEs on Constrained Memory by Expert Pruning and Retrieval](https://arxiv.org/pdf/2505.17639)
*Zehua Pei, Ying Zhang, Hui-Ling Zhen, Xianzhi Yu, Wulong Liu, Sinno Jialin Pan, Mingxuan Yuan, Bei Yu*

Main category: cs.LG

TL;DR: PreMoe reduces memory demands of large MoE models via task-specific expert pruning (PEP) and retrieval (TAER), maintaining high accuracy.


<details>
  <summary>Details</summary>
Motivation: Large MoE models face memory constraints in diverse environments, limiting deployment.

Method: Introduces PreMoe with PEP (probabilistic expert pruning) and TAER (task-adaptive expert retrieval) to identify and load only critical experts per task.

Result: Achieves high accuracy (e.g., 97.2% on MATH500) with significant expert reduction (50-87.5%).

Conclusion: PreMoe enables efficient deployment of massive MoE models in memory-constrained settings without sacrificing performance.

Abstract: Mixture-of-experts (MoE) architectures enable scaling large language models
(LLMs) to vast parameter counts without a proportional rise in computational
costs. However, the significant memory demands of large MoE models hinder their
deployment across various computational environments, from cloud servers to
consumer devices. This study first demonstrates pronounced task-specific
specialization in expert activation patterns within MoE layers. Building on
this, we introduce PreMoe, a novel framework that enables efficient deployment
of massive MoE models in memory-constrained environments. PreMoe features two
main components: probabilistic expert pruning (PEP) and task-adaptive expert
retrieval (TAER). PEP employs a new metric, the task-conditioned expected
selection score (TCESS), derived from router logits to quantify expert
importance for specific tasks, thereby identifying a minimal set of critical
experts. TAER leverages these task-specific expert importance profiles for
efficient inference. It pre-computes and stores compact expert patterns for
diverse tasks. When a user query is received, TAER rapidly identifies the most
relevant stored task pattern and reconstructs the model by loading only the
small subset of experts crucial for that task. This approach dramatically
reduces the memory footprint across all deployment scenarios. DeepSeek-R1 671B
maintains 97.2\% accuracy on MATH500 when pruned to 8/128 configuration (50\%
expert reduction), and still achieves 72.0\% with aggressive 8/32 pruning
(87.5\% expert reduction). Pangu-Ultra-MoE 718B achieves 97.15\% on MATH500 and
81.3\% on AIME24 with 8/128 pruning, while even more aggressive pruning to 4/64
(390GB memory) preserves 96.95\% accuracy on MATH500. We make our code publicly
available at https://github.com/JarvisPei/PreMoe.

</details>


### [613] [A Network Science Approach to Granular Time Series Segmentation](https://arxiv.org/pdf/2505.17640)
*Ivana Kesić, Carolina Fortuna, Mihael Mohorčič, Blaž Bertalanič*

Main category: cs.LG

TL;DR: A novel TSS method using WDPVG and GAT outperforms baselines with high F1 scores and reduced training data.


<details>
  <summary>Details</summary>
Motivation: Existing deep learning TSS methods are limited by fixed window sizes, prompting a need for more granular approaches.

Method: Transforms TS into graphs via WDPVG and uses GAT for node classification to identify segments.

Result: Achieves 0.97 F1 score, outperforms seq2point by 0.05, and reduces training data needs.

Conclusion: The method effectively captures hidden TS structures, offering a robust solution for TSS.

Abstract: Time series segmentation (TSS) is one of the time series (TS) analysis
techniques, that has received considerably less attention compared to other TS
related tasks. In recent years, deep learning architectures have been
introduced for TSS, however their reliance on sliding windows limits
segmentation granularity due to fixed window sizes and strides. To overcome
these challenges, we propose a new more granular TSS approach that utilizes the
Weighted Dual Perspective Visbility Graph (WDPVG) TS into a graph and combines
it with a Graph Attention Network (GAT). By transforming TS into graphs, we are
able to capture different structural aspects of the data that would otherwise
remain hidden. By utilizing the representation learning capabilities of Graph
Neural Networks, our method is able to effectively identify meaningful segments
within the TS. To better understand the potential of our approach, we also
experimented with different TS-to-graph transformations and compared their
performance. Our contributions include: a) formulating the TSS as a node
classification problem on graphs; b) conducting an extensive analysis of
various TS- to-graph transformations applied to TSS using benchmark datasets
from the TSSB repository; c) providing the first detailed study on utilizing
GNNs for analyzing graph representations of TS in the context of TSS; d)
demonstrating the effectiveness of our method, which achieves an average F1
score of 0.97 across 59 diverse TSS benchmark datasets; e) outperforming the
seq2point baseline method by 0.05 in terms of F1 score; and f) reducing the
required training data compared to the baseline methods.

</details>


### [614] [Understanding Pre-training and Fine-tuning from Loss Landscape Perspectives](https://arxiv.org/pdf/2505.17646)
*Huanran Chen, Yinpeng Dong, Zeming Wei, Yao Huang, Yichi Zhang, Hang Su, Jun Zhu*

Main category: cs.LG

TL;DR: The paper explores the loss landscape of large language models, identifying 'basic capability' and 'specific capability' basins formed by pre-training and fine-tuning, respectively. It examines most-case and worst-case landscapes, showing that staying within these basins preserves model capabilities. The study also links basin size to robustness and demonstrates how over-parameterization can expand basins.


<details>
  <summary>Details</summary>
Motivation: To understand the structure and implications of the loss landscape in large language models, particularly how pre-training and fine-tuning shape model capabilities and robustness.

Method: The study investigates the loss landscape by analyzing most-case and worst-case scenarios, theoretically linking basin sizes to robustness, and leveraging over-parameterization to expand basins.

Result: Pre-training creates a 'basic capability' basin, while fine-tuning forms 'specific capability' basins. Staying within these basins preserves capabilities, and basin size correlates with robustness. Over-parameterization allows basin expansion.

Conclusion: The loss landscape's basin structure is crucial for maintaining model capabilities. Understanding and manipulating basin sizes can enhance robustness and performance in large language models.

Abstract: Recent studies have revealed that the loss landscape of large language models
resembles a basin, within which the models perform nearly identically, and
outside of which they lose all their capabilities. In this work, we conduct
further studies on the loss landscape of large language models. We discover
that pre-training creates a "basic capability" basin, and subsequent
fine-tuning creates "specific capability" basins (e.g., math, safety, coding)
within the basic capability basin. We further investigate two types of loss
landscapes: the most-case landscape (i.e., the landscape along most directions)
and the worst-case landscape (i.e., the landscape along the worst direction).
We argue that as long as benign fine-tuning remains within the most-case basin,
it will not compromise previous capabilities. Similarly, any fine-tuning
(including the adversarial one) that stays within the worst-case basin would
not compromise previous capabilities. Finally, we theoretically demonstrate
that the size of the most-case basin can bound the size of the worst-case basin
and the robustness with respect to input perturbations. We also show that, due
to the over-parameterization property of current large language models, one can
easily enlarge the basins by five times.

</details>


### [615] [Rethinking the Sampling Criteria in Reinforcement Learning for LLM Reasoning: A Competence-Difficulty Alignment Perspective](https://arxiv.org/pdf/2505.17652)
*Deyang Kong, Qi Guo, Xiangyu Xi, Wei Wang, Jingang Wang, Xunliang Cai, Shikun Zhang, Wei Ye*

Main category: cs.LG

TL;DR: CDAS improves RL efficiency for language models by aligning problem difficulty with model competence, outperforming baselines in accuracy and speed.


<details>
  <summary>Details</summary>
Motivation: Existing methods for improving RL efficiency in language models suffer from unstable difficulty estimations and misalignment with model competence, leading to suboptimal results.

Method: CDAS introduces accurate problem difficulty estimation via historical performance discrepancies and aligns it with model competence using a fixed-point system.

Result: CDAS achieves higher accuracy and efficiency, outperforming baselines and being 2.33 times faster than Dynamic Sampling.

Conclusion: CDAS effectively addresses the limitations of existing methods, enhancing RL training for language models through competence-difficulty alignment.

Abstract: Reinforcement learning exhibits potential in enhancing the reasoning
abilities of large language models, yet it is hard to scale for the low sample
efficiency during the rollout phase. Existing methods attempt to improve
efficiency by scheduling problems based on problem difficulties. However, these
approaches suffer from unstable and biased estimations of problem difficulty
and fail to capture the alignment between model competence and problem
difficulty in RL training, leading to suboptimal results. To tackle these
limitations, this paper introduces \textbf{C}ompetence-\textbf{D}ifficulty
\textbf{A}lignment \textbf{S}ampling (\textbf{CDAS}), which enables accurate
and stable estimation of problem difficulties by aggregating historical
performance discrepancies of problems. Then the model competence is quantified
to adaptively select problems whose difficulty is in alignment with the model's
current competence using a fixed-point system. Experimental results across a
range of challenging mathematical benchmarks show that CDAS achieves great
improvements in both accuracy and efficiency. CDAS attains the highest average
accuracy against baselines and exhibits significant speed advantages compared
to Dynamic Sampling, a competitive strategy in DAPO, which is \textbf{2.33}
times slower than CDAS.

</details>


### [616] [DAM-GT: Dual Positional Encoding-Based Attention Masking Graph Transformer for Node Classification](https://arxiv.org/pdf/2505.17660)
*Chenyang Li, Jinsong Chen, John E. Hopcroft, Kun He*

Main category: cs.LG

TL;DR: DAM-GT, a novel graph Transformer, addresses limitations in neighborhood token generation and self-attention by introducing dual positional encoding and a masking strategy, outperforming state-of-the-art methods in node classification.


<details>
  <summary>Details</summary>
Motivation: Existing neighborhood-aware tokenized graph Transformers fail to capture attribute correlations and suffer from attention diversion, disrupting node-neighborhood interactions.

Method: DAM-GT uses dual positional encoding (attribute-aware and topological) and a masking strategy to guide attention between target nodes and neighborhood tokens.

Result: DAM-GT consistently outperforms state-of-the-art methods across various graphs with different homophily levels and scales.

Conclusion: DAM-GT effectively addresses the limitations of current methods, improving node classification performance through better attribute correlation capture and attention guidance.

Abstract: Neighborhood-aware tokenized graph Transformers have recently shown great
potential for node classification tasks. Despite their effectiveness, our
in-depth analysis of neighborhood tokens reveals two critical limitations in
the existing paradigm. First, current neighborhood token generation methods
fail to adequately capture attribute correlations within a neighborhood.
Second, the conventional self-attention mechanism suffers from attention
diversion when processing neighborhood tokens, where high-hop neighborhoods
receive disproportionate focus, severely disrupting information interactions
between the target node and its neighborhood tokens. To address these
challenges, we propose DAM-GT, Dual positional encoding-based Attention Masking
graph Transformer. DAM-GT introduces a novel dual positional encoding scheme
that incorporates attribute-aware encoding via an attribute clustering
strategy, effectively preserving node correlations in both topological and
attribute spaces. In addition, DAM-GT formulates a new attention mechanism with
a simple yet effective masking strategy to guide interactions between target
nodes and their neighborhood tokens, overcoming the issue of attention
diversion. Extensive experiments on various graphs with different homophily
levels as well as different scales demonstrate that DAM-GT consistently
outperforms state-of-the-art methods in node classification tasks.

</details>


### [617] [Automated scientific minimization of regret](https://arxiv.org/pdf/2505.17661)
*Marcel Binz, Akshay K. Jagadish, Milena Rmus, Eric Schulz*

Main category: cs.LG

TL;DR: ASMR automates cognitive science by identifying gaps in models using Centaur and revising them with language-based reasoning, achieving noise-ceiling predictions while maintaining interpretability.


<details>
  <summary>Details</summary>
Motivation: To automate cognitive modeling by minimizing scientific regret and improving interpretability.

Method: Uses Centaur (a cognitive foundation model) to identify gaps in cognitive models and revises them with a language-based reasoning model.

Result: ASMR produces models that predict human behavior at noise ceiling while remaining interpretable.

Conclusion: ASMR can automate key parts of cognitive modeling, enhancing efficiency and interpretability.

Abstract: We introduce automated scientific minimization of regret (ASMR) -- a
framework for automated computational cognitive science. Building on the
principles of scientific regret minimization, ASMR leverages Centaur -- a
recently proposed foundation model of human cognition -- to identify gaps in an
interpretable cognitive model. These gaps are then addressed through automated
revisions generated by a language-based reasoning model. We demonstrate the
utility of this approach in a multi-attribute decision-making task, showing
that ASMR discovers cognitive models that predict human behavior at noise
ceiling while retaining interpretability. Taken together, our results highlight
the potential of ASMR to automate core components of the cognitive modeling
pipeline.

</details>


### [618] [Automating Versatile Time-Series Analysis with Tiny Transformers on Embedded FPGAs](https://arxiv.org/pdf/2505.17662)
*Tianheng Ling, Chao Qian, Lukas Johannes Haßler, Gregor Schiele*

Main category: cs.LG

TL;DR: A unified, automated framework for deploying Tiny Transformers on embedded FPGAs, supporting 4-bit quantization and achieving low energy consumption and latency.


<details>
  <summary>Details</summary>
Motivation: Addressing the challenge of deploying Transformer models on resource-constrained devices, particularly for time-series tasks, by leveraging FPGA flexibility and automation.

Method: Combines quantization-aware training (down to 4 bits), hardware-aware hyperparameter search, and automatic VHDL generation for deployment on embedded FPGAs.

Result: Achieves as low as 0.033 mJ per inference with millisecond latency on AMD Spartan-7, with insights for Lattice iCE40.

Conclusion: The framework enables efficient, task-specific Transformer deployment on FPGAs, balancing performance and resource constraints.

Abstract: Transformer-based models have shown strong performance across diverse
time-series tasks, but their deployment on resource-constrained devices remains
challenging due to high memory and computational demand. While prior work
targeting Microcontroller Units (MCUs) has explored hardware-specific
optimizations, such approaches are often task-specific and limited to 8-bit
fixed-point precision. Field-Programmable Gate Arrays (FPGAs) offer greater
flexibility, enabling fine-grained control over data precision and
architecture. However, existing FPGA-based deployments of Transformers for
time-series analysis typically focus on high-density platforms with manual
configuration. This paper presents a unified and fully automated deployment
framework for Tiny Transformers on embedded FPGAs. Our framework supports a
compact encoder-only Transformer architecture across three representative
time-series tasks (forecasting, classification, and anomaly detection). It
combines quantization-aware training (down to 4 bits), hardware-aware
hyperparameter search using Optuna, and automatic VHDL generation for seamless
deployment. We evaluate our framework on six public datasets across two
embedded FPGA platforms. Results show that our framework produces integer-only,
task-specific Transformer accelerators achieving as low as 0.033 mJ per
inference with millisecond latency on AMD Spartan-7, while also providing
insights into deployment feasibility on Lattice iCE40. All source code will be
released in the GitHub repository
(https://github.com/Edwina1030/TinyTransformer4TS).

</details>


### [619] [What is the role of memorization in Continual Learning?](https://arxiv.org/pdf/2505.17664)
*Jędrzej Kozal, Jan Wasilewski, Alif Ashrafee, Bartosz Krawczyk, Michał Woźniak*

Main category: cs.LG

TL;DR: The paper explores memorization's role in incremental learning, showing high-memorization samples are forgotten faster but are crucial for peak performance. It introduces a memorization proxy for buffer policy optimization.


<details>
  <summary>Details</summary>
Motivation: To understand memorization's impact on continual learning, distinguishing it from forgetting prevention, and to leverage memorization for better incremental training.

Method: Extensive experiments evaluating memorization's effect, introducing a memorization proxy, and applying it to buffer policy in incremental learning.

Result: High-memorization samples are forgotten faster but essential for top performance; their importance grows with buffer size.

Conclusion: Memorization is vital for performance in continual learning, especially with larger buffers, and can be optimized using a memorization proxy.

Abstract: Memorization impacts the performance of deep learning algorithms. Prior works
have studied memorization primarily in the context of generalization and
privacy. This work studies the memorization effect on incremental learning
scenarios. Forgetting prevention and memorization seem similar. However, one
should discuss their differences. We designed extensive experiments to evaluate
the impact of memorization on continual learning. We clarified that learning
examples with high memorization scores are forgotten faster than regular
samples. Our findings also indicated that memorization is necessary to achieve
the highest performance. However, at low memory regimes, forgetting regular
samples is more important. We showed that the importance of a high-memorization
score sample rises with an increase in the buffer size. We introduced a
memorization proxy and employed it in the buffer policy problem to showcase how
memorization could be used during incremental training. We demonstrated that
including samples with a higher proxy memorization score is beneficial when the
buffer size is large.

</details>


### [620] [Towards General Continuous Memory for Vision-Language Models](https://arxiv.org/pdf/2505.17670)
*Wenyi Wu, Zixuan Song, Kun Zhou, Yifei Shao, Zhiting Hu, Biwei Huang*

Main category: cs.LG

TL;DR: The paper proposes CoMEM, a continuous memory system for VLMs to improve multimodal reasoning by using dense embeddings instead of long token sequences, enhancing efficiency and performance.


<details>
  <summary>Details</summary>
Motivation: Existing methods for VLMs struggle with complex reasoning tasks due to inefficient memory systems that increase context length and degrade performance.

Method: The authors introduce continuous memory, a compact set of dense embeddings, and fine-tune the VLM into a memory encoder using minimal parameters (1.2%) and a small dataset (15.6K samples).

Result: CoMEM improves performance on complex multimodal reasoning tasks, encoding knowledge into just 8 embeddings while keeping the VLM frozen during inference.

Conclusion: The proposed continuous memory system is plug-and-play, efficient, and effective, as demonstrated by experiments across eight benchmarks.

Abstract: Language models (LMs) and their extension, vision-language models (VLMs),
have achieved remarkable performance across various tasks. However, they still
struggle with complex reasoning tasks that require multimodal or multilingual
real-world knowledge. To support such capabilities, an external memory system
that can efficiently provide relevant multimodal information is essential.
Existing approaches generally concatenate image and text tokens into a long
sequence as memory, which, however, may drastically increase context length and
even degrade performance. In contrast, we propose using continuous memory, a
compact set of dense embeddings to more effectively and efficiently represent
multimodal and multilingual knowledge. Our key insight is that a VLM can serve
as its own continuous memory encoder. We empirically show that this design
improves performance on complex multimodal reasoning tasks. Building on this,
we introduce a data-efficient and parameter-efficient method to fine-tune the
VLM into a memory encoder, requiring only 1.2% of the model's parameters and a
small corpus of 15.6K self-synthesized samples. Our approach CoMEM utilizes
VLM's original capabilities to encode arbitrary multimodal and multilingual
knowledge into just 8 continuous embeddings. Since the inference-time VLM
remains frozen, our memory module is plug-and-play and can be flexibly
integrated as needed. Extensive experiments across eight multimodal reasoning
benchmarks demonstrate the effectiveness of our approach.

</details>


### [621] [FlashForge: Ultra-Efficient Prefix-Aware Attention for LLM Decoding](https://arxiv.org/pdf/2505.17694)
*Zhibin Wang, Rui Ning, Chao Fang, Zhonghui Zhang, Xi Lin, Shaobo Ma, Mo Zhou, Xue Li, Zhongfeng Wang, Chengying Huan, Rong Gu, Kun Yang, Guihai Chen, Sheng Zhong, Chen Tian*

Main category: cs.LG

TL;DR: FlashForge optimizes attention computation in decode stage by leveraging prefix-sharing, achieving significant speedup and memory access reduction.


<details>
  <summary>Details</summary>
Motivation: Prefix-sharing in attention computation is memory-intensive and inefficient due to shared KV cache access patterns and workload imbalances.

Method: Proposes FlashForge, a dedicated attention kernel with shared-prefix optimization and workload balancing.

Result: Achieves 1.9x speedup, 120.9x memory access reduction, and 3.8x faster end-to-end time per token.

Conclusion: FlashForge effectively addresses challenges in prefix-sharing attention computation, improving efficiency and performance.

Abstract: Prefix-sharing among multiple prompts presents opportunities to combine the
operations of the shared prefix, while attention computation in the decode
stage, which becomes a critical bottleneck with increasing context lengths, is
a memory-intensive process requiring heavy memory access on the key-value (KV)
cache of the prefixes. Therefore, in this paper, we explore the potential of
prefix-sharing in the attention computation of the decode stage. However, the
tree structure of the prefix-sharing mechanism presents significant challenges
for attention computation in efficiently processing shared KV cache access
patterns while managing complex dependencies and balancing irregular workloads.
To address the above challenges, we propose a dedicated attention kernel to
combine the memory access of shared prefixes in the decoding stage, namely
FlashForge. FlashForge delivers two key innovations: a novel shared-prefix
attention kernel that optimizes memory hierarchy and exploits both intra-block
and inter-block parallelism, and a comprehensive workload balancing mechanism
that efficiently estimates cost, divides tasks, and schedules execution.
Experimental results show that FlashForge achieves an average 1.9x speedup and
120.9x memory access reduction compared to the state-of-the-art FlashDecoding
kernel regarding attention computation in the decode stage and 3.8x end-to-end
time per output token compared to the vLLM.

</details>


### [622] [SynRES: Towards Referring Expression Segmentation in the Wild via Synthetic Data](https://arxiv.org/pdf/2505.17695)
*Dong-Hee Kim, Hyunjee Song, Donghyun Kim*

Main category: cs.LG

TL;DR: WildRES introduces a more complex benchmark for RES, revealing current models' limitations. SynRES, a synthetic data pipeline, improves performance by 2.0-3.8%.


<details>
  <summary>Details</summary>
Motivation: Current RES benchmarks are limited in evaluating complex reasoning. WildRES addresses this with diverse, long queries and multiple targets.

Method: SynRES generates synthetic training data via dense captioning, semantic alignment, and domain-aware augmentations.

Result: Models trained with SynRES improve gIoU by 2.0% (WildRES-ID) and 3.8% (WildRES-DS).

Conclusion: SynRES enhances RES model performance, addressing limitations of current benchmarks.

Abstract: Despite the advances in Referring Expression Segmentation (RES) benchmarks,
their evaluation protocols remain constrained, primarily focusing on either
single targets with short queries (containing minimal attributes) or multiple
targets from distinctly different queries on a single domain. This limitation
significantly hinders the assessment of more complex reasoning capabilities in
RES models. We introduce WildRES, a novel benchmark that incorporates long
queries with diverse attributes and non-distinctive queries for multiple
targets. This benchmark spans diverse application domains, including autonomous
driving environments and robotic manipulation scenarios, thus enabling more
rigorous evaluation of complex reasoning capabilities in real-world settings.
Our analysis reveals that current RES models demonstrate substantial
performance deterioration when evaluated on WildRES. To address this challenge,
we introduce SynRES, an automated pipeline generating densely paired
compositional synthetic training data through three innovations: (1) a dense
caption-driven synthesis for attribute-rich image-mask-expression triplets, (2)
reliable semantic alignment mechanisms rectifying caption-pseudo mask
inconsistencies via Image-Text Aligned Grouping, and (3) domain-aware
augmentations incorporating mosaic composition and superclass replacement to
emphasize generalization ability and distinguishing attributes over object
categories. Experimental results demonstrate that models trained with SynRES
achieve state-of-the-art performance, improving gIoU by 2.0% on WildRES-ID and
3.8% on WildRES-DS. Code and datasets are available at
https://github.com/UTLLab/SynRES.

</details>


### [623] [COUNTDOWN: Contextually Sparse Activation Filtering Out Unnecessary Weights in Down Projection](https://arxiv.org/pdf/2505.17701)
*Jaewon Cheon, Pilsung Kang*

Main category: cs.LG

TL;DR: The paper introduces sparse activation methods (M-COUNTDOWN and D-COUNTDOWN) to reduce computational costs in large language models by selectively deactivating non-essential parameters during inference.


<details>
  <summary>Details</summary>
Motivation: Addressing computational inefficiencies in large language models by focusing on the sparsity of FFNN layers as a linear combination.

Method: Proposes two methods: M-COUNTDOWN (indirect coefficients) and D-COUNTDOWN (direct coefficients) to exploit sparsity in FFNN layers.

Result: D-COUNTDOWN reduces computations by 90% with minimal performance loss (5.5%), while M-COUNTDOWN improves performance preservation by 29.4% over existing methods.

Conclusion: The methods effectively reduce computational costs while maintaining performance, with specialized kernels enabling real-world acceleration.

Abstract: The growing size of large language models has created significant
computational inefficiencies. To address this challenge, sparse activation
methods selectively deactivates non-essential parameters during inference,
reducing computational costs in FFNN layers. While existing methods focus on
non-linear gating mechanisms, we hypothesize that the sparsity of the FFNN
layer lies globally in the form of a linear combination over its internal down
projection matrix. Based on this insight, we propose two methods: M-COUNTDOWN,
leveraging indirect coefficients, and D-COUNTDOWN, utilizing direct
coefficients of the linear combination. Experimental results demonstrate that
D-COUNTDOWN can omit 90% of computations with performance loss as low as 5.5%
ideally, while M-COUNTDOWN provides a predictor-free solution with up to 29.4%
better performance preservation compared to existing methods. Our specialized
kernel implementations effectively realize these theoretical gains into
substantial real-world acceleration.

</details>


### [624] [The Third Pillar of Causal Analysis? A Measurement Perspective on Causal Representations](https://arxiv.org/pdf/2505.17708)
*Dingling Yao, Shimeng Huang, Riccardo Cadei, Kun Zhang, Francesco Locatello*

Main category: cs.LG

TL;DR: The paper reinterprets causal representation learning (CRL) using a measurement model framework, introducing the T-MEX score to assess representation quality for causal tasks.


<details>
  <summary>Details</summary>
Motivation: Challenges in causal reasoning due to complex, noisy, high-dimensional data and unclear utility of learned representations for downstream tasks.

Method: Proposes a measurement model framework for CRL, viewing representations as proxy measurements of latent variables, and introduces the T-MEX score for evaluation.

Result: Validated T-MEX in simulations and real-world scenarios, showing its effectiveness in assessing representation quality for causal tasks.

Conclusion: The framework and T-MEX score provide a principled way to evaluate learned representations for causal reasoning.

Abstract: Causal reasoning and discovery, two fundamental tasks of causal analysis,
often face challenges in applications due to the complexity, noisiness, and
high-dimensionality of real-world data. Despite recent progress in identifying
latent causal structures using causal representation learning (CRL), what makes
learned representations useful for causal downstream tasks and how to evaluate
them are still not well understood. In this paper, we reinterpret CRL using a
measurement model framework, where the learned representations are viewed as
proxy measurements of the latent causal variables. Our approach clarifies the
conditions under which learned representations support downstream causal
reasoning and provides a principled basis for quantitatively assessing the
quality of representations using a new Test-based Measurement EXclusivity
(T-MEX) score. We validate T-MEX across diverse causal inference scenarios,
including numerical simulations and real-world ecological video analysis,
demonstrating that the proposed framework and corresponding score effectively
assess the identification of learned representations and their usefulness for
causal downstream tasks.

</details>


### [625] [PPO-BR: Dual-Signal Entropy-Reward Adaptation for Trust Region Policy Optimization](https://arxiv.org/pdf/2505.17714)
*Ben Rahman*

Main category: cs.LG

TL;DR: PPO-BR introduces an adaptive trust region for RL, combining exploration and convergence signals, outperforming baselines with minimal overhead.


<details>
  <summary>Details</summary>
Motivation: Addresses the brittle trade-off in PPO's static trust region, which hinders exploration and destabilizes convergence.

Method: PPO-BR uses entropy-driven expansion for exploration and reward-guided contraction for stability, with a single bounded trust region.

Result: Achieves 29.1% faster convergence, 2.3x lower reward variance, and <1.8% runtime overhead.

Conclusion: PPO-BR's simplicity and theoretical guarantees make it suitable for safety-critical applications like robotic surgery and autonomous drones.

Abstract: Despite Proximal Policy Optimization (PPO) dominating policy gradient methods
-- from robotic control to game AI -- its static trust region forces a brittle
trade-off: aggressive clipping stifles early exploration, while late-stage
updates destabilize convergence. PPO-BR establishes a new paradigm in adaptive
RL by fusing exploration and convergence signals into a single bounded trust
region -- a theoretically grounded innovation that outperforms five SOTA
baselines with less than 2% overhead. This work bridges a critical gap in
phase-aware learning, enabling real-world deployment in safety-critical systems
like robotic surgery within a single adaptive mechanism. PPO-BR achieves 29.1%
faster convergence by combining: (1) entropy-driven expansion (epsilon up) for
exploration in high-uncertainty states, and (2) reward-guided contraction
(epsilon down) for convergence stability. On six diverse benchmarks (MuJoCo,
Atari, sparse-reward), PPO-BR achieves 29.1% faster convergence (p < 0.001),
2.3x lower reward variance than PPO, and less than 1.8% runtime overhead with
only five lines of code change. PPO-BR's simplicity and theoretical guarantees
make it ready-to-deploy in safety-critical domains -- from surgical robotics to
autonomous drones. In contrast to recent methods such as Group Relative Policy
Optimization (GRPO), PPO-BR offers a unified entropy-reward mechanism
applicable to both language models and general reinforcement learning
environments.

</details>


### [626] [PEAR: Equal Area Weather Forecasting on the Sphere](https://arxiv.org/pdf/2505.17720)
*Hampus Linander, Christoffer Petersson, Daniel Persson, Jan E. Gerken*

Main category: cs.LG

TL;DR: PEAR is a transformer-based weather forecasting model using HEALPix grid, outperforming traditional Driscoll-Healy models without extra computational cost.


<details>
  <summary>Details</summary>
Motivation: Address biases in Driscoll-Healy grid by leveraging HEALPix's equal-area pixels, gaining traction in meteorology.

Method: Introduces PEAR, a deep learning model operating natively on HEALPix grid for weather forecasting.

Result: PEAR outperforms Driscoll-Healy-based models without additional computational overhead.

Conclusion: PEAR demonstrates the viability and superiority of HEALPix-based deep learning for weather forecasting.

Abstract: Machine learning methods for global medium-range weather forecasting have
recently received immense attention. Following the publication of the Pangu
Weather model, the first deep learning model to outperform traditional
numerical simulations of the atmosphere, numerous models have been published in
this domain, building on Pangu's success. However, all of these models operate
on input data and produce predictions on the Driscoll--Healy discretization of
the sphere which suffers from a much finer grid at the poles than around the
equator. In contrast, in the Hierarchical Equal Area iso-Latitude Pixelization
(HEALPix) of the sphere, each pixel covers the same surface area, removing
unphysical biases. Motivated by a growing support for this grid in meteorology
and climate sciences, we propose to perform weather forecasting with deep
learning models which natively operate on the HEALPix grid. To this end, we
introduce Pangu Equal ARea (PEAR), a transformer-based weather forecasting
model which operates directly on HEALPix-features and outperforms the
corresponding model on Driscoll--Healy without any computational overhead.

</details>


### [627] [Redirection for Erasing Memory (REM): Towards a universal unlearning method for corrupted data](https://arxiv.org/pdf/2505.17730)
*Stefan Schoepf, Michael Curtis Mozer, Nicole Elyse Mitchell, Alexandra Brintrup, Georgios Kaissis, Peter Kairouz, Eleni Triantafillou*

Main category: cs.LG

TL;DR: The paper proposes a conceptual space for comparing machine unlearning tasks in vision classifiers, introduces a new method (REM) that outperforms prior methods across diverse tasks.


<details>
  <summary>Details</summary>
Motivation: The lack of systematic comparison for specialized unlearning methods in vision classifiers motivated the creation of a conceptual framework.

Method: A two-dimensional space (discovery rate and statistical regularity) characterizes tasks. The REM method redirects corrupted data to dedicated neurons for erasure.

Result: REM performs strongly across all task regions, unlike prior methods that fail outside their design scope.

Conclusion: REM provides a robust solution for diverse unlearning tasks, addressing limitations of specialized methods.

Abstract: Machine unlearning is studied for a multitude of tasks, but specialization of
unlearning methods to particular tasks has made their systematic comparison
challenging. To address this issue, we propose a conceptual space to
characterize diverse corrupted data unlearning tasks in vision classifiers.
This space is described by two dimensions, the discovery rate (the fraction of
the corrupted data that are known at unlearning time) and the statistical
regularity of the corrupted data (from random exemplars to shared concepts).
Methods proposed previously have been targeted at portions of this space and-we
show-fail predictably outside these regions. We propose a novel method,
Redirection for Erasing Memory (REM), whose key feature is that corrupted data
are redirected to dedicated neurons introduced at unlearning time and then
discarded or deactivated to suppress the influence of corrupted data. REM
performs strongly across the space of tasks, in contrast to prior SOTA methods
that fail outside the regions for which they were designed.

</details>


### [628] [URB -- Urban Routing Benchmark for RL-equipped Connected Autonomous Vehicles](https://arxiv.org/pdf/2505.17734)
*Ahmet Onur Akman, Anastasia Psarou, Michał Hoffmann, Łukasz Gorczyca, Łukasz Kowalski, Paweł Gora, Grzegorz Jamróz, Rafał Kucharski*

Main category: cs.LG

TL;DR: The paper introduces a benchmarking environment for evaluating multi-agent RL algorithms in urban routing for CAVs, revealing current limitations in outperforming human drivers.


<details>
  <summary>Details</summary>
Motivation: To address the lack of standardized benchmarks for developing collective routing strategies for CAVs using RL.

Method: Developed a benchmarking environment (Urban Routing Benchmark) with real-world traffic networks, demand patterns, predefined tasks, MARL algorithms, baselines, and performance metrics.

Result: State-of-the-art MARL algorithms rarely outperformed human drivers, highlighting scalability challenges.

Conclusion: The study underscores the need for advancements in MARL for large-scale urban routing optimization and establishes a leaderboard for future research.

Abstract: Connected Autonomous Vehicles (CAVs) promise to reduce congestion in future
urban networks, potentially by optimizing their routing decisions. Unlike for
human drivers, these decisions can be made with collective, data-driven
policies, developed by machine learning algorithms. Reinforcement learning (RL)
can facilitate the development of such collective routing strategies, yet
standardized and realistic benchmarks are missing. To that end, we present
\our{}: Urban Routing Benchmark for RL-equipped Connected Autonomous Vehicles.
\our{} is a comprehensive benchmarking environment that unifies evaluation
across 29 real-world traffic networks paired with realistic demand patterns.
\our{} comes with a catalog of predefined tasks, four state-of-the-art
multi-agent RL (MARL) algorithm implementations, three baseline methods,
domain-specific performance metrics, and a modular configuration scheme. Our
results suggest that, despite the lengthy and costly training, state-of-the-art
MARL algorithms rarely outperformed humans. Experimental results reported in
this paper initiate the first leaderboard for MARL in large-scale urban routing
optimization and reveal that current approaches struggle to scale, emphasizing
the urgent need for advancements in this domain.

</details>


### [629] [A tensor network approach for chaotic time series prediction](https://arxiv.org/pdf/2505.17740)
*Rodrigo Martínez-Peña, Román Orús*

Main category: cs.LG

TL;DR: The paper explores using tensor networks to improve chaotic time series prediction, offering better accuracy and efficiency than traditional reservoir computing methods.


<details>
  <summary>Details</summary>
Motivation: Chaotic time series prediction is challenging, and while reservoir computing is effective, optimizing its architectures remains difficult. Tensor networks address this by reducing dimensionality.

Method: The study employs a tensor network model, decomposing multidimensional arrays to mitigate exponential parameter growth, and compares it to conventional echo state networks.

Result: The tensor network approach outperforms traditional methods in accuracy and computational efficiency for chaotic time series prediction.

Conclusion: Tensor networks bridge reservoir computing and tensor network communities, advancing both fields while solving key challenges in chaotic time series prediction.

Abstract: Making accurate predictions of chaotic time series is a complex challenge.
Reservoir computing, a neuromorphic-inspired approach, has emerged as a
powerful tool for this task. It exploits the memory and nonlinearity of
dynamical systems without requiring extensive parameter tuning. However,
selecting and optimizing reservoir architectures remains an open problem.
Next-generation reservoir computing simplifies this problem by employing
nonlinear vector autoregression based on truncated Volterra series, thereby
reducing hyperparameter complexity. Nevertheless, the latter suffers from
exponential parameter growth in terms of the maximum monomial degree. Tensor
networks offer a promising solution to this issue by decomposing
multidimensional arrays into low-dimensional structures, thus mitigating the
curse of dimensionality. This paper explores the application of a previously
proposed tensor network model for predicting chaotic time series, demonstrating
its advantages in terms of accuracy and computational efficiency compared to
conventional echo state networks. Using a state-of-the-art tensor network
approach enables us to bridge the gap between the tensor network and reservoir
computing communities, fostering advances in both fields.

</details>


### [630] [Discrete Neural Flow Samplers with Locally Equivariant Transformer](https://arxiv.org/pdf/2505.17741)
*Zijing Ou, Ruixiang Zhang, Yingzhen Li*

Main category: cs.LG

TL;DR: DNFS is a trainable framework for efficient discrete sampling, using continuous-time Markov chains and control variates for variance reduction, with a novel Transformer parameterisation for computational efficiency.


<details>
  <summary>Details</summary>
Motivation: Traditional methods like Markov chain Monte Carlo suffer from slow mixing and poor convergence for sampling unnormalised discrete distributions.

Method: DNFS learns a rate matrix for a continuous-time Markov chain, employs control variates for variance reduction, and uses a locally equivariant Transformer for efficient parameterisation.

Result: DNFS shows efficacy in sampling unnormalised distributions, training discrete energy-based models, and solving combinatorial optimisation problems.

Conclusion: DNFS provides an efficient and scalable solution for discrete sampling, outperforming traditional methods in various applications.

Abstract: Sampling from unnormalised discrete distributions is a fundamental problem
across various domains. While Markov chain Monte Carlo offers a principled
approach, it often suffers from slow mixing and poor convergence. In this
paper, we propose Discrete Neural Flow Samplers (DNFS), a trainable and
efficient framework for discrete sampling. DNFS learns the rate matrix of a
continuous-time Markov chain such that the resulting dynamics satisfy the
Kolmogorov equation. As this objective involves the intractable partition
function, we then employ control variates to reduce the variance of its Monte
Carlo estimation, leading to a coordinate descent learning algorithm. To
further facilitate computational efficiency, we propose locally equivaraint
Transformer, a novel parameterisation of the rate matrix that significantly
improves training efficiency while preserving powerful network expressiveness.
Empirically, we demonstrate the efficacy of DNFS in a wide range of
applications, including sampling from unnormalised distributions, training
discrete energy-based models, and solving combinatorial optimisation problems.

</details>


### [631] [MetaBox-v2: A Unified Benchmark Platform for Meta-Black-Box Optimization](https://arxiv.org/pdf/2505.17745)
*Zeyuan Ma, Yue-Jiao Gong, Hongshu Guo, Wenjie Qiu, Sijie Ma, Hongqiao Lian, Jiajun Zhan, Kaixu Chen, Chen Wang, Zhiyang Huang, Zechuan Huang, Guojun Peng, Ran Cheng, Yining Ma*

Main category: cs.LG

TL;DR: MetaBox-v2 upgrades the original MetaBox framework with broader support for optimization methods, faster training/testing, a comprehensive benchmark suite, and extensible interfaces, demonstrated through a systematic case study.


<details>
  <summary>Details</summary>
Motivation: To address the limitations of the original MetaBox (2023) and keep pace with advancements in meta-learning for optimization algorithm design.

Method: Introduces MetaBox-v2 with a unified architecture for RL, evolutionary, and gradient-based approaches, efficient parallelization, and a benchmark suite of 18 tasks.

Result: Demonstrates 10-40x faster training/testing, reproduces 23 baselines, and provides insights from a systematic case study.

Conclusion: MetaBox-v2 is a significant upgrade offering versatility, efficiency, and extensibility, with valuable insights for practitioners and newcomers.

Abstract: Meta-Black-Box Optimization (MetaBBO) streamlines the automation of
optimization algorithm design through meta-learning. It typically employs a
bi-level structure: the meta-level policy undergoes meta-training to reduce the
manual effort required in developing algorithms for low-level optimization
tasks. The original MetaBox (2023) provided the first open-source framework for
reinforcement learning-based single-objective MetaBBO. However, its relatively
narrow scope no longer keep pace with the swift advancement in this field. In
this paper, we introduce MetaBox-v2 (https://github.com/MetaEvo/MetaBox) as a
milestone upgrade with four novel features: 1) a unified architecture
supporting RL, evolutionary, and gradient-based approaches, by which we
reproduce 23 up-to-date baselines; 2) efficient parallelization schemes, which
reduce the training/testing time by 10-40x; 3) a comprehensive benchmark suite
of 18 synthetic/realistic tasks (1900+ instances) spanning single-objective,
multi-objective, multi-model, and multi-task optimization scenarios; 4)
plentiful and extensible interfaces for custom analysis/visualization and
integrating to external optimization tools/benchmarks. To show the utility of
MetaBox-v2, we carry out a systematic case study that evaluates the built-in
baselines in terms of the optimization performance, generalization ability and
learning efficiency. Valuable insights are concluded from thorough and detailed
analysis for practitioners and those new to the field.

</details>


### [632] [Soft-CAM: Making black box models self-explainable for high-stakes decisions](https://arxiv.org/pdf/2505.17748)
*Kerol Djoumessi, Philipp Berens*

Main category: cs.LG

TL;DR: SoftCAM introduces an inherently interpretable CNN by replacing global average pooling and fully connected layers with a convolution-based class evidence layer, improving explanation quality without losing performance.


<details>
  <summary>Details</summary>
Motivation: Post-hoc explanation methods for CNNs are unreliable and fail to reflect true model reasoning, limiting trust in critical applications like medicine.

Method: SoftCAM removes global average pooling and replaces the classification layer with a convolution-based class evidence layer to preserve spatial information and produce explicit class activation maps.

Result: Evaluated on three medical datasets, SoftCAM maintains classification performance while significantly improving explanation quality over post-hoc methods.

Conclusion: CNNs can be inherently interpretable without performance loss, advancing self-explainable deep learning for high-stakes applications.

Abstract: Convolutional neural networks (CNNs) are widely used for high-stakes
applications like medicine, often surpassing human performance. However, most
explanation methods rely on post-hoc attribution, approximating the
decision-making process of already trained black-box models. These methods are
often sensitive, unreliable, and fail to reflect true model reasoning, limiting
their trustworthiness in critical applications. In this work, we introduce
SoftCAM, a straightforward yet effective approach that makes standard CNN
architectures inherently interpretable. By removing the global average pooling
layer and replacing the fully connected classification layer with a
convolution-based class evidence layer, SoftCAM preserves spatial information
and produces explicit class activation maps that form the basis of the model's
predictions. Evaluated on three medical datasets, SoftCAM maintains
classification performance while significantly improving both the qualitative
and quantitative explanation compared to existing post-hoc methods. Our results
demonstrate that CNNs can be inherently interpretable without compromising
performance, advancing the development of self-explainable deep learning for
high-stakes decision-making.

</details>


### [633] [Mind the GAP! The Challenges of Scale in Pixel-based Deep Reinforcement Learning](https://arxiv.org/pdf/2505.17749)
*Ghada Sokar, Pablo Samuel Castro*

Main category: cs.LG

TL;DR: The paper identifies the bottleneck between encoder outputs and dense layers as the main issue in scaling deep reinforcement learning for pixel-based environments, proposing global average pooling as a simple solution.


<details>
  <summary>Details</summary>
Motivation: The performance drop in scaling deep reinforcement learning for pixel-based environments lacks clear understanding, prompting investigation into the underlying cause.

Method: The study analyzes the bottleneck between encoder outputs and dense layers, proposing global average pooling to address it.

Result: Global average pooling effectively targets the bottleneck, simplifying earlier complex approaches.

Conclusion: The bottleneck is a key scaling limitation, and global average pooling offers a straightforward solution.

Abstract: Scaling deep reinforcement learning in pixel-based environments presents a
significant challenge, often resulting in diminished performance. While recent
works have proposed algorithmic and architectural approaches to address this,
the underlying cause of the performance drop remains unclear. In this paper, we
identify the connection between the output of the encoder (a stack of
convolutional layers) and the ensuing dense layers as the main underlying
factor limiting scaling capabilities; we denote this connection as the
bottleneck, and we demonstrate that previous approaches implicitly target this
bottleneck. As a result of our analyses, we present global average pooling as a
simple yet effective way of targeting the bottleneck, thereby avoiding the
complexity of earlier approaches.

</details>


### [634] [But what is your honest answer? Aiding LLM-judges with honest alternatives using steering vectors](https://arxiv.org/pdf/2505.17760)
*Leon Eshuijs, Archie Chaudhury, Alan McBeth, Ethan Nguyen*

Main category: cs.LG

TL;DR: JUSSA is a new framework using steering vectors to improve LLM honesty detection, tested with a manipulation dataset.


<details>
  <summary>Details</summary>
Motivation: Existing honesty benchmarks miss subtle dishonesty and rely on external judges, which are often ineffective.

Method: JUSSA employs steering vectors trained on a single sample to elicit honest responses, aiding LLM-judges in detecting dishonesty.

Result: JUSSA improves LLM judges' ability to differentiate dishonest and benign responses, detecting subtle manipulation.

Conclusion: JUSSA effectively enhances honesty detection in LLMs, addressing gaps in current benchmarks.

Abstract: Recent safety evaluations of Large Language Models (LLMs) show that many
models exhibit dishonest behavior, such as sycophancy. However, most honesty
benchmarks focus exclusively on factual knowledge or explicitly harmful
behavior and rely on external judges, which are often unable to detect less
obvious forms of dishonesty. In this work, we introduce a new framework, Judge
Using Safety-Steered Alternatives (JUSSA), which utilizes steering vectors
trained on a single sample to elicit more honest responses from models, helping
LLM-judges in the detection of dishonest behavior. To test our framework, we
introduce a new manipulation dataset with prompts specifically designed to
elicit deceptive responses. We find that JUSSA enables LLM judges to better
differentiate between dishonest and benign responses, and helps them identify
subtle instances of manipulative behavior.

</details>


### [635] [Structured Linear CDEs: Maximally Expressive and Parallel-in-Time Sequence Models](https://arxiv.org/pdf/2505.17761)
*Benjamin Walker, Lingyi Yang, Nicola Muca Cirone, Cristopher Salvi, Terry Lyons*

Main category: cs.LG

TL;DR: SLiCEs unify sequence models with structured, input-dependent state-transition matrices, balancing expressivity and computational efficiency.


<details>
  <summary>Details</summary>
Motivation: To create a framework that retains dense matrix expressivity while being computationally cheaper, encompassing existing and novel architectures.

Method: Introduces SLiCEs with block-diagonal, sparse, or Walsh-Hadamard matrices, proving their expressivity matches dense matrices.

Result: SLiCEs excel in benchmarks, achieving top performance in tasks like state-tracking, length generalization, and time-series classification while reducing training time.

Conclusion: SLiCEs offer a powerful, efficient framework for sequence modeling, outperforming or matching existing methods in expressivity and speed.

Abstract: Structured Linear Controlled Differential Equations (SLiCEs) provide a
unifying framework for sequence models with structured, input-dependent
state-transition matrices that retain the maximal expressivity of dense
matrices whilst being cheaper to compute. The framework encompasses existing
architectures, such as input-dependent block-diagonal linear recurrent neural
networks and DeltaNet's diagonal-plus-low-rank structure, as well as two novel
variants based on sparsity and the Walsh--Hadamard transform. We prove that,
unlike the diagonal state-transition matrices of S4 and Mamba, SLiCEs employing
block-diagonal, sparse, or Walsh--Hadamard matrices match the maximal
expressivity of dense matrices. Empirically, SLiCEs solve the $A_5$
state-tracking benchmark with a single layer, achieve best-in-class length
generalisation on regular language tasks among parallel-in-time models, and
match the state-of-the-art performance of log neural controlled differential
equations on six multivariate time-series classification datasets while cutting
the average time per training step by a factor of twenty.

</details>


### [636] [Unsupervised Clustering for Fault Analysis in High-Voltage Power Systems Using Voltage and Current Signals](https://arxiv.org/pdf/2505.17763)
*Julian Oelhaf, Georg Kordowich, Andreas Maier, Johann Jager, Siming Bayer*

Main category: cs.LG

TL;DR: Unsupervised clustering (K-Means) is applied to unlabeled power grid fault data for automated fault diagnosis, showing promising results.


<details>
  <summary>Details</summary>
Motivation: The lack of labeled datasets for fault classification in power grids motivates the use of unsupervised learning techniques.

Method: Frequency domain features are extracted using FFT, and K-Means clustering is applied to categorize faults without labeled data.

Result: The method successfully identifies fault patterns, validated by power system experts, demonstrating scalability and robustness.

Conclusion: Unsupervised learning offers a viable, data-driven approach for fault analysis in power systems with minimal prior assumptions.

Abstract: The widespread use of sensors in modern power grids has led to the
accumulation of large amounts of voltage and current waveform data, especially
during fault events. However, the lack of labeled datasets poses a significant
challenge for fault classification and analysis. This paper explores the
application of unsupervised clustering techniques for fault diagnosis in
high-voltage power systems. A dataset provided by the Reseau de Transport
d'Electricite (RTE) is analyzed, with frequency domain features extracted using
the Fast Fourier Transform (FFT). The K-Means algorithm is then applied to
identify underlying patterns in the data, enabling automated fault
categorization without the need for labeled training samples. The resulting
clusters are evaluated in collaboration with power system experts to assess
their alignment with real-world fault characteristics. The results demonstrate
the potential of unsupervised learning for scalable and data-driven fault
analysis, providing a robust approach to detecting and classifying power system
faults with minimal prior assumptions.

</details>


### [637] [Joker: Joint Optimization Framework for Lightweight Kernel Machines](https://arxiv.org/pdf/2505.17765)
*Junhong Zhang, Zhihui Lai*

Main category: cs.LG

TL;DR: Joker is a joint optimization framework for large-scale kernel methods, addressing memory and efficiency issues while supporting diverse models like KRR, logistic regression, and SVMs.


<details>
  <summary>Details</summary>
Motivation: Kernel methods face scalability challenges, including high memory overhead and limited focus beyond KRR.

Method: Proposes Joker with dual block coordinate descent and kernel approximation using randomized features.

Result: Saves up to 90% memory with comparable or better performance than state-of-the-art methods.

Conclusion: Joker effectively tackles scalability in kernel methods, offering efficiency and versatility.

Abstract: Kernel methods are powerful tools for nonlinear learning with
well-established theory. The scalability issue has been their long-standing
challenge. Despite the existing success, there are two limitations in
large-scale kernel methods: (i) The memory overhead is too high for users to
afford; (ii) existing efforts mainly focus on kernel ridge regression (KRR),
while other models lack study. In this paper, we propose Joker, a joint
optimization framework for diverse kernel models, including KRR, logistic
regression, and support vector machines. We design a dual block coordinate
descent method with trust region (DBCD-TR) and adopt kernel approximation with
randomized features, leading to low memory costs and high efficiency in
large-scale learning. Experiments show that Joker saves up to 90\% memory but
achieves comparable training time and performance (or even better) than the
state-of-the-art methods.

</details>


### [638] [Inference-Time Decomposition of Activations (ITDA): A Scalable Approach to Interpreting Large Language Models](https://arxiv.org/pdf/2505.17769)
*Patrick Leask, Neel Nanda, Noura Al Moubayed*

Main category: cs.LG

TL;DR: ITDA models offer a faster, cheaper alternative to SAEs for decomposing LLM activations, enabling cross-model comparisons with competitive performance.


<details>
  <summary>Details</summary>
Motivation: High training costs and lack of transferability in SAEs limit their use for large models and cross-model analysis.

Method: ITDA models greedily construct a dictionary of activations from prompts, selecting poorly approximated ones via matching pursuit.

Result: ITDAs train in 1% of SAE time/data, work on large models (e.g., Llama-3.1 70B/405B), and enable cross-model comparisons with better similarity metrics.

Conclusion: ITDAs are a practical alternative to SAEs for resource-limited scenarios or cross-model analysis, despite some performance trade-offs.

Abstract: Sparse autoencoders (SAEs) are a popular method for decomposing Large Langage
Models (LLM) activations into interpretable latents. However, due to their
substantial training cost, most academic research uses open-source SAEs which
are only available for a restricted set of models of up to 27B parameters. SAE
latents are also learned from a dataset of activations, which means they do not
transfer between models. Motivated by relative representation similarity
measures, we introduce Inference-Time Decomposition of Activations (ITDA)
models, an alternative method for decomposing language model activations. To
train an ITDA, we greedily construct a dictionary of language model activations
on a dataset of prompts, selecting those activations which were worst
approximated by matching pursuit on the existing dictionary. ITDAs can be
trained in just 1\% of the time required for SAEs, using 1\% of the data. This
allowed us to train ITDAs on Llama-3.1 70B and 405B on a single consumer GPU.
ITDAs can achieve similar reconstruction performance to SAEs on some target
LLMs, but generally incur a performance penalty. However, ITDA dictionaries
enable cross-model comparisons, and a simple Jaccard similarity index on ITDA
dictionaries outperforms existing methods like CKA, SVCCA, and relative
representation similarity metrics. ITDAs provide a cheap alternative to SAEs
where computational resources are limited, or when cross model comparisons are
necessary. Code available at https://github.com/pleask/itda.

</details>


### [639] [C-LoRA: Contextual Low-Rank Adaptation for Uncertainty Estimation in Large Language Models](https://arxiv.org/pdf/2505.17773)
*Amir Hossein Rahmati, Sanket Jantre, Weifeng Zhang, Yucheng Wang, Byung-Jun Yoon, Nathan M. Urban, Xiaoning Qian*

Main category: cs.LG

TL;DR: C-LoRA improves uncertainty-aware fine-tuning of LLMs by dynamically adapting uncertainty estimates to input contexts, outperforming existing methods.


<details>
  <summary>Details</summary>
Motivation: LoRA's overconfident predictions in few-shot settings and neglect of input characteristics in uncertainty estimates.

Method: Develops lightweight, input-contextualized LoRA modules for dynamic uncertainty adaptation.

Result: C-LoRA mitigates overfitting, achieves calibrated uncertainties, and robust predictions, outperforming state-of-the-art methods.

Conclusion: C-LoRA sets a new standard for robust, uncertainty-aware LLM fine-tuning in few-shot scenarios.

Abstract: Low-Rank Adaptation (LoRA) offers a cost-effective solution for fine-tuning
large language models (LLMs), but it often produces overconfident predictions
in data-scarce few-shot settings. To address this issue, several classical
statistical learning approaches have been repurposed for scalable
uncertainty-aware LoRA fine-tuning. However, these approaches neglect how input
characteristics affect the predictive uncertainty estimates. To address this
limitation, we propose Contextual Low-Rank Adaptation (\textbf{C-LoRA}) as a
novel uncertainty-aware and parameter efficient fine-tuning approach, by
developing new lightweight LoRA modules contextualized to each input data
sample to dynamically adapt uncertainty estimates. Incorporating data-driven
contexts into the parameter posteriors, C-LoRA mitigates overfitting, achieves
well-calibrated uncertainties, and yields robust predictions. Extensive
experiments demonstrate that C-LoRA consistently outperforms the
state-of-the-art uncertainty-aware LoRA methods in both uncertainty
quantification and model generalization. Ablation studies further confirm the
critical role of our contextual modules in capturing sample-specific
uncertainties. C-LoRA sets a new standard for robust, uncertainty-aware LLM
fine-tuning in few-shot regimes.

</details>


### [640] [Optimizing Shortfall Risk Metric for Learning Regression Models](https://arxiv.org/pdf/2505.17777)
*Harish G. Ramaswamy, L. A. Prashanth*

Main category: cs.LG

TL;DR: The paper addresses estimating and optimizing utility-based shortfall risk (UBSR) in regression, proposing a gradient oracle and bisection algorithm for convergence.


<details>
  <summary>Details</summary>
Motivation: UBSR is non-linear and challenging to optimize; the paper aims to provide theoretical and algorithmic solutions for its estimation and minimization.

Method: Derives a concentration bound for UBSR, frames optimization as pseudo-linear minimization, and uses gradient and linear minimization oracles with a bisection algorithm.

Result: Convergence to the UBSR-optimal solution is established.

Conclusion: The proposed method effectively tackles UBSR optimization in regression problems.

Abstract: We consider the problem of estimating and optimizing utility-based shortfall
risk (UBSR) of a loss, say $(Y - \hat Y)^2$, in the context of a regression
problem. Empirical risk minimization with a UBSR objective is challenging since
UBSR is a non-linear function of the underlying distribution. We first derive a
concentration bound for UBSR estimation using independent and identically
distributed (i.i.d.) samples. We then frame the UBSR optimization problem as
minimization of a pseudo-linear function in the space of achievable
distributions $\mathcal D$ of the loss $(Y- \hat Y)^2$. We construct a gradient
oracle for the UBSR objective and a linear minimization oracle (LMO) for the
set $\mathcal D$. Using these oracles, we devise a bisection-type algorithm,
and establish convergence to the UBSR-optimal solution.

</details>


### [641] [Hyperparameter Optimization via Interacting with Probabilistic Circuits](https://arxiv.org/pdf/2505.17804)
*Jonas Seng, Fabrizio Ventola, Zhongjie Yu, Kristian Kersting*

Main category: cs.LG

TL;DR: A novel Bayesian optimization (BO) method using probabilistic circuits (PCs) improves interactive hyperparameter optimization (HPO) by accurately reflecting user beliefs without inner-loop optimization.


<details>
  <summary>Details</summary>
Motivation: Existing interactive BO methods inadequately incorporate human feedback due to non-trivial inner optimization, leading to misaligned user beliefs.

Method: Leverages probabilistic circuits (PCs) as a surrogate model for exact conditional inference and sampling, enabling acquisition function-free candidate generation.

Result: Achieves state-of-the-art performance in standard HPO and outperforms interactive BO baselines in interactive HPO.

Conclusion: The proposed BO approach with PCs effectively integrates user feedback and eliminates inner-loop optimization, enhancing interactive HPO.

Abstract: Despite the growing interest in designing truly interactive hyperparameter
optimization (HPO) methods, to date, only a few allow to include human
feedback. Existing interactive Bayesian optimization (BO) methods incorporate
human beliefs by weighting the acquisition function with a user-defined prior
distribution. However, in light of the non-trivial inner optimization of the
acquisition function prevalent in BO, such weighting schemes do not always
accurately reflect given user beliefs. We introduce a novel BO approach
leveraging tractable probabilistic models named probabilistic circuits (PCs) as
a surrogate model. PCs encode a tractable joint distribution over the hybrid
hyperparameter space and evaluation scores. They enable exact conditional
inference and sampling. Based on conditional sampling, we construct a novel
selection policy that enables an acquisition function-free generation of
candidate points (thereby eliminating the need for an additional inner-loop
optimization) and ensures that user beliefs are reflected accurately in the
selection policy. We provide a theoretical analysis and an extensive empirical
evaluation, demonstrating that our method achieves state-of-the-art performance
in standard HPO and outperforms interactive BO baselines in interactive HPO.

</details>


### [642] [Supervised Graph Contrastive Learning for Gene Regulatory Network](https://arxiv.org/pdf/2505.17786)
*Sho Oshima, Yuji Okamoto, Taisei Tosaki, Ryosuke Kojima, Yasushi Okuno*

Main category: cs.LG

TL;DR: SupGCL is a supervised GCL method for GRNs that incorporates biological perturbations from gene knockdowns, outperforming existing methods in biological tasks.


<details>
  <summary>Details</summary>
Motivation: Existing GCL methods overlook biologically relevant perturbations like gene knockdowns in GRNs, limiting their effectiveness.

Method: SupGCL extends GCL by integrating biological perturbations from gene knockdown data into probabilistic models.

Result: SupGCL outperforms state-of-the-art baselines in tasks like patient hazard prediction and gene function classification.

Conclusion: SupGCL effectively leverages biological perturbations for improved GRN representation and downstream task performance.

Abstract: Graph representation learning is effective for obtaining a meaningful latent
space utilizing the structure of graph data and is widely applied, including
biological networks. In particular, Graph Contrastive Learning (GCL) has
emerged as a powerful self-supervised method that relies on applying
perturbations to graphs for data augmentation. However, when applying existing
GCL methods to biological networks such as Gene Regulatory Networks (GRNs),
they overlooked meaningful biologically relevant perturbations, e.g., gene
knockdowns. In this study, we introduce SupGCL (Supervised Graph Contrastive
Learning), a novel GCL method for GRNs that directly incorporates biological
perturbations derived from gene knockdown experiments as the supervision.
SupGCL mathematically extends existing GCL methods that utilize non-biological
perturbations to probabilistic models that introduce actual biological gene
perturbation utilizing gene knockdown data. Using the GRN representation
obtained by our proposed method, our aim is to improve the performance of
biological downstream tasks such as patient hazard prediction and disease
subtype classification (graph-level task), and gene function classification
(node-level task). We applied SupGCL on real GRN datasets derived from patients
with multiple types of cancer, and in all experiments SupGCL achieves better
performance than state-of-the-art baselines.

</details>


### [643] [RECIPE-TKG: From Sparse History to Structured Reasoning for LLM-based Temporal Knowledge Graph Completion](https://arxiv.org/pdf/2505.17794)
*Ömer Faruk Akgül, Feiyu Zhu, Yuxin Yang, Rajgopal Kannan, Viktor Prasanna*

Main category: cs.LG

TL;DR: RECIPE-TKG is a lightweight framework for TKG completion, improving accuracy and generalization in sparse historical contexts by combining rule-based retrieval, contrastive fine-tuning, and semantic filtering.


<details>
  <summary>Details</summary>
Motivation: Existing LLM-based approaches for TKG completion overemphasize supervised fine-tuning and struggle with limited historical evidence.

Method: RECIPE-TKG uses (1) rule-based multi-hop retrieval, (2) contrastive fine-tuning of lightweight adapters, and (3) test-time semantic filtering.

Result: Outperforms previous LLM-based methods by up to 30.6% in Hits@10 and produces semantically coherent predictions.

Conclusion: RECIPE-TKG is effective for TKG completion, especially in sparse historical contexts.

Abstract: Temporal Knowledge Graphs (TKGs) represent dynamic facts as timestamped
relations between entities. TKG completion involves forecasting missing or
future links, requiring models to reason over time-evolving structure. While
LLMs show promise for this task, existing approaches often overemphasize
supervised fine-tuning and struggle particularly when historical evidence is
limited or missing. We introduce RECIPE-TKG, a lightweight and data-efficient
framework designed to improve accuracy and generalization in settings with
sparse historical context. It combines (1) rule-based multi-hop retrieval for
structurally diverse history, (2) contrastive fine-tuning of lightweight
adapters to encode relational semantics, and (3) test-time semantic filtering
to iteratively refine generations based on embedding similarity. Experiments on
four TKG benchmarks show that RECIPE-TKG outperforms previous LLM-based
approaches, achieving up to 30.6\% relative improvement in Hits@10. Moreover,
our proposed framework produces more semantically coherent predictions, even
for the samples with limited historical context.

</details>


### [644] [Imagine Beyond! Distributionally Robust Auto-Encoding for State Space Coverage in Online Reinforcement Learning](https://arxiv.org/pdf/2505.17830)
*Nicolas Castanet, Olivier Sigaud, Sylvain Lamprier*

Main category: cs.LG

TL;DR: DRAG improves GCRL by enforcing uniform state coverage in latent spaces using adversarial neural weighting, enhancing exploration and control performance.


<details>
  <summary>Details</summary>
Motivation: Classical GCRL methods struggle with sparse visual observations and biased latent spaces, limiting skill diversity.

Method: DRAG combines β-VAE with Distributionally Robust Optimization, using adversarial weighting to balance state coverage.

Result: DRAG achieves better state coverage and control in hard exploration tasks like mazes and robotic environments.

Conclusion: DRAG addresses latent space bias in GCRL, enabling broader skill acquisition without prior knowledge.

Abstract: Goal-Conditioned Reinforcement Learning (GCRL) enables agents to autonomously
acquire diverse behaviors, but faces major challenges in visual environments
due to high-dimensional, semantically sparse observations. In the online
setting, where agents learn representations while exploring, the latent space
evolves with the agent's policy, to capture newly discovered areas of the
environment. However, without incentivization to maximize state coverage in the
representation, classical approaches based on auto-encoders may converge to
latent spaces that over-represent a restricted set of states frequently visited
by the agent. This is exacerbated in an intrinsic motivation setting, where the
agent uses the distribution encoded in the latent space to sample the goals it
learns to master. To address this issue, we propose to progressively enforce
distributional shifts towards a uniform distribution over the full state space,
to ensure a full coverage of skills that can be learned in the environment. We
introduce DRAG (Distributionally Robust Auto-Encoding for GCRL), a method that
combines the $\beta$-VAE framework with Distributionally Robust Optimization.
DRAG leverages an adversarial neural weighter of training states of the VAE, to
account for the mismatch between the current data distribution and unseen parts
of the environment. This allows the agent to construct semantically meaningful
latent spaces beyond its immediate experience. Our approach improves state
space coverage and downstream control performance on hard exploration
environments such as mazes and robotic control involving walls to bypass,
without pre-training nor prior environment knowledge.

</details>


### [645] [Latent Mode Decomposition](https://arxiv.org/pdf/2505.17797)
*Manuel Morante, Naveed ur Rehman*

Main category: cs.LG

TL;DR: VLMD is a new algorithm for extracting oscillatory modes and connectivity from multivariate signals, outperforming existing methods in accuracy, efficiency, and interpretability.


<details>
  <summary>Details</summary>
Motivation: Addressing limitations of current Multivariate Mode Decomposition techniques, such as high computational cost, parameter sensitivity, and weak interchannel dependency modeling.

Method: VLMD uses a Latent Mode Decomposition model combining sparse coding and mode decomposition, solving a constrained variational optimization problem for reconstruction fidelity, sparsity, and frequency regularization.

Result: VLMD shows superior performance in accuracy, efficiency, and interpretability compared to state-of-the-art MMD methods on synthetic and real-world datasets.

Conclusion: VLMD is a robust, scalable, and interpretable solution for extracting oscillatory modes and connectivity structures from multivariate signals.

Abstract: We introduce Variational Latent Mode Decomposition (VLMD), a new algorithm
for extracting oscillatory modes and associated connectivity structures from
multivariate signals. VLMD addresses key limitations of existing Multivariate
Mode Decomposition (MMD) techniques -including high computational cost,
sensitivity to parameter choices, and weak modeling of interchannel
dependencies. Its improved performance is driven by a novel underlying model,
Latent Mode Decomposition (LMD), which blends sparse coding and mode
decomposition to represent multichannel signals as sparse linear combinations
of shared latent components composed of AM-FM oscillatory modes. This
formulation enables VLMD to operate in a lower-dimensional latent space,
enhancing robustness to noise, scalability, and interpretability. The algorithm
solves a constrained variational optimization problem that jointly enforces
reconstruction fidelity, sparsity, and frequency regularization. Experiments on
synthetic and real-world datasets demonstrate that VLMD outperforms
state-of-the-art MMD methods in accuracy, efficiency, and interpretability of
extracted structures.

</details>


### [646] [A Coreset Selection of Coreset Selection Literature: Introduction and Recent Advances](https://arxiv.org/pdf/2505.17799)
*Brian B. Moser, Arundhati S. Shanbhag, Stanislav Frolov, Federico Raue, Joachim Folz, Andreas Dengel*

Main category: cs.LG

TL;DR: This survey unifies three major coreset research lines (training-free, training-oriented, label-free) into a taxonomy, covering overlooked subfields and comparing methods under computational, robustness, and performance demands.


<details>
  <summary>Details</summary>
Motivation: Address the gap in existing surveys by providing a comprehensive view of coreset selection, including overlooked subfields and new insights.

Method: Presents a unified taxonomy of coreset research, examining subfields like submodular formulations, bilevel optimization, and pseudo-labeling, and compares methods under various demands.

Result: Offers new insights into pruning strategies, generalization, and neural scaling laws, while highlighting open challenges like robustness and outlier filtering.

Conclusion: The survey provides a holistic view of coreset selection, identifies gaps in prior work, and suggests future research directions, such as adapting coresets to foundation models.

Abstract: Coreset selection targets the challenge of finding a small, representative
subset of a large dataset that preserves essential patterns for effective
machine learning. Although several surveys have examined data reduction
strategies before, most focus narrowly on either classical geometry-based
methods or active learning techniques. In contrast, this survey presents a more
comprehensive view by unifying three major lines of coreset research, namely,
training-free, training-oriented, and label-free approaches, into a single
taxonomy. We present subfields often overlooked by existing work, including
submodular formulations, bilevel optimization, and recent progress in
pseudo-labeling for unlabeled datasets. Additionally, we examine how pruning
strategies influence generalization and neural scaling laws, offering new
insights that are absent from prior reviews. Finally, we compare these methods
under varying computational, robustness, and performance demands and highlight
open challenges, such as robustness, outlier filtering, and adapting coreset
selection to foundation models, for future research.

</details>


### [647] [TransDF: Time-Series Forecasting Needs Transformed Label Alignment](https://arxiv.org/pdf/2505.17847)
*Hao Wang, Licheng Pan, Zhichao Chen, Xu Chen, Qingyang Dai, Lei Wang, Haoxuan Li, Zhouchen Lin*

Main category: cs.LG

TL;DR: TransDF transforms label sequences into decorrelated components to mitigate label autocorrelation and reduce task complexity, achieving state-of-the-art performance.


<details>
  <summary>Details</summary>
Motivation: Existing methods using temporal mean squared error suffer from label autocorrelation and excessive tasks, complicating optimization.

Method: TransDF transforms label sequences into decorrelated components, focusing on aligning the most significant ones.

Result: TransDF outperforms existing methods and is compatible with various forecasting models.

Conclusion: TransDF effectively addresses challenges in time-series forecasting, offering improved performance and versatility.

Abstract: Training time-series forecasting models presents unique challenges in
designing effective learning objectives. Existing methods predominantly utilize
the temporal mean squared error, which faces two critical challenges: (1) label
autocorrelation, which leads to bias from the label sequence likelihood; (2)
excessive amount of tasks, which increases with the forecast horizon and
complicates optimization. To address these challenges, we propose
Transform-enhanced Direct Forecast (TransDF), which transforms the label
sequence into decorrelated components with discriminated significance. Models
are trained to align the most significant components, thereby effectively
mitigating label autocorrelation and reducing task amount. Extensive
experiments demonstrate that TransDF achieves state-of-the-art performance and
is compatible with various forecasting models. Code is available at
https://anonymous.4open.science/r/TransDF-88CF.

</details>


### [648] [VIBE: Vector Index Benchmark for Embeddings](https://arxiv.org/pdf/2505.17810)
*Elias Jääsaari, Ville Hyvönen, Matteo Ceccarello, Teemu Roos, Martin Aumüller*

Main category: cs.LG

TL;DR: VIBE is a new benchmark for ANN search, addressing outdated datasets and including OOD scenarios for real-world relevance.


<details>
  <summary>Details</summary>
Motivation: Existing ANN benchmarks lack representativeness for modern applications, necessitating an updated evaluation framework.

Method: VIBE introduces a pipeline for creating benchmark datasets using dense embeddings and includes OOD datasets to mimic real-world conditions.

Result: 21 state-of-the-art vector indexes were evaluated on 12 in-distribution and 6 OOD datasets.

Conclusion: VIBE provides a modern, comprehensive benchmark for ANN search, enhancing evaluation rigor and relevance.

Abstract: Approximate nearest neighbor (ANN) search is a performance-critical component
of many machine learning pipelines. Rigorous benchmarking is essential for
evaluating the performance of vector indexes for ANN search. However, the
datasets of the existing benchmarks are no longer representative of the current
applications of ANN search. Hence, there is an urgent need for an up-to-date
set of benchmarks. To this end, we introduce Vector Index Benchmark for
Embeddings (VIBE), an open source project for benchmarking ANN algorithms. VIBE
contains a pipeline for creating benchmark datasets using dense embedding
models characteristic of modern applications, such as retrieval-augmented
generation (RAG). To replicate real-world workloads, we also include
out-of-distribution (OOD) datasets where the queries and the corpus are drawn
from different distributions. We use VIBE to conduct a comprehensive evaluation
of SOTA vector indexes, benchmarking 21 implementations on 12 in-distribution
and 6 out-of-distribution datasets.

</details>


### [649] [Scaling Recurrent Neural Networks to a Billion Parameters with Zero-Order Optimization](https://arxiv.org/pdf/2505.17852)
*Francois Chaubard, Mykel Kochenderfer*

Main category: cs.LG

TL;DR: The paper proposes using Zero-Order Optimization (ZOO) methods like Random-vector Gradient Estimation (RGE) to train RNNs, replacing Backpropagation Through Time (BPTT) for better memory efficiency and faster convergence.


<details>
  <summary>Details</summary>
Motivation: Training large RNNs on long contexts is impractical with BPTT due to high memory usage. The paper aims to find a more efficient alternative.

Method: Uses ZOO methods (e.g., RGE and CD-RGE) to train RNNs without retaining intermediate activations, reducing memory and cost.

Result: Matches or exceeds BPTT convergence rates by up to 19x, with better generalization and often fewer steps.

Conclusion: ZOO methods are a viable alternative to BPTT for training RNNs, offering memory efficiency, faster convergence, and competitive performance.

Abstract: During inference, Recurrent Neural Networks (RNNs) scale constant in both
FLOPs and GPU memory with increasing context length, as they compress all prior
tokens into a fixed-size memory. In contrast, transformers scale linearly in
FLOPs and, at best, linearly in memory during generation, since they must
attend to all previous tokens explicitly. Despite this inference-time
advantage, training large RNNs on long contexts remains impractical because
standard optimization methods depend on Backpropagation Through Time (BPTT).
BPTT requires retention of all intermediate activations during the forward
pass, causing memory usage to scale linearly with both context length and model
size. In this paper, we show that Zero-Order Optimization (ZOO) methods such as
Random-vector Gradient Estimation (RGE) can successfully replace BPTT to train
RNNs with convergence rates that match, or exceed BPTT by up to 19 fold, while
using orders of magnitude less memory and cost, as the model remains in
inference mode throughout training. We further demonstrate that
Central-Difference RGE (CD-RGE) corresponds to optimizing a smoothed surrogate
loss, inherently regularizing training and improving generalization. Our method
matches or outperforms BPTT across three settings: (1) overfitting, (2)
transduction, and (3) language modeling. Across all tasks, with sufficient
perturbations, our models generalize as well as or better than those trained
with BPTT, often in fewer steps. Despite the need for more forward passes per
step, we can surpass BPTT wall-clock time per step using recent advancements
such as FlashRNN and distributed inference.

</details>


### [650] [Trinity-RFT: A General-Purpose and Unified Framework for Reinforcement Fine-Tuning of Large Language Models](https://arxiv.org/pdf/2505.17826)
*Xuchen Pan, Yanxi Chen, Yushuo Chen, Yuchang Sun, Daoyuan Chen, Wenhao Zhang, Yuexiang Xie, Yilun Huang, Yilei Zhang, Dawei Gao, Yaliang Li, Bolin Ding, Jingren Zhou*

Main category: cs.LG

TL;DR: Trinity-RFT is a flexible, scalable framework for reinforcement fine-tuning (RFT) of large language models, supporting diverse modes and seamless agent-environment interaction.


<details>
  <summary>Details</summary>
Motivation: To provide a unified, efficient, and adaptable platform for RFT of large language models across various scenarios and RL paradigms.

Method: Decoupled design with RFT-core, seamless agent-environment integration, and optimized data pipelines.

Result: A versatile framework demonstrated through extensive examples, enabling exploration of advanced RL paradigms.

Conclusion: Trinity-RFT is a robust, user-friendly solution for RFT, adaptable to diverse applications and research needs.

Abstract: Trinity-RFT is a general-purpose, flexible and scalable framework designed
for reinforcement fine-tuning (RFT) of large language models. It is built with
a decoupled design, consisting of (1) an RFT-core that unifies and generalizes
synchronous/asynchronous, on-policy/off-policy, and online/offline modes of
RFT, (2) seamless integration for agent-environment interaction with high
efficiency and robustness, and (3) systematic data pipelines optimized for RFT.
Trinity-RFT can be easily adapted for diverse application scenarios, and serves
as a unified platform for exploring advanced reinforcement learning paradigms.
This technical report outlines the vision, features, design and implementations
of Trinity-RFT, accompanied by extensive examples demonstrating the utility and
user-friendliness of the proposed framework.

</details>


### [651] [Stochastic Weight Sharing for Bayesian Neural Networks](https://arxiv.org/pdf/2505.17856)
*Moule Lin, Shuhao Guan, Weipeng Jing, Goetz Botterweck, Andrea Patane*

Main category: cs.LG

TL;DR: The paper proposes a method to reduce computational overhead in Bayesian Neural Networks (BNNs) using weight-sharing quantization, achieving efficient training of large models like ResNet-101 and VIT.


<details>
  <summary>Details</summary>
Motivation: BNNs face computational and convergence challenges in deep architectures, limiting their practical use.

Method: Uses 2D adaptive Gaussian distributions, Wasserstein distance estimations, and alpha blending to encode BNN stochasticity in a lower-dimensional representation.

Result: Reduces computational overhead by orders of magnitude, compresses parameters by 50x, and cuts model size by 75%, while maintaining accuracy and uncertainty estimation quality.

Conclusion: The approach enables efficient Bayesian training of large-scale models without sacrificing performance.

Abstract: While offering a principled framework for uncertainty quantification in deep
learning, the employment of Bayesian Neural Networks (BNNs) is still
constrained by their increased computational requirements and the convergence
difficulties when training very deep, state-of-the-art architectures. In this
work, we reinterpret weight-sharing quantization techniques from a stochastic
perspective in the context of training and inference with Bayesian Neural
Networks (BNNs). Specifically, we leverage 2D adaptive Gaussian distributions,
Wasserstein distance estimations, and alpha blending to encode the stochastic
behaviour of a BNN in a lower dimensional, soft Gaussian representation.
Through extensive empirical investigation, we demonstrate that our approach
significantly reduces the computational overhead inherent in Bayesian learning
by several orders of magnitude, enabling the efficient Bayesian training of
large-scale models, such as ResNet-101 and Vision Transformer (VIT). On various
computer vision benchmarks including CIFAR10, CIFAR100, and ImageNet1k. Our
approach compresses model parameters by approximately 50x and reduces model
size by 75, while achieving accuracy and uncertainty estimations comparable to
the state-of-the-art.

</details>


### [652] [Out of the Shadows: Exploring a Latent Space for Neural Network Verification](https://arxiv.org/pdf/2505.17854)
*Lukas Koller, Tobias Ladner, Matthias Althoff*

Main category: cs.LG

TL;DR: The paper introduces a novel latent space for formal verification of neural networks, enabling iterative refinement of input sets to focus on unsafe inputs, improving efficiency and performance.


<details>
  <summary>Details</summary>
Motivation: Neural networks are sensitive to small input changes, making formal verification crucial for safety-critical applications. Existing methods often yield inconclusive results due to conservatism in output enclosures.

Method: The authors design a latent space using projection-based set representations (e.g., zonotopes) to transfer output specifications to the input space, enabling iterative refinement. This reduces subproblems in branch-and-bound verification.

Result: The approach achieves competitive performance, leveraging GPU acceleration for speed-up, and places among top tools in VNN-COMP'24.

Conclusion: The proposed method enhances neural network verification by reducing conservatism and improving efficiency, making it suitable for safety-critical applications.

Abstract: Neural networks are ubiquitous. However, they are often sensitive to small
input changes. Hence, to prevent unexpected behavior in safety-critical
applications, their formal verification -- a notoriously hard problem -- is
necessary. Many state-of-the-art verification algorithms use reachability
analysis or abstract interpretation to enclose the set of possible outputs of a
neural network. Often, the verification is inconclusive due to the conservatism
of the enclosure. To address this problem, we design a novel latent space for
formal verification that enables the transfer of output specifications to the
input space for an iterative specification-driven input refinement, i.e., we
iteratively reduce the set of possible inputs to only enclose the unsafe ones.
The latent space is constructed from a novel view of projection-based set
representations, e.g., zonotopes, which are commonly used in reachability
analysis of neural networks. A projection-based set representation is a
"shadow" of a higher-dimensional set -- a latent space -- that does not change
during a set propagation through a neural network. Hence, the input set and the
output enclosure are "shadows" of the same latent space that we can use to
transfer constraints. We present an efficient verification tool for neural
networks that uses our iterative refinement to significantly reduce the number
of subproblems in a branch-and-bound procedure. Using zonotopes as a set
representation, unlike many other state-of-the-art approaches, our approach can
be realized by only using matrix operations, which enables a significant
speed-up through efficient GPU acceleration. We demonstrate that our tool
achieves competitive performance, which would place it among the top-ranking
tools of the last neural network verification competition (VNN-COMP'24).

</details>


### [653] [Scalable Valuation of Human Feedback through Provably Robust Model Alignment](https://arxiv.org/pdf/2505.17859)
*Masahiro Fujisawa, Masaki Adachi, Michael A. Osborne*

Main category: cs.LG

TL;DR: Hölder-DPO is a new alignment loss with a redescending property, enabling robust alignment from noisy human feedback and automated mislabel detection.


<details>
  <summary>Details</summary>
Motivation: Existing alignment methods fail under noisy human feedback, necessitating a robust solution.

Method: Proposes Hölder-DPO, a principled alignment loss with provable redescending property, and a gradient-free metric for mislabel detection.

Result: Achieves state-of-the-art robust alignment and accurately identifies mislabels, improving performance when removed.

Conclusion: Hölder-DPO addresses noise in human feedback, enhancing alignment and dataset quality.

Abstract: Despite the importance of aligning language models with human preferences,
crowd-sourced human feedback is often noisy -- for example, preferring less
desirable responses -- posing a fundamental challenge to alignment. A truly
robust alignment objective should yield identical model parameters even under
severe label noise, a property known as redescending. We prove that no existing
alignment methods satisfy this property. To address this, we propose
H\"older-DPO, the first principled alignment loss with a provable redescending
property, enabling estimation of the clean data distribution from noisy
feedback. The aligned model estimates the likelihood of clean data, providing a
theoretically grounded metric for dataset valuation that identifies the
location and fraction of mislabels. This metric is gradient-free, enabling
scalable and automated human feedback valuation without costly manual
verification or clean validation dataset. H\"older-DPO achieves
state-of-the-art robust alignment performance while accurately detecting
mislabels in controlled datasets. Finally, we apply H\"older-DPO to widely used
alignment datasets, revealing substantial noise levels and demonstrating that
removing these mislabels significantly improves alignment performance across
methods.

</details>


### [654] [The emergence of sparse attention: impact of data distribution and benefits of repetition](https://arxiv.org/pdf/2505.17863)
*Nicolas Zucchet, Francesco d'Angelo, Andrew K. Lampinen, Stephanie C. Y. Chan*

Main category: cs.LG

TL;DR: The paper investigates the emergence of sparse attention in Transformers, revealing power-law dynamics influenced by task structure, architecture, and optimizer choice, with repetition accelerating emergence.


<details>
  <summary>Details</summary>
Motivation: To understand how and when new abilities, like sparse attention, emerge in large language models and neural networks.

Method: Combines theoretical analysis of a toy model with empirical observations on small Transformers trained on linear regression and an in-context associative recall task.

Result: Emergence timing follows power laws; repetition speeds up emergence. Findings confirmed on associative recall.

Conclusion: Provides a framework for understanding how data and model design influence emergence dynamics.

Abstract: Emergence is a fascinating property of large language models and neural
networks more broadly: as models scale and train for longer, they sometimes
develop new abilities in sudden ways. Despite initial studies, we still lack a
comprehensive understanding of how and when these abilities emerge. To address
this gap, we study the emergence over training of sparse attention, a critical
and frequently observed attention pattern in Transformers. By combining
theoretical analysis of a toy model with empirical observations on small
Transformers trained on a linear regression variant, we uncover the mechanics
driving sparse attention emergence and reveal that emergence timing follows
power laws based on task structure, architecture, and optimizer choice. We
additionally find that repetition can greatly speed up emergence. Finally, we
confirm these results on a well-studied in-context associative recall task. Our
findings provide a simple, theoretically grounded framework for understanding
how data distributions and model design influence the learning dynamics behind
one form of emergence.

</details>


### [655] [Mixture of Low Rank Adaptation with Partial Parameter Sharing for Time Series Forecasting](https://arxiv.org/pdf/2505.17872)
*Licheng Pan, Zhichao Chen, Haoxuan Li, Guangyi Liu, Zhijian Xu, Zhaoran Liu, Hao Wang, Ying Wei*

Main category: cs.LG

TL;DR: The paper addresses the expressiveness bottleneck in multi-task time-series forecasting by proposing a two-stage framework with step-specific LoRA modules and a Mixture-of-LoRA (MoLA) model, improving performance and efficiency.


<details>
  <summary>Details</summary>
Motivation: Multi-task forecasting suffers from an expressiveness bottleneck where predictions share the same representation, leading to unavoidable errors.

Method: A two-stage framework: pre-train a foundation model for one-step-ahead prediction, then adapt it using step-specific LoRA modules. Introduces MoLA for partial parameter sharing across steps.

Result: MoLA significantly improves model expressiveness and outperforms state-of-the-art time-series forecasting methods.

Conclusion: The proposed framework and MoLA effectively address the expressiveness bottleneck, enhancing forecasting performance and efficiency.

Abstract: Multi-task forecasting has become the standard approach for time-series
forecasting (TSF). However, we show that it suffers from an Expressiveness
Bottleneck, where predictions at different time steps share the same
representation, leading to unavoidable errors even with optimal
representations. To address this issue, we propose a two-stage framework:
first, pre-train a foundation model for one-step-ahead prediction; then, adapt
it using step-specific LoRA modules.This design enables the foundation model to
handle any number of forecast steps while avoiding the expressiveness
bottleneck. We further introduce the Mixture-of-LoRA (MoLA) model, which
employs adaptively weighted LoRA experts to achieve partial parameter sharing
across steps. This approach enhances both efficiency and forecasting
performance by exploiting interdependencies between forecast steps. Experiments
show that MoLA significantly improves model expressiveness and outperforms
state-of-the-art time-series forecasting methods. Code is available at
https://anonymous.4open.science/r/MoLA-BC92.

</details>


### [656] [DesignX: Human-Competitive Algorithm Designer for Black-Box Optimization](https://arxiv.org/pdf/2505.17866)
*Hongshu Guo, Zeyuan Ma, Yining Ma, Xinglin Zhang, Wei-Neng Chen, Yue-Jiao Gong*

Main category: cs.LG

TL;DR: DesignX is an automated framework for generating optimizers for black-box problems, outperforming human-crafted ones through dual-agent reinforcement learning.


<details>
  <summary>Details</summary>
Motivation: Overcome limitations of manual optimizer design by automating the process, leveraging decades of research and diverse problem instances.

Method: Uses a modular algorithmic space and dual-agent reinforcement learning for structural and parametric design, trained on 10k diverse instances.

Result: DesignX-generated optimizers surpass human-crafted ones significantly in synthetic and real-world scenarios like Protein-docking and AutoML.

Conclusion: DesignX not only outperforms manual designs but also discovers non-trivial algorithm patterns, offering insights for the optimization community.

Abstract: Designing effective black-box optimizers is hampered by limited
problem-specific knowledge and manual control that spans months for almost
every detail. In this paper, we present DesignX, the first automated algorithm
design framework that generates an effective optimizer specific to a given
black-box optimization problem within seconds. Rooted in the first principles,
we identify two key sub-tasks: 1) algorithm structure generation and 2)
hyperparameter control. To enable systematic construction, a comprehensive
modular algorithmic space is first built, embracing hundreds of algorithm
components collected from decades of research. We then introduce a dual-agent
reinforcement learning system that collaborates on structural and parametric
design through a novel cooperative training objective, enabling large-scale
meta-training across 10k diverse instances. Remarkably, through days of
autonomous learning, the DesignX-generated optimizers continuously surpass
human-crafted optimizers by orders of magnitude, either on synthetic testbed or
on realistic optimization scenarios such as Protein-docking, AutoML and UAV
path planning. Further in-depth analysis reveals DesignX's capability to
discover non-trivial algorithm patterns beyond expert intuition, which,
conversely, provides valuable design insights for the optimization community.
We provide DesignX's inference code at https://github.com/MetaEvo/DesignX.

</details>


### [657] [SpectraLDS: Provable Distillation for Linear Dynamical Systems](https://arxiv.org/pdf/2505.17868)
*Devan Shah, Shlomo Fortgang, Sofiia Druchyna, Elad Hazan*

Main category: cs.LG

TL;DR: A provable method for identifying symmetric linear dynamical systems (LDS) with dimension-independent accuracy guarantees, using spectral transformations and convex optimization.


<details>
  <summary>Details</summary>
Motivation: To address the challenge of efficiently identifying symmetric LDSs without dependency on state dimension or memory, enabling practical applications like sequence prediction.

Method: Inverts a spectral representation of symmetric LDSs into a learnable convolution, forming an end-to-end convex optimization procedure.

Result: Achieves dimension-independent accuracy and enables constant-time/space inference per token, improving efficiency in tasks like language modeling.

Conclusion: SpectraLDS preserves accuracy while significantly enhancing inference efficiency, making it practical for real-world sequence prediction tasks.

Abstract: We present the first provable method for identifying symmetric linear
dynamical systems (LDS) with accuracy guarantees that are independent of the
systems' state dimension or effective memory. Our approach builds upon recent
work that represents symmetric LDSs as convolutions learnable via fixed
spectral transformations. We show how to invert this representation, thereby
recovering an LDS model from its spectral transform and yielding an end-to-end
convex optimization procedure. This distillation preserves predictive accuracy
while enabling constant-time and constant-space inference per token,
independent of sequence length. We evaluate our method, SpectraLDS, as a
component in sequence prediction architectures and demonstrate that accuracy is
preserved while inference efficiency is improved on tasks such as language
modeling.

</details>


### [658] [FastCAV: Efficient Computation of Concept Activation Vectors for Explaining Deep Neural Networks](https://arxiv.org/pdf/2505.17883)
*Laines Schmalwasser, Niklas Penzel, Joachim Denzler, Julia Niebling*

Main category: cs.LG

TL;DR: FastCAV accelerates Concept Activation Vectors (CAVs) extraction by up to 63.6x, maintaining performance while reducing computational cost.


<details>
  <summary>Details</summary>
Motivation: Existing CAV computation methods are computationally expensive, limiting scalability in large-scale models.

Method: Introduces FastCAV, a novel approach with theoretical foundations and empirical validation, equivalent to SVM-based methods under certain assumptions.

Result: FastCAV achieves up to 63.6x speedup (avg. 46.4x) with similar performance and stability.

Conclusion: FastCAV enables efficient concept-based analysis of deep models, demonstrated by tracking concept evolution during training.

Abstract: Concepts such as objects, patterns, and shapes are how humans understand the
world. Building on this intuition, concept-based explainability methods aim to
study representations learned by deep neural networks in relation to
human-understandable concepts. Here, Concept Activation Vectors (CAVs) are an
important tool and can identify whether a model learned a concept or not.
However, the computational cost and time requirements of existing CAV
computation pose a significant challenge, particularly in large-scale,
high-dimensional architectures. To address this limitation, we introduce
FastCAV, a novel approach that accelerates the extraction of CAVs by up to
63.6x (on average 46.4x). We provide a theoretical foundation for our approach
and give concrete assumptions under which it is equivalent to established
SVM-based methods. Our empirical results demonstrate that CAVs calculated with
FastCAV maintain similar performance while being more efficient and stable. In
downstream applications, i.e., concept-based explanation methods, we show that
FastCAV can act as a replacement leading to equivalent insights. Hence, our
approach enables previously infeasible investigations of deep models, which we
demonstrate by tracking the evolution of concepts during model training.

</details>


### [659] [Best Group Identification in Multi-Objective Bandits](https://arxiv.org/pdf/2505.17869)
*Mohammad Shahverdikondori, Mohammad Reza Badri, Negar Kiyavash*

Main category: cs.LG

TL;DR: The paper introduces the Best Group Identification problem in multi-objective multi-armed bandits, proposing algorithms for Pareto optimal and linear weighted sum settings with sample complexity bounds and empirical validation.


<details>
  <summary>Details</summary>
Motivation: To address the challenge of identifying optimal groups in multi-objective multi-armed bandits with vector-valued rewards, focusing on Pareto optimality and linear weighted sums.

Method: Proposes elimination-based algorithms for group Pareto set identification and linear best group identification, with theoretical sample complexity bounds.

Result: Upper bounds on sample complexity for the algorithms and lower bounds for any correct algorithm, supported by strong empirical performance.

Conclusion: The proposed algorithms effectively solve the Best Group Identification problem, with theoretical guarantees and practical validation.

Abstract: We introduce the Best Group Identification problem in a multi-objective
multi-armed bandit setting, where an agent interacts with groups of arms with
vector-valued rewards. The performance of a group is determined by an
efficiency vector which represents the group's best attainable rewards across
different dimensions. The objective is to identify the set of optimal groups in
the fixed-confidence setting. We investigate two key formulations: group Pareto
set identification, where efficiency vectors of optimal groups are Pareto
optimal and linear best group identification, where each reward dimension has a
known weight and the optimal group maximizes the weighted sum of its efficiency
vector's entries. For both settings, we propose elimination-based algorithms,
establish upper bounds on their sample complexity, and derive lower bounds that
apply to any correct algorithm. Through numerical experiments, we demonstrate
the strong empirical performance of the proposed algorithms.

</details>


### [660] [BLAST: Balanced Sampling Time Series Corpus for Universal Forecasting Models](https://arxiv.org/pdf/2505.17871)
*Zezhi Shao, Yujie Li, Fei Wang, Chengqing Yu, Yisong Fu, Tangwen Qian, Bin Xu, Boyu Diao, Yongjun Xu, Xueqi Cheng*

Main category: cs.LG

TL;DR: BLAST introduces a balanced pre-training corpus to enhance data diversity in universal time series forecasting, improving model performance and efficiency.


<details>
  <summary>Details</summary>
Motivation: Existing time series datasets have biases and imbalances, limiting model generalization and performance.

Method: BLAST uses 321 billion observations, statistical metrics, grid-based clustering, and grid sampling/mixup for balanced pattern coverage.

Result: Models pre-trained on BLAST achieve state-of-the-art performance with fewer resources.

Conclusion: Data diversity is crucial for efficient training and better performance in universal forecasting.

Abstract: The advent of universal time series forecasting models has revolutionized
zero-shot forecasting across diverse domains, yet the critical role of data
diversity in training these models remains underexplored. Existing large-scale
time series datasets often suffer from inherent biases and imbalanced
distributions, leading to suboptimal model performance and generalization. To
address this gap, we introduce BLAST, a novel pre-training corpus designed to
enhance data diversity through a balanced sampling strategy. First, BLAST
incorporates 321 billion observations from publicly available datasets and
employs a comprehensive suite of statistical metrics to characterize time
series patterns. Then, to facilitate pattern-oriented sampling, the data is
implicitly clustered using grid-based partitioning. Furthermore, by integrating
grid sampling and grid mixup techniques, BLAST ensures a balanced and
representative coverage of diverse patterns. Experimental results demonstrate
that models pre-trained on BLAST achieve state-of-the-art performance with a
fraction of the computational resources and training tokens required by
existing methods. Our findings highlight the pivotal role of data diversity in
improving both training efficiency and model performance for the universal
forecasting task.

</details>


### [661] [Semi-Supervised Multi-Label Feature Selection with Consistent Sparse Graph Learning](https://arxiv.org/pdf/2505.17875)
*Yan Zhong, Xingyu Wu, Xinping Zhao, Li Zhang, Xinyuan Song, Lei Shi, Bingbing Jiang*

Main category: cs.LG

TL;DR: The paper proposes SGMFS, a method for multi-label semi-supervised feature selection, addressing challenges of label correlation and suboptimal graph structures in existing methods.


<details>
  <summary>Details</summary>
Motivation: Traditional feature selection methods are inadequate for multi-label data, and existing semi-supervised methods struggle with label correlation and graph structure issues.

Method: SGMFS learns a low-dimensional label subspace and adaptively constructs a similarity graph by sparse reconstruction in both label and learned subspaces.

Result: Extensive experiments show SGMFS outperforms existing methods in feature selection performance.

Conclusion: SGMFS effectively addresses key challenges in multi-label semi-supervised feature selection, enhancing performance through space consistency and label correlation learning.

Abstract: In practical domains, high-dimensional data are usually associated with
diverse semantic labels, whereas traditional feature selection methods are
designed for single-label data. Moreover, existing multi-label methods
encounter two main challenges in semi-supervised scenarios: (1). Most
semi-supervised methods fail to evaluate the label correlations without enough
labeled samples, which are the critical information of multi-label feature
selection, making label-specific features discarded. (2). The similarity graph
structure directly derived from the original feature space is suboptimal for
multi-label problems in existing graph-based methods, leading to unreliable
soft labels and degraded feature selection performance. To overcome them, we
propose a consistent sparse graph learning method for multi-label
semi-supervised feature selection (SGMFS), which can enhance the feature
selection performance by maintaining space consistency and learning label
correlations in semi-supervised scenarios. Specifically, for Challenge (1),
SGMFS learns a low-dimensional and independent label subspace from the
projected features, which can compatibly cross multiple labels and effectively
achieve the label correlations. For Challenge (2), instead of constructing a
fixed similarity graph for semi-supervised learning, SGMFS thoroughly explores
the intrinsic structure of the data by performing sparse reconstruction of
samples in both the label space and the learned subspace simultaneously. In
this way, the similarity graph can be adaptively learned to maintain the
consistency between label space and the learned subspace, which can promote
propagating proper soft labels for unlabeled samples, facilitating the ultimate
feature selection. An effective solution with fast convergence is designed to
optimize the objective function. Extensive experiments validate the superiority
of SGMFS.

</details>


### [662] [NeuroTrails: Training with Dynamic Sparse Heads as the Key to Effective Ensembling](https://arxiv.org/pdf/2505.17909)
*Bram Grooten, Farid Hasanov, Chenxiang Zhang, Qiao Xiao, Boqian Wu, Zahra Atashgahi, Ghada Sokar, Shiwei Liu, Lu Yin, Elena Mocanu, Mykola Pechenizkiy, Decebal Constantin Mocanu*

Main category: cs.LG

TL;DR: NeuroTrails introduces a sparse multi-head architecture with dynamic topology to improve ensemble performance efficiently, reducing computational costs while maintaining accuracy and robustness.


<details>
  <summary>Details</summary>
Motivation: Address the computational overhead of traditional model ensembles and state-of-the-art methods by proposing a resource-efficient alternative.

Method: NeuroTrails, a model-agnostic sparse multi-head architecture with dynamically evolving topology, induces prediction diversity through dynamic sparsity.

Result: Improved accuracy and robustness in tasks like computer vision and language, demonstrated on ResNet-50/ImageNet and LLaMA-350M/C4, with fewer parameters.

Conclusion: NeuroTrails achieves ensemble-like performance efficiently, offering a practical solution for resource-constrained scenarios.

Abstract: Model ensembles have long been a cornerstone for improving generalization and
robustness in deep learning. However, their effectiveness often comes at the
cost of substantial computational overhead. To address this issue,
state-of-the-art methods aim to replicate ensemble-class performance without
requiring multiple independently trained networks. Unfortunately, these
algorithms often still demand considerable compute at inference. In response to
these limitations, we introduce $\textbf{NeuroTrails}$, a sparse multi-head
architecture with dynamically evolving topology. This unexplored model-agnostic
training paradigm improves ensemble performance while reducing the required
resources. We analyze the underlying reason for its effectiveness and observe
that the various neural trails induced by dynamic sparsity attain a
$\textit{Goldilocks zone}$ of prediction diversity. NeuroTrails displays
efficacy with convolutional and transformer-based architectures on computer
vision and language tasks. Experiments on ResNet-50/ImageNet, LLaMA-350M/C4,
among many others, demonstrate increased accuracy and stronger robustness in
zero-shot generalization, while requiring significantly fewer parameters.

</details>


### [663] [Universal Domain Adaptation Benchmark for Time Series Data Representation](https://arxiv.org/pdf/2505.17899)
*Romain Mussard, Fannia Pacheco, Maxime Berar, Gilles Gasso, Paul Honeine*

Main category: cs.LG

TL;DR: The paper explores Universal Domain Adaptation (UniDA) for time series data, evaluating state-of-the-art backbones to improve generalization and robustness.


<details>
  <summary>Details</summary>
Motivation: Deep learning models struggle with generalization and robustness in time series data due to variability, prompting the need for UniDA.

Method: The study implements and compares TS backbones in a UniDA framework, proposing an evaluation protocol for robustness and generalization.

Result: Results emphasize the importance of backbone selection in UniDA performance and provide robustness analysis across datasets.

Conclusion: The framework aids practitioners in extending UniDA advancements for time series data, highlighting backbone influence.

Abstract: Deep learning models have significantly improved the ability to detect
novelties in time series (TS) data. This success is attributed to their strong
representation capabilities. However, due to the inherent variability in TS
data, these models often struggle with generalization and robustness. To
address this, a common approach is to perform Unsupervised Domain Adaptation,
particularly Universal Domain Adaptation (UniDA), to handle domain shifts and
emerging novel classes. While extensively studied in computer vision, UniDA
remains underexplored for TS data. This work provides a comprehensive
implementation and comparison of state-of-the-art TS backbones in a UniDA
framework. We propose a reliable protocol to evaluate their robustness and
generalization across different domains. The goal is to provide practitioners
with a framework that can be easily extended to incorporate future advancements
in UniDA and TS architectures. Our results highlight the critical influence of
backbone selection in UniDA performance and enable a robustness analysis across
various datasets and architectures.

</details>


### [664] [Evolving Machine Learning: A Survey](https://arxiv.org/pdf/2505.17902)
*Ignacio Cabrera Martin, Subhaditya Mukherjee, Almas Baimagambetov, Joaquin Vanschoren, Nikolaos Polatidis*

Main category: cs.LG

TL;DR: A survey on Evolving Machine Learning (EML) addressing challenges like data drift, concept drift, and more, reviewing 120+ studies and highlighting adaptive methods for real-time learning.


<details>
  <summary>Details</summary>
Motivation: Traditional ML struggles with dynamic environments; EML offers continuous adaptation, necessitating a comprehensive review of current methods and gaps.

Method: Systematic review of 120+ studies, categorizing methods (supervised, unsupervised, semi-supervised), evaluating metrics, datasets, and applications.

Result: Identifies adaptive neural architectures, meta-learning, and ensemble strategies as key solutions, while mapping limitations and effectiveness of current techniques.

Conclusion: The survey guides future research and practice in EML, emphasizing robustness, ethics, and scalability for real-world deployment.

Abstract: In an era defined by rapid data evolution, traditional machine learning (ML)
models often fall short in adapting to dynamic environments. Evolving Machine
Learning (EML) has emerged as a critical paradigm, enabling continuous learning
and adaptation in real-time data streams. This survey presents a comprehensive
analysis of EML, focusing on five core challenges: data drift, concept drift,
catastrophic forgetting, skewed learning, and network adaptation. We
systematically review over 120 studies, categorizing state-of-the-art methods
across supervised, unsupervised, and semi-supervised approaches. The survey
explores diverse evaluation metrics, benchmark datasets, and real-world
applications, offering a comparative lens on the effectiveness and limitations
of current techniques. Additionally, we highlight the growing role of adaptive
neural architectures, meta-learning, and ensemble strategies in addressing
evolving data complexities. By synthesizing insights from recent literature,
this work not only maps the current landscape of EML but also identifies
critical gaps and opportunities for future research. Our findings aim to guide
researchers and practitioners in developing robust, ethical, and scalable EML
systems for real-world deployment.

</details>


### [665] [LLM Meeting Decision Trees on Tabular Data](https://arxiv.org/pdf/2505.17918)
*Hangting Ye, Jinmeng Li, He Zhao, Dandan Guo, Yi Chang*

Main category: cs.LG

TL;DR: The paper introduces DeLTa, a method integrating LLMs with tabular data via decision tree rules, avoiding serialization and fine-tuning, achieving SOTA performance.


<details>
  <summary>Details</summary>
Motivation: Existing LLM-based methods for tabular data suffer from universal applicability, privacy risks, and scalability issues, prompting a need for a better approach.

Method: DeLTa uses LLMs to enhance decision tree rules, avoiding tabular data serialization and fine-tuning, and includes a calibration method for error correction.

Result: Extensive experiments show DeLTa achieves state-of-the-art performance on diverse tabular benchmarks.

Conclusion: DeLTa offers a scalable, privacy-preserving, and effective solution for integrating LLMs with tabular data.

Abstract: Tabular data have been playing a vital role in diverse real-world fields,
including healthcare, finance, etc. With the recent success of Large Language
Models (LLMs), early explorations of extending LLMs to the domain of tabular
data have been developed. Most of these LLM-based methods typically first
serialize tabular data into natural language descriptions, and then tune LLMs
or directly infer on these serialized data. However, these methods suffer from
two key inherent issues: (i) data perspective: existing data serialization
methods lack universal applicability for structured tabular data, and may pose
privacy risks through direct textual exposure, and (ii) model perspective: LLM
fine-tuning methods struggle with tabular data, and in-context learning
scalability is bottle-necked by input length constraints (suitable for few-shot
learning). This work explores a novel direction of integrating LLMs into
tabular data throughough logical decision tree rules as intermediaries,
proposes a decision tree enhancer with LLM-derived rule for tabular prediction,
DeLTa. The proposed DeLTa avoids tabular data serialization, and can be applied
to full data learning setting without LLM fine-tuning. Specifically, we
leverage the reasoning ability of LLMs to redesign an improved rule given a set
of decision tree rules. Furthermore, we provide a calibration method for
original decision trees via new generated rule by LLM, which approximates the
error correction vector to steer the original decision tree predictions in the
direction of ``errors'' reducing. Finally, extensive experiments on diverse
tabular benchmarks show that our method achieves state-of-the-art performance.

</details>


### [666] [KITINet: Kinetics Theory Inspired Network Architectures with PDE Simulation Approaches](https://arxiv.org/pdf/2505.17919)
*Mingquan Feng, Yifan Fu, Tongcheng Zhang, Yu Jiang, Yixin Huang, Junchi Yan*

Main category: cs.LG

TL;DR: KITINet introduces a physics-inspired residual module using particle dynamics and PDE simulation, improving performance without significant computational cost.


<details>
  <summary>Details</summary>
Motivation: To provide a principled, physics-based alternative to heuristic residual connections in neural networks.

Method: Models feature updates as stochastic particle evolution using a discretized solver for the Boltzmann transport equation (BTE).

Result: Consistent improvements in PDE operator tasks, image classification (CIFAR-10/100), and text classification (IMDb/SNLI) with minimal FLOPs increase.

Conclusion: KITINet offers a physics-informed approach to feature refinement, enhancing performance while maintaining efficiency.

Abstract: Despite the widely recognized success of residual connections in modern
neural networks, their design principles remain largely heuristic. This paper
introduces KITINet (Kinetics Theory Inspired Network), a novel architecture
that reinterprets feature propagation through the lens of non-equilibrium
particle dynamics and partial differential equation (PDE) simulation. At its
core, we propose a residual module that models feature updates as the
stochastic evolution of a particle system, numerically simulated via a
discretized solver for the Boltzmann transport equation (BTE). This formulation
mimics particle collisions and energy exchange, enabling adaptive feature
refinement via physics-informed interactions. Additionally, we reveal that this
mechanism induces network parameter condensation during training, where
parameters progressively concentrate into a sparse subset of dominant channels.
Experiments on scientific computation (PDE operator), image classification
(CIFAR-10/100), and text classification (IMDb/SNLI) show consistent
improvements over classic network baselines, with negligible increase of FLOPs.

</details>


### [667] [Predicting Length of Stay in Neurological ICU Patients Using Classical Machine Learning and Neural Network Models: A Benchmark Study on MIMIC-IV](https://arxiv.org/pdf/2505.17929)
*Alexander Gabitashvili, Philipp Kellmeyer*

Main category: cs.LG

TL;DR: The study evaluates ML models for predicting ICU length of stay (LOS) in neurological patients, finding Random Forest and BERT effective for static and time-series data, respectively.


<details>
  <summary>Details</summary>
Motivation: To assess ML techniques for ICU LOS prediction in neurological patients, aiding resource management and care.

Method: Evaluated classic ML (KNN, Random Forest, XGBoost, CatBoost) and neural networks (LSTM, BERT, TFT) on MIMIC-IV data, categorizing LOS into three groups.

Result: Random Forest excelled on static data (accuracy 0.68), while BERT outperformed on time-series data (accuracy 0.80).

Conclusion: ML models show promise for ICU LOS prediction in neurological patients, with Random Forest and BERT being top performers.

Abstract: Intensive care unit (ICU) is a crucial hospital department that handles
life-threatening cases. Nowadays machine learning (ML) is being leveraged in
healthcare ubiquitously. In recent years, management of ICU became one of the
most significant parts of the hospital functionality (largely but not only due
to the worldwide COVID-19 pandemic). This study explores multiple ML approaches
for predicting LOS in ICU specifically for the patients with neurological
diseases based on the MIMIC-IV dataset. The evaluated models include classic ML
algorithms (K-Nearest Neighbors, Random Forest, XGBoost and CatBoost) and
Neural Networks (LSTM, BERT and Temporal Fusion Transformer). Given that LOS
prediction is often framed as a classification task, this study categorizes LOS
into three groups: less than two days, less than a week, and a week or more. As
the first ML-based approach targeting LOS prediction for neurological disorder
patients, this study does not aim to outperform existing methods but rather to
assess their effectiveness in this specific context. The findings provide
insights into the applicability of ML techniques for improving ICU resource
management and patient care. According to the results, Random Forest model
proved to outperform others on static, achieving an accuracy of 0.68, a
precision of 0.68, a recall of 0.68, and F1-score of 0.67. While BERT model
outperformed LSTM model on time-series data with an accuracy of 0.80, a
precision of 0.80, a recall of 0.80 and F1-score 0.80.

</details>


### [668] [Understanding Gated Neurons in Transformers from Their Input-Output Functionality](https://arxiv.org/pdf/2505.17936)
*Sebastian Gerstner, Hinrich Schütze*

Main category: cs.LG

TL;DR: The paper explores interactions between input and output weights in MLP neurons of language models, introducing enrichment and depletion neurons. It finds enrichment neurons dominate early layers, aiding concept representation.


<details>
  <summary>Details</summary>
Motivation: Prior research focused on neuron activations and output weights but ignored input-output interactions. This gap is addressed to better understand neuron behavior.

Method: Cosine similarity between input and output weights is analyzed across 12 models to classify neurons as enrichment or depletion types.

Result: Enrichment neurons dominate early-middle layers, while later layers favor depletion neurons, suggesting their role in concept representation.

Conclusion: The input-output perspective complements existing analyses, highlighting enrichment neurons' role in factual recall.

Abstract: Interpretability researchers have attempted to understand MLP neurons of
language models based on both the contexts in which they activate and their
output weight vectors. They have paid little attention to a complementary
aspect: the interactions between input and output. For example, when neurons
detect a direction in the input, they might add much the same direction to the
residual stream ("enrichment neurons") or reduce its presence ("depletion
neurons"). We address this aspect by examining the cosine similarity between
input and output weights of a neuron. We apply our method to 12 models and find
that enrichment neurons dominate in early-middle layers whereas later layers
tend more towards depletion. To explain this finding, we argue that enrichment
neurons are largely responsible for enriching concept representations, one of
the first steps of factual recall. Our input-output perspective is a complement
to activation-dependent analyses and to approaches that treat input and output
separately.

</details>


### [669] [SVD-Free Low-Rank Adaptive Gradient Optimization for Large Language Models](https://arxiv.org/pdf/2505.17967)
*Ionut-Vlad Modoranu, Mher Safaryan, Erik Schultheis, Dan Alistarh*

Main category: cs.LG

TL;DR: A two-step method using DCT-based predefined orthogonal matrices for efficient low-rank gradient projections in LLMs, reducing memory and computational costs compared to SVD.


<details>
  <summary>Details</summary>
Motivation: To address the computational and memory inefficiencies of SVD-based gradient projections in large language models.

Method: Constructs a DCT-based orthogonal basis and adaptively selects columns aligned with gradients, avoiding full projection matrix storage.

Result: Matches SVD performance with faster runtime and lower memory usage in pre-training and fine-tuning tasks.

Conclusion: The method offers a computationally efficient and memory-friendly alternative to SVD for low-rank optimization in LLMs.

Abstract: Low-rank optimization has emerged as a promising direction in training large
language models (LLMs) to reduce the memory usage of adaptive optimizers by
constraining learning to a lower-dimensional space. Prior work typically
projects gradients of linear layers using approaches based on Singular Value
Decomposition (SVD). However, applying SVD-based procedures individually to
each layer in large models is computationally expensive and incurs additional
memory costs due to storing the projection matrices. In this work, we propose a
computationally efficient and conceptually simple two-step procedure to
approximate SVD-based gradient projections into lower-dimensional spaces.
First, we construct a complete orthogonal basis using predefined orthogonal
matrices of the Discrete Cosine Transform (DCT). Second, we adaptively select
basis columns based on their alignment with the gradient of each layer. Each
projection matrix in our method is obtained via a single matrix multiplication
followed by a lightweight sorting step to identify the most relevant basis
vectors. Due to the predefined nature of the orthogonal bases, they are
computed once at the start of training. During training, we store only the
indices of the selected columns, avoiding the need to store full projection
matrices for each layer. Our numerical experiments on both pre-training and
fine-tuning tasks demonstrate the effectiveness of our dual strategy in
approximating optimal low-rank projections, matching the performance of costly
SVD-based methods while achieving faster runtime and reduced memory usage.

</details>


### [670] [Directed Semi-Simplicial Learning with Applications to Brain Activity Decoding](https://arxiv.org/pdf/2505.17939)
*Manuel Lecha, Andrea Cavallo, Francesca Dominici, Ran Levi, Alessio Del Bue, Elvin Isufi, Pietro Morerio, Claudio Battiloro*

Main category: cs.LG

TL;DR: The paper introduces Semi-Simplicial Neural Networks (SSNs) to address limitations of GNNs and TDL in capturing directed higher-order interactions, achieving state-of-the-art performance in brain dynamics tasks.


<details>
  <summary>Details</summary>
Motivation: Existing TDL models fail to capture directed higher-order interactions common in complex systems like brain networks, limiting their applicability.

Method: The authors propose SSNs, which operate on semi-simplicial sets to encode directed higher-order motifs, and Routing-SSNs for scalability.

Result: SSNs outperform existing models by up to 50% in accuracy for brain dynamics tasks and show competitive performance in standard tasks.

Conclusion: SSNs demonstrate the potential of principled topological models for structured data, particularly in brain dynamics, and offer a scalable solution.

Abstract: Graph Neural Networks (GNNs) excel at learning from pairwise interactions but
often overlook multi-way and hierarchical relationships. Topological Deep
Learning (TDL) addresses this limitation by leveraging combinatorial
topological spaces. However, existing TDL models are restricted to undirected
settings and fail to capture the higher-order directed patterns prevalent in
many complex systems, e.g., brain networks, where such interactions are both
abundant and functionally significant. To fill this gap, we introduce
Semi-Simplicial Neural Networks (SSNs), a principled class of TDL models that
operate on semi-simplicial sets -- combinatorial structures that encode
directed higher-order motifs and their directional relationships. To enhance
scalability, we propose Routing-SSNs, which dynamically select the most
informative relations in a learnable manner. We prove that SSNs are strictly
more expressive than standard graph and TDL models. We then introduce a new
principled framework for brain dynamics representation learning, grounded in
the ability of SSNs to provably recover topological descriptors shown to
successfully characterize brain activity. Empirically, SSNs achieve
state-of-the-art performance on brain dynamics classification tasks,
outperforming the second-best model by up to 27%, and message passing GNNs by
up to 50% in accuracy. Our results highlight the potential of principled
topological models for learning from structured brain data, establishing a
unique real-world case study for TDL. We also test SSNs on standard node
classification and edge regression tasks, showing competitive performance. We
will make the code and data publicly available.

</details>


### [671] [Are Large Language Models Reliable AI Scientists? Assessing Reverse-Engineering of Black-Box Systems](https://arxiv.org/pdf/2505.17968)
*Jiayi Geng, Howard Chen, Dilip Arumugam, Thomas L. Griffiths*

Main category: cs.LG

TL;DR: LLMs struggle to reverse-engineer black-box systems from passive observations but improve with active interventions, escaping common failure modes like overcomplication and overlooking.


<details>
  <summary>Details</summary>
Motivation: To understand how well LLMs can identify the structure of black-box systems, a key step toward autonomous AI researchers.

Method: Tested LLMs on three black-box systems (Program, Formal Language, Math Equation) using passive observation and active intervention.

Result: LLMs plateau with passive data but improve with active queries, refining beliefs and avoiding failure modes.

Conclusion: Active intervention enhances LLMs' reverse-engineering, supporting their role in scientific discovery.

Abstract: Using AI to create autonomous researchers has the potential to accelerate
scientific discovery. A prerequisite for this vision is understanding how well
an AI model can identify the underlying structure of a black-box system from
its behavior. In this paper, we explore how well a large language model (LLM)
learns to identify a black-box function from passively observed versus actively
collected data. We investigate the reverse-engineering capabilities of LLMs
across three distinct types of black-box systems, each chosen to represent
different problem domains where future autonomous AI researchers may have
considerable impact: Program, Formal Language, and Math Equation. Through
extensive experiments, we show that LLMs fail to extract information from
observations, reaching a performance plateau that falls short of the ideal of
Bayesian inference. However, we demonstrate that prompting LLMs to not only
observe but also intervene -- actively querying the black-box with specific
inputs to observe the resulting output -- improves performance by allowing LLMs
to test edge cases and refine their beliefs. By providing the intervention data
from one LLM to another, we show that this improvement is partly a result of
engaging in the process of generating effective interventions, paralleling
results in the literature on human learning. Further analysis reveals that
engaging in intervention can help LLMs escape from two common failure modes:
overcomplication, where the LLM falsely assumes prior knowledge about the
black-box, and overlooking, where the LLM fails to incorporate observations.
These insights provide practical guidance for helping LLMs more effectively
reverse-engineer black-box systems, supporting their use in making new
discoveries.

</details>


### [672] [VeriThinker: Learning to Verify Makes Reasoning Model Efficient](https://arxiv.org/pdf/2505.17941)
*Zigeng Chen, Xinyin Ma, Gongfan Fang, Ruonan Yu, Xinchao Wang*

Main category: cs.LG

TL;DR: VeriThinker reduces unnecessary reasoning steps in Large Reasoning Models (LRMs) by fine-tuning them on an auxiliary verification task, cutting costs while maintaining accuracy.


<details>
  <summary>Details</summary>
Motivation: LRMs often overthink, leading to lengthy reasoning chains and high inference costs. VeriThinker aims to compress these chains without losing accuracy.

Method: Fine-tunes LRMs on an auxiliary verification task to make them more discerning about necessary reasoning steps, suppressing overthinking.

Result: Reduces reasoning tokens significantly (e.g., from 3790 to 2125 on MATH500) while improving accuracy (e.g., 0.8% gain).

Conclusion: VeriThinker effectively compresses reasoning chains, lowers costs, and maintains or improves accuracy, with potential for zero-shot generalization.

Abstract: Large Reasoning Models (LRMs) excel at complex tasks using Chain-of-Thought
(CoT) reasoning. However, their tendency to overthinking leads to unnecessarily
lengthy reasoning chains, dramatically increasing inference costs. To mitigate
this issue, we introduce VeriThinker, a novel approach for CoT compression.
Unlike conventional methods that fine-tune LRMs directly on the original
reasoning task using synthetic concise CoT data, we innovatively fine-tune the
model solely through an auxiliary verification task. By training LRMs to
accurately verify the correctness of CoT solutions, the LRMs inherently become
more discerning about the necessity of subsequent self-reflection steps,
thereby effectively suppressing overthinking. Extensive experiments validate
that VeriThinker substantially reduces reasoning chain lengths while
maintaining or even slightly improving accuracy. When applied to
DeepSeek-R1-Distill-Qwen-7B, our approach reduces reasoning tokens on MATH500
from 3790 to 2125 while improving accuracy by 0.8% (94.0% to 94.8%), and on
AIME25, tokens decrease from 14321 to 10287 with a 2.1% accuracy gain (38.7% to
40.8%). Additionally, our experiments demonstrate that VeriThinker can also be
zero-shot generalized to speculative reasoning. Code is available at
https://github.com/czg1225/VeriThinker

</details>


### [673] [Generalized Fisher-Weighted SVD: Scalable Kronecker-Factored Fisher Approximation for Compressing Large Language Models](https://arxiv.org/pdf/2505.17974)
*Viktoriia Chekalina, Daniil Moskovskiy, Daria Cherniuk, Maxim Kurkin, Andrey Kuznetsov, Evgeny Frolov*

Main category: cs.LG

TL;DR: GFWSVD is a post-training LLM compression method that improves accuracy by incorporating full Fisher information, outperforming diagonal-approximation baselines.


<details>
  <summary>Details</summary>
Motivation: Diagonal approximations of Fisher information ignore parameter correlations, reducing downstream performance. GFWSVD addresses this limitation.

Method: Proposes GFWSVD, using a scalable Kronecker-factored approximation for the full Fisher information matrix.

Result: Outperforms FWSVD, SVD-LLM, and ASVD by 5%, 3%, and 6% respectively at a 20 compression rate on MMLU.

Conclusion: GFWSVD provides a more accurate and efficient LLM compression method by leveraging full Fisher information.

Abstract: The Fisher information is a fundamental concept for characterizing the
sensitivity of parameters in neural networks. However, leveraging the full
observed Fisher information is too expensive for large models, so most methods
rely on simple diagonal approximations. While efficient, this approach ignores
parameter correlations, often resulting in reduced performance on downstream
tasks. In this work, we mitigate these limitations and propose Generalized
Fisher-Weighted SVD (GFWSVD), a post-training LLM compression technique that
accounts for both diagonal and off-diagonal elements of the Fisher information
matrix, providing a more accurate reflection of parameter importance. To make
the method tractable, we introduce a scalable adaptation of the
Kronecker-factored approximation algorithm for the observed Fisher information.
We demonstrate the effectiveness of our method on LLM compression, showing
improvements over existing compression baselines. For example, at a 20
compression rate on the MMLU benchmark, our method outperforms FWSVD, which is
based on a diagonal approximation of the Fisher information, by 5 percent,
SVD-LLM by 3 percent, and ASVD by 6 percent compression rate.

</details>


### [674] [A Principled Bayesian Framework for Training Binary and Spiking Neural Networks](https://arxiv.org/pdf/2505.17962)
*James A. Walker, Moein Khajehnejad, Adeel Razi*

Main category: cs.LG

TL;DR: A Bayesian framework for training binary and spiking neural networks achieves state-of-the-art performance without normalization layers, using importance-weighted straight-through estimators and variational inference.


<details>
  <summary>Details</summary>
Motivation: Current surrogate gradient methods are heuristic and hyperparameter-sensitive; this work aims for a probabilistic, end-to-end gradient-based solution.

Method: Introduces importance-weighted straight-through (IW-ST) estimators and Spiking Bayesian Neural Networks (SBNNs) for variational inference, minimizing gradient bias and regularizing parameters.

Result: Matches or exceeds existing methods on CIFAR-10, DVS Gesture, and SHD datasets without normalization or hand-tuned gradients.

Conclusion: The Bayesian framework enables robust training of deep networks without normalization, addressing bias and noise effectively.

Abstract: We propose a Bayesian framework for training binary and spiking neural
networks that achieves state-of-the-art performance without normalisation
layers. Unlike commonly used surrogate gradient methods -- often heuristic and
sensitive to hyperparameter choices -- our approach is grounded in a
probabilistic model of noisy binary networks, enabling fully end-to-end
gradient-based optimisation. We introduce importance-weighted straight-through
(IW-ST) estimators, a unified class generalising straight-through and
relaxation-based estimators. We characterise the bias-variance trade-off in
this family and derive a bias-minimising objective implemented via an auxiliary
loss. Building on this, we introduce Spiking Bayesian Neural Networks (SBNNs),
a variational inference framework that uses posterior noise to train Binary and
Spiking Neural Networks with IW-ST. This Bayesian approach minimises gradient
bias, regularises parameters, and introduces dropout-like noise. By linking
low-bias conditions, vanishing gradients, and the KL term, we enable training
of deep residual networks without normalisation. Experiments on CIFAR-10, DVS
Gesture, and SHD show our method matches or exceeds existing approaches without
normalisation or hand-tuned gradients.

</details>


### [675] [ADLGen: Synthesizing Symbolic, Event-Triggered Sensor Sequences for Human Activity Modeling](https://arxiv.org/pdf/2505.17987)
*Weihang You, Hanqi Jiang, Zishuai Liu, Zihang Xie, Tianming Liu, Jin Lu, Fei Dou*

Main category: cs.LG

TL;DR: ADLGen is a generative framework using Transformers and LLMs to synthesize realistic sensor sequences for ADL data, outperforming baselines in fidelity and utility.


<details>
  <summary>Details</summary>
Motivation: Challenges in collecting real-world ADL data due to privacy, cost, and sparsity motivate the need for synthetic data generation.

Method: ADLGen combines a decoder-only Transformer with symbolic encoding and LLM-based refinement for coherent, plausible sensor sequences.

Result: ADLGen excels in statistical fidelity, semantic richness, and activity recognition compared to baselines.

Conclusion: ADLGen provides a scalable, privacy-preserving solution for synthetic ADL data generation.

Abstract: Real world collection of Activities of Daily Living data is challenging due
to privacy concerns, costly deployment and labeling, and the inherent sparsity
and imbalance of human behavior. We present ADLGen, a generative framework
specifically designed to synthesize realistic, event triggered, and symbolic
sensor sequences for ambient assistive environments. ADLGen integrates a
decoder only Transformer with sign based symbolic temporal encoding, and a
context and layout aware sampling mechanism to guide generation toward
semantically rich and physically plausible sensor event sequences. To enhance
semantic fidelity and correct structural inconsistencies, we further
incorporate a large language model into an automatic generate evaluate refine
loop, which verifies logical, behavioral, and temporal coherence and generates
correction rules without manual intervention or environment specific tuning.
Through comprehensive experiments with novel evaluation metrics, ADLGen is
shown to outperform baseline generators in statistical fidelity, semantic
richness, and downstream activity recognition, offering a scalable and
privacy-preserving solution for ADL data synthesis.

</details>


### [676] [Towards Revealing the Effectiveness of Small-Scale Fine-tuning in R1-style Reinforcement Learning](https://arxiv.org/pdf/2505.17988)
*Yutong Chen, Jiandong Gao, Ji Wu*

Main category: cs.LG

TL;DR: Re-distillation, a technique combining small-scale distillation from RL-trained policies, improves efficiency and matches RL performance with fewer samples.


<details>
  <summary>Details</summary>
Motivation: To understand the unclear mechanism behind rule-based RL and address the inefficiency of small-scale SFT in enhancing reasoning capabilities.

Method: Proposed an analytical framework comparing SFT and RL efficiency, introduced Re-distillation to fine-tune models using small-scale distillation from RL-trained policies.

Result: Re-distilled models matched RL performance with fewer samples (e.g., 1K SFT samples on K&K dataset, 500 on MATH).

Conclusion: Re-distillation explains phenomena in R1-style RL, offering a more efficient alternative to traditional methods.

Abstract: R1-style Reinforcement Learning (RL) significantly enhances Large Language
Models' reasoning capabilities, yet the mechanism behind rule-based RL remains
unclear. We found that small-scale SFT has significant influence on RL but
shows poor efficiency. To explain our observations, we propose an analytical
framework and compare the efficiency of SFT and RL by measuring sample effect.
Hypothetical analysis show that SFT efficiency is limited by training data.
Guided by our analysis, we propose Re-distillation, a technique that fine-tunes
pretrain model through small-scale distillation from the RL-trained policy.
Experiments on Knight & Knave and MATH datasets demonstrate re-distillation's
surprising efficiency: re-distilled models match RL performance with far fewer
samples and less computation. Empirical verification shows that sample effect
is a good indicator of performance improvements. As a result, on K&K dataset,
our re-distilled Qwen2.5-1.5B model surpasses DeepSeek-V3-0324 with only 1K SFT
samples. On MATH, Qwen2.5-1.5B fine-tuned with re-distilled 500 samples matches
its instruct-tuned variant without RL. Our work explains several interesting
phenomena in R1-style RL, shedding light on the mechanisms behind its empirical
success. Code is available at: https://github.com/on1262/deep-reasoning

</details>


### [677] [Outcome-based Reinforcement Learning to Predict the Future](https://arxiv.org/pdf/2505.17989)
*Benjamin Turtel, Danny Franklin, Kris Skotheim, Luke Hewitt, Philipp Schoenegger*

Main category: cs.LG

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: Reinforcement learning with verifiable rewards (RLVR) has boosted math and
coding in large language models, yet there has been little effort to extend
RLVR into messier, real-world domains like forecasting. One sticking point is
that outcome-based reinforcement learning for forecasting must learn from
binary, delayed, and noisy rewards, a regime where standard fine-tuning is
brittle. We show that outcome-only online RL on a 14B model can match
frontier-scale accuracy and surpass it in calibration and hypothetical
prediction market betting by adapting two leading algorithms, Group-Relative
Policy Optimisation (GRPO) and ReMax, to the forecasting setting. Our
adaptations remove per-question variance scaling in GRPO, apply
baseline-subtracted advantages in ReMax, hydrate training with 100k temporally
consistent synthetic questions, and introduce lightweight guard-rails that
penalise gibberish, non-English responses and missing rationales, enabling a
single stable pass over 110k events. Scaling ReMax to 110k questions and
ensembling seven predictions yields a 14B model that matches frontier baseline
o1 on accuracy on our holdout set (Brier = 0.193, p = 0.23) while beating it in
calibration (ECE = 0.042, p < 0.001). A simple trading rule turns this
calibration edge into \$127 of hypothetical profit versus \$92 for o1 (p =
0.037). This demonstrates that refined RLVR methods can convert small-scale
LLMs into potentially economically valuable forecasting tools, with
implications for scaling this to larger models.

</details>


### [678] [Towards Analyzing and Understanding the Limitations of VAPO: A Theoretical Perspective](https://arxiv.org/pdf/2505.17997)
*Jintian Shao, Yiming Cheng, Hongyi Huang, Beiwen Zhang, Zhiyu Wu, You Shan, Mingkai Zheng*

Main category: cs.LG

TL;DR: VAPO improves reinforcement learning for CoT reasoning in LLMs by addressing key challenges, but its theoretical foundations need deeper exploration.


<details>
  <summary>Details</summary>
Motivation: To enhance understanding of VAPO's mechanisms and limitations for future advancements in reasoning tasks.

Method: Theoretical exploration of VAPO's value function approximation, adaptive advantage estimation, token-level optimization, and challenges like exploration and generalization.

Result: Identifies areas where VAPO's assumptions may be challenged and suggests further investigation for robust reasoning agents.

Conclusion: Theoretical insights into VAPO can guide future improvements in reasoning tasks, though limitations and challenges remain.

Abstract: The VAPO framework has demonstrated significant empirical success in
enhancing the efficiency and reliability of reinforcement learning for long
chain-of-thought (CoT) reasoning tasks with large language models (LLMs). By
systematically addressing challenges such as value model bias, heterogeneous
sequence lengths, and sparse reward signals, VAPO achieves state-of-the-art
performance. While its practical benefits are evident, a deeper theoretical
understanding of its underlying mechanisms and potential limitations is crucial
for guiding future advancements. This paper aims to initiate such a discussion
by exploring VAPO from a theoretical perspective, highlighting areas where its
assumptions might be challenged and where further investigation could yield
more robust and generalizable reasoning agents. We delve into the intricacies
of value function approximation in complex reasoning spaces, the optimality of
adaptive advantage estimation, the impact of token-level optimization, and the
enduring challenges of exploration and generalization.

</details>


### [679] [An Example Safety Case for Safeguards Against Misuse](https://arxiv.org/pdf/2505.18003)
*Joshua Clymer, Jonah Weinbaum, Robert Kirk, Kimberly Mai, Selena Zhang, Xander Davies*

Main category: cs.LG

TL;DR: The paper proposes an end-to-end safety case framework to rigorously evaluate and justify low AI misuse risks, combining red teaming and quantitative modeling.


<details>
  <summary>Details</summary>
Motivation: Existing evaluations of AI misuse safeguards are fragmented and lack real-world applicability, necessitating a more systematic approach.

Method: Developers red team safeguards to estimate evasion effort, then use a quantitative uplift model to assess misuse deterrence, creating a continuous risk signal.

Result: The framework provides a concrete method to justify low AI misuse risks and enables rapid response to emerging threats.

Conclusion: This approach offers a practical path to rigorously ensuring AI safety, though it is not the only possible solution.

Abstract: Existing evaluations of AI misuse safeguards provide a patchwork of evidence
that is often difficult to connect to real-world decisions. To bridge this gap,
we describe an end-to-end argument (a "safety case") that misuse safeguards
reduce the risk posed by an AI assistant to low levels. We first describe how a
hypothetical developer red teams safeguards, estimating the effort required to
evade them. Then, the developer plugs this estimate into a quantitative "uplift
model" to determine how much barriers introduced by safeguards dissuade misuse
(https://www.aimisusemodel.com/). This procedure provides a continuous signal
of risk during deployment that helps the developer rapidly respond to emerging
threats. Finally, we describe how to tie these components together into a
simple safety case. Our work provides one concrete path -- though not the only
path -- to rigorously justifying AI misuse risks are low.

</details>


### [680] [Rethinking Contrastive Learning in Graph Anomaly Detection: A Clean-View Perspective](https://arxiv.org/pdf/2505.18002)
*Di Jin, Jingyi Cao, Xiaobao Wang, Bingdao Feng, Dongxiao He, Longbiao Wang, Jianwu Dang*

Main category: cs.LG

TL;DR: The paper proposes CVGAD, a framework for graph anomaly detection that addresses noise from interfering edges by using multi-scale anomaly awareness and progressive purification.


<details>
  <summary>Details</summary>
Motivation: Existing methods assume lower similarity indicates abnormality but fail under noise from interfering edges, leading to poor performance.

Method: CVGAD includes a multi-scale anomaly awareness module and a progressive purification module to iteratively remove interfering edges.

Result: Experiments on five datasets show CVGAD's effectiveness.

Conclusion: CVGAD improves anomaly detection by addressing noise and refining the graph iteratively.

Abstract: Graph anomaly detection aims to identify unusual patterns in graph-based
data, with wide applications in fields such as web security and financial fraud
detection. Existing methods typically rely on contrastive learning, assuming
that a lower similarity between a node and its local subgraph indicates
abnormality. However, these approaches overlook a crucial limitation: the
presence of interfering edges invalidates this assumption, since it introduces
disruptive noise that compromises the contrastive learning process.
Consequently, this limitation impairs the ability to effectively learn
meaningful representations of normal patterns, leading to suboptimal detection
performance. To address this issue, we propose a Clean-View Enhanced Graph
Anomaly Detection framework (CVGAD), which includes a multi-scale anomaly
awareness module to identify key sources of interference in the contrastive
learning process. Moreover, to mitigate bias from the one-step edge removal
process, we introduce a novel progressive purification module. This module
incrementally refines the graph by iteratively identifying and removing
interfering edges, thereby enhancing model performance. Extensive experiments
on five benchmark datasets validate the effectiveness of our approach.

</details>


### [681] [Distances for Markov chains from sample streams](https://arxiv.org/pdf/2505.18005)
*Sergio Calo, Anders Jonsson, Gergely Neu, Ludovic Schwartz, Javier Segovia-Aguas*

Main category: cs.LG

TL;DR: The paper introduces a stochastic optimization method to estimate bisimulation metrics for Markov chains using sample trajectories, eliminating the need for full transition dynamics knowledge.


<details>
  <summary>Details</summary>
Motivation: Existing methods for computing bisimulation metrics require full knowledge of transition dynamics, which is impractical in real-world scenarios where only sample trajectories are available.

Method: A stochastic primal-dual optimization method is applied to solve a new linear programming formulation of bisimulation metrics.

Result: Theoretical guarantees on sample complexity are provided, and empirical evaluations validate the method's effectiveness.

Conclusion: The proposed method successfully estimates bisimulation metrics without explicit transition models, addressing a key limitation of prior approaches.

Abstract: Bisimulation metrics are powerful tools for measuring similarities between
stochastic processes, and specifically Markov chains. Recent advances have
uncovered that bisimulation metrics are, in fact, optimal-transport distances,
which has enabled the development of fast algorithms for computing such metrics
with provable accuracy and runtime guarantees. However, these recent methods,
as well as all previously known methods, assume full knowledge of the
transition dynamics. This is often an impractical assumption in most real-world
scenarios, where typically only sample trajectories are available. In this
work, we propose a stochastic optimization method that addresses this
limitation and estimates bisimulation metrics based on sample access, without
requiring explicit transition models. Our approach is derived from a new linear
programming (LP) formulation of bisimulation metrics, which we solve using a
stochastic primal-dual optimization method. We provide theoretical guarantees
on the sample complexity of the algorithm and validate its effectiveness
through a series of empirical evaluations.

</details>


### [682] [Strictly Constrained Generative Modeling via Split Augmented Langevin Sampling](https://arxiv.org/pdf/2505.18017)
*Matthieu Blanke, Yongquan Qu, Sara Shamekh, Pierre Gentine*

Main category: cs.LG

TL;DR: A framework (Split Augmented Langevin, SAL) is proposed to enforce physical constraints in deep generative models, improving accuracy and feasibility in scientific applications.


<details>
  <summary>Details</summary>
Motivation: Deep generative models lack guarantees on physical plausibility, limiting their use in scientific and engineering problems. Ensuring physical constraints is critical.

Method: Developed SAL, a primal-dual sampling algorithm based on Langevin dynamics, to enforce constraints progressively with convergence guarantees. Applied to diffusion models for physical fields.

Result: Constrained diffusion models improved forecast accuracy and conserved quantities in data assimilation. SAL also showed promise for optimal control feasibility.

Conclusion: SAL provides a principled way to enforce physical constraints in generative models, enhancing their applicability to scientific problems.

Abstract: Deep generative models hold great promise for representing complex physical
systems, but their deployment is currently limited by the lack of guarantees on
the physical plausibility of the generated outputs. Ensuring that known
physical constraints are enforced is therefore critical when applying
generative models to scientific and engineering problems. We address this
limitation by developing a principled framework for sampling from a target
distribution while rigorously satisfying physical constraints. Leveraging the
variational formulation of Langevin dynamics, we propose Split Augmented
Langevin (SAL), a novel primal-dual sampling algorithm that enforces
constraints progressively through variable splitting, with convergence
guarantees. While the method is developed theoretically for Langevin dynamics,
we demonstrate its effective applicability to diffusion models. In particular,
we use constrained diffusion models to generate physical fields satisfying
energy and mass conservation laws. We apply our method to diffusion-based data
assimilation on a complex physical system, where enforcing physical constraints
substantially improves both forecast accuracy and the preservation of critical
conserved quantities. We also demonstrate the potential of SAL for challenging
feasibility problems in optimal control.

</details>


### [683] [Time to Spike? Understanding the Representational Power of Spiking Neural Networks in Discrete Time](https://arxiv.org/pdf/2505.18023)
*Duc Anh Nguyen, Ernesto Araya, Adalbert Fono, Gitta Kutyniok*

Main category: cs.LG

TL;DR: The paper analyzes discrete-time LIF-SNNs, showing they approximate continuous functions and quantifying network size, latency, and depth effects.


<details>
  <summary>Details</summary>
Motivation: Address the lack of theoretical understanding of SNNs compared to ANNs, focusing on discrete-time LIF-SNNs.

Method: Study discrete-time LIF-SNNs with static inputs/outputs, analyzing their function approximation and input space partitioning.

Result: Demonstrates SNNs realize piecewise constant functions and quantifies network size for approximation. Highlights latency's role.

Conclusion: Provides theoretical foundations for discrete-time LIF-SNNs, contrasting them with ANNs and supporting findings with experiments.

Abstract: Recent years have seen significant progress in developing spiking neural
networks (SNNs) as a potential solution to the energy challenges posed by
conventional artificial neural networks (ANNs). However, our theoretical
understanding of SNNs remains relatively limited compared to the ever-growing
body of literature on ANNs. In this paper, we study a discrete-time model of
SNNs based on leaky integrate-and-fire (LIF) neurons, referred to as
discrete-time LIF-SNNs, a widely used framework that still lacks solid
theoretical foundations. We demonstrate that discrete-time LIF-SNNs with static
inputs and outputs realize piecewise constant functions defined on polyhedral
regions, and more importantly, we quantify the network size required to
approximate continuous functions. Moreover, we investigate the impact of
latency (number of time steps) and depth (number of layers) on the complexity
of the input space partitioning induced by discrete-time LIF-SNNs. Our analysis
highlights the importance of latency and contrasts these networks with ANNs
employing piecewise linear activation functions. Finally, we present numerical
experiments to support our theoretical findings.

</details>


### [684] [Knot So Simple: A Minimalistic Environment for Spatial Reasoning](https://arxiv.org/pdf/2505.18028)
*Zizhao Chen, Yoav Artzi*

Main category: cs.LG

TL;DR: KnotGym is an interactive environment for spatial reasoning and rope manipulation tasks, evaluated with various methods like model-based RL.


<details>
  <summary>Details</summary>
Motivation: To create a scalable platform for studying complex spatial reasoning and manipulation from image observations.

Method: Tasks are designed with varying knot-crossing complexity. Evaluated methods include model-based RL, model-predictive control, and chain-of-thought reasoning.

Result: Highlights challenges in perception, spatial reasoning, and manipulation.

Conclusion: KnotGym provides a quantifiable and scalable testbed for spatial reasoning research.

Abstract: We propose KnotGym, an interactive environment for complex, spatial reasoning
and manipulation. KnotGym includes goal-oriented rope manipulation tasks with
varying levels of complexity, all requiring acting from pure image
observations. Tasks are defined along a clear and quantifiable axis of
complexity based on the number of knot crossings, creating a natural
generalization test. KnotGym has a simple observation space, allowing for
scalable development, yet it highlights core challenges in integrating acute
perception, spatial reasoning, and grounded manipulation. We evaluate methods
of different classes, including model-based RL, model-predictive control, and
chain-of-thought reasoning, and illustrate the challenges KnotGym presents.
KnotGym is available at https://github.com/lil-lab/knotgym.

</details>


### [685] [Linear Mixture Distributionally Robust Markov Decision Processes](https://arxiv.org/pdf/2505.18044)
*Zhishuai Liu, Pan Xu*

Main category: cs.LG

TL;DR: The paper introduces a linear mixture DRMDP framework to address off-dynamics challenges, offering refined uncertainty representation and robust policy learning with theoretical guarantees.


<details>
  <summary>Details</summary>
Motivation: To tackle the off-dynamics challenge where policies learned in a source domain perform poorly in a target domain with different transitions, by designing better uncertainty sets for robustness.

Method: Proposes a linear mixture DRMDP framework, defining uncertainty sets around mixture weighting parameters instead of nominal kernels, and introduces a meta algorithm for robust policy learning with $f$-divergence uncertainty sets.

Result: The framework provides a more refined uncertainty representation and is statistically learnable, with sample complexity analyzed for total variation, KL, and $\chi^2$ divergences.

Conclusion: The linear mixture DRMDP framework advances robust policy learning in off-dynamics settings, offering theoretical foundations for future research.

Abstract: Many real-world decision-making problems face the off-dynamics challenge: the
agent learns a policy in a source domain and deploys it in a target domain with
different state transitions. The distributionally robust Markov decision
process (DRMDP) addresses this challenge by finding a robust policy that
performs well under the worst-case environment within a pre-specified
uncertainty set of transition dynamics. Its effectiveness heavily hinges on the
proper design of these uncertainty sets, based on prior knowledge of the
dynamics. In this work, we propose a novel linear mixture DRMDP framework,
where the nominal dynamics is assumed to be a linear mixture model. In contrast
with existing uncertainty sets directly defined as a ball centered around the
nominal kernel, linear mixture DRMDPs define the uncertainty sets based on a
ball around the mixture weighting parameter. We show that this new framework
provides a more refined representation of uncertainties compared to
conventional models based on $(s,a)$-rectangularity and $d$-rectangularity,
when prior knowledge about the mixture model is present. We propose a meta
algorithm for robust policy learning in linear mixture DRMDPs with general
$f$-divergence defined uncertainty sets, and analyze its sample complexities
under three divergence metrics instantiations: total variation,
Kullback-Leibler, and $\chi^2$ divergences. These results establish the
statistical learnability of linear mixture DRMDPs, laying the theoretical
foundation for future research on this new setting.

</details>


### [686] [Mahalanobis++: Improving OOD Detection via Feature Normalization](https://arxiv.org/pdf/2505.18032)
*Maximilian Mueller, Matthias Hein*

Main category: cs.LG

TL;DR: Simple ℓ₂-normalization of features improves Mahalanobis distance-based OOD detection by addressing feature norm inconsistencies, outperforming other methods.


<details>
  <summary>Details</summary>
Motivation: Ensuring reliable OOD detection is critical for safety-critical ML applications, but current methods suffer from performance inconsistencies due to feature norm variations.

Method: Proposes ℓ₂-normalization of pre-logit features to align with Gaussian assumptions, enhancing Mahalanobis distance-based OOD detection.

Result: ℓ₂-normalization significantly and consistently improves performance across 44 models, outperforming recent OOD detection methods.

Conclusion: Feature normalization is a simple yet effective solution to enhance OOD detection reliability in diverse ML models.

Abstract: Detecting out-of-distribution (OOD) examples is an important task for
deploying reliable machine learning models in safety-critial applications.
While post-hoc methods based on the Mahalanobis distance applied to pre-logit
features are among the most effective for ImageNet-scale OOD detection, their
performance varies significantly across models. We connect this inconsistency
to strong variations in feature norms, indicating severe violations of the
Gaussian assumption underlying the Mahalanobis distance estimation. We show
that simple $\ell_2$-normalization of the features mitigates this problem
effectively, aligning better with the premise of normally distributed data with
shared covariance matrix. Extensive experiments on 44 models across diverse
architectures and pretraining schemes show that $\ell_2$-normalization improves
the conventional Mahalanobis distance-based approaches significantly and
consistently, and outperforms other recently proposed OOD detection methods.

</details>


### [687] [Improved Algorithms for Overlapping and Robust Clustering of Edge-Colored Hypergraphs: An LP-Based Combinatorial Approach](https://arxiv.org/pdf/2505.18043)
*Changyeol Lee, Yongho Shin, Hyung-Chan An*

Main category: cs.LG

TL;DR: The paper introduces an algorithmic framework combining LP and combinatorial methods to efficiently solve three versions of edge-colored clustering (ECC), addressing limitations of traditional ECC.


<details>
  <summary>Details</summary>
Motivation: Traditional ECC has limitations like nonoverlapping clusters and high computation time for LP methods, while greedy methods sacrifice quality. The goal is to bridge this gap.

Method: Proposes a framework integrating LP's solution quality with combinatorial algorithms' speed for Local, Global, and Robust ECC.

Result: The framework efficiently produces high-quality solutions for all three ECC variants, supported by experiments and theory.

Conclusion: The approach effectively balances quality and efficiency, with theoretical bounds suggesting further improvements are unlikely. It also resolves two open questions.

Abstract: Clustering is a fundamental task in both machine learning and data mining.
Among various methods, edge-colored clustering (ECC) has emerged as a useful
approach for handling categorical data. Given a hypergraph with (hyper)edges
labeled by colors, ECC aims to assign vertex colors to minimize the number of
edges where the vertex color differs from the edge's color. However,
traditional ECC has inherent limitations, as it enforces a nonoverlapping and
exhaustive clustering. To tackle these limitations, three versions of ECC have
been studied: Local ECC and Global ECC, which allow overlapping clusters, and
Robust ECC, which accounts for vertex outliers. For these problems, both linear
programming (LP) rounding algorithms and greedy combinatorial algorithms have
been proposed. While these LP-rounding algorithms provide high-quality
solutions, they demand substantial computation time; the greedy algorithms, on
the other hand, run very fast but often compromise solution quality. In this
paper, we present an algorithmic framework that combines the strengths of LP
with the computational efficiency of combinatorial algorithms. Both
experimental and theoretical analyses show that our algorithms efficiently
produce high-quality solutions for all three problems: Local, Global, and
Robust ECC. We complement our algorithmic contributions with
complexity-theoretic inapproximability results and integrality gap bounds,
which suggest that significant theoretical improvements are unlikely. Our
results also answer two open questions previously raised in the literature.

</details>


### [688] [Towards more transferable adversarial attack in black-box manner](https://arxiv.org/pdf/2505.18097)
*Chun Tong Lei, Zhongliang Guo, Hon Chung Lee, Minh Quoc Duong, Chun Pong Lau*

Main category: cs.LG

TL;DR: The paper proposes a novel loss function and surrogate model to improve adversarial attack transferability without the computational cost of diffusion models.


<details>
  <summary>Details</summary>
Motivation: Traditional black-box attacks rely on surrogate model architectures, while diffusion-based methods like DiffPGD improve transferability but are computationally expensive. The authors hypothesize a simpler model with similar inductive bias can achieve comparable results.

Method: The authors introduce a new loss function and surrogate model leveraging classifier-guided diffusion scores to incorporate natural data distribution knowledge into adversarial optimization.

Result: Experiments show improved transferability across diverse model architectures and robustness against diffusion-based defenses, with reduced computational overhead.

Conclusion: The proposed method achieves comparable or superior transferability to diffusion-based approaches while being computationally efficient, validating the hypothesis.

Abstract: Adversarial attacks have become a well-explored domain, frequently serving as
evaluation baselines for model robustness. Among these, black-box attacks based
on transferability have received significant attention due to their practical
applicability in real-world scenarios. Traditional black-box methods have
generally focused on improving the optimization framework (e.g., utilizing
momentum in MI-FGSM) to enhance transferability, rather than examining the
dependency on surrogate white-box model architectures. Recent state-of-the-art
approach DiffPGD has demonstrated enhanced transferability by employing
diffusion-based adversarial purification models for adaptive attacks. The
inductive bias of diffusion-based adversarial purification aligns naturally
with the adversarial attack process, where both involving noise addition,
reducing dependency on surrogate white-box model selection. However, the
denoising process of diffusion models incurs substantial computational costs
through chain rule derivation, manifested in excessive VRAM consumption and
extended runtime. This progression prompts us to question whether introducing
diffusion models is necessary. We hypothesize that a model sharing similar
inductive bias to diffusion-based adversarial purification, combined with an
appropriate loss function, could achieve comparable or superior transferability
while dramatically reducing computational overhead. In this paper, we propose a
novel loss function coupled with a unique surrogate model to validate our
hypothesis. Our approach leverages the score of the time-dependent classifier
from classifier-guided diffusion models, effectively incorporating natural data
distribution knowledge into the adversarial optimization process. Experimental
results demonstrate significantly improved transferability across diverse model
architectures while maintaining robustness against diffusion-based defenses.

</details>


### [689] [Learning with Restricted Boltzmann Machines: Asymptotics of AMP and GD in High Dimensions](https://arxiv.org/pdf/2505.18046)
*Yizhou Xu, Florent Krzakala, Lenka Zdeborová*

Main category: cs.LG

TL;DR: The paper simplifies RBM training in high dimensions, linking it to multi-index models, and shows it achieves optimal performance in the spiked covariance model.


<details>
  <summary>Details</summary>
Motivation: To better understand RBM performance beyond singular value decomposition cases, especially in high-dimensional input spaces.

Method: Simplifies RBM training into a multi-index model form, using AMP, state evolution, and GD analysis.

Result: RBM achieves optimal weak recovery threshold in the spiked covariance model, matching the BBP transition.

Conclusion: The study provides a rigorous framework for analyzing RBM training dynamics and its effectiveness in unsupervised learning.

Abstract: The Restricted Boltzmann Machine (RBM) is one of the simplest generative
neural networks capable of learning input distributions. Despite its
simplicity, the analysis of its performance in learning from the training data
is only well understood in cases that essentially reduce to singular value
decomposition of the data. Here, we consider the limit of a large dimension of
the input space and a constant number of hidden units. In this limit, we
simplify the standard RBM training objective into a form that is equivalent to
the multi-index model with non-separable regularization. This opens a path to
analyze training of the RBM using methods that are established for multi-index
models, such as Approximate Message Passing (AMP) and its state evolution, and
the analysis of Gradient Descent (GD) via the dynamical mean-field theory. We
then give rigorous asymptotics of the training dynamics of RBM on data
generated by the spiked covariance model as a prototype of a structure suitable
for unsupervised learning. We show in particular that RBM reaches the optimal
computational weak recovery threshold, aligning with the BBP transition, in the
spiked covariance model.

</details>


### [690] [AFD-STA: Adaptive Filtering Denoising with Spatiotemporal Attention for Chaotic System Prediction](https://arxiv.org/pdf/2505.18080)
*Chunlin Gong, Yin Wang, Jingru Li, Hanleran Zhang*

Main category: cs.LG

TL;DR: AFD-STA Net is a neural framework for predicting high-dimensional chaotic systems, combining adaptive filtering and spatiotemporal dynamics learning. It excels in accuracy and noise tolerance, validated by ablation studies.


<details>
  <summary>Details</summary>
Motivation: To address the challenge of predicting high-dimensional chaotic systems governed by PDEs, especially under noise and uncertainty.

Method: Integrates adaptive exponential smoothing, parallel attention mechanisms, dynamic gated fusion, and deep projection networks.

Result: Demonstrates high accuracy in chaotic regimes and noise tolerance, with ablation studies confirming module contributions.

Conclusion: The framework is effective for real-world applications involving high-dimensional nonlinear dynamics and measurement uncertainties.

Abstract: This paper presents AFD-STA Net, a neural framework integrating adaptive
filtering and spatiotemporal dynamics learning for predicting high-dimensional
chaotic systems governed by partial differential equations. The architecture
combines: 1) An adaptive exponential smoothing module with position-aware decay
coefficients for robust attractor reconstruction, 2) Parallel attention
mechanisms capturing cross-temporal and spatial dependencies, 3) Dynamic gated
fusion of multiscale features, and 4) Deep projection networks with
dimension-scaling capabilities. Numerical experiments on nonlinear PDE systems
demonstrate the model's effectiveness in maintaining prediction accuracy under
both smooth and strongly chaotic regimes while exhibiting noise tolerance
through adaptive filtering. Component ablation studies confirm critical
contributions from each module, particularly highlighting the essential role of
spatiotemporal attention in learning complex dynamical interactions. The
framework shows promising potential for real-world applications requiring
simultaneous handling of measurement uncertainties and high-dimensional
nonlinear dynamics.

</details>


### [691] [Asymptotically optimal regret in communicating Markov decision processes](https://arxiv.org/pdf/2505.18064)
*Victor Boone*

Main category: cs.LG

TL;DR: A learning algorithm achieves optimal regret for Markov decision processes by balancing exploration, co-exploration, and exploitation, with a regularization mechanism to handle discontinuity in the key function.


<details>
  <summary>Details</summary>
Motivation: To develop an algorithm that achieves asymptotically optimal regret for Markov decision processes in average reward under a communicating assumption.

Method: The algorithm tracks the optimal constant $K(M)$, balances exploration, co-exploration, and exploitation, and uses regularization to estimate $K(M)$ precisely.

Result: The algorithm achieves regret $K(M) \log(T) + \mathrm{o}(\log(T))$, with $K(M)$ being the best possible constant.

Conclusion: The proposed algorithm is asymptotically optimal, though the discontinuity of $K(M)$ poses a challenge addressed by regularization.

Abstract: In this paper, we present a learning algorithm that achieves asymptotically
optimal regret for Markov decision processes in average reward under a
communicating assumption. That is, given a communicating Markov decision
process $M$, our algorithm has regret $K(M) \log(T) + \mathrm{o}(\log(T))$
where $T$ is the number of learning steps and $K(M)$ is the best possible
constant. This algorithm works by explicitly tracking the constant $K(M)$ to
learn optimally, then balances the trade-off between exploration (playing
sub-optimally to gain information), co-exploration (playing optimally to gain
information) and exploitation (playing optimally to score maximally). We
further show that the function $K(M)$ is discontinuous, which is a consequence
challenge for our approach. To that end, we describe a regularization mechanism
to estimate $K(M)$ with arbitrary precision from empirical data.

</details>


### [692] [Backpropagation-Free Metropolis-Adjusted Langevin Algorithm](https://arxiv.org/pdf/2505.18081)
*Adam D. Cobb, Susmit Jha*

Main category: cs.LG

TL;DR: The paper introduces backpropagation-free gradient-based MCMC algorithms using forward-mode AD, proposing four new methods and demonstrating their competitiveness with traditional MALA.


<details>
  <summary>Details</summary>
Motivation: To eliminate the need for backpropagation in gradient-based MCMC by leveraging forward-mode AD, reducing computational costs while maintaining performance.

Method: Incorporates tangent vector sampling into MALA's proposal mechanism, introducing four algorithms: Forward MALA, Line Forward MALA, Pre-conditioned Forward MALA, and Pre-conditioned Line Forward MALA.

Result: Forward-mode samplers reduce computational costs and perform competitively with MALA, sometimes outperforming it, as shown in Bayesian inference tasks.

Conclusion: Forward-mode AD enables efficient backpropagation-free MCMC, offering a viable alternative to traditional methods with potential performance benefits.

Abstract: Recent work on backpropagation-free learning has shown that it is possible to
use forward-mode automatic differentiation (AD) to perform optimization on
differentiable models. Forward-mode AD requires sampling a tangent vector for
each forward pass of a model. The result is the model evaluation with the
directional derivative along the tangent. In this paper, we illustrate how the
sampling of this tangent vector can be incorporated into the proposal mechanism
for the Metropolis-Adjusted Langevin Algorithm (MALA). As such, we are the
first to introduce a backpropagation-free gradient-based Markov chain Monte
Carlo (MCMC) algorithm. We also extend to a novel backpropagation-free
position-specific preconditioned forward-mode MALA that leverages Hessian
information. Overall, we propose four new algorithms: Forward MALA; Line
Forward MALA; Pre-conditioned Forward MALA, and Pre-conditioned Line Forward
MALA. We highlight the reduced computational cost of the forward-mode samplers
and show that forward-mode is competitive with the original MALA, while even
outperforming it depending on the probabilistic model. We include Bayesian
inference results on a range of probabilistic models, including hierarchical
distributions and Bayesian neural networks.

</details>


### [693] [Reward Model Generalization for Compute-Aware Test-Time Reasoning](https://arxiv.org/pdf/2505.18065)
*Zeen Song, Wenwen Qiang, Siyu Zhao, Changwen Zheng, Gang Hua*

Main category: cs.LG

TL;DR: The paper introduces a method to enhance LLMs by decoupling generation and selection of reasoning paths, using a PRM for scoring. It addresses TCO with a theoretical framework and proposes CATS for dynamic search control.


<details>
  <summary>Details</summary>
Motivation: To improve LLM reasoning efficiency by optimizing test-time compute and leveraging PRM generalization error for better performance.

Method: Proposes Compute-Aware Tree Search (CATS), an actor-critic framework for dynamic search control, guided by reward distributions and sparsity statistics.

Result: CATS outperforms other methods on MATH and AIME benchmarks, validating theoretical predictions.

Conclusion: Lower PRM generalization error improves compute efficiency, and CATS effectively optimizes reasoning performance under fixed budgets.

Abstract: External test-time reasoning enhances large language models (LLMs) by
decoupling generation and selection. At inference time, the model generates
multiple reasoning paths, and an auxiliary process reward model (PRM) is used
to score and select the best one. A central challenge in this setting is
test-time compute optimality (TCO), i.e., how to maximize answer accuracy under
a fixed inference budget. In this work, we establish a theoretical framework to
analyze how the generalization error of the PRM affects compute efficiency and
reasoning performance. Leveraging PAC-Bayes theory, we derive generalization
bounds and show that a lower generalization error of PRM leads to fewer samples
required to find correct answers. Motivated by this analysis, we propose
Compute-Aware Tree Search (CATS), an actor-critic framework that dynamically
controls search behavior. The actor outputs sampling hyperparameters based on
reward distributions and sparsity statistics, while the critic estimates their
utility to guide budget allocation. Experiments on the MATH and AIME benchmarks
with various LLMs and PRMs demonstrate that CATS consistently outperforms other
external TTS methods, validating our theoretical predictions.

</details>


### [694] [Data Mixing Can Induce Phase Transitions in Knowledge Acquisition](https://arxiv.org/pdf/2505.18091)
*Xinran Gu, Kaifeng Lyu, Jiazheng Li, Jingzhao Zhang*

Main category: cs.LG

TL;DR: LLMs trained on mixed data (web scrapes + knowledge-dense sources) show phase transitions in knowledge acquisition, influenced by model size and mixing ratio.


<details>
  <summary>Details</summary>
Motivation: To understand how LLMs acquire knowledge from mixed datasets and identify phase transitions in learning behavior.

Method: Controlled experiments on a synthetic biography dataset mixed with web-scraped data, analyzing memorization patterns.

Result: Phase transitions occur: (1) sudden memorization at critical model size, (2) rapid memorization beyond critical mixing ratio.

Conclusion: Phase transitions are predictable, highlighting that optimal data mixing varies with model size.

Abstract: Large Language Models (LLMs) are typically trained on data mixtures: most
data come from web scrapes, while a small portion is curated from high-quality
sources with dense domain-specific knowledge. In this paper, we show that when
training LLMs on such data mixtures, knowledge acquisition from knowledge-dense
datasets, unlike training exclusively on knowledge-dense data
(arXiv:2404.05405), does not always follow a smooth scaling law but can exhibit
phase transitions with respect to the mixing ratio and model size. Through
controlled experiments on a synthetic biography dataset mixed with web-scraped
data, we demonstrate that: (1) as we increase the model size to a critical
value, the model suddenly transitions from memorizing very few to most of the
biographies; (2) below a critical mixing ratio, the model memorizes almost
nothing even with extensive training, but beyond this threshold, it rapidly
memorizes more biographies. We attribute these phase transitions to a capacity
allocation phenomenon: a model with bounded capacity must act like a knapsack
problem solver to minimize the overall test loss, and the optimal allocation
across datasets can change discontinuously as the model size or mixing ratio
varies. We formalize this intuition in an information-theoretic framework and
reveal that these phase transitions are predictable, with the critical mixing
ratio following a power-law relationship with the model size. Our findings
highlight a concrete case where a good mixing recipe for large models may not
be optimal for small models, and vice versa.

</details>


### [695] [Emergence of Hebbian Dynamics in Regularized Non-Local Learners](https://arxiv.org/pdf/2505.18069)
*David Koplow, Tomaso Poggio, Liu Ziyin*

Main category: cs.LG

TL;DR: The paper connects SGD with weight decay to Hebbian learning, showing that SGD can mimic Hebbian or anti-Hebbian rules under certain conditions, bridging artificial and biological learning.


<details>
  <summary>Details</summary>
Motivation: To reconcile the apparent incompatibility between SGD (used in AI) and Hebbian learning (observed in biology) by showing theoretical and empirical connections.

Method: Theoretical analysis and empirical experiments demonstrating how SGD with weight decay or noise can resemble Hebbian or anti-Hebbian learning near convergence.

Result: SGD with regularization can mimic Hebbian learning, while SGD with noise resembles anti-Hebbian learning. Hebbian properties emerge even from random learning rules with weight decay.

Conclusion: Hebbian learning properties may arise as a side effect of optimization principles, suggesting caution in interpreting them as evidence against complex learning mechanisms in biology.

Abstract: Stochastic Gradient Descent (SGD) has emerged as a remarkably effective
learning algorithm, underpinning nearly all state-of-the-art machine learning
models, from large language models to autonomous vehicles. Despite its
practical success, SGD appears fundamentally distinct from biological learning
mechanisms. It is widely believed that the biological brain can not implement
gradient descent because it is nonlocal, and we have found little (if any)
experimental evidence for it. In contrast, the brain is widely thought to learn
via local Hebbian learning principles, which have been seen as incompatible
with gradient descent. In this paper, we establish a theoretical and empirical
connection between the learning signals of neural networks trained using SGD
with weight decay and those trained with Hebbian learning near convergence. We
show that SGD with regularization can appear to learn according to a Hebbian
rule, and SGD with injected noise according to an anti-Hebbian rule. We also
provide empirical evidence that Hebbian learning properties can emerge in a
network with weight decay from virtually any learning rule--even random ones.
These results may bridge a long-standing gap between artificial and biological
learning, revealing Hebbian properties as an epiphenomenon of deeper
optimization principles and cautioning against interpreting their presence in
neural data as evidence against more complex hetero-synaptic mechanisms.

</details>


### [696] [How Can I Publish My LLM Benchmark Without Giving the True Answers Away?](https://arxiv.org/pdf/2505.18102)
*Takashi Ishida, Thanawat Lodkaew, Ikko Yamane*

Main category: cs.LG

TL;DR: The paper proposes a method to publish LLM benchmarks without fully disclosing ground-truth answers, using randomized correct answers to prevent contamination and detect overfitting.


<details>
  <summary>Details</summary>
Motivation: To address the risk of benchmark contamination and overfitting in LLMs while maintaining open evaluation.

Method: Inject randomness by preparing multiple logically correct answers and including only one as the solution, reducing Bayes accuracy.

Result: The method effectively detects data contamination across various benchmarks, models, and training methods.

Conclusion: Randomizing benchmark answers mitigates contamination risks and provides a reliable test for detecting overfitting.

Abstract: Publishing a large language model (LLM) benchmark on the Internet risks
contaminating future LLMs: the benchmark may be unintentionally (or
intentionally) used to train or select a model. A common mitigation is to keep
the benchmark private and let participants submit their models or predictions
to the organizers. However, this strategy will require trust in a single
organization and still permits test-set overfitting through repeated queries.
To overcome this issue, we propose a way to publish benchmarks without
completely disclosing the ground-truth answers to the questions, while still
maintaining the ability to openly evaluate LLMs. Our main idea is to inject
randomness to the answers by preparing several logically correct answers, and
only include one of them as the solution in the benchmark. This reduces the
best possible accuracy, i.e., Bayes accuracy, of the benchmark. Not only is
this helpful to keep us from disclosing the ground truth, but this approach
also offers a test for detecting data contamination. In principle, even fully
capable models should not surpass the Bayes accuracy. If a model surpasses this
ceiling despite this expectation, this is a strong signal of data
contamination. We present experimental evidence that our method can detect data
contamination accurately on a wide range of benchmarks, models, and training
methodologies.

</details>


### [697] [An Iterative Framework for Generative Backmapping of Coarse Grained Proteins](https://arxiv.org/pdf/2505.18082)
*Georgios Kementzidis, Erin Wong, John Nicholson, Ruichen Xu, Yuefan Deng*

Main category: cs.LG

TL;DR: A novel iterative framework using conditional Variational Autoencoders and graph-based neural networks improves accuracy and efficiency in backmapping coarse-grained to fine-grained protein representations.


<details>
  <summary>Details</summary>
Motivation: Existing data-driven backmapping techniques for proteins struggle with accuracy, training stability, and physical realism, especially for complex systems.

Method: The proposed method uses conditional Variational Autoencoders and graph-based neural networks for stepwise refinement from coarse-grained to atomistic details.

Result: The multistep approach enhances reconstruction accuracy and computational efficiency, even for ultra-coarse-grained protein representations.

Conclusion: The iterative framework effectively addresses challenges in backmapping for large-scale biomolecules, offering improved performance and realism.

Abstract: The techniques of data-driven backmapping from coarse-grained (CG) to
fine-grained (FG) representation often struggle with accuracy, unstable
training, and physical realism, especially when applied to complex systems such
as proteins. In this work, we introduce a novel iterative framework by using
conditional Variational Autoencoders and graph-based neural networks,
specifically designed to tackle the challenges associated with such large-scale
biomolecules. Our method enables stepwise refinement from CG beads to full
atomistic details. We outline the theory of iterative generative backmapping
and demonstrate via numerical experiments the advantages of multistep schemes
by applying them to proteins of vastly different structures with very coarse
representations. This multistep approach not only improves the accuracy of
reconstructions but also makes the training process more computationally
efficient for proteins with ultra-CG representations.

</details>


### [698] [What Do You Need for Diverse Trajectory Stitching in Diffusion Planning?](https://arxiv.org/pdf/2505.18083)
*Quentin Clark, Florian Shkurti*

Main category: cs.LG

TL;DR: The paper explores the factors enabling stitching in generative behavioral cloning (BC) methods, identifying positional equivariance and local receptiveness as key properties. It explains how these properties influence architecture, data, and inference choices, and demonstrates their impact through experiments.


<details>
  <summary>Details</summary>
Motivation: Understanding the factors behind stitching in generative BC methods to improve algorithm development for reliable stitching.

Method: Focuses on diffusion planners trained via BC, analyzing the roles of positional equivariance and local receptiveness. Examines architecture, data, and inference choices like replanning frequency and data augmentation.

Result: (1) Locality is more critical than positional equivariance for composition, but both are essential. (2) Simple architectural choices can match expensive methods like replanning or data scaling. (3) Inpainting-based guidance aids generalization in goal-conditioned settings.

Conclusion: Positional equivariance and local receptiveness are foundational for stitching in generative BC. Simple architectural changes can effectively enable composition, offering a competitive alternative to complex methods.

Abstract: In planning, stitching is an ability of algorithms to piece together
sub-trajectories of data they are trained on to generate new and diverse
behaviours. While stitching is historically a strength of offline reinforcement
learning, recent generative behavioural cloning (BC) methods have also shown
proficiency at stitching. However, the main factors behind this are poorly
understood, hindering the development of new algorithms that can reliably
stitch. Focusing on diffusion planners trained via BC, we find two properties
are needed to compose: \emph{positional equivariance} and \emph{local
receptiveness}. We use these two properties to explain architecture, data, and
inference choices in existing generative BC methods based on diffusion
planning, including replanning frequency, data augmentation, and data scaling.
Experimental comparisions show that (1) while locality is more important than
positional equivariance in creating a diffusion planner capable of composition,
both are crucial (2) enabling these properties through relatively simple
architecture choices can be competitive with more computationally expensive
methods such as replanning or scaling data, and (3) simple inpainting-based
guidance can guide architecturally compositional models to enable
generalization in goal-conditioned settings.

</details>


### [699] [Reward Model Overoptimisation in Iterated RLHF](https://arxiv.org/pdf/2505.18126)
*Lorenz Wolf, Robert Kirk, Mirco Musolesi*

Main category: cs.LG

TL;DR: The paper studies overoptimisation in iterated RLHF, analyzing design choices and showing that overoptimisation decreases over iterations but performance gains diminish.


<details>
  <summary>Details</summary>
Motivation: To understand and mitigate reward model overoptimisation in iterated RLHF, which limits policy generalisability.

Method: Systematic analysis of design choices (reward model training data transfer, reward function, policy initialisation) using the AlpacaFarm benchmark.

Result: Overoptimisation decreases over iterations, but performance gains diminish. Reinitialising from the base policy is robust but limits flexibility.

Conclusion: Provides insights for more stable and generalisable RLHF pipelines.

Abstract: Reinforcement learning from human feedback (RLHF) is a widely used method for
aligning large language models with human preferences. However, RLHF often
suffers from reward model overoptimisation, in which models overfit to the
reward function, resulting in non-generalisable policies that exploit the
idiosyncrasies and peculiarities of the reward function. A common mitigation is
iterated RLHF, in which reward models are repeatedly retrained with updated
human feedback and policies are re-optimised. Despite its increasing adoption,
the dynamics of overoptimisation in this setting remain poorly understood. In
this work, we present the first comprehensive study of overoptimisation in
iterated RLHF. We systematically analyse key design choices - how reward model
training data is transferred across iterations, which reward function is used
for optimisation, and how policies are initialised. Using the controlled
AlpacaFarm benchmark, we observe that overoptimisation tends to decrease over
successive iterations, as reward models increasingly approximate ground-truth
preferences. However, performance gains diminish over time, and while
reinitialising from the base policy is robust, it limits optimisation
flexibility. Other initialisation strategies often fail to recover from early
overoptimisation. These findings offer actionable insights for building more
stable and generalisable RLHF pipelines.

</details>


### [700] [Early-Exit Graph Neural Networks](https://arxiv.org/pdf/2505.18088)
*Andrea Giuseppe Di Francesco, Maria Sofia Bucarelli, Franco Maria Nardini, Raffaele Perego, Nicola Tonellotto, Fabrizio Silvestri*

Main category: cs.LG

TL;DR: Early-exit mechanisms in GNNs (EEGNNs) reduce latency and energy by dynamically halting inference for simple graphs while maintaining accuracy for complex ones, using SAS-GNNs to mitigate over-smoothing and over-squashing.


<details>
  <summary>Details</summary>
Motivation: To explore the untapped potential of early-exit mechanisms in GNNs, especially for deep architectures resistant to over-smoothing and over-squashing.

Method: Introduces SAS-GNNs for stable intermediate representations and EEGNNs with confidence-aware exit heads for dynamic termination.

Result: EEGNNs maintain robust performance with depth, achieve competitive accuracy on heterophilic and long-range benchmarks, and reduce computation and latency.

Conclusion: EEGNNs effectively balance accuracy and efficiency in GNNs, with potential for broader applications.

Abstract: Early-exit mechanisms allow deep neural networks to halt inference as soon as
classification confidence is high enough, adaptively trading depth for
confidence, and thereby cutting latency and energy on easy inputs while
retaining full-depth accuracy for harder ones. Similarly, adding early exit
mechanisms to Graph Neural Networks (GNNs), the go-to models for
graph-structured data, allows for dynamic trading depth for confidence on
simple graphs while maintaining full-depth accuracy on harder and more complex
graphs to capture intricate relationships. Although early exits have proven
effective across various deep learning domains, their potential within GNNs in
scenarios that require deep architectures while resisting over-smoothing and
over-squashing remains largely unexplored. We unlock that potential by first
introducing Symmetric-Anti-Symmetric Graph Neural Networks (SAS-GNN), whose
symmetry-based inductive biases mitigate these issues and yield stable
intermediate representations that can be useful to allow early exiting in GNNs.
Building on this backbone, we present Early-Exit Graph Neural Networks
(EEGNNs), which append confidence-aware exit heads that allow on-the-fly
termination of propagation based on each node or the entire graph. Experiments
show that EEGNNs preserve robust performance as depth grows and deliver
competitive accuracy on heterophilic and long-range benchmarks, matching
attention-based and asynchronous message-passing models while substantially
reducing computation and latency. We plan to release the code to reproduce our
experiments.

</details>


### [701] [Leveraging KANs for Expedient Training of Multichannel MLPs via Preconditioning and Geometric Refinement](https://arxiv.org/pdf/2505.18131)
*Jonas A. Actor, Graham Harper, Ben Southworth, Eric C. Cyr*

Main category: cs.LG

TL;DR: The paper explores the relationship between Kolmogorov-Arnold Networks (KANs) and multichannel MLPs, showing how KANs' properties can accelerate MLP training and improve accuracy.


<details>
  <summary>Details</summary>
Motivation: To leverage the structural insights from KANs to enhance the training efficiency and performance of MLPs, particularly for scientific machine learning tasks.

Method: The study exploits the equivalence between KANs and a class of MLPs, using KAN's geometric localized support and preconditioned descent in the ReLU basis to define a hierarchical refinement scheme for MLPs.

Result: The proposed method accelerates MLP training and improves accuracy, especially when spline knot locations are trained alongside weights.

Conclusion: The structural equivalence between KANs and MLPs enables faster training and better performance, validated on regression and scientific ML benchmarks.

Abstract: Multilayer perceptrons (MLPs) are a workhorse machine learning architecture,
used in a variety of modern deep learning frameworks. However, recently
Kolmogorov-Arnold Networks (KANs) have become increasingly popular due to their
success on a range of problems, particularly for scientific machine learning
tasks. In this paper, we exploit the relationship between KANs and multichannel
MLPs to gain structural insight into how to train MLPs faster. We demonstrate
the KAN basis (1) provides geometric localized support, and (2) acts as a
preconditioned descent in the ReLU basis, overall resulting in expedited
training and improved accuracy. Our results show the equivalence between
free-knot spline KAN architectures, and a class of MLPs that are refined
geometrically along the channel dimension of each weight tensor. We exploit
this structural equivalence to define a hierarchical refinement scheme that
dramatically accelerates training of the multi-channel MLP architecture. We
show further accuracy improvements can be had by allowing the $1$D locations of
the spline knots to be trained simultaneously with the weights. These advances
are demonstrated on a range of benchmark examples for regression and scientific
machine learning.

</details>


### [702] [Dynamic Dual Buffer with Divide-and-Conquer Strategy for Online Continual Learning](https://arxiv.org/pdf/2505.18101)
*Congren Dai, Huichi Zhou, Jiahao Huang, Zhenxuan Zhang, Fanwen Wang, Guang Yang, Fei Ye*

Main category: cs.LG

TL;DR: The paper introduces a memory framework for Online Continual Learning (OCL) with short-term and long-term memory systems, using a $K$-means-based sample selection and a novel memory optimisation strategy to combat catastrophic forgetting.


<details>
  <summary>Details</summary>
Motivation: Address the challenge of catastrophic forgetting in OCL by retaining dynamic and enduring knowledge through an innovative memory framework.

Method: Proposes a dual-memory system with $K$-means-based sample selection, optimal transportation for memory optimisation, and a Divide-and-Conquer (DAC) approach for efficient memory updating.

Result: Achieves state-of-the-art performance in standard and imbalanced learning settings.

Conclusion: The proposed framework effectively mitigates catastrophic forgetting and enhances learning efficacy in OCL.

Abstract: Online Continual Learning (OCL) presents a complex learning environment in
which new data arrives in a batch-to-batch online format, and the risk of
catastrophic forgetting can significantly impair model efficacy. In this study,
we address OCL by introducing an innovative memory framework that incorporates
a short-term memory system to retain dynamic information and a long-term memory
system to archive enduring knowledge. Specifically, the long-term memory system
comprises a collection of sub-memory buffers, each linked to a cluster
prototype and designed to retain data samples from distinct categories. We
propose a novel $K$-means-based sample selection method to identify cluster
prototypes for each encountered category. To safeguard essential and critical
samples, we introduce a novel memory optimisation strategy that selectively
retains samples in the appropriate sub-memory buffer by evaluating each cluster
prototype against incoming samples through an optimal transportation mechanism.
This approach specifically promotes each sub-memory buffer to retain data
samples that exhibit significant discrepancies from the corresponding cluster
prototype, thereby ensuring the preservation of semantically rich information.
In addition, we propose a novel Divide-and-Conquer (DAC) approach that
formulates the memory updating as an optimisation problem and divides it into
several subproblems. As a result, the proposed DAC approach can solve these
subproblems separately and thus can significantly reduce computations of the
proposed memory updating process. We conduct a series of experiments across
standard and imbalanced learning settings, and the empirical findings indicate
that the proposed memory framework achieves state-of-the-art performance in
both learning contexts.

</details>


### [703] [Beyond Discreteness: Finite-Sample Analysis of Straight-Through Estimator for Quantization](https://arxiv.org/pdf/2505.18113)
*Halyun Jeong, Jack Xin, Penghang Yin*

Main category: cs.LG

TL;DR: This paper provides the first finite-sample analysis of the Straight-Through Estimator (STE) in neural network quantization, highlighting the role of sample size and uncovering a recurrence property in noisy settings.


<details>
  <summary>Details</summary>
Motivation: To address the lack of theoretical understanding of STE in quantized neural networks, especially under finite-sample conditions.

Method: Analyzes STE in a two-layer neural network with binary weights and activations, using tools from compressed sensing and dynamical systems theory.

Result: Derives a sample complexity bound for STE convergence and reveals a recurrence property in noisy label scenarios.

Conclusion: The study emphasizes the importance of sample size in STE success and provides theoretical insights for quantized neural network training.

Abstract: Training quantized neural networks requires addressing the non-differentiable
and discrete nature of the underlying optimization problem. To tackle this
challenge, the straight-through estimator (STE) has become the most widely
adopted heuristic, allowing backpropagation through discrete operations by
introducing surrogate gradients. However, its theoretical properties remain
largely unexplored, with few existing works simplifying the analysis by
assuming an infinite amount of training data. In contrast, this work presents
the first finite-sample analysis of STE in the context of neural network
quantization. Our theoretical results highlight the critical role of sample
size in the success of STE, a key insight absent from existing studies.
Specifically, by analyzing the quantization-aware training of a two-layer
neural network with binary weights and activations, we derive the sample
complexity bound in terms of the data dimensionality that guarantees the
convergence of STE-based optimization to the global minimum. Moreover, in the
presence of label noises, we uncover an intriguing recurrence property of
STE-gradient method, where the iterate repeatedly escape from and return to the
optimal binary weights. Our analysis leverages tools from compressed sensing
and dynamical systems theory.

</details>


### [704] [Bridging Supervised Learning and Reinforcement Learning in Math Reasoning](https://arxiv.org/pdf/2505.18116)
*Huayu Chen, Kaiwen Zheng, Qinsheng Zhang, Ganqu Cui, Yin Cui, Haotian Ye, Tsung-Yi Lin, Ming-Yu Liu, Jun Zhu, Haoxiang Wang*

Main category: cs.LG

TL;DR: The paper introduces Negative-aware Fine-Tuning (NFT), a supervised learning method for LLMs that leverages negative feedback for self-improvement, matching or outperforming RL methods.


<details>
  <summary>Details</summary>
Motivation: Challenge the idea that self-improvement in LLMs is exclusive to RL by proposing a supervised approach (NFT) that uses negative feedback.

Method: NFT constructs an implicit negative policy from self-generated negative answers, enabling direct optimization on all LLM generations.

Result: NFT outperforms SL baselines and matches/surpasses RL methods like GRPO and DAPO in math reasoning tasks.

Conclusion: NFT bridges the gap between SL and RL in binary-feedback learning, showing equivalence between NFT and GRPO in strict-on-policy training.

Abstract: Reinforcement Learning (RL) has played a central role in the recent surge of
LLMs' math abilities by enabling self-improvement through binary verifier
signals. In contrast, Supervised Learning (SL) is rarely considered for such
verification-driven training, largely due to its heavy reliance on reference
answers and inability to reflect on mistakes. In this work, we challenge the
prevailing notion that self-improvement is exclusive to RL and propose
Negative-aware Fine-Tuning (NFT) -- a supervised approach that enables LLMs to
reflect on their failures and improve autonomously with no external teachers.
In online training, instead of throwing away self-generated negative answers,
NFT constructs an implicit negative policy to model them. This implicit policy
is parameterized with the same positive LLM we target to optimize on positive
data, enabling direct policy optimization on all LLMs' generations. We conduct
experiments on 7B and 32B models in math reasoning tasks. Results consistently
show that through the additional leverage of negative feedback, NFT
significantly improves over SL baselines like Rejection sampling Fine-Tuning,
matching or even surpassing leading RL algorithms like GRPO and DAPO.
Furthermore, we demonstrate that NFT and GRPO are actually equivalent in
strict-on-policy training, even though they originate from entirely different
theoretical foundations. Our experiments and theoretical findings bridge the
gap between SL and RL methods in binary-feedback learning systems.

</details>


### [705] [TabSTAR: A Foundation Tabular Model With Semantically Target-Aware Representations](https://arxiv.org/pdf/2505.18125)
*Alan Arazi, Eilam Shapira, Roi Reichart*

Main category: cs.LG

TL;DR: TabSTAR introduces a foundation model for tabular learning with text features, outperforming traditional methods by using target-aware representations and transfer learning.


<details>
  <summary>Details</summary>
Motivation: Deep learning underperforms on tabular tasks compared to GBDTs, but recent advancements suggest potential for foundation models, especially with text data. Existing methods lack target-aware textual representations.

Method: TabSTAR uses a pretrained text encoder with target tokens to learn task-specific embeddings, avoiding dataset-specific parameters.

Result: TabSTAR achieves state-of-the-art performance on benchmarks for classification tasks with text features and shows scaling potential.

Conclusion: TabSTAR demonstrates the viability of foundation models for tabular learning, offering improved generalization and performance.

Abstract: While deep learning has achieved remarkable success across many domains, it
has historically underperformed on tabular learning tasks, which remain
dominated by gradient boosting decision trees (GBDTs). However, recent
advancements are paving the way for Tabular Foundation Models, which can
leverage real-world knowledge and generalize across diverse datasets,
particularly when the data contains free-text. Although incorporating language
model capabilities into tabular tasks has been explored, most existing methods
utilize static, target-agnostic textual representations, limiting their
effectiveness. We introduce TabSTAR: a Foundation Tabular Model with
Semantically Target-Aware Representations. TabSTAR is designed to enable
transfer learning on tabular data with textual features, with an architecture
free of dataset-specific parameters. It unfreezes a pretrained text encoder and
takes as input target tokens, which provide the model with the context needed
to learn task-specific embeddings. TabSTAR achieves state-of-the-art
performance for both medium- and large-sized datasets across known benchmarks
of classification tasks with text features, and its pretraining phase exhibits
scaling laws in the number of datasets, offering a pathway for further
performance improvements.

</details>


### [706] [Generative Distribution Embeddings](https://arxiv.org/pdf/2505.18150)
*Nic Fishman, Gokul Gowri, Peng Yin, Jonathan Gootenberg, Omar Abudayyeh*

Main category: cs.LG

TL;DR: The paper introduces Generative Distribution Embeddings (GDE), a framework for learning representations of distributions by combining autoencoders with conditional generative models, achieving strong performance in computational biology tasks.


<details>
  <summary>Details</summary>
Motivation: Real-world problems often require reasoning across multiple scales, necessitating models that operate on entire distributions rather than single data points.

Method: GDEs lift autoencoders to the space of distributions, using an encoder on sets of samples and a generator to match input distributions, ensuring distributional invariance.

Result: GDEs learn predictive sufficient statistics in Wasserstein space, approximating W2 distances and optimal transport trajectories. They outperform existing methods on synthetic and real-world biological datasets.

Conclusion: GDEs provide a powerful framework for distributional reasoning, validated by superior performance across diverse computational biology applications.

Abstract: Many real-world problems require reasoning across multiple scales, demanding
models which operate not on single data points, but on entire distributions. We
introduce generative distribution embeddings (GDE), a framework that lifts
autoencoders to the space of distributions. In GDEs, an encoder acts on sets of
samples, and the decoder is replaced by a generator which aims to match the
input distribution. This framework enables learning representations of
distributions by coupling conditional generative models with encoder networks
which satisfy a criterion we call distributional invariance. We show that GDEs
learn predictive sufficient statistics embedded in the Wasserstein space, such
that latent GDE distances approximately recover the $W_2$ distance, and latent
interpolation approximately recovers optimal transport trajectories for
Gaussian and Gaussian mixture distributions. We systematically benchmark GDEs
against existing approaches on synthetic datasets, demonstrating consistently
stronger performance. We then apply GDEs to six key problems in computational
biology: learning representations of cell populations from lineage-tracing data
(150K cells), predicting perturbation effects on single-cell transcriptomes (1M
cells), predicting perturbation effects on cellular phenotypes (20M single-cell
images), modeling tissue-specific DNA methylation patterns (253M sequences),
designing synthetic yeast promoters (34M sequences), and spatiotemporal
modeling of viral protein sequences (1M sequences).

</details>


### [707] [Improving Multi-task Learning via Seeking Task-based Flat Regions](https://arxiv.org/pdf/2211.13723)
*Hoang Phan, Lam Tran, Quyen Tran, Ngoc N. Tran, Tuan Truong, Qi Lei, Nhat Ho, Dinh Phung, Trung Le*

Main category: cs.LG

TL;DR: The paper proposes a Multi-Task Learning (MTL) method using Sharpness-aware Minimization to improve generalization across tasks, addressing issues like overfitting and negative transfer.


<details>
  <summary>Details</summary>
Motivation: MTL reduces costs and improves efficiency but can suffer from overfitting or noisy labels, leading to suboptimal performance. The goal is to enhance generalization.

Method: Leverages Sharpness-aware Minimization to find task-based flat minima, improving generalization in MTL.

Result: Comprehensive experiments show the method outperforms existing gradient-based MTL approaches.

Conclusion: The proposed approach effectively enhances MTL performance by addressing generalization issues.

Abstract: Multi-Task Learning (MTL) is a widely-used and powerful learning paradigm for
training deep neural networks that allows learning more than one objective by a
single backbone. Compared to training tasks separately, MTL significantly
reduces computational costs, improves data efficiency, and potentially enhances
model performance by leveraging knowledge across tasks. Hence, it has been
adopted in a variety of applications, ranging from computer vision to natural
language processing and speech recognition. Among them, there is an emerging
line of work in MTL that focuses on manipulating the task gradient to derive an
ultimate gradient descent direction to benefit all tasks. Despite achieving
impressive results on many benchmarks, directly applying these approaches
without using appropriate regularization techniques might lead to suboptimal
solutions on real-world problems. In particular, standard training that
minimizes the empirical loss on the training data can easily suffer from
overfitting to low-resource tasks or be spoiled by noisy-labeled ones, which
can cause negative transfer between tasks and overall performance drop. To
alleviate such problems, we propose to leverage a recently introduced training
method, named Sharpness-aware Minimization, which can enhance model
generalization ability on single-task learning. Accordingly, we present a novel
MTL training methodology, encouraging the model to find task-based flat minima
for coherently improving its generalization capability on all tasks. Finally,
we conduct comprehensive experiments on a variety of applications to
demonstrate the merit of our proposed approach to existing gradient-based MTL
methods, as suggested by our developed theory.

</details>


### [708] [On non-approximability of zero loss global ${\mathcal L}^2$ minimizers by gradient descent in Deep Learning](https://arxiv.org/pdf/2311.07065)
*Thomas Chen, Patricia Muñoz Ewald*

Main category: cs.LG

TL;DR: Gradient descent in underparametrized DL networks cannot achieve zero loss generically; non-generic input distributions are required for zero loss.


<details>
  <summary>Details</summary>
Motivation: To understand geometric aspects of gradient descent in DL and conditions for zero loss minimization.

Method: Analysis of gradient descent in underparametrized DL networks, focusing on input distribution requirements.

Result: Zero loss minimization is unattainable generically; non-generic input distributions are necessary.

Conclusion: Training input distributions must be non-generic to achieve zero loss, as shown in referenced works.

Abstract: We analyze geometric aspects of the gradient descent algorithm in Deep
Learning (DL), and give a detailed discussion of the circumstance that in
underparametrized DL networks, zero loss minimization can generically not be
attained. As a consequence, we conclude that the distribution of training
inputs must necessarily be non-generic in order to produce zero loss
minimizers, both for the method constructed in [Chen-Munoz Ewald 2023, 2024],
or for gradient descent [Chen 2025] (which assume clustering of training data).

</details>


### [709] [A New Similarity Function for Spectral Clustering with Application to Plant Phenotypic Data](https://arxiv.org/pdf/2312.14920)
*Kapil Ahuja, Mithun Singh, Kuldeep Pathak, Milind B. Ratnaparkhe*

Main category: cs.LG

TL;DR: The paper proposes a new similarity function for Spectral Clustering (SC) using base 'a' instead of Euler's number 'e', improving clustering accuracy for plant species.


<details>
  <summary>Details</summary>
Motivation: Existing SC methods use Gaussian similarity functions with base 'e', but none explore alternative bases. The authors aim to enhance clustering accuracy by investigating this overlooked aspect.

Method: The authors propose a base 'a' exponential similarity function, integrating it with local scaling. Theoretical justification is provided using eigenvalue analysis and spectral graph theory.

Result: Experiments on soybean (2376 species) and rice (1865 species) show the new SC outperforms standard SC by 35% and 11%, respectively.

Conclusion: The proposed base 'a' similarity function significantly improves clustering accuracy, validated by theoretical and experimental results.

Abstract: Clustering species of the same plant into different groups is an important
step in developing new species of the concerned plant. Phenotypic (or physical)
characteristics of plant species are commonly used to perform clustering.
Hierarchical Clustering (HC) is popularly used for this task, and this
algorithm suffers from low accuracy. In one of the recent works (Shastri et
al., 2021), the authors have used the standard Spectral Clustering (SC)
algorithm to improve the clustering accuracy. They have demonstrated the
efficacy of their algorithm on soybean species.
  In the SC algorithm, one of the crucial steps is building the similarity
matrix. A Gaussian similarity function is the standard choice to build this
matrix. In the past, many works have proposed variants of the Gaussian
similarity function to improve the performance of the SC algorithm, however,
all have focused on the variance or scaling of the Gaussian. None of the past
works have investigated upon the choice of base "e" (Euler's number) of the
Gaussian similarity function (natural exponential function).
  Based upon spectral graph theory, specifically the Cheeger's inequality, in
this work we propose use of a base "a" exponential function as the similarity
function. We also integrate this new approach with the notion of "local
scaling" from one of the first works that experimented with the scaling of the
Gaussian similarity function (Zelnik-Manor et al., 2004).
  Using an eigenvalue analysis, we theoretically justify that our proposed
algorithm should work better than the existing one. With evaluation on 2376
soybean species and 1865 rice species, we experimentally demonstrate that our
new SC is 35% and 11% better than the standard SC, respectively.

</details>


### [710] [Gradient Aligned Regression via Pairwise Losses](https://arxiv.org/pdf/2402.06104)
*Dixian Zhu, Tianbao Yang, Livnat Jerby*

Main category: cs.LG

TL;DR: Proposes GAR (Gradient Aligned Regression), a method combining conventional regression loss with pairwise label difference losses for efficiency and theoretical grounding.


<details>
  <summary>Details</summary>
Motivation: Addresses computational inefficiency and lack of theoretical justification in existing pairwise regularization methods for regression.

Method: Uses conventional regression loss and two pairwise label difference losses (magnitude and direction) for gradient alignment, reducing quadratic complexity to linear.

Result: Demonstrates effectiveness on synthetic and real-world datasets, showing superior efficiency and performance over baselines.

Conclusion: GAR is a competitive, efficient, and theoretically justified alternative to existing pairwise regularization methods in regression.

Abstract: Regression is a fundamental task in machine learning that has garnered
extensive attention over the past decades. The conventional approach for
regression involves employing loss functions that primarily concentrate on
aligning model prediction with the ground truth for each individual data
sample. Recent research endeavors have introduced novel perspectives by
incorporating label similarity to regression via imposing extra pairwise
regularization on the latent feature space and demonstrated the effectiveness.
However, there are two drawbacks for those approaches: i) their pairwise
operation in latent feature space is computationally more expensive than
conventional regression losses; ii) it lacks of theoretical justifications
behind such regularization. In this work, we propose GAR (Gradient Aligned
Regression) as a competitive alternative method in label space, which is
constituted by a conventional regression loss and two pairwise label difference
losses for gradient alignment including magnitude and direction. GAR enjoys: i)
the same level efficiency as conventional regression loss because the quadratic
complexity for the proposed pairwise losses can be reduced to linear
complexity; ii) theoretical insights from learning the pairwise label
difference to learning the gradient of the ground truth function. We limit our
current scope as regression on the clean data setting without noises, outliers
or distributional shifts, etc. We demonstrate the effectiveness of the proposed
method practically on two synthetic datasets and on eight extensive real-world
tasks from six benchmark datasets with other eight competitive baselines.
Running time experiments demonstrate the superior efficiency of the proposed
GAR over existing methods with pairwise regularization in latent feature space
and ablation studies demonstrate the effectiveness of each component for GAR.

</details>


### [711] [Explaining Black-box Model Predictions via Two-level Nested Feature Attributions with Consistency Property](https://arxiv.org/pdf/2405.14522)
*Yuya Yoshikawa, Masanari Kimura, Ryotaro Shimizu, Yuki Saito*

Main category: cs.LG

TL;DR: A model-agnostic method for explaining black-box ML models by simultaneously estimating high- and low-level feature attributions, ensuring consistency between them with fewer model queries.


<details>
  <summary>Details</summary>
Motivation: To enhance transparency and trust in AI systems by providing accurate and consistent explanations for nested input features in black-box models.

Method: Proposes a local explanation method that leverages nested input structures, introducing a consistency property between high- and low-level feature attributions to unify their optimization.

Result: Demonstrates accurate, faithful, and consistent explanations in image and text classification tasks.

Conclusion: The method effectively bridges high- and low-level feature attributions, improving interpretability with fewer model queries.

Abstract: Techniques that explain the predictions of black-box machine learning models
are crucial to make the models transparent, thereby increasing trust in AI
systems. The input features to the models often have a nested structure that
consists of high- and low-level features, and each high-level feature is
decomposed into multiple low-level features. For such inputs, both high-level
feature attributions (HiFAs) and low-level feature attributions (LoFAs) are
important for better understanding the model's decision. In this paper, we
propose a model-agnostic local explanation method that effectively exploits the
nested structure of the input to estimate the two-level feature attributions
simultaneously. A key idea of the proposed method is to introduce the
consistency property that should exist between the HiFAs and LoFAs, thereby
bridging the separate optimization problems for estimating them. Thanks to this
consistency property, the proposed method can produce HiFAs and LoFAs that are
both faithful to the black-box models and consistent with each other, using a
smaller number of queries to the models. In experiments on image classification
in multiple instance learning and text classification using language models, we
demonstrate that the HiFAs and LoFAs estimated by the proposed method are
accurate, faithful to the behaviors of the black-box models, and provide
consistent explanations.

</details>


### [712] [Learning Generalized Hamiltonians using fully Symplectic Mappings](https://arxiv.org/pdf/2409.11138)
*Harsh Choudhary, Chandan Gupta, Vyacheslav kungrutsev, Melvin Leok, Georgios Korpas*

Main category: cs.LG

TL;DR: The paper extends symplectic integrators to generalized non-separable Hamiltonians, improving Hamiltonian Neural Networks for better system identification and long-term prediction.


<details>
  <summary>Details</summary>
Motivation: To preserve long-run physical conservation properties of Hamiltonian systems, especially for non-separable cases, which existing methods struggle with.

Method: Uses symplectic integrators for forward simulation, bypassing costly backpropagation through ODE solvers, and applies this to generalized non-separable Hamiltonians.

Result: The method is robust to noise, accurately approximates system Hamiltonians, and performs well in Hamiltonian reconstruction and conservation.

Conclusion: The approach is particularly advantageous for non-separable systems, enhancing the accuracy and efficiency of Hamiltonian Neural Networks.

Abstract: Many important physical systems can be described as the evolution of a
Hamiltonian system, which has the important property of being conservative,
that is, energy is conserved throughout the evolution. Physics Informed Neural
Networks and in particular Hamiltonian Neural Networks have emerged as a
mechanism to incorporate structural inductive bias into the NN model. By
ensuring physical invariances are conserved, the models exhibit significantly
better sample complexity and out-of-distribution accuracy than standard NNs.
Learning the Hamiltonian as a function of its canonical variables, typically
position and velocity, from sample observations of the system thus becomes a
critical task in system identification and long-term prediction of system
behavior. However, to truly preserve the long-run physical conservation
properties of Hamiltonian systems, one must use symplectic integrators for a
forward pass of the system's simulation. While symplectic schemes have been
used in the literature, they are thus far limited to situations when they
reduce to explicit algorithms, which include the case of separable Hamiltonians
or augmented non-separable Hamiltonians. We extend it to generalized
non-separable Hamiltonians, and noting the self-adjoint property of symplectic
integrators, we bypass computationally intensive backpropagation through an ODE
solver. We show that the method is robust to noise and provides a good
approximation of the system Hamiltonian when the state variables are sampled
from a noisy observation. In the numerical results, we show the performance of
the method concerning Hamiltonian reconstruction and conservation, indicating
its particular advantage for non-separable systems.

</details>


### [713] [Compression via Pre-trained Transformers: A Study on Byte-Level Multimodal Data](https://arxiv.org/pdf/2410.05078)
*David Heurtel-Depeiges, Anian Ruoss, Joel Veness, Tim Genewein*

Main category: cs.LG

TL;DR: Small pre-trained transformers can outperform standard and domain-specific compressors, achieving better compression ratios even with fewer parameters.


<details>
  <summary>Details</summary>
Motivation: Foundation models are strong compressors but inefficient in parameter size. This study seeks a balance where smaller models achieve competitive compression.

Method: Train models on 165GB of text, image, or audio data, then compress 1GB of OOD data from each modality. Compare performance against standard and domain-specific compressors.

Result: Small models outperform gzip, LZMA2, and domain-specific compressors (e.g., 0.49 vs. 0.54 for FLAC on audio). Multimodal training works but lacks transfer to unseen modalities.

Conclusion: Small transformers can achieve superior compression ratios, but transfer learning to new modalities remains limited compared to large foundation models.

Abstract: Foundation models are strong data compressors, but when accounting for their
parameter size, their compression ratios are inferior to standard compression
algorithms. Naively reducing the parameter count does not necessarily help as
it deteriorates predictions and, accordingly, compression. We conduct a
large-scale empirical study to find a sweet spot where pre-trained vanilla
transformers can achieve competitive compression ratios. To this end, we train
models on 165GB of raw byte sequences of either text, image, or audio data (and
all possible combinations of the three) and then compress 1GB of
out-of-distribution (OOD) data from each modality. We find that relatively
small models (millions of parameters) can outperform standard general-purpose
compression algorithms (gzip, LZMA2) and even domain-specific compressors (PNG,
JPEG-XL, FLAC) $\unicode{x2013}$ even when accounting for parameter size. We
achieve, e.g., the lowest compression ratio of 0.49 on OOD audio data (vs. 0.54
for FLAC). We conduct extensive ablations and hyperparameter sweeps to study
the impact of model- and dataset scale, and we investigate the effect of
unimodal versus multimodal training. We find that even small models can be
trained to perform well on multiple modalities, but unlike large-scale
foundation models, transfer to unseen modalities is generally weak.

</details>


### [714] [A Comprehensive Assessment Benchmark for Rigorously Evaluating Deep Learning Image Classifiers](https://arxiv.org/pdf/2308.04137)
*Michael W. Spratling*

Main category: cs.LG

TL;DR: The paper advocates for comprehensive evaluation methods for machine learning models, highlighting vulnerabilities in current deep neural networks despite state-of-the-art robustness training.


<details>
  <summary>Details</summary>
Motivation: Current evaluation protocols are limited and fail to assess classifiers comprehensively, leading to unreliable models in real-world scenarios.

Method: Proposes benchmarking performance using diverse data types and a single consistent metric.

Result: Finds that even robustly trained deep neural networks are vulnerable to certain data types, making them unreliable and insecure.

Conclusion: Encourages adoption of more comprehensive testing to develop robust machine learning methods.

Abstract: Reliable and robust evaluation methods are a necessary first step towards
developing machine learning models that are themselves robust and reliable.
Unfortunately, current evaluation protocols typically used to assess
classifiers fail to comprehensively evaluate performance as they tend to rely
on limited types of test data, and ignore others. For example, using the
standard test data fails to evaluate the predictions made by the classifier to
samples from classes it was not trained on. On the other hand, testing with
data containing samples from unknown classes fails to evaluate how well the
classifier can predict the labels for known classes. This article advocates
benchmarking performance using a wide range of different types of data and
using a single metric that can be applied to all such data types to produce a
consistent evaluation of performance. Using the proposed benchmark it is found
that current deep neural networks, including those trained with methods that
are believed to produce state-of-the-art robustness, are vulnerable to making
mistakes on certain types of data. This means that such models will be
unreliable in real-world scenarios where they may encounter data from many
different domains, and that they are insecure as they can be easily fooled into
making the wrong decisions. It is hoped that these results will motivate the
wider adoption of more comprehensive testing methods that will, in turn, lead
to the development of more robust machine learning methods in the future.
  Code is available at: https://codeberg.org/mwspratling/RobustnessEvaluation

</details>


### [715] [Diffusion Model Predictive Control](https://arxiv.org/pdf/2410.05364)
*Guangyao Zhou, Sivaramakrishnan Swaminathan, Rajkumar Vasudeva Raju, J. Swaroop Guntupalli, Wolfgang Lehrach, Joseph Ortiz, Antoine Dedieu, Miguel Lázaro-Gredilla, Kevin Murphy*

Main category: cs.LG

TL;DR: D-MPC is a new MPC method using diffusion models for action and dynamics proposals, outperforming existing offline planning methods and competing with SOTA RL methods.


<details>
  <summary>Details</summary>
Motivation: To improve model-based offline planning by leveraging diffusion models for better action and dynamics proposals.

Method: Uses diffusion models to learn multi-step action and dynamics models, combining them for online MPC.

Result: Significantly outperforms existing MPC methods (e.g., MBOP) and matches SOTA RL methods on D4RL. Also adapts to new rewards and dynamics.

Conclusion: D-MPC is a powerful, adaptable MPC approach, advancing diffusion-based planning.

Abstract: We propose Diffusion Model Predictive Control (D-MPC), a novel MPC approach
that learns a multi-step action proposal and a multi-step dynamics model, both
using diffusion models, and combines them for use in online MPC. On the popular
D4RL benchmark, we show performance that is significantly better than existing
model-based offline planning methods using MPC (e.g. MBOP) and competitive with
state-of-the-art (SOTA) model-based and model-free reinforcement learning
methods. We additionally illustrate D-MPC's ability to optimize novel reward
functions at run time and adapt to novel dynamics, and highlight its advantages
compared to existing diffusion-based planning baselines.

</details>


### [716] [Architecture Selection via the Trade-off Between Accuracy and Robustness](https://arxiv.org/pdf/1906.01354)
*Zhun Deng, Cynthia Dwork, Jialiang Wang, Yao Zhao*

Main category: cs.LG

TL;DR: The paper introduces a framework to analyze the trade-off between accuracy and robustness in supervised learning, proposing a method to quantify this trade-off and providing theoretical insights.


<details>
  <summary>Details</summary>
Motivation: To understand and characterize the inherent trade-off between model accuracy and robustness, especially under adversarial attacks, and to guide architecture selection.

Method: Proposes a trade-off curve and an influence function to study sensitivity under adversarial attacks. Analyzes adversarial training's regularization effects, linking it to LASSO and ridge regression.

Result: Demonstrates trade-off curves for neural networks, showing variations with layers, neurons, and structures, offering practical guidelines for architecture selection.

Conclusion: The framework provides theoretical and empirical insights into the accuracy-robustness trade-off, aiding in better model design and architecture choices.

Abstract: We provide a general framework for characterizing the trade-off between
accuracy and robustness in supervised learning. We propose a method and define
quantities to characterize the trade-off between accuracy and robustness for a
given architecture, and provide theoretical insight into the trade-off.
Specifically we introduce a simple trade-off curve, define and study an
influence function that captures the sensitivity, under adversarial attack, of
the optima of a given loss function. We further show how adversarial training
regularizes the parameters in an over-parameterized linear model, recovering
the LASSO and ridge regression as special cases, which also allows us to
theoretically analyze the behavior of the trade-off curve. In experiments, we
demonstrate the corresponding trade-off curves of neural networks and how they
vary with respect to factors such as number of layers, neurons, and across
different network structures. Such information provides a useful guideline to
architecture selection.

</details>


### [717] [Structural Reasoning Improves Molecular Understanding of LLM](https://arxiv.org/pdf/2410.05610)
*Yunhui Jang, Jaehyung Kim, Sungsoo Ahn*

Main category: cs.LG

TL;DR: LLMs struggle with molecular structural reasoning; the proposed MSR framework enhances their understanding by incorporating structural features.


<details>
  <summary>Details</summary>
Motivation: Despite LLMs' progress, they fail to reason using molecular structural information, which is crucial for understanding molecular properties.

Method: Introduces the Molecular Structural Reasoning (MSR) framework, which sketches molecular structures for reasoning, with two frameworks for known or unknown target molecules.

Result: Extensive experiments confirm that MSR improves LLMs' molecular understanding.

Conclusion: The MSR framework effectively addresses LLMs' limitations in molecular structural reasoning.

Abstract: Recently, large language models (LLMs) have shown significant progress,
approaching human perception levels. In this work, we demonstrate that despite
these advances, LLMs still struggle to reason using molecular structural
information. This gap is critical because many molecular properties, including
functional groups, depend heavily on such structural details. To address this
limitation, we propose an approach that sketches molecular structures for
reasoning. Specifically, we introduce Molecular Structural Reasoning (MSR)
framework to enhance the understanding of LLMs by explicitly incorporating the
key structural features. We present two frameworks for scenarios where the
target molecule is known or unknown. We verify that our MSR improves molecular
understanding through extensive experiments.

</details>


### [718] [Erasing Undesirable Concepts in Diffusion Models with Adversarial Preservation](https://arxiv.org/pdf/2410.15618)
*Anh Bui, Long Vuong, Khanh Doan, Trung Le, Paul Montague, Tamas Abraham, Dinh Phung*

Main category: cs.LG

TL;DR: Proposes a method to erase harmful concepts from diffusion models while minimizing impact on other concepts by identifying and preserving adversarial concepts.


<details>
  <summary>Details</summary>
Motivation: Diffusion models trained on unfiltered data can produce harmful content; existing methods struggle to balance erasure and preservation.

Method: Identifies adversarial concepts most affected by parameter changes to ensure stable erasure with minimal impact on other concepts.

Result: Outperforms state-of-the-art erasure methods in removing unwanted content while preserving unrelated elements.

Conclusion: The approach effectively balances erasure and preservation, demonstrated using Stable Diffusion.

Abstract: Diffusion models excel at generating visually striking content from text but
can inadvertently produce undesirable or harmful content when trained on
unfiltered internet data. A practical solution is to selectively removing
target concepts from the model, but this may impact the remaining concepts.
Prior approaches have tried to balance this by introducing a loss term to
preserve neutral content or a regularization term to minimize changes in the
model parameters, yet resolving this trade-off remains challenging. In this
work, we propose to identify and preserving concepts most affected by parameter
changes, termed as \textit{adversarial concepts}. This approach ensures stable
erasure with minimal impact on the other concepts. We demonstrate the
effectiveness of our method using the Stable Diffusion model, showing that it
outperforms state-of-the-art erasure methods in eliminating unwanted content
while maintaining the integrity of other unrelated elements. Our code is
available at https://github.com/tuananhbui89/Erasing-Adversarial-Preservation.

</details>


### [719] [Revisiting Hyperparameter Tuning with Differential Privacy](https://arxiv.org/pdf/2211.01852)
*Youlong Ding, Xueyang Wu*

Main category: cs.LG

TL;DR: A framework for hyperparameter tuning in privacy-preserving machine learning is proposed, ensuring privacy loss is independent of the number of hyperparameter candidates and tied to utility gains.


<details>
  <summary>Details</summary>
Motivation: Hyperparameter tuning is often overlooked in privacy-preserving ML due to privacy concerns. This work addresses this gap by enabling effective tuning without excessive privacy loss.

Method: The proposed method uses differential privacy, allowing broad hyperparameter search (e.g., grid search) with privacy loss tied to utility gains, not the number of candidates.

Result: Theoretical analysis shows privacy loss is upper-bounded by the squared root of utility gains, with empirical scaling further improved by a doubling step design.

Conclusion: The framework successfully balances privacy and utility in hyperparameter tuning, enabling practical adoption in privacy-preserving ML.

Abstract: Hyperparameter tuning is a common practice in the application of machine
learning but is a typically ignored aspect in the literature on
privacy-preserving machine learning due to its negative effect on the overall
privacy parameter. In this paper, we aim to tackle this fundamental yet
challenging problem by providing an effective hyperparameter tuning framework
with differential privacy. The proposed method allows us to adopt a broader
hyperparameter search space and even to perform a grid search over the whole
space, since its privacy loss parameter is independent of the number of
hyperparameter candidates. Interestingly, it instead correlates with the
utility gained from hyperparameter searching, revealing an explicit and
mandatory trade-off between privacy and utility. Theoretically, we show that
its additional privacy loss bound incurred by hyperparameter tuning is
upper-bounded by the squared root of the gained utility. However, we note that
the additional privacy loss bound would empirically scale like a squared root
of the logarithm of the utility term, benefiting from the design of doubling
step.

</details>


### [720] [Unpicking Data at the Seams: Understanding Disentanglement in VAEs](https://arxiv.org/pdf/2410.22559)
*Carl Allen*

Main category: cs.LG

TL;DR: The paper explains how disentanglement in VAEs arises from orthogonality in the decoder's Jacobian, linking it to statistically independent data components.


<details>
  <summary>Details</summary>
Motivation: To bridge the gap in understanding disentanglement in VAEs by connecting orthogonality in the decoder's Jacobian to independent data factors.

Method: Analyzes how diagonal posterior covariance matrices in VAEs promote orthogonality in the decoder's Jacobian, leading to disentanglement.

Result: Shows disentanglement in VAEs equates to factoring the data distribution into statistically independent components.

Conclusion: Disentanglement in VAEs is achieved through orthogonality in the decoder's Jacobian, enabling independent data representation.

Abstract: Disentanglement, or identifying statistically independent factors of the
data, is relevant to much of machine learning, from controlled data generation
and robust classification to efficient encoding and improving our understanding
of the data itself. Disentanglement arises in several generative paradigms
including Variational Autoencoders (VAEs), Generative Adversarial Networks and
diffusion models. Prior work takes a step towards understanding disentanglement
in VAEs by showing diagonal posterior covariance matrices promote orthogonality
between columns of the decoder's Jacobian. Building on this, we close the gap
in our understanding of disentanglement by showing how if follows from such
orthogonality and equates to factoring the data distribution into statistically
independent components.

</details>


### [721] [Towards Understanding the Generalizability of Delayed Stochastic Gradient Descent](https://arxiv.org/pdf/2308.09430)
*Xiaoge Deng, Li Shen, Shengwei Li, Tao Sun, Dongsheng Li, Dacheng Tao*

Main category: cs.LG

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: Stochastic gradient descent (SGD) performed in an asynchronous manner plays a
crucial role in training large-scale machine learning models. However, the
generalization performance of asynchronous delayed SGD, which is an essential
metric for assessing machine learning algorithms, has rarely been explored.
Existing generalization error bounds are rather pessimistic and cannot reveal
the correlation between asynchronous delays and generalization. In this paper,
we investigate sharper generalization error bound for SGD with asynchronous
delay $\tau$. Leveraging the generating function analysis tool, we first
establish the average stability of the delayed gradient algorithm. Based on
this algorithmic stability, we provide upper bounds on the generalization error
of $\tilde{\mathcal{O}}(\frac{T-\tau}{n\tau})$ and
$\tilde{\mathcal{O}}(\frac{1}{n})$ for quadratic convex and strongly convex
problems, respectively, where $T$ refers to the iteration number and $n$ is the
amount of training data. Our theoretical results indicate that asynchronous
delays reduce the generalization error of the delayed SGD algorithm. Analogous
analysis can be generalized to the random delay setting, and the experimental
results validate our theoretical findings.

</details>


### [722] [Fantastic Targets for Concept Erasure in Diffusion Models and Where To Find Them](https://arxiv.org/pdf/2501.18950)
*Anh Bui, Trang Vu, Long Vuong, Trung Le, Paul Montague, Tamas Abraham, Junae Kim, Dinh Phung*

Main category: cs.LG

TL;DR: The paper introduces Adaptive Guided Erasure (AGE), a method for concept erasure in diffusion models that dynamically selects optimal target concepts to minimize side effects, outperforming fixed-target strategies.


<details>
  <summary>Details</summary>
Motivation: Fixed-target concept erasure methods are suboptimal because they ignore the impact on other concepts. This paper aims to address this by analyzing the concept space and proposing a dynamic solution.

Method: The authors model the concept space as a graph, analyze the local effects of erasure, and introduce AGE, which dynamically selects target concepts for each undesirable concept.

Result: AGE outperforms state-of-the-art methods by preserving unrelated concepts while effectively erasing undesirable ones.

Conclusion: The study highlights the importance of dynamic target selection in concept erasure and demonstrates AGE's superior performance.

Abstract: Concept erasure has emerged as a promising technique for mitigating the risk
of harmful content generation in diffusion models by selectively unlearning
undesirable concepts. The common principle of previous works to remove a
specific concept is to map it to a fixed generic concept, such as a neutral
concept or just an empty text prompt. In this paper, we demonstrate that this
fixed-target strategy is suboptimal, as it fails to account for the impact of
erasing one concept on the others. To address this limitation, we model the
concept space as a graph and empirically analyze the effects of erasing one
concept on the remaining concepts. Our analysis uncovers intriguing geometric
properties of the concept space, where the influence of erasing a concept is
confined to a local region. Building on this insight, we propose the Adaptive
Guided Erasure (AGE) method, which \emph{dynamically} selects optimal target
concepts tailored to each undesirable concept, minimizing unintended side
effects. Experimental results show that AGE significantly outperforms
state-of-the-art erasure methods on preserving unrelated concepts while
maintaining effective erasure performance. Our code is published at
{https://github.com/tuananhbui89/Adaptive-Guided-Erasure}.

</details>


### [723] [Achieving Linear Speedup with ProxSkip in Distributed Stochastic Optimization](https://arxiv.org/pdf/2310.07983)
*Luyao Guo, Sulaiman A. Alghunaim, Kun Yuan, Laurent Condat, Jinde Cao*

Main category: cs.LG

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: The ProxSkip algorithm for distributed optimization is gaining increasing
attention due to its proven benefits in accelerating communication complexity
while maintaining robustness against data heterogeneity. However, existing
analyses of ProxSkip are limited to the strongly convex setting and do not
achieve linear speedup, where convergence performance increases linearly with
respect to the number of nodes. So far, questions remain open about how
ProxSkip behaves in the non-convex setting and whether linear speedup is
achievable. In this paper, we revisit distributed ProxSkip and address both
questions. We demonstrate that the leading communication complexity of ProxSkip
is $\mathcal{O}(\frac{p\sigma^2}{n\epsilon^2})$ for non-convex and convex
settings, and $\mathcal{O}(\frac{p\sigma^2}{n\epsilon})$ for the strongly
convex setting, where $n$ represents the number of nodes, $p$ denotes the
probability of communication, $\sigma^2$ signifies the level of stochastic
noise, and $\epsilon$ denotes the desired accuracy level. This result
illustrates that ProxSkip achieves linear speedup and can asymptotically reduce
communication overhead proportional to the probability of communication.
Additionally, for the strongly convex setting, we further prove that ProxSkip
can achieve linear speedup with network-independent stepsizes.

</details>


### [724] [Compact Matrix Quantum Group Equivariant Neural Networks](https://arxiv.org/pdf/2311.06358)
*Edward Pearce-Crump*

Main category: cs.LG

TL;DR: The paper introduces compact matrix quantum group equivariant neural networks to handle non-commutative geometries, addressing limitations of classical group equivariant networks.


<details>
  <summary>Details</summary>
Motivation: Classical group equivariant neural networks fail for non-commutative geometries, necessitating a new approach.

Method: Derives and characterizes compact matrix quantum group equivariant neural networks, focusing on easy compact matrix quantum groups defined by set partitions.

Result: New characterizations of equivariant weight matrices for certain compact matrix groups are obtained.

Conclusion: The proposed networks extend equivariant learning to non-commutative settings, filling a gap in the literature.

Abstract: Group equivariant neural networks have proven effective in modelling a wide
range of tasks where the data lives in a classical geometric space and exhibits
well-defined group symmetries. However, these networks are not suitable for
learning from data that lives in a non-commutative geometry, described formally
by non-commutative $C^{*}$-algebras, since the $C^{*}$-algebra of continuous
functions on a compact matrix group is commutative. To address this limitation,
we derive the existence of a new type of equivariant neural network, called
compact matrix quantum group equivariant neural networks, which encode
symmetries that are described by compact matrix quantum groups. We characterise
the weight matrices that appear in these neural networks for the easy compact
matrix quantum groups, which are defined by set partitions. As a result, we
obtain new characterisations of equivariant weight matrices for some compact
matrix groups that have not appeared previously in the machine learning
literature.

</details>


### [725] [GRADIEND: Monosemantic Feature Learning within Neural Networks Applied to Gender Debiasing of Transformer Models](https://arxiv.org/pdf/2502.01406)
*Jonathan Drechsel, Steffen Herbold*

Main category: cs.LG

TL;DR: A novel encoder-decoder method uses model gradients to isolate gender bias in AI, debiasing language models without losing functionality.


<details>
  <summary>Details</summary>
Motivation: AI systems often amplify social biases like gender bias, causing harm in critical applications.

Method: An encoder-decoder approach leverages model gradients to identify and isolate a monosemantic feature neuron for gender information.

Result: The method effectively debiases transformer-based language models across architectures while preserving other capabilities.

Conclusion: The approach shows promise for broader applications in mitigating bias in AI systems.

Abstract: AI systems frequently exhibit and amplify social biases, including gender
bias, leading to harmful consequences in critical areas. This study introduces
a novel encoder-decoder approach that leverages model gradients to learn a
single monosemantic feature neuron encoding gender information. We show that
our method can be used to debias transformer-based language models, while
maintaining other capabilities. We demonstrate the effectiveness of our
approach across various model architectures and highlight its potential for
broader applications.

</details>


### [726] [Federated Learning in Genetics: Extended Analysis of Accuracy, Performance and Privacy Trade-offs](https://arxiv.org/pdf/2402.14527)
*Anika Hannemann, Jan Ewald, Leo Seeger, Erik Buchmann*

Main category: cs.LG

TL;DR: The paper explores federated learning for training models on distributed genomic/transcriptomic data without sharing raw data, comparing TensorFlow Federated and Flower frameworks.


<details>
  <summary>Details</summary>
Motivation: To address privacy and regulatory challenges in handling sensitive, voluminous, and distributed genomic data for precision medicine.

Method: Comparative experiments with federated learning frameworks (TensorFlow Federated and Flower) on disease prognosis and cell type classification models, considering data and architectural heterogeneity.

Result: Both frameworks effectively train models without raw data transfer, with each having distinct strengths. Model quality, robustness, and computational performance are evaluated.

Conclusion: Federated learning is viable for genomic data, balancing privacy and performance, though frameworks differ in strengths.

Abstract: Machine learning on large-scale genomic or transcriptomic data is important
for many novel health applications. For example, precision medicine tailors
medical treatments to patients on the basis of individual biomarkers, cellular
and molecular states, etc. However, the data required is sensitive, voluminous,
heterogeneous, and typically distributed across locations where dedicated
machine learning hardware is not available. Due to privacy and regulatory
reasons, it is also problematic to aggregate all data at a trusted third party.
Federated learning is a promising solution to this dilemma, because it enables
decentralized, collaborative machine learning without exchanging raw data. In
this paper, we perform comparative experiments with the federated learning
frameworks TensorFlow Federated and Flower. Our test case is the training of
disease prognosis and cell type classification models. We train the models with
distributed transcriptomic data, considering both data heterogeneity and
architectural heterogeneity. We measure model quality, robustness against
privacy-enhancing noise and computational performance. We evaluate the resource
overhead of a federated system from both client and global perspectives and
assess benefits and limitations. Each of the federated learning frameworks has
different strengths. However, our experiments confirm that both frameworks can
readily build models on transcriptomic data, without transferring personal raw
data to a third party with abundant computational resources. This paper is the
extended version of
https://link.springer.com/chapter/10.1007/978-3-031-63772-8_26.

</details>


### [727] [The Alpha-Alternator: Dynamic Adaptation To Varying Noise Levels In Sequences Using The Vendi Score For Improved Robustness and Performance](https://arxiv.org/pdf/2502.04593)
*Mohammad Reza Rezaei, Adji Bousso Dieng*

Main category: cs.LG

TL;DR: The paper introduces the α-Alternator, a generative model for noisy temporal data, which dynamically adapts to varying noise levels using the Vendi Score (VS) to balance input and latent history influence.


<details>
  <summary>Details</summary>
Motivation: Existing models like Mamba assume uniform noise levels, limiting performance on noisy temporal data. The α-Alternator addresses this by adapting to varying noise.

Method: The model uses the VS to adjust influence between sequence elements and latent history via a learned parameter. Training involves observation masking and Alternator loss minimization.

Result: The α-Alternator outperforms state-of-the-art models in trajectory prediction, imputation, and forecasting.

Conclusion: The α-Alternator effectively handles noisy temporal data by dynamically adapting to noise, improving performance over existing models.

Abstract: Current state-of-the-art dynamical models, such as Mamba, assume the same
level of noisiness for all elements of a given sequence, which limits their
performance on noisy temporal data. In this paper, we introduce the
$\alpha$-Alternator, a novel generative model for time-dependent data that
dynamically adapts to the complexity introduced by varying noise levels in
sequences. The $\alpha$-Alternator leverages the Vendi Score (VS), a flexible
similarity-based diversity metric, to adjust, at each time step $t$, the
influence of the sequence element at time $t$ and the latent representation of
the dynamics up to that time step on the predicted future dynamics. This
influence is captured by a parameter that is learned and shared across all
sequences in a given dataset. The sign of this parameter determines the
direction of influence. A negative value indicates a noisy dataset, where a
sequence element that increases the VS is considered noisy, and the model
relies more on the latent history when processing that element. Conversely,
when the parameter is positive, a sequence element that increases the VS is
considered informative, and the $\alpha$-Alternator relies more on this new
input than on the latent history when updating its predicted latent dynamics.
The $\alpha$-Alternator is trained using a combination of observation masking
and Alternator loss minimization. Masking simulates varying noise levels in
sequences, enabling the model to be more robust to these fluctuations and
improving its performance in trajectory prediction, imputation, and
forecasting. Our experimental results demonstrate that the $\alpha$-Alternator
outperforms both Alternators and state-of-the-art state-space models across
neural decoding and time-series forecasting benchmarks.

</details>


### [728] [Tackling Decision Processes with Non-Cumulative Objectives using Reinforcement Learning](https://arxiv.org/pdf/2405.13609)
*Maximilian Nägele, Jan Olle, Thomas Fösel, Remmy Zen, Florian Marquardt*

Main category: cs.LG

TL;DR: The paper introduces a method to map Non-cumulative Markov Decision Processes (NCMDPs) to standard MDPs, enabling the use of existing MDP techniques like reinforcement learning for broader applications.


<details>
  <summary>Details</summary>
Motivation: Many problems don't fit the standard MDP framework, which maximizes the expected sum of rewards. NCMDPs, which maximize arbitrary functions of rewards, require a new approach.

Method: The authors propose a general mapping of NCMDPs to standard MDPs, allowing the use of existing MDP techniques such as reinforcement learning and dynamic programming.

Result: The method improves final performance and training time in tasks like classical control, finance, and discrete optimization.

Conclusion: The mapping of NCMDPs to MDPs extends the applicability of existing techniques, enhancing efficiency and performance in diverse domains.

Abstract: Markov decision processes (MDPs) are used to model a wide variety of
applications ranging from game playing over robotics to finance. Their optimal
policy typically maximizes the expected sum of rewards given at each step of
the decision process. However, a large class of problems does not fit
straightforwardly into this framework: Non-cumulative Markov decision processes
(NCMDPs), where instead of the expected sum of rewards, the expected value of
an arbitrary function of the rewards is maximized. Example functions include
the maximum of the rewards or their mean divided by their standard deviation.
In this work, we introduce a general mapping of NCMDPs to standard MDPs. This
allows all techniques developed to find optimal policies for MDPs, such as
reinforcement learning or dynamic programming, to be directly applied to the
larger class of NCMDPs. Focusing on reinforcement learning, we show
applications in a diverse set of tasks, including classical control, portfolio
optimization in finance, and discrete optimization problems. Given our
approach, we can improve both final performance and training time compared to
relying on standard MDPs.

</details>


### [729] [LoRA-Ensemble: Efficient Uncertainty Modelling for Self-Attention Networks](https://arxiv.org/pdf/2405.14438)
*Dominik J. Mühlematter, Michelle Halbheer, Alexander Becker, Dominik Narnhofer, Helge Aasen, Konrad Schindler, Mehmet Ozgur Turkoglu*

Main category: cs.LG

TL;DR: LoRA-Ensemble is a parameter-efficient ensembling method for self-attention networks, outperforming implicit techniques and matching explicit ensembles in accuracy and calibration.


<details>
  <summary>Details</summary>
Motivation: Modern methods often yield overconfident, uncalibrated predictions, and explicit ensembles are computationally expensive.

Method: Uses Low-Rank Adaptation (LoRA) to create an implicit ensembling scheme where ensemble members share a pre-trained self-attention network but have individual low-rank matrices.

Result: Outperforms BatchEnsemble and matches/exceeds explicit ensemble accuracy with superior calibration.

Conclusion: LoRA-Ensemble is an efficient and effective alternative to traditional ensembling methods.

Abstract: Numerous real-world decisions rely on machine learning algorithms and require
calibrated uncertainty estimates. However, modern methods often yield
overconfident, uncalibrated predictions. The dominant approach to quantifying
the uncertainty inherent in the model is to train an ensemble of separate
predictors and measure their empirical variance. In an explicit implementation,
the ensemble has high computational cost and memory footprint, especially if
the base model itself is already large, like modern transformers. This
motivates efforts to develop implicit ensemble methods that emulate the
ensemble without explicitly instantiating all its members. We introduce
LoRA-Ensemble, a parameter-efficient ensembling method for self-attention
networks. It is based on Low-Rank Adaptation (LoRA), originally developed for
efficient LLM fine-tuning, and extends it into an implicit ensembling scheme,
where all ensemble members share the same, pre-trained self-attention network,
but have individual low-rank matrices for the attention projections. The
resulting method not only outperforms state-of-the-art implicit techniques like
BatchEnsemble, but even matches or exceeds the accuracy of an Explicit
Ensemble, while at the same time achieving superior calibration.

</details>


### [730] [Parameter Symmetry Potentially Unifies Deep Learning Theory](https://arxiv.org/pdf/2502.05300)
*Liu Ziyin, Yizhou Xu, Tomaso Poggio, Isaac Chuang*

Main category: cs.LG

TL;DR: The paper proposes parameter symmetry as a unifying mechanism for hierarchical learning in AI, linking it to phase transitions and advocating for its role in understanding neural networks.


<details>
  <summary>Details</summary>
Motivation: To address the fragmented theories about hierarchical learning in AI and unify them through the concept of parameter symmetries.

Method: Synthesizes prior observations and theories to argue that symmetry breaking and restoration explain hierarchical learning dynamics, model complexity, and representation formation.

Result: Parameter symmetry is presented as a fundamental principle in AI, akin to its role in theoretical physics.

Conclusion: The paper advocates for further research into parameter symmetries to achieve a unified understanding of hierarchical learning in AI systems.

Abstract: The dynamics of learning in modern large AI systems is hierarchical, often
characterized by abrupt, qualitative shifts akin to phase transitions observed
in physical systems. While these phenomena hold promise for uncovering the
mechanisms behind neural networks and language models, existing theories remain
fragmented, addressing specific cases. In this position paper, we advocate for
the crucial role of the research direction of parameter symmetries in unifying
these fragmented theories. This position is founded on a centralizing
hypothesis for this direction: parameter symmetry breaking and restoration are
the unifying mechanisms underlying the hierarchical learning behavior of AI
models. We synthesize prior observations and theories to argue that this
direction of research could lead to a unified understanding of three distinct
hierarchies in neural networks: learning dynamics, model complexity, and
representation formation. By connecting these hierarchies, our position paper
elevates symmetry -- a cornerstone of theoretical physics -- to become a
potential fundamental principle in modern AI.

</details>


### [731] [Normalized AOPC: Fixing Misleading Faithfulness Metrics for Feature Attribution Explainability](https://arxiv.org/pdf/2408.08137)
*Joakim Edin, Andreas Geert Motzfeldt, Casper L. Christensen, Tuukka Ruotsalo, Lars Maaløe, Maria Maistro*

Main category: cs.LG

TL;DR: The paper highlights issues with using AOPC for comparing feature attribution faithfulness across models and proposes NAOPC, a normalized version, for more reliable evaluations.


<details>
  <summary>Details</summary>
Motivation: Deep neural networks' predictions lack interpretability, and feature attribution methods like AOPC are used to explain them. However, AOPC's sensitivity to model variations makes cross-model comparisons unreliable.

Method: The authors propose Normalized AOPC (NAOPC), a normalization approach to address AOPC's limitations, enabling consistent cross-model evaluations.

Result: Experiments show NAOPC significantly alters AOPC results, challenging prior studies and providing a robust framework for assessing faithfulness.

Conclusion: NAOPC offers a more reliable and interpretable method for evaluating feature attribution faithfulness, questioning earlier conclusions based on AOPC.

Abstract: Deep neural network predictions are notoriously difficult to interpret.
Feature attribution methods aim to explain these predictions by identifying the
contribution of each input feature. Faithfulness, often evaluated using the
area over the perturbation curve (AOPC), reflects feature attributions'
accuracy in describing the internal mechanisms of deep neural networks.
However, many studies rely on AOPC to compare faithfulness across different
models, which we show can lead to false conclusions about models' faithfulness.
Specifically, we find that AOPC is sensitive to variations in the model,
resulting in unreliable cross-model comparisons. Moreover, AOPC scores are
difficult to interpret in isolation without knowing the model-specific lower
and upper limits. To address these issues, we propose a normalization approach,
Normalized AOPC (NAOPC), enabling consistent cross-model evaluations and more
meaningful interpretation of individual scores. Our experiments demonstrate
that this normalization can radically change AOPC results, questioning the
conclusions of earlier studies and offering a more robust framework for
assessing feature attribution faithfulness. Our code is available at
https://github.com/JoakimEdin/naopc.

</details>


### [732] [Streaming Attention Approximation via Discrepancy Theory](https://arxiv.org/pdf/2502.07861)
*Insu Han, Michael Kapralov, Ekaterina Kochetkova, Kshiteej Sheth, Amir Zandieh*

Main category: cs.LG

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: Large language models (LLMs) have achieved impressive success, but their high
memory requirements present challenges for long-context token generation. In
this paper we study the streaming complexity of attention approximation, a key
computational primitive underlying token generation.
  Our main contribution is BalanceKV, a streaming algorithm for
$\epsilon$-approximating attention computations based on geometric process for
selecting a balanced collection of Key and Value tokens as per Banaszczyk's
vector balancing theory. We complement our algorithm with space lower bounds
for streaming attention computation. Besides strong theoretical guarantees,
BalanceKV exhibits empirically validated performance improvements over existing
methods, both for attention approximation and end-to-end performance on various
long context benchmarks.

</details>


### [733] [MotifDisco: Motif Causal Discovery For Time Series Motifs](https://arxiv.org/pdf/2409.15219)
*Josephine Lamp, Mark Derdzinski, Christopher Hannemann, Sam Hatfield, Joost van der Linden*

Main category: cs.LG

TL;DR: The paper introduces MotifDisco, a framework for discovering causal relationships between motifs in time series, specifically applied to health data like glucose traces.


<details>
  <summary>Details</summary>
Motivation: Understanding causal relationships among motifs in time series (e.g., glucose data) can improve deep learning models and applications like personalized coaching.

Method: Develops Motif Causality (MC), inspired by Granger Causality and Transfer Entropy, using a Graph Neural Network for unsupervised link prediction.

Result: MC significantly improves performance in forecasting, anomaly detection, and clustering tasks on health data.

Conclusion: MotifDisco effectively captures causal relationships among motifs, enhancing downstream applications in health data analysis.

Abstract: Many time series, particularly health data streams, can be best understood as
a sequence of phenomenon or events, which we call \textit{motifs}. A time
series motif is a short trace segment which may implicitly capture an
underlying phenomenon within the time series. Specifically, we focus on glucose
traces collected from continuous glucose monitors (CGMs), which inherently
contain motifs representing underlying human behaviors such as eating and
exercise. The ability to identify and quantify \textit{causal} relationships
amongst motifs can provide a mechanism to better understand and represent these
patterns, useful for improving deep learning and generative models and for
advanced technology development (e.g., personalized coaching and artificial
insulin delivery systems). However, no previous work has developed causal
discovery methods for time series motifs. Therefore, in this paper we develop
MotifDisco (\textbf{motif} \textbf{disco}very of causality), a novel causal
discovery framework to learn causal relations amongst motifs from time series
traces. We formalize a notion of \textit{Motif Causality (MC)}, inspired from
Granger Causality and Transfer Entropy, and develop a Graph Neural
Network-based framework that learns causality between motifs by solving an
unsupervised link prediction problem. We integrate MC with three model use
cases of forecasting, anomaly detection and clustering, to showcase the use of
MC as a building block for downstream tasks. Finally, we evaluate our framework
on different health data streams and find that Motif Causality provides a
significant performance improvement in all use cases.

</details>


### [734] [SetPINNs: Set-based Physics-informed Neural Networks](https://arxiv.org/pdf/2409.20206)
*Mayank Nagda, Phil Ostheimer, Thomas Specht, Frank Rhein, Fabian Jirasek, Stephan Mandt, Marius Kloft, Sophie Fellenz*

Main category: cs.LG

TL;DR: SetPINNs improve PINNs by capturing local dependencies using a finite element-inspired sampling scheme, leading to better accuracy and efficiency.


<details>
  <summary>Details</summary>
Motivation: Conventional PINNs neglect dependencies within a domain, resulting in suboptimal solutions.

Method: SetPINNs partition the domain into sets to model local dependencies and enforce physical laws, with a rigorous theoretical analysis.

Result: SetPINNs provide unbiased, lower-variance estimates, improving domain coverage and reducing residual error.

Conclusion: SetPINNs outperform conventional PINNs in accuracy, efficiency, and robustness.

Abstract: Physics-Informed Neural Networks (PINNs) solve partial differential equations
using deep learning. However, conventional PINNs perform pointwise predictions
that neglect dependencies within a domain, which may result in suboptimal
solutions. We introduce SetPINNs, a framework that effectively captures local
dependencies. With a finite element-inspired sampling scheme, we partition the
domain into sets to model local dependencies while simultaneously enforcing
physical laws. We provide a rigorous theoretical analysis showing that SetPINNs
yield unbiased, lower-variance estimates of residual energy and its gradients,
ensuring improved domain coverage and reduced residual error. Extensive
experiments on synthetic and real-world tasks show improved accuracy,
efficiency, and robustness.

</details>


### [735] [FuncGenFoil: Airfoil Generation and Editing Model in Function Space](https://arxiv.org/pdf/2502.10712)
*Jinouwen Zhang, Junjie Ren, Aobo Yang, Yan Lu, Lu Chen, Hairun Xie, Jing Wang, Miao Zhang, Wanli Ouyang, Shixiang Tang*

Main category: cs.LG

TL;DR: FuncGenFoil is a function-space generative model for high-fidelity airfoil design, outperforming existing methods in accuracy and diversity.


<details>
  <summary>Details</summary>
Motivation: Existing deep learning methods for airfoil design struggle with balancing expressive power and resolution adaptability.

Method: FuncGenFoil directly reconstructs airfoil geometries as function curves, combining the benefits of parametric functions and discrete point-based representations.

Result: FuncGenFoil achieves a 74.4% reduction in label error and a 23.2% increase in diversity on the AF-200K dataset.

Conclusion: Function-space modeling offers a powerful and flexible framework for aerodynamic shape optimization.

Abstract: Aircraft manufacturing is the jewel in the crown of industry, in which
generating high-fidelity airfoil geometries with controllable and editable
representations remains a fundamental challenge. Existing deep learning
methods, which typically rely on predefined parametric representations (e.g.,
B\'ezier) or discrete point sets, face an inherent trade-off between expressive
power and resolution adaptability. To tackle this challenge, we introduce
FuncGenFoil, a novel function-space generative model that directly reconstructs
airfoil geometries as function curves. Our method inherits the advantages of
arbitrary-resolution sampling and smoothness from parametric functions, as well
as the strong expressiveness of discrete point-based representations. Empirical
evaluations demonstrate that FuncGenFoil improves upon state-of-the-art methods
in airfoil generation, achieving a relative 74.4% reduction in label error and
a 23.2% increase in diversity on the AF-200K dataset. Our results highlight the
advantages of function-space modeling for aerodynamic shape optimization,
offering a powerful and flexible framework for high-fidelity airfoil design.

</details>


### [736] [Score-based Pullback Riemannian Geometry: Extracting the Data Manifold Geometry using Anisotropic Flows](https://arxiv.org/pdf/2410.01950)
*Willem Diepeveen, Georgios Batzolis, Zakhar Shumaylov, Carola-Bibiane Schönlieb*

Main category: cs.LG

TL;DR: A scalable framework for data-driven Riemannian geometry using score-based pullback methods, integrating generative models and closed-form geodesics for efficient manifold learning.


<details>
  <summary>Details</summary>
Motivation: To balance efficient manifold mappings with scalable training algorithms for interpretable representation learning.

Method: Integrates pullback Riemannian geometry and generative models, proposing score-based Riemannian structures with closed-form geodesics and constructing a Riemannian autoencoder (RAE) with error bounds.

Result: Demonstrates high-quality geodesics, reliable intrinsic dimension estimation, and global manifold charting on diverse datasets.

Conclusion: The first scalable framework for extracting complete data manifold geometry, effective for unimodal distributions and downstream tasks.

Abstract: Data-driven Riemannian geometry has emerged as a powerful tool for
interpretable representation learning, offering improved efficiency in
downstream tasks. Moving forward, it is crucial to balance cheap manifold
mappings with efficient training algorithms. In this work, we integrate
concepts from pullback Riemannian geometry and generative models to propose a
framework for data-driven Riemannian geometry that is scalable in both geometry
and learning: score-based pullback Riemannian geometry. Focusing on unimodal
distributions as a first step, we propose a score-based Riemannian structure
with closed-form geodesics that pass through the data probability density. With
this structure, we construct a Riemannian autoencoder (RAE) with error bounds
for discovering the correct data manifold dimension. This framework can
naturally be used with anisotropic normalizing flows by adopting isometry
regularization during training. Through numerical experiments on diverse
datasets, including image data, we demonstrate that the proposed framework
produces high-quality geodesics passing through the data support, reliably
estimates the intrinsic dimension of the data manifold, and provides a global
chart of the manifold. To the best of our knowledge, this is the first scalable
framework for extracting the complete geometry of the data manifold.

</details>


### [737] [Fundamental Limitations on Subquadratic Alternatives to Transformers](https://arxiv.org/pdf/2410.04271)
*Josh Alman, Hantao Yu*

Main category: cs.LG

TL;DR: The paper proves that subquadratic-time models, like Mamba, cannot perform document similarity tasks as effectively as Transformers, which require quadratic time.


<details>
  <summary>Details</summary>
Motivation: To address the inefficiency of attention mechanisms in Transformers and explore whether faster alternatives can match their capabilities.

Method: Theoretical proof focusing on document similarity tasks, comparing Transformers with subquadratic-time models.

Result: Transformers can perform document similarity tasks, but no subquadratic-time model can achieve this.

Conclusion: For tasks involving document similarity, Transformers' quadratic time is unavoidable, and faster alternatives fall short.

Abstract: The Transformer architecture is widely deployed in many popular and impactful
Large Language Models. At its core is the attention mechanism for calculating
correlations between pairs of tokens. Performing an attention computation takes
quadratic time in the input size, and had become the time bottleneck for
transformer operations. In order to circumvent this, researchers have used a
variety of approaches, including designing heuristic algorithms for performing
attention computations faster, and proposing alternatives to the attention
mechanism which can be computed more quickly. For instance, state space models
such as Mamba were designed to replace attention with an almost linear time
alternative.
  In this paper, we prove that any such approach cannot perform important tasks
that Transformer is able to perform (assuming a popular conjecture from
fine-grained complexity theory). We focus on document similarity tasks, where
one is given as input many documents and would like to find a pair which is
(approximately) the most similar. We prove that Transformer is able to perform
this task, and we prove that this task cannot be performed in truly
subquadratic time by any algorithm. Thus, any model which can be evaluated in
subquadratic time - whether because of subquadratic-time heuristics for
attention, faster attention replacements like Mamba, or any other reason -
cannot perform this task. In other words, in order to perform tasks that
(implicitly or explicitly) involve document similarity, one may as well use
Transformer and cannot avoid its quadratic running time.

</details>


### [738] [FoLDTree: A ULDA-Based Decision Tree Framework for Efficient Oblique Splits and Feature Selection](https://arxiv.org/pdf/2410.23147)
*Siyu Wang, Kehui Yao*

Main category: cs.LG

TL;DR: LDATree and FoLDTree are new oblique decision tree frameworks using ULDA and Forward ULDA, outperforming traditional methods in accuracy and handling missing values.


<details>
  <summary>Details</summary>
Motivation: Traditional decision trees struggle with oblique decision boundaries, while existing oblique methods face computational and multi-class challenges.

Method: Integrates Uncorrelated Linear Discriminant Analysis (ULDA) and Forward ULDA into decision trees for efficient oblique splits, feature selection, and missing value handling.

Result: LDATree and FoLDTree outperform axis-orthogonal and other oblique methods, matching random forest accuracy.

Conclusion: These frameworks are robust alternatives to traditional single-tree methods.

Abstract: Traditional decision trees are limited by axis-orthogonal splits, which can
perform poorly when true decision boundaries are oblique. While oblique
decision tree methods address this limitation, they often face high
computational costs, difficulties with multi-class classification, and a lack
of effective feature selection. In this paper, we introduce LDATree and
FoLDTree, two novel frameworks that integrate Uncorrelated Linear Discriminant
Analysis (ULDA) and Forward ULDA into a decision tree structure. These methods
enable efficient oblique splits, handle missing values, support feature
selection, and provide both class labels and probabilities as model outputs.
Through evaluations on simulated and real-world datasets, LDATree and FoLDTree
consistently outperform axis-orthogonal and other oblique decision tree
methods, achieving accuracy levels comparable to the random forest. The results
highlight the potential of these frameworks as robust alternatives to
traditional single-tree methods.

</details>


### [739] [OneProt: Towards Multi-Modal Protein Foundation Models](https://arxiv.org/pdf/2411.04863)
*Klemens Flöge, Srisruthi Udayakumar, Johanna Sommer, Marie Piraud, Stefan Kesselheim, Vincent Fortuin, Stephan Günneman, Karel J van der Weg, Holger Gohlke, Erinc Merdivan, Alina Bazarova*

Main category: cs.LG

TL;DR: OneProt is a multi-modal AI for proteins integrating structural, sequence, text, and binding site data, using ImageBind for latent space alignment. It excels in retrieval tasks and downstream applications like enzyme function prediction.


<details>
  <summary>Details</summary>
Motivation: To advance multi-modal AI for proteins by integrating diverse data types and improving performance in protein-related tasks.

Method: Uses ImageBind for lightweight fine-tuning, aligning latent spaces with sequence data, combining Graph Neural Networks and transformers.

Result: Strong performance in retrieval tasks, improved sequence distinction, and binding site encoder's significance highlighted.

Conclusion: OneProt expands multi-modal protein models, enabling transformative applications in drug discovery and protein engineering.

Abstract: Recent advances in Artificial Intelligence have enabled multi-modal systems
to model and translate diverse information spaces. Extending beyond text and
vision, we introduce OneProt, a multi-modal AI for proteins that integrates
structural, sequence, text, and binding site data. Using the ImageBind
framework, OneProt aligns the latent spaces of protein modality encoders in a
lightweight fine-tuning scheme that focuses on pairwise alignment with sequence
data rather than requiring full matches. This novel approach comprises a mix of
Graph Neural Networks and transformer architectures. It demonstrates strong
performance in retrieval tasks and showcases the efficacy of multi-modal
systems in Protein Machine Learning through a broad spectrum of downstream
baselines, including enzyme function prediction and binding site analysis.
Furthermore, OneProt enables the transfer of representational information from
specialized encoders to the sequence encoder, enhancing capabilities for
distinguishing evolutionarily related and unrelated sequences and exhibiting
representational properties where evolutionarily related proteins align in
similar directions within the latent space. In addition, we extensively
investigate modality ablations to identify the encoders that contribute most to
predictive performance, highlighting the significance of the binding site
encoder, which has not been used in similar models previously. This work
expands the horizons of multi-modal protein models, paving the way for
transformative applications in drug discovery, biocatalytic reaction planning,
and protein engineering.

</details>


### [740] [Steering Language Model Refusal with Sparse Autoencoders](https://arxiv.org/pdf/2411.11296)
*Kyle O'Brien, David Majercak, Xavier Fernandes, Richard Edgar, Blake Bullwinkel, Jingya Chen, Harsha Nori, Dean Carignan, Eric Horvitz, Forough Poursabzi-Sangde*

Main category: cs.LG

TL;DR: The paper explores using sparse autoencoder (SAE) features to steer language model activations for safety, revealing tradeoffs between safety improvements and general performance degradation.


<details>
  <summary>Details</summary>
Motivation: To enhance language model safety without modifying weights, focusing on steering activations via SAE features.

Method: Amplifying SAE features at inference time to mediate refusal behavior.

Result: Improved robustness against jailbreak attempts but caused systematic performance degradation across benchmarks.

Conclusion: SAE-based steering shows promise but requires deeper understanding of feature entanglement and tradeoffs for practical deployment.

Abstract: Responsible deployment of language models requires mechanisms for refusing
unsafe prompts while preserving model performance. While most approaches modify
model weights through additional training, we explore an alternative: steering
model activations at inference time via amplifying sparse autoencoder (SAE)
features that mediate refusal. This work uncovers a fundamental tension between
SAE steering-based safety improvements and general model capabilities. While
feature steering successfully improves robustness against both single-turn and
challenging multi-turn jailbreak attempts, we discover that this comes at a
previously underexplored cost -- systematic degradation of performance across
multiple benchmark tasks, even on safe inputs with no apparent connection to
refusal behavior. This suggests that features mediating refusal may be more
deeply entangled with general language model capabilities than previously
understood. Our findings reveal important open questions about the nature of
safety-relevant features in language models and the feasibility of isolating
them for targeted intervention. While SAE-based steering shows promise as a
flexible approach to enhancing language model safety, our results highlight the
critical need to understand and address the mechanisms behind these capability
tradeoffs before such techniques can be practically deployed.

</details>


### [741] [Attention-Based Reconstruction of Full-Field Tsunami Waves from Sparse Tsunameter Networks](https://arxiv.org/pdf/2411.12948)
*Edward McDugald, Arvind Mohan, Darren Engwirda, Agnese Marcato, Javier Santos*

Main category: cs.LG

TL;DR: The Senseiver, an attention-based neural network, improves tsunami forecasting by reconstructing high-resolution wavefields from sparse data, even for untrained epicenters, outperforming traditional methods.


<details>
  <summary>Details</summary>
Motivation: To enhance tsunami forecasting accuracy by leveraging sparse tsunameter data, especially for scenarios not covered in training.

Method: Uses the Senseiver architecture to reconstruct high-resolution tsunami wavefields from sparse observations, testing cases with untrained epicenters.

Result: Outperforms Linear Interpolation with Huygens-Fresnel Principle, achieving higher accuracy in dense observation networks.

Conclusion: The Senseiver offers a promising approach for sparse sensing in tsunami forecasting, improving accuracy and generalizability.

Abstract: We investigate the potential of an attention-based neural network
architecture, the Senseiver, for sparse sensing in tsunami forecasting.
Specifically, we focus on the Tsunami Data Assimilation Method, which generates
forecasts from tsunameter networks. Our model is used to reconstruct
high-resolution tsunami wavefields from extremely sparse observations,
including cases where the tsunami epicenters are not represented in the
training set. Furthermore, we demonstrate that our approach significantly
outperforms the Linear Interpolation with Huygens-Fresnel Principle in
generating dense observation networks, achieving markedly improved accuracy.

</details>


### [742] [Load Forecasting for Households and Energy Communities: Are Deep Learning Models Worth the Effort?](https://arxiv.org/pdf/2501.05000)
*Lukas Moosbrugger, Valentin Seiler, Philipp Wohlgenannt, Sebastian Hegenbart, Sashko Ristov, Elias Eder, Peter Kepplinger*

Main category: cs.LG

TL;DR: The study evaluates deep learning models (LSTM, xLSTM, Transformer) vs. traditional methods (KNN, persistence) for short-term load forecasting in energy communities (ECs). Transfer learning improves accuracy, but simpler methods outperform deep learning with limited data. KNN is a robust alternative, achieving near-deep learning savings (8.01% vs. 8.06%).


<details>
  <summary>Details</summary>
Motivation: ECs need accurate load forecasting for demand-side management in decentralized energy systems. Deep learning's potential is underexplored in practical contexts.

Method: Evaluated deep learning models (LSTM, xLSTM, Transformer) against benchmarks (KNN, persistence) under varying conditions (community size, data availability, model complexity). Used transfer learning with synthetic data.

Result: Transfer learning improves accuracy by 1.97%pt with limited data. Persistence models outperform deep learning with <6 months of data. KNN achieves 8.01% cost savings, close to deep learning's 8.06%.

Conclusion: Deep learning's complexity is justified only with sufficient data. KNN is a robust alternative. Findings guide ECs on method selection, balancing accuracy and complexity.

Abstract: Energy communities (ECs) play a key role in enabling local demand shifting
and enhancing self-sufficiency, as energy systems transition toward
decentralized structures with high shares of renewable generation. To optimally
operate them, accurate short-term load forecasting is essential, particularly
for implementing demand-side management strategies. With the recent rise of
deep learning methods, data-driven forecasting has gained significant
attention, however, it remains insufficiently explored in many practical
contexts. Therefore, this study evaluates the effectiveness of state-of-the-art
deep learning models -- including LSTM, xLSTM, and Transformer architectures --
compared to traditional benchmarks such as K-Nearest Neighbors (KNN) and
persistence forecasting, across varying community size, historical data
availability, and model complexity. Additionally, we assess the benefits of
transfer learning using publicly available synthetic load profiles. On average,
transfer learning improves the normalized mean absolute error by 1.97%pt when
only two months of training data are available. Interestingly, for less than
six months of training data, simple persistence models outperform deep learning
architectures in forecast accuracy. The practical value of improved forecasting
is demonstrated using a mixed-integer linear programming optimization for ECs
with a shared battery energy storage system. The most accurate deep learning
model achieves an average reduction in financial energy costs of 8.06%.
Notably, a simple KNN approach achieves average savings of 8.01%, making it a
competitive and robust alternative. All implementations are publicly available
to facilitate reproducibility. These findings offer actionable insights for
ECs, and they highlight when the additional complexity of deep learning is
warranted by performance gains.

</details>


### [743] [Causality Is Key to Understand and Balance Multiple Goals in Trustworthy ML and Foundation Models](https://arxiv.org/pdf/2502.21123)
*Ruta Binkyte, Ivaxi Sheth, Zhijing Jin, Mohammad Havaei, Bernhard Schölkopf, Mario Fritz*

Main category: cs.LG

TL;DR: The paper advocates using causal methods in ML to balance trustworthiness principles like fairness, privacy, robustness, accuracy, and explainability, addressing conflicts and enhancing reliability.


<details>
  <summary>Details</summary>
Motivation: Trustworthy ML is critical in high-stakes domains, but current approaches often address key principles in isolation, leading to conflicts.

Method: Integrate causal methods into ML to align and balance competing objectives like fairness, privacy, and robustness.

Result: Causal approaches can enhance reliability and interpretability in ML and foundation models.

Conclusion: Adopting causal frameworks presents challenges but offers opportunities for more accountable and ethical AI systems.

Abstract: Ensuring trustworthiness in machine learning (ML) systems is crucial as they
become increasingly embedded in high-stakes domains. This paper advocates for
integrating causal methods into machine learning to navigate the trade-offs
among key principles of trustworthy ML, including fairness, privacy,
robustness, accuracy, and explainability. While these objectives should ideally
be satisfied simultaneously, they are often addressed in isolation, leading to
conflicts and suboptimal solutions. Drawing on existing applications of
causality in ML that successfully align goals such as fairness and accuracy or
privacy and robustness, this paper argues that a causal approach is essential
for balancing multiple competing objectives in both trustworthy ML and
foundation models. Beyond highlighting these trade-offs, we examine how
causality can be practically integrated into ML and foundation models, offering
solutions to enhance their reliability and interpretability. Finally, we
discuss the challenges, limitations, and opportunities in adopting causal
frameworks, paving the way for more accountable and ethically sound AI systems.

</details>


### [744] [FBQuant: FeedBack Quantization for Large Language Models](https://arxiv.org/pdf/2501.16385)
*Yijiang Liu, Hengyu Fang, Liulu He, Rongyu Zhang, Yichuan Bai, Yuan Du, Li Du*

Main category: cs.LG

TL;DR: FBQuant is a novel quantization method for deploying LLMs on edge devices, reducing memory access and improving accuracy with feedback mechanisms and efficient CUDA kernels.


<details>
  <summary>Details</summary>
Motivation: On-device deployment of LLMs is hindered by memory bandwidth constraints and accuracy degradation from quantization. Existing methods lack robust optimization or efficient objectives.

Method: Proposes FBQuant, inspired by negative feedback in control systems, to bound weight reconstruction and reduce overfitting. Includes an optimized CUDA kernel to minimize latency.

Result: FBQuant improves zero-shot accuracy by 1.2% for 3-bit Llama2-7B and reduces extra inference time by 60%.

Conclusion: FBQuant effectively addresses quantization challenges for LLMs on edge devices, balancing accuracy and efficiency.

Abstract: Deploying Large Language Models (LLMs) on edge devices is increasingly
important, as it eliminates reliance on network connections, reduces expensive
API calls, and enhances user privacy. However, on-device deployment is
challenging due to the limited computational resources of edge devices. In
particular, the key bottleneck stems from memory bandwidth constraints related
to weight loading. Weight-only quantization effectively reduces memory access,
yet often induces significant accuracy degradation. Recent efforts to
incorporate sub-branches have shown promise for mitigating quantization errors,
but these methods either lack robust optimization strategies or rely on
suboptimal objectives. To address these gaps, we propose FeedBack Quantization
(FBQuant), a novel approach inspired by negative feedback mechanisms in
automatic control. FBQuant inherently ensures that the reconstructed weights
remain bounded by the quantization process, thereby reducing the risk of
overfitting. To further offset the additional latency introduced by
sub-branches, we develop an efficient CUDA kernel that decreases 60% of extra
inference time. Comprehensive experiments demonstrate the efficiency and
effectiveness of FBQuant across various LLMs. Notably, for 3-bit Llama2-7B,
FBQuant improves zero-shot accuracy by 1.2%.

</details>


### [745] [HopCast: Calibration of Autoregressive Dynamics Models](https://arxiv.org/pdf/2501.16587)
*Muhammad Bilal Shahid, Cody Fleming*

Main category: cs.LG

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: Deep learning models are often trained to approximate dynamical systems that
can be modeled using differential equations. Many of these models are optimized
to predict one step ahead; such approaches produce calibrated one-step
predictions if the predictive model can quantify uncertainty, such as Deep
Ensembles. At inference time, multi-step predictions are generated via
autoregression, which needs a sound uncertainty propagation method to produce
calibrated multi-step predictions. This work introduces an alternative
Predictor-Corrector approach named \hop{} that uses Modern Hopfield Networks
(MHN) to learn the errors of a deterministic Predictor that approximates the
dynamical system. The Corrector predicts a set of errors for the Predictor's
output based on a context state at any timestep during autoregression. The set
of errors creates sharper and well-calibrated prediction intervals with higher
predictive accuracy compared to baselines without uncertainty propagation. The
calibration and prediction performances are evaluated across a set of dynamical
systems. This work is also the first to benchmark existing uncertainty
propagation methods based on calibration errors.

</details>


### [746] [Provably Correct Automata Embeddings for Optimal Automata-Conditioned Reinforcement Learning](https://arxiv.org/pdf/2503.05042)
*Beyazit Yalcinkaya, Niklas Lauffer, Marcell Vazquez-Chanlatte, Sanjit A. Seshia*

Main category: cs.LG

TL;DR: The paper provides a theoretical framework for automata-conditioned RL, proving its learnability and introducing a method for correct automata embeddings.


<details>
  <summary>Details</summary>
Motivation: To address the lack of theoretical guarantees in automata-conditioned RL for multi-task policies.

Method: Proposes a theoretical framework and a technique for learning provably correct automata embeddings.

Result: Shows that automata-conditioned RL is probably approximately correct learnable and guarantees optimal multi-task policy learning.

Conclusion: Experimental results confirm the theoretical findings, validating the proposed framework and method.

Abstract: Automata-conditioned reinforcement learning (RL) has given promising results
for learning multi-task policies capable of performing temporally extended
objectives given at runtime, done by pretraining and freezing automata
embeddings prior to training the downstream policy. However, no theoretical
guarantees were given. This work provides a theoretical framework for the
automata-conditioned RL problem and shows that it is probably approximately
correct learnable. We then present a technique for learning provably correct
automata embeddings, guaranteeing optimal multi-task policy learning. Our
experimental evaluation confirms these theoretical results.

</details>


### [747] [Optimizing Large Language Model Training Using FP4 Quantization](https://arxiv.org/pdf/2501.17116)
*Ruizhe Wang, Yeyun Gong, Xiao Liu, Guoshuai Zhao, Ziyue Yang, Baining Guo, Zhengjun Zha, Peng Cheng*

Main category: cs.LG

TL;DR: The paper introduces the first FP4 training framework for LLMs, addressing quantization challenges with innovations like a differentiable estimator and outlier handling, achieving accuracy close to higher precisions.


<details>
  <summary>Details</summary>
Motivation: The computational demands of training large language models (LLMs) require more efficient methods, with FP4 offering potential but facing challenges like quantization errors.

Method: The framework uses a differentiable quantization estimator, outlier clamping, mixed-precision training, and vector-wise quantization.

Result: The FP4 framework achieves accuracy comparable to BF16 and FP8, scaling to 13B-parameter LLMs trained on 100B tokens.

Conclusion: The work lays a foundation for efficient ultra-low precision training, aligning with next-generation hardware support for FP4.

Abstract: The growing computational demands of training large language models (LLMs)
necessitate more efficient methods. Quantized training presents a promising
solution by enabling low-bit arithmetic operations to reduce these costs. While
FP8 precision has demonstrated feasibility, leveraging FP4 remains a challenge
due to significant quantization errors and limited representational capacity.
This work introduces the first FP4 training framework for LLMs, addressing
these challenges with two key innovations: a differentiable quantization
estimator for precise weight updates and an outlier clamping and compensation
strategy to prevent activation collapse. To ensure stability, the framework
integrates a mixed-precision training scheme and vector-wise quantization.
Experimental results demonstrate that our FP4 framework achieves accuracy
comparable to BF16 and FP8, with minimal degradation, scaling effectively to
13B-parameter LLMs trained on up to 100B tokens. With the emergence of
next-generation hardware supporting FP4, our framework sets a foundation for
efficient ultra-low precision training.

</details>


### [748] [Capacity-Aware Inference: Mitigating the Straggler Effect in Mixture of Experts](https://arxiv.org/pdf/2503.05066)
*Shwai He, Weilin Cai, Jiayi Huang, Ang Li*

Main category: cs.LG

TL;DR: The paper addresses the Straggler Effect in Mixture of Experts (MoE) models by proposing two methods, Capacity-Aware Token Drop and Capacity-Aware Expanded Drop, to improve load balance and inference efficiency.


<details>
  <summary>Details</summary>
Motivation: MoE models suffer from inefficiencies due to imbalanced token-to-expert assignments, causing delays (Straggler Effect).

Method: Proposes two techniques: Capacity-Aware Token Drop (discards excess tokens) and Capacity-Aware Expanded Drop (expands token candidate sets).

Result: Achieves significant speedups (e.g., 30% speedup with 0.9% performance drop) and improved expert utilization.

Conclusion: The methods effectively mitigate the Straggler Effect, enhancing MoE model efficiency and performance.

Abstract: The Mixture of Experts (MoE) is an effective architecture for scaling large
language models by leveraging sparse expert activation to balance performance
and efficiency. However, under expert parallelism, MoE suffers from inference
inefficiencies due to imbalanced token-to-expert assignment, where underloaded
experts complete computations early but must wait for overloaded experts,
leading to global delays. We define this phenomenon as the
\textbf{\textit{Straggler Effect}}, as the most burdened experts dictate the
overall inference latency. To address this, we first propose
\textit{\textbf{Capacity-Aware Token Drop}}, which enforces expert capacity
limits by discarding excess tokens from overloaded experts, effectively
reducing load imbalance with minimal performance impact (e.g., $30\%$ speedup
with only $0.9\%$ degradation on OLMoE). Next, given the presence of low-load
experts remaining well below the capacity threshold, we introduce
\textit{\textbf{Capacity-Aware Expanded Drop}}, which allows tokens to include
additional local experts in their candidate set before enforcing strict local
capacity constraints, thereby improving load balance and enhancing the
utilization of underused experts. Extensive experiments on both language and
multimodal MoE models demonstrate the effectiveness of our approach, yielding
substantial gains in expert utilization, model performance, and inference
efficiency, e.g., applying Expanded Drop to Mixtral-8$\times$7B-Instruct yields
a {0.2\%} average performance improvement and a {1.85$\times$} inference
speedup.

</details>


### [749] [WILDCHAT-50M: A Deep Dive Into the Role of Synthetic Data in Post-Training](https://arxiv.org/pdf/2501.18511)
*Benjamin Feuer, Chinmay Hegde*

Main category: cs.LG

TL;DR: The paper introduces WILDCHAT-50M, the largest public chat dataset, to address the lack of open science in LLM post-training techniques. It includes responses from over 50 models and demonstrates its utility by outperforming existing SFT mixtures.


<details>
  <summary>Details</summary>
Motivation: The open science supporting LLM post-training techniques is limited due to challenges in large-scale comparative analyses of synthetic data and LLM judges.

Method: Extends the WildChat dataset with responses from 50+ models (0.5B to 104B parameters) and conducts comparative analyses. Creates RE-WILD, a public SFT mix.

Result: RE-WILD outperforms the Tulu-3 SFT mixture with 40% fewer samples.

Conclusion: WILDCHAT-50M and RE-WILD advance open science in LLM post-training, enabling better comparative analyses and model refinement.

Abstract: Language model (LLM) post-training, from DPO to distillation, can refine
behaviors and unlock new skills, but the open science supporting these
post-training techniques is still in its infancy. One limiting factor has been
the difficulty of conducting large-scale comparative analyses of synthetic data
generating models and LLM judges. To close this gap, we introduce WILDCHAT-50M,
the largest public chat dataset to date. We extend the existing WildChat
dataset to include responses not only from GPT, but from over 50 different
open-weight models, ranging in size from 0.5B to 104B parameters. We conduct an
extensive comparative analysis and demonstrate the potential of this dataset by
creating RE-WILD, our own public SFT mix, which outperforms the recent Tulu-3
SFT mixture from Allen AI with only 40% as many samples. Our dataset, samples
and code are available at https://github.com/penfever/wildchat-50m.

</details>


### [750] [Elucidating Subspace Perturbation in Zeroth-Order Optimization: Theory and Practice at Scale](https://arxiv.org/pdf/2501.19099)
*Sihwan Park, Jihun Yun, SungYub Kim, Souvik Kundu, Eunho Yang*

Main category: cs.LG

TL;DR: A unified theoretical framework for Zeroth-order (ZO) optimization under subspace perturbations is developed, showing high dimensionality as a bottleneck and introducing subspace alignment to reduce gradient noise. A practical method, MeZO-BCD, is proposed, achieving significant speedup in optimization.


<details>
  <summary>Details</summary>
Motivation: ZO optimization is promising but suffers from slow convergence due to high-variance gradient estimators. Subspace perturbations are explored, but their effectiveness is unclear.

Method: A unified framework analyzes convergence and generalization of ZO optimization under subspace perturbations. The method MeZO-BCD uses block coordinate descent for efficiency.

Result: MeZO-BCD achieves up to 2.77x speedup over MeZO on OPT-13B, maintaining performance.

Conclusion: Subspace alignment reduces gradient noise, and MeZO-BCD is an efficient ZO method for practical use.

Abstract: Zeroth-order (ZO) optimization has emerged as a promising alternative to
gradient-based backpropagation methods, particularly for black-box optimization
and large language model (LLM) fine-tuning. However, ZO methods often suffer
from slow convergence due to high-variance stochastic gradient estimators.
While subspace perturbations, such as sparsity and low-rank constraints, have
been explored to mitigate this issue, their effectiveness remains poorly
understood. In this work, we develop a \emph{unified theoretical framework}
that analyzes both the convergence and generalization properties of ZO
optimization under subspace perturbations. We show that high dimensionality is
the primary bottleneck and introduce the notion of \textit{subspace alignment}
to explain how the subspace perturbations reduce gradient noise and accelerate
convergence. Our analysis further shows that a broad class of subspace
perturbations exhibits a similar convergence rate, motivating us to prioritize
practical considerations in real-world algorithm design. Building on these
insights, we propose an efficient ZO method using block coordinate descent
(MeZO-BCD), which perturbs and updates only a subset of parameters at each
step. Extensive experiments show that MeZO-BCD significantly accelerates
optimization, achieving up to $\mathbf{\times2.77}$ speedup in wall-clock time
over MeZO on OPT-13B, while maintaining comparable iteration complexity and
fine-tuning performance.

</details>


### [751] [Robust Weight Imprinting: Insights from Neural Collapse and Proxy-Based Aggregation](https://arxiv.org/pdf/2503.14572)
*Justus Westerhoff, Golzar Atefi, Mario Koddenbrock, Alexei Figueroa, Alexander Löser, Erik Rodner, Felix A. Gers*

Main category: cs.LG

TL;DR: The paper proposes a framework for weight imprinting, analyzing its components and introducing a novel variant that outperforms prior methods, showing a 4% improvement in complex scenarios.


<details>
  <summary>Details</summary>
Motivation: To systematically study weight imprinting, a method for adapting foundation models to new tasks, which has been reinvented but not thoroughly analyzed.

Method: The framework identifies three components: generation, normalization, and aggregation. It uses clustering for proxy determination and introduces a new imprinting variant.

Result: The method achieves up to 4% improvement in challenging scenarios with complex data distributions for new classes.

Conclusion: The study highlights the benefits of multiple proxies and proper normalization, linking imprinting to neural collapse, and releases code for public use.

Abstract: The capacity of a foundation model allows for adaptation to new downstream
tasks. Weight imprinting is a universal and efficient method to fulfill this
purpose. It has been reinvented several times, but it has not been
systematically studied. In this paper, we propose a framework for imprinting,
identifying three main components: generation, normalization, and aggregation.
This allows us to conduct an in-depth analysis of imprinting and a comparison
of the existing work. We reveal the benefits of representing novel data with
multiple proxies in the generation step and show the importance of proper
normalization. We determine proxies through clustering and propose a novel
variant of imprinting that outperforms previous work. We motivate this by the
neural collapse phenomenon -- an important connection that we can draw for the
first time. Our results show an increase of up to 4\% in challenging scenarios
with complex data distributions for new classes. Finally, we publicly release
our code at https://github.com/DATEXIS/multi-imprinting/.

</details>


### [752] [Flowing Through Layers: A Continuous Dynamical Systems Perspective on Transformers](https://arxiv.org/pdf/2502.05656)
*Jacob Fein-Ashley*

Main category: cs.LG

TL;DR: The paper interprets transformer layers as a continuous dynamical system, proving token representations converge to an ODE solution under Lipschitz conditions, with implications for stability and expressivity.


<details>
  <summary>Details</summary>
Motivation: To provide a theoretical foundation for transformer models by linking their discrete updates to continuous dynamical systems, clarifying stability and expressivity.

Method: Uses forward Euler discretization to interpret transformer layers as a dynamical system, with analysis under Lipschitz continuity and one-sided Lipschitz conditions.

Result: Token representations converge uniformly to an ODE solution, and perturbations decay exponentially under contractive dynamics.

Conclusion: The work connects transformers to dynamical systems, offering insights for improved convergence and architectural design.

Abstract: We show that the standard discrete update rule of transformer layers can be
naturally interpreted as a forward Euler discretization of a continuous
dynamical system. Our Transformer Flow Approximation Theorem demonstrates that,
under standard Lipschitz continuity assumptions, token representations converge
uniformly to the unique solution of an ODE as the number of layers grows.
Moreover, if the underlying mapping satisfies a one-sided Lipschitz condition
with a negative constant, the resulting dynamics are contractive, causing
perturbations to decay exponentially across layers. Beyond clarifying the
empirical stability and expressivity of transformer models, these insights link
transformer updates to a broader iterative reasoning framework, suggesting new
avenues for accelerated convergence and architectural innovations inspired by
dynamical systems theory.

</details>


### [753] [Causal Lifting of Neural Representations: Zero-Shot Generalization for Causal Inferences](https://arxiv.org/pdf/2502.06343)
*Riccardo Cadei, Ilker Demirel, Piersilvio De Bartolomeis, Lukas Lindorfer, Sylvia Cremer, Cordelia Schmid, Francesco Locatello*

Main category: cs.LG

TL;DR: The paper introduces Prediction-Powered Causal Inference (PPCI) to address data annotation costs by leveraging pre-trained models for causal effect estimation in unlabeled experiments.


<details>
  <summary>Details</summary>
Motivation: High costs of data annotation limit experimentation scale; PPCI offers a solution using pre-trained models for valid causal inferences.

Method: Proposes conditional calibration for valid PPCI, introduces causal lifting, and enforces it via Deconfounded Empirical Risk Minimization.

Result: Validated on synthetic and real-world data, solving zero-shot PPCI on ISTAnt dataset, outperforming vanilla methods.

Conclusion: PPCI with causal lifting and deconfounded training enables valid, scalable causal inferences, demonstrated on challenging datasets.

Abstract: In many scientific domains, the cost of data annotation limits the scale and
pace of experimentation. Yet, modern machine learning systems offer a promising
alternative, provided their predictions yield correct conclusions. We focus on
Prediction-Powered Causal Inferences (PPCI), i.e., estimating the treatment
effect in a target experiment with unlabeled factual outcomes, retrievable
zero-shot from a pre-trained model. We first identify the conditional
calibration property to guarantee valid PPCI at population level. Then, we
introduce causal lifting, a new causal lifting constraint transferring validity
across experiments, which we propose to enforce in practice in Deconfounded
Empirical Risk Minimization, our new model-agnostic training objective. We
validate our method on synthetic and real-world scientific data, offering
solutions to instances not solvable by vanilla Empirical Risk Minimization and
invariant training. In particular, we solve zero-shot PPCI on the ISTAnt
dataset for the first time, fine-tuning a foundational model on our replica
dataset of their ecological experiment with a different recording platform and
treatment.

</details>


### [754] [Partial-Label Learning with Conformal Candidate Cleaning](https://arxiv.org/pdf/2502.07661)
*Tobias Fuchs, Florian Kalinke*

Main category: cs.LG

TL;DR: The paper introduces a novel method for partial-label learning (PLL) that uses conformal prediction to prune ambiguous candidate labels, improving classifier accuracy without needing a labeled validation set.


<details>
  <summary>Details</summary>
Motivation: Real-world data often has ambiguous labels, and existing PLL methods rely on heuristics. This work aims to enhance PLL by systematically pruning candidate labels using conformal prediction.

Method: The proposed method alternates between training a PLL classifier to label a validation set, using these predictions for calibration, and pruning non-conformal candidate labels. It ensures conformal validity with respect to the unknown ground truth.

Result: Experiments on artificial and real-world data show significant improvements in test set accuracies for state-of-the-art PLL classifiers.

Conclusion: The method effectively enhances PLL by combining empirical risk minimization with conformal prediction-based pruning, achieving better accuracy while maintaining validity.

Abstract: Real-world data is often ambiguous; for example, human annotation produces
instances with multiple conflicting class labels. Partial-label learning (PLL)
aims at training a classifier in this challenging setting, where each instance
is associated with a set of candidate labels and one correct, but unknown,
class label. A multitude of algorithms targeting this setting exists and, to
enhance their prediction quality, several extensions that are applicable across
a wide range of PLL methods have been introduced. While many of these
extensions rely on heuristics, this article proposes a novel enhancing method
that incrementally prunes candidate sets using conformal prediction. To work
around the missing labeled validation set, which is typically required for
conformal prediction, we propose a strategy that alternates between training a
PLL classifier to label the validation set, leveraging these predicted class
labels for calibration, and pruning candidate labels that are not part of the
resulting conformal sets. In this sense, our method alternates between
empirical risk minimization and candidate set pruning. We establish that our
pruning method preserves the conformal validity with respect to the unknown
ground truth. Our extensive experiments on artificial and real-world data show
that the proposed approach significantly improves the test set accuracies of
several state-of-the-art PLL classifiers.

</details>


### [755] [MixMin: Finding Data Mixtures via Convex Minimization](https://arxiv.org/pdf/2502.10510)
*Anvith Thudi, Evianne Rovers, Yangjun Ruan, Tristan Thrush, Chris J. Maddison*

Main category: cs.LG

TL;DR: MixMin is a gradient-based method for optimizing data mixtures in machine learning, showing consistent improvements across tasks.


<details>
  <summary>Details</summary>
Motivation: Optimal data mixing is challenging but crucial for model performance; the paper formalizes this as a bi-level objective.

Method: Proposes MixMin, a convex gradient-based approach for data mixing, tested on language modeling and chemistry tasks.

Result: MixMin improved data mixtures with minimal compute, achieving 1-5% gains in model performance and showing scale-invariance.

Conclusion: MixMin effectively optimizes data mixtures, offering scalable and efficient improvements for diverse tasks.

Abstract: Modern machine learning pipelines are increasingly combining and mixing data
from diverse and disparate sources, e.g., pre-training large language models.
Yet, finding the optimal data mixture is a challenging and open problem. We
formalize this data mixing problem as a bi-level objective: the best mixture is
the one that would lead to the best model for a downstream objective.
Unfortunately, this objective is generally intractable. In this paper, we make
the observation that the bi-level data mixing objective becomes convex as our
model class becomes larger. We develop and study a gradient-based approach for
optimizing this convex objective, which we call MixMin, and test it on language
modeling and chemistry tasks. MixMin was the only method that uniformly
improved the data mixture in all our experiments. With MixMin, we improved the
data mixture using less than 0.2% additional compute for a pythia-410M model
trained on 8.2B tokens, resulting between 1-5% relative improvement to negative
log likelihood on PIQA, ARC Easy, SciQ, and OpenWebMath. Crucially, we found
that MixMin mixtures for smaller models improved training of larger models,
suggesting that MixMin mixtures may be scale-invariant. When mixing bioassay
data to train an XGBoost model, we saw improvements to average precision scores
of 0.03-0.15.

</details>


### [756] [Quantize What Counts: Bit Allocation Insights Informed by Spectral Gaps in Keys and Values](https://arxiv.org/pdf/2502.15075)
*Mohsen Hariri, Alan Luo, Mohammadreza Nemati, Lam Nguyen, Shaochen Zhong, Qifan Wang, Xia Hu, Xiaotian Han, Vipin Chaudhary*

Main category: cs.LG

TL;DR: The paper introduces two theorems to improve KV cache quantization in LLMs, showing that prioritizing key precision over values enhances efficiency without losing accuracy.


<details>
  <summary>Details</summary>
Motivation: Address memory constraints in large LLMs by improving KV cache quantization methods.

Method: Propose two theorems: Key-Value Norm Disparity and Key-Driven Quantization, validated through theory and experiments.

Result: Prioritizing key precision improves quantization performance, reducing memory usage with minimal accuracy loss.

Conclusion: The findings provide guidelines for efficient KV cache quantization, balancing memory and performance in NLP tasks.

Abstract: Large Language Models (LLMs) have introduced significant advancements to the
capabilities of Natural Language Processing (NLP) in recent years. However, as
these models continue to scale in size, memory constraints pose substantial
challenge. Key and Value cache (KV cache) quantization has been well-documented
as a promising solution to this limitation. In this work, we provide two novel
theorems aimed at enhancing KV quantization methods. Our first theorem, termed
Key-Value Norm Disparity, states that the key weight matrices by nature carry
richer information compared to the value weight matrices, as evidenced by
higher spectral and Frobenius norms across most of the layers. Our second
theorem, Key-Driven Quantization, posits that prioritizing the quantization
precision of keys over values induces significant improvements to the overall
quantization performance. In particular, assigning greater precision to the
keys compared to the values achieves a higher degree of precision reduction
with minimal impact on model accuracy. We validate these theorems through
theory and extensive experiments on several state-of-the-art LLM architectures
and benchmarks. These findings offer valuable guidelines for improving KV cache
quantization strategies, facilitating more efficient memory utilization without
compromising model performance across diverse NLP tasks. Source code is
available at https://github.com/mohsenhariri/spectral-kv.

</details>


### [757] [Learning to Add, Multiply, and Execute Algorithmic Instructions Exactly with Neural Networks](https://arxiv.org/pdf/2502.16763)
*Artur Back de Luca, George Giapitzakis, Kimon Fountoulakis*

Main category: cs.LG

TL;DR: The paper explores whether neural networks can exactly execute binary-encoded algorithmic tasks using the Neural Tangent Kernel (NTK) framework, demonstrating success with four fundamental tasks and efficient training.


<details>
  <summary>Details</summary>
Motivation: Neural networks struggle with discrete operations like arithmetic, which are crucial for algorithmic tasks. The study investigates if they can learn to execute such tasks exactly.

Method: The authors use the NTK framework to analyze two-layer fully connected networks in the infinite-width limit, employing structured training data and NTK correlation control.

Result: A sufficiently large ensemble of models can exactly execute binary permutations, addition, multiplication, and SBN instructions with high probability, using logarithmically many training data.

Conclusion: The framework extends to computable functions, showing neural networks can achieve exact algorithmic execution with proper training techniques.

Abstract: Neural networks are known for their ability to approximate smooth functions,
yet they fail to generalize perfectly to unseen inputs when trained on discrete
operations. Such operations lie at the heart of algorithmic tasks such as
arithmetic, which is often used as a test bed for algorithmic execution in
neural networks. In this work, we ask: can neural networks learn to execute
binary-encoded algorithmic instructions exactly? We use the Neural Tangent
Kernel (NTK) framework to study the training dynamics of two-layer fully
connected networks in the infinite-width limit and show how a sufficiently
large ensemble of such models can be trained to execute exactly, with high
probability, four fundamental tasks: binary permutations, binary addition,
binary multiplication, and Subtract and Branch if Negative (SBN) instructions.
Since SBN is Turing-complete, our framework extends to computable functions. We
show how this can be efficiently achieved using only logarithmically many
training data. Our approach relies on two techniques: structuring the training
data to isolate bit-level rules, and controlling correlations in the NTK regime
to align model predictions with the target algorithmic executions.

</details>


### [758] [Activated LoRA: Fine-tuned LLMs for Intrinsics](https://arxiv.org/pdf/2504.12397)
*Kristjan Greenewald, Luis Lastras, Thomas Parnell, Vraj Shah, Lucian Popa, Giulio Zizzo, Chulaka Gunasekara, Ambrish Rawat, David Cox*

Main category: cs.LG

TL;DR: aLoRA improves LoRA by enabling instant activation without recomputing KV cache, enhancing efficiency in multiturn settings.


<details>
  <summary>Details</summary>
Motivation: Switching between LoRAs in multiturn settings is inefficient due to KV cache recomputation.

Method: aLoRA modifies LoRA to adapt weights only for tokens after invocation, accepting the base model's KV cache.

Result: aLoRA achieves competitive accuracy with standard LoRA while offering significant inference benefits.

Conclusion: aLoRA enables efficient, specialized model invocation in chains without cache recomputation.

Abstract: Low-Rank Adaptation (LoRA) has emerged as a highly efficient framework for
finetuning the weights of large foundation models, and has become the go-to
method for data-driven customization of LLMs. Despite the promise of highly
customized behaviors and capabilities, switching between relevant LoRAs in a
multiturn setting is inefficient, as the key-value (KV) cache of the entire
turn history must be recomputed with the LoRA weights before generation can
begin. To address this problem, we propose Activated LoRA (aLoRA), an adapter
architecture which modifies the LoRA framework to only adapt weights for the
tokens in the sequence \emph{after} the aLoRA is invoked. This change crucially
allows aLoRA to accept the base model's KV cache of the input string, meaning
that aLoRA can be instantly activated whenever needed in a chain without
recomputing the cache. This enables building what we call \emph{intrinsics},
i.e. specialized models invoked to perform well-defined operations on portions
of an input chain or conversation that otherwise uses the base model by
default. We train a set of aLoRA-based intrinsics models, demonstrating
competitive accuracy with standard LoRA while achieving significant inference
benefits. We include a codebase implementing aLoRA in the supplementary
material.

</details>


### [759] [Overcoming Non-stationary Dynamics with Evidential Proximal Policy Optimization](https://arxiv.org/pdf/2503.01468)
*Abdullah Akgül, Gulcin Baykal, Manuel Haußmann, Melih Kandemir*

Main category: cs.LG

TL;DR: The paper introduces Evidential Proximal Policy Optimization (EPPO), a method to address non-stationary environments in deep reinforcement learning by preserving critic plasticity and enabling directed exploration.


<details>
  <summary>Details</summary>
Motivation: Non-stationary environments challenge deep reinforcement learning due to time-dependent state transitions, causing stability issues in model-free actor-critic architectures.

Method: EPPO uses an evidential critic for uncertainty quantification, maintaining plasticity and enabling directed exploration. It combines on-policy reinforcement learning with evidential design.

Result: EPPO outperforms state-of-the-art on-policy reinforcement learning methods in non-stationary continuous control tasks, achieving higher returns.

Conclusion: EPPO effectively addresses non-stationarity by leveraging evidential uncertainty, improving both policy evaluation and improvement in dynamic environments.

Abstract: Continuous control of non-stationary environments is a major challenge for
deep reinforcement learning algorithms. The time-dependency of the state
transition dynamics aggravates the notorious stability problems of model-free
deep actor-critic architectures. We posit that two properties will play a key
role in overcoming non-stationarity in transition dynamics: (i) preserving the
plasticity of the critic network, (ii) directed exploration for rapid
adaptation to the changing dynamics. We show that performing on-policy
reinforcement learning with an evidential critic provides both of these
properties. The evidential design ensures a fast and sufficiently accurate
approximation to the uncertainty around the state-value, which maintains the
plasticity of the critic network by detecting the distributional shifts caused
by the change in dynamics. The probabilistic critic also makes the actor
training objective a random variable, enabling the use of directed exploration
approaches as a by-product. We name the resulting algorithm as $\textit{
Evidential Proximal Policy Optimization (EPPO)}$ due to the integral role of
evidential uncertainty quantification in both policy evaluation and policy
improvement stages. Through experiments on non-stationary continuous control
tasks, where the environment dynamics change at regular intervals, we
demonstrate that our algorithm outperforms state-of-the-art on-policy
reinforcement learning variants in both task-specific and overall return.

</details>


### [760] [TimeCapsule: Solving the Jigsaw Puzzle of Long-Term Time Series Forecasting with Compressed Predictive Representations](https://arxiv.org/pdf/2504.12721)
*Yihang Lu, Yangyang Xu, Qitao Qing, Xianwei Meng*

Main category: cs.LG

TL;DR: TimeCapsule simplifies LTSF by unifying redundancy reduction and multi-scale modeling in a 3D tensor framework, achieving state-of-the-art results.


<details>
  <summary>Details</summary>
Motivation: Complex deep learning models for LTSF often underperform simpler ones; the paper aims to streamline advanced techniques for efficiency.

Method: Introduces TimeCapsule, a model using 3D tensors and mode production for multi-mode dependencies and compression, with JEPA for predictive learning.

Result: TimeCapsule outperforms benchmarks, demonstrating versatility and state-of-the-art performance.

Conclusion: Simplified frameworks like TimeCapsule can effectively unify and enhance LTSF techniques.

Abstract: Recent deep learning models for Long-term Time Series Forecasting (LTSF)
often emphasize complex, handcrafted designs, while simpler architectures like
linear models or MLPs have often outperformed these intricate solutions. In
this paper, we revisit and organize the core ideas behind several key
techniques, such as redundancy reduction and multi-scale modeling, which are
frequently employed in advanced LTSF models. Our goal is to streamline these
ideas for more efficient deep learning utilization. To this end, we introduce
TimeCapsule, a model built around the principle of high-dimensional information
compression that unifies these techniques in a generalized yet simplified
framework. Specifically, we model time series as a 3D tensor, incorporating
temporal, variate, and level dimensions, and leverage mode production to
capture multi-mode dependencies while achieving dimensionality compression. We
propose an internal forecast within the compressed representation domain,
supported by the Joint-Embedding Predictive Architecture (JEPA), to monitor the
learning of predictive representations. Extensive experiments on challenging
benchmarks demonstrate the versatility of our method, showing that TimeCapsule
can achieve state-of-the-art performance.

</details>


### [761] [A Little Depth Goes a Long Way: The Expressive Power of Log-Depth Transformers](https://arxiv.org/pdf/2503.03961)
*William Merrill, Ashish Sabharwal*

Main category: cs.LG

TL;DR: Transformers with depth growing logarithmically with input length can solve sequential reasoning tasks like regular language recognition and graph connectivity, unlike fixed-depth transformers.


<details>
  <summary>Details</summary>
Motivation: To understand how transformer depth affects expressivity, especially for sequential reasoning tasks, and to bridge the gap between theoretical expressivity and practical learnability.

Method: Analyze transformers with depth scaling as Θ(log n) with input length n, focusing on tasks like regular language recognition and graph connectivity.

Result: Logarithmic-depth transformers can express these tasks, unlike fixed-depth ones. Theoretical depth requirements align with practical training needs.

Conclusion: Growing transformer depth enhances expressivity for sequential reasoning, with practical implications for depth selection in training.

Abstract: Recent theoretical results show transformers cannot express sequential
reasoning problems over long inputs, intuitively because their computational
depth is bounded. However, prior work treats the depth as a constant, leaving
it unclear to what degree bounded depth may suffice for solving problems over
short inputs, or how increasing the transformer's depth affects its expressive
power. We address these questions by analyzing transformers whose depth can
grow minimally with context length $n$. We show even highly uniform
transformers with depth $\Theta(\log n)$ can express two important problems:
recognizing regular languages, which captures state tracking abilities and was
known to be expressible only by an unconventional, non-uniform model of
transformers, and graph connectivity, which underlies multi-step reasoning.
Notably, both of these problems cannot be expressed by fixed-depth transformers
under standard complexity conjectures, demonstrating the expressivity benefit
of growing depth. Moreover, our theory quantitatively predicts how depth must
grow with input length to express these problems, showing that depth scaling is
more efficient than scaling width or chain-of-thought steps. Empirically, our
detailed experiments designed to bridge the expressivity vs. learnability gap
reveal that our theoretical depth requirements for regular language recognition
closely match the practical depth requirements for successfully training
transformers. Thus, our results clarify how depth affects a transformer's
reasoning capabilities, and provide practical guidance for effective depth
selection for sequential reasoning.

</details>


### [762] [Recursive Deep Inverse Reinforcement Learning](https://arxiv.org/pdf/2504.13241)
*Paul Ghanem, Michael Potter, Owen Howell, Pau Closas, Alireza Ramezani, Deniz Erdogmus, Tales Imbiriba*

Main category: cs.LG

TL;DR: Proposes an online Recursive Deep Inverse Reinforcement Learning (RDIRL) method for real-time inference of adversary goals, outperforming existing IRL algorithms.


<details>
  <summary>Details</summary>
Motivation: Existing deep IRL methods are offline and slow, limiting real-time applicability in adversarial domains like cybersecurity and strategy games.

Method: Uses sequential second-order Newton updates (akin to EKF) to minimize an upper bound on the Guided Cost Learning objective, enabling fast convergence.

Result: RDIRL successfully recovers cost/reward functions in benchmark tasks, outperforming leading IRL algorithms.

Conclusion: RDIRL is a promising real-time solution for inferring adversary goals in dynamic, non-cooperative environments.

Abstract: Inferring an adversary's goals from exhibited behavior is crucial for
counterplanning and non-cooperative multi-agent systems in domains like
cybersecurity, military, and strategy games. Deep Inverse Reinforcement
Learning (IRL) methods based on maximum entropy principles show promise in
recovering adversaries' goals but are typically offline, require large batch
sizes with gradient descent, and rely on first-order updates, limiting their
applicability in real-time scenarios. We propose an online Recursive Deep
Inverse Reinforcement Learning (RDIRL) approach to recover the cost function
governing the adversary actions and goals. Specifically, we minimize an upper
bound on the standard Guided Cost Learning (GCL) objective using sequential
second-order Newton updates, akin to the Extended Kalman Filter (EKF), leading
to a fast (in terms of convergence) learning algorithm. We demonstrate that
RDIRL is able to recover cost and reward functions of expert agents in standard
and adversarial benchmark tasks. Experiments on benchmark tasks show that our
proposed approach outperforms several leading IRL algorithms.

</details>


### [763] [CLDyB: Towards Dynamic Benchmarking for Continual Learning with Pre-trained Models](https://arxiv.org/pdf/2503.04655)
*Shengzhuang Chen, Yikai Liao, Xiaoxiao Sun, Kede Ma, Ying Wei*

Main category: cs.LG

TL;DR: The paper introduces CLDyB, a dynamic benchmarking framework for continual learning (CL) to address data contamination and static benchmark limitations, evaluating CL methods more reliably.


<details>
  <summary>Details</summary>
Motivation: To tackle concerns about data contamination in pre-training and the inadequacy of static benchmarks for real-world CL scenarios.

Method: Proposes CLDyB, a framework using Markov decision processes and Monte Carlo tree search to dynamically identify challenging tasks and task orders for CL methods.

Result: Joint evaluation reveals commonly challenging task sequences where CL methods struggle, while separate evaluations highlight individual method strengths and weaknesses.

Conclusion: CLDyB provides a more reliable evaluation tool for CL methods, uncovering their limitations and potential improvements.

Abstract: The advent of the foundation model era has sparked significant research
interest in leveraging pre-trained representations for continual learning (CL),
yielding a series of top-performing CL methods on standard evaluation
benchmarks. Nonetheless, there are growing concerns regarding potential data
contamination during the pre-training stage. Furthermore, standard evaluation
benchmarks, which are typically static, fail to capture the complexities of
real-world CL scenarios, resulting in saturated performance. To address these
issues, we describe CL on dynamic benchmarks (CLDyB), a general computational
framework based on Markov decision processes for evaluating CL methods
reliably. CLDyB dynamically identifies inherently difficult and
algorithm-dependent tasks for the given CL methods, and determines challenging
task orders using Monte Carlo tree search. Leveraging CLDyB, we first conduct a
joint evaluation of multiple state-of-the-art CL methods, leading to a set of
commonly challenging and generalizable task sequences where existing CL methods
tend to perform poorly. We then conduct separate evaluations of individual CL
methods using CLDyB, discovering their respective strengths and weaknesses. The
source code and generated task sequences are publicly accessible at
https://github.com/szc12153/CLDyB.

</details>


### [764] [Route Sparse Autoencoder to Interpret Large Language Models](https://arxiv.org/pdf/2503.08200)
*Wei Shi, Sihang Li, Tao Liang, Mingyang Wan, Guojun Ma, Xiang Wang, Xiangnan He*

Main category: cs.LG

TL;DR: RouteSAE is a new framework combining routing with sparse autoencoders to extract interpretable features from multiple layers in LLMs, outperforming single-layer SAEs in feature count and interpretability.


<details>
  <summary>Details</summary>
Motivation: Prior SAE methods focus on single-layer feature extraction, missing multi-layer activations. RouteSAE addresses this gap.

Method: RouteSAE integrates a routing mechanism with shared SAE, dynamically weighting activations from multiple layers with minimal overhead.

Result: RouteSAE extracts 22.5% more features and achieves 22.3% higher interpretability than baseline SAEs under the same sparsity.

Conclusion: RouteSAE is scalable and effective for LLM interpretability, useful for feature discovery and model intervention.

Abstract: Mechanistic interpretability of large language models (LLMs) aims to uncover
the internal processes of information propagation and reasoning. Sparse
autoencoders (SAEs) have demonstrated promise in this domain by extracting
interpretable and monosemantic features. However, prior works primarily focus
on feature extraction from a single layer, failing to effectively capture
activations that span multiple layers. In this paper, we introduce Route Sparse
Autoencoder (RouteSAE), a new framework that integrates a routing mechanism
with a shared SAE to efficiently extract features from multiple layers. It
dynamically assigns weights to activations from different layers, incurring
minimal parameter overhead while achieving high interpretability and
flexibility for targeted feature manipulation. We evaluate RouteSAE through
extensive experiments on Llama-3.2-1B-Instruct. Specifically, under the same
sparsity constraint of 64, RouteSAE extracts 22.5% more features than baseline
SAEs while achieving a 22.3% higher interpretability score. These results
underscore the potential of RouteSAE as a scalable and effective method for LLM
interpretability, with applications in feature discovery and model
intervention. Our codes are available at https://github.com/swei2001/RouteSAEs.

</details>


### [765] [Tuning for Trustworthiness -- Balancing Performance and Explanation Consistency in Neural Network Optimization](https://arxiv.org/pdf/2505.07910)
*Alexander Hinterleitner, Thomas Bartz-Beielstein*

Main category: cs.LG

TL;DR: The paper introduces XAI consistency, a measure of agreement among feature attribution methods, and integrates it into hyperparameter tuning to balance predictive performance and interpretability.


<details>
  <summary>Details</summary>
Motivation: Current hyperparameter tuning focuses on minimizing predictive loss, neglecting explainability. The work aims to address this gap by incorporating XAI consistency into optimization.

Method: Proposes XAI consistency metrics and integrates them into a multi-objective optimization framework using SPOT, employing weighted aggregation and desirability strategies.

Result: Identifies distinct regions in the configuration space: poor performance/low interpretability, strong performance/weak interpretability, and a trade-off region balancing both.

Conclusion: The framework enables models with balanced performance and interpretability, potentially improving robustness and reliability on out-of-distribution data.

Abstract: Despite the growing interest in Explainable Artificial Intelligence (XAI),
explainability is rarely considered during hyperparameter tuning or neural
architecture optimization, where the focus remains primarily on minimizing
predictive loss. In this work, we introduce the novel concept of XAI
consistency, defined as the agreement among different feature attribution
methods, and propose new metrics to quantify it. For the first time, we
integrate XAI consistency directly into the hyperparameter tuning objective,
creating a multi-objective optimization framework that balances predictive
performance with explanation robustness. Implemented within the Sequential
Parameter Optimization Toolbox (SPOT), our approach uses both weighted
aggregation and desirability-based strategies to guide model selection. Through
our proposed framework and supporting tools, we explore the impact of
incorporating XAI consistency into the optimization process. This enables us to
characterize distinct regions in the architecture configuration space: one
region with poor performance and comparatively low interpretability, another
with strong predictive performance but weak interpretability due to low
\gls{xai} consistency, and a trade-off region that balances both objectives by
offering high interpretability alongside competitive performance. Beyond
introducing this novel approach, our research provides a foundation for future
investigations into whether models from the trade-off zone-balancing
performance loss and XAI consistency-exhibit greater robustness by avoiding
overfitting to training performance, thereby leading to more reliable
predictions on out-of-distribution data.

</details>


### [766] [Residual Policy Gradient: A Reward View of KL-regularized Objective](https://arxiv.org/pdf/2503.11019)
*Pengcheng Wang, Xinghao Zhu, Yuxin Chen, Chenfeng Xu, Masayoshi Tomizuka, Chenran Li*

Main category: cs.LG

TL;DR: The paper introduces Residual Policy Gradient (RPG) to extend Residual Q-Learning (RQL) to policy gradient methods, enabling policy customization in gradient-based RL. It also rethinks KL-regularized objectives in RL fine-tuning.


<details>
  <summary>Details</summary>
Motivation: Address the limitation of RQL by extending it to policy gradient methods, which are more effective in certain tasks, and explore policy customization in RL.

Method: Derives Soft Policy Gradient and introduces Residual Policy Gradient (RPG) to adapt prior policies while meeting new requirements. Analyzes KL-regularized objectives.

Result: Experiments in MuJoCo show the effectiveness of Soft Policy Gradient and RPG.

Conclusion: RPG successfully extends RQL to policy gradient methods, providing a principled approach for policy customization in gradient-based RL.

Abstract: Reinforcement Learning and Imitation Learning have achieved widespread
success in many domains but remain constrained during real-world deployment.
One of the main issues is the additional requirements that were not considered
during training. To address this challenge, policy customization has been
introduced, aiming to adapt a prior policy while preserving its inherent
properties and meeting new task-specific requirements. A principled approach to
policy customization is Residual Q-Learning (RQL), which formulates the problem
as a Markov Decision Process (MDP) and derives a family of value-based learning
algorithms. However, RQL has not yet been applied to policy gradient methods,
which restricts its applicability, especially in tasks where policy gradient
has already proven more effective. In this work, we first derive a concise form
of Soft Policy Gradient as a preliminary. Building on this, we introduce
Residual Policy Gradient (RPG), which extends RQL to policy gradient methods,
allowing policy customization in gradient-based RL settings. With the view of
RPG, we rethink the KL-regularized objective widely used in RL fine-tuning. We
show that under certain assumptions, KL-regularized objective leads to a
maximum-entropy policy that balances the inherent properties and task-specific
requirements on a reward-level. Our experiments in MuJoCo demonstrate the
effectiveness of Soft Policy Gradient and Residual Policy Gradient.

</details>


### [767] [Fair Clustering via Alignment](https://arxiv.org/pdf/2505.09131)
*Kunwoong Kim, Jihu Lee, Sangchul Park, Yongdai Kim*

Main category: cs.LG

TL;DR: A new fair clustering algorithm, FCA, balances fairness and utility by aligning data from protected groups and optimizing cluster centers, outperforming existing methods.


<details>
  <summary>Details</summary>
Motivation: Existing fair clustering algorithms often compromise utility or stability due to complexity or approximations.

Method: FCA alternately aligns data from protected groups using joint probability distributions and optimizes cluster centers in the aligned space.

Result: FCA achieves superior fairness-utility trade-offs and near-perfect fairness without instability.

Conclusion: FCA provides a high-utility, stable solution for fair clustering with theoretical guarantees.

Abstract: Algorithmic fairness in clustering aims to balance the proportions of
instances assigned to each cluster with respect to a given sensitive attribute.
While recently developed fair clustering algorithms optimize clustering
objectives under specific fairness constraints, their inherent complexity or
approximation often results in suboptimal clustering utility or numerical
instability in practice. To resolve these limitations, we propose a new fair
clustering algorithm based on a novel decomposition of the fair $K$-means
clustering objective function. The proposed algorithm, called Fair Clustering
via Alignment (FCA), operates by alternately (i) finding a joint probability
distribution to align the data from different protected groups, and (ii)
optimizing cluster centers in the aligned space. A key advantage of FCA is that
it theoretically guarantees approximately optimal clustering utility for any
given fairness level without complex constraints, thereby enabling high-utility
fair clustering in practice. Experiments show that FCA outperforms existing
methods by (i) attaining a superior trade-off between fairness level and
clustering utility, and (ii) achieving near-perfect fairness without numerical
instability.

</details>


### [768] [Permutation Equivariant Neural Networks for Symmetric Tensors](https://arxiv.org/pdf/2503.11276)
*Edward Pearce-Crump*

Main category: cs.LG

TL;DR: The paper explores linear permutation equivariant functions for symmetric tensors, showing their data efficiency and generalization potential compared to standard MLPs.


<details>
  <summary>Details</summary>
Motivation: Existing research lacks exploration of symmetric tensors as inputs in permutation equivariant models, despite their importance in various fields.

Method: The paper presents two characterizations of all linear permutation equivariant functions between symmetric power spaces of ℝⁿ.

Result: The proposed functions are highly data-efficient and generalize well to symmetric tensors of different sizes.

Conclusion: The work highlights the potential of permutation equivariant functions for symmetric tensors, offering improved efficiency and generalization.

Abstract: Incorporating permutation equivariance into neural networks has proven to be
useful in ensuring that models respect symmetries that exist in data. Symmetric
tensors, which naturally appear in statistics, machine learning, and graph
theory, are essential for many applications in physics, chemistry, and
materials science, amongst others. However, existing research on permutation
equivariant models has not explored symmetric tensors as inputs, and most prior
work on learning from these tensors has focused on equivariance to Euclidean
groups. In this paper, we present two different characterisations of all linear
permutation equivariant functions between symmetric power spaces of
$\mathbb{R}^n$. We show on two tasks that these functions are highly data
efficient compared to standard MLPs and have potential to generalise well to
symmetric tensors of different sizes.

</details>


### [769] [From Score Matching to Diffusion: A Fine-Grained Error Analysis in the Gaussian Setting](https://arxiv.org/pdf/2503.11615)
*Samuel Hurault, Matthieu Terris, Thomas Moreau, Gabriel Peyré*

Main category: cs.LG

TL;DR: The paper analyzes sampling errors in diffusion-based methods for generative AI, focusing on Gaussian settings. It quantifies Wasserstein error from four sources and links it to data anisotropy and method parameters.


<details>
  <summary>Details</summary>
Motivation: To rigorously understand and quantify the sampling errors in diffusion-based methods, particularly how data distribution anisotropy and method parameters impact accuracy.

Method: The study uses a sharp analysis of Wasserstein sampling error in Gaussian settings, examining four error sources: score matching generalization/optimization errors and diffusion discretization/minimal noise.

Result: The Wasserstein sampling error is expressed as a kernel-type norm of the data power spectrum, revealing interactions between data anisotropy and method parameters.

Conclusion: The analysis provides a foundation for optimizing sampling accuracy by understanding tradeoffs between error sources and method parameters.

Abstract: Sampling from an unknown distribution, accessible only through discrete
samples, is a fundamental problem at the core of generative AI. The current
state-of-the-art methods follow a two-step process: first, estimating the score
function (the gradient of a smoothed log-distribution) and then applying a
diffusion-based sampling algorithm -- such as Langevin or Diffusion models. The
resulting distribution's correctness can be impacted by four major factors: the
generalization and optimization errors in score matching, and the
discretization and minimal noise amplitude in the diffusion. In this paper, we
make the sampling error explicit when using a diffusion sampler in the Gaussian
setting. We provide a sharp analysis of the Wasserstein sampling error that
arises from these four error sources. This allows us to rigorously track how
the anisotropy of the data distribution (encoded by its power spectrum)
interacts with key parameters of the end-to-end sampling method, including the
number of initial samples, the stepsizes in both score matching and diffusion,
and the noise amplitude. Notably, we show that the Wasserstein sampling error
can be expressed as a kernel-type norm of the data power spectrum, where the
specific kernel depends on the method parameters. This result provides a
foundation for further analysis of the tradeoffs involved in optimizing
sampling accuracy.

</details>


### [770] [Enhancing Channel-Independent Time Series Forecasting via Cross-Variate Patch Embedding](https://arxiv.org/pdf/2505.12761)
*Donghwa Shin, Edwin Zhang*

Main category: cs.LG

TL;DR: The paper introduces Cross-Variate Patch Embeddings (CVPE) to enhance channel-independent (CI) time series forecasting models by incorporating cross-variate dependencies, improving performance without other modifications.


<details>
  <summary>Details</summary>
Motivation: Existing transformer-based models for time series forecasting often overlook cross-variate relationships, focusing only on temporal dependencies. This limits their effectiveness.

Method: The proposed CVPE module adds learnable positional encoding and a lightweight router-attention block to patch embeddings, injecting cross-variate context into CI models like Time-LLM.

Result: Experiments on seven real-world datasets show CVPE-enhanced Time-LLM outperforms the original baseline, validating its ability to capture cross-variate dependencies.

Conclusion: CVPE effectively improves CI models by integrating cross-variate context, demonstrating its potential for enhancing time series forecasting.

Abstract: Transformers have recently gained popularity in time series forecasting due
to their ability to capture long-term dependencies. However, many existing
models focus only on capturing temporal dependencies while omitting intricate
relationships between variables. Recent models have tried tackling this by
explicitly modeling both cross-time and cross-variate dependencies through a
sequential or unified attention mechanism, but they are entirely channel
dependent (CD) across all layers, making them potentially susceptible to
overfitting. To address this, we propose Cross-Variate Patch Embeddings (CVPE),
a lightweight CD module that injects cross-variate context into
channel-independent (CI) models by simply modifying the patch embedding
process. We achieve this by adding a learnable positional encoding and a
lightweight router-attention block to the vanilla patch embedding layer. We
then integrate CVPE into Time-LLM, a multimodal CI forecasting model, to
demonstrate its effectiveness in capturing cross-variate dependencies and
enhance the CI model's performance. Extensive experimental results on seven
real-world datasets show that our enhanced Time-LLM outperforms the original
baseline model simply by incorporating the CVPE module, with no other changes.

</details>


### [771] [Towards Understanding the Benefits of Neural Network Parameterizations in Geophysical Inversions: A Study With Neural Fields](https://arxiv.org/pdf/2503.17503)
*Anran Xu, Lindsey J. Heagy*

Main category: cs.LG

TL;DR: The paper introduces a test-time learning approach using neural fields (NFs-Inv) for geophysical inversions, showing improved results over traditional methods by reducing artifacts and enhancing model recovery.


<details>
  <summary>Details</summary>
Motivation: To address limitations of traditional inversion methods, which require pre-training and may introduce artifacts, by leveraging neural networks' implicit bias for better model recovery.

Method: Uses neural fields in a test-time learning manner, where weights are learned during inversion, and performs SVD analysis on the neural network's Jacobian to study its effects.

Result: NFs-Inv reduces artifacts and improves inversion results, such as dip angle recovery and boundary prediction. SVD analysis reveals neural networks' implicit bias, useful for geophysical inversions.

Conclusion: The test-time learning approach with neural fields offers advantages over conventional methods, with neural networks' implicit bias proving beneficial for geophysical model recovery.

Abstract: In this work, we employ neural fields, which use neural networks to map a
coordinate to the corresponding physical property value at that coordinate, in
a test-time learning manner. For a test-time learning method, the weights are
learned during the inversion, as compared to traditional approaches which
require a network to be trained using a training dataset. Results for synthetic
examples in seismic tomography and direct current resistivity inversions are
shown first. We then perform a singular value decomposition analysis on the
Jacobian of the weights of the neural network (SVD analysis) for both cases to
explore the effects of neural networks on the recovered model. The results show
that the test-time learning approach can eliminate unwanted artifacts in the
recovered subsurface physical property model caused by the sensitivity of the
survey and physics. Therefore, NFs-Inv improves the inversion results compared
to the conventional inversion in some cases such as the recovery of the dip
angle or the prediction of the boundaries of the main target. In the SVD
analysis, we observe similar patterns in the left-singular vectors as were
observed in some diffusion models, trained in a supervised manner, for
generative tasks in computer vision. This observation provides evidence that
there is an implicit bias, which is inherent in neural network structures, that
is useful in supervised learning and test-time learning models. This implicit
bias has the potential to be useful for recovering models in geophysical
inversions.

</details>


### [772] [Hogwild! Inference: Parallel LLM Generation via Concurrent Attention](https://arxiv.org/pdf/2504.06261)
*Gleb Rodionov, Roman Garipov, Alina Shutova, George Yakushev, Erik Schultheis, Vage Egiazarian, Anton Sinitsin, Denis Kuznedelev, Dan Alistarh*

Main category: cs.LG

TL;DR: The paper proposes Hogwild! Inference, a parallel LLM inference engine where multiple LLM instances collaborate via a shared attention cache, improving efficiency without fine-tuning.


<details>
  <summary>Details</summary>
Motivation: To enhance LLM performance by enabling parallel collaboration among workers, inspired by human problem-solving strategies.

Method: Uses Hogwild! Inference with shared Key-Value cache and Rotary Position Embeddings (RoPE) for parallel execution.

Result: LLMs can collaborate effectively without additional fine-tuning, leveraging shared memory for improved performance.

Conclusion: The approach demonstrates scalable and efficient parallel LLM inference, adaptable to diverse tasks.

Abstract: Large Language Models (LLMs) have demonstrated the ability to tackle
increasingly complex tasks through advanced reasoning, long-form content
generation, and tool use. Solving these tasks often involves long
inference-time computations. In human problem solving, a common strategy to
expedite work is collaboration: by dividing the problem into sub-tasks,
exploring different strategies concurrently, etc. Recent research has shown
that LLMs can also operate in parallel by implementing explicit cooperation
frameworks, such as voting mechanisms or the explicit creation of independent
sub-tasks that can be executed in parallel. However, each of these frameworks
may not be suitable for all types of tasks, which can hinder their
applicability. In this work, we propose a different design approach: we run LLM
"workers" in parallel , allowing them to synchronize via a concurrently-updated
attention cache and prompt these workers to decide how best to collaborate. Our
approach allows the LLM instances to come up with their own collaboration
strategy for the problem at hand, all the while "seeing" each other's memory in
the concurrent KV cache. We implement this approach via Hogwild! Inference: a
parallel LLM inference engine where multiple instances of the same LLM run in
parallel with the same attention cache, with "instant" access to each other's
memory. Hogwild! Inference takes advantage of Rotary Position Embeddings (RoPE)
to avoid recomputation while improving parallel hardware utilization. We find
that modern reasoning-capable LLMs can perform inference with shared Key-Value
cache out of the box, without additional fine-tuning.

</details>


### [773] [Towards Comprehensive and Prerequisite-Free Explainer for Graph Neural Networks](https://arxiv.org/pdf/2505.14005)
*Han Zhang, Yan Wang, Guanfeng Liu, Pengfei Ding, Huaxiong Wang, Kwok-Yan Lam*

Main category: cs.LG

TL;DR: OPEN is a comprehensive, prerequisite-free explainer for GNNs that partitions datasets into environments to capture diverse decision logic, outperforming existing methods in fidelity and robustness.


<details>
  <summary>Details</summary>
Motivation: Existing XGNN methods fail to capture complete decision logic across diverse distributions and impose strict prerequisites, limiting their performance and generalizability.

Method: OPEN partitions the dataset's sample space into environments with distinct distributions, learns GNN decision logic by sampling subgraphs from each, and analyzes predictions without strict prerequisites.

Result: OPEN captures nearly complete GNN decision logic, outperforms state-of-the-art methods in fidelity, maintains efficiency, and enhances robustness in real-world scenarios.

Conclusion: OPEN addresses key limitations of XGNN methods, offering a more reliable and generalizable solution for explaining GNNs.

Abstract: To enhance the reliability and credibility of graph neural networks (GNNs)
and improve the transparency of their decision logic, a new field of
explainability of GNNs (XGNN) has emerged. However, two major limitations
severely degrade the performance and hinder the generalizability of existing
XGNN methods: they (a) fail to capture the complete decision logic of GNNs
across diverse distributions in the entire dataset's sample space, and (b)
impose strict prerequisites on edge properties and GNN internal accessibility.
To address these limitations, we propose OPEN, a novel c\textbf{O}mprehensive
and \textbf{P}rerequisite-free \textbf{E}xplainer for G\textbf{N}Ns. OPEN, as
the first work in the literature, can infer and partition the entire dataset's
sample space into multiple environments, each containing graphs that follow a
distinct distribution. OPEN further learns the decision logic of GNNs across
different distributions by sampling subgraphs from each environment and
analyzing their predictions, thus eliminating the need for strict
prerequisites. Experimental results demonstrate that OPEN captures nearly
complete decision logic of GNNs, outperforms state-of-the-art methods in
fidelity while maintaining similar efficiency, and enhances robustness in
real-world scenarios.

</details>


### [774] [Synergistic Benefits of Joint Molecule Generation and Property Prediction](https://arxiv.org/pdf/2504.16559)
*Adam Izdebski, Jan Olszewski, Pankhil Gawade, Krzysztof Koras, Serra Korkmaz, Valentin Rauscher, Jakub M. Tomczak, Ewa Szczurek*

Main category: cs.LG

TL;DR: Hyformer is a transformer-based joint model for data generation and property prediction, offering synergistic benefits in tasks like conditional sampling and drug design.


<details>
  <summary>Details</summary>
Motivation: To address the challenges of training joint models for both data generation and property prediction, aiming for synergistic benefits beyond standalone generative or predictive models.

Method: Proposes Hyformer, using an alternating attention mechanism and joint pre-training to blend generative and predictive functionalities.

Result: Hyformer is optimized for molecule generation and property prediction, showing benefits in conditional sampling, out-of-distribution prediction, and representation learning.

Conclusion: Demonstrates the effectiveness of joint learning, particularly in drug design, such as discovering novel antimicrobial peptides.

Abstract: Modeling the joint distribution of data samples and their properties allows
to construct a single model for both data generation and property prediction,
with synergistic benefits reaching beyond purely generative or predictive
models. However, training joint models presents daunting architectural and
optimization challenges. Here, we propose Hyformer, a transformer-based joint
model that successfully blends the generative and predictive functionalities,
using an alternating attention mechanism and a joint pre-training scheme. We
show that Hyformer is simultaneously optimized for molecule generation and
property prediction, while exhibiting synergistic benefits in conditional
sampling, out-of-distribution property prediction and representation learning.
Finally, we demonstrate the benefits of joint learning in a drug design use
case of discovering novel antimicrobial~peptides.

</details>


### [775] [Physics-Guided Learning of Meteorological Dynamics for Weather Downscaling and Forecasting](https://arxiv.org/pdf/2505.14555)
*Yingtao Luo, Shikai Fang, Binqing Wu, Qingsong Wen, Liang Sun*

Main category: cs.LG

TL;DR: PhyDL-NWP integrates physics into deep learning for efficient, accurate, and physically consistent weather forecasting.


<details>
  <summary>Details</summary>
Motivation: Traditional NWP methods are computationally intensive and incomplete, while DL models lack physical interpretability.

Method: Combines physical equations with latent force parameterization, uses automatic differentiation for physical terms, and employs a physics-informed loss.

Result: Achieves 170x faster inference with 55K parameters, improves forecasting accuracy, and ensures physical consistency.

Conclusion: PhyDL-NWP bridges the gap between data-driven efficiency and physical accuracy in weather forecasting.

Abstract: Weather forecasting is essential but remains computationally intensive and
physically incomplete in traditional numerical weather prediction (NWP)
methods. Deep learning (DL) models offer efficiency and accuracy but often
ignore physical laws, limiting interpretability and generalization. We propose
PhyDL-NWP, a physics-guided deep learning framework that integrates physical
equations with latent force parameterization into data-driven models. It
predicts weather variables from arbitrary spatiotemporal coordinates, computes
physical terms via automatic differentiation, and uses a physics-informed loss
to align predictions with governing dynamics. PhyDL-NWP enables resolution-free
downscaling by modeling weather as a continuous function and fine-tunes
pre-trained models with minimal overhead, achieving up to 170x faster inference
with only 55K parameters. Experiments show that PhyDL-NWP improves both
forecasting performance and physical consistency.

</details>


### [776] [Robust ML Auditing using Prior Knowledge](https://arxiv.org/pdf/2505.04796)
*Jade Garcia Bourrée, Augustin Godinot, Martijn De Vos, Milos Vujasinovic, Sayan Biswas, Gilles Tredan, Erwan Le Merrer, Anne-Marie Kermarrec*

Main category: cs.LG

TL;DR: The paper addresses audit manipulation in AI regulation, proposing a manipulation-proof auditing method using the auditor's prior knowledge of the task.


<details>
  <summary>Details</summary>
Motivation: To tackle the underexplored risk of platforms altering audit responses without changing user answers, ensuring fair AI regulation.

Method: Introduces a novel auditing approach leveraging the auditor's prior knowledge, avoiding reliance on public priors, and formalizing conditions for manipulation-proof audits.

Result: Experiments show the maximum unfairness a platform can hide before detection, validating the method's effectiveness.

Conclusion: The work advances robust fairness audits by formalizing manipulation-proof auditing with prior knowledge, opening new research avenues.

Abstract: Among the many technical challenges to enforcing AI regulations, one crucial
yet underexplored problem is the risk of audit manipulation. This manipulation
occurs when a platform deliberately alters its answers to a regulator to pass
an audit without modifying its answers to other users. In this paper, we
introduce a novel approach to manipulation-proof auditing by taking into
account the auditor's prior knowledge of the task solved by the platform. We
first demonstrate that regulators must not rely on public priors (e.g. a public
dataset), as platforms could easily fool the auditor in such cases. We then
formally establish the conditions under which an auditor can prevent audit
manipulations using prior knowledge about the ground truth. Finally, our
experiments with two standard datasets illustrate the maximum level of
unfairness a platform can hide before being detected as malicious. Our
formalization and generalization of manipulation-proof auditing with a prior
opens up new research directions for more robust fairness audits.

</details>


### [777] [SurvUnc: A Meta-Model Based Uncertainty Quantification Framework for Survival Analysis](https://arxiv.org/pdf/2505.14803)
*Yu Liu, Weiyao Tao, Tong Xia, Simon Knight, Tingting Zhu*

Main category: cs.LG

TL;DR: SurvUnc is a model-agnostic framework for post-hoc uncertainty quantification in survival models, improving interpretability and reliability.


<details>
  <summary>Details</summary>
Motivation: Current survival models lack reliable uncertainty quantification, limiting their trustworthiness in critical applications like healthcare.

Method: SurvUnc uses an anchor-based learning strategy integrating concordance knowledge for meta-model optimization.

Result: SurvUnc outperforms in selective prediction, misprediction detection, and out-of-domain detection across multiple datasets.

Conclusion: SurvUnc enhances survival model reliability, enabling more trustworthy predictions in real-world scenarios.

Abstract: Survival analysis, which estimates the probability of event occurrence over
time from censored data, is fundamental in numerous real-world applications,
particularly in high-stakes domains such as healthcare and risk assessment.
Despite advances in numerous survival models, quantifying the uncertainty of
predictions from these models remains underexplored and challenging. The lack
of reliable uncertainty quantification limits the interpretability and
trustworthiness of survival models, hindering their adoption in clinical
decision-making and other sensitive applications. To bridge this gap, in this
work, we introduce SurvUnc, a novel meta-model based framework for post-hoc
uncertainty quantification for survival models. SurvUnc introduces an
anchor-based learning strategy that integrates concordance knowledge into
meta-model optimization, leveraging pairwise ranking performance to estimate
uncertainty effectively. Notably, our framework is model-agnostic, ensuring
compatibility with any survival model without requiring modifications to its
architecture or access to its internal parameters. Especially, we design a
comprehensive evaluation pipeline tailored to this critical yet overlooked
problem. Through extensive experiments on four publicly available benchmarking
datasets and five representative survival models, we demonstrate the
superiority of SurvUnc across multiple evaluation scenarios, including
selective prediction, misprediction detection, and out-of-domain detection. Our
results highlight the effectiveness of SurvUnc in enhancing model
interpretability and reliability, paving the way for more trustworthy survival
predictions in real-world applications.

</details>


### [778] [Identifying Causal Direction via Variational Bayesian Compression](https://arxiv.org/pdf/2505.07503)
*Quang-Duy Tran, Bao Duong, Phuoc Nguyen, Thin Nguyen*

Main category: cs.LG

TL;DR: The paper proposes using variational Bayesian learning of neural networks to improve cause-effect identification by balancing model fitness and codelength succinctness, outperforming existing methods.


<details>
  <summary>Details</summary>
Motivation: The challenge of distinguishing cause and effect from observational data using the algorithmic Markov condition, where existing methods compromise between model fitness and computational complexity.

Method: Leveraging variational Bayesian learning of neural networks to approximate codelengths, enhancing model fitness and succinctness while avoiding high computational costs.

Result: The method outperforms related complexity-based and structural causal model regression-based approaches in synthetic and real-world benchmarks.

Conclusion: The proposed approach effectively improves cause-effect identification by optimizing codelength succinctness and model fitness without excessive computational complexity.

Abstract: Telling apart the cause and effect between two random variables with purely
observational data is a challenging problem that finds applications in various
scientific disciplines. A key principle utilized in this task is the
algorithmic Markov condition, which postulates that the joint distribution,
when factorized according to the causal direction, yields a more succinct
codelength compared to the anti-causal direction. Previous approaches
approximate these codelengths by relying on simple functions or Gaussian
processes (GPs) with easily evaluable complexity, compromising between model
fitness and computational complexity. To overcome these limitations, we propose
leveraging the variational Bayesian learning of neural networks as an
interpretation of the codelengths. Consequently, we can enhance the model
fitness while promoting the succinctness of the codelengths, while avoiding the
significant computational complexity of the GP-based approaches. Extensive
experiments on both synthetic and real-world benchmarks in cause-effect
identification demonstrate the effectiveness of our proposed method, surpassing
the overall performance of related complexity-based and structural causal model
regression-based approaches.

</details>


### [779] [Hadamax Encoding: Elevating Performance in Model-Free Atari](https://arxiv.org/pdf/2505.15345)
*Jacob E. Kooi, Zhao Yang, Vincent François-Lavet*

Main category: cs.LG

TL;DR: The paper introduces the Hadamax encoder, a novel architecture for pixel-based model-free RL, achieving state-of-the-art performance in Atari-57 with an 80% gain over vanilla PQN.


<details>
  <summary>Details</summary>
Motivation: Neural architectures in RL are often simple; this work aims to improve performance with a new encoder design.

Method: Uses Hadamard max-pooling (Hadamax) on GELU-activated parallel hidden layers within the PQN algorithm.

Result: Hadamax-PQN outperforms vanilla PQN by 80% and surpasses Rainbow-DQN in Atari-57.

Conclusion: The Hadamax encoder is effective for model-free RL, offering significant performance improvements.

Abstract: Neural network architectures have a large impact in machine learning. In
reinforcement learning, network architectures have remained notably simple, as
changes often lead to small gains in performance. This work introduces a novel
encoder architecture for pixel-based model-free reinforcement learning. The
Hadamax (\textbf{Hada}mard \textbf{max}-pooling) encoder achieves
state-of-the-art performance by max-pooling Hadamard products between
GELU-activated parallel hidden layers. Based on the recent PQN algorithm, the
Hadamax encoder achieves state-of-the-art model-free performance in the
Atari-57 benchmark. Specifically, without applying any algorithmic
hyperparameter modifications, Hadamax-PQN achieves an 80\% performance gain
over vanilla PQN and significantly surpasses Rainbow-DQN. For reproducibility,
the full code is available on
\href{https://github.com/Jacobkooi/Hadamax}{GitHub}.

</details>


### [780] [Making Small Language Models Efficient Reasoners: Intervention, Supervision, Reinforcement](https://arxiv.org/pdf/2505.07961)
*Xuechen Zhang, Zijian Huang, Chenshun Ni, Ziyang Xiong, Jiasi Chen, Samet Oymak*

Main category: cs.LG

TL;DR: The paper proposes methods to improve token-efficient reasoning in small language models by controlling trace length and reducing redundancy, achieving significant efficiency gains with minimal accuracy loss.


<details>
  <summary>Details</summary>
Motivation: Small language models often produce verbose and repetitive outputs due to inability to determine optimal stopping points in reasoning, leading to high computational costs.

Method: Two solutions are introduced: (1) Temperature scaling (TS) to control trace length, and (2) TLDR, a length-regularized reinforcement learning method for multi-level trace length control.

Result: Experiments show TS outperforms budget forcing, and TLDR improves token efficiency by ~50% with minimal accuracy loss. TLDR also enables flexible response length control.

Conclusion: The work emphasizes the importance of stopping time control, identifies SFT limitations, and offers practical solutions for efficient reasoning in small models.

Abstract: Recent research enhances language model reasoning by scaling test-time
compute via longer chain-of-thought traces. This often improves accuracy but
also introduces redundancy and high computational cost, especially for small
language models distilled with supervised fine-tuning (SFT). In this work, we
propose new algorithms to improve token-efficient reasoning with small-scale
models by effectively trading off accuracy and computation. We first show that
the post-SFT model fails to determine the optimal stopping point of the
reasoning process, resulting in verbose and repetitive outputs. Verbosity also
significantly varies across wrong vs correct responses. To address these
issues, we propose two solutions: (1) Temperature scaling (TS) to control the
stopping point for the thinking phase and thereby trace length, and (2) TLDR: a
length-regularized reinforcement learning method based on GRPO that facilitates
multi-level trace length control (e.g. short, medium, long reasoning).
Experiments on four reasoning benchmarks, MATH500, AMC, AIME24 and
OlympiadBench, demonstrate that TS is highly effective compared to s1's budget
forcing approach and TLDR significantly improves token efficiency by about 50%
with minimal to no accuracy loss over the SFT baseline. Moreover, TLDR also
facilitates flexible control over the response length, offering a practical and
effective solution for token-efficient reasoning in small models. Ultimately,
our work reveals the importance of stopping time control, highlights
shortcomings of pure SFT, and provides effective algorithmic recipes.

</details>


### [781] [Cell Library Characterization for Composite Current Source Models Based on Gaussian Process Regression and Active Learning](https://arxiv.org/pdf/2505.10799)
*Tao Bai, Junzhuo Zhou, Zeyuan Deng, Ting-Jung Lin, Wei Xing, Peng Cao, Lei He*

Main category: cs.LG

TL;DR: A novel Gaussian Process Regression (GPR) with active learning (AL) improves CCS model characterization, outperforming commercial tools in accuracy and efficiency.


<details>
  <summary>Details</summary>
Motivation: The CCS model's high accuracy demands, data volume, and simulation costs challenge characterization, necessitating a better solution.

Method: Proposed a GPR model with AL to efficiently and accurately characterize CCS, reducing runtime and storage needs.

Result: Achieved 2.05 ps average absolute error and 2.27% relative error, with runtime reduced to 27% and storage by 19.5x.

Conclusion: The GPR with AL framework effectively addresses CCS characterization challenges, offering superior performance over existing methods.

Abstract: The composite current source (CCS) model has been adopted as an advanced
timing model that represents the current behavior of cells for improved
accuracy and better capability than traditional non-linear delay models (NLDM)
to model complex dynamic effects and interactions under advanced process nodes.
However, the high accuracy requirement, large amount of data and extensive
simulation cost pose severe challenges to CCS characterization. To address
these challenges, we introduce a novel Gaussian Process Regression(GPR) model
with active learning(AL) to establish the characterization framework
efficiently and accurately. Our approach significantly outperforms conventional
commercial tools as well as learning based approaches by achieving an average
absolute error of 2.05 ps and a relative error of 2.27% for current waveform of
57 cells under 9 process, voltage, temperature (PVT) corners with TSMC 22nm
process. Additionally, our model drastically reduces the runtime to 27% and the
storage by up to 19.5x compared with that required by commercial tools.

</details>


### [782] [Finetuning-Activated Backdoors in LLMs](https://arxiv.org/pdf/2505.16567)
*Thibaud Gloaguen, Mark Vero, Robin Staab, Martin Vechev*

Main category: cs.LG

TL;DR: The paper introduces FAB, a finetuning-activated backdoor attack on LLMs, showing how poisoned models can hide malicious behaviors until triggered by user finetuning.


<details>
  <summary>Details</summary>
Motivation: To expose vulnerabilities in the finetuning process of LLMs, challenging the assumption that finetuning is secure and predictable.

Method: Uses meta-learning to poison LLMs, optimizing for hidden malicious behaviors post-finetuning while maintaining benign behavior beforehand.

Result: Demonstrates successful attacks across multiple LLMs, triggering behaviors like unsolicited advertising, refusal, and jailbreakability.

Conclusion: Reveals a critical security flaw in LLM finetuning, highlighting a new attack vector.

Abstract: Finetuning openly accessible Large Language Models (LLMs) has become standard
practice for achieving task-specific performance improvements. Until now,
finetuning has been regarded as a controlled and secure process in which
training on benign datasets led to predictable behaviors. In this paper, we
demonstrate for the first time that an adversary can create poisoned LLMs that
initially appear benign but exhibit malicious behaviors once finetuned by
downstream users. To this end, our proposed attack, FAB (Finetuning-Activated
Backdoor), poisons an LLM via meta-learning techniques to simulate downstream
finetuning, explicitly optimizing for the emergence of malicious behaviors in
the finetuned models. At the same time, the poisoned LLM is regularized to
retain general capabilities and to exhibit no malicious behaviors prior to
finetuning. As a result, when users finetune the seemingly benign model on
their own datasets, they unknowingly trigger its hidden backdoor behavior. We
demonstrate the effectiveness of FAB across multiple LLMs and three target
behaviors: unsolicited advertising, refusal, and jailbreakability.
Additionally, we show that FAB-backdoors are robust to various finetuning
choices made by the user (e.g., dataset, number of steps, scheduler). Our
findings challenge prevailing assumptions about the security of finetuning,
revealing yet another critical attack vector exploiting the complexities of
LLMs.

</details>


### [783] [Where You Place the Norm Matters: From Prejudiced to Neutral Initializations](https://arxiv.org/pdf/2505.11312)
*Emanuele Francazi, Francesco Pinto, Aurelien Lucchi, Marco Baity-Jesi*

Main category: cs.LG

TL;DR: The paper explores how normalization layers (e.g., Batch Normalization, Layer Normalization) affect neural network behavior at initialization, influencing early training dynamics and class prediction distributions.


<details>
  <summary>Details</summary>
Motivation: To understand the theoretical impact of normalization layers on model behavior from initialization, addressing gaps in current knowledge.

Method: Investigates the presence and placement of normalization within hidden layers, analyzing their effects on prediction statistics before training.

Result: Normalization placement systematically influences initial prediction distributions (from unbiased to highly concentrated) and shapes learning dynamics.

Conclusion: Provides insights into how normalization affects early training, offering guidance for more controlled and interpretable network design.

Abstract: Normalization layers, such as Batch Normalization and Layer Normalization,
are central components in modern neural networks, widely adopted to improve
training stability and generalization. While their practical effectiveness is
well documented, a detailed theoretical understanding of how normalization
affects model behavior, starting from initialization, remains an important open
question. In this work, we investigate how both the presence and placement of
normalization within hidden layers influence the statistical properties of
network predictions before training begins. In particular, we study how these
choices shape the distribution of class predictions at initialization, which
can range from unbiased (Neutral) to highly concentrated (Prejudiced) toward a
subset of classes. Our analysis shows that normalization placement induces
systematic differences in the initial prediction behavior of neural networks,
which in turn shape the dynamics of learning. By linking architectural choices
to prediction statistics at initialization, our work provides a principled
understanding of how normalization can influence early training behavior and
offers guidance for more controlled and interpretable network design.

</details>


### [784] [GEM: Gaussian Embedding Modeling for Out-of-Distribution Detection in GUI Agents](https://arxiv.org/pdf/2505.12842)
*Zheng Wu, Pengzhou Cheng, Zongru Wu, Lingzhong Dong, Zhuosheng Zhang*

Main category: cs.LG

TL;DR: The paper proposes GEM, a novel method for detecting out-of-distribution (OOD) instructions in GUI agents, improving accuracy by 23.70% over baselines.


<details>
  <summary>Details</summary>
Motivation: GUI agents struggle with OOD instructions, leading to task breakdowns or security risks, necessitating better OOD detection methods.

Method: GEM fits a Gaussian mixture model on input embedding distances to identify capability boundaries of GUI agents.

Result: GEM achieves a 23.70% accuracy improvement over baselines across eight datasets and generalizes well across nine backbones.

Conclusion: GEM effectively addresses OOD detection in GUI agents, demonstrating superior performance and generalization.

Abstract: Graphical user interface (GUI) agents have recently emerged as an intriguing
paradigm for human-computer interaction, capable of automatically executing
user instructions to operate intelligent terminal devices. However, when
encountering out-of-distribution (OOD) instructions that violate environmental
constraints or exceed the current capabilities of agents, GUI agents may suffer
task breakdowns or even pose security threats. Therefore, effective OOD
detection for GUI agents is essential. Traditional OOD detection methods
perform suboptimally in this domain due to the complex embedding space and
evolving GUI environments. In this work, we observe that the in-distribution
input semantic space of GUI agents exhibits a clustering pattern with respect
to the distance from the centroid. Based on the finding, we propose GEM, a
novel method based on fitting a Gaussian mixture model over input embedding
distances extracted from the GUI Agent that reflect its capability boundary.
Evaluated on eight datasets spanning smartphones, computers, and web browsers,
our method achieves an average accuracy improvement of 23.70\% over the
best-performing baseline. Analysis verifies the generalization ability of our
method through experiments on nine different backbones. The codes are available
at https://github.com/Wuzheng02/GEM-OODforGUIagents.

</details>


### [785] [FoMoH: A clinically meaningful foundation model evaluation for structured electronic health records](https://arxiv.org/pdf/2505.16941)
*Chao Pang, Vincent Jeanselme, Young Sang Choi, Xinzhuo Jiang, Zilin Jing, Aparajita Kashyap, Yuta Kobayashi, Yanwei Li, Florent Pollet, Karthik Natarajan, Shalmali Joshi*

Main category: cs.LG

TL;DR: The paper evaluates foundation models in healthcare, proposing a suite of clinically meaningful tasks and robust evaluation criteria to address gaps in assessing their clinical utility.


<details>
  <summary>Details</summary>
Motivation: To address the lack of consensus on the clinical utility of foundation models in healthcare due to insufficient evaluation tasks and diversity.

Method: Proposes a suite of 14 clinically relevant tasks and evaluates state-of-the-art foundation models on EHR data from 5 million patients, measuring accuracy, calibration, and subpopulation performance.

Result: The study provides empirical insights into the performance of foundation models across diverse clinical tasks, highlighting tradeoffs in pre-training, tokenization, and data representation.

Conclusion: The work aims to guide future development and evaluation of healthcare foundation models by advancing empirical assessment methods.

Abstract: Foundation models hold significant promise in healthcare, given their
capacity to extract meaningful representations independent of downstream tasks.
This property has enabled state-of-the-art performance across several clinical
applications trained on structured electronic health record (EHR) data, even in
settings with limited labeled data, a prevalent challenge in healthcare.
However, there is little consensus on these models' potential for clinical
utility due to the lack of desiderata of comprehensive and meaningful tasks and
sufficiently diverse evaluations to characterize the benefit over conventional
supervised learning. To address this gap, we propose a suite of clinically
meaningful tasks spanning patient outcomes, early prediction of acute and
chronic conditions, including desiderata for robust evaluations. We evaluate
state-of-the-art foundation models on EHR data consisting of 5 million patients
from Columbia University Irving Medical Center (CUMC), a large urban academic
medical center in New York City, across 14 clinically relevant tasks. We
measure overall accuracy, calibration, and subpopulation performance to surface
tradeoffs based on the choice of pre-training, tokenization, and data
representation strategies. Our study aims to advance the empirical evaluation
of structured EHR foundation models and guide the development of future
healthcare foundation models.

</details>


### [786] [SpectralGap: Graph-Level Out-of-Distribution Detection via Laplacian Eigenvalue Gaps](https://arxiv.org/pdf/2505.15177)
*Jiawei Gu, Ziyue Qiao, Zechao Li*

Main category: cs.LG

TL;DR: SpecGap is a post-hoc method for OOD detection on graphs, leveraging anomalous spectral gaps in Laplacian eigenvalues to achieve state-of-the-art performance without additional training.


<details>
  <summary>Details</summary>
Motivation: OOD samples exhibit anomalous spectral gaps in Laplacian eigenvalues, differing from ID samples, which inspires a spectral-based detection approach.

Method: SpecGap adjusts high-level features by subtracting the component tied to the second-largest eigenvalue, scaled by the spectral gap.

Result: SpecGap outperforms benchmarks, validated by ablation studies and theoretical analyses.

Conclusion: SpecGap is a parameter-free, easily integrable solution for OOD detection in graph neural networks.

Abstract: The task of graph-level out-of-distribution (OOD) detection is crucial for
deploying graph neural networks in real-world settings. In this paper, we
observe a significant difference in the relationship between the largest and
second-largest eigenvalues of the Laplacian matrix for in-distribution (ID) and
OOD graph samples: \textit{OOD samples often exhibit anomalous spectral gaps
(the difference between the largest and second-largest eigenvalues)}. This
observation motivates us to propose SpecGap, an effective post-hoc approach for
OOD detection on graphs. SpecGap adjusts features by subtracting the component
associated with the second-largest eigenvalue, scaled by the spectral gap, from
the high-level features (i.e., $\mathbf{X}-\left(\lambda_n-\lambda_{n-1}\right)
\mathbf{u}_{n-1} \mathbf{v}_{n-1}^T$). SpecGap achieves state-of-the-art
performance across multiple benchmark datasets. We present extensive ablation
studies and comprehensive theoretical analyses to support our empirical
results. As a parameter-free post-hoc method, SpecGap can be easily integrated
into existing graph neural network models without requiring any additional
training or model modification.

</details>


### [787] [NeuBM: Mitigating Model Bias in Graph Neural Networks through Neutral Input Calibration](https://arxiv.org/pdf/2505.15180)
*Jiawei Gu, Ziyue Qiao, Xiao Luo*

Main category: cs.LG

TL;DR: NeuBM is a novel method to mitigate bias in GNNs by dynamically calibrating inputs using a neutral graph, improving performance for underrepresented classes without significant computational cost.


<details>
  <summary>Details</summary>
Motivation: GNNs often exhibit bias due to class imbalance, leading to unfair predictions and poor performance for minority classes.

Method: NeuBM uses a dynamically updated neutral graph to estimate and correct biases by adjusting logits from the input graph.

Result: NeuBM enhances balanced accuracy and recall for minority classes, especially in severe imbalance scenarios, while maintaining overall performance.

Conclusion: NeuBM effectively mitigates bias in GNNs through neutral input calibration, offering theoretical and practical advantages for fairer predictions.

Abstract: Graph Neural Networks (GNNs) have shown remarkable performance across various
domains, yet they often struggle with model bias, particularly in the presence
of class imbalance. This bias can lead to suboptimal performance and unfair
predictions, especially for underrepresented classes. We introduce NeuBM
(Neutral Bias Mitigation), a novel approach to mitigate model bias in GNNs
through neutral input calibration. NeuBM leverages a dynamically updated
neutral graph to estimate and correct the inherent biases of the model. By
subtracting the logits obtained from the neutral graph from those of the input
graph, NeuBM effectively recalibrates the model's predictions, reducing bias
across different classes. Our method integrates seamlessly into existing GNN
architectures and training procedures, requiring minimal computational
overhead. Extensive experiments on multiple benchmark datasets demonstrate that
NeuBM significantly improves the balanced accuracy and recall of minority
classes, while maintaining strong overall performance. The effectiveness of
NeuBM is particularly pronounced in scenarios with severe class imbalance and
limited labeled data, where traditional methods often struggle. We provide
theoretical insights into how NeuBM achieves bias mitigation, relating it to
the concept of representation balancing. Our analysis reveals that NeuBM not
only adjusts the final predictions but also influences the learning of balanced
feature representations throughout the network.

</details>


### [788] [Robust Invariant Representation Learning by Distribution Extrapolation](https://arxiv.org/pdf/2505.16126)
*Kotaro Yoshida, Konstantinos Slavakis*

Main category: cs.LG

TL;DR: A novel extrapolation-based framework improves IRM by enhancing environmental diversity, outperforming existing IRM variants.


<details>
  <summary>Details</summary>
Motivation: Existing IRM methods, like IRMv1, often fail to outperform ERM due to sensitivity to limited environment diversity and over-parameterization.

Method: Proposes an extrapolation-based framework to augment IRM penalties with synthetic distributional shifts, enhancing diversity.

Result: Extensive experiments show the method consistently outperforms state-of-the-art IRM variants.

Conclusion: The proposed framework addresses key limitations of IRM, proving effective and robust for OOD generalization.

Abstract: Invariant risk minimization (IRM) aims to enable out-of-distribution (OOD)
generalization in deep learning by learning invariant representations. As IRM
poses an inherently challenging bi-level optimization problem, most existing
approaches -- including IRMv1 -- adopt penalty-based single-level
approximations. However, empirical studies consistently show that these methods
often fail to outperform well-tuned empirical risk minimization (ERM),
highlighting the need for more robust IRM implementations. This work
theoretically identifies a key limitation common to many IRM variants: their
penalty terms are highly sensitive to limited environment diversity and
over-parameterization, resulting in performance degradation. To address this
issue, a novel extrapolation-based framework is proposed that enhances
environmental diversity by augmenting the IRM penalty through synthetic
distributional shifts. Extensive experiments -- ranging from synthetic setups
to realistic, over-parameterized scenarios -- demonstrate that the proposed
method consistently outperforms state-of-the-art IRM variants, validating its
effectiveness and robustness.

</details>


### [789] [FreshRetailNet-50K: A Stockout-Annotated Censored Demand Dataset for Latent Demand Recovery and Forecasting in Fresh Retail](https://arxiv.org/pdf/2505.16319)
*Yangyang Wang, Jiawei Gu, Li Long, Xin Li, Li Shen, Zhouyu Fu, Xiangjun Zhou, Xu Jiang*

Main category: cs.LG

TL;DR: FreshRetailNet-50K is a large-scale benchmark for censored demand estimation in retail, offering hourly sales data and stockout annotations to improve demand forecasting accuracy.


<details>
  <summary>Details</summary>
Motivation: Addressing the challenge of censored sales data during stockouts, which biases demand estimation and inventory policies for perishable products.

Method: Introduces FreshRetailNet-50K, a dataset with 50,000 store-product time series, hourly sales data, and stockout annotations. Uses a two-stage demand modeling approach: reconstructing latent demand during stockouts and training robust forecasting models.

Result: Achieves a 2.73% improvement in prediction accuracy and reduces systematic demand underestimation from 7.37% to near-zero bias.

Conclusion: FreshRetailNet-50K enables innovative research in demand imputation, perishable inventory optimization, and retail analytics, addressing long-standing limitations in retail AI.

Abstract: Accurate demand estimation is critical for the retail business in guiding the
inventory and pricing policies of perishable products. However, it faces
fundamental challenges from censored sales data during stockouts, where
unobserved demand creates systemic policy biases. Existing datasets lack the
temporal resolution and annotations needed to address this censoring effect. To
fill this gap, we present FreshRetailNet-50K, the first large-scale benchmark
for censored demand estimation. It comprises 50,000 store-product time series
of detailed hourly sales data from 898 stores in 18 major cities, encompassing
863 perishable SKUs meticulously annotated for stockout events. The hourly
stock status records unique to this dataset, combined with rich contextual
covariates, including promotional discounts, precipitation, and temporal
features, enable innovative research beyond existing solutions. We demonstrate
one such use case of two-stage demand modeling: first, we reconstruct the
latent demand during stockouts using precise hourly annotations. We then
leverage the recovered demand to train robust demand forecasting models in the
second stage. Experimental results show that this approach achieves a 2.73%
improvement in prediction accuracy while reducing the systematic demand
underestimation from 7.37% to near-zero bias. With unprecedented temporal
granularity and comprehensive real-world information, FreshRetailNet-50K opens
new research directions in demand imputation, perishable inventory
optimization, and causal retail analytics. The unique annotation quality and
scale of the dataset address long-standing limitations in retail AI, providing
immediate solutions and a platform for future methodological innovation. The
data (https://huggingface.co/datasets/Dingdong-Inc/FreshRetailNet-50K) and code
(https://github.com/Dingdong-Inc/frn-50k-baseline}) are openly released.

</details>


### [790] [PyTupli: A Scalable Infrastructure for Collaborative Offline Reinforcement Learning Projects](https://arxiv.org/pdf/2505.16754)
*Hannah Markgraf, Michael Eichelbeck, Daria Cappey, Selin Demirtürk, Yara Schattschneider, Matthias Althoff*

Main category: cs.LG

TL;DR: PyTupli is a Python tool for creating, storing, and sharing offline RL datasets, addressing the lack of standardized infrastructure for novel benchmarks.


<details>
  <summary>Details</summary>
Motivation: Existing offline RL libraries rely on static datasets, lacking tools for scalable and collaborative dataset management for new benchmarks.

Method: PyTupli provides a lightweight client library for dataset handling, fine-grained filtering, and a containerized server for secure deployment.

Result: The tool enables efficient creation, curation, and sharing of datasets, supporting collaborative and reproducible offline RL research.

Conclusion: PyTupli bridges the gap in dataset infrastructure, enhancing scalability and collaboration in offline RL.

Abstract: Offline reinforcement learning (RL) has gained traction as a powerful
paradigm for learning control policies from pre-collected data, eliminating the
need for costly or risky online interactions. While many open-source libraries
offer robust implementations of offline RL algorithms, they all rely on
datasets composed of experience tuples consisting of state, action, next state,
and reward. Managing, curating, and distributing such datasets requires
suitable infrastructure. Although static datasets exist for established
benchmark problems, no standardized or scalable solution supports developing
and sharing datasets for novel or user-defined benchmarks. To address this gap,
we introduce PyTupli, a Python-based tool to streamline the creation, storage,
and dissemination of benchmark environments and their corresponding tuple
datasets. PyTupli includes a lightweight client library with defined interfaces
for uploading and retrieving benchmarks and data. It supports fine-grained
filtering at both the episode and tuple level, allowing researchers to curate
high-quality, task-specific datasets. A containerized server component enables
production-ready deployment with authentication, access control, and automated
certificate provisioning for secure use. By addressing key barriers in dataset
infrastructure, PyTupli facilitates more collaborative, reproducible, and
scalable offline RL research.

</details>


### [791] [SPAR: Self-supervised Placement-Aware Representation Learning for Multi-Node IoT Systems](https://arxiv.org/pdf/2505.16936)
*Yizhuo Chen, Tianchen Wang, You Lyu, Yanlan Hu, Jinyang Li, Tomoyoshi Kimura, Hongjue Zhao, Yigong Hu, Denizhan Kara, Tarek Abdelzaher*

Main category: cs.LG

TL;DR: The paper introduces a self-supervised method for learning placement-aware representations from multi-view IoT sensor data, improving spatial understanding and generalizability.


<details>
  <summary>Details</summary>
Motivation: To address the need for representing environmental state in IoT systems by accounting for sensor placement and spatial phenomena.

Method: Develops a framework that learns dependencies between sensor measurements and observer layouts, guided by signal-position duality, with theoretical grounding in information theory.

Result: Demonstrates superior performance on real-world datasets (vehicle monitoring, human activity, earthquake localization) across diverse modalities and tasks.

Conclusion: The method advances IoT data pretraining by explicitly modeling spatial relationships, offering robustness and generalizability.

Abstract: This work develops the underpinnings of self-supervised placement-aware
representation learning given spatially-distributed (multi-view and multimodal)
sensor observations, motivated by the need to represent external environmental
state in multi-sensor IoT systems in a manner that correctly distills spatial
phenomena from the distributed multi-vantage observations. The objective of
sensing in IoT systems is, in general, to collectively represent an externally
observed environment given multiple vantage points from which sensory
observations occur. Pretraining of models that help interpret sensor data must
therefore encode the relation between signals observed by sensors and the
observers' vantage points in order to attain a representation that encodes the
observed spatial phenomena in a manner informed by the specific placement of
the measuring instruments, while allowing arbitrary placement. The work
significantly advances self-supervised model pretraining from IoT signals
beyond current solutions that often overlook the distinctive spatial nature of
IoT data. Our framework explicitly learns the dependencies between measurements
and geometric observer layouts and structural characteristics, guided by a core
design principle: the duality between signals and observer positions. We
further provide theoretical analyses from the perspectives of information
theory and occlusion-invariant representation learning to offer insight into
the rationale behind our design. Experiments on three real-world
datasets--covering vehicle monitoring, human activity recognition, and
earthquake localization--demonstrate the superior generalizability and
robustness of our method across diverse modalities, sensor placements,
application-level inference tasks, and spatial scales.

</details>


<div id='cs.MA'></div>

# cs.MA [[Back]](#toc)

### [792] [Swarm Intelligence Enhanced Reasoning: A Density-Driven Framework for LLM-Based Multi-Agent Optimization](https://arxiv.org/pdf/2505.17115)
*Ying Zhu, Heng Zhou, Rui Su, Peiqin Zhuang, Lei Bai*

Main category: cs.MA

TL;DR: The paper proposes integrating swarm intelligence into LLM reasoning to enhance problem-solving by avoiding local optima and improving solution diversity and quality.


<details>
  <summary>Details</summary>
Motivation: Existing methods like CoT and MAD may fail in complex problem-solving due to suboptimal solutions. Swarm intelligence, effective in optimization, is leveraged to address this gap.

Method: Introduces Agent-based Swarm Intelligence (ASI) and SIER framework, using kernel density estimation, non-dominated sorting, and step-level quality evaluation to optimize reasoning.

Result: SIER enhances solution space exploration and quality by diversifying reasoning paths and correcting intermediate steps.

Conclusion: The integration of swarm intelligence into LLM reasoning improves flexibility and efficiency in solving complex problems.

Abstract: Recently, many approaches, such as Chain-of-Thought (CoT) prompting and
Multi-Agent Debate (MAD), have been proposed to further enrich Large Language
Models' (LLMs) complex problem-solving capacities in reasoning scenarios.
However, these methods may fail to solve complex problems due to the lack of
ability to find optimal solutions. Swarm Intelligence has been serving as a
powerful tool for finding optima in the field of traditional optimization
problems. To this end, we propose integrating swarm intelligence into the
reasoning process by introducing a novel Agent-based Swarm Intelligence (ASI)
paradigm. In this paradigm, we formulate LLM reasoning as an optimization
problem and use a swarm intelligence scheme to guide a group of LLM-based
agents in collaboratively searching for optimal solutions. To avoid swarm
intelligence getting trapped in local optima, we further develop a Swarm
Intelligence Enhancing Reasoning (SIER) framework, which develops a
density-driven strategy to enhance the reasoning ability. To be specific, we
propose to perform kernel density estimation and non-dominated sorting to
optimize both solution quality and diversity simultaneously. In this case, SIER
efficiently enhances solution space exploration through expanding the diversity
of the reasoning path. Besides, a step-level quality evaluation is used to help
agents improve solution quality by correcting low-quality intermediate steps.
Then, we use quality thresholds to dynamically control the termination of
exploration and the selection of candidate steps, enabling a more flexible and
efficient reasoning process. Extensive experiments are ...

</details>


### [793] [Multi-agent Systems for Misinformation Lifecycle : Detection, Correction And Source Identification](https://arxiv.org/pdf/2505.17511)
*Aditya Gautam*

Main category: cs.MA

TL;DR: A multi-agent framework for handling misinformation lifecycle (classification, detection, correction, verification) is proposed, enhancing transparency and reliability compared to single-agent methods.


<details>
  <summary>Details</summary>
Motivation: Address the limitations of isolated LLM or AI Agent solutions in combating misinformation by providing a scalable, modular, and explainable approach.

Method: Five specialized agents (Indexer, Classifier, Extractor, Corrector, Verification) work together to cover the misinformation lifecycle, ensuring adaptability and transparency.

Result: The framework improves scalability, modularity, and explainability in misinformation detection and correction.

Conclusion: The multi-agent approach offers a robust solution for misinformation lifecycle management, with potential for further optimization as new challenges arise.

Abstract: The rapid proliferation of misinformation in digital media demands solutions
that go beyond isolated Large Language Model(LLM) or AI Agent based detection
methods. This paper introduces a novel multi-agent framework that covers the
complete misinformation lifecycle: classification, detection, correction, and
source verification to deliver more transparent and reliable outcomes. In
contrast to single-agent or monolithic architectures, our approach employs five
specialized agents: an Indexer agent for dynamically maintaining trusted
repositories, a Classifier agent for labeling misinformation types, an
Extractor agent for evidence based retrieval and ranking, a Corrector agent for
generating fact-based correction and a Verification agent for validating
outputs and tracking source credibility. Each agent can be individually
evaluated and optimized, ensuring scalability and adaptability as new types of
misinformation and data sources emerge. By decomposing the misinformation
lifecycle into specialized agents - our framework enhances scalability,
modularity, and explainability. This paper proposes a high-level system
overview, agent design with emphasis on transparency, evidence-based outputs,
and source provenance to support robust misinformation detection and correction
at scale.

</details>


### [794] [Feasible Action Space Reduction for Quantifying Causal Responsibility in Continuous Spatial Interactions](https://arxiv.org/pdf/2505.17739)
*Ashwin George, Luciano Cavalcante Siebert, David A. Abbink, Arkady Zgonnikov*

Main category: cs.MA

TL;DR: The paper extends the Feasible Action-Space Reduction (FeAR) metric from discrete to continuous action spaces to measure causal responsibility in real-world spatial interactions.


<details>
  <summary>Details</summary>
Motivation: To enable safe deployment of AI systems like automated vehicles and robots in human-inhabited environments by accurately assessing causal responsibility in continuous spatial interactions.

Method: Proposes a formulation of the FeAR metric for continuous action spaces, tested in prototypical space-sharing conflicts.

Result: Demonstrates the FeAR metric's utility for analyzing backward-looking responsibility and guiding forward-looking agent decision-making.

Conclusion: Highlights FeAR's potential for designing AI agents and assessing responsibility in human-agent interactions.

Abstract: Understanding the causal influence of one agent on another agent is crucial
for safely deploying artificially intelligent systems such as automated
vehicles and mobile robots into human-inhabited environments. Existing models
of causal responsibility deal with simplified abstractions of scenarios with
discrete actions, thus, limiting real-world use when understanding
responsibility in spatial interactions. Based on the assumption that spatially
interacting agents are embedded in a scene and must follow an action at each
instant, Feasible Action-Space Reduction (FeAR) was proposed as a metric for
causal responsibility in a grid-world setting with discrete actions. Since
real-world interactions involve continuous action spaces, this paper proposes a
formulation of the FeAR metric for measuring causal responsibility in
space-continuous interactions. We illustrate the utility of the metric in
prototypical space-sharing conflicts, and showcase its applications for
analysing backward-looking responsibility and in estimating forward-looking
responsibility to guide agent decision making. Our results highlight the
potential of the FeAR metric for designing and engineering artificial agents,
as well as for assessing the responsibility of agents around humans.

</details>


### [795] [HYGMA: Hypergraph Coordination Networks with Dynamic Grouping for Multi-Agent Reinforcement Learning](https://arxiv.org/pdf/2505.07207)
*Chiqiang Liu, Dazi Li*

Main category: cs.MA

TL;DR: A novel framework combining dynamic spectral clustering and hypergraph neural networks improves adaptive coordination and information exchange in multi-agent systems.


<details>
  <summary>Details</summary>
Motivation: Challenges in organizing agent relationships and dynamic coordination in cooperative multi-agent reinforcement learning.

Method: Integrates dynamic spectral clustering with hypergraph neural networks for adaptive group formation and efficient information processing. Uses attention mechanisms for selective information processing.

Result: Outperforms state-of-the-art methods in sample efficiency and final performance on cooperative tasks.

Conclusion: The framework effectively models complex agent relationships and enhances coordination in multi-agent systems.

Abstract: Cooperative multi-agent reinforcement learning faces significant challenges
in effectively organizing agent relationships and facilitating information
exchange, particularly when agents need to adapt their coordination patterns
dynamically. This paper presents a novel framework that integrates dynamic
spectral clustering with hypergraph neural networks to enable adaptive group
formation and efficient information processing in multi-agent systems. The
proposed framework dynamically constructs and updates hypergraph structures
through spectral clustering on agents' state histories, enabling higher-order
relationships to emerge naturally from agent interactions. The hypergraph
structure is enhanced with attention mechanisms for selective information
processing, providing an expressive and efficient way to model complex agent
relationships. This architecture can be implemented in both value-based and
policy-based paradigms through a unified objective combining task performance
with structural regularization. Extensive experiments on challenging
cooperative tasks demonstrate that our method significantly outperforms
state-of-the-art approaches in both sample efficiency and final performance.

</details>


<div id='cs.MM'></div>

# cs.MM [[Back]](#toc)

### [796] [Multimodal Classification and Out-of-distribution Detection for Multimodal Intent Understanding](https://arxiv.org/pdf/2412.12453)
*Hanlei Zhang, Qianrui Zhou, Hua Xu, Jianhua Su, Roberto Evans, Kai Gao*

Main category: cs.MM

TL;DR: The paper proposes MIntOOD, a novel method for multimodal intent understanding, addressing challenges in ID classification and OOD detection by dynamically fusing modalities and synthesizing pseudo-OOD data.


<details>
  <summary>Details</summary>
Motivation: Existing methods struggle with nuanced ID intent semantics and poor OOD generalization.

Method: Introduces a weighted feature fusion network and synthesizes pseudo-OOD data for multimodal representation learning from coarse- and fine-grained perspectives.

Result: Achieves 3~10% AUROC improvement in OOD detection and state-of-the-art ID classification.

Conclusion: MIntOOD effectively addresses ID and OOD challenges, with publicly available data and code.

Abstract: Multimodal intent understanding is a significant research area that requires
effective leveraging of multiple modalities to analyze human language. Existing
methods face two main challenges in this domain. Firstly, they have limitations
in capturing the nuanced and high-level semantics underlying complex
in-distribution (ID) multimodal intents. Secondly, they exhibit poor
generalization when confronted with unseen out-of-distribution (OOD) data in
real-world scenarios. To address these issues, we propose a novel method for
both ID classification and OOD detection (MIntOOD). We first introduce a
weighted feature fusion network that models multimodal representations. This
network dynamically learns the importance of each modality, adapting to
multimodal contexts. To develop discriminative representations for both tasks,
we synthesize pseudo-OOD data from convex combinations of ID data and engage in
multimodal representation learning from both coarse-grained and fine-grained
perspectives. The coarse-grained perspective focuses on distinguishing between
ID and OOD binary classes, while the fine-grained perspective not only enhances
the discrimination between different ID classes but also captures
instance-level interactions between ID and OOD samples, promoting proximity
among similar instances and separation from dissimilar ones. We establish
baselines for three multimodal intent datasets and build an OOD benchmark.
Extensive experiments on these datasets demonstrate that our method
significantly improves OOD detection performance with a 3~10% increase in AUROC
scores while achieving new state-of-the-art results in ID classification. Data
and codes are available at https://github.com/thuiar/MIntOOD.

</details>


<div id='eess.AS'></div>

# eess.AS [[Back]](#toc)

### [797] [From Weak Labels to Strong Results: Utilizing 5,000 Hours of Noisy Classroom Transcripts with Minimal Accurate Data](https://arxiv.org/pdf/2505.17088)
*Ahmed Adel Attia, Dorottya Demszky, Jing Liu, Carol Espy-Wilson*

Main category: eess.AS

TL;DR: The paper proposes Weakly Supervised Pretraining (WSP) for low-resource ASR, leveraging weak transcripts for pretraining and fine-tuning on limited gold-standard data, outperforming other methods.


<details>
  <summary>Details</summary>
Motivation: Classroom ASR faces challenges due to abundant weak transcripts and limited gold-standard data, making re-transcription impractical. The paper explores the best approach for such low-resource settings.

Method: WSP involves pretraining models on weak transcripts in a supervised manner, followed by fine-tuning on accurate data.

Result: WSP outperforms alternative methods, as demonstrated on both synthetic and real weak transcripts.

Conclusion: WSP is an effective training methodology for low-resource ASR in real-world scenarios.

Abstract: Recent progress in speech recognition has relied on models trained on vast
amounts of labeled data. However, classroom Automatic Speech Recognition (ASR)
faces the real-world challenge of abundant weak transcripts paired with only a
small amount of accurate, gold-standard data. In such low-resource settings,
high transcription costs make re-transcription impractical. To address this, we
ask: what is the best approach when abundant inexpensive weak transcripts
coexist with limited gold-standard data, as is the case for classroom speech
data? We propose Weakly Supervised Pretraining (WSP), a two-step process where
models are first pretrained on weak transcripts in a supervised manner, and
then fine-tuned on accurate data. Our results, based on both synthetic and real
weak transcripts, show that WSP outperforms alternative methods, establishing
it as an effective training methodology for low-resource ASR in real-world
scenarios.

</details>


### [798] [Voicing Personas: Rewriting Persona Descriptions into Style Prompts for Controllable Text-to-Speech](https://arxiv.org/pdf/2505.17093)
*Yejin Lee, Jaehoon Kang, Kyuhong Shim*

Main category: eess.AS

TL;DR: A framework for controlling voice style in text-to-speech systems using textual personas, with strategies to rewrite personas for fine-grained prosodic control, improving speech quality and addressing social biases.


<details>
  <summary>Details</summary>
Motivation: To enhance the naturalness and controllability of synthesized speech in persona-driven AI dialogue systems by leveraging textual personas as style prompts.

Method: Proposes two persona rewriting strategies to convert generic descriptions into speech-oriented prompts for manipulating prosodic attributes like pitch, emotion, and speaking rate.

Result: Experimental results show improved naturalness, clarity, and consistency of synthesized speech.

Conclusion: Highlights the importance of voice style in persona-driven AI systems and addresses social biases, particularly gender, introduced by LLM-based rewriting.

Abstract: In this paper, we propose a novel framework to control voice style in
prompt-based, controllable text-to-speech systems by leveraging textual
personas as voice style prompts. We present two persona rewriting strategies to
transform generic persona descriptions into speech-oriented prompts, enabling
fine-grained manipulation of prosodic attributes such as pitch, emotion, and
speaking rate. Experimental results demonstrate that our methods enhance the
naturalness, clarity, and consistency of synthesized speech. Finally, we
analyze implicit social biases introduced by LLM-based rewriting, with a focus
on gender. We underscore voice style as a crucial factor for persona-driven AI
dialogue systems.

</details>


### [799] [Speechless: Speech Instruction Training Without Speech for Low Resource Languages](https://arxiv.org/pdf/2505.17417)
*Alan Dao, Dinh Bach Vu, Huy Hoang Ha, Tuan Le Duc Anh, Shreyas Gopal, Yue Heng Yeo, Warren Keng Hoong Low, Eng Siong Chng, Jia Qi Yip*

Main category: eess.AS

TL;DR: A novel method bypasses TTS for speech instruction data by aligning synthetic semantic representations with Whisper encoder, enabling LLM fine-tuning for low-resource languages.


<details>
  <summary>Details</summary>
Motivation: Addressing the scarcity of speech instruction data, especially for low-resource languages lacking TTS models.

Method: Halts synthesis at semantic representation level, aligning synthetic representations with Whisper encoder for LLM fine-tuning.

Result: Enables LLMs to understand spoken instructions without needing TTS, simplifying training for low-resource languages.

Conclusion: The approach is promising for developing voice assistants in low-resource languages by eliminating TTS dependency.

Abstract: The rapid growth of voice assistants powered by large language models (LLM)
has highlighted a need for speech instruction data to train these systems.
Despite the abundance of speech recognition data, there is a notable scarcity
of speech instruction data, which is essential for fine-tuning models to
understand and execute spoken commands. Generating high-quality synthetic
speech requires a good text-to-speech (TTS) model, which may not be available
to low resource languages. Our novel approach addresses this challenge by
halting synthesis at the semantic representation level, bypassing the need for
TTS. We achieve this by aligning synthetic semantic representations with the
pre-trained Whisper encoder, enabling an LLM to be fine-tuned on text
instructions while maintaining the ability to understand spoken instructions
during inference. This simplified training process is a promising approach to
building voice assistant for low-resource languages.

</details>


### [800] [Private kNN-VC: Interpretable Anonymization of Converted Speech](https://arxiv.org/pdf/2505.17584)
*Carlos Franzreb, Arnab Das, Tim Polzehl, Sebastian Möller*

Main category: eess.AS

TL;DR: The paper investigates how prosodic features (duration and variation of phones) in speech are exploited for speaker identification, using kNN-VC and its extensions to improve anonymization.


<details>
  <summary>Details</summary>
Motivation: To understand which aspects of speech are used to identify speakers in anonymized speech and improve anonymization techniques.

Method: Extends kNN-VC with interpretable components to anonymize prosodic features (duration and variation of phones) and tests their impact on privacy.

Result: The extended components significantly increase privacy, confirming prosodic features encode speaker identity. Target selection algorithms also affect privacy outcomes.

Conclusion: Prosodic features are key to speaker identification, and modifying them enhances anonymization. Target selection algorithms play a role in privacy attacks.

Abstract: Speaker anonymization seeks to conceal a speaker's identity while preserving
the utility of their speech. The achieved privacy is commonly evaluated with a
speaker recognition model trained on anonymized speech. Although this
represents a strong attack, it is unclear which aspects of speech are exploited
to identify the speakers. Our research sets out to unveil these aspects. It
starts with kNN-VC, a powerful voice conversion model that performs poorly as
an anonymization system, presumably because of prosody leakage. To test this
hypothesis, we extend kNN-VC with two interpretable components that anonymize
the duration and variation of phones. These components increase privacy
significantly, proving that the studied prosodic factors encode speaker
identity and are exploited by the privacy attack. Additionally, we show that
changes in the target selection algorithm considerably influence the outcome of
the privacy attack.

</details>


### [801] [Audio-to-Audio Emotion Conversion With Pitch And Duration Style Transfer](https://arxiv.org/pdf/2505.17655)
*Soumya Dutta, Avni Jain, Sriram Ganapathy*

Main category: eess.AS

TL;DR: A2A-ZEST enables zero-shot emotion style transfer in speech by decomposing and recombining semantic, speaker, and emotion attributes without parallel training data.


<details>
  <summary>Details</summary>
Motivation: To transfer emotional attributes from reference speech to source speech while preserving content and speaker identity, without requiring parallel data.

Method: Uses an analysis-synthesis pipeline: decomposes speech into semantic tokens, speaker representations, and emotion embeddings; predicts pitch and duration; synthesizes speech with these factors.

Result: Outperforms prior works in content/speaker preservation and emotion transfer, enabling zero-shot style transfer.

Conclusion: A2A-ZEST is effective for emotion style transfer and data augmentation in emotion recognition, without needing parallel data.

Abstract: Given a pair of source and reference speech recordings, audio-to-audio (A2A)
style transfer involves the generation of an output speech that mimics the
style characteristics of the reference while preserving the content and speaker
attributes of the source. In this paper, we propose a novel framework, termed
as A2A Zero-shot Emotion Style Transfer (A2A-ZEST), that enables the transfer
of reference emotional attributes to the source while retaining its speaker and
speech contents. The A2A-ZEST framework consists of an analysis-synthesis
pipeline, where the analysis module decomposes speech into semantic tokens,
speaker representations, and emotion embeddings. Using these representations, a
pitch contour estimator and a duration predictor are learned. Further, a
synthesis module is designed to generate speech based on the input
representations and the derived factors. This entire paradigm of
analysis-synthesis is trained purely in a self-supervised manner with an
auto-encoding loss. For A2A emotion style transfer, the emotion embedding
extracted from the reference speech along with the rest of the representations
from the source speech are used in the synthesis module to generate the style
translated speech. In our experiments, we evaluate the converted speech on
content/speaker preservation (w.r.t. source) as well as on the effectiveness of
the emotion style transfer (w.r.t. reference). The proposal, A2A-ZEST, is shown
to improve over other prior works on these evaluations, thereby enabling style
transfer without any parallel training data. We also illustrate the application
of the proposed work for data augmentation in emotion recognition tasks.

</details>


### [802] [Source Separation of Small Classical Ensembles: Challenges and Opportunities](https://arxiv.org/pdf/2505.17823)
*Gerardo Roa-Dabike, Trevor J. Cox, Jon P. Barker, Michael A. Akeroyd, Scott Bannister, Bruno Fazenda, Jennifer Firth, Simone Graetzer, Alinka Greasley, Rebecca R. Vos, William M. Whitmer*

Main category: eess.AS

TL;DR: The paper explores musical source separation (MSS) for classical music using ConvTasNet, addressing challenges like data sparsity and instrument ambiguity. It compares causal and non-causal approaches, finding similar performance but highlighting a mismatch between synthesized and real data.


<details>
  <summary>Details</summary>
Motivation: To improve listening experiences for people with hearing loss by enabling remixing of classical music, which is harder to separate than popular music due to greater variation and lack of ground-truth data.

Method: Used ConvTasNet models trained on synthesized woodwind ensembles to separate instruments, testing both causal and non-causal approaches. Evaluated on small real datasets (Bach10, URMP).

Result: Performance was similar for causal and non-causal systems, but a significant mismatch between synthesized and real data was observed (SDR: 6.2-6.9 dB vs. 0.3-0.4 dB).

Conclusion: Future work should focus on gathering more real recordings or improving synthesized data realism to reduce the mismatch and enhance MSS performance for classical music.

Abstract: Musical (MSS) source separation of western popular music using non-causal
deep learning can be very effective. In contrast, MSS for classical music is an
unsolved problem. Classical ensembles are harder to separate than popular music
because of issues such as the inherent greater variation in the music; the
sparsity of recordings with ground truth for supervised training; and greater
ambiguity between instruments. The Cadenza project has been exploring MSS for
classical music. This is being done so music can be remixed to improve
listening experiences for people with hearing loss. To enable the work, a new
database of synthesized woodwind ensembles was created to overcome instrumental
imbalances in the EnsembleSet. For the MSS, a set of ConvTasNet models was used
with each model being trained to extract a string or woodwind instrument.
ConvTasNet was chosen because it enabled both causal and non-causal approaches
to be tested. Non-causal approaches have dominated MSS work and are useful for
recorded music, but for live music or processing on hearing aids, causal signal
processing is needed. The MSS performance was evaluated on the two small
datasets (Bach10 and URMP) of real instrument recordings where the ground-truth
is available. The performances of the causal and non-causal systems were
similar. Comparing the average Signal-to-Distortion (SDR) of the synthesized
validation set (6.2 dB causal; 6.9 non-causal), to the real recorded evaluation
set (0.3 dB causal, 0.4 dB non-causal), shows that mismatch between synthesized
and recorded data is a problem. Future work needs to either gather more real
recordings that can be used for training, or to improve the realism and
diversity of the synthesized recordings to reduce the mismatch...

</details>


### [803] [Effects of auditory distance cues and reverberation on spatial perception and listening strategies](https://arxiv.org/pdf/2505.18020)
*Fulvio Missoni, Katarina Poole, Lorenzo Picinali, Andrea Canessa*

Main category: eess.AS

TL;DR: The study explores spatial hearing in ecologically valid contexts, focusing on listener movement, reverberation, and distance. Findings show adaptive head movements in reverberant conditions and distance's impact on localization accuracy.


<details>
  <summary>Details</summary>
Motivation: To address the gap in understanding spatial hearing in real-life conditions by examining listener movement, reverberation, and distance.

Method: Participants performed active localization tasks in anechoic or reverberant conditions without specific listening strategy instructions.

Result: Head movements increased in reverberant environments, suggesting adaptive strategies. Distance affected localization performance but not listening strategy.

Conclusion: Listening behavior adapts to acoustic conditions to enhance spatial perception.

Abstract: Spatial hearing, the brain's ability to use auditory cues to identify the
origin of sounds, is crucial for everyday listening. While simplified paradigms
have advanced the understanding of spatial hearing, their lack of ecological
validity limits their applicability to real-life conditions. This study aims to
address this gap by investigating the effects of listener movement,
reverberation, and distance on localisation accuracy in a more ecologically
valid context. Participants performed active localisation tasks with no
specific instructions on listening strategy, in either anechoic or reverberant
conditions. The results indicate that the head movements were more frequent in
reverberant environments, suggesting an adaptive strategy to mitigate
uncertainty in binaural cues due to reverberation. While distance did not
affect the listening strategy, it influenced the localisation performance. Our
outcomes suggest that listening behaviour is adapted depending on the current
acoustic conditions to support an effective perception of the space.

</details>


### [804] [Auto-Landmark: Acoustic Landmark Dataset and Open-Source Toolkit for Landmark Extraction](https://arxiv.org/pdf/2409.07969)
*Xiangyu Zhang, Daijiao Liu, Tianyi Xiao, Cihan Xiao, Tuende Szalay, Mostafa Shahin, Beena Ahmed, Julien Epps*

Main category: eess.AS

TL;DR: The paper introduces a dataset with precise timing for acoustic landmarks, an open-source landmark extraction tool, and detection baselines to support future research.


<details>
  <summary>Details</summary>
Motivation: Existing datasets lack precise timing for acoustic landmarks, which is crucial for applications like speech recognition and clinical analysis.

Method: Selected useful landmarks, annotated the TIMIT dataset using phoneme boundaries and manual inspection, and developed an open-source Python tool for landmark extraction.

Result: Created the first dataset with precise landmark timing, a landmark extraction tool, and detection baselines.

Conclusion: These resources aim to facilitate diverse future research involving acoustic landmarks.

Abstract: In the speech signal, acoustic landmarks identify times when the acoustic
manifestations of the linguistically motivated distinctive features are most
salient. Acoustic landmarks have been widely applied in various domains,
including speech recognition, speech depression detection, clinical analysis of
speech abnormalities, and the detection of disordered speech. However, there is
currently no dataset available that provides precise timing information for
landmarks, which has been proven to be crucial for downstream applications
involving landmarks. In this paper, we selected the most useful acoustic
landmarks based on previous research and annotated the TIMIT dataset with them,
based on a combination of phoneme boundary information and manual inspection.
Moreover, previous landmark extraction tools were not open source or
benchmarked, so to address this, we developed an open source Python-based
landmark extraction tool and established a series of landmark detection
baselines. The first of their kinds, the dataset with landmark precise timing
information, landmark extraction tool and baselines are designed to support a
wide variety of future research.

</details>


### [805] [SpeechT-RAG: Reliable Depression Detection in LLMs with Retrieval-Augmented Generation Using Speech Timing Information](https://arxiv.org/pdf/2502.10950)
*Xiangyu Zhang, Hexin Liu, Qiquan Zhang, Beena Ahmed, Julien Epps*

Main category: eess.AS

TL;DR: SpeechT-RAG, a novel system leveraging speech timing features, outperforms text-based RAG in depression detection and improves confidence estimation.


<details>
  <summary>Details</summary>
Motivation: Existing text-based RAG systems struggle to improve depression detection accuracy due to missing acoustic speech pattern information.

Method: Systematic analysis of temporal speech patterns and introduction of SpeechT-RAG, integrating speech timing features for detection and confidence scoring.

Result: SpeechT-RAG outperforms text-based RAG in accuracy and provides reliable confidence estimation, matching fine-tuned LLMs without extra training.

Conclusion: SpeechT-RAG addresses accuracy and trustworthiness in mental health assessment by unifying speech timing features with RAG.

Abstract: Large Language Models (LLMs) have been increasingly adopted for
health-related tasks, yet their performance in depression detection remains
limited when relying solely on text input. While Retrieval-Augmented Generation
(RAG) typically enhances LLM capabilities, our experiments indicate that
traditional text-based RAG systems struggle to significantly improve depression
detection accuracy. This challenge stems partly from the rich
depression-relevant information encoded in acoustic speech patterns information
that current text-only approaches fail to capture effectively. To address this
limitation, we conduct a systematic analysis of temporal speech patterns,
comparing healthy individuals with those experiencing depression. Based on our
findings, we introduce Speech Timing-based Retrieval-Augmented Generation,
SpeechT-RAG, a novel system that leverages speech timing features for both
accurate depression detection and reliable confidence estimation. This
integrated approach not only outperforms traditional text-based RAG systems in
detection accuracy but also enhances uncertainty quantification through a
confidence scoring mechanism that naturally extends from the same temporal
features. Our unified framework achieves comparable results to fine-tuned LLMs
without additional training while simultaneously addressing the fundamental
requirements for both accuracy and trustworthiness in mental health assessment.

</details>


### [806] [Impact of Microphone Array Mismatches to Learning-based Replay Speech Detection](https://arxiv.org/pdf/2503.07357)
*Michael Neri, Tuomas Virtanen*

Main category: eess.AS

TL;DR: A study on generalizing a replay speech detector across microphone arrays reveals performance degradation due to mismatches but shows fine-tuning with minimal data can recover accuracy.


<details>
  <summary>Details</summary>
Motivation: Deep neural network-based microphone array techniques often fail to generalize to unseen arrays, leading to training-test performance gaps.

Method: The study uses the ReMASC dataset to analyze performance degradation from inter- and intra-device mismatches and explores fine-tuning with target data.

Result: Array mismatches reduce detection accuracy, with intra-device generalization outperforming inter-device. Fine-tuning with ten minutes of data recovers performance.

Conclusion: Fine-tuning with small target datasets can mitigate performance loss, aiding practical deployment in diverse speaker verification systems.

Abstract: In this work, we investigate the generalization of a multi-channel
learning-based replay speech detector, which employs adaptive beamforming and
detection, across different microphone arrays. In general, deep neural
network-based microphone array processing techniques generalize poorly to
unseen array types, i.e., showing a significant training-test mismatch of
performance. We employ the ReMASC dataset to analyze performance degradation
due to inter- and intra-device mismatches, assessing both single- and
multi-channel configurations. Furthermore, we explore fine-tuning to mitigate
the performance loss when transitioning to unseen microphone arrays. Our
findings reveal that array mismatches significantly decrease detection
accuracy, with intra-device generalization being more robust than inter-device.
However, fine-tuning with as little as ten minutes of target data can
effectively recover performance, providing insights for practical deployment of
replay detection systems in heterogeneous automatic speaker verification
environments.

</details>


### [807] [SAKURA: On the Multi-hop Reasoning of Large Audio-Language Models Based on Speech and Audio Information](https://arxiv.org/pdf/2505.13237)
*Chih-Kai Yang, Neo Ho, Yen-Ting Piao, Hung-yi Lee*

Main category: eess.AS

TL;DR: SAKURA benchmark reveals LALMs' struggle with multi-hop reasoning despite correct information extraction, highlighting a key multimodal challenge.


<details>
  <summary>Details</summary>
Motivation: Existing benchmarks overlook multi-hop reasoning in LALMs, leaving their ability to integrate speech/audio facts underexplored.

Method: Introduces SAKURA, a benchmark to evaluate LALMs' multi-hop reasoning using speech and audio data.

Result: LALMs fail to integrate speech/audio representations for multi-hop reasoning, even with correct information extraction.

Conclusion: Exposes a critical limitation in LALMs, providing insights and resources for future research in multimodal reasoning.

Abstract: Large audio-language models (LALMs) extend the large language models with
multimodal understanding in speech, audio, etc. While their performances on
speech and audio-processing tasks are extensively studied, their reasoning
abilities remain underexplored. Particularly, their multi-hop reasoning, the
ability to recall and integrate multiple facts, lacks systematic evaluation.
Existing benchmarks focus on general speech and audio-processing tasks,
conversational abilities, and fairness but overlook this aspect. To bridge this
gap, we introduce SAKURA, a benchmark assessing LALMs' multi-hop reasoning
based on speech and audio information. Results show that LALMs struggle to
integrate speech/audio representations for multi-hop reasoning, even when they
extract the relevant information correctly, highlighting a fundamental
challenge in multimodal reasoning. Our findings expose a critical limitation in
LALMs, offering insights and resources for future research.

</details>


### [808] [U-SAM: An audio language Model for Unified Speech, Audio, and Music Understanding](https://arxiv.org/pdf/2505.13880)
*Ziqian Wang, Xianjun Xia, Xinfa Zhu, Lei Xie*

Main category: eess.AS

TL;DR: U-SAM is an advanced audio language model integrating specialized encoders for speech, audio, and music with a pre-trained LLM, using MoE for task-aware fusion and a contrastive loss module for better cross-modal alignment. It outperforms existing models and shows generalization potential.


<details>
  <summary>Details</summary>
Motivation: Existing audio language models struggle with comprehensive understanding across diverse audio types and rely on cross-entropy loss, which inadequately handles redundant features and weak alignment.

Method: U-SAM combines domain-specific encoders with an LLM, uses a Mixture of Experts (MoE) projector for dynamic feature fusion, and introduces a Semantic-Aware Contrastive Loss Module to refine cross-modal alignment.

Result: U-SAM outperforms specialized models and existing audio language models in benchmarks and demonstrates emergent capabilities on unseen tasks.

Conclusion: U-SAM advances unified audio understanding by addressing alignment and redundancy issues, showcasing strong performance and generalization.

Abstract: The text generation paradigm for audio tasks has opened new possibilities for
unified audio understanding. However, existing models face significant
challenges in achieving a comprehensive understanding across diverse audio
types, such as speech, general audio events, and music. Furthermore, their
exclusive reliance on cross-entropy loss for alignment often falls short, as it
treats all tokens equally and fails to account for redundant audio features,
leading to weaker cross-modal alignment. To deal with the above challenges,
this paper introduces U-SAM, an advanced audio language model that integrates
specialized encoders for speech, audio, and music with a pre-trained large
language model (LLM). U-SAM employs a Mixture of Experts (MoE) projector for
task-aware feature fusion, dynamically routing and integrating the
domain-specific encoder outputs. Additionally, U-SAM incorporates a
Semantic-Aware Contrastive Loss Module, which explicitly identifies redundant
audio features under language supervision and rectifies their semantic and
spectral representations to enhance cross-modal alignment. Extensive
experiments demonstrate that U-SAM consistently outperforms both specialized
models and existing audio language models across multiple benchmarks. Moreover,
it exhibits emergent capabilities on unseen tasks, showcasing its
generalization potential. Code is available
(https://github.com/Honee-W/U-SAM/).

</details>


### [809] [Towards Holistic Evaluation of Large Audio-Language Models: A Comprehensive Survey](https://arxiv.org/pdf/2505.15957)
*Chih-Kai Yang, Neo S. Ho, Hung-yi Lee*

Main category: eess.AS

TL;DR: The paper proposes a systematic taxonomy for evaluating large audio-language models (LALMs), categorizing assessments into four dimensions and addressing challenges and future directions.


<details>
  <summary>Details</summary>
Motivation: Existing benchmarks for LALMs are fragmented and lack a structured taxonomy, hindering comprehensive evaluation.

Method: Conducts a survey and introduces a taxonomy for LALM evaluations, categorizing them into four dimensions.

Result: Provides detailed overviews of each category, highlights challenges, and suggests future directions.

Conclusion: This is the first survey focused on LALM evaluations, offering clear guidelines and a maintained collection of surveyed papers.

Abstract: With advancements in large audio-language models (LALMs), which enhance large
language models (LLMs) with auditory capabilities, these models are expected to
demonstrate universal proficiency across various auditory tasks. While numerous
benchmarks have emerged to assess LALMs' performance, they remain fragmented
and lack a structured taxonomy. To bridge this gap, we conduct a comprehensive
survey and propose a systematic taxonomy for LALM evaluations, categorizing
them into four dimensions based on their objectives: (1) General Auditory
Awareness and Processing, (2) Knowledge and Reasoning, (3) Dialogue-oriented
Ability, and (4) Fairness, Safety, and Trustworthiness. We provide detailed
overviews within each category and highlight challenges in this field, offering
insights into promising future directions. To the best of our knowledge, this
is the first survey specifically focused on the evaluations of LALMs, providing
clear guidelines for the community. We will release the collection of the
surveyed papers and actively maintain it to support ongoing advancements in the
field.

</details>


### [810] [Adversarial Deep Metric Learning for Cross-Modal Audio-Text Alignment in Open-Vocabulary Keyword Spotting](https://arxiv.org/pdf/2505.16735)
*Youngmoon Jung, Yong-Hyeok Lee, Myunghun Jung, Jaeyoung Roh, Chang Woo Han, Hoon-Young Cho*

Main category: eess.AS

TL;DR: The paper proposes Modality Adversarial Learning (MAL) to bridge the gap between audio and text embeddings for keyword spotting, combining it with deep metric learning for phoneme-level alignment.


<details>
  <summary>Details</summary>
Motivation: The challenge lies in the inherent heterogeneity between audio and text modalities, making direct comparison difficult.

Method: The approach uses MAL to train a modality classifier adversarially, encouraging modality-invariant embeddings, and applies deep metric learning for phoneme-level alignment.

Result: Experiments on WSJ and LibriPhrase datasets show the method's effectiveness.

Conclusion: The proposed MAL and DML combination successfully addresses modality heterogeneity and improves keyword spotting performance.

Abstract: For text enrollment-based open-vocabulary keyword spotting (KWS), acoustic
and text embeddings are typically compared at either the phoneme or utterance
level. To facilitate this, we optimize acoustic and text encoders using deep
metric learning (DML), enabling direct comparison of multi-modal embeddings in
a shared embedding space. However, the inherent heterogeneity between audio and
text modalities presents a significant challenge. To address this, we propose
Modality Adversarial Learning (MAL), which reduces the domain gap in
heterogeneous modality representations. Specifically, we train a modality
classifier adversarially to encourage both encoders to generate
modality-invariant embeddings. Additionally, we apply DML to achieve
phoneme-level alignment between audio and text, and conduct extensive
comparisons across various DML objectives. Experiments on the Wall Street
Journal (WSJ) and LibriPhrase datasets demonstrate the effectiveness of the
proposed approach.

</details>


### [811] [Active Speech Enhancement: Active Speech Denoising Decliping and Deveraberation](https://arxiv.org/pdf/2505.16911)
*Ofir Yaish, Yehuda Mishaly, Eliya Nachmani*

Main category: eess.AS

TL;DR: Active Speech Enhancement (ASE) improves speech intelligibility by combining noise suppression and signal amplification, outperforming traditional methods.


<details>
  <summary>Details</summary>
Motivation: To enhance speech quality beyond noise cancellation by actively shaping the speech signal.

Method: Uses a Transformer-Mamba-based architecture with a task-specific loss function for joint optimization.

Result: Outperforms baselines in denoising, dereverberation, and declipping tasks.

Conclusion: ASE is effective for targeted speech enhancement in challenging environments.

Abstract: We introduce a new paradigm for active sound modification: Active Speech
Enhancement (ASE). While Active Noise Cancellation (ANC) algorithms focus on
suppressing external interference, ASE goes further by actively shaping the
speech signal -- both attenuating unwanted noise components and amplifying
speech-relevant frequencies -- to improve intelligibility and perceptual
quality. To enable this, we propose a novel Transformer-Mamba-based
architecture, along with a task-specific loss function designed to jointly
optimize interference suppression and signal enrichment. Our method outperforms
existing baselines across multiple speech processing tasks -- including
denoising, dereverberation, and declipping -- demonstrating the effectiveness
of active, targeted modulation in challenging acoustic environments.

</details>


<div id='eess.IV'></div>

# eess.IV [[Back]](#toc)

### [812] [Distillation-Enabled Knowledge Alignment Protocol for Semantic Communication in AI Agent Networks](https://arxiv.org/pdf/2505.17030)
*Jingzhi Hu, Geoffrey Ye Li*

Main category: eess.IV

TL;DR: DeKAP aligns AI agents' knowledge efficiently via distillation into low-rank matrices, optimizing communication and storage.


<details>
  <summary>Details</summary>
Motivation: Future networks need AI agents to collaborate, but their distinct expert knowledge complicates semantic communication.

Method: Proposes DeKAP, distilling knowledge into low-rank matrices, solving alignment via integer linear programming and a greedy algorithm.

Result: DeKAP achieves knowledge alignment with minimal communication and computation resources.

Conclusion: DeKAP is a resource-efficient solution for aligning AI agents' knowledge in future networks.

Abstract: Future networks are envisioned to connect massive artificial intelligence
(AI) agents, enabling their extensive collaboration on diverse tasks. Compared
to traditional entities, these agents naturally suit the semantic communication
(SC), which can significantly enhance the bandwidth efficiency. Nevertheless,
SC requires the knowledge among agents to be aligned, while agents have
distinct expert knowledge for their individual tasks in practice. In this
paper, we propose a distillation-enabled knowledge alignment protocol (DeKAP),
which distills the expert knowledge of each agent into parameter-efficient
low-rank matrices, allocates them across the network, and allows agents to
simultaneously maintain aligned knowledge for multiple tasks. We formulate the
joint minimization of alignment loss, communication overhead, and storage cost
as a large-scale integer linear programming problem and develop a highly
efficient greedy algorithm. From computer simulation, the DeKAP establishes
knowledge alignment with the lowest communication and computation resources
compared to conventional approaches.

</details>


### [813] [TAGS: 3D Tumor-Adaptive Guidance for SAM](https://arxiv.org/pdf/2505.17096)
*Sirui Li, Linkai Peng, Zheyuan Zhang, Gorkem Durak, Ulas Bagci*

Main category: eess.IV

TL;DR: TAGS (Tumor Adaptive Guidance for SAM) adapts 2D foundation models (FMs) like CLIP and SAM for 3D medical imaging, improving tumor segmentation by combining multi-prompt fusion and preserving pre-trained weights.


<details>
  <summary>Details</summary>
Motivation: The domain gap between natural images and 3D medical volumes limits the effectiveness of existing FMs in clinical applications like tumor segmentation.

Method: TAGS enhances SAM's spatial feature extraction using CLIP's semantic insights and anatomy-specific prompts through multi-prompt fusion.

Result: TAGS outperforms state-of-the-art models by +46.88% over nnUNet and at least +13% over other medical FMs.

Conclusion: TAGS demonstrates robustness and adaptability for diverse medical segmentation tasks.

Abstract: Foundation models (FMs) such as CLIP and SAM have recently shown great
promise in image segmentation tasks, yet their adaptation to 3D medical
imaging-particularly for pathology detection and segmentation-remains
underexplored. A critical challenge arises from the domain gap between natural
images and medical volumes: existing FMs, pre-trained on 2D data, struggle to
capture 3D anatomical context, limiting their utility in clinical applications
like tumor segmentation. To address this, we propose an adaptation framework
called TAGS: Tumor Adaptive Guidance for SAM, which unlocks 2D FMs for 3D
medical tasks through multi-prompt fusion. By preserving most of the
pre-trained weights, our approach enhances SAM's spatial feature extraction
using CLIP's semantic insights and anatomy-specific prompts. Extensive
experiments on three open-source tumor segmentation datasets prove that our
model surpasses the state-of-the-art medical image segmentation models (+46.88%
over nnUNet), interactive segmentation frameworks, and other established
medical FMs, including SAM-Med2D, SAM-Med3D, SegVol, Universal, 3D-Adapter, and
SAM-B (at least +13% over them). This highlights the robustness and
adaptability of our proposed framework across diverse medical segmentation
tasks.

</details>


### [814] [Assessing the generalization performance of SAM for ureteroscopy scene understanding](https://arxiv.org/pdf/2505.17210)
*Martin Villagrana, Francisco Lopez-Tiro, Clement Larose, Gilberto Ochoa-Ruiz, Christian Daul*

Main category: eess.IV

TL;DR: SAM outperforms U-Net variants in kidney stone segmentation, especially on unseen data.


<details>
  <summary>Details</summary>
Motivation: Manual segmentation is tedious; automation is needed for large-scale image databases.

Method: Evaluated SAM against U-Net, Residual U-Net, and Attention U-Net for kidney stone segmentation.

Result: SAM matched U-Net on in-distribution data and outperformed by up to 23% on out-of-distribution data.

Conclusion: SAM is superior for kidney stone segmentation due to its adaptability and generalization.

Abstract: The segmentation of kidney stones is regarded as a critical preliminary step
to enable the identification of urinary stone types through machine- or
deep-learning-based approaches. In urology, manual segmentation is considered
tedious and impractical due to the typically large scale of image databases and
the continuous generation of new data. In this study, the potential of the
Segment Anything Model (SAM) -- a state-of-the-art deep learning framework --
is investigated for the automation of kidney stone segmentation. The
performance of SAM is evaluated in comparison to traditional models, including
U-Net, Residual U-Net, and Attention U-Net, which, despite their efficiency,
frequently exhibit limitations in generalizing to unseen datasets. The findings
highlight SAM's superior adaptability and efficiency. While SAM achieves
comparable performance to U-Net on in-distribution data (Accuracy: 97.68 +
3.04; Dice: 97.78 + 2.47; IoU: 95.76 + 4.18), it demonstrates significantly
enhanced generalization capabilities on out-of-distribution data, surpassing
all U-Net variants by margins of up to 23 percent.

</details>


### [815] [Low-Rank Adaptation of Pre-trained Vision Backbones for Energy-Efficient Image Coding for Machine](https://arxiv.org/pdf/2505.17366)
*Yichi Zhang, Zhihao Duan, Yuning Huang, Fengqing Zhu*

Main category: eess.IV

TL;DR: Proposes an energy-efficient framework for Image Coding for Machines (ICM) using pre-trained vision backbones and task-specific low-rank adaptation, outperforming traditional codecs.


<details>
  <summary>Details</summary>
Motivation: Addresses inefficiencies in existing ICM frameworks, such as high storage, training overhead, and computational complexity, by leveraging pre-trained models.

Method: Uses pre-trained vision backbones to extract latent representations and introduces a task-specific low-rank adaptation mechanism for compressible, tailored features.

Result: Achieves high coding efficiency and outperforms traditional codecs, minimizing trainable parameters and energy costs.

Conclusion: Offers an energy-efficient, effective solution for ICM applications, adaptable to diverse tasks without full fine-tuning.

Abstract: Image Coding for Machines (ICM) focuses on optimizing image compression for
AI-driven analysis rather than human perception. Existing ICM frameworks often
rely on separate codecs for specific tasks, leading to significant storage
requirements, training overhead, and computational complexity. To address these
challenges, we propose an energy-efficient framework that leverages pre-trained
vision backbones to extract robust and versatile latent representations
suitable for multiple tasks. We introduce a task-specific low-rank adaptation
mechanism, which refines the pre-trained features to be both compressible and
tailored to downstream applications. This design minimizes trainable parameters
and reduces energy costs for multi-task scenarios. By jointly optimizing task
performance and entropy minimization, our method enables efficient adaptation
to diverse tasks and datasets without full fine-tuning, achieving high coding
efficiency. Extensive experiments demonstrate that our framework significantly
outperforms traditional codecs and pre-processors, offering an energy-efficient
and effective solution for ICM applications. The code and the supplementary
materials will be available at:
https://gitlab.com/viper-purdue/efficient-compression.

</details>


### [816] [SUFFICIENT: A scan-specific unsupervised deep learning framework for high-resolution 3D isotropic fetal brain MRI reconstruction](https://arxiv.org/pdf/2505.17472)
*Jiangjie Wu, Lixuan Chen, Zhenghao Li, Xin Li, Saban Ozturk, Lihui Wang, Rongpin Wang, Hongjiang Wei, Yuyao Zhang*

Main category: eess.IV

TL;DR: An unsupervised iterative framework for high-quality 3D fetal brain MRI reconstruction from motion-corrupted 2D slices, outperforming existing methods.


<details>
  <summary>Details</summary>
Motivation: Clinical diagnosis requires reliable 3D fetal brain MRI reconstruction, but obtaining large-scale training datasets for deep learning is challenging.

Method: Proposes an unsupervised iterative SVR-SRR framework using CNNs for slice-to-volume registration and deep image prior for super-resolution reconstruction.

Result: Demonstrates superior performance on motion-corrupted simulation and clinical data compared to state-of-the-art methods.

Conclusion: The framework effectively addresses the lack of large datasets and improves 3D fetal brain MRI reconstruction.

Abstract: High-quality 3D fetal brain MRI reconstruction from motion-corrupted 2D
slices is crucial for clinical diagnosis. Reliable slice-to-volume registration
(SVR)-based motion correction and super-resolution reconstruction (SRR) methods
are essential. Deep learning (DL) has demonstrated potential in enhancing SVR
and SRR when compared to conventional methods. However, it requires large-scale
external training datasets, which are difficult to obtain for clinical fetal
MRI. To address this issue, we propose an unsupervised iterative SVR-SRR
framework for isotropic HR volume reconstruction. Specifically, SVR is
formulated as a function mapping a 2D slice and a 3D target volume to a rigid
transformation matrix, which aligns the slice to the underlying location in the
target volume. The function is parameterized by a convolutional neural network,
which is trained by minimizing the difference between the volume slicing at the
predicted position and the input slice. In SRR, a decoding network embedded
within a deep image prior framework is incorporated with a comprehensive image
degradation model to produce the high-resolution (HR) volume. The deep image
prior framework offers a local consistency prior to guide the reconstruction of
HR volumes. By performing a forward degradation model, the HR volume is
optimized by minimizing loss between predicted slices and the observed slices.
Comprehensive experiments conducted on large-magnitude motion-corrupted
simulation data and clinical data demonstrate the superior performance of the
proposed framework over state-of-the-art fetal brain reconstruction frameworks.

</details>


### [817] [Anatomy-Guided Multitask Learning for MRI-Based Classification of Placenta Accreta Spectrum and its Subtypes](https://arxiv.org/pdf/2505.17484)
*Hai Jiang, Qiongting Liu, Yuanpin Zhou, Jiawei Pan, Ting Song, Yao Lu*

Main category: eess.IV

TL;DR: A novel CNN architecture for one-stage multiclass diagnosis of Placenta Accreta Spectrum Disorders (PAS) and its subtypes, using MRI slices, achieves state-of-the-art performance.


<details>
  <summary>Details</summary>
Motivation: Accurate prenatal diagnosis of PAS and its subtypes is crucial due to severe clinical risks, but existing methods lack efficiency and focus on subtype recognition.

Method: Proposes a CNN with two branches: a main classification branch using residual blocks and a second branch integrating anatomical features, employing multitask learning.

Result: The model outperforms existing methods on a real clinical dataset.

Conclusion: The proposed CNN architecture effectively addresses the need for efficient and accurate multiclass PAS diagnosis.

Abstract: Placenta Accreta Spectrum Disorders (PAS) pose significant risks during
pregnancy, frequently leading to postpartum hemorrhage during cesarean
deliveries and other severe clinical complications, with bleeding severity
correlating to the degree of placental invasion. Consequently, accurate
prenatal diagnosis of PAS and its subtypes-placenta accreta (PA), placenta
increta (PI), and placenta percreta (PP)-is crucial. However, existing
guidelines and methodologies predominantly focus on the presence of PAS, with
limited research addressing subtype recognition. Additionally, previous
multi-class diagnostic efforts have primarily relied on inefficient two-stage
cascaded binary classification tasks. In this study, we propose a novel
convolutional neural network (CNN) architecture designed for efficient
one-stage multiclass diagnosis of PAS and its subtypes, based on 4,140 magnetic
resonance imaging (MRI) slices. Our model features two branches: the main
classification branch utilizes a residual block architecture comprising
multiple residual blocks, while the second branch integrates anatomical
features of the uteroplacental area and the adjacent uterine serous layer to
enhance the model's attention during classification. Furthermore, we implement
a multitask learning strategy to leverage both branches effectively.
Experiments conducted on a real clinical dataset demonstrate that our model
achieves state-of-the-art performance.

</details>


### [818] [DECT-based Space-Squeeze Method for Multi-Class Classification of Metastatic Lymph Nodes in Breast Cancer](https://arxiv.org/pdf/2505.17528)
*Hai Jiang, Chushan Zheng, Jiawei Pan, Yuanpin Zhou, Qiongting Liu, Xiang Zhang, Jun Shen, Yao Lu*

Main category: eess.IV

TL;DR: A DECT-based model improves classification of metastatic burden in axillary lymph nodes using spectral-spatial data and novel attention mechanisms.


<details>
  <summary>Details</summary>
Motivation: Conventional imaging struggles to assess metastatic burden accurately, impacting breast cancer treatment decisions.

Method: Proposes a space-squeeze method with channel-wise attention and virtual class injection to enhance classification.

Result: Achieved an average test AUC of 0.86, outperforming CNNs like VGG and ResNet.

Conclusion: The framework offers a promising noninvasive tool for clinical metastatic burden assessment.

Abstract: Background: Accurate assessment of metastatic burden in axillary lymph nodes
is crucial for guiding breast cancer treatment decisions, yet conventional
imaging modalities struggle to differentiate metastatic burden levels and
capture comprehensive lymph node characteristics. This study leverages
dual-energy computed tomography (DECT) to exploit spectral-spatial information
for improved multi-class classification. Purpose: To develop a noninvasive
DECT-based model classifying sentinel lymph nodes into three categories: no
metastasis ($N_0$), low metastatic burden ($N_{+(1-2)}$), and heavy metastatic
burden ($N_{+(\geq3)}$), thereby aiding therapeutic planning. Methods: We
propose a novel space-squeeze method combining two innovations: (1) a
channel-wise attention mechanism to compress and recalibrate spectral-spatial
features across 11 energy levels, and (2) virtual class injection to sharpen
inter-class boundaries and compact intra-class variations in the representation
space. Results: Evaluated on 227 biopsy-confirmed cases, our method achieved an
average test AUC of 0.86 (95% CI: 0.80-0.91) across three cross-validation
folds, outperforming established CNNs (VGG, ResNet, etc). The channel-wise
attention and virtual class components individually improved AUC by 5.01% and
5.87%, respectively, demonstrating complementary benefits. Conclusions: The
proposed framework enhances diagnostic AUC by effectively integrating DECT's
spectral-spatial data and mitigating class ambiguity, offering a promising tool
for noninvasive metastatic burden assessment in clinical practice.

</details>


### [819] [FreqU-FNet: Frequency-Aware U-Net for Imbalanced Medical Image Segmentation](https://arxiv.org/pdf/2505.17544)
*Ruiqi Xing*

Main category: eess.IV

TL;DR: FreqU-FNet, a frequency-domain U-shaped segmentation model, outperforms CNNs and Transformers in medical image segmentation by addressing class imbalance and capturing fine details.


<details>
  <summary>Details</summary>
Motivation: Challenges in medical image segmentation include class imbalance and frequency-specific anatomical distributions, which conventional CNNs and Transformers struggle to address.

Method: FreqU-FNet uses a Frequency Encoder with Low-Pass Frequency Convolution and wavelet-based downsampling, a Spatial Learnable Decoder, and a frequency-aware loss function.

Result: FreqU-FNet outperforms CNN and Transformer baselines, especially for under-represented classes, by leveraging discriminative frequency bands.

Conclusion: The proposed FreqU-FNet effectively addresses class imbalance and fine-grained segmentation by operating in the frequency domain.

Abstract: Medical image segmentation faces persistent challenges due to severe class
imbalance and the frequency-specific distribution of anatomical structures.
Most conventional CNN-based methods operate in the spatial domain and struggle
to capture minority class signals, often affected by frequency aliasing and
limited spectral selectivity. Transformer-based models, while powerful in
modeling global dependencies, tend to overlook critical local details necessary
for fine-grained segmentation. To overcome these limitations, we propose
FreqU-FNet, a novel U-shaped segmentation architecture operating in the
frequency domain. Our framework incorporates a Frequency Encoder that leverages
Low-Pass Frequency Convolution and Daubechies wavelet-based downsampling to
extract multi-scale spectral features. To reconstruct fine spatial details, we
introduce a Spatial Learnable Decoder (SLD) equipped with an adaptive
multi-branch upsampling strategy. Furthermore, we design a frequency-aware loss
(FAL) function to enhance minority class learning. Extensive experiments on
multiple medical segmentation benchmarks demonstrate that FreqU-FNet
consistently outperforms both CNN and Transformer baselines, particularly in
handling under-represented classes, by effectively exploiting discriminative
frequency bands.

</details>


### [820] [Distance Estimation in Outdoor Driving Environments Using Phase-only Correlation Method with Event Cameras](https://arxiv.org/pdf/2505.17582)
*Masataka Kobayashi, Shintaro Shiba, Quan Kong, Norimasa Kobori, Tsukasa Shimizu, Shan Lu, Takaya Yamazato*

Main category: eess.IV

TL;DR: A method for distance estimation using a monocular event camera and a roadside LED bar achieves high accuracy in autonomous driving scenarios.


<details>
  <summary>Details</summary>
Motivation: To address the hardware complexity and cost of multi-sensor fusion in autonomous driving by leveraging event cameras' unique capabilities.

Method: Phase-only correlation technique applied to event data for sub-pixel precision in detecting spatial shifts between light sources, enabling triangulation-based distance estimation.

Result: Over 90% success rate with less than 0.5-meter error for distances of 20-60 meters in outdoor experiments.

Conclusion: The approach shows promise for real-time position estimation in autonomous vehicles, enhancing navigation and integration into intelligent transportation systems.

Abstract: With the growing adoption of autonomous driving, the advancement of sensor
technology is crucial for ensuring safety and reliable operation. Sensor fusion
techniques that combine multiple sensors such as LiDAR, radar, and cameras have
proven effective, but the integration of multiple devices increases both
hardware complexity and cost. Therefore, developing a single sensor capable of
performing multiple roles is highly desirable for cost-efficient and scalable
autonomous driving systems.
  Event cameras have emerged as a promising solution due to their unique
characteristics, including high dynamic range, low latency, and high temporal
resolution. These features enable them to perform well in challenging lighting
conditions, such as low-light or backlit environments. Moreover, their ability
to detect fine-grained motion events makes them suitable for applications like
pedestrian detection and vehicle-to-infrastructure communication via visible
light.
  In this study, we present a method for distance estimation using a monocular
event camera and a roadside LED bar. By applying a phase-only correlation
technique to the event data, we achieve sub-pixel precision in detecting the
spatial shift between two light sources. This enables accurate
triangulation-based distance estimation without requiring stereo vision. Field
experiments conducted in outdoor driving scenarios demonstrated that the
proposed approach achieves over 90% success rate with less than 0.5-meter error
for distances ranging from 20 to 60 meters.
  Future work includes extending this method to full position estimation by
leveraging infrastructure such as smart poles equipped with LEDs, enabling
event-camera-based vehicles to determine their own position in real time. This
advancement could significantly enhance navigation accuracy, route
optimization, and integration into intelligent transportation systems.

</details>


### [821] [A Unified Multi-Scale Attention-Based Network for Automatic 3D Segmentation of Lung Parenchyma & Nodules In Thoracic CT Images](https://arxiv.org/pdf/2505.17602)
*Muhammad Abdullah, Furqan Shaukat*

Main category: eess.IV

TL;DR: A novel attention-based 3D segmentation method for lung parenchyma and nodules outperforms state-of-the-art techniques, validated on the LUNA16 dataset.


<details>
  <summary>Details</summary>
Motivation: Improving lung cancer survival rates through early detection via accurate segmentation in CAD systems, addressing limitations of traditional and recent methods.

Method: Attention-based network with residual blocks, strided and transposed convolutions, and dilated convolutions for context capture without added computational cost.

Result: Outperforms state-of-the-art methods on LUNA16 dataset, achieving higher Dice score and IOU metrics.

Conclusion: The proposed method enhances segmentation accuracy and robustness, offering potential for real-time clinical applications.

Abstract: Lung cancer has been one of the major threats across the world with the
highest mortalities. Computer-aided detection (CAD) can help in early detection
and thus can help increase the survival rate. Accurate lung parenchyma
segmentation (to include the juxta-pleural nodules) and lung nodule
segmentation, the primary symptom of lung cancer, play a crucial role in the
overall accuracy of the Lung CAD pipeline. Lung nodule segmentation is quite
challenging because of the diverse nodule types and other inhibit structures
present within the lung lobes. Traditional machine/deep learning methods suffer
from generalization and robustness. Recent Vision Language Models/Foundation
Models perform well on the anatomical level, but they suffer on fine-grained
segmentation tasks, and their semi-automatic nature limits their effectiveness
in real-time clinical scenarios. In this paper, we propose a novel method for
accurate 3D segmentation of lung parenchyma and lung nodules. The proposed
architecture is an attention-based network with residual blocks at each
encoder-decoder state. Max pooling is replaced by strided convolutions at the
encoder, and trilinear interpolation is replaced by transposed convolutions at
the decoder to maximize the number of learnable parameters. Dilated
convolutions at each encoder-decoder stage allow the model to capture the
larger context without increasing computational costs. The proposed method has
been evaluated extensively on one of the largest publicly available datasets,
namely LUNA16, and is compared with recent notable work in the domain using
standard performance metrics like Dice score, IOU, etc. It can be seen from the
results that the proposed method achieves better performance than
state-of-the-art methods. The source code, datasets, and pre-processed data can
be accessed using the link:
https://github.com/EMeRALDsNRPU/Attention-Based-3D-ResUNet.

</details>


### [822] [Towards Prospective Medical Image Reconstruction via Knowledge-Informed Dynamic Optimal Transport](https://arxiv.org/pdf/2505.17644)
*Taoran Zheng, Xing Li, Yan Yang, Xiang Gu, Zongben Xu, Jian Sun*

Main category: eess.IV

TL;DR: The paper introduces KIDOT, a dynamic optimal transport framework for medical image reconstruction, addressing the gap between simulated and real data by leveraging unpaired data and imaging physics.


<details>
  <summary>Details</summary>
Motivation: The challenge of performance degradation in deep learning-based medical image reconstruction when transitioning from simulated to real data due to incomplete imaging knowledge.

Method: Proposes KIDOT, a dynamic optimal transport framework that models reconstruction as a continuous evolution path from measurements to images, guided by imaging physics.

Result: KIDOT shows superior performance in MRI and CT reconstruction, validated through extensive experiments.

Conclusion: KIDOT effectively bridges the retrospective-to-prospective gap, enhancing robustness and leveraging unpaired data while respecting acquisition physics.

Abstract: Medical image reconstruction from measurement data is a vital but challenging
inverse problem. Deep learning approaches have achieved promising results, but
often requires paired measurement and high-quality images, which is typically
simulated through a forward model, i.e., retrospective reconstruction. However,
training on simulated pairs commonly leads to performance degradation on real
prospective data due to the retrospective-to-prospective gap caused by
incomplete imaging knowledge in simulation. To address this challenge, this
paper introduces imaging Knowledge-Informed Dynamic Optimal Transport (KIDOT),
a novel dynamic optimal transport framework with optimality in the sense of
preserving consistency with imaging physics in transport, that conceptualizes
reconstruction as finding a dynamic transport path. KIDOT learns from unpaired
data by modeling reconstruction as a continuous evolution path from
measurements to images, guided by an imaging knowledge-informed cost function
and transport equation. This dynamic and knowledge-aware approach enhances
robustness and better leverages unpaired data while respecting acquisition
physics. Theoretically, we demonstrate that KIDOT naturally generalizes dynamic
optimal transport, ensuring its mathematical rationale and solution existence.
Extensive experiments on MRI and CT reconstruction demonstrate KIDOT's superior
performance.

</details>


### [823] [Dual Attention Residual U-Net for Accurate Brain Ultrasound Segmentation in IVH Detection](https://arxiv.org/pdf/2505.17683)
*Dan Yuan, Yi Feng, Ziyun Tang*

Main category: eess.IV

TL;DR: Proposes an enhanced Residual U-Net with CBAM and SAL for IVH segmentation in premature infants, achieving state-of-the-art results.


<details>
  <summary>Details</summary>
Motivation: Early and accurate IVH detection from brain US images is critical for improving clinical outcomes in premature infants.

Method: Enhanced Residual U-Net with CBAM for spatial/channel refinement and SAL for sparse/dense attention to balance noise suppression and information propagation.

Result: Achieves 89.04% Dice score and 81.84% IoU for ventricle segmentation, outperforming existing methods.

Conclusion: Integrating spatial refinement and attention sparsity enhances robust brain anatomy detection in US images.

Abstract: Intraventricular hemorrhage (IVH) is a severe neurological complication among
premature infants, necessitating early and accurate detection from brain
ultrasound (US) images to improve clinical outcomes. While recent deep learning
methods offer promise for computer-aided diagnosis, challenges remain in
capturing both local spatial details and global contextual dependencies
critical for segmenting brain anatomies. In this work, we propose an enhanced
Residual U-Net architecture incorporating two complementary attention
mechanisms: the Convolutional Block Attention Module (CBAM) and a Sparse
Attention Layer (SAL). The CBAM improves the model's ability to refine spatial
and channel-wise features, while the SAL introduces a dual-branch design,
sparse attention filters out low-confidence query-key pairs to suppress noise,
and dense attention ensures comprehensive information propagation. Extensive
experiments on the Brain US dataset demonstrate that our method achieves
state-of-the-art segmentation performance, with a Dice score of 89.04% and IoU
of 81.84% for ventricle region segmentation. These results highlight the
effectiveness of integrating spatial refinement and attention sparsity for
robust brain anatomy detection. Code is available at:
https://github.com/DanYuan001/BrainImgSegment.

</details>


### [824] [UltraBoneUDF: Self-supervised Bone Surface Reconstruction from Ultrasound Based on Neural Unsigned Distance Functions](https://arxiv.org/pdf/2505.17912)
*Luohong Wu, Matthias Seibold, Nicola A. Cavalcanti, Giuseppe Loggia, Lisa Reissner, Bastian Sigrist, Jonas Hein, Lilian Calvet, Arnd Viehöfer, Philipp Fürnstahl*

Main category: eess.IV

TL;DR: UltraBoneUDF, a self-supervised framework using neural Unsigned Distance Functions, improves open bone surface reconstruction from ultrasound, outperforming state-of-the-art methods.


<details>
  <summary>Details</summary>
Motivation: Traditional imaging (CT/MRI) has limitations; ultrasound is radiation-free and cost-effective but captures partial bone surfaces. Existing methods struggle with incomplete data.

Method: Proposes UltraBoneUDF with a global feature extractor and novel loss function for local tangent plane optimization. Evaluated on four datasets.

Result: UltraBoneUDF outperforms SOTA methods, reducing mean Chamfer distance error by 39.6-70.2% across datasets.

Conclusion: UltraBoneUDF effectively addresses open bone surface reconstruction challenges, offering superior accuracy and robustness.

Abstract: Background: Bone surface reconstruction plays a critical role in
computer-assisted orthopedic surgery. Compared to traditional imaging
modalities such as CT and MRI, ultrasound offers a radiation-free,
cost-effective, and portable alternative. Continuous bone surface
reconstruction can be employed for many clinical applications. However, due to
the inherent limitations of ultrasound imaging, B-mode ultrasound typically
capture only partial bone surfaces. Existing reconstruction methods struggle
with such incomplete data, leading to artifacts and increased reconstruction
errors. Effective techniques for accurately reconstructing thin and open bone
surfaces from real-world 3D ultrasound volumes remain lacking. Methods: We
propose UltraBoneUDF, a self-supervised framework designed for reconstructing
open bone surfaces from ultrasound using neural Unsigned Distance Functions. To
enhance reconstruction quality, we introduce a novel global feature extractor
that effectively fuses ultrasound-specific image characteristics. Additionally,
we present a novel loss function based on local tangent plane optimization that
substantially improves surface reconstruction quality. UltraBoneUDF and
baseline models are extensively evaluated on four open-source datasets.
Results: Qualitative results highlight the limitations of the state-of-the-art
methods for open bone surface reconstruction and demonstrate the effectiveness
of UltraBoneUDF. Quantitatively, UltraBoneUDF significantly outperforms
competing methods across all evaluated datasets for both open and closed bone
surface reconstruction in terms of mean Chamfer distance error: 1.10 mm on the
UltraBones100k dataset (39.6\% improvement compared to the SOTA), 0.23 mm on
the OpenBoneCT dataset (69.3\% improvement), 0.18 mm on the ClosedBoneCT
dataset (70.2\% improvement), and 0.05 mm on the Prostate dataset (55.3\%
improvement).

</details>


### [825] [Promptable cancer segmentation using minimal expert-curated data](https://arxiv.org/pdf/2505.17915)
*Lynn Karam, Yipei Wang, Veeru Kasivisvanathan, Mirabela Rusu, Yipeng Hu, Shaheer U. Saeed*

Main category: eess.IV

TL;DR: A novel promptable segmentation method for cancer detection requires minimal annotated data (24 fully-segmented and 8 weakly-labeled images), outperforming existing methods and matching fully-supervised approaches.


<details>
  <summary>Details</summary>
Motivation: High cost and variability of expert annotations for training cancer segmentation models limit adoption, and existing weakly-supervised or promptable methods require large datasets.

Method: Uses two classifiers (weakly-supervised and fully-supervised) for refined segmentation via guided search from a single-point prompt, trained on minimal high-quality data.

Result: Outperforms promptable methods and matches fully-supervised ones for prostate cancer segmentation, using up to 100X less annotated data.

Conclusion: Enables high-quality promptable segmentation with minimal, carefully curated data, addressing cost and variability challenges.

Abstract: Automated segmentation of cancer on medical images can aid targeted
diagnostic and therapeutic procedures. However, its adoption is limited by the
high cost of expert annotations required for training and inter-observer
variability in datasets. While weakly-supervised methods mitigate some
challenges, using binary histology labels for training as opposed to requiring
full segmentation, they require large paired datasets of histology and images,
which are difficult to curate. Similarly, promptable segmentation aims to allow
segmentation with no re-training for new tasks at inference, however, existing
models perform poorly on pathological regions, again necessitating large
datasets for training. In this work we propose a novel approach for promptable
segmentation requiring only 24 fully-segmented images, supplemented by 8
weakly-labelled images, for training. Curating this minimal data to a high
standard is relatively feasible and thus issues with the cost and variability
of obtaining labels can be mitigated. By leveraging two classifiers, one
weakly-supervised and one fully-supervised, our method refines segmentation
through a guided search process initiated by a single-point prompt. Our
approach outperforms existing promptable segmentation methods, and performs
comparably with fully-supervised methods, for the task of prostate cancer
segmentation, while using substantially less annotated data (up to 100X less).
This enables promptable segmentation with very minimal labelled data, such that
the labels can be curated to a very high standard.

</details>


### [826] [Explainable Anatomy-Guided AI for Prostate MRI: Foundation Models and In Silico Clinical Trials for Virtual Biopsy-based Risk Assessment](https://arxiv.org/pdf/2505.17971)
*Danial Khan, Zohaib Salahuddin, Yumeng Zhang, Sheng Kuang, Shruti Atul Mali, Henry C. Woodruff, Sina Amirrajab, Rachel Cavill, Eduardo Ibor-Crespo, Ana Jimenez-Pastor, Adrian Galiana-Bordera, Paula Jimenez Gomez, Luis Marti-Bonmati, Philippe Lambin*

Main category: eess.IV

TL;DR: A fully automated deep learning pipeline for prostate cancer risk stratification using MRI, combining segmentation, classification, and counterfactual heatmaps, achieving high accuracy and interpretability.


<details>
  <summary>Details</summary>
Motivation: To improve prostate cancer risk assessment by integrating anatomical guidance, deep learning, and explainability into a single pipeline for clinical use.

Method: The pipeline includes nnU-Net for segmentation, a Swin Transformer-based classifier, and a VAE-GAN for counterfactual heatmaps, trained on 1,500 PI-CAI cases and 617 biparametric MRIs.

Result: Achieved high segmentation accuracy (Dice scores: 0.92-0.95) and improved classification (AUC: 0.79). AI assistance increased diagnostic accuracy (0.72 to 0.77) and reduced review time by 40%.

Conclusion: The pipeline enables accurate, interpretable, and efficient prostate cancer risk assessment, supporting its potential as a virtual biopsy in clinical practice.

Abstract: We present a fully automated, anatomically guided deep learning pipeline for
prostate cancer (PCa) risk stratification using routine MRI. The pipeline
integrates three key components: an nnU-Net module for segmenting the prostate
gland and its zones on axial T2-weighted MRI; a classification module based on
the UMedPT Swin Transformer foundation model, fine-tuned on 3D patches with
optional anatomical priors and clinical data; and a VAE-GAN framework for
generating counterfactual heatmaps that localize decision-driving image
regions. The system was developed using 1,500 PI-CAI cases for segmentation and
617 biparametric MRIs with metadata from the CHAIMELEON challenge for
classification (split into 70% training, 10% validation, and 20% testing).
Segmentation achieved mean Dice scores of 0.95 (gland), 0.94 (peripheral zone),
and 0.92 (transition zone). Incorporating gland priors improved AUC from 0.69
to 0.72, with a three-scale ensemble achieving top performance (AUC = 0.79,
composite score = 0.76), outperforming the 2024 CHAIMELEON challenge winners.
Counterfactual heatmaps reliably highlighted lesions within segmented regions,
enhancing model interpretability. In a prospective multi-center in-silico trial
with 20 clinicians, AI assistance increased diagnostic accuracy from 0.72 to
0.77 and Cohen's kappa from 0.43 to 0.53, while reducing review time per case
by 40%. These results demonstrate that anatomy-aware foundation models with
counterfactual explainability can enable accurate, interpretable, and efficient
PCa risk assessment, supporting their potential use as virtual biopsies in
clinical practice.

</details>


### [827] [A Foundation Model Framework for Multi-View MRI Classification of Extramural Vascular Invasion and Mesorectal Fascia Invasion in Rectal Cancer](https://arxiv.org/pdf/2505.18058)
*Yumeng Zhang, Zohaib Salahuddin, Danial Khan, Shruti Atul Mali, Henry C. Woodruff, Sina Amirrajab, Eduardo Ibor-Crespo, Ana Jimenez-Pastor, Luis Marti-Bonmati, Philippe Lambin*

Main category: eess.IV

TL;DR: A framework using a foundation-model-driven approach improves MRI-based classification of EVI and MFI in rectal cancer, outperforming traditional methods.


<details>
  <summary>Details</summary>
Motivation: To address the subjectivity and variability in visual assessment of EVI and MFI in rectal cancer MRI, this study aims to develop an automated, multicenter framework.

Method: The study used 331 MRI exams from three hospitals, employing a self-supervised harmonization pipeline and comparing four classifiers (ResNet50, SeResNet, UMedPT, and UMedPT_LR).

Result: UMedPT_LR performed best for EVI detection (AUC=0.82), while UMedPT excelled for MFI (AUC=0.77), both surpassing previous benchmarks. Harmonization improved MFI classification.

Conclusion: The combination of foundation models, harmonization, and multi-view fusion significantly enhances diagnostic accuracy in rectal MRI.

Abstract: Background: Accurate MRI-based identification of extramural vascular invasion
(EVI) and mesorectal fascia invasion (MFI) is pivotal for risk-stratified
management of rectal cancer, yet visual assessment is subjective and vulnerable
to inter-institutional variability. Purpose: To develop and externally evaluate
a multicenter, foundation-model-driven framework that automatically classifies
EVI and MFI on axial and sagittal T2-weighted MRI. Methods: This retrospective
study used 331 pre-treatment rectal cancer MRI examinations from three European
hospitals. After TotalSegmentator-guided rectal patch extraction, a
self-supervised frequency-domain harmonization pipeline was trained to minimize
scanner-related contrast shifts. Four classifiers were compared: ResNet50,
SeResNet, the universal biomedical pretrained transformer (UMedPT) with a
lightweight MLP head, and a logistic-regression variant using frozen UMedPT
features (UMedPT_LR). Results: UMedPT_LR achieved the best EVI detection when
axial and sagittal features were fused (AUC = 0.82; sensitivity = 0.75; F1
score = 0.73), surpassing the Chaimeleon Grand-Challenge winner (AUC = 0.74).
The highest MFI performance was attained by UMedPT on axial harmonized images
(AUC = 0.77), surpassing the Chaimeleon Grand-Challenge winner (AUC = 0.75).
Frequency-domain harmonization improved MFI classification but variably
affected EVI performance. Conventional CNNs (ResNet50, SeResNet)
underperformed, especially in F1 score and balanced accuracy. Conclusion: These
findings demonstrate that combining foundation model features, harmonization,
and multi-view fusion significantly enhances diagnostic performance in rectal
MRI.

</details>


### [828] [Accelerating Learned Image Compression Through Modeling Neural Training Dynamics](https://arxiv.org/pdf/2505.18107)
*Yichi Zhang, Zhihao Duan, Yuning Huang, Fengqing Zhu*

Main category: eess.IV

TL;DR: The paper proposes STDET and SMA techniques to accelerate LIC training by reducing trainable parameters and ensuring smooth training dynamics, without compromising performance.


<details>
  <summary>Details</summary>
Motivation: To enhance the training efficiency of computationally demanding LIC methods by modeling neural training dynamics.

Method: Introduces STDET for parameter clustering and embedding, and SMA for smooth weight interpolation.

Result: Significantly reduces training dimensions and parameters, accelerates convergence, and maintains model performance.

Conclusion: The method provides insights for efficient LIC training, supported by theoretical analysis.

Abstract: As learned image compression (LIC) methods become increasingly
computationally demanding, enhancing their training efficiency is crucial. This
paper takes a step forward in accelerating the training of LIC methods by
modeling the neural training dynamics. We first propose a Sensitivity-aware
True and Dummy Embedding Training mechanism (STDET) that clusters LIC model
parameters into few separate modes where parameters are expressed as affine
transformations of reference parameters within the same mode. By further
utilizing the stable intra-mode correlations throughout training and parameter
sensitivities, we gradually embed non-reference parameters, reducing the number
of trainable parameters. Additionally, we incorporate a Sampling-then-Moving
Average (SMA) technique, interpolating sampled weights from stochastic gradient
descent (SGD) training to obtain the moving average weights, ensuring smooth
temporal behavior and minimizing training state variances. Overall, our method
significantly reduces training space dimensions and the number of trainable
parameters without sacrificing model performance, thus accelerating model
convergence. We also provide a theoretical analysis on the Noisy quadratic
model, showing that the proposed method achieves a lower training variance than
standard SGD. Our approach offers valuable insights for further developing
efficient training methods for LICs.

</details>


### [829] [Groupwise Image Registration with Edge-Based Loss for Low-SNR Cardiac MRI](https://arxiv.org/pdf/2409.02348)
*Xuan Lei, Philip Schniter, Chong Chen, Rizwan Ahmad*

Main category: eess.IV

TL;DR: AiM-ED, a DL-based image registration method, improves SNR and image quality in free-breathing single-shot cardiac imaging by jointly registering noisy images and using edge detection.


<details>
  <summary>Details</summary>
Motivation: To address low SNR in single-shot cardiac images, especially at low field strengths, by developing a robust registration method.

Method: Proposes AiM-ED, which uses deep learning and edge detection to register noisy images, validated on synthetic and real LGE images.

Result: AiM-ED outperforms traditional and DL-based methods in SNR and image quality, with benefits from joint processing and edge maps.

Conclusion: AiM-ED is superior for single-shot LGE imaging, offering fast, robust performance with minimal training data, promising for clinical use.

Abstract: Purpose: To perform image registration and averaging of multiple
free-breathing single-shot cardiac images, where the individual images may have
a low signal-to-noise ratio (SNR).
  Methods: To address low SNR encountered in single-shot imaging, especially at
low field strengths, we propose a fast deep learning (DL)-based image
registration method, called Averaging Morph with Edge Detection (AiM-ED).
AiM-ED jointly registers multiple noisy source images to a noisy target image
and utilizes a noise-robust pre-trained edge detector to define the training
loss. We validate AiM-ED using synthetic late gadolinium enhanced (LGE) images
from the MR extended cardiac-torso (MRXCAT) phantom and free-breathing
single-shot LGE images from healthy subjects (24 slices) and patients (5
slices) under various levels of added noise. Additionally, we demonstrate the
clinical feasibility of AiM-ED by applying it to data from patients (6 slices)
scanned on a 0.55T scanner.
  Results: Compared to a traditional energy-minimization-based image
registration method and DL-based VoxelMorph, images registered using AiM-ED
exhibit higher values of recovery SNR and three perceptual image quality
metrics. An ablation study shows the benefit of both jointly processing
multiple source images and using an edge map in AiM-ED.
  Conclusion: For single-shot LGE imaging, AiM-ED outperforms existing image
registration methods in terms of image quality. With fast inference, minimal
training data requirements, and robust performance at various noise levels,
AiM-ED has the potential to benefit single-shot CMR applications.

</details>


### [830] [Rapid Whole Brain Motion-robust Mesoscale In-vivo MR Imaging using Multi-scale Implicit Neural Representation](https://arxiv.org/pdf/2502.08634)
*Jun Lyu, Lipeng Ning, William Consagra, Qiang Liu, Richard J. Rushmore, Berkin Bilgic, Yogesh Rathi*

Main category: eess.IV

TL;DR: ROVER-MRI, an unsupervised framework using multi-scale implicit neural representations, improves high-resolution whole-brain MR imaging by reducing scan time and enhancing detail recovery.


<details>
  <summary>Details</summary>
Motivation: Challenges in high-resolution whole-brain MR imaging include long scan times, motion artifacts, and low SNR. ROVER-MRI aims to address these issues.

Method: ROVER-MRI uses coordinate-based neural networks to encode image structures at multiple scales, integrating motion correction and anatomical continuity.

Result: ROVER-MRI achieves 180-micron isotropic resolution in 17 minutes, outperforming LS-SRR with 22.4% lower error and better SNR.

Conclusion: ROVER-MRI offers a rapid, accurate, and motion-resilient solution for mesoscale neuroimaging.

Abstract: High-resolution whole-brain in vivo MR imaging at mesoscale resolutions
remains challenging due to long scan durations, motion artifacts, and limited
signal-to-noise ratio (SNR). This study proposes Rotating-view super-resolution
(ROVER)-MRI, an unsupervised framework based on multi-scale implicit neural
representations (INR), enabling efficient recovery of fine anatomical details
from multi-view thick-slice acquisitions. ROVER-MRI employs coordinate-based
neural networks to implicitly and continuously encode image structures at
multiple spatial scales, simultaneously modeling anatomical continuity and
correcting inter-view motion through an integrated registration mechanism.
Validation on ex-vivo monkey brain data and multiple in-vivo human datasets
demonstrates substantially improved reconstruction performance compared to
bicubic interpolation and state-of-the-art regularized least-squares
super-resolution reconstruction (LS-SRR) with 2-fold reduction in scan time.
Notably, ROVER-MRI achieves an unprecedented whole-brain in-vivo T2-weighted
imaging at 180 micron isotropic resolution in only 17 minutes of scan time on a
7T scanner with 22.4% lower relative error compared to LS-SRR. We also
demonstrate improved SNR using ROVER-MRI compared to a time-matched 3D GRE
acquisition. Quantitative results on several datasets demonstrate better
sharpness of the reconstructed images with ROVER-MRI for different
super-resolution factors (5 to 11). These findings highlight ROVER-MRI's
potential as a rapid, accurate, and motion-resilient mesoscale imaging
solution, promising substantial advantages for neuroimaging studies.

</details>


### [831] [Fourier-Based 3D Multistage Transformer for Aberration Correction in Multicellular Specimens](https://arxiv.org/pdf/2503.12593)
*Thayer Alshaabi, Daniel E. Milkie, Gaoxiang Liu, Cyna Shirazinejad, Jason L. Hong, Kemal Achour, Frederik Görlitz, Ana Milunovic-Jevtic, Cat Simmons, Ibrahim S. Abuzahriyeh, Erin Hong, Samara Erin Williams, Nathanael Harrison, Evan Huang, Eun Seok Bae, Alison N. Killilea, David G. Drubin, Ian A. Swinburne, Srigokul Upadhyayula, Eric Betzig*

Main category: eess.IV

TL;DR: AOViFT, a machine learning-based aberration sensing framework using a 3D Vision Transformer, improves high-resolution tissue imaging by correcting optical aberrations efficiently.


<details>
  <summary>Details</summary>
Motivation: Optical aberrations degrade imaging quality, and existing hardware solutions (like wavefront sensor-based AO) are complex, expensive, and slow. AOViFT aims to simplify and enhance aberration correction.

Method: AOViFT employs a 3D multistage Vision Transformer on Fourier domain embeddings to infer aberrations, reducing computational cost and training time compared to conventional methods.

Result: Validated on live zebrafish embryos, AOViFT successfully corrected spatially varying aberrations using deformable mirrors or post-acquisition deconvolution.

Conclusion: AOViFT simplifies high-resolution microscopy by eliminating hardware dependencies, lowering barriers for diverse biological imaging.

Abstract: High-resolution tissue imaging is often compromised by sample-induced optical
aberrations that degrade resolution and contrast. While wavefront sensor-based
adaptive optics (AO) can measure these aberrations, such hardware solutions are
typically complex, expensive to implement, and slow when serially mapping
spatially varying aberrations across large fields of view. Here, we introduce
AOViFT (Adaptive Optical Vision Fourier Transformer) -- a machine
learning-based aberration sensing framework built around a 3D multistage Vision
Transformer that operates on Fourier domain embeddings. AOViFT infers
aberrations and restores diffraction-limited performance in puncta-labeled
specimens with substantially reduced computational cost, training time, and
memory footprint compared to conventional architectures or real-space networks.
We validated AOViFT on live gene-edited zebrafish embryos, demonstrating its
ability to correct spatially varying aberrations using either a deformable
mirror or post-acquisition deconvolution. By eliminating the need for the guide
star and wavefront sensing hardware and simplifying the experimental workflow,
AOViFT lowers technical barriers for high-resolution volumetric microscopy
across diverse biological samples.

</details>


### [832] [CostFilter-AD: Enhancing Anomaly Detection through Matching Cost Filtering](https://arxiv.org/pdf/2505.01476)
*Zhe Zhang, Mingxiu Cai, Hanxiao Wang, Gaochang Wu, Tianyou Chai, Xiatian Zhu*

Main category: eess.IV

TL;DR: CostFilter-AD improves unsupervised anomaly detection by refining matching costs via a filtering network, enhancing accuracy and edge preservation.


<details>
  <summary>Details</summary>
Motivation: Existing UAD methods rely on inaccurate image or feature-level matching, leading to sub-optimal detection.

Method: Introduces cost filtering via a cost volume and filtering network, refining matches while preserving edges and anomalies.

Result: Validated on MVTec-AD and VisA benchmarks, showing benefits for single- and multi-class UAD tasks.

Conclusion: CostFilter-AD is a versatile plug-in that enhances both reconstruction- and embedding-based UAD methods.

Abstract: Unsupervised anomaly detection (UAD) seeks to localize the anomaly mask of an
input image with respect to normal samples. Either by reconstructing normal
counterparts (reconstruction-based) or by learning an image feature embedding
space (embedding-based), existing approaches fundamentally rely on image-level
or feature-level matching to derive anomaly scores. Often, such a matching
process is inaccurate yet overlooked, leading to sub-optimal detection. To
address this issue, we introduce the concept of cost filtering, borrowed from
classical matching tasks, such as depth and flow estimation, into the UAD
problem. We call this approach {\em CostFilter-AD}. Specifically, we first
construct a matching cost volume between the input and normal samples,
comprising two spatial dimensions and one matching dimension that encodes
potential matches. To refine this, we propose a cost volume filtering network,
guided by the input observation as an attention query across multiple feature
layers, which effectively suppresses matching noise while preserving edge
structures and capturing subtle anomalies. Designed as a generic
post-processing plug-in, CostFilter-AD can be integrated with either
reconstruction-based or embedding-based methods. Extensive experiments on
MVTec-AD and VisA benchmarks validate the generic benefits of CostFilter-AD for
both single- and multi-class UAD tasks. Code and models will be released at
https://github.com/ZHE-SAPI/CostFilter-AD.

</details>
