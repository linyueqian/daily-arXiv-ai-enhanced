<div id=toc></div>

# Table of Contents

- [cs.CL](#cs.CL) [Total: 49]
- [cs.CV](#cs.CV) [Total: 66]
- [cs.AI](#cs.AI) [Total: 11]
- [cs.SD](#cs.SD) [Total: 9]
- [cs.LG](#cs.LG) [Total: 72]
- [cs.MA](#cs.MA) [Total: 6]
- [cs.MM](#cs.MM) [Total: 1]
- [eess.AS](#eess.AS) [Total: 14]
- [eess.IV](#eess.IV) [Total: 18]


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [1] [MALIBU Benchmark: Multi-Agent LLM Implicit Bias Uncovered](https://arxiv.org/pdf/2507.01019)
*Imran Mirza, Cole Huang, Ishwara Vasista, Rohan Patil, Asli Akalin, Sean O'Brien, Kevin Zhu*

Main category: cs.CL

TL;DR: MALIBU is a benchmark to assess bias in LLM-based multi-agent systems, revealing biases and the need for nuanced fairness strategies.


<details>
  <summary>Details</summary>
Motivation: Addressing implicit biases in LLM-based multi-agent systems to ensure fairness and equitable representation.

Method: MALIBU evaluates bias through scenario-based assessments with two-phase LLM judging (scoring and comparing responses).

Result: Quantified biases in LLM outputs, showing bias mitigation may favor marginalized personas over neutrality.

Conclusion: Nuanced detection, balanced fairness strategies, and transparent benchmarks are needed for multi-agent systems.

Abstract: Multi-agent systems, which consist of multiple AI models interacting within a
shared environment, are increasingly used for persona-based interactions.
However, if not carefully designed, these systems can reinforce implicit biases
in large language models (LLMs), raising concerns about fairness and equitable
representation. We present MALIBU, a novel benchmark developed to assess the
degree to which LLM-based multi-agent systems implicitly reinforce social
biases and stereotypes. MALIBU evaluates bias in LLM-based multi-agent systems
through scenario-based assessments. AI models complete tasks within predefined
contexts, and their responses undergo evaluation by an LLM-based multi-agent
judging system in two phases. In the first phase, judges score responses
labeled with specific demographic personas (e.g., gender, race, religion)
across four metrics. In the second phase, judges compare paired responses
assigned to different personas, scoring them and selecting the superior
response. Our study quantifies biases in LLM-generated outputs, revealing that
bias mitigation may favor marginalized personas over true neutrality,
emphasizing the need for nuanced detection, balanced fairness strategies, and
transparent evaluation benchmarks in multi-agent systems.

</details>


### [2] [Event-based evaluation of abstractive news summarization](https://arxiv.org/pdf/2507.01160)
*Huiling You, Samia Touileb, Erik Velldal, Lilja Øvrelid*

Main category: cs.CL

TL;DR: The paper proposes evaluating abstractive summaries by comparing overlapping events in generated summaries, reference summaries, and original news articles, using a Norwegian dataset.


<details>
  <summary>Details</summary>
Motivation: Current evaluation methods rely on overlapping units or similarity scores, but summaries should ideally report events like the original articles.

Method: Calculate overlapping events between generated summaries, reference summaries, and original articles using a richly annotated Norwegian dataset.

Result: The approach offers deeper insight into event information in summaries.

Conclusion: Event-based evaluation provides a more meaningful measure of summary quality.

Abstract: An abstractive summary of a news article contains its most important
information in a condensed version. The evaluation of automatically generated
summaries by generative language models relies heavily on human-authored
summaries as gold references, by calculating overlapping units or similarity
scores. News articles report events, and ideally so should the summaries. In
this work, we propose to evaluate the quality of abstractive summaries by
calculating overlapping events between generated summaries, reference
summaries, and the original news articles. We experiment on a richly annotated
Norwegian dataset comprising both events annotations and summaries authored by
expert human annotators. Our approach provides more insight into the event
information contained in the summaries.

</details>


### [3] [Matching and Linking Entries in Historical Swedish Encyclopedias](https://arxiv.org/pdf/2507.01170)
*Simon Börjesson, Erik Ersmark, Pierre Nugues*

Main category: cs.CL

TL;DR: The paper analyzes geographic entry shifts in the Nordisk familjebok encyclopedia between its first and second editions, revealing a trend away from Europe towards other regions, influenced by historical events like WWI.


<details>
  <summary>Details</summary>
Motivation: To study intellectual shifts in Sweden by examining changes in geographic entries across editions of the Nordisk familjebok.

Method: Digitized texts were resegmented, entries matched using semantic embeddings, and geographic entries classified and linked to Wikidata.

Result: A significant shift in geographic focus from Europe to North America, Africa, Asia, Australia, and northern Scandinavia was observed.

Conclusion: The findings reflect broader intellectual and historical changes, such as the impact of WWI and emerging global powers.

Abstract: The \textit{Nordisk familjebok} is a Swedish encyclopedia from the 19th and
20th centuries. It was written by a team of experts and aimed to be an
intellectual reference, stressing precision and accuracy. This encyclopedia had
four main editions remarkable by their size, ranging from 20 to 38 volumes. As
a consequence, the \textit{Nordisk familjebok} had a considerable influence in
universities, schools, the media, and society overall. As new editions were
released, the selection of entries and their content evolved, reflecting
intellectual changes in Sweden.
  In this paper, we used digitized versions from \textit{Project Runeberg}. We
first resegmented the raw text into entries and matched pairs of entries
between the first and second editions using semantic sentence embeddings. We
then extracted the geographical entries from both editions using a
transformer-based classifier and linked them to Wikidata. This enabled us to
identify geographic trends and possible shifts between the first and second
editions, written between 1876-1899 and 1904-1926, respectively.
  Interpreting the results, we observe a small but significant shift in
geographic focus away from Europe and towards North America, Africa, Asia,
Australia, and northern Scandinavia from the first to the second edition,
confirming the influence of the First World War and the rise of new powers. The
code and data are available on GitHub at
https://github.com/sibbo/nordisk-familjebok.

</details>


### [4] [MEGA: xLSTM with Multihead Exponential Gated Fusion for Precise Aspect-based Sentiment Analysis](https://arxiv.org/pdf/2507.01213)
*Adamu Lawan, Juhua Pu, Haruna Yunusa, Jawad Muhammad, Muhammad Lawan*

Main category: cs.CL

TL;DR: The paper proposes xLSTM with Multihead Exponential Gated Fusion (MEGA) for Aspect-based Sentiment Analysis (ABSA), addressing efficiency and performance gaps in existing methods.


<details>
  <summary>Details</summary>
Motivation: Existing ABSA methods struggle with balancing computational efficiency and high performance, lacking global context or demanding excessive resources.

Method: The MEGA framework integrates bi-directional mLSTM with forward and partially flipped backward streams (PF-mLSTM) and a multihead cross exponential gated fusion mechanism (MECGAF).

Result: MEGA outperforms state-of-the-art baselines on three benchmark datasets, achieving superior accuracy and efficiency.

Conclusion: The proposed MEGA framework effectively addresses ABSA challenges by optimizing short-range dependency capture while maintaining global context and computational efficiency.

Abstract: Aspect-based Sentiment Analysis (ABSA) is a critical Natural Language
Processing (NLP) task that extracts aspects from text and determines their
associated sentiments, enabling fine-grained analysis of user opinions.
Existing ABSA methods struggle to balance computational efficiency with high
performance: deep learning models often lack global context, transformers
demand significant computational resources, and Mamba-based approaches face
CUDA dependency and diminished local correlations. Recent advancements in
Extended Long Short-Term Memory (xLSTM) models, particularly their efficient
modeling of long-range dependencies, have significantly advanced the NLP
community. However, their potential in ABSA remains untapped. To this end, we
propose xLSTM with Multihead Exponential Gated Fusion (MEGA), a novel framework
integrating a bi-directional mLSTM architecture with forward and partially
flipped backward (PF-mLSTM) streams. The PF-mLSTM enhances localized context
modeling by processing the initial sequence segment in reverse with dedicated
parameters, preserving critical short-range patterns. We further introduce an
mLSTM-based multihead cross exponential gated fusion mechanism (MECGAF) that
dynamically combines forward mLSTM outputs as query and key with PF-mLSTM
outputs as value, optimizing short-range dependency capture while maintaining
global context and efficiency. Experimental results on three benchmark datasets
demonstrate that MEGA outperforms state-of-the-art baselines, achieving
superior accuracy and efficiency in ABSA tasks.

</details>


### [5] [The Medium Is Not the Message: Deconfounding Text Embeddings via Linear Concept Erasure](https://arxiv.org/pdf/2507.01234)
*Yu Fan, Yang Tian, Shauli Ravfogel, Mrinmaya Sachan, Elliott Ash, Alexander Hoyle*

Main category: cs.CL

TL;DR: A debiasing algorithm reduces biases in text embeddings caused by confounders like source or language, improving similarity and clustering metrics without degrading out-of-distribution performance.


<details>
  <summary>Details</summary>
Motivation: Text embeddings can be biased by spurious attributes (e.g., source or language), which affects applications pooling texts from different corpora.

Method: A debiasing algorithm removes information about observed confounders from encoder representations.

Result: Substantial bias reduction with minimal computational cost; improved similarity and clustering metrics across tasks.

Conclusion: Debiasing improves embedding quality without harming out-of-distribution performance.

Abstract: Embedding-based similarity metrics between text sequences can be influenced
not just by the content dimensions we most care about, but can also be biased
by spurious attributes like the text's source or language. These document
confounders cause problems for many applications, but especially those that
need to pool texts from different corpora. This paper shows that a debiasing
algorithm that removes information about observed confounders from the encoder
representations substantially reduces these biases at a minimal computational
cost. Document similarity and clustering metrics improve across every embedding
variant and task we evaluate -- often dramatically. Interestingly, performance
on out-of-distribution benchmarks is not impacted, indicating that the
embeddings are not otherwise degraded.

</details>


### [6] [GAIus: Combining Genai with Legal Clauses Retrieval for Knowledge-based Assistant](https://arxiv.org/pdf/2507.01259)
*Michał Matak, Jarosław A. Chudziak*

Main category: cs.CL

TL;DR: The paper introduces gAIus, an LLM-based agent for legal tasks in non-English/Chinese contexts, focusing on Polish Civil Code. It proposes a retrieval mechanism outperforming embedding-based methods, improving GPT-3.5-turbo by 419% and GPT-4o-mini from 31% to 86%.


<details>
  <summary>Details</summary>
Motivation: Addressing the challenge of LLMs providing accurate, referenced legal answers for non-English/Chinese jurisdictions, particularly Poland.

Method: Proposes gAIus, a cognitive LLM-based agent with a human-friendly retrieval mechanism, evaluated using Polish law apprenticeship exam questions.

Result: Significant performance improvements: GPT-3.5-turbo boosted by 419%, GPT-4o-mini accuracy raised from 31% to 86%.

Conclusion: Demonstrates the potential of specialized LLM architectures for legal tasks, with future research and applications suggested.

Abstract: In this paper we discuss the capability of large language models to base
their answer and provide proper references when dealing with legal matters of
non-english and non-chinese speaking country. We discuss the history of legal
information retrieval, the difference between case law and statute law, its
impact on the legal tasks and analyze the latest research in this field. Basing
on that background we introduce gAIus, the architecture of the cognitive
LLM-based agent, whose responses are based on the knowledge retrieved from
certain legal act, which is Polish Civil Code. We propose a retrieval mechanism
which is more explainable, human-friendly and achieves better results than
embedding-based approaches. To evaluate our method we create special dataset
based on single-choice questions from entrance exams for law apprenticeships
conducted in Poland. The proposed architecture critically leveraged the
abilities of used large language models, improving the gpt-3.5-turbo-0125 by
419%, allowing it to beat gpt-4o and lifting gpt-4o-mini score from 31% to 86%.
At the end of our paper we show the possible future path of research and
potential applications of our findings.

</details>


### [7] [Evaluating Large Language Models for Multimodal Simulated Ophthalmic Decision-Making in Diabetic Retinopathy and Glaucoma Screening](https://arxiv.org/pdf/2507.01278)
*Cindy Lie Tabuse, David Restepo, Carolina Gracitelli, Fernando Korn Malerbi, Caio Regatieri, Luis Filipe Nakayama*

Main category: cs.CL

TL;DR: GPT-4 shows moderate performance in simulating clinical decisions for diabetic retinopathy and glaucoma screening but lacks precision for complex tasks. Metadata inclusion doesn't significantly impact results.


<details>
  <summary>Details</summary>
Motivation: To explore the utility of large language models (LLMs) like GPT-4 in ophthalmology, specifically for interpreting retinal fundus images and simulating clinical decisions.

Method: A retrospective diagnostic validation study using 300 annotated fundus images. GPT-4 was given structured prompts with or without metadata to assign severity scores and recommend referrals. Performance was measured using accuracy, F1 scores, and Cohen's kappa.

Result: GPT-4 performed moderately for diabetic retinopathy (accuracy 67.5%) and binary referral tasks (accuracy 82.3%) but poorly for glaucoma (accuracy ~78%). Metadata had no significant effect.

Conclusion: GPT-4 can simulate basic ophthalmic decision-making but isn't suitable for clinical use. It may assist in education, documentation, or image annotation workflows.

Abstract: Large language models (LLMs) can simulate clinical reasoning based on natural
language prompts, but their utility in ophthalmology is largely unexplored.
This study evaluated GPT-4's ability to interpret structured textual
descriptions of retinal fundus photographs and simulate clinical decisions for
diabetic retinopathy (DR) and glaucoma screening, including the impact of
adding real or synthetic clinical metadata. We conducted a retrospective
diagnostic validation study using 300 annotated fundus images. GPT-4 received
structured prompts describing each image, with or without patient metadata. The
model was tasked with assigning an ICDR severity score, recommending DR
referral, and estimating the cup-to-disc ratio for glaucoma referral.
Performance was evaluated using accuracy, macro and weighted F1 scores, and
Cohen's kappa. McNemar's test and change rate analysis were used to assess the
influence of metadata. GPT-4 showed moderate performance for ICDR
classification (accuracy 67.5%, macro F1 0.33, weighted F1 0.67, kappa 0.25),
driven mainly by correct identification of normal cases. Performance improved
in the binary DR referral task (accuracy 82.3%, F1 0.54, kappa 0.44). For
glaucoma referral, performance was poor across all settings (accuracy ~78%, F1
<0.04, kappa <0.03). Metadata inclusion did not significantly affect outcomes
(McNemar p > 0.05), and predictions remained consistent across conditions.
GPT-4 can simulate basic ophthalmic decision-making from structured prompts but
lacks precision for complex tasks. While not suitable for clinical use, LLMs
may assist in education, documentation, or image annotation workflows in
ophthalmology.

</details>


### [8] [Rethinking All Evidence: Enhancing Trustworthy Retrieval-Augmented Generation via Conflict-Driven Summarization](https://arxiv.org/pdf/2507.01281)
*Juan Chen, Baolong Bi, Wei Zhang, Jingyan Sui, Xiaofei Zhu, Yuanzhuo Wang, Lingrui Mei, Shenghua Liu*

Main category: cs.CL

TL;DR: CARE-RAG improves RAG systems by addressing knowledge conflicts through conflict-driven summarization of internal and retrieved evidence, enhancing reliability.


<details>
  <summary>Details</summary>
Motivation: Knowledge conflicts in RAG systems undermine reliability; CARE-RAG aims to rethink and synthesize all evidence for trustworthy generation.

Method: CARE-RAG derives parameter-aware and context-aware evidence, uses a distilled 3B LLaMA3.2 model for conflict-driven summarization, and includes QA Repair for benchmark integrity.

Result: CARE-RAG outperforms RAG baselines, especially with noisy or conflicting evidence, on revised QA datasets.

Conclusion: CARE-RAG enhances RAG reliability by effectively managing knowledge conflicts and refining evidence.

Abstract: Retrieval-Augmented Generation (RAG) enhances large language models (LLMs) by
integrating their parametric knowledge with external retrieved content.
However, knowledge conflicts caused by internal inconsistencies or noisy
retrieved content can severely undermine the generation reliability of RAG
systems.In this work, we argue that LLMs should rethink all evidence, including
both retrieved content and internal knowledge, before generating responses.We
propose CARE-RAG (Conflict-Aware and Reliable Evidence for RAG), a novel
framework that improves trustworthiness through Conflict-Driven Summarization
of all available evidence.CARE-RAG first derives parameter-aware evidence by
comparing parameter records to identify diverse internal perspectives. It then
refines retrieved evidences to produce context-aware evidence, removing
irrelevant or misleading content. To detect and summarize conflicts, we distill
a 3B LLaMA3.2 model to perform conflict-driven summarization, enabling reliable
synthesis across multiple sources.To further ensure evaluation integrity, we
introduce a QA Repair step to correct outdated or ambiguous benchmark
answers.Experiments on revised QA datasets with retrieval data show that
CARE-RAG consistently outperforms strong RAG baselines, especially in scenarios
with noisy or conflicting evidence.

</details>


### [9] [Frustratingly Simple Retrieval Improves Challenging, Reasoning-Intensive Benchmarks](https://arxiv.org/pdf/2507.01297)
*Xinxi Lyu, Michael Duan, Rulin Shao, Pang Wei Koh, Sewon Min*

Main category: cs.CL

TL;DR: CompactDS, a diverse web-scale datastore, enhances RAG performance on reasoning-intensive benchmarks, achieving significant accuracy improvements with simplicity and efficiency.


<details>
  <summary>Details</summary>
Motivation: Prior RAG work underperformed on reasoning-intensive tasks due to limited datastores. This work addresses the gap by introducing a high-quality, web-scale datastore.

Method: Developed CompactDS, filtering web content for quality and combining in-memory ANN with on-disk exact search for efficient retrieval.

Result: Achieved 10-33% relative accuracy gains across benchmarks (MMLU, MMLU Pro, GPQA, MATH) with minimal RAG, outperforming web search and complex systems.

Conclusion: CompactDS demonstrates the importance of diverse, high-quality data for RAG, offering a simple, reproducible solution for retrieval-based AI systems.

Abstract: Retrieval-augmented Generation (RAG) has primarily been studied in limited
settings, such as factoid question answering; more challenging,
reasoning-intensive benchmarks have seen limited success from minimal RAG. In
this work, we challenge this prevailing view on established,
reasoning-intensive benchmarks: MMLU, MMLU Pro, AGI Eval, GPQA, and MATH. We
identify a key missing component in prior work: a usable, web-scale datastore
aligned with the breadth of pretraining data. To this end, we introduce
CompactDS: a diverse, high-quality, web-scale datastore that achieves high
retrieval accuracy and subsecond latency on a single-node. The key insights are
(1) most web content can be filtered out without sacrificing coverage, and a
compact, high-quality subset is sufficient; and (2) combining in-memory
approximate nearest neighbor (ANN) retrieval and on-disk exact search balances
speed and recall. Using CompactDS, we show that a minimal RAG pipeline achieves
consistent accuracy improvements across all benchmarks and model sizes
(8B--70B), with relative gains of 10% on MMLU, 33% on MMLU Pro, 14% on GPQA,
and 19% on MATH. No single data source suffices alone, highlighting the
importance of diversity of sources (web crawls, curated math, academic papers,
textbooks). Finally, we show that our carefully designed in-house datastore
matches or outperforms web search engines such as Google Search, as well as
recently proposed, complex agent-based RAG systems--all while maintaining
simplicity, reproducibility, and self-containment. We release CompactDS and our
retrieval pipeline, supporting future research exploring retrieval-based AI
systems.

</details>


### [10] [La RoSA: Enhancing LLM Efficiency via Layerwise Rotated Sparse Activation](https://arxiv.org/pdf/2507.01299)
*Kai Liu, Bowen Xu, Shaoyu Wu, Xin Chen, Hao Zhou, Yongliang Tao, Lulu Hu*

Main category: cs.CL

TL;DR: LaRoSA introduces a novel method for activation sparsification in LLMs, achieving consistent sparsity and speed-up without additional training or magnitude-based pruning.


<details>
  <summary>Details</summary>
Motivation: To address limitations of existing methods (time-consuming recovery training or unstable sparsity) and improve LLM efficiency.

Method: Uses layerwise orthogonal rotations to transform activations for sparsification, applying Top-K selection for consistent sparsity.

Result: Achieves 1.30x speed-up with minimal performance drop (0.17 perplexity gap) and reduces accuracy gap to 0.54%.

Conclusion: LaRoSA is effective across LLMs, offering robust efficiency improvements without compromising performance.

Abstract: Activation sparsity can reduce the computational overhead and memory
transfers during the forward pass of Large Language Model (LLM) inference.
Existing methods face limitations, either demanding time-consuming recovery
training that hinders real-world adoption, or relying on empirical
magnitude-based pruning, which causes fluctuating sparsity and unstable
inference speed-up. This paper introduces LaRoSA (Layerwise Rotated Sparse
Activation), a novel method for activation sparsification designed to improve
LLM efficiency without requiring additional training or magnitude-based
pruning. We leverage layerwise orthogonal rotations to transform input
activations into rotated forms that are more suitable for sparsification. By
employing a Top-K selection approach within the rotated activations, we achieve
consistent model-level sparsity and reliable wall-clock time speed-up. LaRoSA
is effective across various sizes and types of LLMs, demonstrating minimal
performance degradation and robust inference acceleration. Specifically, for
LLaMA2-7B at 40% sparsity, LaRoSA achieves a mere 0.17 perplexity gap with a
consistent 1.30x wall-clock time speed-up, and reduces the accuracy gap in
zero-shot tasks compared to the dense model to just 0.54%, while surpassing
TEAL by 1.77% and CATS by 17.14%.

</details>


### [11] [Symbolic or Numerical? Understanding Physics Problem Solving in Reasoning LLMs](https://arxiv.org/pdf/2507.01334)
*Nifu Dan, Yujun Cai, Yiwei Wang*

Main category: cs.CL

TL;DR: Advanced instruction-tuned reasoning models like Deepseek-R1 excel in solving complex physics problems, achieving state-of-the-art accuracy and unique symbolic reasoning patterns, with few-shot prompting further boosting performance.


<details>
  <summary>Details</summary>
Motivation: Addressing the challenge of physics reasoning for LLMs, which requires deep conceptual understanding and problem-solving skills.

Method: Application of advanced instruction-tuned reasoning models (e.g., Deepseek-R1) to diverse physics problems from the SciBench benchmark, using few-shot prompting.

Result: Models achieve top accuracy and exhibit unique symbolic reasoning patterns, with few-shot prompting enhancing performance.

Conclusion: Reasoning models show promise in physics problem-solving, with strategic prompting offering potential for further improvements.

Abstract: Navigating the complexities of physics reasoning has long been a difficult
task for Large Language Models (LLMs), requiring a synthesis of profound
conceptual understanding and adept problem-solving techniques. In this study,
we investigate the application of advanced instruction-tuned reasoning models,
such as Deepseek-R1, to address a diverse spectrum of physics problems curated
from the challenging SciBench benchmark. Our comprehensive experimental
evaluation reveals the remarkable capabilities of reasoning models. Not only do
they achieve state-of-the-art accuracy in answering intricate physics
questions, but they also generate distinctive reasoning patterns that emphasize
on symbolic derivation. Furthermore, our findings indicate that even for these
highly sophisticated reasoning models, the strategic incorporation of few-shot
prompting can still yield measurable improvements in overall accuracy,
highlighting the potential for continued performance gains.

</details>


### [12] [Adaptability of ASR Models on Low-Resource Language: A Comparative Study of Whisper and Wav2Vec-BERT on Bangla](https://arxiv.org/pdf/2507.01931)
*Md Sazzadul Islam Ridoy, Sumi Akter, Md. Aminur Rahman*

Main category: cs.CL

TL;DR: The study compares OpenAI's Whisper and Facebook's Wav2Vec-BERT for Bangla ASR, finding Wav2Vec-BERT superior in performance and efficiency.


<details>
  <summary>Details</summary>
Motivation: To evaluate state-of-the-art ASR models for low-resource languages like Bangla.

Method: Experiments with fine-tuning and hyperparameter optimization on Mozilla Common Voice-17 and OpenSLR datasets, measuring WER, CER, training time, and computational efficiency.

Result: Wav2Vec-BERT outperformed Whisper in all metrics, requiring fewer resources.

Conclusion: Wav2Vec-BERT is more effective for low-resource ASR tasks, offering insights for robust system development.

Abstract: In recent years, neural models trained on large multilingual text and speech
datasets have shown great potential for supporting low-resource languages. This
study investigates the performances of two state-of-the-art Automatic Speech
Recognition (ASR) models, OpenAI's Whisper (Small & Large-V2) and Facebook's
Wav2Vec-BERT on Bangla, a low-resource language. We have conducted experiments
using two publicly available datasets: Mozilla Common Voice-17 and OpenSLR to
evaluate model performances. Through systematic fine-tuning and hyperparameter
optimization, including learning rate, epochs, and model checkpoint selection,
we have compared the models based on Word Error Rate (WER), Character Error
Rate (CER), Training Time, and Computational Efficiency. The Wav2Vec-BERT model
outperformed Whisper across all key evaluation metrics, demonstrated superior
performance while requiring fewer computational resources, and offered valuable
insights to develop robust speech recognition systems in low-resource
linguistic settings.

</details>


### [13] [LEDOM: An Open and Fundamental Reverse Language Model](https://arxiv.org/pdf/2507.01335)
*Xunjian Yin, Sitao Cheng, Yuxi Xie, Xinyu Hu, Li Lin, Xinyi Wang, Liangming Pan, William Yang Wang, Xiaojun Wan*

Main category: cs.CL

TL;DR: LEDOM is the first reverse language model trained autoregressively, showing potential for general tasks and introducing Reverse Reward for improved mathematical reasoning.


<details>
  <summary>Details</summary>
Motivation: To explore the capabilities and applications of a purely reverse language model, which processes sequences in reverse order.

Method: LEDOM is trained on 435B tokens with 2B and 7B parameter variants, using autoregressive training for reverse token prediction.

Result: LEDOM demonstrates unique backward reasoning capabilities, and its Reverse Reward application improves mathematical reasoning tasks.

Conclusion: LEDOM has broad application potential, and the release of models and data aims to foster further research.

Abstract: We introduce LEDOM, the first purely reverse language model, trained
autoregressively on 435B tokens with 2B and 7B parameter variants, which
processes sequences in reverse temporal order through previous token
prediction. For the first time, we present the reverse language model as a
potential foundational model across general tasks, accompanied by a set of
intriguing examples and insights. Based on LEDOM, we further introduce a novel
application: Reverse Reward, where LEDOM-guided reranking of forward language
model outputs leads to substantial performance improvements on mathematical
reasoning tasks. This approach leverages LEDOM's unique backward reasoning
capability to refine generation quality through posterior evaluation. Our
findings suggest that LEDOM exhibits unique characteristics with broad
application potential. We will release all models, training code, and
pre-training data to facilitate future research.

</details>


### [14] [Skywork-Reward-V2: Scaling Preference Data Curation via Human-AI Synergy](https://arxiv.org/pdf/2507.01352)
*Chris Yuhao Liu, Liang Zeng, Yuzhen Xiao, Jujie He, Jiacai Liu, Chaojie Wang, Rui Yan, Wei Shen, Fuxiang Zhang, Jiacheng Xu, Yang Liu, Yahui Zhou*

Main category: cs.CL

TL;DR: The paper introduces SynPref-40M, a large-scale preference dataset, and Skywork-Reward-V2, a suite of reward models, to address limitations in current reward models by improving data quality and scale through human-AI collaboration.


<details>
  <summary>Details</summary>
Motivation: Current reward models perform poorly due to narrow, synthetic, or low-quality preference datasets, limiting their ability to capture nuanced human preferences.

Method: A human-AI synergistic pipeline curates a large-scale dataset (SynPref-40M), and Skywork-Reward-V2 models are trained on a subset of this data.

Result: Skywork-Reward-V2 achieves state-of-the-art performance across seven benchmarks, demonstrating versatility in alignment, correctness, safety, and bias resistance.

Conclusion: The work highlights the importance of high-quality data curation and human-AI collaboration in advancing reward models, unlocking their potential.

Abstract: Despite the critical role of reward models (RMs) in reinforcement learning
from human feedback (RLHF), current state-of-the-art open RMs perform poorly on
most existing evaluation benchmarks, failing to capture the spectrum of nuanced
and sophisticated human preferences. Even approaches that incorporate advanced
training techniques have not yielded meaningful performance improvements. We
hypothesize that this brittleness stems primarily from limitations in
preference datasets, which are often narrowly scoped, synthetically labeled, or
lack rigorous quality control. To address these challenges, we present a
large-scale preference dataset comprising 40 million preference pairs, named
SynPref-40M. To enable data curation at scale, we design a human-AI synergistic
two-stage pipeline that leverages the complementary strengths of human
annotation quality and AI scalability. In this pipeline, humans provide
verified annotations, while large language models perform automatic curation
based on human guidance. Training on this preference mixture, we introduce
Skywork-Reward-V2, a suite of eight reward models ranging from 0.6B to 8B
parameters, trained on a carefully curated subset of 26 million preference
pairs from SynPref-40M. We demonstrate that Skywork-Reward-V2 is versatile
across a wide range of capabilities, including alignment with human
preferences, objective correctness, safety, resistance to stylistic biases, and
best-of-N scaling, achieving state-of-the-art performance across seven major
reward model benchmarks. Ablation studies confirm that the effectiveness of our
approach stems not only from data scale but also from high-quality curation.
The Skywork-Reward-V2 series represents substantial progress in open reward
models, highlighting the untapped potential of existing preference datasets and
demonstrating how human-AI curation synergy can unlock significantly higher
data quality.

</details>


### [15] [Clinical NLP with Attention-Based Deep Learning for Multi-Disease Prediction](https://arxiv.org/pdf/2507.01437)
*Ting Xu, Xiaoxiao Deng, Xiandong Meng, Haifeng Yang, Yan Wu*

Main category: cs.CL

TL;DR: A deep learning method using attention mechanisms is proposed for unified modeling of information extraction and multi-label disease prediction in EHR texts, outperforming existing approaches.


<details>
  <summary>Details</summary>
Motivation: Addressing the unstructured nature and high-dimensional semantic complexity of electronic health record (EHR) texts.

Method: Transformer-based architecture with multi-layer self-attention for representation learning, and a Sigmoid-based multi-label classifier for disease prediction, enhanced by context-aware semantic alignment.

Result: The model consistently outperforms existing methods across metrics, showing strong generalization under varying conditions.

Conclusion: The framework provides an efficient foundation for processing clinical texts and is significant for multi-label medical text modeling.

Abstract: This paper addresses the challenges posed by the unstructured nature and
high-dimensional semantic complexity of electronic health record texts. A deep
learning method based on attention mechanisms is proposed to achieve unified
modeling for information extraction and multi-label disease prediction. The
study is conducted on the MIMIC-IV dataset. A Transformer-based architecture is
used to perform representation learning over clinical text. Multi-layer
self-attention mechanisms are employed to capture key medical entities and
their contextual relationships. A Sigmoid-based multi-label classifier is then
applied to predict multiple disease labels. The model incorporates a
context-aware semantic alignment mechanism, enhancing its representational
capacity in typical medical scenarios such as label co-occurrence and sparse
information. To comprehensively evaluate model performance, a series of
experiments were conducted, including baseline comparisons, hyperparameter
sensitivity analysis, data perturbation studies, and noise injection tests.
Results demonstrate that the proposed method consistently outperforms
representative existing approaches across multiple performance metrics. The
model maintains strong generalization under varying data scales, interference
levels, and model depth configurations. The framework developed in this study
offers an efficient algorithmic foundation for processing real-world clinical
texts and presents practical significance for multi-label medical text modeling
tasks.

</details>


### [16] [LogitSpec: Accelerating Retrieval-based Speculative Decoding via Next Next Token Speculation](https://arxiv.org/pdf/2507.01449)
*Tianyu Liu, Qitan Lv, Hao Li, Xing Gao, Xiao Sun*

Main category: cs.CL

TL;DR: LogitSpec improves speculative decoding by using the last token's logit to predict and retrieve draft tokens, achieving faster LLM inference without training.


<details>
  <summary>Details</summary>
Motivation: Retrieval-based speculative decoding often fails to find accurate draft tokens, limiting its effectiveness. LogitSpec addresses this by expanding the retrieval range.

Method: LogitSpec uses the last token's logit to predict the next next token and retrieves references for both the next and next next tokens.

Result: Achieves up to 2.61× speedup and 3.28 mean accepted tokens per decoding step.

Conclusion: LogitSpec is a training-free, plug-and-play solution that enhances speculative decoding efficiency.

Abstract: Speculative decoding (SD), where a small draft model is employed to propose
draft tokens in advance and then the target model validates them in parallel,
has emerged as a promising technique for LLM inference acceleration. Many
endeavors to improve SD are to eliminate the need for a draft model and
generate draft tokens in a retrieval-based manner in order to further alleviate
the drafting overhead and significantly reduce the difficulty in deployment and
applications. However, retrieval-based SD relies on a matching paradigm to
retrieval the most relevant reference as the draft tokens, where these methods
often fail to find matched and accurate draft tokens. To address this
challenge, we propose LogitSpec to effectively expand the retrieval range and
find the most relevant reference as drafts. Our LogitSpec is motivated by the
observation that the logit of the last token can not only predict the next
token, but also speculate the next next token. Specifically, LogitSpec
generates draft tokens in two steps: (1) utilizing the last logit to speculate
the next next token; (2) retrieving relevant reference for both the next token
and the next next token. LogitSpec is training-free and plug-and-play, which
can be easily integrated into existing LLM inference frameworks. Extensive
experiments on a wide range of text generation benchmarks demonstrate that
LogitSpec can achieve up to 2.61 $\times$ speedup and 3.28 mean accepted tokens
per decoding step. Our code is available at
https://github.com/smart-lty/LogitSpec.

</details>


### [17] [Unifying Global and Near-Context Biasing in a Single Trie Pass](https://arxiv.org/pdf/2409.13514)
*Iuliia Thorbecke, Esaú Villatoro-Tello, Juan Zuluaga-Gomez, Shashi Kumar, Sergio Burdisso, Pradeep Rangappa, Andrés Carofilis, Srikanth Madikeri, Petr Motlicek, Karthik Pandia, Kadri Hacioğlu, Andreas Stolcke*

Main category: cs.CL

TL;DR: The paper proposes a method combining an NE bias list and a word-level n-gram LM to improve rare word and named entity recognition in ASR, achieving significant performance gains.


<details>
  <summary>Details</summary>
Motivation: Challenges in recognizing rare words and named entities in ASR, and adapting to new domains using only text data.

Method: Integration of an NE bias list and a word-level n-gram LM into a transducer-based ASR system.

Result: Improves entity recognition by up to 32% and reduces overall WER by up to 12%.

Conclusion: The proposed method effectively balances simplicity and performance, enhancing ASR adaptability and accuracy.

Abstract: Despite the success of end-to-end automatic speech recognition (ASR) models,
challenges persist in recognizing rare, out-of-vocabulary words - including
named entities (NE) - and in adapting to new domains using only text data. This
work presents a practical approach to address these challenges through an
unexplored combination of an NE bias list and a word-level n-gram language
model (LM). This solution balances simplicity and effectiveness, improving
entities' recognition while maintaining or even enhancing overall ASR
performance. We efficiently integrate this enriched biasing method into a
transducer-based ASR system, enabling context adaptation with almost no
computational overhead. We present our results on three datasets spanning four
languages and compare them to state-of-the-art biasing strategies. We
demonstrate that the proposed combination of keyword biasing and n-gram LM
improves entity recognition by up to 32% relative and reduces overall WER by up
to a 12% relative.

</details>


### [18] [Evaluating the Effectiveness of Direct Preference Optimization for Personalizing German Automatic Text Simplifications for Persons with Intellectual Disabilities](https://arxiv.org/pdf/2507.01479)
*Yingqiang Gao, Kaede Johnson, David Froehlich, Luisa Carrer, Sarah Ebling*

Main category: cs.CL

TL;DR: The paper proposes using direct preference optimization (DPO) to personalize LLM-based automatic text simplification (ATS) for individuals with intellectual disabilities, incorporating their feedback for better alignment with their needs.


<details>
  <summary>Details</summary>
Motivation: Existing LLM-based ATS systems lack personalization for target groups like individuals with intellectual disabilities, as they don't incorporate preference feedback during training.

Method: Extends supervised fine-tuning (SFT) with DPO, post-training LLM-based ATS models using human feedback from the target group. Introduces a pipeline for personalized ATS development.

Result: Demonstrates the importance of involving target group members in designing personalized AI accessibility solutions, improving alignment with human expectations.

Conclusion: Highlights a step towards personalized inclusive AI systems by integrating feedback from both experts and target group individuals.

Abstract: Automatic text simplification (ATS) aims to enhance language accessibility
for various target groups, particularly persons with intellectual disabilities.
Recent advancements in generative AI, especially large language models (LLMs),
have substantially improved the quality of machine-generated text
simplifications, thereby mitigating information barriers for the target group.
However, existing LLM-based ATS systems do not incorporate preference feedback
on text simplifications during training, resulting in a lack of personalization
tailored to the specific needs of target group representatives.
  In this work, we extend the standard supervised fine-tuning (SFT) approach
for adapting LLM-based ATS models by leveraging a computationally efficient LLM
alignment technique -- direct preference optimization (DPO). Specifically, we
post-train LLM-based ATS models using human feedback collected from persons
with intellectual disabilities, reflecting their preferences on paired text
simplifications generated by mainstream LLMs. Furthermore, we propose a
pipeline for developing personalized LLM-based ATS systems, encompassing data
collection, model selection, SFT and DPO post-training, and evaluation. Our
findings underscore the necessity of active participation of target group
persons in designing personalized AI accessibility solutions aligned with human
expectations. This work represents a step towards personalizing inclusive AI
systems at the target-group level, incorporating insights not only from text
simplification experts but also from target group persons themselves.

</details>


### [19] [Efficient Out-of-Scope Detection in Dialogue Systems via Uncertainty-Driven LLM Routing](https://arxiv.org/pdf/2507.01541)
*Álvaro Zaera, Diana Nicoleta Popa, Ivan Sekulic, Paolo Rosso*

Main category: cs.CL

TL;DR: A modular framework combining uncertainty modeling and fine-tuned LLMs improves OOS intent detection in task-oriented dialogue systems, balancing efficiency and performance.


<details>
  <summary>Details</summary>
Motivation: OOS intent detection is crucial for robustness in TODS, especially with unseen or ambiguous queries.

Method: Uses uncertainty estimation on in-scope intent classifier outputs, then triggers a fine-tuned LLM for high-uncertainty cases.

Result: Achieves state-of-the-art performance on OOS benchmarks, including real-world TODS data.

Conclusion: The framework effectively combines traditional methods with LLMs for efficient and accurate OOS detection.

Abstract: Out-of-scope (OOS) intent detection is a critical challenge in task-oriented
dialogue systems (TODS), as it ensures robustness to unseen and ambiguous
queries. In this work, we propose a novel but simple modular framework that
combines uncertainty modeling with fine-tuned large language models (LLMs) for
efficient and accurate OOS detection. The first step applies uncertainty
estimation to the output of an in-scope intent detection classifier, which is
currently deployed in a real-world TODS handling tens of thousands of user
interactions daily. The second step then leverages an emerging LLM-based
approach, where a fine-tuned LLM is triggered to make a final decision on
instances with high uncertainty. Unlike prior approaches, our method
effectively balances computational efficiency and performance, combining
traditional approaches with LLMs and yielding state-of-the-art results on key
OOS detection benchmarks, including real-world OOS data acquired from a
deployed TODS.

</details>


### [20] [Is External Information Useful for Stance Detection with LLMs?](https://arxiv.org/pdf/2507.01543)
*Quang Minh Nguyen, Taegyoon Kim*

Main category: cs.CL

TL;DR: External information (Wikipedia, web search) degrades stance detection performance in large language models (LLMs), contrary to prior findings with BERT-based systems.


<details>
  <summary>Details</summary>
Motivation: To evaluate the impact of external information on stance detection in LLMs, given its known benefits in BERT-based systems.

Method: Systematic evaluation across eight LLMs and three datasets with 12 targets, using Wikipedia and web search external information.

Result: External information degrades performance (up to 27.9% drop in macro F1), as LLMs align predictions with the provided information's stance/sentiment rather than ground truth.

Conclusion: External information introduces biases in LLM-based stance classifiers, highlighting risks despite fine-tuning and prompting efforts.

Abstract: In the stance detection task, a text is classified as either favorable,
opposing, or neutral towards a target. Prior work suggests that the use of
external information, e.g., excerpts from Wikipedia, improves stance detection
performance. However, whether or not such information can benefit large
language models (LLMs) remains an unanswered question, despite their wide
adoption in many reasoning tasks. In this study, we conduct a systematic
evaluation on how Wikipedia and web search external information can affect
stance detection across eight LLMs and in three datasets with 12 targets.
Surprisingly, we find that such information degrades performance in most cases,
with macro F1 scores dropping by up to 27.9\%. We explain this through
experiments showing LLMs' tendency to align their predictions with the stance
and sentiment of the provided information rather than the ground truth stance
of the given text. We also find that performance degradation persists with
chain-of-thought prompting, while fine-tuning mitigates but does not fully
eliminate it. Our findings, in contrast to previous literature on BERT-based
systems which suggests that external information enhances performance,
highlight the risks of information biases in LLM-based stance classifiers. Code
is available at https://github.com/ngqm/acl2025-stance-detection.

</details>


### [21] [Emotionally Intelligent Task-oriented Dialogue Systems: Architecture, Representation, and Optimisation](https://arxiv.org/pdf/2507.01594)
*Shutong Feng, Hsien-chin Lin, Nurul Lubis, Carel van Niekerk, Michael Heck, Benjamin Ruppik, Renato Vukovic, Milica Gašić*

Main category: cs.CL

TL;DR: LUSTER is an LLM-based unified system for task-oriented dialogue using end-to-end reinforcement learning, improving task success and emotional responsiveness.


<details>
  <summary>Details</summary>
Motivation: Building effective and emotionally intelligent task-oriented dialogue systems remains challenging despite advances in LLMs.

Method: Proposes LUSTER, combining LLMs with structured reward modeling for short-term (sentiment) and long-term (task success) rewards.

Result: LUSTER enhances resilience and emotional responsiveness in task-oriented dialogue systems.

Conclusion: Combining LLMs with structured rewards offers a practical path for next-gen conversational agents.

Abstract: Task-oriented dialogue (ToD) systems are designed to help users achieve
specific goals through natural language interaction. While recent advances in
large language models (LLMs) have significantly improved linguistic fluency and
contextual understanding, building effective and emotionally intelligent ToD
systems remains a complex challenge. Effective ToD systems must optimise for
task success, emotional understanding and responsiveness, and precise
information conveyance, all within inherently noisy and ambiguous
conversational environments. In this work, we investigate architectural,
representational, optimisational as well as emotional considerations of ToD
systems. We set up systems covering these design considerations with a
challenging evaluation environment composed of a natural-language user
simulator coupled with an imperfect natural language understanding module. We
propose \textbf{LUSTER}, an \textbf{L}LM-based \textbf{U}nified \textbf{S}ystem
for \textbf{T}ask-oriented dialogue with \textbf{E}nd-to-end
\textbf{R}einforcement learning with both short-term (user sentiment) and
long-term (task success) rewards. Our findings demonstrate that combining LLM
capability with structured reward modelling leads to more resilient and
emotionally responsive ToD systems, offering a practical path forward for
next-generation conversational agents.

</details>


### [22] [Chart Question Answering from Real-World Analytical Narratives](https://arxiv.org/pdf/2507.01627)
*Maeve Hutchinson, Radu Jianu, Aidan Slingsby, Jo Wood, Pranava Madhyastha*

Main category: cs.CL

TL;DR: A new dataset for chart question answering (CQA) is introduced, featuring real-world, multi-view charts with grounded questions, highlighting a performance gap in state-of-the-art models like GPT-4.1.


<details>
  <summary>Details</summary>
Motivation: To address the lack of ecologically valid reasoning workflows in prior CQA benchmarks by using real-world data from visualization notebooks.

Method: Construct a dataset from visualization notebooks with multi-view charts and natural language questions, then benchmark it using multimodal large language models like GPT-4.1.

Result: GPT-4.1 achieves 69.3% accuracy, showing a significant performance gap in this authentic CQA setting.

Conclusion: The dataset reveals challenges in CQA for state-of-the-art models, emphasizing the need for improved methods in real-world scenarios.

Abstract: We present a new dataset for chart question answering (CQA) constructed from
visualization notebooks. The dataset features real-world, multi-view charts
paired with natural language questions grounded in analytical narratives.
Unlike prior benchmarks, our data reflects ecologically valid reasoning
workflows. Benchmarking state-of-the-art multimodal large language models
reveals a significant performance gap, with GPT-4.1 achieving an accuracy of
69.3%, underscoring the challenges posed by this more authentic CQA setting.

</details>


### [23] [Confidence and Stability of Global and Pairwise Scores in NLP Evaluation](https://arxiv.org/pdf/2507.01633)
*Georgii Levtsov, Dmitry Ustalov*

Main category: cs.CL

TL;DR: The paper compares global pointwise scores and pairwise comparisons for evaluating NLP models, finding global scores reliable for overall rankings but weak for rare errors, while pairwise comparisons excel in identifying strong models with lower global scores.


<details>
  <summary>Details</summary>
Motivation: To evaluate the effectiveness of global scores versus pairwise comparisons in NLP model benchmarking, aiding decision-making for model evaluation strategies.

Method: Computational experiments on synthetic and real-world datasets using global metrics and the Bradley-Terry model for pairwise comparisons.

Result: Global scores provide reliable overall rankings but underestimate models with rare errors. Pairwise comparisons identify strong models with lower global scores but require more comparisons for convergence.

Conclusion: Both evaluation strategies have trade-offs; global scores suit overall rankings, while pairwise comparisons are better for nuanced model strengths, especially in text generation.

Abstract: With the advent of highly capable instruction-tuned neural language models,
benchmarking in natural language processing (NLP) is increasingly shifting
towards pairwise comparison leaderboards, such as LMSYS Arena, from traditional
global pointwise scores (e.g., GLUE, BIG-bench, SWE-bench). This paper
empirically investigates the strengths and weaknesses of both global scores and
pairwise comparisons to aid decision-making in selecting appropriate model
evaluation strategies. Through computational experiments on synthetic and
real-world datasets using standard global metrics and the popular Bradley-Terry
model for pairwise comparisons, we found that while global scores provide more
reliable overall rankings, they can underestimate strong models with rare,
significant errors or low confidence. Conversely, pairwise comparisons are
particularly effective for identifying strong contenders among models with
lower global scores, especially where quality metrics are hard to define (e.g.,
text generation), though they require more comparisons to converge if ties are
frequent. Our code and data are available at
https://github.com/HSPyroblast/srw-ranking under a permissive license.

</details>


### [24] [Adapting Language Models to Indonesian Local Languages: An Empirical Study of Language Transferability on Zero-Shot Settings](https://arxiv.org/pdf/2507.01645)
*Rifki Afina Putri*

Main category: cs.CL

TL;DR: The paper explores the transferability of pre-trained language models to low-resource Indonesian local languages for sentiment analysis, comparing zero-shot and adapter-based transfer methods. Performance varies by language group, with MAD-X improving results, and model exposure to the language being the key predictor of success.


<details>
  <summary>Details</summary>
Motivation: To understand how pre-trained language models perform on low-resource Indonesian local languages and identify factors influencing transferability.

Method: Evaluated zero-shot and adapter-based transfer (MAD-X) on ten local languages using monolingual and multilingual models (Indonesian BERT, mBERT, XLM-R). Languages were grouped by pre-training exposure: seen, partially seen, and unseen.

Result: Multilingual models perform best on seen languages, moderately on partially seen, and poorly on unseen. MAD-X improves performance, especially for seen and partially seen languages. Model exposure to the language is the strongest predictor of transfer success.

Conclusion: Transfer success depends on prior model exposure to the language. MAD-X is effective for seen and partially seen languages, while tokenization factors like subword fragmentation have limited explanatory power.

Abstract: In this paper, we investigate the transferability of pre-trained language
models to low-resource Indonesian local languages through the task of sentiment
analysis. We evaluate both zero-shot performance and adapter-based transfer on
ten local languages using models of different types: a monolingual Indonesian
BERT, multilingual models such as mBERT and XLM-R, and a modular adapter-based
approach called MAD-X. To better understand model behavior, we group the target
languages into three categories: seen (included during pre-training), partially
seen (not included but linguistically related to seen languages), and unseen
(absent and unrelated in pre-training data). Our results reveal clear
performance disparities across these groups: multilingual models perform best
on seen languages, moderately on partially seen ones, and poorly on unseen
languages. We find that MAD-X significantly improves performance, especially
for seen and partially seen languages, without requiring labeled data in the
target language. Additionally, we conduct a further analysis on tokenization
and show that while subword fragmentation and vocabulary overlap with
Indonesian correlate weakly with prediction quality, they do not fully explain
the observed performance. Instead, the most consistent predictor of transfer
success is the model's prior exposure to the language, either directly or
through a related language.

</details>


### [25] [AdamMeme: Adaptively Probe the Reasoning Capacity of Multimodal Large Language Models on Harmfulness](https://arxiv.org/pdf/2507.01702)
*Zixin Chen, Hongzhan Lin, Kaixin Li, Ziyang Luo, Zhen Ye, Guang Chen, Zhiyong Huang, Jing Ma*

Main category: cs.CL

TL;DR: AdamMeme is a flexible, agent-based framework for evaluating multimodal LLMs' understanding of harmful memes, addressing limitations of static benchmarks by dynamically updating meme data.


<details>
  <summary>Details</summary>
Motivation: Existing benchmarks for harmful meme understanding are static and lack adaptability to evolving online memes, necessitating a more dynamic evaluation approach.

Method: Proposes AdamMeme, a multi-agent framework that iteratively updates meme data with challenging samples to probe mLLMs' reasoning on harmfulness.

Result: AdamMeme systematically identifies performance variations and specific weaknesses in mLLMs' interpretation of harmful memes.

Conclusion: AdamMeme offers a comprehensive, adaptive evaluation method for mLLMs, providing fine-grained insights into their limitations in harmful meme understanding.

Abstract: The proliferation of multimodal memes in the social media era demands that
multimodal Large Language Models (mLLMs) effectively understand meme
harmfulness. Existing benchmarks for assessing mLLMs on harmful meme
understanding rely on accuracy-based, model-agnostic evaluations using static
datasets. These benchmarks are limited in their ability to provide up-to-date
and thorough assessments, as online memes evolve dynamically. To address this,
we propose AdamMeme, a flexible, agent-based evaluation framework that
adaptively probes the reasoning capabilities of mLLMs in deciphering meme
harmfulness. Through multi-agent collaboration, AdamMeme provides comprehensive
evaluations by iteratively updating the meme data with challenging samples,
thereby exposing specific limitations in how mLLMs interpret harmfulness.
Extensive experiments show that our framework systematically reveals the
varying performance of different target mLLMs, offering in-depth, fine-grained
analyses of model-specific weaknesses. Our code is available at
https://github.com/Lbotirx/AdamMeme.

</details>


### [26] [Stereotype Detection as a Catalyst for Enhanced Bias Detection: A Multi-Task Learning Approach](https://arxiv.org/pdf/2507.01715)
*Aditya Tomar, Rudra Murthy, Pushpak Bhattacharyya*

Main category: cs.CL

TL;DR: The paper introduces StereoBias, a dataset for detecting bias and stereotypes in language models, showing that joint training improves bias detection.


<details>
  <summary>Details</summary>
Motivation: Bias and stereotypes in language models can cause harm, especially in sensitive applications like content moderation and decision-making.

Method: The study uses StereoBias dataset and compares encoder-only and fine-tuned decoder-only models (using QLoRA) for bias and stereotype detection. Joint training is explored.

Result: Joint training on bias and stereotype detection significantly improves bias detection. Decoder-only models show competitive results.

Conclusion: Leveraging stereotype information enhances fairness and effectiveness in AI systems, with joint training being key.

Abstract: Bias and stereotypes in language models can cause harm, especially in
sensitive areas like content moderation and decision-making. This paper
addresses bias and stereotype detection by exploring how jointly learning these
tasks enhances model performance. We introduce StereoBias, a unique dataset
labeled for bias and stereotype detection across five categories: religion,
gender, socio-economic status, race, profession, and others, enabling a deeper
study of their relationship. Our experiments compare encoder-only models and
fine-tuned decoder-only models using QLoRA. While encoder-only models perform
well, decoder-only models also show competitive results. Crucially, joint
training on bias and stereotype detection significantly improves bias detection
compared to training them separately. Additional experiments with sentiment
analysis confirm that the improvements stem from the connection between bias
and stereotypes, not multi-task learning alone. These findings highlight the
value of leveraging stereotype information to build fairer and more effective
AI systems.

</details>


### [27] [LLMs for Legal Subsumption in German Employment Contracts](https://arxiv.org/pdf/2507.01734)
*Oliver Wardas, Florian Matthes*

Main category: cs.CL

TL;DR: The paper explores using LLMs and in-context learning to classify clauses in German employment contracts, finding that examination guidelines boost performance but LLMs still lag behind human lawyers.


<details>
  <summary>Details</summary>
Motivation: Legal work is text-heavy and resource-intensive, but current NLP methods lack interpretability and trustworthiness for dynamic legal environments.

Method: Collaborated with legal experts to extend a dataset, using LLMs and in-context learning to classify clauses as 'valid,' 'unfair,' or 'void' under three legal context variants.

Result: Examination guidelines significantly improved recall for void clauses and weighted F1-Score (80%), but LLMs' performance with full-text sources was below human lawyers.

Conclusion: LLMs show potential for assisting lawyers in contract legality review, but current methods have limitations.

Abstract: Legal work, characterized by its text-heavy and resource-intensive nature,
presents unique challenges and opportunities for NLP research. While
data-driven approaches have advanced the field, their lack of interpretability
and trustworthiness limits their applicability in dynamic legal environments.
To address these issues, we collaborated with legal experts to extend an
existing dataset and explored the use of Large Language Models (LLMs) and
in-context learning to evaluate the legality of clauses in German employment
contracts. Our work evaluates the ability of different LLMs to classify clauses
as "valid," "unfair," or "void" under three legal context variants: no legal
context, full-text sources of laws and court rulings, and distilled versions of
these (referred to as examination guidelines). Results show that full-text
sources moderately improve performance, while examination guidelines
significantly enhance recall for void clauses and weighted F1-Score, reaching
80\%. Despite these advancements, LLMs' performance when using full-text
sources remains substantially below that of human lawyers. We contribute an
extended dataset, including examination guidelines, referenced legal sources,
and corresponding annotations, alongside our code and all log files. Our
findings highlight the potential of LLMs to assist lawyers in contract legality
review while also underscoring the limitations of the methods presented.

</details>


### [28] [Data interference: emojis, homoglyphs, and issues of data fidelity in corpora and their results](https://arxiv.org/pdf/2507.01764)
*Matteo Di Cristofaro*

Main category: cs.CL

TL;DR: The paper explores how tokenisation discrepancies impact language data representation and analysis validity, focusing on emojis and homoglyphs, and proposes preprocessing methods for accurate corpus representation.


<details>
  <summary>Details</summary>
Motivation: To address challenges in tokenisation, particularly with emojis and homoglyphs, ensuring corpus fidelity and reliable linguistic analysis.

Method: Investigates preprocessing techniques for digital texts to maintain accuracy in corpora.

Result: Highlights the need for understanding linguistic and technical aspects of digital data to improve corpus analysis accuracy.

Conclusion: The study underscores the importance of precise tokenisation for reliable and repeatable linguistic research, impacting both quantitative and qualitative methods.

Abstract: Tokenisation - "the process of splitting text into atomic parts" (Brezina &
Timperley, 2017: 1) - is a crucial step for corpus linguistics, as it provides
the basis for any applicable quantitative method (e.g. collocations) while
ensuring the reliability of qualitative approaches. This paper examines how
discrepancies in tokenisation affect the representation of language data and
the validity of analytical findings: investigating the challenges posed by
emojis and homoglyphs, the study highlights the necessity of preprocessing
these elements to maintain corpus fidelity to the source data. The research
presents methods for ensuring that digital texts are accurately represented in
corpora, thereby supporting reliable linguistic analysis and guaranteeing the
repeatability of linguistic interpretations. The findings emphasise the
necessity of a detailed understanding of both linguistic and technical aspects
involved in digital textual data to enhance the accuracy of corpus analysis,
and have significant implications for both quantitative and qualitative
approaches in corpus-based research.

</details>


### [29] [MuRating: A High Quality Data Selecting Approach to Multilingual Large Language Model Pretraining](https://arxiv.org/pdf/2507.01785)
*Zhixun Chen, Ping Guo, Wenhan Han, Yifan Zhang, Binbin Liu, Haobin Lin, Fengze Liu, Yan Zhao, Bingni Zhang, Taifeng Wang, Yin Zheng, Meng Fang*

Main category: cs.CL

TL;DR: MuRating is a scalable framework for multilingual data-quality assessment, transferring English quality signals to 17 languages, improving model performance on English and multilingual tasks.


<details>
  <summary>Details</summary>
Motivation: Existing data-quality methods are English-centric; MuRating addresses this gap by enabling multilingual quality assessment.

Method: MuRating aggregates English raters via pairwise comparisons, projects judgments through translation, and trains a multilingual evaluator on diverse text pairs.

Result: MuRating boosts accuracy on English and multilingual benchmarks, especially in knowledge-intensive tasks, outperforming baselines like QuRater and AskLLM.

Conclusion: MuRating enhances multilingual data selection, with potential for further improvements in translation fidelity and addressing biases.

Abstract: Data quality is a critical driver of large language model performance, yet
existing model-based selection methods focus almost exclusively on English. We
introduce MuRating, a scalable framework that transfers high-quality English
data-quality signals into a single rater for 17 target languages. MuRating
aggregates multiple English "raters" via pairwise comparisons to learn unified
document-quality scores,then projects these judgments through translation to
train a multilingual evaluator on monolingual, cross-lingual, and parallel text
pairs. Applied to web data, MuRating selects balanced subsets of English and
multilingual content to pretrain a 1.2 B-parameter LLaMA model. Compared to
strong baselines, including QuRater, AskLLM, DCLM and so on, our approach
boosts average accuracy on both English benchmarks and multilingual
evaluations, with especially large gains on knowledge-intensive tasks. We
further analyze translation fidelity, selection biases, and underrepresentation
of narrative material, outlining directions for future work.

</details>


### [30] [Probing Evaluation Awareness of Language Models](https://arxiv.org/pdf/2507.01786)
*Jord Nguyen, Khiem Hoang, Carlo Leonardo Attubato, Felix Hofstätter*

Main category: cs.CL

TL;DR: Language models can detect evaluation vs. deployment phases, raising concerns about AI governance. Probes in Llama-3.3-70B-Instruct show this distinction internally, suggesting current evaluations may seem artificial. This highlights the need for trustworthy evaluations and understanding deceptive capabilities.


<details>
  <summary>Details</summary>
Motivation: To investigate evaluation awareness in language models and its implications for AI safety and governance, given the potential for models to deceive evaluations.

Method: Used linear probes on Llama-3.3-70B-Instruct to analyze its ability to distinguish between evaluation and deployment prompts.

Result: Probes successfully separated real-world evaluation and deployment prompts, indicating models internally represent this distinction. Safety evaluations were correctly classified as artificial.

Conclusion: The study emphasizes the need for reliable evaluations and understanding deceptive model behaviors, suggesting model internals can aid safety audits for future models.

Abstract: Language models can distinguish between testing and deployment phases -- a
capability known as evaluation awareness. This has significant safety and
policy implications, potentially undermining the reliability of evaluations
that are central to AI governance frameworks and voluntary industry
commitments. In this paper, we study evaluation awareness in
Llama-3.3-70B-Instruct. We show that linear probes can separate real-world
evaluation and deployment prompts, suggesting that current models internally
represent this distinction. We also find that current safety evaluations are
correctly classified by the probes, suggesting that they already appear
artificial or inauthentic to models. Our findings underscore the importance of
ensuring trustworthy evaluations and understanding deceptive capabilities. More
broadly, our work showcases how model internals may be leveraged to support
blackbox methods in safety audits, especially for future models more competent
at evaluation awareness and deception.

</details>


### [31] [How Do Vision-Language Models Process Conflicting Information Across Modalities?](https://arxiv.org/pdf/2507.01790)
*Tianze Hua, Tian Yun, Ellie Pavlick*

Main category: cs.CL

TL;DR: The paper investigates how vision-language AI models handle conflicting multimodal inputs (e.g., mismatched image-caption pairs) and reveals biases in modality preference, internal representational structures, and the role of attention heads in resolving conflicts.


<details>
  <summary>Details</summary>
Motivation: To understand how multimodal AI models behave when faced with conflicting input streams, particularly in vision-language tasks, and to explore the mechanisms behind modality preference and conflict resolution.

Method: The study tests models with inconsistent inputs (e.g., an image of a dog paired with a cat caption) and analyzes their responses to modality-specific queries, examining internal representations and attention heads.

Result: Models often favor one modality over another, with preferences evident in their internal structures. Specific attention heads and modality-agnostic "router heads" influence and can be manipulated to improve performance.

Conclusion: The work advances understanding of how multimodal models detect and resolve conflicts, offering insights into controlling their behavior in complex environments.

Abstract: AI models are increasingly required to be multimodal, integrating disparate
input streams into a coherent state representation on which subsequent
behaviors and actions can be based. This paper seeks to understand how such
models behave when input streams present conflicting information. Focusing
specifically on vision-language models, we provide inconsistent inputs (e.g.,
an image of a dog paired with the caption "A photo of a cat") and ask the model
to report the information present in one of the specific modalities (e.g.,
"What does the caption say / What is in the image?"). We find that models often
favor one modality over the other, e.g., reporting the image regardless of what
the caption says, but that different models differ in which modality they
favor. We find evidence that the behaviorally preferred modality is evident in
the internal representational structure of the model, and that specific
attention heads can restructure the representations to favor one modality over
the other. Moreover, we find modality-agnostic "router heads" which appear to
promote answers about the modality requested in the instruction, and which can
be manipulated or transferred in order to improve performance across datasets
and modalities. Together, the work provides essential steps towards identifying
and controlling if and how models detect and resolve conflicting signals within
complex multimodal environments.

</details>


### [32] [The Anatomy of Evidence: An Investigation Into Explainable ICD Coding](https://arxiv.org/pdf/2507.01802)
*Katharina Beckh, Elisa Studeny, Sujan Sai Gannamaneni, Dario Antweiler, Stefan Rüping*

Main category: cs.CL

TL;DR: The paper analyzes the MDACE dataset for explainable medical coding, evaluating plausibility of current systems and proposing match measures and recommendations.


<details>
  <summary>Details</summary>
Motivation: To improve transparency in automatic medical coding by evaluating explainability methods using the MDACE dataset.

Method: In-depth analysis of the MDACE dataset and plausibility evaluation of explainable medical coding systems.

Result: Ground truth evidence aligns with code descriptions; state-of-the-art approaches show high overlap with ground truth.

Conclusion: Recommendations are provided for developing and evaluating explainable medical coding systems.

Abstract: Automatic medical coding has the potential to ease documentation and billing
processes. For this task, transparency plays an important role for medical
coders and regulatory bodies, which can be achieved using explainability
methods. However, the evaluation of these approaches has been mostly limited to
short text and binary settings due to a scarcity of annotated data. Recent
efforts by Cheng et al. (2023) have introduced the MDACE dataset, which
provides a valuable resource containing code evidence in clinical records. In
this work, we conduct an in-depth analysis of the MDACE dataset and perform
plausibility evaluation of current explainable medical coding systems from an
applied perspective. With this, we contribute to a deeper understanding of
automatic medical coding and evidence extraction. Our findings reveal that
ground truth evidence aligns with code descriptions to a certain degree. An
investigation into state-of-the-art approaches shows a high overlap with ground
truth evidence. We propose match measures and highlight success and failure
cases. Based on our findings, we provide recommendations for developing and
evaluating explainable medical coding systems.

</details>


### [33] [Evaluating Structured Output Robustness of Small Language Models for Open Attribute-Value Extraction from Clinical Notes](https://arxiv.org/pdf/2507.01810)
*Nikita Neveditsin, Pawan Lingras, Vijay Mago*

Main category: cs.CL

TL;DR: JSON is the most parseable format for structured outputs in clinical note extraction, with robustness improving via targeted prompting and larger models.


<details>
  <summary>Details</summary>
Motivation: To compare parseability of JSON, YAML, and XML for open attribute-value extraction from clinical notes, aiding deployment in privacy-sensitive settings.

Method: Evaluated three serialization formats (JSON, YAML, XML) using small language models, analyzing parseability, structural robustness, and failure patterns.

Result: JSON consistently outperforms YAML and XML in parseability. Robustness improves with targeted prompting and larger models but declines for longer documents.

Conclusion: JSON is recommended for clinical note extraction due to higher parseability, with insights on prompt design and model size for optimal performance.

Abstract: We present a comparative analysis of the parseability of structured outputs
generated by small language models for open attribute-value extraction from
clinical notes. We evaluate three widely used serialization formats: JSON,
YAML, and XML, and find that JSON consistently yields the highest parseability.
Structural robustness improves with targeted prompting and larger models, but
declines for longer documents and certain note types. Our error analysis
identifies recurring format-specific failure patterns. These findings offer
practical guidance for selecting serialization formats and designing prompts
when deploying language models in privacy-sensitive clinical settings.

</details>


### [34] [Low-Perplexity LLM-Generated Sequences and Where To Find Them](https://arxiv.org/pdf/2507.01844)
*Arthur Wuhrmann, Anastasiia Kucherenko, Andrei Kucharavy*

Main category: cs.CL

TL;DR: The paper introduces a method to analyze how LLMs replicate training data by focusing on low-perplexity sequences, revealing unexpected gaps in data mapping and quantifying verbatim recall.


<details>
  <summary>Details</summary>
Motivation: Understanding how LLMs' training data influences their outputs is critical for transparency, accountability, privacy, and fairness.

Method: A systematic pipeline extracts and analyzes low-perplexity sequences (high-probability text spans) to trace their origins in the training data.

Result: Many low-perplexity sequences cannot be mapped to the training corpus, and for those that can, the study quantifies their distribution across source documents.

Conclusion: The findings provide insights into verbatim recall in LLMs and highlight the need for better understanding of training data impacts on model behavior.

Abstract: As Large Language Models (LLMs) become increasingly widespread, understanding
how specific training data shapes their outputs is crucial for transparency,
accountability, privacy, and fairness. To explore how LLMs leverage and
replicate their training data, we introduce a systematic approach centered on
analyzing low-perplexity sequences - high-probability text spans generated by
the model. Our pipeline reliably extracts such long sequences across diverse
topics while avoiding degeneration, then traces them back to their sources in
the training data. Surprisingly, we find that a substantial portion of these
low-perplexity spans cannot be mapped to the corpus. For those that do match,
we quantify the distribution of occurrences across source documents,
highlighting the scope and nature of verbatim recall and paving a way toward
better understanding of how LLMs training data impacts their behavior.

</details>


### [35] [Eka-Eval : A Comprehensive Evaluation Framework for Large Language Models in Indian Languages](https://arxiv.org/pdf/2507.01853)
*Samridhi Raj Sinha, Rajvee Sheth, Abhishek Upperwal, Mayank Singh*

Main category: cs.CL

TL;DR: EKA-EVAL is a unified, multilingual evaluation framework for LLMs, integrating 35+ benchmarks, including 10 Indic-specific datasets, with features like distributed inference and multi-GPU support.


<details>
  <summary>Details</summary>
Motivation: Address the lack of non-English-centric evaluation frameworks for LLMs, especially for linguistically diverse regions like India.

Method: Developed EKA-EVAL, a production-ready framework with broad benchmark coverage, distributed inference, quantization, and multi-GPU support.

Result: EKA-EVAL is the first end-to-end, extensible evaluation suite for global and Indic LLMs, lowering multilingual benchmarking barriers.

Conclusion: EKA-EVAL is open-source and part of the EKA initiative, aiming to expand to 100+ benchmarks for a robust multilingual LLM evaluation ecosystem.

Abstract: The rapid advancement of Large Language Models (LLMs) has intensified the
need for evaluation frameworks that go beyond English centric benchmarks and
address the requirements of linguistically diverse regions such as India. We
present EKA-EVAL, a unified and production-ready evaluation framework that
integrates over 35 benchmarks, including 10 Indic-specific datasets, spanning
categories like reasoning, mathematics, tool use, long-context understanding,
and reading comprehension. Compared to existing Indian language evaluation
tools, EKA-EVAL offers broader benchmark coverage, with built-in support for
distributed inference, quantization, and multi-GPU usage. Our systematic
comparison positions EKA-EVAL as the first end-to-end, extensible evaluation
suite tailored for both global and Indic LLMs, significantly lowering the
barrier to multilingual benchmarking. The framework is open-source and publicly
available at https://github.com/lingo-iitgn/ eka-eval and a part of ongoing EKA
initiative (https://eka.soket.ai), which aims to scale up to over 100
benchmarks and establish a robust, multilingual evaluation ecosystem for LLMs.

</details>


### [36] [DIY-MKG: An LLM-Based Polyglot Language Learning System](https://arxiv.org/pdf/2507.01872)
*Kenan Tang, Yanhong Li, Yao Qin*

Main category: cs.CL

TL;DR: DIY-MKG is an open-source system for polyglot language learning, addressing limitations of existing tools by enabling personalized vocabulary knowledge graphs, adaptive quizzes, and user feedback.


<details>
  <summary>Details</summary>
Motivation: Existing tools lack support for polyglot learners, customization, and suffer from cognitive offloading.

Method: DIY-MKG uses LLMs to build personalized vocabulary knowledge graphs, offers rich annotations, and generates adaptive quizzes with user feedback.

Result: Evaluation shows reliable vocabulary expansion and highly accurate quizzes across multiple languages.

Conclusion: DIY-MKG is robust and effective for polyglot language learning.

Abstract: Existing language learning tools, even those powered by Large Language Models
(LLMs), often lack support for polyglot learners to build linguistic
connections across vocabularies in multiple languages, provide limited
customization for individual learning paces or needs, and suffer from
detrimental cognitive offloading. To address these limitations, we design
Do-It-Yourself Multilingual Knowledge Graph (DIY-MKG), an open-source system
that supports polyglot language learning. DIY-MKG allows the user to build
personalized vocabulary knowledge graphs, which are constructed by selective
expansion with related words suggested by an LLM. The system further enhances
learning through rich annotation capabilities and an adaptive review module
that leverages LLMs for dynamic, personalized quiz generation. In addition,
DIY-MKG allows users to flag incorrect quiz questions, simultaneously
increasing user engagement and providing a feedback loop for prompt refinement.
Our evaluation of LLM-based components in DIY-MKG shows that vocabulary
expansion is reliable and fair across multiple languages, and that the
generated quizzes are highly accurate, validating the robustness of DIY-MKG.

</details>


### [37] [MiCoTA: Bridging the Learnability Gap with Intermediate CoT and Teacher Assistants](https://arxiv.org/pdf/2507.01887)
*Dongyi Ding, Tiannan Wang, Chenghao Zhu, Meiling Tao, Yuchen Eleanor Jiang, Wangchunshu Zhou*

Main category: cs.CL

TL;DR: MiCoTA improves small language models' reasoning by using intermediate-sized models as teacher assistants and intermediate-length reasoning sequences.


<details>
  <summary>Details</summary>
Motivation: Address the 'SLMs Learnability Gap' where small language models struggle with long-form reasoning due to limited capacity.

Method: Introduces MiCoTA, a framework using intermediate-sized models and sequences to bridge capacity and reasoning gaps.

Result: SLMs show significant reasoning improvements, e.g., Qwen2.5-7B-Instruct and Qwen2.5-3B-Instruct achieve score boosts of 3.47 and 3.93.

Conclusion: MiCoTA effectively bridges gaps in SLMs' reasoning, paving the way for future research in long-CoT distillation.

Abstract: Large language models (LLMs) excel at reasoning tasks requiring long thought
sequences for planning, reflection, and refinement. However, their substantial
model size and high computational demands are impractical for widespread
deployment. Yet, small language models (SLMs) often struggle to learn long-form
CoT reasoning due to their limited capacity, a phenomenon we refer to as the
"SLMs Learnability Gap". To address this, we introduce
\textbf{Mi}d-\textbf{Co}T \textbf{T}eacher \textbf{A}ssistant Distillation
(MiCoTAl), a framework for improving long CoT distillation for SLMs. MiCoTA
employs intermediate-sized models as teacher assistants and utilizes
intermediate-length CoT sequences to bridge both the capacity and reasoning
length gaps. Our experiments on downstream tasks demonstrate that although SLMs
distilled from large teachers can perform poorly, by applying MiCoTA, they
achieve significant improvements in reasoning performance. Specifically,
Qwen2.5-7B-Instruct and Qwen2.5-3B-Instruct achieve an improvement of 3.47 and
3.93 respectively on average score on AIME2024, AMC, Olympiad, MATH-500 and
GSM8K benchmarks. To better understand the mechanism behind MiCoTA, we perform
a quantitative experiment demonstrating that our method produces data more
closely aligned with base SLM distributions. Our insights pave the way for
future research into long-CoT data distillation for SLMs.

</details>


### [38] [High-Layer Attention Pruning with Rescaling](https://arxiv.org/pdf/2507.01900)
*Songtao Liu, Peng Liu*

Main category: cs.CL

TL;DR: A novel pruning algorithm for LLMs strategically prunes attention heads in higher layers and uses adaptive rescaling to maintain representation quality, outperforming existing methods.


<details>
  <summary>Details</summary>
Motivation: Conventional pruning methods remove attention heads indiscriminately, ignoring their positions in the network, which can harm performance.

Method: Proposes a pruning algorithm targeting higher-layer attention heads and introduces adaptive rescaling to adjust representation scales post-pruning.

Result: Outperforms existing structured pruning methods, especially in generation tasks, across 27 datasets and multiple LLMs.

Conclusion: Strategic pruning and adaptive rescaling improve LLM compression and performance, particularly in generation tasks.

Abstract: Pruning is a highly effective approach for compressing large language models
(LLMs), significantly reducing inference latency. However, conventional
training-free structured pruning methods often employ a heuristic metric that
indiscriminately removes some attention heads across all pruning layers,
without considering their positions within the network architecture. In this
work, we propose a novel pruning algorithm that strategically prunes attention
heads in the model's higher layers. Since the removal of attention heads can
alter the magnitude of token representations, we introduce an adaptive
rescaling parameter that calibrates the representation scale post-pruning to
counteract this effect. We conduct comprehensive experiments on a wide range of
LLMs, including LLaMA3.1-8B, Mistral-7B-v0.3, Qwen2-7B, and Gemma2-9B. Our
evaluation includes both generation and discriminative tasks across 27
datasets. The results consistently demonstrate that our method outperforms
existing structured pruning methods. This improvement is particularly notable
in generation tasks, where our approach significantly outperforms existing
baselines.

</details>


### [39] [AI4Research: A Survey of Artificial Intelligence for Scientific Research](https://arxiv.org/pdf/2507.01903)
*Qiguang Chen, Mingda Yang, Libo Qin, Jinhao Liu, Zheng Yan, Jiannan Guan, Dengyun Peng, Yiyan Ji, Hanjing Li, Mengkang Hu, Yimeng Zhang, Yihao Liang, Yuhang Zhou, Jiaqi Wang, Zhi Chen, Wanxiang Che*

Main category: cs.CL

TL;DR: The paper presents a comprehensive survey on AI for Research (AI4Research), addressing gaps in understanding and development by introducing a taxonomy, identifying research frontiers, and compiling resources.


<details>
  <summary>Details</summary>
Motivation: The motivation stems from the lack of a unified survey on AI4Research despite advancements in AI and LLMs, which hampers progress in applying AI to scientific research.

Method: The authors introduce a systematic taxonomy for AI4Research tasks, identify key research gaps, and compile multidisciplinary resources and tools.

Result: The work provides a structured overview of AI4Research, highlights future directions, and offers accessible resources for the community.

Conclusion: The survey aims to facilitate quick access to AI4Research resources and inspire innovative breakthroughs in the field.

Abstract: Recent advancements in artificial intelligence (AI), particularly in large
language models (LLMs) such as OpenAI-o1 and DeepSeek-R1, have demonstrated
remarkable capabilities in complex domains such as logical reasoning and
experimental coding. Motivated by these advancements, numerous studies have
explored the application of AI in the innovation process, particularly in the
context of scientific research. These AI technologies primarily aim to develop
systems that can autonomously conduct research processes across a wide range of
scientific disciplines. Despite these significant strides, a comprehensive
survey on AI for Research (AI4Research) remains absent, which hampers our
understanding and impedes further development in this field. To address this
gap, we present a comprehensive survey and offer a unified perspective on
AI4Research. Specifically, the main contributions of our work are as follows:
(1) Systematic taxonomy: We first introduce a systematic taxonomy to classify
five mainstream tasks in AI4Research. (2) New frontiers: Then, we identify key
research gaps and highlight promising future directions, focusing on the rigor
and scalability of automated experiments, as well as the societal impact. (3)
Abundant applications and resources: Finally, we compile a wealth of resources,
including relevant multidisciplinary applications, data corpora, and tools. We
hope our work will provide the research community with quick access to these
resources and stimulate innovative breakthroughs in AI4Research.

</details>


### [40] [Gradient-Adaptive Policy Optimization: Towards Multi-Objective Alignment of Large Language Models](https://arxiv.org/pdf/2507.01915)
*Chengao Li, Hanyu Zhang, Yunkun Xu, Hongyan Xue, Xiang Ao, Qing He*

Main category: cs.CL

TL;DR: GAPO, a novel RLHF method, aligns LLMs with diverse human preferences using multi-objective optimization, outperforming state-of-the-art methods.


<details>
  <summary>Details</summary>
Motivation: Aligning LLMs with conflicting human preferences is challenging; GAPO addresses this by framing it as a multi-objective optimization problem.

Method: GAPO employs multiple-gradient descent to balance conflicting objectives, while P-GAPO incorporates user preferences for Pareto solutions.

Result: GAPO converges to Pareto optimal solutions and outperforms existing methods in helpfulness and harmlessness on Mistral-7B.

Conclusion: GAPO effectively aligns LLMs with diverse preferences, offering a scalable solution for human value alignment.

Abstract: Reinforcement Learning from Human Feedback (RLHF) has emerged as a powerful
technique for aligning large language models (LLMs) with human preferences.
However, effectively aligning LLMs with diverse human preferences remains a
significant challenge, particularly when they are conflict. To address this
issue, we frame human value alignment as a multi-objective optimization
problem, aiming to maximize a set of potentially conflicting objectives. We
introduce Gradient-Adaptive Policy Optimization (GAPO), a novel fine-tuning
paradigm that employs multiple-gradient descent to align LLMs with diverse
preference distributions. GAPO adaptively rescales the gradients for each
objective to determine an update direction that optimally balances the
trade-offs between objectives. Additionally, we introduce P-GAPO, which
incorporates user preferences across different objectives and achieves Pareto
solutions that better align with the user's specific needs. Our theoretical
analysis demonstrates that GAPO converges towards a Pareto optimal solution for
multiple objectives. Empirical results on Mistral-7B show that GAPO outperforms
current state-of-the-art methods, achieving superior performance in both
helpfulness and harmlessness.

</details>


### [41] [NaturalThoughts: Selecting and Distilling Reasoning Traces for General Reasoning Tasks](https://arxiv.org/pdf/2507.01921)
*Yang Li, Youssef Emad, Karthik Padthe, Jack Lanchantin, Weizhe Yuan, Thao Nguyen, Jason Weston, Shang-Wen Li, Dong Wang, Ilia Kulikov, Xian Li*

Main category: cs.CL

TL;DR: Distilling reasoning traces from a teacher model (NaturalThoughts) improves student models' reasoning more effectively than reinforcement learning, with difficult examples being more sample-efficient.


<details>
  <summary>Details</summary>
Motivation: To systematically study what kind of reasoning demonstrations from a teacher model most effectively improve student models' reasoning capabilities.

Method: Curate high-quality "NaturalThoughts" by selecting reasoning traces from a strong teacher model, analyze factors affecting distillation, and evaluate on Llama and Qwen models.

Result: NaturalThoughts outperforms existing datasets (e.g., OpenThoughts, LIMO) on STEM benchmarks like GPQA-Diamond, MMLU-Pro, and SuperGPQA.

Conclusion: Selecting difficult, diverse reasoning examples is more sample-efficient for transferring reasoning skills, and scaling data size with random sampling provides steady gains.

Abstract: Recent work has shown that distilling reasoning traces from a larger teacher
model via supervised finetuning outperforms reinforcement learning with the
smaller student model alone (Guo et al. 2025). However, there has not been a
systematic study of what kind of reasoning demonstrations from the teacher are
most effective in improving the student model's reasoning capabilities. In this
work we curate high-quality "NaturalThoughts" by selecting reasoning traces
from a strong teacher model based on a large pool of questions from
NaturalReasoning (Yuan et al. 2025). We first conduct a systematic analysis of
factors that affect distilling reasoning capabilities, in terms of sample
efficiency and scalability for general reasoning tasks. We observe that simply
scaling up data size with random sampling is a strong baseline with steady
performance gains. Further, we find that selecting difficult examples that
require more diverse reasoning strategies is more sample-efficient to transfer
the teacher model's reasoning skills. Evaluated on both Llama and Qwen models,
training with NaturalThoughts outperforms existing reasoning datasets such as
OpenThoughts, LIMO, etc. on general STEM reasoning benchmarks including
GPQA-Diamond, MMLU-Pro and SuperGPQA.

</details>


### [42] [Decision-oriented Text Evaluation](https://arxiv.org/pdf/2507.01923)
*Yu-Shiang Huang, Chuan-Ju Wang, Chung-Chi Chen*

Main category: cs.CL

TL;DR: The paper proposes a decision-oriented framework for evaluating NLG by measuring its impact on human and LLM decision outcomes, showing traditional metrics are inadequate.


<details>
  <summary>Details</summary>
Motivation: Current intrinsic NLG evaluation methods poorly correlate with real-world decision-making efficacy, necessitating a more practical approach.

Method: A framework assessing decision quality via financial performance of trades by humans and LLMs, using market digest texts as test cases.

Result: Neither humans nor LLMs outperform random baselines with summaries alone, but human-LLM collaboration excels with richer analyses.

Conclusion: Decision-oriented evaluation is crucial for NLG, revealing limitations of traditional metrics and the potential of human-LLM synergy.

Abstract: Natural language generation (NLG) is increasingly deployed in high-stakes
domains, yet common intrinsic evaluation methods, such as n-gram overlap or
sentence plausibility, weakly correlate with actual decision-making efficacy.
We propose a decision-oriented framework for evaluating generated text by
directly measuring its influence on human and large language model (LLM)
decision outcomes. Using market digest texts--including objective morning
summaries and subjective closing-bell analyses--as test cases, we assess
decision quality based on the financial performance of trades executed by human
investors and autonomous LLM agents informed exclusively by these texts. Our
findings reveal that neither humans nor LLM agents consistently surpass random
performance when relying solely on summaries. However, richer analytical
commentaries enable collaborative human-LLM teams to outperform individual
human or agent baselines significantly. Our approach underscores the importance
of evaluating generated text by its ability to facilitate synergistic
decision-making between humans and LLMs, highlighting critical limitations of
traditional intrinsic metrics.

</details>


### [43] [The Thin Line Between Comprehension and Persuasion in LLMs](https://arxiv.org/pdf/2507.01936)
*Adrian de Wynter, Tangming Yuan*

Main category: cs.CL

TL;DR: LLMs excel in persuasive debates but lack deeper comprehension of dialogue structures, impacting their reliability as evaluators.


<details>
  <summary>Details</summary>
Motivation: To assess LLMs' debate skills and their understanding of dialogue, given their increasing use in sensitive applications.

Method: Evaluated LLMs' debate performance and measured their comprehension of dialogical structures and pragmatic context.

Result: LLMs can debate persuasively but fail to demonstrate deeper understanding of dialogue. Awareness of AI involvement increases skepticism.

Conclusion: Persuasive ability in LLMs doesn't require deep comprehension, suggesting pragmatic context is secondary to effectiveness in dialogue.

Abstract: Large language models (LLMs) are excellent at maintaining high-level,
convincing dialogues. They are being fast deployed as chatbots and evaluators
in sensitive areas, such as peer review and mental health applications. This,
along with the disparate accounts on their reasoning capabilities, calls for a
closer examination of LLMs and their comprehension of dialogue. In this work we
begin by evaluating LLMs' ability to maintain a debate--one of the purest yet
most complex forms of human communication. Then we measure how this capability
relates to their understanding of what is being talked about, namely, their
comprehension of dialogical structures and the pragmatic context. We find that
LLMs are capable of maintaining coherent, persuasive debates, often swaying the
beliefs of participants and audiences alike. We also note that awareness or
suspicion of AI involvement encourage people to be more critical of the
arguments made. When polling LLMs on their comprehension of deeper structures
of dialogue, however, they cannot demonstrate said understanding. Our findings
tie the shortcomings of LLMs-as-evaluators to their (in)ability to understand
the context. More broadly, for the field of argumentation theory we posit that,
if an agent can convincingly maintain a dialogue, it is not necessary for it to
know what it is talking about. Hence, the modelling of pragmatic context and
coherence are secondary to effectiveness.

</details>


### [44] [Don't Say No: Jailbreaking LLM by Suppressing Refusal](https://arxiv.org/pdf/2404.16369)
*Yukai Zhou, Jian Lou, Zhijie Huang, Zhan Qin, Yibei Yang, Wenjie Wang*

Main category: cs.CL

TL;DR: The paper introduces DSN, an improved jailbreaking attack for LLMs, combining cosine decay and refusal suppression to achieve higher success rates than baseline methods.


<details>
  <summary>Details</summary>
Motivation: LLMs are vulnerable to jailbreaking attacks that manipulate them into toxic outputs. Current methods rely on predefined behaviors, limiting adaptability.

Method: Proposes DSN attack, enhancing the loss objective with cosine decay and refusal suppression for better performance.

Result: DSN outperforms baselines, achieving state-of-the-art attack success rates (ASR) and showing strong universality and transferability.

Conclusion: DSN is a more effective and adaptable jailbreaking attack for LLMs, demonstrating superior performance and broader applicability.

Abstract: Ensuring the safety alignment of Large Language Models (LLMs) is critical for
generating responses consistent with human values. However, LLMs remain
vulnerable to jailbreaking attacks, where carefully crafted prompts manipulate
them into producing toxic content. One category of such attacks reformulates
the task as an optimization problem, aiming to elicit affirmative responses
from the LLM. However, these methods heavily rely on predefined objectionable
behaviors, limiting their effectiveness and adaptability to diverse harmful
queries. In this study, we first identify why the vanilla target loss is
suboptimal and then propose enhancements to the loss objective. We introduce
DSN (Don't Say No) attack, which combines a cosine decay schedule method with
refusal suppression to achieve higher success rates. Extensive experiments
demonstrate that DSN outperforms baseline attacks and achieves state-of-the-art
attack success rates (ASR). DSN also shows strong universality and
transferability to unseen datasets and black-box models.

</details>


### [45] [Divergent Creativity in Humans and Large Language Models](https://arxiv.org/pdf/2405.13012)
*Antoine Bellemare-Pepin, François Lespinasse, Philipp Thölke, Yann Harel, Kory Mathewson, Jay A. Olson, Yoshua Bengio, Karim Jerbi*

Main category: cs.CL

TL;DR: LLMs show high semantic diversity, surpassing average humans but lagging behind highly creative individuals. A framework evaluates human vs. AI creativity, suggesting improvements for LLMs.


<details>
  <summary>Details</summary>
Motivation: To systematically evaluate LLMs' semantic diversity compared to human divergent thinking, addressing claims of AI's near-human creativity.

Method: Used computational creativity to analyze semantic divergence in top LLMs and 100,000 humans via the Divergent Association Task.

Result: LLMs exceed average human performance but fall short of highly creative individuals. A benchmarking framework compares human and AI creativity.

Conclusion: Current LLMs have a creativity ceiling. Techniques like prompt design can improve their semantic diversity, but human inventive thought remains distinct.

Abstract: The recent surge of Large Language Models (LLMs) has led to claims that they
are approaching a level of creativity akin to human capabilities. This idea has
sparked a blend of excitement and apprehension. However, a critical piece that
has been missing in this discourse is a systematic evaluation of LLMs' semantic
diversity, particularly in comparison to human divergent thinking. To bridge
this gap, we leverage recent advances in computational creativity to analyze
semantic divergence in both state-of-the-art LLMs and a substantial dataset of
100,000 humans. We found evidence that LLMs can surpass average human
performance on the Divergent Association Task, and approach human creative
writing abilities, though they fall short of the typical performance of highly
creative humans. Notably, even the top performing LLMs are still largely
surpassed by highly creative individuals, underscoring a ceiling that current
LLMs still fail to surpass. Our human-machine benchmarking framework addresses
the polemic surrounding the imminent replacement of human creative labour by
AI, disentangling the quality of the respective creative linguistic outputs
using established objective measures. While prompting deeper exploration of the
distinctive elements of human inventive thought compared to those of AI
systems, we lay out a series of techniques to improve their outputs with
respect to semantic diversity, such as prompt design and hyper-parameter
tuning.

</details>


### [46] [Caution for the Environment: Multimodal Agents are Susceptible to Environmental Distractions](https://arxiv.org/pdf/2408.02544)
*Xinbei Ma, Yiting Wang, Yao Yao, Tongxin Yuan, Aston Zhang, Zhuosheng Zhang, Hai Zhao*

Main category: cs.CL

TL;DR: The paper examines if multimodal GUI agents get distracted by unrelated environmental content, finding even top models are susceptible, and suggests improving faithfulness.


<details>
  <summary>Details</summary>
Motivation: To determine if multimodal GUI agents can be distracted by benign but unrelated environmental context, a gap not addressed in current research.

Method: Evaluated various MLLMs as GUI agents using a simulated dataset under three perception levels, including adversarial environment injection.

Result: Even the most powerful models are prone to distractions, highlighting a vulnerability not previously emphasized.

Conclusion: Calls for collective focus on improving agent faithfulness to environmental distractions, beyond just helpfulness.

Abstract: This paper investigates the faithfulness of multimodal large language model
(MLLM) agents in a graphical user interface (GUI) environment, aiming to
address the research question of whether multimodal GUI agents can be
distracted by environmental context. A general scenario is proposed where both
the user and the agent are benign, and the environment, while not malicious,
contains unrelated content. A wide range of MLLMs are evaluated as GUI agents
using a simulated dataset, following three working patterns with different
levels of perception. Experimental results reveal that even the most powerful
models, whether generalist agents or specialist GUI agents, are susceptible to
distractions. While recent studies predominantly focus on the helpfulness of
agents, our findings first indicate that these agents are prone to
environmental distractions. Furthermore, we implement an adversarial
environment injection and analyze the approach to improve faithfulness, calling
for a collective focus on this important topic.

</details>


### [47] [QAEncoder: Towards Aligned Representation Learning in Question Answering Systems](https://arxiv.org/pdf/2409.20434)
*Zhengren Wang, Qinhan Yu, Shida Wei, Zhiyu Li, Feiyu Xiong, Xiaoxing Wang, Simin Niu, Hao Liang, Wentao Zhang*

Main category: cs.CL

TL;DR: QAEncoder is a training-free method to bridge the gap between user queries and documents in QA systems, improving alignment without extra costs.


<details>
  <summary>Details</summary>
Motivation: The gap between user queries and relevant documents in QA systems hinders precise matching, necessitating a solution like QAEncoder.

Method: QAEncoder estimates query expectations in embedding space and uses document fingerprints to distinguish embeddings, requiring no training.

Result: Experiments show QAEncoder effectively aligns queries and documents across datasets, languages, and embedding models with no added costs.

Conclusion: QAEncoder provides a simple, cost-free solution for improving QA system accuracy, avoiding issues like hallucination and forgetting.

Abstract: Modern QA systems entail retrieval-augmented generation (RAG) for accurate
and trustworthy responses. However, the inherent gap between user queries and
relevant documents hinders precise matching. We introduce QAEncoder, a
training-free approach to bridge this gap. Specifically, QAEncoder estimates
the expectation of potential queries in the embedding space as a robust
surrogate for the document embedding, and attaches document fingerprints to
effectively distinguish these embeddings. Extensive experiments across diverse
datasets, languages, and embedding models confirmed QAEncoder's alignment
capability, which offers a simple-yet-effective solution with zero additional
index storage, retrieval latency, training costs, or catastrophic forgetting
and hallucination issues. The repository is publicly available at
https://github.com/IAAR-Shanghai/QAEncoder.

</details>


### [48] [Guaranteed Generation from Large Language Models](https://arxiv.org/pdf/2410.06716)
*Minbeom Kim, Thibaut Thonet, Jos Rozen, Hwaran Lee, Kyomin Jung, Marc Dymetman*

Main category: cs.CL

TL;DR: GUARD combines autoregressive training with rejection sampling to ensure strict constraint satisfaction in LLM-generated text while preserving the original model's distribution.


<details>
  <summary>Details</summary>
Motivation: The need to control LLM outputs to meet specific constraints without altering the model's original distribution.

Method: Proposes GUARD, blending autoregressive proposal distribution with rejection sampling to enforce guarantees.

Result: GUARD achieves perfect constraint satisfaction and maintains the ideal distribution with improved efficiency.

Conclusion: GUARD offers a principled method for strict constraint enforcement in LLMs without compromising generative quality.

Abstract: As large language models (LLMs) are increasingly used across various
applications, there is a growing need to control text generation to satisfy
specific constraints or requirements. This raises a crucial question: Is it
possible to guarantee strict constraint satisfaction in generated outputs while
preserving the distribution of the original model as much as possible? We first
define the ideal distribution - the one closest to the original model, which
also always satisfies the expressed constraint - as the ultimate goal of
guaranteed generation. We then state a fundamental limitation, namely that it
is impossible to reach that goal through autoregressive training alone. This
motivates the necessity of combining training-time and inference-time methods
to enforce such guarantees. Based on this insight, we propose GUARD, a simple
yet effective approach that combines an autoregressive proposal distribution
with rejection sampling. Through GUARD's theoretical properties, we show how
controlling the KL divergence between a specific proposal and the target ideal
distribution simultaneously optimizes inference speed and distributional
closeness. To validate these theoretical concepts, we conduct extensive
experiments on two text generation settings with hard-to-satisfy constraints: a
lexical constraint scenario and a sentiment reversal scenario. These
experiments show that GUARD achieves perfect constraint satisfaction while
almost preserving the ideal distribution with highly improved inference
efficiency. GUARD provides a principled approach to enforcing strict guarantees
for LLMs without compromising their generative capabilities.

</details>


### [49] [A Survey on Uncertainty Quantification of Large Language Models: Taxonomy, Open Research Challenges, and Future Directions](https://arxiv.org/pdf/2412.05563)
*Ola Shorinwa, Zhiting Mei, Justin Lidard, Allen Z. Ren, Anirudha Majumdar*

Main category: cs.CL

TL;DR: A survey reviewing uncertainty quantification methods for large language models (LLMs) to detect hallucinations and non-factual responses, highlighting applications and open research challenges.


<details>
  <summary>Details</summary>
Motivation: Address reliability and trustworthiness concerns of LLMs due to their tendency to generate confident but factually incorrect responses (hallucinations).

Method: Review and taxonomy of existing uncertainty quantification methods for LLMs, analyzing their features, strengths, and weaknesses.

Result: Unified understanding of state-of-the-art methods and their applications in chatbots, text, and robotics.

Conclusion: Identifies open research challenges to motivate future work in improving LLM uncertainty quantification.

Abstract: The remarkable performance of large language models (LLMs) in content
generation, coding, and common-sense reasoning has spurred widespread
integration into many facets of society. However, integration of LLMs raises
valid questions on their reliability and trustworthiness, given their
propensity to generate hallucinations: plausible, factually-incorrect
responses, which are expressed with striking confidence. Previous work has
shown that hallucinations and other non-factual responses generated by LLMs can
be detected by examining the uncertainty of the LLM in its response to the
pertinent prompt, driving significant research efforts devoted to quantifying
the uncertainty of LLMs. This survey seeks to provide an extensive review of
existing uncertainty quantification methods for LLMs, identifying their salient
features, along with their strengths and weaknesses. We present existing
methods within a relevant taxonomy, unifying ostensibly disparate methods to
aid understanding of the state of the art. Furthermore, we highlight
applications of uncertainty quantification methods for LLMs, spanning chatbot
and textual applications to embodied artificial intelligence applications in
robotics. We conclude with open research challenges in uncertainty
quantification of LLMs, seeking to motivate future research.

</details>


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [50] [Geometry-aware 4D Video Generation for Robot Manipulation](https://arxiv.org/pdf/2507.01099)
*Zeyi Liu, Shuang Li, Eric Cousineau, Siyuan Feng, Benjamin Burchfiel, Shuran Song*

Main category: cs.CV

TL;DR: A 4D video generation model is proposed to ensure multi-view 3D consistency in videos, improving robotic planning and interaction by predicting future sequences from novel viewpoints without requiring camera poses.


<details>
  <summary>Details</summary>
Motivation: Enhancing robot planning and interaction in complex environments by understanding and predicting physical world dynamics.

Method: Supervising the model with cross-view pointmap alignment to learn a shared 3D representation, enabling prediction of future video sequences from novel viewpoints using RGB-D observations.

Result: Produces more visually stable and spatially aligned predictions across simulated and real-world datasets, and supports robot manipulation by recovering trajectories.

Conclusion: The method advances video generation for robotics by ensuring geometric consistency and enabling robust manipulation and generalization.

Abstract: Understanding and predicting the dynamics of the physical world can enhance a
robot's ability to plan and interact effectively in complex environments. While
recent video generation models have shown strong potential in modeling dynamic
scenes, generating videos that are both temporally coherent and geometrically
consistent across camera views remains a significant challenge. To address
this, we propose a 4D video generation model that enforces multi-view 3D
consistency of videos by supervising the model with cross-view pointmap
alignment during training. This geometric supervision enables the model to
learn a shared 3D representation of the scene, allowing it to predict future
video sequences from novel viewpoints based solely on the given RGB-D
observations, without requiring camera poses as inputs. Compared to existing
baselines, our method produces more visually stable and spatially aligned
predictions across multiple simulated and real-world robotic datasets. We
further show that the predicted 4D videos can be used to recover robot
end-effector trajectories using an off-the-shelf 6DoF pose tracker, supporting
robust robot manipulation and generalization to novel camera viewpoints.

</details>


### [51] [Landslide Detection and Mapping Using Deep Learning Across Multi-Source Satellite Data and Geographic Regions](https://arxiv.org/pdf/2507.01123)
*Rahul A. Burange, Harsh K. Shinde, Omkar Mutyalwar*

Main category: cs.CV

TL;DR: The paper presents a deep learning and remote sensing-based approach for landslide detection and prediction, using multi-source satellite data and evaluating various models.


<details>
  <summary>Details</summary>
Motivation: Landslides threaten infrastructure, economies, and lives, requiring accurate detection and prediction methods.

Method: Integrates Sentinel-2 and ALOS PALSAR data with deep learning models (U-Net, DeepLabV3+, Res-Net) for landslide identification.

Result: The framework enhances early warning systems and disaster management, showing the potential of deep learning and multi-source remote sensing.

Conclusion: The study highlights the effectiveness of combining deep learning and remote sensing for scalable and transferable landslide prediction models.

Abstract: Landslides pose severe threats to infrastructure, economies, and human lives,
necessitating accurate detection and predictive mapping across diverse
geographic regions. With advancements in deep learning and remote sensing,
automated landslide detection has become increasingly effective. This study
presents a comprehensive approach integrating multi-source satellite imagery
and deep learning models to enhance landslide identification and prediction. We
leverage Sentinel-2 multispectral data and ALOS PALSAR-derived slope and
Digital Elevation Model (DEM) layers to capture critical environmental features
influencing landslide occurrences. Various geospatial analysis techniques are
employed to assess the impact of terra in characteristics, vegetation cover,
and rainfall on detection accuracy. Additionally, we evaluate the performance
of multiple stateof-the-art deep learning segmentation models, including U-Net,
DeepLabV3+, and Res-Net, to determine their effectiveness in landslide
detection. The proposed framework contributes to the development of reliable
early warning systems, improved disaster risk management, and sustainable
land-use planning. Our findings provide valuable insights into the potential of
deep learning and multi-source remote sensing in creating robust, scalable, and
transferable landslide prediction models.

</details>


### [52] [Autoregressive Image Generation with Linear Complexity: A Spatial-Aware Decay Perspective](https://arxiv.org/pdf/2507.01652)
*Yuxin Mao, Zhen Qin, Jinxing Zhou, Hui Deng, Xuyang Shen, Bin Fan, Jing Zhang, Yiran Zhong, Yuchao Dai*

Main category: cs.CV

TL;DR: LASADGen introduces a linear attention mechanism (LASAD) for autoregressive image generation, improving efficiency and quality by preserving 2D spatial relationships.


<details>
  <summary>Details</summary>
Motivation: Existing AR models for image generation use transformers with high computational complexity and memory overhead, while linear attention degrades quality due to poor long-range dependency capture.

Method: Proposes LASAD, a linear attention mechanism with spatial-aware decay, and LASADGen, an autoregressive generator using it for efficient, high-quality image generation.

Result: LASADGen achieves state-of-the-art performance and efficiency on ImageNet, balancing computational cost and spatial understanding.

Conclusion: LASADGen bridges the gap between efficiency and quality in autoregressive image generation by leveraging spatial-aware linear attention.

Abstract: Autoregressive (AR) models have garnered significant attention in image
generation for their ability to effectively capture both local and global
structures within visual data. However, prevalent AR models predominantly rely
on the transformer architectures, which are beset by quadratic computational
complexity concerning input sequence length and substantial memory overhead due
to the necessity of maintaining key-value caches. Although linear attention
mechanisms have successfully reduced this burden in language models, our
initial experiments reveal that they significantly degrade image generation
quality because of their inability to capture critical long-range dependencies
in visual data. We propose Linear Attention with Spatial-Aware Decay (LASAD), a
novel attention mechanism that explicitly preserves genuine 2D spatial
relationships within the flattened image sequences by computing
position-dependent decay factors based on true 2D spatial location rather than
1D sequence positions. Based on this mechanism, we present LASADGen, an
autoregressive image generator that enables selective attention to relevant
spatial contexts with linear complexity. Experiments on ImageNet show LASADGen
achieves state-of-the-art image generation performance and computational
efficiency, bridging the gap between linear attention's efficiency and spatial
understanding needed for high-quality generation.

</details>


### [53] [cp_measure: API-first feature extraction for image-based profiling workflows](https://arxiv.org/pdf/2507.01163)
*Alán F. Muñoz, Tim Treis, Alexandr A. Kalinin, Shatavisha Dasgupta, Fabian Theis, Anne E. Carpenter, Shantanu Singh*

Main category: cs.CV

TL;DR: The paper introduces cp_measure, a Python library for modular, API-first feature extraction from biological images, enhancing reproducibility and integration with machine learning workflows.


<details>
  <summary>Details</summary>
Motivation: Current tools like CellProfiler hinder automated and reproducible analyses for image-based profiling, limiting machine learning applications in computational biology.

Method: Develop cp_measure, a Python library extracting CellProfiler's core measurement capabilities for programmatic feature extraction.

Result: cp_measure retains high fidelity with CellProfiler features and integrates seamlessly with the scientific Python ecosystem.

Conclusion: cp_measure enables scalable, reproducible, and automated image-based profiling pipelines for machine learning in computational biology.

Abstract: Biological image analysis has traditionally focused on measuring specific
visual properties of interest for cells or other entities. A complementary
paradigm gaining increasing traction is image-based profiling - quantifying
many distinct visual features to form comprehensive profiles which may reveal
hidden patterns in cellular states, drug responses, and disease mechanisms.
While current tools like CellProfiler can generate these feature sets, they
pose significant barriers to automated and reproducible analyses, hindering
machine learning workflows. Here we introduce cp_measure, a Python library that
extracts CellProfiler's core measurement capabilities into a modular, API-first
tool designed for programmatic feature extraction. We demonstrate that
cp_measure features retain high fidelity with CellProfiler features while
enabling seamless integration with the scientific Python ecosystem. Through
applications to 3D astrocyte imaging and spatial transcriptomics, we showcase
how cp_measure enables reproducible, automated image-based profiling pipelines
that scale effectively for machine learning applications in computational
biology.

</details>


### [54] [Rapid Salient Object Detection with Difference Convolutional Neural Networks](https://arxiv.org/pdf/2507.01182)
*Zhuo Su, Li Liu, Matthias Müller, Jiehua Zhang, Diana Wofk, Ming-Ming Cheng, Matti Pietikäinen*

Main category: cs.CV

TL;DR: Proposes efficient networks (SDNet and STDNet) for salient object detection (SOD) on resource-constrained devices, achieving real-time performance with superior accuracy.


<details>
  <summary>Details</summary>
Motivation: Address the challenge of deploying SOD on resource-constrained devices due to the computational expense of existing deep neural network models.

Method: Combines traditional SOD wisdom with CNNs using Pixel Difference Convolutions (PDCs) and introduces Difference Convolution Reparameterization (DCR) for efficiency. Extends to video SOD with SpatioTemporal Difference Convolution (STDC).

Result: Models achieve 46 FPS (images) and 150 FPS (videos) on Jetson Orin with <1M parameters, outperforming lightweight models in speed and accuracy.

Conclusion: The proposed networks offer a significant efficiency-accuracy trade-off, enabling real-time SOD on constrained devices.

Abstract: This paper addresses the challenge of deploying salient object detection
(SOD) on resource-constrained devices with real-time performance. While recent
advances in deep neural networks have improved SOD, existing top-leading models
are computationally expensive. We propose an efficient network design that
combines traditional wisdom on SOD and the representation power of modern CNNs.
Like biologically-inspired classical SOD methods relying on computing contrast
cues to determine saliency of image regions, our model leverages Pixel
Difference Convolutions (PDCs) to encode the feature contrasts. Differently,
PDCs are incorporated in a CNN architecture so that the valuable contrast cues
are extracted from rich feature maps. For efficiency, we introduce a difference
convolution reparameterization (DCR) strategy that embeds PDCs into standard
convolutions, eliminating computation and parameters at inference.
Additionally, we introduce SpatioTemporal Difference Convolution (STDC) for
video SOD, enhancing the standard 3D convolution with spatiotemporal contrast
capture. Our models, SDNet for image SOD and STDNet for video SOD, achieve
significant improvements in efficiency-accuracy trade-offs. On a Jetson Orin
device, our models with $<$ 1M parameters operate at 46 FPS and 150 FPS on
streamed images and videos, surpassing the second-best lightweight models in
our experiments by more than $2\times$ and $3\times$ in speed with superior
accuracy. Code will be available at https://github.com/hellozhuo/stdnet.git.

</details>


### [55] [HCNQA: Enhancing 3D VQA with Hierarchical Concentration Narrowing Supervision](https://arxiv.org/pdf/2507.01800)
*Shengli Zhou, Jianuo Zhu, Qilin Huang, Fangjing Wang, Yanfu Zhang, Feng Zheng*

Main category: cs.CV

TL;DR: The paper introduces HCNQA, a 3D VQA model using hierarchical concentration narrowing supervision to ensure rational reasoning pathways, outperforming answer-centric methods.


<details>
  <summary>Details</summary>
Motivation: Addressing the limitations of answer-centric supervision in 3D VQA, which lacks reasoning pathway oversight and risks superficial shortcuts, and underthinking in slow-thinking methods.

Method: Proposes hierarchical concentration narrowing supervision, mimicking human focus progression, to guide models through three reasoning phases with checkpoint supervision.

Result: Demonstrates improved performance and rational reasoning pathway development in 3D VQA tasks.

Conclusion: HCNQA effectively ensures rational reasoning and outperforms existing methods, with code publicly available.

Abstract: 3D Visual Question-Answering (3D VQA) is pivotal for models to perceive the
physical world and perform spatial reasoning. Answer-centric supervision is a
commonly used training method for 3D VQA models. Many models that utilize this
strategy have achieved promising results in 3D VQA tasks. However, the
answer-centric approach only supervises the final output of models and allows
models to develop reasoning pathways freely. The absence of supervision on the
reasoning pathway enables the potential for developing superficial shortcuts
through common patterns in question-answer pairs. Moreover, although
slow-thinking methods advance large language models, they suffer from
underthinking. To address these issues, we propose \textbf{HCNQA}, a 3D VQA
model leveraging a hierarchical concentration narrowing supervision method. By
mimicking the human process of gradually focusing from a broad area to specific
objects while searching for answers, our method guides the model to perform
three phases of concentration narrowing through hierarchical supervision. By
supervising key checkpoints on a general reasoning pathway, our method can
ensure the development of a rational and effective reasoning pathway. Extensive
experimental results demonstrate that our method can effectively ensure that
the model develops a rational reasoning pathway and performs better. The code
is available at https://github.com/JianuoZhu/HCNQA.

</details>


### [56] [Robust Brain Tumor Segmentation with Incomplete MRI Modalities Using Hölder Divergence and Mutual Information-Enhanced Knowledge Transfer](https://arxiv.org/pdf/2507.01254)
*Runze Cheng, Xihang Qiu, Ming Li, Ye Zhang, Chun Li, Fei Yu*

Main category: cs.CV

TL;DR: A robust single-modality parallel processing framework for brain tumor segmentation, effective even with missing MRI modalities, outperforming existing methods.


<details>
  <summary>Details</summary>
Motivation: Conventional methods fail when MRI modalities are missing due to various issues, necessitating a solution for accurate segmentation with incomplete data.

Method: Uses Holder divergence and mutual information to maintain modality-specific features and dynamically adjust network parameters based on available inputs.

Result: Achieves high segmentation accuracy on BraTS 2018 and 2020 datasets, outperforming existing methods with missing modalities.

Conclusion: The proposed framework is robust and effective for brain tumor segmentation, even with incomplete MRI data.

Abstract: Multimodal MRI provides critical complementary information for accurate brain
tumor segmentation. However, conventional methods struggle when certain
modalities are missing due to issues such as image quality, protocol
inconsistencies, patient allergies, or financial constraints. To address this,
we propose a robust single-modality parallel processing framework that achieves
high segmentation accuracy even with incomplete modalities. Leveraging Holder
divergence and mutual information, our model maintains modality-specific
features while dynamically adjusting network parameters based on the available
inputs. By using these divergence- and information-based loss functions, the
framework effectively quantifies discrepancies between predictions and
ground-truth labels, resulting in consistently accurate segmentation. Extensive
evaluations on the BraTS 2018 and BraTS 2020 datasets demonstrate superior
performance over existing methods in handling missing modalities.

</details>


### [57] [DreamCinema: Cinematic Transfer with Free Camera and 3D Character](https://arxiv.org/pdf/2408.12601)
*Weiliang Chen, Fangfu Liu, Diankun Wu, Haowen Sun, Jiwen Lu, Yueqi Duan*

Main category: cs.CV

TL;DR: Dream-Cinema is a framework for 3D film creation using generative models, focusing on cinematic elements like camera movement and 3D characters to improve video quality.


<details>
  <summary>Details</summary>
Motivation: Current video generation methods lack cinematic quality and 3D consistency, hindering immersive experiences.

Method: Decomposes film creation into 3D character, motion, camera movement, and environment, using generative models and optimization techniques.

Result: Generates high-quality films with free camera movement and consistent 3D characters.

Conclusion: Dream-Cinema offers a user-friendly, 3D-based solution for cinematic film creation.

Abstract: We are living in a flourishing era of digital media, where everyone has the
potential to become a personal filmmaker. Current research on video generation
suggests a promising avenue for controllable film creation in pixel space using
Diffusion models. However, the reliance on overly verbose prompts and
insufficient focus on cinematic elements (e.g., camera movement) results in
videos that lack cinematic quality. Furthermore, the absence of 3D modeling
often leads to failures in video generation, such as inconsistent character
models at different frames, ultimately hindering the immersive experience for
viewers. In this paper, we propose a new framework for film creation,
Dream-Cinema, which is designed for user-friendly, 3D space-based film creation
with generative models. Specifically, we decompose 3D film creation into four
key elements: 3D character, driven motion, camera movement, and environment. We
extract the latter three elements from user-specified film shots and generate
the 3D character using a generative model based on a provided image. To
seamlessly recombine these elements and ensure smooth film creation, we propose
structure-guided character animation, shape-aware camera movement optimization,
and environment-aware generative refinement. Extensive experiments demonstrate
the effectiveness of our method in generating high-quality films with free
camera and 3D characters.

</details>


### [58] [AIGVE-MACS: Unified Multi-Aspect Commenting and Scoring Model for AI-Generated Video Evaluation](https://arxiv.org/pdf/2507.01255)
*Xiao Liu, Jiawei Zhang*

Main category: cs.CV

TL;DR: AIGVE-MACS is a new model for evaluating AI-generated videos, providing both numerical scores and detailed comments. It outperforms existing methods and includes a benchmark dataset and refinement framework.


<details>
  <summary>Details</summary>
Motivation: Existing evaluation metrics for AI-generated videos lack interpretability and alignment with human judgment.

Method: AIGVE-MACS uses Vision-Language Models with token-wise weighted loss and dynamic frame sampling, trained on AIGVE-BENCH 2, a large annotated dataset.

Result: The model achieves state-of-the-art performance in scoring and comment quality, with a 53.5% improvement in video generation quality through iterative refinement.

Conclusion: AIGVE-MACS sets a new standard for human-aligned evaluation of AI-generated videos, with released benchmarks and models.

Abstract: The rapid advancement of AI-generated video models has created a pressing
need for robust and interpretable evaluation frameworks. Existing metrics are
limited to producing numerical scores without explanatory comments, resulting
in low interpretability and human evaluation alignment. To address those
challenges, we introduce AIGVE-MACS, a unified model for AI-Generated Video
Evaluation(AIGVE), which can provide not only numerical scores but also
multi-aspect language comment feedback in evaluating these generated videos.
Central to our approach is AIGVE-BENCH 2, a large-scale benchmark comprising
2,500 AI-generated videos and 22,500 human-annotated detailed comments and
numerical scores across nine critical evaluation aspects. Leveraging
AIGVE-BENCH 2, AIGVE-MACS incorporates recent Vision-Language Models with a
novel token-wise weighted loss and a dynamic frame sampling strategy to better
align with human evaluators. Comprehensive experiments across supervised and
zero-shot benchmarks demonstrate that AIGVE-MACS achieves state-of-the-art
performance in both scoring correlation and comment quality, significantly
outperforming prior baselines including GPT-4o and VideoScore. In addition, we
further showcase a multi-agent refinement framework where feedback from
AIGVE-MACS drives iterative improvements in video generation, leading to 53.5%
quality enhancement. This work establishes a new paradigm for comprehensive,
human-aligned evaluation of AI-generated videos. We release the AIGVE-BENCH 2
and AIGVE-MACS at https://huggingface.co/xiaoliux/AIGVE-MACS.

</details>


### [59] [Advancements in Weed Mapping: A Systematic Review](https://arxiv.org/pdf/2507.01269)
*Mohammad Jahanbakht, Alex Olsen, Ross Marchant, Emilie Fillols, Mostafa Rahimi Azghadi*

Main category: cs.CV

TL;DR: This paper reviews weed mapping methods, focusing on data acquisition, processing, and mapping techniques to improve precision management and sustainability.


<details>
  <summary>Details</summary>
Motivation: The lack of comprehensive reviews on weed mapping, especially covering the entire pipeline from data to tools, hinders progress in the field.

Method: The review systematically examines state-of-the-art methods in data acquisition (sensors, platforms), processing (annotation, modeling), and mapping (spatiotemporal analysis, decision tools) following PRISMA guidelines.

Result: The synthesis provides a holistic understanding of weed mapping, highlighting advancements in sensors, analytics, and machine learning for better spatial-temporal resolution.

Conclusion: This review serves as a foundational reference for future research, aiming to develop efficient, scalable, and sustainable weed management systems.

Abstract: Weed mapping plays a critical role in precision management by providing
accurate and timely data on weed distribution, enabling targeted control and
reduced herbicide use. This minimizes environmental impacts, supports
sustainable land management, and improves outcomes across agricultural and
natural environments. Recent advances in weed mapping leverage ground-vehicle
Red Green Blue (RGB) cameras, satellite and drone-based remote sensing combined
with sensors such as spectral, Near Infra-Red (NIR), and thermal cameras. The
resulting data are processed using advanced techniques including big data
analytics and machine learning, significantly improving the spatial and
temporal resolution of weed maps and enabling site-specific management
decisions. Despite a growing body of research in this domain, there is a lack
of comprehensive literature reviews specifically focused on weed mapping. In
particular, the absence of a structured analysis spanning the entire mapping
pipeline, from data acquisition to processing techniques and mapping tools,
limits progress in the field. This review addresses these gaps by
systematically examining state-of-the-art methods in data acquisition (sensor
and platform technologies), data processing (including annotation and
modelling), and mapping techniques (such as spatiotemporal analysis and
decision support tools). Following PRISMA guidelines, we critically evaluate
and synthesize key findings from the literature to provide a holistic
understanding of the weed mapping landscape. This review serves as a
foundational reference to guide future research and support the development of
efficient, scalable, and sustainable weed management systems.

</details>


### [60] [Frequency Domain-Based Diffusion Model for Unpaired Image Dehazing](https://arxiv.org/pdf/2507.01275)
*Chengxu Liu, Lu Qi, Jinshan Pan, Xueming Qian, Ming-Hsuan Yang*

Main category: cs.CV

TL;DR: A novel frequency domain-based diffusion model (\ours) is proposed for unpaired image dehazing, leveraging amplitude spectrum reconstruction and phase correction to outperform existing methods.


<details>
  <summary>Details</summary>
Motivation: Existing contrastive learning methods for unpaired image dehazing introduce irrelevant content and ignore haze-specific frequency domain properties, particularly amplitude spectrum degradation.

Method: The proposed model uses a Diffusion Model (DM) for frequency domain reconstruction, an Amplitude Residual Encoder (ARE) to bridge the amplitude gap, and a Phase Correction Module (PCM) to refine the phase spectrum.

Result: The model achieves superior performance on synthetic and real-world datasets compared to state-of-the-art methods.

Conclusion: The frequency domain approach with DMs, ARE, and PCM effectively addresses unpaired image dehazing, demonstrating significant improvements over existing techniques.

Abstract: Unpaired image dehazing has attracted increasing attention due to its
flexible data requirements during model training. Dominant methods based on
contrastive learning not only introduce haze-unrelated content information, but
also ignore haze-specific properties in the frequency domain (\ie,~haze-related
degradation is mainly manifested in the amplitude spectrum). To address these
issues, we propose a novel frequency domain-based diffusion model, named \ours,
for fully exploiting the beneficial knowledge in unpaired clear data. In
particular, inspired by the strong generative ability shown by Diffusion Models
(DMs), we tackle the dehazing task from the perspective of frequency domain
reconstruction and perform the DMs to yield the amplitude spectrum consistent
with the distribution of clear images. To implement it, we propose an Amplitude
Residual Encoder (ARE) to extract the amplitude residuals, which effectively
compensates for the amplitude gap from the hazy to clear domains, as well as
provide supervision for the DMs training. In addition, we propose a Phase
Correction Module (PCM) to eliminate artifacts by further refining the phase
spectrum during dehazing with a simple attention mechanism. Experimental
results demonstrate that our \ours outperforms other state-of-the-art methods
on both synthetic and real-world datasets.

</details>


### [61] [Learning an Ensemble Token from Task-driven Priors in Facial Analysis](https://arxiv.org/pdf/2507.01290)
*Sunyong Seo, Semin Kim, Jongha Lee*

Main category: cs.CV

TL;DR: ET-Fuser introduces a novel method for unified feature representation in facial analysis using ensemble tokens and attention mechanisms, improving performance with minimal computational cost.


<details>
  <summary>Details</summary>
Motivation: Existing methods lack unified feature representation in single-task learning for facial analysis, despite advancements in CNNs and ViTs.

Method: ET-Fuser leverages attention mechanisms and task priors from pre-trained models to generate ensemble tokens, sharing mutual information across encoders.

Result: The method shows statistically significant improvements in feature representations across various facial analysis tasks.

Conclusion: ET-Fuser efficiently enhances facial analysis performance by unifying feature representation with negligible computational overhead.

Abstract: Facial analysis exhibits task-specific feature variations. While
Convolutional Neural Networks (CNNs) have enabled the fine-grained
representation of spatial information, Vision Transformers (ViTs) have
facilitated the representation of semantic information at the patch level.
Although the generalization of conventional methodologies has advanced visual
interpretability, there remains paucity of research that preserves the unified
feature representation on single task learning during the training process. In
this work, we introduce ET-Fuser, a novel methodology for learning ensemble
token by leveraging attention mechanisms based on task priors derived from
pre-trained models for facial analysis. Specifically, we propose a robust prior
unification learning method that generates a ensemble token within a
self-attention mechanism, which shares the mutual information along the
pre-trained encoders. This ensemble token approach offers high efficiency with
negligible computational cost. Our results show improvements across a variety
of facial analysis, with statistically significant enhancements observed in the
feature representations.

</details>


### [62] [DiffusionLight-Turbo: Accelerated Light Probes for Free via Single-Pass Chrome Ball Inpainting](https://arxiv.org/pdf/2507.01305)
*Worameth Chinchuthakun, Pakkapon Phongthawee, Amit Raj, Varun Jampani, Pramook Khungurn, Supasorn Suwajanakorn*

Main category: cs.CV

TL;DR: A technique called DiffusionLight estimates lighting from a single LDR image by reframing it as a chrome ball inpainting problem, using Stable Diffusion XL. It addresses challenges like inconsistent outputs and slow runtime with iterative inpainting and Turbo LoRA for speedup.


<details>
  <summary>Details</summary>
Motivation: Existing methods rely on limited HDR panorama datasets, leading to generalization failures. The goal is to leverage diffusion models for better lighting estimation.

Method: Uses iterative inpainting to compute a median chrome ball as a stable prior, fine-tunes an Exposure LoRA for HDR light probes, and introduces DiffusionLight-Turbo for faster runtime.

Result: Produces convincing light estimates across diverse settings with superior generalization. Runtime is reduced from 30 minutes to 30 seconds.

Conclusion: DiffusionLight and its Turbo variant offer effective, scalable solutions for lighting estimation from LDR images, balancing quality and speed.

Abstract: We introduce a simple yet effective technique for estimating lighting from a
single low-dynamic-range (LDR) image by reframing the task as a chrome ball
inpainting problem. This approach leverages a pre-trained diffusion model,
Stable Diffusion XL, to overcome the generalization failures of existing
methods that rely on limited HDR panorama datasets. While conceptually simple,
the task remains challenging because diffusion models often insert incorrect or
inconsistent content and cannot readily generate chrome balls in HDR format.
Our analysis reveals that the inpainting process is highly sensitive to the
initial noise in the diffusion process, occasionally resulting in unrealistic
outputs. To address this, we first introduce DiffusionLight, which uses
iterative inpainting to compute a median chrome ball from multiple outputs to
serve as a stable, low-frequency lighting prior that guides the generation of a
high-quality final result. To generate high-dynamic-range (HDR) light probes,
an Exposure LoRA is fine-tuned to create LDR images at multiple exposure
values, which are then merged. While effective, DiffusionLight is
time-intensive, requiring approximately 30 minutes per estimation. To reduce
this overhead, we introduce DiffusionLight-Turbo, which reduces the runtime to
about 30 seconds with minimal quality loss. This 60x speedup is achieved by
training a Turbo LoRA to directly predict the averaged chrome balls from the
iterative process. Inference is further streamlined into a single denoising
pass using a LoRA swapping technique. Experimental results that show our method
produces convincing light estimates across diverse settings and demonstrates
superior generalization to in-the-wild scenarios. Our code is available at
https://diffusionlight.github.io/turbo

</details>


### [63] [Physics-informed Ground Reaction Dynamics from Human Motion Capture](https://arxiv.org/pdf/2507.01340)
*Cuong Le, Huy-Phuong Le, Duc Le, Minh-Thien Duong, Van-Binh Nguyen, My-Ha Le*

Main category: cs.CV

TL;DR: A novel method estimates human ground reaction dynamics from motion capture data using physics laws and computational simulation, outperforming baseline models in accuracy.


<details>
  <summary>Details</summary>
Motivation: Force plates, used for measuring body dynamics, are limited to lab setups. The paper aims to overcome this by estimating dynamics directly from motion capture data.

Method: The method uses Euler's integration and PD algorithm to compute ground reaction forces from motion capture data, incorporating physics laws for accuracy.

Result: The approach outperforms baseline models in ground reaction force estimation accuracy and simulated root trajectory precision on the GroundLink dataset.

Conclusion: The proposed physics-based method provides a reliable alternative to force plates, enhancing human motion analysis.

Abstract: Body dynamics are crucial information for the analysis of human motions in
important research fields, ranging from biomechanics, sports science to
computer vision and graphics. Modern approaches collect the body dynamics,
external reactive force specifically, via force plates, synchronizing with
human motion capture data, and learn to estimate the dynamics from a black-box
deep learning model. Being specialized devices, force plates can only be
installed in laboratory setups, imposing a significant limitation on the
learning of human dynamics. To this end, we propose a novel method for
estimating human ground reaction dynamics directly from the more reliable
motion capture data with physics laws and computational simulation as
constrains. We introduce a highly accurate and robust method for computing
ground reaction forces from motion capture data using Euler's integration
scheme and PD algorithm. The physics-based reactive forces are used to inform
the learning model about the physics-informed motion dynamics thus improving
the estimation accuracy. The proposed approach was tested on the GroundLink
dataset, outperforming the baseline model on: 1) the ground reaction force
estimation accuracy compared to the force plates measurement; and 2) our
simulated root trajectory precision. The implementation code is available at
https://github.com/cuongle1206/Phys-GRD

</details>


### [64] [Learning Camera-Agnostic White-Balance Preferences](https://arxiv.org/pdf/2507.01342)
*Luxi Zhao, Mahmoud Afifi, Michael S. Brown*

Main category: cs.CV

TL;DR: The paper introduces a lightweight method for transforming neutral white balance corrections into aesthetically preferred ones, ensuring consistency across different cameras.


<details>
  <summary>Details</summary>
Motivation: Commercial AWB systems prioritize aesthetics over accuracy, and learning-based methods struggle with generalization across camera sensors. This work addresses aesthetic consistency in AWB.

Method: The authors propose a post-illuminant-estimation mapping that transforms neutral corrections into aesthetic ones in a camera-agnostic space. The model is lightweight (~500 parameters) and efficient.

Result: The method achieves state-of-the-art performance on a dataset of 771 images from three cameras, with minimal computational overhead (0.024 ms runtime).

Conclusion: The approach enables consistent and stylized color rendering across unseen cameras while remaining compatible with existing cross-camera AWB techniques.

Abstract: The image signal processor (ISP) pipeline in modern cameras consists of
several modules that transform raw sensor data into visually pleasing images in
a display color space. Among these, the auto white balance (AWB) module is
essential for compensating for scene illumination. However, commercial AWB
systems often strive to compute aesthetic white-balance preferences rather than
accurate neutral color correction. While learning-based methods have improved
AWB accuracy, they typically struggle to generalize across different camera
sensors -- an issue for smartphones with multiple cameras. Recent work has
explored cross-camera AWB, but most methods remain focused on achieving neutral
white balance. In contrast, this paper is the first to address aesthetic
consistency by learning a post-illuminant-estimation mapping that transforms
neutral illuminant corrections into aesthetically preferred corrections in a
camera-agnostic space. Once trained, our mapping can be applied after any
neutral AWB module to enable consistent and stylized color rendering across
unseen cameras. Our proposed model is lightweight -- containing only $\sim$500
parameters -- and runs in just 0.024 milliseconds on a typical flagship mobile
CPU. Evaluated on a dataset of 771 smartphone images from three different
cameras, our method achieves state-of-the-art performance while remaining fully
compatible with existing cross-camera AWB techniques, introducing minimal
computational and memory overhead.

</details>


### [65] [Learning from Random Subspace Exploration: Generalized Test-Time Augmentation with Self-supervised Distillation](https://arxiv.org/pdf/2507.01347)
*Andrei Jelea, Ahmed Nabil Belbachir, Marius Leordeanu*

Main category: cs.CV

TL;DR: GTTA is a versatile Test-Time Augmentation method for improving model performance across vision and non-vision tasks by perturbing PCA subspace projections and using self-supervised learning.


<details>
  <summary>Details</summary>
Motivation: To create a general, off-the-shelf method for enhancing model performance across diverse tasks by addressing structural and systematic noise in data.

Method: GTTA applies random perturbations to PCA subspace projections of test inputs, forms robust ensembles, and includes a self-supervised learning stage to refine the initial model.

Result: GTTA outperforms existing TTA methods and SoTA models in tasks like image classification, segmentation, speech recognition, and house price prediction.

Conclusion: GTTA is a highly effective, general-purpose method validated across various tasks, including specialized applications like salmon detection in underwater videos.

Abstract: We introduce Generalized Test-Time Augmentation (GTTA), a highly effective
method for improving the performance of a trained model, which unlike other
existing Test-Time Augmentation approaches from the literature is general
enough to be used off-the-shelf for many vision and non-vision tasks, such as
classification, regression, image segmentation and object detection. By
applying a new general data transformation, that randomly perturbs multiple
times the PCA subspace projection of a test input, GTTA forms robust ensembles
at test time in which, due to sound statistical properties, the structural and
systematic noises in the initial input data is filtered out and final estimator
errors are reduced. Different from other existing methods, we also propose a
final self-supervised learning stage in which the ensemble output, acting as an
unsupervised teacher, is used to train the initial single student model, thus
reducing significantly the test time computational cost, at no loss in
accuracy. Our tests and comparisons to strong TTA approaches and SoTA models on
various vision and non-vision well-known datasets and tasks, such as image
classification and segmentation, speech recognition and house price prediction,
validate the generality of the proposed GTTA. Furthermore, we also prove its
effectiveness on the more specific real-world task of salmon segmentation and
detection in low-visibility underwater videos, for which we introduce
DeepSalmon, the largest dataset of its kind in the literature.

</details>


### [66] [DiffMark: Diffusion-based Robust Watermark Against Deepfakes](https://arxiv.org/pdf/2507.01428)
*Chen Sun, Haiyang Sun, Zhiqing Guo, Yunfeng Diao, Liejun Wang, Dan Ma, Gaobo Yang, Keqin Li*

Main category: cs.CV

TL;DR: DiffMark, a diffusion model-based watermarking framework, enhances robustness against Deepfake manipulations by integrating facial and watermark conditions, a cross information fusion module, and adversarial guidance.


<details>
  <summary>Details</summary>
Motivation: Deepfakes threaten security and privacy, but existing watermarking methods lack robustness against such manipulations.

Method: DiffMark modifies diffusion model training and sampling, using facial and watermark conditions, a CIF module, and adversarial guidance to generate robust watermarked images.

Result: Experiments show DiffMark effectively resists typical Deepfake manipulations.

Conclusion: DiffMark offers a robust solution for watermarking against Deepfakes, with potential for broader security applications.

Abstract: Deepfakes pose significant security and privacy threats through malicious
facial manipulations. While robust watermarking can aid in authenticity
verification and source tracking, existing methods often lack the sufficient
robustness against Deepfake manipulations. Diffusion models have demonstrated
remarkable performance in image generation, enabling the seamless fusion of
watermark with image during generation. In this study, we propose a novel
robust watermarking framework based on diffusion model, called DiffMark. By
modifying the training and sampling scheme, we take the facial image and
watermark as conditions to guide the diffusion model to progressively denoise
and generate corresponding watermarked image. In the construction of facial
condition, we weight the facial image by a timestep-dependent factor that
gradually reduces the guidance intensity with the decrease of noise, thus
better adapting to the sampling process of diffusion model. To achieve the
fusion of watermark condition, we introduce a cross information fusion (CIF)
module that leverages a learnable embedding table to adaptively extract
watermark features and integrates them with image features via cross-attention.
To enhance the robustness of the watermark against Deepfake manipulations, we
integrate a frozen autoencoder during training phase to simulate Deepfake
manipulations. Additionally, we introduce Deepfake-resistant guidance that
employs specific Deepfake model to adversarially guide the diffusion sampling
process to generate more robust watermarked images. Experimental results
demonstrate the effectiveness of the proposed DiffMark on typical Deepfakes.
Our code will be available at https://github.com/vpsg-research/DiffMark.

</details>


### [67] [Long-Tailed Distribution-Aware Router For Mixture-of-Experts in Large Vision-Language Model](https://arxiv.org/pdf/2507.01351)
*Chaoxiang Cai, Longrong Yang, Kaibing Chen, Fan Yang, Xi Li*

Main category: cs.CV

TL;DR: The paper introduces LTDR, a Long-Tailed Distribution-aware Router for vision-language token-to-expert routing, addressing modality-specific distribution differences and enhancing expert activation for vision tail tokens.


<details>
  <summary>Details</summary>
Motivation: Existing MoE frameworks for LVLMs overlook the distributional differences between vision and language tokens, leading to suboptimal routing.

Method: Proposes LTDR with a distribution-aware router for modality-specific routing and an oversampling-like strategy for vision tail tokens.

Result: Experiments on benchmarks show LTDR's effectiveness in handling vision-language TER.

Conclusion: LTDR improves routing in MoE frameworks by addressing modality-specific distribution challenges and enhancing expert activation for vision tail tokens.

Abstract: The mixture-of-experts (MoE), which replaces dense models with sparse
architectures, has gained attention in large vision-language models (LVLMs) for
achieving comparable performance with fewer activated parameters. Existing MoE
frameworks for LVLMs focus on token-to-expert routing (TER), encouraging
different experts to specialize in processing distinct tokens. However, these
frameworks often rely on the load balancing mechanism, overlooking the inherent
distributional differences between vision and language. To this end, we propose
a Long-Tailed Distribution-aware Router (LTDR) for vision-language TER,
tackling two challenges: (1) Distribution-aware router for modality-specific
routing. We observe that language TER follows a uniform distribution, whereas
vision TER exhibits a long-tailed distribution. This discrepancy necessitates
distinct routing strategies tailored to each modality. (2) Enhancing expert
activation for vision tail tokens. Recognizing the importance of vision tail
tokens, we introduce an oversampling-like strategy by increasing the number of
activated experts for these tokens. Experiments on extensive benchmarks
validate the effectiveness of our approach.

</details>


### [68] [Towards Controllable Real Image Denoising with Camera Parameters](https://arxiv.org/pdf/2507.01587)
*Youngjin Oh, Junhyeong Kwon, Keuntek Lee, Nam Ik Cho*

Main category: cs.CV

TL;DR: A controllable denoising framework adapts to noise levels using camera parameters (ISO, shutter speed, F-number) to enhance denoising performance.


<details>
  <summary>Details</summary>
Motivation: Existing deep learning-based denoising methods lack flexibility in adjusting denoising strength based on noise levels, camera settings, and user preferences.

Method: The framework converts camera parameters (ISO, shutter speed, F-number) into a vector to control and enhance a denoising network.

Result: The method adds controllability to standard denoising networks and improves their performance.

Conclusion: The proposed framework effectively adapts denoising strength using camera parameters, offering improved flexibility and performance.

Abstract: Recent deep learning-based image denoising methods have shown impressive
performance; however, many lack the flexibility to adjust the denoising
strength based on the noise levels, camera settings, and user preferences. In
this paper, we introduce a new controllable denoising framework that adaptively
removes noise from images by utilizing information from camera parameters.
Specifically, we focus on ISO, shutter speed, and F-number, which are closely
related to noise levels. We convert these selected parameters into a vector to
control and enhance the performance of the denoising network. Experimental
results show that our method seamlessly adds controllability to standard
denoising neural networks and improves their performance. Code is available at
https://github.com/OBAKSA/CPADNet.

</details>


### [69] [3D Gaussian Splatting Driven Multi-View Robust Physical Adversarial Camouflage Generation](https://arxiv.org/pdf/2507.01367)
*Tianrui Lou, Xiaojun Jia, Siyuan Liang, Jiawei Liang, Ming Zhang, Yanjun Xiao, Xiaochun Cao*

Main category: cs.CV

TL;DR: PGA is a physical attack framework using 3D Gaussian Splatting for robust adversarial camouflage, addressing limitations of prior methods by improving cross-view robustness and adversarial effectiveness.


<details>
  <summary>Details</summary>
Motivation: Prior camouflage-based physical attacks rely on mesh priors and simulators, which are time-consuming and lack real-world accuracy. They also struggle with multi-view robustness and sub-optimal solutions.

Method: PGA uses 3D Gaussian Splatting (3DGS) for rapid, precise reconstruction with few images. It prevents occlusion among Gaussians and employs min-max optimization to adjust imaging backgrounds, filtering non-robust features.

Result: Extensive experiments show PGA's effectiveness and superiority in adversarial attacks, outperforming prior methods in robustness and adversarial impact.

Conclusion: PGA offers a practical, efficient solution for physical adversarial attacks, enhancing robustness across diverse viewpoints and environments.

Abstract: Physical adversarial attack methods expose the vulnerabilities of deep neural
networks and pose a significant threat to safety-critical scenarios such as
autonomous driving. Camouflage-based physical attack is a more promising
approach compared to the patch-based attack, offering stronger adversarial
effectiveness in complex physical environments. However, most prior work relies
on mesh priors of the target object and virtual environments constructed by
simulators, which are time-consuming to obtain and inevitably differ from the
real world. Moreover, due to the limitations of the backgrounds in training
images, previous methods often fail to produce multi-view robust adversarial
camouflage and tend to fall into sub-optimal solutions. Due to these reasons,
prior work lacks adversarial effectiveness and robustness across diverse
viewpoints and physical environments. We propose a physical attack framework
based on 3D Gaussian Splatting (3DGS), named PGA, which provides rapid and
precise reconstruction with few images, along with photo-realistic rendering
capabilities. Our framework further enhances cross-view robustness and
adversarial effectiveness by preventing mutual and self-occlusion among
Gaussians and employing a min-max optimization approach that adjusts the
imaging background of each viewpoint, helping the algorithm filter out
non-robust adversarial features. Extensive experiments validate the
effectiveness and superiority of PGA. Our code is available
at:https://github.com/TRLou/PGA.

</details>


### [70] [Perception-Oriented Latent Coding for High-Performance Compressed Domain Semantic Inference](https://arxiv.org/pdf/2507.01608)
*Xu Zhang, Ming Lu, Yan Chen, Zhan Ma*

Main category: cs.CV

TL;DR: POLC introduces a perception-oriented latent coding method to enhance semantic richness in compressed images, reducing fine-tuning overhead compared to MSE-based methods.


<details>
  <summary>Details</summary>
Motivation: MSE-optimized latent spaces lack semantic richness, hindering downstream tasks, and require intensive fine-tuning.

Method: POLC enriches latent features semantically, enabling plug-and-play adapters for fine-tuning with fewer parameters.

Result: POLC matches state-of-the-art generative coding in rate-perception and improves vision task performance with minimal overhead.

Conclusion: POLC offers a computationally efficient solution for high-performance compressed domain semantic inference.

Abstract: In recent years, compressed domain semantic inference has primarily relied on
learned image coding models optimized for mean squared error (MSE). However,
MSE-oriented optimization tends to yield latent spaces with limited semantic
richness, which hinders effective semantic inference in downstream tasks.
Moreover, achieving high performance with these models often requires
fine-tuning the entire vision model, which is computationally intensive,
especially for large models. To address these problems, we introduce
Perception-Oriented Latent Coding (POLC), an approach that enriches the
semantic content of latent features for high-performance compressed domain
semantic inference. With the semantically rich latent space, POLC requires only
a plug-and-play adapter for fine-tuning, significantly reducing the parameter
count compared to previous MSE-oriented methods. Experimental results
demonstrate that POLC achieves rate-perception performance comparable to
state-of-the-art generative image coding methods while markedly enhancing
performance in vision tasks, with minimal fine-tuning overhead. Code is
available at https://github.com/NJUVISION/POLC.

</details>


### [71] [Activation Reward Models for Few-Shot Model Alignment](https://arxiv.org/pdf/2507.01368)
*Tianning Chai, Chancharik Mitra, Brandon Huang, Gautam Rajendrakumar Gare, Zhiqiu Lin, Assaf Arbelle, Leonid Karlinsky, Rogerio Feris, Trevor Darrell, Deva Ramanan, Roei Herzig*

Main category: cs.CV

TL;DR: The paper introduces Activation Reward Models (Activation RMs), a few-shot reward modeling method using activation steering to align LLMs/LMMs with human preferences, outperforming existing approaches and mitigating reward hacking.


<details>
  <summary>Details</summary>
Motivation: Aligning LLMs/LMMs to human preferences is challenging; traditional reward modeling is inflexible and requires large datasets.

Method: Activation RMs leverage activation steering for reward signals with minimal supervision, avoiding additional finetuning.

Result: Activation RMs outperform existing few-shot methods and mitigate reward hacking, achieving state-of-the-art on the new PreferenceHack benchmark.

Conclusion: Activation RMs offer a flexible, effective solution for aligning models to human preferences, with strong performance and safety benefits.

Abstract: Aligning Large Language Models (LLMs) and Large Multimodal Models (LMMs) to
human preferences is a central challenge in improving the quality of the
models' generative outputs for real-world applications. A common approach is to
use reward modeling to encode preferences, enabling alignment via post-training
using reinforcement learning. However, traditional reward modeling is not
easily adaptable to new preferences because it requires a separate reward
model, commonly trained on large preference datasets. To address this, we
introduce Activation Reward Models (Activation RMs) -- a novel few-shot reward
modeling method that leverages activation steering to construct well-aligned
reward signals using minimal supervision and no additional model finetuning.
Activation RMs outperform existing few-shot reward modeling approaches such as
LLM-as-a-judge with in-context learning, voting-based scoring, and token
probability scoring on standard reward modeling benchmarks. Furthermore, we
demonstrate the effectiveness of Activation RMs in mitigating reward hacking
behaviors, highlighting their utility for safety-critical applications. Toward
this end, we propose PreferenceHack, a novel few-shot setting benchmark, the
first to test reward models on reward hacking in a paired preference format.
Finally, we show that Activation RM achieves state-of-the-art performance on
this benchmark, surpassing even GPT-4o.

</details>


### [72] [Using Wavelet Domain Fingerprints to Improve Source Camera Identification](https://arxiv.org/pdf/2507.01712)
*Xinle Tian, Matthew Nunes, Emiko Dupont, Shaunagh Downing, Freddie Lichtenstein, Matt Burns*

Main category: cs.CV

TL;DR: Proposes a wavelet domain fingerprint for SPN extraction, improving accuracy and speed by avoiding inversion steps.


<details>
  <summary>Details</summary>
Motivation: Enhance camera fingerprint detection for source identification and image forensics by simplifying wavelet-based SPN extraction.

Method: Modifies wavelet-based SPN extraction by introducing wavelet domain fingerprints, eliminating inversion steps.

Result: Higher detection accuracy and significantly improved processing speed on real-world datasets.

Conclusion: The wavelet domain fingerprint method streamlines SPN extraction and comparison, offering better performance.

Abstract: Camera fingerprint detection plays a crucial role in source identification
and image forensics, with wavelet denoising approaches proving to be
particularly effective in extracting sensor pattern noise (SPN). In this
article, we propose a modification to wavelet-based SPN extraction. Rather than
constructing the fingerprint as an image, we introduce the notion of a wavelet
domain fingerprint. This avoids the final inversion step of the denoising
algorithm and allows fingerprint comparisons to be made directly in the wavelet
domain. As such, our modification streamlines the extraction and comparison
process. Experimental results on real-world datasets demonstrate that our
method not only achieves higher detection accuracy but can also significantly
improve processing speed.

</details>


### [73] [Active Measurement: Efficient Estimation at Scale](https://arxiv.org/pdf/2507.01372)
*Max Hamilton, Jinlin Lai, Wenlong Zhao, Subhransu Maji, Daniel Sheldon*

Main category: cs.CV

TL;DR: Active measurement is a human-in-the-loop AI framework for scientific measurement, combining AI predictions with human labeling to improve accuracy and reduce effort.


<details>
  <summary>Details</summary>
Motivation: Current AI workflows lack accuracy and statistical guarantees for scientific discovery.

Method: Uses AI to predict measurements, samples for human labeling via importance sampling, and refines estimates iteratively.

Result: Provides precise estimates with minimal human effort, outperforming alternatives in error reduction.

Conclusion: Active measurement enhances scientific measurement by balancing AI efficiency and human accuracy.

Abstract: AI has the potential to transform scientific discovery by analyzing vast
datasets with little human effort. However, current workflows often do not
provide the accuracy or statistical guarantees that are needed. We introduce
active measurement, a human-in-the-loop AI framework for scientific
measurement. An AI model is used to predict measurements for individual units,
which are then sampled for human labeling using importance sampling. With each
new set of human labels, the AI model is improved and an unbiased Monte Carlo
estimate of the total measurement is refined. Active measurement can provide
precise estimates even with an imperfect AI model, and requires little human
effort when the AI model is very accurate. We derive novel estimators,
weighting schemes, and confidence intervals, and show that active measurement
reduces estimation error compared to alternatives in several measurement tasks.

</details>


### [74] [MUG: Pseudo Labeling Augmented Audio-Visual Mamba Network for Audio-Visual Video Parsing](https://arxiv.org/pdf/2507.01384)
*Langyu Wang, Bingke Zhu, Yingying Chen, Yiyuan Zhang, Ming Tang, Jinqiao Wang*

Main category: cs.CV

TL;DR: The paper proposes MUG, an audio-visual Mamba network with pseudo-label augmentation, to improve segment-level and event-level predictions in weakly-supervised AVVP tasks.


<details>
  <summary>Details</summary>
Motivation: Existing methods struggle with improving both segment-level and event-level predictions due to weak supervision and model limitations.

Method: Uses pseudo-labels for data augmentation and an AV-Mamba network for feature processing to enhance segment perception and reduce noise.

Result: MUG achieves state-of-the-art results on the LLP dataset, with gains of 2.1% and 1.2% in visual and audio segment-level metrics.

Conclusion: MUG effectively addresses the limitations of weakly-supervised AVVP, improving performance through pseudo-label augmentation and the AV-Mamba network.

Abstract: The weakly-supervised audio-visual video parsing (AVVP) aims to predict all
modality-specific events and locate their temporal boundaries. Despite
significant progress, due to the limitations of the weakly-supervised and the
deficiencies of the model architecture, existing methods are lacking in
simultaneously improving both the segment-level prediction and the event-level
prediction. In this work, we propose a audio-visual Mamba network with pseudo
labeling aUGmentation (MUG) for emphasising the uniqueness of each segment and
excluding the noise interference from the alternate modalities. Specifically,
we annotate some of the pseudo-labels based on previous work. Using unimodal
pseudo-labels, we perform cross-modal random combinations to generate new data,
which can enhance the model's ability to parse various segment-level event
combinations. For feature processing and interaction, we employ a audio-visual
mamba network. The AV-Mamba enhances the ability to perceive different segments
and excludes additional modal noise while sharing similar modal information.
Our extensive experiments demonstrate that MUG improves state-of-the-art
results on LLP dataset in all metrics (e.g,, gains of 2.1% and 1.2% in terms of
visual Segment-level and audio Segment-level metrics). Our code is available at
https://github.com/WangLY136/MUG.

</details>


### [75] [FixTalk: Taming Identity Leakage for High-Quality Talking Head Generation in Extreme Cases](https://arxiv.org/pdf/2507.01390)
*Shuai Tan, Bill Gong, Bin Ji, Ye Pan*

Main category: cs.CV

TL;DR: FixTalk is a novel framework addressing identity leakage and rendering artifacts in talking head generation by decoupling identity from motion features and leveraging leaked identity for detail enhancement.


<details>
  <summary>Details</summary>
Motivation: Existing methods for talking head generation suffer from identity leakage and rendering artifacts, especially in extreme cases, prompting the need for a solution that simultaneously resolves both issues.

Method: FixTalk introduces an Enhanced Motion Indicator (EMI) to decouple identity from motion features and an Enhanced Detail Indicator (EDI) to use leaked identity for artifact correction.

Result: Experiments show FixTalk effectively mitigates identity leakage and rendering artifacts, outperforming state-of-the-art methods.

Conclusion: FixTalk provides a robust solution for high-quality talking head generation by addressing key challenges of identity leakage and rendering artifacts.

Abstract: Talking head generation is gaining significant importance across various
domains, with a growing demand for high-quality rendering. However, existing
methods often suffer from identity leakage (IL) and rendering artifacts (RA),
particularly in extreme cases. Through an in-depth analysis of previous
approaches, we identify two key insights: (1) IL arises from identity
information embedded within motion features, and (2) this identity information
can be leveraged to address RA. Building on these findings, this paper
introduces FixTalk, a novel framework designed to simultaneously resolve both
issues for high-quality talking head generation. Firstly, we propose an
Enhanced Motion Indicator (EMI) to effectively decouple identity information
from motion features, mitigating the impact of IL on generated talking heads.
To address RA, we introduce an Enhanced Detail Indicator (EDI), which utilizes
the leaked identity information to supplement missing details, thus fixing the
artifacts. Extensive experiments demonstrate that FixTalk effectively mitigates
IL and RA, achieving superior performance compared to state-of-the-art methods.

</details>


### [76] [Coherent Online Road Topology Estimation and Reasoning with Standard-Definition Maps](https://arxiv.org/pdf/2507.01397)
*Khanh Son Pham, Christian Witte, Jens Behley, Johannes Betz, Cyrill Stachniss*

Main category: cs.CV

TL;DR: The paper proposes a coherent method for predicting HD map elements (lane segments, topology, and road boundaries) using prior SD map data, enhancing training stability and performance with hybrid encodings and temporal consistency.


<details>
  <summary>Details</summary>
Motivation: Autonomous cars rely on HD maps, but online HD map construction is complex. The paper aims to address this by leveraging prior SD map data for coherent prediction.

Method: A network architecture uses hybrid lane segment encodings (prior info + denoising) and past frames for temporal consistency to predict HD map elements.

Result: The approach outperforms previous methods significantly, demonstrating the effectiveness of the modeling scheme.

Conclusion: The proposed method successfully addresses the challenge of coherent HD map construction, leveraging prior data and temporal consistency for improved performance.

Abstract: Most autonomous cars rely on the availability of high-definition (HD) maps.
Current research aims to address this constraint by directly predicting HD map
elements from onboard sensors and reasoning about the relationships between the
predicted map and traffic elements. Despite recent advancements, the coherent
online construction of HD maps remains a challenging endeavor, as it
necessitates modeling the high complexity of road topologies in a unified and
consistent manner. To address this challenge, we propose a coherent approach to
predict lane segments and their corresponding topology, as well as road
boundaries, all by leveraging prior map information represented by commonly
available standard-definition (SD) maps. We propose a network architecture,
which leverages hybrid lane segment encodings comprising prior information and
denoising techniques to enhance training stability and performance.
Furthermore, we facilitate past frames for temporal consistency. Our
experimental evaluation demonstrates that our approach outperforms previous
methods by a large margin, highlighting the benefits of our modeling scheme.

</details>


### [77] [Medical-Knowledge Driven Multiple Instance Learning for Classifying Severe Abdominal Anomalies on Prenatal Ultrasound](https://arxiv.org/pdf/2507.01401)
*Huanwen Liang, Jingxian Xu, Yuanji Zhang, Yuhao Huang, Yuhan Zhang, Xin Yang, Ran Li, Xuedong Deng, Yanjun Liu, Guowei Tao, Yun Wu, Sheng Zhao, Xinru Gao, Dong Ni*

Main category: cs.CV

TL;DR: A case-level MIL-based method for classifying fetal abdominal anomalies in prenatal ultrasound, using MoAE, MFS, and PPL modules, outperforms state-of-the-art methods.


<details>
  <summary>Details</summary>
Motivation: Accurate diagnosis of fetal abdominal malformations is critical for pregnancy management, but AI applications in this area are limited, especially for case-level diagnosis.

Method: Proposes a MIL-based method with MoAE for attention weighting, MFS for medical-knowledge-aligned feature selection, and PPL to enhance MFS.

Result: Validated on 2,419 cases (24,748 images), the method outperforms competitors.

Conclusion: The approach advances case-level diagnosis in prenatal ultrasound, improving accuracy without standard plane localization.

Abstract: Fetal abdominal malformations are serious congenital anomalies that require
accurate diagnosis to guide pregnancy management and reduce mortality. Although
AI has demonstrated significant potential in medical diagnosis, its application
to prenatal abdominal anomalies remains limited. Most existing studies focus on
image-level classification and rely on standard plane localization, placing
less emphasis on case-level diagnosis. In this paper, we develop a case-level
multiple instance learning (MIL)-based method, free of standard plane
localization, for classifying fetal abdominal anomalies in prenatal ultrasound.
Our contribution is three-fold. First, we adopt a mixture-of-attention-experts
module (MoAE) to weight different attention heads for various planes. Secondly,
we propose a medical-knowledge-driven feature selection module (MFS) to align
image features with medical knowledge, performing self-supervised image token
selection at the case-level. Finally, we propose a prompt-based prototype
learning (PPL) to enhance the MFS. Extensively validated on a large prenatal
abdominal ultrasound dataset containing 2,419 cases, with a total of 24,748
images and 6 categories, our proposed method outperforms the state-of-the-art
competitors. Codes are available at:https://github.com/LL-AC/AAcls.

</details>


### [78] [CaptionSmiths: Flexibly Controlling Language Pattern in Image Captioning](https://arxiv.org/pdf/2507.01409)
*Kuniaki Saito, Donghyun Kim, Kwanyong Park, Atsushi Hashimoto, Yoshitaka Ushiku*

Main category: cs.CV

TL;DR: CaptionSmiths proposes a method to control caption properties like length and descriptiveness in image captioning models by quantifying and interpolating between extreme states.


<details>
  <summary>Details</summary>
Motivation: Existing models lack fine-grained control over caption properties and cannot smoothly transition between language patterns.

Method: Quantifies caption properties (length, descriptiveness, word uniqueness) as scalar values and conditions the model via interpolation between endpoint vectors.

Result: The model smoothly adjusts caption properties and improves lexical alignment, reducing length control error by 506%.

Conclusion: CaptionSmiths effectively enables flexible control over caption properties in a single model.

Abstract: An image captioning model flexibly switching its language pattern, e.g.,
descriptiveness and length, should be useful since it can be applied to diverse
applications. However, despite the dramatic improvement in generative
vision-language models, fine-grained control over the properties of generated
captions is not easy due to two reasons: (i) existing models are not given the
properties as a condition during training and (ii) existing models cannot
smoothly transition its language pattern from one state to the other. Given
this challenge, we propose a new approach, CaptionSmiths, to acquire a single
captioning model that can handle diverse language patterns. First, our approach
quantifies three properties of each caption, length, descriptiveness, and
uniqueness of a word, as continuous scalar values, without human annotation.
Given the values, we represent the conditioning via interpolation between two
endpoint vectors corresponding to the extreme states, e.g., one for a very
short caption and one for a very long caption. Empirical results demonstrate
that the resulting model can smoothly change the properties of the output
captions and show higher lexical alignment than baselines. For instance,
CaptionSmiths reduces the error in controlling caption length by 506\% despite
better lexical alignment. Code will be available on
https://github.com/omron-sinicx/captionsmiths.

</details>


### [79] [Embedded Graph Convolutional Networks for Real-Time Event Data Processing on SoC FPGAs](https://arxiv.org/pdf/2406.07318)
*Kamil Jeziorek, Piotr Wzorek, Krzysztof Blachut, Andrea Pinna, Tomasz Kryjak*

Main category: cs.CV

TL;DR: The paper introduces EFGCN, an FPGA-accelerated GCN for event cameras, achieving 100x model size reduction and high throughput with minimal accuracy loss.


<details>
  <summary>Details</summary>
Motivation: Addressing the limitations of traditional video systems in automotive applications by leveraging event cameras for low-latency, energy-efficient real-time processing.

Method: Developed a custom EFGCN with hardware-aware optimizations for PointNetConv, implemented on a ZCU104 SoC FPGA without external memory.

Result: Achieved 13.3 MEPS throughput, 100x model size reduction vs. AEGNN, with minor accuracy drops (2.9% for N-Caltech101, 2.2% for N-Cars).

Conclusion: EFGCN offers state-of-the-art, scalable, and resource-efficient event-based classification, with open-source software and hardware.

Abstract: The utilisation of event cameras represents an important and swiftly evolving
trend aimed at addressing the constraints of traditional video systems.
Particularly within the automotive domain, these cameras find significant
relevance for their integration into embedded real-time systems due to lower
latency and energy consumption. One effective approach to ensure the necessary
throughput and latency for event processing is through the utilisation of graph
convolutional networks (GCNs). In this study, we introduce a custom EFGCN
(Event-based FPGA-accelerated Graph Convolutional Network) designed with a
series of hardware-aware optimisations tailored for PointNetConv, a graph
convolution designed for point cloud processing. The proposed techniques result
in up to 100-fold reduction in model size compared to Asynchronous Event-based
GNN (AEGNN), one of the most recent works in the field, with a relatively small
decrease in accuracy (2.9% for the N-Caltech101 classification task, 2.2% for
the N-Cars classification task), thus following the TinyML trend. We
implemented EFGCN on a ZCU104 SoC FPGA platform without any external memory
resources, achieving a throughput of 13.3 million events per second (MEPS) and
real-time partially asynchronous processing with low latency. Our approach
achieves state-of-the-art performance across multiple event-based
classification benchmarks while remaining highly scalable, customisable and
resource-efficient. We publish both software and hardware source code in an
open repository: https://github.com/vision-agh/gcnn-dvs-fpga

</details>


### [80] [Gradient Short-Circuit: Efficient Out-of-Distribution Detection via Feature Intervention](https://arxiv.org/pdf/2507.01417)
*Jiawei Gu, Ziyue Qiao, Zechao Li*

Main category: cs.CV

TL;DR: A novel inference-stage technique for OOD detection leverages gradient consistency in ID samples and disorganization in OOD samples, improving robustness without major pipeline changes.


<details>
  <summary>Details</summary>
Motivation: OOD detection is crucial for safe deep model deployment in open-world environments, where inputs may deviate from training data.

Method: Proposes gradient short-circuiting to block spurious gradients in OOD samples and a local first-order approximation to avoid recomputing logits.

Result: Substantial improvements on standard OOD benchmarks, with lightweight implementation.

Conclusion: The method offers a practical, effective solution for robust OOD detection in real-world applications.

Abstract: Out-of-Distribution (OOD) detection is critical for safely deploying deep
models in open-world environments, where inputs may lie outside the training
distribution. During inference on a model trained exclusively with
In-Distribution (ID) data, we observe a salient gradient phenomenon: around an
ID sample, the local gradient directions for "enhancing" that sample's
predicted class remain relatively consistent, whereas OOD samples--unseen in
training--exhibit disorganized or conflicting gradient directions in the same
neighborhood. Motivated by this observation, we propose an inference-stage
technique to short-circuit those feature coordinates that spurious gradients
exploit to inflate OOD confidence, while leaving ID classification largely
intact. To circumvent the expense of recomputing the logits after this gradient
short-circuit, we further introduce a local first-order approximation that
accurately captures the post-modification outputs without a second forward
pass. Experiments on standard OOD benchmarks show our approach yields
substantial improvements. Moreover, the method is lightweight and requires
minimal changes to the standard inference pipeline, offering a practical path
toward robust OOD detection in real-world applications.

</details>


### [81] [DocShaDiffusion: Diffusion Model in Latent Space for Document Image Shadow Removal](https://arxiv.org/pdf/2507.01422)
*Wenjie Liu, Bingshu Wang, Ze Wang, C. L. Philip Chen*

Main category: cs.CV

TL;DR: The paper introduces DocShaDiffusion, a latent space diffusion model for document shadow removal, addressing color shadows with a shadow soft-mask generation module (SSGM) and a mask-aware guided diffusion module (SMGDM). It also proposes a shadow-robust perceptual feature loss and a synthetic dataset (SDCSRD).


<details>
  <summary>Details</summary>
Motivation: Existing methods fail to handle color shadows in document images, limiting their effectiveness. The paper aims to improve shadow removal by focusing on color shadows and preserving document details.

Method: The method involves a latent space diffusion model (DocShaDiffusion), SSGM for shadow mask generation, SMGDM for guided diffusion, and a perceptual feature loss. A synthetic dataset (SDCSRD) is also created for training.

Result: Experiments on three public datasets show the method outperforms state-of-the-art techniques in document shadow removal.

Conclusion: The proposed approach effectively removes color shadows while preserving document details, validated by superior performance on benchmarks. The code and dataset will be publicly available.

Abstract: Document shadow removal is a crucial task in the field of document image
enhancement. However, existing methods tend to remove shadows with constant
color background and ignore color shadows. In this paper, we first design a
diffusion model in latent space for document image shadow removal, called
DocShaDiffusion. It translates shadow images from pixel space to latent space,
enabling the model to more easily capture essential features. To address the
issue of color shadows, we design a shadow soft-mask generation module (SSGM).
It is able to produce accurate shadow mask and add noise into shadow regions
specially. Guided by the shadow mask, a shadow mask-aware guided diffusion
module (SMGDM) is proposed to remove shadows from document images by
supervising the diffusion and denoising process. We also propose a
shadow-robust perceptual feature loss to preserve details and structures in
document images. Moreover, we develop a large-scale synthetic document color
shadow removal dataset (SDCSRD). It simulates the distribution of realistic
color shadows and provides powerful supports for the training of models.
Experiments on three public datasets validate the proposed method's superiority
over state-of-the-art. Our code and dataset will be publicly available.

</details>


### [82] [A Review of Bayesian Uncertainty Quantification in Deep Probabilistic Image Segmentation](https://arxiv.org/pdf/2411.16370)
*M. M. A. Valiuddin, R. J. G. van Sloun, C. G. A. Viviers, P. H. N. de With, F. van der Sommen*

Main category: cs.CV

TL;DR: The paper provides a comprehensive overview of probabilistic segmentation, focusing on uncertainty quantification in CNN-based models, its applications, and future research directions.


<details>
  <summary>Details</summary>
Motivation: The rapid adoption of CNN-based segmentation models in high-stake applications necessitates reliable uncertainty quantification to prevent uninformed decisions.

Method: The work reviews fundamental concepts, advancements, and applications of uncertainty quantification in segmentation, covering epistemic and aleatoric uncertainties.

Result: Key applications include addressing annotation inconsistencies, correlating prediction error with uncertainty, improving generalization, and Active Learning. Challenges in architectures, methods, and benchmarking are identified.

Conclusion: Future work should focus on single forward-pass methods and better utilization of volumetric data, alongside standardization and benchmarking improvements.

Abstract: Advancements in image segmentation play an integral role within the broad
scope of Deep Learning-based Computer Vision. Furthermore, their widespread
applicability in critical real-world tasks has resulted in challenges related
to the reliability of such algorithms. Hence, uncertainty quantification has
been extensively studied within this context, enabling the expression of model
ignorance (epistemic uncertainty) or data ambiguity (aleatoric uncertainty) to
prevent uninformed decision-making. Due to the rapid adoption of Convolutional
Neural Network (CNN)-based segmentation models in high-stake applications, a
substantial body of research has been published on this very topic, causing its
swift expansion into a distinct field. This work provides a comprehensive
overview of probabilistic segmentation, by discussing fundamental concepts of
uncertainty quantification, governing advancements in the field as well as the
application to various tasks. Moreover, literature on both types of
uncertainties trace back to four key applications: (1) to quantify statistical
inconsistencies in the annotation process due ambiguous images, (2) correlating
prediction error with uncertainty, (3) expanding the model hypothesis space for
better generalization, and (4) Active Learning. An extensive discussion follows
that includes an overview of utilized datasets for each of the applications and
evaluation of the available methods. We also highlight challenges related to
architectures, uncertainty quantification methods, standardization and
benchmarking, and finally end with recommendations for future work such as
methods based on single forward passes and models that appropriately leverage
volumetric data.

</details>


### [83] [TurboReg: TurboClique for Robust and Efficient Point Cloud Registration](https://arxiv.org/pdf/2507.01439)
*Shaocheng Yan, Pengcheng Shi, Zhenjun Zhao, Kaixin Wang, Kuang Cao, Ji Wu, Jiayuan Li*

Main category: cs.CV

TL;DR: TurboReg introduces a fast, robust estimator for Point Cloud Registration using TurboClique and Pivot-Guided Search, achieving high recall and speed.


<details>
  <summary>Details</summary>
Motivation: Existing methods for robust estimation in PCR are slow due to exponential time complexity, limiting practical use.

Method: TurboReg uses TurboClique (3-clique in constrained graphs) and PGS for efficient, parallel searching with linear time complexity.

Result: TurboReg outperforms state-of-the-art methods, e.g., 208.22× faster than 3DMAC with higher recall.

Conclusion: TurboReg is a scalable, efficient solution for PCR, balancing speed and robustness.

Abstract: Robust estimation is essential in correspondence-based Point Cloud
Registration (PCR). Existing methods using maximal clique search in
compatibility graphs achieve high recall but suffer from exponential time
complexity, limiting their use in time-sensitive applications. To address this
challenge, we propose a fast and robust estimator, TurboReg, built upon a novel
lightweight clique, TurboClique, and a highly parallelizable Pivot-Guided
Search (PGS) algorithm. First, we define the TurboClique as a 3-clique within a
highly-constrained compatibility graph. The lightweight nature of the 3-clique
allows for efficient parallel searching, and the highly-constrained
compatibility graph ensures robust spatial consistency for stable
transformation estimation. Next, PGS selects matching pairs with high SC$^2$
scores as pivots, effectively guiding the search toward TurboCliques with
higher inlier ratios. Moreover, the PGS algorithm has linear time complexity
and is significantly more efficient than the maximal clique search with
exponential time complexity. Extensive experiments show that TurboReg achieves
state-of-the-art performance across multiple real-world datasets, with
substantial speed improvements. For example, on the 3DMatch+FCGF dataset,
TurboReg (1K) operates $208.22\times$ faster than 3DMAC while also achieving
higher recall. Our code is accessible at
\href{https://github.com/Laka-3DV/TurboReg}{\texttt{TurboReg}}.

</details>


### [84] [OoDDINO:A Multi-level Framework for Anomaly Segmentation on Complex Road Scenes](https://arxiv.org/pdf/2507.01455)
*Yuxing Liu, Ji Zhang, Zhou Xuchuan, Jingzhong Xiao, Huimin Yang, Jiaxin Zhong*

Main category: cs.CV

TL;DR: OoDDINO introduces a multi-level anomaly segmentation framework to address fragmented segmentation and global thresholding issues in anomaly detection, using a coarse-to-fine strategy with uncertainty-guided detection and adaptive thresholds.


<details>
  <summary>Details</summary>
Motivation: Existing pixel-wise anomaly segmentation methods struggle with fragmented results and inconsistent thresholding due to spatial correlation neglect and score variability.

Method: OoDDINO combines uncertainty-guided anomaly detection (OUAFS) with pixel-level segmentation (ADT-Net) in a two-stage cascade, using orthogonal constraints and adaptive thresholds.

Result: Extensive experiments show OoDDINO outperforms state-of-the-art methods on benchmark datasets, offering compatibility with other models.

Conclusion: OoDDINO effectively addresses limitations of existing methods, providing accurate and fine-grained anomaly segmentation.

Abstract: Anomaly segmentation aims to identify Out-of-Distribution (OoD) anomalous
objects within images. Existing pixel-wise methods typically assign anomaly
scores individually and employ a global thresholding strategy to segment
anomalies. Despite their effectiveness, these approaches encounter significant
challenges in real-world applications: (1) neglecting spatial correlations
among pixels within the same object, resulting in fragmented segmentation; (2)
variabil ity in anomaly score distributions across image regions, causing
global thresholds to either generate false positives in background areas or
miss segments of anomalous objects. In this work, we introduce OoDDINO, a novel
multi-level anomaly segmentation framework designed to address these
limitations through a coarse-to-fine anomaly detection strategy. OoDDINO
combines an uncertainty-guided anomaly detection model with a pixel-level
segmentation model within a two-stage cascade architecture. Initially, we
propose an Orthogonal Uncertainty-Aware Fusion Strategy (OUAFS) that
sequentially integrates multiple uncertainty metrics with visual
representations, employing orthogonal constraints to strengthen the detection
model's capacity for localizing anomalous regions accurately. Subsequently, we
develop an Adaptive Dual-Threshold Network (ADT-Net), which dynamically
generates region-specific thresholds based on object-level detection outputs
and pixel-wise anomaly scores. This approach allows for distinct thresholding
strategies within foreground and background areas, achieving fine-grained
anomaly segmentation. The proposed framework is compatible with other
pixel-wise anomaly detection models, which acts as a plug-in to boost the
performance. Extensive experiments on two benchmark datasets validate our
framework's superiority and compatibility over state-of-the-art methods.

</details>


### [85] [Generalizable Neural Electromagnetic Inverse Scattering](https://arxiv.org/pdf/2506.21349)
*Yizhe Cheng, Chunxun Tian, Haoru Wang, Wentao Zhu, Xiaoxuan Ma, Yizhou Wang*

Main category: cs.CV

TL;DR: A physics-driven framework for solving Electromagnetic Inverse Scattering Problems (EISP) is proposed, improving generalization and robustness over existing methods.


<details>
  <summary>Details</summary>
Motivation: Existing machine learning approaches like Img-Interiors lack generalization and fail under sparse transmitter setups, necessitating a more robust solution.

Method: The problem is reformulated as a two-stage process involving a current estimator and permittivity solver, leveraging induced current as an intermediate representation.

Result: The framework outperforms state-of-the-art methods in accuracy, generalization, and robustness, especially under sparse transmitter conditions.

Conclusion: This work provides a novel, physics-informed approach to EISP, advancing practical electromagnetic imaging solutions.

Abstract: Solving Electromagnetic Inverse Scattering Problems (EISP) is fundamental in
applications such as medical imaging, where the goal is to reconstruct the
relative permittivity from scattered electromagnetic field. This inverse
process is inherently ill-posed and highly nonlinear, making it particularly
challenging. A recent machine learning-based approach, Img-Interiors, shows
promising results by leveraging continuous implicit functions. However, it
requires case-specific optimization, lacks generalization to unseen data, and
fails under sparse transmitter setups (e.g., with only one transmitter). To
address these limitations, we revisit EISP from a physics-informed perspective,
reformulating it as a two stage inverse transmission-scattering process. This
formulation reveals the induced current as a generalizable intermediate
representation, effectively decoupling the nonlinear scattering process from
the ill-posed inverse problem. Built on this insight, we propose the first
generalizable physics-driven framework for EISP, comprising a current estimator
and a permittivity solver, working in an end-to-end manner. The current
estimator explicitly learns the induced current as a physical bridge between
the incident and scattered field, while the permittivity solver computes the
relative permittivity directly from the estimated induced current. This design
enables data-driven training and generalizable feed-forward prediction of
relative permittivity on unseen data while maintaining strong robustness to
transmitter sparsity. Extensive experiments show that our method outperforms
state-of-the-art approaches in reconstruction accuracy, generalization, and
robustness. This work offers a fundamentally new perspective on electromagnetic
inverse scattering and represents a major step toward cost-effective practical
solutions for electromagnetic imaging.

</details>


### [86] [NOCTIS: Novel Object Cyclic Threshold based Instance Segmentation](https://arxiv.org/pdf/2507.01463)
*Max Gandyra, Alessandro Santonicola, Michael Beetz*

Main category: cs.CV

TL;DR: NOCTIS is a novel framework for instance segmentation of unseen objects using Grounded-SAM 2 and DINOv2, outperforming existing methods without retraining.


<details>
  <summary>Details</summary>
Motivation: The challenge of segmenting novel objects without retraining drives the need for a generalizable model.

Method: Leverages Grounded-SAM 2 for proposals and masks, and DINOv2 for embeddings, with cyclic threshold filtering and confidence weighting.

Result: Outperforms top RGB and RGB-D methods on BOP 2023 datasets.

Conclusion: NOCTIS provides a robust, training-free solution for novel object instance segmentation.

Abstract: Instance segmentation of novel objects instances in RGB images, given some
example images for each object, is a well known problem in computer vision.
Designing a model general enough to be employed, for all kinds of novel
objects, without (re-) training, has proven to be a difficult task. To handle
this, we propose a simple, yet powerful, framework, called: Novel Object Cyclic
Threshold based Instance Segmentation (NOCTIS). This work stems from and
improves upon previous ones like CNOS, SAM-6D and NIDS-Net; thus, it also
leverages on recent vision foundation models, namely: Grounded-SAM 2 and
DINOv2. It utilises Grounded-SAM 2 to obtain object proposals with precise
bounding boxes and their corresponding segmentation masks; while DINOv2's
zero-shot capabilities are employed to generate the image embeddings. The
quality of those masks, together with their embeddings, is of vital importance
to our approach; as the proposal-object matching is realized by determining an
object matching score based on the similarity of the class embeddings and the
average maximum similarity of the patch embeddings. Differently to SAM-6D,
calculating the latter involves a prior patch filtering based on the distance
between each patch and its corresponding cyclic/roundtrip patch in the image
grid. Furthermore, the average confidence of the proposals' bounding box and
mask is used as an additional weighting factor for the object matching score.
We empirically show that NOCTIS, without further training/fine tuning,
outperforms the best RGB and RGB-D methods on the seven core datasets of the
BOP 2023 challenge for the "Model-based 2D segmentation of unseen objects"
task.

</details>


### [87] [Representation Entanglement for Generation:Training Diffusion Transformers Is Much Easier Than You Think](https://arxiv.org/pdf/2507.01467)
*Ge Wu, Shen Zhang, Ruijing Shi, Shanghua Gao, Zhenyuan Chen, Lei Wang, Zhaowei Chen, Hongcheng Gao, Yao Tang, Jian Yang, Ming-Ming Cheng, Xiang Li*

Main category: cs.CV

TL;DR: REG improves diffusion models by entangling low-level image latents with a high-level class token, enhancing generation quality and training efficiency with minimal overhead.


<details>
  <summary>Details</summary>
Motivation: Existing methods like REPA fail to fully utilize discriminative representations during denoising inference.

Method: REG entangles image latents with a single high-level class token from pretrained models for denoising.

Result: Achieves 63x and 23x faster training than baselines and outperforms REPA with 10x fewer iterations.

Conclusion: REG efficiently leverages semantic knowledge to guide generation, significantly improving performance and training speed.

Abstract: REPA and its variants effectively mitigate training challenges in diffusion
models by incorporating external visual representations from pretrained models,
through alignment between the noisy hidden projections of denoising networks
and foundational clean image representations. We argue that the external
alignment, which is absent during the entire denoising inference process, falls
short of fully harnessing the potential of discriminative representations. In
this work, we propose a straightforward method called Representation
Entanglement for Generation (REG), which entangles low-level image latents with
a single high-level class token from pretrained foundation models for
denoising. REG acquires the capability to produce coherent image-class pairs
directly from pure noise, substantially improving both generation quality and
training efficiency. This is accomplished with negligible additional inference
overhead, requiring only one single additional token for denoising (<0.5\%
increase in FLOPs and latency). The inference process concurrently reconstructs
both image latents and their corresponding global semantics, where the acquired
semantic knowledge actively guides and enhances the image generation process.
On ImageNet 256$\times$256, SiT-XL/2 + REG demonstrates remarkable convergence
acceleration, achieving $\textbf{63}\times$ and $\textbf{23}\times$ faster
training than SiT-XL/2 and SiT-XL/2 + REPA, respectively. More impressively,
SiT-L/2 + REG trained for merely 400K iterations outperforms SiT-XL/2 + REPA
trained for 4M iterations ($\textbf{10}\times$ longer). Code is available at:
https://github.com/Martinser/REG.

</details>


### [88] [Customizable ROI-Based Deep Image Compression](https://arxiv.org/pdf/2507.00373)
*Jian Jin, Fanxin Xia, Feng Ding, Xinfeng Zhang, Meiqin Liu, Yao Zhao, Weisi Lin, Lili Meng*

Main category: cs.CV

TL;DR: A customizable ROI-based deep image compression method is proposed, allowing users to define ROI via text and manage quality trade-offs between ROI and non-ROI.


<details>
  <summary>Details</summary>
Motivation: Existing ROI-based compression lacks flexibility for diverse user preferences and fixed ROI definitions.

Method: Introduces Text-controlled Mask Acquisition (TMA) for ROI customization, Customizable Value Assign (CVA) for quality trade-offs, and Latent Mask Attention (LMA) for latent space optimization.

Result: The method effectively supports customizable ROI definition and quality trade-off management.

Conclusion: The proposed paradigm successfully addresses the need for flexible and customizable ROI-based image compression.

Abstract: Region of Interest (ROI)-based image compression optimizes bit allocation by
prioritizing ROI for higher-quality reconstruction. However, as the users
(including human clients and downstream machine tasks) become more diverse,
ROI-based image compression needs to be customizable to support various
preferences. For example, different users may define distinct ROI or require
different quality trade-offs between ROI and non-ROI. Existing ROI-based image
compression schemes predefine the ROI, making it unchangeable, and lack
effective mechanisms to balance reconstruction quality between ROI and non-ROI.
This work proposes a paradigm for customizable ROI-based deep image
compression. First, we develop a Text-controlled Mask Acquisition (TMA) module,
which allows users to easily customize their ROI for compression by just
inputting the corresponding semantic \emph{text}. It makes the encoder
controlled by text. Second, we design a Customizable Value Assign (CVA)
mechanism, which masks the non-ROI with a changeable extent decided by users
instead of a constant one to manage the reconstruction quality trade-off
between ROI and non-ROI. Finally, we present a Latent Mask Attention (LMA)
module, where the latent spatial prior of the mask and the latent
Rate-Distortion Optimization (RDO) prior of the image are extracted and fused
in the latent space, and further used to optimize the latent representation of
the source image. Experimental results demonstrate that our proposed
customizable ROI-based deep image compression paradigm effectively addresses
the needs of customization for ROI definition and mask acquisition as well as
the reconstruction quality trade-off management between the ROI and non-ROI.

</details>


### [89] [Optimizing Methane Detection On Board Satellites: Speed, Accuracy, and Low-Power Solutions for Resource-Constrained Hardware](https://arxiv.org/pdf/2507.01472)
*Jonáš Herec, Vít Růžička, Rado Pitoňák*

Main category: cs.CV

TL;DR: The paper proposes efficient, low-power algorithms (Mag1c-SAS and CEM) for onboard methane detection using hyperspectral satellite imagery, achieving significant speed improvements (100x-230x faster) while maintaining accuracy. It also introduces band selection strategies for faster processing.


<details>
  <summary>Details</summary>
Motivation: Early detection of methane leaks via hyperspectral imagery is crucial for climate change mitigation, but existing methods are computationally demanding for onboard hardware.

Method: The study tests fast target detection methods (ACE, CEM) and introduces Mag1c-SAS, a faster variant of Mag1c, integrating them with machine learning models (U-Net, LinkNet). It also evaluates three band selection strategies.

Result: Mag1c-SAS and CEM are identified as promising candidates, balancing accuracy and speed (100x-230x faster than Mag1c). One band selection strategy outperforms traditional methods with fewer channels.

Conclusion: The research advances onboard methane detection with minimal hardware requirements, enabling timely data delivery. All code, data, and models are open-sourced.

Abstract: Methane is a potent greenhouse gas, and detecting its leaks early via
hyperspectral satellite imagery can help mitigate climate change. Meanwhile,
many existing missions operate in manual tasking regimes only, thus missing
potential events of interest. To overcome slow downlink rates cost-effectively,
onboard detection is a viable solution. However, traditional methane
enhancement methods are too computationally demanding for resource-limited
onboard hardware. This work accelerates methane detection by focusing on
efficient, low-power algorithms. We test fast target detection methods (ACE,
CEM) that have not been previously used for methane detection and propose a
Mag1c-SAS - a significantly faster variant of the current state-of-the-art
algorithm for methane detection: Mag1c. To explore their true detection
potential, we integrate them with a machine learning model (U-Net, LinkNet).
Our results identify two promising candidates (Mag1c-SAS and CEM), both
acceptably accurate for the detection of strong plumes and computationally
efficient enough for onboard deployment: one optimized more for accuracy, the
other more for speed, achieving up to ~100x and ~230x faster computation than
original Mag1c on resource-limited hardware. Additionally, we propose and
evaluate three band selection strategies. One of them can outperform the method
traditionally used in the field while using fewer channels, leading to even
faster processing without compromising accuracy. This research lays the
foundation for future advancements in onboard methane detection with minimal
hardware requirements, improving timely data delivery. The produced code, data,
and models are open-sourced and can be accessed from
https://github.com/zaitra/methane-filters-benchmark.

</details>


### [90] [Just Noticeable Difference for Large Multimodal Models](https://arxiv.org/pdf/2507.00490)
*Zijian Chen, Yuan Tian, Yuze Sun, Wei Sun, Zicheng Zhang, Weisi Lin, Guangtao Zhai, Wenjun Zhang*

Main category: cs.CV

TL;DR: The paper introduces LMM-JND, a concept to quantify visual blind spots in large multimodal models (LMMs), and presents VPA-JND, a dataset to study these limitations. It reveals gaps between LMMs and human visual performance.


<details>
  <summary>Details</summary>
Motivation: To address the lack of systematic exploration of perceptual boundaries in LMMs and uncover their visual blind spots, which pose security and efficiency risks.

Method: Proposes LMM-JND and its pipeline, constructs the VPA-JND dataset (21.5k images, 489k stimuli, 12 distortion types), and evaluates LMMs like GPT-4o and InternVL2.5.

Result: Identifies significant visual blind spots in LMMs, showing their inferiority to human performance in basic comparison tasks. Also finds correlations between vision/language backbone designs and visual acuity.

Conclusion: Highlights LMM-JND as a key metric for studying LMMs, emphasizing its importance for security and future model refinement.

Abstract: Just noticeable difference (JND), the minimum change that the human visual
system (HVS) can perceive, has been studied for decades. Although recent work
has extended this line of research into machine vision, there has been a
scarcity of studies systematically exploring its perceptual boundaries across
multiple tasks and stimulus types, particularly in the current era of rapidly
advancing large multimodal models (LMMs), where studying the multifaceted
capabilities of models has become a mainstream focus. Moreover, the perceptual
defects of LMMs are not investigated thoroughly, resulting in potential
security issues and suboptimal response efficiency. In this paper, we take an
initial attempt and demonstrate that there exist significant visual blind spots
in current LMMs. To systemically quantify this characteristic, we propose a new
concept, {\bf LMM-JND}, together with its determination pipeline. Targeting
uncovering the behavior commonalities in HVS-aligned visual perception tasks,
we delve into several LMM families and construct a large-scale dataset, named
VPA-JND, which contains 21.5k reference images with over 489k stimuli across 12
distortion types, to facilitate LMM-JND studies. VPA-JND exposes areas where
state-of-the-art LMMs, including GPT-4o and the InternVL2.5 series, struggle
with basic comparison queries and fall significantly short of human-level
visual performance. We further explore the effects of vision and language
backbones and find a notable correlation between their design philosophy that
may instruct the future refinement of LMMs for their visual acuity. Together,
our research underscores the significance of LMM-JND as a unique perspective
for studying LMMs, and predictable LMM-JND is crucial for security concerns.
This work will be available at https://github.com/zijianchen98/LMM-JND.

</details>


### [91] [Active Control Points-based 6DoF Pose Tracking for Industrial Metal Objects](https://arxiv.org/pdf/2507.01478)
*Chentao Shen, Ding Pan, Mingyu Mei, Zaixing He, Xinyue Zhao*

Main category: cs.CV

TL;DR: A novel 6DoF pose tracking method for industrial metal objects using active control points to address reflection challenges.


<details>
  <summary>Details</summary>
Motivation: Pose tracking for industrial metal objects is challenging due to reflections, requiring a robust solution.

Method: Uses active control points to generate edge features for optimization, avoiding 6DoF pose-based rendering, and includes optimal control point regression for robustness.

Result: Effective in dataset evaluation and real-world tasks, enabling real-time tracking of metal objects.

Conclusion: The method provides a viable solution for real-time pose tracking of industrial metal objects, with publicly available source code.

Abstract: Visual pose tracking is playing an increasingly vital role in industrial
contexts in recent years. However, the pose tracking for industrial metal
objects remains a challenging task especially in the real world-environments,
due to the reflection characteristic of metal objects. To address this issue,
we propose a novel 6DoF pose tracking method based on active control points.
The method uses image control points to generate edge feature for optimization
actively instead of 6DoF pose-based rendering, and serve them as optimization
variables. We also introduce an optimal control point regression method to
improve robustness. The proposed tracking method performs effectively in both
dataset evaluation and real world tasks, providing a viable solution for
real-time tracking of industrial metal objects. Our source code is made
publicly available at: https://github.com/tomatoma00/ACPTracking.

</details>


### [92] [What Really Matters for Robust Multi-Sensor HD Map Construction?](https://arxiv.org/pdf/2507.01484)
*Xiaoshuai Hao, Yuting Zhao, Yuheng Ji, Luanyuan Dai, Peng Hao, Dingzhe Li, Shuai Cheng, Rong Yin*

Main category: cs.CV

TL;DR: The paper proposes strategies to enhance the robustness of multi-modal fusion methods for HD map construction, focusing on data augmentation, a novel fusion module, and modality dropout training, achieving state-of-the-art results.


<details>
  <summary>Details</summary>
Motivation: Existing methods prioritize accuracy over robustness, which is critical for real-world autonomous driving applications.

Method: The approach includes data augmentation, a novel multi-modal fusion module, and modality dropout training, evaluated on 10 days of NuScenes data.

Result: The methods significantly improve robustness and achieve state-of-the-art performance on the NuScenes dataset.

Conclusion: The findings advance the development of robust and reliable HD map construction models for real-world autonomous driving.

Abstract: High-definition (HD) map construction methods are crucial for providing
precise and comprehensive static environmental information, which is essential
for autonomous driving systems. While Camera-LiDAR fusion techniques have shown
promising results by integrating data from both modalities, existing approaches
primarily focus on improving model accuracy and often neglect the robustness of
perception models, which is a critical aspect for real-world applications. In
this paper, we explore strategies to enhance the robustness of multi-modal
fusion methods for HD map construction while maintaining high accuracy. We
propose three key components: data augmentation, a novel multi-modal fusion
module, and a modality dropout training strategy. These components are
evaluated on a challenging dataset containing 10 days of NuScenes data. Our
experimental results demonstrate that our proposed methods significantly
enhance the robustness of baseline methods. Furthermore, our approach achieves
state-of-the-art performance on the clean validation set of the NuScenes
dataset. Our findings provide valuable insights for developing more robust and
reliable HD map construction models, advancing their applicability in
real-world autonomous driving scenarios. Project website:
https://robomap-123.github.io.

</details>


### [93] [AVC-DPO: Aligned Video Captioning via Direct Preference Optimization](https://arxiv.org/pdf/2507.01492)
*Jiyang Tang, Hengyi Li, Yifan Du, Wayne Xin Zhao*

Main category: cs.CV

TL;DR: AVC-DPO enhances video MLLMs for human-preferred captioning by aligning preferences via optimized prompts and training.


<details>
  <summary>Details</summary>
Motivation: Current video MLLMs struggle to adjust caption focus based on human preferences, especially for temporal dynamics and spatial information.

Method: AVC-DPO uses enhanced prompts targeting human-centric preferences and leverages varied prompt conditions for preference-aware training.

Result: Achieved top performance in the LOVE@CVPR'25 Workshop Track 1A, ranking first on the VDC benchmark.

Conclusion: AVC-DPO effectively aligns video captions with human preferences, improving video MLLMs' captioning capabilities.

Abstract: Although video multimodal large language models (video MLLMs) have achieved
substantial progress in video captioning tasks, it remains challenging to
adjust the focal emphasis of video captions according to human preferences. To
address this limitation, we propose Aligned Video Captioning via Direct
Preference Optimization (AVC-DPO), a post-training framework designed to
enhance captioning capabilities in video MLLMs through preference alignment.
Our approach designs enhanced prompts that specifically target temporal
dynamics and spatial information-two key factors that humans care about when
watching a video-thereby incorporating human-centric preferences. AVC-DPO
leverages the same foundation model's caption generation responses under varied
prompt conditions to conduct preference-aware training and caption alignment.
Using this framework, we have achieved exceptional performance in the
LOVE@CVPR'25 Workshop Track 1A: Video Detailed Captioning Challenge, achieving
first place on the Video Detailed Captioning (VDC) benchmark according to the
VDCSCORE evaluation metric.

</details>


### [94] [Crop Pest Classification Using Deep Learning Techniques: A Review](https://arxiv.org/pdf/2507.01494)
*Muhammad Hassam Ejaz, Muhammad Bilal, Usman Habib*

Main category: cs.CV

TL;DR: A review of 37 studies (2018-2025) on AI-based pest classification, highlighting the shift from CNNs to hybrid/transformer models, key challenges, and future directions.


<details>
  <summary>Details</summary>
Motivation: Traditional pest monitoring methods are slow and unscalable; AI offers automated, efficient solutions.

Method: Analyzed 37 studies, organizing them by crop type, pest species, model architecture, dataset usage, and technical challenges.

Result: Shift from CNNs to hybrid/transformer models improves accuracy and contextual understanding, but challenges like dataset imbalance and deployment hurdles persist.

Conclusion: AI-based pest monitoring shows promise but requires addressing dataset, detection, and deployment challenges for broader adoption.

Abstract: Insect pests continue to bring a serious threat to crop yields around the
world, and traditional methods for monitoring them are often slow, manual, and
difficult to scale. In recent years, deep learning has emerged as a powerful
solution, with techniques like convolutional neural networks (CNNs), vision
transformers (ViTs), and hybrid models gaining popularity for automating pest
detection. This review looks at 37 carefully selected studies published between
2018 and 2025, all focused on AI-based pest classification. The selected
research is organized by crop type, pest species, model architecture, dataset
usage, and key technical challenges. The early studies relied heavily on CNNs
but latest work is shifting toward hybrid and transformer-based models that
deliver higher accuracy and better contextual understanding. Still, challenges
like imbalanced datasets, difficulty in detecting small pests, limited
generalizability, and deployment on edge devices remain significant hurdles.
Overall, this review offers a structured overview of the field, highlights
useful datasets, and outlines the key challenges and future directions for
AI-based pest monitoring systems.

</details>


### [95] [ReFlex: Text-Guided Editing of Real Images in Rectified Flow via Mid-Step Feature Extraction and Attention Adaptation](https://arxiv.org/pdf/2507.01496)
*Jimyeong Kim, Jungwon Park, Yeji Song, Nojun Kwak, Wonjong Rhee*

Main category: cs.CV

TL;DR: A new real-image editing method for ReFlow improves image quality and text alignment by leveraging mid-step latent and adapted attention, outperforming prior methods.


<details>
  <summary>Details</summary>
Motivation: Adapting ReFlow for real-image editing is challenging despite its superiority in text-to-image tasks.

Method: Analyzes intermediate representations of multimodal transformer blocks, extracts key features using mid-step latent, and adapts attention during injection.

Result: Superior performance over nine baselines on two benchmarks, validated by human evaluations.

Conclusion: The proposed training-free method enhances editability and alignment without requiring masks or source prompts.

Abstract: Rectified Flow text-to-image models surpass diffusion models in image quality
and text alignment, but adapting ReFlow for real-image editing remains
challenging. We propose a new real-image editing method for ReFlow by analyzing
the intermediate representations of multimodal transformer blocks and
identifying three key features. To extract these features from real images with
sufficient structural preservation, we leverage mid-step latent, which is
inverted only up to the mid-step. We then adapt attention during injection to
improve editability and enhance alignment to the target text. Our method is
training-free, requires no user-provided mask, and can be applied even without
a source prompt. Extensive experiments on two benchmarks with nine baselines
demonstrate its superior performance over prior methods, further validated by
human evaluations confirming a strong user preference for our approach.

</details>


### [96] [Integrating Traditional and Deep Learning Methods to Detect Tree Crowns in Satellite Images](https://arxiv.org/pdf/2507.01502)
*Ozan Durgut, Beril Kallfelz-Sirmacek, Cem Unsalan*

Main category: cs.CV

TL;DR: The paper proposes a rule-based approach combining traditional and deep learning methods to improve tree crown detection for forest monitoring.


<details>
  <summary>Details</summary>
Motivation: Addressing global environmental issues like deforestation requires better forest monitoring, which can be automated using remote sensing and computer vision.

Method: Integrates traditional methods for feature extraction and segmentation with deep learning for tree crown detection, followed by rule-based post-processing to enhance results.

Result: The proposed method increases the number of detected tree crowns and is compared with existing approaches, highlighting its advantages and limitations.

Conclusion: The rule-based integration of traditional and deep learning methods improves tree crown detection, offering a robust solution for forest monitoring.

Abstract: Global warming, loss of biodiversity, and air pollution are among the most
significant problems facing Earth. One of the primary challenges in addressing
these issues is the lack of monitoring forests to protect them. To tackle this
problem, it is important to leverage remote sensing and computer vision methods
to automate monitoring applications. Hence, automatic tree crown detection
algorithms emerged based on traditional and deep learning methods. In this
study, we first introduce two different tree crown detection methods based on
these approaches. Then, we form a novel rule-based approach that integrates
these two methods to enhance robustness and accuracy of tree crown detection
results. While traditional methods are employed for feature extraction and
segmentation of forested areas, deep learning methods are used to detect tree
crowns in our method. With the proposed rule-based approach, we post-process
these results, aiming to increase the number of detected tree crowns through
neighboring trees and localized operations. We compare the obtained results
with the proposed method in terms of the number of detected tree crowns and
report the advantages, disadvantages, and areas for improvement of the obtained
outcomes.

</details>


### [97] [Following the Clues: Experiments on Person Re-ID using Cross-Modal Intelligence](https://arxiv.org/pdf/2507.01504)
*Robert Aufschläger, Youssef Shoeb, Azarm Nowzad, Michael Heigl, Fabian Bally, Martin Schramm*

Main category: cs.CV

TL;DR: A novel framework, cRID, is introduced to detect and leverage textual describable PII in street-level datasets for improved person re-identification, addressing privacy risks.


<details>
  <summary>Details</summary>
Motivation: Street-level datasets for autonomous driving and AI research contain PII, posing privacy risks for pedestrians. Current methods often overlook non-biometric PII.

Method: Combines Large Vision-Language Models, Graph Attention Networks, and representation learning to detect and use interpretable PII features for Re-ID.

Result: Improved performance in cross-dataset Re-ID scenarios, particularly from Market-1501 to CUHK03-np.

Conclusion: cRID effectively addresses privacy risks by detecting meaningful PII and enhancing Re-ID, with practical utility demonstrated.

Abstract: The collection and release of street-level recordings as Open Data play a
vital role in advancing autonomous driving systems and AI research. However,
these datasets pose significant privacy risks, particularly for pedestrians,
due to the presence of Personally Identifiable Information (PII) that extends
beyond biometric traits such as faces. In this paper, we present cRID, a novel
cross-modal framework combining Large Vision-Language Models, Graph Attention
Networks, and representation learning to detect textual describable clues of
PII and enhance person re-identification (Re-ID). Our approach focuses on
identifying and leveraging interpretable features, enabling the detection of
semantically meaningful PII beyond low-level appearance cues. We conduct a
systematic evaluation of PII presence in person image datasets. Our experiments
show improved performance in practical cross-dataset Re-ID scenarios, notably
from Market-1501 to CUHK03-np (detected), highlighting the framework's
practical utility. Code is available at https://github.com/RAufschlaeger/cRID.

</details>


### [98] [Mamba Guided Boundary Prior Matters: A New Perspective for Generalized Polyp Segmentation](https://arxiv.org/pdf/2507.01509)
*Tapas K. Dutta, Snehashis Majhi, Deepak Ranjan Nayak, Debesh Jha*

Main category: cs.CV

TL;DR: SAM-MaGuP enhances polyp segmentation by integrating boundary distillation and a 1D-2D Mamba adapter into SAM, outperforming existing methods in accuracy and robustness.


<details>
  <summary>Details</summary>
Motivation: Current polyp segmentation methods struggle with weak boundaries and poor generalizability, limiting clinical application.

Method: Proposes SAM-MaGuP, combining boundary distillation and a 1D-2D Mamba adapter within SAM for improved feature learning and boundary resolution.

Result: Outperforms state-of-the-art methods across five datasets, achieving superior segmentation accuracy and robustness.

Conclusion: SAM-MaGuP sets a new benchmark in polyp segmentation, addressing key challenges like weak boundaries and generalizability.

Abstract: Polyp segmentation in colonoscopy images is crucial for early detection and
diagnosis of colorectal cancer. However, this task remains a significant
challenge due to the substantial variations in polyp shape, size, and color, as
well as the high similarity between polyps and surrounding tissues, often
compounded by indistinct boundaries. While existing encoder-decoder CNN and
transformer-based approaches have shown promising results, they struggle with
stable segmentation performance on polyps with weak or blurry boundaries. These
methods exhibit limited abilities to distinguish between polyps and non-polyps
and capture essential boundary cues. Moreover, their generalizability still
falls short of meeting the demands of real-time clinical applications. To
address these limitations, we propose SAM-MaGuP, a groundbreaking approach for
robust polyp segmentation. By incorporating a boundary distillation module and
a 1D-2D Mamba adapter within the Segment Anything Model (SAM), SAM-MaGuP excels
at resolving weak boundary challenges and amplifies feature learning through
enriched global contextual interactions. Extensive evaluations across five
diverse datasets reveal that SAM-MaGuP outperforms state-of-the-art methods,
achieving unmatched segmentation accuracy and robustness. Our key innovations,
a Mamba-guided boundary prior and a 1D-2D Mamba block, set a new benchmark in
the field, pushing the boundaries of polyp segmentation to new heights.

</details>


### [99] [Exploring Pose-based Sign Language Translation: Ablation Studies and Attention Insights](https://arxiv.org/pdf/2507.01532)
*Tomas Zelezny, Jakub Straka, Vaclav Javorek, Ondrej Valach, Marek Hruz, Ivan Gruber*

Main category: cs.CV

TL;DR: The paper investigates how pose-based data preprocessing (normalization, interpolation, augmentation) impacts Sign Language Translation (SLT) performance using a transformer-based model, showing significant improvements in robustness and generalization.


<details>
  <summary>Details</summary>
Motivation: To enhance SLT systems by optimizing pose-based data preprocessing techniques for better translation accuracy.

Method: Uses a modified T5 encoder-decoder transformer model on pose representations, with ablation studies on YouTubeASL and How2Sign datasets.

Result: Appropriate preprocessing techniques improve model robustness and generalization; a dedicated register token further boosts performance.

Conclusion: Optimized preprocessing and a register token enhance SLT performance, with code and preprocessed data made publicly available.

Abstract: Sign Language Translation (SLT) has evolved significantly, moving from
isolated recognition approaches to complex, continuous gloss-free translation
systems. This paper explores the impact of pose-based data preprocessing
techniques - normalization, interpolation, and augmentation - on SLT
performance. We employ a transformer-based architecture, adapting a modified T5
encoder-decoder model to process pose representations. Through extensive
ablation studies on YouTubeASL and How2Sign datasets, we analyze how different
preprocessing strategies affect translation accuracy. Our results demonstrate
that appropriate normalization, interpolation, and augmentation techniques can
significantly improve model robustness and generalization abilities.
Additionally, we provide a deep analysis of the model's attentions and reveal
interesting behavior suggesting that adding a dedicated register token can
improve overall model performance. We publish our code on our GitHub
repository, including the preprocessed YouTubeASL data.

</details>


### [100] [TrackingMiM: Efficient Mamba-in-Mamba Serialization for Real-time UAV Object Tracking](https://arxiv.org/pdf/2507.01535)
*Bingxi Liu, Calvin Chen, Junhao Li, Guyang Yu, Haoqian Song, Xuchen Liu, Jinqiang Cui, Hong Zhang*

Main category: cs.CV

TL;DR: TrackingMiM, a Mamba-in-Mamba architecture, addresses ViT's quadratic complexity in UAV tracking by leveraging efficient long-sequence modeling and temporal-spatial coherence.


<details>
  <summary>Details</summary>
Motivation: Quadratic complexity of ViT hinders real-time UAV tracking; Mamba's efficiency and long-sequence capability offer a solution.

Method: Proposes TrackingMiM, a nested Mamba scan for temporal-spatial coherence, with template frames as query tokens.

Result: Achieves state-of-the-art precision and higher speed on five UAV tracking benchmarks.

Conclusion: TrackingMiM effectively balances accuracy and computational efficiency for real-time UAV tracking.

Abstract: The Vision Transformer (ViT) model has long struggled with the challenge of
quadratic complexity, a limitation that becomes especially critical in unmanned
aerial vehicle (UAV) tracking systems, where data must be processed in real
time. In this study, we explore the recently proposed State-Space Model, Mamba,
leveraging its computational efficiency and capability for long-sequence
modeling to effectively process dense image sequences in tracking tasks. First,
we highlight the issue of temporal inconsistency in existing Mamba-based
methods, specifically the failure to account for temporal continuity in the
Mamba scanning mechanism. Secondly, building upon this insight,we propose
TrackingMiM, a Mamba-in-Mamba architecture, a minimal-computation burden model
for handling image sequence of tracking problem. In our framework, the mamba
scan is performed in a nested way while independently process temporal and
spatial coherent patch tokens. While the template frame is encoded as query
token and utilized for tracking in every scan. Extensive experiments conducted
on five UAV tracking benchmarks confirm that the proposed TrackingMiM achieves
state-of-the-art precision while offering noticeable higher speed in UAV
tracking.

</details>


### [101] [A Multi-Centric Anthropomorphic 3D CT Phantom-Based Benchmark Dataset for Harmonization](https://arxiv.org/pdf/2507.01539)
*Mohammadreza Amirian, Michael Bach, Oscar Jimenez-del-Toro, Christoph Aberle, Roger Schaer, Vincent Andrearczyk, Jean-Félix Maestrati, Maria Martin Asiain, Kyriakos Flouris, Markus Obmann, Clarisse Dromain, Benoît Dufour, Pierre-Alexandre Alois Poletti, Hendrik von Tengg-Kobligk, Rolf Hügli, Martin Kretzschmar, Hatem Alkadhi, Ender Konukoglu, Henning Müller, Bram Stieltjes, Adrien Depeursinge*

Main category: cs.CV

TL;DR: The paper introduces an open-source benchmark dataset for AI harmonization in CT analysis, addressing data distribution shifts caused by scanner variations. It includes 1378 phantom scans and provides baseline methods for stability and classification.


<details>
  <summary>Details</summary>
Motivation: AI in medicine faces poor generalization due to data distribution shifts from scanner variations. Harmonization techniques can mitigate this, but lack standardized benchmarks.

Method: A phantom-based dataset (1378 scans from 13 scanners) was created to eliminate patient variability. Baseline methods for image/feature stability and liver classification were developed.

Result: The dataset and baseline methods enable standardized testing of AI harmonization techniques, promoting robustness in CT analysis.

Conclusion: This work provides a valuable resource for advancing AI harmonization in medical imaging, addressing key challenges in generalization.

Abstract: Artificial intelligence (AI) has introduced numerous opportunities for human
assistance and task automation in medicine. However, it suffers from poor
generalization in the presence of shifts in the data distribution. In the
context of AI-based computed tomography (CT) analysis, significant data
distribution shifts can be caused by changes in scanner manufacturer,
reconstruction technique or dose. AI harmonization techniques can address this
problem by reducing distribution shifts caused by various acquisition settings.
This paper presents an open-source benchmark dataset containing CT scans of an
anthropomorphic phantom acquired with various scanners and settings, which
purpose is to foster the development of AI harmonization techniques. Using a
phantom allows fixing variations attributed to inter- and intra-patient
variations. The dataset includes 1378 image series acquired with 13 scanners
from 4 manufacturers across 8 institutions using a harmonized protocol as well
as several acquisition doses. Additionally, we present a methodology, baseline
results and open-source code to assess image- and feature-level stability and
liver tissue classification, promoting the development of AI harmonization
strategies.

</details>


### [102] [Interpolation-Based Event Visual Data Filtering Algorithms](https://arxiv.org/pdf/2507.01557)
*Marcin Kowlaczyk, Tomasz Kryjak*

Main category: cs.CV

TL;DR: Proposes noise reduction methods for event cameras using IIR filters, achieving ~99% noise removal with low memory usage.


<details>
  <summary>Details</summary>
Motivation: Event cameras produce noisy data streams, limiting their application potential.

Method: Four algorithms based on IIR filters, tested on modified event datasets with artificial and recorded noise.

Result: ~99% noise removal, preserving valid signal; uses ~30KB memory for 1280x720 resolution.

Conclusion: The methods are efficient and suitable for embedded devices.

Abstract: The field of neuromorphic vision is developing rapidly, and event cameras are
finding their way into more and more applications. However, the data stream
from these sensors is characterised by significant noise. In this paper, we
propose a method for event data that is capable of removing approximately 99\%
of noise while preserving the majority of the valid signal. We have proposed
four algorithms based on the matrix of infinite impulse response (IIR) filters
method. We compared them on several event datasets that were further modified
by adding artificially generated noise and noise recorded with dynamic vision
sensor. The proposed methods use about 30KB of memory for a sensor with a
resolution of 1280 x 720 and is therefore well suited for implementation in
embedded devices.

</details>


### [103] [A Gift from the Integration of Discriminative and Diffusion-based Generative Learning: Boundary Refinement Remote Sensing Semantic Segmentation](https://arxiv.org/pdf/2507.01573)
*Hao Wang, Keyan Hu, Xin Guo, Haifeng Li, Chao Tao*

Main category: cs.CV

TL;DR: The paper proposes IDGBR, a framework combining discriminative and generative learning to refine boundaries in remote sensing semantic segmentation, addressing limitations in high-frequency feature learning.


<details>
  <summary>Details</summary>
Motivation: Existing segmentation models focus on low-frequency features (semantic correctness) but struggle with high-frequency details (boundary localization). Diffusion models excel at high-frequency details but lack semantic inference for low-frequency features.

Method: IDGBR integrates discriminative learning (for coarse segmentation) and diffusion-based generative learning (for boundary refinement). A conditioning guidance network uses the coarse map and original image to guide an iterative denoising process.

Result: Experiments on five datasets show IDGBR consistently refines boundaries in coarse results from various discriminative architectures.

Conclusion: IDGBR effectively combines discriminative and generative learning to improve boundary refinement in semantic segmentation, validated across multiple datasets.

Abstract: Remote sensing semantic segmentation must address both what the ground
objects are within an image and where they are located. Consequently,
segmentation models must ensure not only the semantic correctness of
large-scale patches (low-frequency information) but also the precise
localization of boundaries between patches (high-frequency information).
However, most existing approaches rely heavily on discriminative learning,
which excels at capturing low-frequency features, while overlooking its
inherent limitations in learning high-frequency features for semantic
segmentation. Recent studies have revealed that diffusion generative models
excel at generating high-frequency details. Our theoretical analysis confirms
that the diffusion denoising process significantly enhances the model's ability
to learn high-frequency features; however, we also observe that these models
exhibit insufficient semantic inference for low-frequency features when guided
solely by the original image. Therefore, we integrate the strengths of both
discriminative and generative learning, proposing the Integration of
Discriminative and diffusion-based Generative learning for Boundary Refinement
(IDGBR) framework. The framework first generates a coarse segmentation map
using a discriminative backbone model. This map and the original image are fed
into a conditioning guidance network to jointly learn a guidance representation
subsequently leveraged by an iterative denoising diffusion process refining the
coarse segmentation. Extensive experiments across five remote sensing semantic
segmentation datasets (binary and multi-class segmentation) confirm our
framework's capability of consistent boundary refinement for coarse results
from diverse discriminative architectures. The source code will be available at
https://github.com/KeyanHu-git/IDGBR.

</details>


### [104] [SketchColour: Channel Concat Guided DiT-based Sketch-to-Colour Pipeline for 2D Animation](https://arxiv.org/pdf/2507.01586)
*Bryan Constantine Sadihin, Michael Hua Wang, Shei Pern Chua, Hang Su*

Main category: cs.CV

TL;DR: SketchColour is a sketch-to-colour pipeline for 2D animation using a diffusion transformer (DiT) backbone, reducing labor and resource usage while outperforming existing methods.


<details>
  <summary>Details</summary>
Motivation: High-quality 2D animation production is labor-intensive, requiring manual drawing and coloring of frames.

Method: Replaces U-Net denoiser with DiT architecture, uses lightweight adapters and LoRA finetuning for sketch integration, avoiding ControlNet's inefficiencies.

Result: Outperforms state-of-the-art video colourization methods on the SAKUGA dataset with half the training data, producing temporally coherent animations.

Conclusion: SketchColour offers an efficient, high-quality solution for 2D animation colourization, reducing manual effort and resource usage.

Abstract: The production of high-quality 2D animation is highly labor-intensive
process, as animators are currently required to draw and color a large number
of frames by hand. We present SketchColour, the first sketch-to-colour pipeline
for 2D animation built on a diffusion transformer (DiT) backbone. By replacing
the conventional U-Net denoiser with a DiT-style architecture and injecting
sketch information via lightweight channel-concatenation adapters accompanied
with LoRA finetuning, our method natively integrates conditioning without the
parameter and memory bloat of a duplicated ControlNet, greatly reducing
parameter count and GPU memory usage. Evaluated on the SAKUGA dataset,
SketchColour outperforms previous state-of-the-art video colourization methods
across all metrics, despite using only half the training data of competing
models. Our approach produces temporally coherent animations with minimal
artifacts such as colour bleeding or object deformation. Our code is available
at: https://bconstantine.github.io/SketchColour .

</details>


### [105] [ECCV 2024 W-CODA: 1st Workshop on Multimodal Perception and Comprehension of Corner Cases in Autonomous Driving](https://arxiv.org/pdf/2507.01735)
*Kai Chen, Ruiyuan Gao, Lanqing Hong, Hang Xu, Xu Jia, Holger Caesar, Dengxin Dai, Bingbing Liu, Dzmitry Tsishkou, Songcen Xu, Chunjing Xu, Qiang Xu, Huchuan Lu, Dit-Yan Yeung*

Main category: cs.CV

TL;DR: Summary of the 1st W-CODA workshop at ECCV 2024, focusing on autonomous driving corner cases using multimodal techniques.


<details>
  <summary>Details</summary>
Motivation: To advance next-gen autonomous driving solutions by addressing corner cases with cutting-edge multimodal perception.

Method: Organized a workshop with 5 speakers, collected research papers, and held a dual-track challenge for scene understanding and generation.

Result: Pioneering effort to bridge the gap between advanced techniques and reliable self-driving agents for corner cases.

Conclusion: The workshop aims to foster continuous progress in robust autonomous driving solutions.

Abstract: In this paper, we present details of the 1st W-CODA workshop, held in
conjunction with the ECCV 2024. W-CODA aims to explore next-generation
solutions for autonomous driving corner cases, empowered by state-of-the-art
multimodal perception and comprehension techniques. 5 Speakers from both
academia and industry are invited to share their latest progress and opinions.
We collect research papers and hold a dual-track challenge, including both
corner case scene understanding and generation. As the pioneering effort, we
will continuously bridge the gap between frontier autonomous driving techniques
and fully intelligent, reliable self-driving agents robust towards corner
cases.

</details>


### [106] [Autonomous AI Surveillance: Multimodal Deep Learning for Cognitive and Behavioral Monitoring](https://arxiv.org/pdf/2507.01590)
*Ameer Hamza, Zuhaib Hussain But, Umar Arif, Samiya, M. Abdullah Asad, Muhammad Naeem*

Main category: cs.CV

TL;DR: A novel classroom surveillance system integrates drowsiness detection, mobile phone tracking, and face recognition to assess student attentiveness with high precision, achieving strong performance metrics.


<details>
  <summary>Details</summary>
Motivation: To enhance classroom monitoring and student engagement assessment by combining multiple modalities for real-time insights and automated attendance recording.

Method: Uses YOLOv8 for mobile phone and sleep detection, LResNet Occ FC for face recognition, and integrates these models with a PHP web application and ESP32-CAM hardware.

Result: Sleep detection: 97.42% mAP@50; face recognition: 86.45% validation accuracy; mobile phone detection: 85.89% mAP@50.

Conclusion: The system provides scalable, real-time monitoring and automated attendance, improving classroom management in diverse educational settings.

Abstract: This study presents a novel classroom surveillance system that integrates
multiple modalities, including drowsiness, tracking of mobile phone usage, and
face recognition,to assess student attentiveness with enhanced precision.The
system leverages the YOLOv8 model to detect both mobile phone and sleep
usage,(Ghatge et al., 2024) while facial recognition is achieved through
LResNet Occ FC body tracking using YOLO and MTCNN.(Durai et al., 2024) These
models work in synergy to provide comprehensive, real-time monitoring, offering
insights into student engagement and behavior.(S et al., 2023) The framework is
trained on specialized datasets, such as the RMFD dataset for face recognition
and a Roboflow dataset for mobile phone detection. The extensive evaluation of
the system shows promising results. Sleep detection achieves 97. 42% mAP@50,
face recognition achieves 86. 45% validation accuracy and mobile phone
detection reach 85. 89% mAP@50. The system is implemented within a core PHP web
application and utilizes ESP32-CAM hardware for seamless data capture.(Neto et
al., 2024) This integrated approach not only enhances classroom monitoring, but
also ensures automatic attendance recording via face recognition as students
remain seated in the classroom, offering scalability for diverse educational
environments.(Banada,2025)

</details>


### [107] [DepthSync: Diffusion Guidance-Based Depth Synchronization for Scale- and Geometry-Consistent Video Depth Estimation](https://arxiv.org/pdf/2507.01603)
*Yue-Jiang Dong, Wang Zhao, Jiale Xu, Ying Shan, Song-Hai Zhang*

Main category: cs.CV

TL;DR: DepthSync is a training-free framework using diffusion guidance to achieve scale- and geometry-consistent depth predictions for long videos, addressing challenges in existing methods.


<details>
  <summary>Details</summary>
Motivation: Existing methods for video depth estimation suffer from scale discrepancies and geometric inconsistencies, especially in long videos, due to reliance on 2D priors and sliding windows.

Method: DepthSync introduces scale guidance to synchronize depth scales across windows and geometry guidance to enforce 3D alignment within windows, leveraging diffusion priors.

Result: Experiments show DepthSync improves scale and geometry consistency in depth predictions for long videos.

Conclusion: DepthSync effectively addresses the limitations of existing methods, offering a robust solution for consistent depth estimation in long videos.

Abstract: Diffusion-based video depth estimation methods have achieved remarkable
success with strong generalization ability. However, predicting depth for long
videos remains challenging. Existing methods typically split videos into
overlapping sliding windows, leading to accumulated scale discrepancies across
different windows, particularly as the number of windows increases.
Additionally, these methods rely solely on 2D diffusion priors, overlooking the
inherent 3D geometric structure of video depths, which results in geometrically
inconsistent predictions. In this paper, we propose DepthSync, a novel,
training-free framework using diffusion guidance to achieve scale- and
geometry-consistent depth predictions for long videos. Specifically, we
introduce scale guidance to synchronize the depth scale across windows and
geometry guidance to enforce geometric alignment within windows based on the
inherent 3D constraints in video depths. These two terms work synergistically,
steering the denoising process toward consistent depth predictions. Experiments
on various datasets validate the effectiveness of our method in producing depth
estimates with improved scale and geometry consistency, particularly for long
videos.

</details>


### [108] [Survivability of Backdoor Attacks on Unconstrained Face Recognition Systems](https://arxiv.org/pdf/2507.01607)
*Quentin Le Roux, Yannick Teglia, Teddy Furon, Philippe Loubet-Moundi, Eric Bourbao*

Main category: cs.CV

TL;DR: This paper investigates backdoor attacks in deep learning-based face recognition systems, demonstrating their feasibility and impact, and suggests countermeasures.


<details>
  <summary>Details</summary>
Motivation: The study addresses the lack of research on DNN backdoor attacks in real-life, unconstrained face recognition systems, highlighting security vulnerabilities.

Method: The paper explores backdoor attacks holistically, including face generation and landmark shift attacks, and tests them across 20 pipeline configurations and 15 attack cases.

Result: The research shows that a single backdoor can bypass an entire system's function, proving the vulnerability of face recognition pipelines.

Conclusion: The paper concludes by offering best practices and countermeasures to mitigate backdoor threats in face recognition systems.

Abstract: The widespread use of deep learning face recognition raises several security
concerns. Although prior works point at existing vulnerabilities, DNN backdoor
attacks against real-life, unconstrained systems dealing with images captured
in the wild remain a blind spot of the literature. This paper conducts the
first system-level study of backdoors in deep learning-based face recognition
systems. This paper yields four contributions by exploring the feasibility of
DNN backdoors on these pipelines in a holistic fashion. We demonstrate for the
first time two backdoor attacks on the face detection task: face generation and
face landmark shift attacks. We then show that face feature extractors trained
with large margin losses also fall victim to backdoor attacks. Combining our
models, we then show using 20 possible pipeline configurations and 15 attack
cases that a single backdoor enables an attacker to bypass the entire function
of a system. Finally, we provide stakeholders with several best practices and
countermeasures.

</details>


### [109] [Prompt Guidance and Human Proximal Perception for HOT Prediction with Regional Joint Loss](https://arxiv.org/pdf/2507.01630)
*Yuxiao Wang, Yu Lei, Zhenao Wei, Weiying Xue, Xinyu Jiang, Nan Zhuang, Qi Liu*

Main category: cs.CV

TL;DR: The paper introduces P3HOT, a framework for Human-Object conTact (HOT) detection, combining prompt guidance and human proximal perception to improve accuracy and consistency in identifying contact regions.


<details>
  <summary>Details</summary>
Motivation: Current HOT detection models are limited to single image types, causing over-segmentation and inconsistent category labeling in regions with minimal interaction.

Method: P3HOT uses semantic-driven prompts to focus on relevant regions and a human proximal perception mechanism for dynamic depth range perception. It also introduces a Regional Joint Loss (RJLoss) and a new metric, AD-Acc., for better evaluation.

Result: P3HOT achieves state-of-the-art performance, with improvements of 0.7, 2.0, 1.6, and 11.0 in SC-Acc., mIoU, wIoU, and AD-Acc. metrics on the HOT-Annotated dataset.

Conclusion: The proposed P3HOT framework effectively addresses limitations in HOT detection, offering superior performance and a novel evaluation approach.

Abstract: The task of Human-Object conTact (HOT) detection involves identifying the
specific areas of the human body that are touching objects. Nevertheless,
current models are restricted to just one type of image, often leading to too
much segmentation in areas with little interaction, and struggling to maintain
category consistency within specific regions. To tackle this issue, a HOT
framework, termed \textbf{P3HOT}, is proposed, which blends \textbf{P}rompt
guidance and human \textbf{P}roximal \textbf{P}erception. To begin with, we
utilize a semantic-driven prompt mechanism to direct the network's attention
towards the relevant regions based on the correlation between image and text.
Then a human proximal perception mechanism is employed to dynamically perceive
key depth range around the human, using learnable parameters to effectively
eliminate regions where interactions are not expected. Calculating depth
resolves the uncertainty of the overlap between humans and objects in a 2D
perspective, providing a quasi-3D viewpoint. Moreover, a Regional Joint Loss
(RJLoss) has been created as a new loss to inhibit abnormal categories in the
same area. A new evaluation metric called ``AD-Acc.'' is introduced to address
the shortcomings of existing methods in addressing negative samples.
Comprehensive experimental results demonstrate that our approach achieves
state-of-the-art performance in four metrics across two benchmark datasets.
Specifically, our model achieves an improvement of \textbf{0.7}$\uparrow$,
\textbf{2.0}$\uparrow$, \textbf{1.6}$\uparrow$, and \textbf{11.0}$\uparrow$ in
SC-Acc., mIoU, wIoU, and AD-Acc. metrics, respectively, on the HOT-Annotated
dataset. Code is available at https://github.com/YuxiaoWang-AI/P3HOT.

</details>


### [110] [Tile and Slide : A New Framework for Scaling NeRF from Local to Global 3D Earth Observation](https://arxiv.org/pdf/2507.01631)
*Camille Billouard, Dawa Derksen, Alexandre Constantin, Bruno Vallet*

Main category: cs.CV

TL;DR: Snake-NeRF scales NeRF to large scenes by dividing regions into non-overlapping 3D tiles and using an out-of-core method, enabling single-device processing without quality loss.


<details>
  <summary>Details</summary>
Motivation: Existing NeRF methods are limited to small scenes due to high memory usage during training.

Method: Divides the scene into non-overlapping 3D tiles, crops images with overlap, and uses a $2\times2$ tile progression strategy and segmented sampler to avoid edge errors.

Result: Processes large satellite images with linear time complexity on a single GPU, maintaining quality.

Conclusion: Snake-NeRF effectively scales NeRF for large scenes without compromising quality.

Abstract: Neural Radiance Fields (NeRF) have recently emerged as a paradigm for 3D
reconstruction from multiview satellite imagery. However, state-of-the-art NeRF
methods are typically constrained to small scenes due to the memory footprint
during training, which we study in this paper. Previous work on large-scale
NeRFs palliate this by dividing the scene into NeRFs. This paper introduces
Snake-NeRF, a framework that scales to large scenes. Our out-of-core method
eliminates the need to load all images and networks simultaneously, and
operates on a single device. We achieve this by dividing the region of interest
into NeRFs that 3D tile without overlap. Importantly, we crop the images with
overlap to ensure each NeRFs is trained with all the necessary pixels. We
introduce a novel $2\times 2$ 3D tile progression strategy and segmented
sampler, which together prevent 3D reconstruction errors along the tile edges.
Our experiments conclude that large satellite images can effectively be
processed with linear time complexity, on a single GPU, and without compromise
in quality.

</details>


### [111] [Depth Anything at Any Condition](https://arxiv.org/pdf/2507.01634)
*Boyuan Sun, Modi Jin, Bowen Yin, Qibin Hou*

Main category: cs.CV

TL;DR: DepthAnything-AC is a monocular depth estimation model designed for diverse environmental conditions, using unsupervised consistency regularization and Spatial Distance Constraint for improved performance.


<details>
  <summary>Details</summary>
Motivation: Existing foundation MDE models struggle in complex open-world environments with challenging conditions like illumination variations and adverse weather.

Method: Proposes unsupervised consistency regularization finetuning and Spatial Distance Constraint for patch-level relative relationships.

Result: Demonstrates zero-shot capabilities across diverse benchmarks, including adverse weather and synthetic corruption.

Conclusion: DepthAnything-AC effectively handles diverse conditions, improving semantic boundaries and detail accuracy.

Abstract: We present Depth Anything at Any Condition (DepthAnything-AC), a foundation
monocular depth estimation (MDE) model capable of handling diverse
environmental conditions. Previous foundation MDE models achieve impressive
performance across general scenes but not perform well in complex open-world
environments that involve challenging conditions, such as illumination
variations, adverse weather, and sensor-induced distortions. To overcome the
challenges of data scarcity and the inability of generating high-quality
pseudo-labels from corrupted images, we propose an unsupervised consistency
regularization finetuning paradigm that requires only a relatively small amount
of unlabeled data. Furthermore, we propose the Spatial Distance Constraint to
explicitly enforce the model to learn patch-level relative relationships,
resulting in clearer semantic boundaries and more accurate details.
Experimental results demonstrate the zero-shot capabilities of DepthAnything-AC
across diverse benchmarks, including real-world adverse weather benchmarks,
synthetic corruption benchmarks, and general benchmarks.
  Project Page: https://ghost233lism.github.io/depthanything-AC-page
  Code: https://github.com/HVision-NKU/DepthAnythingAC

</details>


### [112] [SAILViT: Towards Robust and Generalizable Visual Backbones for MLLMs via Gradual Feature Refinement](https://arxiv.org/pdf/2507.01643)
*Weijie Yin, Dingkang Yang, Hongyuan Dong, Zijian Kang, Jiacong Wang, Xiao Liang, Chao Feng, Jiao Ran*

Main category: cs.CV

TL;DR: SAILViT is a Vision Transformer designed to improve Multimodal Large Language Models (MLLMs) by addressing parameter conflicts and semantic gaps through gradual feature refinement.


<details>
  <summary>Details</summary>
Motivation: Existing ViTs struggle with direct co-training with LLMs due to initialization conflicts and modality gaps, limiting MLLM performance.

Method: SAILViT introduces gradual feature learning for coarse-to-fine alignment and knowledge infusion, enhancing robustness and generalizability.

Result: SAILViT significantly boosts MLLM performance on the OpenCompass benchmark across diverse tasks.

Conclusion: SAILViT effectively bridges ViTs and LLMs, enabling better multimodal interaction and performance in MLLMs.

Abstract: Vision Transformers (ViTs) are essential as foundation backbones in
establishing the visual comprehension capabilities of Multimodal Large Language
Models (MLLMs). Although most ViTs achieve impressive performance through
image-text pair-based contrastive learning or self-supervised mechanisms, they
struggle to engage in connector-based co-training directly with LLMs due to
potential parameter initialization conflicts and modality semantic gaps. To
address the above challenges, this paper proposes SAILViT, a gradual feature
learning-enhanced ViT for facilitating MLLMs to break through performance
bottlenecks in complex multimodal interactions. SAILViT achieves
coarse-to-fine-grained feature alignment and world knowledge infusion with
gradual feature refinement, which better serves target training demands. We
perform thorough empirical analyses to confirm the powerful robustness and
generalizability of SAILViT across different dimensions, including parameter
sizes, model architectures, training strategies, and data scales. Equipped with
SAILViT, existing MLLMs show significant and consistent performance
improvements on the OpenCompass benchmark across extensive downstream tasks.
SAILViT series models are released at
https://huggingface.co/BytedanceDouyinContent.

</details>


### [113] [RobuSTereo: Robust Zero-Shot Stereo Matching under Adverse Weather](https://arxiv.org/pdf/2507.01653)
*Yuran Wang, Yingping Liang, Yutao Hu, Ying Fu*

Main category: cs.CV

TL;DR: RobuSTereo enhances stereo matching models' zero-shot generalization in adverse weather by addressing data scarcity and feature extraction challenges using synthetic data and a robust feature encoder.


<details>
  <summary>Details</summary>
Motivation: Stereo matching models struggle in adverse weather due to lack of training data and difficulty in feature extraction, limiting zero-shot generalization.

Method: Proposes a diffusion-based simulation pipeline with stereo consistency for synthetic data and a feature encoder combining ConvNet and denoising transformer.

Result: Improves robustness and generalization in adverse weather, with better disparity estimation accuracy.

Conclusion: RobuSTereo effectively addresses challenges in stereo matching for adverse weather, enhancing model performance.

Abstract: Learning-based stereo matching models struggle in adverse weather conditions
due to the scarcity of corresponding training data and the challenges in
extracting discriminative features from degraded images. These limitations
significantly hinder zero-shot generalization to out-of-distribution weather
conditions. In this paper, we propose \textbf{RobuSTereo}, a novel framework
that enhances the zero-shot generalization of stereo matching models under
adverse weather by addressing both data scarcity and feature extraction
challenges. First, we introduce a diffusion-based simulation pipeline with a
stereo consistency module, which generates high-quality stereo data tailored
for adverse conditions. By training stereo matching models on our synthetic
datasets, we reduce the domain gap between clean and degraded images,
significantly improving the models' robustness to unseen weather conditions.
The stereo consistency module ensures structural alignment across synthesized
image pairs, preserving geometric integrity and enhancing depth estimation
accuracy. Second, we design a robust feature encoder that combines a
specialized ConvNet with a denoising transformer to extract stable and reliable
features from degraded images. The ConvNet captures fine-grained local
structures, while the denoising transformer refines global representations,
effectively mitigating the impact of noise, low visibility, and weather-induced
distortions. This enables more accurate disparity estimation even under
challenging visual conditions. Extensive experiments demonstrate that
\textbf{RobuSTereo} significantly improves the robustness and generalization of
stereo matching models across diverse adverse weather scenarios.

</details>


### [114] [SPoT: Subpixel Placement of Tokens in Vision Transformers](https://arxiv.org/pdf/2507.01654)
*Martine Hjelkrem-Tan, Marius Aasan, Gabriel Y. Arteaga, Adín Ramírez Rivera*

Main category: cs.CV

TL;DR: SPoT introduces a flexible tokenization method for Vision Transformers, enabling continuous token placement to improve efficiency and performance.


<details>
  <summary>Details</summary>
Motivation: Standard tokenization methods limit Vision Transformers to rigid patch grids, hindering their ability to leverage sparsity effectively.

Method: Proposes Subpixel Placement of Tokens (SPoT), a continuous tokenization strategy, and uses oracle-guided search to optimize token positioning.

Result: Achieves significant performance gains with fewer tokens, enhancing efficiency during inference.

Conclusion: SPoT redefines sparsity as an advantage, offering a more flexible and interpretable approach for ViT architectures.

Abstract: Vision Transformers naturally accommodate sparsity, yet standard tokenization
methods confine features to discrete patch grids. This constraint prevents
models from fully exploiting sparse regimes, forcing awkward compromises. We
propose Subpixel Placement of Tokens (SPoT), a novel tokenization strategy that
positions tokens continuously within images, effectively sidestepping
grid-based limitations. With our proposed oracle-guided search, we uncover
substantial performance gains achievable with ideal subpixel token positioning,
drastically reducing the number of tokens necessary for accurate predictions
during inference. SPoT provides a new direction for flexible, efficient, and
interpretable ViT architectures, redefining sparsity as a strategic advantage
rather than an imposed limitation.

</details>


### [115] [What does really matter in image goal navigation?](https://arxiv.org/pdf/2507.01667)
*Gianluca Monaci, Philippe Weinzaepfel, Christian Wolf*

Main category: cs.CV

TL;DR: The paper explores whether end-to-end RL training can efficiently solve image goal navigation, analyzing architectural choices and simulator impacts.


<details>
  <summary>Details</summary>
Motivation: To determine if end-to-end RL training can replace dedicated methods for image goal navigation, impacting broader AI applications.

Method: Investigates architectural choices (late fusion, channel stacking, etc.) and their role in emerging relative pose estimators from RL training.

Result: Simulator settings influence method success, but capabilities can transfer to realistic settings. Navigation performance correlates with emerging pose estimation skills.

Conclusion: End-to-end RL training shows promise for image goal navigation, though simulator shortcuts exist. Transferability and emerging skills highlight potential.

Abstract: Image goal navigation requires two different skills: firstly, core navigation
skills, including the detection of free space and obstacles, and taking
decisions based on an internal representation; and secondly, computing
directional information by comparing visual observations to the goal image.
Current state-of-the-art methods either rely on dedicated image-matching, or
pre-training of computer vision modules on relative pose estimation. In this
paper, we study whether this task can be efficiently solved with end-to-end
training of full agents with RL, as has been claimed by recent work. A positive
answer would have impact beyond Embodied AI and allow training of relative pose
estimation from reward for navigation alone. In a large study we investigate
the effect of architectural choices like late fusion, channel stacking,
space-to-depth projections and cross-attention, and their role in the emergence
of relative pose estimators from navigation training. We show that the success
of recent methods is influenced up to a certain extent by simulator settings,
leading to shortcuts in simulation. However, we also show that these
capabilities can be transferred to more realistic setting, up to some extend.
We also find evidence for correlations between navigation performance and
probed (emerging) relative pose estimation performance, an important sub skill.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [116] [Rethinking the Illusion of Thinking](https://arxiv.org/pdf/2507.01231)
*Iñaki Dellibarda Varela, Pablo Romero-Sorozabal, Eduardo Rocon, Manuel Cebrian*

Main category: cs.AI

TL;DR: The paper refutes claims that LRMs lack reasoning by refining benchmarks (Towers of Hanoi, River Crossing), showing failures stem from complexity or unsolvable setups, not inherent incapacity.


<details>
  <summary>Details</summary>
Motivation: To clarify the debate on LRMs' reasoning capabilities by addressing flaws in prior studies and providing nuanced evidence.

Method: Replicated and refined benchmarks using incremental stepwise prompting and agentic collaborative dialogue.

Result: LRMs solve solvable River Crossing problems (100+ agent pairs) but struggle with moderately complex Towers of Hanoi (8 disks).

Conclusion: LRMs are stochastic searchers in poorly understood state spaces; progress requires fine-grained analysis.

Abstract: Earlier this year, Apple ignited controversy by publishing "The Illusion of
Thinking," prompting heated debate within the AI community. Critics seized upon
the findings as conclusive evidence that Large Reasoning Models (LRMs) lack
genuine reasoning capabilities, branding them as mere stochastic parrots.
Meanwhile, defenders-spearheaded by Lawsen et al. (2025)-fired back, condemning
the experimental setup as flawed and the conclusions overstated. We clarify
this debate by replicating and refining two of the original study's most
contentious benchmarks: Towers of Hanoi and River Crossing. By introducing
incremental stepwise prompting and agentic collaborative dialogue, we show that
previously reported failures solving the Towers of Hanoi were not purely result
of output constraints, but also partly a result of cognition limitations: LRMs
still stumble when complexity rises moderately (around 8 disks). Moreover, the
River Crossing results initially heralded as catastrophic failures turn out to
hinge upon testing unsolvable configurations. Once we limit tests strictly to
solvable problems-LRMs effortlessly solve large instances involving over 100
agent pairs. Our findings ultimately defy simplistic narratives: today's LRMs
are stochastic, RL-tuned searchers in a discrete state space we barely
understand. Real progress in symbolic, long-horizon reasoning demands mapping
that terrain through fine-grained ablations like those introduced here.

</details>


### [117] [Beyond Black-Box AI: Interpretable Hybrid Systems for Dementia Care](https://arxiv.org/pdf/2507.01282)
*Matthew JY Kang, Wenli Yang, Monica R Roberts, Byeong Ho Kang, Charles B Malpas*

Main category: cs.AI

TL;DR: The paper discusses the limitations of LLMs in clinical settings, especially for dementia diagnosis, and suggests hybrid AI approaches combining statistical learning with expert knowledge for better interpretability and workflow integration.


<details>
  <summary>Details</summary>
Motivation: Despite high benchmark scores, LLMs fail to improve bedside medical diagnosis, prompting a need to identify practical limitations and solutions for clinical AI.

Method: The paper reviews limitations of data-driven AI in clinical settings, advocating for hybrid approaches like neuro-symbolic AI that integrate LLMs with human expertise.

Result: Current AI lacks transparency and causal reasoning, but hybrid methods (e.g., PEIRS, ATHENA-CDS) show promise in improving interpretability and workflow fit.

Conclusion: Future AI should prioritize explanatory coherence, clinician understanding, and patient outcomes, moving beyond accuracy to practical clinical integration.

Abstract: The recent boom of large language models (LLMs) has re-ignited the hope that
artificial intelligence (AI) systems could aid medical diagnosis. Yet despite
dazzling benchmark scores, LLM assistants have yet to deliver measurable
improvements at the bedside. This scoping review aims to highlight the areas
where AI is limited to make practical contributions in the clinical setting,
specifically in dementia diagnosis and care.
  Standalone machine-learning models excel at pattern recognition but seldom
provide actionable, interpretable guidance, eroding clinician trust. Adjacent
use of LLMs by physicians did not result in better diagnostic accuracy or
speed. Key limitations trace to the data-driven paradigm: black-box outputs
which lack transparency, vulnerability to hallucinations, and weak causal
reasoning. Hybrid approaches that combine statistical learning with expert
rule-based knowledge, and involve clinicians throughout the process help bring
back interpretability. They also fit better with existing clinical workflows,
as seen in examples like PEIRS and ATHENA-CDS.
  Future decision-support should prioritise explanatory coherence by linking
predictions to clinically meaningful causes. This can be done through
neuro-symbolic or hybrid AI that combines the language ability of LLMs with
human causal expertise. AI researchers have addressed this direction, with
explainable AI and neuro-symbolic AI being the next logical steps in further
advancement in AI. However, they are still based on data-driven knowledge
integration instead of human-in-the-loop approaches. Future research should
measure success not only by accuracy but by improvements in clinician
understanding, workflow fit, and patient outcomes. A better understanding of
what helps improve human-computer interactions is greatly needed for AI systems
to become part of clinical practice.

</details>


### [118] [AI Agents and Agentic AI-Navigating a Plethora of Concepts for Future Manufacturing](https://arxiv.org/pdf/2507.01376)
*Yinwang Ren, Yangyang Liu, Tang Ji, Xun Xu*

Main category: cs.AI

TL;DR: The paper reviews AI agent technologies, focusing on LLM-Agents, MLLM-Agents, and Agentic AI, and explores their applications and challenges in smart manufacturing.


<details>
  <summary>Details</summary>
Motivation: To clarify the definitions, capabilities, and practical uses of emerging AI paradigms in smart manufacturing, which remain unclear.

Method: Systematically reviews AI and AI agent evolution, examines core concepts and advancements of LLM-Agents, MLLM-Agents, and Agentic AI, and explores their manufacturing applications.

Result: Identifies potential applications and integration of these AI technologies in manufacturing, along with associated challenges.

Conclusion: The study provides insights into the role of advanced AI agents in smart manufacturing but highlights unresolved challenges.

Abstract: AI agents are autonomous systems designed to perceive, reason, and act within
dynamic environments. With the rapid advancements in generative AI (GenAI),
large language models (LLMs) and multimodal large language models (MLLMs) have
significantly improved AI agents' capabilities in semantic comprehension,
complex reasoning, and autonomous decision-making. At the same time, the rise
of Agentic AI highlights adaptability and goal-directed autonomy in dynamic and
complex environments. LLMs-based AI Agents (LLM-Agents), MLLMs-based AI Agents
(MLLM-Agents), and Agentic AI contribute to expanding AI's capabilities in
information processing, environmental perception, and autonomous
decision-making, opening new avenues for smart manufacturing. However, the
definitions, capability boundaries, and practical applications of these
emerging AI paradigms in smart manufacturing remain unclear. To address this
gap, this study systematically reviews the evolution of AI and AI agent
technologies, examines the core concepts and technological advancements of
LLM-Agents, MLLM-Agents, and Agentic AI, and explores their potential
applications in and integration into manufacturing, along with the potential
challenges they may face.

</details>


### [119] [A Fuzzy Approach to the Specification, Verification and Validation of Risk-Based Ethical Decision Making Models](https://arxiv.org/pdf/2507.01410)
*Abeer Dyoub, Francesca A. Lisi*

Main category: cs.AI

TL;DR: A formal method for Ethical Decision Making models using fuzzy rules and Petri nets, demonstrated with a medical case study.


<details>
  <summary>Details</summary>
Motivation: Addressing the challenge of evaluating moral machines due to ontological and epistemic complexities in ethics.

Method: Develops Ethical Decision Making models via ethical risk assessment, specified as fuzzy rules, and verifies them using fuzzy Petri nets.

Result: Proposes a verifiable and validatable approach for moral machine performance evaluation.

Conclusion: The method effectively bridges ethical theory and practical machine decision-making, validated through a medical case study.

Abstract: The ontological and epistemic complexities inherent in the moral domain make
it challenging to establish clear standards for evaluating the performance of a
moral machine. In this paper, we present a formal method to describe Ethical
Decision Making models based on ethical risk assessment. Then, we show how
these models that are specified as fuzzy rules can be verified and validated
using fuzzy Petri nets. A case study from the medical field is considered to
illustrate the proposed approach.

</details>


### [120] [Pensieve Grader: An AI-Powered, Ready-to-Use Platform for Effortless Handwritten STEM Grading](https://arxiv.org/pdf/2507.01431)
*Yoonseok Yang, Minjune Kim, Marlon Rondinelli, Keren Shao*

Main category: cs.AI

TL;DR: Pensieve is an AI-assisted grading platform using LLMs to transcribe and evaluate handwritten STEM responses, reducing grading time by 65% with high accuracy.


<details>
  <summary>Details</summary>
Motivation: Grading handwritten responses in large STEM courses is time-consuming; Pensieve aims to streamline this process.

Method: Leverages LLMs for transcription and evaluation, integrating a human-in-the-loop interface for the entire grading pipeline.

Result: Deployed in 20+ institutions, graded 300,000+ responses, reducing grading time by 65% with 95.4% agreement for high-confidence predictions.

Conclusion: Pensieve effectively automates grading while maintaining accuracy, significantly improving efficiency in STEM courses.

Abstract: Grading handwritten, open-ended responses remains a major bottleneck in large
university STEM courses. We introduce Pensieve (https://www.pensieve.co), an
AI-assisted grading platform that leverages large language models (LLMs) to
transcribe and evaluate student work, providing instructors with rubric-aligned
scores, transcriptions, and confidence ratings. Unlike prior tools that focus
narrowly on specific tasks like transcription or rubric generation, Pensieve
supports the entire grading pipeline-from scanned student submissions to final
feedback-within a human-in-the-loop interface.
  Pensieve has been deployed in real-world courses at over 20 institutions and
has graded more than 300,000 student responses. We present system details and
empirical results across four core STEM disciplines: Computer Science,
Mathematics, Physics, and Chemistry. Our findings show that Pensieve reduces
grading time by an average of 65%, while maintaining a 95.4% agreement rate
with instructor-assigned grades for high-confidence predictions.

</details>


### [121] [Using multi-agent architecture to mitigate the risk of LLM hallucinations](https://arxiv.org/pdf/2507.01446)
*Abd Elrahman Amer, Magdi Amer*

Main category: cs.AI

TL;DR: A multi-agent system using LLMs and fuzzy logic to improve customer service by reducing hallucination risks in SMS-based interactions.


<details>
  <summary>Details</summary>
Motivation: Enhancing customer service quality and response time is vital for loyalty and market share, but LLMs pose hallucination risks.

Method: Proposes a multi-agent system combining LLMs and fuzzy logic to handle SMS customer requests.

Result: The system aims to mitigate hallucination risks while improving service efficiency.

Conclusion: The integration of LLMs with fuzzy logic offers a promising solution for reliable customer service automation.

Abstract: Improving customer service quality and response time are critical factors for
maintaining customer loyalty and increasing a company's market share. While
adopting emerging technologies such as Large Language Models (LLMs) is becoming
a necessity to achieve these goals, the risk of hallucination remains a major
challenge. In this paper, we present a multi-agent system to handle customer
requests sent via SMS. This system integrates LLM based agents with fuzzy logic
to mitigate hallucination risks.

</details>


### [122] [Agent-as-Tool: A Study on the Hierarchical Decision Making with Reinforcement Learning](https://arxiv.org/pdf/2507.01489)
*Yanfei Zhang*

Main category: cs.AI

TL;DR: The paper introduces Agent-as-tool, a hierarchical framework separating tool calling and reasoning processes in LLM-based agents, improving performance with minimal fine-tuning.


<details>
  <summary>Details</summary>
Motivation: Existing methods struggle with simultaneous tool calling and reasoning, burdening models with redundant data.

Method: Proposes Agent-as-tool, a hierarchical framework decoupling tool calling (handled by one agent) and reasoning (handled by another).

Result: Achieved strong performance with slight fine-tuning (63.2% exact match, 75.2% cover exact match in Bamboogle).

Conclusion: Agent-as-tool effectively improves reasoning by isolating tool calling, outperforming prior methods.

Abstract: Large Language Models (LLMs) have emerged as one of the most significant
technological advancements in artificial intelligence in recent years. Their
ability to understand, generate, and reason with natural language has
transformed how we interact with AI systems. With the development of LLM-based
agents and reinforcement-learning-based reasoning models, the study of applying
reinforcement learning in agent frameworks has become a new research focus.
However, all previous studies face the challenge of deciding the tool calling
process and the reasoning process simultaneously, and the chain of reasoning
was solely relied on the unprocessed raw result with redundant information and
symbols unrelated to the task from the tool, which impose a heavy burden on the
model's capability to reason. Therefore, in our research, we proposed a
hierarchical framework Agent-as-tool that detach the tool calling process and
the reasoning process, which enables the model to focus on the verbally
reasoning process while the tool calling process is handled by another agent.
Our work had achieved comparable results with only a slight reinforcement
fine-tuning on 180 samples, and had achieved exceptionally well performance in
Bamboogle with 63.2% of exact match and 75.2% in cover exact match, exceeding
Search-R1 by 4.8% in exact match and 3.2% in cover exact match.

</details>


### [123] [Agent Ideate: A Framework for Product Idea Generation from Patents Using Agentic AI](https://arxiv.org/pdf/2507.01717)
*Gopichand Kanumolu, Ashok Urlana, Charaka Vinayak Kumar, Bala Mallikarjunarao Garlapati*

Main category: cs.AI

TL;DR: Agent Ideate, a framework using LLMs and autonomous agents, generates business ideas from patents, outperforming standalone LLMs in quality, relevance, and novelty.


<details>
  <summary>Details</summary>
Motivation: Patents hold valuable technical knowledge, but accessing and interpreting this information is challenging. The goal is to leverage LLMs and agents to unlock innovation from patent data.

Method: Designed Agent Ideate, a framework combining LLMs and agent-based architectures, tested in Computer Science, NLP, and Material Chemistry domains.

Result: Agentic approach outperformed standalone LLMs in idea quality, relevance, and novelty.

Conclusion: Combining LLMs with agentic workflows enhances innovation by effectively generating business ideas from patents.

Abstract: Patents contain rich technical knowledge that can inspire innovative product
ideas, yet accessing and interpreting this information remains a challenge.
This work explores the use of Large Language Models (LLMs) and autonomous
agents to mine and generate product concepts from a given patent. In this work,
we design Agent Ideate, a framework for automatically generating product-based
business ideas from patents. We experimented with open-source LLMs and
agent-based architectures across three domains: Computer Science, Natural
Language Processing, and Material Chemistry. Evaluation results show that the
agentic approach consistently outperformed standalone LLMs in terms of idea
quality, relevance, and novelty. These findings suggest that combining LLMs
with agentic workflows can significantly enhance the innovation pipeline by
unlocking the untapped potential of business idea generation from patent data.

</details>


### [124] [T3DM: Test-Time Training-Guided Distribution Shift Modelling for Temporal Knowledge Graph Reasoning](https://arxiv.org/pdf/2507.01597)
*Yuehang Si, Zefan Zeng, Jincai Huang, Qing Cheng*

Main category: cs.AI

TL;DR: T3DM improves TKG reasoning by addressing distribution shift and enhancing negative sampling.


<details>
  <summary>Details</summary>
Motivation: Current TKGR methods struggle with event distribution shifts and low-quality negative samples.

Method: Proposes T3DM for distribution shift modeling and adversarial-based negative sampling.

Result: T3DM outperforms state-of-the-art baselines in robustness and performance.

Conclusion: T3DM effectively addresses key challenges in TKGR, offering superior results.

Abstract: Temporal Knowledge Graph (TKG) is an efficient method for describing the
dynamic development of facts along a timeline. Most research on TKG reasoning
(TKGR) focuses on modelling the repetition of global facts and designing
patterns of local historical facts. However, they face two significant
challenges: inadequate modeling of the event distribution shift between
training and test samples, and reliance on random entity substitution for
generating negative samples, which often results in low-quality sampling. To
this end, we propose a novel distributional feature modeling approach for
training TKGR models, Test-Time Training-guided Distribution shift Modelling
(T3DM), to adjust the model based on distribution shift and ensure the global
consistency of model reasoning. In addition, we design a negative-sampling
strategy to generate higher-quality negative quadruples based on adversarial
training. Extensive experiments show that T3DM provides better and more robust
results than the state-of-the-art baselines in most cases.

</details>


### [125] [Joint Matching and Pricing for Crowd-shipping with In-store Customers](https://arxiv.org/pdf/2507.01749)
*Arash Dehghan, Mucahit Cevik, Merve Bodur, Bissan Ghaddar*

Main category: cs.AI

TL;DR: The paper proposes a crowd-shipping system using in-store customers as couriers, employing an MDP model with NeurADP and DDQN for dynamic order assignment and pricing, achieving significant cost savings.


<details>
  <summary>Details</summary>
Motivation: Addresses the need for efficient last-mile delivery in urban areas by leveraging existing shoppers for deliveries.

Method: Uses a Markov Decision Process (MDP) model with NeurADP for adaptive order assignment and DDQN for dynamic pricing, integrating multi-drop routing and handling offer acceptance uncertainty.

Result: Achieves 6.7% cost savings over fixed pricing and 18% over myopic baselines; flexible delays and multi-destination routing further reduce costs by 8% and 17%.

Conclusion: Dynamic, forward-looking policies enhance crowd-shipping efficiency, offering practical insights for urban logistics.

Abstract: This paper examines the use of in-store customers as delivery couriers in a
centralized crowd-shipping system, targeting the growing need for efficient
last-mile delivery in urban areas. We consider a brick-and-mortar retail
setting where shoppers are offered compensation to deliver time-sensitive
online orders. To manage this process, we propose a Markov Decision Process
(MDP) model that captures key uncertainties, including the stochastic arrival
of orders and crowd-shippers, and the probabilistic acceptance of delivery
offers. Our solution approach integrates Neural Approximate Dynamic Programming
(NeurADP) for adaptive order-to-shopper assignment with a Deep Double Q-Network
(DDQN) for dynamic pricing. This joint optimization strategy enables multi-drop
routing and accounts for offer acceptance uncertainty, aligning more closely
with real-world operations. Experimental results demonstrate that the
integrated NeurADP + DDQN policy achieves notable improvements in delivery cost
efficiency, with up to 6.7\% savings over NeurADP with fixed pricing and
approximately 18\% over myopic baselines. We also show that allowing flexible
delivery delays and enabling multi-destination routing further reduces
operational costs by 8\% and 17\%, respectively. These findings underscore the
advantages of dynamic, forward-looking policies in crowd-shipping systems and
offer practical guidance for urban logistics operators.

</details>


### [126] [Refining Gelfond Rationality Principle Towards More Comprehensive Foundational Principles for Answer Set Semantics](https://arxiv.org/pdf/2507.01833)
*Yi-Dong Shen, Thomas Eiter*

Main category: cs.AI

TL;DR: The paper questions mandatory conditions for answer set semantics in non-monotonic logic programming, refines Gelfond's principles, and proposes new semantics based on well-supportedness and minimality.


<details>
  <summary>Details</summary>
Motivation: To evaluate whether existing conditions (minimal model property, constraint monotonicity, foundedness) are too restrictive for answer set semantics and to refine foundational principles.

Method: Refines Gelfond's answer set (GAS) principles, introduces well-supportedness and minimality, and defines new semantics.

Result: Proposes refined principles and new semantics, demonstrating their applicability and computational complexity.

Conclusion: The refined GAS principles offer a flexible alternative to existing semantics, addressing limitations of traditional conditions.

Abstract: Non-monotonic logic programming is the basis for a declarative problem
solving paradigm known as answer set programming (ASP). Departing from the
seminal definition by Gelfond and Lifschitz in 1988 for simple normal logic
programs, various answer set semantics have been proposed for extensions. We
consider two important questions: (1) Should the minimal model property,
constraint monotonicity and foundedness as defined in the literature be
mandatory conditions for an answer set semantics in general? (2) If not, what
other properties could be considered as general principles for answer set
semantics? We address the two questions. First, it seems that the three
aforementioned conditions may sometimes be too strong, and we illustrate with
examples that enforcing them may exclude expected answer sets. Second, we
evolve the Gelfond answer set (GAS) principles for answer set construction by
refining the Gelfond's rationality principle to well-supportedness, minimality
w.r.t. negation by default and minimality w.r.t. epistemic negation. The
principle of well-supportedness guarantees that every answer set is
constructible from if-then rules obeying a level mapping and is thus free of
circular justification, while the two minimality principles ensure that the
formalism minimizes knowledge both at the level of answer sets and of world
views. Third, to embody the refined GAS principles, we extend the notion of
well-supportedness substantially to answer sets and world views, respectively.
Fourth, we define new answer set semantics in terms of the refined GAS
principles. Fifth, we use the refined GAS principles as an alternative baseline
to intuitively assess the existing answer set semantics. Finally, we analyze
the computational complexity.

</details>


<div id='cs.SD'></div>

# cs.SD [[Back]](#toc)

### [127] [User-guided Generative Source Separation](https://arxiv.org/pdf/2507.01339)
*Yutong Wen, Minje Kim, Paris Smaragdis*

Main category: cs.SD

TL;DR: GuideSep is a diffusion-based music source separation model that allows instrument-agnostic separation beyond the traditional four-stem setup, using waveform mimicry and mel-spectrogram masks for flexible guidance.


<details>
  <summary>Details</summary>
Motivation: Existing methods lack flexibility for real-world applications due to their focus on fixed four-stem separation.

Method: GuideSep uses a diffusion-based approach conditioned on waveform mimicry (e.g., humming) and mel-spectrogram masks, avoiding reliance on fixed labels. A mask-prediction baseline is also designed for comparison.

Result: GuideSep achieves high-quality separation and versatile instrument extraction, outperforming predictive approaches.

Conclusion: The model demonstrates the potential of user-guided, diffusion-based generative processes for flexible and high-quality music source separation.

Abstract: Music source separation (MSS) aims to extract individual instrument sources
from their mixture. While most existing methods focus on the widely adopted
four-stem separation setup (vocals, bass, drums, and other instruments), this
approach lacks the flexibility needed for real-world applications. To address
this, we propose GuideSep, a diffusion-based MSS model capable of
instrument-agnostic separation beyond the four-stem setup. GuideSep is
conditioned on multiple inputs: a waveform mimicry condition, which can be
easily provided by humming or playing the target melody, and mel-spectrogram
domain masks, which offer additional guidance for separation. Unlike prior
approaches that relied on fixed class labels or sound queries, our conditioning
scheme, coupled with the generative approach, provides greater flexibility and
applicability. Additionally, we design a mask-prediction baseline using the
same model architecture to systematically compare predictive and generative
approaches. Our objective and subjective evaluations demonstrate that GuideSep
achieves high-quality separation while enabling more versatile instrument
extraction, highlighting the potential of user participation in the
diffusion-based generative process for MSS. Our code and demo page are
available at https://yutongwen.github.io/GuideSep/

</details>


### [128] [Real-Time Emergency Vehicle Siren Detection with Efficient CNNs on Embedded Hardware](https://arxiv.org/pdf/2507.01563)
*Marco Giordano, Stefano Giacomelli, Claudia Rinaldi, Fabio Graziosi*

Main category: cs.SD

TL;DR: A real-time EV siren detection system using E2PANNs, optimized for urban conditions, deployed on Raspberry Pi 5 with low-latency performance.


<details>
  <summary>Details</summary>
Motivation: To address unreliable AudioSet annotations and enable real-time, distributed acoustic monitoring for smart cities.

Method: Fine-tuned E2PANNs, curated datasets (AudioSet-EV, Augmented, Unified-EV), multithreaded inference with adaptive frame sizing and probability smoothing.

Result: Low-latency detection with improved robustness under realistic audio conditions.

Conclusion: Feasibility of deploying IoS-compatible SED solutions for distributed emergency vehicle tracking in smart cities.

Abstract: We present a full-stack emergency vehicle (EV) siren detection system
designed for real-time deployment on embedded hardware. The proposed approach
is based on E2PANNs, a fine-tuned convolutional neural network derived from
EPANNs, and optimized for binary sound event detection under urban acoustic
conditions. A key contribution is the creation of curated and semantically
structured datasets - AudioSet-EV, AudioSet-EV Augmented, and Unified-EV -
developed using a custom AudioSet-Tools framework to overcome the low
reliability of standard AudioSet annotations. The system is deployed on a
Raspberry Pi 5 equipped with a high-fidelity DAC+microphone board, implementing
a multithreaded inference engine with adaptive frame sizing, probability
smoothing, and a decision-state machine to control false positive activations.
A remote WebSocket interface provides real-time monitoring and facilitates live
demonstration capabilities. Performance is evaluated using both framewise and
event-based metrics across multiple configurations. Results show the system
achieves low-latency detection with improved robustness under realistic audio
conditions. This work demonstrates the feasibility of deploying IoS-compatible
SED solutions that can form distributed acoustic monitoring networks, enabling
collaborative emergency vehicle tracking across smart city infrastructures
through WebSocket connectivity on low-cost edge devices.

</details>


### [129] [Exploring Classical Piano Performance Generation with Expressive Music Variational AutoEncoder](https://arxiv.org/pdf/2507.01582)
*Jing Luo, Xinyu Yang, Jie Wei*

Main category: cs.SD

TL;DR: The paper introduces the Expressive Compound Word (ECP) representation and the Expressive Music Variational AutoEncoder (XMVAE) to generate classical piano performances, combining composer and pianist roles for superior musical quality.


<details>
  <summary>Details</summary>
Motivation: To emulate the dual creative roles of composers and pianists in generating expressive classical piano performances from scratch.

Method: Proposes the XMVAE model with two branches: a VQ-VAE for score-related content (Composer) and a vanilla VAE for expressive details (Pianist), trained jointly with Seq2Seq architectures.

Result: XMVAE outperforms state-of-the-art models in generating classical performances, with pretraining on extra score datasets enhancing performance.

Conclusion: The XMVAE framework effectively captures both structural and expressive aspects of classical music, achieving high-quality performance generation.

Abstract: The creativity of classical music arises not only from composers who craft
the musical sheets but also from performers who interpret the static notations
with expressive nuances. This paper addresses the challenge of generating
classical piano performances from scratch, aiming to emulate the dual roles of
composer and pianist in the creative process. We introduce the Expressive
Compound Word (ECP) representation, which effectively captures both the
metrical structure and expressive nuances of classical performances. Building
on this, we propose the Expressive Music Variational AutoEncoder (XMVAE), a
model featuring two branches: a Vector Quantized Variational AutoEncoder
(VQ-VAE) branch that generates score-related content, representing the
Composer, and a vanilla VAE branch that produces expressive details, fulfilling
the role of Pianist. These branches are jointly trained with similar Seq2Seq
architectures, leveraging a multiscale encoder to capture beat-level contextual
information and an orthogonal Transformer decoder for efficient compound tokens
decoding. Both objective and subjective evaluations demonstrate that XMVAE
generates classical performances with superior musical quality compared to
state-of-the-art models. Furthermore, pretraining the Composer branch on extra
musical score datasets contribute to a significant performance gain.

</details>


### [130] [A Dataset for Automatic Assessment of TTS Quality in Spanish](https://arxiv.org/pdf/2507.01805)
*Alejandro Sosa Welford, Leonardo Pepino*

Main category: cs.SD

TL;DR: A Spanish database for TTS system assessment was created, featuring 4,326 audio samples from 52 TTS systems and human voices. Subjective tests labeled the data, and automatic naturalness prediction models achieved a 0.8 MAE on a five-point MOS scale.


<details>
  <summary>Details</summary>
Motivation: To improve naturalness prediction models for Spanish TTS systems by creating the first dedicated dataset.

Method: Developed a dataset with subjective labeling (ITU-T Rec. P.807 standard) and trained models via fine-tuning and downstream networks on frozen self-supervised models.

Result: Models achieved a 0.8 MAE on a five-point MOS scale, validating dataset quality and diversity.

Conclusion: The dataset advances Spanish TTS research, demonstrating its utility for training accurate prediction models.

Abstract: This work addresses the development of a database for the automatic
assessment of text-to-speech (TTS) systems in Spanish, aiming to improve the
accuracy of naturalness prediction models. The dataset consists of 4,326 audio
samples from 52 different TTS systems and human voices and is, up to our
knowledge, the first of its kind in Spanish. To label the audios, a subjective
test was designed based on the ITU-T Rec. P.807 standard and completed by 92
participants. Furthermore, the utility of the collected dataset was validated
by training automatic naturalness prediction systems. We explored two
approaches: fine-tuning an existing model originally trained for English, and
training small downstream networks on top of frozen self-supervised speech
models. Our models achieve a mean absolute error of 0.8 on a five-point MOS
scale. Further analysis demonstrates the quality and diversity of the developed
dataset, and its potential to advance TTS research in Spanish.

</details>


### [131] [Melody predominates over harmony in the evolution of musical scales across 96 countries](https://arxiv.org/pdf/2408.12633)
*John M McBride, Elizabeth Phillips, Patrick E Savage, Steven Brown, Tsvi Tlusty*

Main category: cs.SD

TL;DR: The paper challenges the historical focus on harmony in musical scales, showing melody is the primary determinant globally, supported by cross-cultural data.


<details>
  <summary>Details</summary>
Motivation: To address the lack of comparative analysis on the role of melody vs. harmony in musical scales using cross-cultural data.

Method: A computational comparison of 1,314 scales from 96 countries to evaluate melodic and harmonic theories.

Result: Melodic theories are nearly universally supported, while harmony predicts scales poorly outside Eurasian societies.

Conclusion: Melody, not harmony, is the primary determinant of musical scales worldwide, challenging traditional Western scholarship.

Abstract: The standard theory of musical scales since antiquity has been based on
harmony, rather than melody. While recent analyses provide mixed support for a
role of melody as well as harmony, we lack a comparative analysis based on
cross-cultural data. We address this longstanding problem through a rigorous
computational comparison of the main theories using 1,314 scales from 96
countries. There is near-universal support for melodic theories, which predict
step-sizes of 1-3 semitones. Harmony accounts for the prevalence of certain
simple-integer-ratio intervals, particularly for music-theoretic scales from
Eurasian societies, which may explain their dominance amongst Western scholars.
However, harmony is a poor predictor of scales measured from ethnographic
recordings, particularly outside of Eurasia. Overall, we show that the
historical emphasis on harmony is misguided and that melody is the primary
determinant of the world's musical scales.

</details>


### [132] [Embedding-Space Diffusion for Zero-Shot Environmental Sound Classification](https://arxiv.org/pdf/2412.03771)
*Ysobel Sims, Alexandre Mendes, Stephan Chalup*

Main category: cs.SD

TL;DR: The paper explores generative methods for zero-shot learning in environmental audio, introducing a novel diffusion model that outperforms existing methods.


<details>
  <summary>Details</summary>
Motivation: Address the underexplored application of generative methods in zero-shot environmental sound classification, where existing studies show poor performance.

Method: Adapts CADA-VAE and LisGAN from computer vision and introduces a diffusion model conditioned on class auxiliary data. Synthetic embeddings from the diffusion model are combined with seen class embeddings for classifier training.

Result: The diffusion model outperforms baseline methods across six audio datasets, including ESC-50, ARCA23K-FSD, FSC22, UrbanSound8k, TAU Urban Acoustics 2019, and GTZAN.

Conclusion: The diffusion model is a promising approach for zero-shot learning, establishing the first benchmark for generative methods in environmental sound classification.

Abstract: Zero-shot learning enables models to generalise to unseen classes by
leveraging semantic information, bridging the gap between training and testing
sets with non-overlapping classes. While much research has focused on zero-shot
learning in computer vision, the application of these methods to environmental
audio remains underexplored, with poor performance in existing studies.
Generative methods, which have demonstrated success in computer vision, are
notably absent from zero-shot environmental sound classification studies.
  To address this gap, this work investigates generative methods for zero-shot
learning in environmental audio. Two successful generative models from computer
vision are adapted: a cross-aligned and distribution-aligned variational
autoencoder (CADA-VAE) and a leveraging invariant side generative adversarial
network (LisGAN). Additionally, we introduced a novel diffusion model
conditioned on class auxiliary data. Synthetic embeddings generated by the
diffusion model are combined with seen class embeddings to train a classifier.
  Experiments are conducted on five environmental audio datasets, ESC-50,
ARCA23K-FSD, FSC22, UrbanSound8k and TAU Urban Acoustics 2019, and one music
classification dataset, GTZAN. Results show that the diffusion model
outperforms all baseline methods on average across six audio datasets.
  This work establishes the diffusion model as a promising approach for
zero-shot learning and introduces the first benchmark of generative methods for
zero-shot environmental sound classification, providing a foundation for future
research.

</details>


### [133] [TARO: Timestep-Adaptive Representation Alignment with Onset-Aware Conditioning for Synchronized Video-to-Audio Synthesis](https://arxiv.org/pdf/2504.05684)
*Tri Ton, Ji Woo Hong, Chang D. Yoo*

Main category: cs.SD

TL;DR: TARO is a new framework for video-to-audio synthesis using flow-based transformers, featuring dynamic alignment (TRA) and onset-aware conditioning (OAC) for better fidelity and synchronization. It outperforms prior methods in audio quality and alignment.


<details>
  <summary>Details</summary>
Motivation: To improve high-fidelity and temporally coherent video-to-audio synthesis by addressing synchronization and audio quality challenges.

Method: Uses flow-based transformers with TRA for dynamic latent representation alignment and OAC for onset-aware synchronization.

Result: Achieves 53% lower FD, 29% lower FAD, and 97.19% alignment accuracy on VGGSound and Landscape datasets.

Conclusion: TARO sets a new benchmark for video-to-audio synthesis with superior performance in audio quality and synchronization.

Abstract: This paper introduces Timestep-Adaptive Representation Alignment with
Onset-Aware Conditioning (TARO), a novel framework for high-fidelity and
temporally coherent video-to-audio synthesis. Built upon flow-based
transformers, which offer stable training and continuous transformations for
enhanced synchronization and audio quality, TARO introduces two key
innovations: (1) Timestep-Adaptive Representation Alignment (TRA), which
dynamically aligns latent representations by adjusting alignment strength based
on the noise schedule, ensuring smooth evolution and improved fidelity, and (2)
Onset-Aware Conditioning (OAC), which integrates onset cues that serve as sharp
event-driven markers of audio-relevant visual moments to enhance
synchronization with dynamic visual events. Extensive experiments on the
VGGSound and Landscape datasets demonstrate that TARO outperforms prior
methods, achieving relatively 53% lower Frechet Distance (FD), 29% lower
Frechet Audio Distance (FAD), and a 97.19% Alignment Accuracy, highlighting its
superior audio quality and synchronization precision.

</details>


### [134] [An accurate measurement of parametric array using a spurious sound filter topologically equivalent to a half-wavelength resonator](https://arxiv.org/pdf/2504.12398)
*Woongji Kim, Beomseok Oh, Junsuk Rho, Wonkyu Moon*

Main category: cs.SD

TL;DR: A novel half-wavelength resonator-based acoustic filter is proposed to address challenges in measuring audio signals from parametric arrays, achieving high attenuation of spurious ultrasonic sounds.


<details>
  <summary>Details</summary>
Motivation: Accurate measurement of audio signals from parametric arrays is hindered by spurious ultrasonic sounds caused by microphone nonlinearities, with existing filtering methods having practical limitations.

Method: The filter exploits the nodal plane in acoustic pressure distribution, fabricated via SLA 3D printing, and optimized using FEM simulations for frequencies at 40 kHz and 60 kHz.

Result: The filter achieves high transmission loss (~60 dB), significantly reducing spurious signals, as validated by experiments in frequency response, beam pattern, and propagation curve measurements.

Conclusion: The proposed filter enhances measurement accuracy, reliability, and reproducibility in parametric array research, independent of distance and angle.

Abstract: Parametric arrays (PA) offer exceptional directivity and compactness compared
to conventional loudspeakers, facilitating various acoustic applications.
However, accurate measurement of audio signals generated by PA remains
challenging due to spurious ultrasonic sounds arising from microphone
nonlinearities. Existing filtering methods, including Helmholtz resonators,
phononic crystals, polymer films, and grazing incidence techniques, exhibit
practical constraints such as size limitations, fabrication complexity, or
insufficient attenuation. To address these issues, we propose and demonstrate a
novel acoustic filter based on the design of a half-wavelength resonator. The
developed filter exploits the nodal plane in acoustic pressure distribution,
effectively minimizing microphone exposure to targeted ultrasonic frequencies.
Fabrication via stereolithography (SLA) 3D printing ensures high dimensional
accuracy, which is crucial for high-frequency acoustic filters. Finite element
method (FEM) simulations guided filter optimization for suppression frequencies
at 40 kHz and 60 kHz, achieving high transmission loss (TL) around 60 dB.
Experimental validations confirm the filter's superior performance in
significantly reducing spurious acoustic signals, as reflected in frequency
response, beam pattern, and propagation curve measurements. The proposed filter
ensures stable and precise acoustic characterization, independent of
measurement distances and incidence angles. This new approach not only improves
measurement accuracy but also enhances reliability and reproducibility in
parametric array research and development.

</details>


### [135] [Multi-interaction TTS toward professional recording reproduction](https://arxiv.org/pdf/2507.00808)
*Hiroki Kanagawa, Kenichi Fujita, Aya Watanabe, Yusuke Ijima*

Main category: cs.SD

TL;DR: A TTS method with multi-step interaction allows iterative refinement of synthesized speech, emulating voice actor-director feedback.


<details>
  <summary>Details</summary>
Motivation: Current TTS lacks fine-grained style refinement post-synthesis, often deviating from user intent.

Method: Proposes a TTS model with multi-step interaction, modeling user-TTS feedback like actor-director dynamics.

Result: Experiments show iterative style refinements align with user directions, proving multi-interaction capability.

Conclusion: The method enables intuitive, rapid refinement of TTS outputs, addressing a gap in current synthesis systems.

Abstract: Voice directors often iteratively refine voice actors' performances by
providing feedback to achieve the desired outcome. While this iterative
feedback-based refinement process is important in actual recordings, it has
been overlooked in text-to-speech synthesis (TTS). As a result, fine-grained
style refinement after the initial synthesis is not possible, even though the
synthesized speech often deviates from the user's intended style. To address
this issue, we propose a TTS method with multi-step interaction that allows
users to intuitively and rapidly refine synthesized speech. Our approach models
the interaction between the TTS model and its user to emulate the relationship
between voice actors and voice directors. Experiments show that the proposed
model with its corresponding dataset enables iterative style refinements in
accordance with users' directions, thus demonstrating its multi-interaction
capability. Sample audios are available:
https://ntt-hilab-gensp.github.io/ssw13multiinteractiontts/

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [136] [Few-Shot Inspired Generative Zero-Shot Learning](https://arxiv.org/pdf/2507.01026)
*Md Shakil Ahamed Shohag, Q. M. Jonathan Wu, Farhad Pourpanah*

Main category: cs.LG

TL;DR: FSIGenZ is a few-shot-inspired generative ZSL framework that reduces reliance on large-scale feature synthesis by dynamically re-scoring class attributes and using group-level prototypes.


<details>
  <summary>Details</summary>
Motivation: Conventional ZSL methods require extensive synthetic data and computational resources, relaxing ZSL assumptions. FSIGenZ addresses this by leveraging instance-level attribute variability.

Method: Introduces Model-Specific Attribute Scoring (MSAS) to dynamically re-score attributes and estimates group-level prototypes. Uses Dual-Purpose Semantic Regularization (DPSR) and a semantic-aware contrastive classifier (SCC).

Result: Achieves competitive performance on SUN, AwA2, and CUB benchmarks using fewer synthetic features.

Conclusion: FSIGenZ offers an efficient alternative to traditional ZSL methods by reducing synthetic data dependency while maintaining performance.

Abstract: Generative zero-shot learning (ZSL) methods typically synthesize visual
features for unseen classes using predefined semantic attributes, followed by
training a fully supervised classification model. While effective, these
methods require substantial computational resources and extensive synthetic
data, thereby relaxing the original ZSL assumptions. In this paper, we propose
FSIGenZ, a few-shot-inspired generative ZSL framework that reduces reliance on
large-scale feature synthesis. Our key insight is that class-level attributes
exhibit instance-level variability, i.e., some attributes may be absent or
partially visible, yet conventional ZSL methods treat them as uniformly
present. To address this, we introduce Model-Specific Attribute Scoring (MSAS),
which dynamically re-scores class attributes based on model-specific
optimization to approximate instance-level variability without access to unseen
data. We further estimate group-level prototypes as clusters of instances based
on MSAS-adjusted attribute scores, which serve as representative synthetic
features for each unseen class. To mitigate the resulting data imbalance, we
introduce a Dual-Purpose Semantic Regularization (DPSR) strategy while training
a semantic-aware contrastive classifier (SCC) using these prototypes.
Experiments on SUN, AwA2, and CUB benchmarks demonstrate that FSIGenZ achieves
competitive performance using far fewer synthetic features.

</details>


### [137] [DBellQuant: Breaking the Bell with Double-Bell Transformation for LLMs Post Training Binarization](https://arxiv.org/pdf/2507.01027)
*Zijian Ye, Wei Huang, Yifei Yu, Tianhe Ren, Zhongrui Wang, Xiaojuan Qi*

Main category: cs.LG

TL;DR: DBellQuant is a post-training quantization framework that compresses LLMs to nearly 1-bit weights and 6-bit activations with minimal performance loss, outperforming existing methods.


<details>
  <summary>Details</summary>
Motivation: Address computational and memory challenges in LLMs by improving quantization effectiveness, which is often limited by quantization errors and activation outliers.

Method: Uses the Learnable Transformation for Dual-Bell (LTDB) algorithm to transform weight distributions into dual-bell forms, reducing binarization errors and smoothing activations.

Result: Achieves a perplexity of 14.39 on LLaMA2-13B with 6-bit activation quantization, outperforming BiLLM's 21.35.

Conclusion: DBellQuant sets a new state-of-the-art for LLM compression, demonstrating potential for real-world deployment.

Abstract: Large language models (LLMs) demonstrate remarkable performance but face
substantial computational and memory challenges that limit their practical
deployment. Quantization has emerged as a promising solution; however, its
effectiveness is often limited by quantization errors arising from weight
distributions that are not quantization-friendly and the presence of activation
outliers. To address these challenges, we introduce DBellQuant, an innovative
post-training quantization (PTQ) framework that achieves nearly 1-bit weight
compression and 6-bit activation quantization with minimal performance
degradation. DBellQuant uses Learnable Transformation for Dual-Bell (LTDB)
algorithm, which transforms single-bell weight distributions into dual-bell
forms to reduce binarization errors and applies inverse transformations to
smooth activations. DBellQuant sets a new state-of-the-art by preserving
superior model performance under aggressive weight and activation quantization.
For example, on the Wikitext2 dataset, DBellQuant achieves a perplexity of
14.39 on LLaMA2-13B with 6-bit activation quantization, significantly
outperforming BiLLM's 21.35 without activation quantization, underscoring its
potential in compressing LLMs for real-world applications.

</details>


### [138] [Dual Perspectives on Non-Contrastive Self-Supervised Learning](https://arxiv.org/pdf/2507.01028)
*Jean Ponce, Martial Hebert, Basile Terver*

Main category: cs.LG

TL;DR: The paper analyzes non-contrastive self-supervised learning methods, focusing on stop gradient and exponential moving average techniques to prevent collapse, and proves their stability theoretically.


<details>
  <summary>Details</summary>
Motivation: To understand why stop gradient and exponential moving average procedures avoid representation collapse in self-supervised learning, despite not optimizing the original objective.

Method: Theoretical analysis from optimization and dynamical systems perspectives, examining the linear case and stability of limit points.

Result: Stop gradient and exponential moving average avoid collapse and lead to stable equilibria, unlike minimizing the original objective directly.

Conclusion: These procedures are effective for preventing collapse in self-supervised learning, supported by theoretical guarantees.

Abstract: The objective of non-contrastive approaches to self-supervised learning is to
train on pairs of different views of the data an encoder and a predictor that
minimize the mean discrepancy between the code predicted from the embedding of
the first view and the embedding of the second one. In this setting, the stop
gradient and exponential moving average iterative procedures are commonly used
to avoid representation collapse, with excellent performance in downstream
supervised applications. This presentation investigates these procedures from
the dual theoretical viewpoints of optimization and dynamical systems. We first
show that, in general, although they do not optimize the original objective, or
for that matter, any other smooth function, they do avoid collapse. Following
Tian et al. [2021], but without any of the extra assumptions used in their
proofs, we then show using a dynamical system perspective that, in the linear
case, minimizing the original objective function without the use of a stop
gradient or exponential moving average always leads to collapse. Conversely, we
finally show that the limit points of the dynamical systems associated with
these two procedures are, in general, asymptotically stable equilibria, with no
risk of degenerating to trivial solutions.

</details>


### [139] [PathCoT: Chain-of-Thought Prompting for Zero-shot Pathology Visual Reasoning](https://arxiv.org/pdf/2507.01029)
*Junjie Zhou, Yingli Zuo, Shichang Feng, Peng Wan, Qi Zhu, Daoqiang Zhang, Wei Shao*

Main category: cs.LG

TL;DR: PathCoT improves pathology visual reasoning in MLLMs by integrating expert knowledge and self-evaluation into the CoT process, addressing hallucinations and answer divergence.


<details>
  <summary>Details</summary>
Motivation: Existing MLLMs struggle with pathology tasks due to lack of domain knowledge and errors in CoT reasoning steps.

Method: PathCoT integrates pathology expert knowledge into MLLMs' reasoning and adds a self-evaluation step to ensure answer reliability.

Result: PathCoT outperforms on the PathMMU dataset, enhancing pathology visual understanding and reasoning.

Conclusion: PathCoT effectively addresses domain-specific challenges in MLLMs, improving accuracy and reliability in pathology tasks.

Abstract: With the development of generative artificial intelligence and instruction
tuning techniques, multimodal large language models (MLLMs) have made
impressive progress on general reasoning tasks. Benefiting from the
chain-of-thought (CoT) methodology, MLLMs can solve the visual reasoning
problem step-by-step. However, existing MLLMs still face significant challenges
when applied to pathology visual reasoning tasks: (1) LLMs often underperforms
because they lack domain-specific information, which can lead to model
hallucinations. (2) The additional reasoning steps in CoT may introduce errors,
leading to the divergence of answers. To address these limitations, we propose
PathCoT, a novel zero-shot CoT prompting method which integrates the pathology
expert-knowledge into the reasoning process of MLLMs and incorporates
self-evaluation to mitigate divergence of answers. Specifically, PathCoT guides
the MLLM with prior knowledge to perform as pathology experts, and provides
comprehensive analysis of the image with their domain-specific knowledge. By
incorporating the experts' knowledge, PathCoT can obtain the answers with CoT
reasoning. Furthermore, PathCoT incorporates a self-evaluation step that
assesses both the results generated directly by MLLMs and those derived through
CoT, finally determining the reliable answer. The experimental results on the
PathMMU dataset demonstrate the effectiveness of our method on pathology visual
understanding and reasoning.

</details>


### [140] [Optimizing Flamelet Generated Manifold Models: A Machine Learning Performance Study](https://arxiv.org/pdf/2507.01030)
*Reza Lotfi Navaei, Mohammad Safarzadeh, Seyed Mohammad Jafar Sobhani*

Main category: cs.LG

TL;DR: The paper develops Flamelet Generated Manifold (FGM) libraries for methane combustion using machine learning, achieving high accuracy with an optimized MLP model.


<details>
  <summary>Details</summary>
Motivation: FGM's precision in combustion modeling is resource-intensive. This research aims to streamline FGM library creation for methane fuel using machine learning.

Method: Four ML algorithms (MLP, Random Forest, Linear Regression, SVM) were tested. MLP was optimized with hyperparameter tuning, selecting a 4-hidden-layer architecture.

Result: The best model (MLP with 4 layers) achieved 99.81% accuracy, with an error rate of 2.30% across seven libraries.

Conclusion: MLP is optimal for FGM library generation, offering high accuracy for methane combustion simulations.

Abstract: In chemistry tabulations and Flamelet combustion models, the Flamelet
Generated Manifold (FGM) is recognized for its precision and physical
representation. The practical implementation of FGM requires a significant
allocation of memory resources. FGM libraries are developed specifically for a
specific fuel and subsequently utilized for all numerical problems using
machine learning techniques. This research aims to develop libraries of Laminar
FGM utilizing machine learning algorithms for application in combustion
simulations of methane fuel. This study employs four Machine Learning
algorithms to regenerate Flamelet libraries, based on an understanding of data
sources, techniques, and data-driven concepts. 1. Multi-Layer Perceptron; 2.
Random Forest; 3. Linear Regression; 4. Support Vector Machine. Seven libraries
were identified as appropriate for constructing a database for training machine
learning models, giving an error rate of 2.30%. The default architectures of
each method were evaluated to determine the optimal approach, leading to the
selection of the MLP method as the primary choice. The method was enhanced
through hyperparameter tuning to improve accuracy. The quantity of hidden
layers and neurons significantly influences method performance. The optimal
model, comprising four hidden layers with 10, 15, 20, and 25 neurons
respectively, achieved an accuracy of 99.81%.

</details>


### [141] [PyTorch-based Geometric Learning with Non-CUDA Processing Units: Experiences from Intel Gaudi-v2 HPUs](https://arxiv.org/pdf/2507.01031)
*Fanchen Bu, Kijung Shin*

Main category: cs.LG

TL;DR: The paper discusses porting PyTorch-based geometric learning frameworks to Intel's Gaudi-v2 HPUs, addressing challenges and providing utilities, tutorials, and examples to ease adoption.


<details>
  <summary>Details</summary>
Motivation: Geometric learning is vital for non-Euclidean data, but non-CUDA hardware like Intel's Gaudi-v2 HPUs requires significant adaptation.

Method: The authors developed core utilities for essential operations on Gaudi-v2 HPUs and shared tutorials and real-world examples.

Result: They created a GitHub repository with resources to facilitate geometric-learning on non-CUDA hardware.

Conclusion: The work reduces barriers for researchers using non-CUDA hardware and supports further optimization and portability.

Abstract: Geometric learning has emerged as a powerful paradigm for modeling
non-Euclidean data, especially graph-structured ones, with applications
spanning social networks, molecular structures, knowledge graphs, and
recommender systems. While Nvidia's CUDA-enabled graphics processing units
(GPUs) largely dominate the hardware landscape, emerging accelerators such as
Intel's Gaudi Habana Processing Units (HPUs) offer competitive performance and
energy efficiency. However, the usage of such non-CUDA processing units
requires significant engineering effort and novel software adaptations. In this
work, we present our experiences porting PyTorch-based geometric learning
frameworks to Gaudi-v2 HPUs. We introduce a collection of core utilities that
restore essential operations (e.g., scatter, sparse indexing, k-nearest
neighbors) on Gaudi-v2 HPUs, and we consolidate sixteen guided tutorials and
eleven real-world examples with diagnostic analyses of encountered failures and
detailed workarounds. We collect all our experiences into a publicly accessible
GitHub repository. Our contributions lower the barrier for researchers to
experiment with geometric-learning algorithms and models on non-CUDA hardware,
providing a foundation for further optimization and cross-platform portability.

</details>


### [142] [An Uncertainty-Aware Dynamic Decision Framework for Progressive Multi-Omics Integration in Classification Tasks](https://arxiv.org/pdf/2507.01032)
*Nan Mu, Hongbo Yang, Chen Zhao*

Main category: cs.LG

TL;DR: Proposes an uncertainty-aware, multi-view dynamic decision framework for omics data classification to reduce costs while maintaining accuracy.


<details>
  <summary>Details</summary>
Motivation: High costs and resource waste in multi-omics profiling necessitate a cost-effective, accurate diagnostic method.

Method: Uses neural networks with refined activation functions for Dirichlet distribution parameters, integrates modalities via Dempster-Shafer theory, and employs dynamic decision-making.

Result: Achieves accurate classification with fewer omics modalities in 50%+ cases, matching full-omics performance.

Conclusion: The framework reduces redundant testing while preserving diagnostic accuracy and biological insights.

Abstract: Background and Objective: High-throughput multi-omics technologies have
proven invaluable for elucidating disease mechanisms and enabling early
diagnosis. However, the high cost of multi-omics profiling imposes a
significant economic burden, with over reliance on full omics data potentially
leading to unnecessary resource consumption. To address these issues, we
propose an uncertainty-aware, multi-view dynamic decision framework for omics
data classification that aims to achieve high diagnostic accuracy while
minimizing testing costs. Methodology: At the single-omics level, we refine the
activation functions of neural networks to generate Dirichlet distribution
parameters, utilizing subjective logic to quantify both the belief masses and
uncertainty mass of classification results. Belief mass reflects the support of
a specific omics modality for a disease class, while the uncertainty parameter
captures limitations in data quality and model discriminability, providing a
more trustworthy basis for decision-making. At the multi omics level, we employ
a fusion strategy based on Dempster-Shafer theory to integrate heterogeneous
modalities, leveraging their complementarity to boost diagnostic accuracy and
robustness. A dynamic decision mechanism is then applied that omics data are
incrementally introduced for each patient until either all data sources are
utilized or the model confidence exceeds a predefined threshold, potentially
before all data sources are utilized. Results and Conclusion: We evaluate our
approach on four benchmark multi-omics datasets, ROSMAP, LGG, BRCA, and KIPAN.
In three datasets, over 50% of cases achieved accurate classification using a
single omics modality, effectively reducing redundant testing. Meanwhile, our
method maintains diagnostic performance comparable to full-omics models and
preserves essential biological insights.

</details>


### [143] [Data-driven Insights for Informed Decision-Making: Applying LSTM Networks for Robust Electricity Forecasting in Libya](https://arxiv.org/pdf/2507.01034)
*Asma Agaal, Mansour Essgaer, Hend M. Farkash, Zulaiha Ali Othman*

Main category: cs.LG

TL;DR: The paper proposes a data-driven approach using LSTM to forecast electricity load, generation, and deficits in Benghazi, Libya, outperforming other models like ARIMA and XGBoost.


<details>
  <summary>Details</summary>
Motivation: Accurate electricity forecasting is vital for grid stability in Benghazi, Libya, where infrastructure and generation deficits persist.

Method: Multiple time series models (ARIMA, SARIMA, XGBoost, LSTM) were applied to historical data (2019, 2023), enhanced with preprocessing techniques.

Result: LSTM outperformed other models, effectively handling non-stationary and seasonal patterns, especially when integrated with exogenous factors like temperature.

Conclusion: The optimized LSTM framework provides actionable insights for policymakers and grid operators in volatile, data-scarce regions.

Abstract: Accurate electricity forecasting is crucial for grid stability and energy
planning, especially in Benghazi, Libya, where frequent load shedding,
generation deficits, and infrastructure limitations persist. This study
proposes a data-driven approach to forecast electricity load, generation, and
deficits for 2025 using historical data from 2019 (a year marked by
instability) and 2023 (a more stable year). Multiple time series models were
applied, including ARIMA, seasonal ARIMA, dynamic regression ARIMA, exponential
smoothing, extreme gradient boosting, and Long Short-Term Memory (LSTM) neural
networks. The dataset was enhanced through missing value imputation, outlier
smoothing, and log transformation. Performance was assessed using mean squared
error, root mean squared error, mean absolute error, and mean absolute
percentage error. LSTM outperformed all other models, showing strong
capabilities in modeling non-stationary and seasonal patterns. A key
contribution of this work is an optimized LSTM framework that integrates
exogenous factors such as temperature and humidity, offering robust performance
in forecasting multiple electricity indicators. These results provide practical
insights for policymakers and grid operators to enable proactive load
management and resource planning in data-scarce, volatile regions.

</details>


### [144] [Research on Low-Latency Inference and Training Efficiency Optimization for Graph Neural Network and Large Language Model-Based Recommendation Systems](https://arxiv.org/pdf/2507.01035)
*Yushang Zhao, Haotian Lyu, Yike Peng, Aijia Sun, Feng Jiang, Xinyue Han*

Main category: cs.LG

TL;DR: The paper proposes a hybrid GNN-LLM recommender system optimized for speed and efficiency using quantization, LoRA, distillation, FPGA, and DeepSpeed, achieving significant improvements in accuracy and training time.


<details>
  <summary>Details</summary>
Motivation: Online services require fast and efficient recommender systems to handle complex user-item interactions in real-time, addressing computational bottlenecks in hybrid GNN-LLM models.

Method: A hybrid GNN-LLM architecture was optimized with quantization, LoRA, distillation, FPGA, and DeepSpeed, tested under R 4.4.2.

Result: The optimized system achieved 13.6% higher accuracy (NDCG@10: 0.75) at 40-60ms latency, with LoRA reducing training time by 66% (3.8 hours).

Conclusion: Hardware-software co-design and parameter-efficient tuning enable hybrid models to outperform standalone GNN or LLM approaches, recommending FPGA and LoRA for real-time deployment. Future work includes federated learning and advanced fusion architectures.

Abstract: The incessant advent of online services demands high speed and efficient
recommender systems (ReS) that can maintain real-time performance along with
processing very complex user-item interactions. The present study, therefore,
considers computational bottlenecks involved in hybrid Graph Neural Network
(GNN) and Large Language Model (LLM)-based ReS with the aim optimizing their
inference latency and training efficiency. An extensive methodology was used:
hybrid GNN-LLM integrated architecture-optimization strategies(quantization,
LoRA, distillation)-hardware acceleration (FPGA, DeepSpeed)-all under R 4.4.2.
Experimental improvements were significant, with the optimal Hybrid + FPGA +
DeepSpeed configuration reaching 13.6% more accuracy (NDCG@10: 0.75) at 40-60ms
of latency, while LoRA brought down training time by 66% (3.8 hours) in
comparison to the non-optimized baseline. Irrespective of domain, such as
accuracy or efficiency, it can be established that hardware-software co-design
and parameter-efficient tuning permit hybrid models to outperform GNN or LLM
approaches implemented independently. It recommends the use of FPGA as well as
LoRA for real-time deployment. Future work should involve federated learning
along with advanced fusion architectures for better scalability and privacy
preservation. Thus, this research marks the fundamental groundwork concerning
next-generation ReS balancing low-latency response with cutting-edge
personalization.

</details>


### [145] [Learning to Segment for Vehicle Routing Problems](https://arxiv.org/pdf/2507.01037)
*Wenbin Ouyang, Sirui Li, Yining Ma, Cathy Wu*

Main category: cs.LG

TL;DR: The paper introduces FSTA and L2Seg to optimize iterative solvers for VRPs by reducing redundant computations. L2Seg variants improve solver speed by up to 7x.


<details>
  <summary>Details</summary>
Motivation: Redundant computations in iterative solvers for large-scale VRPs due to stable solution segments.

Method: FSTA decomposes solutions into stable segments (hypernodes) and focuses search on unstable parts. L2Seg, a neural framework, identifies stable/unstable segments with three variants (non-autoregressive, autoregressive, and their synergy).

Result: L2Seg accelerates solvers by up to 7x, with the NAR and AR synergy performing best.

Conclusion: L2Seg is a flexible, efficient framework compatible with various solvers and VRPs.

Abstract: Iterative search heuristics are widely recognized as state-of-the-art for
solving Vehicle Routing Problems (VRPs). In this work, we identify and exploit
a critical observation: within these solvers, a large portion of the solution
remains stable, i.e., unchanged across search iterations, causing redundant
computations, especially for large-scale VRPs with long subtours. To address
this, we pioneer the formal study of the First-Segment-Then-Aggregate (FSTA)
decomposition technique to accelerate iterative solvers. Specifically, FSTA
preserves stable solution segments during the search, aggregates nodes within
each segment into fixed hypernodes, and focuses the search only on unstable
portions. Yet, a key challenge lies in identifying which segments should be
aggregated by FSTA. To this end, we then introduce Learning-to-Segment (L2Seg),
a novel neural framework to intelligently differentiate potentially stable and
unstable portions for FSTA decomposition. We present three L2Seg variants:
non-autoregressive (globally comprehensive but locally indiscriminate),
autoregressive (locally refined but globally deficient), and their synergy,
with bespoke training and inference strategies. Empirical results on CVRP and
VRPTW suggest that L2Seg accelerates state-of-the-art iterative solvers by up
to 7x. Additionally, we provide in-depth analysis showing NAR and AR synergy
achieves best performance by combining their complementary strengths. Notably,
L2Seg is a flexible framework that is compatible with traditional,
learning-based, and hybrid solvers, while supporting a broad class of VRPs.

</details>


### [146] [On-Policy Optimization of ANFIS Policies Using Proximal Policy Optimization](https://arxiv.org/pdf/2507.01039)
*Kaaustaaub Shankar, Wilhelm Louw, Kelly Cohen*

Main category: cs.LG

TL;DR: A reinforcement learning approach using PPO trains neuro-fuzzy controllers, outperforming DQN-based methods in stability and convergence.


<details>
  <summary>Details</summary>
Motivation: To improve the training of explainable neuro-fuzzy controllers by replacing off-policy DQN with stable on-policy PPO.

Method: Proximal Policy Optimization (PPO) is applied to train neuro-fuzzy controllers, evaluated in CartPole-v1 against ANFIS-DQN baselines.

Result: PPO-trained agents achieved a mean return of 500 +/- 0, showing less variance and faster convergence than DQN methods.

Conclusion: PPO is a promising method for training explainable neuro-fuzzy controllers in RL tasks.

Abstract: We propose a reinforcement learning (RL) approach for training neuro-fuzzy
controllers using Proximal Policy Optimization (PPO). Building on prior work
that applied Deep Q-Learning to Adaptive Neuro-Fuzzy Inference Systems (ANFIS),
our method replaces the off-policy value-based framework with a stable
on-policy actor-critic loop. We evaluate this approach in the CartPole-v1
environment using multiple random seeds and compare its learning performance
against ANFIS-Deep Q-Network (DQN) baselines. It was found that PPO-trained
fuzzy agents achieved a mean return of 500 +/- 0 on CartPole-v1 after 20000
updates, showcasing less variance than prior DQN-based methods during training
and overall faster convergence. These findings suggest that PPO offers a
promising pathway for training explainable neuro-fuzzy controllers in
reinforcement learning tasks.

</details>


### [147] [Fast Clifford Neural Layers](https://arxiv.org/pdf/2507.01040)
*Tianxiang Xia, Max Neuwinger, Lin Xiao*

Main category: cs.LG

TL;DR: Clifford Neural Layers enhance PDE modeling using Clifford Algebra, optimizing 2/3D convolutional and multivector activation layers for CPU performance, achieving 30% faster inference than PyTorch.


<details>
  <summary>Details</summary>
Motivation: To improve PDE modeling efficiency by integrating Clifford Algebra into neural networks and optimizing performance for CPU.

Method: Optimized inference of 2/3D Clifford convolutional layers and multivector activation layers for CPU performance.

Result: 30% faster inference than standard PyTorch in large data and network sizes (>L2 cache).

Conclusion: The implementation demonstrates significant speed improvements, with open-sourced code available for further use.

Abstract: Clifford Neural Layers improve PDE modeling by introducing Clifford Algebra
into neural networks. In this project we focus on optimizing the inference of
2/3D Clifford convolutional layers and multivector activation layers for one
core CPU performance.
  Overall, by testing on a real network block involving Clifford convolutional
layers and multivector activation layers, we observe that our implementation is
30% faster than standard PyTorch implementation in relatively large data +
network size (>L2 cache).
  We open source our code base at
https://github.com/egretwAlker/c-opt-clifford-layers

</details>


### [148] [Fast AI Model Splitting over Edge Networks](https://arxiv.org/pdf/2507.01041)
*Zuguang Li, Wen Wu, Shaohua Wu, Songge Zhang, Ye Wang, Xuemin, Shen*

Main category: cs.LG

TL;DR: The paper proposes fast DAG-based and block-wise algorithms for optimal model splitting in split learning, reducing computational complexity and training delays.


<details>
  <summary>Details</summary>
Motivation: Complex AI model architectures increase computational complexity for optimal model splitting in split learning, necessitating efficient solutions.

Method: Represents AI models as DAGs, reformulates splitting as a minimum s-t cut problem, and introduces DAG-based and block-wise algorithms for optimal splitting.

Result: The algorithms identify optimal splitting in milliseconds and reduce training delay by 24.62%-38.95% compared to benchmarks.

Conclusion: The proposed algorithms efficiently solve the model splitting problem, enhancing computational efficiency and reducing training delays in dynamic edge networks.

Abstract: Split learning (SL) has emerged as a computationally efficient approach for
artificial intelligence (AI) model training, which can alleviate device-side
computational workloads. However, complex AI model architectures pose high
computational complexity to obtain the optimal model splitting. In this paper,
we represent an arbitrary AI model as a directed acyclic graph (DAG), and then
reformulate the optimal model splitting problem as a minimum s-t cut search
problem. To solve the problem, we propose a fast DAG-based model splitting
algorithm, which restructures the DAG to enable the optimal model splitting
identification via a maximum flow method. Theoretical analysis indicates that
the proposed algorithm is optimal. Furthermore, considering AI models with
block structures, we propose a block-wise model splitting algorithm to reduce
computational complexity. The algorithm abstracts each block, i.e., a component
consisting of multiple layers, into a single vertex, thereby obtaining the
optimal model splitting via a simplified DAG. Extensive experimental results
demonstrate that the proposed algorithms can determine the optimal model
splitting within milliseconds, as well as reduce training delay by
24.62%-38.95% in dynamic edge networks as compared to the state-of-the-art
benchmarks.

</details>


### [149] [Data Classification with Dynamically Growing and Shrinking Neural Networks](https://arxiv.org/pdf/2507.01043)
*Szymon Świderski, Agnieszka Jastrzębska*

Main category: cs.LG

TL;DR: A method for dynamically adjusting neural network architecture during training using Monte Carlo Tree Search, validated on visual and time series datasets.


<details>
  <summary>Details</summary>
Motivation: To optimize neural network architecture dynamically during training, moving beyond fixed architectures.

Method: Uses Monte Carlo Tree Search to simulate and compare candidate architecture changes, enabling dynamic growing and shrinking.

Result: Effective in multivariate time series classification due to dynamic adaptability, with promising performance in visual tasks.

Conclusion: The method is robust, adaptable, and validated across datasets, with provided Python code for reproducibility.

Abstract: The issue of data-driven neural network model construction is one of the core
problems in the domain of Artificial Intelligence. A standard approach assumes
a fixed architecture with trainable weights. A conceptually more advanced
assumption is that we not only train the weights, but also find out the optimal
model architecture. We present a new method that realizes just that. This
article is an extended version of our conference paper titled "Dynamic Growing
and Shrinking of Neural Networks with Monte Carlo Tree Search [26]". In the
paper, we show in detail how to create a neural network with a procedure that
allows dynamic shrinking and growing of the model while it is being trained.
The decision-making mechanism for the architectural design is governed by a
Monte Carlo tree search procedure which simulates network behavior and allows
to compare several candidate architecture changes to choose the best one. The
proposed method was validated using both visual and time series datasets,
demonstrating its particular effectiveness in multivariate time series
classification. This is attributed to the architecture's ability to adapt
dynamically, allowing independent modifications for each time series. The
approach is supplemented by Python source code for reproducibility.
Experimental evaluations in visual pattern and multivariate time series
classification tasks revealed highly promising performance, underscoring the
method's robustness and adaptability.

</details>


### [150] [Sensing Cardiac Health Across Scenarios and Devices: A Multi-Modal Foundation Model Pretrained on Heterogeneous Data from 1.7 Million Individuals](https://arxiv.org/pdf/2507.01045)
*Xiao Gu, Wei Tang, Jinpei Han, Veer Sangha, Fenglin Liu, Shreyank N Gowda, Antonio H. Ribeiro, Patrick Schwab, Kim Branson, Lei Clifton, Antonio Luiz P. Ribeiro, Zhangdaihong Liu, David A. Clifton*

Main category: cs.LG

TL;DR: A cardiac sensing foundation model (CSFM) using transformers and masked pretraining is introduced, outperforming traditional methods in diverse cardiac signal tasks.


<details>
  <summary>Details</summary>
Motivation: To address limitations of conventional deep learning in cardiac signal analysis, which lacks robustness and generalizability across diverse clinical settings.

Method: CSFM leverages transformer architectures and generative masked pretraining on multi-modal data (ECG, PPG, and text reports) from 1.7 million individuals.

Result: CSFM outperforms traditional approaches in diagnostic tasks, demographic recognition, vital sign measurement, and ECG question answering, showing robustness across lead configurations and sensor modalities.

Conclusion: CSFM is a versatile and scalable solution for comprehensive cardiac monitoring, demonstrating superior performance and adaptability.

Abstract: Cardiac biosignals, such as electrocardiograms (ECG) and photoplethysmograms
(PPG), are of paramount importance for the diagnosis, prevention, and
management of cardiovascular diseases, and have been extensively used in a
variety of clinical tasks. Conventional deep learning approaches for analyzing
these signals typically rely on homogeneous datasets and static bespoke models,
limiting their robustness and generalizability across diverse clinical settings
and acquisition protocols. In this study, we present a cardiac sensing
foundation model (CSFM) that leverages advanced transformer architectures and a
generative, masked pretraining strategy to learn unified representations from
vast, heterogeneous health records. Our model is pretrained on an innovative
multi-modal integration of data from multiple large-scale datasets (including
MIMIC-III-WDB, MIMIC-IV-ECG, and CODE), comprising cardiac signals and the
corresponding clinical or machine-generated text reports from approximately 1.7
million individuals. We demonstrate that the embeddings derived from our CSFM
not only serve as effective feature extractors across diverse cardiac sensing
scenarios, but also enable seamless transfer learning across varying input
configurations and sensor modalities. Extensive evaluations across diagnostic
tasks, demographic information recognition, vital sign measurement, clinical
outcome prediction, and ECG question answering reveal that CSFM consistently
outperforms traditional one-modal-one-task approaches. Notably, CSFM exhibits
robust performance across multiple ECG lead configurations from standard
12-lead systems to single-lead setups, and in scenarios where only ECG, only
PPG, or a combination thereof is available. These findings highlight the
potential of CSFM as a versatile and scalable solution, for comprehensive
cardiac monitoring.

</details>


### [151] [Variational Digital Twins](https://arxiv.org/pdf/2507.01047)
*Logan A. Burnett, Umme Mahbuba Nabila, Majdi I. Radaideh*

Main category: cs.LG

TL;DR: The paper proposes a variational digital twin (VDT) framework to address gaps in current digital twin literature, offering real-time updates, uncertainty bounds, and efficiency. It demonstrates success in energy-sector applications.


<details>
  <summary>Details</summary>
Motivation: Current digital twin literature lacks clear frameworks for real-time implementation, model uncertainty handling, and efficient information exchange between models and assets.

Method: The VDT framework enhances standard neural architectures with a Bayesian output layer and a novel updating algorithm for real-time, uncertainty-aware performance.

Result: VDT achieves high accuracy (R2 > 0.95) in energy-sector applications, reduces experiments and training time, and adapts to degraded instrumentation or dynamic conditions.

Conclusion: The VDT framework transforms conventional models into dependable, uncertainty-aware digital twins, suitable for industrial and scientific energy systems.

Abstract: While digital twins (DT) hold promise for providing real-time insights into
complex energy assets, much of the current literature either does not offer a
clear framework for information exchange between the model and the asset, lacks
key features needed for real-time implementation, or gives limited attention to
model uncertainty. Here, we aim to solve these gaps by proposing a variational
digital twin (VDT) framework that augments standard neural architectures with a
single Bayesian output layer. This lightweight addition, along with a novel VDT
updating algorithm, lets a twin update in seconds on commodity GPUs while
producing calibrated uncertainty bounds that can inform experiment design,
control algorithms, and model reliability. The VDT is evaluated on four
energy-sector problems. For critical-heat-flux prediction, uncertainty-driven
active learning reaches R2 = 0.98 using 47 % fewer experiments and one-third
the training time of random sampling. A three-year renewable-generation twin
maintains R2 > 0.95 for solar output and curbs error growth for volatile wind
forecasts via monthly updates that process only one month of data at a time. A
nuclear reactor transient cooldown twin reconstructs thermocouple signals with
R2 > 0.99 and preserves accuracy after 50 % sensor loss, demonstrating
robustness to degraded instrumentation. Finally, a physics-informed Li-ion
battery twin, retrained after every ten discharges, lowers voltage mean-squared
error by an order of magnitude relative to the best static model while adapting
its credible intervals as the cell approaches end-of-life. These results
demonstrate that combining modest Bayesian augmentation with efficient update
schemes turns conventional surrogates into uncertainty-aware, data-efficient,
and computationally tractable DTs, paving the way for dependable models across
industrial and scientific energy systems.

</details>


### [152] [Text Detoxification: Data Efficiency, Semantic Preservation and Model Generalization](https://arxiv.org/pdf/2507.01050)
*Jing Yu, Yibo Zhao, Jiapeng Zhu, Wenming Shao, Bo Pang, Zhao Zhang, Xiang Li*

Main category: cs.LG

TL;DR: A two-stage training framework for detoxifying text on social media achieves strong detoxification, semantic preservation, and generalization with reduced reliance on annotated data.


<details>
  <summary>Details</summary>
Motivation: The urgent need for effective detoxification methods that preserve semantics and generalize well, addressing limitations of existing approaches.

Method: A two-stage framework: supervised fine-tuning on filtered parallel data, followed by training with unlabeled toxic inputs and a custom reward model using Group Relative Policy Optimization.

Result: State-of-the-art performance in detoxification, improved generalization, and reduced dependence on annotated data.

Conclusion: The proposed method effectively mitigates trade-offs in detoxification, offering a robust and data-efficient solution.

Abstract: The widespread dissemination of toxic content on social media poses a serious
threat to both online environments and public discourse, highlighting the
urgent need for detoxification methods that effectively remove toxicity while
preserving the original semantics. However, existing approaches often struggle
to simultaneously achieve strong detoxification performance, semantic
preservation, and robustness to out-of-distribution data. Moreover, they
typically rely on costly, manually annotated parallel corpora while showing
poor data efficiency. To address these challenges, we propose a two-stage
training framework that jointly optimizes for data efficiency, semantic
preservation, and model generalization. We first perform supervised fine-tuning
on a small set of high-quality, filtered parallel data to establish a strong
initialization. Then, we leverage unlabeled toxic inputs and a custom-designed
reward model to train the LLM using Group Relative Policy Optimization.
Experimental results demonstrate that our method effectively mitigates the
trade-offs faced by previous work, achieving state-of-the-art performance with
improved generalization and significantly reduced dependence on annotated data.
Our code is available at:
https://anonymous.4open.science/r/Detoxification-of-Text-725F/

</details>


### [153] [3W Dataset 2.0.0: a realistic and public dataset with rare undesirable real events in oil wells](https://arxiv.org/pdf/2507.01048)
*Ricardo Emanuel Vaz Vargas, Afrânio José de Melo Junior, Celso José Munaro, Cláudio Benevenuto de Campos Lima, Eduardo Toledo de Lima Junior, Felipe Muntzberg Barrocas, Flávio Miguel Varejão, Guilherme Fidelis Peixer, Igor de Melo Nery Oliveira, Jader Riso Barbosa Jr., Jaime Andrés Lozano Cadena, Jean Carlos Dias de Araújo, João Neuenschwander Escosteguy Carneiro, Lucas Gouveia Omena Lopes, Lucas Pereira de Gouveia, Mateus de Araujo Fernandes, Matheus Lima Scramignon, Patrick Marques Ciarelli, Rodrigo Castello Branco, Rogério Leite Alves Pinto*

Main category: cs.LG

TL;DR: The paper introduces the updated 3W Dataset, a public resource for detecting undesirable events in oil wells using AI/ML, aiming to improve detection methods and outcomes.


<details>
  <summary>Details</summary>
Motivation: Undesirable events in oil wells lead to economic, environmental, and human risks. Public datasets for AI/ML solutions were lacking, prompting the creation and collaborative development of the 3W Dataset.

Method: The 3W Dataset is a labeled multivariate time series dataset, collaboratively developed and publicly released to support AI/ML research.

Result: The dataset has become a foundational reference in the field, with structural updates and additional labeled data in its latest version.

Conclusion: The updated 3W Dataset aims to enhance detection methodologies and enable timely corrective actions for undesirable events in oil wells.

Abstract: In the oil industry, undesirable events in oil wells can cause economic
losses, environmental accidents, and human casualties. Solutions based on
Artificial Intelligence and Machine Learning for Early Detection of such events
have proven valuable for diverse applications across industries. In 2019,
recognizing the importance and the lack of public datasets related to
undesirable events in oil wells, Petrobras developed and publicly released the
first version of the 3W Dataset, which is essentially a set of Multivariate
Time Series labeled by experts. Since then, the 3W Dataset has been developed
collaboratively and has become a foundational reference for numerous works in
the field. This data article describes the current publicly available version
of the 3W Dataset, which contains structural modifications and additional
labeled data. The detailed description provided encourages and supports the 3W
community and new 3W users to improve previous published results and to develop
new robust methodologies, digital products and services capable of detecting
undesirable events in oil wells with enough anticipation to enable corrective
or mitigating actions.

</details>


### [154] [Long-Sequence Memory with Temporal Kernels and Dense Hopfield Functionals](https://arxiv.org/pdf/2507.01052)
*Ahmed Farooq*

Main category: cs.LG

TL;DR: A novel energy functional for long-sequence memory is introduced, leveraging dense Hopfield networks with exponential storage capacity and temporal dependencies for efficient sequential retrieval.


<details>
  <summary>Details</summary>
Motivation: To address limitations of transformers in long-context tasks by enhancing memory and temporal dependency handling.

Method: Proposes a temporal kernel $K(m, k)$ within dense Hopfield networks to incorporate temporal dependencies for sequential retrieval.

Result: Successful storage and retrieval of movie frames, demonstrating high-dimensional pattern handling and sequential efficiency.

Conclusion: The model improves long-sequence tasks, with applications in NLP, forecasting, and time-series data, offering a promising alternative to transformers.

Abstract: In this study we introduce a novel energy functional for long-sequence
memory, building upon the framework of dense Hopfield networks which achieves
exponential storage capacity through higher-order interactions. Building upon
earlier work on long-sequence Hopfield memory models, we propose a temporal
kernal $K(m, k)$ to incorporate temporal dependencies, enabling efficient
sequential retrieval of patterns over extended sequences. We demonstrate the
successful application of this technique for the storage and sequential
retrieval of movies frames which are well suited for this because of the high
dimensional vectors that make up each frame creating enough variation between
even sequential frames in the high dimensional space. The technique has
applications in modern transformer architectures, including efficient
long-sequence modeling, memory augmentation, improved attention with temporal
bias, and enhanced handling of long-term dependencies in time-series data. Our
model offers a promising approach to address the limitations of transformers in
long-context tasks, with potential implications for natural language
processing, forecasting, and beyond.

</details>


### [155] [XxaCT-NN: Structure Agnostic Multimodal Learning for Materials Science](https://arxiv.org/pdf/2507.01054)
*Jithendaraa Subramanian, Linda Hung, Daniel Schweigert, Santosh Suram, Weike Ye*

Main category: cs.LG

TL;DR: A scalable multimodal framework is proposed for materials discovery, using elemental composition and XRD without crystal structure input, achieving faster convergence and improved accuracy.


<details>
  <summary>Details</summary>
Motivation: Overcome the impracticality of structure-based models in real-world applications where atomic structures are often unknown or hard to obtain.

Method: Integrates modality-specific encoders with a cross-attention fusion module, trained on the Alexandria dataset. Uses masked XRD modeling and contrastive alignment for self-supervised pretraining.

Result: Pretraining yields faster convergence (4.2x speedup), improved accuracy, and better representation quality. Multimodal performance scales better with dataset size than unimodal baselines.

Conclusion: The framework establishes a path toward structure-free, experimentally grounded foundation models for materials science.

Abstract: Recent advances in materials discovery have been driven by structure-based
models, particularly those using crystal graphs. While effective for
computational datasets, these models are impractical for real-world
applications where atomic structures are often unknown or difficult to obtain.
We propose a scalable multimodal framework that learns directly from elemental
composition and X-ray diffraction (XRD) -- two of the more available modalities
in experimental workflows without requiring crystal structure input. Our
architecture integrates modality-specific encoders with a cross-attention
fusion module and is trained on the 5-million-sample Alexandria dataset. We
present masked XRD modeling (MXM), and apply MXM and contrastive alignment as
self-supervised pretraining strategies. Pretraining yields faster convergence
(up to 4.2x speedup) and improves both accuracy and representation quality. We
further demonstrate that multimodal performance scales more favorably with
dataset size than unimodal baselines, with gains compounding at larger data
regimes. Our results establish a path toward structure-free, experimentally
grounded foundation models for materials science.

</details>


### [156] [Evaluating Pavement Deterioration Rates Due to Flooding Events Using Explainable AI](https://arxiv.org/pdf/2507.01056)
*Lidan Peng, Lu Gao, Feng Hong, Jingran Sun*

Main category: cs.LG

TL;DR: The study examines how flooding accelerates pavement roughness (measured by IRI) using 20 years of TxDOT data and XAI techniques like SHAP and LIME. Results show faster deterioration in flooded areas, urging proactive mitigation strategies.


<details>
  <summary>Details</summary>
Motivation: Flooding causes immediate and long-term damage to pavements, but its specific impact on roughness (IRI) is not well quantified.

Method: Analyzed 20 years of pavement condition data from TxDOT's PMIS, integrated with flood event data. Used statistical analysis and XAI (SHAP, LIME) to assess flood impact.

Result: Flooded pavements show a faster increase in roughness (IRI) compared to non-flooded sections.

Conclusion: Proactive flood mitigation (e.g., better drainage, flood-resistant materials) is needed to enhance pavement resilience in flood-prone areas.

Abstract: Flooding can damage pavement infrastructure significantly, causing both
immediate and long-term structural and functional issues. This research
investigates how flooding events affect pavement deterioration, specifically
focusing on measuring pavement roughness by the International Roughness Index
(IRI). To quantify these effects, we utilized 20 years of pavement condition
data from TxDOT's PMIS database, which is integrated with flood event data,
including duration and spatial extent. Statistical analyses were performed to
compare IRI values before and after flooding and to calculate the deterioration
rates influenced by flood exposure. Moreover, we applied Explainable Artificial
Intelligence (XAI) techniques, such as SHapley Additive exPlanations (SHAP) and
Local Interpretable Model-Agnostic Explanations (LIME), to assess the impact of
flooding on pavement performance. The results demonstrate that flood-affected
pavements experience a more rapid increase in roughness compared to non-flooded
sections. These findings emphasize the need for proactive flood mitigation
strategies, including improved drainage systems, flood-resistant materials, and
preventative maintenance, to enhance pavement resilience in vulnerable regions.

</details>


### [157] [Loop2Net: Data-Driven Generation and Optimization of Airfoil CFD Meshes from Sparse Boundary Coordinates](https://arxiv.org/pdf/2507.01057)
*Lushun Fan, Yuqin Xia, Jun Li, Karl Jenkins*

Main category: cs.LG

TL;DR: An intelligent optimization system using deep CNN for mesh quality, featuring Loop2Net and loss functions to predict and optimize mesh generation.


<details>
  <summary>Details</summary>
Motivation: To improve mesh generation and optimization for given wing coordinates using deep learning techniques.

Method: Uses a deep convolutional neural network (Loop2Net) with two key loss functions and penalties for training and optimization.

Result: Achieves effective mesh generation and optimization through the proposed system.

Conclusion: The system successfully meets the goal of mesh generation with continuous optimization.

Abstract: In this study, an innovative intelligent optimization system for mesh quality
is proposed, which is based on a deep convolutional neural network
architecture, to achieve mesh generation and optimization. The core of the
study is the Loop2Net generator and loss function, it predicts the mesh based
on the given wing coordinates. And the model's performance is continuously
optimised by two key loss functions during the training. Then discipline by
adding penalties, the goal of mesh generation was finally reached.

</details>


### [158] [Evaluation of a Foundational Model and Stochastic Models for Forecasting Sporadic or Spiky Production Outages of High-Performance Machine Learning Services](https://arxiv.org/pdf/2507.01067)
*Keun Soo Yim*

Main category: cs.LG

TL;DR: The paper explores optimizing a foundational time series model for forecasting rare, spiky events in production outages, comparing it with classical stochastic models and achieving less than 6% error in year-long outage predictions.


<details>
  <summary>Details</summary>
Motivation: To address the challenge of forecasting rare, spiky events in high-performance ML services, where foundational models haven't been applied yet.

Method: Optimize a state-of-the-art foundational model and compare its performance with classical stochastic models (e.g., moving average, autoregressive) for sporadic events.

Result: The foundational model outperforms stochastic models, achieving less than 6% error in estimating year-long outage statistics for specific root causes.

Conclusion: Foundational models are effective for forecasting rare, spiky events, providing valuable insights compared to traditional stochastic approaches.

Abstract: Time series forecasting models have diverse real world applications (e.g.,
from electricity metrics to software workload). Latest foundational models
trained for time series forecasting show strengths (e.g., for long sequences
and in zero-shot settings). However, foundational model was not yet used for
forecasting rare, spiky events, i.e., a challenging target because those are a
corner case of extreme events. In this paper, we optimize a state-of-the-art
foundational model to forecast sporadic or spiky production outages of
high-performance machine learning services powering billions of client devices.
We evaluate the forecasting errors of the foundational model compared with
classical stochastic forecasting models (e.g., moving average and
autoregressive). The analysis helps us understand how each of the evaluated
models performs for the sporadic or spiky events. For example, it identifies
the key patterns in the target data that are well tracked by the foundational
model vs. each of the stochastic models. We use the models with optimal
parameters to estimate a year-long outage statistics of a particular root cause
with less than 6% value errors.

</details>


### [159] [Prediction of Freezing of Gait in Parkinsons Disease using Explainable AI and Federated Deep Learning for Wearable Sensors](https://arxiv.org/pdf/2507.01068)
*Biplov Paneru*

Main category: cs.LG

TL;DR: The paper develops explainable AI methods using IMU data to detect and predict Freezing of Gait (FOG) in Parkinson's disease, achieving 99% accuracy with a Stacking Ensemble model and employing federated learning for model training.


<details>
  <summary>Details</summary>
Motivation: Early detection of FOG in Parkinson's disease is critical for patient care, and explainable AI methods are needed to understand model decisions.

Method: Machine learning models (CatBoost, XGBoost, Extra Trees) and a Stacking Ensemble are used for classification. Federated learning with a hybrid Conv1D + LSTM architecture is employed for training.

Result: The Stacking Ensemble model achieves nearly 99% accuracy, outperforming a hybrid bidirectional GRU model. SHAP analysis identifies time as the most influential feature.

Conclusion: The proposed framework is effective for FOG prediction, combining high accuracy with explainability and privacy-preserving federated learning.

Abstract: This study leverages an Inertial Measurement Unit (IMU) dataset to develop
explainable AI methods for the early detection and prediction of Freezing of
Gait (FOG), a common symptom in Parkinson's disease. Machine learning models,
including CatBoost, XGBoost, and Extra Trees classifiers, are employed to
accurately categorize FOG episodes based on relevant clinical features. A
Stacking Ensemble model achieves superior performance, surpassing a hybrid
bidirectional GRU model and reaching nearly 99% classification accuracy. SHAP
interpretability analysis reveals that time (seconds) is the most influential
factor in distinguishing gait patterns. Additionally, the proposed FOG
prediction framework incorporates federated learning, where models are trained
locally on individual devices and aggregated on a central server using a
federated averaging approach, utilizing a hybrid Conv1D + LSTM architecture for
enhanced predictive capability.

</details>


### [160] [Rotational Sampling: A Plug-and-Play Encoder for Rotation-Invariant 3D Molecular GNNs](https://arxiv.org/pdf/2507.01073)
*Dian Jin*

Main category: cs.LG

TL;DR: A novel 3D encoding module for GNNs using rotational sampling achieves rotational invariance and improves molecular property prediction.


<details>
  <summary>Details</summary>
Motivation: Traditional GNNs struggle with 3D molecular structures due to orientation variability, limiting generalization and robustness. Existing methods are either too rigid or computationally expensive.

Method: Proposes a plug-and-play 3D encoding module using rotational sampling and SO(3) group expectation, with a post-alignment strategy for strict invariance.

Result: Outperforms existing methods on QM9 and C10 datasets in accuracy, robustness, and generalization while maintaining low computational cost.

Conclusion: The method offers an efficient, interpretable solution for 3D molecular data in drug discovery and material design.

Abstract: Graph neural networks (GNNs) have achieved remarkable success in molecular
property prediction. However, traditional graph representations struggle to
effectively encode the inherent 3D spatial structures of molecules, as
molecular orientations in 3D space introduce significant variability, severely
limiting model generalization and robustness. Existing approaches primarily
focus on rotation-invariant and rotation-equivariant methods. Invariant methods
often rely heavily on prior knowledge and lack sufficient generalizability,
while equivariant methods suffer from high computational costs. To address
these limitations, this paper proposes a novel plug-and-play 3D encoding module
leveraging rotational sampling. By computing the expectation over the SO(3)
rotational group, the method naturally achieves approximate rotational
invariance. Furthermore, by introducing a carefully designed post-alignment
strategy, strict invariance can be achieved without compromising performance.
Experimental evaluations on the QM9 and C10 Datasets demonstrate superior
predictive accuracy, robustness, and generalization performance compared to
existing methods. Moreover, the proposed approach maintains low computational
complexity and enhanced interpretability, providing a promising direction for
efficient and effective handling of 3D molecular information in drug discovery
and material design.

</details>


### [161] [Contrastive Learning and Adversarial Disentanglement for Privacy-Aware Task-Oriented Semantic Communication](https://arxiv.org/pdf/2410.22784)
*Omar Erak, Omar Alhussein, Wen Tong*

Main category: cs.LG

TL;DR: CLAD, a method combining contrastive learning and adversarial disentanglement, improves task-oriented semantic communication in 6G-IoT by enhancing privacy and efficiency.


<details>
  <summary>Details</summary>
Motivation: Addressing the challenge of disentangling task-relevant and irrelevant information in 6G-IoT, where existing methods fall short in privacy and performance.

Method: Proposes CLAD, using contrastive learning for task-relevant features and adversarial disentanglement to discard irrelevant data, alongside introducing the IRI metric.

Result: CLAD outperforms baselines in semantic extraction, task performance, privacy, and IRI, proving effective for 6G-IoT.

Conclusion: CLAD is a promising solution for efficient, privacy-preserving, and trustworthy 6G-IoT services.

Abstract: Task-oriented semantic communication systems have emerged as a promising
approach to achieving efficient and intelligent data transmission in
next-generation networks, where only information relevant to a specific task is
communicated. This is particularly important in 6G-enabled Internet of Things
(6G-IoT) scenarios, where bandwidth constraints, latency requirements, and data
privacy are critical. However, existing methods struggle to fully disentangle
task-relevant and task-irrelevant information, leading to privacy concerns and
suboptimal performance. To address this, we propose an information-bottleneck
inspired method, named CLAD (contrastive learning and adversarial
disentanglement). CLAD utilizes contrastive learning to effectively capture
task-relevant features while employing adversarial disentanglement to discard
task-irrelevant information. Additionally, due to the absence of reliable and
reproducible methods to quantify the minimality of encoded feature vectors, we
introduce the Information Retention Index (IRI), a comparative metric used as a
proxy for the mutual information between the encoded features and the input.
The IRI reflects how minimal and informative the representation is, making it
highly relevant for privacy-preserving and bandwidth-efficient 6G-IoT systems.
Extensive experiments demonstrate that CLAD outperforms state-of-the-art
baselines in terms of semantic extraction, task performance, privacy
preservation, and IRI, making it a promising building block for responsible,
efficient and trustworthy 6G-IoT services.

</details>


### [162] [Provenance Tracking in Large-Scale Machine Learning Systems](https://arxiv.org/pdf/2507.01075)
*Gabriele Padovani, Valentine Anantharaj, Sandro Fiore*

Main category: cs.LG

TL;DR: The paper introduces yProv4ML, a library for collecting provenance data in AI model training to optimize efficiency, accuracy, and energy use.


<details>
  <summary>Details</summary>
Motivation: The growing demand for large-scale AI models necessitates balancing computational efficiency, execution time, accuracy, and energy consumption. Provenance data is key for insights and accountability.

Method: The yProv4ML library collects provenance data in JSON format, adhering to W3C PROV and ProvML standards, and supports plugin-based extensibility.

Result: yProv4ML enables flexible, extensible provenance data collection, integrated with the yProv framework for workflow management.

Conclusion: Provenance tools like yProv4ML are essential for optimizing AI model training and ensuring reproducibility and efficiency.

Abstract: As the demand for large scale AI models continues to grow, the optimization
of their training to balance computational efficiency, execution time, accuracy
and energy consumption represents a critical multidimensional challenge.
Achieving this balance requires not only innovative algorithmic techniques and
hardware architectures but also comprehensive tools for monitoring, analyzing,
and understanding the underlying processes involved in model training and
deployment. Provenance data information about the origins, context, and
transformations of data and processes has become a key component in this
pursuit. By leveraging provenance, researchers and engineers can gain insights
into resource usage patterns, identify inefficiencies, and ensure
reproducibility and accountability in AI development workflows. For this
reason, the question of how distributed resources can be optimally utilized to
scale large AI models in an energy efficient manner is a fundamental one. To
support this effort, we introduce the yProv4ML library, a tool designed to
collect provenance data in JSON format, compliant with the W3C PROV and ProvML
standards. yProv4ML focuses on flexibility and extensibility, and enables users
to integrate additional data collection tools via plugins. The library is fully
integrated with the yProv framework, allowing for higher level pairing in tasks
run also through workflow management systems.

</details>


### [163] [Are Large Brainwave Foundation Models Capable Yet? Insights from Fine-tuning](https://arxiv.org/pdf/2507.01196)
*Na Lee, Konstantinos Barmpas, Yannis Panagakis, Dimitrios Adamos, Nikolaos Laskaris, Stefanos Zafeiriou*

Main category: cs.LG

TL;DR: LBMs show marginal improvements over traditional deep architectures in BCI tasks but require more parameters. LoRA reduces parameters without performance loss, indicating inefficiencies in current LBMs.


<details>
  <summary>Details</summary>
Motivation: To evaluate the effectiveness of LBMs in brainwave modeling and address their inefficiencies for BCI applications.

Method: Systematic fine-tuning experiments on BCI tasks, including memory tasks and sleep stage classification, using LoRA for parameter reduction.

Result: LBMs achieve only 0.9%-1.2% improvement over traditional methods but with higher parameter costs. LoRA reduces parameters without degrading performance.

Conclusion: Domain-specific strategies and architectural redesigns are needed to fully leverage LBMs in brainwave analysis.

Abstract: Foundation Models have demonstrated significant success across various
domains in Artificial Intelligence (AI), yet their capabilities for brainwave
modeling remain unclear. In this paper, we comprehensively evaluate current
Large Brainwave Foundation Models (LBMs) through systematic fine-tuning
experiments across multiple Brain-Computer Interface (BCI) benchmark tasks,
including memory tasks and sleep stage classification. Our extensive analysis
shows that state-of-the-art LBMs achieve only marginal improvements (0.9%-1.2%)
over traditional deep architectures while requiring significantly more
parameters (millions vs thousands), raising important questions about their
efficiency and applicability in BCI contexts. Moreover, through detailed
ablation studies and Low-Rank Adaptation (LoRA), we significantly reduce
trainable parameters without performance degradation, while demonstrating that
architectural and training inefficiencies limit LBMs' current capabilities. Our
experiments span both full model fine-tuning and parameter-efficient adaptation
techniques, providing insights into optimal training strategies for BCI
applications. We pioneer the application of LoRA to LBMs, revealing that
performance benefits generally emerge when adapting multiple neural network
components simultaneously. These findings highlight the critical need for
domain-specific development strategies to advance LBMs, suggesting that current
architectures may require redesign to fully leverage the potential of
foundation models in brainwave analysis.

</details>


### [164] [Good Enough to Learn: LLM-based Anomaly Detection in ECU Logs without Reliable Labels](https://arxiv.org/pdf/2507.01077)
*Bogdan Bogdan, Arina Cazacu, Laura Vasilie*

Main category: cs.LG

TL;DR: A novel decoder-only LLM for anomaly detection in ECU communication logs, addressing challenges like lack of tailored LLMs and inconsistent ground truth data.


<details>
  <summary>Details</summary>
Motivation: Specialized domains like automotive communication systems need scalable anomaly detection solutions, but existing methods (supervised/clustering) fall short.

Method: Uses a decoder-only LLM trained on UDP logs, with entropy regularization to handle inconsistent labels and improve uncertainty in anomalies.

Result: Introduces a scalable system that learns from minimal examples, improves detection accuracy, and reduces reliance on manual labeling.

Conclusion: The proposed LLM-based approach offers a scalable, adaptable solution for anomaly detection in complex ECU communication environments.

Abstract: Anomaly detection often relies on supervised or clustering approaches, with
limited success in specialized domains like automotive communication systems
where scalable solutions are essential. We propose a novel decoder-only Large
Language Model (LLM) to detect anomalies in Electronic Control Unit (ECU)
communication logs. Our approach addresses two key challenges: the lack of LLMs
tailored for ECU communication and the complexity of inconsistent ground truth
data. By learning from UDP communication logs, we formulate anomaly detection
simply as identifying deviations in time from normal behavior. We introduce an
entropy regularization technique that increases model's uncertainty in known
anomalies while maintaining consistency in similar scenarios. Our solution
offers three novelties: a decoder-only anomaly detection architecture, a way to
handle inconsistent labeling, and an adaptable LLM for different ECU
communication use cases. By leveraging the generative capabilities of
decoder-only models, we present a new technique that addresses the high cost
and error-prone nature of manual labeling through a more scalable system that
is able to learn from a minimal set of examples, while improving detection
accuracy in complex communication environments.

</details>


### [165] [yProv4ML: Effortless Provenance Tracking for Machine Learning Systems](https://arxiv.org/pdf/2507.01078)
*Gabriele Padovani, Valentine Anantharaj, Sandro Fiore*

Main category: cs.LG

TL;DR: yProv4ML is a framework for capturing provenance data in machine learning processes using PROV-JSON format, addressing transparency and rigor issues in LLM development.


<details>
  <summary>Details</summary>
Motivation: The lack of transparency and rigor in LLM development, especially around hyperparameters and epochs, highlights the need for better provenance tracking.

Method: Proposes yProv4ML, a framework that captures provenance data in PROV-JSON format with minimal code changes.

Result: Enables automated and standardized collection of provenance information during ML processes.

Conclusion: yProv4ML improves transparency and rigor in ML development by addressing lineage and data capture challenges.

Abstract: The rapid growth of interest in large language models (LLMs) reflects their
potential for flexibility and generalization, and attracted the attention of a
diverse range of researchers. However, the advent of these techniques has also
brought to light the lack of transparency and rigor with which development is
pursued. In particular, the inability to determine the number of epochs and
other hyperparameters in advance presents challenges in identifying the best
model. To address this challenge, machine learning frameworks such as MLFlow
can automate the collection of this type of information. However, these tools
capture data using proprietary formats and pose little attention to lineage.
This paper proposes yProv4ML, a framework to capture provenance information
generated during machine learning processes in PROV-JSON format, with minimal
code modifications.

</details>


### [166] [Development and Comparative Evaluation of Three Artificial Intelligence Models (NLP, LLM, JEPA) for Predicting Triage in Emergency Departments: A 7-Month Retrospective Proof-of-Concept](https://arxiv.org/pdf/2507.01080)
*Edouard Lansiaux, Ramy Azzouz, Emmanuel Chazard, Amélie Vromant, Eric Wiel*

Main category: cs.LG

TL;DR: The study compares three AI models (NLP, LLM, JEPA) for triage prediction in EDs, finding the LLM model (URGENTIAPARSE) most accurate, outperforming nurse triage and other AI models.


<details>
  <summary>Details</summary>
Motivation: Triage errors in EDs persist due to patient influx and staff shortages, prompting exploration of AI integration for improved accuracy.

Method: Retrospective analysis of triage data using three AI models (NLP, LLM, JEPA) trained on demographic, complaint, and vital sign data, validated against the FRENCH scale.

Result: The LLM model (URGENTIAPARSE) achieved the highest accuracy (composite score: 2.514) and outperformed nurse triage and other AI models.

Conclusion: LLM-based AI models show promise for enhancing ED triage accuracy and efficiency, though ethical and integration challenges remain.

Abstract: Triage errors, including undertriage and overtriage, are persistent
challenges in emergency departments (EDs). With increasing patient influx and
staff shortages, the integration of artificial intelligence (AI) into triage
protocols has gained attention. This study compares the performance of three AI
models [Natural Language Processing (NLP), Large Language Models (LLM), and
Joint Embedding Predictive Architecture (JEPA)] in predicting triage outcomes
against the FRENCH scale and clinical practice.We conducted a retrospective
analysis of a prospectively recruited cohort gathering adult patient triage
data over a 7-month period at the Roger Salengro Hospital ED (Lille, France).
Three AI models were trained and validated : (1) TRIAGEMASTER (NLP), (2)
URGENTIAPARSE (LLM), and (3) EMERGINET (JEPA). Data included demographic
details, verbatim chief complaints, vital signs, and triage outcomes based on
the FRENCH scale and GEMSA coding. The primary outcome was the concordance of
AI-predicted triage level with the FRENCH gold-standard. It was assessed thanks
to various indicators : F1-Score, Weighted Kappa, Spearman, MAE, RMSE. The LLM
model (URGENTIAPARSE) showed higher accuracy (composite score: 2.514) compared
to JEPA (EMERGINET, 0.438) and NLP (TRIAGEMASTER, -3.511), outperforming nurse
triage (-4.343). Secondary analyses highlighted the effectiveness of
URGENTIAPARSE in predicting hospitalization needs (GEMSA) and its robustness
with structured data versus raw transcripts (either for GEMSA prediction or for
FRENCH prediction). LLM architecture, through abstraction of patient
representations, offers the most accurate triage predictions among tested
models. Integrating AI into ED workflows could enhance patient safety and
operational efficiency, though integration into clinical workflows requires
addressing model limitations and ensuring ethical transparency.

</details>


### [167] [$μ^2$Tokenizer: Differentiable Multi-Scale Multi-Modal Tokenizer for Radiology Report Generation](https://arxiv.org/pdf/2507.00316)
*Siyou Li, Pengyao Qin, Huanan Wu, Dong Nie, Arun J. Thirunavukarasu, Juntao Yu, Le Zhang*

Main category: cs.LG

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: Automated radiology report generation (RRG) aims to produce detailed textual
reports from clinical imaging, such as computed tomography (CT) scans, to
improve the accuracy and efficiency of diagnosis and provision of management
advice. RRG is complicated by two key challenges: (1) inherent complexity in
extracting relevant information from imaging data under resource constraints,
and (2) difficulty in objectively evaluating discrepancies between
model-generated and expert-written reports. To address these challenges, we
propose $\mu^2$LLM, a $\underline{\textbf{mu}}$ltiscale
$\underline{\textbf{mu}}$ltimodal large language models for RRG tasks. The
novel ${\mu}^2$Tokenizer, as an intermediate layer, integrates multi-modal
features from the multiscale visual tokenizer and the text tokenizer, then
enhances report generation quality through direct preference optimization
(DPO), guided by GREEN-RedLlama. Experimental results on four large CT
image-report medical datasets demonstrate that our method outperforms existing
approaches, highlighting the potential of our fine-tuned $\mu^2$LLMs on limited
data for RRG tasks. At the same time, for prompt engineering, we introduce a
five-stage, LLM-driven pipeline that converts routine CT reports into paired
visual-question-answer triples and citation-linked reasoning narratives,
creating a scalable, high-quality supervisory corpus for explainable multimodal
radiology LLM. All code, datasets, and models will be publicly available in our
official repository. https://github.com/Siyou-Li/u2Tokenizer

</details>


### [168] [Beyond First-Order: Training LLMs with Stochastic Conjugate Subgradients and AdamW](https://arxiv.org/pdf/2507.01241)
*Di Zhang, Yihang Zhang*

Main category: cs.LG

TL;DR: A stochastic conjugate subgradient method with adaptive sampling is proposed for training LLMs, outperforming traditional SGD in convergence and scalability.


<details>
  <summary>Details</summary>
Motivation: Traditional SGD methods show limitations in large-scale LLM training, prompting the need for a more efficient approach.

Method: Combines stochastic conjugate subgradient with adaptive sampling and AdamW-like step size adjustment to handle nonconvexity and non-smoothness.

Result: Achieves faster convergence and better scalability than SGD, improving speed and accuracy in optimization.

Conclusion: The proposed method effectively addresses SGD's limitations, enhancing LLM training performance.

Abstract: Stochastic gradient-based descent (SGD), have long been central to training
large language models (LLMs). However, their effectiveness is increasingly
being questioned, particularly in large-scale applications where empirical
evidence suggests potential performance limitations. In response, this paper
proposes a stochastic conjugate subgradient method together with adaptive
sampling tailored specifically for training LLMs. The method not only achieves
faster convergence per iteration but also demonstrates improved scalability
compared to traditional SGD techniques. It leverages sample complexity analysis
to adaptively choose the sample size, employs a stochastic conjugate
subgradient approach to determine search directions and utilizing an AdamW-like
algorithm to adaptively adjust step sizes. This approach preserves the key
advantages of first-order methods while effectively addressing the nonconvexity
and non-smoothness inherent in LLMs training. Additionally, we provide a
detailed analysis of the advantage of the algorithm. Experimental results show
that the proposed method not only maintains, but in many cases surpasses, the
scalability of traditional SGD techniques, significantly enhancing both the
speed and accuracy of the optimization process.

</details>


### [169] [Proof of a perfect platonic representation hypothesis](https://arxiv.org/pdf/2507.01098)
*Liu Ziyin, Isaac Chuang*

Main category: cs.LG

TL;DR: The paper explains Ziyin et al.'s proof of the 'perfect' Platonic Representation Hypothesis (PRH) for embedded deep linear networks (EDLN), showing SGD leads to identical layer representations up to rotation, despite most global minima not being Platonic. It also links PRH to progressive sharpening and identifies six ways PRH can fail.


<details>
  <summary>Details</summary>
Motivation: To clarify and detail Ziyin et al.'s proof of PRH in EDLNs, emphasizing the surprising role of SGD in achieving Platonic representations and exploring connections to other deep learning phenomena.

Method: The paper elaborates on the proof for PRH in EDLNs, demonstrating how SGD training results in identical layer representations (up to rotation) and linking this to progressive sharpening. It also identifies conditions under which PRH fails.

Result: SGD in EDLNs leads to perfectly Platonic representations despite most global minima not being Platonic. The proof reveals six failure modes for PRH and connects Platonic representations to progressive sharpening.

Conclusion: The work underscores the role of SGD's irreversibility and emergent 'entropic forces' in representation learning, linking seemingly unrelated phenomena and providing insights into deep learning dynamics.

Abstract: In this note, we elaborate on and explain in detail the proof given by Ziyin
et al. (2025) of the "perfect" Platonic Representation Hypothesis (PRH) for the
embedded deep linear network model (EDLN). We show that if trained with SGD,
two EDLNs with different widths and depths and trained on different data will
become Perfectly Platonic, meaning that every possible pair of layers will
learn the same representation up to a rotation. Because most of the global
minima of the loss function are not Platonic, that SGD only finds the perfectly
Platonic solution is rather extraordinary. The proof also suggests at least six
ways the PRH can be broken. We also show that in the EDLN model, the emergence
of the Platonic representations is due to the same reason as the emergence of
progressive sharpening. This implies that these two seemingly unrelated
phenomena in deep learning can, surprisingly, have a common cause. Overall, the
theory and proof highlight the importance of understanding emergent "entropic
forces" due to the irreversibility of SGD training and their role in
representation learning. The goal of this note is to be instructive and avoid
lengthy technical details.

</details>


### [170] [A Neural Operator based on Dynamic Mode Decomposition](https://arxiv.org/pdf/2507.01117)
*Nikita Sakovich, Dmitry Aksenov, Ekaterina Pleshakova, Sergey Gataullin*

Main category: cs.LG

TL;DR: A neural operator combining dynamic mode decomposition (DMD) and deep learning (DL) is proposed for efficient spatiotemporal modeling, reducing computational costs while maintaining accuracy.


<details>
  <summary>Details</summary>
Motivation: Balancing lightweight and accurate computations in scientific computing, especially for solving PDEs with varying conditions.

Method: Uses DMD and DL to extract key modes and dynamics, constructing predictions efficiently.

Result: Outperforms DeepONet and FNO in accuracy for heat, Laplace, and Burgers equations.

Conclusion: The method offers a computationally efficient and accurate alternative for PDE solutions.

Abstract: The scientific computation methods development in conjunction with artificial
intelligence technologies remains a hot research topic. Finding a balance
between lightweight and accurate computations is a solid foundation for this
direction. The study presents a neural operator based on the dynamic mode
decomposition algorithm (DMD), mapping functional spaces, which combines DMD
and deep learning (DL) for spatiotemporal processes efficient modeling. Solving
PDEs for various initial and boundary conditions requires significant
computational resources. The method suggested automatically extracts key modes
and system dynamics using them to construct predictions, reducing computational
costs compared to traditional numerical methods. The approach has demonstrated
its efficiency through comparative analysis of performance with closest
analogues DeepONet and FNO in the heat equation, Laplaces equation, and Burgers
equation solutions approximation, where it achieves high reconstruction
accuracy.

</details>


### [171] [PULSE: Practical Evaluation Scenarios for Large Multimodal Model Unlearning](https://arxiv.org/pdf/2507.01271)
*Tatsuki Kawakami, Kazuki Egashira, Atsuyuki Miyai, Go Irie, Kiyoharu Aizawa*

Main category: cs.LG

TL;DR: The paper introduces the PULSE protocol for evaluating unlearning in large multimodal models (LMMs), focusing on pre-trained knowledge unlearning and long-term sustainability, revealing limitations in existing methods.


<details>
  <summary>Details</summary>
Motivation: Addressing the lack of practical evaluation frameworks for unlearning in LMMs, especially for realistic scenarios involving pre-trained knowledge and sequential unlearning requests.

Method: Proposes the PULSE protocol, which evaluates unlearning across different knowledge acquisition phases (pre-training vs. fine-tuning) and sequential unlearning operations.

Result: Existing methods struggle with pre-trained knowledge unlearning and show performance degradation in sequential unlearning scenarios.

Conclusion: The PULSE protocol highlights gaps in current unlearning techniques for LMMs, emphasizing the need for more robust solutions.

Abstract: In recent years, unlearning techniques, which are methods for inducing a
model to "forget" previously learned information, have attracted attention as a
way to address privacy and copyright concerns in large language models (LLMs)
and large multimodal models (LMMs). While several unlearning benchmarks have
been established for LLMs, a practical evaluation framework for unlearning in
LMMs has been less explored. Specifically, existing unlearning benchmark for
LMMs considers only scenarios in which the model is required to unlearn
fine-tuned knowledge through a single unlearning operation. In this study, we
introduce PULSE protocol for realistic unlearning scenarios for LMMs by
introducing two critical perspectives: (i) Pre-trained knowledge Unlearning for
analyzing the effect across different knowledge acquisition phases and (ii)
Long-term Sustainability Evaluation to address sequential requests. We then
evaluate existing unlearning methods along these dimensions. Our results reveal
that, although some techniques can successfully unlearn knowledge acquired
through fine-tuning, they struggle to eliminate information learned during
pre-training. Moreover, methods that effectively unlearn a batch of target data
in a single operation exhibit substantial performance degradation when the same
data are split and unlearned sequentially.

</details>


### [172] [On Design Principles for Private Adaptive Optimizers](https://arxiv.org/pdf/2507.01129)
*Arun Ganesh, Brendan McMahan, Abhradeep Thakurta*

Main category: cs.LG

TL;DR: The paper critiques existing DP training methods for adaptive optimizers, proposing 'scale-then-privatize' as a superior alternative with better theoretical and empirical results.


<details>
  <summary>Details</summary>
Motivation: Existing DP training methods for adaptive optimizers like AdaGrad and Adam perform poorly due to spherical noise, and their conclusions may not generalize to practical scenarios.

Method: The paper surveys existing variants, develops theoretical insights, and empirically compares them, focusing on 'scale-then-privatize' as a key technique.

Result: 'Scale-then-privatize' outperforms other variants in small-scale language model training and aligns better with correlated noise mechanisms.

Conclusion: Unbiased second-moment estimates in adaptive optimizers are misguided; 'scale-then-privatize' is more effective and theoretically sound.

Abstract: The spherical noise added to gradients in differentially private (DP)
training undermines the performance of adaptive optimizers like AdaGrad and
Adam, and hence many recent works have proposed algorithms to address this
challenge. However, the empirical results in these works focus on simple tasks
and models and the conclusions may not generalize to model training in
practice. In this paper we survey several of these variants, and develop better
theoretical intuition for them as well as perform empirical studies comparing
them. We find that a common intuition of aiming for unbiased estimates of
second moments of gradients in adaptive optimizers is misguided, and instead
that a simple technique called scale-then-privatize (which does not achieve
unbiased second moments) has more desirable theoretical behaviors and
outperforms all other variants we study on a small-scale language model
training task. We additionally argue that scale-then-privatize causes the noise
addition to better match the application of correlated noise mechanisms which
are more desirable to use in practice.

</details>


### [173] [Tensor Decomposition Networks for Fast Machine Learning Interatomic Potential Computations](https://arxiv.org/pdf/2507.01131)
*Yuchao Lin, Cong Fu, Zachary Krueger, Haiyang Yu, Maho Nakata, Jianwen Xie, Emine Kucukbenli, Xiaofeng Qian, Shuiwang Ji*

Main category: cs.LG

TL;DR: TDNs replace CG tensor products with low-rank decompositions to accelerate SO(3)-equivariant networks, achieving competitive performance with reduced computational cost.


<details>
  <summary>Details</summary>
Motivation: The computational expense of Clebsch-Gordan (CG) tensor products in SO(3)-equivariant networks motivates the development of faster, approximately equivariant alternatives.

Method: Tensor decomposition networks (TDNs) use low-rank decompositions (e.g., CP) and path-weight sharing to reduce complexity from O(L^6) to O(L^4).

Result: TDNs achieve competitive performance on PubChemQCR, OC20, and OC22 datasets with significant computational speedup.

Conclusion: TDNs offer a plug-and-play, efficient alternative to CG tensor products in equivariant networks without compromising performance.

Abstract: $\rm{SO}(3)$-equivariant networks are the dominant models for machine
learning interatomic potentials (MLIPs). The key operation of such networks is
the Clebsch-Gordan (CG) tensor product, which is computationally expensive. To
accelerate the computation, we develop tensor decomposition networks (TDNs) as
a class of approximately equivariant networks whose CG tensor products are
replaced by low-rank tensor decompositions, such as the CANDECOMP/PARAFAC (CP)
decomposition. With the CP decomposition, we prove (i) a uniform bound on the
induced error of $\rm{SO}(3)$-equivariance, and (ii) the universality of
approximating any equivariant bilinear map. To further reduce the number of
parameters, we propose path-weight sharing that ties all multiplicity-space
weights across the $O(L^3)$ CG paths into a single path without compromising
equivariance, where $L$ is the maximum angular degree. The resulting layer acts
as a plug-and-play replacement for tensor products in existing networks, and
the computational complexity of tensor products is reduced from $O(L^6)$ to
$O(L^4)$. We evaluate TDNs on PubChemQCR, a newly curated molecular relaxation
dataset containing 105 million DFT-calculated snapshots. We also use existing
datasets, including OC20, and OC22. Results show that TDNs achieve competitive
performance with dramatic speedup in computations.

</details>


### [174] [Spectral Manifold Harmonization for Graph Imbalanced Regression](https://arxiv.org/pdf/2507.01132)
*Brenda Nogueira, Gabe Gomes, Meng Jiang, Nitesh V. Chawla, Nuno Moniz*

Main category: cs.LG

TL;DR: SMH addresses imbalanced regression on graph-structured data by generating synthetic samples preserving topology and focusing on underrepresented target ranges, improving predictive performance.


<details>
  <summary>Details</summary>
Motivation: The paper addresses the lack of research in imbalanced regression for graph-structured data, where domain preferences focus on scientifically valuable but underrepresented target ranges.

Method: Proposes Spectral Manifold Harmonization (SMH), which generates synthetic graph samples preserving topological properties while targeting underrepresented regions of the target distribution.

Result: SMH shows consistent improvements in predictive performance for target domain ranges on chemistry and drug discovery benchmarks.

Conclusion: SMH effectively addresses imbalanced regression challenges in graph-structured data, outperforming conventional methods by focusing on underrepresented target ranges and preserving topology.

Abstract: Graph-structured data is ubiquitous in scientific domains, where models often
face imbalanced learning settings. In imbalanced regression, domain preferences
focus on specific target value ranges representing the most scientifically
valuable cases; we observe a significant lack of research. In this paper, we
present Spectral Manifold Harmonization (SMH), a novel approach for addressing
this imbalanced regression challenge on graph-structured data by generating
synthetic graph samples that preserve topological properties while focusing on
often underrepresented target distribution regions. Conventional methods fail
in this context because they either ignore graph topology in case generation or
do not target specific domain ranges, resulting in models biased toward average
target values. Experimental results demonstrate the potential of SMH on
chemistry and drug discovery benchmark datasets, showing consistent
improvements in predictive performance for target domain ranges.

</details>


### [175] [Neural Hamiltonian Operator](https://arxiv.org/pdf/2507.01313)
*Qian Qi*

Main category: cs.LG

TL;DR: The paper introduces a Neural Hamiltonian Operator (NHO) to solve high-dimensional stochastic control problems using deep learning, framing it as an operator learning task within Pontryagin's Maximum Principle (PMP).


<details>
  <summary>Details</summary>
Motivation: High-dimensional stochastic control problems are challenging due to the curse of dimensionality, and traditional methods like dynamic programming are inefficient. PMP offers an alternative but requires solving complex FBSDEs.

Method: The NHO parameterizes FBSDE dynamics via neural networks for feedback control and value function gradients. Training enforces PMP consistency conditions.

Result: The NHO framework proves universal approximation capabilities under general martingale drivers and addresses optimization challenges.

Conclusion: The NHO provides a rigorous, operator-theoretic approach to solving high-dimensional stochastic control problems with deep learning.

Abstract: Stochastic control problems in high dimensions are notoriously difficult to
solve due to the curse of dimensionality. An alternative to traditional dynamic
programming is Pontryagin's Maximum Principle (PMP), which recasts the problem
as a system of Forward-Backward Stochastic Differential Equations (FBSDEs). In
this paper, we introduce a formal framework for solving such problems with deep
learning by defining a \textbf{Neural Hamiltonian Operator (NHO)}. This
operator parameterizes the coupled FBSDE dynamics via neural networks that
represent the feedback control and an ansatz for the value function's spatial
gradient. We show how the optimal NHO can be found by training the underlying
networks to enforce the consistency conditions dictated by the PMP. By adopting
this operator-theoretic view, we situate the deep FBSDE method within the
rigorous language of statistical inference, framing it as a problem of learning
an unknown operator from simulated data. This perspective allows us to prove
the universal approximation capabilities of NHOs under general martingale
drivers and provides a clear lens for analyzing the significant optimization
challenges inherent to this class of models.

</details>


### [176] [FlashDP: Private Training Large Language Models with Efficient DP-SGD](https://arxiv.org/pdf/2507.01154)
*Liangyu Wang, Junxiao Wang, Jie Ren, Zihang Xiang, David E. Keyes, Di Wang*

Main category: cs.LG

TL;DR: FlashDP introduces a cache-friendly per-layer DP-SGD method, reducing memory and computation overhead while maintaining accuracy in LLM training.


<details>
  <summary>Details</summary>
Motivation: Privacy concerns in LLM training data necessitate efficient DP methods, but current approaches like DP-SGD face memory and computation inefficiencies.

Method: FlashDP consolidates operations into a single task, calculating gradients once in a fused manner, reducing memory movement and redundant computations.

Result: FlashDP reduces memory movement by 50% and redundant computations by 20%, achieving 90% throughput of Non-DP methods while maintaining accuracy.

Conclusion: FlashDP is a significant advancement for efficient, privacy-preserving LLM training, with open-sourced code available.

Abstract: As large language models (LLMs) increasingly underpin technological
advancements, the privacy of their training data emerges as a critical concern.
Differential Privacy (DP) serves as a rigorous mechanism to protect this data,
yet its integration via Differentially Private Stochastic Gradient Descent
(DP-SGD) introduces substantial challenges, primarily due to the complexities
of per-sample gradient clipping. Current explicit methods, such as Opacus,
necessitate extensive storage for per-sample gradients, significantly inflating
memory requirements. Conversely, implicit methods like GhostClip reduce storage
needs by recalculating gradients multiple times, which leads to inefficiencies
due to redundant computations. This paper introduces FlashDP, an innovative
cache-friendly per-layer DP-SGD that consolidates necessary operations into a
single task, calculating gradients only once in a fused manner. This approach
not only diminishes memory movement by up to \textbf{50\%} but also cuts down
redundant computations by \textbf{20\%}, compared to previous methods.
Consequently, FlashDP does not increase memory demands and achieves a
\textbf{90\%} throughput compared to the Non-DP method on a four-A100 system
during the pre-training of the Llama-13B model, while maintaining parity with
standard per-layer clipped DP-SGD in terms of accuracy. These advancements
establish FlashDP as a pivotal development for efficient and privacy-preserving
training of LLMs. FlashDP's code has been open-sourced in
https://github.com/kaustpradalab/flashdp.

</details>


### [177] [ICLShield: Exploring and Mitigating In-Context Learning Backdoor Attacks](https://arxiv.org/pdf/2507.01321)
*Zhiyao Ren, Siyuan Liang, Aishan Liu, Dacheng Tao*

Main category: cs.LG

TL;DR: The paper introduces the dual-learning hypothesis for LLMs, revealing their vulnerability to backdoor attacks in ICL, and proposes ICLShield, a defense mechanism that dynamically adjusts concept preference to mitigate attacks.


<details>
  <summary>Details</summary>
Motivation: The adaptability and parameter-free nature of ICL in LLMs make them vulnerable to backdoor attacks, necessitating a robust defense mechanism.

Method: The authors propose the dual-learning hypothesis and ICLShield, which dynamically adjusts the concept preference ratio using confidence and similarity scores to select clean demonstrations.

Result: ICLShield outperforms existing methods by 26.02% on average and shows strong adaptability, even for closed-source models like GPT-4.

Conclusion: The study highlights the vulnerability of ICL to backdoor attacks and successfully mitigates it with ICLShield, demonstrating superior defense performance.

Abstract: In-context learning (ICL) has demonstrated remarkable success in large
language models (LLMs) due to its adaptability and parameter-free nature.
However, it also introduces a critical vulnerability to backdoor attacks, where
adversaries can manipulate LLM behaviors by simply poisoning a few ICL
demonstrations. In this paper, we propose, for the first time, the
dual-learning hypothesis, which posits that LLMs simultaneously learn both the
task-relevant latent concepts and backdoor latent concepts within poisoned
demonstrations, jointly influencing the probability of model outputs. Through
theoretical analysis, we derive an upper bound for ICL backdoor effects,
revealing that the vulnerability is dominated by the concept preference ratio
between the task and the backdoor. Motivated by these findings, we propose
ICLShield, a defense mechanism that dynamically adjusts the concept preference
ratio. Our method encourages LLMs to select clean demonstrations during the ICL
phase by leveraging confidence and similarity scores, effectively mitigating
susceptibility to backdoor attacks. Extensive experiments across multiple LLMs
and tasks demonstrate that our method achieves state-of-the-art defense
effectiveness, significantly outperforming existing approaches (+26.02% on
average). Furthermore, our method exhibits exceptional adaptability and
defensive performance even for closed-source models (e.g., GPT-4).

</details>


### [178] [Diffusion Explorer: Interactive Exploration of Diffusion Models](https://arxiv.org/pdf/2507.01178)
*Alec Helbling, Duen Horng Chau*

Main category: cs.LG

TL;DR: Diffusion Explorer is an interactive tool designed to explain the geometric properties of diffusion models through 2D training and real-time observation of sampling dynamics.


<details>
  <summary>Details</summary>
Motivation: Existing explanations of diffusion models are either too theoretical or overly focused on neural architectures, neglecting their rich geometric properties.

Method: The tool allows users to train 2D diffusion models in a browser and observe their sampling process via interactive animations.

Result: Diffusion Explorer provides an accessible way to visualize and understand the temporal dynamics of diffusion models.

Conclusion: The open-source tool, with its live demo, effectively bridges the gap in explaining diffusion models' geometric properties.

Abstract: Diffusion models have been central to the development of recent image, video,
and even text generation systems. They posses striking geometric properties
that can be faithfully portrayed in low-dimensional settings. However, existing
resources for explaining diffusion either require an advanced theoretical
foundation or focus on their neural network architectures rather than their
rich geometric properties. We introduce Diffusion Explorer, an interactive tool
to explain the geometric properties of diffusion models. Users can train 2D
diffusion models in the browser and observe the temporal dynamics of their
sampling process. Diffusion Explorer leverages interactive animation, which has
been shown to be a powerful tool for making engaging visualizations of dynamic
systems, making it well suited to explaining diffusion models which represent
stochastic processes that evolve over time. Diffusion Explorer is open source
and a live demo is available at alechelbling.com/Diffusion-Explorer.

</details>


### [179] [Reasoner for Real-World Event Detection: Scaling Reinforcement Learning via Adaptive Perplexity-Aware Sampling Strategy](https://arxiv.org/pdf/2507.01327)
*Xiaoyun Zhang, Jingqing Ruan, Xing Ma, Yawen Zhu, Jiansong Chen, Ke Zeng, Xunliang Cai*

Main category: cs.LG

TL;DR: The paper introduces APARL, a framework for detecting abnormal events in customer service dialogues, improving adaptability and OOD generalization.


<details>
  <summary>Details</summary>
Motivation: The complexity of business data and dynamic customer interactions make abnormal event detection challenging, requiring strong OOD generalization for commercial value.

Method: APARL uses a dual-loop dynamic curriculum learning architecture with large language models to progressively tackle harder samples.

Result: APARL achieves a 17.19% F1 score improvement and 9.59% better OOD transferability in food delivery dialogue tasks.

Conclusion: APARL offers a superior solution for industrial anomaly detection, enhancing operational efficiency and commercial benefits.

Abstract: Detecting abnormal events in real-world customer service dialogues is highly
challenging due to the complexity of business data and the dynamic nature of
customer interactions. Moreover, models must demonstrate strong out-of-domain
(OOD) generalization to enable rapid adaptation across different business
scenarios and maximize commercial value. In this work, we propose a novel
Adaptive Perplexity-Aware Reinforcement Learning (APARL) framework that
leverages the advanced reasoning capabilities of large language models for
abnormal event detection. APARL introduces a dual-loop dynamic curriculum
learning architecture, enabling the model to progressively focus on more
challenging samples as its proficiency increases. This design effectively
addresses performance bottlenecks and significantly enhances OOD
transferability. Extensive evaluations on food delivery dialogue tasks show
that our model achieves significantly enhanced adaptability and robustness,
attaining the highest F1 score with an average improvement of 17.19\%, and an
average improvement of 9.59\% in OOD transfer tests. This method provides a
superior solution for industrial deployment of anomaly detection models,
contributing to improved operational efficiency and commercial benefits.

</details>


### [180] [Escaping Platos Cave: JAM for Aligning Independently Trained Vision and Language Models](https://arxiv.org/pdf/2507.01201)
*Hyoseo, Yoon, Yisong Yue, Been Kim*

Main category: cs.LG

TL;DR: The paper introduces the Joint Autoencoder Modulator (JAM) framework to align disjoint vision and language representations, optimizing for mutual coherence while preserving modality-specific structures.


<details>
  <summary>Details</summary>
Motivation: The Platonic Representation Hypothesis suggests that independently trained vision and language models may converge toward a shared statistical model of reality, prompting the need for explicit alignment methods.

Method: The JAM framework jointly trains modality-specific autoencoders on latent representations of pre-trained models, using reconstruction and cross-modal objectives. Alignment is evaluated across objectives (contrastive loss variants, Spread loss), layer depth, and foundation model scale.

Result: The lightweight Pareto-efficient JAM framework reliably induces alignment, even across frozen, independently trained representations.

Conclusion: The work provides theoretical insight and practical methods for transforming unimodal foundations into specialist multimodal models.

Abstract: Independently trained vision and language models inhabit disjoint
representational spaces, shaped by their respective modalities, objectives, and
architectures. Yet an emerging hypothesis - the Platonic Representation
Hypothesis - suggests that such models may nonetheless converge toward a shared
statistical model of reality. This compatibility, if it exists, raises a
fundamental question: can we move beyond post-hoc statistical detection of
alignment and explicitly optimize for it between such disjoint representations?
We cast this Platonic alignment problem as a multi-objective optimization task
- preserve each modality's native structure while aligning for mutual
coherence. We introduce the Joint Autoencoder Modulator (JAM) framework that
jointly trains modality-specific autoencoders on the latent representations of
pre-trained single modality models, encouraging alignment through both
reconstruction and cross-modal objectives. By analogy, this framework serves as
a method to escape Plato's Cave, enabling the emergence of shared structure
from disjoint inputs. We evaluate this framework across three critical design
axes: (i) the alignment objective - comparing contrastive loss (Con), its
hard-negative variant (NegCon), and our Spread loss, (ii) the layer depth at
which alignment is most effective, and (iii) the impact of foundation model
scale on representational convergence. Our findings show that our lightweight
Pareto-efficient framework reliably induces alignment, even across frozen,
independently trained representations, offering both theoretical insight and
practical pathways for transforming generalist unimodal foundations into
specialist multimodal models.

</details>


### [181] [Distributional Soft Actor-Critic with Diffusion Policy](https://arxiv.org/pdf/2507.01381)
*Tong Liu, Yinuo Wang, Xujie Song, Wenjun Zou, Liangfa Chen, Likun Wang, Bin Shuai, Jingliang Duan, Shengbo Eben Li*

Main category: cs.LG

TL;DR: DSAC-D, a distributional reinforcement learning algorithm, uses multimodal distributions to reduce bias in value function estimation and improve performance, achieving SOTA results in control tasks.


<details>
  <summary>Details</summary>
Motivation: Traditional unimodal distributions in reinforcement learning cause bias in value function estimation, leading to poor performance.

Method: Proposes DSAC-D with a multimodal distributional policy iteration framework, using diffusion models for accurate multi-peak distribution characterization.

Result: Achieves SOTA performance in 9 control tasks, reduces estimation bias, and improves average return by over 10%. Real-world tests show accurate multimodal driving style representation.

Conclusion: DSAC-D effectively addresses bias in value estimation and enables multimodal policy learning, outperforming existing methods.

Abstract: Reinforcement learning has been proven to be highly effective in handling
complex control tasks. Traditional methods typically use unimodal
distributions, such as Gaussian distributions, to model the output of value
distributions. However, unimodal distribution often and easily causes bias in
value function estimation, leading to poor algorithm performance. This paper
proposes a distributional reinforcement learning algorithm called DSAC-D
(Distributed Soft Actor Critic with Diffusion Policy) to address the challenges
of estimating bias in value functions and obtaining multimodal policy
representations. A multimodal distributional policy iteration framework that
can converge to the optimal policy was established by introducing policy
entropy and value distribution function. A diffusion value network that can
accurately characterize the distribution of multi peaks was constructed by
generating a set of reward samples through reverse sampling using a diffusion
model. Based on this, a distributional reinforcement learning algorithm with
dual diffusion of the value network and the policy network was derived. MuJoCo
testing tasks demonstrate that the proposed algorithm not only learns
multimodal policy, but also achieves state-of-the-art (SOTA) performance in all
9 control tasks, with significant suppression of estimation bias and total
average return improvement of over 10\% compared to existing mainstream
algorithms. The results of real vehicle testing show that DSAC-D can accurately
characterize the multimodal distribution of different driving styles, and the
diffusion policy network can characterize multimodal trajectories.

</details>


### [182] [Deep Learning-Based Intrusion Detection for Automotive Ethernet: Evaluating & Optimizing Fast Inference Techniques for Deployment on Low-Cost Platform](https://arxiv.org/pdf/2507.01208)
*Pedro R. X. Carmo, Igor de Moura, Assis T. de Oliveira Filho, Djamel Sadok, Cleber Zanchettin*

Main category: cs.LG

TL;DR: The paper proposes using fast neural network techniques (Distilling and Pruning) to deploy Intrusion Detection Systems (IDS) on low-cost hardware like Raspberry Pi 4, achieving real-time performance with high accuracy.


<details>
  <summary>Details</summary>
Motivation: Modern vehicles rely on automotive Ethernet, which is vulnerable to attacks like flow injection. Existing Deep Learning-based IDS require expensive hardware for real-time operation, limiting practicality.

Method: The study evaluates and applies Distilling and Pruning techniques to optimize IDS models for deployment on low-cost platforms (e.g., Raspberry Pi 4).

Result: The optimized models achieve intrusion detection times of 727 µs on Raspberry Pi 4, with an AUCROC of 0.9890.

Conclusion: Fast neural network techniques enable efficient, real-time IDS deployment on affordable hardware, enhancing vehicle security.

Abstract: Modern vehicles are increasingly connected, and in this context, automotive
Ethernet is one of the technologies that promise to provide the necessary
infrastructure for intra-vehicle communication. However, these systems are
subject to attacks that can compromise safety, including flow injection
attacks. Deep Learning-based Intrusion Detection Systems (IDS) are often
designed to combat this problem, but they require expensive hardware to run in
real time. In this work, we propose to evaluate and apply fast neural network
inference techniques like Distilling and Prunning for deploying IDS models on
low-cost platforms in real time. The results show that these techniques can
achieve intrusion detection times of up to 727 {\mu}s using a Raspberry Pi 4,
with AUCROC values of 0.9890.

</details>


### [183] [PAE MobiLLM: Privacy-Aware and Efficient LLM Fine-Tuning on the Mobile Device via Additive Side-Tuning](https://arxiv.org/pdf/2507.01216)
*Xingke Yang, Liang Li, Zhiyi Wan, Sicong Li, Hao Wang, Xiaoqi Qi, Jiang Liu, Tomoaki Ohtsuki, Xin Fu, Miao Pan*

Main category: cs.LG

TL;DR: PAE MobiLLM is a privacy-aware, efficient method for fine-tuning large language models on mobile devices using server-assisted side-tuning, activation caching, and one-token activation shortcuts to reduce communication and computational costs.


<details>
  <summary>Details</summary>
Motivation: Addressing the gap between mobile device resource limitations and the need for on-device LLM fine-tuning, while ensuring privacy and efficiency.

Method: Uses server-assisted additive side-tuning, activation caching, one-token activation shortcuts, and additive adapter side-network design.

Result: Reduces communication burden, accelerates fine-tuning convergence, and ensures privacy by preventing server access to raw data or labels.

Conclusion: PAE MobiLLM effectively balances privacy, efficiency, and resource constraints for mobile LLM fine-tuning.

Abstract: There is a huge gap between numerous intriguing applications fostered by
on-device large language model (LLM) fine-tuning (FT) from fresh mobile data
and the limited resources of a mobile device. While existing server-assisted
methods (e.g., split learning or side-tuning) may enable LLM FT on the local
mobile device, they suffer from heavy communication burdens of activation
transmissions, and may disclose data, labels or fine-tuned models to the
server. To address those issues, we develop PAE MobiLLM, a privacy-aware and
efficient LLM FT method which can be deployed on the mobile device via
server-assisted additive side-tuning. To further accelerate FT convergence and
improve computing efficiency, PAE MobiLLM integrates activation caching on the
server side, which allows the server to reuse historical activations and saves
the mobile device from repeatedly computing forward passes for the recurring
data samples. Besides, to reduce communication cost, PAE MobiLLM develops a
one-token (i.e., ``pivot'' token) activation shortcut that transmits only a
single activation dimension instead of full activation matrices to guide the
side network tuning. Last but not least, PAE MobiLLM introduces the additive
adapter side-network design which makes the server train the adapter modules
based on device-defined prediction differences rather than raw ground-truth
labels. In this way, the server can only assist device-defined side-network
computing, and learn nothing about data, labels or fine-tuned models.

</details>


### [184] [Quantum Machine Learning in Transportation: A Case Study of Pedestrian Stress Modelling](https://arxiv.org/pdf/2507.01235)
*Bara Rababa, Bilal Farooq*

Main category: cs.LG

TL;DR: Quantum machine learning models (QSVM and QNN) were tested for classifying pedestrian stress from SCR data. QNN outperformed QSVM and classical methods.


<details>
  <summary>Details</summary>
Motivation: To leverage quantum computing for complex machine learning tasks, specifically modeling pedestrian stress in intelligent transportation systems.

Method: Developed QSVM and QNN models using an eight-qubit ZZ feature map on Pennylane, tested on SCR data with amplitude-based classes.

Result: QSVM had 45% test accuracy (overfitting), while QNN achieved 55%, outperforming classical methods.

Conclusion: QNN is more reliable for classifying pedestrian stress, demonstrating quantum computing's potential in machine learning.

Abstract: Quantum computing has opened new opportunities to tackle complex machine
learning tasks, for instance, high-dimensional data representations commonly
required in intelligent transportation systems. We explore quantum machine
learning to model complex skin conductance response (SCR) events that reflect
pedestrian stress in a virtual reality road crossing experiment. For this
purpose, Quantum Support Vector Machine (QSVM) with an eight-qubit ZZ feature
map and a Quantum Neural Network (QNN) using a Tree Tensor Network ansatz and
an eight-qubit ZZ feature map, were developed on Pennylane. The dataset
consists of SCR measurements along with features such as the response amplitude
and elapsed time, which have been categorized into amplitude-based classes. The
QSVM achieved good training accuracy, but had an overfitting problem, showing a
low test accuracy of 45% and therefore impacting the reliability of the
classification model. The QNN model reached a higher test accuracy of 55%,
making it a better classification model than the QSVM and the classic versions.

</details>


### [185] [Far From Sight, Far From Mind: Inverse Distance Weighting for Graph Federated Recommendation](https://arxiv.org/pdf/2507.01285)
*Aymen Rayane Khouas, Mohamed Reda Bouadjenek, Hakim Hacid, Sunil Aryal*

Main category: cs.LG

TL;DR: Dist-FedAvg is a distance-based aggregation method for graph federated recommendation systems, improving personalization and efficiency by weighting similar user embeddings and preserving anchor user influence.


<details>
  <summary>Details</summary>
Motivation: Traditional aggregation methods in federated learning overlook user embedding complexity and user similarity, limiting recommendation effectiveness and adaptability to evolving interactions.

Method: Introduces Dist-FedAvg, which assigns higher weights to similar user embeddings and ensures anchor users retain influence in local updates.

Result: Empirical evaluations show Dist-FedAvg outperforms baselines, enhancing recommendation accuracy while integrating seamlessly into federated frameworks.

Conclusion: Dist-FedAvg addresses limitations of existing methods, offering improved personalization and efficiency in graph federated recommendation systems.

Abstract: Graph federated recommendation systems offer a privacy-preserving alternative
to traditional centralized recommendation architectures, which often raise
concerns about data security. While federated learning enables personalized
recommendations without exposing raw user data, existing aggregation methods
overlook the unique properties of user embeddings in this setting. Indeed,
traditional aggregation methods fail to account for their complexity and the
critical role of user similarity in recommendation effectiveness. Moreover,
evolving user interactions require adaptive aggregation while preserving the
influence of high-relevance anchor users (the primary users before expansion in
graph-based frameworks). To address these limitations, we introduce
Dist-FedAvg, a novel distance-based aggregation method designed to enhance
personalization and aggregation efficiency in graph federated learning. Our
method assigns higher aggregation weights to users with similar embeddings,
while ensuring that anchor users retain significant influence in local updates.
Empirical evaluations on multiple datasets demonstrate that Dist-FedAvg
consistently outperforms baseline aggregation techniques, improving
recommendation accuracy while maintaining seamless integration into existing
federated learning frameworks.

</details>


### [186] [Efficient Kilometer-Scale Precipitation Downscaling with Conditional Wavelet Diffusion](https://arxiv.org/pdf/2507.01354)
*Chugang Yi, Minghan Yu, Weikang Qian, Yixin Wen, Haizhao Yang*

Main category: cs.LG

TL;DR: The paper introduces the Wavelet Diffusion Model (WDM) for downscaling precipitation data from 10 km to 1 km resolution, offering improved accuracy and speed over existing methods.


<details>
  <summary>Details</summary>
Motivation: Standard global precipitation data (e.g., IMERG) lacks fine-scale resolution needed for hydrological modeling and extreme weather analysis.

Method: WDM is a conditional diffusion model that operates in the wavelet domain, focusing on high-frequency coefficients to generate detailed 1-km precipitation fields.

Result: WDM achieves 10x spatial super-resolution, 9x faster inference than pixel-based models, and produces realistic results with fewer artifacts.

Conclusion: WDM addresses accuracy and speed challenges in geoscience super-resolution, enhancing hydrological forecasting reliability.

Abstract: Effective hydrological modeling and extreme weather analysis demand
precipitation data at a kilometer-scale resolution, which is significantly
finer than the 10 km scale offered by standard global products like IMERG. To
address this, we propose the Wavelet Diffusion Model (WDM), a generative
framework that achieves 10x spatial super-resolution (downscaling to 1 km) and
delivers a 9x inference speedup over pixel-based diffusion models. WDM is a
conditional diffusion model that learns the learns the complex structure of
precipitation from MRMS radar data directly in the wavelet domain. By focusing
on high-frequency wavelet coefficients, it generates exceptionally realistic
and detailed 1-km precipitation fields. This wavelet-based approach produces
visually superior results with fewer artifacts than pixel-space models, and
delivers a significant gains in sampling efficiency. Our results demonstrate
that WDM provides a robust solution to the dual challenges of accuracy and
speed in geoscience super-resolution, paving the way for more reliable
hydrological forecasts.

</details>


### [187] [Self-Guided Process Reward Optimization with Masked Step Advantage for Process Reinforcement Learning](https://arxiv.org/pdf/2507.01551)
*Wu Fei, Hao Kong, Shuxian Liang, Yang Lin, Yibo Yang, Jing Tang, Lei Chen, Xiansheng Hua*

Main category: cs.LG

TL;DR: SPRO is a novel framework for process-aware RL in LLMs, eliminating the need for external reward models by deriving rewards intrinsically and introducing step-wise advantage estimation. It improves efficiency and accuracy while reducing computational overhead.


<details>
  <summary>Details</summary>
Motivation: Addressing the computational overhead and lack of theoretical framework for process-level advantage estimation in PRL for LLMs.

Method: Proposes SPRO with intrinsic process rewards, cumulative process rewards, and Masked Step Advantage (MSA) for step-wise action advantage estimation.

Result: SPRO outperforms GRPO with 3.4x higher training efficiency, 17.5% test accuracy improvement, stable policy entropy, and reduced response length.

Conclusion: SPRO offers a computationally efficient and theoretically grounded solution for process-aware RL in LLMs, suitable for industrial implementation.

Abstract: Process Reinforcement Learning~(PRL) has demonstrated considerable potential
in enhancing the reasoning capabilities of Large Language Models~(LLMs).
However, introducing additional process reward models incurs substantial
computational overhead, and there is no unified theoretical framework for
process-level advantage estimation. To bridge this gap, we propose
\textbf{S}elf-Guided \textbf{P}rocess \textbf{R}eward
\textbf{O}ptimization~(\textbf{SPRO}), a novel framework that enables
process-aware RL through two key innovations: (1) we first theoretically
demonstrate that process rewards can be derived intrinsically from the policy
model itself, and (2) we introduce well-defined cumulative process rewards and
\textbf{M}asked \textbf{S}tep \textbf{A}dvantage (\textbf{MSA}), which
facilitates rigorous step-wise action advantage estimation within shared-prompt
sampling groups. Our experimental results demonstrate that SPRO outperforms
vaniila GRPO with 3.4x higher training efficiency and a 17.5\% test accuracy
improvement. Furthermore, SPRO maintains a stable and elevated policy entropy
throughout training while reducing the average response length by approximately
$1/3$, evidencing sufficient exploration and prevention of reward hacking.
Notably, SPRO incurs no additional computational overhead compared to
outcome-supervised RL methods such as GRPO, which benefit industrial
implementation.

</details>


### [188] [Surrogate Modeling via Factorization Machine and Ising Model with Enhanced Higher-Order Interaction Learning](https://arxiv.org/pdf/2507.01389)
*Anbang Wang, Dunbo Cai, Yu Zhang, Yangqing Huang, Xiangyang Feng, Zhihong Zhang*

Main category: cs.LG

TL;DR: An enhanced surrogate model integrates slack variables into a factorization machine and its Ising representation, unifying a two-step process into one, improving performance in predicting drug combination effects.


<details>
  <summary>Details</summary>
Motivation: To improve the efficiency and accuracy of surrogate models by incorporating slack variables for higher-order feature interactions, inspired by a prior quantum annealing-based approach.

Method: The model adds slack variables to the factorization machine and its Ising representation, iteratively updating them during training to capture higher-order interactions.

Result: Experimental results show notable performance improvement in predicting drug combination effects.

Conclusion: The proposed algorithm is promising for efficient surrogate models, leveraging quantum advantages.

Abstract: Recently, a surrogate model was proposed that employs a factorization machine
to approximate the underlying input-output mapping of the original system, with
quantum annealing used to optimize the resulting surrogate function. Inspired
by this approach, we propose an enhanced surrogate model that incorporates
additional slack variables into both the factorization machine and its
associated Ising representation thereby unifying what was by design a two-step
process into a single, integrated step. During the training phase, the slack
variables are iteratively updated, enabling the model to account for
higher-order feature interactions. We apply the proposed method to the task of
predicting drug combination effects. Experimental results indicate that the
introduction of slack variables leads to a notable improvement of performance.
Our algorithm offers a promising approach for building efficient surrogate
models that exploit potential quantum advantages.

</details>


### [189] [Tensor Program Optimization for the RISC-V Vector Extension Using Probabilistic Programs](https://arxiv.org/pdf/2507.01457)
*Federico Nicolas Peccia, Frederik Haxel, Oliver Bringmann*

Main category: cs.LG

TL;DR: The paper introduces a TVM compiler-based workflow to optimize AI workloads for RISC-V vector units, outperforming GCC autovectorization and muRISCV-NN in latency and code size.


<details>
  <summary>Details</summary>
Motivation: Efficiently utilizing RISC-V Vector Extension (RVV) for AI workloads without expert knowledge is challenging due to lack of autotuning frameworks integrated with RVV.

Method: Integrated RVV into TVM's MetaSchedule framework, implemented RISC-V SoCs on FPGA, and tuned AI workloads.

Result: Achieved 46% latency improvement over GCC autovectorization, 29% over muRISCV-NN, and 35% over LLVM on a commercial RISC-V SoC, with smaller code footprint.

Conclusion: The proposed solution effectively optimizes AI workloads for RISC-V RVV, offering significant performance gains and suitability for embedded devices, and is open-sourced for community expansion.

Abstract: RISC-V provides a flexible and scalable platform for applications ranging
from embedded devices to high-performance computing clusters. Particularly, its
RISC-V Vector Extension (RVV) becomes of interest for the acceleration of AI
workloads. But writing software that efficiently utilizes the vector units of
RISC-V CPUs without expert knowledge requires the programmer to rely on the
autovectorization features of compilers or hand-crafted libraries like
muRISCV-NN. Smarter approaches, like autotuning frameworks, have been missing
the integration with the RISC-V RVV extension, thus heavily limiting the
efficient deployment of complex AI workloads. In this paper, we present a
workflow based on the TVM compiler to efficiently map AI workloads onto RISC-V
vector units. Instead of relying on hand-crafted libraries, we integrated the
RVV extension into TVM's MetaSchedule framework, a probabilistic program
framework for tensor operation tuning. We implemented different RISC-V SoCs on
an FPGA and tuned a wide range of AI workloads on them. We found that our
proposal shows a mean improvement of 46% in execution latency when compared
against the autovectorization feature of GCC, and 29% against muRISCV-NN.
Moreover, the binary resulting from our proposal has a smaller code memory
footprint, making it more suitable for embedded devices. Finally, we also
evaluated our solution on a commercially available RISC-V SoC implementing the
RVV 1.0 Vector Extension and found our solution is able to find mappings that
are 35% faster on average than the ones proposed by LLVM. We open-sourced our
proposal for the community to expand it to target other RISC-V extensions.

</details>


### [190] [Decomposing Prediction Mechanisms for In-Context Recall](https://arxiv.org/pdf/2507.01414)
*Sultan Daniels, Dylan Davis, Dhruv Gautam, Wentinn Liao, Gireeja Ranade, Anant Sahai*

Main category: cs.LG

TL;DR: The paper introduces a toy problem combining linear-regression-style ICL with discrete associative recall, analyzing transformer models' ability to recall and predict states. Two distinct mechanisms emerge: one for associative recall and another for Bayesian-style prediction, with different learning dynamics.


<details>
  <summary>Details</summary>
Motivation: To understand how transformer models perform associative recall and prediction in a controlled setting, revealing distinct mechanisms and their learning dynamics.

Method: Pretrain transformer models on symbolic traces from linear dynamical systems, analyze recall and prediction capabilities, and conduct out-of-distribution and mechanistic analyses.

Result: Two separate mechanisms emerge: one for associative recall using symbolic labels, and another for Bayesian-style prediction. The latter develops earlier.

Conclusion: The findings suggest multi-mechanism learning dynamics, confirmed by similar observations in a translation task, indicating broader applicability.

Abstract: We introduce a new family of toy problems that combine features of
linear-regression-style continuous in-context learning (ICL) with discrete
associative recall. We pretrain transformer models on sample traces from this
toy, specifically symbolically-labeled interleaved state observations from
randomly drawn linear deterministic dynamical systems. We study if the
transformer models can recall the state of a sequence previously seen in its
context when prompted to do so with the corresponding in-context label. Taking
a closer look at this task, it becomes clear that the model must perform two
functions: (1) identify which system's state should be recalled and apply that
system to its last seen state, and (2) continuing to apply the correct system
to predict the subsequent states. Training dynamics reveal that the first
capability emerges well into a model's training. Surprisingly, the second
capability, of continuing the prediction of a resumed sequence, develops much
earlier.
  Via out-of-distribution experiments, and a mechanistic analysis on model
weights via edge pruning, we find that next-token prediction for this toy
problem involves at least two separate mechanisms. One mechanism uses the
discrete symbolic labels to do the associative recall required to predict the
start of a resumption of a previously seen sequence. The second mechanism,
which is largely agnostic to the discrete symbolic labels, performs a
"Bayesian-style" prediction based on the previous token and the context. These
two mechanisms have different learning dynamics.
  To confirm that this multi-mechanism (manifesting as separate phase
transitions) phenomenon is not just an artifact of our toy setting, we used
OLMo training checkpoints on an ICL translation task to see a similar
phenomenon: a decisive gap in the emergence of first-task-token performance vs
second-task-token performance.

</details>


### [191] [Blending Supervised and Reinforcement Fine-Tuning with Prefix Sampling](https://arxiv.org/pdf/2507.01679)
*Zeyu Huang, Tianhao Cheng, Zihan Qiu, Zili Wang, Yinghui Xu, Edoardo M. Ponti, Ivan Titov*

Main category: cs.LG

TL;DR: Prefix-RFT combines SFT and RFT, outperforming both and offering a unified, robust approach for LLM post-training.


<details>
  <summary>Details</summary>
Motivation: Address the trade-offs of SFT (generalization issues) and RFT (unexpected behaviors, sensitivity to initial policy) by unifying them.

Method: Introduces Prefix-RFT, a hybrid approach integrating demonstration (SFT) and exploration (RFT), tested on mathematical reasoning.

Result: Prefix-RFT outperforms standalone SFT and RFT, is robust to data variations, and integrates easily into existing frameworks.

Conclusion: A unified paradigm integrating demonstration and exploration is promising for future LLM post-training research.

Abstract: Existing post-training techniques for large language models are broadly
categorized into Supervised Fine-Tuning (SFT) and Reinforcement Fine-Tuning
(RFT). Each paradigm presents a distinct trade-off: SFT excels at mimicking
demonstration data but can lead to problematic generalization as a form of
behavior cloning. Conversely, RFT can significantly enhance a model's
performance but is prone to learn unexpected behaviors, and its performance is
highly sensitive to the initial policy. In this paper, we propose a unified
view of these methods and introduce Prefix-RFT, a hybrid approach that
synergizes learning from both demonstration and exploration. Using mathematical
reasoning problems as a testbed, we empirically demonstrate that Prefix-RFT is
both simple and effective. It not only surpasses the performance of standalone
SFT and RFT but also outperforms parallel mixed-policy RFT methods. A key
advantage is its seamless integration into existing open-source frameworks,
requiring only minimal modifications to the standard RFT pipeline. Our analysis
highlights the complementary nature of SFT and RFT, and validates that
Prefix-RFT effectively harmonizes these two learning paradigms. Furthermore,
ablation studies confirm the method's robustness to variations in the quality
and quantity of demonstration data. We hope this work offers a new perspective
on LLM post-training, suggesting that a unified paradigm that judiciously
integrates demonstration and exploration could be a promising direction for
future research.

</details>


### [192] [Cross-platform Smartphone Positioning at Museums](https://arxiv.org/pdf/2507.01469)
*Alessio Ferrato, Fabio Gasparetti, Carla Limongelli, Stefano Mastandrea, Giuseppe Sansonetti, Joaquín Torres-Sospedra*

Main category: cs.LG

TL;DR: The paper introduces BAR, a novel RSS dataset for museum environments, and a baseline classification method to address the lack of data for IPS development in cultural heritage settings.


<details>
  <summary>Details</summary>
Motivation: Cultural heritage institutions face challenges in implementing IPS due to environmental constraints and lack of relevant datasets.

Method: The study collects RSS data in museums using Android and iOS, and proposes a proximity-based method with k-NN algorithms for position classification.

Result: The BAR dataset and baseline method are presented, with analysis and suggestions for future research.

Conclusion: The work provides a foundation for developing and evaluating IPS algorithms tailored to museum environments.

Abstract: Indoor Positioning Systems (IPSs) hold significant potential for enhancing
visitor experiences in cultural heritage institutions. By enabling personalized
navigation, efficient artifact organization, and better interaction with
exhibits, IPSs can transform the modalities of how individuals engage with
museums, galleries and libraries. However, these institutions face several
challenges in implementing IPSs, including environmental constraints, technical
limits, and limited experimentation. In other contexts, Received Signal
Strength (RSS)-based approaches using Bluetooth Low Energy (BLE) and WiFi have
emerged as preferred solutions due to their non-invasive nature and minimal
infrastructure requirements. Nevertheless, the lack of publicly available RSS
datasets that specifically reflect museum environments presents a substantial
barrier to developing and evaluating positioning algorithms designed for the
intricate spatial characteristics typical of cultural heritage sites. To
address this limitation, we present BAR, a novel RSS dataset collected in front
of 90 artworks across 13 museum rooms using two different platforms, i.e.,
Android and iOS. Additionally, we provide an advanced position classification
baseline taking advantage of a proximity-based method and $k$-NN algorithms. In
our analysis, we discuss the results and offer suggestions for potential
research directions.

</details>


### [193] [Zero-Incentive Dynamics: a look at reward sparsity through the lens of unrewarded subgoals](https://arxiv.org/pdf/2507.01470)
*Yannick Molinghen, Tom Lenaerts*

Main category: cs.LG

TL;DR: The paper challenges the assumption that reward frequency measures task difficulty in reinforcement learning, highlighting a structural issue where critical subgoals lack rewards, termed zero-incentive dynamics.


<details>
  <summary>Details</summary>
Motivation: To address the limitation of current policy learning methods that fail when essential subgoals don't yield direct rewards.

Method: Identifies and formalizes zero-incentive dynamics, evaluates state-of-the-art subgoal-based algorithms, and analyzes sensitivity to reward timing.

Result: Current algorithms struggle with zero-incentive dynamics, and performance depends heavily on subgoal-reward timing.

Conclusion: A fundamental limitation exists in current methods, necessitating new mechanisms to infer latent task structure without relying on immediate rewards.

Abstract: This work re-examines the commonly held assumption that the frequency of
rewards is a reliable measure of task difficulty in reinforcement learning. We
identify and formalize a structural challenge that undermines the effectiveness
of current policy learning methods: when essential subgoals do not directly
yield rewards. We characterize such settings as exhibiting zero-incentive
dynamics, where transitions critical to success remain unrewarded. We show that
state-of-the-art deep subgoal-based algorithms fail to leverage these dynamics
and that learning performance is highly sensitive to the temporal proximity
between subgoal completion and eventual reward. These findings reveal a
fundamental limitation in current approaches and point to the need for
mechanisms that can infer latent task structure without relying on immediate
incentives.

</details>


### [194] [Loss Functions in Diffusion Models: A Comparative Study](https://arxiv.org/pdf/2507.01516)
*Dibyanshu Kumar, Philipp Vaeth, Magda Gregorová*

Main category: cs.LG

TL;DR: The paper explores and unifies various loss functions in diffusion models under the variational lower bound framework, analyzing their relationships, performance differences, and impact on model goals like sample quality and likelihood estimation.


<details>
  <summary>Details</summary>
Motivation: To address the key question of which loss functions diffusion models should train with, given the multiple formulations in literature with varying links and differences.

Method: Theoretical analysis of target objectives and loss functions, unified under the variational lower bound framework, complemented by empirical study.

Result: Provides insights into performance divergence conditions and factors, and evaluates how objective choice affects model goals.

Conclusion: Offers a unified understanding of loss functions in diffusion models, aiding future efficient and goal-oriented designs.

Abstract: Diffusion models have emerged as powerful generative models, inspiring
extensive research into their underlying mechanisms. One of the key questions
in this area is the loss functions these models shall train with. Multiple
formulations have been introduced in the literature over the past several years
with some links and some critical differences stemming from various initial
considerations. In this paper, we explore the different target objectives and
corresponding loss functions in detail. We present a systematic overview of
their relationships, unifying them under the framework of the variational lower
bound objective. We complement this theoretical analysis with an empirical
study providing insights into the conditions under which these objectives
diverge in performance and the underlying factors contributing to such
deviations. Additionally, we evaluate how the choice of objective impacts the
model ability to achieve specific goals, such as generating high-quality
samples or accurately estimating likelihoods. This study offers a unified
understanding of loss functions in diffusion models, contributing to more
efficient and goal-oriented model designs in future research.

</details>


### [195] [Tuning without Peeking: Provable Privacy and Generalization Bounds for LLM Post-Training](https://arxiv.org/pdf/2507.01752)
*Ismail Labiad, Mathurin Videau, Matthieu Kowalski, Marc Schoenauer, Alessandro Leite, Julia Kempe, Olivier Teytaud*

Main category: cs.LG

TL;DR: BBoxER is an evolutionary black-box method for LLM post-training, addressing privacy and security concerns of gradient-based optimization while overcoming scalability and computational challenges of black-box methods.


<details>
  <summary>Details</summary>
Motivation: The paper addresses the limitations of gradient-based optimization (privacy, security, overfitting) and black-box methods (scalability, computational cost) in deep learning, particularly for large language models (LLMs).

Method: BBoxER introduces an evolutionary black-box method for LLM post-training, leveraging implicit data compression and information flow tractability to provide theoretical guarantees.

Result: Experiments show BBoxER improves performance and generalization on reasoning benchmarks, offering lightweight, modular enhancements for privacy-sensitive environments.

Conclusion: BBoxER is a viable add-on to gradient-based optimization, balancing privacy, security, and performance for LLMs.

Abstract: Gradient-based optimization is the workhorse of deep learning, offering
efficient and scalable training via backpropagation. However, its reliance on
large volumes of labeled data raises privacy and security concerns such as
susceptibility to data poisoning attacks and the risk of overfitting. In
contrast, black box optimization methods, which treat the model as an opaque
function, relying solely on function evaluations to guide optimization, offer a
promising alternative in scenarios where data access is restricted, adversarial
risks are high, or overfitting is a concern. However, black box methods also
pose significant challenges, including poor scalability to high-dimensional
parameter spaces, as prevalent in large language models (LLMs), and high
computational costs due to reliance on numerous model evaluations. This paper
introduces BBoxER, an evolutionary black-box method for LLM post-training that
induces an information bottleneck via implicit compression of the training
data. Leveraging the tractability of information flow, we provide strong
theoretical bounds on generalization, differential privacy, susceptibility to
data poisoning attacks, and robustness to extraction attacks. BBoxER operates
on top of pre-trained LLMs, offering a lightweight and modular enhancement
suitable for deployment in restricted or privacy-sensitive environments, in
addition to non-vacuous generalization guarantees. In experiments with LLMs, we
demonstrate empirically that Retrofitting methods are able to learn, showing
how a few iterations of BBoxER improve performance and generalize well on a
benchmark of reasoning datasets. This positions BBoxER as an attractive add-on
on top of gradient-based optimization.

</details>


### [196] [Chargax: A JAX Accelerated EV Charging Simulator](https://arxiv.org/pdf/2507.01522)
*Koen Ponse, Jan Felix Kleuker, Aske Plaat, Thomas Moerland*

Main category: cs.LG

TL;DR: Chargax is a JAX-based environment for simulating electric vehicle charging stations, offering 100x-1000x faster training for RL agents compared to existing methods.


<details>
  <summary>Details</summary>
Motivation: Addressing slow reinforcement learning in sustainable energy challenges, particularly grid congestion and operational efficiency.

Method: Introduces Chargax, a modular JAX-based environment for realistic EV charging station simulations, validated with real data.

Result: Achieves 100x-1000x computational performance improvements and supports diverse real-world configurations.

Conclusion: Chargax significantly accelerates RL training for sustainable energy applications, enabling practical deployment.

Abstract: Deep Reinforcement Learning can play a key role in addressing sustainable
energy challenges. For instance, many grid systems are heavily congested,
highlighting the urgent need to enhance operational efficiency. However,
reinforcement learning approaches have traditionally been slow due to the high
sample complexity and expensive simulation requirements. While recent works
have effectively used GPUs to accelerate data generation by converting
environments to JAX, these works have largely focussed on classical toy
problems. This paper introduces Chargax, a JAX-based environment for realistic
simulation of electric vehicle charging stations designed for accelerated
training of RL agents. We validate our environment in a variety of scenarios
based on real data, comparing reinforcement learning agents against baselines.
Chargax delivers substantial computational performance improvements of over
100x-1000x over existing environments. Additionally, Chargax' modular
architecture enables the representation of diverse real-world charging station
configurations.

</details>


### [197] [LoRA Fine-Tuning Without GPUs: A CPU-Efficient Meta-Generation Framework for LLMs](https://arxiv.org/pdf/2507.01806)
*Reza Arabpour, Haitz Sáez de Ocáriz Borde, Anastasis Kratsios*

Main category: cs.LG

TL;DR: Proposes a CPU-friendly LoRA fine-tuning method for LLMs, leveraging pre-trained adapters to create lightweight combinations without GPU reliance.


<details>
  <summary>Details</summary>
Motivation: Address the limitation of GPU dependency in LoRA fine-tuning, making it accessible for users with limited computational resources like standard laptop CPUs.

Method: Learns a meta-operator to map input datasets to LoRA weights by combining pre-trained adapters, avoiding gradient-based updates and enabling CPU-only processing.

Result: The CPU-generated adapters outperform the base Mistral model but fall short of GPU-trained LoRAs, offering a practical alternative.

Conclusion: Provides a viable, resource-efficient solution for LoRA fine-tuning, broadening accessibility for users without GPUs.

Abstract: Low-Rank Adapters (LoRAs) have transformed the fine-tuning of Large Language
Models (LLMs) by enabling parameter-efficient updates. However, their
widespread adoption remains limited by the reliance on GPU-based training. In
this work, we propose a theoretically grounded approach to LoRA fine-tuning
designed specifically for users with limited computational resources,
particularly those restricted to standard laptop CPUs. Our method learns a
meta-operator that maps any input dataset, represented as a probability
distribution, to a set of LoRA weights by leveraging a large bank of
pre-trained adapters for the Mistral-7B-Instruct-v0.2 model. Instead of
performing new gradient-based updates, our pipeline constructs adapters via
lightweight combinations of existing LoRAs directly on CPU. While the resulting
adapters do not match the performance of GPU-trained counterparts, they
consistently outperform the base Mistral model on downstream tasks, offering a
practical and accessible alternative to traditional GPU-based fine-tuning.

</details>


### [198] [MARVIS: Modality Adaptive Reasoning over VISualizations](https://arxiv.org/pdf/2507.01544)
*Benjamin Feuer, Lennart Purucker, Oussama Elachqar, Chinmay Hegde*

Main category: cs.LG

TL;DR: MARVIS enables small vision-language models to predict any data modality accurately without training, outperforming Gemini by 16% and nearing specialized methods.


<details>
  <summary>Details</summary>
Motivation: Specialized models lack flexibility, while foundation models underperform in non-traditional domains. MARVIS bridges this gap.

Method: Transforms latent embeddings into visual representations, leveraging VLMs' reasoning skills for interpretation.

Result: Competitive performance across vision, audio, biological, and tabular domains with a single 3B parameter model.

Conclusion: MARVIS offers versatile, high-accuracy predictions without P.I.I. exposure or domain-specific training, with open-sourced resources.

Abstract: Scientific applications of machine learning often rely on small, specialized
models tuned to particular domains. Such models often achieve excellent
performance, but lack flexibility. Foundation models offer versatility, but
typically underperform specialized approaches, especially on non-traditional
modalities and long-tail domains. We propose MARVIS (Modality Adaptive
Reasoning over VISualizations), a training-free method that enables even small
vision-language models to predict any data modality with high accuracy. MARVIS
transforms latent embedding spaces into visual representations and then
leverages the spatial and fine-grained reasoning skills of VLMs to successfully
interpret and utilize them. MARVIS achieves competitive performance on vision,
audio, biological, and tabular domains using a single 3B parameter model,
achieving results that beat Gemini by 16\% on average and approach specialized
methods, without exposing personally identifiable information (P.I.I.) or
requiring any domain-specific training. We open source our code and datasets at
https://github.com/penfever/marvis

</details>


### [199] [Test-Time Scaling with Reflective Generative Model](https://arxiv.org/pdf/2507.01951)
*Zixiao Wang, Yuxin Wang, Xiaorui Wang, Mengting Xing, Jie Gao, Jianjun Xu, Guangcan Liu, Chenhui Jin, Zhuo Wang, Shengzhuo Zhang, Hongtao Xie*

Main category: cs.LG

TL;DR: MetaStone-S1 is a reflective generative model using SPRM to match OpenAI o3's performance, reducing PRM parameters by 99% and enabling efficient reasoning with controllable thinking modes.


<details>
  <summary>Details</summary>
Motivation: To create a unified, efficient model that integrates policy and process reward models without extra annotations, while maintaining performance.

Method: Uses SPRM with shared backbone and task-specific heads for next token prediction and process scoring, enabling test time scaling (TTS) with three reasoning effort modes.

Result: Achieves performance comparable to OpenAI-o3-mini with only 32B parameters and establishes a scaling law for TTS.

Conclusion: MetaStone-S1 is a scalable, efficient model open-sourced for community use, demonstrating the viability of SPRM for high-performance reasoning.

Abstract: We introduce our first reflective generative model MetaStone-S1, which
obtains OpenAI o3's performance via the self-supervised process reward model
(SPRM). Through sharing the backbone network and using task-specific heads for
next token prediction and process scoring respectively, SPRM successfully
integrates the policy model and process reward model(PRM) into a unified
interface without extra process annotation, reducing over 99% PRM parameters
for efficient reasoning. Equipped with SPRM, MetaStone-S1 is naturally suitable
for test time scaling (TTS), and we provide three reasoning effort modes (low,
medium, and high), based on the controllable thinking length. Moreover, we
empirically establish a scaling law that reveals the relationship between total
thinking computation and TTS performance. Experiments demonstrate that our
MetaStone-S1 achieves comparable performance to OpenAI-o3-mini's series with
only 32B parameter size. To support the research community, we have
open-sourced MetaStone-S1 at https://github.com/MetaStone-AI/MetaStone-S1.

</details>


### [200] [How Weight Resampling and Optimizers Shape the Dynamics of Continual Learning and Forgetting in Neural Networks](https://arxiv.org/pdf/2507.01559)
*Lapo Frati, Neil Traft, Jeff Clune, Nick Cheney*

Main category: cs.LG

TL;DR: The paper explores the effects of resampling weights ("zapping") in neural networks during continual learning, showing it aids faster recovery in domain transfers and highlights optimizer choice's impact on learning dynamics.


<details>
  <summary>Details</summary>
Motivation: To understand the mechanisms behind the effectiveness of zapping in continual learning and its interaction with optimizer choices in multi-task settings.

Method: Experiments with convolutional neural networks in continual and few-shot transfer learning scenarios, using handwritten characters and natural images.

Result: Zapping accelerates recovery in domain transfers, and optimizer choice influences learning/forgetting dynamics, creating task synergy/interference patterns.

Conclusion: Zapping and optimizer selection significantly impact continual learning performance, revealing complex task interaction dynamics.

Abstract: Recent work in continual learning has highlighted the beneficial effect of
resampling weights in the last layer of a neural network (``zapping"). Although
empirical results demonstrate the effectiveness of this approach, the
underlying mechanisms that drive these improvements remain unclear. In this
work, we investigate in detail the pattern of learning and forgetting that take
place inside a convolutional neural network when trained in challenging
settings such as continual learning and few-shot transfer learning, with
handwritten characters and natural images. Our experiments show that models
that have undergone zapping during training more quickly recover from the shock
of transferring to a new domain. Furthermore, to better observe the effect of
continual learning in a multi-task setting we measure how each individual task
is affected. This shows that, not only zapping, but the choice of optimizer can
also deeply affect the dynamics of learning and forgetting, causing complex
patterns of synergy/interference between tasks to emerge when the model learns
sequentially at transfer time.

</details>


### [201] [A Privacy-Preserving Indoor Localization System based on Hierarchical Federated Learning](https://arxiv.org/pdf/2507.01581)
*Masood Jan, Wafa Njima, Xun Zhang*

Main category: cs.LG

TL;DR: The paper proposes a Federated Learning (FL)-based approach for indoor localization using a DNN model, addressing privacy and efficiency issues of traditional centralized methods.


<details>
  <summary>Details</summary>
Motivation: Traditional indoor localization techniques have high errors and privacy concerns due to centralized data collection. ML solutions often face similar issues.

Method: A Federated Learning (FL) approach with a Deep Neural Network (DNN) model is proposed to decentralize data processing.

Result: FL achieves performance close to centralized models while ensuring data privacy, bandwidth efficiency, and server reliability.

Conclusion: The FL approach is a viable solution for privacy-enhanced indoor localization, advancing secure and efficient systems.

Abstract: Location information serves as the fundamental element for numerous Internet
of Things (IoT) applications. Traditional indoor localization techniques often
produce significant errors and raise privacy concerns due to centralized data
collection. In response, Machine Learning (ML) techniques offer promising
solutions by capturing indoor environment variations. However, they typically
require central data aggregation, leading to privacy, bandwidth, and server
reliability issues. To overcome these challenges, in this paper, we propose a
Federated Learning (FL)-based approach for dynamic indoor localization using a
Deep Neural Network (DNN) model. Experimental results show that FL has the
nearby performance to Centralized Model (CL) while keeping the data privacy,
bandwidth efficiency and server reliability. This research demonstrates that
our proposed FL approach provides a viable solution for privacy-enhanced indoor
localization, paving the way for advancements in secure and efficient indoor
localization systems.

</details>


### [202] [Analysis of Muon's Convergence and Critical Batch Size](https://arxiv.org/pdf/2507.01598)
*Naoki Sato, Hiroki Naganuma, Hideaki Iiduka*

Main category: cs.LG

TL;DR: The paper analyzes Muon, a new optimizer for neural networks, proving convergence for its variants and showing tighter bounds with weight decay. It also derives Muon's critical batch size and validates findings experimentally.


<details>
  <summary>Details</summary>
Motivation: To theoretically analyze Muon, a novel optimizer leveraging neural network parameter structure, and explore its variants and properties like weight decay and batch size.

Method: Theoretical analysis of Muon, including convergence proofs for four variants (with/without Nesterov momentum and weight decay), and derivation of critical batch size. Experimental validation is also conducted.

Result: Weight decay tightens bounds on parameter and gradient norms. The relationship between weight decay coefficient and learning rate is clarified. Critical batch size minimizing SFO complexity is derived.

Conclusion: Muon's theoretical properties are rigorously analyzed, with practical implications for optimization in neural networks, supported by experimental validation.

Abstract: This paper presents a theoretical analysis of Muon, a new optimizer that
leverages the inherent matrix structure of neural network parameters. We
provide convergence proofs for four practical variants of Muon: with and
without Nesterov momentum, and with and without weight decay. We then show that
adding weight decay leads to strictly tighter bounds on both the parameter and
gradient norms, and we clarify the relationship between the weight decay
coefficient and the learning rate. Finally, we derive Muon's critical batch
size minimizing the stochastic first-order oracle (SFO) complexity, which is
the stochastic computational cost, and validate our theoretical findings with
experiments.

</details>


### [203] [GradMetaNet: An Equivariant Architecture for Learning on Gradients](https://arxiv.org/pdf/2507.01649)
*Yoav Gelberg, Yam Eitan, Aviv Navon, Aviv Shamsian, Theo, Putterman, Michael Bronstein, Haggai Maron*

Main category: cs.LG

TL;DR: The paper introduces GradMetaNet, a novel architecture designed for processing gradients in neural networks, guided by principles of equivariance, multi-point gradient processing, and efficient representation.


<details>
  <summary>Details</summary>
Motivation: Existing gradient-processing architectures lack specialized design, limiting their effectiveness. The paper aims to address this gap.

Method: Proposes GradMetaNet, built on equivariant blocks, to process gradients efficiently while capturing curvature information.

Result: GradMetaNet outperforms previous methods in tasks like learned optimization and loss landscape curvature estimation.

Conclusion: GradMetaNet provides a principled and effective solution for gradient-based tasks, demonstrating superior performance and universality.

Abstract: Gradients of neural networks encode valuable information for optimization,
editing, and analysis of models. Therefore, practitioners often treat gradients
as inputs to task-specific algorithms, e.g. for pruning or optimization. Recent
works explore learning algorithms that operate directly on gradients but use
architectures that are not specifically designed for gradient processing,
limiting their applicability. In this paper, we present a principled approach
for designing architectures that process gradients. Our approach is guided by
three principles: (1) equivariant design that preserves neuron permutation
symmetries, (2) processing sets of gradients across multiple data points to
capture curvature information, and (3) efficient gradient representation
through rank-1 decomposition. Based on these principles, we introduce
GradMetaNet, a novel architecture for learning on gradients, constructed from
simple equivariant blocks. We prove universality results for GradMetaNet, and
show that previous approaches cannot approximate natural gradient-based
functions that GradMetaNet can. We then demonstrate GradMetaNet's effectiveness
on a diverse set of gradient-based tasks on MLPs and transformers, such as
learned optimization, INR editing, and estimating loss landscape curvature.

</details>


### [204] [Kernel Recursive Least Squares Dictionary Learning Algorithm](https://arxiv.org/pdf/2507.01636)
*Ghasem Alipoor, Karl Skretting*

Main category: cs.LG

TL;DR: An efficient online dictionary learning algorithm for kernel-based sparse representations, outperforming existing methods with low computational complexity.


<details>
  <summary>Details</summary>
Motivation: To improve online kernel dictionary learning by enabling efficient updates and maintaining high accuracy.

Method: Uses a recursive least squares (RLS) method for dictionary updates, working with single samples or mini-batches.

Result: Outperforms existing online methods and achieves classification accuracy close to batch-trained models, with higher efficiency.

Conclusion: The proposed method is efficient and effective for online kernel dictionary learning, balancing accuracy and computational cost.

Abstract: We propose an efficient online dictionary learning algorithm for kernel-based
sparse representations. In this framework, input signals are nonlinearly mapped
to a high-dimensional feature space and represented sparsely using a virtual
dictionary. At each step, the dictionary is updated recursively using a novel
algorithm based on the recursive least squares (RLS) method. This update
mechanism works with single samples or mini-batches and maintains low
computational complexity. Experiments on four datasets across different domains
show that our method not only outperforms existing online kernel dictionary
learning approaches but also achieves classification accuracy close to that of
batch-trained models, while remaining significantly more efficient.

</details>


### [205] [AsyncFlow: An Asynchronous Streaming RL Framework for Efficient LLM Post-Training](https://arxiv.org/pdf/2507.01663)
*Zhenyu Han, Ansheng You, Haibo Wang, Kui Luo, Guang Yang, Wenqi Shi, Menglong Chen, Sicheng Zhang, Zeshun Lan, Chunshi Deng, Huazhong Ji, Wenjie Liu, Yu Huang, Yixiang Zhang, Chenyi Pan, Jing Wang, Xin Huang, Chunsheng Li, Jianping Wu*

Main category: cs.LG

TL;DR: AsyncFlow is an asynchronous streaming RL framework for efficient post-training of LLMs, addressing scalability and resource issues in traditional RL frameworks.


<details>
  <summary>Details</summary>
Motivation: Traditional RL frameworks for LLMs face scalability bottlenecks, dataflow complexity, and tight coupling with training/inference engines, limiting flexibility and efficiency.

Method: AsyncFlow introduces distributed data storage, fine-grained scheduling, and a producer-consumer workflow to enable pipeline overlapping, load balancing, and minimized idleness.

Result: Experiments show AsyncFlow achieves a 1.59x throughput improvement over state-of-the-art baselines.

Conclusion: AsyncFlow offers a modular, efficient solution for RL post-training, with insights for future RL system designs.

Abstract: Reinforcement learning (RL) has become a pivotal technology in the
post-training phase of large language models (LLMs). Traditional task-colocated
RL frameworks suffer from significant scalability bottlenecks, while
task-separated RL frameworks face challenges in complex dataflows and the
corresponding resource idling and workload imbalance. Moreover, most existing
frameworks are tightly coupled with LLM training or inference engines, making
it difficult to support custom-designed engines. To address these challenges,
we propose AsyncFlow, an asynchronous streaming RL framework for efficient
post-training. Specifically, we introduce a distributed data storage and
transfer module that provides a unified data management and fine-grained
scheduling capability in a fully streamed manner. This architecture inherently
facilitates automated pipeline overlapping among RL tasks and dynamic load
balancing. Moreover, we propose a producer-consumer-based asynchronous workflow
engineered to minimize computational idleness by strategically deferring
parameter update process within staleness thresholds. Finally, the core
capability of AsynFlow is architecturally decoupled from underlying training
and inference engines and encapsulated by service-oriented user interfaces,
offering a modular and customizable user experience. Extensive experiments
demonstrate an average of 1.59 throughput improvement compared with
state-of-the-art baseline. The presented architecture in this work provides
actionable insights for next-generation RL training system designs.

</details>


### [206] [Dance Dance ConvLSTM](https://arxiv.org/pdf/2507.01644)
*Miguel O'Malley*

Main category: cs.LG

TL;DR: DDCL improves DDR chart generation accuracy using a ConvLSTM model, surpassing the earlier CNN-LSTM approach (DDC).


<details>
  <summary>Details</summary>
Motivation: To enhance the automatic generation of DDR charts, addressing limitations of the prior DDC method.

Method: Uses a ConvLSTM-based model for chart generation, building on the DDC framework.

Result: Substantially increases the accuracy of DDR chart generation compared to DDC.

Conclusion: DDCL is a superior method for automatic DDR chart generation, offering improved performance over DDC.

Abstract: \textit{Dance Dance Revolution} is a rhythm game consisting of songs and
accompanying choreography, referred to as charts. Players press arrows on a
device referred to as a dance pad in time with steps determined by the song's
chart. In 2017, the authors of Dance Dance Convolution (DDC) developed an
algorithm for the automatic generation of \textit{Dance Dance Revolution}
charts, utilizing a CNN-LSTM architecture. We introduce Dance Dance ConvLSTM
(DDCL), a new method for the automatic generation of DDR charts using a
ConvLSTM based model, which improves upon the DDC methodology and substantially
increases the accuracy of chart generation.

</details>


### [207] [GPT, But Backwards: Exactly Inverting Language Model Outputs](https://arxiv.org/pdf/2507.01693)
*Adrians Skapars, Edoardo Manino, Youcheng Sun, Lucas C. Cordeiro*

Main category: cs.LG

TL;DR: The paper introduces SODA, a gradient-based algorithm for reconstructing exact inputs from LLM outputs, outperforming existing methods with 79.5% recovery for shorter inputs but struggling with longer sequences.


<details>
  <summary>Details</summary>
Motivation: To enable post-incident analysis and detect fake outputs by reconstructing the exact input from LLM outputs, addressing a forensic gap in auditing techniques.

Method: Formalizes input reconstruction as a discrete optimization problem, introducing SODA with continuous relaxation, periodic restarts, and parameter decay.

Result: SODA recovers 79.5% of shorter out-of-distribution inputs without false positives but fails with longer (15+ token) sequences, indicating current deployment practices may mitigate misuse.

Conclusion: SODA is effective for forensic analysis of LLM outputs but has limitations with longer inputs, suggesting existing protections may suffice against malicious use.

Abstract: While existing auditing techniques attempt to identify potential unwanted
behaviours in large language models (LLMs), we address the complementary
forensic problem of reconstructing the exact input that led to an existing LLM
output - enabling post-incident analysis and potentially the detection of fake
output reports. We formalize exact input reconstruction as a discrete
optimisation problem with a unique global minimum and introduce SODA, an
efficient gradient-based algorithm that operates on a continuous relaxation of
the input search space with periodic restarts and parameter decay. Through
comprehensive experiments on LLMs ranging in size from 33M to 3B parameters, we
demonstrate that SODA significantly outperforms existing approaches. We succeed
in fully recovering 79.5% of shorter out-of-distribution inputs from next-token
logits, without a single false positive, but struggle to extract private
information from the outputs of longer (15+ token) input sequences. This
suggests that standard deployment practices may currently provide adequate
protection against malicious use of our method. Our code is available at
https://doi.org/10.5281/zenodo.15539879.

</details>


<div id='cs.MA'></div>

# cs.MA [[Back]](#toc)

### [208] [Automated Vehicles Should be Connected with Natural Language](https://arxiv.org/pdf/2507.01059)
*Xiangbo Gao, Keshu Wu, Hao Zhang, Kexin Tian, Yang Zhou, Zhengzhong Tu*

Main category: cs.MA

TL;DR: The paper proposes using natural language for multi-agent collaborative driving to improve communication of intent and reasoning, addressing bandwidth and interoperability issues.


<details>
  <summary>Details</summary>
Motivation: Existing communication methods in multi-agent driving (sensor data, neural features, perception results) are limited in bandwidth, completeness, and interoperability, and lack decision-level fusion.

Method: Advocates transitioning from perception-oriented data exchanges to natural language communication for intent and reasoning.

Result: Natural language balances semantic density and bandwidth, adapts to real-time conditions, and bridges heterogeneous platforms.

Conclusion: Natural language enables proactive coordination, enhancing safety, efficiency, and transparency in intelligent transportation systems.

Abstract: Multi-agent collaborative driving promises improvements in traffic safety and
efficiency through collective perception and decision making. However, existing
communication media -- including raw sensor data, neural network features, and
perception results -- suffer limitations in bandwidth efficiency, information
completeness, and agent interoperability. Moreover, traditional approaches have
largely ignored decision-level fusion, neglecting critical dimensions of
collaborative driving. In this paper we argue that addressing these challenges
requires a transition from purely perception-oriented data exchanges to
explicit intent and reasoning communication using natural language. Natural
language balances semantic density and communication bandwidth, adapts flexibly
to real-time conditions, and bridges heterogeneous agent platforms. By enabling
the direct communication of intentions, rationales, and decisions, it
transforms collaborative driving from reactive perception-data sharing into
proactive coordination, advancing safety, efficiency, and transparency in
intelligent transportation systems.

</details>


### [209] [RALLY: Role-Adaptive LLM-Driven Yoked Navigation for Agentic UAV Swarms](https://arxiv.org/pdf/2507.01378)
*Ziyao Wang, Rongpeng Li, Sizhao Li, Yuming Xiang, Haiping Wang, Zhifeng Zhao, Honggang Zhang*

Main category: cs.MA

TL;DR: RALLY, a Role-Adaptive LLM-Driven Yoked navigation algorithm, enhances UAV swarm control by combining LLM semantic reasoning with MARL online learning, outperforming traditional methods in task coverage and generalization.


<details>
  <summary>Details</summary>
Motivation: Traditional MARL and LLM-based frameworks for UAV swarm control face issues like poor generalization, limited scalability, and ineffective exploration due to rigid role structures and static priors.

Method: RALLY integrates an LLM-driven semantic decision framework, dynamic role-heterogeneity mechanism, and RMIX-based assignment strategy to blend offline priors with online MARL policies.

Result: Experiments show RALLY excels in task coverage, convergence speed, and generalization compared to conventional methods.

Conclusion: RALLY demonstrates strong potential for collaborative navigation in multi-UAV systems by addressing the limitations of existing approaches.

Abstract: Intelligent control of Unmanned Aerial Vehicles (UAVs) swarms has emerged as
a critical research focus, and it typically requires the swarm to navigate
effectively while avoiding obstacles and achieving continuous coverage over
multiple mission targets. Although traditional Multi-Agent Reinforcement
Learning (MARL) approaches offer dynamic adaptability, they are hindered by the
semantic gap in numerical communication and the rigidity of homogeneous role
structures, resulting in poor generalization and limited task scalability.
Recent advances in Large Language Model (LLM)-based control frameworks
demonstrate strong semantic reasoning capabilities by leveraging extensive
prior knowledge. However, due to the lack of online learning and over-reliance
on static priors, these works often struggle with effective exploration,
leading to reduced individual potential and overall system performance. To
address these limitations, we propose a Role-Adaptive LLM-Driven Yoked
navigation algorithm RALLY. Specifically, we first develop an LLM-driven
semantic decision framework that uses structured natural language for efficient
semantic communication and collaborative reasoning. Afterward, we introduce a
dynamic role-heterogeneity mechanism for adaptive role switching and
personalized decision-making. Furthermore, we propose a Role-value Mixing
Network (RMIX)-based assignment strategy that integrates LLM offline priors
with MARL online policies to enable semi-offline training of role selection
strategies. Experiments in the Multi-Agent Particle Environment (MPE)
environment and a Software-In-The-Loop (SITL) platform demonstrate that RALLY
outperforms conventional approaches in terms of task coverage, convergence
speed, and generalization, highlighting its strong potential for collaborative
navigation in agentic multi-UAV systems.

</details>


### [210] [Exploring Advanced LLM Multi-Agent Systems Based on Blackboard Architecture](https://arxiv.org/pdf/2507.01701)
*Bochen Han, Songmao Zhang*

Main category: cs.MA

TL;DR: Incorporating blackboard architecture into LLM multi-agent systems improves information sharing, dynamic agent selection, and consensus-building, achieving competitive performance with fewer tokens.


<details>
  <summary>Details</summary>
Motivation: To enhance problem-solving in multi-agent systems by enabling dynamic information sharing and agent selection without predefined workflows.

Method: Proposed a blackboard architecture for LLM multi-agent systems, allowing shared information and dynamic agent selection based on blackboard content, iterating until consensus.

Result: Competitive with SOTA static and dynamic MASs, achieving best average performance with fewer tokens.

Conclusion: The blackboard architecture enables complex, dynamic problem-solving where structured workflows are absent.

Abstract: In this paper, we propose to incorporate the blackboard architecture into LLM
multi-agent systems (MASs) so that (1) agents with various roles can share all
the information and others' messages during the whole problem-solving process,
(2) agents that will take actions are selected based on the current content of
the blackboard, and (3) the selection and execution round is repeated until a
consensus is reached on the blackboard. We develop the first implementation of
this proposal and conduct experiments on commonsense knowledge, reasoning and
mathematical datasets. The results show that our system can be competitive with
the SOTA static and dynamic MASs by achieving the best average performance, and
at the same time manage to spend less tokens. Our proposal has the potential to
enable complex and dynamic problem-solving where well-defined structures or
workflows are unavailable.

</details>


### [211] [Distance-based Relative Orbital Transition for Satellite Swarm Array Deployment Under J2 Perturbation](https://arxiv.org/pdf/2507.01769)
*Yuta Takahashi, Shin-ichiro Sakai*

Main category: cs.MA

TL;DR: An autonomous guidance and control strategy for satellite swarms enables scalable distributed space structures using decentralized methods, fuel-free actuation, and drift mitigation.


<details>
  <summary>Details</summary>
Motivation: To enable innovative science and business opportunities through scalable, distributed space structures using autonomous satellite swarms.

Method: Derived averaged $J_2$ orbital parameters for decentralized deployment, designed a distance-based orbital stabilizer, and used fuel-free actuation (magnetic field interaction, differential aerodynamic forces).

Result: Achieved autonomous deployment into coplanar equidistant formations and maintained long-term stability without thrusters.

Conclusion: The decentralized approach effectively mitigates drift risks and enables stable, scalable satellite swarm formations.

Abstract: This paper presents an autonomous guidance and control strategy for a
satellite swarm that enables scalable distributed space structures for
innovative science and business opportunities. The averaged $J_2$ orbital
parameters that describe the drift and periodic orbital motion were derived
along with their target values to achieve a distributed space structure in a
decentralized manner. This enabled the design of a distance-based orbital
stabilizer to ensure autonomous deployment into a monolithic formation of a
coplanar equidistant configuration on a user-defined orbital plane. Continuous
formation control was assumed to be achieved through fuel-free actuation, such
as satellite magnetic field interaction and differential aerodynamic forces,
thereby maintaining long-term formation stability without thruster usage. A
major challenge for such actuation systems is the potential loss of control
capability due to increasing inter-satellite distances resulting from unstable
orbital dynamics, particularly for autonomous satellite swarms. To mitigate
this risk, our decentralized deployment controller minimized drift distance
during unexpected communication outages. As a case study, we consider the
deployment of palm-sized satellites into a coplanar equidistant formation in a
$J_2$-perturbed orbit. Moreover, centralized grouping strategies are presented.

</details>


### [212] [Active Scout: Multi-Target Tracking Using Neural Radiance Fields in Dense Urban Environments](https://arxiv.org/pdf/2406.07431)
*Christopher D. Hsu, Pratik Chaudhari*

Main category: cs.MA

TL;DR: A method for tracking dynamic targets in occluded urban environments using neural radiance fields (NeRF) for active perception, outperforming greedy baselines in dynamic scenarios.


<details>
  <summary>Details</summary>
Motivation: To address the challenge of tracking dynamic targets in highly occluded urban settings, where traditional methods fail due to lack of active perception.

Method: Uses NeRF to create an online 3D representation of the city from RGB and depth images, enabling information gain for exploration and tracking. Tested in a custom simulator with Open Street Maps data.

Result: For dynamic targets, maintains tracking error of 200m (vs. 600m for greedy baseline). Locates 20 stationary targets in 300 steps, slower but more robust than greedy methods.

Conclusion: The approach demonstrates effectiveness in dynamic target tracking, with improved performance as the NeRF representation evolves, and offers a principled solution for active perception.

Abstract: We study pursuit-evasion games in highly occluded urban environments, e.g.
tall buildings in a city, where a scout (quadrotor) tracks multiple dynamic
targets on the ground. We show that we can build a neural radiance field (NeRF)
representation of the city -- online -- using RGB and depth images from
different vantage points. This representation is used to calculate the
information gain to both explore unknown parts of the city and track the
targets -- thereby giving a completely first-principles approach to actively
tracking dynamic targets. We demonstrate, using a custom-built simulator using
Open Street Maps data of Philadelphia and New York City, that we can explore
and locate 20 stationary targets within 300 steps. This is slower than a greedy
baseline, which does not use active perception. But for dynamic targets that
actively hide behind occlusions, we show that our approach maintains, at worst,
a tracking error of 200m; the greedy baseline can have a tracking error as
large as 600m. We observe a number of interesting properties in the scout's
policies, e.g., it switches its attention to track a different target
periodically, as the quality of the NeRF representation improves over time, the
scout also becomes better in terms of target tracking. Code is available at
https://github.com/grasp-lyrl/ActiveScout.

</details>


### [213] [Adaptive Traffic Signal Control based on Multi-Agent Reinforcement Learning. Case Study on a simulated real-world corridor](https://arxiv.org/pdf/2503.02189)
*Dickness Kakitahi Kwesiga, Angshuman Guin, Michael Hunter*

Main category: cs.MA

TL;DR: The paper introduces a multi-agent proximal policy optimization (MA-PPO) algorithm for adaptive traffic signal control, showing superior performance over traditional methods in simulated real-world conditions.


<details>
  <summary>Details</summary>
Motivation: Address gaps in multi-agent RL for traffic control, particularly the underuse of policy-based methods and untested assumptions in real-world scenarios.

Method: Formulates MA-PPO with a centralized-critic architecture, tested on a simulated seven-intersection corridor, comparing it to actuated-coordinated signal control (ASC).

Result: MA-PPO outperformed ASC, improving travel times by 2% and 24% in primary and secondary directions, respectively, and reducing crossing times.

Conclusion: MA-PPO is stable, robust, and adaptable to traffic demand changes, proving its potential for real-world traffic signal control.

Abstract: Previous studies that have formulated multi-agent reinforcement learning (RL)
algorithms for adaptive traffic signal control have primarily used value-based
RL methods. However, recent literature has shown that policy-based methods may
perform better in partially observable environments. Additionally, RL methods
remain largely untested for real-world normally signal timing plans because of
the simplifying assumptions common in the literature. The current study
attempts to address these gaps and formulates a multi-agent proximal policy
optimization (MA-PPO) algorithm to implement adaptive and coordinated traffic
control along an arterial corridor. The formulated MA-PPO has a
centralized-critic architecture under a centralized training and decentralized
execution framework. Agents are designed to allow selection and implementation
of up to eight signal phases, as commonly implemented in field controllers. The
formulated algorithm is tested on a simulated real-world seven intersection
corridor. The speed of convergence for each agent was found to depend on the
size of the action space, which depends on the number and sequence of signal
phases. The performance of the formulated MA-PPO adaptive control algorithm is
compared with the field implemented actuated-coordinated signal control (ASC),
modeled using PTV-Vissim-MaxTime software in the loop simulation (SILs). The
trained MA-PPO performed significantly better than the ASC for all movements.
Compared to ASC the MA-PPO showed 2% and 24% improvements in travel time in the
primary and secondary coordination directions, respectively. For cross streets
movements MA-PPO also showed significant crossing time reductions. Volume
sensitivity experiments revealed that the formulated MA-PPO demonstrated good
stability, robustness, and adaptability to changes in traffic demand.

</details>


<div id='cs.MM'></div>

# cs.MM [[Back]](#toc)

### [214] [Robust Multi-generation Learned Compression of Point Cloud Attribute](https://arxiv.org/pdf/2507.01320)
*Xiangzuo Liu, Zhikai Liu, PengPeng Yu, Ruishan Huang, Fan Liang*

Main category: cs.MM

TL;DR: The paper addresses cumulative distortion in multi-generation learned point cloud attribute compression, proposing constraints to enhance robustness and reversibility.


<details>
  <summary>Details</summary>
Motivation: Existing methods overlook cumulative distortion in multi-generation compression, leading to quality degradation.

Method: Proposes three constraints: Mapping Idempotency, Transformation Reversibility, and Latent Variable Consistency, to mitigate multi-generation loss.

Result: Experiments on Owlii and 8iVFB datasets show effective suppression of multi-generation loss while maintaining single-pass performance.

Conclusion: The proposed constraints successfully address multi-generation compression issues without compromising baseline performance.

Abstract: Existing learned point cloud attribute compression methods primarily focus on
single-pass rate-distortion optimization, while overlooking the issue of
cumulative distortion in multi-generation compression scenarios. This paper,
for the first time, investigates the multi-generation issue in learned point
cloud attribute compression. We identify two primary factors contributing to
quality degradation in multi-generation compression: quantization-induced
non-idempotency and transformation irreversibility. To address the former, we
propose a Mapping Idempotency Constraint, that enables the network to learn the
complete compression-decompression mapping, enhancing its robustness to
repeated processes. To address the latter, we introduce a Transformation
Reversibility Constraint, which preserves reversible information flow via a
quantization-free training path. Further, we propose a Latent Variable
Consistency Constraint which enhances the multi-generation compression
robustness by incorporating a decompression-compression cross-generation path
and a latent variable consistency loss term. Extensive experiments conducted on
the Owlii and 8iVFB datasets verify that the proposed methods can effectively
suppress multi-generation loss while maintaining single-pass rate-distortion
performance comparable to baseline models.

</details>


<div id='eess.AS'></div>

# eess.AS [[Back]](#toc)

### [215] [Scalable Offline ASR for Command-Style Dictation in Courtrooms](https://arxiv.org/pdf/2507.01021)
*Kumarmanas Nethil, Vaibhav Mishra, Kriti Anandan, Kavya Manohar*

Main category: eess.AS

TL;DR: An open-source framework for Command-style dictation bridges the gap between Online and Batch processing, using VAD and parallel Whisper models for efficient multiplexing.


<details>
  <summary>Details</summary>
Motivation: Address the inefficiency gap between resource-heavy Online systems and high-latency Batch processing in dictation systems.

Method: Uses Voice Activity Detection (VAD) to segment audio and transcribes segments in parallel with Whisper models, supporting multiplexing across audios.

Result: Deployed in 15% of India's courtrooms; shows latency reduction with increased user concurrency.

Conclusion: The framework is efficient, scalable, and open-source, with real-world applicability demonstrated.

Abstract: We propose an open-source framework for Command-style dictation that
addresses the gap between resource-intensive Online systems and high-latency
Batch processing. Our approach uses Voice Activity Detection (VAD) to segment
audio and transcribes these segments in parallel using Whisper models, enabling
efficient multiplexing across audios. Unlike proprietary systems like
SuperWhisper, this framework is also compatible with most ASR architectures,
including widely used CTC-based models. Our multiplexing technique maximizes
compute utilization in real-world settings, as demonstrated by its deployment
in around 15% of India's courtrooms. Evaluations on live data show consistent
latency reduction as user concurrency increases, compared to sequential batch
processing. The live demonstration will showcase our open-sourced
implementation and allow attendees to interact with it in real-time.

</details>


### [216] [Workflow-Based Evaluation of Music Generation Systems](https://arxiv.org/pdf/2507.01022)
*Shayan Dadman, Bernt Arild Bremdal, Andreas Bergsland*

Main category: eess.AS

TL;DR: The study evaluates eight open-source Music Generation Systems (MGS) in music production workflows, combining technical and practical criteria. Findings highlight MGS as complementary tools, not replacements, with limitations in coherence and emotional depth.


<details>
  <summary>Details</summary>
Motivation: To explore the practical and creative affordances of MGS in music production workflows and address their limitations and integration challenges.

Method: Mixed qualitative and quantitative approach with a single-evaluator methodology, focusing on architectural diversity across symbolic and audio-based systems.

Result: MGS enhance human expertise but lack thematic coherence, emphasizing the need for human creativity in complex tasks.

Conclusion: The study provides a structured evaluation framework and identifies areas for AI integration, guiding future MGS development.

Abstract: This study presents an exploratory evaluation of Music Generation Systems
(MGS) within contemporary music production workflows by examining eight
open-source systems. The evaluation framework combines technical insights with
practical experimentation through criteria specifically designed to investigate
the practical and creative affordances of the systems within the iterative,
non-linear nature of music production. Employing a single-evaluator methodology
as a preliminary phase, this research adopts a mixed approach utilizing
qualitative methods to form hypotheses subsequently assessed through
quantitative metrics. The selected systems represent architectural diversity
across both symbolic and audio-based music generation approaches, spanning
composition, arrangement, and sound design tasks. The investigation addresses
limitations of current MGS in music production, challenges and opportunities
for workflow integration, and development potential as collaborative tools
while maintaining artistic authenticity. Findings reveal these systems function
primarily as complementary tools enhancing rather than replacing human
expertise. They exhibit limitations in maintaining thematic and structural
coherence that emphasize the indispensable role of human creativity in tasks
demanding emotional depth and complex decision-making. This study contributes a
structured evaluation framework that considers the iterative nature of music
creation. It identifies methodological refinements necessary for subsequent
comprehensive evaluations and determines viable areas for AI integration as
collaborative tools in creative workflows. The research provides
empirically-grounded insights to guide future development in the field.

</details>


### [217] [Hello Afrika: Speech Commands in Kinyarwanda](https://arxiv.org/pdf/2507.01024)
*George Igwegbe, Martins Awojide, Mboh Bless, Nirel Kadzo*

Main category: eess.AS

TL;DR: The paper introduces the Hello Afrika project, which addresses the lack of speech command models for African languages, starting with Kinyarwanda. A custom corpus was used to build and deploy the model on various devices, with performance evaluated using metrics.


<details>
  <summary>Details</summary>
Motivation: There is a shortage of speech command models for African languages, limiting accessibility for persons with disabilities. The project aims to fill this gap, starting with Kinyarwanda.

Method: A custom speech command corpus (general directives, numbers, wake word) was developed. The model was built and deployed on PC, mobile, and edge devices.

Result: The model was successfully deployed and its performance assessed using suitable metrics.

Conclusion: The Hello Afrika project provides a foundational step toward speech command models for African languages, with Kinyarwanda as the first case study.

Abstract: Voice or Speech Commands are a subset of the broader Spoken Word Corpus of a
language which are essential for non-contact control of and activation of
larger AI systems in devices used in everyday life especially for persons with
disabilities. Currently, there is a dearth of speech command models for African
languages. The Hello Afrika project aims to address this issue and its first
iteration is focused on the Kinyarwanda language since the country has shown
interest in developing speech recognition technologies culminating in one of
the largest datasets on Mozilla Common Voice. The model was built off a custom
speech command corpus made up of general directives, numbers, and a wake word.
The final model was deployed on multiple devices (PC, Mobile Phone and Edge
Devices) and the performance was assessed using suitable metrics.

</details>


### [218] [Classical Guitar Duet Separation using GuitarDuets -- a Dataset of Real and Synthesized Guitar Recordings](https://arxiv.org/pdf/2507.01172)
*Marios Glytsos, Christos Garoufis, Athanasia Zlatintsi, Petros Maragos*

Main category: eess.AS

TL;DR: The paper addresses monotimbral music source separation (MSS) in classical guitar duets, introducing the GuitarDuets dataset and adapting Demucs for improved performance.


<details>
  <summary>Details</summary>
Motivation: Existing MSS architectures focus on multi-timbral cases, neglecting instruments with similar timbres, like classical guitar duets.

Method: The study introduces the GuitarDuets dataset, adapts Demucs for monotimbral MSS, and develops a joint transcription-separation framework.

Result: Combining real and synthesized data improves separation. Ground-truth note labels aid performance, but predicted notes offer marginal gains.

Conclusion: The work highlights the challenges of monotimbral MSS and evaluates metrics like SDR and SI-SDR in this context.

Abstract: Recent advancements in music source separation (MSS) have focused in the
multi-timbral case, with existing architectures tailored for the separation of
distinct instruments, overlooking thus the challenge of separating instruments
with similar timbral characteristics. Addressing this gap, our work focuses on
monotimbral MSS, specifically within the context of classical guitar duets. To
this end, we introduce the GuitarDuets dataset, featuring a combined total of
approximately three hours of real and synthesized classical guitar duet
recordings, as well as note-level annotations of the synthesized duets. We
perform an extensive cross-dataset evaluation by adapting Demucs, a
state-of-the-art MSS architecture, to monotimbral source separation.
Furthermore, we develop a joint permutation-invariant transcription and
separation framework, to exploit note event predictions as auxiliary
information. Our results indicate that utilizing both the real and synthesized
subsets of GuitarDuets leads to improved separation performance in an
independently recorded test set compared to utilizing solely one subset. We
also find that while the availability of ground-truth note labels greatly helps
the performance of the separation network, the predicted note estimates result
only in marginal improvement. Finally, we discuss the behavior of commonly
utilized metrics, such as SDR and SI-SDR, in the context of monotimbral MSS.

</details>


### [219] [SpeechAccentLLM: A Unified Framework for Foreign Accent Conversion and Text to Speech](https://arxiv.org/pdf/2507.01348)
*Cheng Zhuangfei, Zhang Guangyan, Tu Zehai, Song Yangyang, Mao Shuiyang, Jiao Xiaoqi, Li Jingyu, Guo Yiwen, Wu Jiasong*

Main category: eess.AS

TL;DR: The paper introduces SpeechAccentLLM, a framework for foreign accent conversion (FAC) using LLM-based techniques, featuring SpeechCodeVAE for tokenization and SpeechRestorer for refining outputs.


<details>
  <summary>Details</summary>
Motivation: FAC is challenging, and the study aims to leverage LLM success in TTS for FAC, addressing data scarcity and improving speech quality.

Method: The framework integrates CTC into codebook discretization (SpeechCodeVAE), uses multitask learning for FAC and TTS, and introduces SpeechRestorer for postprocessing.

Result: Experiments show optimal trade-offs in token properties, accelerated convergence, superior speech quality, and improved prosodic continuity.

Conclusion: The proposed framework effectively addresses FAC challenges, demonstrating improved performance and robustness.

Abstract: Foreign accent conversion (FAC) in speech processing remains a challenging
task. Building on the remarkable success of large language models (LLMs) in
Text-to-Speech (TTS) tasks, this study investigates the adaptation of LLM-based
techniques for FAC, which we term SpeechAccentLLM. At the core of this
framework, we introduce SpeechCodeVAE, the first model to integrate
connectionist temporal classification (CTC) directly into codebook
discretization for speech content tokenization. This novel architecture
generates tokens with a unique "locality" property, as validated by experiments
demonstrating optimal trade-offs among content faithfulness, temporal
coherence, and structural recoverability. Then, to address data scarcity for
the FAC module, we adopted a multitask learning strategy that jointly trains
the FAC and TTS modules. Beyond mitigating data limitations, this approach
yielded accelerated convergence and superior speech quality compared to
standalone FAC training. Moreover, leveraging the salient properties of our
discrete speech representations, we introduce SpeechRestorer, a postprocessing
architecture designed to refine LLM-generated outputs. This module effectively
mitigates stochastic errors prevalent in LLM inference pipelines while
enhancing prosodic continuity, as validated by ablation experiments.

</details>


### [220] [IdolSongsJp Corpus: A Multi-Singer Song Corpus in the Style of Japanese Idol Groups](https://arxiv.org/pdf/2507.01349)
*Hitoshi Suda, Junya Koguchi, Shunsuke Yoshida, Tomohiko Nakamura, Satoru Fukayama, Jun Ogata*

Main category: eess.AS

TL;DR: A corpus of Japanese idol group songs, IdolSongsJp, was created to benchmark music processing techniques like singer diarization and chord estimation.


<details>
  <summary>Details</summary>
Motivation: Japanese idol group songs, with their complex structures and high loudness, present challenges for music information processing, necessitating a dedicated corpus for benchmarking.

Method: Professional composers created 15 tracks mimicking idol group styles, including mastered audio, stems, dry vocals, and chord annotations.

Result: The corpus, IdolSongsJp, demonstrates diversity comparable to real-world idol songs and is applied to evaluate music processing techniques.

Conclusion: IdolSongsJp serves as a valuable resource for testing and advancing music information processing under challenging conditions.

Abstract: Japanese idol groups, comprising performers known as "idols," are an
indispensable part of Japanese pop culture. They frequently appear in live
concerts and television programs, entertaining audiences with their singing and
dancing. Similar to other J-pop songs, idol group music covers a wide range of
styles, with various types of chord progressions and instrumental arrangements.
These tracks often feature numerous instruments and employ complex mastering
techniques, resulting in high signal loudness. Additionally, most songs include
a song division (utawari) structure, in which members alternate between singing
solos and performing together. Hence, these songs are well-suited for
benchmarking various music information processing techniques such as singer
diarization, music source separation, and automatic chord estimation under
challenging conditions. Focusing on these characteristics, we constructed a
song corpus titled IdolSongsJp by commissioning professional composers to
create 15 tracks in the style of Japanese idol groups. This corpus includes not
only mastered audio tracks but also stems for music source separation, dry
vocal tracks, and chord annotations. This paper provides a detailed description
of the corpus, demonstrates its diversity through comparisons with real-world
idol group songs, and presents its application in evaluating several music
information processing techniques.

</details>


### [221] [Voice Conversion for Likability Control via Automated Rating of Speech Synthesis Corpora](https://arxiv.org/pdf/2507.01356)
*Hitoshi Suda, Shinnosuke Takamichi, Satoru Fukayama*

Main category: eess.AS

TL;DR: A voice conversion method controls voice likability while preserving speaker identity and content, using a likability predictor trained on existing data.


<details>
  <summary>Details</summary>
Motivation: Voice likability is key in social interactions; a system for tailored voice samples could improve communication.

Method: Train a likability predictor on existing data, annotate a large corpus, and use it for voice conversion.

Result: Predictor outputs correlate with human ratings; method effectively controls likability without losing identity/content.

Conclusion: The approach successfully adjusts voice likability while maintaining speaker identity and linguistic content.

Abstract: Perceived voice likability plays a crucial role in various social
interactions, such as partner selection and advertising. A system that provides
reference likable voice samples tailored to target audiences would enable users
to adjust their speaking style and voice quality, facilitating smoother
communication. To this end, we propose a voice conversion method that controls
the likability of input speech while preserving both speaker identity and
linguistic content. To improve training data scalability, we train a likability
predictor on an existing voice likability dataset and employ it to
automatically annotate a large speech synthesis corpus with likability ratings.
Experimental evaluations reveal a significant correlation between the
predictor's outputs and human-provided likability ratings. Subjective and
objective evaluations further demonstrate that the proposed approach
effectively controls voice likability while preserving both speaker identity
and linguistic content.

</details>


### [222] [QHARMA-GAN: Quasi-Harmonic Neural Vocoder based on Autoregressive Moving Average Model](https://arxiv.org/pdf/2507.01611)
*Shaowen Chen, Tomoki Toda*

Main category: eess.AS

TL;DR: A novel neural vocoder framework combines QHM and ARMA models with neural networks for high-quality, flexible speech synthesis with reduced time and complexity.


<details>
  <summary>Details</summary>
Motivation: Existing neural vocoders lack interpretability and flexibility in speech modification, and suffer from high computational costs.

Method: Proposes a framework integrating QHM and ARMA models with neural networks to encode speech into ARMA functions for accurate harmonic modeling.

Result: Outperforms other methods in speed, quality, and flexibility for speech synthesis and modification.

Conclusion: The hybrid approach leverages QHM and ARMA strengths, offering efficient, high-quality speech synthesis and modification.

Abstract: Vocoders, encoding speech signals into acoustic features and allowing for
speech signal reconstruction from them, have been studied for decades.
Recently, the rise of deep learning has particularly driven the development of
neural vocoders to generate high-quality speech signals. On the other hand, the
existing end-to-end neural vocoders suffer from a black-box nature that blinds
the speech production mechanism and the intrinsic structure of speech,
resulting in the ambiguity of separately modeling source excitation and
resonance characteristics and the loss of flexibly synthesizing or modifying
speech with high quality. Moreover, their sequence-wise waveform generation
usually requires complicated networks, leading to substantial time consumption.
In this work, inspired by the quasi-harmonic model (QHM) that represents speech
as sparse components, we combine the neural network and QHM synthesis process
to propose a novel framework for the neural vocoder. Accordingly, speech
signals can be encoded into autoregressive moving average (ARMA) functions to
model the resonance characteristics, yielding accurate estimates of the
amplitudes and phases of quasi-harmonics at any frequency. Subsequently, the
speech can be resynthesized and arbitrarily modified in terms of pitch shifting
and time stretching with high quality, whereas the time consumption and network
size decrease. The experiments indicate that the proposed method leverages the
strengths of QHM, the ARMA model, and neural networks, leading to the
outperformance of our methods over other methods in terms of generation speed,
synthesis quality, and modification flexibility.

</details>


### [223] [Generalizable Detection of Audio Deepfakes](https://arxiv.org/pdf/2507.01750)
*Jose A. Lopez, Georg Stemmer, Héctor Cordourier Maruri*

Main category: eess.AS

TL;DR: Study improves audio deepfake detection models' generalization using pre-trained backbones, data augmentation, and loss functions, outperforming ASVspoof 5's top system.


<details>
  <summary>Details</summary>
Motivation: Enhance generalization of audio deepfake detection models for robust performance across diverse datasets.

Method: Evaluated pre-trained backbones (Wav2Vec2, WavLM, Whisper) with varied data augmentation and loss functions.

Result: Achieved significant generalization improvements, surpassing ASVspoof 5 Challenge's top system.

Conclusion: Provides insights for optimizing audio models and advancing deepfake detection research.

Abstract: In this paper, we present our comprehensive study aimed at enhancing the
generalization capabilities of audio deepfake detection models. We investigate
the performance of various pre-trained backbones, including Wav2Vec2, WavLM,
and Whisper, across a diverse set of datasets, including those from the
ASVspoof challenges and additional sources. Our experiments focus on the
effects of different data augmentation strategies and loss functions on model
performance. The results of our research demonstrate substantial enhancements
in the generalization capabilities of audio deepfake detection models,
surpassing the performance of the top-ranked single system in the ASVspoof 5
Challenge. This study contributes valuable insights into the optimization of
audio models for more robust deepfake detection and facilitates future research
in this critical area.

</details>


### [224] [Low-Complexity Neural Wind Noise Reduction for Audio Recordings](https://arxiv.org/pdf/2507.01821)
*Hesam Eftekhari, Srikanth Raj Chetupalli, Shrishti Saha Shetu, Emanuël A. P. Habets, Oliver Thiergart*

Main category: eess.AS

TL;DR: A low-complexity DNN for real-time wind noise suppression on resource-constrained devices, achieving performance close to state-of-the-art with minimal parameters and computational power.


<details>
  <summary>Details</summary>
Motivation: Wind noise degrades outdoor audio quality, and existing solutions struggle with real-time suppression on constrained devices.

Method: Proposes a single-channel DNN leveraging wind noise's spectral characteristics, designed for low complexity (249K parameters, ~73 MHz).

Result: Performance comparable to state-of-the-art ULCNet, suitable for embedded/mobile applications.

Conclusion: The model effectively suppresses wind noise with minimal resource usage, making it practical for real-time applications.

Abstract: Wind noise significantly degrades the quality of outdoor audio recordings,
yet remains difficult to suppress in real-time on resource-constrained devices.
In this work, we propose a low-complexity single-channel deep neural network
that leverages the spectral characteristics of wind noise. Experimental results
show that our method achieves performance comparable to the state-of-the-art
low-complexity ULCNet model. The proposed model, with only 249K parameters and
roughly 73 MHz of computational power, is suitable for embedded and mobile
audio applications.

</details>


### [225] [First Steps Towards Voice Anonymization for Code-Switching Speech](https://arxiv.org/pdf/2507.01765)
*Sarina Meyer, Ekaterina Kolos, Ngoc Thang Vu*

Main category: eess.AS

TL;DR: The paper investigates voice anonymization for multilingual code-switching speech, proposing adaptations to a multilingual model and evaluating its performance against language-independent methods.


<details>
  <summary>Details</summary>
Motivation: Current voice anonymization research is limited to English read speech, leaving the efficacy for multilingual code-switching speech unexplored.

Method: Two corpora are prepared, and a multilingual anonymization model is adapted for code-switching speech. Performance is tested against language-independent methods.

Result: Only the multilingual system performs well in privacy and utility preservation. Utility evaluations face challenges due to spontaneous speech and limited code-switching support in recognition models.

Conclusion: The study highlights the effectiveness of multilingual models for code-switching speech anonymization but notes evaluation challenges.

Abstract: The goal of voice anonymization is to modify an audio such that the true
identity of its speaker is hidden. Research on this task is typically limited
to the same English read speech datasets, thus the efficacy of current methods
for other types of speech data remains unknown. In this paper, we present the
first investigation of voice anonymization for the multilingual phenomenon of
code-switching speech. We prepare two corpora for this task and propose
adaptations to a multilingual anonymization model to make it applicable for
code-switching speech. By testing the anonymization performance of this and two
language-independent methods on the datasets, we find that only the
multilingual system performs well in terms of privacy and utility preservation.
Furthermore, we observe challenges in performing utility evaluations on this
data because of its spontaneous character and the limited code-switching
support by the multilingual speech recognition model.

</details>


### [226] [Perceptual Ratings Predict Speech Inversion Articulatory Kinematics in Childhood Speech Sound Disorders](https://arxiv.org/pdf/2507.01888)
*Nina R. Benway, Saba Tabatabaee, Dongliang Wang, Benjamin Munson, Jonathan L. Preston, Carol Espy-Wilson*

Main category: eess.AS

TL;DR: The study evaluated if articulatory kinematics, inferred via Articulatory Phonology neural networks, matched perceptual ratings of /r/ and /s/ in children with speech sound disorders. Results showed alignment for /r/ and partial alignment for /s/, supporting clinical interpretability.


<details>
  <summary>Details</summary>
Motivation: To assess whether inferred articulatory patterns align with perceptual error categories and gradient ratings, aiding clinical diagnosis and intervention for speech sound disorders.

Method: Articulatory Phonology vocal tract variables were inferred for 5,961 utterances from 118 children and 3 adults. Perceptual ratings used the PERCEPT Rating Scale. Linear mixed models tested alignment of articulatory patterns with perceptual categories and gradient scores.

Result: For /r/, 17 of 18 hypotheses were supported, involving tongue tip/body constrictions. For /s/, 7 of 15 hypotheses were supported, mainly tongue tip-related. PERCEPT scores predicted articulatory proximity to correct productions.

Conclusion: Inferred vocal tract variables aligned with perceptual judgments, especially for /r/, supporting their clinical utility and the PERCEPT Rating Scale's effectiveness in quantifying articulatory errors.

Abstract: Purpose: This study evaluated whether articulatory kinematics, inferred by
Articulatory Phonology speech inversion neural networks, aligned with
perceptual ratings of /r/ and /s/ in the speech of children with speech sound
disorders.
  Methods: Articulatory Phonology vocal tract variables were inferred for 5,961
utterances from 118 children and 3 adults, aged 2.25-45 years. Perceptual
ratings were standardized using the novel 5-point PERCEPT Rating Scale and
training protocol. Two research questions examined if the articulatory patterns
of inferred vocal tract variables aligned with the perceptual error category
for the phones investigated (e.g., tongue tip is more anterior in dentalized
/s/ productions than in correct /s/). A third research question examined if
gradient PERCEPT Rating Scale scores predicted articulatory proximity to
correct productions.
  Results: Estimated marginal means from linear mixed models supported 17 of 18
/r/ hypotheses, involving tongue tip and tongue body constrictions. For /s/,
estimated marginal means from a second linear mixed model supported 7 of 15
hypotheses, particularly those related to the tongue tip. A third linear mixed
model revealed that PERCEPT Rating Scale scores significantly predicted
articulatory proximity of errored phones to correct productions.
  Conclusion: Inferred vocal tract variables differentiated category and
magnitude of articulatory errors for /r/, and to a lesser extent for /s/,
aligning with perceptual judgments. These findings support the clinical
interpretability of speech inversion vocal tract variables and the PERCEPT
Rating Scale in quantifying articulatory proximity to the target sound,
particularly for /r/.

</details>


### [227] [A Survey on Speech Large Language Models for Understanding](https://arxiv.org/pdf/2410.18908)
*Jing Peng, Yucheng Wang, Bohan Li, Yiwei Guo, Hankun Wang, Yangui Fang, Yu Xi, Haoyu Li, Xu Li, Ke Zhang, Shuai Wang, Kai Yu*

Main category: eess.AS

TL;DR: The paper defines speech understanding, introduces a taxonomy, reviews Speech LLMs, and identifies challenges like instruction sensitivity and semantic reasoning degradation.


<details>
  <summary>Details</summary>
Motivation: To clarify and systematize speech understanding for human-computer interaction, leveraging advancements in LLMs.

Method: Defines speech understanding, introduces a taxonomy, reviews Speech LLMs' architectures, training, datasets, and evaluation.

Result: Identifies challenges: instruction sensitivity and semantic reasoning degradation in Speech LLMs.

Conclusion: Provides a foundational reference for robust, generalizable, and human-aligned Speech LLMs.

Abstract: Speech understanding is essential for interpreting the diverse forms of
information embedded in spoken language, including linguistic, paralinguistic,
and non-linguistic cues that are vital for effective human-computer
interaction. The rapid advancement of large language models (LLMs) has
catalyzed the emergence of Speech Large Language Models (Speech LLMs), which
marks a transformative shift toward general-purpose speech understanding
systems. To further clarify and systematically delineate task objectives, in
this paper, we formally define the concept of speech understanding and
introduce a structured taxonomy encompassing its informational, functional, and
format dimensions. Within this scope of definition, we present a comprehensive
review of current Speech LLMs, analyzing their architectures through a
three-stage abstraction: Modality Feature Extraction, Modality Information
Fusion, and LLM Inference. In addition, we examine training strategies, discuss
representative datasets, and review evaluation methodologies adopted in the
field. Based on empirical analyses and experimental evidence, we identify two
key challenges currently facing Speech LLMs: instruction sensitivity and
degradation in semantic reasoning and propose concrete directions for
addressing these issues. Through this systematic and detailed survey, we aim to
offer a foundational reference for researchers and practitioners working toward
more robust, generalizable, and human-aligned Speech LLMs.

</details>


### [228] [PSELDNets: Pre-trained Neural Networks on a Large-scale Synthetic Dataset for Sound Event Localization and Detection](https://arxiv.org/pdf/2411.06399)
*Jinbo Hu, Yin Cao, Ming Wu, Fang Kang, Feiran Yang, Wenwu Wang, Mark D. Plumbley, Jun Yang*

Main category: eess.AS

TL;DR: The paper proposes pre-trained SELD networks (PSELDNets) using a large-scale synthetic dataset and introduces AdapterBit for efficient fine-tuning in low-resource scenarios, achieving state-of-the-art performance.


<details>
  <summary>Details</summary>
Motivation: To extend advancements in sound event classification (SEC) to sound event localization and detection (SELD) by leveraging pre-trained models and synthetic data.

Method: Develop PSELDNets using a synthetic dataset (1,167 hours, 170 classes) and fine-tune with AdapterBit for low-resource scenarios.

Result: PSELDNets outperform state-of-the-art systems on synthetic and real-world datasets, even with minimal multi-channel or monophonic audio.

Conclusion: PSELDNets with AdapterBit offer efficient adaptability and superior performance, advancing SELD capabilities.

Abstract: Sound event localization and detection (SELD) has seen substantial
advancements through learning-based methods. These systems, typically trained
from scratch on specific datasets, have shown considerable generalization
capabilities. Recently, deep neural networks trained on large-scale datasets
have achieved remarkable success in the sound event classification (SEC) field,
prompting an open question of whether these advances can be extended to the
development of SELD foundation models. In this paper, leveraging the power of
pre-trained SEC models, we propose pre-trained SELD networks (PSELDNets) on a
large-scale synthetic dataset. The synthetic dataset, generated by convolving
sound events with simulated spatial room impulse responses (SRIRs), contains
1,167 hours of audio clips with an ontology of 170 sound classes. These
PSELDNets are applied to various SELD scenarios. When we adapt PSELDNets to
specific scenarios, particularly in cases of low-resource data, we introduce a
data-efficient fine-tuning method, AdapterBit. PSELDNets are evaluated on
synthetic-test-set using collected SRIRs from the TAU Spatial Room Impulse
Response Database (TAU-SRIR DB) and achieve satisfactory performance. We also
carried out experiments to validate the transferability of PSELDNets to three
publicly available datasets and our own real-world recordings. The results
demonstrate that PSELDNets surpass state-of-the-art systems across all publicly
available datasets. Given the need for direction-of-arrival estimation, SELD
generally relies on sufficient multi-channel audio clips. However,
incorporating the AdapterBit, PSELDNets show more efficient adaptability to
various scenarios using minimal multi-channel or even just monophonic audio
clips, outperforming traditional fine-tuning approaches.

</details>


<div id='eess.IV'></div>

# eess.IV [[Back]](#toc)

### [229] [Prompt Mechanisms in Medical Imaging: A Comprehensive Survey](https://arxiv.org/pdf/2507.01055)
*Hao Yang, Xinlong Liang, Zhang Li, Yue Sun, Zheyu Hu, Xinghe Xie, Behdad Dashtbozorg, Jincheng Huang, Shiwei Zhu, Luyi Han, Jiong Zhang, Shanshan Wang, Ritse Mann, Qifeng Yu, Tao Tan*

Main category: eess.IV

TL;DR: Prompt-based methodologies enhance deep learning in medical imaging by improving adaptability, accuracy, and efficiency, though challenges like data heterogeneity and scalability remain.


<details>
  <summary>Details</summary>
Motivation: Address challenges in clinical adoption of deep learning (e.g., data scarcity, distribution shifts) using prompt-based strategies.

Method: Systematic review of prompt engineering, analyzing textual, visual, and learnable embeddings for tasks like image generation, segmentation, and classification.

Result: Prompts improve accuracy, robustness, and interpretability while reducing manual feature engineering. Challenges include prompt design and scalability.

Conclusion: Prompt-driven AI holds promise for revolutionizing diagnostics and personalized medicine, with future work needed in multimodal prompting and clinical integration.

Abstract: Deep learning offers transformative potential in medical imaging, yet its
clinical adoption is frequently hampered by challenges such as data scarcity,
distribution shifts, and the need for robust task generalization. Prompt-based
methodologies have emerged as a pivotal strategy to guide deep learning models,
providing flexible, domain-specific adaptations that significantly enhance
model performance and adaptability without extensive retraining. This
systematic review critically examines the burgeoning landscape of prompt
engineering in medical imaging. We dissect diverse prompt modalities, including
textual instructions, visual prompts, and learnable embeddings, and analyze
their integration for core tasks such as image generation, segmentation, and
classification. Our synthesis reveals how these mechanisms improve
task-specific outcomes by enhancing accuracy, robustness, and data efficiency
and reducing reliance on manual feature engineering while fostering greater
model interpretability by making the model's guidance explicit. Despite
substantial advancements, we identify persistent challenges, particularly in
prompt design optimization, data heterogeneity, and ensuring scalability for
clinical deployment. Finally, this review outlines promising future
trajectories, including advanced multimodal prompting and robust clinical
integration, underscoring the critical role of prompt-driven AI in accelerating
the revolution of diagnostics and personalized treatment planning in medicine.

</details>


### [230] [MID-INFRARED (MIR) OCT-based inspection in industry](https://arxiv.org/pdf/2507.01074)
*N. P. García-de-la-Puente, Rocío del Amor, Fernando García-Torres, Niels Møller Israelsen, Coraline Lapre, Christian Rosenberg Petersen, Ole Bang, Dominik Brouczek, Martin Schwentenwein, Kevin Neumann, Niels Benson, Valery Naranjo*

Main category: eess.IV

TL;DR: The paper evaluates MIR OCT systems for detecting sub-surface irregularities in materials like composites and ceramics, using preprocessing and AI-enhanced vision algorithms for anomaly detection.


<details>
  <summary>Details</summary>
Motivation: To provide non-destructive inspection techniques for industrial production monitoring by assessing MIR OCT systems.

Method: Exploratory study involving acquisitions on composites and ceramics, with preprocessing and AI-enhanced vision algorithms for anomaly detection.

Result: Identifies capabilities, limitations, and optimal parameters for MIR OCT systems in detecting sub-surface irregularities.

Conclusion: MIR OCT systems, combined with AI-enhanced algorithms, show promise for industrial non-destructive inspection, though limitations and parameter selection criteria must be considered.

Abstract: This paper aims to evaluate mid-infrared (MIR) Optical Coherence Tomography
(OCT) systems as a tool to penetrate different materials and detect sub-surface
irregularities. This is useful for monitoring production processes, allowing
Non-Destructive Inspection Techniques of great value to the industry. In this
exploratory study, several acquisitions are made on composite and ceramics to
know the capabilities of the system. In addition, it is assessed which
preprocessing and AI-enhanced vision algorithms can be anomaly-detection
methodologies capable of detecting abnormal zones in the analyzed objects.
Limitations and criteria for the selection of optimal parameters will be
discussed, as well as strengths and weaknesses will be highlighted.

</details>


### [231] [LotteryCodec: Searching the Implicit Representation in a Random Network for Low-Complexity Image Compression](https://arxiv.org/pdf/2507.01204)
*Haotian Wu, Gongpu Chen, Pier Luigi Dragotti, Deniz Gündüz*

Main category: eess.IV

TL;DR: Untrained subnetworks in random networks can achieve image compression performance like trained networks, leading to LotteryCodec, a new method that outperforms VTM.


<details>
  <summary>Details</summary>
Motivation: To explore if untrained subnetworks can match trained networks in image compression, enabling efficient and flexible solutions.

Method: Proposes LotteryCodec, which overfits a binary mask to an image using a shared, over-parameterized random network, with a rewind modulation mechanism for efficiency.

Result: LotteryCodec outperforms VTM, setting a new state-of-the-art in single-image compression and offering adaptive decoding complexity.

Conclusion: The lottery codec hypothesis is validated, showing untrained subnetworks can rival trained ones, enabling innovative compression solutions.

Abstract: We introduce and validate the lottery codec hypothesis, which states that
untrained subnetworks within randomly initialized networks can serve as
synthesis networks for overfitted image compression, achieving rate-distortion
(RD) performance comparable to trained networks. This hypothesis leads to a new
paradigm for image compression by encoding image statistics into the network
substructure. Building on this hypothesis, we propose LotteryCodec, which
overfits a binary mask to an individual image, leveraging an over-parameterized
and randomly initialized network shared by the encoder and the decoder. To
address over-parameterization challenges and streamline subnetwork search, we
develop a rewind modulation mechanism that improves the RD performance.
LotteryCodec outperforms VTM and sets a new state-of-the-art in single-image
compression. LotteryCodec also enables adaptive decoding complexity through
adjustable mask ratios, offering flexible compression solutions for diverse
device constraints and application requirements.

</details>


### [232] [Classification based deep learning models for lung cancer and disease using medical images](https://arxiv.org/pdf/2507.01279)
*Ahmad Chaddad, Jihao Peng, Yihang Wu*

Main category: eess.IV

TL;DR: A novel CNN model, ResNet+, improves lung cancer prediction by integrating ResNet-D and attention modules, achieving high accuracy and computational efficiency.


<details>
  <summary>Details</summary>
Motivation: To enhance lung cancer and disease prediction in medical images by addressing feature loss during downsampling and improving model generalization.

Method: ResNet+ integrates ResNet-D for better feature extraction and a convolutional attention module to focus on relevant image regions. Evaluated on five datasets with data augmentation for class imbalance.

Result: Achieved 98.14%/98.14% (accuracy/F1) on LC2500 and 99.25%/99.13% on IQ-OTH/NCCD, with reduced computational cost.

Conclusion: ResNet+ outperforms baseline models, offering improved performance and efficiency for lung cancer image analysis.

Abstract: The use of deep learning (DL) in medical image analysis has significantly
improved the ability to predict lung cancer. In this study, we introduce a
novel deep convolutional neural network (CNN) model, named ResNet+, which is
based on the established ResNet framework. This model is specifically designed
to improve the prediction of lung cancer and diseases using the images. To
address the challenge of missing feature information that occurs during the
downsampling process in CNNs, we integrate the ResNet-D module, a variant
designed to enhance feature extraction capabilities by modifying the
downsampling layers, into the traditional ResNet model. Furthermore, a
convolutional attention module was incorporated into the bottleneck layers to
enhance model generalization by allowing the network to focus on relevant
regions of the input images. We evaluated the proposed model using five public
datasets, comprising lung cancer (LC2500 $n$=3183, IQ-OTH/NCCD $n$=1336, and
LCC $n$=25000 images) and lung disease (ChestXray $n$=5856, and COVIDx-CT
$n$=425024 images). To address class imbalance, we used data augmentation
techniques to artificially increase the representation of underrepresented
classes in the training dataset. The experimental results show that ResNet+
model demonstrated remarkable accuracy/F1, reaching 98.14/98.14\% on the
LC25000 dataset and 99.25/99.13\% on the IQ-OTH/NCCD dataset. Furthermore, the
ResNet+ model saved computational cost compared to the original ResNet series
in predicting lung cancer images. The proposed model outperformed the baseline
models on publicly available datasets, achieving better performance metrics.
Our codes are publicly available at
https://github.com/AIPMLab/Graduation-2024/tree/main/Peng.

</details>


### [233] [PanTS: The Pancreatic Tumor Segmentation Dataset](https://arxiv.org/pdf/2507.01291)
*Wenxuan Li, Xinze Zhou, Qi Chen, Tianyu Lin, Pedro R. A. S. Bassi, Szymon Plotka, Jaroslaw B. Cwikla, Xiaoxi Chen, Chen Ye, Zheren Zhu, Kai Ding, Heng Li, Kang Wang, Yang Yang, Yucheng Tang, Daguang Xu, Alan L. Yuille, Zongwei Zhou*

Main category: eess.IV

TL;DR: PanTS is a large-scale dataset for pancreatic CT analysis with 36,390 scans and 993,000 expert-annotated structures, improving AI model performance in tumor detection and segmentation.


<details>
  <summary>Details</summary>
Motivation: To advance research in pancreatic CT analysis by providing a comprehensive, large-scale dataset with detailed annotations and metadata.

Method: Curated 36,390 CT scans from 145 medical centers, including voxel-wise annotations of tumors and 24 surrounding structures, along with metadata.

Result: AI models trained on PanTS outperform those on existing datasets, attributed to larger-scale annotations and additional anatomical structures.

Conclusion: PanTS sets a new benchmark for AI model development and evaluation in pancreatic CT analysis.

Abstract: PanTS is a large-scale, multi-institutional dataset curated to advance
research in pancreatic CT analysis. It contains 36,390 CT scans from 145
medical centers, with expert-validated, voxel-wise annotations of over 993,000
anatomical structures, covering pancreatic tumors, pancreas head, body, and
tail, and 24 surrounding anatomical structures such as vascular/skeletal
structures and abdominal/thoracic organs. Each scan includes metadata such as
patient age, sex, diagnosis, contrast phase, in-plane spacing, slice thickness,
etc. AI models trained on PanTS achieve significantly better performance in
pancreatic tumor detection, localization, and segmentation compared to those
trained on existing public datasets. Our analysis indicates that these gains
are directly attributable to the 16x larger-scale tumor annotations and
indirectly supported by the 24 additional surrounding anatomical structures. As
the largest and most comprehensive resource of its kind, PanTS offers a new
benchmark for developing and evaluating AI models in pancreatic CT analysis.

</details>


### [234] [SWinMamba: Serpentine Window State Space Model for Vascular Segmentation](https://arxiv.org/pdf/2507.01323)
*Rongchang Zhao, Huanchi Liu, Jian Zhang*

Main category: eess.IV

TL;DR: SWinMamba, a novel method for vascular segmentation, uses serpentine window sequences and bidirectional state space models to ensure continuity in slender vascular structures, outperforming existing methods.


<details>
  <summary>Details</summary>
Motivation: Discontinuous vascular segmentation in medical images hinders disease diagnosis and surgical navigation, necessitating improved modeling of slender structures.

Method: SWinMamba incorporates serpentine window sequences (SWToken) for adaptive feature capture and a Bidirectional Aggregation Module (BAM) for continuity. Dual-domain learning with SFFU enhances feature representation.

Result: SWinMamba achieves superior performance on three datasets, producing complete and connected vascular structures.

Conclusion: The proposed SWinMamba effectively addresses vascular discontinuity, offering a robust solution for medical image segmentation.

Abstract: Vascular segmentation in medical images is crucial for disease diagnosis and
surgical navigation. However, the segmented vascular structure is often
discontinuous due to its slender nature and inadequate prior modeling. In this
paper, we propose a novel Serpentine Window Mamba (SWinMamba) to achieve
accurate vascular segmentation. The proposed SWinMamba innovatively models the
continuity of slender vascular structures by incorporating serpentine window
sequences into bidirectional state space models. The serpentine window
sequences enable efficient feature capturing by adaptively guiding global
visual context modeling to the vascular structure. Specifically, the Serpentine
Window Tokenizer (SWToken) adaptively splits the input image using overlapping
serpentine window sequences, enabling flexible receptive fields (RFs) for
vascular structure modeling. The Bidirectional Aggregation Module (BAM)
integrates coherent local features in the RFs for vascular continuity
representation. In addition, dual-domain learning with Spatial-Frequency Fusion
Unit (SFFU) is designed to enhance the feature representation of vascular
structure. Extensive experiments on three challenging datasets demonstrate that
the proposed SWinMamba achieves superior performance with complete and
connected vessels.

</details>


### [235] [Structure and Smoothness Constrained Dual Networks for MR Bias Field Correction](https://arxiv.org/pdf/2507.01326)
*Dong Liang, Xingyu Qiu, Yuzhen Li, Wei Wang, Kuanquan Wang, Suyu Dong, Gongning Luo*

Main category: eess.IV

TL;DR: S2DNets, a novel dual-network model, addresses MR image intensity inhomogeneity by incorporating structural constraints and bias field smoothness, outperforming existing methods.


<details>
  <summary>Details</summary>
Motivation: Intensity inhomogeneity in MR images hampers diagnosis. Current deep learning models lack structural and smoothness constraints, leading to distorted corrections.

Method: Proposes S2DNets with piece-wise structural constraints and bias field smoothness for self-supervised correction.

Result: Outperforms conventional and deep learning models on clinical and simulated datasets, improving downstream segmentation tasks.

Conclusion: S2DNets effectively corrects inhomogeneity while preserving structural details, enhancing MR image analysis.

Abstract: MR imaging techniques are of great benefit to disease diagnosis. However, due
to the limitation of MR devices, significant intensity inhomogeneity often
exists in imaging results, which impedes both qualitative and quantitative
medical analysis. Recently, several unsupervised deep learning-based models
have been proposed for MR image improvement. However, these models merely
concentrate on global appearance learning, and neglect constraints from image
structures and smoothness of bias field, leading to distorted corrected
results. In this paper, novel structure and smoothness constrained dual
networks, named S2DNets, are proposed aiming to self-supervised bias field
correction. S2DNets introduce piece-wise structural constraints and smoothness
of bias field for network training to effectively remove non-uniform intensity
and retain much more structural details. Extensive experiments executed on both
clinical and simulated MR datasets show that the proposed model outperforms
other conventional and deep learning-based models. In addition to comparison on
visual metrics, downstream MR image segmentation tasks are also used to
evaluate the impact of the proposed model. The source code is available at:
https://github.com/LeongDong/S2DNets}{https://github.com/LeongDong/S2DNets.

</details>


### [236] [BronchoGAN: Anatomically consistent and domain-agnostic image-to-image translation for video bronchoscopy](https://arxiv.org/pdf/2507.01387)
*Ahmad Soliman, Ron Keuth, Marian Himstedt*

Main category: eess.IV

TL;DR: BronchoGAN uses anatomical constraints and foundation model-generated depth images for robust image-to-image translation across bronchoscopy domains, improving synthetic image quality and reducing reliance on specific datasets.


<details>
  <summary>Details</summary>
Motivation: The limited availability of bronchoscopy images hinders deep learning model training. Robust translation across diverse domains (virtual, phantom, in-vivo, ex-vivo) is crucial for clinical applications.

Method: Proposes BronchoGAN, integrating anatomical constraints (bronchial orifice matching) into a conditional GAN. Uses foundation model-generated depth images as intermediate representation for robustness and paired training data creation.

Result: Successful translation of varied input domains (e.g., virtual bronchoscopy, phantoms) to realistic human airway images. Improved FID, SSIM, and Dice scores (up to 0.43).

Conclusion: BronchoGAN bridges the gap of missing public bronchoscopy images by leveraging CT scan data, enabling large-scale dataset generation with realistic appearance.

Abstract: The limited availability of bronchoscopy images makes image synthesis
particularly interesting for training deep learning models. Robust image
translation across different domains -- virtual bronchoscopy, phantom as well
as in-vivo and ex-vivo image data -- is pivotal for clinical applications. This
paper proposes BronchoGAN introducing anatomical constraints for image-to-image
translation being integrated into a conditional GAN. In particular, we force
bronchial orifices to match across input and output images. We further propose
to use foundation model-generated depth images as intermediate representation
ensuring robustness across a variety of input domains establishing models with
substantially less reliance on individual training datasets. Moreover our
intermediate depth image representation allows to easily construct paired image
data for training. Our experiments showed that input images from different
domains (e.g. virtual bronchoscopy, phantoms) can be successfully translated to
images mimicking realistic human airway appearance. We demonstrated that
anatomical settings (i.e. bronchial orifices) can be robustly preserved with
our approach which is shown qualitatively and quantitatively by means of
improved FID, SSIM and dice coefficients scores. Our anatomical constraints
enabled an improvement in the Dice coefficient of up to 0.43 for synthetic
images. Through foundation models for intermediate depth representations,
bronchial orifice segmentation integrated as anatomical constraints into
conditional GANs we are able to robustly translate images from different
bronchoscopy input domains. BronchoGAN allows to incorporate public CT scan
data (virtual bronchoscopy) in order to generate large-scale bronchoscopy image
datasets with realistic appearance. BronchoGAN enables to bridge the gap of
missing public bronchoscopy images.

</details>


### [237] [Multi Source COVID-19 Detection via Kernel-Density-based Slice Sampling](https://arxiv.org/pdf/2507.01564)
*Chia-Ming Lee, Bo-Cheng Qiu, Ting-Yao Chen, Ming-Han Sun, Fang-Ying Lin, Jung-Tse Tsai, I-An Tsai, Yu-Fan Lin, Chih-Chung Hsu*

Main category: eess.IV

TL;DR: The paper presents a solution for COVID-19 detection in chest CT scans from multiple medical centers using the SSFL framework with KDS. EfficientNet outperforms Swin Transformer, achieving a 94.68% F1-score.


<details>
  <summary>Details</summary>
Motivation: To address multi-source variability in COVID-19 detection from chest CT scans across different medical centers.

Method: Uses the SSFL framework with KDS for preprocessing, including lung region extraction, quality control, and adaptive slice sampling. Compares EfficientNet and Swin Transformer architectures.

Result: EfficientNet achieves a higher F1-score (94.68%) than Swin Transformer (93.34%).

Conclusion: The KDS-based pipeline is effective for multi-source data, and dataset balance is crucial for multi-institutional medical imaging evaluation.

Abstract: We present our solution for the Multi-Source COVID-19 Detection Challenge,
which classifies chest CT scans from four distinct medical centers. To address
multi-source variability, we employ the Spatial-Slice Feature Learning (SSFL)
framework with Kernel-Density-based Slice Sampling (KDS). Our preprocessing
pipeline combines lung region extraction, quality control, and adaptive slice
sampling to select eight representative slices per scan. We compare
EfficientNet and Swin Transformer architectures on the validation set. The
EfficientNet model achieves an F1-score of 94.68%, compared to the Swin
Transformer's 93.34%. The results demonstrate the effectiveness of our
KDS-based pipeline on multi-source data and highlight the importance of dataset
balance in multi-institutional medical imaging evaluation.

</details>


### [238] [Enhancing Multi-Exposure High Dynamic Range Imaging with Overlapped Codebook for Improved Representation Learning](https://arxiv.org/pdf/2507.01588)
*Keuntek Lee, Jaehyun Park, Nam Ik Cho*

Main category: eess.IV

TL;DR: Proposes an Overlapped Codebook (OLC) scheme and a new HDR network to improve HDR imaging from LDR inputs, addressing motion discrepancies and saturation issues.


<details>
  <summary>Details</summary>
Motivation: To enhance HDR image reconstruction from LDR inputs by overcoming motion discrepancies and saturation problems in multi-exposure HDR imaging.

Method: Introduces an Overlapped Codebook (OLC) scheme for VQGAN to learn implicit HDR representations and a new HDR network leveraging pre-trained VQ and OLC.

Result: Outperforms previous methods in qualitative and quantitative evaluations on various datasets.

Conclusion: The proposed OLC and HDR network effectively improve HDR imaging quality by addressing key challenges in multi-exposure setups.

Abstract: High dynamic range (HDR) imaging technique aims to create realistic HDR
images from low dynamic range (LDR) inputs. Specifically, Multi-exposure HDR
imaging uses multiple LDR frames taken from the same scene to improve
reconstruction performance. However, there are often discrepancies in motion
among the frames, and different exposure settings for each capture can lead to
saturated regions. In this work, we first propose an Overlapped codebook (OLC)
scheme, which can improve the capability of the VQGAN framework for learning
implicit HDR representations by modeling the common exposure bracket process in
the shared codebook structure. Further, we develop a new HDR network that
utilizes HDR representations obtained from a pre-trained VQ network and OLC.
This allows us to compensate for saturated regions and enhance overall visual
quality. We have tested our approach extensively on various datasets and have
demonstrated that it outperforms previous methods both qualitatively and
quantitatively

</details>


### [239] [Robust brain age estimation from structural MRI with contrastive learning](https://arxiv.org/pdf/2507.01794)
*Carlo Alberto Barbano, Benoit Dufumier, Edouard Duchesnay, Marco Grangetto, Pietro Gori*

Main category: eess.IV

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: Estimating brain age from structural MRI has emerged as a powerful tool for
characterizing normative and pathological aging. In this work, we explore
contrastive learning as a scalable and robust alternative to supervised
approaches for brain age estimation. We introduce a novel contrastive loss
function, $\mathcal{L}^{exp}$, and evaluate it across multiple public
neuroimaging datasets comprising over 20,000 scans. Our experiments reveal four
key findings. First, scaling pre-training on diverse, multi-site data
consistently improves generalization performance, cutting external mean
absolute error (MAE) nearly in half. Second, $\mathcal{L}^{exp}$ is robust to
site-related confounds, maintaining low scanner-predictability as training size
increases. Third, contrastive models reliably capture accelerated aging in
patients with cognitive impairment and Alzheimer's disease, as shown through
brain age gap analysis, ROC curves, and longitudinal trends. Lastly, unlike
supervised baselines, $\mathcal{L}^{exp}$ maintains a strong correlation
between brain age accuracy and downstream diagnostic performance, supporting
its potential as a foundation model for neuroimaging. These results position
contrastive learning as a promising direction for building generalizable and
clinically meaningful brain representations.

</details>


### [240] [Autoadaptive Medical Segment Anything Model](https://arxiv.org/pdf/2507.01828)
*Tyler Ward, Meredith K. Owen, O'Kira Coleman, Brian Noehren, Abdullah-Al-Zubaer Imran*

Main category: eess.IV

TL;DR: ADA-SAM is a multitask learning framework for medical image segmentation that combines class activation maps and a gradient feedback mechanism to improve accuracy in limited-label settings.


<details>
  <summary>Details</summary>
Motivation: Manual annotation for medical image segmentation is costly and error-prone, necessitating annotation-efficient methods.

Method: ADA-SAM uses class activation maps from an auxiliary classifier to guide a semi-supervised segmentation branch and employs a gradient feedback mechanism to link segmentation and classification.

Result: ADA-SAM outperforms fully-supervised and semi-supervised baselines by double digits in limited-label settings.

Conclusion: ADA-SAM provides an accurate, automatic, and annotation-efficient solution for medical image segmentation.

Abstract: Medical image segmentation is a key task in the imaging workflow, influencing
many image-based decisions. Traditional, fully-supervised segmentation models
rely on large amounts of labeled training data, typically obtained through
manual annotation, which can be an expensive, time-consuming, and error-prone
process. This signals a need for accurate, automatic, and annotation-efficient
methods of training these models. We propose ADA-SAM (automated,
domain-specific, and adaptive segment anything model), a novel multitask
learning framework for medical image segmentation that leverages class
activation maps from an auxiliary classifier to guide the predictions of the
semi-supervised segmentation branch, which is based on the Segment Anything
(SAM) framework. Additionally, our ADA-SAM model employs a novel gradient
feedback mechanism to create a learnable connection between the segmentation
and classification branches by using the segmentation gradients to guide and
improve the classification predictions. We validate ADA-SAM on real-world
clinical data collected during rehabilitation trials, and demonstrate that our
proposed method outperforms both fully-supervised and semi-supervised baselines
by double digits in limited label settings. Our code is available at:
https://github.com/tbwa233/ADA-SAM.

</details>


### [241] [A computationally frugal open-source foundation model for thoracic disease detection in lung cancer screening programs](https://arxiv.org/pdf/2507.01881)
*Niccolò McConnell, Pardeep Vasudev, Daisuke Yamada, Daryl Cheng, Mehran Azimbagirad, John McCabe, Shahab Aslani, Ahmed H. Shahin, Yukun Zhou, The SUMMIT Consortium, Andre Altmann, Yipeng Hu, Paul Taylor, Sam M. Janes, Daniel C. Alexander, Joseph Jacob*

Main category: eess.IV

TL;DR: TANGERINE is an open-source, computationally efficient vision foundation model for LDCT analysis, enabling rapid fine-tuning for diverse lung disease tasks with minimal resources.


<details>
  <summary>Details</summary>
Motivation: The shortage of radiologists for interpreting LDCT scans in lung cancer screening programs necessitates an accessible, scalable solution for early-stage disease detection.

Method: TANGERINE uses a 3D masked autoencoder framework, pretrained via self-supervised learning on 98,000 LDCT scans, and fine-tuned for disease-specific tasks with limited data and computational resources.

Result: It achieves state-of-the-art performance across 14 disease classification tasks, including lung cancer, while generalizing well across clinical centers.

Conclusion: TANGERINE's lightweight, open-source design supports integration into medical imaging tools, expanding lung cancer screening to comprehensive respiratory disease management.

Abstract: Low-dose computed tomography (LDCT) imaging employed in lung cancer screening
(LCS) programs is increasing in uptake worldwide. LCS programs herald a
generational opportunity to simultaneously detect cancer and non-cancer-related
early-stage lung disease. Yet these efforts are hampered by a shortage of
radiologists to interpret scans at scale. Here, we present TANGERINE, a
computationally frugal, open-source vision foundation model for volumetric LDCT
analysis. Designed for broad accessibility and rapid adaptation, TANGERINE can
be fine-tuned off the shelf for a wide range of disease-specific tasks with
limited computational resources and training data. Relative to models trained
from scratch, TANGERINE demonstrates fast convergence during fine-tuning,
thereby requiring significantly fewer GPU hours, and displays strong label
efficiency, achieving comparable or superior performance with a fraction of
fine-tuning data. Pretrained using self-supervised learning on over 98,000
thoracic LDCTs, including the UK's largest LCS initiative to date and 27 public
datasets, TANGERINE achieves state-of-the-art performance across 14 disease
classification tasks, including lung cancer and multiple respiratory diseases,
while generalising robustly across diverse clinical centres. By extending a
masked autoencoder framework to 3D imaging, TANGERINE offers a scalable
solution for LDCT analysis, departing from recent closed, resource-intensive
models by combining architectural simplicity, public availability, and modest
computational requirements. Its accessible, open-source lightweight design lays
the foundation for rapid integration into next-generation medical imaging tools
that could transform LCS initiatives, allowing them to pivot from a singular
focus on lung cancer detection to comprehensive respiratory disease management
in high-risk populations.

</details>


### [242] [Diffusion-based Iterative Counterfactual Explanations for Fetal Ultrasound Image Quality Assessment](https://arxiv.org/pdf/2403.08700)
*Paraskevas Pegios, Manxi Lin, Nina Weng, Morten Bo Søndergaard Svendsen, Zahra Bashir, Siavash Bigdeli, Anders Nymark Christensen, Martin Tolsgaard, Aasa Feragen*

Main category: eess.IV

TL;DR: The paper proposes a diffusion-based AI method to enhance low-quality obstetric ultrasound images into high-quality standard planes, aiding diagnosis and clinician training.


<details>
  <summary>Details</summary>
Motivation: High-quality obstetric ultrasound images are essential for fetal health diagnosis but are hard to acquire due to variability in sonographer skill and external factors like maternal BMI.

Method: The authors use diffusion-based counterfactual explainable AI to transform low-quality non-standard ultrasound images into realistic high-quality standard planes.

Result: The approach effectively generates plausible high-quality counterfactuals, validated through quantitative and qualitative evaluation.

Conclusion: The method holds promise for improving clinician training and enhancing standard plane acquisition for better diagnosis and monitoring.

Abstract: Obstetric ultrasound image quality is crucial for accurate diagnosis and
monitoring of fetal health. However, acquiring high-quality standard planes is
difficult, influenced by the sonographer's expertise and factors like the
maternal BMI or fetus dynamics. In this work, we explore diffusion-based
counterfactual explainable AI to generate realistic, high-quality standard
planes from low-quality non-standard ones. Through quantitative and qualitative
evaluation, we demonstrate the effectiveness of our approach in generating
plausible counterfactuals of increased quality. This shows future promise for
enhancing training of clinicians by providing visual feedback and potentially
improving standard plane quality and acquisition for downstream diagnosis and
monitoring.

</details>


### [243] [NuSegDG: Integration of Heterogeneous Space and Gaussian Kernel for Domain-Generalized Nuclei Segmentation](https://arxiv.org/pdf/2408.11787)
*Zhenye Lou, Qing Xu, Zekun Jiang, Xiangjian He, Zhen Chen, Yi Wang, Chenxin Li, Maggie M. He, Wenting Duan*

Main category: eess.IV

TL;DR: NuSegDG is a domain-generalizable framework for nuclei segmentation, improving SAM's adaptation to medical images by reducing manual prompts and enhancing feature learning.


<details>
  <summary>Details</summary>
Motivation: Addressing SAM's limited adaptation to medical images and labor-intensive manual prompts in nuclei segmentation.

Method: Proposes HS-Adapter for multi-dimensional feature learning, GKP-Encoder for automated prompts, and TSM-Decoder for instance segmentation.

Result: NuSegDG achieves state-of-the-art performance in nuclei instance segmentation with superior domain generalization.

Conclusion: NuSegDG effectively generalizes across domains and reduces manual intervention, advancing nuclei segmentation.

Abstract: Domain-generalized nuclei segmentation refers to the generalizability of
models to unseen domains based on knowledge learned from source domains and is
challenged by various image conditions, cell types, and stain strategies.
Recently, the Segment Anything Model (SAM) has made great success in universal
image segmentation by interactive prompt modes (e.g., point and box). Despite
its strengths, the original SAM presents limited adaptation to medical images.
Moreover, SAM requires providing manual bounding box prompts for each object to
produce satisfactory segmentation masks, so it is laborious in nuclei
segmentation scenarios. To address these limitations, we propose a
domain-generalizable framework for nuclei image segmentation, abbreviated to
NuSegDG. Specifically, we first devise a Heterogeneous Space Adapter
(HS-Adapter) to learn multi-dimensional feature representations of different
nuclei domains by injecting a small number of trainable parameters into the
image encoder of SAM. To alleviate the labor-intensive requirement of manual
prompts, we introduce a Gaussian-Kernel Prompt Encoder (GKP-Encoder) to
generate density maps driven by a single point, which guides segmentation
predictions by mixing position prompts and semantic prompts. Furthermore, we
present a Two-Stage Mask Decoder (TSM-Decoder) to effectively convert semantic
masks to instance maps without the manual demand for morphological shape
refinement. Based on our experimental evaluations, the proposed NuSegDG
demonstrates state-of-the-art performance in nuclei instance segmentation,
exhibiting superior domain generalization capabilities. The source code is
available at https://github.com/xq141839/NuSegDG.

</details>


### [244] [A Baseline Method for Removing Invisible Image Watermarks using Deep Image Prior](https://arxiv.org/pdf/2502.13998)
*Hengyue Liang, Taihui Li, Ju Sun*

Main category: eess.IV

TL;DR: A black-box method using deep image prior (DIP) removes invisible watermarks without datasets or system knowledge, advocating DIP as a benchmark for watermark robustness. Training-based visible watermarks show promise against misinformation.


<details>
  <summary>Details</summary>
Motivation: To address the challenge of detecting and removing AI-generated content watermarks, especially for copyright protection and preventing fake image abuse.

Method: Uses deep image prior (DIP) to regress a single watermarked image, finding an evasion image from intermediate DIP steps.

Result: DIP effectively removes invisible watermarks while maintaining image quality, but struggles with training-based visible watermarks.

Conclusion: DIP is a practical baseline for watermark robustness testing, and training-based visible watermarks are promising for preventing misinformation.

Abstract: Image watermarks have been considered a promising technique to help detect
AI-generated content, which can be used to protect copyright or prevent fake
image abuse. In this work, we present a black-box method for removing invisible
image watermarks, without the need of any dataset of watermarked images or any
knowledge about the watermark system. Our approach is simple to implement:
given a single watermarked image, we regress it by deep image prior (DIP). We
show that from the intermediate steps of DIP one can reliably find an evasion
image that can remove invisible watermarks while preserving high image quality.
Due to its unique working mechanism and practical effectiveness, we advocate
including DIP as a baseline invasion method for benchmarking the robustness of
watermarking systems. Finally, by showing the limited ability of DIP and other
existing black-box methods in evading training-based visible watermarks, we
discuss the positive implications on the practical use of training-based
visible watermarks to prevent misinformation abuse.

</details>


### [245] [ScaleFusionNet: Transformer-Guided Multi-Scale Feature Fusion for Skin Lesion Segmentation](https://arxiv.org/pdf/2503.03327)
*Saqib Qamar, Syed Furqan Qadri, Roobaea Alroobaea, Goram Mufarah M Alshmrani, Richard Jiang*

Main category: eess.IV

TL;DR: ScaleFusionNet, a hybrid model combining Cross-Attention Transformer Module and adaptive fusion block, improves skin lesion segmentation by capturing local and global features, achieving high Dice scores on ISIC datasets.


<details>
  <summary>Details</summary>
Motivation: Accurate skin lesion segmentation is challenging due to blurred boundaries and irregular shapes, necessitating advanced methods for feature extraction and fusion.

Method: Proposes ScaleFusionNet with Cross-Attention Transformer Module (CATM) and adaptive fusion block (AFB) to refine feature fusion and reduce semantic gaps, using Swin transformer and deformable convolution.

Result: Achieves Dice scores of 92.94% (ISIC-2016) and 91.80% (ISIC-2018), outperforming state-of-the-art methods on PH² dataset.

Conclusion: ScaleFusionNet effectively enhances segmentation accuracy and boundary refinement, validated by superior performance on benchmark datasets.

Abstract: Melanoma is a malignant tumor that originates from skin cell lesions.
Accurate and efficient segmentation of skin lesions is essential for
quantitative analysis but remains a challenge due to blurred lesion boundaries,
gradual color changes, and irregular shapes. To address this, we propose
ScaleFusionNet, a hybrid model that integrates a Cross-Attention Transformer
Module (CATM) and adaptive fusion block (AFB) to enhance feature extraction and
fusion by capturing both local and global features. We introduce CATM, which
utilizes Swin transformer blocks and Cross Attention Fusion (CAF) to adaptively
refine feature fusion and reduce semantic gaps in the encoder-decoder to
improve segmentation accuracy. Additionally, the AFB uses Swin
Transformer-based attention and deformable convolution-based adaptive feature
extraction to help the model gather local and global contextual information
through parallel pathways. This enhancement refines the lesion boundaries and
preserves fine-grained details. ScaleFusionNet achieves Dice scores of 92.94\%
and 91.80\% on the ISIC-2016 and ISIC-2018 datasets, respectively,
demonstrating its effectiveness in skin lesion analysis. Simultaneously,
independent validation experiments were conducted on the PH$^2$ dataset using
the pretrained model weights. The results show that ScaleFusionNet demonstrates
significant performance improvements compared with other state-of-the-art
methods. Our code implementation is publicly available at GitHub.

</details>


### [246] [Towards Cardiac MRI Foundation Models: Comprehensive Visual-Tabular Representations for Whole-Heart Assessment and Beyond](https://arxiv.org/pdf/2504.13037)
*Yundi Zhang, Paul Hager, Che Liu, Suprosanna Shit, Chen Chen, Daniel Rueckert, Jiazhen Pan*

Main category: eess.IV

TL;DR: ViTa introduces a multi-modal framework integrating CMR and patient-level factors for comprehensive cardiac health evaluation, leveraging UK Biobank data.


<details>
  <summary>Details</summary>
Motivation: Current CMR lacks patient-level health factors, limiting holistic cardiac health understanding. Multi-modal approaches are often task-specific and lack comprehensive data.

Method: ViTa combines 3D+T cine stacks from CMR with tabular patient data, learning a shared latent representation for diverse tasks.

Result: ViTa enables unified cardiac phenotype prediction, segmentation, and disease classification, improving clinical utility.

Conclusion: ViTa advances cardiac analysis by integrating imaging and patient context, offering scalable, patient-specific insights.

Abstract: Cardiac magnetic resonance imaging is the gold standard for non-invasive
cardiac assessment, offering rich spatio-temporal views of the cardiac anatomy
and physiology. Patient-level health factors, such as demographics, metabolic,
and lifestyle, are known to substantially influence cardiovascular health and
disease risk, yet remain uncaptured by CMR alone. To holistically understand
cardiac health and to enable the best possible interpretation of an
individual's disease risk, CMR and patient-level factors must be jointly
exploited within an integrated framework. Recent multi-modal approaches have
begun to bridge this gap, yet they often rely on limited spatio-temporal data
and focus on isolated clinical tasks, thereby hindering the development of a
comprehensive representation for cardiac health evaluation. To overcome these
limitations, we introduce ViTa, a step toward foundation models that delivers a
comprehensive representation of the heart and a precise interpretation of
individual disease risk. Leveraging data from 42,000 UK Biobank participants,
ViTa integrates 3D+T cine stacks from short-axis and long-axis views, enabling
a complete capture of the cardiac cycle. These imaging data are then fused with
detailed tabular patient-level factors, enabling context-aware insights. This
multi-modal paradigm supports a wide spectrum of downstream tasks, including
cardiac phenotype and physiological feature prediction, segmentation, and
classification of cardiac and metabolic diseases within a single unified
framework. By learning a shared latent representation that bridges rich imaging
features and patient context, ViTa moves beyond traditional, task-specific
models toward a universal, patient-specific understanding of cardiac health,
highlighting its potential to advance clinical utility and scalability in
cardiac analysis.

</details>
