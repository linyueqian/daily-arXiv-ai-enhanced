{"id": "2505.24113", "pdf": "https://arxiv.org/pdf/2505.24113", "abs": "https://arxiv.org/abs/2505.24113", "authors": ["Pengcheng Dai", "Yuanqiu Mo", "Wenwu Yu", "Wei Ren"], "title": "Distributed Neural Policy Gradient Algorithm for Global Convergence of Networked Multi-Agent Reinforcement Learning", "categories": ["cs.MA"], "comment": null, "summary": "This paper studies the networked multi-agent reinforcement learning (NMARL)\nproblem, where the objective of agents is to collaboratively maximize the\ndiscounted average cumulative rewards. Different from the existing methods that\nsuffer from poor expression due to linear function approximation, we propose a\ndistributed neural policy gradient algorithm that features two innovatively\ndesigned neural networks, specifically for the approximate Q-functions and\npolicy functions of agents. This distributed neural policy gradient algorithm\nconsists of two key components: the distributed critic step and the\ndecentralized actor step. In the distributed critic step, agents receive the\napproximate Q-function parameters from their neighboring agents via a\ntime-varying communication networks to collaboratively evaluate the joint\npolicy. In contrast, in the decentralized actor step, each agent updates its\nlocal policy parameter solely based on its own approximate Q-function. In the\nconvergence analysis, we first establish the global convergence of agents for\nthe joint policy evaluation in the distributed critic step. Subsequently, we\nrigorously demonstrate the global convergence of the overall distributed neural\npolicy gradient algorithm with respect to the objective function. Finally, the\neffectiveness of the proposed algorithm is demonstrated by comparing it with a\ncentralized algorithm through simulation in the robot path planning\nenvironment."}
{"id": "2505.24239", "pdf": "https://arxiv.org/pdf/2505.24239", "abs": "https://arxiv.org/abs/2505.24239", "authors": ["Sana Ebrahimi", "Mohsen Dehghankar", "Abolfazl Asudeh"], "title": "An Adversary-Resistant Multi-Agent LLM System via Credibility Scoring", "categories": ["cs.MA", "cs.AI", "cs.CL", "cs.LG"], "comment": null, "summary": "While multi-agent LLM systems show strong capabilities in various domains,\nthey are highly vulnerable to adversarial and low-performing agents. To resolve\nthis issue, in this paper, we introduce a general and adversary-resistant\nmulti-agent LLM framework based on credibility scoring. We model the\ncollaborative query-answering process as an iterative game, where the agents\ncommunicate and contribute to a final system output. Our system associates a\ncredibility score that is used when aggregating the team outputs. The\ncredibility scores are learned gradually based on the past contributions of\neach agent in query answering. Our experiments across multiple tasks and\nsettings demonstrate our system's effectiveness in mitigating adversarial\ninfluence and enhancing the resilience of multi-agent cooperation, even in the\nadversary-majority settings."}
{"id": "2505.24265", "pdf": "https://arxiv.org/pdf/2505.24265", "abs": "https://arxiv.org/abs/2505.24265", "authors": ["Harsh Goel", "Mohammad Omama", "Behdad Chalaki", "Vaishnav Tadiparthi", "Ehsan Moradi Pari", "Sandeep Chinchali"], "title": "R3DM: Enabling Role Discovery and Diversity Through Dynamics Models in Multi-agent Reinforcement Learning", "categories": ["cs.MA"], "comment": "21 pages, To appear in the International Conference of Machine\n  Learning (ICML 2025)", "summary": "Multi-agent reinforcement learning (MARL) has achieved significant progress\nin large-scale traffic control, autonomous vehicles, and robotics. Drawing\ninspiration from biological systems where roles naturally emerge to enable\ncoordination, role-based MARL methods have been proposed to enhance cooperation\nlearning for complex tasks. However, existing methods exclusively derive roles\nfrom an agent's past experience during training, neglecting their influence on\nits future trajectories. This paper introduces a key insight: an agent's role\nshould shape its future behavior to enable effective coordination. Hence, we\npropose Role Discovery and Diversity through Dynamics Models (R3DM), a novel\nrole-based MARL framework that learns emergent roles by maximizing the mutual\ninformation between agents' roles, observed trajectories, and expected future\nbehaviors. R3DM optimizes the proposed objective through contrastive learning\non past trajectories to first derive intermediate roles that shape intrinsic\nrewards to promote diversity in future behaviors across different roles through\na learned dynamics model. Benchmarking on SMAC and SMACv2 environments\ndemonstrates that R3DM outperforms state-of-the-art MARL approaches, improving\nmulti-agent coordination to increase win rates by up to 20%."}
{"id": "2505.23962", "pdf": "https://arxiv.org/pdf/2505.23962", "abs": "https://arxiv.org/abs/2505.23962", "authors": ["Aurosweta Mahapatra", "Ismail Rasim Ulgen", "Abinay Reddy Naini", "Carlos Busso", "Berrak Sisman"], "title": "Can Emotion Fool Anti-spoofing?", "categories": ["eess.AS", "cs.LG", "cs.SD"], "comment": "Accepted to Interspeech 2025", "summary": "Traditional anti-spoofing focuses on models and datasets built on synthetic\nspeech with mostly neutral state, neglecting diverse emotional variations. As a\nresult, their robustness against high-quality, emotionally expressive synthetic\nspeech is uncertain. We address this by introducing EmoSpoof-TTS, a corpus of\nemotional text-to-speech samples. Our analysis shows existing anti-spoofing\nmodels struggle with emotional synthetic speech, exposing risks of\nemotion-targeted attacks. Even trained on emotional data, the models\nunderperform due to limited focus on emotional aspect and show performance\ndisparities across emotions. This highlights the need for emotion-focused\nanti-spoofing paradigm in both dataset and methodology. We propose GEM, a gated\nensemble of emotion-specialized models with a speech emotion recognition gating\nnetwork. GEM performs effectively across all emotions and neutral state,\nimproving defenses against spoofing attacks. We release the EmoSpoof-TTS\nDataset: https://emospoof-tts.github.io/Dataset/"}
{"id": "2505.18334", "pdf": "https://arxiv.org/pdf/2505.18334", "abs": "https://arxiv.org/abs/2505.18334", "authors": ["Jiaxun Cui", "Chen Tang", "Jarrett Holtz", "Janice Nguyen", "Alessandro G. Allievi", "Hang Qiu", "Peter Stone"], "title": "Towards Natural Language Communication for Cooperative Autonomous Driving via Self-Play", "categories": ["cs.RO", "cs.AI", "cs.MA"], "comment": null, "summary": "Past work has demonstrated that autonomous vehicles can drive more safely if\nthey communicate with one another than if they do not. However, their\ncommunication has often not been human-understandable. Using natural language\nas a vehicle-to-vehicle (V2V) communication protocol offers the potential for\nautonomous vehicles to drive cooperatively not only with each other but also\nwith human drivers. In this work, we propose a suite of traffic tasks in\nautonomous driving where vehicles in a traffic scenario need to communicate in\nnatural language to facilitate coordination in order to avoid an imminent\ncollision and/or support efficient traffic flow. To this end, this paper\nintroduces a novel method, LLM+Debrief, to learn a message generation and\nhigh-level decision-making policy for autonomous vehicles through multi-agent\ndiscussion. To evaluate LLM agents for driving, we developed a gym-like\nsimulation environment that contains a range of driving scenarios. Our\nexperimental results demonstrate that LLM+Debrief is more effective at\ngenerating meaningful and human-understandable natural language messages to\nfacilitate cooperation and coordination than a zero-shot LLM agent. Our code\nand demo videos are available at https://talking-vehicles.github.io/."}
{"id": "2505.24111", "pdf": "https://arxiv.org/pdf/2505.24111", "abs": "https://arxiv.org/abs/2505.24111", "authors": ["Jiangyu Han", "Federico Landini", "Johan Rohdin", "Anna Silnova", "Mireia Diez", "Jan Cernocky", "Lukas Burget"], "title": "Fine-tune Before Structured Pruning: Towards Compact and Accurate Self-Supervised Models for Speaker Diarization", "categories": ["eess.AS"], "comment": "Accepted by INTERSPEECH 2025", "summary": "Self-supervised learning (SSL) models like WavLM can be effectively utilized\nwhen building speaker diarization systems but are often large and slow,\nlimiting their use in resource constrained scenarios. Previous studies have\nexplored compression techniques, but usually for the price of degraded\nperformance at high pruning ratios. In this work, we propose to compress SSL\nmodels through structured pruning by introducing knowledge distillation.\nDifferent from the existing works, we emphasize the importance of fine-tuning\nSSL models before pruning. Experiments on far-field single-channel AMI,\nAISHELL-4, and AliMeeting datasets show that our method can remove redundant\nparameters of WavLM Base+ and WavLM Large by up to 80% without any performance\ndegradation. After pruning, the inference speeds on a single GPU for the Base+\nand Large models are 4.0 and 2.6 times faster, respectively. Our source code is\npublicly available."}
{"id": "2505.23781", "pdf": "https://arxiv.org/pdf/2505.23781", "abs": "https://arxiv.org/abs/2505.23781", "authors": ["Hamideh Khaleghpour", "Brett McKinney"], "title": "Unified AI for Accurate Audio Anomaly Detection", "categories": ["cs.SD", "cs.LG", "eess.AS"], "comment": "6 pages, 14 figures. Based on original research. Submitted to arXiv\n  for public preprint", "summary": "This paper presents a unified AI framework for high-accuracy audio anomaly\ndetection by integrating advanced noise reduction, feature extraction, and\nmachine learning modeling techniques. The approach combines spectral\nsubtraction and adaptive filtering to enhance audio quality, followed by\nfeature extraction using traditional methods like MFCCs and deep embeddings\nfrom pre-trained models such as OpenL3. The modeling pipeline incorporates\nclassical models (SVM, Random Forest), deep learning architectures (CNNs), and\nensemble methods to boost robustness and accuracy. Evaluated on benchmark\ndatasets including TORGO and LibriSpeech, the proposed framework demonstrates\nsuperior performance in precision, recall, and classification of slurred vs.\nnormal speech. This work addresses challenges in noisy environments and\nreal-time applications and provides a scalable solution for audio-based anomaly\ndetection."}
{"id": "2505.23785", "pdf": "https://arxiv.org/pdf/2505.23785", "abs": "https://arxiv.org/abs/2505.23785", "authors": ["Cody Kommers", "Drew Hemment", "Maria Antoniak", "Joel Z. Leibo", "Hoyt Long", "Emily Robinson", "Adam Sobey"], "title": "Meaning Is Not A Metric: Using LLMs to make cultural context legible at scale", "categories": ["cs.CL", "cs.AI", "cs.CY"], "comment": "Position paper", "summary": "This position paper argues that large language models (LLMs) can make\ncultural context, and therefore human meaning, legible at an unprecedented\nscale in AI-based sociotechnical systems. We argue that such systems have\npreviously been unable to represent human meaning because they rely on thin\ndescriptions: numerical representations that enforce standardization and\ntherefore strip human activity of the cultural context that gives it meaning.\nBy contrast, scholars in the humanities and qualitative social sciences have\ndeveloped frameworks for representing meaning through thick description: verbal\nrepresentations that accommodate heterogeneity and retain contextual\ninformation needed to represent human meaning. While these methods can\neffectively codify meaning, they are difficult to deploy at scale. However, the\nverbal capabilities of LLMs now provide a means of (at least partially)\nautomating the generation and processing of thick descriptions, potentially\novercoming this bottleneck. We argue that the problem of rendering human\nmeaning legible is not just about selecting better metrics, but about\ndeveloping new representational formats (based on thick description). We frame\nthis as a crucial direction for the application of generative AI and identify\nfive key challenges: preserving context, maintaining interpretive pluralism,\nintegrating perspectives based on lived experience and critical distance,\ndistinguishing qualitative content from quantitative magnitude, and\nacknowledging meaning as dynamic rather than static. Furthermore, we suggest\nthat thick description has the potential to serve as a unifying framework to\naddress a number of emerging concerns about the difficulties of representing\nculture in (or using) LLMs."}
{"id": "2505.23883", "pdf": "https://arxiv.org/pdf/2505.23883", "abs": "https://arxiv.org/abs/2505.23883", "authors": ["Jianyang Gu", "Samuel Stevens", "Elizabeth G Campolongo", "Matthew J Thompson", "Net Zhang", "Jiaman Wu", "Andrei Kopanev", "Zheda Mai", "Alexander E. White", "James Balhoff", "Wasila Dahdul", "Daniel Rubenstein", "Hilmar Lapp", "Tanya Berger-Wolf", "Wei-Lun Chao", "Yu Su"], "title": "BioCLIP 2: Emergent Properties from Scaling Hierarchical Contrastive Learning", "categories": ["cs.CV", "cs.CL", "cs.LG"], "comment": "Project page: https://imageomics.github.io/bioclip-2/", "summary": "Foundation models trained at scale exhibit remarkable emergent behaviors,\nlearning new capabilities beyond their initial training objectives. We find\nsuch emergent behaviors in biological vision models via large-scale contrastive\nvision-language training. To achieve this, we first curate TreeOfLife-200M,\ncomprising 214 million images of living organisms, the largest and most diverse\nbiological organism image dataset to date. We then train BioCLIP 2 on\nTreeOfLife-200M to distinguish different species. Despite the narrow training\nobjective, BioCLIP 2 yields extraordinary accuracy when applied to various\nbiological visual tasks such as habitat classification and trait prediction. We\nidentify emergent properties in the learned embedding space of BioCLIP 2. At\nthe inter-species level, the embedding distribution of different species aligns\nclosely with functional and ecological meanings (e.g., beak sizes and\nhabitats). At the intra-species level, instead of being diminished, the\nintra-species variations (e.g., life stages and sexes) are preserved and better\nseparated in subspaces orthogonal to inter-species distinctions. We provide\nformal proof and analyses to explain why hierarchical supervision and\ncontrastive objectives encourage these emergent properties. Crucially, our\nresults reveal that these properties become increasingly significant with\nlarger-scale training data, leading to a biologically meaningful embedding\nspace."}
{"id": "2505.23872", "pdf": "https://arxiv.org/pdf/2505.23872", "abs": "https://arxiv.org/abs/2505.23872", "authors": ["Anam Hashmi", "Julia Dietlmeier", "Kathleen M. Curran", "Noel E. O'Connor"], "title": "Parameter-Free Bio-Inspired Channel Attention for Enhanced Cardiac MRI Reconstruction", "categories": ["eess.IV", "cs.CV"], "comment": "presented at the 28th UK Conference on Medical Image Understanding\n  and Analysis - MIUA, 24 - 26 July 2024", "summary": "Attention is a fundamental component of the human visual recognition system.\nThe inclusion of attention in a convolutional neural network amplifies relevant\nvisual features and suppresses the less important ones. Integrating attention\nmechanisms into convolutional neural networks enhances model performance and\ninterpretability. Spatial and channel attention mechanisms have shown\nsignificant advantages across many downstream tasks in medical imaging. While\nexisting attention modules have proven to be effective, their design often\nlacks a robust theoretical underpinning. In this study, we address this gap by\nproposing a non-linear attention architecture for cardiac MRI reconstruction\nand hypothesize that insights from ecological principles can guide the\ndevelopment of effective and efficient attention mechanisms. Specifically, we\ninvestigate a non-linear ecological difference equation that describes\nsingle-species population growth to devise a parameter-free attention module\nsurpassing current state-of-the-art parameter-free methods."}
{"id": "2505.24176", "pdf": "https://arxiv.org/pdf/2505.24176", "abs": "https://arxiv.org/abs/2505.24176", "authors": ["Zihao Yu", "Xiang Li", "Jing Zhang"], "title": "ISMAF: Intrinsic-Social Modality Alignment and Fusion for Multimodal Rumor Detection", "categories": ["cs.MM"], "comment": null, "summary": "The rapid dissemination of rumors on social media highlights the urgent need\nfor automatic detection methods to safeguard societal trust and stability.\nWhile existing multimodal rumor detection models primarily emphasize capturing\nconsistency between intrinsic modalities (e.g., news text and images), they\noften overlook the intricate interplay between intrinsic and social modalities.\nThis limitation hampers the ability to fully capture nuanced relationships that\nare crucial for a comprehensive understanding. Additionally, current methods\nstruggle with effectively fusing social context with textual and visual\ninformation, resulting in fragmented interpretations. To address these\nchallenges, this paper proposes a novel Intrinsic-Social Modality Alignment and\nFusion (ISMAF) framework for multimodal rumor detection. ISMAF first employs a\ncross-modal consistency alignment strategy to align complex interactions\nbetween intrinsic and social modalities. It then leverages a mutual learning\napproach to facilitate collaborative refinement and integration of\ncomplementary information across modalities. Finally, an adaptive fusion\nmechanism is incorporated to dynamically adjust the contribution of each\nmodality, tackling the complexities of three-modality fusion. Extensive\nexperiments on both English and Chinese real-world multimedia datasets\ndemonstrate that ISMAF consistently outperforms state-of-the-art models."}
{"id": "2505.23881", "pdf": "https://arxiv.org/pdf/2505.23881", "abs": "https://arxiv.org/abs/2505.23881", "authors": ["Christopher D. Rosin"], "title": "Using Reasoning Models to Generate Search Heuristics that Solve Open Instances of Combinatorial Design Problems", "categories": ["cs.AI", "cs.CL", "math.CO"], "comment": "arXiv admin note: text overlap with arXiv:2501.17725", "summary": "Large Language Models (LLMs) with reasoning are trained to iteratively\ngenerate and refine their answers before finalizing them, which can help with\napplications to mathematics and code generation. We apply code generation with\nreasoning LLMs to a specific task in the mathematical field of combinatorial\ndesign. This field studies diverse types of combinatorial designs, many of\nwhich have lists of open instances for which existence has not yet been\ndetermined. The Constructive Protocol CPro1 uses LLMs to generate search\nheuristics that have the potential to construct solutions to small open\ninstances. Starting with a textual definition and a validity verifier for a\nparticular type of design, CPro1 guides LLMs to select and implement\nstrategies, while providing automated hyperparameter tuning and execution\nfeedback. CPro1 with reasoning LLMs successfully solves long-standing open\ninstances for 7 of 16 combinatorial design problems selected from the 2006\nHandbook of Combinatorial Designs, including new solved instances for 3 of\nthese (Bhaskar Rao Designs, Symmetric Weighing Matrices, Balanced Ternary\nDesigns) that were unsolved by CPro1 with non-reasoning LLMs. It also solves\nopen instances for several problems from recent (2025) literature, generating\nnew Covering Sequences, Johnson Clique Covers, Deletion Codes, and a Uniform\nNested Steiner Quadruple System."}
{"id": "2505.23857", "pdf": "https://arxiv.org/pdf/2505.23857", "abs": "https://arxiv.org/abs/2505.23857", "authors": ["Wuhao Wang", "Zhiyong Chen"], "title": "DATD3: Depthwise Attention Twin Delayed Deep Deterministic Policy Gradient For Model Free Reinforcement Learning Under Output Feedback Control", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Reinforcement learning in real-world applications often involves\noutput-feedback settings, where the agent receives only partial state\ninformation. To address this challenge, we propose the Output-Feedback Markov\nDecision Process (OPMDP), which extends the standard MDP formulation to\naccommodate decision-making based on observation histories. Building on this\nframework, we introduce Depthwise Attention Twin Delayed Deep Deterministic\nPolicy Gradient (DATD3), a novel actor-critic algorithm that employs depthwise\nseparable convolution and multi-head attention to encode historical\nobservations. DATD3 maintains policy expressiveness while avoiding the\ninstability of recurrent models. Extensive experiments on continuous control\ntasks demonstrate that DATD3 outperforms existing memory-based and recurrent\nbaselines under both partial and full observability."}
{"id": "2505.23846", "pdf": "https://arxiv.org/pdf/2505.23846", "abs": "https://arxiv.org/abs/2505.23846", "authors": ["Atanu Barai", "Stephan Eidenbenz", "Nandakishore Santhi"], "title": "Scalable, Symbiotic, AI and Non-AI Agent Based Parallel Discrete Event Simulations", "categories": ["cs.CL", "cs.MA"], "comment": null, "summary": "To fully leverage the potential of artificial intelligence (AI) systems in a\ntrustworthy manner, it is desirable to couple multiple AI and non-AI systems\ntogether seamlessly for constraining and ensuring correctness of the output.\nThis paper introduces a novel parallel discrete event simulation (PDES) based\nmethodology to combine multiple AI and non-AI agents in a causal, rule-based\nway. Our approach tightly integrates the concept of passage of time, with each\nagent considered as an entity in the PDES framework and responding to prior\nrequests from other agents. Such coupling mechanism enables the agents to work\nin a co-operative environment towards a common goal while many tasks run in\nparallel throughout the simulation. It further enables setting up boundaries to\nthe outputs of the AI agents by applying necessary dynamic constraints using\nnon-AI agents while allowing for scalability through deployment of hundreds of\nsuch agents in a larger compute cluster. Distributing smaller AI agents can\nenable extremely scalable simulations in the future, addressing local memory\nbottlenecks for model parameter storage. Within a PDES involving both AI and\nnon-AI agents, we break down the problem at hand into structured steps, when\nnecessary, providing a set of multiple choices to the AI agents, and then\nprogressively solve these steps towards a final goal. At each step, the non-AI\nagents act as unbiased auditors, verifying each action by the AI agents so that\ncertain rules of engagement are followed. We evaluate our approach by solving\nfour problems from four different domains and comparing the results with those\nfrom AI models alone. Our results show greater accuracy in solving problems\nfrom various domains where the AI models struggle to solve the problems solely\nby themselves. Results show that overall accuracy of our approach is 68% where\nas the accuracy of vanilla models is less than 23%."}
{"id": "2505.24224", "pdf": "https://arxiv.org/pdf/2505.24224", "abs": "https://arxiv.org/abs/2505.24224", "authors": ["Chengxi Deng", "Xurong Xie", "Shujie Hu", "Mengzhe Geng", "Yicong Jiang", "Jiankun Zhao", "Jiajun Deng", "Guinan Li", "Youjun Chen", "Huimeng Wang", "Haoning Xu", "Mingyu Cui", "Xunying Liu"], "title": "MOPSA: Mixture of Prompt-Experts Based Speaker Adaptation for Elderly Speech Recognition", "categories": ["eess.AS"], "comment": "Accepted by Interspeech 2025", "summary": "This paper proposes a novel Mixture of Prompt-Experts based Speaker\nAdaptation approach (MOPSA) for elderly speech recognition. It allows\nzero-shot, real-time adaptation to unseen speakers, and leverages domain\nknowledge tailored to elderly speakers. Top-K most distinctive speaker prompt\nclusters derived using K-means serve as experts. A router network is trained to\ndynamically combine clustered prompt-experts. Acoustic and language level\nvariability among elderly speakers are modelled using separate encoder and\ndecoder prompts for Whisper. Experiments on the English DementiaBank Pitt and\nCantonese JCCOCC MoCA elderly speech datasets suggest that online MOPSA\nadaptation outperforms the speaker-independent (SI) model by statistically\nsignificant word error rate (WER) or character error rate (CER) reductions of\n0.86% and 1.47% absolute (4.21% and 5.40% relative). Real-time factor (RTF)\nspeed-up ratios of up to 16.12 times are obtained over offline batch-mode\nadaptation."}
{"id": "2505.23782", "pdf": "https://arxiv.org/pdf/2505.23782", "abs": "https://arxiv.org/abs/2505.23782", "authors": ["Andrew P. Berg", "Qian Zhang", "Mia Y. Wang"], "title": "4,500 Seconds: Small Data Training Approaches for Deep UAV Audio Classification", "categories": ["cs.SD", "cs.AI", "eess.AS"], "comment": "Accepted at the 14th International Conference on Data Science,\n  Technology, and Applications (DATA), 2025", "summary": "Unmanned aerial vehicle (UAV) usage is expected to surge in the coming\ndecade, raising the need for heightened security measures to prevent airspace\nviolations and security threats. This study investigates deep learning\napproaches to UAV classification focusing on the key issue of data scarcity. To\ninvestigate this we opted to train the models using a total of 4,500 seconds of\naudio samples, evenly distributed across a 9-class dataset. We leveraged\nparameter efficient fine-tuning (PEFT) and data augmentations to mitigate the\ndata scarcity. This paper implements and compares the use of convolutional\nneural networks (CNNs) and attention-based transformers. Our results show that,\nCNNs outperform transformers by 1-2\\% accuracy, while still being more\ncomputationally efficient. These early findings, however, point to potential in\nusing transformers models; suggesting that with more data and further\noptimizations they could outperform CNNs. Future works aims to upscale the\ndataset to better understand the trade-offs between these approaches."}
{"id": "2505.23788", "pdf": "https://arxiv.org/pdf/2505.23788", "abs": "https://arxiv.org/abs/2505.23788", "authors": ["Aakash Sen Sharma", "Debdeep Sanyal", "Priyansh Srivastava", "Sundar Atreya H.", "Shirish Karande", "Mohan Kankanhalli", "Murari Mandal"], "title": "Nine Ways to Break Copyright Law and Why Our LLM Won't: A Fair Use Aligned Generation Framework", "categories": ["cs.CL", "cs.AI"], "comment": "30 Pages", "summary": "Large language models (LLMs) commonly risk copyright infringement by\nreproducing protected content verbatim or with insufficient transformative\nmodifications, posing significant ethical, legal, and practical concerns.\nCurrent inference-time safeguards predominantly rely on restrictive\nrefusal-based filters, often compromising the practical utility of these\nmodels. To address this, we collaborated closely with intellectual property\nexperts to develop FUA-LLM (Fair Use Aligned Language Models), a\nlegally-grounded framework explicitly designed to align LLM outputs with\nfair-use doctrine. Central to our method is FairUseDB, a carefully constructed\ndataset containing 18,000 expert-validated examples covering nine realistic\ninfringement scenarios. Leveraging this dataset, we apply Direct Preference\nOptimization (DPO) to fine-tune open-source LLMs, encouraging them to produce\nlegally compliant and practically useful alternatives rather than resorting to\nblunt refusal. Recognizing the shortcomings of traditional evaluation metrics,\nwe propose new measures: Weighted Penalty Utility and Compliance Aware Harmonic\nMean (CAH) to balance infringement risk against response utility. Extensive\nquantitative experiments coupled with expert evaluations confirm that FUA-LLM\nsubstantially reduces problematic outputs (up to 20\\%) compared to\nstate-of-the-art approaches, while preserving real-world usability."}
{"id": "2505.23886", "pdf": "https://arxiv.org/pdf/2505.23886", "abs": "https://arxiv.org/abs/2505.23886", "authors": ["Bowei Chen", "Brian Curless", "Ira Kemelmacher-Shlizerman", "Steven M. Seitz"], "title": "Generating Fit Check Videos with a Handheld Camera", "categories": ["cs.CV"], "comment": null, "summary": "Self-captured full-body videos are popular, but most deployments require\nmounted cameras, carefully-framed shots, and repeated practice. We propose a\nmore convenient solution that enables full-body video capture using handheld\nmobile devices. Our approach takes as input two static photos (front and back)\nof you in a mirror, along with an IMU motion reference that you perform while\nholding your mobile phone, and synthesizes a realistic video of you performing\na similar target motion. We enable rendering into a new scene, with consistent\nillumination and shadows. We propose a novel video diffusion-based model to\nachieve this. Specifically, we propose a parameter-free frame generation\nstrategy, as well as a multi-reference attention mechanism, that effectively\nintegrate appearance information from both the front and back selfies into the\nvideo diffusion model. Additionally, we introduce an image-based fine-tuning\nstrategy to enhance frame sharpness and improve the generation of shadows and\nreflections, achieving a more realistic human-scene composition."}
{"id": "2505.23916", "pdf": "https://arxiv.org/pdf/2505.23916", "abs": "https://arxiv.org/abs/2505.23916", "authors": ["Charles Bricout", "Samira Ebrahimi Kahou", "Sylvain Bouix"], "title": "Estimating Head Motion in Structural MRI Using a Deep Neural Network Trained on Synthetic Artifacts", "categories": ["eess.IV", "cs.CV"], "comment": null, "summary": "Motion-related artifacts are inevitable in Magnetic Resonance Imaging (MRI)\nand can bias automated neuroanatomical metrics such as cortical thickness.\nManual review cannot objectively quantify motion in anatomical scans, and\nexisting automated approaches often require specialized hardware or rely on\nunbalanced noisy training data. Here, we train a 3D convolutional neural\nnetwork to estimate motion severity using only synthetically corrupted volumes.\nWe validate our method with one held-out site from our training cohort and with\n14 fully independent datasets, including one with manual ratings, achieving a\nrepresentative $R^2 = 0.65$ versus manual labels and significant\nthickness-motion correlations in 12/15 datasets. Furthermore, our predicted\nmotion correlates with subject age in line with prior studies. Our approach\ngeneralizes across scanner brands and protocols, enabling objective, scalable\nmotion assessment in structural MRI studies without prospective motion\ncorrection."}
{"id": "2505.23784", "pdf": "https://arxiv.org/pdf/2505.23784", "abs": "https://arxiv.org/abs/2505.23784", "authors": ["Shayan Dadman", "Bernt Arild Bremdal", "BÃ¸rre Bang", "Rune Dalmo"], "title": "Learning Normal Patterns in Musical Loops", "categories": ["cs.SD", "cs.IR", "cs.LG", "cs.MM", "eess.AS"], "comment": "27 pages, 10 figures", "summary": "This paper introduces an unsupervised framework for detecting audio patterns\nin musical samples (loops) through anomaly detection techniques, addressing\nchallenges in music information retrieval (MIR). Existing methods are often\nconstrained by reliance on handcrafted features, domain-specific limitations,\nor dependence on iterative user interaction. We address these limitations\nthrough an architecture combining deep feature extraction with unsupervised\nanomaly detection. Our approach leverages a pre-trained Hierarchical\nToken-semantic Audio Transformer (HTS-AT), paired with a Feature Fusion\nMechanism (FFM), to generate representations from variable-length audio loops.\nThese embeddings are processed using one-class Deep Support Vector Data\nDescription (Deep SVDD), which learns normative audio patterns by mapping them\nto a compact latent hypersphere. Evaluations on curated bass and guitar\ndatasets compare standard and residual autoencoder variants against baselines\nlike Isolation Forest (IF) and and principle component analysis (PCA) methods.\nResults show our Deep SVDD models, especially the residual autoencoder variant,\ndeliver improved anomaly separation, particularly for larger variations. This\nresearch contributes a flexible, fully unsupervised solution for processing\ndiverse audio samples, overcoming previous structural and input limitations\nwhile enabling effective pattern identification through distance-based latent\nspace scoring."}
{"id": "2505.23885", "pdf": "https://arxiv.org/pdf/2505.23885", "abs": "https://arxiv.org/abs/2505.23885", "authors": ["Mengkang Hu", "Yuhang Zhou", "Wendong Fan", "Yuzhou Nie", "Bowei Xia", "Tao Sun", "Ziyu Ye", "Zhaoxuan Jin", "Yingru Li", "Qiguang Chen", "Zeyu Zhang", "Yifeng Wang", "Qianshuo Ye", "Bernard Ghanem", "Ping Luo", "Guohao Li"], "title": "OWL: Optimized Workforce Learning for General Multi-Agent Assistance in Real-World Task Automation", "categories": ["cs.AI", "cs.CL"], "comment": "Project Page: https://github.com/camel-ai/owl", "summary": "Large Language Model (LLM)-based multi-agent systems show promise for\nautomating real-world tasks but struggle to transfer across domains due to\ntheir domain-specific nature. Current approaches face two critical\nshortcomings: they require complete architectural redesign and full retraining\nof all components when applied to new domains. We introduce Workforce, a\nhierarchical multi-agent framework that decouples strategic planning from\nspecialized execution through a modular architecture comprising: (i) a\ndomain-agnostic Planner for task decomposition, (ii) a Coordinator for subtask\nmanagement, and (iii) specialized Workers with domain-specific tool-calling\ncapabilities. This decoupling enables cross-domain transferability during both\ninference and training phases: During inference, Workforce seamlessly adapts to\nnew domains by adding or modifying worker agents; For training, we introduce\nOptimized Workforce Learning (OWL), which improves generalization across\ndomains by optimizing a domain-agnostic planner with reinforcement learning\nfrom real-world feedback. To validate our approach, we evaluate Workforce on\nthe GAIA benchmark, covering various realistic, multi-domain agentic tasks.\nExperimental results demonstrate Workforce achieves open-source\nstate-of-the-art performance (69.70%), outperforming commercial systems like\nOpenAI's Deep Research by 2.34%. More notably, our OWL-trained 32B model\nachieves 52.73% accuracy (+16.37%) and demonstrates performance comparable to\nGPT-4o on challenging tasks. To summarize, by enabling scalable generalization\nand modular domain transfer, our work establishes a foundation for the next\ngeneration of general-purpose AI assistants."}
{"id": "2505.23859", "pdf": "https://arxiv.org/pdf/2505.23859", "abs": "https://arxiv.org/abs/2505.23859", "authors": ["Wenju Sun", "Qingyong Li", "Wen Wang", "Yang Liu", "Yangli-ao Geng", "Boyang Li"], "title": "Towards Minimizing Feature Drift in Model Merging: Layer-wise Task Vector Fusion for Adaptive Knowledge Integration", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Multi-task model merging aims to consolidate knowledge from multiple\nfine-tuned task-specific experts into a unified model while minimizing\nperformance degradation. Existing methods primarily approach this by minimizing\ndifferences between task-specific experts and the unified model, either from a\nparameter-level or a task-loss perspective. However, parameter-level methods\nexhibit a significant performance gap compared to the upper bound, while\ntask-loss approaches entail costly secondary training procedures. In contrast,\nwe observe that performance degradation closely correlates with feature drift,\ni.e., differences in feature representations of the same sample caused by model\nmerging. Motivated by this observation, we propose Layer-wise Optimal Task\nVector Merging (LOT Merging), a technique that explicitly minimizes feature\ndrift between task-specific experts and the unified model in a layer-by-layer\nmanner. LOT Merging can be formulated as a convex quadratic optimization\nproblem, enabling us to analytically derive closed-form solutions for the\nparameters of linear and normalization layers. Consequently, LOT Merging\nachieves efficient model consolidation through basic matrix operations.\nExtensive experiments across vision and vision-language benchmarks demonstrate\nthat LOT Merging significantly outperforms baseline methods, achieving\nimprovements of up to 4.4% (ViT-B/32) over state-of-the-art approaches."}
{"id": "2505.23852", "pdf": "https://arxiv.org/pdf/2505.23852", "abs": "https://arxiv.org/abs/2505.23852", "authors": ["Nic Dobbins", "Christelle Xiong", "Kristine Lan", "Meliha Yetisgen"], "title": "Large Language Model-Based Agents for Automated Research Reproducibility: An Exploratory Study in Alzheimer's Disease", "categories": ["cs.CL", "cs.AI", "cs.MA", "stat.AP"], "comment": null, "summary": "Objective: To demonstrate the capabilities of Large Language Models (LLMs) as\nautonomous agents to reproduce findings of published research studies using the\nsame or similar dataset.\n  Materials and Methods: We used the \"Quick Access\" dataset of the National\nAlzheimer's Coordinating Center (NACC). We identified highly cited published\nresearch manuscripts using NACC data and selected five studies that appeared\nreproducible using this dataset alone. Using GPT-4o, we created a simulated\nresearch team of LLM-based autonomous agents tasked with writing and executing\ncode to dynamically reproduce the findings of each study, given only study\nAbstracts, Methods sections, and data dictionary descriptions of the dataset.\n  Results: We extracted 35 key findings described in the Abstracts across 5\nAlzheimer's studies. On average, LLM agents approximately reproduced 53.2% of\nfindings per study. Numeric values and range-based findings often differed\nbetween studies and agents. The agents also applied statistical methods or\nparameters that varied from the originals, though overall trends and\nsignificance were sometimes similar.\n  Discussion: In some cases, LLM-based agents replicated research techniques\nand findings. In others, they failed due to implementation flaws or missing\nmethodological detail. These discrepancies show the current limits of LLMs in\nfully automating reproducibility assessments. Still, this early investigation\nhighlights the potential of structured agent-based systems to provide scalable\nevaluation of scientific rigor.\n  Conclusion: This exploratory work illustrates both the promise and\nlimitations of LLMs as autonomous agents for automating reproducibility in\nbiomedical research."}
{"id": "2505.24248", "pdf": "https://arxiv.org/pdf/2505.24248", "abs": "https://arxiv.org/abs/2505.24248", "authors": ["Wei-Cheng Tseng", "David Harwath"], "title": "Probing the Robustness Properties of Neural Speech Codecs", "categories": ["eess.AS", "cs.SD"], "comment": "Interspeech 2025", "summary": "Neural speech codecs have revolutionized speech coding, achieving higher\ncompression while preserving audio fidelity. Beyond compression, they have\nemerged as tokenization strategies, enabling language modeling on speech and\ndriving paradigm shifts across various speech processing tasks. Despite these\nadvancements, their robustness in noisy environments remains underexplored,\nraising concerns about their generalization to real-world scenarios. In this\nwork, we systematically evaluate neural speech codecs under various noise\nconditions, revealing non-trivial differences in their robustness. We further\nexamine their linearity properties, uncovering non-linear distortions which\npartly explain observed variations in robustness. Lastly, we analyze their\nfrequency response to identify factors affecting audio fidelity. Our findings\nprovide critical insights into codec behavior and future codec design, as well\nas emphasizing the importance of noise robustness for their real-world\nintegration."}
{"id": "2505.23834", "pdf": "https://arxiv.org/pdf/2505.23834", "abs": "https://arxiv.org/abs/2505.23834", "authors": ["Seung Gyu Jeong", "Seong Eun Kim"], "title": "Patient-Aware Feature Alignment for Robust Lung Sound Classification:Cohesion-Separation and Global Alignment Losses", "categories": ["cs.SD", "cs.LG", "eess.AS"], "comment": "Accepted INTERSPEECH 2025", "summary": "Lung sound classification is vital for early diagnosis of respiratory\ndiseases. However, biomedical signals often exhibit inter-patient variability\neven among patients with the same symptoms, requiring a learning approach that\nconsiders individual differences. We propose a Patient-Aware Feature Alignment\n(PAFA) framework with two novel losses, Patient Cohesion-Separation Loss (PCSL)\nand Global Patient Alignment Loss (GPAL). PCSL clusters features of the same\npatient while separating those from other patients to capture patient\nvariability, whereas GPAL draws each patient's centroid toward a global center,\npreventing feature space fragmentation. Our method achieves outstanding results\non the ICBHI dataset with a score of 64.84\\% for four-class and 72.08\\% for\ntwo-class classification. These findings highlight PAFA's ability to capture\nindividualized patterns and demonstrate performance gains in distinct patient\nclusters, offering broader applications for patient-centered healthcare."}
{"id": "2505.23789", "pdf": "https://arxiv.org/pdf/2505.23789", "abs": "https://arxiv.org/abs/2505.23789", "authors": ["Mingyu Huang", "Shasha Zhou", "Yuxuan Chen", "Ke Li"], "title": "Conversational Exploration of Literature Landscape with LitChat", "categories": ["cs.CL", "cs.IR", "cs.LG"], "comment": null, "summary": "We are living in an era of \"big literature\", where the volume of digital\nscientific publications is growing exponentially. While offering new\nopportunities, this also poses challenges for understanding literature\nlandscapes, as traditional manual reviewing is no longer feasible. Recent large\nlanguage models (LLMs) have shown strong capabilities for literature\ncomprehension, yet they are incapable of offering \"comprehensive, objective,\nopen and transparent\" views desired by systematic reviews due to their limited\ncontext windows and trust issues like hallucinations. Here we present LitChat,\nan end-to-end, interactive and conversational literature agent that augments\nLLM agents with data-driven discovery tools to facilitate literature\nexploration. LitChat automatically interprets user queries, retrieves relevant\nsources, constructs knowledge graphs, and employs diverse data-mining\ntechniques to generate evidence-based insights addressing user needs. We\nillustrate the effectiveness of LitChat via a case study on AI4Health,\nhighlighting its capacity to quickly navigate the users through large-scale\nliterature landscape with data-based evidence that is otherwise infeasible with\ntraditional means."}
{"id": "2505.23907", "pdf": "https://arxiv.org/pdf/2505.23907", "abs": "https://arxiv.org/abs/2505.23907", "authors": ["Amirhossein Almohammadi", "Aryan Mikaeili", "Sauradip Nag", "Negar Hassanpour", "Andrea Tagliasacchi", "Ali Mahdavi-Amiri"], "title": "Cora: Correspondence-aware image editing using few step diffusion", "categories": ["cs.CV", "I.4.10; I.3.7; I.2.10"], "comment": "Published in SIGGRAPH 2025", "summary": "Image editing is an important task in computer graphics, vision, and VFX,\nwith recent diffusion-based methods achieving fast and high-quality results.\nHowever, edits requiring significant structural changes, such as non-rigid\ndeformations, object modifications, or content generation, remain challenging.\nExisting few step editing approaches produce artifacts such as irrelevant\ntexture or struggle to preserve key attributes of the source image (e.g.,\npose). We introduce Cora, a novel editing framework that addresses these\nlimitations by introducing correspondence-aware noise correction and\ninterpolated attention maps. Our method aligns textures and structures between\nthe source and target images through semantic correspondence, enabling accurate\ntexture transfer while generating new content when necessary. Cora offers\ncontrol over the balance between content generation and preservation. Extensive\nexperiments demonstrate that, quantitatively and qualitatively, Cora excels in\nmaintaining structure, textures, and identity across diverse edits, including\npose changes, object addition, and texture refinements. User studies confirm\nthat Cora delivers superior results, outperforming alternatives."}
{"id": "2505.23984", "pdf": "https://arxiv.org/pdf/2505.23984", "abs": "https://arxiv.org/abs/2505.23984", "authors": ["Vahid Danesh", "Paul Arauz", "Maede Boroji", "Andrew Zhu", "Mia Cottone", "Elaine Gould", "Fazel A. Khan", "Imin Kao"], "title": "Improved Accuracy in Pelvic Tumor Resections Using a Real-Time Vision-Guided Surgical System", "categories": ["eess.IV", "cs.HC"], "comment": "9 Pages, 5 figures, Submitted to Journal of Orthopaedic Research", "summary": "Pelvic bone tumor resections remain significantly challenging due to complex\nthree-dimensional anatomy and limited surgical visualization. Current\nnavigation systems and patient-specific instruments, while accurate, present\nlimitations including high costs, radiation exposure, workflow disruption, long\nproduction time, and lack of reusability. This study evaluates a real-time\nvision-guided surgical system combined with modular jigs to improve accuracy in\npelvic bone tumor resections. A vision-guided surgical system combined with\nmodular cutting jigs and real-time optical tracking was developed and\nvalidated. Five female pelvis sawbones were used, with each hemipelvis randomly\nassigned to either the vision-guided and modular jig system or traditional\nfreehand method. A total of twenty resection planes were analyzed for each\nmethod. Accuracy was assessed by measuring distance and angular deviations from\nthe planned resection planes. The vision-guided and modular jig system\nsignificantly improved resection accuracy compared to the freehand method,\nreducing the mean distance deviation from 2.07 $\\pm$ 1.71 mm to 1.01 $\\pm$ 0.78\nmm (p=0.0193). In particular, all specimens resected using the vision-guided\nsystem exhibited errors of less than 3 mm. Angular deviations also showed\nsignificant improvements with roll angle deviation reduced from 15.36 $\\pm$\n17.57$^\\circ$ to 4.21 $\\pm$ 3.46$^\\circ$ (p=0.0275), and pitch angle deviation\ndecreased from 6.17 $\\pm$ 4.58$^\\circ$ to 1.84 $\\pm$ 1.48$^\\circ$ (p<0.001).\nThe proposed vision-guided and modular jig system significantly improves the\naccuracy of pelvic bone tumor resections while maintaining workflow efficiency.\nThis cost-effective solution provides real-time guidance without the need for\nreferencing external monitors, potentially improving surgical outcomes in\ncomplex pelvic bone tumor cases."}
{"id": "2505.23822", "pdf": "https://arxiv.org/pdf/2505.23822", "abs": "https://arxiv.org/abs/2505.23822", "authors": ["Mai Ali", "Christopher Lucasius", "Tanmay P. Patel", "Madison Aitken", "Jacob Vorstman", "Peter Szatmari", "Marco Battaglia", "Deepa Kundur"], "title": "Speech as a Multimodal Digital Phenotype for Multi-Task LLM-based Mental Health Prediction", "categories": ["cs.CL", "cs.MM"], "comment": "6 pages, 1 figure, 3 tables. Submitted to ICSM 2025. The\n  corresponding author is Mai Ali (maia.ali@mail.utoronto.ca). Christopher\n  Lucasius and Tanmay P. Patel contributed equally", "summary": "Speech is a noninvasive digital phenotype that can offer valuable insights\ninto mental health conditions, but it is often treated as a single modality. In\ncontrast, we propose the treatment of patient speech data as a trimodal\nmultimedia data source for depression detection. This study explores the\npotential of large language model-based architectures for speech-based\ndepression prediction in a multimodal regime that integrates speech-derived\ntext, acoustic landmarks, and vocal biomarkers. Adolescent depression presents\na significant challenge and is often comorbid with multiple disorders, such as\nsuicidal ideation and sleep disturbances. This presents an additional\nopportunity to integrate multi-task learning (MTL) into our study by\nsimultaneously predicting depression, suicidal ideation, and sleep disturbances\nusing the multimodal formulation. We also propose a longitudinal analysis\nstrategy that models temporal changes across multiple clinical interactions,\nallowing for a comprehensive understanding of the conditions' progression. Our\nproposed approach, featuring trimodal, longitudinal MTL is evaluated on the\nDepression Early Warning dataset. It achieves a balanced accuracy of 70.8%,\nwhich is higher than each of the unimodal, single-task, and non-longitudinal\nmethods."}
{"id": "2505.23946", "pdf": "https://arxiv.org/pdf/2505.23946", "abs": "https://arxiv.org/abs/2505.23946", "authors": ["Yuanzhe Liu", "Ryan Deng", "Tim Kaler", "Xuhao Chen", "Charles E. Leiserson", "Yao Ma", "Jie Chen"], "title": "Lessons Learned: A Multi-Agent Framework for Code LLMs to Learn and Improve", "categories": ["cs.AI", "cs.LG", "cs.MA", "cs.SE"], "comment": null, "summary": "Recent studies show that LLMs possess different skills and specialize in\ndifferent tasks. In fact, we observe that their varied performance occur in\nseveral levels of granularity. For example, in the code optimization task, code\nLLMs excel at different optimization categories and no one dominates others.\nThis observation prompts the question of how one leverages multiple LLM agents\nto solve a coding problem without knowing their complementary strengths a\npriori. We argue that a team of agents can learn from each other's successes\nand failures so as to improve their own performance. Thus, a lesson is the\nknowledge produced by an agent and passed on to other agents in the collective\nsolution process. We propose a lesson-based collaboration framework, design the\nlesson solicitation--banking--selection mechanism, and demonstrate that a team\nof small LLMs with lessons learned can outperform a much larger LLM and other\nmulti-LLM collaboration methods."}
{"id": "2505.23861", "pdf": "https://arxiv.org/pdf/2505.23861", "abs": "https://arxiv.org/abs/2505.23861", "authors": ["Renye Zhang", "Mengyun Yang", "Qichang Zhao", "Jianxin Wang"], "title": "BiBLDR: Bidirectional Behavior Learning for Drug Repositioning", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Drug repositioning aims to identify potential new indications for existing\ndrugs to reduce the time and financial costs associated with developing new\ndrugs. Most existing deep learning-based drug repositioning methods\npredominantly utilize graph-based representations. However, graph-based drug\nrepositioning methods struggle to perform effective inference in cold-start\nscenarios involving novel drugs because of the lack of association information\nwith the diseases. Unlike traditional graph-based approaches, we propose a\nbidirectional behavior learning strategy for drug repositioning, known as\nBiBLDR. This innovative framework redefines drug repositioning as a behavior\nsequential learning task to capture drug-disease interaction patterns. First,\nwe construct bidirectional behavioral sequences based on drug and disease\nsides. The consideration of bidirectional information ensures a more meticulous\nand rigorous characterization of the behavioral sequences. Subsequently, we\npropose a two-stage strategy for drug repositioning. In the first stage, we\nconstruct prototype spaces to characterize the representational attributes of\ndrugs and diseases. In the second stage, these refined prototypes and\nbidirectional behavior sequence data are leveraged to predict potential\ndrug-disease associations. Based on this learning approach, the model can more\nrobustly and precisely capture the interactive relationships between drug and\ndisease features from bidirectional behavioral sequences. Extensive experiments\ndemonstrate that our method achieves state-of-the-art performance on benchmark\ndatasets. Meanwhile, BiBLDR demonstrates significantly superior performance\ncompared to previous methods in cold-start scenarios. Our code is published in\nhttps://github.com/Renyeeah/BiBLDR."}
{"id": "2505.24618", "pdf": "https://arxiv.org/pdf/2505.24618", "abs": "https://arxiv.org/abs/2505.24618", "authors": ["Victor Casamayor Pujol", "Boris Sedlak", "Tommaso Salvatori", "Karl Friston", "Schahram Dustdar"], "title": "Distributed Intelligence in the Computing Continuum with Active Inference", "categories": ["cs.DC", "cs.MA", "cs.SY", "eess.SY"], "comment": null, "summary": "The Computing Continuum (CC) is an emerging Internet-based computing paradigm\nthat spans from local Internet of Things sensors and constrained edge devices\nto large-scale cloud data centers. Its goal is to orchestrate a vast array of\ndiverse and distributed computing resources to support the next generation of\nInternet-based applications. However, the distributed, heterogeneous, and\ndynamic nature of CC platforms demands distributed intelligence for adaptive\nand resilient service management. This article introduces a distributed stream\nprocessing pipeline as a CC use case, where each service is managed by an\nActive Inference (AIF) agent. These agents collaborate to fulfill service needs\nspecified by SLOiDs, a term we introduce to denote Service Level Objectives\nthat are aware of its deployed devices, meaning that non-functional\nrequirements must consider the characteristics of the hosting device. We\ndemonstrate how AIF agents can be modeled and deployed alongside distributed\nservices to manage them autonomously. Our experiments show that AIF agents\nachieve over 90% SLOiD fulfillment when using tested transition models, and\naround 80% when learning the models during deployment. We compare their\nperformance to a multi-agent reinforcement learning algorithm, finding that\nwhile both approaches yield similar results, MARL requires extensive training,\nwhereas AIF agents can operate effectively from the start. Additionally, we\nevaluate the behavior of AIF agents in offloading scenarios, observing a strong\ncapacity for adaptation. Finally, we outline key research directions to advance\nAIF integration in CC platforms."}
{"id": "2505.24304", "pdf": "https://arxiv.org/pdf/2505.24304", "abs": "https://arxiv.org/abs/2505.24304", "authors": ["Haopeng Geng", "Daisuke Saito", "Nobuaki Minematsu"], "title": "A Perception-Based L2 Speech Intelligibility Indicator: Leveraging a Rater's Shadowing and Sequence-to-sequence Voice Conversion", "categories": ["eess.AS", "cs.SD"], "comment": "Accepted by Interspeech 2025", "summary": "Evaluating L2 speech intelligibility is crucial for effective\ncomputer-assisted language learning (CALL). Conventional ASR-based methods\noften focus on native-likeness, which may fail to capture the actual\nintelligibility perceived by human listeners. In contrast, our work introduces\na novel, perception based L2 speech intelligibility indicator that leverages a\nnative rater's shadowing data within a sequence-to-sequence (seq2seq) voice\nconversion framework. By integrating an alignment mechanism and acoustic\nfeature reconstruction, our approach simulates the auditory perception of\nnative listeners, identifying segments in L2 speech that are likely to cause\ncomprehension difficulties. Both objective and subjective evaluations indicate\nthat our method aligns more closely with native judgments than traditional\nASR-based metrics, offering a promising new direction for CALL systems in a\nglobal, multilingual contexts."}
{"id": "2505.23964", "pdf": "https://arxiv.org/pdf/2505.23964", "abs": "https://arxiv.org/abs/2505.23964", "authors": ["Jonas Elsborg", "Tejs Vegge", "Arghya Bhowmik"], "title": "Acoustic Classification of Maritime Vessels using Learnable Filterbanks", "categories": ["cs.SD", "cs.LG", "eess.AS"], "comment": "9 pages, 5 figures, 2 tables", "summary": "Reliably monitoring and recognizing maritime vessels based on acoustic\nsignatures is complicated by the variability of different recording scenarios.\nA robust classification framework must be able to generalize across diverse\nacoustic environments and variable source-sensor distances. To this end, we\npresent a deep learning model with robust performance across different\nrecording scenarios. Using a trainable spectral front-end and temporal feature\nencoder to learn a Gabor filterbank, the model can dynamically emphasize\ndifferent frequency components. Trained on the VTUAD hydrophone recordings from\nthe Strait of Georgia, our model, CATFISH, achieves a state-of-the-art 96.63 %\npercent test accuracy across varying source-sensor distances, surpassing the\nprevious benchmark by over 12 percentage points. We present the model, justify\nour architectural choices, analyze the learned Gabor filters, and perform\nablation studies on sensor data fusion and attention-based pooling."}
{"id": "2505.23790", "pdf": "https://arxiv.org/pdf/2505.23790", "abs": "https://arxiv.org/abs/2505.23790", "authors": ["Shaojie Wang", "Sirui Ding", "Na Zou"], "title": "Rethinking the Understanding Ability across LLMs through Mutual Information", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Recent advances in large language models (LLMs) have revolutionized natural\nlanguage processing, yet evaluating their intrinsic linguistic understanding\nremains challenging. Moving beyond specialized evaluation tasks, we propose an\ninformation-theoretic framework grounded in mutual information (MI) to achieve\nthis. We formalize the understanding as MI between an input sentence and its\nlatent representation (sentence-level MI), measuring how effectively input\ninformation is preserved in latent representation. Given that LLMs learn\nembeddings for individual tokens, we decompose sentence-level MI into\ntoken-level MI between tokens and sentence embeddings, establishing theoretical\nbounds connecting these measures. Based on this foundation, we theoretically\nderive a computable lower bound for token-level MI using Fano's inequality,\nwhich directly relates to token-level recoverability-the ability to predict\noriginal tokens from sentence embedding. We implement this recoverability task\nto comparatively measure MI across different LLMs, revealing that encoder-only\nmodels consistently maintain higher information fidelity than their\ndecoder-only counterparts, with the latter exhibiting a distinctive late-layer\n\"forgetting\" pattern where mutual information is first enhanced and then\ndiscarded. Moreover, fine-tuning to maximize token-level recoverability\nconsistently improves understanding ability of LLMs on tasks without\ntask-specific supervision, demonstrating that mutual information can serve as a\nfoundation for understanding and improving language model capabilities."}
{"id": "2505.23917", "pdf": "https://arxiv.org/pdf/2505.23917", "abs": "https://arxiv.org/abs/2505.23917", "authors": ["Neehar Kondapaneni", "Oisin Mac Aodha", "Pietro Perona"], "title": "Representational Difference Explanations", "categories": ["cs.CV", "cs.AI", "cs.LG", "q-bio.NC"], "comment": "9 pages, 6 figures, 21 supplementary pages, 14 supp figs", "summary": "We propose a method for discovering and visualizing the differences between\ntwo learned representations, enabling more direct and interpretable model\ncomparisons. We validate our method, which we call Representational Differences\nExplanations (RDX), by using it to compare models with known conceptual\ndifferences and demonstrate that it recovers meaningful distinctions where\nexisting explainable AI (XAI) techniques fail. Applied to state-of-the-art\nmodels on challenging subsets of the ImageNet and iNaturalist datasets, RDX\nreveals both insightful representational differences and subtle patterns in the\ndata. Although comparison is a cornerstone of scientific analysis, current\ntools in machine learning, namely post hoc XAI methods, struggle to support\nmodel comparison effectively. Our work addresses this gap by introducing an\neffective and explainable tool for contrasting model representations."}
{"id": "2505.24015", "pdf": "https://arxiv.org/pdf/2505.24015", "abs": "https://arxiv.org/abs/2505.24015", "authors": ["Cheng-Lin Wu", "Hyomin Choi", "Ivan V. BajiÄ"], "title": "Semantics-Guided Generative Image Compression", "categories": ["eess.IV"], "comment": "6 pages, 4 figures, IEEE ICIP 2025", "summary": "Advancements in text-to-image generative AI with large multimodal models are\nspreading into the field of image compression, creating high-quality\nrepresentation of images at extremely low bit rates. This work introduces novel\ncomponents to the existing multimodal image semantic compression (MISC)\napproach, enhancing the quality of the generated images in terms of PSNR and\nperceptual metrics. The new components include semantic segmentation guidance\nfor the generative decoder, as well as content-adaptive diffusion, which\ncontrols the number of diffusion steps based on image characteristics. The\nresults show that our newly introduced methods significantly improve the\nbaseline MISC model while also decreasing the complexity. As a result, both the\nencoding and decoding time are reduced by more than 36%. Moreover, the proposed\ncompression framework outperforms mainstream codecs in terms of perceptual\nsimilarity and quality. The code and visual examples are available."}
{"id": "2505.24253", "pdf": "https://arxiv.org/pdf/2505.24253", "abs": "https://arxiv.org/abs/2505.24253", "authors": ["Ishaan Rawal", "Suryansh Kumar"], "title": "Interactive Video Generation via Domain Adaptation", "categories": ["cs.CV", "cs.AI", "cs.MM"], "comment": "Preprint. Under Review", "summary": "Text-conditioned diffusion models have emerged as powerful tools for\nhigh-quality video generation. However, enabling Interactive Video Generation\n(IVG), where users control motion elements such as object trajectory, remains\nchallenging. Recent training-free approaches introduce attention masking to\nguide trajectory, but this often degrades perceptual quality. We identify two\nkey failure modes in these methods, both of which we interpret as domain shift\nproblems, and propose solutions inspired by domain adaptation. First, we\nattribute the perceptual degradation to internal covariate shift induced by\nattention masking, as pretrained models are not trained to handle masked\nattention. To address this, we propose mask normalization, a pre-normalization\nlayer designed to mitigate this shift via distribution matching. Second, we\naddress initialization gap, where the randomly sampled initial noise does not\nalign with IVG conditioning, by introducing a temporal intrinsic diffusion\nprior that enforces spatio-temporal consistency at each denoising step.\nExtensive qualitative and quantitative evaluations demonstrate that mask\nnormalization and temporal intrinsic denoising improve both perceptual quality\nand trajectory control over the existing state-of-the-art IVG techniques."}
{"id": "2505.23950", "pdf": "https://arxiv.org/pdf/2505.23950", "abs": "https://arxiv.org/abs/2505.23950", "authors": ["Boyuan Chen", "Donghai Hong", "Jiaming Ji", "Jiacheng Zheng", "Bowen Dong", "Jiayi Zhou", "Kaile Wang", "Juntao Dai", "Xuyao Wang", "Wenqi Chen", "Qirui Zheng", "Wenxin Li", "Sirui Han", "Yike Guo", "Yaodong Yang"], "title": "InterMT: Multi-Turn Interleaved Preference Alignment with Human Feedback", "categories": ["cs.AI"], "comment": null, "summary": "As multimodal large models (MLLMs) continue to advance across challenging\ntasks, a key question emerges: What essential capabilities are still missing? A\ncritical aspect of human learning is continuous interaction with the\nenvironment -- not limited to language, but also involving multimodal\nunderstanding and generation. To move closer to human-level intelligence,\nmodels must similarly support multi-turn, multimodal interaction. In\nparticular, they should comprehend interleaved multimodal contexts and respond\ncoherently in ongoing exchanges. In this work, we present an initial\nexploration through the InterMT -- the first preference dataset for multi-turn\nmultimodal interaction, grounded in real human feedback. In this exploration,\nwe particularly emphasize the importance of human oversight, introducing expert\nannotations to guide the process, motivated by the fact that current MLLMs lack\nsuch complex interactive capabilities. InterMT captures human preferences at\nboth global and local levels into nine sub-dimensions, consists of 15.6k\nprompts, 52.6k multi-turn dialogue instances, and 32.4k human-labeled\npreference pairs. To compensate for the lack of capability for multi-modal\nunderstanding and generation, we introduce an agentic workflow that leverages\ntool-augmented MLLMs to construct multi-turn QA instances. To further this\ngoal, we introduce InterMT-Bench to assess the ability of MLLMs in assisting\njudges with multi-turn, multimodal tasks. We demonstrate the utility of\n\\InterMT through applications such as judge moderation and further reveal the\nmulti-turn scaling law of judge model. We hope the open-source of our data can\nhelp facilitate further research on aligning current MLLMs to the next step.\nOur project website can be found at https://pku-intermt.github.io ."}
{"id": "2505.23863", "pdf": "https://arxiv.org/pdf/2505.23863", "abs": "https://arxiv.org/abs/2505.23863", "authors": ["Chang Liu", "Bohao Zhao", "Jingtao Ding", "Huandong Wang", "Yong Li"], "title": "Mamba Integrated with Physics Principles Masters Long-term Chaotic System Forecasting", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Long-term forecasting of chaotic systems from short-term observations remains\na fundamental and underexplored challenge due to the intrinsic sensitivity to\ninitial conditions and the complex geometry of strange attractors. Existing\napproaches often rely on long-term training data or focus on short-term\nsequence correlations, struggling to maintain predictive stability and\ndynamical coherence over extended horizons. We propose PhyxMamba, a novel\nframework that integrates a Mamba-based state-space model with physics-informed\nprinciples to capture the underlying dynamics of chaotic systems. By\nreconstructing the attractor manifold from brief observations using time-delay\nembeddings, PhyxMamba extracts global dynamical features essential for accurate\nforecasting. Our generative training scheme enables Mamba to replicate the\nphysical process, augmented by multi-token prediction and attractor geometry\nregularization for physical constraints, enhancing prediction accuracy and\npreserving key statistical invariants. Extensive evaluations on diverse\nsimulated and real-world chaotic systems demonstrate that PhyxMamba delivers\nsuperior long-term forecasting and faithfully captures essential dynamical\ninvariants from short-term data. This framework opens new avenues for reliably\npredicting chaotic systems under observation-scarce conditions, with broad\nimplications across climate science, neuroscience, epidemiology, and beyond.\nOur code is open-source at https://github.com/tsinghua-fib-lab/PhyxMamba."}
{"id": "2503.02390", "pdf": "https://arxiv.org/pdf/2503.02390", "abs": "https://arxiv.org/abs/2503.02390", "authors": ["Heng Zhou", "Hejia Geng", "Xiangyuan Xue", "Li Kang", "Yiran Qin", "Zhiyong Wang", "Zhenfei Yin", "Lei Bai"], "title": "ReSo: A Reward-driven Self-organizing LLM-based Multi-Agent System for Reasoning Tasks", "categories": ["cs.MA"], "comment": null, "summary": "Multi-agent systems (MAS) have emerged as a promising approach for enhancing\nthe reasoning capabilities of large language models in complex problem-solving;\nhowever, current MAS frameworks suffer from poor flexibility and scalability\nwith underdeveloped optimization strategies. To address these challenges, we\npropose ReSo, which integrates task graph generation with a reward-driven\ntwo-stage agent selection process centered on our Collaborative Reward Model\nthat provides fine-grained reward signals to optimize MAS cooperation. We also\nintroduce an automated data synthesis framework for generating MAS benchmarks\nwithout any human annotations. Experimental results show that ReSo matches or\noutperforms existing methods, achieving 33.7 percent accuracy on Math-MAS and\n32.3 percent accuracy on SciBench-MAS, where other approaches completely fail."}
{"id": "2505.24336", "pdf": "https://arxiv.org/pdf/2505.24336", "abs": "https://arxiv.org/abs/2505.24336", "authors": ["Minsu Kang", "Seolhee Lee", "Choonghyeon Lee", "Namhyun Cho"], "title": "When Humans Growl and Birds Speak: High-Fidelity Voice Conversion from Human to Animal and Designed Sounds", "categories": ["eess.AS", "cs.AI", "cs.LG", "cs.SD", "eess.SP"], "comment": "INTERSPEECH 2025 accepted", "summary": "Human to non-human voice conversion (H2NH-VC) transforms human speech into\nanimal or designed vocalizations. Unlike prior studies focused on dog-sounds\nand 16 or 22.05kHz audio transformation, this work addresses a broader range of\nnon-speech sounds, including natural sounds (lion-roars, birdsongs) and\ndesigned voice (synthetic growls). To accomodate generation of diverse\nnon-speech sounds and 44.1kHz high-quality audio transformation, we introduce a\npreprocessing pipeline and an improved CVAE-based H2NH-VC model, both optimized\nfor human and non-human voices. Experimental results showed that the proposed\nmethod outperformed baselines in quality, naturalness, and similarity MOS,\nachieving effective voice conversion across diverse non-human timbres. Demo\nsamples are available at\nhttps://nc-ai.github.io/speech/publications/nonhuman-vc/"}
{"id": "2505.24115", "pdf": "https://arxiv.org/pdf/2505.24115", "abs": "https://arxiv.org/abs/2505.24115", "authors": ["Bhawana Chhaglani", "Sarmistha Sarna Gomasta", "Yuvraj Agarwal", "Jeremy Gummeson", "Prashant Shenoy"], "title": "FeatureSense: Protecting Speaker Attributes in Always-On Audio Sensing System", "categories": ["cs.SD", "cs.HC", "eess.AS"], "comment": null, "summary": "Audio is a rich sensing modality that is useful for a variety of human\nactivity recognition tasks. However, the ubiquitous nature of smartphones and\nsmart speakers with always-on microphones has led to numerous privacy concerns\nand a lack of trust in deploying these audio-based sensing systems. This paper\naddresses this critical challenge of preserving user privacy when using audio\nfor sensing applications while maintaining utility. While prior work focuses\nprimarily on protecting recoverable speech content, we show that sensitive\nspeaker-specific attributes such as age and gender can still be inferred after\nmasking speech and propose a comprehensive privacy evaluation framework to\nassess this speaker attribute leakage. We design and implement FeatureSense, an\nopen-source library that provides a set of generalizable privacy-aware audio\nfeatures that can be used for wide range of sensing applications. We present an\nadaptive task-specific feature selection algorithm that optimizes the\nprivacy-utility-cost trade-off based on the application requirements. Through\nour extensive evaluation, we demonstrate the high utility of FeatureSense\nacross a diverse set of sensing tasks. Our system outperforms existing privacy\ntechniques by 60.6% in preserving user-specific privacy. This work provides a\nfoundational framework for ensuring trust in audio sensing by enabling\neffective privacy-aware audio classification systems."}
{"id": "2505.23794", "pdf": "https://arxiv.org/pdf/2505.23794", "abs": "https://arxiv.org/abs/2505.23794", "authors": ["Yuan Li", "Qi Luo", "Xiaonan Li", "Bufan Li", "Qinyuan Cheng", "Bo Wang", "Yining Zheng", "Yuxin Wang", "Zhangyue Yin", "Xipeng Qiu"], "title": "R3-RAG: Learning Step-by-Step Reasoning and Retrieval for LLMs via Reinforcement Learning", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Retrieval-Augmented Generation (RAG) integrates external knowledge with Large\nLanguage Models (LLMs) to enhance factual correctness and mitigate\nhallucination. However, dense retrievers often become the bottleneck of RAG\nsystems due to their limited parameters compared to LLMs and their inability to\nperform step-by-step reasoning. While prompt-based iterative RAG attempts to\naddress these limitations, it is constrained by human-designed workflows. To\naddress these limitations, we propose $\\textbf{R3-RAG}$, which uses\n$\\textbf{R}$einforcement learning to make the LLM learn how to\n$\\textbf{R}$eason and $\\textbf{R}$etrieve step by step, thus retrieving\ncomprehensive external knowledge and leading to correct answers. R3-RAG is\ndivided into two stages. We first use cold start to make the model learn the\nmanner of iteratively interleaving reasoning and retrieval. Then we use\nreinforcement learning to further harness its ability to better explore the\nexternal retrieval environment. Specifically, we propose two rewards for\nR3-RAG: 1) answer correctness for outcome reward, which judges whether the\ntrajectory leads to a correct answer; 2) relevance-based document verification\nfor process reward, encouraging the model to retrieve documents that are\nrelevant to the user question, through which we can let the model learn how to\niteratively reason and retrieve relevant documents to get the correct answer.\nExperimental results show that R3-RAG significantly outperforms baselines and\ncan transfer well to different retrievers. We release R3-RAG at\nhttps://github.com/Yuan-Li-FNLP/R3-RAG."}
{"id": "2505.23922", "pdf": "https://arxiv.org/pdf/2505.23922", "abs": "https://arxiv.org/abs/2505.23922", "authors": ["David Ma", "Huaqing Yuan", "Xingjian Wang", "Qianbo Zang", "Tianci Liu", "Xinyang He", "Yanbin Wei", "Jiawei Guo", "Ni Jiahui", "Zhenzhu Yang", "Meng Cao", "Shanghaoran Quan", "Yizhi Li", "Wangchunshu Zhou", "Jiaheng Liu", "Wenhao Huang", "Ge Zhang", "Shiwen Ni", "Xiaojie Jin"], "title": "ScaleLong: A Multi-Timescale Benchmark for Long Video Understanding", "categories": ["cs.CV", "cs.CL"], "comment": null, "summary": "Although long-video understanding demands that models capture hierarchical\ntemporal information -- from clip (seconds) and shot (tens of seconds) to event\n(minutes) and story (hours) -- existing benchmarks either neglect this\nmulti-scale design or scatter scale-specific questions across different videos,\npreventing direct comparison of model performance across timescales on the same\ncontent. To address this, we introduce ScaleLong, the first benchmark to\ndisentangle these factors by embedding questions targeting four hierarchical\ntimescales -- clip (seconds), shot (tens of seconds), event (minutes), and\nstory (hours) -- all within the same video content. This within-content\nmulti-timescale questioning design enables direct comparison of model\nperformance across timescales on identical videos. ScaleLong features 269 long\nvideos (avg.\\ 86\\,min) from 5 main categories and 36 sub-categories, with 4--8\ncarefully designed questions, including at least one question for each\ntimescale. Evaluating 23 MLLMs reveals a U-shaped performance curve, with\nhigher accuracy at the shortest and longest timescales and a dip at\nintermediate levels. Furthermore, ablation studies show that increased visual\ntoken capacity consistently enhances reasoning across all timescales. ScaleLong\noffers a fine-grained, multi-timescale benchmark for advancing MLLM\ncapabilities in long-video understanding. The code and dataset are available\nhttps://github.com/multimodal-art-projection/ScaleLong."}
{"id": "2505.24136", "pdf": "https://arxiv.org/pdf/2505.24136", "abs": "https://arxiv.org/abs/2505.24136", "authors": ["YaÅar Utku AlÃ§alar", "Mehmet AkÃ§akaya"], "title": "Sparsity-Driven Parallel Imaging Consistency for Improved Self-Supervised MRI Reconstruction", "categories": ["eess.IV", "cs.AI", "cs.CV", "cs.LG", "physics.med-ph"], "comment": "IEEE International Conference on Image Processing (ICIP), 2025", "summary": "Physics-driven deep learning (PD-DL) models have proven to be a powerful\napproach for improved reconstruction of rapid MRI scans. In order to train\nthese models in scenarios where fully-sampled reference data is unavailable,\nself-supervised learning has gained prominence. However, its application at\nhigh acceleration rates frequently introduces artifacts, compromising image\nfidelity. To mitigate this shortcoming, we propose a novel way to train PD-DL\nnetworks via carefully-designed perturbations. In particular, we enhance the\nk-space masking idea of conventional self-supervised learning with a novel\nconsistency term that assesses the model's ability to accurately predict the\nadded perturbations in a sparse domain, leading to more reliable and\nartifact-free reconstructions. The results obtained from the fastMRI knee and\nbrain datasets show that the proposed training strategy effectively reduces\naliasing artifacts and mitigates noise amplification at high acceleration\nrates, outperforming state-of-the-art self-supervised methods both visually and\nquantitatively."}
{"id": "2505.24518", "pdf": "https://arxiv.org/pdf/2505.24518", "abs": "https://arxiv.org/abs/2505.24518", "authors": ["Jiatong Shi", "Yifan Cheng", "Bo-Hao Su", "Hye-jin Shim", "Jinchuan Tian", "Samuele Cornell", "Yiwen Zhao", "Siddhant Arora", "Shinji Watanabe"], "title": "ARECHO: Autoregressive Evaluation via Chain-Based Hypothesis Optimization for Speech Multi-Metric Estimation", "categories": ["cs.SD", "cs.MM", "eess.AS"], "comment": null, "summary": "Speech signal analysis poses significant challenges, particularly in tasks\nsuch as speech quality evaluation and profiling, where the goal is to predict\nmultiple perceptual and objective metrics. For instance, metrics like PESQ\n(Perceptual Evaluation of Speech Quality), STOI (Short-Time Objective\nIntelligibility), and MOS (Mean Opinion Score) each capture different aspects\nof speech quality. However, these metrics often have different scales,\nassumptions, and dependencies, making joint estimation non-trivial. To address\nthese issues, we introduce ARECHO (Autoregressive Evaluation via Chain-based\nHypothesis Optimization), a chain-based, versatile evaluation system for speech\nassessment grounded in autoregressive dependency modeling. ARECHO is\ndistinguished by three key innovations: (1) a comprehensive speech information\ntokenization pipeline; (2) a dynamic classifier chain that explicitly captures\ninter-metric dependencies; and (3) a two-step confidence-oriented decoding\nalgorithm that enhances inference reliability. Experiments demonstrate that\nARECHO significantly outperforms the baseline framework across diverse\nevaluation scenarios, including enhanced speech analysis, speech generation\nevaluation, and noisy speech evaluation. Furthermore, its dynamic dependency\nmodeling improves interpretability by capturing inter-metric relationships."}
{"id": "2505.23982", "pdf": "https://arxiv.org/pdf/2505.23982", "abs": "https://arxiv.org/abs/2505.23982", "authors": ["Jerry Junyang Cheung", "Shiyao Shen", "Yuchen Zhuang", "Yinghao Li", "Rampi Ramprasad", "Chao Zhang"], "title": "MSQA: Benchmarking LLMs on Graduate-Level Materials Science Reasoning and Knowledge", "categories": ["cs.AI"], "comment": null, "summary": "Despite recent advances in large language models (LLMs) for materials\nscience, there is a lack of benchmarks for evaluating their domain-specific\nknowledge and complex reasoning abilities. To bridge this gap, we introduce\nMSQA, a comprehensive evaluation benchmark of 1,757 graduate-level materials\nscience questions in two formats: detailed explanatory responses and binary\nTrue/False assessments. MSQA distinctively challenges LLMs by requiring both\nprecise factual knowledge and multi-step reasoning across seven materials\nscience sub-fields, such as structure-property relationships, synthesis\nprocesses, and computational modeling. Through experiments with 10\nstate-of-the-art LLMs, we identify significant gaps in current LLM performance.\nWhile API-based proprietary LLMs achieve up to 84.5% accuracy, open-source\n(OSS) LLMs peak around 60.5%, and domain-specific LLMs often underperform\nsignificantly due to overfitting and distributional shifts. MSQA represents the\nfirst benchmark to jointly evaluate the factual and reasoning capabilities of\nLLMs crucial for LLMs in advanced materials science."}
{"id": "2505.23864", "pdf": "https://arxiv.org/pdf/2505.23864", "abs": "https://arxiv.org/abs/2505.23864", "authors": ["Wei Zhuo", "Zhaohuan Zhan", "Ziduo Yang", "Han Yu"], "title": "Personalized Subgraph Federated Learning with Differentiable Auxiliary Projections", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Federated learning (FL) on graph-structured data typically faces non-IID\nchallenges, particularly in scenarios where each client holds a distinct\nsubgraph sampled from a global graph. In this paper, we introduce Federated\nlearning with Auxiliary projections (FedAux), a personalized subgraph FL\nframework that learns to align, compare, and aggregate heterogeneously\ndistributed local models without sharing raw data or node embeddings. In\nFedAux, each client jointly trains (i) a local GNN and (ii) a learnable\nauxiliary projection vector (APV) that differentiably projects node embeddings\nonto a 1D space. A soft-sorting operation followed by a lightweight 1D\nconvolution refines these embeddings in the ordered space, enabling the APV to\neffectively capture client-specific information. After local training, these\nAPVs serve as compact signatures that the server uses to compute inter-client\nsimilarities and perform similarity-weighted parameter mixing, yielding\npersonalized models while preserving cross-client knowledge transfer. Moreover,\nwe provide rigorous theoretical analysis to establish the convergence and\nrationality of our design. Empirical evaluations across diverse graph\nbenchmarks demonstrate that FedAux substantially outperforms existing baselines\nin both accuracy and personalization performance."}
{"id": "2504.00587", "pdf": "https://arxiv.org/pdf/2504.00587", "abs": "https://arxiv.org/abs/2504.00587", "authors": ["Yingxuan Yang", "Huacan Chai", "Shuai Shao", "Yuanyi Song", "Siyuan Qi", "Renting Rui", "Weinan Zhang"], "title": "AgentNet: Decentralized Evolutionary Coordination for LLM-based Multi-Agent Systems", "categories": ["cs.MA", "cs.CL"], "comment": null, "summary": "The rapid advancement of large language models (LLMs) has enabled the\ndevelopment of multi-agent systems where multiple LLM-based agents collaborate\non complex tasks. However, existing systems often rely on centralized\ncoordination, leading to scalability bottlenecks, reduced adaptability, and\nsingle points of failure. Privacy and proprietary knowledge concerns further\nhinder cross-organizational collaboration, resulting in siloed expertise. We\npropose AgentNet, a decentralized, Retrieval-Augmented Generation (RAG)-based\nframework that enables LLM-based agents to specialize, evolve, and collaborate\nautonomously in a dynamically structured Directed Acyclic Graph (DAG). Unlike\nprior approaches with static roles or centralized control, AgentNet allows\nagents to adjust connectivity and route tasks based on local expertise and\ncontext. AgentNet introduces three key innovations: (1) a fully decentralized\ncoordination mechanism that eliminates the need for a central orchestrator,\nenhancing robustness and emergent intelligence; (2) dynamic agent graph\ntopology that adapts in real time to task demands, ensuring scalability and\nresilience; and (3) a retrieval-based memory system for agents that supports\ncontinual skill refinement and specialization. By minimizing centralized\ncontrol and data exchange, AgentNet enables fault-tolerant, privacy-preserving\ncollaboration across organizations. Experiments show that AgentNet achieves\nhigher task accuracy than both single-agent and centralized multi-agent\nbaselines."}
{"id": "2505.24496", "pdf": "https://arxiv.org/pdf/2505.24496", "abs": "https://arxiv.org/abs/2505.24496", "authors": ["Wenrui Liu", "Qian Chen", "Wen Wang", "Yafeng Chen", "Jin Xu", "Zhifang Guo", "Guanrou Yang", "Weiqin Li", "Xiaoda Yang", "Tao Jin", "Minghui Fang", "Jialong Zuo", "Bai Jionghao", "Zemin Liu"], "title": "Speech Token Prediction via Compressed-to-fine Language Modeling for Speech Generation", "categories": ["eess.AS"], "comment": null, "summary": "Neural audio codecs, used as speech tokenizers, have demonstrated remarkable\npotential in the field of speech generation. However, to ensure high-fidelity\naudio reconstruction, neural audio codecs typically encode audio into long\nsequences of speech tokens, posing a significant challenge for downstream\nlanguage models in long-context modeling. We observe that speech token\nsequences exhibit short-range dependency: due to the monotonic alignment\nbetween text and speech in text-to-speech (TTS) tasks, the prediction of the\ncurrent token primarily relies on its local context, while long-range tokens\ncontribute less to the current token prediction and often contain redundant\ninformation. Inspired by this observation, we propose a\n\\textbf{compressed-to-fine language modeling} approach to address the challenge\nof long sequence speech tokens within neural codec language models: (1)\n\\textbf{Fine-grained Initial and Short-range Information}: Our approach retains\nthe prompt and local tokens during prediction to ensure text alignment and the\nintegrity of paralinguistic information; (2) \\textbf{Compressed Long-range\nContext}: Our approach compresses long-range token spans into compact\nrepresentations to reduce redundant information while preserving essential\nsemantics. Extensive experiments on various neural audio codecs and downstream\nlanguage models validate the effectiveness and generalizability of the proposed\napproach, highlighting the importance of token compression in improving speech\ngeneration within neural codec language models. The demo of audio samples will\nbe available at\nhttps://anonymous.4open.science/r/SpeechTokenPredictionViaCompressedToFinedLM."}
{"id": "2505.24200", "pdf": "https://arxiv.org/pdf/2505.24200", "abs": "https://arxiv.org/abs/2505.24200", "authors": ["Qingzheng Wang", "Jiancheng Sun", "Yifan Peng", "Shinji Watanabe"], "title": "Improving Multilingual Speech Models on ML-SUPERB 2.0: Fine-tuning with Data Augmentation and LID-Aware CTC", "categories": ["cs.SD", "cs.CL", "eess.AS"], "comment": null, "summary": "Multilingual speech processing with self-supervised or supervised pre-trained\nSpeech Foundation Models (SFM) has achieved strong performance on tasks like\nLanguage Identification (LID) and Automatic Speech Recognition (ASR). However,\nthese models struggle with limited resources during fine-tuning. This paper\nenhances multilingual LID and ASR on ML-SUPERB 2.0 by exploring multiple\nstrategies for adapting SFMs, including frozen upstream training, partial\nfine-tuning, and low-rank adaptation. Furthermore, we employ data augmentation\nto mitigate performance gaps in few-shot settings and introduce LID\nConnectionist Temporal Classification (CTC) loss for regularization. Our\napproach achieves a 14% relative improvement in LID accuracy and a 30% relative\nreduction in ASR CER over the baseline on ML-SUPERB 2.0, securing second place\nin the Interspeech 2025 ML-SUPERB 2.0 Challenge."}
{"id": "2505.23796", "pdf": "https://arxiv.org/pdf/2505.23796", "abs": "https://arxiv.org/abs/2505.23796", "authors": ["Christopher Barrie", "Petter TÃ¶rnberg"], "title": "Emergent LLM behaviors are observationally equivalent to data leakage", "categories": ["cs.CL", "cs.GT"], "comment": null, "summary": "Ashery et al. recently argue that large language models (LLMs), when paired\nto play a classic \"naming game,\" spontaneously develop linguistic conventions\nreminiscent of human social norms. Here, we show that their results are better\nexplained by data leakage: the models simply reproduce conventions they already\nencountered during pre-training. Despite the authors' mitigation measures, we\nprovide multiple analyses demonstrating that the LLMs recognize the structure\nof the coordination game and recall its outcomes, rather than exhibit\n\"emergent\" conventions. Consequently, the observed behaviors are\nindistinguishable from memorization of the training corpus. We conclude by\npointing to potential alternative strategies and reflecting more generally on\nthe place of LLMs for social science models."}
{"id": "2505.23926", "pdf": "https://arxiv.org/pdf/2505.23926", "abs": "https://arxiv.org/abs/2505.23926", "authors": ["Xuweiyi Chen", "Wentao Zhou", "Aruni RoyChowdhury", "Zezhou Cheng"], "title": "Point-MoE: Towards Cross-Domain Generalization in 3D Semantic Segmentation via Mixture-of-Experts", "categories": ["cs.CV"], "comment": "Project page: https://uva-computer-vision-lab.github.io/point-moe/", "summary": "While scaling laws have transformed natural language processing and computer\nvision, 3D point cloud understanding has yet to reach that stage. This can be\nattributed to both the comparatively smaller scale of 3D datasets, as well as\nthe disparate sources of the data itself. Point clouds are captured by diverse\nsensors (e.g., depth cameras, LiDAR) across varied domains (e.g., indoor,\noutdoor), each introducing unique scanning patterns, sampling densities, and\nsemantic biases. Such domain heterogeneity poses a major barrier towards\ntraining unified models at scale, especially under the realistic constraint\nthat domain labels are typically inaccessible at inference time. In this work,\nwe propose Point-MoE, a Mixture-of-Experts architecture designed to enable\nlarge-scale, cross-domain generalization in 3D perception. We show that\nstandard point cloud backbones degrade significantly in performance when\ntrained on mixed-domain data, whereas Point-MoE with a simple top-k routing\nstrategy can automatically specialize experts, even without access to domain\nlabels. Our experiments demonstrate that Point-MoE not only outperforms strong\nmulti-domain baselines but also generalizes better to unseen domains. This work\nhighlights a scalable path forward for 3D understanding: letting the model\ndiscover structure in diverse 3D data, rather than imposing it via manual\ncuration or domain supervision."}
{"id": "2505.24160", "pdf": "https://arxiv.org/pdf/2505.24160", "abs": "https://arxiv.org/abs/2505.24160", "authors": ["Junyu Chen", "Shuwen Wei", "Joel Honkamaa", "Pekka Marttinen", "Hang Zhang", "Min Liu", "Yichao Zhou", "Zuopeng Tan", "Zhuoyuan Wang", "Yi Wang", "Hongchao Zhou", "Shunbo Hu", "Yi Zhang", "Qian Tao", "Lukas FÃ¶rner", "Thomas Wendler", "Bailiang Jian", "Benedikt Wiestler", "Tim Hable", "Jin Kim", "Dan Ruan", "Frederic Madesta", "Thilo Sentker", "Wiebke Heyer", "Lianrui Zuo", "Yuwei Dai", "Jing Wu", "Jerry L. Prince", "Harrison Bai", "Yong Du", "Yihao Liu", "Alessa Hering", "Reuben Dorent", "Lasse Hansen", "Mattias P. Heinrich", "Aaron Carass"], "title": "Beyond the LUMIR challenge: The pathway to foundational registration models", "categories": ["eess.IV", "cs.CV"], "comment": null, "summary": "Medical image challenges have played a transformative role in advancing the\nfield, catalyzing algorithmic innovation and establishing new performance\nstandards across diverse clinical applications. Image registration, a\nfoundational task in neuroimaging pipelines, has similarly benefited from the\nLearn2Reg initiative. Building on this foundation, we introduce the Large-scale\nUnsupervised Brain MRI Image Registration (LUMIR) challenge, a next-generation\nbenchmark designed to assess and advance unsupervised brain MRI registration.\nDistinct from prior challenges that leveraged anatomical label maps for\nsupervision, LUMIR removes this dependency by providing over 4,000 preprocessed\nT1-weighted brain MRIs for training without any label maps, encouraging\nbiologically plausible deformation modeling through self-supervision. In\naddition to evaluating performance on 590 held-out test subjects, LUMIR\nintroduces a rigorous suite of zero-shot generalization tasks, spanning\nout-of-domain imaging modalities (e.g., FLAIR, T2-weighted, T2*-weighted),\ndisease populations (e.g., Alzheimer's disease), acquisition protocols (e.g.,\n9.4T MRI), and species (e.g., macaque brains). A total of 1,158 subjects and\nover 4,000 image pairs were included for evaluation. Performance was assessed\nusing both segmentation-based metrics (Dice coefficient, 95th percentile\nHausdorff distance) and landmark-based registration accuracy (target\nregistration error). Across both in-domain and zero-shot tasks, deep\nlearning-based methods consistently achieved state-of-the-art accuracy while\nproducing anatomically plausible deformation fields. The top-performing deep\nlearning-based models demonstrated diffeomorphic properties and inverse\nconsistency, outperforming several leading optimization-based methods, and\nshowing strong robustness to most domain shifts, the exception being a drop in\nperformance on out-of-domain contrasts."}
{"id": "2505.06685", "pdf": "https://arxiv.org/pdf/2505.06685", "abs": "https://arxiv.org/abs/2505.06685", "authors": ["Dawei Huang", "Qing Li", "Chuan Yan", "Zebang Cheng", "Yurong Huang", "Xiang Li", "Bin Li", "Xiaohui Wang", "Zheng Lian", "Xiaojiang Peng"], "title": "Emotion-Qwen: Training Hybrid Experts for Unified Emotion and General Vision-Language Understanding", "categories": ["cs.MM", "cs.CV"], "comment": null, "summary": "Emotion understanding in videos aims to accurately recognize and interpret\nindividuals' emotional states by integrating contextual, visual, textual, and\nauditory cues. While Large Multimodal Models (LMMs) have demonstrated\nsignificant progress in general vision-language (VL) tasks, their performance\nin emotion-specific scenarios remains limited. Moreover, fine-tuning LMMs on\nemotion-related tasks often leads to catastrophic forgetting, hindering their\nability to generalize across diverse tasks. To address these challenges, we\npresent Emotion-Qwen, a tailored multimodal framework designed to enhance both\nemotion understanding and general VL reasoning. Emotion-Qwen incorporates a\nsophisticated Hybrid Compressor based on the Mixture of Experts (MoE) paradigm,\nwhich dynamically routes inputs to balance emotion-specific and general-purpose\nprocessing. The model is pre-trained in a three-stage pipeline on large-scale\ngeneral and emotional image datasets to support robust multimodal\nrepresentations. Furthermore, we construct the Video Emotion Reasoning (VER)\ndataset, comprising more than 40K bilingual video clips with fine-grained\ndescriptive annotations, to further enrich Emotion-Qwen's emotional reasoning\ncapability. Experimental results demonstrate that Emotion-Qwen achieves\nstate-of-the-art performance on multiple emotion recognition benchmarks, while\nmaintaining competitive results on general VL tasks. Code and models are\navailable at https://github.com/24DavidHuang/Emotion-Qwen."}
{"id": "2505.23990", "pdf": "https://arxiv.org/pdf/2505.23990", "abs": "https://arxiv.org/abs/2505.23990", "authors": ["Mingyang Mao", "Mariela M. Perez-Cabarcas", "Utteja Kallakuri", "Nicholas R. Waytowich", "Xiaomin Lin", "Tinoosh Mohsenin"], "title": "Multi-RAG: A Multimodal Retrieval-Augmented Generation System for Adaptive Video Understanding", "categories": ["cs.AI"], "comment": null, "summary": "To effectively engage in human society, the ability to adapt, filter\ninformation, and make informed decisions in ever-changing situations is\ncritical. As robots and intelligent agents become more integrated into human\nlife, there is a growing opportunity-and need-to offload the cognitive burden\non humans to these systems, particularly in dynamic, information-rich\nscenarios.\n  To fill this critical need, we present Multi-RAG, a multimodal\nretrieval-augmented generation system designed to provide adaptive assistance\nto humans in information-intensive circumstances. Our system aims to improve\nsituational understanding and reduce cognitive load by integrating and\nreasoning over multi-source information streams, including video, audio, and\ntext. As an enabling step toward long-term human-robot partnerships, Multi-RAG\nexplores how multimodal information understanding can serve as a foundation for\nadaptive robotic assistance in dynamic, human-centered situations. To evaluate\nits capability in a realistic human-assistance proxy task, we benchmarked\nMulti-RAG on the MMBench-Video dataset, a challenging multimodal video\nunderstanding benchmark. Our system achieves superior performance compared to\nexisting open-source video large language models (Video-LLMs) and large\nvision-language models (LVLMs), while utilizing fewer resources and less input\ndata. The results demonstrate Multi- RAG's potential as a practical and\nefficient foundation for future human-robot adaptive assistance systems in\ndynamic, real-world contexts."}
{"id": "2505.23865", "pdf": "https://arxiv.org/pdf/2505.23865", "abs": "https://arxiv.org/abs/2505.23865", "authors": ["Emanuele Masiero", "Vito Trianni", "Giuseppe Vizzari", "Dimitri Ognibene"], "title": "Combining Deep Architectures for Information Gain estimation and Reinforcement Learning for multiagent field exploration", "categories": ["cs.LG", "cs.AI"], "comment": "4 pages, presented at RLDM 2025", "summary": "Precision agriculture requires efficient autonomous systems for crop\nmonitoring, where agents must explore large-scale environments while minimizing\nresource consumption. This work addresses the problem as an active exploration\ntask in a grid environment representing an agricultural field. Each cell may\ncontain targets (e.g., damaged crops) observable from nine predefined points of\nview (POVs). Agents must infer the number of targets per cell using partial,\nsequential observations.\n  We propose a two-stage deep learning framework. A pre-trained LSTM serves as\na belief model, updating a probabilistic map of the environment and its\nassociated entropy, which defines the expected information gain (IG). This\nallows agents to prioritize informative regions. A key contribution is the\ninclusion of a POV visibility mask in the input, preserving the Markov property\nunder partial observability and avoiding revisits to already explored views.\n  Three agent architectures were compared: an untrained IG-based agent\nselecting actions to maximize entropy reduction; a DQN agent using CNNs over\nlocal 3x3 inputs with belief, entropy, and POV mask; and a Double-CNN DQN agent\nwith wider spatial context. Simulations on 20x20 maps showed that the untrained\nagent performs well despite its simplicity. The DQN agent matches this\nperformance when the POV mask is included, while the Double-CNN agent\nconsistently achieves superior exploration efficiency, especially in larger\nenvironments.\n  Results show that uncertainty-aware policies leveraging entropy, belief\nstates, and visibility tracking lead to robust and scalable exploration. Future\nwork includes curriculum learning, multi-agent cooperation with shared rewards,\ntransformer-based models, and intrinsic motivation mechanisms to further\nenhance learning efficiency and policy generalization."}
{"id": "2505.17115", "pdf": "https://arxiv.org/pdf/2505.17115", "abs": "https://arxiv.org/abs/2505.17115", "authors": ["Ying Zhu", "Heng Zhou", "Rui Su", "Peiqin Zhuang", "Lei Bai"], "title": "Swarm Intelligence Enhanced Reasoning: A Density-Driven Framework for LLM-Based Multi-Agent Optimization", "categories": ["cs.MA", "cs.AI"], "comment": null, "summary": "Recently, many approaches, such as Chain-of-Thought (CoT) prompting and\nMulti-Agent Debate (MAD), have been proposed to further enrich Large Language\nModels' (LLMs) complex problem-solving capacities in reasoning scenarios.\nHowever, these methods may fail to solve complex problems due to the lack of\nability to find optimal solutions. Swarm Intelligence has been serving as a\npowerful tool for finding optima in the field of traditional optimization\nproblems. To this end, we propose integrating swarm intelligence into the\nreasoning process by introducing a novel Agent-based Swarm Intelligence (ASI)\nparadigm. In this paradigm, we formulate LLM reasoning as an optimization\nproblem and use a swarm intelligence scheme to guide a group of LLM-based\nagents in collaboratively searching for optimal solutions. To avoid swarm\nintelligence getting trapped in local optima, we further develop a Swarm\nIntelligence Enhancing Reasoning (SIER) framework, which develops a\ndensity-driven strategy to enhance the reasoning ability. To be specific, we\npropose to perform kernel density estimation and non-dominated sorting to\noptimize both solution quality and diversity simultaneously. In this case, SIER\nefficiently enhances solution space exploration through expanding the diversity\nof the reasoning path. Besides, a step-level quality evaluation is used to help\nagents improve solution quality by correcting low-quality intermediate steps.\nThen, we use quality thresholds to dynamically control the termination of\nexploration and the selection of candidate steps, enabling a more flexible and\nefficient reasoning process. Extensive experiments are ..."}
{"id": "2505.24545", "pdf": "https://arxiv.org/pdf/2505.24545", "abs": "https://arxiv.org/abs/2505.24545", "authors": ["Shota Horiguchi", "Atsushi Ando", "Marc Delcroix", "Naohiro Tawara"], "title": "Pretraining Multi-Speaker Identification for Neural Speaker Diarization", "categories": ["eess.AS", "cs.SD"], "comment": "Accepted to Interspeech 2025", "summary": "End-to-end speaker diarization enables accurate overlap-aware diarization by\njointly estimating multiple speakers' speech activities in parallel. This\napproach is data-hungry, requiring a large amount of labeled conversational\ndata, which cannot be fully obtained from real datasets alone. To address this\nissue, large-scale simulated data is often used for pretraining, but it\nrequires enormous storage and I/O capacity, and simulating data that closely\nresembles real conversations remains challenging. In this paper, we propose\npretraining a model to identify multiple speakers from an input fully\noverlapped mixture as an alternative to pretraining a diarization model. This\nmethod eliminates the need to prepare a large-scale simulated dataset while\nleveraging large-scale speaker recognition datasets for training. Through\ncomprehensive experiments, we demonstrate that the proposed method enables a\nhighly accurate yet lightweight local diarization model without simulated\nconversational data."}
{"id": "2505.24291", "pdf": "https://arxiv.org/pdf/2505.24291", "abs": "https://arxiv.org/abs/2505.24291", "authors": ["Kaidi Wang", "Wenhao Guan", "Ziyue Jiang", "Hukai Huang", "Peijie Chen", "Weijie Wu", "Qingyang Hong", "Lin Li"], "title": "Discl-VC: Disentangled Discrete Tokens and In-Context Learning for Controllable Zero-Shot Voice Conversion", "categories": ["cs.SD", "cs.AI", "eess.AS"], "comment": null, "summary": "Currently, zero-shot voice conversion systems are capable of synthesizing the\nvoice of unseen speakers. However, most existing approaches struggle to\naccurately replicate the speaking style of the source speaker or mimic the\ndistinctive speaking style of the target speaker, thereby limiting the\ncontrollability of voice conversion. In this work, we propose Discl-VC, a novel\nvoice conversion framework that disentangles content and prosody information\nfrom self-supervised speech representations and synthesizes the target\nspeaker's voice through in-context learning with a flow matching transformer.\nTo enable precise control over the prosody of generated speech, we introduce a\nmask generative transformer that predicts discrete prosody tokens in a\nnon-autoregressive manner based on prompts. Experimental results demonstrate\nthe superior performance of Discl-VC in zero-shot voice conversion and its\nremarkable accuracy in prosody control for synthesized speech."}
{"id": "2505.23797", "pdf": "https://arxiv.org/pdf/2505.23797", "abs": "https://arxiv.org/abs/2505.23797", "authors": ["Zaihan Yang", "Ryan Leonard", "Hien Tran", "Rory Driscoll", "Chadbourne Davis"], "title": "Detection of Suicidal Risk on Social Media: A Hybrid Model", "categories": ["cs.CL", "cs.AI", "cs.CY", "cs.LG", "cs.SI"], "comment": null, "summary": "Suicidal thoughts and behaviors are increasingly recognized as a critical\nsocietal concern, highlighting the urgent need for effective tools to enable\nearly detection of suicidal risk. In this work, we develop robust machine\nlearning models that leverage Reddit posts to automatically classify them into\nfour distinct levels of suicide risk severity. We frame this as a multi-class\nclassification task and propose a RoBERTa-TF-IDF-PCA Hybrid model, integrating\nthe deep contextual embeddings from Robustly Optimized BERT Approach (RoBERTa),\na state-of-the-art deep learning transformer model, with the statistical\nterm-weighting of TF-IDF, further compressed with PCA, to boost the accuracy\nand reliability of suicide risk assessment. To address data imbalance and\noverfitting, we explore various data resampling techniques and data\naugmentation strategies to enhance model generalization. Additionally, we\ncompare our model's performance against that of using RoBERTa only, the BERT\nmodel and other traditional machine learning classifiers. Experimental results\ndemonstrate that the hybrid model can achieve improved performance, giving a\nbest weighted $F_{1}$ score of 0.7512."}
{"id": "2505.23952", "pdf": "https://arxiv.org/pdf/2505.23952", "abs": "https://arxiv.org/abs/2505.23952", "authors": ["Adriano Fragomeni", "Dima Damen", "Michael Wray"], "title": "Leveraging Auxiliary Information in Text-to-Video Retrieval: A Review", "categories": ["cs.CV"], "comment": null, "summary": "Text-to-Video (T2V) retrieval aims to identify the most relevant item from a\ngallery of videos based on a user's text query. Traditional methods rely solely\non aligning video and text modalities to compute the similarity and retrieve\nrelevant items. However, recent advancements emphasise incorporating auxiliary\ninformation extracted from video and text modalities to improve retrieval\nperformance and bridge the semantic gap between these modalities. Auxiliary\ninformation can include visual attributes, such as objects; temporal and\nspatial context; and textual descriptions, such as speech and rephrased\ncaptions. This survey comprehensively reviews 81 research papers on\nText-to-Video retrieval that utilise such auxiliary information. It provides a\ndetailed analysis of their methodologies; highlights state-of-the-art results\non benchmark datasets; and discusses available datasets and their auxiliary\ninformation. Additionally, it proposes promising directions for future\nresearch, focusing on different ways to further enhance retrieval performance\nusing this information."}
{"id": "2505.24166", "pdf": "https://arxiv.org/pdf/2505.24166", "abs": "https://arxiv.org/abs/2505.24166", "authors": ["Junyu Chen", "Zirui Jiang", "Jennifer M. Coughlin", "Martin G. Pomper", "Yong Du"], "title": "Deep learning-derived arterial input function", "categories": ["eess.IV"], "comment": null, "summary": "Dynamic positron emission tomography (PET) imaging combined with radiotracer\nkinetic modeling is a powerful technique for visualizing biological processes\nin the brain, offering valuable insights into brain functions and neurological\ndisorders such as Alzheimer's and Parkinson's diseases. Accurate kinetic\nmodeling relies heavily on the use of a metabolite-corrected arterial input\nfunction (AIF), which typically requires invasive and labor-intensive arterial\nblood sampling. While alternative non-invasive approaches have been proposed,\nthey often compromise accuracy or still necessitate at least one invasive blood\nsampling. In this study, we present the deep learning-derived arterial input\nfunction (DLIF), a deep learning framework capable of estimating a\nmetabolite-corrected AIF directly from dynamic PET image sequences without any\nblood sampling. We validated DLIF using existing dynamic PET patient data. We\ncompared DLIF and resulting parametric maps against ground truth measurements.\nOur evaluation shows that DLIF achieves accurate and robust AIF estimation. By\nleveraging deep learning's ability to capture complex temporal dynamics and\nincorporating prior knowledge of typical AIF shapes through basis functions,\nDLIF provides a rapid, accurate, and entirely non-invasive alternative to\ntraditional AIF measurement methods."}
{"id": "2505.23018", "pdf": "https://arxiv.org/pdf/2505.23018", "abs": "https://arxiv.org/abs/2505.23018", "authors": ["Haoqin Sun", "Xuechen Wang", "Jinghua Zhao", "Shiwan Zhao", "Jiaming Zhou", "Hui Wang", "Jiabei He", "Aobo Kong", "Xi Yang", "Yequan Wang", "Yonghua Lin", "Yong Qin"], "title": "EmotionTalk: An Interactive Chinese Multimodal Emotion Dataset With Rich Annotations", "categories": ["cs.MM"], "comment": null, "summary": "In recent years, emotion recognition plays a critical role in applications\nsuch as human-computer interaction, mental health monitoring, and sentiment\nanalysis. While datasets for emotion analysis in languages such as English have\nproliferated, there remains a pressing need for high-quality, comprehensive\ndatasets tailored to the unique linguistic, cultural, and multimodal\ncharacteristics of Chinese. In this work, we propose \\textbf{EmotionTalk}, an\ninteractive Chinese multimodal emotion dataset with rich annotations. This\ndataset provides multimodal information from 19 actors participating in dyadic\nconversational settings, incorporating acoustic, visual, and textual\nmodalities. It includes 23.6 hours of speech (19,250 utterances), annotations\nfor 7 utterance-level emotion categories (happy, surprise, sad, disgust, anger,\nfear, and neutral), 5-dimensional sentiment labels (negative, weakly negative,\nneutral, weakly positive, and positive) and 4-dimensional speech captions\n(speaker, speaking style, emotion and overall). The dataset is well-suited for\nresearch on unimodal and multimodal emotion recognition, missing modality\nchallenges, and speech captioning tasks. To our knowledge, it represents the\nfirst high-quality and versatile Chinese dialogue multimodal emotion dataset,\nwhich is a valuable contribution to research on cross-cultural emotion analysis\nand recognition. Additionally, we conduct experiments on EmotionTalk to\ndemonstrate the effectiveness and quality of the dataset. It will be\nopen-source and freely available for all academic purposes. The dataset and\ncodes will be made available at: https://github.com/NKU-HLT/EmotionTalk."}
{"id": "2505.24036", "pdf": "https://arxiv.org/pdf/2505.24036", "abs": "https://arxiv.org/abs/2505.24036", "authors": ["Amel Gader", "Alsayed Algergawy"], "title": "GenIC: An LLM-Based Framework for Instance Completion in Knowledge Graphs", "categories": ["cs.AI", "cs.IR"], "comment": null, "summary": "Knowledge graph completion aims to address the gaps of knowledge bases by\nadding new triples that represent facts. The complexity of this task depends on\nhow many parts of a triple are already known. Instance completion involves\npredicting the relation-tail pair when only the head is given (h, ?, ?).\nNotably, modern knowledge bases often contain entity descriptions and types,\nwhich can provide valuable context for inferring missing facts. By leveraging\nthese textual descriptions and the ability of large language models to extract\nfacts from them and recognize patterns within the knowledge graph schema, we\npropose an LLM-powered, end-to-end instance completion approach. Specifically,\nwe introduce GenIC: a two-step Generative Instance Completion framework. The\nfirst step focuses on property prediction, treated as a multi-label\nclassification task. The second step is link prediction, framed as a generative\nsequence-to-sequence task. Experimental results on three datasets show that our\nmethod outperforms existing baselines. Our code is available at\nhttps://github.com/amal-gader/genic."}
{"id": "2505.23866", "pdf": "https://arxiv.org/pdf/2505.23866", "abs": "https://arxiv.org/abs/2505.23866", "authors": ["Chengli Tan", "Yubo Zhou", "Haishan Ye", "Guang Dai", "Junmin Liu", "Zengjie Song", "Jiangshe Zhang", "Zixiang Zhao", "Yunda Hao", "Yong Xu"], "title": "Towards Understanding The Calibration Benefits of Sharpness-Aware Minimization", "categories": ["cs.LG", "cs.AI"], "comment": "16 pages", "summary": "Deep neural networks have been increasingly used in safety-critical\napplications such as medical diagnosis and autonomous driving. However, many\nstudies suggest that they are prone to being poorly calibrated and have a\npropensity for overconfidence, which may have disastrous consequences. In this\npaper, unlike standard training such as stochastic gradient descent, we show\nthat the recently proposed sharpness-aware minimization (SAM) counteracts this\ntendency towards overconfidence. The theoretical analysis suggests that SAM\nallows us to learn models that are already well-calibrated by implicitly\nmaximizing the entropy of the predictive distribution. Inspired by this\nfinding, we further propose a variant of SAM, coined as CSAM, to ameliorate\nmodel calibration. Extensive experiments on various datasets, including\nImageNet-1K, demonstrate the benefits of SAM in reducing calibration error.\nMeanwhile, CSAM performs even better than SAM and consistently achieves lower\ncalibration error than other approaches"}
{"id": "2308.13301", "pdf": "https://arxiv.org/pdf/2308.13301", "abs": "https://arxiv.org/abs/2308.13301", "authors": ["Songhua Li", "Lingjie Duan"], "title": "On Incentivizing Social Information Sharing Through Routing Games", "categories": ["cs.GT", "cs.MA"], "comment": "This manuscript is a revised version of our earlier submission", "summary": "Crowdsourcing services, such as Waze, leverage a mass of mobile users to\nlearn massive point-of-interest (PoI) information while traveling and share it\nas a public good. Given that crowdsourced users mind their travel costs and\npossess various preferences over the PoI information along different paths, we\nformulate the problem as a novel non-atomic multi-path routing game with\npositive network externalities among users in social information sharing. In\nthe absence of any incentive design, our price of anarchy (PoA) analysis shows\nthat users' selfish routing on the path with the lowest cost will limit\ninformation diversity and lead to $PoA = 0$ with an arbitrarily large\nefficiency loss from the social optimum. This motivates us to design effective\nincentive mechanisms to remedy while upholding desirable properties such as\nindividual rationality, incentive compatibility, and budget balance for\npractical users. Without requiring a specific user's path preference, we\npresent a non-monetary mechanism called Adaptive Information Restriction (AIR)\nthat reduces non-cooperative users' access to the public good as an indirect\npenalty, which meets all the desirable properties. By meticulously adapting\npenalty fractions to the actual user flows along different paths, our AIR\nachieves non-trivial $PoA = \\frac{1}{4}$ with low complexity $O(k\\log k+\\log\nm)$, where $k$ and $m$ denote the numbers of involved paths and user types,\nrespectively. If the system can further enable pricing for users, we then\npropose a new monetary mechanism called Adaptive Side-Payment (ASP), which\nadaptively charges and rewards users according to their chosen paths,\nrespectively. Our ASP mechanism successively achieves a $PoA = \\frac{1}{2}$\nwith even reduced complexity $O(k\\log k)$. Finally, our theoretical findings\nare well corroborated by our experimental results using a real-world public\ndataset."}
{"id": "2505.24571", "pdf": "https://arxiv.org/pdf/2505.24571", "abs": "https://arxiv.org/abs/2505.24571", "authors": ["Nikola LjubeÅ¡iÄ", "Ivan Porupski", "Peter Rupnik"], "title": "Identifying Primary Stress Across Related Languages and Dialects with Transformer-based Speech Encoder Models", "categories": ["eess.AS", "cs.CL", "cs.SD"], "comment": "Accepted to InterSpeech2025", "summary": "Automating primary stress identification has been an active research field\ndue to the role of stress in encoding meaning and aiding speech comprehension.\nPrevious studies relied mainly on traditional acoustic features and English\ndatasets. In this paper, we investigate the approach of fine-tuning a\npre-trained transformer model with an audio frame classification head. Our\nexperiments use a new Croatian training dataset, with test sets in Croatian,\nSerbian, the Chakavian dialect, and Slovenian. By comparing an SVM classifier\nusing traditional acoustic features with the fine-tuned speech transformer, we\ndemonstrate the transformer's superiority across the board, achieving\nnear-perfect results for Croatian and Serbian, with a 10-point performance drop\nfor the more distant Chakavian and Slovenian. Finally, we show that only a few\nhundred multi-syllabic training words suffice for strong performance. We\nrelease our datasets and model under permissive licenses."}
{"id": "2505.24314", "pdf": "https://arxiv.org/pdf/2505.24314", "abs": "https://arxiv.org/abs/2505.24314", "authors": ["Peijie Chen", "Wenhao Guan", "Kaidi Wang", "Weijie Wu", "Hukai Huang", "Qingyang Hong", "Lin Li"], "title": "DS-Codec: Dual-Stage Training with Mirror-to-NonMirror Architecture Switching for Speech Codec", "categories": ["cs.SD", "eess.AS"], "comment": "Accepted to Interspeech 2025", "summary": "Neural speech codecs are essential for advancing text-to-speech (TTS)\nsystems. With the recent success of large language models in text generation,\ndeveloping high-quality speech tokenizers has become increasingly important.\nThis paper introduces DS-Codec, a novel neural speech codec featuring a\ndual-stage training framework with mirror and non-mirror architectures\nswitching, designed to achieve superior speech reconstruction. We conduct\nextensive experiments and ablation studies to evaluate the effectiveness of our\ntraining strategy and compare the performance of the two architectures. Our\nresults show that the mirrored structure significantly enhances the robustness\nof the learned codebooks, and the training strategy balances the advantages\nbetween mirrored and non-mirrored structures, leading to improved high-fidelity\nspeech reconstruction."}
{"id": "2505.23798", "pdf": "https://arxiv.org/pdf/2505.23798", "abs": "https://arxiv.org/abs/2505.23798", "authors": ["Jian Lan", "Yifei Fu", "Udo Schlegel", "Gengyuan Zhang", "Tanveer Hannan", "Haokun Chen", "Thomas Seidl"], "title": "My Answer Is NOT 'Fair': Mitigating Social Bias in Vision-Language Models via Fair and Biased Residuals", "categories": ["cs.CL", "cs.AI", "cs.CV"], "comment": null, "summary": "Social bias is a critical issue in large vision-language models (VLMs), where\nfairness- and ethics-related problems harm certain groups of people in society.\nIt is unknown to what extent VLMs yield social bias in generative responses. In\nthis study, we focus on evaluating and mitigating social bias on both the\nmodel's response and probability distribution. To do so, we first evaluate four\nstate-of-the-art VLMs on PAIRS and SocialCounterfactuals datasets with the\nmultiple-choice selection task. Surprisingly, we find that models suffer from\ngenerating gender-biased or race-biased responses. We also observe that models\nare prone to stating their responses are fair, but indeed having mis-calibrated\nconfidence levels towards particular social groups. While investigating why\nVLMs are unfair in this study, we observe that VLMs' hidden layers exhibit\nsubstantial fluctuations in fairness levels. Meanwhile, residuals in each layer\nshow mixed effects on fairness, with some contributing positively while some\nlead to increased bias. Based on these findings, we propose a post-hoc method\nfor the inference stage to mitigate social bias, which is training-free and\nmodel-agnostic. We achieve this by ablating bias-associated residuals while\namplifying fairness-associated residuals on model hidden layers during\ninference. We demonstrate that our post-hoc method outperforms the competing\ntraining strategies, helping VLMs have fairer responses and more reliable\nconfidence levels."}
{"id": "2505.23961", "pdf": "https://arxiv.org/pdf/2505.23961", "abs": "https://arxiv.org/abs/2505.23961", "authors": ["Rafi Hassan Chowdhury", "Sabbir Ahmed"], "title": "MangoLeafViT: Leveraging Lightweight Vision Transformer with Runtime Augmentation for Efficient Mango Leaf Disease Classification", "categories": ["cs.CV"], "comment": "Accepted in 27th ICCIT 2024, 6 pages, 5 Figures, 4 tables", "summary": "Ensuring food safety is critical due to its profound impact on public health,\neconomic stability, and global supply chains. Cultivation of Mango, a major\nagricultural product in several South Asian countries, faces high financial\nlosses due to different diseases, affecting various aspects of the entire\nsupply chain. While deep learning-based methods have been explored for mango\nleaf disease classification, there remains a gap in designing solutions that\nare computationally efficient and compatible with low-end devices. In this\nwork, we propose a lightweight Vision Transformer-based pipeline with a\nself-attention mechanism to classify mango leaf diseases, achieving\nstate-of-the-art performance with minimal computational overhead. Our approach\nleverages global attention to capture intricate patterns among disease types\nand incorporates runtime augmentation for enhanced performance. Evaluation on\nthe MangoLeafBD dataset demonstrates a 99.43% accuracy, outperforming existing\nmethods in terms of model size, parameter count, and FLOPs count."}
{"id": "2505.24351", "pdf": "https://arxiv.org/pdf/2505.24351", "abs": "https://arxiv.org/abs/2505.24351", "authors": ["Peng Qi", "Wenxi Qu", "Tianliang Yao", "Haonan Ma", "Dylan Wintle", "Yinyi Lai", "Giorgos Papanastasiou", "Chengjia Wang"], "title": "A Novel Coronary Artery Registration Method Based on Super-pixel Particle Swarm Optimization", "categories": ["eess.IV", "cs.CV"], "comment": null, "summary": "Percutaneous Coronary Intervention (PCI) is a minimally invasive procedure\nthat improves coronary blood flow and treats coronary artery disease. Although\nPCI typically requires 2D X-ray angiography (XRA) to guide catheter placement\nat real-time, computed tomography angiography (CTA) may substantially improve\nPCI by providing precise information of 3D vascular anatomy and status. To\nleverage real-time XRA and detailed 3D CTA anatomy for PCI, accurate multimodal\nimage registration of XRA and CTA is required, to guide the procedure and avoid\ncomplications. This is a challenging process as it requires registration of\nimages from different geometrical modalities (2D -> 3D and vice versa), with\nvariations in contrast and noise levels. In this paper, we propose a novel\nmultimodal coronary artery image registration method based on a swarm\noptimization algorithm, which effectively addresses challenges such as large\ndeformations, low contrast, and noise across these imaging modalities. Our\nalgorithm consists of two main modules: 1) preprocessing of XRA and CTA images\nseparately, and 2) a registration module based on feature extraction using the\nSteger and Superpixel Particle Swarm Optimization algorithms. Our technique was\nevaluated on a pilot dataset of 28 pairs of XRA and CTA images from 10 patients\nwho underwent PCI. The algorithm was compared with four state-of-the-art (SOTA)\nmethods in terms of registration accuracy, robustness, and efficiency. Our\nmethod outperformed the selected SOTA baselines in all aspects. Experimental\nresults demonstrate the significant effectiveness of our algorithm, surpassing\nthe previous benchmarks and proposes a novel clinical approach that can\npotentially have merit for improving patient outcomes in coronary artery\ndisease."}
{"id": "2505.23449", "pdf": "https://arxiv.org/pdf/2505.23449", "abs": "https://arxiv.org/abs/2505.23449", "authors": ["Fanxiao Li", "Jiaying Wu", "Canyuan He", "Wei Zhou"], "title": "CMIE: Combining MLLM Insights with External Evidence for Explainable Out-of-Context Misinformation Detection", "categories": ["cs.MM"], "comment": null, "summary": "Multimodal large language models (MLLMs) have demonstrated impressive\ncapabilities in visual reasoning and text generation. While previous studies\nhave explored the application of MLLM for detecting out-of-context (OOC)\nmisinformation, our empirical analysis reveals two persisting challenges of\nthis paradigm. Evaluating the representative GPT-4o model on direct reasoning\nand evidence augmented reasoning, results indicate that MLLM struggle to\ncapture the deeper relationships-specifically, cases in which the image and\ntext are not directly connected but are associated through underlying semantic\nlinks. Moreover, noise in the evidence further impairs detection accuracy. To\naddress these challenges, we propose CMIE, a novel OOC misinformation detection\nframework that incorporates a Coexistence Relationship Generation (CRG)\nstrategy and an Association Scoring (AS) mechanism. CMIE identifies the\nunderlying coexistence relationships between images and text, and selectively\nutilizes relevant evidence to enhance misinformation detection. Experimental\nresults demonstrate that our approach outperforms existing methods."}
{"id": "2505.24037", "pdf": "https://arxiv.org/pdf/2505.24037", "abs": "https://arxiv.org/abs/2505.24037", "authors": ["Qiao Xiao", "Alan Ansell", "Boqian Wu", "Lu Yin", "Mykola Pechenizkiy", "Shiwei Liu", "Decebal Constantin Mocanu"], "title": "Leave it to the Specialist: Repair Sparse LLMs with Sparse Fine-Tuning via Sparsity Evolution", "categories": ["cs.AI"], "comment": null, "summary": "Large language models (LLMs) have achieved remarkable success across various\ntasks but face deployment challenges due to their massive computational\ndemands. While post-training pruning methods like SparseGPT and Wanda can\neffectively reduce the model size, but struggle to maintain model performance\nat high sparsity levels, limiting their utility for downstream tasks. Existing\nfine-tuning methods, such as full fine-tuning and LoRA, fail to preserve\nsparsity as they require updating the whole dense metrics, not well-suited for\nsparse LLMs. In this paper, we propose Sparsity Evolution Fine-Tuning (SEFT), a\nnovel method designed specifically for sparse LLMs. SEFT dynamically evolves\nthe sparse topology of pruned models during fine-tuning, while preserving the\noverall sparsity throughout the process. The strengths of SEFT lie in its\nability to perform task-specific adaptation through a weight drop-and-grow\nstrategy, enabling the pruned model to self-adapt its sparse connectivity\npattern based on the target dataset. Furthermore, a sensitivity-driven pruning\ncriterion is employed to ensure that the desired sparsity level is consistently\nmaintained throughout fine-tuning. Our experiments on various LLMs, including\nLLaMA families, DeepSeek, and Mistral, across a diverse set of benchmarks\ndemonstrate that SEFT achieves stronger performance while offering superior\nmemory and time efficiency compared to existing baselines. Our code is publicly\navailable at: https://github.com/QiaoXiao7282/SEFT."}
{"id": "2505.23868", "pdf": "https://arxiv.org/pdf/2505.23868", "abs": "https://arxiv.org/abs/2505.23868", "authors": ["Zhaokun Wang", "Jinyu Guo", "Jingwen Pu", "Lingfeng Chen", "Hongli Pu", "Jie Ou. Libo Qin", "Wenhong Tian"], "title": "Noise-Robustness Through Noise: Asymmetric LoRA Adaption with Poisoning Expert", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Current parameter-efficient fine-tuning methods for adapting pre-trained\nlanguage models to downstream tasks are susceptible to interference from noisy\ndata. Conventional noise-handling approaches either rely on laborious data\npre-processing or employ model architecture modifications prone to error\naccumulation. In contrast to existing noise-process paradigms, we propose a\nnoise-robust adaptation method via asymmetric LoRA poisoning experts (LoPE), a\nnovel framework that enhances model robustness to noise only with generated\nnoisy data. Drawing inspiration from the mixture-of-experts architecture, LoPE\nstrategically integrates a dedicated poisoning expert in an asymmetric LoRA\nconfiguration. Through a two-stage paradigm, LoPE performs noise injection on\nthe poisoning expert during fine-tuning to enhance its noise discrimination and\nprocessing ability. During inference, we selectively mask the dedicated\npoisoning expert to leverage purified knowledge acquired by normal experts for\nnoise-robust output. Extensive experiments demonstrate that LoPE achieves\nstrong performance and robustness purely through the low-cost noise injection,\nwhich completely eliminates the requirement of data cleaning."}
{"id": "2502.11705", "pdf": "https://arxiv.org/pdf/2502.11705", "abs": "https://arxiv.org/abs/2502.11705", "authors": ["Georg WÃ¶lflein", "Dyke Ferber", "Daniel Truhn", "Ognjen ArandjeloviÄ", "Jakob Nikolas Kather"], "title": "LLM Agents Making Agent Tools", "categories": ["cs.CL", "cs.AI", "cs.LG", "cs.MA"], "comment": "Accepted at ACL 2025", "summary": "Tool use has turned large language models (LLMs) into powerful agents that\ncan perform complex multi-step tasks by dynamically utilising external software\ncomponents. However, these tools must be implemented in advance by human\ndevelopers, hindering the applicability of LLM agents in domains demanding\nlarge numbers of highly specialised tools, like in life sciences and medicine.\nMotivated by the growing trend of scientific studies accompanied by public code\nrepositories, we propose ToolMaker, an agentic framework that autonomously\ntransforms papers with code into LLM-compatible tools. Given a GitHub URL and\nshort task description, ToolMaker autonomously installs dependencies and\ngenerates code to perform the task, using a closed-loop self-correction\nmechanism for debugging. To evaluate our approach, we introduce a benchmark\ncomprising 15 complex computational tasks spanning various domains with over\n100 unit tests to assess correctness and robustness. Our method correctly\nimplements 80% of the tasks, substantially outperforming current\nstate-of-the-art software engineering agents. ToolMaker therefore is a step\ntowards fully autonomous agent-based scientific workflows. Our code and\nbenchmark are publicly available at https://github.com/KatherLab/ToolMaker."}
{"id": "2505.24576", "pdf": "https://arxiv.org/pdf/2505.24576", "abs": "https://arxiv.org/abs/2505.24576", "authors": ["Jie Zhang", "Haoyin Yan", "Xiaofei Li"], "title": "A Composite Predictive-Generative Approach to Monaural Universal Speech Enhancement", "categories": ["eess.AS"], "comment": "Accepted by IEEE Transactions on Audio, Speech and Language\n  Processing", "summary": "It is promising to design a single model that can suppress various\ndistortions and improve speech quality, i.e., universal speech enhancement\n(USE). Compared to supervised learning-based predictive methods,\ndiffusion-based generative models have shown greater potential due to the\ngenerative capacities from degraded speech with severely damaged information.\nHowever, artifacts may be introduced in highly adverse conditions, and\ndiffusion models often suffer from a heavy computational burden due to many\nsteps for inference. In order to jointly leverage the superiority of prediction\nand generation and overcome the respective defects, in this work we propose a\nuniversal speech enhancement model called PGUSE by combining predictive and\ngenerative modeling. Our model consists of two branches: the predictive branch\ndirectly predicts clean samples from degraded signals, while the generative\nbranch optimizes the denoising objective of diffusion models. We utilize the\noutput fusion and truncated diffusion scheme to effectively integrate\npredictive and generative modeling, where the former directly combines results\nfrom both branches and the latter modifies the reverse diffusion process with\ninitial estimates from the predictive branch. Extensive experiments on several\ndatasets verify the superiority of the proposed model over state-of-the-art\nbaselines, demonstrating the complementarity and benefits of combining\npredictive and generative modeling."}
{"id": "2505.24437", "pdf": "https://arxiv.org/pdf/2505.24437", "abs": "https://arxiv.org/abs/2505.24437", "authors": ["Jin Wang", "Wenbin Jiang", "Xiangbo Wang"], "title": "SwitchCodec: A High-Fidelity Nerual Audio Codec With Sparse Quantization", "categories": ["cs.SD", "eess.AS"], "comment": "5 pages,4 figures", "summary": "We present a universal high-fidelity neural audio compression algorithm that\ncan compress speech, music, and general audio below 3 kbps bandwidth. Although\ncurrent state-of-the-art audio codecs excel in audio compression, their\neffectiveness significantly declines when embedding space is sharply reduced,\nwhich corresponds to higher compression. To address this problem, we propose\nResidual Experts Vector Quantization (REVQ), which significantly expands the\navailable embedding space and improves the performance while hardly sacrificing\nthe bandwidth. Furthermore, we introduce a strategy to ensure that the vast\nembedding space can be fully utilized. Additionally, we propose a STFT-based\ndiscriminator to guide the generator in producing indistinguishable\nspectrograms. We demonstrate that the proposed approach outperforms baseline\nmethods through detailed ablations."}
{"id": "2505.23799", "pdf": "https://arxiv.org/pdf/2505.23799", "abs": "https://arxiv.org/abs/2505.23799", "authors": ["Xiaoyuan Wu", "Weiran Lin", "Omer Akgul", "Lujo Bauer"], "title": "Estimating LLM Consistency: A User Baseline vs Surrogate Metrics", "categories": ["cs.CL", "cs.AI", "cs.HC", "cs.LG"], "comment": null, "summary": "Large language models (LLMs) are prone to hallucinations and sensitive to\nprompt perturbations, often resulting in inconsistent or unreliable generated\ntext. Different methods have been proposed to mitigate such hallucinations and\nfragility -- one of them being measuring the consistency (the model's\nconfidence in the response, or likelihood of generating a similar response when\nresampled) of LLM responses. In previous work, measuring consistency often\nrelied on the probability of a response appearing within a pool of resampled\nresponses, or internal states or logits of responses. However, it is not yet\nclear how well these approaches approximate how humans perceive the consistency\nof LLM responses. We performed a user study (n=2,976) and found current methods\ntypically do not approximate users' perceptions of LLM consistency very well.\nWe propose a logit-based ensemble method for estimating LLM consistency, and we\nshow that this method matches the performance of the best-performing existing\nmetric in estimating human ratings of LLM consistency. Our results suggest that\nmethods of estimating LLM consistency without human evaluation are sufficiently\nimperfect that we suggest evaluation with human input be more broadly used."}
{"id": "2505.23977", "pdf": "https://arxiv.org/pdf/2505.23977", "abs": "https://arxiv.org/abs/2505.23977", "authors": ["Yichen Feng", "Zhangchen Xu", "Fengqing Jiang", "Yuetai Li", "Bhaskar Ramasubramanian", "Luyao Niu", "Bill Yuchen Lin", "Radha Poovendran"], "title": "VisualSphinx: Large-Scale Synthetic Vision Logic Puzzles for RL", "categories": ["cs.CV", "cs.AI", "cs.LG"], "comment": "Project page at https://visualsphinx.github.io/", "summary": "Vision language models (VLMs) are expected to perform effective multimodal\nreasoning and make logically coherent decisions, which is critical to tasks\nsuch as diagram understanding and spatial problem solving. However, current VLM\nreasoning lacks large-scale and well-structured training datasets. To bridge\nthis gap, we propose VisualSphinx, a first-of-its-kind large-scale synthetic\nvisual logical reasoning training data. To tackle the challenge of image\nsynthesis with grounding answers, we propose a rule-to-image synthesis\npipeline, which extracts and expands puzzle rules from seed questions and\ngenerates the code of grounding synthesis image synthesis for puzzle sample\nassembly. Experiments demonstrate that VLM trained using GRPO on VisualSphinx\nbenefit from logical coherence and readability of our dataset and exhibit\nimproved performance on logical reasoning tasks. The enhanced reasoning\ncapabilities developed from VisualSphinx also benefit other reasoning tasks\nsuch as algebraic reasoning, arithmetic reasoning and geometry reasoning."}
{"id": "2505.24407", "pdf": "https://arxiv.org/pdf/2505.24407", "abs": "https://arxiv.org/abs/2505.24407", "authors": ["Wenlong Jiao", "Binglong Li", "Wei Shang", "Ping Wang", "Dongwei Ren"], "title": "Efficient RAW Image Deblurring with Adaptive Frequency Modulation", "categories": ["eess.IV", "cs.CV"], "comment": "Preprint. Submitted to NeurIPS 2025", "summary": "Image deblurring plays a crucial role in enhancing visual clarity across\nvarious applications. Although most deep learning approaches primarily focus on\nsRGB images, which inherently lose critical information during the image signal\nprocessing pipeline, RAW images, being unprocessed and linear, possess superior\nrestoration potential but remain underexplored. Deblurring RAW images presents\nunique challenges, particularly in handling frequency-dependent blur while\nmaintaining computational efficiency. To address these issues, we propose\nFrequency Enhanced Network (FrENet), a framework specifically designed for\nRAW-to-RAW deblurring that operates directly in the frequency domain. We\nintroduce a novel Adaptive Frequency Positional Modulation module, which\ndynamically adjusts frequency components according to their spectral positions,\nthereby enabling precise control over the deblurring process. Additionally,\nfrequency domain skip connections are adopted to further preserve\nhigh-frequency details. Experimental results demonstrate that FrENet surpasses\nstate-of-the-art deblurring methods in RAW image deblurring, achieving\nsignificantly better restoration quality while maintaining high efficiency in\nterms of reduced MACs. Furthermore, FrENet's adaptability enables it to be\nextended to sRGB images, where it delivers comparable or superior performance\ncompared to methods specifically designed for sRGB data. The code will be\navailable at https://github.com/WenlongJiao/FrENet ."}
{"id": "2504.20630", "pdf": "https://arxiv.org/pdf/2504.20630", "abs": "https://arxiv.org/abs/2504.20630", "authors": ["Yu Zhang", "Wenxiang Guo", "Changhao Pan", "Zhiyuan Zhu", "Tao Jin", "Zhou Zhao"], "title": "ISDrama: Immersive Spatial Drama Generation through Multimodal Prompting", "categories": ["eess.AS", "cs.MM", "cs.SD"], "comment": null, "summary": "Multimodal immersive spatial drama generation focuses on creating continuous\nmulti-speaker binaural speech with dramatic prosody based on multimodal\nprompts, with potential applications in AR, VR, and others. This task requires\nsimultaneous modeling of spatial information and dramatic prosody based on\nmultimodal inputs, with high data collection costs. To the best of our\nknowledge, our work is the first attempt to address these challenges. We\nconstruct MRSDrama, the first multimodal recorded spatial drama dataset,\ncontaining binaural drama audios, scripts, videos, geometric poses, and textual\nprompts. Then, we propose ISDrama, the first immersive spatial drama generation\nmodel through multimodal prompting. ISDrama comprises these primary components:\n1) Multimodal Pose Encoder, based on contrastive learning, considering the\nDoppler effect caused by moving speakers to extract unified pose information\nfrom multimodal prompts. 2) Immersive Drama Transformer, a flow-based\nmamba-transformer model that generates high-quality drama, incorporating\nDrama-MOE to select proper experts for enhanced prosody and pose control. We\nalso design a context-consistent classifier-free guidance strategy to\ncoherently generate complete drama. Experimental results show that ISDrama\noutperforms baseline models on objective and subjective metrics. The demos and\ndataset are available at https://aaronz345.github.io/ISDramaDemo."}
{"id": "2505.24073", "pdf": "https://arxiv.org/pdf/2505.24073", "abs": "https://arxiv.org/abs/2505.24073", "authors": ["Chan-Wei Hu", "Yueqi Wang", "Shuo Xing", "Chia-Ju Chen", "Zhengzhong Tu"], "title": "mRAG: Elucidating the Design Space of Multi-modal Retrieval-Augmented Generation", "categories": ["cs.AI"], "comment": "16 pages, 11 figures", "summary": "Large Vision-Language Models (LVLMs) have made remarkable strides in\nmultimodal tasks such as visual question answering, visual grounding, and\ncomplex reasoning. However, they remain limited by static training data,\nsusceptibility to hallucinations, and inability to verify claims against\nup-to-date, external evidence, compromising their performance in dynamic\nreal-world applications. Retrieval-Augmented Generation (RAG) offers a\npractical solution to mitigate these challenges by allowing the LVLMs to access\nlarge-scale knowledge databases via retrieval mechanisms, thereby grounding\nmodel outputs in factual, contextually relevant information. Here in this\npaper, we conduct the first systematic dissection of the multimodal RAG\npipeline for LVLMs, explicitly investigating (1) the retrieval phase: on the\nmodality configurations and retrieval strategies, (2) the re-ranking stage: on\nstrategies to mitigate positional biases and improve the relevance of retrieved\nevidence, and (3) the generation phase: we further investigate how to best\nintegrate retrieved candidates into the final generation process. Finally, we\nextend to explore a unified agentic framework that integrates re-ranking and\ngeneration through self-reflection, enabling LVLMs to select relevant evidence\nand suppress irrelevant context dynamically. Our full-stack exploration of RAG\nfor LVLMs yields substantial insights, resulting in an average performance\nboost of 5% without any fine-tuning."}
{"id": "2505.23870", "pdf": "https://arxiv.org/pdf/2505.23870", "abs": "https://arxiv.org/abs/2505.23870", "authors": ["Yixian Shen", "Qi Bi", "Jia-Hong Huang", "Hongyi Zhu", "Andy D. Pimentel", "Anuj Pathania"], "title": "MaCP: Minimal yet Mighty Adaptation via Hierarchical Cosine Projection", "categories": ["cs.LG", "cs.AI"], "comment": "arXiv admin note: substantial text overlap with arXiv:2410.09103", "summary": "We present a new adaptation method MaCP, Minimal yet Mighty adaptive Cosine\nProjection, that achieves exceptional performance while requiring minimal\nparameters and memory for fine-tuning large foundation models. Its general idea\nis to exploit the superior energy compaction and decorrelation properties of\ncosine projection to improve both model efficiency and accuracy. Specifically,\nit projects the weight change from the low-rank adaptation into the discrete\ncosine space. Then, the weight change is partitioned over different levels of\nthe discrete cosine spectrum, and each partition's most critical frequency\ncomponents are selected. Extensive experiments demonstrate the effectiveness of\nMaCP across a wide range of single-modality tasks, including natural language\nunderstanding, natural language generation, text summarization, as well as\nmulti-modality tasks such as image classification and video understanding. MaCP\nconsistently delivers superior accuracy, significantly reduced computational\ncomplexity, and lower memory requirements compared to existing alternatives."}
{"id": "2505.24736", "pdf": "https://arxiv.org/pdf/2505.24736", "abs": "https://arxiv.org/abs/2505.24736", "authors": ["Julio Cesar Cavalcanti", "Gabriel Skantze"], "title": "\"Dyadosyncrasy\", Idiosyncrasy and Demographic Factors in Turn-Taking", "categories": ["eess.AS", "cs.CL"], "comment": "Accepted to Interspeech 2025", "summary": "Turn-taking in dialogue follows universal constraints but also varies\nsignificantly. This study examines how demographic (sex, age, education) and\nindividual factors shape turn-taking using a large dataset of US English\nconversations (Fisher). We analyze Transition Floor Offset (TFO) and find\nnotable interspeaker variation. Sex and age have small but significant effects\nfemale speakers and older individuals exhibit slightly shorter offsets - while\neducation shows no effect. Lighter topics correlate with shorter TFOs. However,\nindividual differences have a greater impact, driven by a strong idiosyncratic\nand an even stronger \"dyadosyncratic\" component - speakers in a dyad resemble\neach other more than they resemble themselves in different dyads. This suggests\nthat the dyadic relationship and joint activity are the strongest determinants\nof TFO, outweighing demographic influences."}
{"id": "2505.24446", "pdf": "https://arxiv.org/pdf/2505.24446", "abs": "https://arxiv.org/abs/2505.24446", "authors": ["Longjie Luo", "Shenghui Lu", "Lin Li", "Qingyang Hong"], "title": "Pseudo Labels-based Neural Speech Enhancement for the AVSR Task in the MISP-Meeting Challenge", "categories": ["cs.SD", "eess.AS"], "comment": "Accepted by InterSpeech 2025", "summary": "This paper presents our system for the MISP-Meeting Challenge Track 2. The\nprimary difficulty lies in the dataset, which contains strong background noise,\nreverberation, overlapping speech, and diverse meeting topics. To address these\nissues, we (a) designed G-SpatialNet, a speech enhancement (SE) model to\nimprove Guided Source Separation (GSS) signals; (b) proposed TLS, a framework\ncomprising time alignment, level alignment, and signal-to-noise ratio\nfiltering, to generate signal-level pseudo labels for real-recorded far-field\naudio data, thereby facilitating SE models' training; and (c) explored\nfine-tuning strategies, data augmentation, and multimodal information to\nenhance the performance of pre-trained Automatic Speech Recognition (ASR)\nmodels in meeting scenarios. Finally, our system achieved character error rates\n(CERs) of 5.44% and 9.52% on the Dev and Eval sets, respectively, with relative\nimprovements of 64.8% and 52.6% over the baseline, securing second place."}
{"id": "2505.23801", "pdf": "https://arxiv.org/pdf/2505.23801", "abs": "https://arxiv.org/abs/2505.23801", "authors": ["Sajid Hussain", "Muhammad Sohail", "Nauman Ali Khan"], "title": "SEMFED: Semantic-Aware Resource-Efficient Federated Learning for Heterogeneous NLP Tasks", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "13 pages", "summary": "Background: Federated Learning (FL) has emerged as a promising paradigm for\ntraining machine learning models while preserving data privacy. However,\napplying FL to Natural Language Processing (NLP) tasks presents unique\nchallenges due to semantic heterogeneity across clients, vocabulary mismatches,\nand varying resource constraints on edge devices. Objectives: This paper\nintroduces SEMFED, a novel semantic-aware resource-efficient federated learning\nframework specifically designed for heterogeneous NLP tasks. Methods: SEMFED\nincorporates three key innovations: (1) a semantic-aware client selection\nmechanism that balances semantic diversity with resource constraints, (2)\nadaptive NLP-specific model architectures tailored to device capabilities while\npreserving semantic information, and (3) a communication-efficient semantic\nfeature compression technique that significantly reduces bandwidth\nrequirements. Results: Experimental results on various NLP classification tasks\ndemonstrate that SEMFED achieves an 80.5% reduction in communication costs\nwhile maintaining model accuracy above 98%, outperforming state-of-the-art FL\napproaches. Conclusion: SEMFED effectively manages heterogeneous client\nenvironments with varying computational resources, network reliability, and\nsemantic data distributions, making it particularly suitable for real-world\nfederated NLP deployments."}
{"id": "2505.23980", "pdf": "https://arxiv.org/pdf/2505.23980", "abs": "https://arxiv.org/abs/2505.23980", "authors": ["Bayu Adhi Tama", "Mansa Krishna", "Homayra Alam", "Mostafa Cham", "Omar Faruque", "Gong Cheng", "Jianwu Wang", "Mathieu Morlighem", "Vandana Janeja"], "title": "DeepTopoNet: A Framework for Subglacial Topography Estimation on the Greenland Ice Sheets", "categories": ["cs.CV", "cs.LG", "eess.IV"], "comment": "Submitted to SIGSPATIAL 2025", "summary": "Understanding Greenland's subglacial topography is critical for projecting\nthe future mass loss of the ice sheet and its contribution to global sea-level\nrise. However, the complex and sparse nature of observational data,\nparticularly information about the bed topography under the ice sheet,\nsignificantly increases the uncertainty in model projections. Bed topography is\ntraditionally measured by airborne ice-penetrating radar that measures the ice\nthickness directly underneath the aircraft, leaving data gap of tens of\nkilometers in between flight lines. This study introduces a deep learning\nframework, which we call as DeepTopoNet, that integrates radar-derived ice\nthickness observations and BedMachine Greenland data through a novel dynamic\nloss-balancing mechanism. Among all efforts to reconstruct bed topography,\nBedMachine has emerged as one of the most widely used datasets, combining mass\nconservation principles and ice thickness measurements to generate\nhigh-resolution bed elevation estimates. The proposed loss function adaptively\nadjusts the weighting between radar and BedMachine data, ensuring robustness in\nareas with limited radar coverage while leveraging the high spatial resolution\nof BedMachine predictions i.e. bed estimates. Our approach incorporates\ngradient-based and trend surface features to enhance model performance and\nutilizes a CNN architecture designed for subgrid-scale predictions. By\nsystematically testing on the Upernavik Isstr{\\o}m) region, the model achieves\nhigh accuracy, outperforming baseline methods in reconstructing subglacial\nterrain. This work demonstrates the potential of deep learning in bridging\nobservational gaps, providing a scalable and efficient solution to inferring\nsubglacial topography."}
{"id": "2505.24421", "pdf": "https://arxiv.org/pdf/2505.24421", "abs": "https://arxiv.org/abs/2505.24421", "authors": ["Abdul-mojeed Olabisi Ilyas", "Adeleke Maradesa", "Jamal Banzi", "Jianpan Huang", "Henry K. F. Mak", "Kannie W. Y. Chan"], "title": "pyMEAL: A Multi-Encoder Augmentation-Aware Learning for Robust and Generalizable Medical Image Translation", "categories": ["eess.IV", "cs.CV"], "comment": "36 pages, 9 figures, 2 tables", "summary": "Medical imaging is critical for diagnostics, but clinical adoption of\nadvanced AI-driven imaging faces challenges due to patient variability, image\nartifacts, and limited model generalization. While deep learning has\ntransformed image analysis, 3D medical imaging still suffers from data scarcity\nand inconsistencies due to acquisition protocols, scanner differences, and\npatient motion. Traditional augmentation uses a single pipeline for all\ntransformations, disregarding the unique traits of each augmentation and\nstruggling with large data volumes.\n  To address these challenges, we propose a Multi-encoder Augmentation-Aware\nLearning (MEAL) framework that leverages four distinct augmentation variants\nprocessed through dedicated encoders. Three fusion strategies such as\nconcatenation (CC), fusion layer (FL), and adaptive controller block (BD) are\nintegrated to build multi-encoder models that combine augmentation-specific\nfeatures before decoding. MEAL-BD uniquely preserves augmentation-aware\nrepresentations, enabling robust, protocol-invariant feature learning.\n  As demonstrated in a Computed Tomography (CT)-to-T1-weighted Magnetic\nResonance Imaging (MRI) translation study, MEAL-BD consistently achieved the\nbest performance on both unseen- and predefined-test data. On both geometric\ntransformations (like rotations and flips) and non-augmented inputs, MEAL-BD\noutperformed other competing methods, achieving higher mean peak\nsignal-to-noise ratio (PSNR) and structural similarity index measure (SSIM)\nscores. These results establish MEAL as a reliable framework for preserving\nstructural fidelity and generalizing across clinically relevant variability. By\nreframing augmentation as a source of diverse, generalizable features, MEAL\nsupports robust, protocol-invariant learning, advancing clinically reliable\nmedical imaging solutions."}
{"id": "2505.24181", "pdf": "https://arxiv.org/pdf/2505.24181", "abs": "https://arxiv.org/abs/2505.24181", "authors": ["Guanghao Li", "Wenhao Jiang", "Mingfeng Chen", "Yan Li", "Hao Yu", "Shuting Dong", "Tao Ren", "Ming Tang", "Chun Yuan"], "title": "SCOUT: Teaching Pre-trained Language Models to Enhance Reasoning via Flow Chain-of-Thought", "categories": ["cs.AI"], "comment": null, "summary": "Chain of Thought (CoT) prompting improves the reasoning performance of large\nlanguage models (LLMs) by encouraging step by step thinking. However, CoT-based\nmethods depend on intermediate reasoning steps, which limits scalability and\ngeneralization. Recent work explores recursive reasoning, where LLMs reuse\ninternal layers across iterations to refine latent representations without\nexplicit CoT supervision. While promising, these approaches often require\ncostly pretraining and lack a principled framework for how reasoning should\nevolve across iterations. We address this gap by introducing Flow Chain of\nThought (Flow CoT), a reasoning paradigm that models recursive inference as a\nprogressive trajectory of latent cognitive states. Flow CoT frames each\niteration as a distinct cognitive stage deepening reasoning across iterations\nwithout relying on manual supervision. To realize this, we propose SCOUT\n(Stepwise Cognitive Optimization Using Teachers), a lightweight fine tuning\nframework that enables Flow CoT style reasoning without the need for\npretraining. SCOUT uses progressive distillation to align each iteration with a\nteacher of appropriate capacity, and a cross attention based retrospective\nmodule that integrates outputs from previous iterations while preserving the\nmodels original computation flow. Experiments across eight reasoning benchmarks\nshow that SCOUT consistently improves both accuracy and explanation quality,\nachieving up to 1.8% gains under fine tuning. Qualitative analyses further\nreveal that SCOUT enables progressively deeper reasoning across iterations\nrefining both belief formation and explanation granularity. These results not\nonly validate the effectiveness of SCOUT, but also demonstrate the practical\nviability of Flow CoT as a scalable framework for enhancing reasoning in LLMs."}
{"id": "2505.23871", "pdf": "https://arxiv.org/pdf/2505.23871", "abs": "https://arxiv.org/abs/2505.23871", "authors": ["Zeyuan Liu", "Zhihe Yang", "Jiawei Xu", "Rui Yang", "Jiafei Lyu", "Baoxiang Wang", "Yunjian Xu", "Xiu Li"], "title": "ADG: Ambient Diffusion-Guided Dataset Recovery for Corruption-Robust Offline Reinforcement Learning", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Real-world datasets collected from sensors or human inputs are prone to noise\nand errors, posing significant challenges for applying offline reinforcement\nlearning (RL). While existing methods have made progress in addressing\ncorrupted actions and rewards, they remain insufficient for handling corruption\nin high-dimensional state spaces and for cases where multiple elements in the\ndataset are corrupted simultaneously. Diffusion models, known for their strong\ndenoising capabilities, offer a promising direction for this problem-but their\ntendency to overfit noisy samples limits their direct applicability. To\novercome this, we propose Ambient Diffusion-Guided Dataset Recovery (ADG), a\nnovel approach that pioneers the use of diffusion models to tackle data\ncorruption in offline RL. First, we introduce Ambient Denoising Diffusion\nProbabilistic Models (DDPM) from approximated distributions, which enable\nlearning on partially corrupted datasets with theoretical guarantees. Second,\nwe use the noise-prediction property of Ambient DDPM to distinguish between\nclean and corrupted data, and then use the clean subset to train a standard\nDDPM. Third, we employ the trained standard DDPM to refine the previously\nidentified corrupted data, enhancing data quality for subsequent offline RL\ntraining. A notable strength of ADG is its versatility-it can be seamlessly\nintegrated with any offline RL algorithm. Experiments on a range of benchmarks,\nincluding MuJoCo, Kitchen, and Adroit, demonstrate that ADG effectively\nmitigates the impact of corrupted data and improves the robustness of offline\nRL under various noise settings, achieving state-of-the-art results."}
{"id": "2505.23780", "pdf": "https://arxiv.org/pdf/2505.23780", "abs": "https://arxiv.org/abs/2505.23780", "authors": ["Ãmilie Fabre", "Katie Seaborn", "Shuta Koiwai", "Mizuki Watanabe", "Paul Riesch"], "title": "More-than-Human Storytelling: Designing Longitudinal Narrative Engagements with Generative AI", "categories": ["cs.HC", "cs.AI", "cs.CY", "cs.SD", "eess.AS"], "comment": "CHI EA '25", "summary": "Longitudinal engagement with generative AI (GenAI) storytelling agents is a\ntimely but less charted domain. We explored multi-generational experiences with\n\"Dreamsmithy,\" a daily dream-crafting app, where participants (N = 28)\nco-created stories with AI narrator \"Makoto\" every day. Reflections and\ninteractions were captured through a two-week diary study. Reflexive thematic\nanalysis revealed themes likes \"oscillating ambivalence\" and\n\"socio-chronological bonding,\" highlighting the complex dynamics that emerged\nbetween individuals and the AI narrator over time. Findings suggest that while\npeople appreciated the personal notes, opportunities for reflection, and AI\ncreativity, limitations in narrative coherence and control occasionally caused\nfrustration. The results underscore the potential of GenAI for longitudinal\nstorytelling, but also raise critical questions about user agency and ethics.\nWe contribute initial empirical insights and design considerations for\ndeveloping adaptive, more-than-human storytelling systems."}
{"id": "2505.24450", "pdf": "https://arxiv.org/pdf/2505.24450", "abs": "https://arxiv.org/abs/2505.24450", "authors": ["Longjie Luo", "Lin Li", "Qingyang Hong"], "title": "SuPseudo: A Pseudo-supervised Learning Method for Neural Speech Enhancement in Far-field Speech Recognition", "categories": ["cs.SD", "eess.AS"], "comment": "Accepted by InterSpeech 2025", "summary": "Due to the lack of target speech annotations in real-recorded far-field\nconversational datasets, speech enhancement (SE) models are typically trained\non simulated data. However, the trained models often perform poorly in\nreal-world conditions, hindering their application in far-field speech\nrecognition. To address the issue, we (a) propose direct sound estimation (DSE)\nto estimate the oracle direct sound of real-recorded data for SE; and (b)\npresent a novel pseudo-supervised learning method, SuPseudo, which leverages\nDSE-estimates as pseudo-labels and enables SE models to directly learn from and\nadapt to real-recorded data, thereby improving their generalization capability.\nFurthermore, an SE model called FARNET is designed to fully utilize SuPseudo.\nExperiments on the MISP2023 corpus demonstrate the effectiveness of SuPseudo,\nand our system significantly outperforms the previous state-of-the-art. A demo\nof our method can be found at https://EeLLJ.github.io/SuPseudo/."}
{"id": "2505.23802", "pdf": "https://arxiv.org/pdf/2505.23802", "abs": "https://arxiv.org/abs/2505.23802", "authors": ["Suhana Bedi", "Hejie Cui", "Miguel Fuentes", "Alyssa Unell", "Michael Wornow", "Juan M. Banda", "Nikesh Kotecha", "Timothy Keyes", "Yifan Mai", "Mert Oez", "Hao Qiu", "Shrey Jain", "Leonardo Schettini", "Mehr Kashyap", "Jason Alan Fries", "Akshay Swaminathan", "Philip Chung", "Fateme Nateghi", "Asad Aali", "Ashwin Nayak", "Shivam Vedak", "Sneha S. Jain", "Birju Patel", "Oluseyi Fayanju", "Shreya Shah", "Ethan Goh", "Dong-han Yao", "Brian Soetikno", "Eduardo Reis", "Sergios Gatidis", "Vasu Divi", "Robson Capasso", "Rachna Saralkar", "Chia-Chun Chiang", "Jenelle Jindal", "Tho Pham", "Faraz Ghoddusi", "Steven Lin", "Albert S. Chiou", "Christy Hong", "Mohana Roy", "Michael F. Gensheimer", "Hinesh Patel", "Kevin Schulman", "Dev Dash", "Danton Char", "Lance Downing", "Francois Grolleau", "Kameron Black", "Bethel Mieso", "Aydin Zahedivash", "Wen-wai Yim", "Harshita Sharma", "Tony Lee", "Hannah Kirsch", "Jennifer Lee", "Nerissa Ambers", "Carlene Lugtu", "Aditya Sharma", "Bilal Mawji", "Alex Alekseyev", "Vicky Zhou", "Vikas Kakkar", "Jarrod Helzer", "Anurang Revri", "Yair Bannett", "Roxana Daneshjou", "Jonathan Chen", "Emily Alsentzer", "Keith Morse", "Nirmal Ravi", "Nima Aghaeepour", "Vanessa Kennedy", "Akshay Chaudhari", "Thomas Wang", "Sanmi Koyejo", "Matthew P. Lungren", "Eric Horvitz", "Percy Liang", "Mike Pfeffer", "Nigam H. Shah"], "title": "MedHELM: Holistic Evaluation of Large Language Models for Medical Tasks", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "While large language models (LLMs) achieve near-perfect scores on medical\nlicensing exams, these evaluations inadequately reflect the complexity and\ndiversity of real-world clinical practice. We introduce MedHELM, an extensible\nevaluation framework for assessing LLM performance for medical tasks with three\nkey contributions. First, a clinician-validated taxonomy spanning 5 categories,\n22 subcategories, and 121 tasks developed with 29 clinicians. Second, a\ncomprehensive benchmark suite comprising 35 benchmarks (17 existing, 18 newly\nformulated) providing complete coverage of all categories and subcategories in\nthe taxonomy. Third, a systematic comparison of LLMs with improved evaluation\nmethods (using an LLM-jury) and a cost-performance analysis. Evaluation of 9\nfrontier LLMs, using the 35 benchmarks, revealed significant performance\nvariation. Advanced reasoning models (DeepSeek R1: 66% win-rate; o3-mini: 64%\nwin-rate) demonstrated superior performance, though Claude 3.5 Sonnet achieved\ncomparable results at 40% lower estimated computational cost. On a normalized\naccuracy scale (0-1), most models performed strongly in Clinical Note\nGeneration (0.73-0.85) and Patient Communication & Education (0.78-0.83),\nmoderately in Medical Research Assistance (0.65-0.75), and generally lower in\nClinical Decision Support (0.56-0.72) and Administration & Workflow\n(0.53-0.63). Our LLM-jury evaluation method achieved good agreement with\nclinician ratings (ICC = 0.47), surpassing both average clinician-clinician\nagreement (ICC = 0.43) and automated baselines including ROUGE-L (0.36) and\nBERTScore-F1 (0.44). Claude 3.5 Sonnet achieved comparable performance to top\nmodels at lower estimated cost. These findings highlight the importance of\nreal-world, task-specific evaluation for medical use of LLMs and provides an\nopen source framework to enable this."}
{"id": "2505.24002", "pdf": "https://arxiv.org/pdf/2505.24002", "abs": "https://arxiv.org/abs/2505.24002", "authors": ["Vaishnav Ramesh", "Junliang Liu", "Haining Wang", "Md Jahidul Islam"], "title": "DGIQA: Depth-guided Feature Attention and Refinement for Generalizable Image Quality Assessment", "categories": ["cs.CV"], "comment": "18 pages", "summary": "A long-held challenge in no-reference image quality assessment (NR-IQA)\nlearning from human subjective perception is the lack of objective\ngeneralization to unseen natural distortions. To address this, we integrate a\nnovel Depth-Guided cross-attention and refinement (Depth-CAR) mechanism, which\ndistills scene depth and spatial features into a structure-aware representation\nfor improved NR-IQA. This brings in the knowledge of object saliency and\nrelative contrast of the scene for more discriminative feature learning.\nAdditionally, we introduce the idea of TCB (Transformer-CNN Bridge) to fuse\nhigh-level global contextual dependencies from a transformer backbone with\nlocal spatial features captured by a set of hierarchical CNN (convolutional\nneural network) layers. We implement TCB and Depth-CAR as multimodal\nattention-based projection functions to select the most informative features,\nwhich also improve training time and inference efficiency. Experimental results\ndemonstrate that our proposed DGIQA model achieves state-of-the-art (SOTA)\nperformance on both synthetic and authentic benchmark datasets. More\nimportantly, DGIQA outperforms SOTA models on cross-dataset evaluations as well\nas in assessing natural image distortions such as low-light effects, hazy\nconditions, and lens flares."}
{"id": "2505.24605", "pdf": "https://arxiv.org/pdf/2505.24605", "abs": "https://arxiv.org/abs/2505.24605", "authors": ["Ivan Pereira-SÃ¡nchez", "Julia Navarro", "Ana BelÃ©n Petro", "Joan Duran"], "title": "Model-Guided Network with Cluster-Based Operators for Spatio-Spectral Super-Resolution", "categories": ["eess.IV", "cs.CV"], "comment": null, "summary": "This paper addresses the problem of reconstructing a high-resolution\nhyperspectral image from a low-resolution multispectral observation. While\nspatial super-resolution and spectral super-resolution have been extensively\nstudied, joint spatio-spectral super-resolution remains relatively explored. We\npropose an end-to-end model-driven framework that explicitly decomposes the\njoint spatio-spectral super-resolution problem into spatial super-resolution,\nspectral super-resolution and fusion tasks. Each sub-task is addressed by\nunfolding a variational-based approach, where the operators involved in the\nproximal gradient iterative scheme are replaced with tailored learnable\nmodules. In particular, we design an upsampling operator for spatial\nsuper-resolution based on classical back-projection algorithms, adapted to\nhandle arbitrary scaling factors. Spectral reconstruction is performed using\nlearnable cluster-based upsampling and downsampling operators. For image\nfusion, we integrate low-frequency estimation and high-frequency injection\nmodules to combine the spatial and spectral information from spatial\nsuper-resolution and spectral super-resolution outputs. Additionally, we\nintroduce an efficient nonlocal post-processing step that leverages image\nself-similarity by combining a multi-head attention mechanism with residual\nconnections. Extensive evaluations on several datasets and sampling factors\ndemonstrate the effectiveness of our approach. The source code will be\navailable at https://github.com/TAMI-UIB/JSSUNet"}
{"id": "2505.24197", "pdf": "https://arxiv.org/pdf/2505.24197", "abs": "https://arxiv.org/abs/2505.24197", "authors": ["Bhrij Patel", "Ashish Jagmohan", "Aditya Vempaty"], "title": "Learning API Functionality from Demonstrations for Tool-based Agents", "categories": ["cs.AI"], "comment": "18 Pages, 13 Figures, 5 Tables", "summary": "Digital tool-based agents that invoke external Application Programming\nInterfaces (APIs) often rely on documentation to understand API functionality.\nHowever, such documentation is frequently missing, outdated, privatized, or\ninconsistent-hindering the development of reliable, general-purpose agents. In\nthis work, we propose learning API functionality directly from demonstrations\nas a new paradigm applicable in scenarios without documentation. Using existing\nAPI benchmarks, we collect demonstrations from both expert API-based agents and\nfrom self-exploration. To understand what information demonstrations must\nconvey for successful task completion, we extensively study how the number of\ndemonstrations and the use of LLM-generated summaries and evaluations affect\nthe task success rate of the API-based agent. Our experiments across 3 datasets\nand 5 models show that learning functionality from demonstrations remains a\nnon-trivial challenge, even for state-of-the-art LLMs. We find that providing\nexplicit function calls and natural language critiques significantly improves\nthe agent's task success rate due to more accurate parameter filling. We\nanalyze failure modes, identify sources of error, and highlight key open\nchallenges for future work in documentation-free, self-improving, API-based\nagents."}
{"id": "2505.23875", "pdf": "https://arxiv.org/pdf/2505.23875", "abs": "https://arxiv.org/abs/2505.23875", "authors": ["Peter Samoaa", "Marcus Vukojevic", "Morteza Haghir Chehreghani", "Antonio Longa"], "title": "A Benchmark Dataset for Graph Regression with Homogeneous and Multi-Relational Variants", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Graph-level regression underpins many real-world applications, yet public\nbenchmarks remain heavily skewed toward molecular graphs and citation networks.\nThis limited diversity hinders progress on models that must generalize across\nboth homogeneous and heterogeneous graph structures. We introduce RelSC, a new\ngraph-regression dataset built from program graphs that combine syntactic and\nsemantic information extracted from source code. Each graph is labelled with\nthe execution-time cost of the corresponding program, providing a continuous\ntarget variable that differs markedly from those found in existing benchmarks.\nRelSC is released in two complementary variants. RelSC-H supplies rich node\nfeatures under a single (homogeneous) edge type, while RelSC-M preserves the\noriginal multi-relational structure, connecting nodes through multiple edge\ntypes that encode distinct semantic relationships. Together, these variants let\nresearchers probe how representation choice influences model behaviour. We\nevaluate a diverse set of graph neural network architectures on both variants\nof RelSC. The results reveal consistent performance differences between the\nhomogeneous and multi-relational settings, emphasising the importance of\nstructural representation. These findings demonstrate RelSC's value as a\nchallenging and versatile benchmark for advancing graph regression methods."}
{"id": "2505.23821", "pdf": "https://arxiv.org/pdf/2505.23821", "abs": "https://arxiv.org/abs/2505.23821", "authors": ["Lingfeng Yao", "Chenpei Huang", "Shengyao Wang", "Junpei Xue", "Hanqing Guo", "Jiang Liu", "Xun Chen", "Miao Pan"], "title": "SpeechVerifier: Robust Acoustic Fingerprint against Tampering Attacks via Watermarking", "categories": ["cs.CR", "cs.SD", "eess.AS"], "comment": null, "summary": "With the surge of social media, maliciously tampered public speeches,\nespecially those from influential figures, have seriously affected social\nstability and public trust. Existing speech tampering detection methods remain\ninsufficient: they either rely on external reference data or fail to be both\nsensitive to attacks and robust to benign operations, such as compression and\nresampling. To tackle these challenges, we introduce SpeechVerifer to\nproactively verify speech integrity using only the published speech itself,\ni.e., without requiring any external references. Inspired by audio\nfingerprinting and watermarking, SpeechVerifier can (i) effectively detect\ntampering attacks, (ii) be robust to benign operations and (iii) verify the\nintegrity only based on published speeches. Briefly, SpeechVerifier utilizes\nmultiscale feature extraction to capture speech features across different\ntemporal resolutions. Then, it employs contrastive learning to generate\nfingerprints that can detect modifications at varying granularities. These\nfingerprints are designed to be robust to benign operations, but exhibit\nsignificant changes when malicious tampering occurs. To enable speech\nverification in a self-contained manner, the generated fingerprints are then\nembedded into the speech signal by segment-wise watermarking. Without external\nreferences, SpeechVerifier can retrieve the fingerprint from the published\naudio and check it with the embedded watermark to verify the integrity of the\nspeech. Extensive experimental results demonstrate that the proposed\nSpeechVerifier is effective in detecting tampering attacks and robust to benign\noperations."}
{"id": "2505.24486", "pdf": "https://arxiv.org/pdf/2505.24486", "abs": "https://arxiv.org/abs/2505.24486", "authors": ["Falih Gozi Febrinanto", "Kristen Moore", "Chandra Thapa", "Jiangang Ma", "Vidya Saikrishna", "Feng Xia"], "title": "Rehearsal with Auxiliary-Informed Sampling for Audio Deepfake Detection", "categories": ["cs.SD", "cs.AI", "cs.CR", "cs.LG", "eess.AS"], "comment": "Accepted by Interspeech 2025", "summary": "The performance of existing audio deepfake detection frameworks degrades when\nconfronted with new deepfake attacks. Rehearsal-based continual learning (CL),\nwhich updates models using a limited set of old data samples, helps preserve\nprior knowledge while incorporating new information. However, existing\nrehearsal techniques don't effectively capture the diversity of audio\ncharacteristics, introducing bias and increasing the risk of forgetting. To\naddress this challenge, we propose Rehearsal with Auxiliary-Informed Sampling\n(RAIS), a rehearsal-based CL approach for audio deepfake detection. RAIS\nemploys a label generation network to produce auxiliary labels, guiding diverse\nsample selection for the memory buffer. Extensive experiments show RAIS\noutperforms state-of-the-art methods, achieving an average Equal Error Rate\n(EER) of 1.953 % across five experiences. The code is available at:\nhttps://github.com/falihgoz/RAIS."}
{"id": "2505.23804", "pdf": "https://arxiv.org/pdf/2505.23804", "abs": "https://arxiv.org/abs/2505.23804", "authors": ["Terrance Liu", "Shuyi Wang", "Daniel Preotiuc-Pietro", "Yash Chandarana", "Chirag Gupta"], "title": "Calibrating LLMs for Text-to-SQL Parsing by Leveraging Sub-clause Frequencies", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": null, "summary": "While large language models (LLMs) achieve strong performance on text-to-SQL\nparsing, they sometimes exhibit unexpected failures in which they are\nconfidently incorrect. Building trustworthy text-to-SQL systems thus requires\neliciting reliable uncertainty measures from the LLM. In this paper, we study\nthe problem of providing a calibrated confidence score that conveys the\nlikelihood of an output query being correct. Our work is the first to establish\na benchmark for post-hoc calibration of LLM-based text-to-SQL parsing. In\nparticular, we show that Platt scaling, a canonical method for calibration,\nprovides substantial improvements over directly using raw model output\nprobabilities as confidence scores. Furthermore, we propose a method for\ntext-to-SQL calibration that leverages the structured nature of SQL queries to\nprovide more granular signals of correctness, named \"sub-clause frequency\"\n(SCF) scores. Using multivariate Platt scaling (MPS), our extension of the\ncanonical Platt scaling technique, we combine individual SCF scores into an\noverall accurate and calibrated score. Empirical evaluation on two popular\ntext-to-SQL datasets shows that our approach of combining MPS and SCF yields\nfurther improvements in calibration and the related task of error detection\nover traditional Platt scaling."}
{"id": "2505.24007", "pdf": "https://arxiv.org/pdf/2505.24007", "abs": "https://arxiv.org/abs/2505.24007", "authors": ["Nokimul Hasan Arif", "Shadman Rabby", "Md Hefzul Hossain Papon", "Sabbir Ahmed"], "title": "Preemptive Hallucination Reduction: An Input-Level Approach for Multimodal Language Model", "categories": ["cs.CV"], "comment": "Submitted for review in AJSE Springer, 21 pages, 4 figures, 4 Tables", "summary": "Visual hallucinations in Large Language Models (LLMs), where the model\ngenerates responses that are inconsistent with the visual input, pose a\nsignificant challenge to their reliability, particularly in contexts where\nprecise and trustworthy outputs are critical. Current research largely\nemphasizes post-hoc correction or model-specific fine-tuning strategies, with\nlimited exploration of preprocessing techniques to address hallucination issues\nat the input stage. This study presents a novel ensemble-based preprocessing\nframework that adaptively selects the most appropriate filtering approach --\nnoise reduced (NR), edge enhanced (EE), or unaltered input (org) based on the\ntype of question posed, resulting into reduced hallucination without requiring\nany modifications to the underlying model architecture or training pipeline.\nEvaluated on the `HaloQuest' dataset -- a benchmark designed to test multimodal\nreasoning on visually complex inputs, our method achieves a 44.3% reduction in\nhallucination rates, as measured by Natural Language Inference (NLI) scores\nusing SelfCheckGPT. This demonstrates that intelligent input conditioning alone\ncan significantly enhance factual grounding in LLM responses. The findings\nhighlight the importance of adaptive preprocessing techniques in mitigating\nhallucinations, paving the way for more reliable multimodal systems capable of\naddressing real-world challenges."}
{"id": "2505.24687", "pdf": "https://arxiv.org/pdf/2505.24687", "abs": "https://arxiv.org/abs/2505.24687", "authors": ["Shengyuan Liu", "Wenting Chen", "Boyun Zheng", "Wentao Pan", "Xiang Li", "Yixuan Yuan"], "title": "TumorGen: Boundary-Aware Tumor-Mask Synthesis with Rectified Flow Matching", "categories": ["eess.IV", "cs.CV"], "comment": "10 pages, 4 figures", "summary": "Tumor data synthesis offers a promising solution to the shortage of annotated\nmedical datasets. However, current approaches either limit tumor diversity by\nusing predefined masks or employ computationally expensive two-stage processes\nwith multiple denoising steps, causing computational inefficiency.\nAdditionally, these methods typically rely on binary masks that fail to capture\nthe gradual transitions characteristic of tumor boundaries. We present\nTumorGen, a novel Boundary-Aware Tumor-Mask Synthesis with Rectified Flow\nMatching for efficient 3D tumor synthesis with three key components: a\nBoundary-Aware Pseudo Mask Generation module that replaces strict binary masks\nwith flexible bounding boxes; a Spatial-Constraint Vector Field Estimator that\nsimultaneously synthesizes tumor latents and masks using rectified flow\nmatching to ensure computational efficiency; and a VAE-guided mask refiner that\nenhances boundary realism. TumorGen significantly improves computational\nefficiency by requiring fewer sampling steps while maintaining pathological\naccuracy through coarse and fine-grained spatial constraints. Experimental\nresults demonstrate TumorGen's superior performance over existing tumor\nsynthesis methods in both efficiency and realism, offering a valuable\ncontribution to AI-driven cancer diagnostics."}
{"id": "2505.24201", "pdf": "https://arxiv.org/pdf/2505.24201", "abs": "https://arxiv.org/abs/2505.24201", "authors": ["Xu He", "Di Wu", "Yan Zhai", "Kun Sun"], "title": "SentinelAgent: Graph-based Anomaly Detection in Multi-Agent Systems", "categories": ["cs.AI"], "comment": null, "summary": "The rise of large language model (LLM)-based multi-agent systems (MAS)\nintroduces new security and reliability challenges. While these systems show\ngreat promise in decomposing and coordinating complex tasks, they also face\nmulti-faceted risks across prompt manipulation, unsafe tool usage, and emergent\nagent miscoordination. Existing guardrail mechanisms offer only partial\nprotection, primarily at the input-output level, and fall short in addressing\nsystemic or multi-point failures in MAS. In this work, we present a\nsystem-level anomaly detection framework tailored for MAS, integrating\nstructural modeling with runtime behavioral oversight. Our approach consists of\ntwo components. First, we propose a graph-based framework that models agent\ninteractions as dynamic execution graphs, enabling semantic anomaly detection\nat node, edge, and path levels. Second, we introduce a pluggable SentinelAgent,\nan LLM-powered oversight agent that observes, analyzes, and intervenes in MAS\nexecution based on security policies and contextual reasoning. By bridging\nabstract detection logic with actionable enforcement, our method detects not\nonly single-point faults and prompt injections but also multi-agent collusion\nand latent exploit paths. We validate our framework through two case studies,\nincluding an email assistant and Microsoft's Magentic-One system, demonstrating\nits ability to detect covert risks and provide explainable root-cause\nattribution. Our work lays the foundation for more trustworthy, monitorable,\nand secure agent-based AI ecosystems."}
{"id": "2505.23876", "pdf": "https://arxiv.org/pdf/2505.23876", "abs": "https://arxiv.org/abs/2505.23876", "authors": ["Polad Geidarov"], "title": "A comparative analysis of a neural network with calculated weights and a neural network with random generation of weights based on the training dataset size", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "The paper discusses the capabilities of multilayer perceptron neural networks\nimplementing metric recognition methods, for which the values of the weights\nare calculated analytically by formulas. Comparative experiments in training a\nneural network with pre-calculated weights and with random initialization of\nweights on different sizes of the MNIST training dataset are carried out. The\nresults of the experiments show that a multilayer perceptron with\npre-calculated weights can be trained much faster and is much more robust to\nthe reduction of the training dataset."}
{"id": "2505.24229", "pdf": "https://arxiv.org/pdf/2505.24229", "abs": "https://arxiv.org/abs/2505.24229", "authors": ["Luong Ho", "Khanh Le", "Vinh Pham", "Bao Nguyen", "Tan Tran", "Duc Chau"], "title": "Dynamic Context-Aware Streaming Pretrained Language Model For Inverse Text Normalization", "categories": ["cs.CL", "cs.SD", "eess.AS"], "comment": "Accepted to INTERSPEECH 2025", "summary": "Inverse Text Normalization (ITN) is crucial for converting spoken Automatic\nSpeech Recognition (ASR) outputs into well-formatted written text, enhancing\nboth readability and usability. Despite its importance, the integration of\nstreaming ITN within streaming ASR remains largely unexplored due to challenges\nin accuracy, efficiency, and adaptability, particularly in low-resource and\nlimited-context scenarios. In this paper, we introduce a streaming pretrained\nlanguage model for ITN, leveraging pretrained linguistic representations for\nimproved robustness. To address streaming constraints, we propose Dynamic\nContext-Aware during training and inference, enabling adaptive chunk size\nadjustments and the integration of right-context information. Experimental\nresults demonstrate that our method achieves accuracy comparable to\nnon-streaming ITN and surpasses existing streaming ITN models on a Vietnamese\ndataset, all while maintaining low latency, ensuring seamless integration into\nASR systems."}
{"id": "2505.24820", "pdf": "https://arxiv.org/pdf/2505.24820", "abs": "https://arxiv.org/abs/2505.24820", "authors": ["Yu Xi", "Xiaoyu Gu", "Haoyu Li", "Jun Song", "Bo Zheng", "Kai Yu"], "title": "Masked Self-distilled Transducer-based Keyword Spotting with Semi-autoregressive Decoding", "categories": ["cs.SD", "eess.AS"], "comment": null, "summary": "RNN-T-based keyword spotting (KWS) with autoregressive decoding~(AR) has\ngained attention due to its streaming architecture and superior performance.\nHowever, the simplicity of the prediction network in RNN-T poses an overfitting\nissue, especially under challenging scenarios, resulting in degraded\nperformance. In this paper, we propose a masked self-distillation (MSD)\ntraining strategy that avoids RNN-Ts overly relying on prediction networks to\nalleviate overfitting. Such training enables masked non-autoregressive (NAR)\ndecoding, which fully masks the RNN-T predictor output during KWS decoding. In\naddition, we propose a semi-autoregressive (SAR) decoding approach to integrate\nthe advantages of AR and NAR decoding. Our experiments across multiple KWS\ndatasets demonstrate that MSD training effectively alleviates overfitting. The\nSAR decoding method preserves the superior performance of AR decoding while\nbenefits from the overfitting suppression of NAR decoding, achieving excellent\nresults."}
{"id": "2505.23806", "pdf": "https://arxiv.org/pdf/2505.23806", "abs": "https://arxiv.org/abs/2505.23806", "authors": ["Sihyeon Lee", "Hyunjoo Song", "Jong-chan Lee", "Yoon Jin Lee", "Boram Lee", "Hee-Eon Lim", "Dongyeong Kim", "Jinwook Seo", "Bohyoung Kim"], "title": "MedOrchestra: A Hybrid Cloud-Local LLM Approach for Clinical Data Interpretation", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Deploying large language models (LLMs) in clinical settings faces critical\ntrade-offs: cloud LLMs, with their extensive parameters and superior\nperformance, pose risks to sensitive clinical data privacy, while local LLMs\npreserve privacy but often fail at complex clinical interpretation tasks. We\npropose MedOrchestra, a hybrid framework where a cloud LLM decomposes complex\nclinical tasks into manageable subtasks and prompt generation, while a local\nLLM executes these subtasks in a privacy-preserving manner. Without accessing\nclinical data, the cloud LLM generates and validates subtask prompts using\nclinical guidelines and synthetic test cases. The local LLM executes subtasks\nlocally and synthesizes outputs generated by the cloud LLM. We evaluate\nMedOrchestra on pancreatic cancer staging using 100 radiology reports under\nNCCN guidelines. On free-text reports, MedOrchestra achieves 70.21% accuracy,\noutperforming local model baselines (without guideline: 48.94%, with guideline:\n56.59%) and board-certified clinicians (gastroenterologists: 59.57%, surgeons:\n65.96%, radiologists: 55.32%). On structured reports, MedOrchestra reaches\n85.42% accuracy, showing clear superiority across all settings."}
{"id": "2505.24023", "pdf": "https://arxiv.org/pdf/2505.24023", "abs": "https://arxiv.org/abs/2505.24023", "authors": ["Sangwon Jung", "Alex Oesterling", "Claudio Mayrink Verdun", "Sajani Vithana", "Taesup Moon", "Flavio P. Calmon"], "title": "Multi-Group Proportional Representation for Text-to-Image Models", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Text-to-image (T2I) generative models can create vivid, realistic images from\ntextual descriptions. As these models proliferate, they expose new concerns\nabout their ability to represent diverse demographic groups, propagate\nstereotypes, and efface minority populations. Despite growing attention to the\n\"safe\" and \"responsible\" design of artificial intelligence (AI), there is no\nestablished methodology to systematically measure and control representational\nharms in image generation. This paper introduces a novel framework to measure\nthe representation of intersectional groups in images generated by T2I models\nby applying the Multi-Group Proportional Representation (MPR) metric. MPR\nevaluates the worst-case deviation of representation statistics across given\npopulation groups in images produced by a generative model, allowing for\nflexible and context-specific measurements based on user requirements. We also\ndevelop an algorithm to optimize T2I models for this metric. Through\nexperiments, we demonstrate that MPR can effectively measure representation\nstatistics across multiple intersectional groups and, when used as a training\nobjective, can guide models toward a more balanced generation across\ndemographic groups while maintaining generation quality."}
{"id": "2505.24739", "pdf": "https://arxiv.org/pdf/2505.24739", "abs": "https://arxiv.org/abs/2505.24739", "authors": ["Xinliu Zhong", "Ruiying Liu", "Emily S. Nichols", "Xuzhe Zhang", "Andrew F. Laine", "Emma G. Duerden", "Yun Wang"], "title": "Contrast-Invariant Self-supervised Segmentation for Quantitative Placental MRI", "categories": ["eess.IV", "cs.CV"], "comment": "8 pages, 20 figures", "summary": "Accurate placental segmentation is essential for quantitative analysis of the\nplacenta. However, this task is particularly challenging in T2*-weighted\nplacental imaging due to: (1) weak and inconsistent boundary contrast across\nindividual echoes; (2) the absence of manual ground truth annotations for all\necho times; and (3) motion artifacts across echoes caused by fetal and maternal\nmovement. In this work, we propose a contrast-augmented segmentation framework\nthat leverages complementary information across multi-echo T2*-weighted MRI to\nlearn robust, contrast-invariant representations. Our method integrates: (i)\nmasked autoencoding (MAE) for self-supervised pretraining on unlabeled\nmulti-echo slices; (ii) masked pseudo-labeling (MPL) for unsupervised domain\nadaptation across echo times; and (iii) global-local collaboration to align\nfine-grained features with global anatomical context. We further introduce a\nsemantic matching loss to encourage representation consistency across echoes of\nthe same subject. Experiments on a clinical multi-echo placental MRI dataset\ndemonstrate that our approach generalizes effectively across echo times and\noutperforms both single-echo and naive fusion baselines. To our knowledge, this\nis the first work to systematically exploit multi-echo T2*-weighted MRI for\nplacental segmentation."}
{"id": "2505.24208", "pdf": "https://arxiv.org/pdf/2505.24208", "abs": "https://arxiv.org/abs/2505.24208", "authors": ["Wenhan Yang", "Spencer Stice", "Ali Payani", "Baharan Mirzasoleiman"], "title": "Bootstrapping LLM Robustness for VLM Safety via Reducing the Pretraining Modality Gap", "categories": ["cs.AI"], "comment": null, "summary": "Ensuring Vision-Language Models (VLMs) generate safe outputs is crucial for\ntheir reliable deployment. However, LVLMs suffer from drastic safety\ndegradation compared to their LLM backbone. Even blank or irrelevant images can\ntrigger LVLMs to generate harmful responses to prompts that would otherwise be\nrefused in text-only contexts. The modality gap between image and text\nrepresentations has been recently hypothesized to contribute to safety\ndegradation of LVLMs. However, if and how the amount of modality gap affects\nLVLMs' safety is not studied. In this work, we show that the amount of modality\ngap is highly inversely correlated with VLMs' safety. Then, we show that this\nmodality gap is introduced during pretraining LVLMs and persists through\nfine-tuning. Inspired by this observation, we propose a regularization to\nreduce the modality gap during pretraining. Our extensive experiments on LLaVA\nv1.5, ShareGPT4V, and MiniGPT-4 show that our method substantially improves\nsafety alignment of LVLMs, reducing unsafe rate by up to 16.3% without\ncompromising performance, and can further boost existing defenses by up to\n18.2%."}
{"id": "2505.23878", "pdf": "https://arxiv.org/pdf/2505.23878", "abs": "https://arxiv.org/abs/2505.23878", "authors": ["Jing Ma", "Chenhao Dang", "Mingjie Liao"], "title": "Actor-Critic based Online Data Mixing For Language Model Pre-Training", "categories": ["cs.LG", "cs.AI"], "comment": "11 pages, 4 figures", "summary": "The coverage and composition of pretraining data significantly impacts the\ngeneralization ability of Large Language Models (LLMs). To reduce the carbon\nfootprint and financial costs of training, some data mixing methods, which\napplied the optimized domain weights of a small proxy model to train a larger\none, were proposed. However, these methods did not evolute with the training\ndynamics. The existing online data mixing (ODM) method addressed this\nlimitation by applying the multi-armed bandit algorithm as data sampling\nstrategy. Yet, it did not consider the intra-domain interactions. In this\npaper, we develop an actor-critic based online data mixing (AC-ODM) method,\nwhich captures the varying domain weights by auxiliary actor-critic networks\nand consider the intra-domain interactions with the reward function. While\nconstructing the dataset to pretrain a large target LLM, we directly apply the\nactor, which is trained with a small proxy LLM as the environment, as the\nsampling strategy. The transfer of sampling strategy can not only ensure the\nefficiency of dynamical data mixing, but also expedite the convergence of\npretraining the target LLM. Numerical results demonstrate that AC-ODM-410M,\nwhich invokes the sampling strategy obtained by a proxy LLM with 410M\nparameters, reaching the optimal validation perplexity of ODM 71% faster, and\nimproves performance on the zero-shot MMLU benchmark by 27.5% of accuracy,\nabout 2.23x better on pass@1 of HumanEval benchmark."}
{"id": "2505.23807", "pdf": "https://arxiv.org/pdf/2505.23807", "abs": "https://arxiv.org/abs/2505.23807", "authors": ["Yuli Chen", "Bo Cheng", "Jiale Han", "Yingying Zhang", "Yingting Li", "Shuhao Zhang"], "title": "DLP: Dynamic Layerwise Pruning in Large Language Models", "categories": ["cs.CL", "cs.AI"], "comment": "Accepted by ICML 2025", "summary": "Pruning has recently been widely adopted to reduce the parameter scale and\nimprove the inference efficiency of Large Language Models (LLMs). Mainstream\npruning techniques often rely on uniform layerwise pruning strategies, which\ncan lead to severe performance degradation at high sparsity levels. Recognizing\nthe varying contributions of different layers in LLMs, recent studies have\nshifted their focus toward non-uniform layerwise pruning. However, these\napproaches often rely on pre-defined values, which can result in suboptimal\nperformance. To overcome these limitations, we propose a novel method called\nDynamic Layerwise Pruning (DLP). This approach adaptively determines the\nrelative importance of each layer by integrating model weights with input\nactivation information, assigning pruning rates accordingly. Experimental\nresults show that DLP effectively preserves model performance at high sparsity\nlevels across multiple LLMs. Specifically, at 70% sparsity, DLP reduces the\nperplexity of LLaMA2-7B by 7.79 and improves the average accuracy by 2.7%\ncompared to state-of-the-art methods. Moreover, DLP is compatible with various\nexisting LLM compression techniques and can be seamlessly integrated into\nParameter-Efficient Fine-Tuning (PEFT). We release the code at\nhttps://github.com/ironartisan/DLP to facilitate future research."}
{"id": "2505.24025", "pdf": "https://arxiv.org/pdf/2505.24025", "abs": "https://arxiv.org/abs/2505.24025", "authors": ["Chenbin Pan", "Wenbin He", "Zhengzhong Tu", "Liu Ren"], "title": "DINO-R1: Incentivizing Reasoning Capability in Vision Foundation Models", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "The recent explosive interest in the reasoning capabilities of large language\nmodels, such as DeepSeek-R1, has demonstrated remarkable success through\nreinforcement learning-based fine-tuning frameworks, exemplified by methods\nlike Group Relative Policy Optimization (GRPO). However, such reasoning\nabilities remain underexplored and notably absent in vision foundation models,\nincluding representation models like the DINO series. In this work, we propose\n\\textbf{DINO-R1}, the first such attempt to incentivize visual in-context\nreasoning capabilities of vision foundation models using reinforcement\nlearning. Specifically, DINO-R1 introduces \\textbf{Group Relative Query\nOptimization (GRQO)}, a novel reinforcement-style training strategy explicitly\ndesigned for query-based representation models, which computes query-level\nrewards based on group-normalized alignment quality. We also apply\nKL-regularization to stabilize the objectness distribution to reduce the\ntraining instability. This joint optimization enables dense and expressive\nsupervision across queries while mitigating overfitting and distributional\ndrift. Building upon Grounding-DINO, we train a series of DINO-R1 family models\nthat integrate a visual prompt encoder and a visual-guided query selection\nmechanism. Extensive experiments on COCO, LVIS, and ODinW demonstrate that\nDINO-R1 significantly outperforms supervised fine-tuning baselines, achieving\nstrong generalization in both open-vocabulary and closed-set visual prompting\nscenarios."}
{"id": "2505.24799", "pdf": "https://arxiv.org/pdf/2505.24799", "abs": "https://arxiv.org/abs/2505.24799", "authors": ["Aditya Retnanto", "Son Le", "Sebastian Mueller", "Armin Leitner", "Konrad Schindler", "Yohan Iddawela", "Michael Riffler"], "title": "Beyond Pretty Pictures: Combined Single- and Multi-Image Super-resolution for Sentinel-2 Images", "categories": ["eess.IV", "cs.CV"], "comment": null, "summary": "Super-resolution aims to increase the resolution of satellite images by\nreconstructing high-frequency details, which go beyond na\\\"ive upsampling. This\nhas particular relevance for Earth observation missions like Sentinel-2, which\noffer frequent, regular coverage at no cost; but at coarse resolution. Its\npixel footprint is too large to capture small features like houses, streets, or\nhedge rows. To address this, we present SEN4X, a hybrid super-resolution\narchitecture that combines the advantages of single-image and multi-image\ntechniques. It combines temporal oversampling from repeated Sentinel-2\nacquisitions with a learned prior from high-resolution Pl\\'eiades Neo data. In\ndoing so, SEN4X upgrades Sentinel-2 imagery to 2.5 m ground sampling distance.\nWe test the super-resolved images on urban land-cover classification in Hanoi,\nVietnam. We find that they lead to a significant performance improvement over\nstate-of-the-art super-resolution baselines."}
{"id": "2505.24226", "pdf": "https://arxiv.org/pdf/2505.24226", "abs": "https://arxiv.org/abs/2505.24226", "authors": ["Yibo Zhao", "Jiapeng Zhu", "Ye Guo", "Kangkang He", "Xiang Li"], "title": "E^2GraphRAG: Streamlining Graph-based RAG for High Efficiency and Effectiveness", "categories": ["cs.AI"], "comment": "16 pages", "summary": "Graph-based RAG methods like GraphRAG have shown promising global\nunderstanding of the knowledge base by constructing hierarchical entity graphs.\nHowever, they often suffer from inefficiency and rely on manually pre-defined\nquery modes, limiting practical use. In this paper, we propose E^2GraphRAG, a\nstreamlined graph-based RAG framework that improves both Efficiency and\nEffectiveness. During the indexing stage, E^2GraphRAG constructs a summary tree\nwith large language models and an entity graph with SpaCy based on document\nchunks. We then construct bidirectional indexes between entities and chunks to\ncapture their many-to-many relationships, enabling fast lookup during both\nlocal and global retrieval. For the retrieval stage, we design an adaptive\nretrieval strategy that leverages the graph structure to retrieve and select\nbetween local and global modes. Experiments show that E^2GraphRAG achieves up\nto 10 times faster indexing than GraphRAG and 100 times speedup over LightRAG\nin retrieval while maintaining competitive QA performance."}
{"id": "2505.23879", "pdf": "https://arxiv.org/pdf/2505.23879", "abs": "https://arxiv.org/abs/2505.23879", "authors": ["Caio Cheohen", "VinnÃ­cius M. S. Gomes", "Manuela L. da Silva"], "title": "CNN-LSTM Hybrid Model for AI-Driven Prediction of COVID-19 Severity from Spike Sequences and Clinical Data", "categories": ["cs.LG", "cs.AI", "68T07, 62P10, 92C50, 68T05", "I.2.6; I.5.1; J.3"], "comment": "12 pages, 4 figures, 4 tables", "summary": "The COVID-19 pandemic, caused by SARS-CoV-2, highlighted the critical need\nfor accurate prediction of disease severity to optimize healthcare resource\nallocation and patient management. The spike protein, which facilitates viral\nentry into host cells, exhibits high mutation rates, particularly in the\nreceptor-binding domain, influencing viral pathogenicity. Artificial\nintelligence approaches, such as deep learning, offer promising solutions for\nleveraging genomic and clinical data to predict disease outcomes. Objective:\nThis study aimed to develop a hybrid CNN-LSTM deep learning model to predict\nCOVID-19 severity using spike protein sequences and associated clinical\nmetadata from South American patients. Methods: We retrieved 9,570 spike\nprotein sequences from the GISAID database, of which 3,467 met inclusion\ncriteria after standardization. The dataset included 2,313 severe and 1,154\nmild cases. A feature engineering pipeline extracted features from sequences,\nwhile demographic and clinical variables were one-hot encoded. A hybrid\nCNN-LSTM architecture was trained, combining CNN layers for local pattern\nextraction and an LSTM layer for long-term dependency modeling. Results: The\nmodel achieved an F1 score of 82.92%, ROC-AUC of 0.9084, precision of 83.56%,\nand recall of 82.85%, demonstrating robust classification performance. Training\nstabilized at 85% accuracy with minimal overfitting. The most prevalent\nlineages (P.1, AY.99.2) and clades (GR, GK) aligned with regional\nepidemiological trends, suggesting potential associations between viral\ngenetics and clinical outcomes. Conclusion: The CNN-LSTM hybrid model\neffectively predicted COVID-19 severity using spike protein sequences and\nclinical data, highlighting the utility of AI in genomic surveillance and\nprecision public health. Despite limitations, this approach provides a\nframework for early severity prediction in future outbreaks."}
{"id": "2505.24347", "pdf": "https://arxiv.org/pdf/2505.24347", "abs": "https://arxiv.org/abs/2505.24347", "authors": ["Yangui Fang", "Baixu Cheng", "Jing Peng", "Xu Li", "Yu Xi", "Chengwei Zhang", "Guohui Zhong"], "title": "Fewer Hallucinations, More Verification: A Three-Stage LLM-Based Framework for ASR Error Correction", "categories": ["cs.CL", "eess.AS"], "comment": null, "summary": "Automatic Speech Recognition (ASR) error correction aims to correct\nrecognition errors while preserving accurate text. Although traditional\napproaches demonstrate moderate effectiveness, LLMs offer a paradigm that\neliminates the need for training and labeled data. However, directly using LLMs\nwill encounter hallucinations problem, which may lead to the modification of\nthe correct text. To address this problem, we propose the Reliable LLM\nCorrection Framework (RLLM-CF), which consists of three stages: (1) error\npre-detection, (2) chain-of-thought sub-tasks iterative correction, and (3)\nreasoning process verification. The advantage of our method is that it does not\nrequire additional information or fine-tuning of the model, and ensures the\ncorrectness of the LLM correction under multi-pass programming. Experiments on\nAISHELL-1, AISHELL-2, and Librispeech show that the GPT-4o model enhanced by\nour framework achieves 21%, 11%, 9%, and 11.4% relative reductions in CER/WER."}
{"id": "2505.24493", "pdf": "https://arxiv.org/pdf/2505.24493", "abs": "https://arxiv.org/abs/2505.24493", "authors": ["Xin Jing", "Jiadong Wang", "Iosif Tsangko", "Andreas Triantafyllopoulos", "BjÃ¶rn W. Schuller"], "title": "MELT: Towards Automated Multimodal Emotion Data Annotation by Leveraging LLM Embedded Knowledge", "categories": ["cs.AI", "cs.SD", "eess.AS"], "comment": null, "summary": "Although speech emotion recognition (SER) has advanced significantly with\ndeep learning, annotation remains a major hurdle. Human annotation is not only\ncostly but also subject to inconsistencies annotators often have different\npreferences and may lack the necessary contextual knowledge, which can lead to\nvaried and inaccurate labels. Meanwhile, Large Language Models (LLMs) have\nemerged as a scalable alternative for annotating text data. However, the\npotential of LLMs to perform emotional speech data annotation without human\nsupervision has yet to be thoroughly investigated. To address these problems,\nwe apply GPT-4o to annotate a multimodal dataset collected from the sitcom\nFriends, using only textual cues as inputs. By crafting structured text\nprompts, our methodology capitalizes on the knowledge GPT-4o has accumulated\nduring its training, showcasing that it can generate accurate and contextually\nrelevant annotations without direct access to multimodal inputs. Therefore, we\npropose MELT, a multimodal emotion dataset fully annotated by GPT-4o. We\ndemonstrate the effectiveness of MELT by fine-tuning four self-supervised\nlearning (SSL) backbones and assessing speech emotion recognition performance\nacross emotion datasets. Additionally, our subjective experiments\\' results\ndemonstrate a consistence performance improvement on SER."}
{"id": "2505.23808", "pdf": "https://arxiv.org/pdf/2505.23808", "abs": "https://arxiv.org/abs/2505.23808", "authors": ["Lin Mu", "Xiaoyu Wang", "Li Ni", "Yang Li", "Zhize Wu", "Peiquan Jin", "Yiwen Zhang"], "title": "DenseLoRA: Dense Low-Rank Adaptation of Large Language Models", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Low-rank adaptation (LoRA) has been developed as an efficient approach for\nadapting large language models (LLMs) by fine-tuning two low-rank matrices,\nthereby reducing the number of trainable parameters. However, prior research\nindicates that many of the weights in these matrices are redundant, leading to\ninefficiencies in parameter utilization. To address this limitation, we\nintroduce Dense Low-Rank Adaptation (DenseLoRA), a novel approach that enhances\nparameter efficiency while achieving superior performance compared to LoRA.\nDenseLoRA builds upon the concept of representation fine-tuning, incorporating\na single Encoder-Decoder to refine and compress hidden representations across\nall adaptation layers before applying adaptation. Instead of relying on two\nredundant low-rank matrices as in LoRA, DenseLoRA adapts LLMs through a dense\nlow-rank matrix, improving parameter utilization and adaptation efficiency. We\nevaluate DenseLoRA on various benchmarks, showing that it achieves 83.8%\naccuracy with only 0.01% of trainable parameters, compared to LoRA's 80.8%\naccuracy with 0.70% of trainable parameters on LLaMA3-8B. Additionally, we\nconduct extensive experiments to systematically assess the impact of\nDenseLoRA's components on overall model performance. Code is available at\nhttps://github.com/mulin-ahu/DenseLoRA."}
{"id": "2505.24026", "pdf": "https://arxiv.org/pdf/2505.24026", "abs": "https://arxiv.org/abs/2505.24026", "authors": ["Numair Nadeem", "Muhammad Hamza Asad", "Saeed Anwar", "Abdul Bais"], "title": "MaskAdapt: Unsupervised Geometry-Aware Domain Adaptation Using Multimodal Contextual Learning and RGB-Depth Masking", "categories": ["cs.CV", "cs.AI"], "comment": "11 pages, 5 figures, presented at the IEEE/CVF Conference on Computer\n  Vision and Pattern Recognition (CVPR) 2025. Reviewer comments available upon\n  request", "summary": "Semantic segmentation of crops and weeds is crucial for site-specific farm\nmanagement; however, most existing methods depend on labor intensive\npixel-level annotations. A further challenge arises when models trained on one\nfield (source domain) fail to generalize to new fields (target domain) due to\ndomain shifts, such as variations in lighting, camera setups, soil composition,\nand crop growth stages. Unsupervised Domain Adaptation (UDA) addresses this by\nenabling adaptation without target-domain labels, but current UDA methods\nstruggle with occlusions and visual blending between crops and weeds, leading\nto misclassifications in real-world conditions. To overcome these limitations,\nwe introduce MaskAdapt, a novel approach that enhances segmentation accuracy\nthrough multimodal contextual learning by integrating RGB images with features\nderived from depth data. By computing depth gradients from depth maps, our\nmethod captures spatial transitions that help resolve texture ambiguities.\nThese gradients, through a cross-attention mechanism, refines RGB feature\nrepresentations, resulting in sharper boundary delineation. In addition, we\npropose a geometry-aware masking strategy that applies horizontal, vertical,\nand stochastic masks during training. This encourages the model to focus on the\nbroader spatial context for robust visual recognition. Evaluations on real\nagricultural datasets demonstrate that MaskAdapt consistently outperforms\nexisting State-of-the-Art (SOTA) UDA methods, achieving improved segmentation\nmean Intersection over Union (mIOU) across diverse field conditions."}
{"id": "2505.23992", "pdf": "https://arxiv.org/pdf/2505.23992", "abs": "https://arxiv.org/abs/2505.23992", "authors": ["Weijian Zhang", "Hashan K. Weerasooriya", "Stanley Chan"], "title": "Ultrafast High-Flux Single-Photon LiDAR Simulator via Neural Mapping", "categories": ["eess.SP", "eess.IV", "physics.optics"], "comment": "Accepted to ICIP 2025", "summary": "Efficient simulation of photon registrations in single-photon LiDAR (SPL) is\nessential for applications such as depth estimation under high-flux conditions,\nwhere hardware dead time significantly distorts photon measurements. However,\nthe conventional wisdom is computationally intensive due to their inherently\nsequential, photon-by-photon processing. In this paper, we propose a\nlearning-based framework that accelerates the simulation process by modeling\nthe photon count and directly predicting the photon registration probability\ndensity function (PDF) using an autoencoder (AE). Our method achieves high\naccuracy in estimating both the total number of registered photons and their\ntemporal distribution, while substantially reducing simulation time. Extensive\nexperiments validate the effectiveness and efficiency of our approach,\nhighlighting its potential to enable fast and accurate SPL simulations for\ndata-intensive imaging tasks in the high-flux regime."}
{"id": "2505.24230", "pdf": "https://arxiv.org/pdf/2505.24230", "abs": "https://arxiv.org/abs/2505.24230", "authors": ["Murari Ambati"], "title": "ProofNet++: A Neuro-Symbolic System for Formal Proof Verification with Self-Correction", "categories": ["cs.AI"], "comment": "6 pages, 2 figures", "summary": "We propose ProofNet++, a neuro-symbolic framework that enhances automated\ntheorem proving by combining large language models (LLMs) with formal proof\nverification and self-correction mechanisms. Current LLM-based systems suffer\nfrom hallucinated logical steps and unverifiable reasoning. ProofNet++\nmitigates these limitations by integrating symbolic proof tree supervision, a\nreinforcement learning loop using verifiers as reward functions, and an\niterative self-correction module. Our experiments on miniF2F, Lean's mathlib,\nand HOL Light show that ProofNet++ significantly improves proof accuracy,\ncorrectness, and formal verifiability over prior models. We provide theoretical\nanalysis of the convergence and stability of the verifier-guided RL framework\nand release our datasets and codebase for future research."}
{"id": "2505.23884", "pdf": "https://arxiv.org/pdf/2505.23884", "abs": "https://arxiv.org/abs/2505.23884", "authors": ["Tianyuan Zhang", "Sai Bi", "Yicong Hong", "Kai Zhang", "Fujun Luan", "Songlin Yang", "Kalyan Sunkavalli", "William T. Freeman", "Hao Tan"], "title": "Test-Time Training Done Right", "categories": ["cs.LG", "cs.CL", "cs.CV"], "comment": "32 pages, 11 figures", "summary": "Test-Time Training (TTT) models context dependencies by adapting part of the\nmodel's weights (referred to as fast weights) during inference. This fast\nweight, akin to recurrent states in RNNs, stores temporary memories of past\ntokens in the current sequence. Existing TTT methods struggled to show\neffectiveness in handling long-context data, due to their inefficiency on\nmodern GPUs. The TTT layers in many of these approaches operate with extremely\nlow FLOPs utilization (often <5%) because they deliberately apply small online\nminibatch sizes (e.g., updating fast weights every 16 or 64 tokens). Moreover,\na small minibatch implies fine-grained block-wise causal dependencies in the\ndata, unsuitable for data beyond 1D ordered sequences, like sets or\nN-dimensional grids such as images or videos. In contrast, we pursue the\nopposite direction by using an extremely large chunk update, ranging from 2K to\n1M tokens across tasks of varying modalities, which we refer to as Large Chunk\nTest-Time Training (LaCT). It improves hardware utilization by orders of\nmagnitude, and more importantly, facilitates scaling of nonlinear state size\n(up to 40% of model parameters), hence substantially improving state capacity,\nall without requiring cumbersome and error-prone kernel implementations. It\nalso allows easy integration of sophisticated optimizers, e.g. Muon for online\nupdates. We validate our approach across diverse modalities and tasks,\nincluding novel view synthesis with image set, language models, and\nauto-regressive video diffusion. Our approach can scale up to 14B-parameter AR\nvideo diffusion model on sequences up to 56K tokens. In our longest sequence\nexperiment, we perform novel view synthesis with 1 million context length. We\nhope this work will inspire and accelerate new research in the field of\nlong-context modeling and test-time training. Website:\nhttps://tianyuanzhang.com/projects/ttt-done-right"}
{"id": "2505.24656", "pdf": "https://arxiv.org/pdf/2505.24656", "abs": "https://arxiv.org/abs/2505.24656", "authors": ["Dimitrios Damianos", "Georgios Paraskevopoulos", "Alexandros Potamianos"], "title": "MSDA: Combining Pseudo-labeling and Self-Supervision for Unsupervised Domain Adaptation in ASR", "categories": ["cs.CL", "cs.SD", "eess.AS"], "comment": null, "summary": "In this work, we investigate the Meta PL unsupervised domain adaptation\nframework for Automatic Speech Recognition (ASR). We introduce a Multi-Stage\nDomain Adaptation pipeline (MSDA), a sample-efficient, two-stage adaptation\napproach that integrates self-supervised learning with semi-supervised\ntechniques. MSDA is designed to enhance the robustness and generalization of\nASR models, making them more adaptable to diverse conditions. It is\nparticularly effective for low-resource languages like Greek and in weakly\nsupervised scenarios where labeled data is scarce or noisy. Through extensive\nexperiments, we demonstrate that Meta PL can be applied effectively to ASR\ntasks, achieving state-of-the-art results, significantly outperforming\nstate-of-the-art methods, and providing more robust solutions for unsupervised\ndomain adaptation in ASR. Our ablations highlight the necessity of utilizing a\ncascading approach when combining self-supervision with self-training."}
{"id": "2505.24691", "pdf": "https://arxiv.org/pdf/2505.24691", "abs": "https://arxiv.org/abs/2505.24691", "authors": ["Gerard I. GÃ¡llego", "Oriol Pareras", "MartÃ­ Cortada Garcia", "Lucas Takanori", "Javier Hernando"], "title": "Speech-to-Text Translation with Phoneme-Augmented CoT: Enhancing Cross-Lingual Transfer in Low-Resource Scenarios", "categories": ["cs.CL", "cs.SD", "eess.AS"], "comment": "Accepted at Interspeech 2025", "summary": "We propose a Speech-to-Text Translation (S2TT) approach that integrates\nphoneme representations into a Chain-of-Thought (CoT) framework to improve\ntranslation in low-resource and zero-resource settings. By introducing phoneme\nrecognition as an intermediate step, we enhance cross-lingual transfer,\nenabling translation even for languages with no labeled speech data. Our system\nbuilds on a multilingual LLM, which we extend to process speech and phonemes.\nTraining follows a curriculum learning strategy that progressively introduces\nmore complex tasks. Experiments on multilingual S2TT benchmarks show that\nphoneme-augmented CoT improves translation quality in low-resource conditions\nand enables zero-resource translation, while slightly impacting high-resource\nperformance. Despite this trade-off, our findings demonstrate that\nphoneme-based CoT is a promising step toward making S2TT more accessible across\ndiverse languages."}
{"id": "2505.23809", "pdf": "https://arxiv.org/pdf/2505.23809", "abs": "https://arxiv.org/abs/2505.23809", "authors": ["Haowei Yang", "Haotian Lyu", "Tianle Zhang", "Dingzhou Wang", "Yushang Zhao"], "title": "LLM-Driven E-Commerce Marketing Content Optimization: Balancing Creativity and Conversion", "categories": ["cs.CL", "cs.AI", "cs.IR"], "comment": null, "summary": "As e-commerce competition intensifies, balancing creative content with\nconversion effectiveness becomes critical. Leveraging LLMs' language generation\ncapabilities, we propose a framework that integrates prompt engineering,\nmulti-objective fine-tuning, and post-processing to generate marketing copy\nthat is both engaging and conversion-driven. Our fine-tuning method combines\nsentiment adjustment, diversity enhancement, and CTA embedding. Through offline\nevaluations and online A/B tests across categories, our approach achieves a\n12.5 % increase in CTR and an 8.3 % increase in CVR while maintaining content\nnovelty. This provides a practical solution for automated copy generation and\nsuggests paths for future multimodal, real-time personalization."}
{"id": "2505.24076", "pdf": "https://arxiv.org/pdf/2505.24076", "abs": "https://arxiv.org/abs/2505.24076", "authors": ["Huan Ning", "Zhenlong Li", "Manzhu Yu", "Wenpeng Yin"], "title": "SIM: A mapping framework for built environment auditing based on street view imagery", "categories": ["cs.CV"], "comment": null, "summary": "Built environment auditing refers to the systematic documentation and\nassessment of urban and rural spaces' physical, social, and environmental\ncharacteristics, such as walkability, road conditions, and traffic lights. It\nis used to collect data for the evaluation of how built environments impact\nhuman behavior, health, mobility, and overall urban functionality.\nTraditionally, built environment audits were conducted using field surveys and\nmanual observations, which were time-consuming and costly. The emerging street\nview imagery, e.g., Google Street View, has become a widely used data source\nfor conducting built environment audits remotely. Deep learning and computer\nvision techniques can extract and classify objects from street images to\nenhance auditing productivity. Before meaningful analysis, the detected objects\nneed to be geospatially mapped for accurate documentation. However, the mapping\nmethods and tools based on street images are underexplored, and there are no\nuniversal frameworks or solutions yet, imposing difficulties in auditing the\nstreet objects. In this study, we introduced an open source street view mapping\nframework, providing three pipelines to map and measure: 1) width measurement\nfor ground objects, such as roads; 2) 3D localization for objects with a known\ndimension (e.g., doors and stop signs); and 3) diameter measurements (e.g.,\nstreet trees). These pipelines can help researchers, urban planners, and other\nprofessionals automatically measure and map target objects, promoting built\nenvironment auditing productivity and accuracy. Three case studies, including\nroad width measurement, stop sign localization, and street tree diameter\nmeasurement, are provided in this paper to showcase pipeline usage."}
{"id": "2505.24081", "pdf": "https://arxiv.org/pdf/2505.24081", "abs": "https://arxiv.org/abs/2505.24081", "authors": ["Sayed T. Nowroz", "Nermeen M. Saleh", "Siam Shakur", "Sean Banerjee", "Fathi Amsaad"], "title": "A Benchmark Reference for ESP32-CAM Module", "categories": ["cs.RO", "cs.SY", "eess.IV", "eess.SY"], "comment": "Full work available at GitHub:\n  https://github.com/TNeutron/ESP32-CAM-Performence-Reference-Benchmark", "summary": "The ESP32-CAM is one of the most widely adopted open-source modules for\nprototyping embedded vision applications. Since its release in 2019, it has\ngained popularity among both hobbyists and professional developers due to its\naffordability, versatility, and integrated wireless capabilities. Despite its\nwidespread use, comprehensive documentation of the performance metrics remains\nlimited. This study addresses this gap by collecting and analyzing over six\nhours of real-time video streaming logs across all supported resolutions of the\nOV2640 image sensor, tested under five distinct voltage conditions via an\nHTTP-based WiFi connection. A long standing bug in the official Arduino ESP32\ndriver, responsible for inaccurate frame rate logging, was fixed. The resulting\nanalysis includes key performance metrics such as instantaneous and average\nframe rate, total streamed data, transmission count, and internal chip\ntemperature. The influence of varying power levels was evaluated to assess the\nreliability of the module."}
{"id": "2505.24258", "pdf": "https://arxiv.org/pdf/2505.24258", "abs": "https://arxiv.org/abs/2505.24258", "authors": ["Vishal Pallagani", "Nitin Gupta", "John Aydin", "Biplav Srivastava"], "title": "FABLE: A Novel Data-Flow Analysis Benchmark on Procedural Text for Large Language Model Evaluation", "categories": ["cs.AI"], "comment": null, "summary": "Understanding how data moves, transforms, and persists, known as data flow,\nis fundamental to reasoning in procedural tasks. Despite their fluency in\nnatural and programming languages, large language models (LLMs), although\nincreasingly being applied to decisions with procedural tasks, have not been\nsystematically evaluated for their ability to perform data-flow reasoning. We\nintroduce FABLE, an extensible benchmark designed to assess LLMs' understanding\nof data flow using structured, procedural text. FABLE adapts eight classical\ndata-flow analyses from software engineering: reaching definitions, very busy\nexpressions, available expressions, live variable analysis, interval analysis,\ntype-state analysis, taint analysis, and concurrency analysis. These analyses\nare instantiated across three real-world domains: cooking recipes, travel\nroutes, and automated plans. The benchmark includes 2,400 question-answer\npairs, with 100 examples for each domain-analysis combination. We evaluate\nthree types of LLMs: a reasoning-focused model (DeepSeek-R1 8B), a\ngeneral-purpose model (LLaMA 3.1 8B), and a code-specific model (Granite Code\n8B). Each model is tested using majority voting over five sampled completions\nper prompt. Results show that the reasoning model achieves higher accuracy, but\nat the cost of over 20 times slower inference compared to the other models. In\ncontrast, the general-purpose and code-specific models perform close to random\nchance. FABLE provides the first diagnostic benchmark to systematically\nevaluate data-flow reasoning and offers insights for developing models with\nstronger procedural understanding."}
{"id": "2505.23913", "pdf": "https://arxiv.org/pdf/2505.23913", "abs": "https://arxiv.org/abs/2505.23913", "authors": ["Gustavo Sutter Pessurno de Carvalho", "Mohammed Abdulrahman", "Hao Wang", "Sriram Ganapathi Subramanian", "Marc St-Aubin", "Sharon O'Sullivan", "Lawrence Wan", "Luis Ricardez-Sandoval", "Pascal Poupart", "Agustinus Kristiadi"], "title": "Simplifying Bayesian Optimization Via In-Context Direct Optimum Sampling", "categories": ["cs.LG", "stat.ML"], "comment": null, "summary": "The optimization of expensive black-box functions is ubiquitous in science\nand engineering. A common solution to this problem is Bayesian optimization\n(BO), which is generally comprised of two components: (i) a surrogate model and\n(ii) an acquisition function, which generally require expensive re-training and\noptimization steps at each iteration, respectively. Although recent work\nenabled in-context surrogate models that do not require re-training, virtually\nall existing BO methods still require acquisition function maximization to\nselect the next observation, which introduces many knobs to tune, such as Monte\nCarlo samplers and multi-start optimizers. In this work, we propose a\ncompletely in-context, zero-shot solution for BO that does not require\nsurrogate fitting or acquisition function optimization. This is done by using a\npre-trained deep generative model to directly sample from the posterior over\nthe optimum point. We show that this process is equivalent to Thompson sampling\nand demonstrate the capabilities and cost-effectiveness of our foundation model\non a suite of real-world benchmarks. We achieve an efficiency gain of more than\n35x in terms of wall-clock time when compared with Gaussian process-based BO,\nenabling efficient parallel and distributed BO, e.g., for high-throughput\noptimization."}
{"id": "2505.24713", "pdf": "https://arxiv.org/pdf/2505.24713", "abs": "https://arxiv.org/abs/2505.24713", "authors": ["Badr M. Abdullah", "Matthew Baas", "Bernd MÃ¶bius", "Dietrich Klakow"], "title": "Voice Conversion Improves Cross-Domain Robustness for Spoken Arabic Dialect Identification", "categories": ["cs.CL", "cs.SD", "eess.AS"], "comment": "Accepted in Interspeech 2025", "summary": "Arabic dialect identification (ADI) systems are essential for large-scale\ndata collection pipelines that enable the development of inclusive speech\ntechnologies for Arabic language varieties. However, the reliability of current\nADI systems is limited by poor generalization to out-of-domain speech. In this\npaper, we present an effective approach based on voice conversion for training\nADI models that achieves state-of-the-art performance and significantly\nimproves robustness in cross-domain scenarios. Evaluated on a newly collected\nreal-world test set spanning four different domains, our approach yields\nconsistent improvements of up to +34.1% in accuracy across domains.\nFurthermore, we present an analysis of our approach and demonstrate that voice\nconversion helps mitigate the speaker bias in the ADI dataset. We release our\nrobust ADI model and cross-domain evaluation dataset to support the development\nof inclusive speech technologies for Arabic."}
{"id": "2406.15119", "pdf": "https://arxiv.org/pdf/2406.15119", "abs": "https://arxiv.org/abs/2406.15119", "authors": ["Yi Chang", "Zhao Ren", "Zhonghao Zhao", "Thanh Tam Nguyen", "Kun Qian", "Tanja Schultz", "BjÃ¶rn W. Schuller"], "title": "Breaking Resource Barriers in Speech Emotion Recognition via Data Distillation", "categories": ["cs.SD", "cs.AI", "eess.AS"], "comment": "Accepted by Interspeech 2025", "summary": "Speech emotion recognition (SER) plays a crucial role in human-computer\ninteraction. The emergence of edge devices in the Internet of Things (IoT)\npresents challenges in constructing intricate deep learning models due to\nconstraints in memory and computational resources. Moreover, emotional speech\ndata often contains private information, raising concerns about privacy leakage\nduring the deployment of SER models. To address these challenges, we propose a\ndata distillation framework to facilitate efficient development of SER models\nin IoT applications using a synthesised, smaller, and distilled dataset. Our\nexperiments demonstrate that the distilled dataset can be effectively utilised\nto train SER models with fixed initialisation, achieving performances\ncomparable to those developed using the original full emotional speech dataset."}
{"id": "2505.23810", "pdf": "https://arxiv.org/pdf/2505.23810", "abs": "https://arxiv.org/abs/2505.23810", "authors": ["Chenghao Yang", "Yinbo Luo", "Zhoufutu Wen", "Qi Chu", "Tao Gong", "Longxiang Liu", "Kaiyuan Zhang", "Jianpeng Jiao", "Ge Zhang", "Wenhao Huang", "Nenghai Yu"], "title": "MARS-Bench: A Multi-turn Athletic Real-world Scenario Benchmark for Dialogue Evaluation", "categories": ["cs.CL", "cs.AI"], "comment": "29 pages, 13 figures", "summary": "Large Language Models (\\textbf{LLMs}), e.g. ChatGPT, have been widely adopted\nin real-world dialogue applications. However, LLMs' robustness, especially in\nhandling long complex dialogue sessions, including frequent motivation\ntransfer, sophisticated cross-turn dependency, is criticized all along.\nNevertheless, no existing benchmarks can fully reflect these weaknesses. We\npresent \\textbf{MARS-Bench}, a \\textbf{M}ulti-turn \\textbf{A}thletic\n\\textbf{R}eal-world \\textbf{S}cenario Dialogue \\textbf{Bench}mark, designed to\nremedy the gap. MARS-Bench is constructed from play-by-play text commentary so\nto feature realistic dialogues specifically designed to evaluate three critical\naspects of multi-turn conversations: Ultra Multi-turn, Interactive Multi-turn,\nand Cross-turn Tasks. Extensive experiments on MARS-Bench also reveal that\nclosed-source LLMs significantly outperform open-source alternatives, explicit\nreasoning significantly boosts LLMs' robustness on handling long complex\ndialogue sessions, and LLMs indeed face significant challenges when handling\nmotivation transfer and sophisticated cross-turn dependency. Moreover, we\nprovide mechanistic interpretability on how attention sinks due to special\ntokens lead to LLMs' performance degradation when handling long complex\ndialogue sessions based on attention visualization experiment in\nQwen2.5-7B-Instruction."}
{"id": "2505.24086", "pdf": "https://arxiv.org/pdf/2505.24086", "abs": "https://arxiv.org/abs/2505.24086", "authors": ["Zeeshan Khan", "Shizhe Chen", "Cordelia Schmid"], "title": "ComposeAnything: Composite Object Priors for Text-to-Image Generation", "categories": ["cs.CV"], "comment": null, "summary": "Generating images from text involving complex and novel object arrangements\nremains a significant challenge for current text-to-image (T2I) models.\nAlthough prior layout-based methods improve object arrangements using spatial\nconstraints with 2D layouts, they often struggle to capture 3D positioning and\nsacrifice quality and coherence. In this work, we introduce ComposeAnything, a\nnovel framework for improving compositional image generation without retraining\nexisting T2I models. Our approach first leverages the chain-of-thought\nreasoning abilities of LLMs to produce 2.5D semantic layouts from text,\nconsisting of 2D object bounding boxes enriched with depth information and\ndetailed captions. Based on this layout, we generate a spatial and depth aware\ncoarse composite of objects that captures the intended composition, serving as\na strong and interpretable prior that replaces stochastic noise initialization\nin diffusion-based T2I models. This prior guides the denoising process through\nobject prior reinforcement and spatial-controlled denoising, enabling seamless\ngeneration of compositional objects and coherent backgrounds, while allowing\nrefinement of inaccurate priors. ComposeAnything outperforms state-of-the-art\nmethods on the T2I-CompBench and NSR-1K benchmarks for prompts with 2D/3D\nspatial arrangements, high object counts, and surreal compositions. Human\nevaluations further demonstrate that our model generates high-quality images\nwith compositions that faithfully reflect the text."}
{"id": "2503.00741", "pdf": "https://arxiv.org/pdf/2503.00741", "abs": "https://arxiv.org/abs/2503.00741", "authors": ["Henrui Tian", "Wenhui Lei", "Linrui Dai", "Hanyu Chen", "Xiaofan Zhang"], "title": "LesionDiffusion: Towards Text-controlled General Lesion Synthesis", "categories": ["eess.IV", "cs.CV"], "comment": "10 pages, 4 figures", "summary": "Fully-supervised lesion recognition methods in medical imaging face\nchallenges due to the reliance on large annotated datasets, which are expensive\nand difficult to collect. To address this, synthetic lesion generation has\nbecome a promising approach. However, existing models struggle with\nscalability, fine-grained control over lesion attributes, and the generation of\ncomplex structures. We propose LesionDiffusion, a text-controllable lesion\nsynthesis framework for 3D CT imaging that generates both lesions and\ncorresponding masks. By utilizing a structured lesion report template, our\nmodel provides greater control over lesion attributes and supports a wider\nvariety of lesion types. We introduce a dataset of 1,505 annotated CT scans\nwith paired lesion masks and structured reports, covering 14 lesion types\nacross 8 organs. LesionDiffusion consists of two components: a lesion mask\nsynthesis network (LMNet) and a lesion inpainting network (LINet), both guided\nby lesion attributes and image features. Extensive experiments demonstrate that\nLesionDiffusion significantly improves segmentation performance, with strong\ngeneralization to unseen lesion types and organs, outperforming current\nstate-of-the-art models. Code is available at\nhttps://github.com/HengruiTianSJTU/LesionDiffusion."}
{"id": "2505.24260", "pdf": "https://arxiv.org/pdf/2505.24260", "abs": "https://arxiv.org/abs/2505.24260", "authors": ["Mingyi He", "Yuebing Liang", "Shenhao Wang", "Yunhan Zheng", "Qingyi Wang", "Dingyi Zhuang", "Li Tian", "Jinhua Zhao"], "title": "Generative AI for Urban Design: A Stepwise Approach Integrating Human Expertise with Multimodal Diffusion Models", "categories": ["cs.AI"], "comment": null, "summary": "Urban design is a multifaceted process that demands careful consideration of\nsite-specific constraints and collaboration among diverse professionals and\nstakeholders. The advent of generative artificial intelligence (GenAI) offers\ntransformative potential by improving the efficiency of design generation and\nfacilitating the communication of design ideas. However, most existing\napproaches are not well integrated with human design workflows. They often\nfollow end-to-end pipelines with limited control, overlooking the iterative\nnature of real-world design. This study proposes a stepwise generative urban\ndesign framework that integrates multimodal diffusion models with human\nexpertise to enable more adaptive and controllable design processes. Instead of\ngenerating design outcomes in a single end-to-end process, the framework\ndivides the process into three key stages aligned with established urban design\nworkflows: (1) road network and land use planning, (2) building layout\nplanning, and (3) detailed planning and rendering. At each stage, multimodal\ndiffusion models generate preliminary designs based on textual prompts and\nimage-based constraints, which can then be reviewed and refined by human\ndesigners. We design an evaluation framework to assess the fidelity,\ncompliance, and diversity of the generated designs. Experiments using data from\nChicago and New York City demonstrate that our framework outperforms baseline\nmodels and end-to-end approaches across all three dimensions. This study\nunderscores the benefits of multimodal diffusion models and stepwise generation\nin preserving human control and facilitating iterative refinements, laying the\ngroundwork for human-AI interaction in urban design solutions."}
{"id": "2505.23927", "pdf": "https://arxiv.org/pdf/2505.23927", "abs": "https://arxiv.org/abs/2505.23927", "authors": ["Songtao Feng", "Jie Fu"], "title": "Thompson Sampling in Online RLHF with General Function Approximation", "categories": ["cs.LG"], "comment": null, "summary": "Reinforcement learning from human feedback (RLHF) has achieved great\nempirical success in aligning large language models (LLMs) with human\npreference, and it is of great importance to study the statistical efficiency\nof RLHF algorithms from a theoretical perspective. In this work, we consider\nthe online RLHF setting where the preference data is revealed during the\nlearning process and study action value function approximation. We design a\nmodel-free posterior sampling algorithm for online RLHF inspired by Thompson\nsampling and provide its theoretical guarantee. Specifically, we adopt Bellman\neluder (BE) dimension as the complexity measure of the function class and\nestablish $O(\\sqrt{T})$ regret bound for the proposed algorithm with other\nmultiplicative factor depending on the horizon, BE dimension and the\n$log$-bracketing number of the function class. Further, in the analysis, we\nfirst establish the concentration-type inequality of the squared Bellman error\nbound based on the maximum likelihood estimator (MLE) generalization bound,\nwhich plays the crucial rules in obtaining the eluder-type regret bound and may\nbe of independent interest."}
{"id": "2312.10741", "pdf": "https://arxiv.org/pdf/2312.10741", "abs": "https://arxiv.org/abs/2312.10741", "authors": ["Yu Zhang", "Rongjie Huang", "Ruiqi Li", "JinZheng He", "Yan Xia", "Feiyang Chen", "Xinyu Duan", "Baoxing Huai", "Zhou Zhao"], "title": "StyleSinger: Style Transfer for Out-of-Domain Singing Voice Synthesis", "categories": ["eess.AS", "cs.CL", "cs.SD"], "comment": "Accepted by AAAI 2024", "summary": "Style transfer for out-of-domain (OOD) singing voice synthesis (SVS) focuses\non generating high-quality singing voices with unseen styles (such as timbre,\nemotion, pronunciation, and articulation skills) derived from reference singing\nvoice samples. However, the endeavor to model the intricate nuances of singing\nvoice styles is an arduous task, as singing voices possess a remarkable degree\nof expressiveness. Moreover, existing SVS methods encounter a decline in the\nquality of synthesized singing voices in OOD scenarios, as they rest upon the\nassumption that the target vocal attributes are discernible during the training\nphase. To overcome these challenges, we propose StyleSinger, the first singing\nvoice synthesis model for zero-shot style transfer of out-of-domain reference\nsinging voice samples. StyleSinger incorporates two critical approaches for\nenhanced effectiveness: 1) the Residual Style Adaptor (RSA) which employs a\nresidual quantization module to capture diverse style characteristics in\nsinging voices, and 2) the Uncertainty Modeling Layer Normalization (UMLN) to\nperturb the style attributes within the content representation during the\ntraining phase and thus improve the model generalization. Our extensive\nevaluations in zero-shot style transfer undeniably establish that StyleSinger\noutperforms baseline models in both audio quality and similarity to the\nreference singing voice samples. Access to singing voice samples can be found\nat https://aaronz345.github.io/StyleSingerDemo/."}
{"id": "2502.13128", "pdf": "https://arxiv.org/pdf/2502.13128", "abs": "https://arxiv.org/abs/2502.13128", "authors": ["Zihan Liu", "Shuangrui Ding", "Zhixiong Zhang", "Xiaoyi Dong", "Pan Zhang", "Yuhang Zang", "Yuhang Cao", "Dahua Lin", "Jiaqi Wang"], "title": "SongGen: A Single Stage Auto-regressive Transformer for Text-to-Song Generation", "categories": ["cs.SD", "cs.AI"], "comment": null, "summary": "Text-to-song generation, the task of creating vocals and accompaniment from\ntextual inputs, poses significant challenges due to domain complexity and data\nscarcity. Existing approaches often employ multi-stage generation procedures,\nleading to cumbersome training and inference pipelines, as well as suboptimal\noverall generation quality due to error accumulation across stages. In this\npaper, we propose SongGen, a fully open-source, single-stage auto-regressive\ntransformer designed for controllable song generation. The proposed model\nfacilitates fine-grained control over diverse musical attributes, including\nlyrics and textual descriptions of instrumentation, genre, mood, and timbre,\nwhile also offering an optional three-second reference clip for voice cloning.\nWithin a unified auto-regressive framework, SongGen supports two output modes:\nmixed mode, which generates a mixture of vocals and accompaniment directly, and\ndual-track mode, which synthesizes them separately for greater flexibility in\ndownstream applications. We explore diverse token pattern strategies for each\nmode, leading to notable improvements and valuable insights. Furthermore, we\ndesign an automated data preprocessing pipeline with effective quality control.\nTo foster community engagement and future research, we will release our model\nweights, training code, annotated data, and preprocessing pipeline. The code is\navailable at https://github.com/LiuZH-19/SongGen."}
{"id": "2505.23811", "pdf": "https://arxiv.org/pdf/2505.23811", "abs": "https://arxiv.org/abs/2505.23811", "authors": ["Hadi Askari", "Shivanshu Gupta", "Fei Wang", "Anshuman Chhabra", "Muhao Chen"], "title": "LayerIF: Estimating Layer Quality for Large Language Models using Influence Functions", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "Under Review", "summary": "Pretrained Large Language Models (LLMs) achieve strong performance across a\nwide range of tasks, yet exhibit substantial variability in the various layers'\ntraining quality with respect to specific downstream applications, limiting\ntheir downstream performance.It is therefore critical to estimate layer-wise\ntraining quality in a manner that accounts for both model architecture and\ntraining data. However, existing approaches predominantly rely on model-centric\nheuristics (such as spectral statistics, outlier detection, or uniform\nallocation) while overlooking the influence of data. To address these\nlimitations, we propose LayerIF, a data-driven framework that leverages\nInfluence Functions to quantify the training quality of individual layers in a\nprincipled and task-sensitive manner. By isolating each layer's gradients and\nmeasuring the sensitivity of the validation loss to training examples by\ncomputing layer-wise influences, we derive data-driven estimates of layer\nimportance. Notably, our method produces task-specific layer importance\nestimates for the same LLM, revealing how layers specialize for different\ntest-time evaluation tasks. We demonstrate the utility of our scores by\nleveraging them for two downstream applications: (a) expert allocation in\nLoRA-MoE architectures and (b) layer-wise sparsity distribution for LLM\npruning. Experiments across multiple LLM architectures demonstrate that our\nmodel-agnostic, influence-guided allocation leads to consistent gains in task\nperformance."}
{"id": "2505.24103", "pdf": "https://arxiv.org/pdf/2505.24103", "abs": "https://arxiv.org/abs/2505.24103", "authors": ["Peiran Xu", "Yadong Mu"], "title": "Weakly-Supervised Affordance Grounding Guided by Part-Level Semantic Priors", "categories": ["cs.CV"], "comment": "ICLR 2025", "summary": "In this work, we focus on the task of weakly supervised affordance grounding,\nwhere a model is trained to identify affordance regions on objects using\nhuman-object interaction images and egocentric object images without dense\nlabels. Previous works are mostly built upon class activation maps, which are\neffective for semantic segmentation but may not be suitable for locating\nactions and functions. Leveraging recent advanced foundation models, we develop\na supervised training pipeline based on pseudo labels. The pseudo labels are\ngenerated from an off-the-shelf part segmentation model, guided by a mapping\nfrom affordance to part names. Furthermore, we introduce three key enhancements\nto the baseline model: a label refining stage, a fine-grained feature alignment\nprocess, and a lightweight reasoning module. These techniques harness the\nsemantic knowledge of static objects embedded in off-the-shelf foundation\nmodels to improve affordance learning, effectively bridging the gap between\nobjects and actions. Extensive experiments demonstrate that the performance of\nthe proposed model has achieved a breakthrough improvement over existing\nmethods. Our codes are available at https://github.com/woyut/WSAG-PLSP ."}
{"id": "2412.15032", "pdf": "https://arxiv.org/pdf/2412.15032", "abs": "https://arxiv.org/abs/2412.15032", "authors": ["Mang Ning", "Mingxiao Li", "Jianlin Su", "Haozhe Jia", "Lanmiao Liu", "Martin BeneÅ¡", "Wenshuo Chen", "Albert Ali Salah", "Itir Onal Ertugrul"], "title": "DCTdiff: Intriguing Properties of Image Generative Modeling in the DCT Space", "categories": ["cs.CV", "cs.LG", "eess.IV"], "comment": "ICML 2025", "summary": "This paper explores image modeling from the frequency space and introduces\nDCTdiff, an end-to-end diffusion generative paradigm that efficiently models\nimages in the discrete cosine transform (DCT) space. We investigate the design\nspace of DCTdiff and reveal the key design factors. Experiments on different\nframeworks (UViT, DiT), generation tasks, and various diffusion samplers\ndemonstrate that DCTdiff outperforms pixel-based diffusion models regarding\ngenerative quality and training efficiency. Remarkably, DCTdiff can seamlessly\nscale up to 512$\\times$512 resolution without using the latent diffusion\nparadigm and beats latent diffusion (using SD-VAE) with only 1/4 training cost.\nFinally, we illustrate several intriguing properties of DCT image modeling. For\nexample, we provide a theoretical proof of why 'image diffusion can be seen as\nspectral autoregression', bridging the gap between diffusion and autoregressive\nmodels. The effectiveness of DCTdiff and the introduced properties suggest a\npromising direction for image modeling in the frequency space. The code is\nhttps://github.com/forever208/DCTdiff."}
{"id": "2505.24273", "pdf": "https://arxiv.org/pdf/2505.24273", "abs": "https://arxiv.org/abs/2505.24273", "authors": ["Hongyi James Cai", "Junlin Wang", "Xiaoyin Chen", "Bhuwan Dhingra"], "title": "How Much Backtracking is Enough? Exploring the Interplay of SFT and RL in Enhancing LLM Reasoning", "categories": ["cs.AI"], "comment": null, "summary": "Recent breakthroughs in large language models (LLMs) have effectively\nimproved their reasoning abilities, particularly on mathematical and logical\nproblems that have verifiable answers, through techniques such as supervised\nfinetuning (SFT) and reinforcement learning (RL). Prior research indicates that\nRL effectively internalizes search strategies, enabling long chain-of-thought\n(CoT) reasoning, with backtracking emerging naturally as a learned capability.\nHowever, the precise benefits of backtracking, specifically, how significantly\nit contributes to reasoning improvements and the optimal extent of its use,\nremain poorly understood. In this work, we systematically investigate the\ndynamics between SFT and RL on eight reasoning tasks: Countdown, Sudoku, Arc\n1D, Geometry, Color Cube Rotation, List Functions, Zebra Puzzles, and Self\nReference. Our findings highlight that short CoT sequences used in SFT as a\nwarm-up do have moderate contribution to RL training, compared with cold-start\nRL; however such contribution diminishes when tasks become increasingly\ndifficult. Motivated by this observation, we construct synthetic datasets\nvarying systematically in the number of backtracking steps and conduct\ncontrolled experiments to isolate the influence of either the correctness\n(content) or the structure (i.e., backtrack frequency). We find that (1) longer\nCoT with backtracks generally induce better and more stable RL training, (2)\nmore challenging problems with larger search space tend to need higher numbers\nof backtracks during the SFT stage. Additionally, we demonstrate through\nexperiments on distilled data that RL training is largely unaffected by the\ncorrectness of long CoT sequences, suggesting that RL prioritizes structural\npatterns over content correctness. Collectively, our results offer practical\ninsights into designing optimal training strategies to effectively scale\nreasoning in LLMs."}
{"id": "2505.23933", "pdf": "https://arxiv.org/pdf/2505.23933", "abs": "https://arxiv.org/abs/2505.23933", "authors": ["Galen Pogoncheff", "Michael Beyeler"], "title": "BIRD: Behavior Induction via Representation-structure Distillation", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Human-aligned deep learning models exhibit behaviors consistent with human\nvalues, such as robustness, fairness, and honesty. Transferring these\nbehavioral properties to models trained on different tasks or data\ndistributions remains challenging: aligned behavior is easily forgotten during\nfine-tuning, and collecting task-specific data that preserves this behavior can\nbe prohibitively costly. We introduce BIRD (Behavior Induction via\nRepresentation-structure Distillation), a flexible framework for transferring\naligned behavior by matching the internal representation structure of a student\nmodel to that of a teacher. Applied to out-of-distribution robustness in image\nclassification, BIRD outperforms fine-tuning, transfer learning, and continual\nlearning methods, improving robust accuracy by up to 16% over the next\nstrongest baseline. It remains effective even when the teacher is trained on a\nmuch simpler dataset and is $25 \\times$ smaller than the student. In a\nlarge-scale study of over 400 teacher-student pairs, we show that three\ninterpretable and computable properties of the teacher's representations (i.e.,\ntask relevance, behavioral relevance, and complementary knowledge) explain up\nto 85% of the variance in transfer success. These insights offer practical\nguidance for teacher selection and design. BIRD turns small, well-aligned\nmodels into scalable alignment seeds, removing a key bottleneck in deploying\nsafe AI systems in the wild."}
{"id": "2504.19062", "pdf": "https://arxiv.org/pdf/2504.19062", "abs": "https://arxiv.org/abs/2504.19062", "authors": ["Yu Zhang", "Wenxiang Guo", "Changhao Pan", "Zhiyuan Zhu", "Ruiqi Li", "Jingyu Lu", "Rongjie Huang", "Ruiyuan Zhang", "Zhiqing Hong", "Ziyue Jiang", "Zhou Zhao"], "title": "Versatile Framework for Song Generation with Prompt-based Control", "categories": ["eess.AS", "cs.CL", "cs.SD"], "comment": null, "summary": "Song generation focuses on producing controllable high-quality songs based on\nvarious prompts. However, existing methods struggle to generate vocals and\naccompaniments with prompt-based control and proper alignment. Additionally,\nthey fall short in supporting various tasks. To address these challenges, we\nintroduce VersBand, a multi-task song generation framework for synthesizing\nhigh-quality, aligned songs with prompt-based control. VersBand comprises these\nprimary models: 1) VocalBand, a decoupled model, leverages the flow-matching\nmethod for generating singing styles, pitches, and mel-spectrograms, allowing\nfast, high-quality vocal generation with style control. 2) AccompBand, a\nflow-based transformer model, incorporates the Band-MOE, selecting suitable\nexperts for enhanced quality, alignment, and control. This model allows for\ngenerating controllable, high-quality accompaniments aligned with vocals. 3)\nTwo generation models, LyricBand for lyrics and MelodyBand for melodies,\ncontribute to the comprehensive multi-task song generation system, allowing for\nextensive control based on multiple prompts. Experimental results demonstrate\nthat VersBand performs better over baseline models across multiple song\ngeneration tasks using objective and subjective metrics. Audio samples are\navailable at https://aaronz345.github.io/VersBandDemo."}
{"id": "2503.07217", "pdf": "https://arxiv.org/pdf/2503.07217", "abs": "https://arxiv.org/abs/2503.07217", "authors": ["Zixuan Wang", "Chi-Keung Tang", "Yu-Wing Tai"], "title": "ReelWave: Multi-Agentic Movie Sound Generation through Multimodal LLM Conversation", "categories": ["cs.SD", "cs.CV"], "comment": null, "summary": "Current audio generation conditioned by text or video focuses on aligning\naudio with text/video modalities. Despite excellent alignment results, these\nmultimodal frameworks still cannot be directly applied to compelling movie\nstorytelling involving multiple scenes, where \"on-screen\" sounds require\ntemporally-aligned audio generation, while \"off-screen\" sounds contribute to\nappropriate environment sounds accompanied by background music when applicable.\nInspired by professional movie production, this paper proposes a multi-agentic\nframework for audio generation supervised by an autonomous Sound Director\nagent, engaging multi-turn conversations with other agents for on-screen and\noff-screen sound generation through multimodal LLM. To address on-screen sound\ngeneration, after detecting any talking humans in videos, we capture\nsemantically and temporally synchronized sound by training a prediction model\nthat forecasts interpretable, time-varying audio control signals: loudness,\npitch, and timbre, which are used by a Foley Artist agent to condition a\ncross-attention module in the sound generation. The Foley Artist works\ncooperatively with the Composer and Voice Actor agents, and together they\nautonomously generate off-screen sound to complement the overall production.\nEach agent takes on specific roles similar to those of a movie production team.\nTo temporally ground audio language models, in ReelWave, text/video conditions\nare decomposed into atomic, specific sound generation instructions synchronized\nwith visuals when applicable. Consequently, our framework can generate rich and\nrelevant audio content conditioned on video clips extracted from movies."}
{"id": "2505.23812", "pdf": "https://arxiv.org/pdf/2505.23812", "abs": "https://arxiv.org/abs/2505.23812", "authors": ["Lata Pangtey", "Mohammad Zia Ur Rehman", "Prasad Chaudhari", "Shubhi Bansal", "Nagendra Kumar"], "title": "Emotion-aware Dual Cross-Attentive Neural Network with Label Fusion for Stance Detection in Misinformative Social Media Content", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": null, "summary": "The rapid evolution of social media has generated an overwhelming volume of\nuser-generated content, conveying implicit opinions and contributing to the\nspread of misinformation. The method aims to enhance the detection of stance\nwhere misinformation can polarize user opinions. Stance detection has emerged\nas a crucial approach to effectively analyze underlying biases in shared\ninformation and combating misinformation. This paper proposes a novel method\nfor \\textbf{S}tance \\textbf{P}rediction through a \\textbf{L}abel-fused dual\ncross-\\textbf{A}ttentive \\textbf{E}motion-aware neural \\textbf{Net}work\n(SPLAENet) in misinformative social media user-generated content. The proposed\nmethod employs a dual cross-attention mechanism and a hierarchical attention\nnetwork to capture inter and intra-relationships by focusing on the relevant\nparts of source text in the context of reply text and vice versa. We\nincorporate emotions to effectively distinguish between different stance\ncategories by leveraging the emotional alignment or divergence between the\ntexts. We also employ label fusion that uses distance-metric learning to align\nextracted features with stance labels, improving the method's ability to\naccurately distinguish between stances. Extensive experiments demonstrate the\nsignificant improvements achieved by SPLAENet over existing state-of-the-art\nmethods. SPLAENet demonstrates an average gain of 8.92\\% in accuracy and\n17.36\\% in F1-score on the RumourEval dataset. On the SemEval dataset, it\nachieves average gains of 7.02\\% in accuracy and 10.92\\% in F1-score. On the\nP-stance dataset, it demonstrates average gains of 10.03\\% in accuracy and\n11.18\\% in F1-score. These results validate the effectiveness of the proposed\nmethod for stance detection in the context of misinformative social media\ncontent."}
{"id": "2505.24108", "pdf": "https://arxiv.org/pdf/2505.24108", "abs": "https://arxiv.org/abs/2505.24108", "authors": ["Alina Devkota", "Annahita Amireskandari", "Joel Palko", "Shyam Thakkar", "Donald Adjeroh", "Xiajun Jiang", "Binod Bhattarai", "Prashnna K. Gyawali"], "title": "Federated Foundation Model for GI Endoscopy Images", "categories": ["cs.CV", "cs.LG", "I.2.10; I.4; I.5"], "comment": "11 pages, 11 figures, submitted to BHI2025", "summary": "Gastrointestinal (GI) endoscopy is essential in identifying GI tract\nabnormalities in order to detect diseases in their early stages and improve\npatient outcomes. Although deep learning has shown success in supporting GI\ndiagnostics and decision-making, these models require curated datasets with\nlabels that are expensive to acquire. Foundation models offer a promising\nsolution by learning general-purpose representations, which can be finetuned\nfor specific tasks, overcoming data scarcity. Developing foundation models for\nmedical imaging holds significant potential, but the sensitive and protected\nnature of medical data presents unique challenges. Foundation model training\ntypically requires extensive datasets, and while hospitals generate large\nvolumes of data, privacy restrictions prevent direct data sharing, making\nfoundation model training infeasible in most scenarios. In this work, we\npropose a FL framework for training foundation models for gastroendoscopy\nimaging, enabling data to remain within local hospital environments while\ncontributing to a shared model. We explore several established FL algorithms,\nassessing their suitability for training foundation models without relying on\ntask-specific labels, conducting experiments in both homogeneous and\nheterogeneous settings. We evaluate the trained foundation model on three\ncritical downstream tasks--classification, detection, and segmentation--and\ndemonstrate that it achieves improved performance across all tasks,\nhighlighting the effectiveness of our approach in a federated,\nprivacy-preserving setting."}
{"id": "2501.02333", "pdf": "https://arxiv.org/pdf/2501.02333", "abs": "https://arxiv.org/abs/2501.02333", "authors": ["Ali Bavafa", "Gholam-Ali Hossein-Zadeh"], "title": "On The Causal Network Of Face-selective Regions In Human Brain During Movie Watching", "categories": ["q-bio.NC", "cs.LG", "eess.IV"], "comment": null, "summary": "Understanding the causal interactions in some brain tasks, such as face\nprocessing, remains a challenging and ambiguous process for researchers. In\nthis study, we address this issue by employing a novel causal discovery method\n-Directed Acyclic Graphs via M-matrices for Acyclicity (DAGMA)- to investigate\nthe causal structure of the brain's face-selective network and gain deeper\ninsights into its mechanism. Using fMRI data of natural movie stimuli, we\nextract causal network of face-selective regions and analyze how frames\ncontaining faces influence this network. Specifically, our findings reveal that\nthe presence of faces in the stimuli, causally affects the number of identified\nconnections within the network. Additionally, our results highlight the crucial\nrole of subcortical regions in satisfying causal sufficiency, emphasizing it's\nimportance in causal studies of brain. This study provides a new perspective on\nunderstanding the causal architecture of the face-selective network of the\nbrain, motivating further research on neural causality."}
{"id": "2505.24292", "pdf": "https://arxiv.org/pdf/2505.24292", "abs": "https://arxiv.org/abs/2505.24292", "authors": ["Yueqi Zhang", "Peiwen Yuan", "Shaoxiong Feng", "Yiwei Li", "Xinglin Wang", "Jiayi Shi", "Chuyi Tan", "Boyuan Pan", "Yao Hu", "Kan Li"], "title": "Mind the Quote: Enabling Quotation-Aware Dialogue in LLMs via Plug-and-Play Modules", "categories": ["cs.AI", "cs.CL"], "comment": null, "summary": "Human-AI conversation frequently relies on quoting earlier text-\"check it\nwith the formula I just highlighted\"-yet today's large language models (LLMs)\nlack an explicit mechanism for locating and exploiting such spans. We formalise\nthe challenge as span-conditioned generation, decomposing each turn into the\ndialogue history, a set of token-offset quotation spans, and an intent\nutterance. Building on this abstraction, we introduce a quotation-centric data\npipeline that automatically synthesises task-specific dialogues, verifies\nanswer correctness through multi-stage consistency checks, and yields both a\nheterogeneous training corpus and the first benchmark covering five\nrepresentative scenarios. To meet the benchmark's zero-overhead and\nparameter-efficiency requirements, we propose QuAda, a lightweight\ntraining-based method that attaches two bottleneck projections to every\nattention head, dynamically amplifying or suppressing attention to quoted spans\nat inference time while leaving the prompt unchanged and updating < 2.8% of\nbackbone weights. Experiments across models show that QuAda is suitable for all\nscenarios and generalises to unseen topics, offering an effective,\nplug-and-play solution for quotation-aware dialogue."}
{"id": "2505.23939", "pdf": "https://arxiv.org/pdf/2505.23939", "abs": "https://arxiv.org/abs/2505.23939", "authors": ["Andrea Mattia Garavagno", "Edoardo Ragusa", "Antonio Frisoli", "Paolo Gastaldo"], "title": "Searching Neural Architectures for Sensor Nodes on IoT Gateways", "categories": ["cs.LG", "cs.NI"], "comment": null, "summary": "This paper presents an automatic method for the design of Neural Networks\n(NNs) at the edge, enabling Machine Learning (ML) access even in\nprivacy-sensitive Internet of Things (IoT) applications. The proposed method\nruns on IoT gateways and designs NNs for connected sensor nodes without sharing\nthe collected data outside the local network, keeping the data in the site of\ncollection. This approach has the potential to enable ML for Healthcare\nInternet of Things (HIoT) and Industrial Internet of Things (IIoT), designing\nhardware-friendly and custom NNs at the edge for personalized healthcare and\nadvanced industrial services such as quality control, predictive maintenance,\nor fault diagnosis. By preventing data from being disclosed to cloud services,\nthis method safeguards sensitive information, including industrial secrets and\npersonal data. The outcomes of a thorough experimental session confirm that --\non the Visual Wake Words dataset -- the proposed approach can achieve\nstate-of-the-art results by exploiting a search procedure that runs in less\nthan 10 hours on the Raspberry Pi Zero 2."}
{"id": "2505.13455", "pdf": "https://arxiv.org/pdf/2505.13455", "abs": "https://arxiv.org/abs/2505.13455", "authors": ["Von Ralph Dane Marquez Herbuela", "Yukie Nagai"], "title": "Spatiotemporal Emotional Synchrony in Dyadic Interactions: The Role of Speech Conditions in Facial and Vocal Affective Alignment", "categories": ["eess.AS", "cs.AI"], "comment": null, "summary": "Understanding how humans express and synchronize emotions across multiple\ncommunication channels particularly facial expressions and speech has\nsignificant implications for emotion recognition systems and human computer\ninteraction. Motivated by the notion that non-overlapping speech promotes\nclearer emotional coordination, while overlapping speech disrupts synchrony,\nthis study examines how these conversational dynamics shape the spatial and\ntemporal alignment of arousal and valence across facial and vocal modalities.\nUsing dyadic interactions from the IEMOCAP dataset, we extracted continuous\nemotion estimates via EmoNet (facial video) and a Wav2Vec2-based model (speech\naudio). Segments were categorized based on speech overlap, and emotional\nalignment was assessed using Pearson correlation, lag adjusted analysis, and\nDynamic Time Warping (DTW). Across analyses, non overlapping speech was\nassociated with more stable and predictable emotional synchrony than\noverlapping speech. While zero-lag correlations were low and not statistically\ndifferent, non overlapping speech showed reduced variability, especially for\narousal. Lag adjusted correlations and best-lag distributions revealed clearer,\nmore consistent temporal alignment in these segments. In contrast, overlapping\nspeech exhibited higher variability and flatter lag profiles, though DTW\nindicated unexpectedly tighter alignment suggesting distinct coordination\nstrategies. Notably, directionality patterns showed that facial expressions\nmore often preceded speech during turn-taking, while speech led during\nsimultaneous vocalizations. These findings underscore the importance of\nconversational structure in regulating emotional communication and provide new\ninsight into the spatial and temporal dynamics of multimodal affective\nalignment in real world interaction."}
{"id": "2505.21356", "pdf": "https://arxiv.org/pdf/2505.21356", "abs": "https://arxiv.org/abs/2505.21356", "authors": ["Whenty Ariyanti", "Kuan-Yu Chen", "Sabato Marco Siniscalchi", "Hsin-Min Wang", "Yu Tsao"], "title": "Towards Robust Assessment of Pathological Voices via Combined Low-Level Descriptors and Foundation Model Representations", "categories": ["cs.SD", "cs.LG", "eess.AS"], "comment": null, "summary": "Perceptual voice quality assessment is essential for diagnosing and\nmonitoring voice disorders by providing standardized evaluations of vocal\nfunction. Traditionally, expert raters use standard scales such as the\nConsensus Auditory-Perceptual Evaluation of Voice (CAPE-V) and Grade,\nRoughness, Breathiness, Asthenia, and Strain (GRBAS). However, these metrics\nare subjective and prone to inter-rater variability, motivating the need for\nautomated, objective assessment methods. This study proposes Voice Quality\nAssessment Network (VOQANet), a deep learning-based framework with an attention\nmechanism that leverages a Speech Foundation Model (SFM) to extract high-level\nacoustic and prosodic information from raw speech. To enhance robustness and\ninterpretability, we also introduce VOQANet+, which integrates low-level speech\ndescriptors such as jitter, shimmer, and harmonics-to-noise ratio (HNR) with\nSFM embeddings into a hybrid representation. Unlike prior studies focused only\non vowel-based phonation (PVQD-A subset) of the Perceptual Voice Quality\nDataset (PVQD), we evaluate our models on both vowel-based and sentence-level\nspeech (PVQD-S subset) to improve generalizability. Results show that\nsentence-based input outperforms vowel-based input, especially at the patient\nlevel, underscoring the value of longer utterances for capturing perceptual\nvoice attributes. VOQANet consistently surpasses baseline methods in root mean\nsquared error (RMSE) and Pearson correlation coefficient (PCC) across CAPE-V\nand GRBAS dimensions, with VOQANet+ achieving even better performance.\nAdditional experiments under noisy conditions show that VOQANet+ maintains high\nprediction accuracy and robustness, supporting its potential for real-world and\ntelehealth deployment."}
{"id": "2505.23815", "pdf": "https://arxiv.org/pdf/2505.23815", "abs": "https://arxiv.org/abs/2505.23815", "authors": ["StÃ©phane Aroca-Ouellette", "Natalie Mackraz", "Barry-John Theobald", "Katherine Metcalf"], "title": "Aligning LLMs by Predicting Preferences from User Writing Samples", "categories": ["cs.CL", "cs.LG"], "comment": "Accepted to ICML 2025. 32 pages total: 9 main, 2 references, 21\n  appendix. arXiv admin note: substantial text overlap with arXiv:2410.06273", "summary": "Accommodating human preferences is essential for creating aligned LLM agents\nthat deliver personalized and effective interactions. Recent work has shown the\npotential for LLMs acting as writing agents to infer a description of user\npreferences. Agent alignment then comes from conditioning on the inferred\npreference description. However, existing methods often produce generic\npreference descriptions that fail to capture the unique and individualized\nnature of human preferences. This paper introduces PROSE, a method designed to\nenhance the precision of preference descriptions inferred from user writing\nsamples. PROSE incorporates two key elements: (1) iterative refinement of\ninferred preferences, and (2) verification of inferred preferences across\nmultiple user writing samples. We evaluate PROSE with several LLMs (i.e.,\nQwen2.5 7B and 72B Instruct, GPT-mini, and GPT-4o) on a summarization and an\nemail writing task. We find that PROSE more accurately infers nuanced human\npreferences, improving the quality of the writing agent's generations over\nCIPHER (a state-of-the-art method for inferring preferences) by 33\\%. Lastly,\nwe demonstrate that ICL and PROSE are complementary methods, and combining them\nprovides up to a 9\\% improvement over ICL alone."}
{"id": "2505.24120", "pdf": "https://arxiv.org/pdf/2505.24120", "abs": "https://arxiv.org/abs/2505.24120", "authors": ["Ai Jian", "Weijie Qiu", "Xiaokun Wang", "Peiyu Wang", "Yunzhuo Hao", "Jiangbo Pei", "Yichen Wei", "Yi Peng", "Xuchen Song"], "title": "CSVQA: A Chinese Multimodal Benchmark for Evaluating STEM Reasoning Capabilities of VLMs", "categories": ["cs.CV", "cs.AI"], "comment": "36 pages", "summary": "Vision-Language Models (VLMs) have demonstrated remarkable progress in\nmultimodal understanding, yet their capabilities for scientific reasoning\nremains inadequately assessed. Current multimodal benchmarks predominantly\nevaluate generic image comprehension or text-driven reasoning, lacking\nauthentic scientific contexts that require domain-specific knowledge\nintegration with visual evidence analysis. To fill this gap, we present CSVQA,\na diagnostic multimodal benchmark specifically designed for evaluating\nscientific reasoning through domain-grounded visual question answering.Our\nbenchmark features 1,378 carefully constructed question-answer pairs spanning\ndiverse STEM disciplines, each demanding domain knowledge, integration of\nvisual evidence, and higher-order reasoning. Compared to prior multimodal\nbenchmarks, CSVQA places greater emphasis on real-world scientific content and\ncomplex reasoning.We additionally propose a rigorous evaluation protocol to\nsystematically assess whether model predictions are substantiated by valid\nintermediate reasoning steps based on curated explanations. Our comprehensive\nevaluation of 15 VLMs on this benchmark reveals notable performance\ndisparities, as even the top-ranked proprietary model attains only 49.6\\%\naccuracy.This empirical evidence underscores the pressing need for advancing\nscientific reasoning capabilities in VLMs. Our CSVQA is released at\nhttps://huggingface.co/datasets/Skywork/CSVQA."}
{"id": "2502.10682", "pdf": "https://arxiv.org/pdf/2502.10682", "abs": "https://arxiv.org/abs/2502.10682", "authors": ["Kafi Anan", "Anindya Bhattacharjee", "Ashir Intesher", "Kaidul Islam", "Abrar Assaeem Fuad", "Utsab Saha", "Hafiz Imtiaz"], "title": "CAE-Net: Generalized Deepfake Image Detection using Convolution and Attention Mechanisms with Spatial and Frequency Domain Features", "categories": ["cs.CV", "cs.LG", "eess.IV"], "comment": "Under review in Elsevier Journal of Visual Communication and Image\n  Representation", "summary": "Effective deepfake detection tools are becoming increasingly essential to the\ngrowing usage of deepfakes in unethical practices. There exists a wide range of\ndeepfake generation techniques, which makes it challenging to develop an\naccurate universal detection mechanism. The 2025 IEEE Signal Processing Cup\n(\\textit{DFWild-Cup} competition) provided a diverse dataset of deepfake images\ncontaining significant class imbalance. The images in the dataset are generated\nfrom multiple deepfake image generators, for training machine learning model(s)\nto emphasize the generalization of deepfake detection. To this end, we proposed\na disjoint set-based multistage training method to address the class imbalance\nand devised an ensemble-based architecture \\emph{CAE-Net}. Our architecture\nconsists of a convolution- and attention-based ensemble network, and employs\nthree different neural network architectures: EfficientNet, Data-Efficient\nImage Transformer (DeiT), and ConvNeXt with wavelet transform to capture both\nlocal and global features of deepfakes. We visualize the specific regions that\nthese models focus on for classification using Grad-CAM, and empirically\ndemonstrate the effectiveness of these models in grouping real and fake images\ninto cohesive clusters using t-SNE plots. Individually, the EfficientNet B0\narchitecture has achieved 90.79\\% accuracy, whereas the ConvNeXt and the DeiT\narchitecture have achieved 89.49\\% and 89.32\\% accuracy, respectively. With\nthese networks, our weighted ensemble model achieves an excellent accuracy of\n94.63\\% on the validation dataset of the SP Cup 2025 competition. The equal\nerror rate of 4.72\\% and the Area Under the ROC curve of 97.37\\% further\nconfirm the stability of our proposed method. Finally, the robustness of our\nproposed model against adversarial perturbation attacks is tested as well,\nshowing the inherent defensive properties of the ensemble approach."}
{"id": "2505.24306", "pdf": "https://arxiv.org/pdf/2505.24306", "abs": "https://arxiv.org/abs/2505.24306", "authors": ["Kechen Li", "Yaotian Tao", "Ximing Wen", "Quanwei Sun", "Zifei Gong", "Chang Xu", "Xizhe Zhang", "Tianbo Ji"], "title": "GridRoute: A Benchmark for LLM-Based Route Planning with Cardinal Movement in Grid Environments", "categories": ["cs.AI"], "comment": "8 pages", "summary": "Recent advancements in Large Language Models (LLMs) have demonstrated their\npotential in planning and reasoning tasks, offering a flexible alternative to\nclassical pathfinding algorithms. However, most existing studies focus on LLMs'\nindependent reasoning capabilities and overlook the potential synergy between\nLLMs and traditional algorithms. To fill this gap, we propose a comprehensive\nevaluation benchmark GridRoute to assess how LLMs can take advantage of\ntraditional algorithms. We also propose a novel hybrid prompting technique\ncalled Algorithm of Thought (AoT), which introduces traditional algorithms'\nguidance into prompting. Our benchmark evaluates six LLMs ranging from 7B to\n72B parameters across various map sizes, assessing their performance in\ncorrectness, optimality, and efficiency in grid environments with varying\nsizes. Our results show that AoT significantly boosts performance across all\nmodel sizes, particularly in larger or more complex environments, suggesting a\npromising approach to addressing path planning challenges. Our code is\nopen-sourced at https://github.com/LinChance/GridRoute."}
{"id": "2505.23941", "pdf": "https://arxiv.org/pdf/2505.23941", "abs": "https://arxiv.org/abs/2505.23941", "authors": ["An Vo", "Khai-Nguyen Nguyen", "Mohammad Reza Taesiri", "Vy Tuong Dang", "Anh Totti Nguyen", "Daeyoung Kim"], "title": "Vision Language Models are Biased", "categories": ["cs.LG", "cs.CV"], "comment": "Code and qualitative examples are available at:\n  vlmsarebiased.github.io", "summary": "Large language models (LLMs) memorize a vast amount of prior knowledge from\nthe Internet that help them on downstream tasks but also may notoriously sway\ntheir outputs towards wrong or biased answers. In this work, we test how the\nknowledge about popular subjects hurt the accuracy of vision language models\n(VLMs) on standard, objective visual tasks of counting and identification. We\nfind that state-of-the-art VLMs are strongly biased (e.g, unable to recognize a\nfourth stripe has been added to a 3-stripe Adidas logo) scoring an average of\n17.05% accuracy in counting (e.g., counting stripes in an Adidas-like logo)\nacross 7 diverse domains from animals, logos, chess, board games, optical\nillusions, to patterned grids. Insert text (e.g., \"Adidas\") describing the\nsubject name into the counterfactual image further decreases VLM accuracy. The\nbiases in VLMs are so strong that instructing them to double-check their\nresults or rely exclusively on image details to answer improves counting\naccuracy by only +2 points, on average. Our work presents an interesting\nfailure mode in VLMs and an automated framework for testing VLM biases. Code\nand data are available at: vlmsarebiased.github.io."}
{"id": "2505.14449", "pdf": "https://arxiv.org/pdf/2505.14449", "abs": "https://arxiv.org/abs/2505.14449", "authors": ["Yi-Cheng Lin", "Huang-Cheng Chou", "Hung-yi Lee"], "title": "Mitigating Subgroup Disparities in Multi-Label Speech Emotion Recognition: A Pseudo-Labeling and Unsupervised Learning Approach", "categories": ["eess.AS", "cs.CL", "cs.SD"], "comment": "Accepted by InterSpeech 2025. 7 pages including 2 pages of appendix", "summary": "While subgroup disparities and performance bias are increasingly studied in\ncomputational research, fairness in categorical Speech Emotion Recognition\n(SER) remains underexplored. Existing methods often rely on explicit\ndemographic labels, which are difficult to obtain due to privacy concerns. To\naddress this limitation, we introduce an Implicit Demography Inference (IDI)\nmodule that leverages pseudo-labeling from a pre-trained model and unsupervised\nlearning using k-means clustering to mitigate bias in SER. Our experiments show\nthat pseudo-labeling IDI reduces subgroup disparities, improving fairness\nmetrics by over 28% with less than a 2% decrease in SER accuracy. Also, the\nunsupervised IDI yields more than a 4.6% improvement in fairness metrics with a\ndrop of less than 3.6% in SER performance. Further analyses reveal that the\nunsupervised IDI consistently mitigates race and age disparities, demonstrating\nits potential when explicit demographic information is unavailable."}
{"id": "2505.21568", "pdf": "https://arxiv.org/pdf/2505.21568", "abs": "https://arxiv.org/abs/2505.21568", "authors": ["Haiyun Li", "Zhiyong Wu", "Xiaofeng Xie", "Jingran Xie", "Yaoxun Xu", "Hanyang Peng"], "title": "VoiceMark: Zero-Shot Voice Cloning-Resistant Watermarking Approach Leveraging Speaker-Specific Latents", "categories": ["cs.SD", "cs.AI", "cs.CR", "eess.AS"], "comment": "Accepted by Interspeech 2025", "summary": "Voice cloning (VC)-resistant watermarking is an emerging technique for\ntracing and preventing unauthorized cloning. Existing methods effectively trace\ntraditional VC models by training them on watermarked audio but fail in\nzero-shot VC scenarios, where models synthesize audio from an audio prompt\nwithout training. To address this, we propose VoiceMark, the first zero-shot\nVC-resistant watermarking method that leverages speaker-specific latents as the\nwatermark carrier, allowing the watermark to transfer through the zero-shot VC\nprocess into the synthesized audio. Additionally, we introduce VC-simulated\naugmentations and VAD-based loss to enhance robustness against distortions.\nExperiments on multiple zero-shot VC models demonstrate that VoiceMark achieves\nover 95% accuracy in watermark detection after zero-shot VC synthesis,\nsignificantly outperforming existing methods, which only reach around 50%. See\nour code and demos at: https://huggingface.co/spaces/haiyunli/VoiceMark"}
{"id": "2505.23816", "pdf": "https://arxiv.org/pdf/2505.23816", "abs": "https://arxiv.org/abs/2505.23816", "authors": ["Trenton Chang", "Tobias Schnabel", "Adith Swaminathan", "Jenna Wiens"], "title": "A Course Correction in Steerability Evaluation: Revealing Miscalibration and Side Effects in LLMs", "categories": ["cs.CL", "cs.LG"], "comment": "10 pages, 8 figures. 26 pages of references and supplementary\n  material, 20 additional figures", "summary": "Despite advances in large language models (LLMs) on reasoning and\ninstruction-following benchmarks, it remains unclear whether they can reliably\nproduce outputs aligned with a broad variety of user goals, a concept we refer\nto as steerability. The abundance of methods proposed to modify LLM behavior\nmakes it unclear whether current LLMs are already steerable, or require further\nintervention. In particular, LLMs may exhibit (i) poor coverage, where rare\nuser goals are underrepresented; (ii) miscalibration, where models overshoot\nrequests; and (iii) side effects, where changes to one dimension of text\ninadvertently affect others. To systematically evaluate these failures, we\nintroduce a framework based on a multi-dimensional goal space that models user\ngoals and LLM outputs as vectors with dimensions corresponding to text\nattributes (e.g., reading difficulty). Applied to a text-rewriting task, we\nfind that current LLMs struggle with steerability, as side effects are\npersistent. Interventions to improve steerability, such as prompt engineering,\nbest-of-$N$ sampling, and reinforcement learning fine-tuning, have varying\neffectiveness, yet side effects remain problematic. Our findings suggest that\neven strong LLMs struggle with steerability, and existing alignment strategies\nmay be insufficient. We open-source our steerability evaluation framework at\nhttps://github.com/MLD3/steerability."}
{"id": "2505.24139", "pdf": "https://arxiv.org/pdf/2505.24139", "abs": "https://arxiv.org/abs/2505.24139", "authors": ["Yichen Xie", "Runsheng Xu", "Tong He", "Jyh-Jing Hwang", "Katie Luo", "Jingwei Ji", "Hubert Lin", "Letian Chen", "Yiren Lu", "Zhaoqi Leng", "Dragomir Anguelov", "Mingxing Tan"], "title": "S4-Driver: Scalable Self-Supervised Driving Multimodal Large Language Modelwith Spatio-Temporal Visual Representation", "categories": ["cs.CV", "cs.AI"], "comment": "Accepted by CVPR2025", "summary": "The latest advancements in multi-modal large language models (MLLMs) have\nspurred a strong renewed interest in end-to-end motion planning approaches for\nautonomous driving. Many end-to-end approaches rely on human annotations to\nlearn intermediate perception and prediction tasks, while purely\nself-supervised approaches--which directly learn from sensor inputs to generate\nplanning trajectories without human annotations often underperform the state of\nthe art. We observe a key gap in the input representation space: end-to-end\napproaches built on MLLMs are often pretrained with reasoning tasks in 2D image\nspace rather than the native 3D space in which autonomous vehicles plan. To\nthis end, we propose S4-Driver, a scalable self-supervised motion planning\nalgorithm with spatio-temporal visual representation, based on the popular PaLI\nmultimodal large language model. S4-Driver uses a novel sparse volume strategy\nto seamlessly transform the strong visual representation of MLLMs from\nperspective view to 3D space without the need to finetune the vision encoder.\nThis representation aggregates multi-view and multi-frame visual inputs and\nenables better prediction of planning trajectories in 3D space. To validate our\nmethod, we run experiments on both nuScenes and Waymo Open Motion Dataset (with\nin-house camera data). Results show that S4-Driver performs favorably against\nexisting supervised multi-task approaches while requiring no human annotations.\nIt also demonstrates great scalability when pretrained on large volumes of\nunannotated driving logs."}
{"id": "2505.24422", "pdf": "https://arxiv.org/pdf/2505.24422", "abs": "https://arxiv.org/abs/2505.24422", "authors": ["Zhenghua Pan", "Yong Wang"], "title": "Three Kinds of Negation in Knowledge and Their Mathematical Foundations", "categories": ["cs.AI"], "comment": "32 pages,13 figures", "summary": "In the field of artificial intelligence, understanding, distinguishing,\nexpressing, and computing the negation in knowledge is a fundamental issue in\nknowledge processing and research. In this paper, we examine and analyze the\nunderstanding and characteristics of negation in various fields such as\nphilosophy, logic, and linguistics etc. Based on the distinction between the\nconcepts of contradiction and opposition, we propose that there are three\ndifferent types of negation in knowledge from a conceptual perspective:\ncontradictory negation, opposite negation, and intermediary negation. To\nestablish a mathematical foundation that fully reflects the intrinsic\nconnections, properties, and laws of these different forms of negation, we\nintroduce SCOI: sets with contradictory negation, opposite negation and\nintermediary negation, and LCOI: logic with contradictory negation, opposite\nnegation and intermediary negation, and we proved the main operational\nproperties of SCOI as well as the formal inference relations in LCOI."}
{"id": "2505.23942", "pdf": "https://arxiv.org/pdf/2505.23942", "abs": "https://arxiv.org/abs/2505.23942", "authors": ["Gaurav Sarkar", "Jay Gala", "Subarna Tripathi"], "title": "SG-Blend: Learning an Interpolation Between Improved Swish and GELU for Robust Neural Representations", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "The design of activation functions remains a pivotal component in optimizing\ndeep neural networks. While prevailing choices like Swish and GELU demonstrate\nconsiderable efficacy, they often exhibit domain-specific optima. This work\nintroduces SG-Blend, a novel activation function that blends our proposed\nSSwish, a first-order symmetric variant of Swish and the established GELU\nthrough dynamic interpolation. By adaptively blending these constituent\nfunctions via learnable parameters, SG-Blend aims to harness their\ncomplementary strengths: SSwish's controlled non-monotonicity and symmetry, and\nGELU's smooth, probabilistic profile, to achieve a more universally robust\nbalance between model expressivity and gradient stability. We conduct\ncomprehensive empirical evaluations across diverse modalities and\narchitectures, showing performance improvements across all considered natural\nlanguage and computer vision tasks and models. These results, achieved with\nnegligible computational overhead, underscore SG-Blend's potential as a\nversatile, drop-in replacement that consistently outperforms strong\ncontemporary baselines. The code is available at\nhttps://anonymous.4open.science/r/SGBlend-6CBC."}
{"id": "2505.14910", "pdf": "https://arxiv.org/pdf/2505.14910", "abs": "https://arxiv.org/abs/2505.14910", "authors": ["Yu Zhang", "Wenxiang Guo", "Changhao Pan", "Dongyu Yao", "Zhiyuan Zhu", "Ziyue Jiang", "Yuhan Wang", "Tao Jin", "Zhou Zhao"], "title": "TCSinger 2: Customizable Multilingual Zero-shot Singing Voice Synthesis", "categories": ["eess.AS", "cs.CL", "cs.SD"], "comment": "Accepted by Findings of ACL 2025", "summary": "Customizable multilingual zero-shot singing voice synthesis (SVS) has various\npotential applications in music composition and short video dubbing. However,\nexisting SVS models overly depend on phoneme and note boundary annotations,\nlimiting their robustness in zero-shot scenarios and producing poor transitions\nbetween phonemes and notes. Moreover, they also lack effective multi-level\nstyle control via diverse prompts. To overcome these challenges, we introduce\nTCSinger 2, a multi-task multilingual zero-shot SVS model with style transfer\nand style control based on various prompts. TCSinger 2 mainly includes three\nkey modules: 1) Blurred Boundary Content (BBC) Encoder, predicts duration,\nextends content embedding, and applies masking to the boundaries to enable\nsmooth transitions. 2) Custom Audio Encoder, uses contrastive learning to\nextract aligned representations from singing, speech, and textual prompts. 3)\nFlow-based Custom Transformer, leverages Cus-MOE, with F0 supervision,\nenhancing both the synthesis quality and style modeling of the generated\nsinging voice. Experimental results show that TCSinger 2 outperforms baseline\nmodels in both subjective and objective metrics across multiple related tasks.\nSinging voice samples are available at\nhttps://aaronz345.github.io/TCSinger2Demo/."}
{"id": "2411.05872", "pdf": "https://arxiv.org/pdf/2411.05872", "abs": "https://arxiv.org/abs/2411.05872", "authors": ["Amirbek Djanibekov", "Hawau Olamide Toyin", "Raghad Alshalan", "Abdullah Alitr", "Hanan Aldarmaki"], "title": "Dialectal Coverage And Generalization in Arabic Speech Recognition", "categories": ["cs.CL", "cs.SD", "eess.AS"], "comment": null, "summary": "Developing robust automatic speech recognition (ASR) systems for Arabic\nrequires effective strategies to manage its diversity. Existing ASR systems\nmainly cover the modern standard Arabic (MSA) variety and few high-resource\ndialects, but fall short in coverage and generalization across the multitude of\nspoken variants. Code-switching with English and French is also common in\ndifferent regions of the Arab world, which challenges the performance of\nmonolingual Arabic models. In this work, we introduce a suite of ASR models\noptimized to effectively recognize multiple variants of spoken Arabic,\nincluding MSA, various dialects, and code-switching. We provide open-source\npre-trained models that cover data from 17 Arabic-speaking countries, and\nfine-tuned MSA and dialectal ASR models that include at least 11 variants, as\nwell as multi-lingual ASR models covering embedded languages in code-switched\nutterances. We evaluate ASR performance across these spoken varieties and\ndemonstrate both coverage and performance gains compared to prior models."}
{"id": "2505.23818", "pdf": "https://arxiv.org/pdf/2505.23818", "abs": "https://arxiv.org/abs/2505.23818", "authors": ["Masoud Safilian", "Amin Beheshti", "Stephen Elbourn"], "title": "Ratas framework: A comprehensive genai-based approach to rubric-based marking of real-world textual exams", "categories": ["cs.CL"], "comment": null, "summary": "Automated answer grading is a critical challenge in educational technology,\nwith the potential to streamline assessment processes, ensure grading\nconsistency, and provide timely feedback to students. However, existing\napproaches are often constrained to specific exam formats, lack\ninterpretability in score assignment, and struggle with real-world\napplicability across diverse subjects and assessment types. To address these\nlimitations, we introduce RATAS (Rubric Automated Tree-based Answer Scoring), a\nnovel framework that leverages state-of-the-art generative AI models for\nrubric-based grading of textual responses. RATAS is designed to support a wide\nrange of grading rubrics, enable subject-agnostic evaluation, and generate\nstructured, explainable rationales for assigned scores. We formalize the\nautomatic grading task through a mathematical framework tailored to\nrubric-based assessment and present an architecture capable of handling\ncomplex, real-world exam structures. To rigorously evaluate our approach, we\nconstruct a unique, contextualized dataset derived from real-world\nproject-based courses, encompassing diverse response formats and varying levels\nof complexity. Empirical results demonstrate that RATAS achieves high\nreliability and accuracy in automated grading while providing interpretable\nfeedback that enhances transparency for both students and nstructors."}
{"id": "2505.24141", "pdf": "https://arxiv.org/pdf/2505.24141", "abs": "https://arxiv.org/abs/2505.24141", "authors": ["Jiashuai Liu", "Yingjia Shang", "Yingkang Zhan", "Di Zhang", "Yi Niu", "Dong Wei", "Xian Wu", "Zeyu Gao", "Chen Li", "Yefeng Zheng"], "title": "The Butterfly Effect in Pathology: Exploring Security in Pathology Foundation Models", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "With the widespread adoption of pathology foundation models in both research\nand clinical decision support systems, exploring their security has become a\ncritical concern. However, despite their growing impact, the vulnerability of\nthese models to adversarial attacks remains largely unexplored. In this work,\nwe present the first systematic investigation into the security of pathology\nfoundation models for whole slide image~(WSI) analysis against adversarial\nattacks. Specifically, we introduce the principle of \\textit{local perturbation\nwith global impact} and propose a label-free attack framework that operates\nwithout requiring access to downstream task labels. Under this attack\nframework, we revise four classical white-box attack methods and redefine the\nperturbation budget based on the characteristics of WSI. We conduct\ncomprehensive experiments on three representative pathology foundation models\nacross five datasets and six downstream tasks. Despite modifying only 0.1\\% of\npatches per slide with imperceptible noise, our attack leads to downstream\naccuracy degradation that can reach up to 20\\% in the worst cases. Furthermore,\nwe analyze key factors that influence attack success, explore the relationship\nbetween patch-level vulnerability and semantic content, and conduct a\npreliminary investigation into potential defence strategies. These findings lay\nthe groundwork for future research on the adversarial robustness and reliable\ndeployment of pathology foundation models. Our code is publicly available at:\nhttps://github.com/Jiashuai-Liu-hmos/Attack-WSI-pathology-foundation-models."}
{"id": "2505.24426", "pdf": "https://arxiv.org/pdf/2505.24426", "abs": "https://arxiv.org/abs/2505.24426", "authors": ["David Gamez"], "title": "P: A Universal Measure of Predictive Intelligence", "categories": ["cs.AI"], "comment": null, "summary": "Over the last thirty years, considerable progress has been made with the\ndevelopment of systems that can drive cars, play games, predict protein folding\nand generate natural language. These systems are described as intelligent and\nthere has been a great deal of talk about the rapid increase in artificial\nintelligence and its potential dangers. However, our theoretical understanding\nof intelligence and ability to measure it lag far behind our capacity for\nbuilding systems that mimic intelligent human behaviour. There is no commonly\nagreed definition of the intelligence that AI systems are said to possess.\nNo-one has developed a practical measure that would enable us to compare the\nintelligence of humans, animals and AIs on a single ratio scale.\n  This paper sets out a new universal measure of intelligence that is based on\nthe hypothesis that prediction is the most important component of intelligence.\nAs an agent interacts with its normal environment, the accuracy of its\npredictions is summed up and the complexity of its predictions and perceived\nenvironment is accounted for using Kolmogorov complexity. Two experiments were\ncarried out to evaluate the practical feasibility of the algorithm. These\ndemonstrated that it could measure the intelligence of an agent embodied in a\nvirtual maze and an agent that makes predictions about time-series data. This\nuniversal measure could be the starting point for a new comparative science of\nintelligence that ranks humans, animals and AIs on a single ratio scale."}
{"id": "2505.23947", "pdf": "https://arxiv.org/pdf/2505.23947", "abs": "https://arxiv.org/abs/2505.23947", "authors": ["Samuel MÃ¼ller", "Arik Reuter", "Noah Hollmann", "David RÃ¼gamer", "Frank Hutter"], "title": "Position: The Future of Bayesian Prediction Is Prior-Fitted", "categories": ["cs.LG", "cs.AI"], "comment": "Accepted as position paper at ICML 2025", "summary": "Training neural networks on randomly generated artificial datasets yields\nBayesian models that capture the prior defined by the dataset-generating\ndistribution. Prior-data Fitted Networks (PFNs) are a class of methods designed\nto leverage this insight. In an era of rapidly increasing computational\nresources for pre-training and a near stagnation in the generation of new\nreal-world data in many applications, PFNs are poised to play a more important\nrole across a wide range of applications. They enable the efficient allocation\nof pre-training compute to low-data scenarios. Originally applied to small\nBayesian modeling tasks, the field of PFNs has significantly expanded to\naddress more complex domains and larger datasets. This position paper argues\nthat PFNs and other amortized inference approaches represent the future of\nBayesian inference, leveraging amortized learning to tackle data-scarce\nproblems. We thus believe they are a fruitful area of research. In this\nposition paper, we explore their potential and directions to address their\ncurrent limitations."}
{"id": "2505.15004", "pdf": "https://arxiv.org/pdf/2505.15004", "abs": "https://arxiv.org/abs/2505.15004", "authors": ["Jixun Yao", "Hexin Liu", "Eng Siong Chng", "Lei Xie"], "title": "EASY: Emotion-aware Speaker Anonymization via Factorized Distillation", "categories": ["eess.AS", "cs.SD"], "comment": "Accepted by INTERSPEECH 2025", "summary": "Emotion plays a significant role in speech interaction, conveyed through\ntone, pitch, and rhythm, enabling the expression of feelings and intentions\nbeyond words to create a more personalized experience. However, most existing\nspeaker anonymization systems employ parallel disentanglement methods, which\nonly separate speech into linguistic content and speaker identity, often\nneglecting the preservation of the original emotional state. In this study, we\nintroduce EASY, an emotion-aware speaker anonymization framework. EASY employs\na novel sequential disentanglement process to disentangle speaker identity,\nlinguistic content, and emotional representation, modeling each speech\nattribute in distinct subspaces through a factorized distillation approach. By\nindependently constraining speaker identity and emotional representation, EASY\nminimizes information leakage, enhancing privacy protection while preserving\noriginal linguistic content and emotional state. Experimental results on the\nVoicePrivacy Challenge official datasets demonstrate that our proposed approach\noutperforms all baseline systems, effectively protecting speaker privacy while\nmaintaining linguistic content and emotional state."}
{"id": "2411.19486", "pdf": "https://arxiv.org/pdf/2411.19486", "abs": "https://arxiv.org/abs/2411.19486", "authors": ["Jeongsoo Choi", "Ji-Hoon Kim", "Jinyu Li", "Joon Son Chung", "Shujie Liu"], "title": "V2SFlow: Video-to-Speech Generation with Speech Decomposition and Rectified Flow", "categories": ["cs.CV", "cs.SD", "eess.AS"], "comment": "ICASSP 2025", "summary": "In this paper, we introduce V2SFlow, a novel Video-to-Speech (V2S) framework\ndesigned to generate natural and intelligible speech directly from silent\ntalking face videos. While recent V2S systems have shown promising results on\nconstrained datasets with limited speakers and vocabularies, their performance\noften degrades on real-world, unconstrained datasets due to the inherent\nvariability and complexity of speech signals. To address these challenges, we\ndecompose the speech signal into manageable subspaces (content, pitch, and\nspeaker information), each representing distinct speech attributes, and predict\nthem directly from the visual input. To generate coherent and realistic speech\nfrom these predicted attributes, we employ a rectified flow matching decoder\nbuilt on a Transformer architecture, which models efficient probabilistic\npathways from random noise to the target speech distribution. Extensive\nexperiments demonstrate that V2SFlow significantly outperforms state-of-the-art\nmethods, even surpassing the naturalness of ground truth utterances. Code and\nmodels are available at: https://github.com/kaistmm/V2SFlow"}
{"id": "2505.23820", "pdf": "https://arxiv.org/pdf/2505.23820", "abs": "https://arxiv.org/abs/2505.23820", "authors": ["Bhaktipriya Radharapu", "Manon Revel", "Megan Ung", "Sebastian Ruder", "Adina Williams"], "title": "Arbiters of Ambivalence: Challenges of Using LLMs in No-Consensus Tasks", "categories": ["cs.CL"], "comment": null, "summary": "The increasing use of LLMs as substitutes for humans in ``aligning'' LLMs has\nraised questions about their ability to replicate human judgments and\npreferences, especially in ambivalent scenarios where humans disagree. This\nstudy examines the biases and limitations of LLMs in three roles: answer\ngenerator, judge, and debater. These roles loosely correspond to previously\ndescribed alignment frameworks: preference alignment (judge) and scalable\noversight (debater), with the answer generator reflecting the typical setting\nwith user interactions. We develop a ``no-consensus'' benchmark by curating\nexamples that encompass a variety of a priori ambivalent scenarios, each\npresenting two possible stances. Our results show that while LLMs can provide\nnuanced assessments when generating open-ended answers, they tend to take a\nstance on no-consensus topics when employed as judges or debaters. These\nfindings underscore the necessity for more sophisticated methods for aligning\nLLMs without human oversight, highlighting that LLMs cannot fully capture human\ndisagreement even on topics where humans themselves are divided."}
{"id": "2505.24156", "pdf": "https://arxiv.org/pdf/2505.24156", "abs": "https://arxiv.org/abs/2505.24156", "authors": ["Chenyou Fan", "Fangzheng Yan", "Chenjia Bai", "Jiepeng Wang", "Chi Zhang", "Zhen Wang", "Xuelong Li"], "title": "Towards a Generalizable Bimanual Foundation Policy via Flow-based Video Prediction", "categories": ["cs.CV", "cs.RO"], "comment": null, "summary": "Learning a generalizable bimanual manipulation policy is extremely\nchallenging for embodied agents due to the large action space and the need for\ncoordinated arm movements. Existing approaches rely on Vision-Language-Action\n(VLA) models to acquire bimanual policies. However, transferring knowledge from\nsingle-arm datasets or pre-trained VLA models often fails to generalize\neffectively, primarily due to the scarcity of bimanual data and the fundamental\ndifferences between single-arm and bimanual manipulation. In this paper, we\npropose a novel bimanual foundation policy by fine-tuning the leading\ntext-to-video models to predict robot trajectories and training a lightweight\ndiffusion policy for action generation. Given the lack of embodied knowledge in\ntext-to-video models, we introduce a two-stage paradigm that fine-tunes\nindependent text-to-flow and flow-to-video models derived from a pre-trained\ntext-to-video model. Specifically, optical flow serves as an intermediate\nvariable, providing a concise representation of subtle movements between\nimages. The text-to-flow model predicts optical flow to concretize the intent\nof language instructions, and the flow-to-video model leverages this flow for\nfine-grained video prediction. Our method mitigates the ambiguity of language\nin single-stage text-to-video prediction and significantly reduces the\nrobot-data requirement by avoiding direct use of low-level actions. In\nexperiments, we collect high-quality manipulation data for real dual-arm robot,\nand the results of simulation and real-world experiments demonstrate the\neffectiveness of our method."}
{"id": "2505.24442", "pdf": "https://arxiv.org/pdf/2505.24442", "abs": "https://arxiv.org/abs/2505.24442", "authors": ["Zhentao Xie", "Chengcheng Han", "Jinxin Shi", "Wenjun Cui", "Xin Zhao", "Xingjiao Wu", "Jiabao Zhao"], "title": "RMoA: Optimizing Mixture-of-Agents through Diversity Maximization and Residual Compensation", "categories": ["cs.AI"], "comment": "Accepted by ACL 2025 (Findings)", "summary": "Although multi-agent systems based on large language models show strong\ncapabilities on multiple tasks, they are still limited by high computational\noverhead, information loss, and robustness. Inspired by ResNet's residual\nlearning, we propose Residual Mixture-of-Agents (RMoA), integrating residual\nconnections to optimize efficiency and reliability. To maximize information\nutilization from model responses while minimizing computational costs, we\ninnovatively design an embedding-based diversity selection mechanism that\ngreedily selects responses via vector similarity. Furthermore, to mitigate\niterative information degradation, we introduce a Residual Extraction Agent to\npreserve cross-layer incremental information by capturing inter-layer response\ndifferences, coupled with a Residual Aggregation Agent for hierarchical\ninformation integration. Additionally, we propose an adaptive termination\nmechanism that dynamically halts processing based on residual convergence,\nfurther improving inference efficiency. RMoA achieves state-of-the-art\nperformance on the benchmarks of across alignment, mathematical reasoning, code\ngeneration, and multitasking understanding, while significantly reducing\ncomputational overhead. Code is available at\nhttps://github.com/mindhunter01/RMoA."}
{"id": "2505.23949", "pdf": "https://arxiv.org/pdf/2505.23949", "abs": "https://arxiv.org/abs/2505.23949", "authors": ["Xiang Meng", "Mehdi Makni", "Rahul Mazumder"], "title": "TSENOR: Highly-Efficient Algorithm for Finding Transposable N:M Sparse Masks", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Network pruning reduces the computational requirements of large neural\nnetworks, with N:M sparsity -- retaining only N out of every M consecutive\nweights -- offering a compelling balance between compressed model quality and\nhardware acceleration. However, N:M sparsity only accelerates forward-pass\ncomputations, as N:M patterns are not preserved during matrix transposition,\nlimiting efficiency during training where both passes are computationally\nintensive. While transposable N:M sparsity has been proposed to address this\nlimitation, existing methods for finding transposable N:M sparse masks either\nfail to scale to large models or are restricted to M=4 which results in\nsuboptimal compression-accuracy trade-off. We introduce an efficient solver for\ntransposable N:M masks that scales to billion-parameter models. We formulate\nmask generation as optimal transport problems and solve through entropy\nregularization and Dykstra's algorithm, followed by a rounding procedure. Our\ntensor-based implementation exploits GPU parallelism, achieving up to 100x\nspeedup with only 1-10% error compared to existing methods. Our approach can be\nintegrated with layer-wise N:M pruning frameworks including Wanda, SparseGPT\nand ALPS to produce transposable N:M sparse models with arbitrary N:M values.\nExperiments show that LLaMA3.2-8B with transposable 16:32 sparsity maintains\nperformance close to its standard N:M counterpart and outperforms standard 2:4\nsparse model, showing the practical value of our approach."}
{"id": "2505.19577", "pdf": "https://arxiv.org/pdf/2505.19577", "abs": "https://arxiv.org/abs/2505.19577", "authors": ["Yu Xi", "Haoyu Li", "Xiaoyu Gu", "Yidi Jiang", "Kai Yu"], "title": "MFA-KWS: Effective Keyword Spotting with Multi-head Frame-asynchronous Decoding", "categories": ["eess.AS", "cs.SD"], "comment": "TASLP under review", "summary": "Keyword spotting (KWS) is essential for voice-driven applications, demanding\nboth accuracy and efficiency. Traditional ASR-based KWS methods, such as greedy\nand beam search, explore the entire search space without explicitly\nprioritizing keyword detection, often leading to suboptimal performance. In\nthis paper, we propose an effective keyword-specific KWS framework by\nintroducing a streaming-oriented CTC-Transducer-combined frame-asynchronous\nsystem with multi-head frame-asynchronous decoding (MFA-KWS). Specifically,\nMFA-KWS employs keyword-specific phone-synchronous decoding for CTC and\nreplaces conventional RNN-T with Token-and-Duration Transducer to enhance both\nperformance and efficiency. Furthermore, we explore various score fusion\nstrategies, including single-frame-based and consistency-based methods.\nExtensive experiments demonstrate the superior performance of MFA-KWS, which\nachieves state-of-the-art results on both fixed keyword and arbitrary keywords\ndatasets, such as Snips, MobvoiHotwords, and LibriKWS-20, while exhibiting\nstrong robustness in noisy environments. Among fusion strategies, the\nconsistency-based CDC-Last method delivers the best performance. Additionally,\nMFA-KWS achieves a 47% to 63% speed-up over the frame-synchronous baselines\nacross various datasets. Extensive experimental results confirm that MFA-KWS is\nan effective and efficient KWS framework, making it well-suited for on-device\ndeployment."}
{"id": "2505.14874", "pdf": "https://arxiv.org/pdf/2505.14874", "abs": "https://arxiv.org/abs/2505.14874", "authors": ["Chin-Jou Li", "Eunjung Yeo", "Kwanghee Choi", "Paula Andrea PÃ©rez-Toro", "Masao Someki", "Rohan Kumar Das", "Zhengjun Yue", "Juan Rafael Orozco-Arroyave", "Elmar NÃ¶th", "David R. Mortensen"], "title": "Towards Inclusive ASR: Investigating Voice Conversion for Dysarthric Speech Recognition in Low-Resource Languages", "categories": ["cs.CL", "cs.SD", "eess.AS"], "comment": "5 pages, 1 figure, Accepted to Interspeech 2025", "summary": "Automatic speech recognition (ASR) for dysarthric speech remains challenging\ndue to data scarcity, particularly in non-English languages. To address this,\nwe fine-tune a voice conversion model on English dysarthric speech (UASpeech)\nto encode both speaker characteristics and prosodic distortions, then apply it\nto convert healthy non-English speech (FLEURS) into non-English dysarthric-like\nspeech. The generated data is then used to fine-tune a multilingual ASR model,\nMassively Multilingual Speech (MMS), for improved dysarthric speech\nrecognition. Evaluation on PC-GITA (Spanish), EasyCall (Italian), and SSNCE\n(Tamil) demonstrates that VC with both speaker and prosody conversion\nsignificantly outperforms the off-the-shelf MMS performance and conventional\naugmentation techniques such as speed and tempo perturbation. Objective and\nsubjective analyses of the generated data further confirm that the generated\nspeech simulates dysarthric characteristics."}
{"id": "2505.23823", "pdf": "https://arxiv.org/pdf/2505.23823", "abs": "https://arxiv.org/abs/2505.23823", "authors": ["Youngseung Jeon", "Ziwen Li", "Thomas Li", "JiaSyuan Chang", "Morteza Ziyadi", "Xiang 'Anthony' Chen"], "title": "RAGPPI: RAG Benchmark for Protein-Protein Interactions in Drug Discovery", "categories": ["cs.CL"], "comment": "17 pages, 4 figures, 8 tables", "summary": "Retrieving the biological impacts of protein-protein interactions (PPIs) is\nessential for target identification (Target ID) in drug development. Given the\nvast number of proteins involved, this process remains time-consuming and\nchallenging. Large Language Models (LLMs) and Retrieval-Augmented Generation\n(RAG) frameworks have supported Target ID; however, no benchmark currently\nexists for identifying the biological impacts of PPIs. To bridge this gap, we\nintroduce the RAG Benchmark for PPIs (RAGPPI), a factual question-answer\nbenchmark of 4,420 question-answer pairs that focus on the potential biological\nimpacts of PPIs. Through interviews with experts, we identified criteria for a\nbenchmark dataset, such as a type of QA and source. We built a gold-standard\ndataset (500 QA pairs) through expert-driven data annotation. We developed an\nensemble auto-evaluation LLM that reflected expert labeling characteristics,\nwhich facilitates the construction of a silver-standard dataset (3,720 QA\npairs). We are committed to maintaining RAGPPI as a resource to support the\nresearch community in advancing RAG systems for drug discovery QA solutions."}
{"id": "2505.24158", "pdf": "https://arxiv.org/pdf/2505.24158", "abs": "https://arxiv.org/abs/2505.24158", "authors": ["Bo Fang", "Wenhao Wu", "Qiangqiang Wu", "Yuxin Song", "Antoni B. Chan"], "title": "Threading Keyframe with Narratives: MLLMs as Strong Long Video Comprehenders", "categories": ["cs.CV"], "comment": null, "summary": "Employing Multimodal Large Language Models (MLLMs) for long video\nunderstanding remains a challenging problem due to the dilemma between the\nsubstantial number of video frames (i.e., visual tokens) versus the limited\ncontext length of language models. Traditional uniform sampling often leads to\nselection of irrelevant content, while post-training MLLMs on thousands of\nframes imposes a substantial computational burden. In this paper, we propose\nthreading keyframes with narratives (Nar-KFC), a plug-and-play module to\nfacilitate effective and efficient long video perception. Nar-KFC generally\ninvolves two collaborative steps. First, we formulate the keyframe selection\nprocess as an integer quadratic programming problem, jointly optimizing\nquery-relevance and frame-diversity. To avoid its computational complexity, a\ncustomized greedy search strategy is designed as an efficient alternative.\nSecond, to mitigate the temporal discontinuity caused by sparse keyframe\nsampling, we further introduce interleaved textual narratives generated from\nnon-keyframes using off-the-shelf captioners. These narratives are inserted\nbetween keyframes based on their true temporal order, forming a coherent and\ncompact representation. Nar-KFC thus serves as a temporal- and content-aware\ncompression strategy that complements visual and textual modalities.\nExperimental results on multiple long-video benchmarks demonstrate that Nar-KFC\nsignificantly improves the performance of popular MLLMs. Code will be made\npublicly available."}
{"id": "2505.24458", "pdf": "https://arxiv.org/pdf/2505.24458", "abs": "https://arxiv.org/abs/2505.24458", "authors": ["Tianlong Yu", "Chenghang Ye", "Zheyu Yang", "Ziyi Zhou", "Cui Tang", "Zui Tao", "Jun Zhang", "Kailong Wang", "Liting Zhou", "Yang Yang", "Ting Bi"], "title": "SEAR: A Multimodal Dataset for Analyzing AR-LLM-Driven Social Engineering Behaviors", "categories": ["cs.AI"], "comment": null, "summary": "The SEAR Dataset is a novel multimodal resource designed to study the\nemerging threat of social engineering (SE) attacks orchestrated through\naugmented reality (AR) and multimodal large language models (LLMs). This\ndataset captures 180 annotated conversations across 60 participants in\nsimulated adversarial scenarios, including meetings, classes and networking\nevents. It comprises synchronized AR-captured visual/audio cues (e.g., facial\nexpressions, vocal tones), environmental context, and curated social media\nprofiles, alongside subjective metrics such as trust ratings and susceptibility\nassessments. Key findings reveal SEAR's alarming efficacy in eliciting\ncompliance (e.g., 93.3% phishing link clicks, 85% call acceptance) and\nhijacking trust (76.7% post-interaction trust surge). The dataset supports\nresearch in detecting AR-driven SE attacks, designing defensive frameworks, and\nunderstanding multimodal adversarial manipulation. Rigorous ethical safeguards,\nincluding anonymization and IRB compliance, ensure responsible use. The SEAR\ndataset is available at https://github.com/INSLabCN/SEAR-Dataset."}
{"id": "2505.23954", "pdf": "https://arxiv.org/pdf/2505.23954", "abs": "https://arxiv.org/abs/2505.23954", "authors": ["Dylan Zapzalka", "Trenton Chang", "Lindsay Warrenburg", "Sae-Hwan Park", "Daniel K. Shenfeld", "Ravi B. Parikh", "Jenna Wiens", "Maggie Makar"], "title": "Estimating Misreporting in the Presence of Genuine Modification: A Causal Perspective", "categories": ["cs.LG"], "comment": null, "summary": "In settings where ML models are used to inform the allocation of resources,\nagents affected by the allocation decisions might have an incentive to\nstrategically change their features to secure better outcomes. While prior work\nhas studied strategic responses broadly, disentangling misreporting from\ngenuine modification remains a fundamental challenge. In this paper, we propose\na causally-motivated approach to identify and quantify how much an agent\nmisreports on average by distinguishing deceptive changes in their features\nfrom genuine modification. Our key insight is that, unlike genuine\nmodification, misreported features do not causally affect downstream variables\n(i.e., causal descendants). We exploit this asymmetry by comparing the causal\neffect of misreported features on their causal descendants as derived from\nmanipulated datasets against those from unmanipulated datasets. We formally\nprove identifiability of the misreporting rate and characterize the variance of\nour estimator. We empirically validate our theoretical results using a\nsemi-synthetic and real Medicare dataset with misreported data, demonstrating\nthat our approach can be employed to identify misreporting in real-world\nscenarios."}
{"id": "2505.19595", "pdf": "https://arxiv.org/pdf/2505.19595", "abs": "https://arxiv.org/abs/2505.19595", "authors": ["Jeongsoo Choi", "Zhikang Niu", "Ji-Hoon Kim", "Chunhui Wang", "Joon Son Chung", "Xie Chen"], "title": "Accelerating Diffusion-based Text-to-Speech Model Training with Dual Modality Alignment", "categories": ["eess.AS", "cs.SD"], "comment": "Interspeech 2025", "summary": "The goal of this paper is to optimize the training process of diffusion-based\ntext-to-speech models. While recent studies have achieved remarkable\nadvancements, their training demands substantial time and computational costs,\nlargely due to the implicit guidance of diffusion models in learning complex\nintermediate representations. To address this, we propose A-DMA, an effective\nstrategy for Accelerating training with Dual Modality Alignment. Our method\nintroduces a novel alignment pipeline leveraging both text and speech\nmodalities: text-guided alignment, which incorporates contextual\nrepresentations, and speech-guided alignment, which refines semantic\nrepresentations. By aligning hidden states with discriminative features, our\ntraining scheme reduces the reliance on diffusion models for learning complex\nrepresentations. Extensive experiments demonstrate that A-DMA doubles the\nconvergence speed while achieving superior performance over baselines. Code and\ndemo samples are available at: https://github.com/ZhikangNiu/A-DMA"}
{"id": "2505.17076", "pdf": "https://arxiv.org/pdf/2505.17076", "abs": "https://arxiv.org/abs/2505.17076", "authors": ["Haoyang Zhang", "Hexin Liu", "Xiangyu Zhang", "Qiquan Zhang", "Yuchen Hu", "Junqi Zhao", "Fei Tian", "Xuerui Yang", "Eng Siong Chng"], "title": "Impact of Frame Rates on Speech Tokenizer: A Case Study on Mandarin and English", "categories": ["cs.CL", "cs.AI", "cs.SD", "eess.AS", "68T10", "I.2.7"], "comment": "6 pages, 5 figures", "summary": "The speech tokenizer plays a crucial role in recent speech tasks, generally\nserving as a bridge between speech signals and language models. While\nlow-frame-rate codecs are widely employed as speech tokenizers, the impact of\nframe rates on speech tokens remains underexplored. In this study, we\ninvestigate how varying frame rates affect speech tokenization by examining\nMandarin and English, two typologically distinct languages. We encode speech at\ndifferent frame rates and evaluate the resulting semantic tokens in the speech\nrecognition task. Our findings reveal that frame rate variations influence\nspeech tokenization differently for each language, highlighting the interplay\nbetween frame rates, phonetic density, and language-specific acoustic features.\nThe results provide insights into optimizing frame rate selection for speech\ntokenizers, with implications for automatic speech recognition, text-to-speech,\nand other speech-related applications."}
{"id": "2505.23824", "pdf": "https://arxiv.org/pdf/2505.23824", "abs": "https://arxiv.org/abs/2505.23824", "authors": ["Tianmai M. Zhang", "Neil F. Abernethy"], "title": "Reviewing Scientific Papers for Critical Problems With Reasoning LLMs: Baseline Approaches and Automatic Evaluation", "categories": ["cs.CL"], "comment": "Work in progress. Conclusions may be updated", "summary": "Recent advancements in large language models have sparked interest in\nutilizing them to assist the peer review process of scientific publication.\nInstead of having AI models generate reviews in the same way as human\nreviewers, we propose adopting them as manuscript quality checkers. We\nintroduce several baseline approaches and an extendable automatic evaluation\nframework using top LLMs as judges to tackle the difficulty of recruiting\ndomain experts for manual evaluation. Utilizing papers withdrawn from arXiv, we\nvalidated our proposed methods with several leading reasoning LLMs from\ndifferent providers and assessed their performance and API costs for\nidentifying critical errors and unsoundness problems. The OpenAI o3 model\nperformed the best, while o4-mini was the most cost-effective one in our\nevaluation. This paper provides insights into document-based scientific\nunderstanding/reasoning and lays the foundation for future applications."}
{"id": "2505.24162", "pdf": "https://arxiv.org/pdf/2505.24162", "abs": "https://arxiv.org/abs/2505.24162", "authors": ["Isaac Aguirre", "Ivan Sipiran"], "title": "Training-free zero-shot 3D symmetry detection with visual features back-projected to geometry", "categories": ["cs.CV"], "comment": null, "summary": "We present a simple yet effective training-free approach for zero-shot 3D\nsymmetry detection that leverages visual features from foundation vision models\nsuch as DINOv2. Our method extracts features from rendered views of 3D objects\nand backprojects them onto the original geometry. We demonstrate the symmetric\ninvariance of these features and use them to identify reflection-symmetry\nplanes through a proposed algorithm. Experiments on a subset of ShapeNet\ndemonstrate that our approach outperforms both traditional geometric methods\nand learning-based approaches without requiring any training data. Our work\ndemonstrates how foundation vision models can help in solving complex 3D\ngeometric problems such as symmetry detection."}
{"id": "2505.24478", "pdf": "https://arxiv.org/pdf/2505.24478", "abs": "https://arxiv.org/abs/2505.24478", "authors": ["Vasilije Markovic", "Lazar Obradovic", "Laszlo Hajdu", "Jovan Pavlovic"], "title": "Optimizing the Interface Between Knowledge Graphs and LLMs for Complex Reasoning", "categories": ["cs.AI", "cs.CL"], "comment": "This is a preliminary version. A revised and expanded version is in\n  preparation", "summary": "Integrating Large Language Models (LLMs) with Knowledge Graphs (KGs) results\nin complex systems with numerous hyperparameters that directly affect\nperformance. While such systems are increasingly common in retrieval-augmented\ngeneration, the role of systematic hyperparameter optimization remains\nunderexplored. In this paper, we study this problem in the context of Cognee, a\nmodular framework for end-to-end KG construction and retrieval. Using three\nmulti-hop QA benchmarks (HotPotQA, TwoWikiMultiHop, and MuSiQue) we optimize\nparameters related to chunking, graph construction, retrieval, and prompting.\nEach configuration is scored using established metrics (exact match, F1, and\nDeepEval's LLM-based correctness metric). Our results demonstrate that\nmeaningful gains can be achieved through targeted tuning. While the gains are\nconsistent, they are not uniform, with performance varying across datasets and\nmetrics. This variability highlights both the value of tuning and the\nlimitations of standard evaluation measures. While demonstrating the immediate\npotential of hyperparameter tuning, we argue that future progress will depend\nnot only on architectural advances but also on clearer frameworks for\noptimization and evaluation in complex, modular systems."}
{"id": "2505.23960", "pdf": "https://arxiv.org/pdf/2505.23960", "abs": "https://arxiv.org/abs/2505.23960", "authors": ["Henry Conklin"], "title": "Information Structure in Mappings: An Approach to Learning, Representation, and Generalisation", "categories": ["cs.LG", "cs.AI", "cs.CL"], "comment": "PhD Thesis, 204 pages; entropy estimation discussed from p.94", "summary": "Despite the remarkable success of large large-scale neural networks, we still\nlack unified notation for thinking about and describing their representational\nspaces. We lack methods to reliably describe how their representations are\nstructured, how that structure emerges over training, and what kinds of\nstructures are desirable. This thesis introduces quantitative methods for\nidentifying systematic structure in a mapping between spaces, and leverages\nthem to understand how deep-learning models learn to represent information,\nwhat representational structures drive generalisation, and how design decisions\ncondition the structures that emerge. To do this I identify structural\nprimitives present in a mapping, along with information theoretic\nquantifications of each. These allow us to analyse learning, structure, and\ngeneralisation across multi-agent reinforcement learning models,\nsequence-to-sequence models trained on a single task, and Large Language\nModels. I also introduce a novel, performant, approach to estimating the\nentropy of vector space, that allows this analysis to be applied to models\nranging in size from 1 million to 12 billion parameters.\n  The experiments here work to shed light on how large-scale distributed models\nof cognition learn, while allowing us to draw parallels between those systems\nand their human analogs. They show how the structures of language and the\nconstraints that give rise to them in many ways parallel the kinds of\nstructures that drive performance of contemporary neural networks."}
{"id": "2505.23688", "pdf": "https://arxiv.org/pdf/2505.23688", "abs": "https://arxiv.org/abs/2505.23688", "authors": ["James Tanner", "Morgan Sonderegger", "Jane Stuart-Smith", "Jeff Mielke", "Tyler Kendall"], "title": "Automatic classification of stop realisation with wav2vec2.0", "categories": ["cs.CL", "cs.SD", "eess.AS"], "comment": "Accepted for Interspeech 2025. 5 pages, 3 figures", "summary": "Modern phonetic research regularly makes use of automatic tools for the\nannotation of speech data, however few tools exist for the annotation of many\nvariable phonetic phenomena. At the same time, pre-trained self-supervised\nmodels, such as wav2vec2.0, have been shown to perform well at speech\nclassification tasks and latently encode fine-grained phonetic information. We\ndemonstrate that wav2vec2.0 models can be trained to automatically classify\nstop burst presence with high accuracy in both English and Japanese, robust\nacross both finely-curated and unprepared speech corpora. Patterns of\nvariability in stop realisation are replicated with the automatic annotations,\nand closely follow those of manual annotations. These results demonstrate the\npotential of pre-trained speech models as tools for the automatic annotation\nand processing of speech corpus data, enabling researchers to 'scale-up' the\nscope of phonetic research with relative ease."}
{"id": "2505.23827", "pdf": "https://arxiv.org/pdf/2505.23827", "abs": "https://arxiv.org/abs/2505.23827", "authors": ["Bangde Du", "Ziyi Ye", "Zhijing Wu", "Jankowska Monika", "Shuqi Zhu", "Qingyao Ai", "Yujia Zhou", "Yiqun Liu"], "title": "ValueSim: Generating Backstories to Model Individual Value Systems", "categories": ["cs.CL"], "comment": "8 pages main paper + 13 pages appendix, 3 figures, 2 tables", "summary": "As Large Language Models (LLMs) continue to exhibit increasingly human-like\ncapabilities, aligning them with human values has become critically important.\nContemporary advanced techniques, such as prompt learning and reinforcement\nlearning, are being deployed to better align LLMs with human values. However,\nwhile these approaches address broad ethical considerations and helpfulness,\nthey rarely focus on simulating individualized human value systems. To address\nthis gap, we present ValueSim, a framework that simulates individual values\nthrough the generation of personal backstories reflecting past experiences and\ndemographic information. ValueSim converts structured individual data into\nnarrative backstories and employs a multi-module architecture inspired by the\nCognitive-Affective Personality System to simulate individual values based on\nthese narratives. Testing ValueSim on a self-constructed benchmark derived from\nthe World Values Survey demonstrates an improvement in top-1 accuracy by over\n10% compared to retrieval-augmented generation methods. Further analysis\nreveals that performance enhances as additional user interaction history\nbecomes available, indicating the model's ability to refine its persona\nsimulation capabilities over time."}
{"id": "2505.24167", "pdf": "https://arxiv.org/pdf/2505.24167", "abs": "https://arxiv.org/abs/2505.24167", "authors": ["Junyu Chen", "Shuwen Wei", "Yihao Liu", "Aaron Carass", "Yong Du"], "title": "Pretraining Deformable Image Registration Networks with Random Images", "categories": ["cs.CV"], "comment": "Accepted by MIDL 2025. Code available at\n  https://github.com/junyuchen245/Pretraining_Image_Registration_DNNs", "summary": "Recent advances in deep learning-based medical image registration have shown\nthat training deep neural networks~(DNNs) does not necessarily require medical\nimages. Previous work showed that DNNs trained on randomly generated images\nwith carefully designed noise and contrast properties can still generalize well\nto unseen medical data. Building on this insight, we propose using registration\nbetween random images as a proxy task for pretraining a foundation model for\nimage registration. Empirical results show that our pretraining strategy\nimproves registration accuracy, reduces the amount of domain-specific data\nneeded to achieve competitive performance, and accelerates convergence during\ndownstream training, thereby enhancing computational efficiency."}
{"id": "2505.24479", "pdf": "https://arxiv.org/pdf/2505.24479", "abs": "https://arxiv.org/abs/2505.24479", "authors": ["Sania Nayab", "Marco Simoni", "Giulio Rossolini"], "title": "Leveraging Knowledge Graphs and LLMs for Structured Generation of Misinformation", "categories": ["cs.AI", "cs.CL", "cs.SI"], "comment": null, "summary": "The rapid spread of misinformation, further amplified by recent advances in\ngenerative AI, poses significant threats to society, impacting public opinion,\ndemocratic stability, and national security. Understanding and proactively\nassessing these threats requires exploring methodologies that enable structured\nand scalable misinformation generation. In this paper, we propose a novel\napproach that leverages knowledge graphs (KGs) as structured semantic resources\nto systematically generate fake triplets. By analyzing the structural\nproperties of KGs, such as the distance between entities and their predicates,\nwe identify plausibly false relationships. These triplets are then used to\nguide large language models (LLMs) in generating misinformation statements with\nvarying degrees of credibility. By utilizing structured semantic relationships,\nour deterministic approach produces misinformation inherently challenging for\nhumans to detect, drawing exclusively upon publicly available KGs (e.g.,\nWikiGraphs).\n  Additionally, we investigate the effectiveness of LLMs in distinguishing\nbetween genuine and artificially generated misinformation. Our analysis\nhighlights significant limitations in current LLM-based detection methods,\nunderscoring the necessity for enhanced detection strategies and a deeper\nexploration of inherent biases in generative models."}
{"id": "2505.23967", "pdf": "https://arxiv.org/pdf/2505.23967", "abs": "https://arxiv.org/abs/2505.23967", "authors": ["Anders Aamand", "Justin Y. Chen", "Siddharth Gollapudi", "Sandeep Silwal", "Hao Wu"], "title": "Improved Approximations for Hard Graph Problems using Predictions", "categories": ["cs.LG", "cs.DS"], "comment": null, "summary": "We design improved approximation algorithms for NP-hard graph problems by\nincorporating predictions (e.g., learned from past data). Our prediction model\nbuilds upon and extends the $\\varepsilon$-prediction framework by Cohen-Addad,\nd'Orsi, Gupta, Lee, and Panigrahi (NeurIPS 2024). We consider an edge-based\nversion of this model, where each edge provides two bits of information,\ncorresponding to predictions about whether each of its endpoints belong to an\noptimal solution. Even with weak predictions where each bit is only\n$\\varepsilon$-correlated with the true solution, this information allows us to\nbreak approximation barriers in the standard setting. We develop algorithms\nwith improved approximation ratios for MaxCut, Vertex Cover, Set Cover, and\nMaximum Independent Set problems (among others). Across these problems, our\nalgorithms share a unifying theme, where we separately satisfy constraints\nrelated to high degree vertices (using predictions) and low-degree vertices\n(without using predictions) and carefully combine the answers."}
{"id": "2505.23829", "pdf": "https://arxiv.org/pdf/2505.23829", "abs": "https://arxiv.org/abs/2505.23829", "authors": ["Xiaoqing Cheng", "Ruizhe Chen", "Hongying Zan", "Yuxiang Jia", "Min Peng"], "title": "BiasFilter: An Inference-Time Debiasing Framework for Large Language Models", "categories": ["cs.CL"], "comment": null, "summary": "Mitigating social bias in large language models (LLMs) has become an\nincreasingly important research objective. However, existing debiasing methods\noften incur high human and computational costs, exhibit limited effectiveness,\nand struggle to scale to larger models and open-ended generation tasks. To\naddress these limitations, this paper proposes BiasFilter, a model-agnostic,\ninference-time debiasing framework that integrates seamlessly with both\nopen-source and API-based LLMs. Instead of relying on retraining with balanced\ndata or modifying model parameters, BiasFilter enforces fairness by filtering\ngeneration outputs in real time. Specifically, it periodically evaluates\nintermediate outputs every few tokens, maintains an active set of candidate\ncontinuations, and incrementally completes generation by discarding low-reward\nsegments based on a fairness reward signal. To support this process, we\nconstruct a fairness preference dataset and train an implicit reward model to\nassess token-level fairness in generated responses. Extensive experiments\ndemonstrate that BiasFilter effectively mitigates social bias across a range of\nLLMs while preserving overall generation quality."}
{"id": "2505.24173", "pdf": "https://arxiv.org/pdf/2505.24173", "abs": "https://arxiv.org/abs/2505.24173", "authors": ["Tianhong Zhou", "Yin Xu", "Yingtao Zhu", "Chuxi Xiao", "Haiyang Bian", "Lei Wei", "Xuegong Zhang"], "title": "DrVD-Bench: Do Vision-Language Models Reason Like Human Doctors in Medical Image Diagnosis?", "categories": ["cs.CV"], "comment": null, "summary": "Vision-language models (VLMs) exhibit strong zero-shot generalization on\nnatural images and show early promise in interpretable medical image analysis.\nHowever, existing benchmarks do not systematically evaluate whether these\nmodels truly reason like human clinicians or merely imitate superficial\npatterns. To address this gap, we propose DrVD-Bench, the first multimodal\nbenchmark for clinical visual reasoning. DrVD-Bench consists of three modules:\nVisual Evidence Comprehension, Reasoning Trajectory Assessment, and Report\nGeneration Evaluation, comprising a total of 7,789 image-question pairs. Our\nbenchmark covers 20 task types, 17 diagnostic categories, and five imaging\nmodalities-CT, MRI, ultrasound, radiography, and pathology. DrVD-Bench is\nexplicitly structured to reflect the clinical reasoning workflow from modality\nrecognition to lesion identification and diagnosis. We benchmark 19 VLMs,\nincluding general-purpose and medical-specific, open-source and proprietary\nmodels, and observe that performance drops sharply as reasoning complexity\nincreases. While some models begin to exhibit traces of human-like reasoning,\nthey often still rely on shortcut correlations rather than grounded visual\nunderstanding. DrVD-Bench offers a rigorous and structured evaluation framework\nto guide the development of clinically trustworthy VLMs."}
{"id": "2505.24597", "pdf": "https://arxiv.org/pdf/2505.24597", "abs": "https://arxiv.org/abs/2505.24597", "authors": ["Shuai Liu", "Ning Cao", "Yile Chen", "Yue Jiang", "Gao Cong"], "title": "Mixture-of-Experts for Personalized and Semantic-Aware Next Location Prediction", "categories": ["cs.AI"], "comment": null, "summary": "Next location prediction plays a critical role in understanding human\nmobility patterns. However, existing approaches face two core limitations: (1)\nthey fall short in capturing the complex, multi-functional semantics of\nreal-world locations; and (2) they lack the capacity to model heterogeneous\nbehavioral dynamics across diverse user groups. To tackle these challenges, we\nintroduce NextLocMoE, a novel framework built upon large language models (LLMs)\nand structured around a dual-level Mixture-of-Experts (MoE) design. Our\narchitecture comprises two specialized modules: a Location Semantics MoE that\noperates at the embedding level to encode rich functional semantics of\nlocations, and a Personalized MoE embedded within the Transformer backbone to\ndynamically adapt to individual user mobility patterns. In addition, we\nincorporate a history-aware routing mechanism that leverages long-term\ntrajectory data to enhance expert selection and ensure prediction stability.\nEmpirical evaluations across several real-world urban datasets show that\nNextLocMoE achieves superior performance in terms of predictive accuracy,\ncross-domain generalization, and interpretability"}
{"id": "2505.23971", "pdf": "https://arxiv.org/pdf/2505.23971", "abs": "https://arxiv.org/abs/2505.23971", "authors": ["William Merrill", "Shane Arora", "Dirk Groeneveld", "Hannaneh Hajishirzi"], "title": "Critical Batch Size Revisited: A Simple Empirical Approach to Large-Batch Language Model Training", "categories": ["cs.LG"], "comment": null, "summary": "The right batch size is important when training language models at scale: a\nlarge batch size is necessary for fast training, but a batch size that is too\nlarge will harm token efficiency. To navigate this tradeoff, McCandlish et al.\n(2018) suggest that a critical batch size (CBS), below which training will not\nsubstantially degrade loss, can be estimated based on the gradient noise scale\nduring training. While their method has been adopted in practice, e.g., when\ntraining GPT-3, strong assumptions are required to justify gradient noise as a\nproxy for the CBS, which makes it unclear whether their approach should be\ntrusted in practice, limiting its applicability. In this paper, we introduce a\nsimple, empirical approach to directly measure the CBS and show how the CBS\nevolves over training. Applying our approach to the OLMo models, we find that\nCBS is near 0 at initialization, increases rapidly at first, and then plateaus\nas training progresses. Furthermore, we find that this trend holds across\ndifferent model sizes (1B and 7B), suggesting CBS from small training runs can\ninform larger-scale training runs. Our findings about how the CBS changes over\ntraining motivate batch size warmup as a natural way to reliably train language\nmodels at large batch size: start the batch size small and increase it as the\nCBS grows. To validate this claim, we use batch size warmup to train OLMo 1B to\nslightly better loss than the original training run with 43% fewer gradient\nsteps. This shows how our framework can be applied to reliably train language\nmodels at larger batch sizes, increasing data parallelism without compromising\nperformance."}
{"id": "2505.23830", "pdf": "https://arxiv.org/pdf/2505.23830", "abs": "https://arxiv.org/abs/2505.23830", "authors": ["Linglin Jing", "Yuting Gao", "Zhigang Wang", "Wang Lan", "Yiwen Tang", "Wenhai Wang", "Kaipeng Zhang", "Qingpei Guo"], "title": "EvoMoE: Expert Evolution in Mixture of Experts for Multimodal Large Language Models", "categories": ["cs.CL"], "comment": null, "summary": "Recent advancements have shown that the Mixture of Experts (MoE) approach\nsignificantly enhances the capacity of large language models (LLMs) and\nimproves performance on downstream tasks. Building on these promising results,\nmulti-modal large language models (MLLMs) have increasingly adopted MoE\ntechniques. However, existing multi-modal MoE tuning methods typically face two\nkey challenges: expert uniformity and router rigidity. Expert uniformity occurs\nbecause MoE experts are often initialized by simply replicating the FFN\nparameters from LLMs, leading to homogenized expert functions and weakening the\nintended diversification of the MoE architecture. Meanwhile, router rigidity\nstems from the prevalent use of static linear routers for expert selection,\nwhich fail to distinguish between visual and textual tokens, resulting in\nsimilar expert distributions for image and text. To address these limitations,\nwe propose EvoMoE, an innovative MoE tuning framework. EvoMoE introduces a\nmeticulously designed expert initialization strategy that progressively evolves\nmultiple robust experts from a single trainable expert, a process termed expert\nevolution that specifically targets severe expert homogenization. Furthermore,\nwe introduce the Dynamic Token-aware Router (DTR), a novel routing mechanism\nthat allocates input tokens to appropriate experts based on their modality and\nintrinsic token values. This dynamic routing is facilitated by hypernetworks,\nwhich dynamically generate routing weights tailored for each individual token.\nExtensive experiments demonstrate that EvoMoE significantly outperforms other\nsparse MLLMs across a variety of multi-modal benchmarks, including MME,\nMMBench, TextVQA, and POPE. Our results highlight the effectiveness of EvoMoE\nin enhancing the performance of MLLMs by addressing the critical issues of\nexpert uniformity and router rigidity."}
{"id": "2505.24182", "pdf": "https://arxiv.org/pdf/2505.24182", "abs": "https://arxiv.org/abs/2505.24182", "authors": ["Zhuobai Dong", "Junchao Yi", "Ziyuan Zheng", "Haochen Han", "Xiangxi Zheng", "Alex Jinpeng Wang", "Fangming Liu", "Linjie Li"], "title": "Seeing is Not Reasoning: MVPBench for Graph-based Evaluation of Multi-path Visual Physical CoT", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Understanding the physical world - governed by laws of motion, spatial\nrelations, and causality - poses a fundamental challenge for multimodal large\nlanguage models (MLLMs). While recent advances such as OpenAI o3 and GPT-4o\ndemonstrate impressive perceptual and reasoning capabilities, our investigation\nreveals these models struggle profoundly with visual physical reasoning,\nfailing to grasp basic physical laws, spatial interactions, and causal effects\nin complex scenes. More importantly, they often fail to follow coherent\nreasoning chains grounded in visual evidence, especially when multiple steps\nare needed to arrive at the correct answer. To rigorously evaluate this\ncapability, we introduce MVPBench, a curated benchmark designed to rigorously\nevaluate visual physical reasoning through the lens of visual chain-of-thought\n(CoT). Each example features interleaved multi-image inputs and demands not\nonly the correct final answer but also a coherent, step-by-step reasoning path\ngrounded in evolving visual cues. This setup mirrors how humans reason through\nreal-world physical processes over time. To ensure fine-grained evaluation, we\nintroduce a graph-based CoT consistency metric that verifies whether the\nreasoning path of model adheres to valid physical logic. Additionally, we\nminimize shortcut exploitation from text priors, encouraging models to rely on\nvisual understanding. Experimental results reveal a concerning trend: even\ncutting-edge MLLMs exhibit poor visual reasoning accuracy and weak image-text\nalignment in physical domains. Surprisingly, RL-based post-training alignment -\ncommonly believed to improve visual reasoning performance - often harms spatial\nreasoning, suggesting a need to rethink current fine-tuning practices."}
{"id": "2505.24601", "pdf": "https://arxiv.org/pdf/2505.24601", "abs": "https://arxiv.org/abs/2505.24601", "authors": ["Zekun Wang", "Ethan L. Haarer", "Nicki Barari", "Christopher J. MacLellan"], "title": "Taxonomic Networks: A Representation for Neuro-Symbolic Pairing", "categories": ["cs.AI"], "comment": "10 pages, 3 figures, NeuS 2025", "summary": "We introduce the concept of a \\textbf{neuro-symbolic pair} -- neural and\nsymbolic approaches that are linked through a common knowledge representation.\nNext, we present \\textbf{taxonomic networks}, a type of discrimination network\nin which nodes represent hierarchically organized taxonomic concepts. Using\nthis representation, we construct a novel neuro-symbolic pair and evaluate its\nperformance. We show that our symbolic method learns taxonomic nets more\nefficiently with less data and compute, while the neural method finds\nhigher-accuracy taxonomic nets when provided with greater resources. As a\nneuro-symbolic pair, these approaches can be used interchangeably based on\nsituational needs, with seamless translation between them when necessary. This\nwork lays the foundation for future systems that more fundamentally integrate\nneural and symbolic computation."}
{"id": "2505.23973", "pdf": "https://arxiv.org/pdf/2505.23973", "abs": "https://arxiv.org/abs/2505.23973", "authors": ["Asaf Goren", "Natalie Lang", "Nir Shlezinger", "Alejandro Cohen"], "title": "Adaptive Deadline and Batch Layered Synchronized Federated Learning", "categories": ["cs.LG"], "comment": null, "summary": "Federated learning (FL) enables collaborative model training across\ndistributed edge devices while preserving data privacy, and typically operates\nin a round-based synchronous manner. However, synchronous FL suffers from\nlatency bottlenecks due to device heterogeneity, where slower clients\n(stragglers) delay or degrade global updates. Prior solutions, such as fixed\ndeadlines, client selection, and layer-wise partial aggregation, alleviate the\neffect of stragglers, but treat round timing and local workload as static\nparameters, limiting their effectiveness under strict time constraints. We\npropose ADEL-FL, a novel framework that jointly optimizes per-round deadlines\nand user-specific batch sizes for layer-wise aggregation. Our approach\nformulates a constrained optimization problem minimizing the expected L2\ndistance to the global optimum under total training time and global rounds. We\nprovide a convergence analysis under exponential compute models and prove that\nADEL-FL yields unbiased updates with bounded variance. Extensive experiments\ndemonstrate that ADEL-FL outperforms alternative methods in both convergence\nrate and final accuracy under heterogeneous conditions."}
{"id": "2505.23831", "pdf": "https://arxiv.org/pdf/2505.23831", "abs": "https://arxiv.org/abs/2505.23831", "authors": ["Wenhao Ye", "Tiansheng Zheng", "Yue Qi", "Wenhua Zhao", "Xiyu Wang", "Xue Zhao", "Jiacheng He", "Yaya Zheng", "Dongbo Wang"], "title": "ICH-Qwen: A Large Language Model Towards Chinese Intangible Cultural Heritage", "categories": ["cs.CL"], "comment": "16 pages, 2 figures", "summary": "The intangible cultural heritage (ICH) of China, a cultural asset transmitted\nacross generations by various ethnic groups, serves as a significant testament\nto the evolution of human civilization and holds irreplaceable value for the\npreservation of historical lineage and the enhancement of cultural\nself-confidence. However, the rapid pace of modernization poses formidable\nchallenges to ICH, including threats damage, disappearance and discontinuity of\ninheritance. China has the highest number of items on the UNESCO Intangible\nCultural Heritage List, which is indicative of the nation's abundant cultural\nresources and emphasises the pressing need for ICH preservation. In recent\nyears, the rapid advancements in large language modelling have provided a novel\ntechnological approach for the preservation and dissemination of ICH. This\nstudy utilises a substantial corpus of open-source Chinese ICH data to develop\na large language model, ICH-Qwen, for the ICH domain. The model employs natural\nlanguage understanding and knowledge reasoning capabilities of large language\nmodels, augmented with synthetic data and fine-tuning techniques. The\nexperimental results demonstrate the efficacy of ICH-Qwen in executing tasks\nspecific to the ICH domain. It is anticipated that the model will provide\nintelligent solutions for the protection, inheritance and dissemination of\nintangible cultural heritage, as well as new theoretical and practical\nreferences for the sustainable development of intangible cultural heritage.\nFurthermore, it is expected that the study will open up new paths for digital\nhumanities research."}
{"id": "2505.24207", "pdf": "https://arxiv.org/pdf/2505.24207", "abs": "https://arxiv.org/abs/2505.24207", "authors": ["Gang Wu", "Junjun Jiang", "Kui Jiang", "Xianming Liu"], "title": "Boosting All-in-One Image Restoration via Self-Improved Privilege Learning", "categories": ["cs.CV"], "comment": null, "summary": "Unified image restoration models for diverse and mixed degradations often\nsuffer from unstable optimization dynamics and inter-task conflicts. This paper\nintroduces Self-Improved Privilege Learning (SIPL), a novel paradigm that\novercomes these limitations by innovatively extending the utility of privileged\ninformation (PI) beyond training into the inference stage. Unlike conventional\nPrivilege Learning, where ground-truth-derived guidance is typically discarded\nafter training, SIPL empowers the model to leverage its own preliminary outputs\nas pseudo-privileged signals for iterative self-refinement at test time.\nCentral to SIPL is Proxy Fusion, a lightweight module incorporating a learnable\nPrivileged Dictionary. During training, this dictionary distills essential\nhigh-frequency and structural priors from privileged feature representations.\nCritically, at inference, the same learned dictionary then interacts with\nfeatures derived from the model's initial restoration, facilitating a\nself-correction loop. SIPL can be seamlessly integrated into various backbone\narchitectures, offering substantial performance improvements with minimal\ncomputational overhead. Extensive experiments demonstrate that SIPL\nsignificantly advances the state-of-the-art on diverse all-in-one image\nrestoration benchmarks. For instance, when integrated with the PromptIR model,\nSIPL achieves remarkable PSNR improvements of +4.58 dB on composite degradation\ntasks and +1.28 dB on diverse five-task benchmarks, underscoring its\neffectiveness and broad applicability. Codes are available at our project page\nhttps://github.com/Aitical/SIPL."}
{"id": "2505.24622", "pdf": "https://arxiv.org/pdf/2505.24622", "abs": "https://arxiv.org/abs/2505.24622", "authors": ["Ben Griffin", "Joseph Ternasky", "Fuat Alican", "Yigit Ihlamur"], "title": "Random Rule Forest (RRF): Interpretable Ensembles of LLM-Generated Questions for Predicting Startup Success", "categories": ["cs.AI", "cs.LG", "I.2.7"], "comment": "9 pages, 4 figures", "summary": "Predicting startup success requires models that are both accurate and\ninterpretable. We present a lightweight ensemble framework that combines YES/NO\nquestions generated by large language models (LLMs), forming a transparent\ndecision-making system. Each question acts as a weak heuristic, and by\nfiltering, ranking, and aggregating them through a threshold-based voting\nmechanism, we construct a strong ensemble predictor. On a test set where 10% of\nstartups are classified as successful, our approach achieves a precision rate\nof 50%, representing a 5x improvement over random selection, while remaining\nfully transparent. When we incorporate expert-guided heuristics into the\ngeneration process, performance improves further to 54% precision. These\nresults highlight the value of combining LLM reasoning with human insight and\ndemonstrate that simple, interpretable ensembles can support high-stakes\ndecisions in domains such as venture capital (VC)."}
{"id": "2505.23987", "pdf": "https://arxiv.org/pdf/2505.23987", "abs": "https://arxiv.org/abs/2505.23987", "authors": ["Vishal Dey", "Xiao Hu", "Xia Ning"], "title": "Large Language Models for Controllable Multi-property Multi-objective Molecule Optimization", "categories": ["cs.LG", "cs.AI", "cs.CL", "q-bio.BM"], "comment": null, "summary": "In real-world drug design, molecule optimization requires selectively\nimproving multiple molecular properties up to pharmaceutically relevant levels,\nwhile maintaining others that already meet such criteria. However, existing\ncomputational approaches and instruction-tuned LLMs fail to capture such\nnuanced property-specific objectives, limiting their practical applicability.\nTo address this, we introduce C-MuMOInstruct, the first instruction-tuning\ndataset focused on multi-property optimization with explicit, property-specific\nobjectives. Leveraging C-MuMOInstruct, we develop GeLLMO-Cs, a series of\ninstruction-tuned LLMs that can perform targeted property-specific\noptimization. Our experiments across 5 in-distribution and 5\nout-of-distribution tasks show that GeLLMO-Cs consistently outperform strong\nbaselines, achieving up to 126% higher success rate. Notably, GeLLMO-Cs exhibit\nimpressive 0-shot generalization to novel optimization tasks and unseen\ninstructions. This offers a step toward a foundational LLM to support\nrealistic, diverse optimizations with property-specific objectives.\nC-MuMOInstruct and code are accessible through\nhttps://github.com/ninglab/GeLLMO-C."}
{"id": "2505.23832", "pdf": "https://arxiv.org/pdf/2505.23832", "abs": "https://arxiv.org/abs/2505.23832", "authors": ["Chaeeun Kim", "Jinu Lee", "Wonseok Hwang"], "title": "LegalSearchLM: Rethinking Legal Case Retrieval as Legal Elements Generation", "categories": ["cs.CL", "cs.IR"], "comment": "Under review", "summary": "Legal Case Retrieval (LCR), which retrieves relevant cases from a query case,\nis a fundamental task for legal professionals in research and decision-making.\nHowever, existing studies on LCR face two major limitations. First, they are\nevaluated on relatively small-scale retrieval corpora (e.g., 100-55K cases) and\nuse a narrow range of criminal query types, which cannot sufficiently reflect\nthe complexity of real-world legal retrieval scenarios. Second, their reliance\non embedding-based or lexical matching methods often results in limited\nrepresentations and legally irrelevant matches. To address these issues, we\npresent: (1) LEGAR BENCH, the first large-scale Korean LCR benchmark, covering\n411 diverse crime types in queries over 1.2M legal cases; and (2)\nLegalSearchLM, a retrieval model that performs legal element reasoning over the\nquery case and directly generates content grounded in the target cases through\nconstrained decoding. Experimental results show that LegalSearchLM outperforms\nbaselines by 6-20% on LEGAR BENCH, achieving state-of-the-art performance. It\nalso demonstrates strong generalization to out-of-domain cases, outperforming\nnaive generative models trained on in-domain data by 15%."}
{"id": "2505.24210", "pdf": "https://arxiv.org/pdf/2505.24210", "abs": "https://arxiv.org/abs/2505.24210", "authors": ["Zheng Tan", "Weizhen Wang", "Andrea L. Bertozzi", "Ernest K. Ryu"], "title": "STORK: Improving the Fidelity of Mid-NFE Sampling for Diffusion and Flow Matching Models", "categories": ["cs.CV", "cs.NA", "math.NA"], "comment": null, "summary": "Diffusion models (DMs) have demonstrated remarkable performance in\nhigh-fidelity image and video generation. Because high-quality generations with\nDMs typically require a large number of function evaluations (NFEs), resulting\nin slow sampling, there has been extensive research successfully reducing the\nNFE to a small range (<10) while maintaining acceptable image quality. However,\nmany practical applications, such as those involving Stable Diffusion 3.5,\nFLUX, and SANA, commonly operate in the mid-NFE regime (20-50 NFE) to achieve\nsuperior results, and, despite the practical relevance, research on the\neffective sampling within this mid-NFE regime remains underexplored. In this\nwork, we propose a novel, training-free, and structure-independent DM ODE\nsolver called the Stabilized Taylor Orthogonal Runge--Kutta (STORK) method,\nbased on a class of stiff ODE solvers with a Taylor expansion adaptation.\nUnlike prior work such as DPM-Solver, which is dependent on the semi-linear\nstructure of the DM ODE, STORK is applicable to any DM sampling, including\nnoise-based and flow matching-based models. Within the 20-50 NFE range, STORK\nachieves improved generation quality, as measured by FID scores, across\nunconditional pixel-level generation and conditional latent-space generation\ntasks using models like Stable Diffusion 3.5 and SANA. Code is available at\nhttps://github.com/ZT220501/STORK."}
{"id": "2505.24655", "pdf": "https://arxiv.org/pdf/2505.24655", "abs": "https://arxiv.org/abs/2505.24655", "authors": ["Frederike LÃ¼beck", "Jonas Wildberger", "Frederik TrÃ¤uble", "Maximilian Mordig", "Sergios Gatidis", "Andreas Krause", "Bernhard SchÃ¶lkopf"], "title": "Adaptable Cardiovascular Disease Risk Prediction from Heterogeneous Data using Large Language Models", "categories": ["cs.AI", "cs.LG"], "comment": null, "summary": "Cardiovascular disease (CVD) risk prediction models are essential for\nidentifying high-risk individuals and guiding preventive actions. However,\nexisting models struggle with the challenges of real-world clinical practice as\nthey oversimplify patient profiles, rely on rigid input schemas, and are\nsensitive to distribution shifts. We developed AdaCVD, an adaptable CVD risk\nprediction framework built on large language models extensively fine-tuned on\nover half a million participants from the UK Biobank. In benchmark comparisons,\nAdaCVD surpasses established risk scores and standard machine learning\napproaches, achieving state-of-the-art performance. Crucially, for the first\ntime, it addresses key clinical challenges across three dimensions: it flexibly\nincorporates comprehensive yet variable patient information; it seamlessly\nintegrates both structured data and unstructured text; and it rapidly adapts to\nnew patient populations using minimal additional data. In stratified analyses,\nit demonstrates robust performance across demographic, socioeconomic, and\nclinical subgroups, including underrepresented cohorts. AdaCVD offers a\npromising path toward more flexible, AI-driven clinical decision support tools\nsuited to the realities of heterogeneous and dynamic healthcare environments."}
{"id": "2505.24003", "pdf": "https://arxiv.org/pdf/2505.24003", "abs": "https://arxiv.org/abs/2505.24003", "authors": ["ChengAo Shen", "Wenchao Yu", "Ziming Zhao", "Dongjin Song", "Wei Cheng", "Haifeng Chen", "Jingchao Ni"], "title": "Multi-Modal View Enhanced Large Vision Models for Long-Term Time Series Forecasting", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Time series, typically represented as numerical sequences, can also be\ntransformed into images and texts, offering multi-modal views (MMVs) of the\nsame underlying signal. These MMVs can reveal complementary patterns and enable\nthe use of powerful pre-trained large models, such as large vision models\n(LVMs), for long-term time series forecasting (LTSF). However, as we identified\nin this work, applying LVMs to LTSF poses an inductive bias towards\n\"forecasting periods\". To harness this bias, we propose DMMV, a novel\ndecomposition-based multi-modal view framework that leverages trend-seasonal\ndecomposition and a novel backcast residual based adaptive decomposition to\nintegrate MMVs for LTSF. Comparative evaluations against 14 state-of-the-art\n(SOTA) models across diverse datasets show that DMMV outperforms single-view\nand existing multi-modal baselines, achieving the best mean squared error (MSE)\non 6 out of 8 benchmark datasets."}
{"id": "2505.23833", "pdf": "https://arxiv.org/pdf/2505.23833", "abs": "https://arxiv.org/abs/2505.23833", "authors": ["Qingchuan Ma", "Yuhang Wu", "Xiawu Zheng", "Rongrong Ji"], "title": "Benchmarking Abstract and Reasoning Abilities Through A Theoretical Perspective", "categories": ["cs.CL"], "comment": null, "summary": "In this paper, we aim to establish a simple, effective, and theoretically\ngrounded benchmark for rigorously probing abstract reasoning in Large Language\nModels (LLMs). To achieve this, we first develop a mathematic framework that\ndefines abstract reasoning as the ability to: (i) extract essential patterns\nindependent of surface representations, and (ii) apply consistent rules to\nthese abstract patterns. Based on this framework, we introduce two novel\ncomplementary metrics: \\(\\scoreGamma\\) measures basic reasoning accuracy, while\n\\(\\scoreDelta\\) quantifies a model's reliance on specific symbols rather than\nunderlying patterns - a key indicator of true abstraction versus mere\nmemorization. To implement this measurement, we design a benchmark: systematic\nsymbol remapping in rule-based tasks, which forces models to demonstrate\ngenuine pattern recognition beyond superficial token matching. Extensive LLM\nevaluations using this benchmark (commercial API models, 7B-70B, multi-agent)\nreveal:1) critical limitations in non-decimal arithmetic and symbolic\nreasoning; 2) persistent abstraction gaps despite chain-of-thought prompting;\nand 3) \\(\\scoreDelta\\)'s effectiveness in robustly measuring memory dependence\nby quantifying performance degradation under symbol remapping, particularly\nhighlighting operand-specific memorization. These findings underscore that\ncurrent LLMs, despite domain-specific strengths, still lack robust abstract\nreasoning, highlighting key areas for future improvement."}
{"id": "2505.24214", "pdf": "https://arxiv.org/pdf/2505.24214", "abs": "https://arxiv.org/abs/2505.24214", "authors": ["Redwan Sony", "Parisa Farmanifard", "Hamzeh Alzwairy", "Nitish Shukla", "Arun Ross"], "title": "Benchmarking Foundation Models for Zero-Shot Biometric Tasks", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "The advent of foundation models, particularly Vision-Language Models (VLMs)\nand Multi-modal Large Language Models (MLLMs), has redefined the frontiers of\nartificial intelligence, enabling remarkable generalization across diverse\ntasks with minimal or no supervision. Yet, their potential in biometric\nrecognition and analysis remains relatively underexplored. In this work, we\nintroduce a comprehensive benchmark that evaluates the zero-shot and few-shot\nperformance of state-of-the-art publicly available VLMs and MLLMs across six\nbiometric tasks spanning the face and iris modalities: face verification, soft\nbiometric attribute prediction (gender and race), iris recognition,\npresentation attack detection (PAD), and face manipulation detection (morphs\nand deepfakes). A total of 41 VLMs were used in this evaluation. Experiments\nshow that embeddings from these foundation models can be used for diverse\nbiometric tasks with varying degrees of success. For example, in the case of\nface verification, a True Match Rate (TMR) of 96.77 percent was obtained at a\nFalse Match Rate (FMR) of 1 percent on the Labeled Face in the Wild (LFW)\ndataset, without any fine-tuning. In the case of iris recognition, the TMR at 1\npercent FMR on the IITD-R-Full dataset was 97.55 percent without any\nfine-tuning. Further, we show that applying a simple classifier head to these\nembeddings can help perform DeepFake detection for faces, Presentation Attack\nDetection (PAD) for irides, and extract soft biometric attributes like gender\nand ethnicity from faces with reasonably high accuracy. This work reiterates\nthe potential of pretrained models in achieving the long-term vision of\nArtificial General Intelligence."}
{"id": "2505.24784", "pdf": "https://arxiv.org/pdf/2505.24784", "abs": "https://arxiv.org/abs/2505.24784", "authors": ["Conor Heins", "Toon Van de Maele", "Alexander Tschantz", "Hampus Linander", "Dimitrije Markovic", "Tommaso Salvatori", "Corrado Pezzato", "Ozan Catal", "Ran Wei", "Magnus Koudahl", "Marco Perin", "Karl Friston", "Tim Verbelen", "Christopher Buckley"], "title": "AXIOM: Learning to Play Games in Minutes with Expanding Object-Centric Models", "categories": ["cs.AI", "cs.LG", "stat.ML"], "comment": "10 pages main text, 4 figures, 2 tables; 25 pages supplementary\n  material, 8 figures", "summary": "Current deep reinforcement learning (DRL) approaches achieve state-of-the-art\nperformance in various domains, but struggle with data efficiency compared to\nhuman learning, which leverages core priors about objects and their\ninteractions. Active inference offers a principled framework for integrating\nsensory information with prior knowledge to learn a world model and quantify\nthe uncertainty of its own beliefs and predictions. However, active inference\nmodels are usually crafted for a single task with bespoke knowledge, so they\nlack the domain flexibility typical of DRL approaches. To bridge this gap, we\npropose a novel architecture that integrates a minimal yet expressive set of\ncore priors about object-centric dynamics and interactions to accelerate\nlearning in low-data regimes. The resulting approach, which we call AXIOM,\ncombines the usual data efficiency and interpretability of Bayesian approaches\nwith the across-task generalization usually associated with DRL. AXIOM\nrepresents scenes as compositions of objects, whose dynamics are modeled as\npiecewise linear trajectories that capture sparse object-object interactions.\nThe structure of the generative model is expanded online by growing and\nlearning mixture models from single events and periodically refined through\nBayesian model reduction to induce generalization. AXIOM masters various games\nwithin only 10,000 interaction steps, with both a small number of parameters\ncompared to DRL, and without the computational expense of gradient-based\noptimization."}
{"id": "2505.24005", "pdf": "https://arxiv.org/pdf/2505.24005", "abs": "https://arxiv.org/abs/2505.24005", "authors": ["Priya Kasimbeg", "Vincent Roulet", "Naman Agarwal", "Sourabh Medapati", "Fabian Pedregosa", "Atish Agarwala", "George E. Dahl"], "title": "How far away are truly hyperparameter-free learning algorithms?", "categories": ["cs.LG"], "comment": null, "summary": "Despite major advances in methodology, hyperparameter tuning remains a\ncrucial (and expensive) part of the development of machine learning systems.\nEven ignoring architectural choices, deep neural networks have a large number\nof optimization and regularization hyperparameters that need to be tuned\ncarefully per workload in order to obtain the best results. In a perfect world,\ntraining algorithms would not require workload-specific hyperparameter tuning,\nbut would instead have default settings that performed well across many\nworkloads. Recently, there has been a growing literature on optimization\nmethods which attempt to reduce the number of hyperparameters -- particularly\nthe learning rate and its accompanying schedule. Given these developments, how\nfar away is the dream of neural network training algorithms that completely\nobviate the need for painful tuning?\n  In this paper, we evaluate the potential of learning-rate-free methods as\ncomponents of hyperparameter-free methods. We freeze their (non-learning rate)\nhyperparameters to default values, and score their performance using the\nrecently-proposed AlgoPerf: Training Algorithms benchmark. We found that\nliterature-supplied default settings performed poorly on the benchmark, so we\nperformed a search for hyperparameter configurations that performed well across\nall workloads simultaneously. The best AlgoPerf-calibrated learning-rate-free\nmethods had much improved performance but still lagged slightly behind a\nsimilarly calibrated NadamW baseline in overall benchmark score. Our results\nsuggest that there is still much room for improvement for learning-rate-free\nmethods, and that testing against a strong, workload-agnostic baseline is\nimportant to improve hyperparameter reduction techniques."}
{"id": "2505.23835", "pdf": "https://arxiv.org/pdf/2505.23835", "abs": "https://arxiv.org/abs/2505.23835", "authors": ["Ye Cheng", "Minghui Xu", "Yue Zhang", "Kun Li", "Hao Wu", "Yechao Zhang", "Shaoyong Guo", "Wangjie Qiu", "Dongxiao Yu", "Xiuzhen Cheng"], "title": "Say What You Mean: Natural Language Access Control with Large Language Models for Internet of Things", "categories": ["cs.CL"], "comment": null, "summary": "Access control in the Internet of Things (IoT) is becoming increasingly\ncomplex, as policies must account for dynamic and contextual factors such as\ntime, location, user behavior, and environmental conditions. However, existing\nplatforms either offer only coarse-grained controls or rely on rigid rule\nmatching, making them ill-suited for semantically rich or ambiguous access\nscenarios. Moreover, the policy authoring process remains fragmented: domain\nexperts describe requirements in natural language, but developers must manually\ntranslate them into code, introducing semantic gaps and potential\nmisconfiguration. In this work, we present LACE, the Language-based Access\nControl Engine, a hybrid framework that leverages large language models (LLMs)\nto bridge the gap between human intent and machine-enforceable logic. LACE\ncombines prompt-guided policy generation, retrieval-augmented reasoning, and\nformal validation to support expressive, interpretable, and verifiable access\ncontrol. It enables users to specify policies in natural language,\nautomatically translates them into structured rules, validates semantic\ncorrectness, and makes access decisions using a hybrid LLM-rule-based engine.\nWe evaluate LACE in smart home environments through extensive experiments. LACE\nachieves 100% correctness in verified policy generation and up to 88% decision\naccuracy with 0.79 F1-score using DeepSeek-V3, outperforming baselines such as\nGPT-3.5 and Gemini. The system also demonstrates strong scalability under\nincreasing policy volume and request concurrency. Our results highlight LACE's\npotential to enable secure, flexible, and user-friendly access control across\nreal-world IoT platforms."}
{"id": "2505.24216", "pdf": "https://arxiv.org/pdf/2505.24216", "abs": "https://arxiv.org/abs/2505.24216", "authors": ["Prasanna Reddy Pulakurthi", "Majid Rabbani", "Jamison Heard", "Sohail Dianat", "Celso M. de Melo", "Raghuveer Rao"], "title": "Shuffle PatchMix Augmentation with Confidence-Margin Weighted Pseudo-Labels for Enhanced Source-Free Domain Adaptation", "categories": ["cs.CV"], "comment": "6 pages, 3 figures, 5 tables, Accepted to IEEE ICIP 2025", "summary": "This work investigates Source-Free Domain Adaptation (SFDA), where a model\nadapts to a target domain without access to source data. A new augmentation\ntechnique, Shuffle PatchMix (SPM), and a novel reweighting strategy are\nintroduced to enhance performance. SPM shuffles and blends image patches to\ngenerate diverse and challenging augmentations, while the reweighting strategy\nprioritizes reliable pseudo-labels to mitigate label noise. These techniques\nare particularly effective on smaller datasets like PACS, where overfitting and\npseudo-label noise pose greater risks. State-of-the-art results are achieved on\nthree major benchmarks: PACS, VisDA-C, and DomainNet-126. Notably, on PACS,\nimprovements of 7.3% (79.4% to 86.7%) and 7.2% are observed in single-target\nand multi-target settings, respectively, while gains of 2.8% and 0.7% are\nattained on DomainNet-126 and VisDA-C. This combination of advanced\naugmentation and robust pseudo-label reweighting establishes a new benchmark\nfor SFDA. The code is available at: https://github.com/PrasannaPulakurthi/SPM"}
{"id": "2505.24785", "pdf": "https://arxiv.org/pdf/2505.24785", "abs": "https://arxiv.org/abs/2505.24785", "authors": ["Patrick Tser Jern Kon", "Jiachen Liu", "Xinyi Zhu", "Qiuyi Ding", "Jingjia Peng", "Jiarong Xing", "Yibo Huang", "Yiming Qiu", "Jayanth Srinivasa", "Myungjin Lee", "Mosharaf Chowdhury", "Matei Zaharia", "Ang Chen"], "title": "EXP-Bench: Can AI Conduct AI Research Experiments?", "categories": ["cs.AI"], "comment": "45 pages, 13 figures", "summary": "Automating AI research holds immense potential for accelerating scientific\nprogress, yet current AI agents struggle with the complexities of rigorous,\nend-to-end experimentation. We introduce EXP-Bench, a novel benchmark designed\nto systematically evaluate AI agents on complete research experiments sourced\nfrom influential AI publications. Given a research question and incomplete\nstarter code, EXP-Bench challenges AI agents to formulate hypotheses, design\nand implement experimental procedures, execute them, and analyze results. To\nenable the creation of such intricate and authentic tasks with high-fidelity,\nwe design a semi-autonomous pipeline to extract and structure crucial\nexperimental details from these research papers and their associated\nopen-source code. With the pipeline, EXP-Bench curated 461 AI research tasks\nfrom 51 top-tier AI research papers. Evaluations of leading LLM-based agents,\nsuch as OpenHands and IterativeAgent on EXP-Bench demonstrate partial\ncapabilities: while scores on individual experimental aspects such as design or\nimplementation correctness occasionally reach 20-35%, the success rate for\ncomplete, executable experiments was a mere 0.5%. By identifying these\nbottlenecks and providing realistic step-by-step experiment procedures,\nEXP-Bench serves as a vital tool for future AI agents to improve their ability\nto conduct AI research experiments. EXP-Bench is open-sourced at\nhttps://github.com/Just-Curieous/Curie/tree/main/benchmark/exp_bench."}
{"id": "2505.24022", "pdf": "https://arxiv.org/pdf/2505.24022", "abs": "https://arxiv.org/abs/2505.24022", "authors": ["Bhavya Vasudeva", "Jung Whan Lee", "Vatsal Sharan", "Mahdi Soltanolkotabi"], "title": "The Rich and the Simple: On the Implicit Bias of Adam and SGD", "categories": ["cs.LG", "math.OC", "stat.ML"], "comment": "27 pages, 11 figures, 16 tables", "summary": "Adam is the de facto optimization algorithm for several deep learning\napplications, but an understanding of its implicit bias and how it differs from\nother algorithms, particularly standard first-order methods such as\n(stochastic) gradient descent (GD), remains limited. In practice, neural\nnetworks trained with SGD are known to exhibit simplicity bias -- a tendency to\nfind simple solutions. In contrast, we show that Adam is more resistant to such\nsimplicity bias. To demystify this phenomenon, in this paper, we investigate\nthe differences in the implicit biases of Adam and GD when training two-layer\nReLU neural networks on a binary classification task involving synthetic data\nwith Gaussian clusters. We find that GD exhibits a simplicity bias, resulting\nin a linear decision boundary with a suboptimal margin, whereas Adam leads to\nmuch richer and more diverse features, producing a nonlinear boundary that is\ncloser to the Bayes' optimal predictor. This richer decision boundary also\nallows Adam to achieve higher test accuracy both in-distribution and under\ncertain distribution shifts. We theoretically prove these results by analyzing\nthe population gradients. To corroborate our theoretical findings, we present\nempirical results showing that this property of Adam leads to superior\ngeneralization across datasets with spurious correlations where neural networks\ntrained with SGD are known to show simplicity bias and don't generalize well\nunder certain distributional shifts."}
{"id": "2505.23836", "pdf": "https://arxiv.org/pdf/2505.23836", "abs": "https://arxiv.org/abs/2505.23836", "authors": ["Joe Needham", "Giles Edkins", "Govind Pimpale", "Henning Bartsch", "Marius Hobbhahn"], "title": "Large Language Models Often Know When They Are Being Evaluated", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "If AI models can detect when they are being evaluated, the effectiveness of\nevaluations might be compromised. For example, models could have systematically\ndifferent behavior during evaluations, leading to less reliable benchmarks for\ndeployment and governance decisions. We investigate whether frontier language\nmodels can accurately classify transcripts based on whether they originate from\nevaluations or real-world deployment, a capability we call evaluation\nawareness. To achieve this, we construct a diverse benchmark of 1,000 prompts\nand transcripts from 61 distinct datasets. These span public benchmarks (e.g.,\nMMLU, SWEBench), real-world deployment interactions, and agent trajectories\nfrom scaffolding frameworks (e.g., web-browsing agents). Frontier models\nclearly demonstrate above-random evaluation awareness (Gemini-2.5-Pro reaches\nan AUC of $0.83$), but do not yet surpass our simple human baseline (AUC of\n$0.92$). Furthermore, both AI models and humans are better at identifying\nevaluations in agentic settings compared to chat settings. Additionally, we\ntest whether models can identify the purpose of the evaluation. Under\nmultiple-choice and open-ended questioning, AI models far outperform random\nchance in identifying what an evaluation is testing for. Our results indicate\nthat frontier models already exhibit a substantial, though not yet superhuman,\nlevel of evaluation-awareness. We recommend tracking this capability in future\nmodels."}
{"id": "2505.24222", "pdf": "https://arxiv.org/pdf/2505.24222", "abs": "https://arxiv.org/abs/2505.24222", "authors": ["Fangyikang Wang", "Hubery Yin", "Lei Qian", "Yinan Li", "Shaobin Zhuang", "Huminhao Zhu", "Yilin Zhang", "Yanlong Tang", "Chao Zhang", "Hanbin Zhao", "Hui Qian", "Chen Li"], "title": "Unleashing High-Quality Image Generation in Diffusion Sampling Using Second-Order Levenberg-Marquardt-Langevin", "categories": ["cs.CV", "cs.LG"], "comment": null, "summary": "The diffusion models (DMs) have demonstrated the remarkable capability of\ngenerating images via learning the noised score function of data distribution.\nCurrent DM sampling techniques typically rely on first-order Langevin dynamics\nat each noise level, with efforts concentrated on refining inter-level\ndenoising strategies. While leveraging additional second-order Hessian geometry\nto enhance the sampling quality of Langevin is a common practice in Markov\nchain Monte Carlo (MCMC), the naive attempts to utilize Hessian geometry in\nhigh-dimensional DMs lead to quadratic-complexity computational costs,\nrendering them non-scalable. In this work, we introduce a novel\nLevenberg-Marquardt-Langevin (LML) method that approximates the diffusion\nHessian geometry in a training-free manner, drawing inspiration from the\ncelebrated Levenberg-Marquardt optimization algorithm. Our approach introduces\ntwo key innovations: (1) A low-rank approximation of the diffusion Hessian,\nleveraging the DMs' inherent structure and circumventing explicit\nquadratic-complexity computations; (2) A damping mechanism to stabilize the\napproximated Hessian. This LML approximated Hessian geometry enables the\ndiffusion sampling to execute more accurate steps and improve the image\ngeneration quality. We further conduct a theoretical analysis to substantiate\nthe approximation error bound of low-rank approximation and the convergence\nproperty of the damping mechanism. Extensive experiments across multiple\npretrained DMs validate that the LML method significantly improves image\ngeneration quality, with negligible computational overhead."}
{"id": "2505.24846", "pdf": "https://arxiv.org/pdf/2505.24846", "abs": "https://arxiv.org/abs/2505.24846", "authors": ["Jingyan Shen", "Jiarui Yao", "Rui Yang", "Yifan Sun", "Feng Luo", "Rui Pan", "Tong Zhang", "Han Zhao"], "title": "MiCRo: Mixture Modeling and Context-aware Routing for Personalized Preference Learning", "categories": ["cs.AI", "cs.CL"], "comment": null, "summary": "Reward modeling is a key step in building safe foundation models when\napplying reinforcement learning from human feedback (RLHF) to align Large\nLanguage Models (LLMs). However, reward modeling based on the Bradley-Terry\n(BT) model assumes a global reward function, failing to capture the inherently\ndiverse and heterogeneous human preferences. Hence, such oversimplification\nlimits LLMs from supporting personalization and pluralistic alignment.\nTheoretically, we show that when human preferences follow a mixture\ndistribution of diverse subgroups, a single BT model has an irreducible error.\nWhile existing solutions, such as multi-objective learning with fine-grained\nannotations, help address this issue, they are costly and constrained by\npredefined attributes, failing to fully capture the richness of human values.\nIn this work, we introduce MiCRo, a two-stage framework that enhances\npersonalized preference learning by leveraging large-scale binary preference\ndatasets without requiring explicit fine-grained annotations. In the first\nstage, MiCRo introduces context-aware mixture modeling approach to capture\ndiverse human preferences. In the second stage, MiCRo integrates an online\nrouting strategy that dynamically adapts mixture weights based on specific\ncontext to resolve ambiguity, allowing for efficient and scalable preference\nadaptation with minimal additional supervision. Experiments on multiple\npreference datasets demonstrate that MiCRo effectively captures diverse human\npreferences and significantly improves downstream personalization."}
{"id": "2505.24030", "pdf": "https://arxiv.org/pdf/2505.24030", "abs": "https://arxiv.org/abs/2505.24030", "authors": ["Ziming Zhao", "ChengAo Shen", "Hanghang Tong", "Dongjin Song", "Zhigang Deng", "Qingsong Wen", "Jingchao Ni"], "title": "From Images to Signals: Are Large Vision Models Useful for Time Series Analysis?", "categories": ["cs.LG", "cs.AI", "cs.CV"], "comment": null, "summary": "Transformer-based models have gained increasing attention in time series\nresearch, driving interest in Large Language Models (LLMs) and foundation\nmodels for time series analysis. As the field moves toward multi-modality,\nLarge Vision Models (LVMs) are emerging as a promising direction. In the past,\nthe effectiveness of Transformer and LLMs in time series has been debated. When\nit comes to LVMs, a similar question arises: are LVMs truely useful for time\nseries analysis? To address it, we design and conduct the first principled\nstudy involving 4 LVMs, 8 imaging methods, 18 datasets and 26 baselines across\nboth high-level (classification) and low-level (forecasting) tasks, with\nextensive ablation analysis. Our findings indicate LVMs are indeed useful for\ntime series classification but face challenges in forecasting. Although\neffective, the contemporary best LVM forecasters are limited to specific types\nof LVMs and imaging methods, exhibit a bias toward forecasting periods, and\nhave limited ability to utilize long look-back windows. We hope our findings\ncould serve as a cornerstone for future research on LVM- and multimodal-based\nsolutions to different time series tasks."}
{"id": "2505.23837", "pdf": "https://arxiv.org/pdf/2505.23837", "abs": "https://arxiv.org/abs/2505.23837", "authors": ["Lin Zhong", "Lingzhi Wang", "Xu Yang", "Qing Liao"], "title": "CoMaPOI: A Collaborative Multi-Agent Framework for Next POI Prediction Bridging the Gap Between Trajectory and Language", "categories": ["cs.CL", "cs.IR", "I.2.0"], "comment": "This paper has been accepted by SIGIR 2025", "summary": "Large Language Models (LLMs) offer new opportunities for the next\nPoint-Of-Interest (POI) prediction task, leveraging their capabilities in\nsemantic understanding of POI trajectories. However, previous LLM-based\nmethods, which are superficially adapted to next POI prediction, largely\noverlook critical challenges associated with applying LLMs to this task.\nSpecifically, LLMs encounter two critical challenges: (1) a lack of intrinsic\nunderstanding of numeric spatiotemporal data, which hinders accurate modeling\nof users' spatiotemporal distributions and preferences; and (2) an excessively\nlarge and unconstrained candidate POI space, which often results in random or\nirrelevant predictions. To address these issues, we propose a Collaborative\nMulti Agent Framework for Next POI Prediction, named CoMaPOI. Through the close\ninteraction of three specialized agents (Profiler, Forecaster, and Predictor),\nCoMaPOI collaboratively addresses the two critical challenges. The Profiler\nagent is responsible for converting numeric data into language descriptions,\nenhancing semantic understanding. The Forecaster agent focuses on dynamically\nconstraining and refining the candidate POI space. The Predictor agent\nintegrates this information to generate high-precision predictions. Extensive\nexperiments on three benchmark datasets (NYC, TKY, and CA) demonstrate that\nCoMaPOI achieves state of the art performance, improving all metrics by 5% to\n10% compared to SOTA baselines. This work pioneers the investigation of\nchallenges associated with applying LLMs to complex spatiotemporal tasks by\nleveraging tailored collaborative agents."}
{"id": "2505.24225", "pdf": "https://arxiv.org/pdf/2505.24225", "abs": "https://arxiv.org/abs/2505.24225", "authors": ["Haibo Jin", "Peiyan Zhang", "Man Luo", "Haohan Wang"], "title": "Reasoning Can Hurt the Inductive Abilities of Large Language Models", "categories": ["cs.CV", "cs.AI", "cs.CL"], "comment": "26 pages", "summary": "Large Language Models (LLMs) have shown remarkable progress across domains,\nyet their ability to perform inductive reasoning - inferring latent rules from\nsparse examples - remains limited. It is often assumed that chain-of-thought\n(CoT) prompting, as used in Large Reasoning Models (LRMs), enhances such\nreasoning. We investigate this assumption with creating four controlled,\ndiagnostic game-based tasks - chess, Texas Hold'em, dice games, and blackjack -\nwith hidden human-defined rules. We find that CoT reasoning can degrade\ninductive performance, with LRMs often underperforming their non-reasoning\ncounterparts.\n  To explain this, we present a theoretical framework that reveals how\nreasoning steps can amplify error through three failure modes: incorrect\nsub-task decomposition, incorrect sub-task solving, and incorrect final answer\nsummarization. Based on our theoretical and empirical analysis, we introduce\nstructured interventions that adapt CoT generation according to our identified\nfailure types. These interventions improve inductive accuracy without\nretraining. Our findings suggest that effective (CoT) reasoning depends not\nonly on taking more steps but also on ensuring those steps are well-structured."}
{"id": "2505.24878", "pdf": "https://arxiv.org/pdf/2505.24878", "abs": "https://arxiv.org/abs/2505.24878", "authors": ["Yaxin Luo", "Zhaoyi Li", "Jiacheng Liu", "Jiacheng Cui", "Xiaohan Zhao", "Zhiqiang Shen"], "title": "Open CaptchaWorld: A Comprehensive Web-based Platform for Testing and Benchmarking Multimodal LLM Agents", "categories": ["cs.AI", "cs.CL", "cs.CV", "cs.LG"], "comment": "Code at: https://github.com/MetaAgentX/OpenCaptchaWorld", "summary": "CAPTCHAs have been a critical bottleneck for deploying web agents in\nreal-world applications, often blocking them from completing end-to-end\nautomation tasks. While modern multimodal LLM agents have demonstrated\nimpressive performance in static perception tasks, their ability to handle\ninteractive, multi-step reasoning challenges like CAPTCHAs is largely untested.\nTo address this gap, we introduce Open CaptchaWorld, the first web-based\nbenchmark and platform specifically designed to evaluate the visual reasoning\nand interaction capabilities of MLLM-powered agents through diverse and dynamic\nCAPTCHA puzzles. Our benchmark spans 20 modern CAPTCHA types, totaling 225\nCAPTCHAs, annotated with a new metric we propose: CAPTCHA Reasoning Depth,\nwhich quantifies the number of cognitive and motor steps required to solve each\npuzzle. Experimental results show that humans consistently achieve near-perfect\nscores, state-of-the-art MLLM agents struggle significantly, with success rates\nat most 40.0% by Browser-Use Openai-o3, far below human-level performance,\n93.3%. This highlights Open CaptchaWorld as a vital benchmark for diagnosing\nthe limits of current multimodal agents and guiding the development of more\nrobust multimodal reasoning systems. Code and Data are available at this https\nURL."}
{"id": "2505.24034", "pdf": "https://arxiv.org/pdf/2505.24034", "abs": "https://arxiv.org/abs/2505.24034", "authors": ["Bo Wu", "Sid Wang", "Yunhao Tang", "Jia Ding", "Eryk Helenowski", "Liang Tan", "Tengyu Xu", "Tushar Gowda", "Zhengxing Chen", "Chen Zhu", "Xiaocheng Tang", "Yundi Qian", "Beibei Zhu", "Rui Hou"], "title": "LlamaRL: A Distributed Asynchronous Reinforcement Learning Framework for Efficient Large-scale LLM Trainin", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Reinforcement Learning (RL) has become the most effective post-training\napproach for improving the capabilities of Large Language Models (LLMs). In\npractice, because of the high demands on latency and memory, it is particularly\nchallenging to develop an efficient RL framework that reliably manages policy\nmodels with hundreds to thousands of billions of parameters.\n  In this paper, we present LlamaRL, a fully distributed, asynchronous RL\nframework optimized for efficient training of large-scale LLMs with various\nmodel sizes (8B, 70B, and 405B parameters) on GPU clusters ranging from a\nhandful to thousands of devices. LlamaRL introduces a streamlined,\nsingle-controller architecture built entirely on native PyTorch, enabling\nmodularity, ease of use, and seamless scalability to thousands of GPUs. We also\nprovide a theoretical analysis of LlamaRL's efficiency, including a formal\nproof that its asynchronous design leads to strict RL speed-up. Empirically, by\nleveraging best practices such as colocated model offloading, asynchronous\noff-policy training, and distributed direct memory access for weight\nsynchronization, LlamaRL achieves significant efficiency gains -- up to 10.7x\nspeed-up compared to DeepSpeed-Chat-like systems on a 405B-parameter policy\nmodel. Furthermore, the efficiency advantage continues to grow with increasing\nmodel scale, demonstrating the framework's suitability for future large-scale\nRL training."}
{"id": "2505.23838", "pdf": "https://arxiv.org/pdf/2505.23838", "abs": "https://arxiv.org/abs/2505.23838", "authors": ["Yiming Huang", "Jiyu Guo", "Wenxin Mao", "Cuiyun Gao", "Peiyi Han", "Chuanyi Liu", "Qing Ling"], "title": "Exploring the Landscape of Text-to-SQL with Large Language Models: Progresses, Challenges and Opportunities", "categories": ["cs.CL", "cs.IR"], "comment": "Submitted to ACM Computing Surveys (CSUR). Currently under review", "summary": "Converting natural language (NL) questions into SQL queries, referred to as\nText-to-SQL, has emerged as a pivotal technology for facilitating access to\nrelational databases, especially for users without SQL knowledge. Recent\nprogress in large language models (LLMs) has markedly propelled the field of\nnatural language processing (NLP), opening new avenues to improve text-to-SQL\nsystems. This study presents a systematic review of LLM-based text-to-SQL,\nfocusing on four key aspects: (1) an analysis of the research trends in\nLLM-based text-to-SQL; (2) an in-depth analysis of existing LLM-based\ntext-to-SQL techniques from diverse perspectives; (3) summarization of existing\ntext-to-SQL datasets and evaluation metrics; and (4) discussion on potential\nobstacles and avenues for future exploration in this domain. This survey seeks\nto furnish researchers with an in-depth understanding of LLM-based text-to-SQL,\nsparking new innovations and advancements in this field."}
{"id": "2505.24227", "pdf": "https://arxiv.org/pdf/2505.24227", "abs": "https://arxiv.org/abs/2505.24227", "authors": ["Ying Yang", "Jie Zhang", "Xiao Lv", "Di Lin", "Tao Xiang", "Qing Guo"], "title": "Light as Deception: GPT-driven Natural Relighting Against Vision-Language Pre-training Models", "categories": ["cs.CV", "cs.CR"], "comment": null, "summary": "While adversarial attacks on vision-and-language pretraining (VLP) models\nhave been explored, generating natural adversarial samples crafted through\nrealistic and semantically meaningful perturbations remains an open challenge.\nExisting methods, primarily designed for classification tasks, struggle when\nadapted to VLP models due to their restricted optimization spaces, leading to\nineffective attacks or unnatural artifacts. To address this, we propose\n\\textbf{LightD}, a novel framework that generates natural adversarial samples\nfor VLP models via semantically guided relighting. Specifically, LightD\nleverages ChatGPT to propose context-aware initial lighting parameters and\nintegrates a pretrained relighting model (IC-light) to enable diverse lighting\nadjustments. LightD expands the optimization space while ensuring perturbations\nalign with scene semantics. Additionally, gradient-based optimization is\napplied to the reference lighting image to further enhance attack effectiveness\nwhile maintaining visual naturalness. The effectiveness and superiority of the\nproposed LightD have been demonstrated across various VLP models in tasks such\nas image captioning and visual question answering."}
{"id": "2505.18754", "pdf": "https://arxiv.org/pdf/2505.18754", "abs": "https://arxiv.org/abs/2505.18754", "authors": ["Elsen Ronando", "Sozo Inoue"], "title": "Few-Shot Optimization for Sensor Data Using Large Language Models: A Case Study on Fatigue Detection", "categories": ["cs.CL", "cs.AI", "cs.LG", "I.2.7"], "comment": "43 pages, 18 figures. Accepted for publication in MDPI Sensors\n  (2025). Final version before journal publication", "summary": "In this paper, we propose a novel few-shot optimization with HED-LM (Hybrid\nEuclidean Distance with Large Language Models) to improve example selection for\nsensor-based classification tasks. While few-shot prompting enables efficient\ninference with limited labeled data, its performance largely depends on the\nquality of selected examples. HED-LM addresses this challenge through a hybrid\nselection pipeline that filters candidate examples based on Euclidean distance\nand re-ranks them using contextual relevance scored by large language models\n(LLMs). To validate its effectiveness, we apply HED-LM to a fatigue detection\ntask using accelerometer data characterized by overlapping patterns and high\ninter-subject variability. Unlike simpler tasks such as activity recognition,\nfatigue detection demands more nuanced example selection due to subtle\ndifferences in physiological signals. Our experiments show that HED-LM achieves\na mean macro F1-score of 69.13$\\pm$10.71%, outperforming both random selection\n(59.30$\\pm$10.13%) and distance-only filtering (67.61$\\pm$11.39%). These\nrepresent relative improvements of 16.6% and 2.3%, respectively. The results\nconfirm that combining numerical similarity with contextual relevance improves\nthe robustness of few-shot prompting. Overall, HED-LM offers a practical\nsolution to improve performance in real-world sensor-based learning tasks and\nshows potential for broader applications in healthcare monitoring, human\nactivity recognition, and industrial safety scenarios."}
{"id": "2505.24048", "pdf": "https://arxiv.org/pdf/2505.24048", "abs": "https://arxiv.org/abs/2505.24048", "authors": ["Guangtao Zheng", "Wenqian Ye", "Aidong Zhang"], "title": "NeuronTune: Towards Self-Guided Spurious Bias Mitigation", "categories": ["cs.LG"], "comment": "Accepted to ICML 2025", "summary": "Deep neural networks often develop spurious bias, reliance on correlations\nbetween non-essential features and classes for predictions. For example, a\nmodel may identify objects based on frequently co-occurring backgrounds rather\nthan intrinsic features, resulting in degraded performance on data lacking\nthese correlations. Existing mitigation approaches typically depend on external\nannotations of spurious correlations, which may be difficult to obtain and are\nnot relevant to the spurious bias in a model. In this paper, we take a step\ntowards self-guided mitigation of spurious bias by proposing NeuronTune, a post\nhoc method that directly intervenes in a model's internal decision process. Our\nmethod probes in a model's latent embedding space to identify and regulate\nneurons that lead to spurious prediction behaviors. We theoretically justify\nour approach and show that it brings the model closer to an unbiased one.\nUnlike previous methods, NeuronTune operates without requiring spurious\ncorrelation annotations, making it a practical and effective tool for improving\nmodel robustness. Experiments across different architectures and data\nmodalities demonstrate that our method significantly mitigates spurious bias in\na self-guided way."}
{"id": "2505.23840", "pdf": "https://arxiv.org/pdf/2505.23840", "abs": "https://arxiv.org/abs/2505.23840", "authors": ["Jiseung Hong", "Grace Byun", "Seungone Kim", "Kai Shu"], "title": "Measuring Sycophancy of Language Models in Multi-turn Dialogues", "categories": ["cs.CL"], "comment": null, "summary": "Large Language Models (LLMs) are expected to provide helpful and harmless\nresponses, yet they often exhibit sycophancy--conforming to user beliefs\nregardless of factual accuracy or ethical soundness. Prior research on\nsycophancy has primarily focused on single-turn factual correctness,\noverlooking the dynamics of real-world interactions. In this work, we introduce\nSYCON Bench, a novel benchmark for evaluating sycophantic behavior in\nmulti-turn, free-form conversational settings. Our benchmark measures how\nquickly a model conforms to the user (Turn of Flip) and how frequently it\nshifts its stance under sustained user pressure (Number of Flip). Applying\nSYCON Bench to 17 LLMs across three real-world scenarios, we find that\nsycophancy remains a prevalent failure mode. Our analysis shows that alignment\ntuning amplifies sycophantic behavior, whereas model scaling and reasoning\noptimization strengthen the model's ability to resist undesirable user views.\nReasoning models generally outperform instruction-tuned models but often fail\nwhen they over-index on logical exposition instead of directly addressing the\nuser's underlying beliefs. Finally, we evaluate four additional prompting\nstrategies and demonstrate that adopting a third-person perspective reduces\nsycophancy by up to 63.8% in debate scenario. We release our code and data at\nhttps://github.com/JiseungHong/SYCON-Bench."}
{"id": "2505.24232", "pdf": "https://arxiv.org/pdf/2505.24232", "abs": "https://arxiv.org/abs/2505.24232", "authors": ["Haibo Jin", "Peiyan Zhang", "Peiran Wang", "Man Luo", "Haohan Wang"], "title": "From Hallucinations to Jailbreaks: Rethinking the Vulnerability of Large Foundation Models", "categories": ["cs.CV", "cs.AI", "cs.CL"], "comment": null, "summary": "Large foundation models (LFMs) are susceptible to two distinct\nvulnerabilities: hallucinations and jailbreak attacks. While typically studied\nin isolation, we observe that defenses targeting one often affect the other,\nhinting at a deeper connection.\n  We propose a unified theoretical framework that models jailbreaks as\ntoken-level optimization and hallucinations as attention-level optimization.\nWithin this framework, we establish two key propositions: (1) \\textit{Similar\nLoss Convergence} - the loss functions for both vulnerabilities converge\nsimilarly when optimizing for target-specific outputs; and (2) \\textit{Gradient\nConsistency in Attention Redistribution} - both exhibit consistent gradient\nbehavior driven by shared attention dynamics.\n  We validate these propositions empirically on LLaVA-1.5 and MiniGPT-4,\nshowing consistent optimization trends and aligned gradients. Leveraging this\nconnection, we demonstrate that mitigation techniques for hallucinations can\nreduce jailbreak success rates, and vice versa. Our findings reveal a shared\nfailure mode in LFMs and suggest that robustness strategies should jointly\naddress both vulnerabilities."}
{"id": "2505.23770", "pdf": "https://arxiv.org/pdf/2505.23770", "abs": "https://arxiv.org/abs/2505.23770", "authors": ["Sudhanshu Sekhar Tripathy"], "title": "A comprehensive survey of cybercrimes in India over the last decade", "categories": ["cs.CR", "cs.AI"], "comment": null, "summary": "Since the 1990s, the integration of technology into daily life has led to the\ncreation of an extensive network of interconnected devices, transforming how\nindividuals and organizations operate. However, this digital transformation has\nalso spurred the rise of cybercrime, criminal activities perpetrated through\nnetworks or computer systems. Cybercrime has become a global concern,\npresenting significant challenges to security systems. Although advancements in\ndigital technology have enhanced efficiency, they have also opened new avenues\nfor exploitation by cybercriminals, highlighting the urgent need for advanced\ncybersecurity measures. The escalating number of cyberattacks and associated\nrisks in the past decade highlights the critical importance of protecting\nsensitive data and safeguarding information systems. Cybercrimes range from\nfinancial fraud and phishing scams to identity theft and online harassment,\nposing substantial risks to both individuals and organizations. In response,\ngovernments, law enforcement agencies, and cybersecurity units have intensified\ntheir efforts to address these threats. In recent years, India has experienced\na significant surge in cybercrime incidents, with a notable increase in cases\ninvolving ransomware, data breaches, and social engineering attacks. The\ngrowing penetration of internet services, the expansion of e-commerce, and the\nrapid adoption of digital payment systems have made individuals and\norganizations more vulnerable to cyber threats. Key areas affected include\nbanking, healthcare, and government sectors, which are frequently targeted due\nto the sensitive nature of the data they handle. To combat these risks, there\nis an increasing focus on public awareness, cybersecurity education, and robust\nregulatory frameworks. This paper examines cybercrime, prevention strategies,\nsecurity protocols, and terminology to safeguard digital infrastructure."}
{"id": "2505.24054", "pdf": "https://arxiv.org/pdf/2505.24054", "abs": "https://arxiv.org/abs/2505.24054", "authors": ["Elpiniki Maria Lygizou", "MÃ³nika Farsang", "Radu Grosu"], "title": "Differential Gated Self-Attention", "categories": ["cs.LG"], "comment": null, "summary": "Transformers excel across a large variety of tasks but remain susceptible to\ncorrupted inputs, since standard self-attention treats all query-key\ninteractions uniformly. Inspired by lateral inhibition in biological neural\ncircuits and building on the recent use by the Differential Transformer's use\nof two parallel softmax subtraction for noise cancellation, we propose\nMultihead Differential Gated Self-Attention (M-DGSA) that learns per-head\ninput-dependent gating to dynamically suppress attention noise. Each head\nsplits into excitatory and inhibitory branches whose dual softmax maps are\nfused by a sigmoid gate predicted from the token embedding, yielding a\ncontext-aware contrast enhancement. M-DGSA integrates seamlessly into existing\nTransformer stacks with minimal computational overhead. We evaluate on both\nvision and language benchmarks, demonstrating consistent robustness gains over\nvanilla Transformer, Vision Transformer, and Differential Transformer\nbaselines. Our contributions are (i) a novel input-dependent gating mechanism\nfor self-attention grounded in lateral inhibition, (ii) a principled synthesis\nof biological contrast-enhancement and self-attention theory, and (iii)\ncomprehensive experiments demonstrating noise resilience and cross-domain\napplicability."}
{"id": "2505.23842", "pdf": "https://arxiv.org/pdf/2505.23842", "abs": "https://arxiv.org/abs/2505.23842", "authors": ["Zikun Ye", "Hema Yoganarasimhan"], "title": "Document Valuation in LLM Summaries: A Cluster Shapley Approach", "categories": ["cs.CL", "econ.GN", "q-fin.EC"], "comment": null, "summary": "Large Language Models (LLMs) are increasingly used in systems that retrieve\nand summarize content from multiple sources, such as search engines and AI\nassistants. While these models enhance user experience by generating coherent\nsummaries, they obscure the contributions of original content creators, raising\nconcerns about credit attribution and compensation. We address the challenge of\nvaluing individual documents used in LLM-generated summaries. We propose using\nShapley values, a game-theoretic method that allocates credit based on each\ndocument's marginal contribution. Although theoretically appealing, Shapley\nvalues are expensive to compute at scale. We therefore propose Cluster Shapley,\nan efficient approximation algorithm that leverages semantic similarity between\ndocuments. By clustering documents using LLM-based embeddings and computing\nShapley values at the cluster level, our method significantly reduces\ncomputation while maintaining attribution quality. We demonstrate our approach\nto a summarization task using Amazon product reviews. Cluster Shapley\nsignificantly reduces computational complexity while maintaining high accuracy,\noutperforming baseline methods such as Monte Carlo sampling and Kernel SHAP\nwith a better efficient frontier. Our approach is agnostic to the exact LLM\nused, the summarization process used, and the evaluation procedure, which makes\nit broadly applicable to a variety of summarization settings."}
{"id": "2505.24238", "pdf": "https://arxiv.org/pdf/2505.24238", "abs": "https://arxiv.org/abs/2505.24238", "authors": ["Bowen Dong", "Minheng Ni", "Zitong Huang", "Guanglei Yang", "Wangmeng Zuo", "Lei Zhang"], "title": "MIRAGE: Assessing Hallucination in Multimodal Reasoning Chains of MLLM", "categories": ["cs.CV", "cs.LG"], "comment": null, "summary": "Multimodal hallucination in multimodal large language models (MLLMs)\nrestricts the correctness of MLLMs. However, multimodal hallucinations are\nmulti-sourced and arise from diverse causes. Existing benchmarks fail to\nadequately distinguish between perception-induced hallucinations and\nreasoning-induced hallucinations. This failure constitutes a significant issue\nand hinders the diagnosis of multimodal reasoning failures within MLLMs. To\naddress this, we propose the {\\dataset} benchmark, which isolates reasoning\nhallucinations by constructing questions where input images are correctly\nperceived by MLLMs yet reasoning errors persist. {\\dataset} introduces\nmulti-granular evaluation metrics: accuracy, factuality, and LLMs hallucination\nscore for hallucination quantification. Our analysis reveals that (1) the model\nscale, data scale, and training stages significantly affect the degree of\nlogical, fabrication, and factual hallucinations; (2) current MLLMs show no\neffective improvement on spatial hallucinations caused by misinterpreted\nspatial relationships, indicating their limited visual reasoning capabilities;\nand (3) question types correlate with distinct hallucination patterns,\nhighlighting targeted challenges and potential mitigation strategies. To\naddress these challenges, we propose {\\method}, a method that combines\ncurriculum reinforcement fine-tuning to encourage models to generate\nlogic-consistent reasoning chains by stepwise reducing learning difficulty, and\ncollaborative hint inference to reduce reasoning complexity. {\\method}\nestablishes a baseline on {\\dataset}, and reduces the logical hallucinations in\noriginal base models."}
{"id": "2505.23783", "pdf": "https://arxiv.org/pdf/2505.23783", "abs": "https://arxiv.org/abs/2505.23783", "authors": ["Korel Gundem", "Juncheng Dong", "Dennis Zhang", "Vahid Tarokh", "Zhengling Qi"], "title": "Boosting In-Context Learning in LLMs Through the Lens of Classical Supervised Learning", "categories": ["stat.ML", "cs.AI", "cs.CL", "cs.LG"], "comment": null, "summary": "In-Context Learning (ICL) allows Large Language Models (LLMs) to adapt to new\ntasks with just a few examples, but their predictions often suffer from\nsystematic biases, leading to unstable performances in classification. While\ncalibration techniques are proposed to mitigate these biases, we show that, in\nthe logit space, many of these methods are equivalent to merely shifting the\nLLM's decision boundary without having the ability to alter its orientation.\nThis proves inadequate when biases cause the LLM to be severely misdirected. To\naddress these limitations and provide a unifying framework, we propose\nSupervised Calibration (SC), a loss-minimization based framework which learns\nan optimal, per-class affine transformation of the LLM's predictive\nprobabilities in the logit space without requiring external data beyond the\ncontext. By using a more expressive functional class, SC not only subsumes many\nexisting calibration methods in ICL as special cases, but also enables the\nability to alter and even completely reverse the orientation of the LLM's\ndecision boundary. Furthermore, SC's loss-based nature facilitates the seamless\nintegration of two purpose-built regularization techniques: context-invariance\nand directional trust-region. The former is designed to tackle the instability\nissue in ICL, while the latter controls the degree of calibration. Finally, SC\ndelivers state-of-the-art performance over calibration baselines in the 4-shot,\n8-shot, and 16-shot settings across all nine datasets for\nMistral-7B-Instruct-v0.3, LLaMA-2-7B-chat, and Qwen2-7B-Instruct."}
{"id": "2505.24055", "pdf": "https://arxiv.org/pdf/2505.24055", "abs": "https://arxiv.org/abs/2505.24055", "authors": ["Yilong Wang", "Tianxiang Zhao", "Zongyu Wu", "Suhang Wang"], "title": "Bridging Source and Target Domains via Link Prediction for Unsupervised Domain Adaptation on Graphs", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Graph neural networks (GNNs) have shown great ability for node classification\non graphs. However, the success of GNNs relies on abundant labeled data, while\nobtaining high-quality labels is costly and challenging, especially for newly\nemerging domains. Hence, unsupervised domain adaptation (UDA), which trains a\nclassifier on the labeled source graph and adapts it to the unlabeled target\ngraph, is attracting increasing attention. Various approaches have been\nproposed to alleviate the distribution shift between the source and target\ngraphs to facilitate the classifier adaptation. However, most of them simply\nadopt existing UDA techniques developed for independent and identically\ndistributed data to gain domain-invariant node embeddings for graphs, which do\nnot fully consider the graph structure and message-passing mechanism of GNNs\nduring the adaptation and will fail when label distribution shift exists among\ndomains. In this paper, we proposed a novel framework that adopts link\nprediction to connect nodes between source and target graphs, which can\nfacilitate message-passing between the source and target graphs and augment the\ntarget nodes to have ``in-distribution'' neighborhoods with the source domain.\nThis strategy modified the target graph on the input level to reduce its\ndeviation from the source domain in the embedding space and is insensitive to\ndisproportional label distributions across domains. To prevent the loss of\ndiscriminative information in the target graph, we further design a novel\nidentity-preserving learning objective, which guides the learning of the edge\ninsertion module together with reconstruction and adaptation losses.\nExperimental results on real-world datasets demonstrate the effectiveness of\nour framework."}
{"id": "2505.23843", "pdf": "https://arxiv.org/pdf/2505.23843", "abs": "https://arxiv.org/abs/2505.23843", "authors": ["Wenhan Dong", "Tianyi Hu", "Jingyi Zheng", "Zhen Sun", "Yuemeng Zhao", "Yule Liu", "Xinlei He", "Xinyi Huang"], "title": "Evaluation Hallucination in Multi-Round Incomplete Information Lateral-Driven Reasoning Tasks", "categories": ["cs.CL", "cs.LG"], "comment": null, "summary": "Multi-round incomplete information tasks are crucial for evaluating the\nlateral thinking capabilities of large language models (LLMs). Currently,\nresearch primarily relies on multiple benchmarks and automated evaluation\nmetrics to assess these abilities. However, our study reveals novel insights\ninto the limitations of existing methods, as they often yield misleading\nresults that fail to uncover key issues, such as shortcut-taking behaviors,\nrigid patterns, and premature task termination. These issues obscure the true\nreasoning capabilities of LLMs and undermine the reliability of evaluations. To\naddress these limitations, we propose a refined set of evaluation standards,\nincluding inspection of reasoning paths, diversified assessment metrics, and\ncomparative analyses with human performance."}
{"id": "2505.24245", "pdf": "https://arxiv.org/pdf/2505.24245", "abs": "https://arxiv.org/abs/2505.24245", "authors": ["Xin Kang", "Zihan Zheng", "Lei Chu", "Yue Gao", "Jiahao Li", "Hao Pan", "Xuejin Chen", "Yan Lu"], "title": "LTM3D: Bridging Token Spaces for Conditional 3D Generation with Auto-Regressive Diffusion Framework", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "We present LTM3D, a Latent Token space Modeling framework for conditional 3D\nshape generation that integrates the strengths of diffusion and auto-regressive\n(AR) models. While diffusion-based methods effectively model continuous latent\nspaces and AR models excel at capturing inter-token dependencies, combining\nthese paradigms for 3D shape generation remains a challenge. To address this,\nLTM3D features a Conditional Distribution Modeling backbone, leveraging a\nmasked autoencoder and a diffusion model to enhance token dependency learning.\nAdditionally, we introduce Prefix Learning, which aligns condition tokens with\nshape latent tokens during generation, improving flexibility across modalities.\nWe further propose a Latent Token Reconstruction module with\nReconstruction-Guided Sampling to reduce uncertainty and enhance structural\nfidelity in generated shapes. Our approach operates in token space, enabling\nsupport for multiple 3D representations, including signed distance fields,\npoint clouds, meshes, and 3D Gaussian Splatting. Extensive experiments on\nimage- and text-conditioned shape generation tasks demonstrate that LTM3D\noutperforms existing methods in prompt fidelity and structural accuracy while\noffering a generalizable framework for multi-modal, multi-representation 3D\ngeneration."}
{"id": "2505.23786", "pdf": "https://arxiv.org/pdf/2505.23786", "abs": "https://arxiv.org/abs/2505.23786", "authors": ["Kazuki Egashira", "Robin Staab", "Mark Vero", "Jingxuan He", "Martin Vechev"], "title": "Mind the Gap: A Practical Attack on GGUF Quantization", "categories": ["cs.CR", "cs.AI", "cs.LG"], "comment": "ICML 2025", "summary": "With the increasing size of frontier LLMs, post-training quantization has\nbecome the standard for memory-efficient deployment. Recent work has shown that\nbasic rounding-based quantization schemes pose security risks, as they can be\nexploited to inject malicious behaviors into quantized models that remain\nhidden in full precision. However, existing attacks cannot be applied to more\ncomplex quantization methods, such as the GGUF family used in the popular\nollama and llama.cpp frameworks. In this work, we address this gap by\nintroducing the first attack on GGUF. Our key insight is that the quantization\nerror -- the difference between the full-precision weights and their\n(de-)quantized version -- provides sufficient flexibility to construct\nmalicious quantized models that appear benign in full precision. Leveraging\nthis, we develop an attack that trains the target malicious LLM while\nconstraining its weights based on quantization errors. We demonstrate the\neffectiveness of our attack on three popular LLMs across nine GGUF quantization\ndata types on three diverse attack scenarios: insecure code generation\n($\\Delta$=$88.7\\%$), targeted content injection ($\\Delta$=$85.0\\%$), and benign\ninstruction refusal ($\\Delta$=$30.1\\%$). Our attack highlights that (1) the\nmost widely used post-training quantization method is susceptible to\nadversarial interferences, and (2) the complexity of quantization schemes alone\nis insufficient as a defense."}
{"id": "2505.24059", "pdf": "https://arxiv.org/pdf/2505.24059", "abs": "https://arxiv.org/abs/2505.24059", "authors": ["Sean Foley", "Hong Nguyen", "Jihwan Lee", "Sudarsana Reddy Kadiri", "Dani Byrd", "Louis Goldstein", "Shrikanth Narayanan"], "title": "Towards disentangling the contributions of articulation and acoustics in multimodal phoneme recognition", "categories": ["cs.LG"], "comment": null, "summary": "Although many previous studies have carried out multimodal learning with\nreal-time MRI data that captures the audio-visual kinematics of the vocal tract\nduring speech, these studies have been limited by their reliance on\nmulti-speaker corpora. This prevents such models from learning a detailed\nrelationship between acoustics and articulation due to considerable\ncross-speaker variability. In this study, we develop unimodal audio and video\nmodels as well as multimodal models for phoneme recognition using a long-form\nsingle-speaker MRI corpus, with the goal of disentangling and interpreting the\ncontributions of each modality. Audio and multimodal models show similar\nperformance on different phonetic manner classes but diverge on places of\narticulation. Interpretation of the models' latent space shows similar encoding\nof the phonetic space across audio and multimodal models, while the models'\nattention weights highlight differences in acoustic and articulatory timing for\ncertain phonemes."}
{"id": "2505.23844", "pdf": "https://arxiv.org/pdf/2505.23844", "abs": "https://arxiv.org/abs/2505.23844", "authors": ["Zhenglun Kong", "Zheng Zhan", "Shiyue Hou", "Yifan Gong", "Xin Meng", "Pengwei Sui", "Peiyan Dong", "Xuan Shen", "Zifeng Wang", "Pu Zhao", "Hao Tang", "Stratis Ioannidis", "Yanzhi Wang"], "title": "Enabling Flexible Multi-LLM Integration for Scalable Knowledge Aggregation", "categories": ["cs.CL"], "comment": null, "summary": "Large language models (LLMs) have shown remarkable promise but remain\nchallenging to continually improve through traditional finetuning, particularly\nwhen integrating capabilities from other specialized LLMs. Popular methods like\nensemble and weight merging require substantial memory and struggle to adapt to\nchanging data environments. Recent efforts have transferred knowledge from\nmultiple LLMs into a single target model; however, they suffer from\ninterference and degraded performance among tasks, largely due to limited\nflexibility in candidate selection and training pipelines. To address these\nissues, we propose a framework that adaptively selects and aggregates knowledge\nfrom diverse LLMs to build a single, stronger model, avoiding the high memory\noverhead of ensemble and inflexible weight merging. Specifically, we design an\nadaptive selection network that identifies the most relevant source LLMs based\non their scores, thereby reducing knowledge interference. We further propose a\ndynamic weighted fusion strategy that accounts for the inherent strengths of\ncandidate LLMs, along with a feedback-driven loss function that prevents the\nselector from converging on a single subset of sources. Experimental results\ndemonstrate that our method can enable a more stable and scalable knowledge\naggregation process while reducing knowledge interference by up to 50% compared\nto existing approaches. Code is avaliable at\nhttps://github.com/ZLKong/LLM_Integration"}
{"id": "2505.24247", "pdf": "https://arxiv.org/pdf/2505.24247", "abs": "https://arxiv.org/abs/2505.24247", "authors": ["Minchul Kim", "Anil Jain", "Xiaoming Liu"], "title": "50 Years of Automated Face Recognition", "categories": ["cs.CV"], "comment": null, "summary": "Over the past 50 years, automated face recognition has evolved from\nrudimentary, handcrafted systems into sophisticated deep learning models that\nrival and often surpass human performance. This paper chronicles the history\nand technological progression of FR, from early geometric and statistical\nmethods to modern deep neural architectures leveraging massive real and\nAI-generated datasets. We examine key innovations that have shaped the field,\nincluding developments in dataset, loss function, neural network design and\nfeature fusion. We also analyze how the scale and diversity of training data\ninfluence model generalization, drawing connections between dataset growth and\nbenchmark improvements. Recent advances have achieved remarkable milestones:\nstate-of-the-art face verification systems now report False Negative\nIdentification Rates of 0.13% against a 12.4 million gallery in NIST FRVT\nevaluations for 1:N visa-to-border matching. While recent advances have enabled\nremarkable accuracy in high- and low-quality face scenarios, numerous\nchallenges persist. While remarkable progress has been achieved, several open\nresearch problems remain. We outline critical challenges and promising\ndirections for future face recognition research, including scalability,\nmulti-modal fusion, synthetic identity generation, and explainable systems."}
{"id": "2505.23791", "pdf": "https://arxiv.org/pdf/2505.23791", "abs": "https://arxiv.org/abs/2505.23791", "authors": ["Sayyed Farid Ahamed", "Sandip Roy", "Soumya Banerjee", "Marc Vucovich", "Kevin Choi", "Abdul Rahman", "Alison Hu", "Edward Bowen", "Sachin Shetty"], "title": "Evaluating Query Efficiency and Accuracy of Transfer Learning-based Model Extraction Attack in Federated Learning", "categories": ["cs.CR", "cs.AI", "cs.LG", "I.2.6; D.4.6"], "comment": "Accepted at IEEE IWCMC. 6 pages, 4 Figures, 3 tables", "summary": "Federated Learning (FL) is a collaborative learning framework designed to\nprotect client data, yet it remains highly vulnerable to Intellectual Property\n(IP) threats. Model extraction (ME) attacks pose a significant risk to Machine\nLearning as a Service (MLaaS) platforms, enabling attackers to replicate\nconfidential models by querying black-box (without internal insight) APIs.\nDespite FL's privacy-preserving goals, its distributed nature makes it\nparticularly susceptible to such attacks. This paper examines the vulnerability\nof FL-based victim models to two types of model extraction attacks. For various\nfederated clients built under the NVFlare platform, we implemented ME attacks\nacross two deep learning architectures and three image datasets. We evaluate\nthe proposed ME attack performance using various metrics, including accuracy,\nfidelity, and KL divergence. The experiments show that for different FL\nclients, the accuracy and fidelity of the extracted model are closely related\nto the size of the attack query set. Additionally, we explore a transfer\nlearning based approach where pretrained models serve as the starting point for\nthe extraction process. The results indicate that the accuracy and fidelity of\nthe fine-tuned pretrained extraction models are notably higher, particularly\nwith smaller query sets, highlighting potential advantages for attackers."}
{"id": "2505.24060", "pdf": "https://arxiv.org/pdf/2505.24060", "abs": "https://arxiv.org/abs/2505.24060", "authors": ["Chris Mingard", "Lukas Seier", "Niclas GÃ¶ring", "Andrei-Vlad Badelita", "Charles London", "Ard Louis"], "title": "Characterising the Inductive Biases of Neural Networks on Boolean Data", "categories": ["cs.LG", "stat.ML"], "comment": null, "summary": "Deep neural networks are renowned for their ability to generalise well across\ndiverse tasks, even when heavily overparameterized. Existing works offer only\npartial explanations (for example, the NTK-based task-model alignment\nexplanation neglects feature learning). Here, we provide an end-to-end,\nanalytically tractable case study that links a network's inductive prior, its\ntraining dynamics including feature learning, and its eventual generalisation.\nSpecifically, we exploit the one-to-one correspondence between depth-2 discrete\nfully connected networks and disjunctive normal form (DNF) formulas by training\non Boolean functions. Under a Monte Carlo learning algorithm, our model\nexhibits predictable training dynamics and the emergence of interpretable\nfeatures. This framework allows us to trace, in detail, how inductive bias and\nfeature formation drive generalisation."}
{"id": "2505.23845", "pdf": "https://arxiv.org/pdf/2505.23845", "abs": "https://arxiv.org/abs/2505.23845", "authors": ["Jakub Podolak", "Rajeev Verma"], "title": "Read Your Own Mind: Reasoning Helps Surface Self-Confidence Signals in LLMs", "categories": ["cs.CL"], "comment": null, "summary": "We study the source of uncertainty in DeepSeek R1-32B by analyzing its\nself-reported verbal confidence on question answering (QA) tasks. In the\ndefault answer-then-confidence setting, the model is regularly over-confident,\nwhereas semantic entropy - obtained by sampling many responses - remains\nreliable. We hypothesize that this is because of semantic entropy's larger\ntest-time compute, which lets us explore the model's predictive distribution.\nWe show that granting DeepSeek the budget to explore its distribution by\nforcing a long chain-of-thought before the final answer greatly improves its\nverbal score effectiveness, even on simple fact-retrieval questions that\nnormally require no reasoning. Furthermore, a separate reader model that sees\nonly the chain can reconstruct very similar confidences, indicating the verbal\nscore might be merely a statistic of the alternatives surfaced during\nreasoning. Our analysis concludes that reliable uncertainty estimation requires\nexplicit exploration of the generative space, and self-reported confidence is\ntrustworthy only after such exploration."}
{"id": "2505.24249", "pdf": "https://arxiv.org/pdf/2505.24249", "abs": "https://arxiv.org/abs/2505.24249", "authors": ["Qingyao Tian", "Huai Liao", "Xinyan Huang", "Bingyu Yang", "Hongbin Liu"], "title": "Harnessing Foundation Models for Robust and Generalizable 6-DOF Bronchoscopy Localization", "categories": ["cs.CV"], "comment": null, "summary": "Vision-based 6-DOF bronchoscopy localization offers a promising solution for\naccurate and cost-effective interventional guidance. However, existing methods\nstruggle with 1) limited generalization across patient cases due to scarce\nlabeled data, and 2) poor robustness under visual degradation, as bronchoscopy\nprocedures frequently involve artifacts such as occlusions and motion blur that\nimpair visual information. To address these challenges, we propose PANSv2, a\ngeneralizable and robust bronchoscopy localization framework. Motivated by PANS\nthat leverages multiple visual cues for pose likelihood measurement, PANSv2\nintegrates depth estimation, landmark detection, and centerline constraints\ninto a unified pose optimization framework that evaluates pose probability and\nsolves for the optimal bronchoscope pose. To further enhance generalization\ncapabilities, we leverage the endoscopic foundation model EndoOmni for depth\nestimation and the video foundation model EndoMamba for landmark detection,\nincorporating both spatial and temporal analyses. Pretrained on diverse\nendoscopic datasets, these models provide stable and transferable visual\nrepresentations, enabling reliable performance across varied bronchoscopy\nscenarios. Additionally, to improve robustness to visual degradation, we\nintroduce an automatic re-initialization module that detects tracking failures\nand re-establishes pose using landmark detections once clear views are\navailable. Experimental results on bronchoscopy dataset encompassing 10 patient\ncases show that PANSv2 achieves the highest tracking success rate, with an\n18.1% improvement in SR-5 (percentage of absolute trajectory error under 5 mm)\ncompared to existing methods, showing potential towards real clinical usage."}
{"id": "2505.23792", "pdf": "https://arxiv.org/pdf/2505.23792", "abs": "https://arxiv.org/abs/2505.23792", "authors": ["Kai Li", "Conggai Li", "Xin Yuan", "Shenghong Li", "Sai Zou", "Syed Sohail Ahmed", "Wei Ni", "Dusit Niyato", "Abbas Jamalipour", "Falko Dressler", "Ozgur B. Akan"], "title": "Zero-Trust Foundation Models: A New Paradigm for Secure and Collaborative Artificial Intelligence for Internet of Things", "categories": ["cs.CR", "cs.AI"], "comment": null, "summary": "This paper focuses on Zero-Trust Foundation Models (ZTFMs), a novel paradigm\nthat embeds zero-trust security principles into the lifecycle of foundation\nmodels (FMs) for Internet of Things (IoT) systems. By integrating core tenets,\nsuch as continuous verification, least privilege access (LPA), data\nconfidentiality, and behavioral analytics into the design, training, and\ndeployment of FMs, ZTFMs can enable secure, privacy-preserving AI across\ndistributed, heterogeneous, and potentially adversarial IoT environments. We\npresent the first structured synthesis of ZTFMs, identifying their potential to\ntransform conventional trust-based IoT architectures into resilient,\nself-defending ecosystems. Moreover, we propose a comprehensive technical\nframework, incorporating federated learning (FL), blockchain-based identity\nmanagement, micro-segmentation, and trusted execution environments (TEEs) to\nsupport decentralized, verifiable intelligence at the network edge. In\naddition, we investigate emerging security threats unique to ZTFM-enabled\nsystems and evaluate countermeasures, such as anomaly detection, adversarial\ntraining, and secure aggregation. Through this analysis, we highlight key open\nresearch challenges in terms of scalability, secure orchestration,\ninterpretable threat attribution, and dynamic trust calibration. This survey\nlays a foundational roadmap for secure, intelligent, and trustworthy IoT\ninfrastructures powered by FMs."}
{"id": "2505.24061", "pdf": "https://arxiv.org/pdf/2505.24061", "abs": "https://arxiv.org/abs/2505.24061", "authors": ["Jiashun Liu", "Zihao Wu", "Johan Obando-Ceron", "Pablo Samuel Castro", "Aaron Courville", "Ling Pan"], "title": "Measure gradients, not activations! Enhancing neuronal activity in deep reinforcement learning", "categories": ["cs.LG"], "comment": null, "summary": "Deep reinforcement learning (RL) agents frequently suffer from neuronal\nactivity loss, which impairs their ability to adapt to new data and learn\ncontinually. A common method to quantify and address this issue is the\ntau-dormant neuron ratio, which uses activation statistics to measure the\nexpressive ability of neurons. While effective for simple MLP-based agents,\nthis approach loses statistical power in more complex architectures. To address\nthis, we argue that in advanced RL agents, maintaining a neuron's learning\ncapacity, its ability to adapt via gradient updates, is more critical than\npreserving its expressive ability. Based on this insight, we shift the\nstatistical objective from activations to gradients, and introduce GraMa\n(Gradient Magnitude Neural Activity Metric), a lightweight,\narchitecture-agnostic metric for quantifying neuron-level learning capacity. We\nshow that GraMa effectively reveals persistent neuron inactivity across diverse\narchitectures, including residual networks, diffusion models, and agents with\nvaried activation functions. Moreover, resetting neurons guided by GraMa\n(ReGraMa) consistently improves learning performance across multiple deep RL\nalgorithms and benchmarks, such as MuJoCo and the DeepMind Control Suite."}
{"id": "2505.23848", "pdf": "https://arxiv.org/pdf/2505.23848", "abs": "https://arxiv.org/abs/2505.23848", "authors": ["Harvey Dam", "Jonas Knochelmann", "Vinu Joseph", "Ganesh Gopalakrishnan"], "title": "Derailing Non-Answers via Logit Suppression at Output Subspace Boundaries in RLHF-Aligned Language Models", "categories": ["cs.CL", "cs.LG"], "comment": null, "summary": "We introduce a method to reduce refusal rates of large language models (LLMs)\non sensitive content without modifying model weights or prompts. Motivated by\nthe observation that refusals in certain models were often preceded by the\nspecific token sequence of a token marking the beginning of the\nchain-of-thought (CoT) block (<think>) followed by a double newline token\n(\\n\\n), we investigate the impact of two simple formatting adjustments during\ngeneration: suppressing \\n\\n after <think> and suppressing the end-of-sequence\ntoken after the end of the CoT block (</think>). Our method requires no\ndatasets, parameter changes, or training, relying solely on modifying token\nprobabilities during generation. In our experiments with official DeepSeek-R1\ndistillations, these interventions increased the proportion of substantive\nanswers to sensitive prompts without affecting performance on standard\nbenchmarks. Our findings suggest that refusal behaviors can be circumvented by\nblocking refusal subspaces at specific points in the generation process."}
{"id": "2505.24257", "pdf": "https://arxiv.org/pdf/2505.24257", "abs": "https://arxiv.org/abs/2505.24257", "authors": ["Sahithya Ravi", "Gabriel Sarch", "Vibhav Vineet", "Andrew D. Wilson", "Balasaravanan Thoravi Kumaravel"], "title": "Out of Sight, Not Out of Context? Egocentric Spatial Reasoning in VLMs Across Disjoint Frames", "categories": ["cs.CV"], "comment": null, "summary": "An embodied AI assistant operating on egocentric video must integrate spatial\ncues across time - for instance, determining where an object A, glimpsed a few\nmoments ago lies relative to an object B encountered later. We introduce\nDisjoint-3DQA , a generative QA benchmark that evaluates this ability of VLMs\nby posing questions about object pairs that are not co-visible in the same\nframe. We evaluated seven state-of-the-art VLMs and found that models lag\nbehind human performance by 28%, with steeper declines in accuracy (60% to 30\n%) as the temporal gap widens. Our analysis further reveals that providing\ntrajectories or bird's-eye-view projections to VLMs results in only marginal\nimprovements, whereas providing oracle 3D coordinates leads to a substantial\n20% performance increase. This highlights a core bottleneck of multi-frame VLMs\nin constructing and maintaining 3D scene representations over time from visual\nsignals. Disjoint-3DQA therefore sets a clear, measurable challenge for\nlong-horizon spatial reasoning and aims to catalyze future research at the\nintersection of vision, language, and embodied AI."}
{"id": "2505.23793", "pdf": "https://arxiv.org/pdf/2505.23793", "abs": "https://arxiv.org/abs/2505.23793", "authors": ["Baolin Zheng", "Guanlin Chen", "Hongqiong Zhong", "Qingyang Teng", "Yingshui Tan", "Zhendong Liu", "Weixun Wang", "Jiaheng Liu", "Jian Yang", "Huiyun Jing", "Jincheng Wei", "Wenbo Su", "Xiaoyong Zhu", "Bo Zheng", "Kaifu Zhang"], "title": "USB: A Comprehensive and Unified Safety Evaluation Benchmark for Multimodal Large Language Models", "categories": ["cs.CR", "cs.AI"], "comment": null, "summary": "Despite their remarkable achievements and widespread adoption, Multimodal\nLarge Language Models (MLLMs) have revealed significant security\nvulnerabilities, highlighting the urgent need for robust safety evaluation\nbenchmarks. Existing MLLM safety benchmarks, however, fall short in terms of\ndata quality and coverge, and modal risk combinations, resulting in inflated\nand contradictory evaluation results, which hinders the discovery and\ngovernance of security concerns. Besides, we argue that vulnerabilities to\nharmful queries and oversensitivity to harmless ones should be considered\nsimultaneously in MLLMs safety evaluation, whereas these were previously\nconsidered separately. In this paper, to address these shortcomings, we\nintroduce Unified Safety Benchmarks (USB), which is one of the most\ncomprehensive evaluation benchmarks in MLLM safety. Our benchmark features\nhigh-quality queries, extensive risk categories, comprehensive modal\ncombinations, and encompasses both vulnerability and oversensitivity\nevaluations. From the perspective of two key dimensions: risk categories and\nmodality combinations, we demonstrate that the available benchmarks -- even the\nunion of the vast majority of them -- are far from being truly comprehensive.\nTo bridge this gap, we design a sophisticated data synthesis pipeline that\ngenerates extensive, high-quality complementary data addressing previously\nunexplored aspects. By combining open-source datasets with our synthetic data,\nour benchmark provides 4 distinct modality combinations for each of the 61 risk\nsub-categories, covering both English and Chinese across both vulnerability and\noversensitivity dimensions."}
{"id": "2505.24067", "pdf": "https://arxiv.org/pdf/2505.24067", "abs": "https://arxiv.org/abs/2505.24067", "authors": ["Yu He", "Ellen Vitercik"], "title": "Primal-Dual Neural Algorithmic Reasoning", "categories": ["cs.LG"], "comment": "The 42nd International Conference on Machine Learning, 2025", "summary": "Neural Algorithmic Reasoning (NAR) trains neural networks to simulate\nclassical algorithms, enabling structured and interpretable reasoning over\ncomplex data. While prior research has predominantly focused on learning exact\nalgorithms for polynomial-time-solvable problems, extending NAR to harder\nproblems remains an open challenge. In this work, we introduce a general NAR\nframework grounded in the primal-dual paradigm, a classical method for\ndesigning efficient approximation algorithms. By leveraging a bipartite\nrepresentation between primal and dual variables, we establish an alignment\nbetween primal-dual algorithms and Graph Neural Networks. Furthermore, we\nincorporate optimal solutions from small instances to greatly enhance the\nmodel's reasoning capabilities. Our empirical results demonstrate that our\nmodel not only simulates but also outperforms approximation algorithms for\nmultiple tasks, exhibiting robust generalization to larger and\nout-of-distribution graphs. Moreover, we highlight the framework's practical\nutility by integrating it with commercial solvers and applying it to real-world\ndatasets."}
{"id": "2505.23851", "pdf": "https://arxiv.org/pdf/2505.23851", "abs": "https://arxiv.org/abs/2505.23851", "authors": ["Michael Shalyt", "Rotem Elimelech", "Ido Kaminer"], "title": "ASyMOB: Algebraic Symbolic Mathematical Operations Benchmark", "categories": ["cs.CL", "cs.AI", "cs.SC"], "comment": "Code repository: https://github.com/RamanujanMachine/ASyMOB Complete\n  benchmark dataset:\n  https://huggingface.co/datasets/Shalyt/ASyMOB-Algebraic_Symbolic_Mathematical_Operations_Benchmark", "summary": "Large language models (LLMs) are rapidly approaching the level of proficiency\nin university-level symbolic mathematics required for applications in advanced\nscience and technology. However, existing benchmarks fall short in assessing\nthe core skills of LLMs in symbolic mathematics-such as integration,\ndifferential equations, and algebraic simplification. To address this gap, we\nintroduce ASyMOB, a novel assessment framework focused exclusively on symbolic\nmanipulation, featuring 17,092 unique math challenges, organized by similarity\nand complexity. ASyMOB enables analysis of LLM generalization capabilities by\ncomparing performance in problems that differ by simple numerical or symbolic\n`perturbations'. Evaluated LLMs exhibit substantial degradation in performance\nfor all perturbation types (up to -70.3%), suggesting reliance on memorized\npatterns rather than deeper understanding of symbolic math, even among models\nachieving high baseline accuracy. Comparing LLM performance to computer algebra\nsystems, we identify examples where they fail while LLMs succeed, as well as\nproblems solved only by combining both approaches. Models capable of integrated\ncode execution yielded higher accuracy compared to their performance without\ncode, particularly stabilizing weaker models (up to +33.1% for certain\nperturbation types). Notably, the most advanced models (o4-mini, Gemini 2.5\nFlash) demonstrate not only high symbolic math proficiency (scoring 96.8% and\n97.6% on the unperturbed set), but also remarkable robustness against\nperturbations, (-21.7% and -21.2% vs. average -50.4% for the other models).\nThis may indicate a recent \"phase transition\" in the generalization\ncapabilities of frontier LLMs. It remains to be seen whether the path forward\nlies in deeper integration with sophisticated external tools, or in developing\nmodels so capable that symbolic math systems like CAS become unnecessary."}
{"id": "2505.24282", "pdf": "https://arxiv.org/pdf/2505.24282", "abs": "https://arxiv.org/abs/2505.24282", "authors": ["Zirui Shang", "Xinxiao Wu", "Shuo Yang"], "title": "LLM-powered Query Expansion for Enhancing Boundary Prediction in Language-driven Action Localization", "categories": ["cs.CV"], "comment": null, "summary": "Language-driven action localization in videos requires not only semantic\nalignment between language query and video segment, but also prediction of\naction boundaries.\n  However, the language query primarily describes the main content of an action\nand usually lacks specific details of action start and end boundaries, which\nincreases the subjectivity of manual boundary annotation and leads to boundary\nuncertainty in training data.\n  In this paper, on one hand, we propose to expand the original query by\ngenerating textual descriptions of the action start and end boundaries through\nLLMs, which can provide more detailed boundary cues for localization and thus\nreduce the impact of boundary uncertainty.\n  On the other hand, to enhance the tolerance to boundary uncertainty during\ntraining, we propose to model probability scores of action boundaries by\ncalculating the semantic similarities between frames and the expanded query as\nwell as the temporal distances between frames and the annotated boundary\nframes. They can provide more consistent boundary supervision, thus improving\nthe stability of training.\n  Our method is model-agnostic and can be seamlessly and easily integrated into\nany existing models of language-driven action localization in an off-the-shelf\nmanner. Experimental results on several datasets demonstrate the effectiveness\nof our method."}
{"id": "2505.23803", "pdf": "https://arxiv.org/pdf/2505.23803", "abs": "https://arxiv.org/abs/2505.23803", "authors": ["Yinuo Xue", "Eric Spero", "Yun Sing Koh", "Giovanni Russello"], "title": "MultiPhishGuard: An LLM-based Multi-Agent System for Phishing Email Detection", "categories": ["cs.CR", "cs.AI"], "comment": null, "summary": "Phishing email detection faces critical challenges from evolving adversarial\ntactics and heterogeneous attack patterns. Traditional detection methods, such\nas rule-based filters and denylists, often struggle to keep pace with these\nevolving tactics, leading to false negatives and compromised security. While\nmachine learning approaches have improved detection accuracy, they still face\nchallenges adapting to novel phishing strategies. We present MultiPhishGuard, a\ndynamic LLM-based multi-agent detection system that synergizes specialized\nexpertise with adversarial-aware reinforcement learning. Our framework employs\nfive cooperative agents (text, URL, metadata, explanation simplifier, and\nadversarial agents) with automatically adjusted decision weights powered by a\nProximal Policy Optimization reinforcement learning algorithm. To address\nemerging threats, we introduce an adversarial training loop featuring an\nadversarial agent that generates subtle context-aware email variants, creating\na self-improving defense ecosystem and enhancing system robustness.\nExperimental evaluations on public datasets demonstrate that MultiPhishGuard\nsignificantly outperforms Chain-of-Thoughts, single-agent baselines and\nstate-of-the-art detectors, as validated by ablation studies and comparative\nanalyses. Experiments demonstrate that MultiPhishGuard achieves high accuracy\n(97.89\\%) with low false positive (2.73\\%) and false negative rates (0.20\\%).\nAdditionally, we incorporate an explanation simplifier agent, which provides\nusers with clear and easily understandable explanations for why an email is\nclassified as phishing or legitimate. This work advances phishing defense\nthrough dynamic multi-agent collaboration and generative adversarial\nresilience."}
{"id": "2505.24069", "pdf": "https://arxiv.org/pdf/2505.24069", "abs": "https://arxiv.org/abs/2505.24069", "authors": ["Yu He", "Yingxi Li", "Colin White", "Ellen Vitercik"], "title": "DSR-Bench: Evaluating the Structural Reasoning Abilities of LLMs via Data Structures", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Large language models (LLMs) are increasingly deployed for real-world tasks\nthat fundamentally involve data manipulation. A core requirement across these\ntasks is the ability to perform structural reasoning--that is, to understand\nand reason about data relationships. For example, customer requests require a\ntemporal ordering, which can be represented by data structures such as queues.\nHowever, existing benchmarks primarily focus on high-level, application-driven\nevaluations without isolating this fundamental capability. To address this gap,\nwe introduce DSR-Bench, a novel benchmark evaluating LLMs' structural reasoning\ncapabilities through data structures, which provide interpretable\nrepresentations of data relationships. DSR-Bench includes 20 data structures,\n35 operations, and 4,140 problem instances, organized hierarchically for\nfine-grained analysis of reasoning limitations. Our evaluation pipeline is\nfully automated and deterministic, eliminating subjective human or model-based\njudgments. Its synthetic nature also ensures scalability and minimizes data\ncontamination risks. We benchmark nine state-of-the-art LLMs. Our analysis\nshows that instruction-tuned models struggle with basic multi-attribute and\nmulti-hop reasoning. Furthermore, while reasoning-oriented models perform\nbetter, they remain fragile on complex and hybrid structures, with the best\nmodel achieving an average score of only 47% on the challenge subset.\nCrucially, models often perform poorly on multi-dimensional data and natural\nlanguage task descriptions, highlighting a critical gap for real-world\ndeployment."}
{"id": "2505.23854", "pdf": "https://arxiv.org/pdf/2505.23854", "abs": "https://arxiv.org/abs/2505.23854", "authors": ["Linwei Tao", "Yi-Fan Yeh", "Minjing Dong", "Tao Huang", "Philip Torr", "Chang Xu"], "title": "Revisiting Uncertainty Estimation and Calibration of Large Language Models", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": null, "summary": "As large language models (LLMs) are increasingly deployed in high-stakes\napplications, robust uncertainty estimation is essential for ensuring the safe\nand trustworthy deployment of LLMs. We present the most comprehensive study to\ndate of uncertainty estimation in LLMs, evaluating 80 models spanning open- and\nclosed-source families, dense and Mixture-of-Experts (MoE) architectures,\nreasoning and non-reasoning modes, quantization variants and parameter scales\nfrom 0.6B to 671B. Focusing on three representative black-box single-pass\nmethods, including token probability-based uncertainty (TPU), numerical verbal\nuncertainty (NVU), and linguistic verbal uncertainty (LVU), we systematically\nevaluate uncertainty calibration and selective classification using the\nchallenging MMLU-Pro benchmark, which covers both reasoning-intensive and\nknowledge-based tasks. Our results show that LVU consistently outperforms TPU\nand NVU, offering stronger calibration and discrimination while being more\ninterpretable. We also find that high accuracy does not imply reliable\nuncertainty, and that model scale, post-training, reasoning ability and\nquantization all influence estimation performance. Notably, LLMs exhibit better\nuncertainty estimates on reasoning tasks than on knowledge-heavy ones, and good\ncalibration does not necessarily translate to effective error ranking. These\nfindings highlight the need for multi-perspective evaluation and position LVU\nas a practical tool for improving the reliability of LLMs in real-world\nsettings."}
{"id": "2505.24287", "pdf": "https://arxiv.org/pdf/2505.24287", "abs": "https://arxiv.org/abs/2505.24287", "authors": ["Ege Ãzsoy", "Arda Mamur", "Felix Tristram", "Chantal Pellegrini", "Magdalena Wysocki", "Benjamin Busam", "Nassir Navab"], "title": "EgoExOR: An Ego-Exo-Centric Operating Room Dataset for Surgical Activity Understanding", "categories": ["cs.CV"], "comment": null, "summary": "Operating rooms (ORs) demand precise coordination among surgeons, nurses, and\nequipment in a fast-paced, occlusion-heavy environment, necessitating advanced\nperception models to enhance safety and efficiency. Existing datasets either\nprovide partial egocentric views or sparse exocentric multi-view context, but\ndo not explore the comprehensive combination of both. We introduce EgoExOR, the\nfirst OR dataset and accompanying benchmark to fuse first-person and\nthird-person perspectives. Spanning 94 minutes (84,553 frames at 15 FPS) of two\nemulated spine procedures, Ultrasound-Guided Needle Insertion and Minimally\nInvasive Spine Surgery, EgoExOR integrates egocentric data (RGB, gaze, hand\ntracking, audio) from wearable glasses, exocentric RGB and depth from RGB-D\ncameras, and ultrasound imagery. Its detailed scene graph annotations, covering\n36 entities and 22 relations (568,235 triplets), enable robust modeling of\nclinical interactions, supporting tasks like action recognition and\nhuman-centric perception. We evaluate the surgical scene graph generation\nperformance of two adapted state-of-the-art models and offer a new baseline\nthat explicitly leverages EgoExOR's multimodal and multi-perspective signals.\nThis new dataset and benchmark set a new foundation for OR perception, offering\na rich, multimodal resource for next-generation clinical perception."}
{"id": "2505.23805", "pdf": "https://arxiv.org/pdf/2505.23805", "abs": "https://arxiv.org/abs/2505.23805", "authors": ["Akram Sheriff", "Ken Huang", "Zsolt Nemeth", "Madjid Nakhjiri"], "title": "ADA: Automated Moving Target Defense for AI Workloads via Ephemeral Infrastructure-Native Rotation in Kubernetes", "categories": ["cs.CR", "cs.AI"], "comment": null, "summary": "This paper introduces the Adaptive Defense Agent (ADA), an innovative\nAutomated Moving Target Defense (AMTD) system designed to fundamentally enhance\nthe security posture of AI workloads. ADA operates by continuously and\nautomatically rotating these workloads at the infrastructure level, leveraging\nthe inherent ephemerality of Kubernetes pods. This constant managed churn\nsystematically invalidates attacker assumptions and disrupts potential kill\nchains by regularly destroying and respawning AI service instances. This\nmethodology, applying principles of chaos engineering as a continuous,\nproactive defense, offers a paradigm shift from traditional static defenses\nthat rely on complex and expensive confidential or trusted computing solutions\nto secure the underlying compute platforms, while at the same time agnostically\nsupporting the latest advancements in agentic and nonagentic AI ecosystems and\nsolutions such as agent-to-agent (A2A) communication frameworks or model\ncontext protocols (MCP). This AI-native infrastructure design, relying on the\nwidely proliferated cloud-native Kubernetes technologies, facilitates easier\ndeployment, simplifies maintenance through an inherent zero trust posture\nachieved by rotation, and promotes faster adoption. We posit that ADA's novel\napproach to AMTD provides a more robust, agile, and operationally efficient\nzero-trust model for AI services, achieving security through proactive\nenvironmental manipulation rather than reactive patching."}
{"id": "2505.24085", "pdf": "https://arxiv.org/pdf/2505.24085", "abs": "https://arxiv.org/abs/2505.24085", "authors": ["Alireza Jafari", "Fereshteh Yousefirizi", "Vahid Seydi"], "title": "DeepBoost-AF: A Novel Unsupervised Feature Learning and Gradient Boosting Fusion for Robust Atrial Fibrillation Detection in Raw ECG Signals", "categories": ["cs.LG"], "comment": "12-page,4 figures,3 tables, Achieves 95.20% F1-score (99.99%\n  sensitivity) on 8,528 PhysioNet 2017 recordings, Mean inference time: 4\n  seconds, Python implementation will be open-sourced upon publication", "summary": "Atrial fibrillation (AF) is a prevalent cardiac arrhythmia associated with\nelevated health risks, where timely detection is pivotal for mitigating\nstroke-related morbidity. This study introduces an innovative hybrid\nmethodology integrating unsupervised deep learning and gradient boosting models\nto improve AF detection. A 19-layer deep convolutional autoencoder (DCAE) is\ncoupled with three boosting classifiers-AdaBoost, XGBoost, and LightGBM\n(LGBM)-to harness their complementary advantages while addressing individual\nlimitations. The proposed framework uniquely combines DCAE with gradient\nboosting, enabling end-to-end AF identification devoid of manual feature\nextraction. The DCAE-LGBM model attains an F1-score of 95.20%, sensitivity of\n99.99%, and inference latency of four seconds, outperforming existing methods\nand aligning with clinical deployment requirements. The DCAE integration\nsignificantly enhances boosting models, positioning this hybrid system as a\nreliable tool for automated AF detection in clinical settings."}
{"id": "2505.23856", "pdf": "https://arxiv.org/pdf/2505.23856", "abs": "https://arxiv.org/abs/2505.23856", "authors": ["Sahil Verma", "Keegan Hines", "Jeff Bilmes", "Charlotte Siska", "Luke Zettlemoyer", "Hila Gonen", "Chandan Singh"], "title": "OMNIGUARD: An Efficient Approach for AI Safety Moderation Across Modalities", "categories": ["cs.CL", "cs.AI", "cs.HC", "cs.LG"], "comment": null, "summary": "The emerging capabilities of large language models (LLMs) have sparked\nconcerns about their immediate potential for harmful misuse. The core approach\nto mitigate these concerns is the detection of harmful queries to the model.\nCurrent detection approaches are fallible, and are particularly susceptible to\nattacks that exploit mismatched generalization of model capabilities (e.g.,\nprompts in low-resource languages or prompts provided in non-text modalities\nsuch as image and audio). To tackle this challenge, we propose OMNIGUARD, an\napproach for detecting harmful prompts across languages and modalities. Our\napproach (i) identifies internal representations of an LLM/MLLM that are\naligned across languages or modalities and then (ii) uses them to build a\nlanguage-agnostic or modality-agnostic classifier for detecting harmful\nprompts. OMNIGUARD improves harmful prompt classification accuracy by 11.57\\%\nover the strongest baseline in a multilingual setting, by 20.44\\% for\nimage-based prompts, and sets a new SOTA for audio-based prompts. By\nrepurposing embeddings computed during generation, OMNIGUARD is also very\nefficient ($\\approx 120 \\times$ faster than the next fastest baseline). Code\nand data are available at: https://github.com/vsahil/OmniGuard."}
{"id": "2505.24301", "pdf": "https://arxiv.org/pdf/2505.24301", "abs": "https://arxiv.org/abs/2505.24301", "authors": ["Enshang Zhang", "Zhicheng Zhang", "Takashi Hanakawa"], "title": "Category-aware EEG image generation based on wavelet transform and contrast semantic loss", "categories": ["cs.CV", "cs.HC"], "comment": null, "summary": "Reconstructing visual stimuli from EEG signals is a crucial step in realizing\nbrain-computer interfaces. In this paper, we propose a transformer-based EEG\nsignal encoder integrating the Discrete Wavelet Transform (DWT) and the gating\nmechanism. Guided by the feature alignment and category-aware fusion losses,\nthis encoder is used to extract features related to visual stimuli from EEG\nsignals. Subsequently, with the aid of a pre-trained diffusion model, these\nfeatures are reconstructed into visual stimuli. To verify the effectiveness of\nthe model, we conducted EEG-to-image generation and classification tasks using\nthe THINGS-EEG dataset. To address the limitations of quantitative analysis at\nthe semantic level, we combined WordNet-based classification and semantic\nsimilarity metrics to propose a novel semantic-based score, emphasizing the\nability of our model to transfer neural activities into visual representations.\nExperimental results show that our model significantly improves semantic\nalignment and classification accuracy, which achieves a maximum single-subject\naccuracy of 43\\%, outperforming other state-of-the-art methods. The source code\nand supplementary material is available at\nhttps://github.com/zes0v0inn/DWT_EEG_Reconstruction/tree/main."}
{"id": "2505.23813", "pdf": "https://arxiv.org/pdf/2505.23813", "abs": "https://arxiv.org/abs/2505.23813", "authors": ["Abhijit Talluri"], "title": "DP-RTFL: Differentially Private Resilient Temporal Federated Learning for Trustworthy AI in Regulated Industries", "categories": ["cs.CR", "cs.AI", "Computing methodologies~Machine learning~Machine learning\n  paradigms~Federated learning", "I.2.6; K.6.5; C.4"], "comment": "6 pages (IEEE conference format), 10 figures. Source code available\n  at https://github.com/abhitall/federated-credit-risk-rtfl.git", "summary": "Federated Learning (FL) has emerged as a critical paradigm for enabling\nprivacy-preserving machine learning, particularly in regulated sectors such as\nfinance and healthcare. However, standard FL strategies often encounter\nsignificant operational challenges related to fault tolerance, system\nresilience against concurrent client and server failures, and the provision of\nrobust, verifiable privacy guarantees essential for handling sensitive data.\nThese deficiencies can lead to training disruptions, data loss, compromised\nmodel integrity, and non-compliance with data protection regulations (e.g.,\nGDPR, CCPA). This paper introduces Differentially Private Resilient Temporal\nFederated Learning (DP-RTFL), an advanced FL framework designed to ensure\ntraining continuity, precise state recovery, and strong data privacy. DP-RTFL\nintegrates local Differential Privacy (LDP) at the client level with resilient\ntemporal state management and integrity verification mechanisms, such as\nhash-based commitments (referred to as Zero-Knowledge Integrity Proofs or ZKIPs\nin this context). The framework is particularly suited for critical\napplications like credit risk assessment using sensitive financial data, aiming\nto be operationally robust, auditable, and scalable for enterprise AI\ndeployments. The implementation of the DP-RTFL framework is available as\nopen-source."}
{"id": "2505.24088", "pdf": "https://arxiv.org/pdf/2505.24088", "abs": "https://arxiv.org/abs/2505.24088", "authors": ["Chen Huang", "Skyler Seto", "Hadi Pouransari", "Mehrdad Farajtabar", "Raviteja Vemulapalli", "Fartash Faghri", "Oncel Tuzel", "Barry-John Theobald", "Josh Susskind"], "title": "Proxy-FDA: Proxy-based Feature Distribution Alignment for Fine-tuning Vision Foundation Models without Forgetting", "categories": ["cs.LG", "cs.CV"], "comment": "ICML 2025", "summary": "Vision foundation models pre-trained on massive data encode rich\nrepresentations of real-world concepts, which can be adapted to downstream\ntasks by fine-tuning. However, fine-tuning foundation models on one task often\nleads to the issue of concept forgetting on other tasks. Recent methods of\nrobust fine-tuning aim to mitigate forgetting of prior knowledge without\naffecting the fine-tuning performance. Knowledge is often preserved by matching\nthe original and fine-tuned model weights or feature pairs. However, such\npoint-wise matching can be too strong, without explicit awareness of the\nfeature neighborhood structures that encode rich knowledge as well. We propose\na novel regularization method Proxy-FDA that explicitly preserves the\nstructural knowledge in feature space. Proxy-FDA performs Feature Distribution\nAlignment (using nearest neighbor graphs) between the pre-trained and\nfine-tuned feature spaces, and the alignment is further improved by informative\nproxies that are generated dynamically to increase data diversity. Experiments\nshow that Proxy-FDA significantly reduces concept forgetting during\nfine-tuning, and we find a strong correlation between forgetting and a\ndistributional distance metric (in comparison to L2 distance). We further\ndemonstrate Proxy-FDA's benefits in various fine-tuning settings (end-to-end,\nfew-shot and continual tuning) and across different tasks like image\nclassification, captioning and VQA."}
{"id": "2505.23867", "pdf": "https://arxiv.org/pdf/2505.23867", "abs": "https://arxiv.org/abs/2505.23867", "authors": ["Zeyu Liu", "Zhitian Hou", "Yining Di", "Kejing Yang", "Zhijie Sang", "Congkai Xie", "Jingwen Yang", "Siyuan Liu", "Jialu Wang", "Chunming Li", "Ming Li", "Hongxia Yang"], "title": "Infi-Med: Low-Resource Medical MLLMs with Robust Reasoning Evaluation", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Multimodal large language models (MLLMs) have demonstrated promising\nprospects in healthcare, particularly for addressing complex medical tasks,\nsupporting multidisciplinary treatment (MDT), and enabling personalized\nprecision medicine. However, their practical deployment faces critical\nchallenges in resource efficiency, diagnostic accuracy, clinical\nconsiderations, and ethical privacy. To address these limitations, we propose\nInfi-Med, a comprehensive framework for medical MLLMs that introduces three key\ninnovations: (1) a resource-efficient approach through curating and\nconstructing high-quality supervised fine-tuning (SFT) datasets with minimal\nsample requirements, with a forward-looking design that extends to both\npretraining and posttraining phases; (2) enhanced multimodal reasoning\ncapabilities for cross-modal integration and clinical task understanding; and\n(3) a systematic evaluation system that assesses model performance across\nmedical modalities and task types. Our experiments demonstrate that Infi-Med\nachieves state-of-the-art (SOTA) performance in general medical reasoning while\nmaintaining rapid adaptability to clinical scenarios. The framework establishes\na solid foundation for deploying MLLMs in real-world healthcare settings by\nbalancing model effectiveness with operational constraints."}
{"id": "2505.24310", "pdf": "https://arxiv.org/pdf/2505.24310", "abs": "https://arxiv.org/abs/2505.24310", "authors": ["Jiayan Li", "Jun Li", "Zhourui Zhang", "Jianhua Xu"], "title": "Progressive Class-level Distillation", "categories": ["cs.CV"], "comment": null, "summary": "In knowledge distillation (KD), logit distillation (LD) aims to transfer\nclass-level knowledge from a more powerful teacher network to a small student\nmodel via accurate teacher-student alignment at the logits level. Since\nhigh-confidence object classes usually dominate the distillation process,\nlow-probability classes which also contain discriminating information are\ndownplayed in conventional methods, leading to insufficient knowledge transfer.\nTo address this issue, we propose a simple yet effective LD method termed\nProgressive Class-level Distillation (PCD). In contrast to existing methods\nwhich perform all-class ensemble distillation, our PCD approach performs\nstage-wise distillation for step-by-step knowledge transfer. More specifically,\nwe perform ranking on teacher-student logits difference for identifying\ndistillation priority from scratch, and subsequently divide the entire LD\nprocess into multiple stages. Next, bidirectional stage-wise distillation\nincorporating fine-to-coarse progressive learning and reverse coarse-to-fine\nrefinement is conducted, allowing comprehensive knowledge transfer via\nsufficient logits alignment within separate class groups in different\ndistillation stages. Extension experiments on public benchmarking datasets\ndemonstrate the superiority of our method compared to state-of-the-arts for\nboth classification and detection tasks."}
{"id": "2505.23847", "pdf": "https://arxiv.org/pdf/2505.23847", "abs": "https://arxiv.org/abs/2505.23847", "authors": ["Ronny Ko", "Jiseong Jeong", "Shuyuan Zheng", "Chuan Xiao", "Taewan Kim", "Makoto Onizuka", "Wonyong Shin"], "title": "Seven Security Challenges That Must be Solved in Cross-domain Multi-agent LLM Systems", "categories": ["cs.CR", "cs.AI"], "comment": null, "summary": "Large language models (LLMs) are rapidly evolving into autonomous agents that\ncooperate across organizational boundaries, enabling joint disaster response,\nsupply-chain optimization, and other tasks that demand decentralized expertise\nwithout surrendering data ownership. Yet, cross-domain collaboration shatters\nthe unified trust assumptions behind current alignment and containment\ntechniques. An agent benign in isolation may, when receiving messages from an\nuntrusted peer, leak secrets or violate policy, producing risks driven by\nemergent multi-agent dynamics rather than classical software bugs. This\nposition paper maps the security agenda for cross-domain multi-agent LLM\nsystems. We introduce seven categories of novel security challenges, for each\nof which we also present plausible attacks, security evaluation metrics, and\nfuture research guidelines."}
{"id": "2505.24089", "pdf": "https://arxiv.org/pdf/2505.24089", "abs": "https://arxiv.org/abs/2505.24089", "authors": ["Marcus Lassila", "Johan Ãstman", "Khac-Hoang Ngo", "Alexandre Graell i Amat"], "title": "Practical Bayes-Optimal Membership Inference Attacks", "categories": ["cs.LG", "cs.CR"], "comment": "9 pages plus 13 pages of appendices", "summary": "We develop practical and theoretically grounded membership inference attacks\n(MIAs) against both independent and identically distributed (i.i.d.) data and\ngraph-structured data. Building on the Bayesian decision-theoretic framework of\nSablayrolles et al., we derive the Bayes-optimal membership inference rule for\nnode-level MIAs against graph neural networks, addressing key open questions\nabout optimal query strategies in the graph setting. We introduce BASE and\nG-BASE, computationally efficient approximations of the Bayes-optimal attack.\nG-BASE achieves superior performance compared to previously proposed\nclassifier-based node-level MIA attacks. BASE, which is also applicable to\nnon-graph data, matches or exceeds the performance of prior state-of-the-art\nMIAs, such as LiRA and RMIA, at a significantly lower computational cost.\nFinally, we show that BASE and RMIA are equivalent under a specific\nhyperparameter setting, providing a principled, Bayes-optimal justification for\nthe RMIA attack."}
{"id": "2505.23911", "pdf": "https://arxiv.org/pdf/2505.23911", "abs": "https://arxiv.org/abs/2505.23911", "authors": ["Pavel Tikhonov", "Ivan Oseledets", "Elena Tutubalina"], "title": "One Task Vector is not Enough: A Large-Scale Study for In-Context Learning", "categories": ["cs.CL"], "comment": null, "summary": "In-context learning (ICL) enables Large Language Models (LLMs) to adapt to\nnew tasks using few examples, with task vectors - specific hidden state\nactivations - hypothesized to encode task information. Existing studies are\nlimited by small-scale benchmarks, restricting comprehensive analysis. We\nintroduce QuiteAFew, a novel dataset of 3,096 diverse few-shot tasks, each with\n30 input-output pairs derived from the Alpaca dataset. Experiments with\nLlama-3-8B on QuiteAFew reveal: (1) task vector performance peaks at an\nintermediate layer (e.g., 15th), (2) effectiveness varies significantly by task\ntype, and (3) complex tasks rely on multiple, subtask-specific vectors rather\nthan a single vector, suggesting distributed task knowledge representation."}
{"id": "2505.24315", "pdf": "https://arxiv.org/pdf/2505.24315", "abs": "https://arxiv.org/abs/2505.24315", "authors": ["Jinlu Zhang", "Yixin Chen", "Zan Wang", "Jie Yang", "Yizhou Wang", "Siyuan Huang"], "title": "InteractAnything: Zero-shot Human Object Interaction Synthesis via LLM Feedback and Object Affordance Parsing", "categories": ["cs.CV"], "comment": "CVPR 2025", "summary": "Recent advances in 3D human-aware generation have made significant progress.\nHowever, existing methods still struggle with generating novel Human Object\nInteraction (HOI) from text, particularly for open-set objects. We identify\nthree main challenges of this task: precise human-object relation reasoning,\naffordance parsing for any object, and detailed human interaction pose\nsynthesis aligning description and object geometry. In this work, we propose a\nnovel zero-shot 3D HOI generation framework without training on specific\ndatasets, leveraging the knowledge from large-scale pre-trained models.\nSpecifically, the human-object relations are inferred from large language\nmodels (LLMs) to initialize object properties and guide the optimization\nprocess. Then we utilize a pre-trained 2D image diffusion model to parse unseen\nobjects and extract contact points, avoiding the limitations imposed by\nexisting 3D asset knowledge. The initial human pose is generated by sampling\nmultiple hypotheses through multi-view SDS based on the input text and object\ngeometry. Finally, we introduce a detailed optimization to generate\nfine-grained, precise, and natural interaction, enforcing realistic 3D contact\nbetween the 3D object and the involved body parts, including hands in grasping.\nThis is achieved by distilling human-level feedback from LLMs to capture\ndetailed human-object relations from the text instruction. Extensive\nexperiments validate the effectiveness of our approach compared to prior works,\nparticularly in terms of the fine-grained nature of interactions and the\nability to handle open-set 3D objects."}
{"id": "2505.23849", "pdf": "https://arxiv.org/pdf/2505.23849", "abs": "https://arxiv.org/abs/2505.23849", "authors": ["Kaveen Hiniduma", "Zilinghan Li", "Aditya Sinha", "Ravi Madduri", "Suren Byna"], "title": "CADRE: Customizable Assurance of Data Readiness in Privacy-Preserving Federated Learning", "categories": ["cs.CR", "cs.AI", "cs.LG"], "comment": "10 pages, 7 figures, 2 tables", "summary": "Privacy-Preserving Federated Learning (PPFL) is a decentralized machine\nlearning approach where multiple clients train a model collaboratively. PPFL\npreserves privacy and security of the client's data by not exchanging it.\nHowever, ensuring that data at each client is of high quality and ready for\nfederated learning (FL) is a challenge due to restricted data access. In this\npaper, we introduce CADRE (Customizable Assurance of Data REadiness) for FL, a\nnovel framework that allows users to define custom data readiness (DR)\nstandards, metrics, rules, and remedies tailored to specific FL tasks. Our\nframework generates comprehensive DR reports based on the user-defined metrics,\nrules, and remedies to ensure datasets are optimally prepared for FL while\npreserving privacy. We demonstrate the framework's practical application by\nintegrating it into an existing PPFL framework. We conducted experiments across\nsix diverse datasets, addressing seven different DR issues. The results\nillustrate the framework's versatility and effectiveness in ensuring DR across\nvarious dimensions, including data quality, privacy, and fairness. This\napproach enhances the performance and reliability of FL models as well as\nutilizes valuable resources by identifying and addressing data-related issues\nbefore the training phase."}
{"id": "2505.24101", "pdf": "https://arxiv.org/pdf/2505.24101", "abs": "https://arxiv.org/abs/2505.24101", "authors": ["Zhenran Xu"], "title": "A SHAP-based explainable multi-level stacking ensemble learning method for predicting the length of stay in acute stroke", "categories": ["cs.LG"], "comment": "Master Minor Thesis, Preprint", "summary": "Length of stay (LOS) prediction in acute stroke is critical for improving\ncare planning. Existing machine learning models have shown suboptimal\npredictive performance, limited generalisability, and have overlooked\nsystem-level factors. We aimed to enhance model efficiency, performance, and\ninterpretability by refining predictors and developing an interpretable\nmulti-level stacking ensemble model. Data were accessed from the biennial\nStroke Foundation Acute Audit (2015, 2017, 2019, 2021) in Australia. Models\nwere developed for ischaemic and haemorrhagic stroke separately. The outcome\nwas prolonged LOS (the LOS above the 75th percentile). Candidate predictors\n(ischaemic: n=89; haemorrhagic: n=83) were categorised into patient, clinical,\nand system domains. Feature selection with correlation-based approaches was\nused to refine key predictors. The evaluation of models included discrimination\n(AUC), calibration curves, and interpretability (SHAP plots). In ischaemic\nstroke (N=12,575), prolonged LOS was >=9 days, compared to >=11 days in\nhaemorrhagic stroke (N=1,970). The ensemble model achieved superior performance\n[AUC: 0.824 (95% CI: 0.801-0.846)] and statistically outperformed logistic\nregression [AUC: 0.805 (95% CI: 0.782-0.829); P=0.0004] for ischaemic. However,\nthe model [AUC: 0.843 (95% CI: 0.790-0.895)] did not statistically outperform\nlogistic regression [AUC: 0.828 (95% CI: 0.774-0.882); P=0.136] for\nhaemorrhagic. SHAP analysis identified shared predictors for both types of\nstroke: rehabilitation assessment, urinary incontinence, stroke unit care,\ninability to walk independently, physiotherapy, and stroke care coordinators\ninvolvement. An explainable ensemble model effectively predicted the prolonged\nLOS in ischaemic stroke. Further validation in larger cohorts is needed for\nhaemorrhagic stroke."}
{"id": "2505.23912", "pdf": "https://arxiv.org/pdf/2505.23912", "abs": "https://arxiv.org/abs/2505.23912", "authors": ["Caiqi Zhang", "Xiaochen Zhu", "Chengzu Li", "Nigel Collier", "Andreas Vlachos"], "title": "Reinforcement Learning for Better Verbalized Confidence in Long-Form Generation", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Hallucination remains a major challenge for the safe and trustworthy\ndeployment of large language models (LLMs) in factual content generation. Prior\nwork has explored confidence estimation as an effective approach to\nhallucination detection, but often relies on post-hoc self-consistency methods\nthat require computationally expensive sampling. Verbalized confidence offers a\nmore efficient alternative, but existing approaches are largely limited to\nshort-form question answering (QA) tasks and do not generalize well to\nopen-ended generation. In this paper, we propose LoVeC (Long-form Verbalized\nConfidence), an on-the-fly verbalized confidence estimation method for\nlong-form generation. Specifically, we use reinforcement learning (RL) to train\nLLMs to append numerical confidence scores to each generated statement, serving\nas a direct and interpretable signal of the factuality of generation. Our\nexperiments consider both on-policy and off-policy RL methods, including DPO,\nORPO, and GRPO, to enhance the model calibration. We introduce two novel\nevaluation settings, free-form tagging and iterative tagging, to assess\ndifferent verbalized confidence estimation methods. Experiments on three\nlong-form QA datasets show that our RL-trained models achieve better\ncalibration and generalize robustly across domains. Also, our method is highly\nefficient, as it only requires adding a few tokens to the output being decoded."}
{"id": "2505.24327", "pdf": "https://arxiv.org/pdf/2505.24327", "abs": "https://arxiv.org/abs/2505.24327", "authors": ["Jingjing Liu", "Jiashun Jin", "Xianchao Xiu", "Jianhua Zhang", "Wanquan Liu"], "title": "STAR-Net: An Interpretable Model-Aided Network for Remote Sensing Image Denoising", "categories": ["cs.CV", "cs.LG"], "comment": null, "summary": "Remote sensing image (RSI) denoising is an important topic in the field of\nremote sensing. Despite the impressive denoising performance of RSI denoising\nmethods, most current deep learning-based approaches function as black boxes\nand lack integration with physical information models, leading to limited\ninterpretability. Additionally, many methods may struggle with insufficient\nattention to non-local self-similarity in RSI and require tedious tuning of\nregularization parameters to achieve optimal performance, particularly in\nconventional iterative optimization approaches. In this paper, we first propose\na novel RSI denoising method named sparse tensor-aided representation network\n(STAR-Net), which leverages a low-rank prior to effectively capture the\nnon-local self-similarity within RSI. Furthermore, we extend STAR-Net to a\nsparse variant called STAR-Net-S to deal with the interference caused by\nnon-Gaussian noise in original RSI for the purpose of improving robustness.\nDifferent from conventional iterative optimization, we develop an alternating\ndirection method of multipliers (ADMM)-guided deep unrolling network, in which\nall regularization parameters can be automatically learned, thus inheriting the\nadvantages of both model-based and deep learning-based approaches and\nsuccessfully addressing the above-mentioned shortcomings. Comprehensive\nexperiments on synthetic and real-world datasets demonstrate that STAR-Net and\nSTAR-Net-S outperform state-of-the-art RSI denoising methods."}
{"id": "2505.23860", "pdf": "https://arxiv.org/pdf/2505.23860", "abs": "https://arxiv.org/abs/2505.23860", "authors": ["Giovanni Acampora", "Andris Ambainis", "Natalia Ares", "Leonardo Banchi", "Pallavi Bhardwaj", "Daniele Binosi", "G. Andrew D. Briggs", "Tommaso Calarco", "Vedran Dunjko", "Jens Eisert", "Olivier Ezratty", "Paul Erker", "Federico Fedele", "Elies Gil-Fuster", "Martin GÃ¤rttner", "Mats Granath", "Markus Heyl", "Iordanis Kerenidis", "Matthias Klusch", "Anton Frisk Kockum", "Richard Kueng", "Mario Krenn", "JÃ¶rg LÃ¤ssig", "Antonio Macaluso", "Sabrina Maniscalco", "Florian Marquardt", "Kristel Michielsen", "Gorka MuÃ±oz-Gil", "Daniel MÃ¼ssig", "Hendrik Poulsen Nautrup", "Evert van Nieuwenburg", "Roman Orus", "JÃ¶rg Schmiedmayer", "Markus Schmitt", "Philipp Slusallek", "Filippo Vicentini", "Christof Weitenberg", "Frank K. Wilhelm"], "title": "Quantum computing and artificial intelligence: status and perspectives", "categories": ["quant-ph", "cs.AI", "cs.LG"], "comment": "32 pages, 3 figures", "summary": "This white paper discusses and explores the various points of intersection\nbetween quantum computing and artificial intelligence (AI). It describes how\nquantum computing could support the development of innovative AI solutions. It\nalso examines use cases of classical AI that can empower research and\ndevelopment in quantum technologies, with a focus on quantum computing and\nquantum sensing. The purpose of this white paper is to provide a long-term\nresearch agenda aimed at addressing foundational questions about how AI and\nquantum computing interact and benefit one another. It concludes with a set of\nrecommendations and challenges, including how to orchestrate the proposed\ntheoretical work, align quantum AI developments with quantum hardware roadmaps,\nestimate both classical and quantum resources - especially with the goal of\nmitigating and optimizing energy consumption - advance this emerging hybrid\nsoftware engineering discipline, and enhance European industrial\ncompetitiveness while considering societal implications."}
{"id": "2505.24110", "pdf": "https://arxiv.org/pdf/2505.24110", "abs": "https://arxiv.org/abs/2505.24110", "authors": ["Sahil Rajesh Dhayalkar"], "title": "Neural Networks as Universal Finite-State Machines: A Constructive ReLU Simulation Framework for NFAs", "categories": ["cs.LG", "cs.FL"], "comment": "16 pages, with proofs in Appendix", "summary": "We present a formal and constructive framework establishing the equivalence\nbetween nondeterministic finite automata (NFAs) and standard feedforward ReLU\nneural networks. By encoding automaton states as binary vectors and transitions\nas sparse linear layers, we show that ReLU activations simulate\nnondeterministic branching, subset construction, and $\\epsilon$-closures in a\nmathematically precise manner. Our core theoretical results prove that a\nthree-layer ReLU network of width $\\mathcal{O}(n)$ can exactly recognize any\nregular language accepted by an $n$-state NFA-without recurrence, memory, or\napproximation. Furthermore, we show that gradient descent over\nstructure-preserving networks preserves symbolic semantics and acceptance\nbehavior. Extensive experiments across multiple validation tasks-including\nparallel path tracking, symbolic subset construction, $\\epsilon$-closure\nconvergence, acceptance classification, structural training invariants, and\nfunctional equivalence-achieve perfect or near-perfect empirical alignment with\nground-truth automata. This work provides the first provably complete symbolic\nsimulation of NFAs within standard deep learning architectures, uniting\nautomata theory with neural computation through ReLU dynamics."}
{"id": "2505.23914", "pdf": "https://arxiv.org/pdf/2505.23914", "abs": "https://arxiv.org/abs/2505.23914", "authors": ["Yuxin Wang", "Botao Yu", "Ivory Yang", "Saeed Hassanpour", "Soroush Vosoughi"], "title": "Probing Association Biases in LLM Moderation Over-Sensitivity", "categories": ["cs.CL", "cs.AI"], "comment": "Under review", "summary": "Large Language Models are widely used for content moderation but often\nmisclassify benign comments as toxic, leading to over-sensitivity. While\nprevious research attributes this issue primarily to the presence of offensive\nterms, we reveal a potential cause beyond token level: LLMs exhibit systematic\ntopic biases in their implicit associations. Inspired by cognitive psychology's\nimplicit association tests, we introduce Topic Association Analysis, a\nsemantic-level approach to quantify how LLMs associate certain topics with\ntoxicity. By prompting LLMs to generate free-form scenario imagination for\nmisclassified benign comments and analyzing their topic amplification levels,\nwe find that more advanced models (e.g., GPT-4 Turbo) demonstrate stronger\ntopic stereotype despite lower overall false positive rates. These biases\nsuggest that LLMs do not merely react to explicit, offensive language but rely\non learned topic associations, shaping their moderation decisions. Our findings\nhighlight the need for refinement beyond keyword-based filtering, providing\ninsights into the underlying mechanisms driving LLM over-sensitivity."}
{"id": "2505.24329", "pdf": "https://arxiv.org/pdf/2505.24329", "abs": "https://arxiv.org/abs/2505.24329", "authors": ["Yingsen Zeng", "Zepeng Huang", "Yujie Zhong", "Chengjian Feng", "Jie Hu", "Lin Ma", "Yang Liu"], "title": "DisTime: Distribution-based Time Representation for Video Large Language Models", "categories": ["cs.CV"], "comment": null, "summary": "Despite advances in general video understanding, Video Large Language Models\n(Video-LLMs) face challenges in precise temporal localization due to discrete\ntime representations and limited temporally aware datasets. Existing methods\nfor temporal expression either conflate time with text-based numerical values,\nadd a series of dedicated temporal tokens, or regress time using specialized\ntemporal grounding heads. To address these issues, we introduce DisTime, a\nlightweight framework designed to enhance temporal comprehension in Video-LLMs.\nDisTime employs a learnable token to create a continuous temporal embedding\nspace and incorporates a Distribution-based Time Decoder that generates\ntemporal probability distributions, effectively mitigating boundary ambiguities\nand maintaining temporal continuity. Additionally, the Distribution-based Time\nEncoder re-encodes timestamps to provide time markers for Video-LLMs. To\novercome temporal granularity limitations in existing datasets, we propose an\nautomated annotation paradigm that combines the captioning capabilities of\nVideo-LLMs with the localization expertise of dedicated temporal models. This\nleads to the creation of InternVid-TG, a substantial dataset with 1.25M\ntemporally grounded events across 179k videos, surpassing ActivityNet-Caption\nby 55 times. Extensive experiments demonstrate that DisTime achieves\nstate-of-the-art performance across benchmarks in three time-sensitive tasks\nwhile maintaining competitive performance in Video QA tasks. Code and data are\nreleased at https://github.com/josephzpng/DisTime."}
{"id": "2505.23862", "pdf": "https://arxiv.org/pdf/2505.23862", "abs": "https://arxiv.org/abs/2505.23862", "authors": ["Zheng Gong", "Ziyi Jiang", "Weihao Gao", "Deng Zhuo", "Lan Ma"], "title": "A New Deep-learning-Based Approach For mRNA Optimization: High Fidelity, Computation Efficiency, and Multiple Optimization Factors", "categories": ["q-bio.QM", "cs.AI", "cs.LG"], "comment": "You can also contact hudenjear@gmail.com for more information", "summary": "The mRNA optimization is critical for therapeutic and biotechnological\napplications, since sequence features directly govern protein expression levels\nand efficacy. However, current methods face significant challenges in\nsimultaneously achieving three key objectives: (1) fidelity (preventing\nunintended amino acid changes), (2) computational efficiency (speed and\nscalability), and (3) the scope of optimization variables considered\n(multi-objective capability). Furthermore, existing methods often fall short of\ncomprehensively incorporating the factors related to the mRNA lifecycle and\ntranslation process, including intrinsic mRNA sequence properties, secondary\nstructure, translation elongation kinetics, and tRNA availability. To address\nthese limitations, we introduce \\textbf{RNop}, a novel deep learning-based\nmethod for mRNA optimization. We collect a large-scale dataset containing over\n3 million sequences and design four specialized loss functions, the GPLoss,\nCAILoss, tAILoss, and MFELoss, which simultaneously enable explicit control\nover sequence fidelity while optimizing species-specific codon adaptation, tRNA\navailability, and desirable mRNA secondary structure features. Then, we\ndemonstrate RNop's effectiveness through extensive in silico and in vivo\nexperiments. RNop ensures high sequence fidelity, achieves significant\ncomputational throughput up to 47.32 sequences/s, and yields optimized mRNA\nsequences resulting in a significant increase in protein expression for\nfunctional proteins compared to controls. RNop surpasses current methodologies\nin both quantitative metrics and experimental validation, enlightening a new\ndawn for efficient and effective mRNA design. Code and models will be available\nat https://github.com/HudenJear/RPLoss."}
{"id": "2505.24138", "pdf": "https://arxiv.org/pdf/2505.24138", "abs": "https://arxiv.org/abs/2505.24138", "authors": ["Yichen Shi", "Ze Zhang", "Hongyang Wang", "Zhuofu Tao", "Zhongyi Li", "Bingyu Chen", "Yaxin Wang", "Zhiping Yu", "Ting-Jung Lin", "Lei He"], "title": "AMSbench: A Comprehensive Benchmark for Evaluating MLLM Capabilities in AMS Circuits", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Analog/Mixed-Signal (AMS) circuits play a critical role in the integrated\ncircuit (IC) industry. However, automating Analog/Mixed-Signal (AMS) circuit\ndesign has remained a longstanding challenge due to its difficulty and\ncomplexity. Recent advances in Multi-modal Large Language Models (MLLMs) offer\npromising potential for supporting AMS circuit analysis and design. However,\ncurrent research typically evaluates MLLMs on isolated tasks within the domain,\nlacking a comprehensive benchmark that systematically assesses model\ncapabilities across diverse AMS-related challenges. To address this gap, we\nintroduce AMSbench, a benchmark suite designed to evaluate MLLM performance\nacross critical tasks including circuit schematic perception, circuit analysis,\nand circuit design. AMSbench comprises approximately 8000 test questions\nspanning multiple difficulty levels and assesses eight prominent models,\nencompassing both open-source and proprietary solutions such as Qwen 2.5-VL and\nGemini 2.5 Pro. Our evaluation highlights significant limitations in current\nMLLMs, particularly in complex multi-modal reasoning and sophisticated circuit\ndesign tasks. These results underscore the necessity of advancing MLLMs'\nunderstanding and effective application of circuit-specific knowledge, thereby\nnarrowing the existing performance gap relative to human expertise and moving\ntoward fully automated AMS circuit design workflows. Our data is released at\nhttps://huggingface.co/datasets/wwhhyy/AMSBench"}
{"id": "2505.23923", "pdf": "https://arxiv.org/pdf/2505.23923", "abs": "https://arxiv.org/abs/2505.23923", "authors": ["Feiteng Fang", "Ting-En Lin", "Yuchuan Wu", "Xiong Liu", "Xiang Huang", "Dingwei Chen", "Jing Ye", "Haonan Zhang", "Liang Zhu", "Hamid Alinejad-Rokny", "Min Yang", "Fei Huang", "Yongbin Li"], "title": "ChARM: Character-based Act-adaptive Reward Modeling for Advanced Role-Playing Language Agents", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Role-Playing Language Agents (RPLAs) aim to simulate characters for realistic\nand engaging human-computer interactions. However, traditional reward models\noften struggle with scalability and adapting to subjective conversational\npreferences. We propose ChARM, a Character-based Act-adaptive Reward Model,\naddressing these challenges through two innovations: (1) an act-adaptive margin\nthat significantly enhances learning efficiency and generalizability, and (2) a\nself-evolution mechanism leveraging large-scale unlabeled data to improve\ntraining coverage. Additionally, we introduce RoleplayPref, the first\nlarge-scale preference dataset specifically for RPLAs, featuring 1,108\ncharacters, 13 subcategories, and 16,888 bilingual dialogues, alongside\nRoleplayEval, a dedicated evaluation benchmark. Experimental results show a 13%\nimprovement over the conventional Bradley-Terry model in preference rankings.\nFurthermore, applying ChARM-generated rewards to preference learning techniques\n(e.g., direct preference optimization) achieves state-of-the-art results on\nCharacterEval and RoleplayEval. Code and dataset are available at\nhttps://github.com/calubkk/ChARM."}
{"id": "2505.24334", "pdf": "https://arxiv.org/pdf/2505.24334", "abs": "https://arxiv.org/abs/2505.24334", "authors": ["Uzair Khan", "Franco Fummi", "Luigi Capogrosso"], "title": "KairosAD: A SAM-Based Model for Industrial Anomaly Detection on Embedded Devices", "categories": ["cs.CV"], "comment": "Accepted at the 23rd International Conference on Image Analysis and\n  Processing (ICIAP 2025)", "summary": "In the era of intelligent manufacturing, anomaly detection has become\nessential for maintaining quality control on modern production lines. However,\nwhile many existing models show promising performance, they are often too\nlarge, computationally demanding, and impractical to deploy on\nresource-constrained embedded devices that can be easily installed on the\nproduction lines of Small and Medium Enterprises (SMEs). To bridge this gap, we\npresent KairosAD, a novel supervised approach that uses the power of the Mobile\nSegment Anything Model (MobileSAM) for image-based anomaly detection. KairosAD\nhas been evaluated on the two well-known industrial anomaly detection datasets,\ni.e., MVTec-AD and ViSA. The results show that KairosAD requires 78% fewer\nparameters and boasts a 4x faster inference time compared to the leading\nstate-of-the-art model, while maintaining comparable AUROC performance. We\ndeployed KairosAD on two embedded devices, the NVIDIA Jetson NX, and the NVIDIA\nJetson AGX. Finally, KairosAD was successfully installed and tested on the real\nproduction line of the Industrial Computer Engineering Laboratory (ICE Lab) at\nthe University of Verona. The code is available at\nhttps://github.com/intelligolabs/KairosAD."}
{"id": "2505.23873", "pdf": "https://arxiv.org/pdf/2505.23873", "abs": "https://arxiv.org/abs/2505.23873", "authors": ["Hongrui Peng", "Haolang Lu", "Yuanlong Yu", "Weiye Fu", "Kun Wang", "Guoshun Nan"], "title": "KGMark: A Diffusion Watermark for Knowledge Graphs", "categories": ["cs.CR", "cs.AI", "68T07", "I.2.8"], "comment": "20pages, 6figures", "summary": "Knowledge graphs (KGs) are ubiquitous in numerous real-world applications,\nand watermarking facilitates protecting intellectual property and preventing\npotential harm from AI-generated content. Existing watermarking methods mainly\nfocus on static plain text or image data, while they can hardly be applied to\ndynamic graphs due to spatial and temporal variations of structured data. This\nmotivates us to propose KGMARK, the first graph watermarking framework that\naims to generate robust, detectable, and transparent diffusion fingerprints for\ndynamic KG data. Specifically, we propose a novel clustering-based alignment\nmethod to adapt the watermark to spatial variations. Meanwhile, we present a\nredundant embedding strategy to harden the diffusion watermark against various\nattacks, facilitating the robustness of the watermark to the temporal\nvariations. Additionally, we introduce a novel learnable mask matrix to improve\nthe transparency of diffusion fingerprints. By doing so, our KGMARK properly\ntackles the variation challenges of structured data. Experiments on various\npublic benchmarks show the effectiveness of our proposed KGMARK."}
{"id": "2505.24145", "pdf": "https://arxiv.org/pdf/2505.24145", "abs": "https://arxiv.org/abs/2505.24145", "authors": ["Wilfried Genuist", "Ãric Savin", "Filippo Gatti", "Didier Clouteau"], "title": "Autoregressive regularized score-based diffusion models for multi-scenarios fluid flow prediction", "categories": ["cs.LG", "physics.flu-dyn", "65N75, 35Q30, 60H15, 76F55, 68T07", "I.2.6; G.1.7; I.6.1; I.6.3"], "comment": "34 pages, 17 figures", "summary": "Building on recent advances in scientific machine learning and generative\nmodeling for computational fluid dynamics, we propose a conditional score-based\ndiffusion model designed for multi-scenarios fluid flow prediction. Our model\nintegrates an energy constraint rooted in the statistical properties of\nturbulent flows, improving prediction quality with minimal training, while\nenabling efficient sampling at low cost. The method features a simple and\ngeneral architecture that requires no problem-specific design, supports\nplug-and-play enhancements, and enables fast and flexible solution generation.\nIt also demonstrates an efficient conditioning mechanism that simplifies\ntraining across different scenarios without demanding a redesign of existing\nmodels. We further explore various stochastic differential equation\nformulations to demonstrate how thoughtful design choices enhance performance.\nWe validate the proposed methodology through extensive experiments on complex\nfluid dynamics datasets encompassing a variety of flow regimes and\nconfigurations. Results demonstrate that our model consistently achieves\nstable, robust, and physically faithful predictions, even under challenging\nturbulent conditions. With properly tuned parameters, it achieves accurate\nresults across multiple scenarios while preserving key physical and statistical\nproperties. We present a comprehensive analysis of stochastic differential\nequation impact and discuss our approach across diverse fluid mechanics tasks."}
{"id": "2505.23931", "pdf": "https://arxiv.org/pdf/2505.23931", "abs": "https://arxiv.org/abs/2505.23931", "authors": ["Daniel Wurgaft", "Ben Prystawski", "Kanishk Gandhi", "Cedegao E. Zhang", "Joshua B. Tenenbaum", "Noah D. Goodman"], "title": "Scaling up the think-aloud method", "categories": ["cs.CL", "cs.AI"], "comment": "8 pages, 4 figures. Daniel Wurgaft and Ben Prystawski contributed\n  equally", "summary": "The think-aloud method, where participants voice their thoughts as they solve\na task, is a valuable source of rich data about human reasoning processes. Yet,\nit has declined in popularity in contemporary cognitive science, largely\nbecause labor-intensive transcription and annotation preclude large sample\nsizes. Here, we develop methods to automate the transcription and annotation of\nverbal reports of reasoning using natural language processing tools, allowing\nfor large-scale analysis of think-aloud data. In our study, 640 participants\nthought aloud while playing the Game of 24, a mathematical reasoning task. We\nautomatically transcribed the recordings and coded the transcripts as search\ngraphs, finding moderate inter-rater reliability with humans. We analyze these\ngraphs and characterize consistency and variation in human reasoning traces.\nOur work demonstrates the value of think-aloud data at scale and serves as a\nproof of concept for the automated analysis of verbal reports."}
{"id": "2505.24340", "pdf": "https://arxiv.org/pdf/2505.24340", "abs": "https://arxiv.org/abs/2505.24340", "authors": ["Gilles Quentin Hacheme", "Girmaw Abebe Tadesse", "Caleb Robinson", "Akram Zaytar", "Rahul Dodhia", "Juan M. Lavista Ferres"], "title": "GeoVision Labeler: Zero-Shot Geospatial Classification with Vision and Language Models", "categories": ["cs.CV", "cs.CL", "cs.LG", "I.2.10; I.2.7; I.4.8; I.5.3"], "comment": null, "summary": "Classifying geospatial imagery remains a major bottleneck for applications\nsuch as disaster response and land-use monitoring-particularly in regions where\nannotated data is scarce or unavailable. Existing tools (e.g., RS-CLIP) that\nclaim zero-shot classification capabilities for satellite imagery nonetheless\nrely on task-specific pretraining and adaptation to reach competitive\nperformance. We introduce GeoVision Labeler (GVL), a strictly zero-shot\nclassification framework: a vision Large Language Model (vLLM) generates rich,\nhuman-readable image descriptions, which are then mapped to user-defined\nclasses by a conventional Large Language Model (LLM). This modular, and\ninterpretable pipeline enables flexible image classification for a large range\nof use cases. We evaluated GVL across three benchmarks-SpaceNet v7, UC Merced,\nand RESISC45. It achieves up to 93.2% zero-shot accuracy on the binary\nBuildings vs. No Buildings task on SpaceNet v7. For complex multi-class\nclassification tasks (UC Merced, RESISC45), we implemented a recursive\nLLM-driven clustering to form meta-classes at successive depths, followed by\nhierarchical classification-first resolving coarse groups, then finer\ndistinctions-to deliver competitive zero-shot performance. GVL is open-sourced\nat https://github.com/microsoft/geo-vision-labeler to catalyze adoption in\nreal-world geospatial workflows."}
{"id": "2505.24149", "pdf": "https://arxiv.org/pdf/2505.24149", "abs": "https://arxiv.org/abs/2505.24149", "authors": ["Adam Piaseczny", "Md Kamran Chowdhury Shisher", "Shiqiang Wang", "Christopher G. Brinton"], "title": "RCCDA: Adaptive Model Updates in the Presence of Concept Drift under a Constrained Resource Budget", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Machine learning (ML) algorithms deployed in real-world environments are\noften faced with the challenge of adapting models to concept drift, where the\ntask data distributions are shifting over time. The problem becomes even more\ndifficult when model performance must be maintained under adherence to strict\nresource constraints. Existing solutions often depend on drift-detection\nmethods that produce high computational overhead for resource-constrained\nenvironments, and fail to provide strict guarantees on resource usage or\ntheoretical performance assurances. To address these shortcomings, we propose\nRCCDA: a dynamic model update policy that optimizes ML training dynamics while\nensuring strict compliance to predefined resource constraints, utilizing only\npast loss information and a tunable drift threshold. In developing our policy,\nwe analytically characterize the evolution of model loss under concept drift\nwith arbitrary training update decisions. Integrating these results into a\nLyapunov drift-plus-penalty framework produces a lightweight policy based on a\nmeasurable accumulated loss threshold that provably limits update frequency and\ncost. Experimental results on three domain generalization datasets demonstrate\nthat our policy outperforms baseline methods in inference accuracy while\nadhering to strict resource constraints under several schedules of concept\ndrift, making our solution uniquely suited for real-time ML deployments."}
{"id": "2505.23932", "pdf": "https://arxiv.org/pdf/2505.23932", "abs": "https://arxiv.org/abs/2505.23932", "authors": ["Wendong Xu", "Jing Xiong", "Chenyang Zhao", "Qiujiang Chen", "Haoran Wang", "Hui Shen", "Zhongwei Wan", "Jianbo Dai", "Taiqiang Wu", "He Xiao", "Chaofan Tao", "Z. Morley Mao", "Ying Sheng", "Zhijiang Guo", "Hongxia Yang", "Bei Yu", "Lingpeng Kong", "Quanquan Gu", "Ngai Wong"], "title": "SwingArena: Competitive Programming Arena for Long-context GitHub Issue Solving", "categories": ["cs.CL"], "comment": null, "summary": "We present SwingArena, a competitive evaluation framework for Large Language\nModels (LLMs) that closely mirrors real-world software development workflows.\nUnlike traditional static benchmarks, SwingArena models the collaborative\nprocess of software iteration by pairing LLMs as submitters, who generate\npatches, and reviewers, who create test cases and verify the patches through\ncontinuous integration (CI) pipelines. To support these interactive\nevaluations, we introduce a retrieval-augmented code generation (RACG) module\nthat efficiently handles long-context challenges by providing syntactically and\nsemantically relevant code snippets from large codebases, supporting multiple\nprogramming languages (C++, Python, Rust, and Go). This enables the framework\nto scale across diverse tasks and contexts while respecting token limitations.\nOur experiments, using over 400 high-quality real-world GitHub issues selected\nfrom a pool of 2,300 issues, show that models like GPT-4o excel at aggressive\npatch generation, whereas DeepSeek and Gemini prioritize correctness in CI\nvalidation. SwingArena presents a scalable and extensible methodology for\nevaluating LLMs in realistic, CI-driven software development settings. More\ndetails are available on our project page: swing-bench.github.io"}
{"id": "2505.24342", "pdf": "https://arxiv.org/pdf/2505.24342", "abs": "https://arxiv.org/abs/2505.24342", "authors": ["Fanhang Man", "Xiaoyue Chen", "Huandong Wang", "Baining Zhao", "Han Li", "Xinlei Chen", "Yong Li"], "title": "KEVER^2: Knowledge-Enhanced Visual Emotion Reasoning and Retrieval", "categories": ["cs.CV", "cs.CL"], "comment": null, "summary": "Understanding what emotions images evoke in their viewers is a foundational\ngoal in human-centric visual computing. While recent advances in\nvision-language models (VLMs) have shown promise for visual emotion analysis\n(VEA), several key challenges remain unresolved. Emotional cues in images are\noften abstract, overlapping, and entangled, making them difficult to model and\ninterpret. Moreover, VLMs struggle to align these complex visual patterns with\nemotional semantics due to limited supervision and sparse emotional grounding.\nFinally, existing approaches lack structured affective knowledge to resolve\nambiguity and ensure consistent emotional reasoning across diverse visual\ndomains.\n  To address these limitations, we propose \\textbf{K-EVER\\textsuperscript{2}},\na knowledge-enhanced framework for emotion reasoning and retrieval. Our\napproach introduces a semantically structured formulation of visual emotion\ncues and integrates external affective knowledge through multimodal alignment.\nWithout relying on handcrafted labels or direct emotion supervision,\nK-EVER\\textsuperscript{2} achieves robust and interpretable emotion predictions\nacross heterogeneous image types.\n  We validate our framework on three representative benchmarks, Emotion6,\nEmoSet, and M-Disaster, covering social media imagery, human-centric scenes,\nand disaster contexts. K-EVER\\textsuperscript{2} consistently outperforms\nstrong CNN and VLM baselines, achieving up to a \\textbf{19\\% accuracy gain} for\nspecific emotions and a \\textbf{12.3\\% average accuracy gain} across all\nemotion categories. Our results demonstrate a scalable and generalizable\nsolution for advancing emotional understanding of visual content."}
{"id": "2505.23930", "pdf": "https://arxiv.org/pdf/2505.23930", "abs": "https://arxiv.org/abs/2505.23930", "authors": ["Naomi Omeonga wa Kayembe"], "title": "Exploring Societal Concerns and Perceptions of AI: A Thematic Analysis through the Lens of Problem-Seeking", "categories": ["cs.CY", "cs.AI"], "comment": "48 pages", "summary": "This study introduces a novel conceptual framework distinguishing\nproblem-seeking from problem-solving to clarify the unique features of human\nintelligence in contrast to AI. Problem-seeking refers to the embodied,\nemotionally grounded process by which humans identify and set goals, while\nproblem-solving denotes the execution of strategies aimed at achieving such\npredefined objectives. The framework emphasizes that while AI excels at\nefficiency and optimization, it lacks the orientation derived from experiential\ngrounding and the embodiment flexibility intrinsic to human cognition. To\nempirically explore this distinction, the research analyzes metadata from 157\nYouTube videos discussing AI. Conducting a thematic analysis combining\nqualitative insights with keyword-based quantitative metrics, this\nmixed-methods approach uncovers recurring themes in public discourse, including\nprivacy, job displacement, misinformation, optimism, and ethical concerns. The\nresults reveal a dual sentiment: public fascination with AI's capabilities\ncoexists with anxiety and skepticism about its societal implications. The\ndiscussion critiques the orthogonality thesis, which posits that intelligence\nis separable from goal content, and instead argues that human intelligence\nintegrates goal-setting and goal-pursuit. It underscores the centrality of\nembodied cognition in human reasoning and highlights how AI's limitations come\nfrom its current reliance on computational processing. The study advocates for\nenhancing emotional and digital literacy to foster responsible AI engagement.\nIt calls for reframing public discourse to recognize AI as a tool that augments\n-- rather than replaces -- human intelligence. By positioning problem seeking\nat the core of cognition and as a critical dimension of intelligence, this\nresearch offers new perspectives on ethically aligned and human-centered AI\ndevelopment."}
{"id": "2505.24155", "pdf": "https://arxiv.org/pdf/2505.24155", "abs": "https://arxiv.org/abs/2505.24155", "authors": ["Ehtesamul Azim", "Dongjie Wang", "Tae Hyun Hwang", "Yanjie Fu", "Wei Zhang"], "title": "Biological Pathway Guided Gene Selection Through Collaborative Reinforcement Learning", "categories": ["cs.LG"], "comment": "31st SIGKDD Conference on Knowledge Discovery and Data Mining (KDD\n  2025)", "summary": "Gene selection in high-dimensional genomic data is essential for\nunderstanding disease mechanisms and improving therapeutic outcomes.\nTraditional feature selection methods effectively identify predictive genes but\noften ignore complex biological pathways and regulatory networks, leading to\nunstable and biologically irrelevant signatures. Prior approaches, such as\nLasso-based methods and statistical filtering, either focus solely on\nindividual gene-outcome associations or fail to capture pathway-level\ninteractions, presenting a key challenge: how to integrate biological pathway\nknowledge while maintaining statistical rigor in gene selection? To address\nthis gap, we propose a novel two-stage framework that integrates statistical\nselection with biological pathway knowledge using multi-agent reinforcement\nlearning (MARL). First, we introduce a pathway-guided pre-filtering strategy\nthat leverages multiple statistical methods alongside KEGG pathway information\nfor initial dimensionality reduction. Next, for refined selection, we model\ngenes as collaborative agents in a MARL framework, where each agent optimizes\nboth predictive power and biological relevance. Our framework incorporates\npathway knowledge through Graph Neural Network-based state representations, a\nreward mechanism combining prediction performance with gene centrality and\npathway coverage, and collaborative learning strategies using shared memory and\na centralized critic component. Extensive experiments on multiple gene\nexpression datasets demonstrate that our approach significantly improves both\nprediction accuracy and biological interpretability compared to traditional\nmethods."}
{"id": "2505.23944", "pdf": "https://arxiv.org/pdf/2505.23944", "abs": "https://arxiv.org/abs/2505.23944", "authors": ["Thushara Manjari Naduvilakandy", "Hyeju Jang", "Mohammad Al Hasan"], "title": "Retrieval Augmented Generation based Large Language Models for Causality Mining", "categories": ["cs.CL"], "comment": "13 pages, 6 figures, published in knowledgeNLP-NAACL2025", "summary": "Causality detection and mining are important tasks in information retrieval\ndue to their enormous use in information extraction, and knowledge graph\nconstruction. To solve these tasks, in existing literature there exist several\nsolutions -- both unsupervised and supervised. However, the unsupervised\nmethods suffer from poor performance and they often require significant human\nintervention for causal rule selection, leading to poor generalization across\ndifferent domains. On the other hand, supervised methods suffer from the lack\nof large training datasets. Recently, large language models (LLMs) with\neffective prompt engineering are found to be effective to overcome the issue of\nunavailability of large training dataset. Yet, in existing literature, there\ndoes not exist comprehensive works on causality detection and mining using LLM\nprompting. In this paper, we present several retrieval-augmented generation\n(RAG) based dynamic prompting schemes to enhance LLM performance in causality\ndetection and extraction tasks. Extensive experiments over three datasets and\nfive LLMs validate the superiority of our proposed RAG-based dynamic prompting\nover other static prompting schemes."}
{"id": "2505.24346", "pdf": "https://arxiv.org/pdf/2505.24346", "abs": "https://arxiv.org/abs/2505.24346", "authors": ["Ziyi Wang", "Zhi Gao", "Boxuan Yu", "Zirui Dai", "Yuxiang Song", "Qingyuan Lu", "Jin Chen", "Xinxiao Wu"], "title": "VUDG: A Dataset for Video Understanding Domain Generalization", "categories": ["cs.CV"], "comment": null, "summary": "Video understanding has made remarkable progress in recent years, largely\ndriven by advances in deep models and the availability of large-scale annotated\ndatasets. However, existing works typically ignore the inherent domain shifts\nencountered in real-world video applications, leaving domain generalization\n(DG) in video understanding underexplored. Hence, we propose Video\nUnderstanding Domain Generalization (VUDG), a novel dataset designed\nspecifically for evaluating the DG performance in video understanding. VUDG\ncontains videos from 11 distinct domains that cover three types of domain\nshifts, and maintains semantic similarity across different domains to ensure\nfair and meaningful evaluation. We propose a multi-expert progressive\nannotation framework to annotate each video with both multiple-choice and\nopen-ended question-answer pairs. Extensive experiments on 9 representative\nlarge video-language models (LVLMs) and several traditional video question\nanswering methods show that most models (including state-of-the-art LVLMs)\nsuffer performance degradation under domain shifts. These results highlight the\nchallenges posed by VUDG and the difference in the robustness of current models\nto data distribution shifts. We believe VUDG provides a valuable resource for\nprompting future research in domain generalization video understanding."}
{"id": "2505.23945", "pdf": "https://arxiv.org/pdf/2505.23945", "abs": "https://arxiv.org/abs/2505.23945", "authors": ["Sriram Balasubramanian", "Samyadeep Basu", "Soheil Feizi"], "title": "A Closer Look at Bias and Chain-of-Thought Faithfulness of Large (Vision) Language Models", "categories": ["cs.CL", "cs.AI", "I.2.10; I.2.7"], "comment": "34 pages, 25 figures", "summary": "Chain-of-thought (CoT) reasoning enhances performance of large language\nmodels, but questions remain about whether these reasoning traces faithfully\nreflect the internal processes of the model. We present the first comprehensive\nstudy of CoT faithfulness in large vision-language models (LVLMs),\ninvestigating how both text-based and previously unexplored image-based biases\naffect reasoning and bias articulation. Our work introduces a novel,\nfine-grained evaluation pipeline for categorizing bias articulation patterns,\nenabling significantly more precise analysis of CoT reasoning than previous\nmethods. This framework reveals critical distinctions in how models process and\nrespond to different types of biases, providing new insights into LVLM CoT\nfaithfulness. Our findings reveal that subtle image-based biases are rarely\narticulated compared to explicit text-based ones, even in models specialized\nfor reasoning. Additionally, many models exhibit a previously unidentified\nphenomenon we term ``inconsistent'' reasoning - correctly reasoning before\nabruptly changing answers, serving as a potential canary for detecting biased\nreasoning from unfaithful CoTs. We then apply the same evaluation pipeline to\nrevisit CoT faithfulness in LLMs across various levels of implicit cues. Our\nfindings reveal that current language-only reasoning models continue to\nstruggle with articulating cues that are not overtly stated."}
{"id": "2505.24157", "pdf": "https://arxiv.org/pdf/2505.24157", "abs": "https://arxiv.org/abs/2505.24157", "authors": ["Seungjoon Lee", "Suhwan Kim", "Minhyeon Oh", "Youngsik Yoon", "Jungseul Ok"], "title": "Don't Just Follow MLLM Plans: Robust and Efficient Planning for Open-world Agents", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Developing autonomous agents capable of mastering complex, multi-step tasks\nin unpredictable, interactive environments presents a significant challenge.\nWhile Large Language Models (LLMs) offer promise for planning, existing\napproaches often rely on problematic internal knowledge or make unrealistic\nenvironmental assumptions. Although recent work explores learning planning\nknowledge, they still retain limitations due to partial reliance on external\nknowledge or impractical setups. Indeed, prior research has largely overlooked\ndeveloping agents capable of acquiring planning knowledge from scratch,\ndirectly in realistic settings. While realizing this capability is necessary,\nit presents significant challenges, primarily achieving robustness given the\nsubstantial risk of incorporating LLMs' inaccurate knowledge. Moreover,\nefficiency is crucial for practicality as learning can demand prohibitive\nexploration. In response, we introduce Robust and Efficient Planning for\nOpen-world Agents (REPOA), a novel framework designed to tackle these issues.\nREPOA features three key components: adaptive dependency learning and\nfine-grained failure-aware operation memory to enhance robustness to knowledge\ninaccuracies, and difficulty-based exploration to improve learning efficiency.\nOur evaluation in two established open-world testbeds demonstrates REPOA's\nrobust and efficient planning, showcasing its capability to successfully obtain\nchallenging late-game items that were beyond the reach of prior approaches."}
{"id": "2505.23966", "pdf": "https://arxiv.org/pdf/2505.23966", "abs": "https://arxiv.org/abs/2505.23966", "authors": ["Jiayi Tian", "Ryan Solgi", "Jinming Lu", "Yifan Yang", "Hai Li", "Zheng Zhang"], "title": "FLAT-LLM: Fine-grained Low-rank Activation Space Transformation for Large Language Model Compression", "categories": ["cs.CL"], "comment": null, "summary": "Large Language Models (LLMs) have enabled remarkable progress in natural\nlanguage processing, yet their high computational and memory demands pose\nchallenges for deployment in resource-constrained environments. Although recent\nlow-rank decomposition methods offer a promising path for structural\ncompression, they often suffer from accuracy degradation, expensive calibration\nprocedures, and result in inefficient model architectures that hinder\nreal-world inference speedups. In this paper, we propose FLAT-LLM, a fast and\naccurate, training-free structural compression method based on fine-grained\nlow-rank transformations in the activation space. Specifically, we reduce the\nhidden dimension by transforming the weights using truncated eigenvectors\ncomputed via head-wise Principal Component Analysis (PCA), and employ an\nimportance-based metric to adaptively allocate ranks across decoders. FLAT-LLM\nachieves efficient and effective weight compression without recovery\nfine-tuning, which could complete the calibration within a few minutes.\nEvaluated across 4 models and 11 datasets, FLAT-LLM outperforms structural\npruning baselines in generalization and downstream performance, while\ndelivering inference speedups over decomposition-based methods."}
{"id": "2505.24361", "pdf": "https://arxiv.org/pdf/2505.24361", "abs": "https://arxiv.org/abs/2505.24361", "authors": ["Roger Ferrod", "CÃ¡ssio F. Dantas", "Luigi Di Caro", "Dino Ienco"], "title": "Revisiting Cross-Modal Knowledge Distillation: A Disentanglement Approach for RGBD Semantic Segmentation", "categories": ["cs.CV"], "comment": null, "summary": "Multi-modal RGB and Depth (RGBD) data are predominant in many domains such as\nrobotics, autonomous driving and remote sensing. The combination of these\nmulti-modal data enhances environmental perception by providing 3D spatial\ncontext, which is absent in standard RGB images. Although RGBD multi-modal data\ncan be available to train computer vision models, accessing all sensor\nmodalities during the inference stage may be infeasible due to sensor failures\nor resource constraints, leading to a mismatch between data modalities\navailable during training and inference. Traditional Cross-Modal Knowledge\nDistillation (CMKD) frameworks, developed to address this task, are typically\nbased on a teacher/student paradigm, where a multi-modal teacher distills\nknowledge into a single-modality student model. However, these approaches face\nchallenges in teacher architecture choices and distillation process selection,\nthus limiting their adoption in real-world scenarios. To overcome these issues,\nwe introduce CroDiNo-KD (Cross-Modal Disentanglement: a New Outlook on\nKnowledge Distillation), a novel cross-modal knowledge distillation framework\nfor RGBD semantic segmentation. Our approach simultaneously learns\nsingle-modality RGB and Depth models by exploiting disentanglement\nrepresentation, contrastive learning and decoupled data augmentation with the\naim to structure the internal manifolds of neural network models through\ninteraction and collaboration. We evaluated CroDiNo-KD on three RGBD datasets\nacross diverse domains, considering recent CMKD frameworks as competitors. Our\nfindings illustrate the quality of CroDiNo-KD, and they suggest reconsidering\nthe conventional teacher/student paradigm to distill information from\nmulti-modal data to single-modality neural networks."}
{"id": "2505.23953", "pdf": "https://arxiv.org/pdf/2505.23953", "abs": "https://arxiv.org/abs/2505.23953", "authors": ["Melika Sepidband", "Hamed Taherkhani", "Song Wang", "Hadi Hemmati"], "title": "Enhancing LLM-Based Code Generation with Complexity Metrics: A Feedback-Driven Approach", "categories": ["cs.SE", "cs.AI"], "comment": "11 pages, 5 figures. Accepted to COMPSAC 2025", "summary": "Automatic code generation has gained significant momentum with the advent of\nLarge Language Models (LLMs) such as GPT-4. Although many studies focus on\nimproving the effectiveness of LLMs for code generation, very limited work\ntries to understand the generated code's characteristics and leverage that to\nimprove failed cases. In this paper, as the most straightforward characteristic\nof code, we investigate the relationship between code complexity and the\nsuccess of LLM generated code. Using a large set of standard complexity\nmetrics, we first conduct an empirical analysis to explore their correlation\nwith LLM's performance on code generation (i.e., Pass@1). Using logistic\nregression models, we identify which complexity metrics are most predictive of\ncode correctness. Building on these findings, we propose an iterative feedback\nmethod, where LLMs are prompted to generate correct code based on complexity\nmetrics from previous failed outputs. We validate our approach across multiple\nbenchmarks (i.e., HumanEval, MBPP, LeetCode, and BigCodeBench) and various LLMs\n(i.e., GPT-4o, GPT-3.5 Turbo, Llama 3.1, and GPT-o3 mini), comparing the\nresults with two baseline methods: (a) zero-shot generation, and (b) iterative\nexecution-based feedback without our code complexity insights. Experiment\nresults show that our approach makes notable improvements, particularly with a\nsmaller LLM (GPT3.5 Turbo), where, e.g., Pass@1 increased by 35.71% compared to\nthe baseline's improvement of 12.5% on the HumanEval dataset. The study expands\nexperiments to BigCodeBench and integrates the method with the Reflexion code\ngeneration agent, leading to Pass@1 improvements of 20% (GPT-4o) and 23.07%\n(GPT-o3 mini). The results highlight that complexity-aware feedback enhances\nboth direct LLM prompting and agent-based workflows."}
{"id": "2505.24178", "pdf": "https://arxiv.org/pdf/2505.24178", "abs": "https://arxiv.org/abs/2505.24178", "authors": ["Katherine Tieu", "Dongqi Fu", "Jun Wu", "Jingrui He"], "title": "Invariant Link Selector for Spatial-Temporal Out-of-Distribution Problem", "categories": ["cs.LG", "cs.AI"], "comment": "Accepted by AISTATS 2025. 22 pages, 2 figures, 6 tables", "summary": "In the era of foundation models, Out-of- Distribution (OOD) problems, i.e.,\nthe data discrepancy between the training environments and testing\nenvironments, hinder AI generalization. Further, relational data like graphs\ndisobeying the Independent and Identically Distributed (IID) condition makes\nthe problem more challenging, especially much harder when it is associated with\ntime. Motivated by this, to realize the robust invariant learning over temporal\ngraphs, we want to investigate what components in temporal graphs are most\ninvariant and representative with respect to labels. With the Information\nBottleneck (IB) method, we propose an error-bounded Invariant Link Selector\nthat can distinguish invariant components and variant components during the\ntraining process to make the deep learning model generalizable for different\ntesting scenarios. Besides deriving a series of rigorous generalizable\noptimization functions, we also equip the training with task-specific loss\nfunctions, e.g., temporal link prediction, to make pretrained models solve\nreal-world application tasks like citation recommendation and merchandise\nrecommendation, as demonstrated in our experiments with state-of-the-art (SOTA)\nmethods. Our code is available at https://github.com/kthrn22/OOD-Linker."}
{"id": "2505.23996", "pdf": "https://arxiv.org/pdf/2505.23996", "abs": "https://arxiv.org/abs/2505.23996", "authors": ["Yinong Oliver Wang", "Nivedha Sivakumar", "Falaah Arif Khan", "Rin Metcalf Susa", "Adam Golinski", "Natalie Mackraz", "Barry-John Theobald", "Luca Zappella", "Nicholas Apostoloff"], "title": "Is Your Model Fairly Certain? Uncertainty-Aware Fairness Evaluation for LLMs", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "9 pages, 8 figures, and 1 table in main paper. Supplementary appendix\n  attached. Accepted at ICML 2025", "summary": "The recent rapid adoption of large language models (LLMs) highlights the\ncritical need for benchmarking their fairness. Conventional fairness metrics,\nwhich focus on discrete accuracy-based evaluations (i.e., prediction\ncorrectness), fail to capture the implicit impact of model uncertainty (e.g.,\nhigher model confidence about one group over another despite similar accuracy).\nTo address this limitation, we propose an uncertainty-aware fairness metric,\nUCerF, to enable a fine-grained evaluation of model fairness that is more\nreflective of the internal bias in model decisions compared to conventional\nfairness measures. Furthermore, observing data size, diversity, and clarity\nissues in current datasets, we introduce a new gender-occupation fairness\nevaluation dataset with 31,756 samples for co-reference resolution, offering a\nmore diverse and suitable dataset for evaluating modern LLMs. We establish a\nbenchmark, using our metric and dataset, and apply it to evaluate the behavior\nof ten open-source LLMs. For example, Mistral-7B exhibits suboptimal fairness\ndue to high confidence in incorrect predictions, a detail overlooked by\nEqualized Odds but captured by UCerF. Overall, our proposed LLM benchmark,\nwhich evaluates fairness with uncertainty awareness, paves the way for\ndeveloping more transparent and accountable AI systems."}
{"id": "2505.24371", "pdf": "https://arxiv.org/pdf/2505.24371", "abs": "https://arxiv.org/abs/2505.24371", "authors": ["Md Intisar Chowdhury", "Kittinun Aukkapinyo", "Hiroshi Fujimura", "Joo Ann Woo", "Wasu Wasusatein", "Fadoua Ghourabi"], "title": "Grid-LOGAT: Grid Based Local and Global Area Transcription for Video Question Answering", "categories": ["cs.CV", "cs.AI"], "comment": "\\c{opyright} 2025 IEEE. Personal use of this material is permitted.\n  Permission from IEEE must be obtained for all other uses, in any current or\n  future media, including reprinting/republishing this material for advertising\n  or promotional purposes, creating new collective works, for resale or\n  redistribution to servers or lists, or reuse of any copyrighted component of\n  this work in other works", "summary": "In this paper, we propose a Grid-based Local and Global Area Transcription\n(Grid-LoGAT) system for Video Question Answering (VideoQA). The system operates\nin two phases. First, extracting text transcripts from video frames using a\nVision-Language Model (VLM). Next, processing questions using these transcripts\nto generate answers through a Large Language Model (LLM). This design ensures\nimage privacy by deploying the VLM on edge devices and the LLM in the cloud. To\nimprove transcript quality, we propose grid-based visual prompting, which\nextracts intricate local details from each grid cell and integrates them with\nglobal information. Evaluation results show that Grid-LoGAT, using the\nopen-source VLM (LLaVA-1.6-7B) and LLM (Llama-3.1-8B), outperforms\nstate-of-the-art methods with similar baseline models on NExT-QA and STAR-QA\ndatasets with an accuracy of 65.9% and 50.11% respectively. Additionally, our\nmethod surpasses the non-grid version by 24 points on localization-based\nquestions we created using NExT-QA."}
{"id": "2505.23968", "pdf": "https://arxiv.org/pdf/2505.23968", "abs": "https://arxiv.org/abs/2505.23968", "authors": ["Stephan Rabanser", "Ali Shahin Shamsabadi", "Olive Franzese", "Xiao Wang", "Adrian Weller", "Nicolas Papernot"], "title": "Confidential Guardian: Cryptographically Prohibiting the Abuse of Model Abstention", "categories": ["cs.CR", "cs.AI", "cs.CY", "cs.LG", "stat.ML"], "comment": "Proceedings of the 42nd International Conference on Machine Learning", "summary": "Cautious predictions -- where a machine learning model abstains when\nuncertain -- are crucial for limiting harmful errors in safety-critical\napplications. In this work, we identify a novel threat: a dishonest institution\ncan exploit these mechanisms to discriminate or unjustly deny services under\nthe guise of uncertainty. We demonstrate the practicality of this threat by\nintroducing an uncertainty-inducing attack called Mirage, which deliberately\nreduces confidence in targeted input regions, thereby covertly disadvantaging\nspecific individuals. At the same time, Mirage maintains high predictive\nperformance across all data points. To counter this threat, we propose\nConfidential Guardian, a framework that analyzes calibration metrics on a\nreference dataset to detect artificially suppressed confidence. Additionally,\nit employs zero-knowledge proofs of verified inference to ensure that reported\nconfidence scores genuinely originate from the deployed model. This prevents\nthe provider from fabricating arbitrary model confidence values while\nprotecting the model's proprietary details. Our results confirm that\nConfidential Guardian effectively prevents the misuse of cautious predictions,\nproviding verifiable assurances that abstention reflects genuine model\nuncertainty rather than malicious intent."}
{"id": "2505.24179", "pdf": "https://arxiv.org/pdf/2505.24179", "abs": "https://arxiv.org/abs/2505.24179", "authors": ["Xiaodong Ji", "Hailin Zhang", "Fangcheng Fu", "Bin Cui"], "title": "SALE : Low-bit Estimation for Efficient Sparse Attention in Long-context LLM Prefilling", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Many advanced Large Language Model (LLM) applications require long-context\nprocessing, but the self-attention module becomes a bottleneck during the\nprefilling stage of inference due to its quadratic time complexity with respect\nto sequence length. Existing sparse attention methods accelerate attention\ncomputation by skipping less significant regions of the attention map. However,\nthese approaches typically perform coarse-grained inspection of the attention\nmap, rendering considerable loss in model accuracy. In this paper, we propose\nSALE, a fine-grained sparse attention method that accelerates the long-context\nprefilling stage of LLM with negligible loss in model accuracy. SALE achieves\nfast and accurate fine-grained attention weight estimation through 4-bit\nquantized query-key products, followed by block-sparse attention to accelerate\nprefilling computations. For importance evaluation for query-key pairs, we\nadopt our Relative Attention Score metric, which offers significantly higher\nefficiency within our framework. We implement a custom CUDA kernel optimized\nfor our approach for hardware efficiency, reducing the additional overhead to\napproximately 11% of the full attention latency. Notably, SALE requires no\nparameter training and can be seamlessly integrated into existing systems with\ntrivial code modifications. Experiments on long-context benchmarks demonstrate\nthat our method outperforms existing approaches in accuracy-efficiency\ntrade-offs, achieving at least 3.36x speedups on Llama-3.1-8B for sequences\nlonger than 64K while maintaining model quality."}
{"id": "2505.24009", "pdf": "https://arxiv.org/pdf/2505.24009", "abs": "https://arxiv.org/abs/2505.24009", "authors": ["Hidetaka Kamigaito", "Ying Zhang", "Jingun Kwon", "Katsuhiko Hayashi", "Manabu Okumura", "Taro Watanabe"], "title": "Diversity of Transformer Layers: One Aspect of Parameter Scaling Laws", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": null, "summary": "Transformers deliver outstanding performance across a wide range of tasks and\nare now a dominant backbone architecture for large language models (LLMs).\nTheir task-solving performance is improved by increasing parameter size, as\nshown in the recent studies on parameter scaling laws. Although recent\nmechanistic-interpretability studies have deepened our understanding of the\ninternal behavior of Transformers by analyzing their residual stream, the\nrelationship between these internal mechanisms and the parameter scaling laws\nremains unclear. To bridge this gap, we focus on layers and their size, which\nmainly decide the parameter size of Transformers. For this purpose, we first\ntheoretically investigate the layers within the residual stream through a\nbias-diversity decomposition. The decomposition separates (i) bias, the error\nof each layer's output from the ground truth, and (ii) diversity, which\nindicates how much the outputs of each layer differ from each other. Analyzing\nTransformers under this theory reveals that performance improves when\nindividual layers make predictions close to the correct answer and remain\nmutually diverse. We show that diversity becomes especially critical when\nindividual layers' outputs are far from the ground truth. Finally, we introduce\nan information-theoretic diversity and show our main findings that adding\nlayers enhances performance only when those layers behave differently, i.e.,\nare diverse. We also reveal the performance gains from increasing the number of\nlayers exhibit submodularity: marginal improvements diminish as additional\nlayers increase, mirroring the logarithmic convergence predicted by the\nparameter scaling laws. Experiments on multiple semantic-understanding tasks\nwith various LLMs empirically confirm the theoretical properties derived in\nthis study."}
{"id": "2505.24372", "pdf": "https://arxiv.org/pdf/2505.24372", "abs": "https://arxiv.org/abs/2505.24372", "authors": ["Yichi Zhang", "Gongwei Chen", "Jun Zhu", "Jia Wan"], "title": "D2AF: A Dual-Driven Annotation and Filtering Framework for Visual Grounding", "categories": ["cs.CV"], "comment": "16pages, 8figures", "summary": "Visual Grounding is a task that aims to localize a target region in an image\nbased on a free-form natural language description. With the rise of Transformer\narchitectures, there is an increasing need for larger datasets to boost\nperformance. However, the high cost of manual annotation poses a challenge,\nhindering the scale of data and the ability of large models to enhance their\neffectiveness. Previous pseudo label generation methods heavily rely on\nhuman-labeled captions of the original dataset, limiting scalability and\ndiversity. To address this, we propose D2AF, a robust annotation framework for\nvisual grounding using only input images. This approach overcomes dataset size\nlimitations and enriches both the quantity and diversity of referring\nexpressions. Our approach leverages multimodal large models and object\ndetection models. By implementing dual-driven annotation strategies, we\neffectively generate detailed region-text pairs using both closed-set and\nopen-set approaches. We further conduct an in-depth analysis of data quantity\nand data distribution. Our findings demonstrate that increasing data volume\nenhances model performance. However, the degree of improvement depends on how\nwell the pseudo labels broaden the original data distribution. Based on these\ninsights, we propose a consistency and distribution aware filtering method to\nfurther improve data quality by effectively removing erroneous and redundant\ndata. This approach effectively eliminates noisy data, leading to improved\nperformance. Experiments on three visual grounding tasks demonstrate that our\nmethod significantly improves the performance of existing models and achieves\nstate-of-the-art results."}
{"id": "2505.24001", "pdf": "https://arxiv.org/pdf/2505.24001", "abs": "https://arxiv.org/abs/2505.24001", "authors": ["Wonjun Yi", "Wonho Jung", "Kangmin Jang", "Yong-Hwa Park"], "title": "Multi-output Classification using a Cross-talk Architecture for Compound Fault Diagnosis of Motors in Partially Labeled Condition", "categories": ["eess.SP", "cs.AI"], "comment": "Submitted to Mechanical Systems and Signal Processing on May 9th,\n  2025", "summary": "The increasing complexity of rotating machinery and the diversity of\noperating conditions, such as rotating speed and varying torques, have\namplified the challenges in fault diagnosis in scenarios requiring domain\nadaptation, particularly involving compound faults. This study addresses these\nchallenges by introducing a novel multi-output classification (MOC) framework\ntailored for domain adaptation in partially labeled (PL) target datasets.\nUnlike conventional multi-class classification (MCC) approaches, the proposed\nMOC framework classifies the severity levels of compound faults simultaneously.\nFurthermore, we explore various single-task and multi-task architectures\napplicable to the MOC formulation-including shared trunk and cross-talk-based\ndesigns-for compound fault diagnosis under PL conditions. Based on this\ninvestigation, we propose a novel cross-talk layer structure that enables\nselective information sharing across diagnostic tasks, effectively enhancing\nclassification performance in compound fault scenarios. In addition,\nfrequency-layer normalization was incorporated to improve domain adaptation\nperformance on motor vibration data. Compound fault conditions were implemented\nusing a motor-based test setup, and the proposed model was evaluated across six\ndomain adaptation scenarios. The experimental results demonstrate its superior\nmacro F1 performance compared to baseline models. We further showed that the\nproposed mode's structural advantage is more pronounced in compound fault\nsettings through a single-fault comparison. We also found that frequency-layer\nnormalization fits the fault diagnosis task better than conventional methods.\nLastly, we discuss that this improvement primarily stems from the model's\nstructural ability to leverage inter-fault classification task interactions,\nrather than from a simple increase in model parameters."}
{"id": "2505.24183", "pdf": "https://arxiv.org/pdf/2505.24183", "abs": "https://arxiv.org/abs/2505.24183", "authors": ["Yaoyu Zhu", "Di Huang", "Hanqi Lyu", "Xiaoyun Zhang", "Chongxiao Li", "Wenxuan Shi", "Yutong Wu", "Jianan Mu", "Jinghua Wang", "Yang Zhao", "Pengwei Jin", "Shuyao Cheng", "Shengwen Liang", "Xishan Zhang", "Rui Zhang", "Zidong Du", "Qi Guo", "Xing Hu", "Yunji Chen"], "title": "CodeV-R1: Reasoning-Enhanced Verilog Generation", "categories": ["cs.LG", "cs.AR", "cs.PL"], "comment": null, "summary": "Large language models (LLMs) trained via reinforcement learning with\nverifiable reward (RLVR) have achieved breakthroughs on tasks with explicit,\nautomatable verification, such as software programming and mathematical\nproblems. Extending RLVR to electronic design automation (EDA), especially\nautomatically generating hardware description languages (HDLs) like Verilog\nfrom natural-language (NL) specifications, however, poses three key challenges:\nthe lack of automated and accurate verification environments, the scarcity of\nhigh-quality NL-code pairs, and the prohibitive computation cost of RLVR. To\nthis end, we introduce CodeV-R1, an RLVR framework for training Verilog\ngeneration LLMs. First, we develop a rule-based testbench generator that\nperforms robust equivalence checking against golden references. Second, we\npropose a round-trip data synthesis method that pairs open-source Verilog\nsnippets with LLM-generated NL descriptions, verifies code-NL-code consistency\nvia the generated testbench, and filters out inequivalent examples to yield a\nhigh-quality dataset. Third, we employ a two-stage \"distill-then-RL\" training\npipeline: distillation for the cold start of reasoning abilities, followed by\nadaptive DAPO, our novel RLVR algorithm that can reduce training cost by\nadaptively adjusting sampling rate. The resulting model, CodeV-R1-7B, achieves\n68.6% and 72.9% pass@1 on VerilogEval v2 and RTLLM v1.1, respectively,\nsurpassing prior state-of-the-art by 12~20%, while matching or even exceeding\nthe performance of 671B DeepSeek-R1. We will release our model, training\npipeline, and dataset to facilitate research in EDA and LLM communities."}
{"id": "2505.24012", "pdf": "https://arxiv.org/pdf/2505.24012", "abs": "https://arxiv.org/abs/2505.24012", "authors": ["Alexandre Bonlarron", "Florian RÃ©gin", "Elisabetta De Maria", "Jean-Charles RÃ©gin"], "title": "Large Language Model Meets Constraint Propagation", "categories": ["cs.CL", "cs.AI"], "comment": "To appear in the Proceedings of the Thirty-Fourth International Joint\n  Conference on Artificial Intelligence (IJCAI 2025)", "summary": "Large Language Models (LLMs) excel at generating fluent text but struggle to\nenforce external constraints because they generate tokens sequentially without\nexplicit control mechanisms. GenCP addresses this limitation by combining LLM\npredictions with Constraint Programming (CP) reasoning, formulating text\ngeneration as a Constraint Satisfaction Problem (CSP). In this paper, we\nimprove GenCP by integrating Masked Language Models (MLMs) for domain\ngeneration, which allows bidirectional constraint propagation that leverages\nboth past and future tokens. This integration bridges the gap between\ntoken-level prediction and structured constraint enforcement, leading to more\nreliable and constraint-aware text generation. Our evaluation on COLLIE\nbenchmarks demonstrates that incorporating domain preview via MLM calls\nsignificantly improves GenCP's performance. Although this approach incurs\nadditional MLM calls and, in some cases, increased backtracking, the overall\neffect is a more efficient use of LLM inferences and an enhanced ability to\ngenerate feasible and meaningful solutions, particularly in tasks with strict\ncontent constraints."}
{"id": "2505.24375", "pdf": "https://arxiv.org/pdf/2505.24375", "abs": "https://arxiv.org/abs/2505.24375", "authors": ["Maciej Wielgosz", "Simon Berg", "Heikki Korpunen", "Stephan Hoffmann"], "title": "Spatiotemporal Analysis of Forest Machine Operations Using 3D Video Classification", "categories": ["cs.CV"], "comment": null, "summary": "This paper presents a deep learning-based framework for classifying forestry\noperations from dashcam video footage. Focusing on four key work elements -\ncrane-out, cutting-and-to-processing, driving, and processing - the approach\nemploys a 3D ResNet-50 architecture implemented with PyTorchVideo. Trained on a\nmanually annotated dataset of field recordings, the model achieves strong\nperformance, with a validation F1 score of 0.88 and precision of 0.90. These\nresults underscore the effectiveness of spatiotemporal convolutional networks\nfor capturing both motion patterns and appearance in real-world forestry\nenvironments.\n  The system integrates standard preprocessing and augmentation techniques to\nimprove generalization, but overfitting is evident, highlighting the need for\nmore training data and better class balance. Despite these challenges, the\nmethod demonstrates clear potential for reducing the manual workload associated\nwith traditional time studies, offering a scalable solution for operational\nmonitoring and efficiency analysis in forestry.\n  This work contributes to the growing application of AI in natural resource\nmanagement and sets the foundation for future systems capable of real-time\nactivity recognition in forest machinery. Planned improvements include dataset\nexpansion, enhanced regularization, and deployment trials on embedded systems\nfor in-field use."}
{"id": "2505.24019", "pdf": "https://arxiv.org/pdf/2505.24019", "abs": "https://arxiv.org/abs/2505.24019", "authors": ["Kaiyuan Zhang", "Zian Su", "Pin-Yu Chen", "Elisa Bertino", "Xiangyu Zhang", "Ninghui Li"], "title": "LLM Agents Should Employ Security Principles", "categories": ["cs.CR", "cs.AI"], "comment": null, "summary": "Large Language Model (LLM) agents show considerable promise for automating\ncomplex tasks using contextual reasoning; however, interactions involving\nmultiple agents and the system's susceptibility to prompt injection and other\nforms of context manipulation introduce new vulnerabilities related to privacy\nleakage and system exploitation. This position paper argues that the\nwell-established design principles in information security, which are commonly\nreferred to as security principles, should be employed when deploying LLM\nagents at scale. Design principles such as defense-in-depth, least privilege,\ncomplete mediation, and psychological acceptability have helped guide the\ndesign of mechanisms for securing information systems over the last five\ndecades, and we argue that their explicit and conscientious adoption will help\nsecure agentic systems. To illustrate this approach, we introduce AgentSandbox,\na conceptual framework embedding these security principles to provide\nsafeguards throughout an agent's life-cycle. We evaluate with state-of-the-art\nLLMs along three dimensions: benign utility, attack utility, and attack success\nrate. AgentSandbox maintains high utility for its intended functions under both\nbenign and adversarial evaluations while substantially mitigating privacy\nrisks. By embedding secure design principles as foundational elements within\nemerging LLM agent protocols, we aim to promote trustworthy agent ecosystems\naligned with user privacy expectations and evolving regulatory requirements."}
{"id": "2505.24185", "pdf": "https://arxiv.org/pdf/2505.24185", "abs": "https://arxiv.org/abs/2505.24185", "authors": ["Yipan Wei", "Yuchen Zou", "Yapeng Li", "Bo Du"], "title": "Towards Unified Modeling in Federated Multi-Task Learning via Subspace Decoupling", "categories": ["cs.LG", "cs.AI", "cs.CV"], "comment": null, "summary": "Federated Multi-Task Learning (FMTL) enables multiple clients performing\nheterogeneous tasks without exchanging their local data, offering broad\npotential for privacy preserving multi-task collaboration. However, most\nexisting methods focus on building personalized models for each client and\nunable to support the aggregation of multiple heterogeneous tasks into a\nunified model. As a result, in real-world scenarios where task objectives,\nlabel spaces, and optimization paths vary significantly, conventional FMTL\nmethods struggle to achieve effective joint training. To address this\nchallenge, we propose FedDEA (Federated Decoupled Aggregation), an\nupdate-structure-aware aggregation method specifically designed for multi-task\nmodel integration. Our method dynamically identifies task-relevant dimensions\nbased on the response strength of local updates and enhances their optimization\neffectiveness through rescaling. This mechanism effectively suppresses\ncross-task interference and enables task-level decoupled aggregation within a\nunified global model. FedDEA does not rely on task labels or architectural\nmodifications, making it broadly applicable and deployment-friendly.\nExperimental results demonstrate that it can be easily integrated into various\nmainstream federated optimization algorithms and consistently delivers\nsignificant overall performance improvements on widely used NYUD-V2 and\nPASCAL-Context. These results validate the robustness and generalization\ncapabilities of FedDEA under highly heterogeneous task settings."}
{"id": "2505.24016", "pdf": "https://arxiv.org/pdf/2505.24016", "abs": "https://arxiv.org/abs/2505.24016", "authors": ["Matthew Raffel", "Victor Agostinelli", "Lizhong Chen"], "title": "BeaverTalk: Oregon State University's IWSLT 2025 Simultaneous Speech Translation System", "categories": ["cs.CL", "cs.LG"], "comment": "Accepted at IWSLT 2025", "summary": "This paper discusses the construction, fine-tuning, and deployment of\nBeaverTalk, a cascaded system for speech-to-text translation as part of the\nIWSLT 2025 simultaneous translation task. The system architecture employs a VAD\nsegmenter for breaking a speech stream into segments, Whisper Large V2 for\nautomatic speech recognition (ASR), and Gemma 3 12B for simultaneous\ntranslation. Regarding the simultaneous translation LLM, it is fine-tuned via\nlow-rank adaptors (LoRAs) for a conversational prompting strategy that\nleverages a single prior-sentence memory bank from the source language as\ncontext. The cascaded system participated in the English$\\rightarrow$German and\nEnglish$\\rightarrow$Chinese language directions for both the low and high\nlatency regimes. In particular, on the English$\\rightarrow$German task, the\nsystem achieves a BLEU of 24.64 and 27.83 at a StreamLAAL of 1837.86 and\n3343.73, respectively. Then, on the English$\\rightarrow$Chinese task, the\nsystem achieves a BLEU of 34.07 and 37.23 at a StreamLAAL of 2216.99 and\n3521.35, respectively."}
{"id": "2505.24380", "pdf": "https://arxiv.org/pdf/2505.24380", "abs": "https://arxiv.org/abs/2505.24380", "authors": ["Zheng Wang"], "title": "SASP: Strip-Aware Spatial Perception for Fine-Grained Bird Image Classification", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Fine-grained bird image classification (FBIC) is not only of great\nsignificance for ecological monitoring and species identification, but also\nholds broad research value in the fields of image recognition and fine-grained\nvisual modeling. Compared with general image classification tasks, FBIC poses\nmore formidable challenges: 1) the differences in species size and imaging\ndistance result in the varying sizes of birds presented in the images; 2)\ncomplex natural habitats often introduce strong background interference; 3) and\nhighly flexible poses such as flying, perching, or foraging result in\nsubstantial intra-class variability. These factors collectively make it\ndifficult for traditional methods to stably extract discriminative features,\nthereby limiting the generalizability and interpretability of models in\nreal-world applications. To address these challenges, this paper proposes a\nfine-grained bird classification framework based on strip-aware spatial\nperception, which aims to capture long-range spatial dependencies across entire\nrows or columns in bird images, thereby enhancing the model's robustness and\ninterpretability. The proposed method incorporates two novel modules:\nextensional perception aggregator (EPA) and channel semantic weaving (CSW).\nSpecifically, EPA integrates local texture details with global structural cues\nby aggregating information across horizontal and vertical spatial directions.\nCSW further refines the semantic representations by adaptively fusing\nlong-range and short-range information along the channel dimension. Built upon\na ResNet-50 backbone, the model enables jump-wise connection of extended\nstructural features across the spatial domain. Experimental results on the\nCUB-200-2011 dataset demonstrate that our framework achieves significant\nperformance improvements while maintaining architectural efficiency."}
{"id": "2505.24040", "pdf": "https://arxiv.org/pdf/2505.24040", "abs": "https://arxiv.org/abs/2505.24040", "authors": ["Yuexing Hao", "Kumail Alhamoud", "Hyewon Jeong", "Haoran Zhang", "Isha Puri", "Philip Torr", "Mike Schaekermann", "Ariel D. Stern", "Marzyeh Ghassemi"], "title": "MedPAIR: Measuring Physicians and AI Relevance Alignment in Medical Question Answering", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Large Language Models (LLMs) have demonstrated remarkable performance on\nvarious medical question-answering (QA) benchmarks, including standardized\nmedical exams. However, correct answers alone do not ensure correct logic, and\nmodels may reach accurate conclusions through flawed processes. In this study,\nwe introduce the MedPAIR (Medical Dataset Comparing Physicians and AI Relevance\nEstimation and Question Answering) dataset to evaluate how physician trainees\nand LLMs prioritize relevant information when answering QA questions. We obtain\nannotations on 1,300 QA pairs from 36 physician trainees, labeling each\nsentence within the question components for relevance. We compare these\nrelevance estimates to those for LLMs, and further evaluate the impact of these\n\"relevant\" subsets on downstream task performance for both physician trainees\nand LLMs. We find that LLMs are frequently not aligned with the content\nrelevance estimates of physician trainees. After filtering out physician\ntrainee-labeled irrelevant sentences, accuracy improves for both the trainees\nand the LLMs. All LLM and physician trainee-labeled data are available at:\nhttp://medpair.csail.mit.edu/."}
{"id": "2505.24189", "pdf": "https://arxiv.org/pdf/2505.24189", "abs": "https://arxiv.org/abs/2505.24189", "authors": ["Orlando Marquez Ayala", "Patrice Bechard", "Emily Chen", "Maggie Baird", "Jingfei Chen"], "title": "Fine-Tune an SLM or Prompt an LLM? The Case of Generating Low-Code Workflows", "categories": ["cs.LG", "cs.AI", "cs.CL"], "comment": null, "summary": "Large Language Models (LLMs) such as GPT-4o can handle a wide range of\ncomplex tasks with the right prompt. As per token costs are reduced, the\nadvantages of fine-tuning Small Language Models (SLMs) for real-world\napplications -- faster inference, lower costs -- may no longer be clear. In\nthis work, we present evidence that, for domain-specific tasks that require\nstructured outputs, SLMs still have a quality advantage. We compare fine-tuning\nan SLM against prompting LLMs on the task of generating low-code workflows in\nJSON form. We observe that while a good prompt can yield reasonable results,\nfine-tuning improves quality by 10% on average. We also perform systematic\nerror analysis to reveal model limitations."}
{"id": "2505.24028", "pdf": "https://arxiv.org/pdf/2505.24028", "abs": "https://arxiv.org/abs/2505.24028", "authors": ["Kateryna Akhynko", "Oleksandr Kosovan", "Mykola Trokhymovych"], "title": "Hidden Persuasion: Detecting Manipulative Narratives on Social Media During the 2022 Russian Invasion of Ukraine", "categories": ["cs.CL"], "comment": null, "summary": "This paper presents one of the top-performing solutions to the UNLP 2025\nShared Task on Detecting Manipulation in Social Media. The task focuses on\ndetecting and classifying rhetorical and stylistic manipulation techniques used\nto influence Ukrainian Telegram users. For the classification subtask, we\nfine-tuned the Gemma 2 language model with LoRA adapters and applied a\nsecond-level classifier leveraging meta-features and threshold optimization.\nFor span detection, we employed an XLM-RoBERTa model trained for multi-target,\nincluding token binary classification. Our approach achieved 2nd place in\nclassification and 3rd place in span detection."}
{"id": "2505.24389", "pdf": "https://arxiv.org/pdf/2505.24389", "abs": "https://arxiv.org/abs/2505.24389", "authors": ["Liangyang Ouyang", "Yuki Sakai", "Ryosuke Furuta", "Hisataka Nozawa", "Hikoro Matsui", "Yoichi Sato"], "title": "Leadership Assessment in Pediatric Intensive Care Unit Team Training", "categories": ["cs.CV"], "comment": "This paper is accepted by EgoVis Workshop at CVPR 2025", "summary": "This paper addresses the task of assessing PICU team's leadership skills by\ndeveloping an automated analysis framework based on egocentric vision. We\nidentify key behavioral cues, including fixation object, eye contact, and\nconversation patterns, as essential indicators of leadership assessment. In\norder to capture these multimodal signals, we employ Aria Glasses to record\negocentric video, audio, gaze, and head movement data. We collect one-hour\nvideos of four simulated sessions involving doctors with different roles and\nlevels. To automate data processing, we propose a method leveraging REMoDNaV,\nSAM, YOLO, and ChatGPT for fixation object detection, eye contact detection,\nand conversation classification. In the experiments, significant correlations\nare observed between leadership skills and behavioral metrics, i.e., the output\nof our proposed methods, such as fixation time, transition patterns, and direct\norders in speech. These results indicate that our proposed data collection and\nanalysis framework can effectively solve skill assessment for training PICU\nteams."}
{"id": "2505.24090", "pdf": "https://arxiv.org/pdf/2505.24090", "abs": "https://arxiv.org/abs/2505.24090", "authors": ["Karan Hanswadkar", "Anika Kanchi", "Shivani Tripathi", "Shi Qiao", "Rony Chatterjee", "Alekh Jindal"], "title": "Searching Clinical Data Using Generative AI", "categories": ["cs.DB", "cs.AI"], "comment": null, "summary": "Artificial Intelligence (AI) is making a major impact on healthcare,\nparticularly through its application in natural language processing (NLP) and\npredictive analytics. The healthcare sector has increasingly adopted AI for\ntasks such as clinical data analysis and medical code assignment. However,\nsearching for clinical information in large and often unorganized datasets\nremains a manual and error-prone process. Assisting this process with\nautomations can help physicians improve their operational productivity\nsignificantly.\n  In this paper, we present a generative AI approach, coined SearchAI, to\nenhance the accuracy and efficiency of searching clinical data. Unlike\ntraditional code assignment, which is a one-to-one problem, clinical data\nsearch is a one-to-many problem, i.e., a given search query can map to a family\nof codes. Healthcare professionals typically search for groups of related\ndiseases, drugs, or conditions that map to many codes, and therefore, they need\nsearch tools that can handle keyword synonyms, semantic variants, and broad\nopen-ended queries. SearchAI employs a hierarchical model that respects the\ncoding hierarchy and improves the traversal of relationships from parent to\nchild nodes. SearchAI navigates these hierarchies predictively and ensures that\nall paths are reachable without losing any relevant nodes.\n  To evaluate the effectiveness of SearchAI, we conducted a series of\nexperiments using both public and production datasets. Our results show that\nSearchAI outperforms default hierarchical traversals across several metrics,\nincluding accuracy, robustness, performance, and scalability. SearchAI can help\nmake clinical data more accessible, leading to streamlined workflows, reduced\nadministrative burden, and enhanced coding and diagnostic accuracy."}
{"id": "2505.24190", "pdf": "https://arxiv.org/pdf/2505.24190", "abs": "https://arxiv.org/abs/2505.24190", "authors": ["Lan-Cuong Nguyen", "Quan Nguyen-Tri", "Bang Tran Khanh", "Dung D. Le", "Long Tran-Thanh", "Khoat Than"], "title": "Provably Improving Generalization of Few-Shot Models with Synthetic Data", "categories": ["cs.LG", "cs.CV"], "comment": "ICML 2025. Our code will be released soon", "summary": "Few-shot image classification remains challenging due to the scarcity of\nlabeled training examples. Augmenting them with synthetic data has emerged as a\npromising way to alleviate this issue, but models trained on synthetic samples\noften face performance degradation due to the inherent gap between real and\nsynthetic distributions. To address this limitation, we develop a theoretical\nframework that quantifies the impact of such distribution discrepancies on\nsupervised learning, specifically in the context of image classification. More\nimportantly, our framework suggests practical ways to generate good synthetic\nsamples and to train a predictor with high generalization ability. Building\nupon this framework, we propose a novel theoretical-based algorithm that\nintegrates prototype learning to optimize both data partitioning and model\ntraining, effectively bridging the gap between real few-shot data and synthetic\ndata. Extensive experiments results show that our approach demonstrates\nsuperior performance compared to state-of-the-art methods, outperforming them\nacross multiple datasets."}
{"id": "2505.24033", "pdf": "https://arxiv.org/pdf/2505.24033", "abs": "https://arxiv.org/abs/2505.24033", "authors": ["Yasaman Jafari", "Zixian Wang", "Leon Bergen", "Taylor Berg-Kirkpatrick"], "title": "The Surprising Soupability of Documents in State Space Models", "categories": ["cs.CL", "cs.CE", "cs.LG"], "comment": null, "summary": "We investigate whether hidden states from Structured State Space Models\n(SSMs) can be merged post-hoc to support downstream reasoning. Inspired by\nmodel souping, we propose a strategy where documents are encoded independently\nand their representations are pooled -- via simple operations like averaging --\ninto a single context state. This approach, which we call document souping,\nenables modular encoding and reuse without reprocessing the full input for each\nquery. We finetune Mamba2 models to produce soupable representations and find\nthat they support multi-hop QA, sparse retrieval, and long-document reasoning\nwith strong accuracy. On HotpotQA, souping ten independently encoded documents\nnearly matches the performance of a cross-encoder trained on the same inputs."}
{"id": "2505.24401", "pdf": "https://arxiv.org/pdf/2505.24401", "abs": "https://arxiv.org/abs/2505.24401", "authors": ["Xianheng Ma", "Hongchen Tan", "Xiuping Liu", "Yi Zhang", "Huasheng Wang", "Jiang Liu", "Ying Chen", "Hantao Liu"], "title": "S3CE-Net: Spike-guided Spatiotemporal Semantic Coupling and Expansion Network for Long Sequence Event Re-Identification", "categories": ["cs.CV"], "comment": null, "summary": "In this paper, we leverage the advantages of event cameras to resist harsh\nlighting conditions, reduce background interference, achieve high time\nresolution, and protect facial information to study the long-sequence\nevent-based person re-identification (Re-ID) task. To this end, we propose a\nsimple and efficient long-sequence event Re-ID model, namely the Spike-guided\nSpatiotemporal Semantic Coupling and Expansion Network (S3CE-Net). To better\nhandle asynchronous event data, we build S3CE-Net based on spiking neural\nnetworks (SNNs). The S3CE-Net incorporates the Spike-guided Spatial-temporal\nAttention Mechanism (SSAM) and the Spatiotemporal Feature Sampling Strategy\n(STFS). The SSAM is designed to carry out semantic interaction and association\nin both spatial and temporal dimensions, leveraging the capabilities of SNNs.\nThe STFS involves sampling spatial feature subsequences and temporal feature\nsubsequences from the spatiotemporal dimensions, driving the Re-ID model to\nperceive broader and more robust effective semantics. Notably, the STFS\nintroduces no additional parameters and is only utilized during the training\nstage. Therefore, S3CE-Net is a low-parameter and high-efficiency model for\nlong-sequence event-based person Re-ID. Extensive experiments have verified\nthat our S3CE-Net achieves outstanding performance on many mainstream\nlong-sequence event-based person Re-ID datasets. Code is available\nat:https://github.com/Mhsunshine/SC3E_Net."}
{"id": "2505.24099", "pdf": "https://arxiv.org/pdf/2505.24099", "abs": "https://arxiv.org/abs/2505.24099", "authors": ["Mohammad Shah Alam", "William Ott", "Ilya Timofeyev"], "title": "Attractor learning for spatiotemporally chaotic dynamical systems using echo state networks with transfer learning", "categories": ["math.DS", "cs.AI", "cs.LG", "nlin.CD", "stat.ML", "37N99, 68T30"], "comment": null, "summary": "In this paper, we explore the predictive capabilities of echo state networks\n(ESNs) for the generalized Kuramoto-Sivashinsky (gKS) equation, an archetypal\nnonlinear PDE that exhibits spatiotemporal chaos. We introduce a novel\nmethodology that integrates ESNs with transfer learning, aiming to enhance\npredictive performance across various parameter regimes of the gKS model. Our\nresearch focuses on predicting changes in long-term statistical patterns of the\ngKS model that result from varying the dispersion relation or the length of the\nspatial domain. We use transfer learning to adapt ESNs to different parameter\nsettings and successfully capture changes in the underlying chaotic attractor."}
{"id": "2505.24193", "pdf": "https://arxiv.org/pdf/2505.24193", "abs": "https://arxiv.org/abs/2505.24193", "authors": ["Ofir Schlisselberg", "Tal Lancewicki", "Peter Auer", "Yishay Mansour"], "title": "Improved Best-of-Both-Worlds Regret for Bandits with Delayed Feedback", "categories": ["cs.LG"], "comment": null, "summary": "We study the multi-armed bandit problem with adversarially chosen delays in\nthe Best-of-Both-Worlds (BoBW) framework, which aims to achieve near-optimal\nperformance in both stochastic and adversarial environments. While prior work\nhas made progress toward this goal, existing algorithms suffer from significant\ngaps to the known lower bounds, especially in the stochastic settings. Our main\ncontribution is a new algorithm that, up to logarithmic factors, matches the\nknown lower bounds in each setting individually.\n  In the adversarial case, our algorithm achieves regret of\n$\\widetilde{O}(\\sqrt{KT} + \\sqrt{D})$, which is optimal up to logarithmic\nterms, where $T$ is the number of rounds, $K$ is the number of arms, and $D$ is\nthe cumulative delay. In the stochastic case, we provide a regret bound which\nscale as $\\sum_{i:\\Delta_i>0}\\left(\\log T/\\Delta_i\\right) + \\frac{1}{K}\\sum\n\\Delta_i \\sigma_{max}$, where $\\Delta_i$ is the sub-optimality gap of arm $i$\nand $\\sigma_{\\max}$ is the maximum number of missing observations.\n  To the best of our knowledge, this is the first BoBW algorithm to\nsimultaneously match the lower bounds in both stochastic and adversarial\nregimes in delayed environment. Moreover, even beyond the BoBW setting, our\nstochastic regret bound is the first to match the known lower bound under\nadversarial delays, improving the second term over the best known result by a\nfactor of $K$."}
{"id": "2505.24063", "pdf": "https://arxiv.org/pdf/2505.24063", "abs": "https://arxiv.org/abs/2505.24063", "authors": ["Jiacheng Xie", "Yang Yu", "Ziyang Zhang", "Shuai Zeng", "Jiaxuan He", "Ayush Vasireddy", "Xiaoting Tang", "Congyu Guo", "Lening Zhao", "Congcong Jing", "Guanghui An", "Dong Xu"], "title": "TCM-Ladder: A Benchmark for Multimodal Question Answering on Traditional Chinese Medicine", "categories": ["cs.CL", "cs.DB"], "comment": "22 pages, 4 figures", "summary": "Traditional Chinese Medicine (TCM), as an effective alternative medicine, has\nbeen receiving increasing attention. In recent years, the rapid development of\nlarge language models (LLMs) tailored for TCM has underscored the need for an\nobjective and comprehensive evaluation framework to assess their performance on\nreal-world tasks. However, existing evaluation datasets are limited in scope\nand primarily text-based, lacking a unified and standardized multimodal\nquestion-answering (QA) benchmark. To address this issue, we introduce\nTCM-Ladder, the first multimodal QA dataset specifically designed for\nevaluating large TCM language models. The dataset spans multiple core\ndisciplines of TCM, including fundamental theory, diagnostics, herbal formulas,\ninternal medicine, surgery, pharmacognosy, and pediatrics. In addition to\ntextual content, TCM-Ladder incorporates various modalities such as images and\nvideos. The datasets were constructed using a combination of automated and\nmanual filtering processes and comprise 52,000+ questions in total. These\nquestions include single-choice, multiple-choice, fill-in-the-blank, diagnostic\ndialogue, and visual comprehension tasks. We trained a reasoning model on\nTCM-Ladder and conducted comparative experiments against 9 state-of-the-art\ngeneral domain and 5 leading TCM-specific LLMs to evaluate their performance on\nthe datasets. Moreover, we propose Ladder-Score, an evaluation method\nspecifically designed for TCM question answering that effectively assesses\nanswer quality regarding terminology usage and semantic expression. To our\nknowledge, this is the first work to evaluate mainstream general domain and\nTCM-specific LLMs on a unified multimodal benchmark. The datasets and\nleaderboard are publicly available at https://tcmladder.com or\nhttps://54.211.107.106 and will be continuously updated."}
{"id": "2505.24402", "pdf": "https://arxiv.org/pdf/2505.24402", "abs": "https://arxiv.org/abs/2505.24402", "authors": ["Mika Feng", "Koichi Ito", "Takafumi Aoki", "Tetsushi Ohki", "Masakatsu Nishigaki"], "title": "Leveraging Intermediate Features of Vision Transformer for Face Anti-Spoofing", "categories": ["cs.CV"], "comment": "2025 IEEE/CVF Conference on Computer Vision and Pattern Recognition\n  Workshops (CVPRW)", "summary": "Face recognition systems are designed to be robust against changes in head\npose, illumination, and blurring during image capture. If a malicious person\npresents a face photo of the registered user, they may bypass the\nauthentication process illegally. Such spoofing attacks need to be detected\nbefore face recognition. In this paper, we propose a spoofing attack detection\nmethod based on Vision Transformer (ViT) to detect minute differences between\nlive and spoofed face images. The proposed method utilizes the intermediate\nfeatures of ViT, which have a good balance between local and global features\nthat are important for spoofing attack detection, for calculating loss in\ntraining and score in inference. The proposed method also introduces two data\naugmentation methods: face anti-spoofing data augmentation and patch-wise data\naugmentation, to improve the accuracy of spoofing attack detection. We\ndemonstrate the effectiveness of the proposed method through experiments using\nthe OULU-NPU and SiW datasets."}
{"id": "2505.24133", "pdf": "https://arxiv.org/pdf/2505.24133", "abs": "https://arxiv.org/abs/2505.24133", "authors": ["Zefan Cai", "Wen Xiao", "Hanshi Sun", "Cheng Luo", "Yikai Zhang", "Ke Wan", "Yucheng Li", "Yeyang Zhou", "Li-Wen Chang", "Jiuxiang Gu", "Zhen Dong", "Anima Anandkumar", "Abedelkadir Asi", "Junjie Hu"], "title": "R-KV: Redundancy-aware KV Cache Compression for Training-Free Reasoning Models Acceleration", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Reasoning models have demonstrated impressive performance in self-reflection\nand chain-of-thought reasoning. However, they often produce excessively long\noutputs, leading to prohibitively large key-value (KV) caches during inference.\nWhile chain-of-thought inference significantly improves performance on complex\nreasoning tasks, it can also lead to reasoning failures when deployed with\nexisting KV cache compression approaches. To address this, we propose\nRedundancy-aware KV Cache Compression for Reasoning models (R-KV), a novel\nmethod specifically targeting redundant tokens in reasoning models. Our method\npreserves nearly 100% of the full KV cache performance using only 10% of the KV\ncache, substantially outperforming existing KV cache baselines, which reach\nonly 60% of the performance. Remarkably, R-KV even achieves 105% of full KV\ncache performance with 16% of the KV cache. This KV-cache reduction also leads\nto a 90% memory saving and a 6.6X throughput over standard chain-of-thought\nreasoning inference. Experimental results show that R-KV consistently\noutperforms existing KV cache compression baselines across two mathematical\nreasoning datasets."}
{"id": "2505.24205", "pdf": "https://arxiv.org/pdf/2505.24205", "abs": "https://arxiv.org/abs/2505.24205", "authors": ["Mingze Wang", "Weinan E"], "title": "On the Expressive Power of Mixture-of-Experts for Structured Complex Tasks", "categories": ["cs.LG", "stat.ML"], "comment": "18 pages", "summary": "Mixture-of-experts networks (MoEs) have demonstrated remarkable efficiency in\nmodern deep learning. Despite their empirical success, the theoretical\nfoundations underlying their ability to model complex tasks remain poorly\nunderstood. In this work, we conduct a systematic study of the expressive power\nof MoEs in modeling complex tasks with two common structural priors:\nlow-dimensionality and sparsity. For shallow MoEs, we prove that they can\nefficiently approximate functions supported on low-dimensional manifolds,\novercoming the curse of dimensionality. For deep MoEs, we show that\n$\\cO(L)$-layer MoEs with $E$ experts per layer can approximate piecewise\nfunctions comprising $E^L$ pieces with compositional sparsity, i.e., they can\nexhibit an exponential number of structured tasks. Our analysis reveals the\nroles of critical architectural components and hyperparameters in MoEs,\nincluding the gating mechanism, expert networks, the number of experts, and the\nnumber of layers, and offers natural suggestions for MoE variants."}
{"id": "2505.24098", "pdf": "https://arxiv.org/pdf/2505.24098", "abs": "https://arxiv.org/abs/2505.24098", "authors": ["Zhongmou He", "Yee Man Choi", "Kexun Zhang", "Jiabao Ji", "Junting Zhou", "Dejia Xu", "Ivan Bercovich", "Aidan Zhang", "Lei Li"], "title": "HardTests: Synthesizing High-Quality Test Cases for LLM Coding", "categories": ["cs.CL"], "comment": null, "summary": "Verifiers play a crucial role in large language model (LLM) reasoning, needed\nby post-training techniques such as reinforcement learning. However, reliable\nverifiers are hard to get for difficult coding problems, because a\nwell-disguised wrong solution may only be detected by carefully human-written\nedge cases that are difficult to synthesize. To address this issue, we propose\nHARDTESTGEN, a pipeline for high-quality test synthesis using LLMs. With this\npipeline, we curate a comprehensive competitive programming dataset HARDTESTS\nwith 47k problems and synthetic high-quality tests. Compared with existing\ntests, HARDTESTGEN tests demonstrate precision that is 11.3 percentage points\nhigher and recall that is 17.5 percentage points higher when evaluating\nLLM-generated code. For harder problems, the improvement in precision can be as\nlarge as 40 points. HARDTESTS also proves to be more effective for model\ntraining, measured by downstream code generation performance. We will\nopen-source our dataset and synthesis pipeline at\nhttps://leililab.github.io/HardTests/."}
{"id": "2505.24404", "pdf": "https://arxiv.org/pdf/2505.24404", "abs": "https://arxiv.org/abs/2505.24404", "authors": ["Kanokphan Lertniphonphan", "Feng Chen", "Junda Xu", "Fengbu Lan", "Jun Xie", "Tao Zhang", "Zhepeng Wang"], "title": "PCIE_Interaction Solution for Ego4D Social Interaction Challenge", "categories": ["cs.CV"], "comment": null, "summary": "This report presents our team's PCIE_Interaction solution for the Ego4D\nSocial Interaction Challenge at CVPR 2025, addressing both Looking At Me (LAM)\nand Talking To Me (TTM) tasks. The challenge requires accurate detection of\nsocial interactions between subjects and the camera wearer, with LAM relying\nexclusively on face crop sequences and TTM combining speaker face crops with\nsynchronized audio segments. In the LAM track, we employ face quality\nenhancement and ensemble methods. For the TTM task, we extend visual\ninteraction analysis by fusing audio and visual cues, weighted by a visual\nquality score. Our approach achieved 0.81 and 0.71 mean average precision (mAP)\non the LAM and TTM challenges leader board. Code is available at\nhttps://github.com/KanokphanL/PCIE_Ego4D_Social_Interaction"}
{"id": "2505.24163", "pdf": "https://arxiv.org/pdf/2505.24163", "abs": "https://arxiv.org/abs/2505.24163", "authors": ["Jiaqi Sun", "Shiyou Qian", "Zhangchi Han", "Wei Li", "Zelin Qian", "Dingyu Yang", "Jian Cao", "Guangtao Xue"], "title": "LKD-KGC: Domain-Specific KG Construction via LLM-driven Knowledge Dependency Parsing", "categories": ["cs.CL", "cs.AI"], "comment": "Submitting to EDBT 2026", "summary": "Knowledge Graphs (KGs) structure real-world entities and their relationships\ninto triples, enhancing machine reasoning for various tasks. While\ndomain-specific KGs offer substantial benefits, their manual construction is\noften inefficient and requires specialized knowledge. Recent approaches for\nknowledge graph construction (KGC) based on large language models (LLMs), such\nas schema-guided KGC and reference knowledge integration, have proven\nefficient. However, these methods are constrained by their reliance on manually\ndefined schema, single-document processing, and public-domain references,\nmaking them less effective for domain-specific corpora that exhibit complex\nknowledge dependencies and specificity, as well as limited reference knowledge.\nTo address these challenges, we propose LKD-KGC, a novel framework for\nunsupervised domain-specific KG construction. LKD-KGC autonomously analyzes\ndocument repositories to infer knowledge dependencies, determines optimal\nprocessing sequences via LLM driven prioritization, and autoregressively\ngenerates entity schema by integrating hierarchical inter-document contexts.\nThis schema guides the unsupervised extraction of entities and relationships,\neliminating reliance on predefined structures or external knowledge. Extensive\nexperiments show that compared with state-of-the-art baselines, LKD-KGC\ngenerally achieves improvements of 10% to 20% in both precision and recall\nrate, demonstrating its potential in constructing high-quality domain-specific\nKGs."}
{"id": "2505.24231", "pdf": "https://arxiv.org/pdf/2505.24231", "abs": "https://arxiv.org/abs/2505.24231", "authors": ["Md Shahnawaz", "Bishwajit Prasad Gond", "Durga Prasad Mohapatra"], "title": "Dynamic Malware Classification of Windows PE Files using CNNs and Greyscale Images Derived from Runtime API Call Argument Conversion", "categories": ["cs.LG", "cs.AI", "cs.CR"], "comment": null, "summary": "Malware detection and classification remains a topic of concern for\ncybersecurity, since it is becoming common for attackers to use advanced\nobfuscation on their malware to stay undetected. Conventional static analysis\nis not effective against polymorphic and metamorphic malware as these change\ntheir appearance without modifying their behavior, thus defying the analysis by\ncode structure alone. This makes it important to use dynamic detection that\nmonitors malware behavior at runtime. In this paper, we present a dynamic\nmalware categorization framework that extracts API argument calls at the\nruntime execution of Windows Portable Executable (PE) files. Extracting and\nencoding the dynamic features of API names, argument return values, and other\nrelative features, we convert raw behavioral data to temporal patterns. To\nenhance feature portrayal, the generated patterns are subsequently converted\ninto grayscale pictures using a magma colormap. These improved photos are used\nto teach a Convolutional Neural Network (CNN) model discriminative features,\nwhich allows for reliable and accurate malware classification. Results from\nexperiments indicate that our method, with an average accuracy of 98.36% is\neffective in classifying different classes of malware and benign by integrating\ndynamic analysis and deep learning. It not only achieves high classification\naccuracy but also demonstrates significant resilience against typical evasion\nstrategies."}
{"id": "2505.24105", "pdf": "https://arxiv.org/pdf/2505.24105", "abs": "https://arxiv.org/abs/2505.24105", "authors": ["Jiacheng Lin", "Zhenbang Wu", "Jimeng Sun"], "title": "Training LLMs for EHR-Based Reasoning Tasks via Reinforcement Learning", "categories": ["cs.CL"], "comment": null, "summary": "We present EHRMIND, a practical recipe for adapting large language models\n(LLMs) to complex clinical reasoning tasks using reinforcement learning with\nverifiable rewards (RLVR). While RLVR has succeeded in mathematics and coding,\nits application to healthcare contexts presents unique challenges due to the\nspecialized knowledge and reasoning required for electronic health record (EHR)\ninterpretation. Our pilot study on the MEDCALC benchmark reveals two key\nfailure modes: (1) misapplied knowledge, where models possess relevant medical\nknowledge but apply it incorrectly, and (2) missing knowledge, where models\nlack essential domain knowledge. To address these cases, EHRMIND applies a\ntwo-stage solution: a lightweight supervised fine-tuning (SFT) warm-up that\ninjects missing domain knowledge, stabilizes subsequent training, and\nencourages structured, interpretable outputs; followed by RLVR, which\nreinforces outcome correctness and refines the model's decision-making. We\ndemonstrate the effectiveness of our method across diverse clinical\napplications, including medical calculations (MEDCALC), patient-trial matching\n(TREC CLINICAL TRIALS), and disease diagnosis (EHRSHOT). EHRMIND delivers\nconsistent gains in accuracy, interpretability, and cross-task generalization.\nThese findings offer practical guidance for applying RLVR to enhance LLM\ncapabilities in healthcare settings."}
{"id": "2505.24406", "pdf": "https://arxiv.org/pdf/2505.24406", "abs": "https://arxiv.org/abs/2505.24406", "authors": ["Hanting Wang", "Tao Jin", "Wang Lin", "Shulei Wang", "Hai Huang", "Shengpeng Ji", "Zhou Zhao"], "title": "IRBridge: Solving Image Restoration Bridge with Pre-trained Generative Diffusion Models", "categories": ["cs.CV"], "comment": null, "summary": "Bridge models in image restoration construct a diffusion process from\ndegraded to clear images. However, existing methods typically require training\na bridge model from scratch for each specific type of degradation, resulting in\nhigh computational costs and limited performance. This work aims to efficiently\nleverage pretrained generative priors within existing image restoration bridges\nto eliminate this requirement. The main challenge is that standard generative\nmodels are typically designed for a diffusion process that starts from pure\nnoise, while restoration tasks begin with a low-quality image, resulting in a\nmismatch in the state distributions between the two processes. To address this\nchallenge, we propose a transition equation that bridges two diffusion\nprocesses with the same endpoint distribution. Based on this, we introduce the\nIRBridge framework, which enables the direct utilization of generative models\nwithin image restoration bridges, offering a more flexible and adaptable\napproach to image restoration. Extensive experiments on six image restoration\ntasks demonstrate that IRBridge efficiently integrates generative priors,\nresulting in improved robustness and generalization performance. Code will be\navailable at GitHub."}
{"id": "2505.24243", "pdf": "https://arxiv.org/pdf/2505.24243", "abs": "https://arxiv.org/abs/2505.24243", "authors": ["Joohwan Ko", "Justin Domke"], "title": "Model Informed Flows for Bayesian Inference of Probabilistic Programs", "categories": ["cs.LG", "stat.ML"], "comment": null, "summary": "Variational inference often struggles with the posterior geometry exhibited\nby complex hierarchical Bayesian models. Recent advances in flow-based\nvariational families and Variationally Inferred Parameters (VIP) each address\naspects of this challenge, but their formal relationship is unexplored. Here,\nwe prove that the combination of VIP and a full-rank Gaussian can be\nrepresented exactly as a forward autoregressive flow augmented with a\ntranslation term and input from the model's prior. Guided by this theoretical\ninsight, we introduce the Model-Informed Flow (MIF) architecture, which adds\nthe necessary translation mechanism, prior information, and hierarchical\nordering. Empirically, MIF delivers tighter posterior approximations and\nmatches or exceeds state-of-the-art performance across a suite of hierarchical\nand non-hierarchical benchmarks."}
{"id": "2505.24119", "pdf": "https://arxiv.org/pdf/2505.24119", "abs": "https://arxiv.org/abs/2505.24119", "authors": ["Zheng-Xin Yong", "Beyza Ermis", "Marzieh Fadaee", "Stephen H. Bach", "Julia Kreutzer"], "title": "The State of Multilingual LLM Safety Research: From Measuring the Language Gap to Mitigating It", "categories": ["cs.CL"], "comment": null, "summary": "This paper presents a comprehensive analysis of the linguistic diversity of\nLLM safety research, highlighting the English-centric nature of the field.\nThrough a systematic review of nearly 300 publications from 2020--2024 across\nmajor NLP conferences and workshops at *ACL, we identify a significant and\ngrowing language gap in LLM safety research, with even high-resource\nnon-English languages receiving minimal attention. We further observe that\nnon-English languages are rarely studied as a standalone language and that\nEnglish safety research exhibits poor language documentation practice. To\nmotivate future research into multilingual safety, we make several\nrecommendations based on our survey, and we then pose three concrete future\ndirections on safety evaluation, training data generation, and crosslingual\nsafety generalization. Based on our survey and proposed directions, the field\ncan develop more robust, inclusive AI safety practices for diverse global\npopulations."}
{"id": "2505.24411", "pdf": "https://arxiv.org/pdf/2505.24411", "abs": "https://arxiv.org/abs/2505.24411", "authors": ["Feng Chen", "Kanokphan Lertniphonphan", "Qiancheng Yan", "Xiaohui Fan", "Jun Xie", "Tao Zhang", "Zhepeng Wang"], "title": "PCIE_Pose Solution for EgoExo4D Pose and Proficiency Estimation Challenge", "categories": ["cs.CV"], "comment": null, "summary": "This report introduces our team's (PCIE_EgoPose) solutions for the EgoExo4D\nPose and Proficiency Estimation Challenges at CVPR2025. Focused on the\nintricate task of estimating 21 3D hand joints from RGB egocentric videos,\nwhich are complicated by subtle movements and frequent occlusions, we developed\nthe Hand Pose Vision Transformer (HP-ViT+). This architecture synergizes a\nVision Transformer and a CNN backbone, using weighted fusion to refine the hand\npose predictions. For the EgoExo4D Body Pose Challenge, we adopted a multimodal\nspatio-temporal feature integration strategy to address the complexities of\nbody pose estimation across dynamic contexts. Our methods achieved remarkable\nperformance: 8.31 PA-MPJPE in the Hand Pose Challenge and 11.25 MPJPE in the\nBody Pose Challenge, securing championship titles in both competitions. We\nextended our pose estimation solutions to the Proficiency Estimation task,\napplying core technologies such as transformer-based architectures. This\nextension enabled us to achieve a top-1 accuracy of 0.53, a SOTA result, in the\nDemonstrator Proficiency Estimation competition."}
{"id": "2505.24252", "pdf": "https://arxiv.org/pdf/2505.24252", "abs": "https://arxiv.org/abs/2505.24252", "authors": ["Yizhong Ding"], "title": "A Reward-driven Automated Webshell Malicious-code Generator for Red-teaming", "categories": ["cs.CR", "cs.AI"], "comment": null, "summary": "Frequent cyber-attacks have elevated WebShell exploitation and defense to a\ncritical research focus within network security. However, there remains a\nsignificant shortage of publicly available, well-categorized malicious-code\ndatasets organized by obfuscation method. Existing malicious-code generation\nmethods, which primarily rely on prompt engineering, often suffer from limited\ndiversity and high redundancy in the payloads they produce. To address these\nlimitations, we propose \\textbf{RAWG}, a \\textbf{R}eward-driven\n\\textbf{A}utomated \\textbf{W}ebshell Malicious-code \\textbf{G}enerator designed\nfor red-teaming applications. Our approach begins by categorizing webshell\nsamples from common datasets into seven distinct types of obfuscation. We then\nemploy a large language model (LLM) to extract and normalize key tokens from\neach sample, creating a standardized, high-quality corpus. Using this curated\ndataset, we perform supervised fine-tuning (SFT) on an open-source large model\nto enable the generation of diverse, highly obfuscated webshell malicious\npayloads. To further enhance generation quality, we apply Proximal Policy\nOptimization (PPO), treating malicious-code samples as \"chosen\" data and benign\ncode as \"rejected\" data during reinforcement learning. Extensive experiments\ndemonstrate that RAWG significantly outperforms current state-of-the-art\nmethods in both payload diversity and escape effectiveness."}
{"id": "2505.24254", "pdf": "https://arxiv.org/pdf/2505.24254", "abs": "https://arxiv.org/abs/2505.24254", "authors": ["Zheng Wang", "Wanhao Yu", "Li Yang", "Sen Lin"], "title": "Rethinking Continual Learning with Progressive Neural Collapse", "categories": ["cs.LG"], "comment": null, "summary": "Continual Learning (CL) seeks to build an agent that can continuously learn a\nsequence of tasks, where a key challenge, namely Catastrophic Forgetting,\npersists due to the potential knowledge interference among different tasks. On\nthe other hand, deep neural networks (DNNs) are shown to converge to a terminal\nstate termed Neural Collapse during training, where all class prototypes\ngeometrically form a static simplex equiangular tight frame (ETF). These\nmaximally and equally separated class prototypes make the ETF an ideal target\nfor model learning in CL to mitigate knowledge interference. Thus inspired,\nseveral studies have emerged very recently to leverage a fixed global ETF in\nCL, which however suffers from key drawbacks, such as impracticability and\nlimited performance.To address these challenges and fully unlock the potential\nof ETF in CL, we propose Progressive Neural Collapse (ProNC), a novel framework\nthat completely removes the need of a fixed global ETF in CL. Specifically,\nProNC progressively expands the ETF target in a principled way by adding new\nclass prototypes as vertices for new tasks, ensuring maximal separability\nacross all encountered classes with minimal shifts from the previous ETF. We\nnext develop a new CL framework by plugging ProNC into commonly used CL\nalgorithm designs, where distillation is further leveraged to balance between\ntarget shifting for old classes and target aligning for new classes. Extensive\nexperiments show that our approach significantly outperforms related baselines\nwhile maintaining superior flexibility, simplicity, and efficiency."}
{"id": "2505.24143", "pdf": "https://arxiv.org/pdf/2505.24143", "abs": "https://arxiv.org/abs/2505.24143", "authors": ["Jinglong Gao", "Xiao Ding", "Lingxiao Zou", "Bing Qin", "Ting Liu"], "title": "CrossICL: Cross-Task In-Context Learning via Unsupervised Demonstration Transfer", "categories": ["cs.CL"], "comment": "9 pages", "summary": "In-Context Learning (ICL) enhances the performance of large language models\n(LLMs) with demonstrations. However, obtaining these demonstrations primarily\nrelies on manual effort. In most real-world scenarios, users are often\nunwilling or unable to provide such demonstrations. Inspired by the human\nanalogy, we explore a new ICL paradigm CrossICL to study how to utilize\nexisting source task demonstrations in the ICL for target tasks, thereby\nobtaining reliable guidance without any additional manual effort. To explore\nthis, we first design a two-stage alignment strategy to mitigate the\ninterference caused by gaps across tasks, as the foundation for our\nexperimental exploration. Based on it, we conduct comprehensive exploration of\nCrossICL, with 875 NLP tasks from the Super-NI benchmark and six types of LLMs,\nincluding GPT-4o. Experimental results demonstrate the effectiveness of\nCrossICL and provide valuable insights on questions like the criteria for\nselecting cross-task demonstrations, as well as the types of task-gap-induced\ninterference in CrossICL."}
{"id": "2505.24417", "pdf": "https://arxiv.org/pdf/2505.24417", "abs": "https://arxiv.org/abs/2505.24417", "authors": ["Runnan Lu", "Yuxuan Zhang", "Jiaming Liu", "Haofan Wang", "Yiren Song"], "title": "EasyText: Controllable Diffusion Transformer for Multilingual Text Rendering", "categories": ["cs.CV"], "comment": null, "summary": "Generating accurate multilingual text with diffusion models has long been\ndesired but remains challenging. Recent methods have made progress in rendering\ntext in a single language, but rendering arbitrary languages is still an\nunexplored area. This paper introduces EasyText, a text rendering framework\nbased on DiT (Diffusion Transformer), which connects denoising latents with\nmultilingual character tokens encoded as character tokens. We propose character\npositioning encoding and position encoding interpolation techniques to achieve\ncontrollable and precise text rendering. Additionally, we construct a\nlarge-scale synthetic text image dataset with 1 million multilingual image-text\nannotations as well as a high-quality dataset of 20K annotated images, which\nare used for pretraining and fine-tuning respectively. Extensive experiments\nand evaluations demonstrate the effectiveness and advancement of our approach\nin multilingual text rendering, visual quality, and layout-aware text\nintegration."}
{"id": "2505.24255", "pdf": "https://arxiv.org/pdf/2505.24255", "abs": "https://arxiv.org/abs/2505.24255", "authors": ["Neemesh Yadav", "Palakorn Achananuparp", "Jing Jiang", "Ee-Peng Lim"], "title": "Effects of Theory of Mind and Prosocial Beliefs on Steering Human-Aligned Behaviors of LLMs in Ultimatum Games", "categories": ["cs.CL", "cs.AI", "cs.HC"], "comment": "17 pages, 1 figure, 6 tables", "summary": "Large Language Models (LLMs) have shown potential in simulating human\nbehaviors and performing theory-of-mind (ToM) reasoning, a crucial skill for\ncomplex social interactions. In this study, we investigate the role of ToM\nreasoning in aligning agentic behaviors with human norms in negotiation tasks,\nusing the ultimatum game as a controlled environment. We initialized LLM agents\nwith different prosocial beliefs (including Greedy, Fair, and Selfless) and\nreasoning methods like chain-of-thought (CoT) and varying ToM levels, and\nexamined their decision-making processes across diverse LLMs, including\nreasoning models like o3-mini and DeepSeek-R1 Distilled Qwen 32B. Results from\n2,700 simulations indicated that ToM reasoning enhances behavior alignment,\ndecision-making consistency, and negotiation outcomes. Consistent with previous\nfindings, reasoning models exhibit limited capability compared to models with\nToM reasoning, different roles of the game benefits with different orders of\nToM reasoning. Our findings contribute to the understanding of ToM's role in\nenhancing human-AI interaction and cooperative decision-making. The code used\nfor our experiments can be found at https://github.com/Stealth-py/UltimatumToM."}
{"id": "2505.24261", "pdf": "https://arxiv.org/pdf/2505.24261", "abs": "https://arxiv.org/abs/2505.24261", "authors": ["Weiyi Wang", "Junwei Deng", "Yuzheng Hu", "Shiyuan Zhang", "Xirui Jiang", "Runting Zhang", "Han Zhao", "Jiaqi W. Ma"], "title": "Taming Hyperparameter Sensitivity in Data Attribution: Practical Selection Without Costly Retraining", "categories": ["cs.LG", "stat.ML"], "comment": null, "summary": "Data attribution methods, which quantify the influence of individual training\ndata points on a machine learning model, have gained increasing popularity in\ndata-centric applications in modern AI. Despite a recent surge of new methods\ndeveloped in this space, the impact of hyperparameter tuning in these methods\nremains under-explored. In this work, we present the first large-scale\nempirical study to understand the hyperparameter sensitivity of common data\nattribution methods. Our results show that most methods are indeed sensitive to\ncertain key hyperparameters. However, unlike typical machine learning\nalgorithms -- whose hyperparameters can be tuned using computationally-cheap\nvalidation metrics -- evaluating data attribution performance often requires\nretraining models on subsets of training data, making such metrics\nprohibitively costly for hyperparameter tuning. This poses a critical open\nchallenge for the practical application of data attribution methods. To address\nthis challenge, we advocate for better theoretical understandings of\nhyperparameter behavior to inform efficient tuning strategies. As a case study,\nwe provide a theoretical analysis of the regularization term that is critical\nin many variants of influence function methods. Building on this analysis, we\npropose a lightweight procedure for selecting the regularization value without\nmodel retraining, and validate its effectiveness across a range of standard\ndata attribution benchmarks. Overall, our study identifies a fundamental yet\noverlooked challenge in the practical application of data attribution, and\nhighlights the importance of careful discussion on hyperparameter selection in\nfuture method development."}
{"id": "2505.24147", "pdf": "https://arxiv.org/pdf/2505.24147", "abs": "https://arxiv.org/abs/2505.24147", "authors": ["Chiwei Zhu", "Benfeng Xu", "An Yang", "Junyang Lin", "Quan Wang", "Chang Zhou", "Zhendong Mao"], "title": "Rationales Are Not Silver Bullets: Measuring the Impact of Rationales on Model Performance and Reliability", "categories": ["cs.CL"], "comment": "To be published in ACL 2025 Findings. (Work originally done in Jan\n  2024)", "summary": "Training language models with rationales augmentation has been shown to be\nbeneficial in many existing works. In this paper, we identify that such a\nprevailing view does not hold consistently. We conduct comprehensive\ninvestigations to thoroughly inspect the impact of rationales on model\nperformance as well as a novel perspective of model reliability. The results\nlead to several key findings that add new insights upon existing\nunderstandings: 1) Rationales can, at times, deteriorate model performance; 2)\nRationales can, at times, improve model reliability, even outperforming their\nuntrained counterparts; 3) A linear correspondence exists in between the\nperformance and reliability improvements, while both are driven by the\nintrinsic difficulty of the task. These findings provide informative\nregulations on the broad utilization of rationales and raise critical\nimplications on the procedure of explicitly aligning language models with\nimplicit human thoughts. Codes can be found at\nhttps://github.com/Ignoramus0817/rationales."}
{"id": "2505.24431", "pdf": "https://arxiv.org/pdf/2505.24431", "abs": "https://arxiv.org/abs/2505.24431", "authors": ["Bozhong Zheng", "Jinye Gan", "Xiaohao Xu", "Wenqiao Li", "Xiaonan Huang", "Na Ni", "Yingna Wu"], "title": "Bridging 3D Anomaly Localization and Repair via High-Quality Continuous Geometric Representation", "categories": ["cs.CV"], "comment": null, "summary": "3D point cloud anomaly detection is essential for robust vision systems but\nis challenged by pose variations and complex geometric anomalies. Existing\npatch-based methods often suffer from geometric fidelity issues due to discrete\nvoxelization or projection-based representations, limiting fine-grained anomaly\nlocalization. We introduce Pose-Aware Signed Distance Field (PASDF), a novel\nframework that integrates 3D anomaly detection and repair by learning a\ncontinuous, pose-invariant shape representation. PASDF leverages a Pose\nAlignment Module for canonicalization and a SDF Network to dynamically\nincorporate pose, enabling implicit learning of high-fidelity anomaly repair\ntemplates from the continuous SDF. This facilitates precise pixel-level anomaly\nlocalization through an Anomaly-Aware Scoring Module. Crucially, the continuous\n3D representation in PASDF extends beyond detection, facilitating in-situ\nanomaly repair. Experiments on Real3D-AD and Anomaly-ShapeNet demonstrate\nstate-of-the-art performance, achieving high object-level AUROC scores of 80.2%\nand 90.0%, respectively. These results highlight the effectiveness of\ncontinuous geometric representations in advancing 3D anomaly detection and\nfacilitating practical anomaly region repair. The code is available at\nhttps://github.com/ZZZBBBZZZ/PASDF to support further research."}
{"id": "2505.24264", "pdf": "https://arxiv.org/pdf/2505.24264", "abs": "https://arxiv.org/abs/2505.24264", "authors": ["Xin Quan", "Marco Valentino", "Louise A. Dennis", "AndrÃ© Freitas"], "title": "Faithful and Robust LLM-Driven Theorem Proving for NLI Explanations", "categories": ["cs.CL", "cs.AI"], "comment": "Camera-ready for ACL 2025", "summary": "Natural language explanations play a fundamental role in Natural Language\nInference (NLI) by revealing how premises logically entail hypotheses. Recent\nwork has shown that the interaction of large language models (LLMs) with\ntheorem provers (TPs) can help verify and improve the validity of NLI\nexplanations. However, TPs require translating natural language into\nmachine-verifiable formal representations, a process that introduces the risk\nof semantic information loss and unfaithful interpretation, an issue compounded\nby LLMs' challenges in capturing critical logical structures with sufficient\nprecision. Moreover, LLMs are still limited in their capacity for rigorous and\nrobust proof construction within formal verification frameworks. To mitigate\nissues related to faithfulness and robustness, this paper investigates\nstrategies to (1) alleviate semantic loss during autoformalisation, (2)\nefficiently identify and correct syntactic errors in logical representations,\n(3) explicitly use logical expressions to guide LLMs in generating structured\nproof sketches, and (4) increase LLMs' capacity of interpreting TP's feedback\nfor iterative refinement. Our empirical results on e-SNLI, QASC and WorldTree\nusing different LLMs demonstrate that the proposed strategies yield significant\nimprovements in autoformalisation (+18.46%, +34.2%, +39.77%) and explanation\nrefinement (+29.5%, +51.5%, +41.25%) over the state-of-the-art model. Moreover,\nwe show that specific interventions on the hybrid LLM-TP architecture can\nsubstantially improve efficiency, drastically reducing the number of iterations\nrequired for successful verification."}
{"id": "2505.24262", "pdf": "https://arxiv.org/pdf/2505.24262", "abs": "https://arxiv.org/abs/2505.24262", "authors": ["Hiroki Naganuma", "Kotaro Yoshida", "Laura Gomezjurado Gonzalez", "Takafumi Horie", "Yuji Naraki", "Ryotaro Shimizu"], "title": "On Fairness of Task Arithmetic: The Role of Task Vectors", "categories": ["cs.LG"], "comment": null, "summary": "Model editing techniques, particularly task arithmetic using task vectors,\nhave shown promise in efficiently modifying pre-trained models through\narithmetic operations like task addition and negation. Despite computational\nadvantages, these methods may inadvertently affect model fairness, creating\nrisks in sensitive applications like hate speech detection. However, the\nfairness implications of task arithmetic remain largely unexplored, presenting\na critical gap in the existing literature. We systematically examine how\nmanipulating task vectors affects fairness metrics, including Demographic\nParity and Equalized Odds. To rigorously assess these effects, we benchmark\ntask arithmetic against full fine-tuning, a costly but widely used baseline,\nand Low-Rank Adaptation (LoRA), a prevalent parameter-efficient fine-tuning\nmethod. Additionally, we explore merging task vectors from models fine-tuned on\ndemographic subgroups vulnerable to hate speech, investigating whether fairness\noutcomes can be controlled by adjusting task vector coefficients, potentially\nenabling tailored model behavior. Our results offer novel insights into the\nfairness implications of model editing and establish a foundation for\nfairness-aware and responsible model editing practices."}
{"id": "2505.24164", "pdf": "https://arxiv.org/pdf/2505.24164", "abs": "https://arxiv.org/abs/2505.24164", "authors": ["Shilin Xu", "Yanwei Li", "Rui Yang", "Tao Zhang", "Yueyi Sun", "Wei Chow", "Linfeng Li", "Hang Song", "Qi Xu", "Yunhai Tong", "Xiangtai Li", "Hao Fei"], "title": "Mixed-R1: Unified Reward Perspective For Reasoning Capability in Multimodal Large Language Models", "categories": ["cs.CL", "cs.CV"], "comment": null, "summary": "Recent works on large language models (LLMs) have successfully demonstrated\nthe emergence of reasoning capabilities via reinforcement learning (RL).\nAlthough recent efforts leverage group relative policy optimization (GRPO) for\nMLLMs post-training, they constantly explore one specific aspect, such as\ngrounding tasks, math problems, or chart analysis. There are no works that can\nleverage multi-source MLLM tasks for stable reinforcement learning. In this\nwork, we present a unified perspective to solve this problem. We present\nMixed-R1, a unified yet straightforward framework that contains a mixed reward\nfunction design (Mixed-Reward) and a mixed post-training dataset (Mixed-45K).\nWe first design a data engine to select high-quality examples to build the\nMixed-45K post-training dataset. Then, we present a Mixed-Reward design, which\ncontains various reward functions for various MLLM tasks. In particular, it has\nfour different reward functions: matching reward for binary answer or\nmultiple-choice problems, chart reward for chart-aware datasets, IoU reward for\ngrounding problems, and open-ended reward for long-form text responses such as\ncaption datasets. To handle the various long-form text content, we propose a\nnew open-ended reward named Bidirectional Max-Average Similarity (BMAS) by\nleveraging tokenizer embedding matching between the generated response and the\nground truth. Extensive experiments show the effectiveness of our proposed\nmethod on various MLLMs, including Qwen2.5-VL and Intern-VL on various sizes.\nOur dataset and model are available at https://github.com/xushilin1/mixed-r1."}
{"id": "2505.24441", "pdf": "https://arxiv.org/pdf/2505.24441", "abs": "https://arxiv.org/abs/2505.24441", "authors": ["Chunxu Liu", "Chi Xie", "Xiaxu Chen", "Wei Li", "Feng Zhu", "Rui Zhao", "Limin Wang"], "title": "SORCE: Small Object Retrieval in Complex Environments", "categories": ["cs.CV"], "comment": "Project Page: https://github.com/MCG-NJU/SORCE", "summary": "Text-to-Image Retrieval (T2IR) is a highly valuable task that aims to match a\ngiven textual query to images in a gallery. Existing benchmarks primarily focus\non textual queries describing overall image semantics or foreground salient\nobjects, possibly overlooking inconspicuous small objects, especially in\ncomplex environments. Such small object retrieval is crucial, as in real-world\napplications, the targets of interest are not always prominent in the image.\nThus, we introduce SORCE (Small Object Retrieval in Complex Environments), a\nnew subfield of T2IR, focusing on retrieving small objects in complex images\nwith textual queries. We propose a new benchmark, SORCE-1K, consisting of\nimages with complex environments and textual queries describing less\nconspicuous small objects with minimal contextual cues from other salient\nobjects. Preliminary analysis on SORCE-1K finds that existing T2IR methods\nstruggle to capture small objects and encode all the semantics into a single\nembedding, leading to poor retrieval performance on SORCE-1K. Therefore, we\npropose to represent each image with multiple distinctive embeddings. We\nleverage Multimodal Large Language Models (MLLMs) to extract multiple\nembeddings for each image instructed by a set of Regional Prompts (ReP).\nExperimental results show that our multi-embedding approach through MLLM and\nReP significantly outperforms existing T2IR methods on SORCE-1K. Our\nexperiments validate the effectiveness of SORCE-1K for benchmarking SORCE\nperformances, highlighting the potential of multi-embedding representation and\ntext-customized MLLM features for addressing this task."}
{"id": "2505.24269", "pdf": "https://arxiv.org/pdf/2505.24269", "abs": "https://arxiv.org/abs/2505.24269", "authors": ["Aleksandr Algazinov", "Joydeep Chandra", "Matt Laing"], "title": "INSIGHT: A Survey of In-Network Systems for Intelligent, High-Efficiency AI and Topology Optimization", "categories": ["cs.NI", "cs.AI"], "comment": null, "summary": "In-network computation represents a transformative approach to addressing the\nescalating demands of Artificial Intelligence (AI) workloads on network\ninfrastructure. By leveraging the processing capabilities of network devices\nsuch as switches, routers, and Network Interface Cards (NICs), this paradigm\nenables AI computations to be performed directly within the network fabric,\nsignificantly reducing latency, enhancing throughput, and optimizing resource\nutilization. This paper provides a comprehensive analysis of optimizing\nin-network computation for AI, exploring the evolution of programmable network\narchitectures, such as Software-Defined Networking (SDN) and Programmable Data\nPlanes (PDPs), and their convergence with AI. It examines methodologies for\nmapping AI models onto resource-constrained network devices, addressing\nchallenges like limited memory and computational capabilities through efficient\nalgorithm design and model compression techniques. The paper also highlights\nadvancements in distributed learning, particularly in-network aggregation, and\nthe potential of federated learning to enhance privacy and scalability.\nFrameworks like Planter and Quark are discussed for simplifying development,\nalongside key applications such as intelligent network monitoring, intrusion\ndetection, traffic management, and Edge AI. Future research directions,\nincluding runtime programmability, standardized benchmarks, and new\napplications paradigms, are proposed to advance this rapidly evolving field.\nThis survey underscores the potential of in-network AI to create intelligent,\nefficient, and responsive networks capable of meeting the demands of\nnext-generation AI applications."}
{"id": "2505.24275", "pdf": "https://arxiv.org/pdf/2505.24275", "abs": "https://arxiv.org/abs/2505.24275", "authors": ["Mingze Wang", "Jinbo Wang", "Jiaqi Zhang", "Wei Wang", "Peng Pei", "Xunliang Cai", "Weinan E", "Lei Wu"], "title": "GradPower: Powering Gradients for Faster Language Model Pre-Training", "categories": ["cs.LG", "math.OC", "stat.ML"], "comment": "22 pages", "summary": "We propose GradPower, a lightweight gradient-transformation technique for\naccelerating language model pre-training. Given a gradient vector $g=(g_i)_i$,\nGradPower first applies the elementwise sign-power transformation:\n$\\varphi_p(g)=({\\rm sign}(g_i)|g_i|^p)_{i}$ for a fixed $p>0$, and then feeds\nthe transformed gradient into a base optimizer. Notably, GradPower requires\nonly a single-line code change and no modifications to the base optimizer's\ninternal logic, including the hyperparameters. When applied to Adam (termed\nAdamPower), GradPower consistently achieves lower terminal loss across diverse\narchitectures (LLaMA, Qwen2MoE), parameter scales (66M to 2B), datasets (C4,\nOpenWebText), and learning-rate schedules (cosine, warmup-stable-decay). The\nmost pronounced gains are observed when training modern mixture-of-experts\nmodels with warmup-stable-decay schedules. GradPower also integrates seamlessly\nwith other state-of-the-art optimizers, such as Muon, yielding further\nimprovements. Finally, we provide theoretical analyses that reveal the\nunderlying mechanism of GradPower and highlights the influence of gradient\nnoise."}
{"id": "2505.24165", "pdf": "https://arxiv.org/pdf/2505.24165", "abs": "https://arxiv.org/abs/2505.24165", "authors": ["Yixuan Wang", "Shiqi Zhou", "Chuanzhe Guo", "Qingfu Zhu"], "title": "Tag-Evol: Achieving Efficient Instruction Evolving via Tag Injection", "categories": ["cs.CL"], "comment": "Accepted as Findings of ACL 2025", "summary": "Evol-Instruct has made significant improvements as a data synthesis method in\nseveral areas. Existing methods typically rely on a fixed set of strategies to\nevolve, which require manual design and are monolithic in form. In addition,\niterative evolution also makes the acquisition of hard samples expensive. In\nview of this, we propose the Tag-Evol framework, a more diverse and efficient\ninstruction evolving method. Specifically, Tag-Evol uses diverse and specific\nknowledge tags as strategies to achieve controlled evolution by injecting\ndifferent combinations of tags into the original instructions. Experiments with\nmultiple backbones in diverse domain benchmarks show that the proposed method\ngenerates significantly better evolved data than other methods. Furthermore, we\nconduct a thorough analysis of the evolved data, demonstrating that Tag-Evol is\nnot only efficient but also generates more diverse and challenging data."}
{"id": "2505.24443", "pdf": "https://arxiv.org/pdf/2505.24443", "abs": "https://arxiv.org/abs/2505.24443", "authors": ["Heejo Kong", "Sung-Jin Kim", "Gunho Jung", "Seong-Whan Lee"], "title": "Diversify and Conquer: Open-set Disagreement for Robust Semi-supervised Learning with Outliers", "categories": ["cs.CV", "cs.LG"], "comment": "Accepted by IEEE Transactions on Neural Networks and Learning Systems\n  (TNNLS)", "summary": "Conventional semi-supervised learning (SSL) ideally assumes that labeled and\nunlabeled data share an identical class distribution, however in practice, this\nassumption is easily violated, as unlabeled data often includes unknown class\ndata, i.e., outliers. The outliers are treated as noise, considerably degrading\nthe performance of SSL models. To address this drawback, we propose a novel\nframework, Diversify and Conquer (DAC), to enhance SSL robustness in the\ncontext of open-set semi-supervised learning. In particular, we note that\nexisting open-set SSL methods rely on prediction discrepancies between inliers\nand outliers from a single model trained on labeled data. This approach can be\neasily failed when the labeled data is insufficient, leading to performance\ndegradation that is worse than naive SSL that do not account for outliers. In\ncontrast, our approach exploits prediction disagreements among multiple models\nthat are differently biased towards the unlabeled distribution. By leveraging\nthe discrepancies arising from training on unlabeled data, our method enables\nrobust outlier detection even when the labeled data is underspecified. Our key\ncontribution is constructing a collection of differently biased models through\na single training process. By encouraging divergent heads to be differently\nbiased towards outliers while making consistent predictions for inliers, we\nexploit the disagreement among these heads as a measure to identify unknown\nconcepts. Our code is available at https://github.com/heejokong/DivCon."}
{"id": "2505.24293", "pdf": "https://arxiv.org/pdf/2505.24293", "abs": "https://arxiv.org/abs/2505.24293", "authors": ["James R. Golden"], "title": "Large Language Models are Locally Linear Mappings", "categories": ["cs.LG", "cs.AI", "cs.CL"], "comment": "Version 0", "summary": "We demonstrate that the inference operations of several open-weight large\nlanguage models (LLMs) can be mapped to an exactly equivalent linear system for\nan input sequence without modifying the model weights or altering output\npredictions. Extending techniques from image diffusion models that exhibit\nlocal or piecewise linearity, we strategically alter the gradient computation\nwith respect to a given input sequence for a next-token prediction such that\nthe Jacobian of the model nearly exactly reproduces the forward prediction with\na linear system. We demonstrate this approach across models (Llama 3, Gemma 3,\nQwen 3, Phi 4, Mistral Ministral and OLMo 2, up to Llama 3.3 70B Q4) and show\nthrough the singular value decomposition of the detached Jacobian that these\nLLMs operate in extremely low-dimensional subspaces where many of the largest\nsingular vectors decode to concepts related to the most-likely output token.\nThis approach also allows us to examine the operation of each successive layer\n(and its attention and MLP components) as nearly-exact linear systems and\nobserve the emergence of semantic concepts. Despite their expressive power and\nglobal nonlinearity, modern LLMs can be interpreted through nearly-exact\nlocally linear decompositions that provide insights into their internal\nrepresentations and reveal interpretable semantic structures in the next-token\nprediction process."}
{"id": "2505.24298", "pdf": "https://arxiv.org/pdf/2505.24298", "abs": "https://arxiv.org/abs/2505.24298", "authors": ["Wei Fu", "Jiaxuan Gao", "Xujie Shen", "Chen Zhu", "Zhiyu Mei", "Chuyi He", "Shusheng Xu", "Guo Wei", "Jun Mei", "Jiashu Wang", "Tongkai Yang", "Binhang Yuan", "Yi Wu"], "title": "AReaL: A Large-Scale Asynchronous Reinforcement Learning System for Language Reasoning", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Reinforcement learning (RL) has become a trending paradigm for training large\nlanguage models (LLMs), particularly for reasoning tasks. Effective RL for LLMs\nrequires massive parallelization and poses an urgent need for efficient\ntraining systems. Most existing large-scale RL systems for LLMs are synchronous\nby alternating generation and training in a batch setting, where the rollouts\nin each training batch are generated by the same (or latest) model. This\nstabilizes RL training but suffers from severe system-level inefficiency.\nGeneration must wait until the longest output in the batch is completed before\nmodel update, resulting in GPU underutilization. We present AReaL, a\n\\emph{fully asynchronous} RL system that completely decouples generation from\ntraining. Rollout workers in AReaL continuously generate new outputs without\nwaiting, while training workers update the model whenever a batch of data is\ncollected. AReaL also incorporates a collection of system-level optimizations,\nleading to substantially higher GPU utilization. To stabilize RL training,\nAReaL balances the workload of rollout and training workers to control data\nstaleness, and adopts a staleness-enhanced PPO variant to better handle\noutdated training samples. Extensive experiments on math and code reasoning\nbenchmarks show that AReaL achieves \\textbf{up to 2.57$\\times$ training\nspeedup} compared to the best synchronous systems with the same number of GPUs\nand matched or even improved final performance. The code of AReaL is available\nat https://github.com/inclusionAI/AReaL/."}
{"id": "2505.24174", "pdf": "https://arxiv.org/pdf/2505.24174", "abs": "https://arxiv.org/abs/2505.24174", "authors": ["Ryota Miyano", "Yuki Arase"], "title": "Adaptive LoRA Merge with Parameter Pruning for Low-Resource Generation", "categories": ["cs.CL", "cs.LG"], "comment": "Accepted at ACL2025 Findings", "summary": "This study proposes a simple yet effective LoRA merge method to achieve LLM\nadaptation for low-resource language generation tasks. The LoRA merge\ntechnique, which integrates multiple LoRA modules trained on different tasks,\nhas gained attention as an effective and efficient approach for adapting LLMs\nto target tasks. However, previous methods are limited in adaptability as they\nkeep the LoRA parameters frozen. Additionally, the low-resource problem has\nbeen out of their scope. We propose a LoRA merge method that updates and prunes\nLoRA parameters through fine-tuning with minimal target task data, which allows\nfiner-grained adjustments of LoRA parameters and enhancement of task\nadaptability. Extensive experiments have been conducted taking summarization as\na benchmark task. Our datasets cover various domains and multiple languages of\nEnglish and Japanese. The results confirm that the proposed method achieves\nsignificant and consistent improvements in task adaptability over the previous\nmethods."}
{"id": "2505.24466", "pdf": "https://arxiv.org/pdf/2505.24466", "abs": "https://arxiv.org/abs/2505.24466", "authors": ["Yingjia Xu", "Jinlin Wu", "Zhen Chen", "Daming Gao", "Yang Yang", "Zhen Lei", "Min Cao"], "title": "SA-Person: Text-Based Person Retrieval with Scene-aware Re-ranking", "categories": ["cs.CV"], "comment": "22 pages, 7 figures. Under review", "summary": "Text-based person retrieval aims to identify a target individual from a\ngallery of images based on a natural language description. It presents a\nsignificant challenge due to the complexity of real-world scenes and the\nambiguity of appearance-related descriptions. Existing methods primarily\nemphasize appearance-based cross-modal retrieval, often neglecting the\ncontextual information embedded within the scene, which can offer valuable\ncomplementary insights for retrieval. To address this, we introduce\nSCENEPERSON-13W, a large-scale dataset featuring over 100,000 scenes with rich\nannotations covering both pedestrian appearance and environmental cues. Based\non this, we propose SA-Person, a two-stage retrieval framework. In the first\nstage, it performs discriminative appearance grounding by aligning textual cues\nwith pedestrian-specific regions. In the second stage, it introduces\nSceneRanker, a training-free, scene-aware re-ranking method leveraging\nmultimodal large language models to jointly reason over pedestrian appearance\nand the global scene context. Experiments on SCENEPERSON-13W validate the\neffectiveness of our framework in challenging scene-level retrieval scenarios.\nThe code and dataset will be made publicly available."}
{"id": "2505.24341", "pdf": "https://arxiv.org/pdf/2505.24341", "abs": "https://arxiv.org/abs/2505.24341", "authors": ["Shujian Yang", "Shiyao Cui", "Chuanrui Hu", "Haicheng Wang", "Tianwei Zhang", "Minlie Huang", "Jialiang Lu", "Han Qiu"], "title": "Exploring Multimodal Challenges in Toxic Chinese Detection: Taxonomy, Benchmark, and Findings", "categories": ["cs.CL", "cs.AI", "cs.CY"], "comment": "Accepted to ACL 2025 (Findings). Camera-ready version", "summary": "Detecting toxic content using language models is important but challenging.\nWhile large language models (LLMs) have demonstrated strong performance in\nunderstanding Chinese, recent studies show that simple character substitutions\nin toxic Chinese text can easily confuse the state-of-the-art (SOTA) LLMs. In\nthis paper, we highlight the multimodal nature of Chinese language as a key\nchallenge for deploying LLMs in toxic Chinese detection. First, we propose a\ntaxonomy of 3 perturbation strategies and 8 specific approaches in toxic\nChinese content. Then, we curate a dataset based on this taxonomy, and\nbenchmark 9 SOTA LLMs (from both the US and China) to assess if they can detect\nperturbed toxic Chinese text. Additionally, we explore cost-effective\nenhancement solutions like in-context learning (ICL) and supervised fine-tuning\n(SFT). Our results reveal two important findings. (1) LLMs are less capable of\ndetecting perturbed multimodal Chinese toxic contents. (2) ICL or SFT with a\nsmall number of perturbed examples may cause the LLMs \"overcorrect'':\nmisidentify many normal Chinese contents as toxic."}
{"id": "2505.24313", "pdf": "https://arxiv.org/pdf/2505.24313", "abs": "https://arxiv.org/abs/2505.24313", "authors": ["Gengze Xu", "Wei Yao", "Ziqiao Wang", "Yong Liu"], "title": "On the Emergence of Weak-to-Strong Generalization: A Bias-Variance Perspective", "categories": ["cs.LG"], "comment": null, "summary": "Weak-to-strong generalization (W2SG) refers to the phenomenon where a strong\nstudent model, trained on a dataset labeled by a weak teacher, ultimately\noutperforms the teacher on the target task. Recent studies attribute this\nperformance gain to the prediction misfit between the student and teacher\nmodels. In this work, we theoretically investigate the emergence of W2SG\nthrough a generalized bias-variance decomposition of Bregman divergence.\nSpecifically, we show that the expected population risk gap between the student\nand teacher is quantified by the expected misfit between the two models. While\nthis aligns with previous results, our analysis removes several restrictive\nassumptions, most notably, the convexity of the student's hypothesis class,\nrequired in earlier works. Moreover, we show that W2SG is more likely to emerge\nwhen the student model approximates its posterior mean teacher, rather than\nmimicking an individual teacher. Using a concrete example, we demonstrate that\nif the student model has significantly larger capacity than the teacher, it can\nindeed converge to this posterior mean. Our analysis also suggests that\navoiding overfitting to the teacher's supervision and reducing the entropy of\nstudent's prediction further facilitate W2SG. In addition, we show that the\nreverse cross-entropy loss, unlike the standard forward cross-entropy, is less\nsensitive to the predictive uncertainty of the teacher. Finally, we empirically\nverify our theoretical insights and demonstrate that incorporating the reverse\ncross-entropy loss consistently improves student performance."}
{"id": "2505.24187", "pdf": "https://arxiv.org/pdf/2505.24187", "abs": "https://arxiv.org/abs/2505.24187", "authors": ["Mikhail L. Arbuzov", "Alexey A. Shvets", "Sisong Beir"], "title": "Beyond Exponential Decay: Rethinking Error Accumulation in Large Language Models", "categories": ["cs.CL"], "comment": null, "summary": "The prevailing assumption of an exponential decay in large language model\n(LLM) reliability with sequence length, predicated on independent per-token\nerror probabilities, posits an inherent limitation for long autoregressive\noutputs. Our research fundamentally challenges this view by synthesizing\nemerging evidence that LLM errors are not uniformly distributed but are\nconcentrated at sparse \"key tokens\" ($5-10\\%$ of total tokens) representing\ncritical decision junctions. By distinguishing these high-impact tokens from\nthe increasingly predictable majority, we introduce a new reliability formula\nexplaining the sustained coherence of modern LLMs over thousands of tokens.\nConverging research streams reveal that long-context performance primarily\ndepends on accurately navigating a few crucial semantic decision points rather\nthan on uniform token-level accuracy, enabling targeted strategies that\nsignificantly outperform brute-force approaches. We thus propose a framework\nfor next-generation systems centered on selective preservation of semantically\nvital tokens, dynamic computational allocation at uncertain decision\nboundaries, multi-path exploration at ambiguities, and architectures aligned\nwith natural semantic domains. This marks a fundamental shift from raw scaling\nto strategic reasoning, promising breakthrough performance without\nproportionate computational scaling and offering a more nuanced understanding\nthat supersedes the exponential decay hypothesis, thereby opening pathways\ntoward substantially more powerful and efficient language systems."}
{"id": "2505.24475", "pdf": "https://arxiv.org/pdf/2505.24475", "abs": "https://arxiv.org/abs/2505.24475", "authors": ["Cheng Zeng", "Xiatian Qi", "Chi Chen", "Kai Sun", "Wangle Zhang", "Yuxuan Liu", "Yan Meng", "Bisheng Yang"], "title": "SPPSFormer: High-quality Superpoint-based Transformer for Roof Plane Instance Segmentation from Point Clouds", "categories": ["cs.CV"], "comment": "18 pages, 8 figures", "summary": "Transformers have been seldom employed in point cloud roof plane instance\nsegmentation, which is the focus of this study, and existing superpoint\nTransformers suffer from limited performance due to the use of low-quality\nsuperpoints. To address this challenge, we establish two criteria that\nhigh-quality superpoints for Transformers should satisfy and introduce a\ncorresponding two-stage superpoint generation process. The superpoints\ngenerated by our method not only have accurate boundaries, but also exhibit\nconsistent geometric sizes and shapes, both of which greatly benefit the\nfeature learning of superpoint Transformers. To compensate for the limitations\nof deep learning features when the training set size is limited, we incorporate\nmultidimensional handcrafted features into the model. Additionally, we design a\ndecoder that combines a Kolmogorov-Arnold Network with a Transformer module to\nimprove instance prediction and mask extraction. Finally, our network's\npredictions are refined using traditional algorithm-based postprocessing. For\nevaluation, we annotated a real-world dataset and corrected annotation errors\nin the existing RoofN3D dataset. Experimental results show that our method\nachieves state-of-the-art performance on our dataset, as well as both the\noriginal and reannotated RoofN3D datasets. Moreover, our model is not sensitive\nto plane boundary annotations during training, significantly reducing the\nannotation burden. Through comprehensive experiments, we also identified key\nfactors influencing roof plane segmentation performance: in addition to roof\ntypes, variations in point cloud density, density uniformity, and 3D point\nprecision have a considerable impact. These findings underscore the importance\nof incorporating data augmentation strategies that account for point cloud\nquality to enhance model robustness under diverse and challenging conditions."}
{"id": "2505.24357", "pdf": "https://arxiv.org/pdf/2505.24357", "abs": "https://arxiv.org/abs/2505.24357", "authors": ["Xianglong Yan", "Zhiteng Li", "Tianao Zhang", "Linghe Kong", "Yulun Zhang", "Xiaokang Yang"], "title": "ReCalKV: Low-Rank KV Cache Compression via Head Reordering and Offline Calibration", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Large language models (LLMs) have achieved remarkable performance, yet their\ncapability on long-context reasoning is often constrained by the excessive\nmemory required to store the Key-Value (KV) cache. This makes KV cache\ncompression an essential step toward enabling efficient long-context reasoning.\nRecent methods have explored reducing the hidden dimensions of the KV cache,\nbut many introduce additional computation through projection layers or suffer\nfrom significant performance degradation under high compression ratios. To\naddress these challenges, we propose ReCalKV, a post-training KV cache\ncompression method that reduces the hidden dimensions of the KV cache. We\ndevelop distinct compression strategies for Keys and Values based on their\ndifferent roles and varying importance in the attention mechanism. For Keys, we\npropose Head-wise Similarity-aware Reordering (HSR), which clusters similar\nheads and applies grouped SVD to the key projection matrix, reducing additional\ncomputation while preserving accuracy. For Values, we propose Offline\nCalibration and Matrix Fusion (OCMF) to preserve accuracy without extra\ncomputational overhead. Experiments show that ReCalKV outperforms existing\nlow-rank compression methods, achieving high compression ratios with minimal\nperformance loss. Code is available at:\nhttps://github.com/XIANGLONGYAN/ReCalKV."}
{"id": "2505.24317", "pdf": "https://arxiv.org/pdf/2505.24317", "abs": "https://arxiv.org/abs/2505.24317", "authors": ["Yongming Chen", "Miner Chen", "Liewen Liao", "Mingyang Jiang", "Xiang Zuo", "Hengrui Zhang", "Yuchen Xi", "Songan Zhang"], "title": "ROAD: Responsibility-Oriented Reward Design for Reinforcement Learning in Autonomous Driving", "categories": ["cs.LG"], "comment": null, "summary": "Reinforcement learning (RL) in autonomous driving employs a trial-and-error\nmechanism, enhancing robustness in unpredictable environments. However,\ncrafting effective reward functions remains challenging, as conventional\napproaches rely heavily on manual design and demonstrate limited efficacy in\ncomplex scenarios. To address this issue, this study introduces a\nresponsibility-oriented reward function that explicitly incorporates traffic\nregulations into the RL framework. Specifically, we introduced a Traffic\nRegulation Knowledge Graph and leveraged Vision-Language Models alongside\nRetrieval-Augmented Generation techniques to automate reward assignment. This\nintegration guides agents to adhere strictly to traffic laws, thus minimizing\nrule violations and optimizing decision-making performance in diverse driving\nconditions. Experimental validations demonstrate that the proposed methodology\nsignificantly improves the accuracy of assigning accident responsibilities and\neffectively reduces the agent's liability in traffic incidents."}
{"id": "2505.24196", "pdf": "https://arxiv.org/pdf/2505.24196", "abs": "https://arxiv.org/abs/2505.24196", "authors": ["Longze Chen", "Renke Shan", "Huiming Wang", "Lu Wang", "Ziqiang Liu", "Run Luo", "Jiawei Wang", "Hamid Alinejad-Rokny", "Min Yang"], "title": "CLaSp: In-Context Layer Skip for Self-Speculative Decoding", "categories": ["cs.CL"], "comment": "11 pages, 7 figures, ACL 2025", "summary": "Speculative decoding (SD) is a promising method for accelerating the decoding\nprocess of Large Language Models (LLMs). The efficiency of SD primarily hinges\non the consistency between the draft model and the verify model. However,\nexisting drafting approaches typically require additional modules to be\ntrained, which can be challenging to implement and ensure compatibility across\nvarious LLMs. In this paper, we propose CLaSp, an in-context layer-skipping\nstrategy for self-speculative decoding. Unlike prior methods, CLaSp does not\nrequire additional drafting modules or extra training. Instead, it employs a\nplug-and-play mechanism by skipping intermediate layers of the verify model to\nconstruct a compressed draft model. Specifically, we develop a dynamic\nprogramming algorithm that optimizes the layer-skipping process by leveraging\nthe complete hidden states from the last verification stage as an objective.\nThis enables CLaSp to dynamically adjust its layer-skipping strategy after each\nverification stage, without relying on pre-optimized sets of skipped layers.\nExperimental results across diverse downstream tasks demonstrate that CLaSp\nachieves a speedup of 1.3x ~ 1.7x on LLaMA3 series models without altering the\noriginal distribution of the generated text."}
{"id": "2505.24476", "pdf": "https://arxiv.org/pdf/2505.24476", "abs": "https://arxiv.org/abs/2505.24476", "authors": ["Yuting Zhang", "Hao Lu", "Qingyong Hu", "Yin Wang", "Kaishen Yuan", "Xin Liu", "Kaishun Wu"], "title": "Period-LLM: Extending the Periodic Capability of Multimodal Large Language Model", "categories": ["cs.CV"], "comment": "Accepted by CVPR 2025", "summary": "Periodic or quasi-periodic phenomena reveal intrinsic characteristics in\nvarious natural processes, such as weather patterns, movement behaviors,\ntraffic flows, and biological signals. Given that these phenomena span multiple\nmodalities, the capabilities of Multimodal Large Language Models (MLLMs) offer\npromising potential to effectively capture and understand their complex nature.\nHowever, current MLLMs struggle with periodic tasks due to limitations in: 1)\nlack of temporal modelling and 2) conflict between short and long periods. This\npaper introduces Period-LLM, a multimodal large language model designed to\nenhance the performance of periodic tasks across various modalities, and\nconstructs a benchmark of various difficulty for evaluating the cross-modal\nperiodic capabilities of large models. Specially, We adopt an \"Easy to Hard\nGeneralization\" paradigm, starting with relatively simple text-based tasks and\nprogressing to more complex visual and multimodal tasks, ensuring that the\nmodel gradually builds robust periodic reasoning capabilities. Additionally, we\npropose a \"Resisting Logical Oblivion\" optimization strategy to maintain\nperiodic reasoning abilities during semantic alignment. Extensive experiments\ndemonstrate the superiority of the proposed Period-LLM over existing MLLMs in\nperiodic tasks. The code is available at\nhttps://github.com/keke-nice/Period-LLM."}
{"id": "2505.24369", "pdf": "https://arxiv.org/pdf/2505.24369", "abs": "https://arxiv.org/abs/2505.24369", "authors": ["Yuanfu Wang", "Pengyu Wang", "Chenyang Xi", "Bo Tang", "Junyi Zhu", "Wenqiang Wei", "Chen Chen", "Chao Yang", "Jingfeng Zhang", "Chaochao Lu", "Yijun Niu", "Keming Mao", "Zhiyu Li", "Feiyu Xiong", "Jie Hu", "Mingchuan Yang"], "title": "Adversarial Preference Learning for Robust LLM Alignment", "categories": ["cs.LG", "cs.AI"], "comment": "Accepted at ACL2025 Findings", "summary": "Modern language models often rely on Reinforcement Learning from Human\nFeedback (RLHF) to encourage safe behaviors. However, they remain vulnerable to\nadversarial attacks due to three key limitations: (1) the inefficiency and high\ncost of human annotation, (2) the vast diversity of potential adversarial\nattacks, and (3) the risk of feedback bias and reward hacking. To address these\nchallenges, we introduce Adversarial Preference Learning (APL), an iterative\nadversarial training method incorporating three key innovations. First, a\ndirect harmfulness metric based on the model's intrinsic preference\nprobabilities, eliminating reliance on external assessment. Second, a\nconditional generative attacker that synthesizes input-specific adversarial\nvariations. Third, an iterative framework with automated closed-loop feedback,\nenabling continuous adaptation through vulnerability discovery and mitigation.\nExperiments on Mistral-7B-Instruct-v0.3 demonstrate that APL significantly\nenhances robustness, achieving 83.33% harmlessness win rate over the base model\n(evaluated by GPT-4o), reducing harmful outputs from 5.88% to 0.43% (measured\nby LLaMA-Guard), and lowering attack success rate by up to 65% according to\nHarmBench. Notably, APL maintains competitive utility, with an MT-Bench score\nof 6.59 (comparable to the baseline 6.78) and an LC-WinRate of 46.52% against\nthe base model."}
{"id": "2505.24324", "pdf": "https://arxiv.org/pdf/2505.24324", "abs": "https://arxiv.org/abs/2505.24324", "authors": ["Ivan Petrukha", "Yana Kurliak", "Nataliia Stulova"], "title": "SwiftEval: Developing a Language-Specific Benchmark for LLM-generated Code Evaluation", "categories": ["cs.LG", "cs.CL", "cs.PL", "cs.SE"], "comment": "Accepted to FORGE'25 Benchmarking on 15.01.2025, to be published by\n  IEEE under the CC BY-NC-ND 4.0 license. This is the accepted version of the\n  article (5 pages, 2 figures, 1 table). DOI will be added upon publication", "summary": "In recent years, large language models (LLMs) have showcased significant\nadvancements in code generation. However, most evaluation benchmarks are\nprimarily oriented towards Python, making it difficult to evaluate other\nprogramming languages, such as Swift, with high quality. By examining widely\nestablished multilingual benchmarks like HumanEval-XL and MultiPL-E, we\nidentified critical issues specific to their Swift components, making them\ninsufficient or even irrelevant for assessing LLM coding capabilities on Swift.\nUnlike these existing approaches, which prioritize rapid scaling and\ngeneralization by automatically translating Python-centric benchmarks with\nLLMs, we adopt a quality-over-quantity methodology. We present SwiftEval, the\nfirst Swift-oriented benchmark consisting of 28 carefully hand-crafted\nproblems, and evaluate 44 popular Code LLMs on it. Our results show significant\nLLM scores drop for problems requiring language-specific features, most\nnoticeable in the models of smaller sizes."}
{"id": "2505.24199", "pdf": "https://arxiv.org/pdf/2505.24199", "abs": "https://arxiv.org/abs/2505.24199", "authors": ["Yimin Du"], "title": "Intuitionistic Fuzzy Sets for Large Language Model Data Annotation: A Novel Approach to Side-by-Side Preference Labeling", "categories": ["cs.CL"], "comment": "7 pages", "summary": "The quality of human preference data is crucial for training and evaluating\nlarge language models (LLMs), particularly in reinforcement learning from human\nfeedback (RLHF) and direct preference optimization (DPO) scenarios. Traditional\nside-by-side (SBS) annotation approaches often struggle with inherent\nuncertainty, annotator disagreement, and the complexity of preference\njudgments. This paper introduces a novel framework based on intuitionistic\nfuzzy sets (IFS) for modeling and aggregating human preferences in LLM data\nannotation tasks. Our approach captures not only the degree of preference but\nalso the uncertainty and hesitation inherent in human judgment through\nmembership, non-membership, and hesitation degrees. We propose an IFS-based\nannotation protocol that enables more nuanced preference modeling, develops\naggregation methods for handling annotator disagreement, and introduces quality\nmetrics for preference data assessment. Experimental validation on multiple\ndatasets demonstrates that our IFS-based approach significantly improves\nannotation consistency, reduces annotator fatigue, and produces higher-quality\npreference data compared to traditional binary and Likert-scale methods. The\nresulting preference datasets lead to improved model performance in downstream\ntasks, with 12.3\\% improvement in win-rate against baseline models and 15.7\\%\nreduction in annotation time. Our framework provides a principled approach to\nhandling uncertainty in human preference annotation and offers practical\nbenefits for large-scale LLM training."}
{"id": "2505.24481", "pdf": "https://arxiv.org/pdf/2505.24481", "abs": "https://arxiv.org/abs/2505.24481", "authors": ["Jing Huang", "Yongkang Zhao", "Yuhan Li", "Zhitao Dai", "Cheng Chen", "Qiying Lai"], "title": "ACM-UNet: Adaptive Integration of CNNs and Mamba for Efficient Medical Image Segmentation", "categories": ["cs.CV"], "comment": "10 pages, 3 figures, 5 tables", "summary": "The U-shaped encoder-decoder architecture with skip connections has become a\nprevailing paradigm in medical image segmentation due to its simplicity and\neffectiveness. While many recent works aim to improve this framework by\ndesigning more powerful encoders and decoders, employing advanced convolutional\nneural networks (CNNs) for local feature extraction, Transformers or state\nspace models (SSMs) such as Mamba for global context modeling, or hybrid\ncombinations of both, these methods often struggle to fully utilize pretrained\nvision backbones (e.g., ResNet, ViT, VMamba) due to structural mismatches. To\nbridge this gap, we introduce ACM-UNet, a general-purpose segmentation\nframework that retains a simple UNet-like design while effectively\nincorporating pretrained CNNs and Mamba models through a lightweight adapter\nmechanism. This adapter resolves architectural incompatibilities and enables\nthe model to harness the complementary strengths of CNNs and SSMs-namely,\nfine-grained local detail extraction and long-range dependency modeling.\nAdditionally, we propose a hierarchical multi-scale wavelet transform module in\nthe decoder to enhance feature fusion and reconstruction fidelity. Extensive\nexperiments on the Synapse and ACDC benchmarks demonstrate that ACM-UNet\nachieves state-of-the-art performance while remaining computationally\nefficient. Notably, it reaches 85.12% Dice Score and 13.89mm HD95 on the\nSynapse dataset with 17.93G FLOPs, showcasing its effectiveness and\nscalability. Code is available at: https://github.com/zyklcode/ACM-UNet."}
{"id": "2505.24378", "pdf": "https://arxiv.org/pdf/2505.24378", "abs": "https://arxiv.org/abs/2505.24378", "authors": ["Yilun Kong", "Guozheng Ma", "Qi Zhao", "Haoyu Wang", "Li Shen", "Xueqian Wang", "Dacheng Tao"], "title": "Mastering Massive Multi-Task Reinforcement Learning via Mixture-of-Expert Decision Transformer", "categories": ["cs.LG", "cs.AI"], "comment": "ICML 2025", "summary": "Despite recent advancements in offline multi-task reinforcement learning\n(MTRL) have harnessed the powerful capabilities of the Transformer\narchitecture, most approaches focus on a limited number of tasks, with scaling\nto extremely massive tasks remaining a formidable challenge. In this paper, we\nfirst revisit the key impact of task numbers on current MTRL method, and\nfurther reveal that naively expanding the parameters proves insufficient to\ncounteract the performance degradation as the number of tasks escalates.\nBuilding upon these insights, we propose M3DT, a novel mixture-of-experts (MoE)\nframework that tackles task scalability by further unlocking the model's\nparameter scalability. Specifically, we enhance both the architecture and the\noptimization of the agent, where we strengthen the Decision Transformer (DT)\nbackbone with MoE to reduce task load on parameter subsets, and introduce a\nthree-stage training mechanism to facilitate efficient training with optimal\nperformance. Experimental results show that, by increasing the number of\nexperts, M3DT not only consistently enhances its performance as model expansion\non the fixed task numbers, but also exhibits remarkable task scalability,\nsuccessfully extending to 160 tasks with superior performance."}
{"id": "2505.24353", "pdf": "https://arxiv.org/pdf/2505.24353", "abs": "https://arxiv.org/abs/2505.24353", "authors": ["Federico Milanesio", "Matteo Santoro", "Pietro G. FrÃ©", "Guido Sanguinetti"], "title": "Cartan Networks: Group theoretical Hyperbolic Deep Learning", "categories": ["cs.LG"], "comment": "20 pages, 3 figures, under review", "summary": "Hyperbolic deep learning leverages the metric properties of hyperbolic spaces\nto develop efficient and informative embeddings of hierarchical data. Here, we\nfocus on the solvable group structure of hyperbolic spaces, which follows\nnaturally from their construction as symmetric spaces. This dual nature of Lie\ngroup and Riemannian manifold allows us to propose a new class of hyperbolic\ndeep learning algorithms where group homomorphisms are interleaved with\nmetric-preserving diffeomorphisms. The resulting algorithms, which we call\nCartan networks, show promising results on various benchmark data sets and open\nthe way to a novel class of hyperbolic deep learning architectures."}
{"id": "2505.24211", "pdf": "https://arxiv.org/pdf/2505.24211", "abs": "https://arxiv.org/abs/2505.24211", "authors": ["Jiwan Chung", "Janghan Yoon", "Junhyeong Park", "Sangeyl Lee", "Joowon Yang", "Sooyeon Park", "Youngjae Yu"], "title": "Are Any-to-Any Models More Consistent Across Modality Transfers Than Specialists?", "categories": ["cs.CL"], "comment": null, "summary": "Any-to-any generative models aim to enable seamless interpretation and\ngeneration across multiple modalities within a unified framework, yet their\nability to preserve relationships across modalities remains uncertain. Do\nunified models truly achieve cross-modal coherence, or is this coherence merely\nperceived? To explore this, we introduce ACON, a dataset of 1,000 images (500\nnewly contributed) paired with captions, editing instructions, and Q&A pairs to\nevaluate cross-modal transfers rigorously. Using three consistency\ncriteria-cyclic consistency, forward equivariance, and conjugated\nequivariance-our experiments reveal that any-to-any models do not consistently\ndemonstrate greater cross-modal consistency than specialized models in\npointwise evaluations such as cyclic consistency. However, equivariance\nevaluations uncover weak but observable consistency through structured analyses\nof the intermediate latent space enabled by multiple editing operations. We\nrelease our code and data at https://github.com/JiwanChung/ACON."}
{"id": "2505.24489", "pdf": "https://arxiv.org/pdf/2505.24489", "abs": "https://arxiv.org/abs/2505.24489", "authors": ["Anasse Boutayeb", "Iyad Lahsen-cherif", "Ahmed El Khadimi"], "title": "Deformable Attention Mechanisms Applied to Object Detection, case of Remote Sensing", "categories": ["cs.CV", "cs.AI"], "comment": "10 pages, 5 figures, paper accepted at the 29th International\n  Conference on Knowledge-Based and Intelligent Information and Engineering\n  Systems (KES 2025), Osaka, Japan", "summary": "Object detection has recently seen an interesting trend in terms of the most\ninnovative research work, this task being of particular importance in the field\nof remote sensing, given the consistency of these images in terms of\ngeographical coverage and the objects present. Furthermore, Deep Learning (DL)\nmodels, in particular those based on Transformers, are especially relevant for\nvisual computing tasks in general, and target detection in particular. Thus,\nthe present work proposes an application of Deformable-DETR model, a specific\narchitecture using deformable attention mechanisms, on remote sensing images in\ntwo different modes, especially optical and Synthetic Aperture Radar (SAR). To\nachieve this objective, two datasets are used, one optical, which is Pleiades\nAircraft dataset, and the other SAR, in particular SAR Ship Detection Dataset\n(SSDD). The results of a 10-fold stratified validation showed that the proposed\nmodel performed particularly well, obtaining an F1 score of 95.12% for the\noptical dataset and 94.54% for SSDD, while comparing these results with several\nmodels detections, especially those based on CNNs and transformers, as well as\nthose specifically designed to detect different object classes in remote\nsensing images."}
{"id": "2505.24379", "pdf": "https://arxiv.org/pdf/2505.24379", "abs": "https://arxiv.org/abs/2505.24379", "authors": ["Xiaoyu Wu", "Yifei Pang", "Terrance Liu", "Zhiwei Steven Wu"], "title": "Breaking the Gold Standard: Extracting Forgotten Data under Exact Unlearning in Large Language Models", "categories": ["cs.LG", "cs.AI", "cs.CL", "cs.CR"], "comment": null, "summary": "Large language models are typically trained on datasets collected from the\nweb, which may inadvertently contain harmful or sensitive personal information.\nTo address growing privacy concerns, unlearning methods have been proposed to\nremove the influence of specific data from trained models. Of these, exact\nunlearning -- which retrains the model from scratch without the target data --\nis widely regarded the gold standard, believed to be robust against\nprivacy-related attacks. In this paper, we challenge this assumption by\nintroducing a novel data extraction attack that compromises even exact\nunlearning. Our method leverages both the pre- and post-unlearning models: by\nguiding the post-unlearning model using signals from the pre-unlearning model,\nwe uncover patterns that reflect the removed data distribution. Combining model\nguidance with a token filtering strategy, our attack significantly improves\nextraction success rates -- doubling performance in some cases -- across common\nbenchmarks such as MUSE, TOFU, and WMDP. Furthermore, we demonstrate our\nattack's effectiveness on a simulated medical diagnosis dataset to highlight\nreal-world privacy risks associated with exact unlearning. In light of our\nfindings, which suggest that unlearning may, in a contradictory way, increase\nthe risk of privacy leakage, we advocate for evaluation of unlearning methods\nto consider broader threat models that account not only for post-unlearning\nmodels but also for adversarial access to prior checkpoints."}
{"id": "2505.24360", "pdf": "https://arxiv.org/pdf/2505.24360", "abs": "https://arxiv.org/abs/2505.24360", "authors": ["Stepan Shabalin", "Ayush Panda", "Dmitrii Kharlapenko", "Abdur Raheem Ali", "Yixiong Hao", "Arthur Conmy"], "title": "Interpreting Large Text-to-Image Diffusion Models with Dictionary Learning", "categories": ["cs.LG"], "comment": "10 pages, 10 figures, Mechanistic Interpretability for Vision at CVPR\n  2025", "summary": "Sparse autoencoders are a promising new approach for decomposing language\nmodel activations for interpretation and control. They have been applied\nsuccessfully to vision transformer image encoders and to small-scale diffusion\nmodels. Inference-Time Decomposition of Activations (ITDA) is a recently\nproposed variant of dictionary learning that takes the dictionary to be a set\nof data points from the activation distribution and reconstructs them with\ngradient pursuit. We apply Sparse Autoencoders (SAEs) and ITDA to a large\ntext-to-image diffusion model, Flux 1, and consider the interpretability of\nembeddings of both by introducing a visual automated interpretation pipeline.\nWe find that SAEs accurately reconstruct residual stream embeddings and beat\nMLP neurons on interpretability. We are able to use SAE features to steer image\ngeneration through activation addition. We find that ITDA has comparable\ninterpretability to SAEs."}
{"id": "2505.24217", "pdf": "https://arxiv.org/pdf/2505.24217", "abs": "https://arxiv.org/abs/2505.24217", "authors": ["Jixuan Leng", "Cassandra A. Cohen", "Zhixian Zhang", "Chenyan Xiong", "William W. Cohen"], "title": "Semi-structured LLM Reasoners Can Be Rigorously Audited", "categories": ["cs.CL"], "comment": null, "summary": "As Large Language Models (LLMs) become increasingly capable at reasoning, the\nproblem of \"faithfulness\" persists: LLM \"reasoning traces\" can contain errors\nand omissions that are difficult to detect, and may obscure biases in model\noutputs. To address these limitations, we introduce Semi-Structured Reasoning\nModels (SSRMs), which internalize a semi-structured Chain-of-Thought (CoT)\nreasoning format within the model. Our SSRMs generate reasoning traces in a\nPythonic syntax. While SSRM traces are not executable, they adopt a restricted,\ntask-specific vocabulary to name distinct reasoning steps, and to mark each\nstep's inputs and outputs. Through extensive evaluation on ten benchmarks,\nSSRMs demonstrate strong performance and generality: they outperform comparably\nsized baselines by nearly ten percentage points on in-domain tasks while\nremaining competitive with specialized models on out-of-domain medical\nbenchmarks. Furthermore, we show that semi-structured reasoning is more\namenable to analysis: in particular, they can be automatically audited to\nidentify reasoning flaws. We explore both hand-crafted structured audits, which\ndetect task-specific problematic reasoning patterns, and learned typicality\naudits, which apply probabilistic models over reasoning patterns, and show that\nboth audits can be used to effectively flag probable reasoning errors."}
{"id": "2505.24499", "pdf": "https://arxiv.org/pdf/2505.24499", "abs": "https://arxiv.org/abs/2505.24499", "authors": ["Ximing Xing", "Yandong Guan", "Jing Zhang", "Dong Xu", "Qian Yu"], "title": "Reason-SVG: Hybrid Reward RL for Aha-Moments in Vector Graphics Generation", "categories": ["cs.CV"], "comment": "17 pages, 5 figures", "summary": "Generating high-quality Scalable Vector Graphics (SVGs) is challenging for\nLarge Language Models (LLMs), as it requires advanced reasoning for structural\nvalidity, semantic faithfulness, and visual coherence -- capabilities in which\ncurrent LLMs often fall short. In this work, we introduce Reason-SVG, a novel\nframework designed to enhance LLM reasoning for SVG generation. Reason-SVG\npioneers the \"Drawing-with-Thought\" (DwT) paradigm, in which models generate\nboth SVG code and explicit design rationales, mimicking the human creative\nprocess. Reason-SVG adopts a two-stage training strategy: First, Supervised\nFine-Tuning (SFT) trains the LLM on the DwT paradigm to activate foundational\nreasoning abilities. Second, Reinforcement Learning (RL), utilizing Group\nRelative Policy Optimization (GRPO), empowers the model to generate both DwT\nand SVGs rationales through refined, reward-driven reasoning. To facilitate\nreasoning-driven SVG generation, we design a Hybrid Reward function that\nevaluates the presence and utility of DwT reasoning, along with structural\nvalidity, semantic alignment, and visual quality. We also introduce the\nSVGX-DwT-10k dataset, a high-quality corpus of 10,000 SVG-DwT pairs, where each\nSVG code is generated based on explicit DwT reasoning. By integrating DwT, SFT,\nand Hybrid Reward-guided RL, Reason-SVG significantly improves LLM performance\nin generating accurate and visually compelling SVGs, potentially fostering \"Aha\nmoments\" in design."}
{"id": "2505.24409", "pdf": "https://arxiv.org/pdf/2505.24409", "abs": "https://arxiv.org/abs/2505.24409", "authors": ["Eojin Kang", "Juae Kim"], "title": "LLMs Are Globally Multilingual Yet Locally Monolingual: Exploring Knowledge Transfer via Language and Thought Theory", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Multilingual large language models (LLMs) open up new possibilities for\nleveraging information across languages, but their factual knowledge recall\nremains inconsistent depending on the input language. While previous studies\nhave attempted to address this issue through English-based prompting and\nevaluation, we explore non-English to English transfer via Language and Thought\nTheory. This perspective allows us to examine language-thought binding in LLMs\nand uncover why factual knowledge often fails to transfer effectively. We\npropose the Language-to-Thought (L2T) prompting strategy, which analyzes the\nrelationship between input language, internal cognitive processes, and\nknowledge. Experimental results challenge the assumption that English-based\napproaches consistently outperform other languages and offer a novel insight\nthat aligning the model's internal thought with the knowledge required for the\ntask is critical for successful cross-lingual transfer. Furthermore, we show\nthat applying L2T during training can alleviate LLMs' reliance on the input\nlanguage and facilitate cross-linguistic knowledge integration without\ntranslation-based learning. Code and datasets will be available."}
{"id": "2505.24365", "pdf": "https://arxiv.org/pdf/2505.24365", "abs": "https://arxiv.org/abs/2505.24365", "authors": ["Vardhan Shorewala", "Shivam Shorewala"], "title": "Anomaly Detection and Improvement of Clusters using Enhanced K-Means Algorithm", "categories": ["cs.LG", "cs.PF"], "comment": "IEEE ICCCSP", "summary": "This paper introduces a unified approach to cluster refinement and anomaly\ndetection in datasets. We propose a novel algorithm that iteratively reduces\nthe intra-cluster variance of N clusters until a global minimum is reached,\nyielding tighter clusters than the standard k-means algorithm. We evaluate the\nmethod using intrinsic measures for unsupervised learning, including the\nsilhouette coefficient, Calinski-Harabasz index, and Davies-Bouldin index, and\nextend it to anomaly detection by identifying points whose assignment causes a\nsignificant variance increase. External validation on synthetic data and the\nUCI Breast Cancer and UCI Wine Quality datasets employs the Jaccard similarity\nscore, V-measure, and F1 score. Results show variance reductions of 18.7% and\n88.1% on the synthetic and Wine Quality datasets, respectively, along with\naccuracy and F1 score improvements of 22.5% and 20.8% on the Wine Quality\ndataset."}
{"id": "2505.24219", "pdf": "https://arxiv.org/pdf/2505.24219", "abs": "https://arxiv.org/abs/2505.24219", "authors": ["Lam Thanh Do", "Aaditya Bodke", "Pritom Saha Akash", "Kevin Chen-Chuan Chang"], "title": "ERU-KG: Efficient Reference-aligned Unsupervised Keyphrase Generation", "categories": ["cs.CL"], "comment": "Accepted to ACL 2025", "summary": "Unsupervised keyphrase prediction has gained growing interest in recent\nyears. However, existing methods typically rely on heuristically defined\nimportance scores, which may lead to inaccurate informativeness estimation. In\naddition, they lack consideration for time efficiency. To solve these problems,\nwe propose ERU-KG, an unsupervised keyphrase generation (UKG) model that\nconsists of an informativeness and a phraseness module. The former estimates\nthe relevance of keyphrase candidates, while the latter generate those\ncandidates. The informativeness module innovates by learning to model\ninformativeness through references (e.g., queries, citation contexts, and\ntitles) and at the term-level, thereby 1) capturing how the key concepts of\ndocuments are perceived in different contexts and 2) estimating informativeness\nof phrases more efficiently by aggregating term informativeness, removing the\nneed for explicit modeling of the candidates. ERU-KG demonstrates its\neffectiveness on keyphrase generation benchmarks by outperforming unsupervised\nbaselines and achieving on average 89\\% of the performance of a supervised\nmodel for top 10 predictions. Additionally, to highlight its practical utility,\nwe evaluate the model on text retrieval tasks and show that keyphrases\ngenerated by ERU-KG are effective when employed as query and document\nexpansions. Furthermore, inference speed tests reveal that ERU-KG is the\nfastest among baselines of similar model sizes. Finally, our proposed model can\nswitch between keyphrase generation and extraction by adjusting\nhyperparameters, catering to diverse application requirements."}
{"id": "2505.24517", "pdf": "https://arxiv.org/pdf/2505.24517", "abs": "https://arxiv.org/abs/2505.24517", "authors": ["Yinqi Li", "Jiahe Zhao", "Hong Chang", "Ruibing Hou", "Shiguang Shan", "Xilin Chen"], "title": "un$^2$CLIP: Improving CLIP's Visual Detail Capturing Ability via Inverting unCLIP", "categories": ["cs.CV"], "comment": null, "summary": "Contrastive Language-Image Pre-training (CLIP) has become a foundation model\nand has been applied to various vision and multimodal tasks. However, recent\nworks indicate that CLIP falls short in distinguishing detailed differences in\nimages and shows suboptimal performance on dense-prediction and vision-centric\nmultimodal tasks. Therefore, this work focuses on improving existing CLIP\nmodels, aiming to capture as many visual details in images as possible. We find\nthat a specific type of generative models, unCLIP, provides a suitable\nframework for achieving our goal. Specifically, unCLIP trains an image\ngenerator conditioned on the CLIP image embedding. In other words, it inverts\nthe CLIP image encoder. Compared to discriminative models like CLIP, generative\nmodels are better at capturing image details because they are trained to learn\nthe data distribution of images. Additionally, the conditional input space of\nunCLIP aligns with CLIP's original image-text embedding space. Therefore, we\npropose to invert unCLIP (dubbed un$^2$CLIP) to improve the CLIP model. In this\nway, the improved image encoder can gain unCLIP's visual detail capturing\nability while preserving its alignment with the original text encoder\nsimultaneously. We evaluate our improved CLIP across various tasks to which\nCLIP has been applied, including the challenging MMVP-VLM benchmark, the\ndense-prediction open-vocabulary segmentation task, and multimodal large\nlanguage model tasks. Experiments show that un$^2$CLIP significantly improves\nthe original CLIP and previous CLIP improvement methods. Code and models will\nbe available at https://github.com/LiYinqi/un2CLIP."}
{"id": "2505.24415", "pdf": "https://arxiv.org/pdf/2505.24415", "abs": "https://arxiv.org/abs/2505.24415", "authors": ["Andreas Spilz", "Heiko Oppel", "Michael Munz"], "title": "Boosting Automatic Exercise Evaluation Through Musculoskeletal Simulation-Based IMU Data Augmentation", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Automated evaluation of movement quality holds significant potential for\nenhancing physiotherapeutic treatments and sports training by providing\nobjective, real-time feedback. However, the effectiveness of deep learning\nmodels in assessing movements captured by inertial measurement units (IMUs) is\noften hampered by limited data availability, class imbalance, and label\nambiguity. In this work, we present a novel data augmentation method that\ngenerates realistic IMU data using musculoskeletal simulations integrated with\nsystematic modifications of movement trajectories. Crucially, our approach\nensures biomechanical plausibility and allows for automatic, reliable labeling\nby combining inverse kinematic parameters with a knowledge-based evaluation\nstrategy. Extensive evaluations demonstrate that augmented variants closely\nresembles real-world data, significantly improving the classification accuracy\nand generalization capability of neural network models. Additionally, we\nhighlight the benefits of augmented data for patient-specific fine-tuning\nscenarios, particularly when only limited subject-specific training examples\nare available. Our findings underline the practicality and efficacy of this\naugmentation method in overcoming common challenges faced by deep learning\napplications in physiotherapeutic exercise evaluation."}
{"id": "2505.24399", "pdf": "https://arxiv.org/pdf/2505.24399", "abs": "https://arxiv.org/abs/2505.24399", "authors": ["Yifei Cheng", "Li Shen", "Hao Sun", "Nan Yin", "Xiaochun Cao", "Enhong Chen"], "title": "LightSAM: Parameter-Agnostic Sharpness-Aware Minimization", "categories": ["cs.LG"], "comment": null, "summary": "Sharpness-Aware Minimization (SAM) optimizer enhances the generalization\nability of the machine learning model by exploring the flat minima landscape\nthrough weight perturbations. Despite its empirical success, SAM introduces an\nadditional hyper-parameter, the perturbation radius, which causes the\nsensitivity of SAM to it. Moreover, it has been proved that the perturbation\nradius and learning rate of SAM are constrained by problem-dependent parameters\nto guarantee convergence. These limitations indicate the requirement of\nparameter-tuning in practical applications. In this paper, we propose the\nalgorithm LightSAM which sets the perturbation radius and learning rate of SAM\nadaptively, thus extending the application scope of SAM. LightSAM employs three\npopular adaptive optimizers, including AdaGrad-Norm, AdaGrad and Adam, to\nreplace the SGD optimizer for weight perturbation and model updating, reducing\nsensitivity to parameters. Theoretical results show that under weak\nassumptions, LightSAM could converge ideally with any choices of perturbation\nradius and learning rate, thus achieving parameter-agnostic. We conduct\npreliminary experiments on several deep learning tasks, which together with the\ntheoretical findings validate the the effectiveness of LightSAM."}
{"id": "2505.24223", "pdf": "https://arxiv.org/pdf/2505.24223", "abs": "https://arxiv.org/abs/2505.24223", "authors": ["Jean-Benoit Delbrouck", "Justin Xu", "Johannes Moll", "Alois Thomas", "Zhihong Chen", "Sophie Ostmeier", "Asfandyar Azhar", "Kelvin Zhenghao Li", "Andrew Johnston", "Christian Bluethgen", "Eduardo Reis", "Mohamed Muneer", "Maya Varma", "Curtis Langlotz"], "title": "Automated Structured Radiology Report Generation", "categories": ["cs.CL"], "comment": "Accepted to ACL Main 2025", "summary": "Automated radiology report generation from chest X-ray (CXR) images has the\npotential to improve clinical efficiency and reduce radiologists' workload.\nHowever, most datasets, including the publicly available MIMIC-CXR and CheXpert\nPlus, consist entirely of free-form reports, which are inherently variable and\nunstructured. This variability poses challenges for both generation and\nevaluation: existing models struggle to produce consistent, clinically\nmeaningful reports, and standard evaluation metrics fail to capture the nuances\nof radiological interpretation. To address this, we introduce Structured\nRadiology Report Generation (SRRG), a new task that reformulates free-text\nradiology reports into a standardized format, ensuring clarity, consistency,\nand structured clinical reporting. We create a novel dataset by restructuring\nreports using large language models (LLMs) following strict structured\nreporting desiderata. Additionally, we introduce SRR-BERT, a fine-grained\ndisease classification model trained on 55 labels, enabling more precise and\nclinically informed evaluation of structured reports. To assess report quality,\nwe propose F1-SRR-BERT, a metric that leverages SRR-BERT's hierarchical disease\ntaxonomy to bridge the gap between free-text variability and structured\nclinical reporting. We validate our dataset through a reader study conducted by\nfive board-certified radiologists and extensive benchmarking experiments."}
{"id": "2505.24519", "pdf": "https://arxiv.org/pdf/2505.24519", "abs": "https://arxiv.org/abs/2505.24519", "authors": ["Yuqi Zhang", "Yuchun Miao", "Zuchao Li", "Liang Ding"], "title": "AMIA: Automatic Masking and Joint Intention Analysis Makes LVLMs Robust Jailbreak Defenders", "categories": ["cs.CV", "cs.CL"], "comment": "11 pages, 7 figures", "summary": "We introduce AMIA, a lightweight, inference-only defense for Large\nVision-Language Models (LVLMs) that (1) Automatically Masks a small set of\ntext-irrelevant image patches to disrupt adversarial perturbations, and (2)\nconducts joint Intention Analysis to uncover and mitigate hidden harmful\nintents before response generation. Without any retraining, AMIA improves\ndefense success rates across diverse LVLMs and jailbreak benchmarks from an\naverage of 52.4% to 81.7%, preserves general utility with only a 2% average\naccuracy drop, and incurs only modest inference overhead. Ablation confirms\nboth masking and intention analysis are essential for a robust safety-utility\ntrade-off."}
{"id": "2505.24429", "pdf": "https://arxiv.org/pdf/2505.24429", "abs": "https://arxiv.org/abs/2505.24429", "authors": ["Giovanny C-LondoÃ±o", "Javier SÃ¡nchez", "Ãngel RodrÃ­guez-Santana"], "title": "Deep Learning Weather Models for Subregional Ocean Forecasting: A Case Study on the Canary Current Upwelling System", "categories": ["physics.ao-ph", "cs.AI", "cs.LG"], "comment": "28 pages, 8 figures", "summary": "Oceanographic forecasting impacts various sectors of society by supporting\nenvironmental conservation and economic activities. Based on global circulation\nmodels, traditional forecasting methods are computationally expensive and slow,\nlimiting their ability to provide rapid forecasts. Recent advances in deep\nlearning offer faster and more accurate predictions, although these data-driven\nmodels are often trained with global data from numerical simulations, which may\nnot reflect reality. The emergence of such models presents great potential for\nimproving ocean prediction at a subregional domain. However, their ability to\npredict fine-scale ocean processes, like mesoscale structures, remains largely\nunknown. This work aims to adapt a graph neural network initially developed for\nglobal weather forecasting to improve subregional ocean prediction,\nspecifically focusing on the Canary Current upwelling system. The model is\ntrained with satellite data and compared to state-of-the-art physical ocean\nmodels to assess its performance in capturing ocean dynamics. Our results show\nthat the deep learning model surpasses traditional methods in precision despite\nsome challenges in upwelling areas. It demonstrated superior performance in\nreducing RMSE errors compared to ConvLSTM and the GLORYS reanalysis,\nparticularly in regions with complex oceanic dynamics such as Cape Ghir, Cape\nBojador, and Cape Blanc. The model achieved improvements of up to 26.5%\nrelative to ConvLSTM and error reductions of up to 76% in 5-day forecasts\ncompared to the GLORYS reanalysis at these critical locations, highlighting its\nenhanced capability to capture spatial variability and improve predictive\naccuracy in complex areas. These findings suggest the viability of adapting\nmeteorological data-driven models for improving subregional medium-term ocean\nforecasting."}
{"id": "2505.24403", "pdf": "https://arxiv.org/pdf/2505.24403", "abs": "https://arxiv.org/abs/2505.24403", "authors": ["Giannis Nikolentzos", "Konstantinos Skianis"], "title": "On the Lipschitz Continuity of Set Aggregation Functions and Neural Networks for Sets", "categories": ["cs.LG"], "comment": null, "summary": "The Lipschitz constant of a neural network is connected to several important\nproperties of the network such as its robustness and generalization. It is thus\nuseful in many settings to estimate the Lipschitz constant of a model. Prior\nwork has focused mainly on estimating the Lipschitz constant of multi-layer\nperceptrons and convolutional neural networks. Here we focus on data modeled as\nsets or multisets of vectors and on neural networks that can handle such data.\nThese models typically apply some permutation invariant aggregation function,\nsuch as the sum, mean or max operator, to the input multisets to produce a\nsingle vector for each input sample. In this paper, we investigate whether\nthese aggregation functions are Lipschitz continuous with respect to three\ndistance functions for unordered multisets, and we compute their Lipschitz\nconstants. In the general case, we find that each aggregation function is\nLipschitz continuous with respect to only one of the three distance functions.\nThen, we build on these results to derive upper bounds on the Lipschitz\nconstant of neural networks that can process multisets of vectors, while we\nalso study their stability to perturbations and generalization under\ndistribution shifts. To empirically verify our theoretical analysis, we conduct\na series of experiments on datasets from different domains."}
{"id": "2505.24241", "pdf": "https://arxiv.org/pdf/2505.24241", "abs": "https://arxiv.org/abs/2505.24241", "authors": ["Naibin Gu", "Yilong Chen", "Zhenyu Zhang", "Peng Fu", "Zheng Lin", "Shuohuan Wang", "Yu Sun", "Hua Wu", "Weiping Wang", "Haifeng Wang"], "title": "Advantageous Parameter Expansion Training Makes Better Large Language Models", "categories": ["cs.CL"], "comment": null, "summary": "Although scaling up the number of trainable parameters in both pre-training\nand fine-tuning can effectively improve the performance of large language\nmodels, it also leads to increased computational overhead. When delving into\nthe parameter difference, we find that a subset of parameters, termed\nadvantageous parameters, plays a crucial role in determining model performance.\nFurther analysis reveals that stronger models tend to possess more such\nparameters. In this paper, we propose Advantageous Parameter EXpansion Training\n(APEX), a method that progressively expands advantageous parameters into the\nspace of disadvantageous ones, thereby increasing their proportion and\nenhancing training effectiveness. Further theoretical analysis from the\nperspective of matrix effective rank explains the performance gains of APEX.\nExtensive experiments on both instruction tuning and continued pre-training\ndemonstrate that, in instruction tuning, APEX outperforms full-parameter tuning\nwhile using only 52% of the trainable parameters. In continued pre-training,\nAPEX achieves the same perplexity level as conventional training with just 33%\nof the training data, and yields significant improvements on downstream tasks."}
{"id": "2505.24521", "pdf": "https://arxiv.org/pdf/2505.24521", "abs": "https://arxiv.org/abs/2505.24521", "authors": ["Yang-Tian Sun", "Xin Yu", "Zehuan Huang", "Yi-Hua Huang", "Yuan-Chen Guo", "Ziyi Yang", "Yan-Pei Cao", "Xiaojuan Qi"], "title": "UniGeo: Taming Video Diffusion for Unified Consistent Geometry Estimation", "categories": ["cs.CV"], "comment": "Project page: https://sunyangtian.github.io/UniGeo-web/", "summary": "Recently, methods leveraging diffusion model priors to assist monocular\ngeometric estimation (e.g., depth and normal) have gained significant attention\ndue to their strong generalization ability. However, most existing works focus\non estimating geometric properties within the camera coordinate system of\nindividual video frames, neglecting the inherent ability of diffusion models to\ndetermine inter-frame correspondence. In this work, we demonstrate that,\nthrough appropriate design and fine-tuning, the intrinsic consistency of video\ngeneration models can be effectively harnessed for consistent geometric\nestimation. Specifically, we 1) select geometric attributes in the global\ncoordinate system that share the same correspondence with video frames as the\nprediction targets, 2) introduce a novel and efficient conditioning method by\nreusing positional encodings, and 3) enhance performance through joint training\non multiple geometric attributes that share the same correspondence. Our\nresults achieve superior performance in predicting global geometric attributes\nin videos and can be directly applied to reconstruction tasks. Even when\ntrained solely on static video data, our approach exhibits the potential to\ngeneralize to dynamic video scenes."}
{"id": "2505.24445", "pdf": "https://arxiv.org/pdf/2505.24445", "abs": "https://arxiv.org/abs/2505.24445", "authors": ["Xin Chen", "Yarden As", "Andreas Krause"], "title": "Learning Safety Constraints for Large Language Models", "categories": ["cs.LG", "cs.AI"], "comment": "ICML 2025 (Spotlight)", "summary": "Large language models (LLMs) have emerged as powerful tools but pose\nsignificant safety risks through harmful outputs and vulnerability to\nadversarial attacks. We propose SaP, short for Safety Polytope, a geometric\napproach to LLM safety that learns and enforces multiple safety constraints\ndirectly in the model's representation space. We develop a framework that\nidentifies safe and unsafe regions via the polytope's facets, enabling both\ndetection and correction of unsafe outputs through geometric steering. Unlike\nexisting approaches that modify model weights, SaP operates post-hoc in the\nrepresentation space, preserving model capabilities while enforcing safety\nconstraints. Experiments across multiple LLMs demonstrate that our method can\neffectively detect unethical inputs, reduce adversarial attack success rates\nwhile maintaining performance on standard tasks, thus highlighting the\nimportance of having an explicit geometric model for safety. Analysis of the\nlearned polytope facets reveals emergence of specialization in detecting\ndifferent semantic notions of safety, providing interpretable insights into how\nsafety is captured in LLMs' representation space."}
{"id": "2505.24413", "pdf": "https://arxiv.org/pdf/2505.24413", "abs": "https://arxiv.org/abs/2505.24413", "authors": ["Yang Sui", "Qi Xu", "Yang Bai", "Annie Qu"], "title": "Multi-task Learning for Heterogeneous Multi-source Block-Wise Missing Data", "categories": ["cs.LG", "stat.CO"], "comment": null, "summary": "Multi-task learning (MTL) has emerged as an imperative machine learning tool\nto solve multiple learning tasks simultaneously and has been successfully\napplied to healthcare, marketing, and biomedical fields. However, in order to\nborrow information across different tasks effectively, it is essential to\nutilize both homogeneous and heterogeneous information. Among the extensive\nliterature on MTL, various forms of heterogeneity are presented in MTL\nproblems, such as block-wise, distribution, and posterior heterogeneity.\nExisting methods, however, struggle to tackle these forms of heterogeneity\nsimultaneously in a unified framework. In this paper, we propose a two-step\nlearning strategy for MTL which addresses the aforementioned heterogeneity.\nFirst, we impute the missing blocks using shared representations extracted from\nhomogeneous source across different tasks. Next, we disentangle the mappings\nbetween input features and responses into a shared component and a\ntask-specific component, respectively, thereby enabling information borrowing\nthrough the shared component. Our numerical experiments and real-data analysis\nfrom the ADNI database demonstrate the superior MTL performance of the proposed\nmethod compared to other competing methods."}
{"id": "2505.24244", "pdf": "https://arxiv.org/pdf/2505.24244", "abs": "https://arxiv.org/abs/2505.24244", "authors": ["Nir Endy", "Idan Daniel Grosbard", "Yuval Ran-Milo", "Yonatan Slutzky", "Itay Tshuva", "Raja Giryes"], "title": "Mamba Knockout for Unraveling Factual Information Flow", "categories": ["cs.CL", "cs.LG"], "comment": "Accepted to ACL 2025", "summary": "This paper investigates the flow of factual information in Mamba State-Space\nModel (SSM)-based language models. We rely on theoretical and empirical\nconnections to Transformer-based architectures and their attention mechanisms.\nExploiting this relationship, we adapt attentional interpretability techniques\noriginally developed for Transformers--specifically, the Attention Knockout\nmethodology--to both Mamba-1 and Mamba-2. Using them we trace how information\nis transmitted and localized across tokens and layers, revealing patterns of\nsubject-token information emergence and layer-wise dynamics. Notably, some\nphenomena vary between mamba models and Transformer based models, while others\nappear universally across all models inspected--hinting that these may be\ninherent to LLMs in general. By further leveraging Mamba's structured\nfactorization, we disentangle how distinct \"features\" either enable\ntoken-to-token information exchange or enrich individual tokens, thus offering\na unified lens to understand Mamba internal operations."}
{"id": "2505.24527", "pdf": "https://arxiv.org/pdf/2505.24527", "abs": "https://arxiv.org/abs/2505.24527", "authors": ["Simone Cammarasana", "Giuseppe PatanÃ¨"], "title": "Optimal Density Functions for Weighted Convolution in Learning Models", "categories": ["cs.CV", "cs.LG", "42A85"], "comment": "5 figures, 5 tables, 21 pages", "summary": "The paper introduces the weighted convolution, a novel approach to the\nconvolution for signals defined on regular grids (e.g., 2D images) through the\napplication of an optimal density function to scale the contribution of\nneighbouring pixels based on their distance from the central pixel. This choice\ndiffers from the traditional uniform convolution, which treats all neighbouring\npixels equally. Our weighted convolution can be applied to convolutional neural\nnetwork problems to improve the approximation accuracy. Given a convolutional\nnetwork, we define a framework to compute the optimal density function through\na minimisation model. The framework separates the optimisation of the\nconvolutional kernel weights (using stochastic gradient descent) from the\noptimisation of the density function (using DIRECT-L). Experimental results on\na learning model for an image-to-image task (e.g., image denoising) show that\nthe weighted convolution significantly reduces the loss (up to 53% improvement)\nand increases the test accuracy compared to standard convolution. While this\nmethod increases execution time by 11%, it is robust across several\nhyperparameters of the learning model. Future work will apply the weighted\nconvolution to real-case 2D and 3D image convolutional learning problems."}
{"id": "2505.24451", "pdf": "https://arxiv.org/pdf/2505.24451", "abs": "https://arxiv.org/abs/2505.24451", "authors": ["Luis Ibanez-Lissen", "Lorena Gonzalez-Manzano", "Jose Maria de Fuentes", "Nicolas Anciaux"], "title": "LPASS: Linear Probes as Stepping Stones for vulnerability detection using compressed LLMs", "categories": ["cs.CR", "cs.AI"], "comment": null, "summary": "Large Language Models (LLMs) are being extensively used for cybersecurity\npurposes. One of them is the detection of vulnerable codes. For the sake of\nefficiency and effectiveness, compression and fine-tuning techniques are being\ndeveloped, respectively. However, they involve spending substantial\ncomputational efforts. In this vein, we analyse how Linear Probes (LPs) can be\nused to provide an estimation on the performance of a compressed LLM at an\nearly phase -- before fine-tuning. We also show their suitability to set the\ncut-off point when applying layer pruning compression. Our approach, dubbed\n$LPASS$, is applied in BERT and Gemma for the detection of 12 of MITRE's Top 25\nmost dangerous vulnerabilities on 480k C/C++ samples. LPs can be computed in\n142.97 s. and provide key findings: (1) 33.3 \\% and 72.2\\% of layers can be\nremoved, respectively, with no precision loss; (2) they provide an early\nestimate of the post-fine-tuning and post-compression model effectiveness, with\n3\\% and 8.68\\% as the lowest and average precision errors, respectively.\n$LPASS$-based LLMs outperform the state of the art, reaching 86.9\\% of accuracy\nin multi-class vulnerability detection. Interestingly, $LPASS$-based compressed\nversions of Gemma outperform the original ones by 1.6\\% of F1-score at a\nmaximum while saving 29.4 \\% and 23.8\\% of training and inference time and\n42.98\\% of model size."}
{"id": "2505.24424", "pdf": "https://arxiv.org/pdf/2505.24424", "abs": "https://arxiv.org/abs/2505.24424", "authors": ["Amit Peleg", "Naman Deep Singh", "Matthias Hein"], "title": "Advancing Compositional Awareness in CLIP with Efficient Fine-Tuning", "categories": ["cs.LG", "cs.CV"], "comment": null, "summary": "Vision-language models like CLIP have demonstrated remarkable zero-shot\ncapabilities in classification and retrieval. However, these models often\nstruggle with compositional reasoning - the ability to understand the\nrelationships between concepts. A recent benchmark, SugarCrepe++, reveals that\nprevious works on improving compositionality have mainly improved lexical\nsensitivity but neglected semantic understanding. In addition, downstream\nretrieval performance often deteriorates, although one would expect that\nimproving compositionality should enhance retrieval. In this work, we introduce\nCLIC (Compositionally-aware Learning in CLIP), a fine-tuning method based on a\nnovel training technique combining multiple images and their associated\ncaptions. CLIC improves compositionality across architectures as well as\ndifferently pre-trained CLIP models, both in terms of lexical and semantic\nunderstanding, and achieves consistent gains in retrieval performance. This\neven applies to the recent CLIPS, which achieves SOTA retrieval performance.\nNevertheless, the short fine-tuning with CLIC leads to an improvement in\nretrieval and to the best compositional CLIP model on SugarCrepe++. All our\nmodels and code are available at https://clic-compositional-clip.github.io"}
{"id": "2505.24251", "pdf": "https://arxiv.org/pdf/2505.24251", "abs": "https://arxiv.org/abs/2505.24251", "authors": ["Xiaoyu Li", "Xiao Li", "Li Gao", "Yiding Liu", "Xiaoyang Wang", "Shuaiqiang Wang", "Junfeng Wang", "Dawei Yin"], "title": "Proactive Guidance of Multi-Turn Conversation in Industrial Search", "categories": ["cs.CL", "cs.IR"], "comment": "ACL'25 (Industry)", "summary": "The evolution of Large Language Models (LLMs) has significantly advanced\nmulti-turn conversation systems, emphasizing the need for proactive guidance to\nenhance users' interactions. However, these systems face challenges in\ndynamically adapting to shifts in users' goals and maintaining low latency for\nreal-time interactions. In the Baidu Search AI assistant, an industrial-scale\nmulti-turn search system, we propose a novel two-phase framework to provide\nproactive guidance. The first phase, Goal-adaptive Supervised Fine-Tuning\n(G-SFT), employs a goal adaptation agent that dynamically adapts to user goal\nshifts and provides goal-relevant contextual information. G-SFT also\nincorporates scalable knowledge transfer to distill insights from LLMs into a\nlightweight model for real-time interaction. The second phase, Click-oriented\nReinforcement Learning (C-RL), adopts a generate-rank paradigm, systematically\nconstructs preference pairs from user click signals, and proactively improves\nclick-through rates through more engaging guidance. This dual-phase\narchitecture achieves complementary objectives: G-SFT ensures accurate goal\ntracking, while C-RL optimizes interaction quality through click signal-driven\nreinforcement learning. Extensive experiments demonstrate that our framework\nachieves 86.10% accuracy in offline evaluation (+23.95% over baseline) and\n25.28% CTR in online deployment (149.06% relative improvement), while reducing\ninference latency by 69.55% through scalable knowledge distillation."}
{"id": "2505.24528", "pdf": "https://arxiv.org/pdf/2505.24528", "abs": "https://arxiv.org/abs/2505.24528", "authors": ["Pedram Ghamisi", "Weikang Yu", "Xiaokang Zhang", "Aldino Rizaldy", "Jian Wang", "Chufeng Zhou", "Richard Gloaguen", "Gustau Camps-Valls"], "title": "Geospatial Foundation Models to Enable Progress on Sustainable Development Goals", "categories": ["cs.CV", "cs.LG"], "comment": null, "summary": "Foundation Models (FMs) are large-scale, pre-trained AI systems that have\nrevolutionized natural language processing and computer vision, and are now\nadvancing geospatial analysis and Earth Observation (EO). They promise improved\ngeneralization across tasks, scalability, and efficient adaptation with minimal\nlabeled data. However, despite the rapid proliferation of geospatial FMs, their\nreal-world utility and alignment with global sustainability goals remain\nunderexplored. We introduce SustainFM, a comprehensive benchmarking framework\ngrounded in the 17 Sustainable Development Goals with extremely diverse tasks\nranging from asset wealth prediction to environmental hazard detection. This\nstudy provides a rigorous, interdisciplinary assessment of geospatial FMs and\noffers critical insights into their role in attaining sustainability goals. Our\nfindings show: (1) While not universally superior, FMs often outperform\ntraditional approaches across diverse tasks and datasets. (2) Evaluating FMs\nshould go beyond accuracy to include transferability, generalization, and\nenergy efficiency as key criteria for their responsible use. (3) FMs enable\nscalable, SDG-grounded solutions, offering broad utility for tackling complex\nsustainability challenges. Critically, we advocate for a paradigm shift from\nmodel-centric development to impact-driven deployment, and emphasize metrics\nsuch as energy efficiency, robustness to domain shifts, and ethical\nconsiderations."}
{"id": "2505.24472", "pdf": "https://arxiv.org/pdf/2505.24472", "abs": "https://arxiv.org/abs/2505.24472", "authors": ["Hieu Tran", "Phuong-Anh Nguyen-Le", "Huy Nghiem", "Quang-Nhan Nguyen", "Wei Ai", "Marine Carpuat"], "title": "VietMix: A Naturally Occurring Vietnamese-English Code-Mixed Corpus with Iterative Augmentation for Machine Translation", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": null, "summary": "Machine translation systems fail when processing code-mixed inputs for\nlow-resource languages. We address this challenge by curating VietMix, a\nparallel corpus of naturally occurring code-mixed Vietnamese text paired with\nexpert English translations. Augmenting this resource, we developed a\ncomplementary synthetic data generation pipeline. This pipeline incorporates\nfiltering mechanisms to ensure syntactic plausibility and pragmatic\nappropriateness in code-mixing patterns. Experimental validation shows our\nnaturalistic and complementary synthetic data boost models' performance,\nmeasured by translation quality estimation scores, of up to 71.84 on COMETkiwi\nand 81.77 on XCOMET. Triangulating positive results with LLM-based assessments,\naugmented models are favored over seed fine-tuned counterparts in approximately\n49% of judgments (54-56% excluding ties). VietMix and our augmentation\nmethodology advance ecological validity in neural MT evaluations and establish\na framework for addressing code-mixed translation challenges across other\nlow-resource pairs."}
{"id": "2505.24434", "pdf": "https://arxiv.org/pdf/2505.24434", "abs": "https://arxiv.org/abs/2505.24434", "authors": ["Md Shahriar Rahim Siddiqui", "Moshe Eliasof", "Eldad Haber"], "title": "Graph Flow Matching: Enhancing Image Generation with Neighbor-Aware Flow Fields", "categories": ["cs.LG", "cs.CV"], "comment": null, "summary": "Flow matching casts sample generation as learning a continuous-time velocity\nfield that transports noise to data. Existing flow matching networks typically\npredict each point's velocity independently, considering only its location and\ntime along its flow trajectory, and ignoring neighboring points. However, this\npointwise approach may overlook correlations between points along the\ngeneration trajectory that could enhance velocity predictions, thereby\nimproving downstream generation quality. To address this, we propose Graph Flow\nMatching (GFM), a lightweight enhancement that decomposes the learned velocity\ninto a reaction term -- any standard flow matching network -- and a diffusion\nterm that aggregates neighbor information via a graph neural module. This\nreaction-diffusion formulation retains the scalability of deep flow models\nwhile enriching velocity predictions with local context, all at minimal\nadditional computational cost. Operating in the latent space of a pretrained\nvariational autoencoder, GFM consistently improves Fr\\'echet Inception Distance\n(FID) and recall across five image generation benchmarks (LSUN Church, LSUN\nBedroom, FFHQ, AFHQ-Cat, and CelebA-HQ at $256\\times256$), demonstrating its\neffectiveness as a modular enhancement to existing flow matching architectures."}
{"id": "2505.24263", "pdf": "https://arxiv.org/pdf/2505.24263", "abs": "https://arxiv.org/abs/2505.24263", "authors": ["Naila Shafirni Hidayat", "Muhammad Dehan Al Kautsar", "Alfan Farizki Wicaksono", "Fajri Koto"], "title": "Simulating Training Data Leakage in Multiple-Choice Benchmarks for LLM Evaluation", "categories": ["cs.CL"], "comment": null, "summary": "The performance of large language models (LLMs) continues to improve, as\nreflected in rising scores on standard benchmarks. However, the lack of\ntransparency around training data raises concerns about potential overlap with\nevaluation sets and the fairness of reported results. Although prior work has\nproposed methods for detecting data leakage, these approaches primarily focus\non identifying outliers and have not been evaluated under controlled simulated\nleakage conditions. In this work, we compare existing leakage detection\ntechniques, namely permutation and n-gram-based methods, under a continual\npretraining setup that simulates real-world leakage scenarios, and additionally\nexplore a lightweight method we call semi-half question. Although semi-half\noffers a low-cost alternative, our analysis shows that the n-gram method\nconsistently achieves the highest F1-score. We also refine these techniques to\nsupport instance-level detection and reduce computational overhead. Leveraging\nthe best-performing method, we create cleaned versions of MMLU and HellaSwag,\nand re-evaluate several LLMs. Our findings present a practical path toward more\nreliable and transparent evaluations, and we recommend contamination checks as\na standard step before releasing benchmark results."}
{"id": "2505.24541", "pdf": "https://arxiv.org/pdf/2505.24541", "abs": "https://arxiv.org/abs/2505.24541", "authors": ["Xin He", "Xumeng Han", "Longhui Wei", "Lingxi Xie", "Qi Tian"], "title": "Mixpert: Mitigating Multimodal Learning Conflicts with Efficient Mixture-of-Vision-Experts", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Multimodal large language models (MLLMs) require a nuanced interpretation of\ncomplex image information, typically leveraging a vision encoder to perceive\nvarious visual scenarios. However, relying solely on a single vision encoder to\nhandle diverse task domains proves difficult and inevitably leads to conflicts.\nRecent work enhances data perception by directly integrating multiple\ndomain-specific vision encoders, yet this structure adds complexity and limits\nthe potential for joint optimization. In this paper, we introduce Mixpert, an\nefficient mixture-of-vision-experts architecture that inherits the joint\nlearning advantages from a single vision encoder while being restructured into\na multi-expert paradigm for task-specific fine-tuning across different visual\ntasks. Additionally, we design a dynamic routing mechanism that allocates input\nimages to the most suitable visual expert. Mixpert effectively alleviates\ndomain conflicts encountered by a single vision encoder in multi-task learning\nwith minimal additional computational cost, making it more efficient than\nmultiple encoders. Furthermore, Mixpert integrates seamlessly into any MLLM,\nwith experimental results demonstrating substantial performance gains across\nvarious tasks."}
{"id": "2505.24473", "pdf": "https://arxiv.org/pdf/2505.24473", "abs": "https://arxiv.org/abs/2505.24473", "authors": ["Nikita Balagansky", "Yaroslav Aksenov", "Daniil Laptev", "Vadim Kurochkin", "Gleb Gerasimov", "Nikita Koryagin", "Daniil Gavrilov"], "title": "Train One Sparse Autoencoder Across Multiple Sparsity Budgets to Preserve Interpretability and Accuracy", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Sparse Autoencoders (SAEs) have proven to be powerful tools for interpreting\nneural networks by decomposing hidden representations into disentangled,\ninterpretable features via sparsity constraints. However, conventional SAEs are\nconstrained by the fixed sparsity level chosen during training; meeting\ndifferent sparsity requirements therefore demands separate models and increases\nthe computational footprint during both training and evaluation. We introduce a\nnovel training objective, \\emph{HierarchicalTopK}, which trains a single SAE to\noptimise reconstructions across multiple sparsity levels simultaneously.\nExperiments with Gemma-2 2B demonstrate that our approach achieves\nPareto-optimal trade-offs between sparsity and explained variance,\noutperforming traditional SAEs trained at individual sparsity levels. Further\nanalysis shows that HierarchicalTopK preserves high interpretability scores\neven at higher sparsity. The proposed objective thus closes an important gap\nbetween flexibility and interpretability in SAE design."}
{"id": "2505.24438", "pdf": "https://arxiv.org/pdf/2505.24438", "abs": "https://arxiv.org/abs/2505.24438", "authors": ["Franziska Heeg", "Jonas Sauer", "Petra Mutzel", "Ingo Scholtes"], "title": "Weisfeiler and Leman Follow the Arrow of Time: Expressive Power of Message Passing in Temporal Event Graphs", "categories": ["cs.LG"], "comment": null, "summary": "An important characteristic of temporal graphs is how the directed arrow of\ntime influences their causal topology, i.e., which nodes can possibly influence\neach other causally via time-respecting paths. The resulting patterns are often\nneglected by temporal graph neural networks (TGNNs). To formally analyze the\nexpressive power of TGNNs, we lack a generalization of graph isomorphism to\ntemporal graphs that fully captures their causal topology. Addressing this gap,\nwe introduce the notion of consistent event graph isomorphism, which utilizes a\ntime-unfolded representation of time-respecting paths in temporal graphs. We\ncompare this definition with existing notions of temporal graph isomorphisms.\nWe illustrate and highlight the advantages of our approach and develop a\ntemporal generalization of the Weisfeiler-Leman algorithm to heuristically\ndistinguish non-isomorphic temporal graphs. Building on this theoretical\nfoundation, we derive a novel message passing scheme for temporal graph neural\nnetworks that operates on the event graph representation of temporal graphs. An\nexperimental evaluation shows that our approach performs well in a temporal\ngraph classification experiment."}
{"id": "2505.24302", "pdf": "https://arxiv.org/pdf/2505.24302", "abs": "https://arxiv.org/abs/2505.24302", "authors": ["Yike Wang", "Shangbin Feng", "Yulia Tsvetkov", "Hannaneh Hajishirzi"], "title": "ScienceMeter: Tracking Scientific Knowledge Updates in Language Models", "categories": ["cs.CL"], "comment": null, "summary": "Large Language Models (LLMs) are increasingly used to support scientific\nresearch, but their knowledge of scientific advancements can quickly become\noutdated. We introduce ScienceMeter, a new framework for evaluating scientific\nknowledge update methods over scientific knowledge spanning the past, present,\nand future. ScienceMeter defines three metrics: knowledge preservation, the\nextent to which models' understanding of previously learned papers are\npreserved; knowledge acquisition, how well scientific claims from newly\nintroduced papers are acquired; and knowledge projection, the ability of the\nupdated model to anticipate or generalize to related scientific claims that may\nemerge in the future. Using ScienceMeter, we examine the scientific knowledge\nof LLMs on claim judgment and generation tasks across a curated dataset of\n15,444 scientific papers and 30,888 scientific claims from ten domains\nincluding medicine, biology, materials science, and computer science. We\nevaluate five representative knowledge update approaches including training-\nand inference-time methods. With extensive experiments, we find that the\nbest-performing knowledge update methods can preserve only 85.9% of existing\nknowledge, acquire 71.7% of new knowledge, and project 37.7% of future\nknowledge. Inference-based methods work for larger models, whereas smaller\nmodels require training to achieve comparable performance. Cross-domain\nanalysis reveals that performance on these objectives is correlated. Even when\napplying on specialized scientific LLMs, existing knowledge update methods fail\nto achieve these objectives collectively, underscoring that developing robust\nscientific knowledge update mechanisms is both crucial and challenging."}
{"id": "2505.24558", "pdf": "https://arxiv.org/pdf/2505.24558", "abs": "https://arxiv.org/abs/2505.24558", "authors": ["Simone Cammarasana", "Giuseppe PatanÃ¨"], "title": "Optimal Weighted Convolution for Classification and Denosing", "categories": ["cs.CV", "68T05"], "comment": "17 pages, 3 figures, 6 tables", "summary": "We introduce a novel weighted convolution operator that enhances traditional\nconvolutional neural networks (CNNs) by integrating a spatial density function\ninto the convolution operator. This extension enables the network to\ndifferentially weight neighbouring pixels based on their relative position to\nthe reference pixel, improving spatial characterisation and feature extraction.\nThe proposed operator maintains the same number of trainable parameters and is\nfully compatible with existing CNN architectures. Although developed for 2D\nimage data, the framework is generalisable to signals on regular grids of\narbitrary dimensions, such as 3D volumetric data or 1D time series. We propose\nan efficient implementation of the weighted convolution by pre-computing the\ndensity function and achieving execution times comparable to standard\nconvolution layers. We evaluate our method on two deep learning tasks: image\nclassification using the CIFAR-100 dataset [KH+09] and image denoising using\nthe DIV2K dataset [AT17]. Experimental results with state-of-the-art\nclassification (e.g., VGG [SZ15], ResNet [HZRS16]) and denoising (e.g., DnCNN\n[ZZC+17], NAFNet [CCZS22]) methods show that the weighted convolution improves\nperformance with respect to standard convolution across different quantitative\nmetrics. For example, VGG achieves an accuracy of 66.94% with weighted\nconvolution versus 56.89% with standard convolution on the classification\nproblem, while DnCNN improves the PSNR value from 20.17 to 22.63 on the\ndenoising problem. All models were trained on the CINECA Leonardo cluster to\nreduce the execution time and improve the tuning of the density function\nvalues. The PyTorch implementation of the weighted convolution is publicly\navailable at: https://github.com/cammarasana123/weightedConvolution2.0."}
{"id": "2505.24477", "pdf": "https://arxiv.org/pdf/2505.24477", "abs": "https://arxiv.org/abs/2505.24477", "authors": ["LearnLM Team", "Abhinit Modi", "Aditya Srikanth Veerubhotla", "Aliya Rysbek", "Andrea Huber", "Ankit Anand", "Avishkar Bhoopchand", "Brett Wiltshire", "Daniel Gillick", "Daniel Kasenberg", "Eleni Sgouritsa", "Gal Elidan", "Hengrui Liu", "Holger Winnemoeller", "Irina Jurenka", "James Cohan", "Jennifer She", "Julia Wilkowski", "Kaiz Alarakyia", "Kevin R. McKee", "Komal Singh", "Lisa Wang", "Markus Kunesch", "Miruna PÃ®slar", "Niv Efron", "Parsa Mahmoudieh", "Pierre-Alexandre Kamienny", "Sara Wiltberger", "Shakir Mohamed", "Shashank Agarwal", "Shubham Milind Phal", "Sun Jae Lee", "Theofilos Strinopoulos", "Wei-Jen Ko", "Yael Gold-Zamir", "Yael Haramaty", "Yannis Assael"], "title": "Evaluating Gemini in an arena for learning", "categories": ["cs.CY", "cs.AI", "cs.LG"], "comment": null, "summary": "Artificial intelligence (AI) is poised to transform education, but the\nresearch community lacks a robust, general benchmark to evaluate AI models for\nlearning. To assess state-of-the-art support for educational use cases, we ran\nan \"arena for learning\" where educators and pedagogy experts conduct blind,\nhead-to-head, multi-turn comparisons of leading AI models. In particular, $N =\n189$ educators drew from their experience to role-play realistic learning use\ncases, interacting with two models sequentially, after which $N = 206$ experts\njudged which model better supported the user's learning goals. The arena\nevaluated a slate of state-of-the-art models: Gemini 2.5 Pro, Claude 3.7\nSonnet, GPT-4o, and OpenAI o3. Excluding ties, experts preferred Gemini 2.5 Pro\nin 73.2% of these match-ups -- ranking it first overall in the arena. Gemini\n2.5 Pro also demonstrated markedly higher performance across key principles of\ngood pedagogy. Altogether, these results position Gemini 2.5 Pro as a leading\nmodel for learning."}
{"id": "2505.24452", "pdf": "https://arxiv.org/pdf/2505.24452", "abs": "https://arxiv.org/abs/2505.24452", "authors": ["Anda Tang", "Yiming Dong", "Yutao Zeng", "zhou Xun", "Zhouchen Lin"], "title": "Stepsize anything: A unified learning rate schedule for budgeted-iteration training", "categories": ["cs.LG"], "comment": null, "summary": "The expanding computational costs and limited resources underscore the\ncritical need for budgeted-iteration training, which aims to achieve optimal\nlearning within predetermined iteration budgets.While learning rate schedules\nfundamentally govern the performance of different networks and tasks,\nparticularly in budgeted-iteration scenarios, their design remains largely\nheuristic, lacking theoretical foundations.In addition, the optimal learning\nrate schedule requires extensive trial-and-error selection, making the training\nprocess inefficient.In this work, we propose the Unified Budget-Aware (UBA)\nschedule, a theoretically grounded learning rate schedule that consistently\noutperforms commonly-used schedules among diverse architectures and tasks under\ndifferent constrained training budgets.First, we bridge the gap by constructing\na novel training budget-aware optimization framework, which explicitly accounts\nfor the robustness to landscape curvature variations.From this framework, we\nderive the UBA schedule, controlled by a single hyper-parameter $\\varphi$ that\nprovides a trade-off between flexibility and simplicity, eliminating the need\nfor per-network numerical optimization. Moreover, we establish a theoretical\nconnection between $\\varphi$ and the condition number, adding interpretation\nand justification to our approach. Besides, we prove the convergence for\ndifferent values of $\\varphi$.We offer practical guidelines for its selection\nvia theoretical analysis and empirical results.xtensive experimental results\nshow that UBA \\textit{consistently surpasses} the commonly-used schedules\nacross diverse vision and language tasks, spanning network architectures (e.g.,\nResNet, OLMo) and scales, under different training-iteration budgets."}
{"id": "2505.24319", "pdf": "https://arxiv.org/pdf/2505.24319", "abs": "https://arxiv.org/abs/2505.24319", "authors": ["Yuntao Shi", "Yi Luo", "Yeyun Gong", "Chen Lin"], "title": "HiCaM: A Hierarchical-Causal Modification Framework for Long-Form Text Modification", "categories": ["cs.CL"], "comment": null, "summary": "Large Language Models (LLMs) have achieved remarkable success in various\ndomains. However, when handling long-form text modification tasks, they still\nface two major problems: (1) producing undesired modifications by\ninappropriately altering or summarizing irrelevant content, and (2) missing\nnecessary modifications to implicitly related passages that are crucial for\nmaintaining document coherence. To address these issues, we propose HiCaM, a\nHierarchical-Causal Modification framework that operates through a hierarchical\nsummary tree and a causal graph. Furthermore, to evaluate HiCaM, we derive a\nmulti-domain dataset from various benchmarks, providing a resource for\nassessing its effectiveness. Comprehensive evaluations on the dataset\ndemonstrate significant improvements over strong LLMs, with our method\nachieving up to a 79.50\\% win rate. These results highlight the\ncomprehensiveness of our approach, showing consistent performance improvements\nacross multiple models and domains."}
{"id": "2505.24567", "pdf": "https://arxiv.org/pdf/2505.24567", "abs": "https://arxiv.org/abs/2505.24567", "authors": ["Qinghe Ma", "Jian Zhang", "Lei Qi", "Qian Yu", "Yinghuan Shi", "Yang Gao"], "title": "Unleashing the Power of Intermediate Domains for Mixed Domain Semi-Supervised Medical Image Segmentation", "categories": ["cs.CV"], "comment": "Accepted by IEEE TMI 2025. arXiv admin note: text overlap with\n  arXiv:2404.08951", "summary": "Both limited annotation and domain shift are prevalent challenges in medical\nimage segmentation. Traditional semi-supervised segmentation and unsupervised\ndomain adaptation methods address one of these issues separately. However, the\ncoexistence of limited annotation and domain shift is quite common, which\nmotivates us to introduce a novel and challenging scenario: Mixed Domain\nSemi-supervised medical image Segmentation (MiDSS), where limited labeled data\nfrom a single domain and a large amount of unlabeled data from multiple\ndomains. To tackle this issue, we propose the UST-RUN framework, which fully\nleverages intermediate domain information to facilitate knowledge transfer. We\nemploy Unified Copy-paste (UCP) to construct intermediate domains, and propose\na Symmetric GuiDance training strategy (SymGD) to supervise unlabeled data by\nmerging pseudo-labels from intermediate samples. Subsequently, we introduce a\nTraining Process aware Random Amplitude MixUp (TP-RAM) to progressively\nincorporate style-transition components into intermediate samples. To generate\nmore diverse intermediate samples, we further select reliable samples with\nhigh-quality pseudo-labels, which are then mixed with other unlabeled data.\nAdditionally, we generate sophisticated intermediate samples with high-quality\npseudo-labels for unreliable samples, ensuring effective knowledge transfer for\nthem. Extensive experiments on four public datasets demonstrate the superiority\nof UST-RUN. Notably, UST-RUN achieves a 12.94% improvement in Dice score on the\nProstate dataset. Our code is available at https://github.com/MQinghe/UST-RUN"}
{"id": "2505.24480", "pdf": "https://arxiv.org/pdf/2505.24480", "abs": "https://arxiv.org/abs/2505.24480", "authors": ["Fei Bai", "Yingqian Min", "Beichen Zhang", "Zhipeng Chen", "Wayne Xin Zhao", "Lei Fang", "Zheng Liu", "Zhongyuan Wang", "Ji-Rong Wen"], "title": "Towards Effective Code-Integrated Reasoning", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "Technical Report on Slow Thinking with LLMs: Code-Integrated\n  Reasoning", "summary": "In this paper, we investigate code-integrated reasoning, where models\ngenerate code when necessary and integrate feedback by executing it through a\ncode interpreter. To acquire this capability, models must learn when and how to\nuse external code tools effectively, which is supported by tool-augmented\nreinforcement learning (RL) through interactive learning. Despite its benefits,\ntool-augmented RL can still suffer from potential instability in the learning\ndynamics. In light of this challenge, we present a systematic approach to\nimproving the training effectiveness and stability of tool-augmented RL for\ncode-integrated reasoning. Specifically, we develop enhanced training\nstrategies that balance exploration and stability, progressively building\ntool-use capabilities while improving reasoning performance. Through extensive\nexperiments on five mainstream mathematical reasoning benchmarks, our model\ndemonstrates significant performance improvements over multiple competitive\nbaselines. Furthermore, we conduct an in-depth analysis of the mechanism and\neffect of code-integrated reasoning, revealing several key insights, such as\nthe extension of model's capability boundaries and the simultaneous improvement\nof reasoning efficiency through code integration. All data and code for\nreproducing this work are available at: https://github.com/RUCAIBox/CIR."}
{"id": "2505.24461", "pdf": "https://arxiv.org/pdf/2505.24461", "abs": "https://arxiv.org/abs/2505.24461", "authors": ["Jingyao Li", "Senqiao Yang", "Sitong Wu", "Han Shi", "Chuanyang Zheng", "Hong Xu", "Jiaya Jia"], "title": "Logits-Based Finetuning", "categories": ["cs.LG"], "comment": null, "summary": "The core of out-of-distribution (OOD) detection is to learn the\nin-distribution (ID) representation, which is distinguishable from OOD samples.\nPrevious work applied recognition-based methods to learn the ID features, which\ntend to learn shortcuts instead of comprehensive representations. In this work,\nwe find surprisingly that simply using reconstruction-based methods could boost\nthe performance of OOD detection significantly. We deeply explore the main\ncontributors of OOD detection and find that reconstruction-based pretext tasks\nhave the potential to provide a generally applicable and efficacious prior,\nwhich benefits the model in learning intrinsic data distributions of the ID\ndataset. Specifically, we take Masked Image Modeling as a pretext task for our\nOOD detection framework (MOOD). Without bells and whistles, MOOD outperforms\nprevious SOTA of one-class OOD detection by 5.7%, multi-class OOD detection by\n3.0%, and near-distribution OOD detection by 2.1%. It even defeats the\n10-shot-per-class outlier exposure OOD detection, although we do not include\nany OOD samples for our detection. Codes are available at\nhttps://github.com/JulietLJY/MOOD."}
{"id": "2505.24331", "pdf": "https://arxiv.org/pdf/2505.24331", "abs": "https://arxiv.org/abs/2505.24331", "authors": ["Fanhang Man", "Huandong Wang", "Jianjie Fang", "Zhaoyi Deng", "Baining Zhao", "Xinlei Chen", "Yong Li"], "title": "Context-Aware Sentiment Forecasting via LLM-based Multi-Perspective Role-Playing Agents", "categories": ["cs.CL"], "comment": null, "summary": "User sentiment on social media reveals the underlying social trends, crises,\nand needs. Researchers have analyzed users' past messages to trace the\nevolution of sentiments and reconstruct sentiment dynamics. However, predicting\nthe imminent sentiment of an ongoing event is rarely studied. In this paper, we\naddress the problem of \\textbf{sentiment forecasting} on social media to\npredict the user's future sentiment in response to the development of the\nevent. We extract sentiment-related features to enhance the modeling skill and\npropose a multi-perspective role-playing framework to simulate the process of\nhuman response. Our preliminary results show significant improvement in\nsentiment forecasting on both microscopic and macroscopic levels."}
{"id": "2505.24600", "pdf": "https://arxiv.org/pdf/2505.24600", "abs": "https://arxiv.org/abs/2505.24600", "authors": ["Omer Nacar", "Yasser Al-Habashi", "Serry Sibaee", "Adel Ammar", "Wadii Boulila"], "title": "SARD: A Large-Scale Synthetic Arabic OCR Dataset for Book-Style Text Recognition", "categories": ["cs.CV"], "comment": null, "summary": "Arabic Optical Character Recognition (OCR) is essential for converting vast\namounts of Arabic print media into digital formats. However, training modern\nOCR models, especially powerful vision-language models, is hampered by the lack\nof large, diverse, and well-structured datasets that mimic real-world book\nlayouts. Existing Arabic OCR datasets often focus on isolated words or lines or\nare limited in scale, typographic variety, or structural complexity found in\nbooks. To address this significant gap, we introduce SARD (Large-Scale\nSynthetic Arabic OCR Dataset). SARD is a massive, synthetically generated\ndataset specifically designed to simulate book-style documents. It comprises\n843,622 document images containing 690 million words, rendered across ten\ndistinct Arabic fonts to ensure broad typographic coverage. Unlike datasets\nderived from scanned documents, SARD is free from real-world noise and\ndistortions, offering a clean and controlled environment for model training.\nIts synthetic nature provides unparalleled scalability and allows for precise\ncontrol over layout and content variation. We detail the dataset's composition\nand generation process and provide benchmark results for several OCR models,\nincluding traditional and deep learning approaches, highlighting the challenges\nand opportunities presented by this dataset. SARD serves as a valuable resource\nfor developing and evaluating robust OCR and vision-language models capable of\nprocessing diverse Arabic book-style texts."}
{"id": "2505.24492", "pdf": "https://arxiv.org/pdf/2505.24492", "abs": "https://arxiv.org/abs/2505.24492", "authors": ["David Steinmann", "Wolfgang Stammer", "Antonia WÃ¼st", "Kristian Kersting"], "title": "Object Centric Concept Bottlenecks", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Developing high-performing, yet interpretable models remains a critical\nchallenge in modern AI. Concept-based models (CBMs) attempt to address this by\nextracting human-understandable concepts from a global encoding (e.g., image\nencoding) and then applying a linear classifier on the resulting concept\nactivations, enabling transparent decision-making. However, their reliance on\nholistic image encodings limits their expressiveness in object-centric\nreal-world settings and thus hinders their ability to solve complex vision\ntasks beyond single-label classification. To tackle these challenges, we\nintroduce Object-Centric Concept Bottlenecks (OCB), a framework that combines\nthe strengths of CBMs and pre-trained object-centric foundation models,\nboosting performance and interpretability. We evaluate OCB on complex image\ndatasets and conduct a comprehensive ablation study to analyze key components\nof the framework, such as strategies for aggregating object-concept encodings.\nThe results show that OCB outperforms traditional CBMs and allows one to make\ninterpretable decisions for complex visual tasks."}
{"id": "2505.24469", "pdf": "https://arxiv.org/pdf/2505.24469", "abs": "https://arxiv.org/abs/2505.24469", "authors": ["Christina Runkel", "Natacha Kuete Meli", "Jovita Lukasik", "Ander Biguri", "Carola-Bibiane SchÃ¶nlieb", "Michael Moeller"], "title": "Smooth Model Compression without Fine-Tuning", "categories": ["cs.LG"], "comment": null, "summary": "Compressing and pruning large machine learning models has become a critical\nstep towards their deployment in real-world applications. Standard pruning and\ncompression techniques are typically designed without taking the structure of\nthe network's weights into account, limiting their effectiveness. We explore\nthe impact of smooth regularization on neural network training and model\ncompression. By applying nuclear norm, first- and second-order derivative\npenalties of the weights during training, we encourage structured smoothness\nwhile preserving predictive performance on par with non-smooth models. We find\nthat standard pruning methods often perform better when applied to these smooth\nmodels. Building on this observation, we apply a\nSingular-Value-Decomposition-based compression method that exploits the\nunderlying smooth structure and approximates the model's weight tensors by\nsmaller low-rank tensors. Our approach enables state-of-the-art compression\nwithout any fine-tuning - reaching up to $91\\%$ accuracy on a smooth ResNet-18\non CIFAR-10 with $70\\%$ fewer parameters."}
{"id": "2505.24332", "pdf": "https://arxiv.org/pdf/2505.24332", "abs": "https://arxiv.org/abs/2505.24332", "authors": ["Wenxuan Shi", "Haochen Tan", "Chuqiao Kuang", "Xiaoguang Li", "Xiaozhe Ren", "Chen Zhang", "Hanting Chen", "Yasheng Wang", "Lifeng Shang", "Fisher Yu", "Yunhe Wang"], "title": "Pangu DeepDiver: Adaptive Search Intensity Scaling via Open-Web Reinforcement Learning", "categories": ["cs.CL"], "comment": null, "summary": "Information seeking demands iterative evidence gathering and reflective\nreasoning, yet large language models (LLMs) still struggle with it in open-web\nquestion answering. Existing methods rely on static prompting rules or training\nwith Wikipedia-based corpora and retrieval environments, limiting adaptability\nto the real-world web environment where ambiguity, conflicting evidence, and\nnoise are prevalent. These constrained training settings hinder LLMs from\nlearning to dynamically decide when and where to search, and how to adjust\nsearch depth and frequency based on informational demands. We define this\nmissing capacity as Search Intensity Scaling (SIS)--the emergent skill to\nintensify search efforts under ambiguous or conflicting conditions, rather than\nsettling on overconfident, under-verification answers.\n  To study SIS, we introduce WebPuzzle, the first dataset designed to foster\ninformation-seeking behavior in open-world internet environments. WebPuzzle\nconsists of 24K training instances and 275 test questions spanning both\nwiki-based and open-web queries. Building on this dataset, we propose\nDeepDiver, a Reinforcement Learning (RL) framework that promotes SIS by\nencouraging adaptive search policies through exploration under a real-world\nopen-web environment. Experimental results show that Pangu-7B-Reasoner\nempowered by DeepDiver achieve performance on real-web tasks comparable to the\n671B-parameter DeepSeek-R1. We detail DeepDiver's training curriculum from\ncold-start supervised fine-tuning to a carefully designed RL phase, and present\nthat its capability of SIS generalizes from closed-form QA to open-ended tasks\nsuch as long-form writing. Our contributions advance adaptive information\nseeking in LLMs and provide a valuable benchmark and dataset for future\nresearch."}
{"id": "2505.24608", "pdf": "https://arxiv.org/pdf/2505.24608", "abs": "https://arxiv.org/abs/2505.24608", "authors": ["Panagiotis Rigas", "Panagiotis Drivas", "Charalambos Tzamos", "Ioannis Chamodrakas", "George Ioannakis", "Leonidas J. Guibas", "Ioannis Z. Emiris"], "title": "GARLIC: GAussian Representation LearnIng for spaCe partitioning", "categories": ["cs.CV"], "comment": null, "summary": "We introduce GARLIC (GAussian Representation LearnIng for spaCe\npartitioning), a novel indexing structure based on \\(N\\)-dimensional Gaussians\nfor efficiently learning high-dimensional vector spaces. Our approach is\ninspired from Gaussian splatting techniques, typically used in 3D rendering,\nwhich we adapt for high-dimensional search and classification. We optimize\nGaussian parameters using information-theoretic objectives that balance\ncoverage, assignment confidence, and structural and semantic consistency. A key\ncontribution is to progressively refine the representation through split and\nclone operations, handling hundreds of dimensions, thus handling varying data\ndensities. GARLIC offers the fast building times of traditional space\npartitioning methods (e.g., under \\(\\sim5\\) min build time for SIFT1M) while\nachieving \\(\\sim50\\%\\) Recall10@10 in low-candidate regimes. Experimental\nresults on standard benchmarks demonstrate our method's consistency in (a)\n\\(k\\)-NN retrieval, outperforming methods, such as Faiss-IVF, in fast-recall by\nusing about half their probes for the same Recall10@10 in Fashion-MNIST, and\n(b) in classification tasks, beating by \\(\\sim15\\%\\) accuracy other majority\nvoting methods. Further, we show strong generalization capabilities,\nmaintaining high accuracy even with downsampled training data: using just\n\\(1\\%\\) of the training data returns \\(\\sim 45\\%\\) Recall@1, thus making GARLIC\nquite powerful for applications requiring both speed and accuracy."}
{"id": "2505.24500", "pdf": "https://arxiv.org/pdf/2505.24500", "abs": "https://arxiv.org/abs/2505.24500", "authors": ["Guiyang Hou", "Xing Gao", "Yuchuan Wu", "Xiang Huang", "Wenqi Zhang", "Zhe Zheng", "Yongliang Shen", "Jialu Du", "Fei Huang", "Yongbin Li", "Weiming Lu"], "title": "TimeHC-RL: Temporal-aware Hierarchical Cognitive Reinforcement Learning for Enhancing LLMs' Social Intelligence", "categories": ["cs.CL", "cs.AI"], "comment": "22 pages, 12 figures", "summary": "Recently, Large Language Models (LLMs) have made significant progress in\nIQ-related domains that require careful thinking, such as mathematics and\ncoding. However, enhancing LLMs' cognitive development in social domains,\nparticularly from a post-training perspective, remains underexplored.\nRecognizing that the social world follows a distinct timeline and requires a\nricher blend of cognitive modes (from intuitive reactions (System 1) and\nsurface-level thinking to deliberate thinking (System 2)) than mathematics,\nwhich primarily relies on System 2 cognition (careful, step-by-step reasoning),\nwe introduce Temporal-aware Hierarchical Cognitive Reinforcement Learning\n(TimeHC-RL) for enhancing LLMs' social intelligence. In our experiments, we\nsystematically explore improving LLMs' social intelligence and validate the\neffectiveness of the TimeHC-RL method, through five other post-training\nparadigms and two test-time intervention paradigms on eight datasets with\ndiverse data patterns. Experimental results reveal the superiority of our\nproposed TimeHC-RL method compared to the widely adopted System 2 RL method. It\ngives the 7B backbone model wings, enabling it to rival the performance of\nadvanced models like DeepSeek-R1 and OpenAI-O3. Additionally, the systematic\nexploration from post-training and test-time interventions perspectives to\nimprove LLMs' social intelligence has uncovered several valuable insights."}
{"id": "2505.24498", "pdf": "https://arxiv.org/pdf/2505.24498", "abs": "https://arxiv.org/abs/2505.24498", "authors": ["Andres Fernandez", "Juan Azcarreta", "Cagdas Bilen", "Jesus Monge Alvarez"], "title": "Efficient Neural and Numerical Methods for High-Quality Online Speech Spectrogram Inversion via Gradient Theorem", "categories": ["cs.LG"], "comment": "Accepted at InterSpeech 2025", "summary": "Recent work in online speech spectrogram inversion effectively combines Deep\nLearning with the Gradient Theorem to predict phase derivatives directly from\nmagnitudes. Then, phases are estimated from their derivatives via least\nsquares, resulting in a high quality reconstruction. In this work, we introduce\nthree innovations that drastically reduce computational cost, while maintaining\nhigh quality: Firstly, we introduce a novel neural network architecture with\njust 8k parameters, 30 times smaller than previous state of the art. Secondly,\nincreasing latency by 1 hop size allows us to further halve the cost of the\nneural inference step. Thirdly, we we observe that the least squares problem\nfeatures a tridiagonal matrix and propose a linear-complexity solver for the\nleast squares step that leverages tridiagonality and positive-semidefiniteness,\nachieving a speedup of several orders of magnitude. We release samples online."}
{"id": "2505.24354", "pdf": "https://arxiv.org/pdf/2505.24354", "abs": "https://arxiv.org/abs/2505.24354", "authors": ["Qianqian Zhang", "Jiajia Liao", "Heting Ying", "Yibo Ma", "Haozhan Shen", "Jingcheng Li", "Peng Liu", "Lu Zhang", "Chunxin Fang", "Kyusong Lee", "Ruochen Xu", "Tiancheng Zhao"], "title": "Unifying Language Agent Algorithms with Graph-based Orchestration Engine for Reproducible Agent Research", "categories": ["cs.CL"], "comment": "Accepted by ACL 2025 Demo", "summary": "Language agents powered by large language models (LLMs) have demonstrated\nremarkable capabilities in understanding, reasoning, and executing complex\ntasks. However, developing robust agents presents significant challenges:\nsubstantial engineering overhead, lack of standardized components, and\ninsufficient evaluation frameworks for fair comparison. We introduce Agent\nGraph-based Orchestration for Reasoning and Assessment (AGORA), a flexible and\nextensible framework that addresses these challenges through three key\ncontributions: (1) a modular architecture with a graph-based workflow engine,\nefficient memory management, and clean component abstraction; (2) a\ncomprehensive suite of reusable agent algorithms implementing state-of-the-art\nreasoning approaches; and (3) a rigorous evaluation framework enabling\nsystematic comparison across multiple dimensions. Through extensive experiments\non mathematical reasoning and multimodal tasks, we evaluate various agent\nalgorithms across different LLMs, revealing important insights about their\nrelative strengths and applicability. Our results demonstrate that while\nsophisticated reasoning approaches can enhance agent capabilities, simpler\nmethods like Chain-of-Thought often exhibit robust performance with\nsignificantly lower computational overhead. AGORA not only simplifies language\nagent development but also establishes a foundation for reproducible agent\nresearch through standardized evaluation protocols."}
{"id": "2505.24625", "pdf": "https://arxiv.org/pdf/2505.24625", "abs": "https://arxiv.org/abs/2505.24625", "authors": ["Duo Zheng", "Shijia Huang", "Yanyang Li", "Liwei Wang"], "title": "Learning from Videos for 3D World: Enhancing MLLMs with 3D Vision Geometry Priors", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Previous research has investigated the application of Multimodal Large\nLanguage Models (MLLMs) in understanding 3D scenes by interpreting them as\nvideos. These approaches generally depend on comprehensive 3D data inputs, such\nas point clouds or reconstructed Bird's-Eye View (BEV) maps. In our research,\nwe advance this field by enhancing the capability of MLLMs to understand and\nreason in 3D spaces directly from video data, without the need for additional\n3D input. We propose a novel and efficient method, the Video-3D Geometry Large\nLanguage Model (VG LLM). Our approach employs a 3D visual geometry encoder that\nextracts 3D prior information from video sequences. This information is\nintegrated with visual tokens and fed into the MLLM. Extensive experiments have\nshown that our method has achieved substantial improvements in various tasks\nrelated to 3D scene understanding and spatial reasoning, all directly learned\nfrom video sources. Impressively, our 4B model, which does not rely on explicit\n3D data inputs, achieves competitive results compared to existing\nstate-of-the-art methods, and even surpasses the Gemini-1.5-Pro in the\nVSI-Bench evaluations."}
{"id": "2505.24503", "pdf": "https://arxiv.org/pdf/2505.24503", "abs": "https://arxiv.org/abs/2505.24503", "authors": ["Tzeh Yuan Neoh", "Jannik Peters", "Nicholas Teh"], "title": "Online Fair Division with Additional Information", "categories": ["cs.GT", "cs.AI"], "comment": null, "summary": "We study the problem of fairly allocating indivisible goods to agents in an\nonline setting, where goods arrive sequentially and must be allocated\nirrevocably to agents. Focusing on the popular fairness notions of\nenvy-freeness, proportionality, and maximin share fairness (and their\napproximate variants), we ask how the availability of information on future\ngoods influences the existence and approximability of fair allocations. In the\nabsence of any such information, we establish strong impossibility results,\ndemonstrating the inherent difficulty of achieving even approximate fairness\nguarantees. In contrast, we demonstrate that knowledge of additional\ninformation -- such as aggregate of each agent's total valuations\n(equivalently, normalized valuations) or the multiset of future goods values\n(frequency predictions) -- would enable the design of fairer online algorithms.\nGiven normalization information, we propose an algorithm that achieves stronger\nfairness guarantees than previously known results. Given frequency predictions,\nwe introduce a meta-algorithm that leverages frequency predictions to match the\nbest-known offline guarantees for a broad class of ''share-based'' fairness\nnotions. Our complementary impossibility results in each setting underscore\nboth the limitations imposed by uncertainty about future goods and the\npotential of leveraging structured information to achieve fairer outcomes in\nonline fair division."}
{"id": "2505.24505", "pdf": "https://arxiv.org/pdf/2505.24505", "abs": "https://arxiv.org/abs/2505.24505", "authors": ["Ignacio Boero", "Santiago Diaz", "TomÃ¡s VÃ¡zquez", "Enzo Coppes", "Pablo Belzarena", "Federico Larroca"], "title": "Learning to Optimally Dispatch Power: Performance on a Nation-Wide Real-World Dataset", "categories": ["cs.LG", "math.OC"], "comment": null, "summary": "The Optimal Reactive Power Dispatch (ORPD) problem plays a crucial role in\npower system operations, ensuring voltage stability and minimizing power\nlosses. Recent advances in machine learning, particularly within the ``learning\nto optimize'' framework, have enabled fast and efficient approximations of ORPD\nsolutions, typically by training models on precomputed optimization results.\nWhile these approaches have demonstrated promising performance on synthetic\ndatasets, their effectiveness under real-world grid conditions remains largely\nunexplored. This paper makes two key contributions. First, we introduce a\npublicly available power system dataset that includes both the structural\ncharacteristics of Uruguay's electrical grid and nearly two years of real-world\noperational data, encompassing actual demand and generation profiles. Given\nUruguay's high penetration of renewable energy, the ORPD problem has become the\nprimary optimization challenge in its power network. Second, we assess the\nimpact of real-world data on learning-based ORPD solutions, revealing a\nsignificant increase in prediction errors when transitioning from synthetic to\nactual demand and generation inputs. Our results highlight the limitations of\nexisting models in learning under the complex statistical properties of real\ngrid conditions and emphasize the need for more expressive architectures. By\nproviding this dataset, we aim to facilitate further research into robust\nlearning-based optimization techniques for power system management."}
{"id": "2505.24355", "pdf": "https://arxiv.org/pdf/2505.24355", "abs": "https://arxiv.org/abs/2505.24355", "authors": ["Sihan Tan", "Taro Miyazaki", "Kazuhiro Nakadai"], "title": "Multilingual Gloss-free Sign Language Translation: Towards Building a Sign Language Foundation Model", "categories": ["cs.CL"], "comment": null, "summary": "Sign Language Translation (SLT) aims to convert sign language (SL) videos\ninto spoken language text, thereby bridging the communication gap between the\nsign and the spoken community. While most existing works focus on translating a\nsingle sign language into a single spoken language (one-to-one SLT), leveraging\nmultilingual resources could mitigate low-resource issues and enhance\naccessibility. However, multilingual SLT (MLSLT) remains unexplored due to\nlanguage conflicts and alignment difficulties across SLs and spoken languages.\nTo address these challenges, we propose a multilingual gloss-free model with\ndual CTC objectives for token-level SL identification and spoken text\ngeneration. Our model supports 10 SLs and handles one-to-one, many-to-one, and\nmany-to-many SLT tasks, achieving competitive performance compared to\nstate-of-the-art methods on three widely adopted benchmarks: multilingual\nSP-10, PHOENIX14T, and CSL-Daily."}
{"id": "2505.24634", "pdf": "https://arxiv.org/pdf/2505.24634", "abs": "https://arxiv.org/abs/2505.24634", "authors": ["Xuzhi Wang", "Wei Feng", "Lingdong Kong", "Liang Wan"], "title": "NUC-Net: Non-uniform Cylindrical Partition Network for Efficient LiDAR Semantic Segmentation", "categories": ["cs.CV"], "comment": null, "summary": "LiDAR semantic segmentation plays a vital role in autonomous driving.\nExisting voxel-based methods for LiDAR semantic segmentation apply uniform\npartition to the 3D LiDAR point cloud to form a structured representation based\non cartesian/cylindrical coordinates. Although these methods show impressive\nperformance, the drawback of existing voxel-based methods remains in two\naspects: (1) it requires a large enough input voxel resolution, which brings a\nlarge amount of computation cost and memory consumption. (2) it does not well\nhandle the unbalanced point distribution of LiDAR point cloud. In this paper,\nwe propose a non-uniform cylindrical partition network named NUC-Net to tackle\nthe above challenges. Specifically, we propose the Arithmetic Progression of\nInterval (API) method to non-uniformly partition the radial axis and generate\nthe voxel representation which is representative and efficient. Moreover, we\npropose a non-uniform multi-scale aggregation method to improve contextual\ninformation. Our method achieves state-of-the-art performance on SemanticKITTI\nand nuScenes datasets with much faster speed and much less training time. And\nour method can be a general component for LiDAR semantic segmentation, which\nsignificantly improves both the accuracy and efficiency of the uniform\ncounterpart by $4 \\times$ training faster and $2 \\times$ GPU memory reduction\nand $3 \\times$ inference speedup. We further provide theoretical analysis\ntowards understanding why NUC is effective and how point distribution affects\nperformance. Code is available at\n\\href{https://github.com/alanWXZ/NUC-Net}{https://github.com/alanWXZ/NUC-Net}."}
{"id": "2505.24511", "pdf": "https://arxiv.org/pdf/2505.24511", "abs": "https://arxiv.org/abs/2505.24511", "authors": ["Jiahao Wang", "Mingyue Cheng", "Qi Liu"], "title": "Can Slow-thinking LLMs Reason Over Time? Empirical Studies in Time Series Forecasting", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Time series forecasting (TSF) is a fundamental and widely studied task,\nspanning methods from classical statistical approaches to modern deep learning\nand multimodal language modeling. Despite their effectiveness, these methods\noften follow a fast thinking paradigm emphasizing pattern extraction and direct\nvalue mapping, while overlooking explicit reasoning over temporal dynamics and\ncontextual dependencies. Meanwhile, emerging slow-thinking LLMs (e.g.,\nChatGPT-o1, DeepSeek-R1) have demonstrated impressive multi-step reasoning\ncapabilities across diverse domains, suggesting a new opportunity for reframing\nTSF as a structured reasoning task. This motivates a key question: can\nslow-thinking LLMs effectively reason over temporal patterns to support time\nseries forecasting, even in zero-shot manner? To investigate this, in this\npaper, we propose TimeReasoner, an extensive empirical study that formulates\nTSF as a conditional reasoning task. We design a series of prompting strategies\nto elicit inference-time reasoning from pretrained slow-thinking LLMs and\nevaluate their performance across diverse TSF benchmarks. Our findings reveal\nthat slow-thinking LLMs exhibit non-trivial zero-shot forecasting capabilities,\nespecially in capturing high-level trends and contextual shifts. While\npreliminary, our study surfaces important insights into the reasoning behaviors\nof LLMs in temporal domains highlighting both their potential and limitations.\nWe hope this work catalyzes further research into reasoning-based forecasting\nparadigms and paves the way toward more interpretable and generalizable TSF\nframeworks."}
{"id": "2505.24513", "pdf": "https://arxiv.org/pdf/2505.24513", "abs": "https://arxiv.org/abs/2505.24513", "authors": ["Paritosh Ranjan", "Surajit Majumder", "Prodip Roy"], "title": "Airborne Neural Network", "categories": ["cs.LG", "cs.NE"], "comment": "11 pages, 3 figures", "summary": "Deep Learning, driven by neural networks, has led to groundbreaking\nadvancements in Artificial Intelligence by enabling systems to learn and adapt\nlike the human brain. These models have achieved remarkable results,\nparticularly in data-intensive domains, supported by massive computational\ninfrastructure. However, deploying such systems in Aerospace, where real time\ndata processing and ultra low latency are critical, remains a challenge due to\ninfrastructure limitations. This paper proposes a novel concept: the Airborne\nNeural Network a distributed architecture where multiple airborne devices each\nhost a subset of neural network neurons. These devices compute collaboratively,\nguided by an airborne network controller and layer specific controllers,\nenabling real-time learning and inference during flight. This approach has the\npotential to revolutionize Aerospace applications, including airborne air\ntraffic control, real-time weather and geographical predictions, and dynamic\ngeospatial data processing. By enabling large-scale neural network operations\nin airborne environments, this work lays the foundation for the next generation\nof AI powered Aerospace systems."}
{"id": "2505.24362", "pdf": "https://arxiv.org/pdf/2505.24362", "abs": "https://arxiv.org/abs/2505.24362", "authors": ["Anum Afzal", "Florian Matthes", "Gal Chechik", "Yftah Ziser"], "title": "Knowing Before Saying: LLM Representations Encode Information About Chain-of-Thought Success Before Completion", "categories": ["cs.CL"], "comment": null, "summary": "We investigate whether the success of a zero-shot Chain-of-Thought (CoT)\nprocess can be predicted before completion. We discover that a probing\nclassifier, based on LLM representations, performs well \\emph{even before a\nsingle token is generated}, suggesting that crucial information about the\nreasoning process is already present in the initial steps representations. In\ncontrast, a strong BERT-based baseline, which relies solely on the generated\ntokens, performs worse, likely because it depends on shallow linguistic cues\nrather than deeper reasoning dynamics. Surprisingly, using later reasoning\nsteps does not always improve classification. When additional context is\nunhelpful, earlier representations resemble later ones more, suggesting LLMs\nencode key information early. This implies reasoning can often stop early\nwithout loss. To test this, we conduct early stopping experiments, showing that\ntruncating CoT reasoning still improves performance over not using CoT at all,\nthough a gap remains compared to full reasoning. However, approaches like\nsupervised learning or reinforcement learning designed to shorten CoT chains\ncould leverage our classifier's guidance to identify when early stopping is\neffective. Our findings provide insights that may support such methods, helping\nto optimize CoT's efficiency while preserving its benefits.\\footnote{Code and\ndata is available at\n\\href{https://github.com/anum94/CoTpred}{\\texttt{github.com/anum94/CoTpred}}."}
{"id": "2505.24636", "pdf": "https://arxiv.org/pdf/2505.24636", "abs": "https://arxiv.org/abs/2505.24636", "authors": ["Marios Glytsos", "Panagiotis P. Filntisis", "George Retsinas", "Petros Maragos"], "title": "Category-Level 6D Object Pose Estimation in Agricultural Settings Using a Lattice-Deformation Framework and Diffusion-Augmented Synthetic Data", "categories": ["cs.CV"], "comment": "7 pages, 4 figures. Submitted to the IEEE/RSJ International\n  Conference on Intelligent Robots and Systems (IROS) 2025. This work has been\n  submitted to the IEEE for possible publication", "summary": "Accurate 6D object pose estimation is essential for robotic grasping and\nmanipulation, particularly in agriculture, where fruits and vegetables exhibit\nhigh intra-class variability in shape, size, and texture. The vast majority of\nexisting methods rely on instance-specific CAD models or require depth sensors\nto resolve geometric ambiguities, making them impractical for real-world\nagricultural applications. In this work, we introduce PLANTPose, a novel\nframework for category-level 6D pose estimation that operates purely on RGB\ninput. PLANTPose predicts both the 6D pose and deformation parameters relative\nto a base mesh, allowing a single category-level CAD model to adapt to unseen\ninstances. This enables accurate pose estimation across varying shapes without\nrelying on instance-specific data. To enhance realism and improve\ngeneralization, we also leverage Stable Diffusion to refine synthetic training\nimages with realistic texturing, mimicking variations due to ripeness and\nenvironmental factors and bridging the domain gap between synthetic data and\nthe real world. Our evaluations on a challenging benchmark that includes\nbananas of various shapes, sizes, and ripeness status demonstrate the\neffectiveness of our framework in handling large intraclass variations while\nmaintaining accurate 6D pose predictions, significantly outperforming the\nstate-of-the-art RGB-based approach MegaPose."}
{"id": "2505.24523", "pdf": "https://arxiv.org/pdf/2505.24523", "abs": "https://arxiv.org/abs/2505.24523", "authors": ["Andrea Pedrotti", "Michele Papucci", "Cristiano Ciaccio", "Alessio Miaschi", "Giovanni Puccetti", "Felice Dell'Orletta", "Andrea Esuli"], "title": "Stress-testing Machine Generated Text Detection: Shifting Language Models Writing Style to Fool Detectors", "categories": ["cs.CL", "cs.AI"], "comment": "Accepted at Findings of ACL 2025", "summary": "Recent advancements in Generative AI and Large Language Models (LLMs) have\nenabled the creation of highly realistic synthetic content, raising concerns\nabout the potential for malicious use, such as misinformation and manipulation.\nMoreover, detecting Machine-Generated Text (MGT) remains challenging due to the\nlack of robust benchmarks that assess generalization to real-world scenarios.\nIn this work, we present a pipeline to test the resilience of state-of-the-art\nMGT detectors (e.g., Mage, Radar, LLM-DetectAIve) to linguistically informed\nadversarial attacks. To challenge the detectors, we fine-tune language models\nusing Direct Preference Optimization (DPO) to shift the MGT style toward\nhuman-written text (HWT). This exploits the detectors' reliance on stylistic\nclues, making new generations more challenging to detect. Additionally, we\nanalyze the linguistic shifts induced by the alignment and which features are\nused by detectors to detect MGT texts. Our results show that detectors can be\neasily fooled with relatively few examples, resulting in a significant drop in\ndetection performance. This highlights the importance of improving detection\nmethods and making them robust to unseen in-domain texts."}
{"id": "2505.24531", "pdf": "https://arxiv.org/pdf/2505.24531", "abs": "https://arxiv.org/abs/2505.24531", "authors": ["Sagar Ghosh", "Kushal Bose", "Swagatam Das"], "title": "Transformers Are Universally Consistent", "categories": ["cs.LG"], "comment": null, "summary": "Despite their central role in the success of foundational models and\nlarge-scale language modeling, the theoretical foundations governing the\noperation of Transformers remain only partially understood. Contemporary\nresearch has largely focused on their representational capacity for language\ncomprehension and their prowess in in-context learning, frequently under\nidealized assumptions such as linearized attention mechanisms. Initially\nconceived to model sequence-to-sequence transformations, a fundamental and\nunresolved question is whether Transformers can robustly perform functional\nregression over sequences of input tokens. This question assumes heightened\nimportance given the inherently non-Euclidean geometry underlying real-world\ndata distributions. In this work, we establish that Transformers equipped with\nsoftmax-based nonlinear attention are uniformly consistent when tasked with\nexecuting Ordinary Least Squares (OLS) regression, provided both the inputs and\noutputs are embedded in hyperbolic space. We derive deterministic upper bounds\non the empirical error which, in the asymptotic regime, decay at a provable\nrate of $\\mathcal{O}(t^{-1/2d})$, where $t$ denotes the number of input tokens\nand $d$ the embedding dimensionality. Notably, our analysis subsumes the\nEuclidean setting as a special case, recovering analogous convergence\nguarantees parameterized by the intrinsic dimensionality of the data manifold.\nThese theoretical insights are corroborated through empirical evaluations on\nreal-world datasets involving both continuous and categorical response\nvariables."}
{"id": "2505.24377", "pdf": "https://arxiv.org/pdf/2505.24377", "abs": "https://arxiv.org/abs/2505.24377", "authors": ["Yu-Hsuan Lin", "Qian-Hui Chen", "Yi-Jie Cheng", "Jia-Ren Zhang", "Yi-Hung Liu", "Liang-Yu Hsia", "Yun-Nung Chen"], "title": "LLM Inference Enhanced by External Knowledge: A Survey", "categories": ["cs.CL"], "comment": null, "summary": "Recent advancements in large language models (LLMs) have enhanced\nnatural-language reasoning. However, their limited parametric memory and\nsusceptibility to hallucination present persistent challenges for tasks\nrequiring accurate, context-based inference. To overcome these limitations, an\nincreasing number of studies have proposed leveraging external knowledge to\nenhance LLMs. This study offers a systematic exploration of strategies for\nusing external knowledge to enhance LLMs, beginning with a taxonomy that\ncategorizes external knowledge into unstructured and structured data. We then\nfocus on structured knowledge, presenting distinct taxonomies for tables and\nknowledge graphs (KGs), detailing their integration paradigms with LLMs, and\nreviewing representative methods. Our comparative analysis further highlights\nthe trade-offs among interpretability, scalability, and performance, providing\ninsights for developing trustworthy and generalizable knowledge-enhanced LLMs."}
{"id": "2505.24638", "pdf": "https://arxiv.org/pdf/2505.24638", "abs": "https://arxiv.org/abs/2505.24638", "authors": ["Zahid Hassan Tushar", "Adeleke Ademakinwa", "Jianwu Wang", "Zhibo Zhang", "Sanjay Purushotham"], "title": "Cloud Optical Thickness Retrievals Using Angle Invariant Attention Based Deep Learning Models", "categories": ["cs.CV", "cs.AI"], "comment": "6 pages, 7 figures, to be published in 2025 IEEE International\n  Conference on Image Processing (ICIP)", "summary": "Cloud Optical Thickness (COT) is a critical cloud property influencing\nEarth's climate, weather, and radiation budget. Satellite radiance measurements\nenable global COT retrieval, but challenges like 3D cloud effects, viewing\nangles, and atmospheric interference must be addressed to ensure accurate\nestimation. Traditionally, the Independent Pixel Approximation (IPA) method,\nwhich treats individual pixels independently, has been used for COT estimation.\nHowever, IPA introduces significant bias due to its simplified assumptions.\nRecently, deep learning-based models have shown improved performance over IPA\nbut lack robustness, as they are sensitive to variations in radiance intensity,\ndistortions, and cloud shadows. These models also introduce substantial errors\nin COT estimation under different solar and viewing zenith angles. To address\nthese challenges, we propose a novel angle-invariant, attention-based deep\nmodel called Cloud-Attention-Net with Angle Coding (CAAC). Our model leverages\nattention mechanisms and angle embeddings to account for satellite viewing\ngeometry and 3D radiative transfer effects, enabling more accurate retrieval of\nCOT. Additionally, our multi-angle training strategy ensures angle invariance.\nThrough comprehensive experiments, we demonstrate that CAAC significantly\noutperforms existing state-of-the-art deep learning models, reducing cloud\nproperty retrieval errors by at least a factor of nine."}
{"id": "2505.24533", "pdf": "https://arxiv.org/pdf/2505.24533", "abs": "https://arxiv.org/abs/2505.24533", "authors": ["Mahesh Godavarti"], "title": "Directional Non-Commutative Monoidal Structures with Interchange Law via Commutative Generators", "categories": ["cs.LG", "cs.AI", "cs.SC", "20-XX, 08A02", "F.4.1; I.2"], "comment": null, "summary": "We introduce a novel framework consisting of a class of algebraic structures\nthat generalize one-dimensional monoidal systems into higher dimensions by\ndefining per-axis composition operators subject to non-commutativity and a\nglobal interchange law. These structures, defined recursively from a base case\nof vector-matrix pairs, model directional composition in multiple dimensions\nwhile preserving structural coherence through commutative linear operators.\n  We show that the framework that unifies several well-known linear transforms\nin signal processing and data analysis. In this framework, data indices are\nembedded into a composite structure that decomposes into simpler components. We\nshow that classic transforms such as the Discrete Fourier Transform (DFT), the\nWalsh transform, and the Hadamard transform are special cases of our algebraic\nstructure. The framework provides a systematic way to derive these transforms\nby appropriately choosing vector and matrix pairs. By subsuming classical\ntransforms within a common structure, the framework also enables the\ndevelopment of learnable transformations tailored to specific data modalities\nand tasks."}
{"id": "2505.24534", "pdf": "https://arxiv.org/pdf/2505.24534", "abs": "https://arxiv.org/abs/2505.24534", "authors": ["Florian Frantzen", "Michael T. Schaub"], "title": "HLSAD: Hodge Laplacian-based Simplicial Anomaly Detection", "categories": ["cs.LG", "cs.SI"], "comment": "Accepted for KDD 2025", "summary": "In this paper, we propose HLSAD, a novel method for detecting anomalies in\ntime-evolving simplicial complexes. While traditional graph anomaly detection\ntechniques have been extensively studied, they often fail to capture changes in\nhigher-order interactions that are crucial for identifying complex structural\nanomalies. These higher-order interactions can arise either directly from the\nunderlying data itself or through graph lifting techniques. Our approach\nleverages the spectral properties of Hodge Laplacians of simplicial complexes\nto effectively model multi-way interactions among data points. By incorporating\nhigher-dimensional simplicial structures into our method, our method enhances\nboth detection accuracy and computational efficiency. Through comprehensive\nexperiments on both synthetic and real-world datasets, we demonstrate that our\napproach outperforms existing graph methods in detecting both events and change\npoints."}
{"id": "2505.24388", "pdf": "https://arxiv.org/pdf/2505.24388", "abs": "https://arxiv.org/abs/2505.24388", "authors": ["Hao Chen", "Yukun Yan", "Sen Mei", "Wanxiang Che", "Zhenghao Liu", "Qi Shi", "Xinze Li", "Yuchun Fan", "Pengcheng Huang", "Qiushi Xiong", "Zhiyuan Liu", "Maosong Sun"], "title": "ClueAnchor: Clue-Anchored Knowledge Reasoning Exploration and Optimization for Retrieval-Augmented Generation", "categories": ["cs.CL"], "comment": null, "summary": "Retrieval-Augmented Generation (RAG) augments Large Language Models (LLMs)\nwith external knowledge to improve factuality. However, existing RAG systems\nfrequently underutilize the retrieved documents, failing to extract and\nintegrate the key clues needed to support faithful and interpretable reasoning,\nespecially in cases where relevant evidence is implicit, scattered, or obscured\nby noise. To address this issue, we propose ClueAnchor, a novel framework for\nenhancing RAG via clue-anchored reasoning exploration and optimization.\nClueAnchor extracts key clues from retrieved content and generates multiple\nreasoning paths based on different knowledge configurations, optimizing the\nmodel by selecting the most effective one through reward-based preference\noptimization. Experiments show that ClueAnchor significantly outperforms prior\nRAG baselines in reasoning completeness and robustness. Further analysis\nconfirms its strong resilience to noisy or partially relevant retrieved\ncontent, as well as its capability to identify supporting evidence even in the\nabsence of explicit clue supervision during inference."}
{"id": "2505.24641", "pdf": "https://arxiv.org/pdf/2505.24641", "abs": "https://arxiv.org/abs/2505.24641", "authors": ["Chengzhi Wu", "Qianliang Huang", "Kun Jin", "Julius Pfrommer", "JÃ¼rgen Beyerer"], "title": "A Cross Branch Fusion-Based Contrastive Learning Framework for Point Cloud Self-supervised Learning", "categories": ["cs.CV"], "comment": null, "summary": "Contrastive learning is an essential method in self-supervised learning. It\nprimarily employs a multi-branch strategy to compare latent representations\nobtained from different branches and train the encoder. In the case of\nmulti-modal input, diverse modalities of the same object are fed into distinct\nbranches. When using single-modal data, the same input undergoes various\naugmentations before being fed into different branches. However, all existing\ncontrastive learning frameworks have so far only performed contrastive\noperations on the learned features at the final loss end, with no information\nexchange between different branches prior to this stage. In this paper, for\npoint cloud unsupervised learning without the use of extra training data, we\npropose a Contrastive Cross-branch Attention-based framework for Point cloud\ndata (termed PoCCA), to learn rich 3D point cloud representations. By\nintroducing sub-branches, PoCCA allows information exchange between different\nbranches before the loss end. Experimental results demonstrate that in the case\nof using no extra training data, the representations learned with our\nself-supervised model achieve state-of-the-art performances when used for\ndownstream tasks on point clouds."}
{"id": "2505.24535", "pdf": "https://arxiv.org/pdf/2505.24535", "abs": "https://arxiv.org/abs/2505.24535", "authors": ["Narmeen Oozeer", "Luke Marks", "Fazl Barez", "Amirali Abdullah"], "title": "Beyond Linear Steering: Unified Multi-Attribute Control for Language Models", "categories": ["cs.LG", "cs.AI", "cs.CL"], "comment": null, "summary": "Controlling multiple behavioral attributes in large language models (LLMs) at\ninference time is a challenging problem due to interference between attributes\nand the limitations of linear steering methods, which assume additive behavior\nin activation space and require per-attribute tuning. We introduce K-Steering,\na unified and flexible approach that trains a single non-linear multi-label\nclassifier on hidden activations and computes intervention directions via\ngradients at inference time. This avoids linearity assumptions, removes the\nneed for storing and tuning separate attribute vectors, and allows dynamic\ncomposition of behaviors without retraining. To evaluate our method, we propose\ntwo new benchmarks, ToneBank and DebateMix, targeting compositional behavioral\ncontrol. Empirical results across 3 model families, validated by both\nactivation-based classifiers and LLM-based judges, demonstrate that K-Steering\noutperforms strong baselines in accurately steering multiple behaviors."}
{"id": "2505.24578", "pdf": "https://arxiv.org/pdf/2505.24578", "abs": "https://arxiv.org/abs/2505.24578", "authors": ["Abhishek Chandra", "Taniya Kapoor", "Mitrofan Curti", "Koen Tiels", "Elena A. Lomonova"], "title": "Neuro-Symbolic Operator for Interpretable and Generalizable Characterization of Complex Piezoelectric Systems", "categories": ["cs.LG"], "comment": null, "summary": "Complex piezoelectric systems are foundational in industrial applications.\nTheir performance, however, is challenged by the nonlinear voltage-displacement\nhysteretic relationships. Efficient characterization methods are, therefore,\nessential for reliable design, monitoring, and maintenance. Recently proposed\nneural operator methods serve as surrogates for system characterization but\nface two pressing issues: interpretability and generalizability.\nState-of-the-art (SOTA) neural operators are black-boxes, providing little\ninsight into the learned operator. Additionally, generalizing them to novel\nvoltages and predicting displacement profiles beyond the training domain is\nchallenging, limiting their practical use. To address these limitations, this\npaper proposes a neuro-symbolic operator (NSO) framework that derives the\nanalytical operators governing hysteretic relationships. NSO first learns a\nFourier neural operator mapping voltage fields to displacement profiles,\nfollowed by a library-based sparse model discovery method, generating white-box\nparsimonious models governing the underlying hysteresis. These models enable\naccurate and interpretable prediction of displacement profiles across varying\nand out-of-distribution voltage fields, facilitating generalizability. The\npotential of NSO is demonstrated by accurately predicting voltage-displacement\nhysteresis, including butterfly-shaped relationships. Moreover, NSO predicts\ndisplacement profiles even for noisy and low-fidelity voltage data, emphasizing\nits robustness. The results highlight the advantages of NSO compared to SOTA\nneural operators and model discovery methods on several evaluation metrics.\nConsequently, NSO contributes to characterizing complex piezoelectric systems\nwhile improving the interpretability and generalizability of neural operators,\nessential for design, monitoring, maintenance, and other real-world scenarios."}
{"id": "2505.24423", "pdf": "https://arxiv.org/pdf/2505.24423", "abs": "https://arxiv.org/abs/2505.24423", "authors": ["Zhiwei Liu", "Lingfei Qian", "Qianqian Xie", "Jimin Huang", "Kailai Yang", "Sophia Ananiadou"], "title": "MMAFFBen: A Multilingual and Multimodal Affective Analysis Benchmark for Evaluating LLMs and VLMs", "categories": ["cs.CL"], "comment": "Work in progress", "summary": "Large language models and vision-language models (which we jointly call LMs)\nhave transformed NLP and CV, demonstrating remarkable potential across various\nfields. However, their capabilities in affective analysis (i.e. sentiment\nanalysis and emotion detection) remain underexplored. This gap is largely due\nto the absence of comprehensive evaluation benchmarks, and the inherent\ncomplexity of affective analysis tasks. In this paper, we introduce MMAFFBen,\nthe first extensive open-source benchmark for multilingual multimodal affective\nanalysis. MMAFFBen encompasses text, image, and video modalities across 35\nlanguages, covering four key affective analysis tasks: sentiment polarity,\nsentiment intensity, emotion classification, and emotion intensity. Moreover,\nwe construct the MMAFFIn dataset for fine-tuning LMs on affective analysis\ntasks, and further develop MMAFFLM-3b and MMAFFLM-7b based on it. We evaluate\nvarious representative LMs, including GPT-4o-mini, providing a systematic\ncomparison of their affective understanding capabilities. This project is\navailable at https://github.com/lzw108/MMAFFBen."}
{"id": "2505.24649", "pdf": "https://arxiv.org/pdf/2505.24649", "abs": "https://arxiv.org/abs/2505.24649", "authors": ["Huu-Thien Tran", "Thanh-Dat Truong", "Khoa Luu"], "title": "BIMA: Bijective Maximum Likelihood Learning Approach to Hallucination Prediction and Mitigation in Large Vision-Language Models", "categories": ["cs.CV", "cs.AI"], "comment": "CVPRW 2025, 8 pages, 4 figures", "summary": "Large vision-language models have become widely adopted to advance in various\ndomains. However, developing a trustworthy system with minimal interpretable\ncharacteristics of large-scale models presents a significant challenge. One of\nthe most prevalent terms associated with the fallacy functions caused by these\nsystems is hallucination, where the language model generates a response that\ndoes not correspond to the visual content. To mitigate this problem, several\napproaches have been developed, and one prominent direction is to ameliorate\nthe decoding process. In this paper, we propose a new Bijective Maximum\nLikelihood Learning (BIMA) approach to hallucination mitigation using\nnormalizing flow theories. The proposed BIMA method can efficiently mitigate\nthe hallucination problem in prevailing vision-language models, resulting in\nsignificant improvements. Notably, BIMA achieves the average F1 score of 85.06%\non POPE benchmark and remarkably reduce CHAIRS and CHAIRI by 7.6% and 2.6%,\nrespectively. To the best of our knowledge, this is one of the first studies\nthat contemplates the bijection means to reduce hallucination induced by large\nvision-language models."}
{"id": "2505.24536", "pdf": "https://arxiv.org/pdf/2505.24536", "abs": "https://arxiv.org/abs/2505.24536", "authors": ["Chaohui Xu", "Qi Cui", "Chip-Hong Chang"], "title": "CHIP: Chameleon Hash-based Irreversible Passport for Robust Deep Model Ownership Verification and Active Usage Control", "categories": ["cs.CR", "cs.AI"], "comment": null, "summary": "The pervasion of large-scale Deep Neural Networks (DNNs) and their enormous\ntraining costs make their intellectual property (IP) protection of paramount\nimportance. Recently introduced passport-based methods attempt to steer DNN\nwatermarking towards strengthening ownership verification against ambiguity\nattacks by modulating the affine parameters of normalization layers.\nUnfortunately, neither watermarking nor passport-based methods provide a\nholistic protection with robust ownership proof, high fidelity, active usage\nauthorization and user traceability for offline access distributed models and\nmulti-user Machine-Learning as a Service (MLaaS) cloud model. In this paper, we\npropose a Chameleon Hash-based Irreversible Passport (CHIP) protection\nframework that utilizes the cryptographic chameleon hash function to achieve\nall these goals. The collision-resistant property of chameleon hash allows for\nstrong model ownership claim upon IP infringement and liable user traceability,\nwhile the trapdoor-collision property enables hashing of multiple user\npassports and licensee certificates to the same immutable signature to realize\nactive usage control. Using the owner passport as an oracle, multiple\nuser-specific triplets, each contains a passport-aware user model, a user\npassport, and a licensee certificate can be created for secure offline\ndistribution. The watermarked master model can also be deployed for MLaaS with\nusage permission verifiable by the provision of any trapdoor-colliding user\npassports. CHIP is extensively evaluated on four datasets and two architectures\nto demonstrate its protection versatility and robustness. Our code is released\nat https://github.com/Dshm212/CHIP."}
{"id": "2505.24579", "pdf": "https://arxiv.org/pdf/2505.24579", "abs": "https://arxiv.org/abs/2505.24579", "authors": ["Chaoyu Liu", "Yangming Li", "Zhongying Deng", "Chris Budd", "Carola-Bibiane SchÃ¶nlieb"], "title": "Conservation-preserved Fourier Neural Operator through Adaptive Correction", "categories": ["cs.LG"], "comment": null, "summary": "Fourier Neural Operators (FNOs) have recently emerged as a promising and\nefficient approach for learning the numerical solutions to partial differential\nequations (PDEs) from data. However, standard FNO often fails to preserve key\nconservation laws, such as mass conservation, momentum conservation, norm\nconservation, etc., which are crucial for accurately modeling physical systems.\nExisting methods for incorporating these conservation laws into Fourier neural\noperators are achieved by designing related loss function or incorporating\npost-processing method at the training time. None of them can both exactly and\nadaptively correct the outputs to satisfy conservation laws, and our\nexperiments show that these methods can lead to inferior performance while\npreserving conservation laws. In this work, we propose a novel adaptive\ncorrection approach to ensure the conservation of fundamental quantities. Our\nmethod introduces a learnable matrix to adaptively adjust the solution to\nsatisfy the conservation law during training. It ensures that the outputs\nexactly satisfy the goal conservation law and allow for more flexibility and\nadaptivity for the model to correct the outputs. We theoretically show that\napplying our adaptive correction to an unconstrained FNO yields a solution with\ndata loss no worse than that of the best conservation-satisfying FNO. We\ncompare our approach with existing methods on a range of representative PDEs.\nExperiment results show that our method consistently outperform other methods."}
{"id": "2505.24427", "pdf": "https://arxiv.org/pdf/2505.24427", "abs": "https://arxiv.org/abs/2505.24427", "authors": ["Christopher Bagdon", "Aidan Combs", "Carina Silberer", "Roman Klinger"], "title": "Donate or Create? Comparing Data Collection Strategies for Emotion-labeled Multimodal Social Media Posts", "categories": ["cs.CL"], "comment": "Published at ACL 2025", "summary": "Accurate modeling of subjective phenomena such as emotion expression requires\ndata annotated with authors' intentions. Commonly such data is collected by\nasking study participants to donate and label genuine content produced in the\nreal world, or create content fitting particular labels during the study.\nAsking participants to create content is often simpler to implement and\npresents fewer risks to participant privacy than data donation. However, it is\nunclear if and how study-created content may differ from genuine content, and\nhow differences may impact models. We collect study-created and genuine\nmultimodal social media posts labeled for emotion and compare them on several\ndimensions, including model performance. We find that compared to genuine\nposts, study-created posts are longer, rely more on their text and less on\ntheir images for emotion expression, and focus more on emotion-prototypical\nevents. The samples of participants willing to donate versus create posts are\ndemographically different. Study-created data is valuable to train models that\ngeneralize well to genuine data, but realistic effectiveness estimates require\ngenuine data."}
{"id": "2505.24667", "pdf": "https://arxiv.org/pdf/2505.24667", "abs": "https://arxiv.org/abs/2505.24667", "authors": ["Jiahe Chen", "Jiahe Ying", "Shen Wang", "Jianwei Zheng"], "title": "Decoupled Competitive Framework for Semi-supervised Medical Image Segmentation", "categories": ["cs.CV"], "comment": "Published in ECAI 2024", "summary": "Confronting the critical challenge of insufficiently annotated samples in\nmedical domain, semi-supervised medical image segmentation (SSMIS) emerges as a\npromising solution. Specifically, most methodologies following the Mean Teacher\n(MT) or Dual Students (DS) architecture have achieved commendable results.\nHowever, to date, these approaches face a performance bottleneck due to two\ninherent limitations, \\textit{e.g.}, the over-coupling problem within MT\nstructure owing to the employment of exponential moving average (EMA)\nmechanism, as well as the severe cognitive bias between two students of DS\nstructure, both of which potentially lead to reduced efficacy, or even model\ncollapse eventually. To mitigate these issues, a Decoupled Competitive\nFramework (DCF) is elaborated in this work, which utilizes a straightforward\ncompetition mechanism for the update of EMA, effectively decoupling students\nand teachers in a dynamical manner. In addition, the seamless exchange of\ninvaluable and precise insights is facilitated among students, guaranteeing a\nbetter learning paradigm. The DCF introduced undergoes rigorous validation on\nthree publicly accessible datasets, which encompass both 2D and 3D datasets.\nThe results demonstrate the superiority of our method over previous\ncutting-edge competitors. Code will be available at\nhttps://github.com/JiaheChen2002/DCF."}
{"id": "2505.24539", "pdf": "https://arxiv.org/pdf/2505.24539", "abs": "https://arxiv.org/abs/2505.24539", "authors": ["Celia Cintas", "Miriam Rateike", "Erik Miehling", "Elizabeth Daly", "Skyler Speakman"], "title": "Localizing Persona Representations in LLMs", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "We present a study on how and where personas -- defined by distinct sets of\nhuman characteristics, values, and beliefs -- are encoded in the representation\nspace of large language models (LLMs). Using a range of dimension reduction and\npattern recognition methods, we first identify the model layers that show the\ngreatest divergence in encoding these representations. We then analyze the\nactivations within a selected layer to examine how specific personas are\nencoded relative to others, including their shared and distinct embedding\nspaces. We find that, across multiple pre-trained decoder-only LLMs, the\nanalyzed personas show large differences in representation space only within\nthe final third of the decoder layers. We observe overlapping activations for\nspecific ethical perspectives -- such as moral nihilism and utilitarianism --\nsuggesting a degree of polysemy. In contrast, political ideologies like\nconservatism and liberalism appear to be represented in more distinct regions.\nThese findings help to improve our understanding of how LLMs internally\nrepresent information and can inform future efforts in refining the modulation\nof specific human traits in LLM outputs. Warning: This paper includes\npotentially offensive sample statements."}
{"id": "2505.24584", "pdf": "https://arxiv.org/pdf/2505.24584", "abs": "https://arxiv.org/abs/2505.24584", "authors": ["Sakhinana Sagar Srinivas", "Shivam Gupta", "Venkataramana Runkana"], "title": "AutoChemSchematic AI: A Closed-Loop, Physics-Aware Agentic Framework for Auto-Generating Chemical Process and Instrumentation Diagrams", "categories": ["cs.LG", "cs.AI", "cs.IR"], "comment": null, "summary": "Recent advancements in generative AI have accelerated the discovery of novel\nchemicals and materials; however, transitioning these discoveries to\nindustrial-scale production remains a critical bottleneck, as it requires the\ndevelopment of entirely new chemical manufacturing processes. Current AI\nmethods cannot auto-generate PFDs or PIDs, despite their critical role in\nscaling chemical processes, while adhering to engineering constraints. We\npresent a closed loop, physics aware framework for the automated generation of\nindustrially viable PFDs and PIDs. The framework integrates domain specialized\nsmall scale language models (SLMs) (trained for chemical process QA tasks) with\nfirst principles simulation, leveraging three key components: (1) a\nhierarchical knowledge graph of process flow and instrumentation descriptions\nfor 1,020+ chemicals, (2) a multi-stage training pipeline that fine tunes\ndomain specialized SLMs on synthetic datasets via Supervised Fine-Tuning (SFT),\nDirect Preference Optimization (DPO), and Retrieval-Augmented Instruction\nTuning (RAIT), and (3) DWSIM based simulator in the loop validation to ensure\nfeasibility. To improve both runtime efficiency and model compactness, the\nframework incorporates advanced inference time optimizations including\nFlashAttention, Lookahead Decoding, PagedAttention with KV-cache quantization,\nand Test Time Inference Scaling and independently applies structural pruning\ntechniques (width and depth) guided by importance heuristics to reduce model\nsize with minimal accuracy loss. Experiments demonstrate that the framework\ngenerates simulator-validated process descriptions with high fidelity,\noutperforms baseline methods in correctness, and generalizes to unseen\nchemicals. By bridging AI-driven design with industrial-scale feasibility, this\nwork significantly reduces R&D timelines from lab discovery to plant\ndeployment."}
{"id": "2505.24428", "pdf": "https://arxiv.org/pdf/2505.24428", "abs": "https://arxiv.org/abs/2505.24428", "authors": ["Xu Wang", "Zihao Li", "Benyou Wang", "Yan Hu", "Difan Zou"], "title": "Model Unlearning via Sparse Autoencoder Subspace Guided Projections", "categories": ["cs.CL", "cs.LG"], "comment": null, "summary": "Large language models (LLMs) store vast amounts of information, making them\npowerful yet raising privacy and safety concerns when selective knowledge\nremoval is required. Existing unlearning strategies, ranging from\ngradient-based fine-tuning and model editing to sparse autoencoder (SAE)\nsteering, either lack interpretability or fail to provide a robust defense\nagainst adversarial prompts. We propose SAE-Guided Subspace Projection\nUnlearning (SSPU), a novel framework that leverages SAE features to drive\ntargeted updates in the model's parameter space, enabling precise,\ninterpretable, and robust unlearning. SSPU's three-stage pipeline performs\ndata-driven layer and feature selection, subspace construction via QR\ndecomposition, and constrained optimization that controls activations into an\n\"irrelevant\" subspace while preserving retained knowledge. Overall, we use SAE\nfeatures to construct a subspace that supervises unlearning, refining the loss\nand adding a regularization term to guide interpretable parameter updates. In\nexperiments on the WMDP-Cyber forget set and three utility benchmarks (MMLU,\nTruthfulQA, GSM8K), SSPU reduces harmful knowledge accuracy by 3.22% compared\nto the strongest baseline. It also improves adversarial robustness, lowering\nmalicious accuracy under jailbreak prompts compared to baselines. Our findings\nexpose the limitations of prior unlearning methods and demonstrate how\ninterpretable subspace-guided optimization can achieve robust, controllable\nmodel behavior."}
{"id": "2505.24669", "pdf": "https://arxiv.org/pdf/2505.24669", "abs": "https://arxiv.org/abs/2505.24669", "authors": ["Chengzhi Wu", "Hao Fu", "Jan-Philipp Kaiser", "Erik Tabuchi Barczak", "Julius Pfrommer", "Gisela Lanza", "Michael Heizmann", "JÃ¼rgen Beyerer"], "title": "6D Pose Estimation on Point Cloud Data through Prior Knowledge Integration: A Case Study in Autonomous Disassembly", "categories": ["cs.CV"], "comment": null, "summary": "The accurate estimation of 6D pose remains a challenging task within the\ncomputer vision domain, even when utilizing 3D point cloud data. Conversely, in\nthe manufacturing domain, instances arise where leveraging prior knowledge can\nyield advancements in this endeavor. This study focuses on the disassembly of\nstarter motors to augment the engineering of product life cycles. A pivotal\nobjective in this context involves the identification and 6D pose estimation of\nbolts affixed to the motors, facilitating automated disassembly within the\nmanufacturing workflow. Complicating matters, the presence of occlusions and\nthe limitations of single-view data acquisition, notably when motors are placed\nin a clamping system, obscure certain portions and render some bolts\nimperceptible. Consequently, the development of a comprehensive pipeline\ncapable of acquiring complete bolt information is imperative to avoid oversight\nin bolt detection. In this paper, employing the task of bolt detection within\nthe scope of our project as a pertinent use case, we introduce a meticulously\ndevised pipeline. This multi-stage pipeline effectively captures the 6D\ninformation with regard to all bolts on the motor, thereby showcasing the\neffective utilization of prior knowledge in handling this challenging task. The\nproposed methodology not only contributes to the field of 6D pose estimation\nbut also underscores the viability of integrating domain-specific insights to\ntackle complex problems in manufacturing and automation."}
{"id": "2505.24544", "pdf": "https://arxiv.org/pdf/2505.24544", "abs": "https://arxiv.org/abs/2505.24544", "authors": ["Wei Zhong", "Manasa Bharadwaj", "Yixiao Wang", "Nikhil Verma", "Yipeng Ji", "Chul Lee"], "title": "Cross-Attention Speculative Decoding", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Speculative decoding (SD) is a widely adopted approach for accelerating\ninference in large language models (LLMs), particularly when the draft and\ntarget models are well aligned. However, state-of-the-art SD methods typically\nrely on tightly coupled, self-attention-based Transformer decoders, often\naugmented with auxiliary pooling or fusion layers. This coupling makes them\nincreasingly complex and harder to generalize across different models. We\npresent Budget EAGLE (Beagle), the first, to our knowledge,\ncross-attention-based Transformer decoder SD model that achieves performance on\npar with leading self-attention SD models (EAGLE-v2) while eliminating the need\nfor pooling or auxiliary components, simplifying the architecture, improving\ntraining efficiency, and maintaining stable memory usage during training-time\nsimulation. To enable effective training of this novel architecture, we propose\nTwo-Stage Block-Attention Training, a new method that achieves training\nstability and convergence efficiency in block-level attention scenarios.\nExtensive experiments across multiple LLMs and datasets show that Beagle\nachieves competitive inference speedups and higher training efficiency than\nEAGLE-v2, offering a strong alternative for architectures in speculative\ndecoding."}
{"id": "2505.24592", "pdf": "https://arxiv.org/pdf/2505.24592", "abs": "https://arxiv.org/abs/2505.24592", "authors": ["Weebum Yoo", "Sung Whan Yoon"], "title": "A Flat Minima Perspective on Understanding Augmentations and Model Robustness", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Model robustness indicates a model's capability to generalize well on\nunforeseen distributional shifts, including data corruption, adversarial\nattacks, and domain shifts. Data augmentation is one of the prevalent and\neffective ways to enhance robustness. Despite the great success of\naugmentations in different fields, a general theoretical understanding of their\nefficacy in improving model robustness is lacking. We offer a unified\ntheoretical framework to clarify how augmentations can enhance model robustness\nthrough the lens of loss surface flatness and PAC generalization bound. Our\nwork diverges from prior studies in that our analysis i) broadly encompasses\nmuch of the existing augmentation methods, and ii) is not limited to specific\ntypes of distribution shifts like adversarial attacks. We confirm our theories\nthrough simulations on the existing common corruption and adversarial\nrobustness benchmarks based on the CIFAR and ImageNet datasets, as well as\ndomain generalization benchmarks including PACS and OfficeHome."}
{"id": "2505.24448", "pdf": "https://arxiv.org/pdf/2505.24448", "abs": "https://arxiv.org/abs/2505.24448", "authors": ["Eojin Kang", "Jaehyuk Yu", "Juae Kim"], "title": "Exploring the Impact of Occupational Personas on Domain-Specific QA", "categories": ["cs.CL"], "comment": null, "summary": "Recent studies on personas have improved the way Large Language Models (LLMs)\ninteract with users. However, the effect of personas on domain-specific\nquestion-answering (QA) tasks remains a subject of debate. This study analyzes\nwhether personas enhance specialized QA performance by introducing two types of\npersona: Profession-Based Personas (PBPs) (e.g., scientist), which directly\nrelate to domain expertise, and Occupational Personality-Based Personas (OPBPs)\n(e.g., scientific person), which reflect cognitive tendencies rather than\nexplicit expertise. Through empirical evaluations across multiple scientific\ndomains, we demonstrate that while PBPs can slightly improve accuracy, OPBPs\noften degrade performance, even when semantically related to the task. Our\nfindings suggest that persona relevance alone does not guarantee effective\nknowledge utilization and that they may impose cognitive constraints that\nhinder optimal knowledge application. Future research can explore how nuanced\ndistinctions in persona representations guide LLMs, potentially contributing to\nreasoning and knowledge retrieval that more closely mirror human social\nconceptualization."}
{"id": "2505.24679", "pdf": "https://arxiv.org/pdf/2505.24679", "abs": "https://arxiv.org/abs/2505.24679", "authors": ["Evangelos Sariyanidi", "Lisa Yankowitz", "Robert T. Schultz", "John D. Herrington", "Birkan Tunc", "Jeffrey Cohn"], "title": "Beyond FACS: Data-driven Facial Expression Dictionaries, with Application to Predicting Autism", "categories": ["cs.CV"], "comment": "To appear in the Proceedings of the 19th IEEE International\n  Conference on Automatic Face and Gesture Recognition (2025)", "summary": "The Facial Action Coding System (FACS) has been used by numerous studies to\ninvestigate the links between facial behavior and mental health. The laborious\nand costly process of FACS coding has motivated the development of machine\nlearning frameworks for Action Unit (AU) detection. Despite intense efforts\nspanning three decades, the detection accuracy for many AUs is considered to be\nbelow the threshold needed for behavioral research. Also, many AUs are excluded\naltogether, making it impossible to fulfill the ultimate goal of FACS-the\nrepresentation of any facial expression in its entirety. This paper considers\nan alternative approach. Instead of creating automated tools that mimic FACS\nexperts, we propose to use a new coding system that mimics the key properties\nof FACS. Specifically, we construct a data-driven coding system called the\nFacial Basis, which contains units that correspond to localized and\ninterpretable 3D facial movements, and overcomes three structural limitations\nof automated FACS coding. First, the proposed method is completely\nunsupervised, bypassing costly, laborious and variable manual annotation.\nSecond, Facial Basis reconstructs all observable movement, rather than relying\non a limited repertoire of recognizable movements (as in automated FACS).\nFinally, the Facial Basis units are additive, whereas AUs may fail detection\nwhen they appear in a non-additive combination. The proposed method outperforms\nthe most frequently used AU detector in predicting autism diagnosis from\nin-person and remote conversations, highlighting the importance of encoding\nfacial behavior comprehensively. To our knowledge, Facial Basis is the first\nalternative to FACS for deconstructing facial expressions in videos into\nlocalized movements. We provide an open source implementation of the method at\ngithub.com/sariyanidi/FacialBasis."}
{"id": "2505.24553", "pdf": "https://arxiv.org/pdf/2505.24553", "abs": "https://arxiv.org/abs/2505.24553", "authors": ["Ye Eun Chun", "Taeyoon Hwang", "Seung-won Hwang", "Byung-Hak Kim"], "title": "CREFT: Sequential Multi-Agent LLM for Character Relation Extraction", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Understanding complex character relations is crucial for narrative analysis\nand efficient script evaluation, yet existing extraction methods often fail to\nhandle long-form narratives with nuanced interactions. To address this\nchallenge, we present CREFT, a novel sequential framework leveraging\nspecialized Large Language Model (LLM) agents. First, CREFT builds a base\ncharacter graph through knowledge distillation, then iteratively refines\ncharacter composition, relation extraction, role identification, and group\nassignments. Experiments on a curated Korean drama dataset demonstrate that\nCREFT significantly outperforms single-agent LLM baselines in both accuracy and\ncompleteness. By systematically visualizing character networks, CREFT\nstreamlines narrative comprehension and accelerates script review -- offering\nsubstantial benefits to the entertainment, publishing, and educational sectors."}
{"id": "2505.24595", "pdf": "https://arxiv.org/pdf/2505.24595", "abs": "https://arxiv.org/abs/2505.24595", "authors": ["Andrei Chernov", "Vitaliy Pozdnyakov", "Ilya Makarov"], "title": "Binary Cumulative Encoding meets Time Series Forecasting", "categories": ["cs.LG", "cs.AI", "stat.ML"], "comment": null, "summary": "Recent studies in time series forecasting have explored formulating\nregression via classification task. By discretizing the continuous target space\ninto bins and predicting over a fixed set of classes, these approaches benefit\nfrom stable training, robust uncertainty modeling, and compatibility with\nmodern deep learning architectures. However, most existing methods rely on\none-hot encoding that ignores the inherent ordinal structure of the underlying\nvalues. As a result, they fail to provide information about the relative\ndistance between predicted and true values during training. In this paper, we\npropose to address this limitation by introducing binary cumulative encoding\n(BCE), that represents scalar targets into monotonic binary vectors. This\nencoding implicitly preserves order and magnitude information, allowing the\nmodel to learn distance-aware representations while still operating within a\nclassification framework. We propose a convolutional neural network\narchitecture specifically designed for BCE, incorporating residual and dilated\nconvolutions to enable fast and expressive temporal modeling. Through extensive\nexperiments on benchmark forecasting datasets, we show that our approach\noutperforms widely used methods in both point and probabilistic forecasting,\nwhile requiring fewer parameters and enabling faster training."}
{"id": "2505.24449", "pdf": "https://arxiv.org/pdf/2505.24449", "abs": "https://arxiv.org/abs/2505.24449", "authors": ["Kailin Jiang", "Yuntao Du", "Yukai Ding", "Yuchen Ren", "Ning Jiang", "Zhi Gao", "Zilong Zheng", "Lei Liu", "Bin Li", "Qing Li"], "title": "When Large Multimodal Models Confront Evolving Knowledge:Challenges and Pathways", "categories": ["cs.CL"], "comment": null, "summary": "Large language/multimodal models (LLMs/LMMs) store extensive pre-trained\nknowledge but struggle to maintain consistency with real-world updates, making\nit difficult to avoid catastrophic forgetting while acquiring evolving\nknowledge. Previous work focused on constructing textual knowledge datasets and\nexploring knowledge injection in LLMs, lacking exploration of multimodal\nevolving knowledge injection in LMMs. To address this, we propose the EVOKE\nbenchmark to evaluate LMMs' ability to inject multimodal evolving knowledge in\nreal-world scenarios. Meanwhile, a comprehensive evaluation of multimodal\nevolving knowledge injection revealed two challenges: (1) Existing knowledge\ninjection methods perform terribly on evolving knowledge. (2) Supervised\nfine-tuning causes catastrophic forgetting, particularly instruction following\nability is severely compromised. Additionally, we provide pathways and find\nthat: (1) Text knowledge augmentation during the training phase improves\nperformance, while image augmentation cannot achieve it. (2) Continual learning\nmethods, especially Replay and MoELoRA, effectively mitigate forgetting. Our\nfindings indicate that current knowledge injection methods have many\nlimitations on evolving knowledge, which motivates further research on more\nefficient and stable knowledge injection methods."}
{"id": "2505.24690", "pdf": "https://arxiv.org/pdf/2505.24690", "abs": "https://arxiv.org/abs/2505.24690", "authors": ["Simone Alberto Peirone", "Francesca Pistilli", "Antonio Alliegro", "Tatiana Tommasi", "Giuseppe Averta"], "title": "Learning reusable concepts across different egocentric video understanding tasks", "categories": ["cs.CV"], "comment": "Extended abstract derived from arXiv:2502.02487. Presented at the\n  Second Joint Egocentric Vision (EgoVis) Workshop (CVPR 2025)", "summary": "Our comprehension of video streams depicting human activities is naturally\nmultifaceted: in just a few moments, we can grasp what is happening, identify\nthe relevance and interactions of objects in the scene, and forecast what will\nhappen soon, everything all at once. To endow autonomous systems with such\nholistic perception, learning how to correlate concepts, abstract knowledge\nacross diverse tasks, and leverage tasks synergies when learning novel skills\nis essential. In this paper, we introduce Hier-EgoPack, a unified framework\nable to create a collection of task perspectives that can be carried across\ndownstream tasks and used as a potential source of additional insights, as a\nbackpack of skills that a robot can carry around and use when needed."}
{"id": "2505.24554", "pdf": "https://arxiv.org/pdf/2505.24554", "abs": "https://arxiv.org/abs/2505.24554", "authors": ["Anna Sofia Lippolis", "Minh Davide Ragagni", "Paolo Ciancarini", "Andrea Giovanni Nuzzolese", "Valentina Presutti"], "title": "Bench4KE: Benchmarking Automated Competency Question Generation", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "The availability of Large Language Models (LLMs) presents a unique\nopportunity to reinvigorate research on Knowledge Engineering (KE) automation,\na trend already evident in recent efforts developing LLM-based methods and\ntools for the automatic generation of Competency Questions (CQs). However, the\nevaluation of these tools lacks standardisation. This undermines the\nmethodological rigour and hinders the replication and comparison of results. To\naddress this gap, we introduce Bench4KE, an extensible API-based benchmarking\nsystem for KE automation. Its first release focuses on evaluating tools that\ngenerate CQs automatically. CQs are natural language questions used by ontology\nengineers to define the functional requirements of an ontology. Bench4KE\nprovides a curated gold standard consisting of CQ datasets from four real-world\nontology projects. It uses a suite of similarity metrics to assess the quality\nof the CQs generated. We present a comparative analysis of four recent CQ\ngeneration systems, which are based on LLMs, establishing a baseline for future\nresearch. Bench4KE is also designed to accommodate additional KE automation\ntasks, such as SPARQL query generation, ontology testing and drafting. Code and\ndatasets are publicly available under the Apache 2.0 license."}
{"id": "2505.24603", "pdf": "https://arxiv.org/pdf/2505.24603", "abs": "https://arxiv.org/abs/2505.24603", "authors": ["Omri Lev", "Vishwak Srinivasan", "Moshe Shenfeld", "Katrina Ligett", "Ayush Sekhari", "Ashia C. Wilson"], "title": "The Gaussian Mixing Mechanism: Renyi Differential Privacy via Gaussian Sketches", "categories": ["cs.LG"], "comment": null, "summary": "Gaussian sketching, which consists of pre-multiplying the data with a random\nGaussian matrix, is a widely used technique for multiple problems in data\nscience and machine learning, with applications spanning computationally\nefficient optimization, coded computing, and federated learning. This operation\nalso provides differential privacy guarantees due to its inherent randomness.\nIn this work, we revisit this operation through the lens of Renyi Differential\nPrivacy (RDP), providing a refined privacy analysis that yields significantly\ntighter bounds than prior results. We then demonstrate how this improved\nanalysis leads to performance improvement in different linear regression\nsettings, establishing theoretical utility guarantees. Empirically, our methods\nimprove performance across multiple datasets and, in several cases, reduce\nruntime."}
{"id": "2505.24455", "pdf": "https://arxiv.org/pdf/2505.24455", "abs": "https://arxiv.org/abs/2505.24455", "authors": ["Cesar Gonzalez-Gutierrez", "Ariadna Quattoni"], "title": "Domain Pre-training Impact on Representations", "categories": ["cs.CL"], "comment": null, "summary": "This empirical study analyzes the effects of the pre-training corpus on the\nquality of learned transformer representations. We focus on the representation\nquality induced solely through pre-training. Our experiments show that\npre-training on a small, specialized corpus can yield effective\nrepresentations, and that the success of combining a generic and a specialized\ncorpus depends on the distributional similarity between the target task and the\nspecialized corpus."}
{"id": "2505.24693", "pdf": "https://arxiv.org/pdf/2505.24693", "abs": "https://arxiv.org/abs/2505.24693", "authors": ["Julio Silva-RodrÃ­guez", "Ismail Ben Ayed", "Jose Dolz"], "title": "Conformal Prediction for Zero-Shot Models", "categories": ["cs.CV"], "comment": "CVPR 2025. Code: https://github.com/jusiro/CLIP-Conformal", "summary": "Vision-language models pre-trained at large scale have shown unprecedented\nadaptability and generalization to downstream tasks. Although its\ndiscriminative potential has been widely explored, its reliability and\nuncertainty are still overlooked. In this work, we investigate the capabilities\nof CLIP models under the split conformal prediction paradigm, which provides\ntheoretical guarantees to black-box models based on a small, labeled\ncalibration set. In contrast to the main body of literature on conformal\npredictors in vision classifiers, foundation models exhibit a particular\ncharacteristic: they are pre-trained on a one-time basis on an inaccessible\nsource domain, different from the transferred task. This domain drift\nnegatively affects the efficiency of the conformal sets and poses additional\nchallenges. To alleviate this issue, we propose Conf-OT, a transfer learning\nsetting that operates transductive over the combined calibration and query\nsets. Solving an optimal transport problem, the proposed method bridges the\ndomain gap between pre-training and adaptation without requiring additional\ndata splits but still maintaining coverage guarantees. We comprehensively\nexplore this conformal prediction strategy on a broad span of 15 datasets and\nthree non-conformity scores. Conf-OT provides consistent relative improvements\nof up to 20% on set efficiency while being 15 times faster than popular\ntransductive approaches."}
{"id": "2505.24575", "pdf": "https://arxiv.org/pdf/2505.24575", "abs": "https://arxiv.org/abs/2505.24575", "authors": ["Hyuntak Kim", "Byung-Hak Kim"], "title": "NexusSum: Hierarchical LLM Agents for Long-Form Narrative Summarization", "categories": ["cs.CL", "cs.AI"], "comment": "Accepted to the main track of ACL 2025", "summary": "Summarizing long-form narratives--such as books, movies, and TV\nscripts--requires capturing intricate plotlines, character interactions, and\nthematic coherence, a task that remains challenging for existing LLMs. We\nintroduce NexusSum, a multi-agent LLM framework for narrative summarization\nthat processes long-form text through a structured, sequential\npipeline--without requiring fine-tuning. Our approach introduces two key\ninnovations: (1) Dialogue-to-Description Transformation: A narrative-specific\npreprocessing method that standardizes character dialogue and descriptive text\ninto a unified format, improving coherence. (2) Hierarchical Multi-LLM\nSummarization: A structured summarization pipeline that optimizes chunk\nprocessing and controls output length for accurate, high-quality summaries. Our\nmethod establishes a new state-of-the-art in narrative summarization, achieving\nup to a 30.0% improvement in BERTScore (F1) across books, movies, and TV\nscripts. These results demonstrate the effectiveness of multi-agent LLMs in\nhandling long-form content, offering a scalable approach for structured\nsummarization in diverse storytelling domains."}
{"id": "2505.24612", "pdf": "https://arxiv.org/pdf/2505.24612", "abs": "https://arxiv.org/abs/2505.24612", "authors": ["Sujoy Chatterjee", "Everton Romanzini Colombo", "Marcos Medeiros Raimundo"], "title": "Multi-criteria Rank-based Aggregation for Explainable AI", "categories": ["cs.LG"], "comment": "Accepted at the 2025 International Joint Conference on Neural\n  Networks (IJCNN)", "summary": "Explainability is crucial for improving the transparency of black-box machine\nlearning models. With the advancement of explanation methods such as LIME and\nSHAP, various XAI performance metrics have been developed to evaluate the\nquality of explanations. However, different explainers can provide contrasting\nexplanations for the same prediction, introducing trade-offs across conflicting\nquality metrics. Although available aggregation approaches improve robustness,\nreducing explanations' variability, very limited research employed a\nmulti-criteria decision-making approach. To address this gap, this paper\nintroduces a multi-criteria rank-based weighted aggregation method that\nbalances multiple quality metrics simultaneously to produce an ensemble of\nexplanation models. Furthermore, we propose rank-based versions of existing XAI\nmetrics (complexity, faithfulness and stability) to better evaluate ranked\nfeature importance explanations. Extensive experiments on publicly available\ndatasets demonstrate the robustness of the proposed model across these metrics.\nComparative analyses of various multi-criteria decision-making and rank\naggregation algorithms showed that TOPSIS and WSUM are the best candidates for\nthis use case."}
{"id": "2505.24456", "pdf": "https://arxiv.org/pdf/2505.24456", "abs": "https://arxiv.org/abs/2505.24456", "authors": ["Emilio Villa-Cueva", "Sholpan Bolatzhanova", "Diana Turmakhan", "Kareem Elzeky", "Henok Biadglign Ademtew", "Alham Fikri Aji", "Israel Abebe Azime", "Jinheon Baek", "Frederico Belcavello", "Fermin Cristobal", "Jan Christian Blaise Cruz", "Mary Dabre", "Raj Dabre", "Toqeer Ehsan", "Naome A Etori", "Fauzan Farooqui", "Jiahui Geng", "Guido Ivetta", "Thanmay Jayakumar", "Soyeong Jeong", "Zheng Wei Lim", "Aishik Mandal", "Sofia Martinelli", "Mihail Minkov Mihaylov", "Daniil Orel", "Aniket Pramanick", "Sukannya Purkayastha", "Israfel Salazar", "Haiyue Song", "Tiago Timponi Torrent", "Debela Desalegn Yadeta", "Injy Hamed", "Atnafu Lambebo Tonja", "Thamar Solorio"], "title": "CaMMT: Benchmarking Culturally Aware Multimodal Machine Translation", "categories": ["cs.CL"], "comment": null, "summary": "Cultural content poses challenges for machine translation systems due to the\ndifferences in conceptualizations between cultures, where language alone may\nfail to convey sufficient context to capture region-specific meanings. In this\nwork, we investigate whether images can act as cultural context in multimodal\ntranslation. We introduce CaMMT, a human-curated benchmark of over 5,800\ntriples of images along with parallel captions in English and regional\nlanguages. Using this dataset, we evaluate five Vision Language Models (VLMs)\nin text-only and text+image settings. Through automatic and human evaluations,\nwe find that visual context generally improves translation quality, especially\nin handling Culturally-Specific Items (CSIs), disambiguation, and correct\ngender usage. By releasing CaMMT, we aim to support broader efforts in building\nand evaluating multimodal translation systems that are better aligned with\ncultural nuance and regional variation."}
{"id": "2505.24705", "pdf": "https://arxiv.org/pdf/2505.24705", "abs": "https://arxiv.org/abs/2505.24705", "authors": ["Raman Jha", "Adithya Lenka", "Mani Ramanagopal", "Aswin Sankaranarayanan", "Kaushik Mitra"], "title": "RT-X Net: RGB-Thermal cross attention network for Low-Light Image Enhancement", "categories": ["cs.CV"], "comment": "Accepted at ICIP 2025", "summary": "In nighttime conditions, high noise levels and bright illumination sources\ndegrade image quality, making low-light image enhancement challenging. Thermal\nimages provide complementary information, offering richer textures and\nstructural details. We propose RT-X Net, a cross-attention network that fuses\nRGB and thermal images for nighttime image enhancement. We leverage\nself-attention networks for feature extraction and a cross-attention mechanism\nfor fusion to effectively integrate information from both modalities. To\nsupport research in this domain, we introduce the Visible-Thermal Image\nEnhancement Evaluation (V-TIEE) dataset, comprising 50 co-located visible and\nthermal images captured under diverse nighttime conditions. Extensive\nevaluations on the publicly available LLVIP dataset and our V-TIEE dataset\ndemonstrate that RT-X Net outperforms state-of-the-art methods in low-light\nimage enhancement. The code and the V-TIEE can be found here\nhttps://github.com/jhakrraman/rt-xnet."}
{"id": "2505.24593", "pdf": "https://arxiv.org/pdf/2505.24593", "abs": "https://arxiv.org/abs/2505.24593", "authors": ["Junzhuo Li", "Bo Wang", "Xiuze Zhou", "Peijie Jiang", "Jia Liu", "Xuming Hu"], "title": "Decoding Knowledge Attribution in Mixture-of-Experts: A Framework of Basic-Refinement Collaboration and Efficiency Analysis", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "ACL 2025", "summary": "The interpretability of Mixture-of-Experts (MoE) models, especially those\nwith heterogeneous designs, remains underexplored. Existing attribution methods\nfor dense models fail to capture dynamic routing-expert interactions in sparse\nMoE architectures. To address this issue, we propose a cross-level attribution\nalgorithm to analyze sparse MoE architectures (Qwen 1.5-MoE, OLMoE,\nMixtral-8x7B) against dense models (Qwen 1.5-7B, Llama-7B, Mixtral-7B). Results\nshow MoE models achieve 37% higher per-layer efficiency via a \"mid-activation,\nlate-amplification\" pattern: early layers screen experts, while late layers\nrefine knowledge collaboratively. Ablation studies reveal a \"basic-refinement\"\nframework--shared experts handle general tasks (entity recognition), while\nrouted experts specialize in domain-specific processing (geographic\nattributes). Semantic-driven routing is evidenced by strong correlations\nbetween attention heads and experts (r=0.68), enabling task-aware coordination.\nNotably, architectural depth dictates robustness: deep Qwen 1.5-MoE mitigates\nexpert failures (e.g., 43% MRR drop in geographic tasks when blocking top-10\nexperts) through shared expert redundancy, whereas shallow OLMoE suffers severe\ndegradation (76% drop). Task sensitivity further guides design: core-sensitive\ntasks (geography) require concentrated expertise, while distributed-tolerant\ntasks (object attributes) leverage broader participation. These insights\nadvance MoE interpretability, offering principles to balance efficiency,\nspecialization, and robustness."}
{"id": "2505.24623", "pdf": "https://arxiv.org/pdf/2505.24623", "abs": "https://arxiv.org/abs/2505.24623", "authors": ["Wenyuan Li", "Guang Li", "Keisuke Maeda", "Takahiro Ogawa", "Miki Haseyama"], "title": "Hyperbolic Dataset Distillation", "categories": ["cs.LG", "cs.AI", "cs.CV"], "comment": null, "summary": "To address the computational and storage challenges posed by large-scale\ndatasets in deep learning, dataset distillation has been proposed to synthesize\na compact dataset that replaces the original while maintaining comparable model\nperformance. Unlike optimization-based approaches that require costly bi-level\noptimization, distribution matching (DM) methods improve efficiency by aligning\nthe distributions of synthetic and original data, thereby eliminating nested\noptimization. DM achieves high computational efficiency and has emerged as a\npromising solution. However, existing DM methods, constrained to Euclidean\nspace, treat data as independent and identically distributed points,\noverlooking complex geometric and hierarchical relationships. To overcome this\nlimitation, we propose a novel hyperbolic dataset distillation method, termed\nHDD. Hyperbolic space, characterized by negative curvature and exponential\nvolume growth with distance, naturally models hierarchical and tree-like\nstructures. HDD embeds features extracted by a shallow network into the Lorentz\nhyperbolic space, where the discrepancy between synthetic and original data is\nmeasured by the hyperbolic (geodesic) distance between their centroids. By\noptimizing this distance, the hierarchical structure is explicitly integrated\ninto the distillation process, guiding synthetic samples to gravitate towards\nthe root-centric regions of the original data distribution while preserving\ntheir underlying geometric characteristics. Furthermore, we find that pruning\nin hyperbolic space requires only 20% of the distilled core set to retain model\nperformance, while significantly improving training stability. Notably, HDD is\nseamlessly compatible with most existing DM methods, and extensive experiments\non different datasets validate its effectiveness."}
{"id": "2505.24525", "pdf": "https://arxiv.org/pdf/2505.24525", "abs": "https://arxiv.org/abs/2505.24525", "authors": ["Marcell Fekete", "Nathaniel R. Robinson", "Ernests Lavrinovics", "E. Djeride Jean-Baptiste", "Raj Dabre", "Johannes Bjerva", "Heather Lent"], "title": "Limited-Resource Adapters Are Regularizers, Not Linguists", "categories": ["cs.CL"], "comment": null, "summary": "Cross-lingual transfer from related high-resource languages is a\nwell-established strategy to enhance low-resource language technologies. Prior\nwork has shown that adapters show promise for, e.g., improving low-resource\nmachine translation (MT). In this work, we investigate an adapter souping\nmethod combined with cross-attention fine-tuning of a pre-trained MT model to\nleverage language transfer for three low-resource Creole languages, which\nexhibit relatedness to different language groups across distinct linguistic\ndimensions. Our approach improves performance substantially over baselines.\nHowever, we find that linguistic relatedness -- or even a lack thereof -- does\nnot covary meaningfully with adapter performance. Surprisingly, our\ncross-attention fine-tuning approach appears equally effective with randomly\ninitialized adapters, implying that the benefit of adapters in this setting\nlies in parameter regularization, and not in meaningful information transfer.\nWe provide analysis supporting this regularization hypothesis. Our findings\nunderscore the reality that neural language processing involves many success\nfactors, and that not all neural methods leverage linguistic knowledge in\nintuitive ways."}
{"id": "2505.24718", "pdf": "https://arxiv.org/pdf/2505.24718", "abs": "https://arxiv.org/abs/2505.24718", "authors": ["Jisheng Dang", "Jingze Wu", "Teng Wang", "Xuanhui Lin", "Nannan Zhu", "Hongbo Chen", "Wei-Shi Zheng", "Meng Wang", "Tat-Seng Chua"], "title": "Reinforcing Video Reasoning with Focused Thinking", "categories": ["cs.CV"], "comment": null, "summary": "Recent advancements in reinforcement learning, particularly through Group\nRelative Policy Optimization (GRPO), have significantly improved multimodal\nlarge language models for complex reasoning tasks. However, two critical\nlimitations persist: 1) they often produce unfocused, verbose reasoning chains\nthat obscure salient spatiotemporal cues and 2) binary rewarding fails to\naccount for partially correct answers, resulting in high reward variance and\ninefficient learning. In this paper, we propose TW-GRPO, a novel framework that\nenhances visual reasoning with focused thinking and dense reward granularity.\nSpecifically, we employs a token weighting mechanism that prioritizes tokens\nwith high informational density (estimated by intra-group variance),\nsuppressing redundant tokens like generic reasoning prefixes. Furthermore, we\nreformulate RL training by shifting from single-choice to multi-choice QA\ntasks, where soft rewards enable finer-grained gradient estimation by\ndistinguishing partial correctness. Additionally, we propose question-answer\ninversion, a data augmentation strategy to generate diverse multi-choice\nsamples from existing benchmarks. Experiments demonstrate state-of-the-art\nperformance on several video reasoning and general understanding benchmarks.\nNotably, TW-GRPO achieves 50.4\\% accuracy on CLEVRER (18.8\\% improvement over\nVideo-R1) and 65.8\\% on MMVU. Our codes are available at\n\\href{https://github.com/longmalongma/TW-GRPO}{https://github.com/longmalongma/TW-GRPO}."}
{"id": "2505.24616", "pdf": "https://arxiv.org/pdf/2505.24616", "abs": "https://arxiv.org/abs/2505.24616", "authors": ["Nikita Martynov", "Anastasia Mordasheva", "Dmitriy Gorbetskiy", "Danil Astafurov", "Ulyana Isaeva", "Elina Basyrova", "Sergey Skachkov", "Victoria Berestova", "Nikolay Ivanov", "Valeriia Zanina", "Alena Fenogenova"], "title": "Eye of Judgement: Dissecting the Evaluation of Russian-speaking LLMs with POLLUX", "categories": ["cs.CL", "cs.AI"], "comment": "179 pages", "summary": "We introduce POLLUX, a comprehensive open-source benchmark designed to\nevaluate the generative capabilities of large language models (LLMs) in\nRussian. Our main contribution is a novel evaluation methodology that enhances\nthe interpretability of LLM assessment. For each task type, we define a set of\ndetailed criteria and develop a scoring protocol where models evaluate\nresponses and provide justifications for their ratings. This enables\ntransparent, criteria-driven evaluation beyond traditional resource-consuming,\nside-by-side human comparisons. POLLUX includes a detailed, fine-grained\ntaxonomy of 35 task types covering diverse generative domains such as code\ngeneration, creative writing, and practical assistant use cases, totaling 2,100\nmanually crafted and professionally authored prompts. Each task is categorized\nby difficulty (easy/medium/hard), with experts constructing the dataset\nentirely from scratch. We also release a family of LLM-as-a-Judge (7B and 32B)\nevaluators trained for nuanced assessment of generative outputs. This approach\nprovides scalable, interpretable evaluation and annotation tools for model\ndevelopment, effectively replacing costly and less precise human judgments."}
{"id": "2505.24627", "pdf": "https://arxiv.org/pdf/2505.24627", "abs": "https://arxiv.org/abs/2505.24627", "authors": ["Fu Luo", "Yaoxin Wu", "Zhi Zheng", "Zhenkun Wang"], "title": "Rethinking Neural Combinatorial Optimization for Vehicle Routing Problems with Different Constraint Tightness Degrees", "categories": ["cs.LG"], "comment": null, "summary": "Recent neural combinatorial optimization (NCO) methods have shown promising\nproblem-solving ability without requiring domain-specific expertise. Most\nexisting NCO methods use training and testing data with a fixed constraint\nvalue and lack research on the effect of constraint tightness on the\nperformance of NCO methods. This paper takes the capacity-constrained vehicle\nrouting problem (CVRP) as an example to empirically analyze the NCO performance\nunder different tightness degrees of the capacity constraint. Our analysis\nreveals that existing NCO methods overfit the capacity constraint, and they can\nonly perform satisfactorily on a small range of the constraint values but\npoorly on other values. To tackle this drawback of existing NCO methods, we\ndevelop an efficient training scheme that explicitly considers varying degrees\nof constraint tightness and proposes a multi-expert module to learn a generally\nadaptable solving strategy. Experimental results show that the proposed method\ncan effectively overcome the overfitting issue, demonstrating superior\nperformances on the CVRP and CVRP with time windows (CVRPTW) with various\nconstraint tightness degrees."}
{"id": "2505.24532", "pdf": "https://arxiv.org/pdf/2505.24532", "abs": "https://arxiv.org/abs/2505.24532", "authors": ["Ali Khoramfar", "Ali Ramezani", "Mohammad Mahdi Mohajeri", "Mohammad Javad Dousti", "Majid Nili Ahmadabadi", "Heshaam Faili"], "title": "DEEPQUESTION: Systematic Generation of Real-World Challenges for Evaluating LLMs Performance", "categories": ["cs.CL"], "comment": null, "summary": "LLMs often excel on standard benchmarks but falter on real-world tasks. We\nintroduce DeepQuestion, a scalable automated framework that augments existing\ndatasets based on Bloom's taxonomy and creates novel questions that trace\noriginal solution paths to probe evaluative and creative skills. Extensive\nexperiments across ten open-source and proprietary models, covering both\ngeneral-purpose and reasoning LLMs, reveal substantial performance drops (even\nup to 70% accuracy loss) on higher-order tasks, underscoring persistent gaps in\ndeep reasoning. Our work highlights the need for cognitively diverse benchmarks\nto advance LLM progress. DeepQuestion and related datasets will be released\nupon acceptance of the paper."}
{"id": "2505.24733", "pdf": "https://arxiv.org/pdf/2505.24733", "abs": "https://arxiv.org/abs/2505.24733", "authors": ["Jiaxu Zhang", "Xianfang Zeng", "Xin Chen", "Wei Zuo", "Gang Yu", "Guosheng Lin", "Zhigang Tu"], "title": "DreamDance: Animating Character Art via Inpainting Stable Gaussian Worlds", "categories": ["cs.CV"], "comment": null, "summary": "This paper presents DreamDance, a novel character art animation framework\ncapable of producing stable, consistent character and scene motion conditioned\non precise camera trajectories. To achieve this, we re-formulate the animation\ntask as two inpainting-based steps: Camera-aware Scene Inpainting and\nPose-aware Video Inpainting. The first step leverages a pre-trained image\ninpainting model to generate multi-view scene images from the reference art and\noptimizes a stable large-scale Gaussian field, which enables coarse background\nvideo rendering with camera trajectories. However, the rendered video is rough\nand only conveys scene motion. To resolve this, the second step trains a\npose-aware video inpainting model that injects the dynamic character into the\nscene video while enhancing background quality. Specifically, this model is a\nDiT-based video generation model with a gating strategy that adaptively\nintegrates the character's appearance and pose information into the base\nbackground video. Through extensive experiments, we demonstrate the\neffectiveness and generalizability of DreamDance, producing high-quality and\nconsistent character animations with remarkable camera dynamics."}
{"id": "2505.24630", "pdf": "https://arxiv.org/pdf/2505.24630", "abs": "https://arxiv.org/abs/2505.24630", "authors": ["Junyi Li", "Hwee Tou Ng"], "title": "The Hallucination Dilemma: Factuality-Aware Reinforcement Learning for Large Reasoning Models", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Large language models (LLMs) have significantly advanced in reasoning tasks\nthrough reinforcement learning (RL) optimization, achieving impressive\ncapabilities across various challenging benchmarks. However, our empirical\nanalysis reveals a critical drawback: reasoning-oriented RL fine-tuning\nsignificantly increases the prevalence of hallucinations. We theoretically\nanalyze the RL training dynamics, identifying high-variance gradient,\nentropy-induced randomness, and susceptibility to spurious local optima as key\nfactors leading to hallucinations. To address this drawback, we propose\nFactuality-aware Step-wise Policy Optimization (FSPO), an innovative RL\nfine-tuning algorithm incorporating explicit factuality verification at each\nreasoning step. FSPO leverages automated verification against given evidence to\ndynamically adjust token-level advantage values, incentivizing factual\ncorrectness throughout the reasoning process. Experiments across mathematical\nreasoning and hallucination benchmarks using Qwen2.5 and Llama models\ndemonstrate that FSPO effectively reduces hallucinations while enhancing\nreasoning accuracy, substantially improving both reliability and performance."}
{"id": "2505.24629", "pdf": "https://arxiv.org/pdf/2505.24629", "abs": "https://arxiv.org/abs/2505.24629", "authors": ["Lotte Bransen", "Tim Janssen", "Jesse Davis"], "title": "Stop Guessing: Optimizing Goalkeeper Policies for Soccer Penalty Kicks", "categories": ["cs.LG", "cs.GT"], "comment": "24 pages, 7 figures", "summary": "Penalties are fraught and game-changing moments in soccer games that teams\nexplicitly prepare for. Consequently, there has been substantial interest in\nanalyzing them in order to provide advice to practitioners. From a data science\nperspective, such analyses suffer from a significant limitation: they make the\nunrealistic simplifying assumption that goalkeepers and takers select their\naction -- where to dive and where to the place the kick -- independently of\neach other. In reality, the choices that some goalkeepers make depend on the\ntaker's movements and vice-versa. This adds substantial complexity to the\nproblem because not all players have the same action capacities, that is, only\nsome players are capable of basing their decisions on their opponent's\nmovements. However, the small sample sizes on the player level mean that one\nmay have limited insights into a specific opponent's capacities. We address\nthese challenges by developing a player-agnostic simulation framework that can\nevaluate the efficacy of different goalkeeper strategies. It considers a rich\nset of choices and incorporates information about a goalkeeper's skills. Our\nwork is grounded in a large dataset of penalties that were annotated by penalty\nexperts and include aspects of both kicker and goalkeeper strategies. We show\nhow our framework can be used to optimize goalkeeper policies in real-world\nsituations."}
{"id": "2505.24538", "pdf": "https://arxiv.org/pdf/2505.24538", "abs": "https://arxiv.org/abs/2505.24538", "authors": ["Orfeas Menis Mastromichalakis", "Jason Liartis", "Kristina Rose", "Antoine Isaac", "Giorgos Stamou"], "title": "Don't Erase, Inform! Detecting and Contextualizing Harmful Language in Cultural Heritage Collections", "categories": ["cs.CL"], "comment": null, "summary": "Cultural Heritage (CH) data hold invaluable knowledge, reflecting the\nhistory, traditions, and identities of societies, and shaping our understanding\nof the past and present. However, many CH collections contain outdated or\noffensive descriptions that reflect historical biases. CH Institutions (CHIs)\nface significant challenges in curating these data due to the vast scale and\ncomplexity of the task. To address this, we develop an AI-powered tool that\ndetects offensive terms in CH metadata and provides contextual insights into\ntheir historical background and contemporary perception. We leverage a\nmultilingual vocabulary co-created with marginalized communities, researchers,\nand CH professionals, along with traditional NLP techniques and Large Language\nModels (LLMs). Available as a standalone web app and integrated with major CH\nplatforms, the tool has processed over 7.9 million records, contextualizing the\ncontentious terms detected in their metadata. Rather than erasing these terms,\nour approach seeks to inform, making biases visible and providing actionable\ninsights for creating more inclusive and accessible CH collections."}
{"id": "2505.24746", "pdf": "https://arxiv.org/pdf/2505.24746", "abs": "https://arxiv.org/abs/2505.24746", "authors": ["Jiazhong Cen", "Xudong Zhou", "Jiemin Fang", "Changsong Wen", "Lingxi Xie", "Xiaopeng Zhang", "Wei Shen", "Qi Tian"], "title": "Tackling View-Dependent Semantics in 3D Language Gaussian Splatting", "categories": ["cs.CV"], "comment": "ICML 2025 camera ready. Project Page:\n  https://jumpat.github.io/laga-page/", "summary": "Recent advancements in 3D Gaussian Splatting (3D-GS) enable high-quality 3D\nscene reconstruction from RGB images. Many studies extend this paradigm for\nlanguage-driven open-vocabulary scene understanding. However, most of them\nsimply project 2D semantic features onto 3D Gaussians and overlook a\nfundamental gap between 2D and 3D understanding: a 3D object may exhibit\nvarious semantics from different viewpoints--a phenomenon we term\nview-dependent semantics. To address this challenge, we propose LaGa (Language\nGaussians), which establishes cross-view semantic connections by decomposing\nthe 3D scene into objects. Then, it constructs view-aggregated semantic\nrepresentations by clustering semantic descriptors and reweighting them based\non multi-view semantics. Extensive experiments demonstrate that LaGa\neffectively captures key information from view-dependent semantics, enabling a\nmore comprehensive understanding of 3D scenes. Notably, under the same\nsettings, LaGa achieves a significant improvement of +18.7% mIoU over the\nprevious SOTA on the LERF-OVS dataset. Our code is available at:\nhttps://github.com/SJTU-DeepVisionLab/LaGa."}
{"id": "2505.24640", "pdf": "https://arxiv.org/pdf/2505.24640", "abs": "https://arxiv.org/abs/2505.24640", "authors": ["Jens-Joris Decorte", "Jeroen Van Hautte", "Chris Develder", "Thomas Demeester"], "title": "Efficient Text Encoders for Labor Market Analysis", "categories": ["cs.CL", "cs.AI"], "comment": "This work has been submitted to the IEEE for possible publication", "summary": "Labor market analysis relies on extracting insights from job advertisements,\nwhich provide valuable yet unstructured information on job titles and\ncorresponding skill requirements. While state-of-the-art methods for skill\nextraction achieve strong performance, they depend on large language models\n(LLMs), which are computationally expensive and slow. In this paper, we propose\n\\textbf{ConTeXT-match}, a novel contrastive learning approach with token-level\nattention that is well-suited for the extreme multi-label classification task\nof skill classification. \\textbf{ConTeXT-match} significantly improves skill\nextraction efficiency and performance, achieving state-of-the-art results with\na lightweight bi-encoder model. To support robust evaluation, we introduce\n\\textbf{Skill-XL}, a new benchmark with exhaustive, sentence-level skill\nannotations that explicitly address the redundancy in the large label space.\nFinally, we present \\textbf{JobBERT V2}, an improved job title normalization\nmodel that leverages extracted skills to produce high-quality job title\nrepresentations. Experiments demonstrate that our models are efficient,\naccurate, and scalable, making them ideal for large-scale, real-time labor\nmarket analysis."}
{"id": "2505.24642", "pdf": "https://arxiv.org/pdf/2505.24642", "abs": "https://arxiv.org/abs/2505.24642", "authors": ["Masahiro Negishi", "Thomas GÃ¤rtner", "Pascal Welke"], "title": "WILTing Trees: Interpreting the Distance Between MPNN Embeddings", "categories": ["cs.LG", "I.2.6"], "comment": "25 pages, 10 figures. Accepted to ICML 2025. See\n  https://github.com/masahiro-negishi/wilt for code", "summary": "We investigate the distance function learned by message passing neural\nnetworks (MPNNs) in specific tasks, aiming to capture the functional distance\nbetween prediction targets that MPNNs implicitly learn. This contrasts with\nprevious work, which links MPNN distances on arbitrary tasks to structural\ndistances on graphs that ignore task-specific information. To address this gap,\nwe distill the distance between MPNN embeddings into an interpretable graph\ndistance. Our method uses optimal transport on the Weisfeiler Leman Labeling\nTree (WILT), where the edge weights reveal subgraphs that strongly influence\nthe distance between embeddings. This approach generalizes two well-known graph\nkernels and can be computed in linear time. Through extensive experiments, we\ndemonstrate that MPNNs define the relative position of embeddings by focusing\non a small set of subgraphs that are known to be functionally important in the\ndomain."}
{"id": "2505.24550", "pdf": "https://arxiv.org/pdf/2505.24550", "abs": "https://arxiv.org/abs/2505.24550", "authors": ["Xiaoang Xu", "Shuo Wang", "Xu Han", "Zhenghao Liu", "Huijia Wu", "Peipei Li", "Zhiyuan Liu", "Maosong Sun", "Zhaofeng He"], "title": "A*-Thought: Efficient Reasoning via Bidirectional Compression for Low-Resource Settings", "categories": ["cs.CL"], "comment": null, "summary": "Large Reasoning Models (LRMs) achieve superior performance by extending the\nthought length. However, a lengthy thinking trajectory leads to reduced\nefficiency. Most of the existing methods are stuck in the assumption of\noverthinking and attempt to reason efficiently by compressing the\nChain-of-Thought, but this often leads to performance degradation. To address\nthis problem, we introduce A*-Thought, an efficient tree search-based unified\nframework designed to identify and isolate the most essential thoughts from the\nextensive reasoning chains produced by these models. It formulates the\nreasoning process of LRMs as a search tree, where each node represents a\nreasoning span in the giant reasoning space. By combining the A* search\nalgorithm with a cost function specific to the reasoning path, it can\nefficiently compress the chain of thought and determine a reasoning path with\nhigh information density and low cost. In addition, we also propose a\nbidirectional importance estimation mechanism, which further refines this\nsearch process and enhances its efficiency beyond uniform sampling. Extensive\nexperiments on several advanced math tasks show that A*-Thought effectively\nbalances performance and efficiency over a huge search space. Specifically,\nA*-Thought can improve the performance of QwQ-32B by 2.39$\\times$ with\nlow-budget and reduce the length of the output token by nearly 50% with\nhigh-budget. The proposed method is also compatible with several other LRMs,\ndemonstrating its generalization capability. The code can be accessed at:\nhttps://github.com/AI9Stars/AStar-Thought."}
{"id": "2505.24787", "pdf": "https://arxiv.org/pdf/2505.24787", "abs": "https://arxiv.org/abs/2505.24787", "authors": ["Yucheng Zhou", "Jiahao Yuan", "Qianning Wang"], "title": "Draw ALL Your Imagine: A Holistic Benchmark and Agent Framework for Complex Instruction-based Image Generation", "categories": ["cs.CV", "cs.CL"], "comment": null, "summary": "Recent advancements in text-to-image (T2I) generation have enabled models to\nproduce high-quality images from textual descriptions. However, these models\noften struggle with complex instructions involving multiple objects,\nattributes, and spatial relationships. Existing benchmarks for evaluating T2I\nmodels primarily focus on general text-image alignment and fail to capture the\nnuanced requirements of complex, multi-faceted prompts. Given this gap, we\nintroduce LongBench-T2I, a comprehensive benchmark specifically designed to\nevaluate T2I models under complex instructions. LongBench-T2I consists of 500\nintricately designed prompts spanning nine diverse visual evaluation\ndimensions, enabling a thorough assessment of a model's ability to follow\ncomplex instructions. Beyond benchmarking, we propose an agent framework\n(Plan2Gen) that facilitates complex instruction-driven image generation without\nrequiring additional model training. This framework integrates seamlessly with\nexisting T2I models, using large language models to interpret and decompose\ncomplex prompts, thereby guiding the generation process more effectively. As\nexisting evaluation metrics, such as CLIPScore, fail to adequately capture the\nnuances of complex instructions, we introduce an evaluation toolkit that\nautomates the quality assessment of generated images using a set of\nmulti-dimensional metrics. The data and code are released at\nhttps://github.com/yczhou001/LongBench-T2I."}
{"id": "2505.24671", "pdf": "https://arxiv.org/pdf/2505.24671", "abs": "https://arxiv.org/abs/2505.24671", "authors": ["Dayeon Ki", "Rachel Rudinger", "Tianyi Zhou", "Marine Carpuat"], "title": "Multiple LLM Agents Debate for Equitable Cultural Alignment", "categories": ["cs.CL", "cs.AI"], "comment": "37 pages, 18 figures", "summary": "Large Language Models (LLMs) need to adapt their predictions to diverse\ncultural contexts to benefit diverse communities across the world. While\nprevious efforts have focused on single-LLM, single-turn approaches, we propose\nto exploit the complementary strengths of multiple LLMs to promote cultural\nadaptability. We introduce a Multi-Agent Debate framework, where two LLM-based\nagents debate over a cultural scenario and collaboratively reach a final\ndecision. We propose two variants: one where either LLM agents exclusively\ndebate and another where they dynamically choose between self-reflection and\ndebate during their turns. We evaluate these approaches on 7 open-weight LLMs\n(and 21 LLM combinations) using the NormAd-ETI benchmark for social etiquette\nnorms in 75 countries. Experiments show that debate improves both overall\naccuracy and cultural group parity over single-LLM baselines. Notably,\nmulti-agent debate enables relatively small LLMs (7-9B) to achieve accuracies\ncomparable to that of a much larger model (27B parameters)."}
{"id": "2505.24664", "pdf": "https://arxiv.org/pdf/2505.24664", "abs": "https://arxiv.org/abs/2505.24664", "authors": ["Daniel Severo", "Brian Karrer", "Niklas Nolte"], "title": "Learning Distributions over Permutations and Rankings with Factorized Representations", "categories": ["cs.LG"], "comment": null, "summary": "Learning distributions over permutations is a fundamental problem in machine\nlearning, with applications in ranking, combinatorial optimization, structured\nprediction, and data association. Existing methods rely on mixtures of\nparametric families or neural networks with expensive variational inference\nprocedures. In this work, we propose a novel approach that leverages\nalternative representations for permutations, including Lehmer codes,\nFisher-Yates draws, and Insertion-Vectors. These representations form a\nbijection with the symmetric group, allowing for unconstrained learning using\nconventional deep learning techniques, and can represent any probability\ndistribution over permutations. Our approach enables a trade-off between\nexpressivity of the model family and computational requirements. In the least\nexpressive and most computationally efficient case, our method subsumes\nprevious families of well established probabilistic models over permutations,\nincluding Mallow's and the Repeated Insertion Model. Experiments indicate our\nmethod significantly outperforms current approaches on the jigsaw puzzle\nbenchmark, a common task for permutation learning. However, we argue this\nbenchmark is limited in its ability to assess learning probability\ndistributions, as the target is a delta distribution (i.e., a single correct\nsolution exists). We therefore propose two additional benchmarks: learning\ncyclic permutations and re-ranking movies based on user preference. We show\nthat our method learns non-trivial distributions even in the least expressive\nmode, while traditional models fail to even generate valid permutations in this\nsetting."}
{"id": "2505.24561", "pdf": "https://arxiv.org/pdf/2505.24561", "abs": "https://arxiv.org/abs/2505.24561", "authors": ["Ioannis Tsiamas", "David Dale", "Marta R. Costa-jussÃ "], "title": "Improving Language and Modality Transfer in Translation by Character-level Modeling", "categories": ["cs.CL"], "comment": "ACL 2025", "summary": "Current translation systems, despite being highly multilingual, cover only 5%\nof the world's languages. Expanding language coverage to the long-tail of\nlow-resource languages requires data-efficient methods that rely on\ncross-lingual and cross-modal knowledge transfer. To this end, we propose a\ncharacter-based approach to improve adaptability to new languages and\nmodalities. Our method leverages SONAR, a multilingual fixed-size embedding\nspace with different modules for encoding and decoding. We use a\nteacher-student approach with parallel translation data to obtain a\ncharacter-level encoder. Then, using ASR data, we train a lightweight adapter\nto connect a massively multilingual CTC ASR model (MMS), to the character-level\nencoder, potentially enabling speech translation from 1,000+ languages.\nExperimental results in text translation for 75 languages on FLORES+\ndemonstrate that our character-based approach can achieve better language\ntransfer than traditional subword-based models, especially outperforming them\nin low-resource settings, and demonstrating better zero-shot generalizability\nto unseen languages. Our speech adaptation, maximizing knowledge transfer from\nthe text modality, achieves state-of-the-art results in speech-to-text\ntranslation on the FLEURS benchmark on 33 languages, surpassing previous\nsupervised and cascade models, albeit being a zero-shot model with minimal\nsupervision from ASR data."}
{"id": "2505.24792", "pdf": "https://arxiv.org/pdf/2505.24792", "abs": "https://arxiv.org/abs/2505.24792", "authors": ["Xinliu Zhong", "Leo Hwa Liang", "Angela S. Koh", "Yeo Si Yong"], "title": "Lightweight Relational Embedding in Task-Interpolated Few-Shot Networks for Enhanced Gastrointestinal Disease Classification", "categories": ["cs.CV"], "comment": "6 pages, 15 figures", "summary": "Traditional diagnostic methods like colonoscopy are invasive yet critical\ntools necessary for accurately diagnosing colorectal cancer (CRC). Detection of\nCRC at early stages is crucial for increasing patient survival rates. However,\ncolonoscopy is dependent on obtaining adequate and high-quality endoscopic\nimages. Prolonged invasive procedures are inherently risky for patients, while\nsuboptimal or insufficient images hamper diagnostic accuracy. These images,\ntypically derived from video frames, often exhibit similar patterns, posing\nchallenges in discrimination. To overcome these challenges, we propose a novel\nDeep Learning network built on a Few-Shot Learning architecture, which includes\na tailored feature extractor, task interpolation, relational embedding, and a\nbi-level routing attention mechanism. The Few-Shot Learning paradigm enables\nour model to rapidly adapt to unseen fine-grained endoscopic image patterns,\nand the task interpolation augments the insufficient images artificially from\nvaried instrument viewpoints. Our relational embedding approach discerns\ncritical intra-image features and captures inter-image transitions between\nconsecutive endoscopic frames, overcoming the limitations of Convolutional\nNeural Networks (CNNs). The integration of a light-weight attention mechanism\nensures a concentrated analysis of pertinent image regions. By training on\ndiverse datasets, the model's generalizability and robustness are notably\nimproved for handling endoscopic images. Evaluated on Kvasir dataset, our model\ndemonstrated superior performance, achieving an accuracy of 90.1\\%, precision\nof 0.845, recall of 0.942, and an F1 score of 0.891. This surpasses current\nstate-of-the-art methods, presenting a promising solution to the challenges of\ninvasive colonoscopy by optimizing CRC detection through advanced image\nanalysis."}
{"id": "2505.24681", "pdf": "https://arxiv.org/pdf/2505.24681", "abs": "https://arxiv.org/abs/2505.24681", "authors": ["Katalin Feher", "Marton Demeter"], "title": "Generative Knowledge Production Pipeline Driven by Academic Influencers", "categories": ["cs.CY", "cs.AI", "cs.HC", "cs.SI", "1.2, J.4, K.4"], "comment": "15 pages, 1 figure, 2 tables, Horizon Europe NGI funding", "summary": "Generative AI transforms knowledge production, validation, and dissemination,\nraising academic integrity and credibility concerns. This study examines 53\nacademic influencer videos that reached 5.3 million viewers to identify an\nemerging, structured, implementation-ready pipeline balancing originality,\nethical compliance, and human-AI collaboration despite the disruptive impacts.\nFindings highlight generative AI's potential to automate publication workflows\nand democratize participation in knowledge production while challenging\ntraditional scientific norms. Academic influencers emerge as key intermediaries\nin this paradigm shift, connecting bottom-up practices with institutional\npolicies to improve adaptability. Accordingly, the study proposes a generative\npublication production pipeline and a policy framework for co-intelligence\nadaptation and reinforcing credibility-centered standards in AI-powered\nresearch. These insights support scholars, educators, and policymakers in\nunderstanding AI's transformative impact by advocating responsible and\ninnovation-driven knowledge production. Additionally, they reveal pathways for\nautomating best practices, optimizing scholarly workflows, and fostering\ncreativity in academic research and publication."}
{"id": "2505.24665", "pdf": "https://arxiv.org/pdf/2505.24665", "abs": "https://arxiv.org/abs/2505.24665", "authors": ["Hanlin Yu", "SÃ¸ren Hauberg", "Marcelo Hartmann", "Arto Klami", "Georgios Arvanitidis"], "title": "Learning geometry and topology via multi-chart flows", "categories": ["cs.LG"], "comment": null, "summary": "Real world data often lie on low-dimensional Riemannian manifolds embedded in\nhigh-dimensional spaces. This motivates learning degenerate normalizing flows\nthat map between the ambient space and a low-dimensional latent space. However,\nif the manifold has a non-trivial topology, it can never be correctly learned\nusing a single flow. Instead multiple flows must be `glued together'. In this\npaper, we first propose the general training scheme for learning such a\ncollection of flows, and secondly we develop the first numerical algorithms for\ncomputing geodesics on such manifolds. Empirically, we demonstrate that this\nleads to highly significant improvements in topology estimation."}
{"id": "2505.24581", "pdf": "https://arxiv.org/pdf/2505.24581", "abs": "https://arxiv.org/abs/2505.24581", "authors": ["Omer Nacar", "Anis Koubaa", "Serry Sibaee", "Yasser Al-Habashi", "Adel Ammar", "Wadii Boulila"], "title": "GATE: General Arabic Text Embedding for Enhanced Semantic Textual Similarity with Matryoshka Representation Learning and Hybrid Loss Training", "categories": ["cs.CL"], "comment": null, "summary": "Semantic textual similarity (STS) is a critical task in natural language\nprocessing (NLP), enabling applications in retrieval, clustering, and\nunderstanding semantic relationships between texts. However, research in this\narea for the Arabic language remains limited due to the lack of high-quality\ndatasets and pre-trained models. This scarcity of resources has restricted the\naccurate evaluation and advance of semantic similarity in Arabic text. This\npaper introduces General Arabic Text Embedding (GATE) models that achieve\nstate-of-the-art performance on the Semantic Textual Similarity task within the\nMTEB benchmark. GATE leverages Matryoshka Representation Learning and a hybrid\nloss training approach with Arabic triplet datasets for Natural Language\nInference, which are essential for enhancing model performance in tasks that\ndemand fine-grained semantic understanding. GATE outperforms larger models,\nincluding OpenAI, with a 20-25% performance improvement on STS benchmarks,\neffectively capturing the unique semantic nuances of Arabic."}
{"id": "2505.24816", "pdf": "https://arxiv.org/pdf/2505.24816", "abs": "https://arxiv.org/abs/2505.24816", "authors": ["Jiangpeng He", "Zhihao Duan", "Fengqing Zhu"], "title": "CL-LoRA: Continual Low-Rank Adaptation for Rehearsal-Free Class-Incremental Learning", "categories": ["cs.CV"], "comment": "Accepted to CVPR 2025", "summary": "Class-Incremental Learning (CIL) aims to learn new classes sequentially while\nretaining the knowledge of previously learned classes. Recently, pre-trained\nmodels (PTMs) combined with parameter-efficient fine-tuning (PEFT) have shown\nremarkable performance in rehearsal-free CIL without requiring exemplars from\nprevious tasks. However, existing adapter-based methods, which incorporate\nlightweight learnable modules into PTMs for CIL, create new adapters for each\nnew task, leading to both parameter redundancy and failure to leverage shared\nknowledge across tasks. In this work, we propose ContinuaL Low-Rank Adaptation\n(CL-LoRA), which introduces a novel dual-adapter architecture combining\n\\textbf{task-shared adapters} to learn cross-task knowledge and\n\\textbf{task-specific adapters} to capture unique features of each new task.\nSpecifically, the shared adapters utilize random orthogonal matrices and\nleverage knowledge distillation with gradient reassignment to preserve\nessential shared knowledge. In addition, we introduce learnable block-wise\nweights for task-specific adapters, which mitigate inter-task interference\nwhile maintaining the model's plasticity. We demonstrate CL-LoRA consistently\nachieves promising performance under multiple benchmarks with reduced training\nand inference computation, establishing a more efficient and scalable paradigm\nfor continual learning with pre-trained models."}
{"id": "2505.24683", "pdf": "https://arxiv.org/pdf/2505.24683", "abs": "https://arxiv.org/abs/2505.24683", "authors": ["Dayeon Ki", "Kevin Duh", "Marine Carpuat"], "title": "Should I Share this Translation? Evaluating Quality Feedback for User Reliance on Machine Translation", "categories": ["cs.CL", "cs.AI"], "comment": "22 pages, 7 figures", "summary": "As people increasingly use AI systems in work and daily life, feedback\nmechanisms that help them use AI responsibly are urgently needed, particularly\nin settings where users are not equipped to assess the quality of AI\npredictions. We study a realistic Machine Translation (MT) scenario where\nmonolingual users decide whether to share an MT output, first without and then\nwith quality feedback. We compare four types of quality feedback: explicit\nfeedback that directly give users an assessment of translation quality using 1)\nerror highlights and 2) LLM explanations, and implicit feedback that helps\nusers compare MT inputs and outputs through 3) backtranslation and 4)\nquestion-answer (QA) tables. We find that all feedback types, except error\nhighlights, significantly improve both decision accuracy and appropriate\nreliance. Notably, implicit feedback, especially QA tables, yields\nsignificantly greater gains than explicit feedback in terms of decision\naccuracy, appropriate reliance, and user perceptions, receiving the highest\nratings for helpfulness and trust, and the lowest for mental burden."}
{"id": "2505.24676", "pdf": "https://arxiv.org/pdf/2505.24676", "abs": "https://arxiv.org/abs/2505.24676", "authors": ["Mihir Bhaskar", "Jun Tao Luo", "Zihan Geng", "Asmita Hajra", "Junia Howell", "Matthew R. Gormley"], "title": "Predicting the Past: Estimating Historical Appraisals with OCR and Machine Learning", "categories": ["cs.LG"], "comment": "Accepted to COMPASS 2025", "summary": "Despite well-documented consequences of the U.S. government's 1930s housing\npolicies on racial wealth disparities, scholars have struggled to quantify its\nprecise financial effects due to the inaccessibility of historical property\nappraisal records. Many counties still store these records in physical formats,\nmaking large-scale quantitative analysis difficult. We present an approach\nscholars can use to digitize historical housing assessment data, applying it to\nbuild and release a dataset for one county. Starting from publicly available\nscanned documents, we manually annotated property cards for over 12,000\nproperties to train and validate our methods. We use OCR to label data for an\nadditional 50,000 properties, based on our two-stage approach combining\nclassical computer vision techniques with deep learning-based OCR. For cases\nwhere OCR cannot be applied, such as when scanned documents are not available,\nwe show how a regression model based on building feature data can estimate the\nhistorical values, and test the generalizability of this model to other\ncounties. With these cost-effective tools, scholars, community activists, and\npolicy makers can better analyze and understand the historical impacts of\nredlining."}
{"id": "2505.24609", "pdf": "https://arxiv.org/pdf/2505.24609", "abs": "https://arxiv.org/abs/2505.24609", "authors": ["Patawee Prakrankamanant", "Shinji Watanabe", "Ekapol Chuangsuwanich"], "title": "Explainable Depression Detection using Masked Hard Instance Mining", "categories": ["cs.CL"], "comment": null, "summary": "This paper addresses the critical need for improved explainability in\ntext-based depression detection. While offering predictive outcomes, current\nsolutions often overlook the understanding of model predictions which can\nhinder trust in the system. We propose the use of Masked Hard Instance Mining\n(MHIM) to enhance the explainability in the depression detection task. MHIM\nstrategically masks attention weights within the model, compelling it to\ndistribute attention across a wider range of salient features. We evaluate MHIM\non two datasets representing distinct languages: Thai (Thai-Maywe) and English\n(DAIC-WOZ). Our results demonstrate that MHIM significantly improves\nperformance in terms of both prediction accuracy and explainability metrics."}
{"id": "2505.24824", "pdf": "https://arxiv.org/pdf/2505.24824", "abs": "https://arxiv.org/abs/2505.24824", "authors": ["Marta LÃ³pez-Rauhut", "Hongyu Zhou", "Mathieu Aubry", "Loic Landrieu"], "title": "Segmenting France Across Four Centuries", "categories": ["cs.CV"], "comment": "20 pages, 8 figures, 3 tables", "summary": "Historical maps offer an invaluable perspective into territory evolution\nacross past centuries--long before satellite or remote sensing technologies\nexisted. Deep learning methods have shown promising results in segmenting\nhistorical maps, but publicly available datasets typically focus on a single\nmap type or period, require extensive and costly annotations, and are not\nsuited for nationwide, long-term analyses. In this paper, we introduce a new\ndataset of historical maps tailored for analyzing large-scale, long-term land\nuse and land cover evolution with limited annotations. Spanning metropolitan\nFrance (548,305 km^2), our dataset contains three map collections from the\n18th, 19th, and 20th centuries. We provide both comprehensive modern labels and\n22,878 km^2 of manually annotated historical labels for the 18th and 19th\ncentury maps. Our dataset illustrates the complexity of the segmentation task,\nfeaturing stylistic inconsistencies, interpretive ambiguities, and significant\nlandscape changes (e.g., marshlands disappearing in favor of forests). We\nassess the difficulty of these challenges by benchmarking three approaches: a\nfully-supervised model trained with historical labels, and two\nweakly-supervised models that rely only on modern annotations. The latter\neither use the modern labels directly or first perform image-to-image\ntranslation to address the stylistic gap between historical and contemporary\nmaps. Finally, we discuss how these methods can support long-term environment\nmonitoring, offering insights into centuries of landscape transformation. Our\nofficial project repository is publicly available at\nhttps://github.com/Archiel19/FRAx4.git."}
{"id": "2505.24684", "pdf": "https://arxiv.org/pdf/2505.24684", "abs": "https://arxiv.org/abs/2505.24684", "authors": ["Zihao Chen", "Yu Xiang", "Wenyong Wang"], "title": "Disentangling Granularity: An Implicit Inductive Bias in Factorized VAEs", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Despite the success in learning semantically meaningful, unsupervised\ndisentangled representations, variational autoencoders (VAEs) and their\nvariants face a fundamental theoretical challenge: substantial evidence\nindicates that unsupervised disentanglement is unattainable without implicit\ninductive bias, yet such bias remains elusive. In this work, we focus on\nexploring the implicit inductive bias that drive disentanglement in VAEs with\nfactorization priors. By analyzing the total correlation in \\b{eta}-TCVAE, we\nuncover a crucial implicit inductive bias called disentangling granularity,\nwhich leads to the discovery of an interesting \"V\"-shaped optimal Evidence\nLower Bound (ELBO) trajectory within the parameter space. This finding is\nvalidated through over 100K experiments using factorized VAEs and our newly\nproposed model, \\b{eta}-STCVAE. Notably, experimental results reveal that\nconventional factorized VAEs, constrained by fixed disentangling granularity,\ninherently tend to disentangle low-complexity feature. Whereas, appropriately\ntuning disentangling granularity, as enabled by \\b{eta}-STCVAE, broadens the\nrange of disentangled representations, allowing for the disentanglement of\nhigh-complexity features. Our findings unveil that disentangling granularity as\nan implicit inductive bias in factorized VAEs influence both disentanglement\nperformance and the inference of the ELBO, offering fresh insights into the\ninterpretability and inherent biases of VAEs."}
{"id": "2505.24692", "pdf": "https://arxiv.org/pdf/2505.24692", "abs": "https://arxiv.org/abs/2505.24692", "authors": ["Derek Everett", "Fred Lu", "Edward Raff", "Fernando Camacho", "James Holt"], "title": "Quick-Draw Bandits: Quickly Optimizing in Nonstationary Environments with Extremely Many Arms", "categories": ["cs.LG", "stat.ML"], "comment": "KDD 2025, Research Track", "summary": "Canonical algorithms for multi-armed bandits typically assume a stationary\nreward environment where the size of the action space (number of arms) is\nsmall. More recently developed methods typically relax only one of these\nassumptions: existing non-stationary bandit policies are designed for a small\nnumber of arms, while Lipschitz, linear, and Gaussian process bandit policies\nare designed to handle a large (or infinite) number of arms in stationary\nreward environments under constraints on the reward function. In this\nmanuscript, we propose a novel policy to learn reward environments over a\ncontinuous space using Gaussian interpolation. We show that our method\nefficiently learns continuous Lipschitz reward functions with\n$\\mathcal{O}^*(\\sqrt{T})$ cumulative regret. Furthermore, our method naturally\nextends to non-stationary problems with a simple modification. We finally\ndemonstrate that our method is computationally favorable (100-10000x faster)\nand experimentally outperforms sliding Gaussian process policies on datasets\nwith non-stationarity and an extremely large number of arms."}
{"id": "2505.24613", "pdf": "https://arxiv.org/pdf/2505.24613", "abs": "https://arxiv.org/abs/2505.24613", "authors": ["Daniela Occhipinti", "Marco Guerini", "Malvina Nissim"], "title": "When Harry Meets Superman: The Role of The Interlocutor in Persona-Based Dialogue Generation", "categories": ["cs.CL"], "comment": null, "summary": "Endowing dialogue agents with persona information has proven to significantly\nimprove the consistency and diversity of their generations. While much focus\nhas been placed on aligning dialogues with provided personas, the adaptation to\nthe interlocutor's profile remains largely underexplored. In this work, we\ninvestigate three key aspects: (1) a model's ability to align responses with\nboth the provided persona and the interlocutor's; (2) its robustness when\ndealing with familiar versus unfamiliar interlocutors and topics, and (3) the\nimpact of additional fine-tuning on specific persona-based dialogues. We\nevaluate dialogues generated with diverse speaker pairings and topics, framing\nthe evaluation as an author identification task and employing both\nLLM-as-a-judge and human evaluations. By systematically masking or disclosing\ninformation about the interlocutor, we assess its impact on dialogue\ngeneration. Results show that access to the interlocutor's persona improves the\nrecognition of the target speaker, while masking it does the opposite. Although\nmodels generalise well across topics, they struggle with unfamiliar\ninterlocutors. Finally, we found that in zero-shot settings, LLMs often copy\nbiographical details, facilitating identification but trivialising the task."}
{"id": "2505.24837", "pdf": "https://arxiv.org/pdf/2505.24837", "abs": "https://arxiv.org/abs/2505.24837", "authors": ["Yinglian Zhu", "Haiyang Yu", "Qizao Wang", "Wei Lu", "Xiangyang Xue", "Bin Li"], "title": "Zero-Shot Chinese Character Recognition with Hierarchical Multi-Granularity Image-Text Aligning", "categories": ["cs.CV"], "comment": "The first three authors contributed equally", "summary": "Chinese Character Recognition (CCR) is a fundamental technology for\nintelligent document processing. Unlike Latin characters, Chinese characters\nexhibit unique spatial structures and compositional rules, allowing for the use\nof fine-grained semantic information in representation. However, existing\napproaches are usually based on auto-regressive as well as edit distance\npost-process and typically rely on a single-level character representation. In\nthis paper, we propose a Hierarchical Multi-Granularity Image-Text Aligning\n(Hi-GITA) framework based on a contrastive paradigm. To leverage the abundant\nfine-grained semantic information of Chinese characters, we propose\nmulti-granularity encoders on both image and text sides. Specifically, the\nImage Multi-Granularity Encoder extracts hierarchical image representations\nfrom character images, capturing semantic cues from localized strokes to\nholistic structures. The Text Multi-Granularity Encoder extracts stroke and\nradical sequence representations at different levels of granularity. To better\ncapture the relationships between strokes and radicals, we introduce\nMulti-Granularity Fusion Modules on the image and text sides, respectively.\nFurthermore, to effectively bridge the two modalities, we further introduce a\nFine-Grained Decoupled Image-Text Contrastive loss, which aligns image and text\nrepresentations across multiple granularities. Extensive experiments\ndemonstrate that our proposed Hi-GITA significantly outperforms existing\nzero-shot CCR methods. For instance, it brings about 20% accuracy improvement\nin handwritten character and radical zero-shot settings. Code and models will\nbe released soon."}
{"id": "2505.24701", "pdf": "https://arxiv.org/pdf/2505.24701", "abs": "https://arxiv.org/abs/2505.24701", "authors": ["Tejul Pandit", "Meet Raval", "Dhvani Upadhyay"], "title": "Multi-Domain ABSA Conversation Dataset Generation via LLMs for Real-World Evaluation and Model Comparison", "categories": ["cs.CL", "cs.AI"], "comment": "11 pages, 3 figures, 5 tables, 6th International Conference on\n  Natural Language Computing and AI (NLCAI 2025), ISBN : 978-1-923107-59-5,\n  Computer Science & Information Technology (CS & IT), ISSN : 2231 - 5403,\n  Volume 15, Number 10, May 2025", "summary": "Aspect-Based Sentiment Analysis (ABSA) offers granular insights into opinions\nbut often suffers from the scarcity of diverse, labeled datasets that reflect\nreal-world conversational nuances. This paper presents an approach for\ngenerating synthetic ABSA data using Large Language Models (LLMs) to address\nthis gap. We detail the generation process aimed at producing data with\nconsistent topic and sentiment distributions across multiple domains using\nGPT-4o. The quality and utility of the generated data were evaluated by\nassessing the performance of three state-of-the-art LLMs (Gemini 1.5 Pro,\nClaude 3.5 Sonnet, and DeepSeek-R1) on topic and sentiment classification\ntasks. Our results demonstrate the effectiveness of the synthetic data,\nrevealing distinct performance trade-offs among the models: DeepSeekR1 showed\nhigher precision, Gemini 1.5 Pro and Claude 3.5 Sonnet exhibited strong recall,\nand Gemini 1.5 Pro offered significantly faster inference. We conclude that\nLLM-based synthetic data generation is a viable and flexible method for\ncreating valuable ABSA resources, facilitating research and model evaluation\nwithout reliance on limited or inaccessible real-world labeled data."}
{"id": "2505.24709", "pdf": "https://arxiv.org/pdf/2505.24709", "abs": "https://arxiv.org/abs/2505.24709", "authors": ["Soichiro Nishimori", "Yu-Jie Zhang", "Thanawat Lodkaew", "Masashi Sugiyama"], "title": "On Symmetric Losses for Robust Policy Optimization with Noisy Preferences", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Optimizing policies based on human preferences is key to aligning language\nmodels with human intent. This work focuses on reward modeling, a core\ncomponent in reinforcement learning from human feedback (RLHF), and offline\npreference optimization, such as direct preference optimization. Conventional\napproaches typically assume accurate annotations. However, real-world\npreference data often contains noise due to human errors or biases. We propose\na principled framework for robust policy optimization under noisy preferences,\nviewing reward modeling as a classification problem. This allows us to leverage\nsymmetric losses, known for their robustness to label noise in classification,\nleading to our Symmetric Preference Optimization (SymPO) method. We prove that\nsymmetric losses enable successful policy optimization even under noisy labels,\nas the resulting reward remains rank-preserving -- a property sufficient for\npolicy improvement. Experiments on synthetic and real-world tasks demonstrate\nthe effectiveness of SymPO."}
{"id": "2505.24615", "pdf": "https://arxiv.org/pdf/2505.24615", "abs": "https://arxiv.org/abs/2505.24615", "authors": ["Yan Liu", "Zonglin Yang", "Soujanya Poria", "Thanh-Son Nguyen", "Erik Cambria"], "title": "Harnessing Large Language Models for Scientific Novelty Detection", "categories": ["cs.CL", "H.4.0"], "comment": "15 pages, 3 figures, 3 tables", "summary": "In an era of exponential scientific growth, identifying novel research ideas\nis crucial and challenging in academia. Despite potential, the lack of an\nappropriate benchmark dataset hinders the research of novelty detection. More\nimportantly, simply adopting existing NLP technologies, e.g., retrieving and\nthen cross-checking, is not a one-size-fits-all solution due to the gap between\ntextual similarity and idea conception. In this paper, we propose to harness\nlarge language models (LLMs) for scientific novelty detection (ND), associated\nwith two new datasets in marketing and NLP domains. To construct the\nconsiderate datasets for ND, we propose to extract closure sets of papers based\non their relationship, and then summarize their main ideas based on LLMs. To\ncapture idea conception, we propose to train a lightweight retriever by\ndistilling the idea-level knowledge from LLMs to align ideas with similar\nconception, enabling efficient and accurate idea retrieval for LLM novelty\ndetection. Experiments show our method consistently outperforms others on the\nproposed benchmark datasets for idea retrieval and ND tasks. Codes and data are\navailable at https://anonymous.4open.science/r/NoveltyDetection-10FB/."}
{"id": "2505.24838", "pdf": "https://arxiv.org/pdf/2505.24838", "abs": "https://arxiv.org/abs/2505.24838", "authors": ["Brandon Man", "Ghadi Nehme", "Md Ferdous Alam", "Faez Ahmed"], "title": "VideoCAD: A Large-Scale Video Dataset for Learning UI Interactions and 3D Reasoning from CAD Software", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Computer-Aided Design (CAD) is a time-consuming and complex process,\nrequiring precise, long-horizon user interactions with intricate 3D interfaces.\nWhile recent advances in AI-driven user interface (UI) agents show promise,\nmost existing datasets and methods focus on short, low-complexity tasks in\nmobile or web applications, failing to capture the demands of professional\nengineering tools. In this work, we introduce VideoCAD, the first attempt at\nengineering UI interaction learning for precision tasks. Specifically, VideoCAD\nis a large-scale synthetic dataset consisting of over 41K annotated video\nrecordings of CAD operations, generated using an automated framework for\ncollecting high-fidelity UI action data from human-made CAD designs. Compared\nto existing datasets, VideoCAD offers an order of magnitude higher complexity\nin UI interaction learning for real-world engineering tasks, having up to a 20x\nlonger time horizon than other datasets. We show two important downstream\napplications of VideoCAD: learning UI interactions from professional precision\n3D CAD tools and a visual question-answering (VQA) benchmark designed to\nevaluate multimodal large language models' (LLM) spatial reasoning and video\nunderstanding abilities. To learn the UI interactions, we propose\nVideoCADFormer - a state-of-the-art model in learning CAD interactions directly\nfrom video, which outperforms multiple behavior cloning baselines. Both\nVideoCADFormer and the VQA benchmark derived from VideoCAD reveal key\nchallenges in the current state of video-based UI understanding, including the\nneed for precise action grounding, multi-modal and spatial reasoning, and\nlong-horizon dependencies."}
{"id": "2505.24710", "pdf": "https://arxiv.org/pdf/2505.24710", "abs": "https://arxiv.org/abs/2505.24710", "authors": ["Wei Chen", "Jiahao Zhang", "Haipeng Zhu", "Boyan Xu", "Zhifeng Hao", "Keli Zhang", "Junjian Ye", "Ruichu Cai"], "title": "Causal-aware Large Language Models: Enhancing Decision-Making Through Learning, Adapting and Acting", "categories": ["cs.LG", "cs.AI", "cs.CL"], "comment": "Accepted by IJCAI 2025", "summary": "Large language models (LLMs) have shown great potential in decision-making\ndue to the vast amount of knowledge stored within the models. However, these\npre-trained models are prone to lack reasoning abilities and are difficult to\nadapt to new environments, further hindering their application to complex\nreal-world tasks. To address these challenges, inspired by the human cognitive\nprocess, we propose Causal-aware LLMs, which integrate the structural causal\nmodel (SCM) into the decision-making process to model, update, and utilize\nstructured knowledge of the environment in a ``learning-adapting-acting\"\nparadigm. Specifically, in the learning stage, we first utilize an LLM to\nextract the environment-specific causal entities and their causal relations to\ninitialize a structured causal model of the environment. Subsequently,in the\nadapting stage, we update the structured causal model through external feedback\nabout the environment, via an idea of causal intervention. Finally, in the\nacting stage, Causal-aware LLMs exploit structured causal knowledge for more\nefficient policy-making through the reinforcement learning agent. The above\nprocesses are performed iteratively to learn causal knowledge, ultimately\nenabling the causal-aware LLMs to achieve a more accurate understanding of the\nenvironment and make more efficient decisions. Experimental results across 22\ndiverse tasks within the open-world game ``Crafter\" validate the effectiveness\nof our proposed method."}
{"id": "2505.24715", "pdf": "https://arxiv.org/pdf/2505.24715", "abs": "https://arxiv.org/abs/2505.24715", "authors": ["Fabio Fehr", "Prabhu Teja Sivaprasad", "Luca Franceschi", "Giovanni Zappella"], "title": "CoRet: Improved Retriever for Code Editing", "categories": ["cs.LG", "cs.AI", "cs.CL"], "comment": "ACL 2025", "summary": "In this paper, we introduce CoRet, a dense retrieval model designed for\ncode-editing tasks that integrates code semantics, repository structure, and\ncall graph dependencies. The model focuses on retrieving relevant portions of a\ncode repository based on natural language queries such as requests to implement\nnew features or fix bugs. These retrieved code chunks can then be presented to\na user or to a second code-editing model or agent. To train CoRet, we propose a\nloss function explicitly designed for repository-level retrieval. On SWE-bench\nand Long Code Arena's bug localisation datasets, we show that our model\nsubstantially improves retrieval recall by at least 15 percentage points over\nexisting models, and ablate the design choices to show their importance in\nachieving these results."}
{"id": "2505.24619", "pdf": "https://arxiv.org/pdf/2505.24619", "abs": "https://arxiv.org/abs/2505.24619", "authors": ["Vittorio Torri", "Machteld J. Boonstra", "Marielle C. van de Veerdonk", "Deborah N. Kalkman", "Alicia Uijl", "Francesca Ieva", "Ameen Abu-Hanna", "Folkert W. Asselbergs", "Iacer Calixto"], "title": "Interpretable phenotyping of Heart Failure patients with Dutch discharge letters", "categories": ["cs.CL", "cs.LG", "68T50", "I.2.7; J.3"], "comment": "43 pages, 8 figures", "summary": "Objective: Heart failure (HF) patients present with diverse phenotypes\naffecting treatment and prognosis. This study evaluates models for phenotyping\nHF patients based on left ventricular ejection fraction (LVEF) classes, using\nstructured and unstructured data, assessing performance and interpretability.\n  Materials and Methods: The study analyzes all HF hospitalizations at both\nAmsterdam UMC hospitals (AMC and VUmc) from 2015 to 2023 (33,105\nhospitalizations, 16,334 patients). Data from AMC were used for model training,\nand from VUmc for external validation. The dataset was unlabelled and included\ntabular clinical measurements and discharge letters. Silver labels for LVEF\nclasses were generated by combining diagnosis codes, echocardiography results,\nand textual mentions. Gold labels were manually annotated for 300 patients for\ntesting. Multiple Transformer-based (black-box) and Aug-Linear (white-box)\nmodels were trained and compared with baselines on structured and unstructured\ndata. To evaluate interpretability, two clinicians annotated 20 discharge\nletters by highlighting information they considered relevant for LVEF\nclassification. These were compared to SHAP and LIME explanations from\nblack-box models and the inherent explanations of Aug-Linear models.\n  Results: BERT-based and Aug-Linear models, using discharge letters alone,\nachieved the highest classification results (AUC=0.84 for BERT, 0.81 for\nAug-Linear on external validation), outperforming baselines. Aug-Linear\nexplanations aligned more closely with clinicians' explanations than post-hoc\nexplanations on black-box models.\n  Conclusions: Discharge letters emerged as the most informative source for\nphenotyping HF patients. Aug-Linear models matched black-box performance while\nproviding clinician-aligned interpretability, supporting their use in\ntransparent clinical decision-making."}
{"id": "2505.24840", "pdf": "https://arxiv.org/pdf/2505.24840", "abs": "https://arxiv.org/abs/2505.24840", "authors": ["Yuwen Tan", "Yuan Qing", "Boqing Gong"], "title": "Vision LLMs Are Bad at Hierarchical Visual Understanding, and LLMs Are the Bottleneck", "categories": ["cs.CV", "cs.AI", "cs.CL", "cs.LG"], "comment": "28 pages, 13 figures", "summary": "This paper reveals that many state-of-the-art large language models (LLMs)\nlack hierarchical knowledge about our visual world, unaware of even\nwell-established biology taxonomies. This shortcoming makes LLMs a bottleneck\nfor vision LLMs' hierarchical visual understanding (e.g., recognizing Anemone\nFish but not Vertebrate). We arrive at these findings using about one million\nfour-choice visual question answering (VQA) tasks constructed from six\ntaxonomies and four image datasets. Interestingly, finetuning a vision LLM\nusing our VQA tasks reaffirms LLMs' bottleneck effect to some extent because\nthe VQA tasks improve the LLM's hierarchical consistency more than the vision\nLLM's. We conjecture that one cannot make vision LLMs understand visual\nconcepts fully hierarchical until LLMs possess corresponding taxonomy\nknowledge."}
{"id": "2505.24716", "pdf": "https://arxiv.org/pdf/2505.24716", "abs": "https://arxiv.org/abs/2505.24716", "authors": ["Christopher Buss", "Mahdis Safari", "Arash Termehchy", "Stefan Lee", "David Maier"], "title": "Towards Scalable Schema Mapping using Large Language Models", "categories": ["cs.DB", "cs.AI"], "comment": null, "summary": "The growing need to integrate information from a large number of diverse\nsources poses significant scalability challenges for data integration systems.\nThese systems often rely on manually written schema mappings, which are\ncomplex, source-specific, and costly to maintain as sources evolve. While\nrecent advances suggest that large language models (LLMs) can assist in\nautomating schema matching by leveraging both structural and natural language\ncues, key challenges remain. In this paper, we identify three core issues with\nusing LLMs for schema mapping: (1) inconsistent outputs due to sensitivity to\ninput phrasing and structure, which we propose methods to address through\nsampling and aggregation techniques; (2) the need for more expressive mappings\n(e.g., GLaV), which strain the limited context windows of LLMs; and (3) the\ncomputational cost of repeated LLM calls, which we propose to mitigate through\nstrategies like data type prefiltering."}
{"id": "2505.24717", "pdf": "https://arxiv.org/pdf/2505.24717", "abs": "https://arxiv.org/abs/2505.24717", "authors": ["Benjamin Holzschuh", "Qiang Liu", "Georg Kohl", "Nils Thuerey"], "title": "PDE-Transformer: Efficient and Versatile Transformers for Physics Simulations", "categories": ["cs.LG"], "comment": "ICML 2025. Code available at\n  https://github.com/tum-pbs/pde-transformer", "summary": "We introduce PDE-Transformer, an improved transformer-based architecture for\nsurrogate modeling of physics simulations on regular grids. We combine recent\narchitectural improvements of diffusion transformers with adjustments specific\nfor large-scale simulations to yield a more scalable and versatile\ngeneral-purpose transformer architecture, which can be used as the backbone for\nbuilding large-scale foundation models in physical sciences. We demonstrate\nthat our proposed architecture outperforms state-of-the-art transformer\narchitectures for computer vision on a large dataset of 16 different types of\nPDEs. We propose to embed different physical channels individually as\nspatio-temporal tokens, which interact via channel-wise self-attention. This\nhelps to maintain a consistent information density of tokens when learning\nmultiple types of PDEs simultaneously. We demonstrate that our pre-trained\nmodels achieve improved performance on several challenging downstream tasks\ncompared to training from scratch and also beat other foundation model\narchitectures for physics simulations."}
{"id": "2505.24621", "pdf": "https://arxiv.org/pdf/2505.24621", "abs": "https://arxiv.org/abs/2505.24621", "authors": ["Utsav Maskey", "Chencheng Zhu", "Usman Naseem"], "title": "Benchmarking Large Language Models for Cryptanalysis and Mismatched-Generalization", "categories": ["cs.CL"], "comment": "Preprint", "summary": "Recent advancements in Large Language Models (LLMs) have transformed natural\nlanguage understanding and generation, leading to extensive benchmarking across\ndiverse tasks. However, cryptanalysis a critical area for data security and\nencryption has not yet been thoroughly explored in LLM evaluations. To address\nthis gap, we evaluate cryptanalytic potential of state of the art LLMs on\nencrypted texts generated using a range of cryptographic algorithms. We\nintroduce a novel benchmark dataset comprising diverse plain texts spanning\nvarious domains, lengths, writing styles, and topics paired with their\nencrypted versions. Using zero-shot and few shot settings, we assess multiple\nLLMs for decryption accuracy and semantic comprehension across different\nencryption schemes. Our findings reveal key insights into the strengths and\nlimitations of LLMs in side-channel communication while raising concerns about\ntheir susceptibility to jailbreaking attacks. This research highlights the\ndual-use nature of LLMs in security contexts and contributes to the ongoing\ndiscussion on AI safety and security."}
{"id": "2505.24848", "pdf": "https://arxiv.org/pdf/2505.24848", "abs": "https://arxiv.org/abs/2505.24848", "authors": ["Charig Yang", "Samiul Alam", "Shakhrul Iman Siam", "Michael J. Proulx", "Lambert Mathias", "Kiran Somasundaram", "Luis Pesqueira", "James Fort", "Sheroze Sheriffdeen", "Omkar Parkhi", "Carl Ren", "Mi Zhang", "Yuning Chai", "Richard Newcombe", "Hyo Jin Kim"], "title": "Reading Recognition in the Wild", "categories": ["cs.CV", "cs.LG"], "comment": null, "summary": "To enable egocentric contextual AI in always-on smart glasses, it is crucial\nto be able to keep a record of the user's interactions with the world,\nincluding during reading. In this paper, we introduce a new task of reading\nrecognition to determine when the user is reading. We first introduce the\nfirst-of-its-kind large-scale multimodal Reading in the Wild dataset,\ncontaining 100 hours of reading and non-reading videos in diverse and realistic\nscenarios. We then identify three modalities (egocentric RGB, eye gaze, head\npose) that can be used to solve the task, and present a flexible transformer\nmodel that performs the task using these modalities, either individually or\ncombined. We show that these modalities are relevant and complementary to the\ntask, and investigate how to efficiently and effectively encode each modality.\nAdditionally, we show the usefulness of this dataset towards classifying types\nof reading, extending current reading understanding studies conducted in\nconstrained settings to larger scale, diversity and realism. Code, model, and\ndata will be public."}
{"id": "2505.24722", "pdf": "https://arxiv.org/pdf/2505.24722", "abs": "https://arxiv.org/abs/2505.24722", "authors": ["Neil He", "Rishabh Anand", "Hiren Madhu", "Ali Maatouk", "Smita Krishnaswamy", "Leandros Tassiulas", "Menglin Yang", "Rex Ying"], "title": "HELM: Hyperbolic Large Language Models via Mixture-of-Curvature Experts", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Large language models (LLMs) have shown great success in text modeling tasks\nacross domains. However, natural language exhibits inherent semantic\nhierarchies and nuanced geometric structure, which current LLMs do not capture\ncompletely owing to their reliance on Euclidean operations. Recent studies have\nalso shown that not respecting the geometry of token embeddings leads to\ntraining instabilities and degradation of generative capabilities. These\nfindings suggest that shifting to non-Euclidean geometries can better align\nlanguage models with the underlying geometry of text. We thus propose to\noperate fully in Hyperbolic space, known for its expansive, scale-free, and\nlow-distortion properties. We thus introduce HELM, a family of HypErbolic Large\nLanguage Models, offering a geometric rethinking of the Transformer-based LLM\nthat addresses the representational inflexibility, missing set of necessary\noperations, and poor scalability of existing hyperbolic LMs. We additionally\nintroduce a Mixture-of-Curvature Experts model, HELM-MICE, where each expert\noperates in a distinct curvature space to encode more fine-grained geometric\nstructure from text, as well as a dense model, HELM-D. For HELM-MICE, we\nfurther develop hyperbolic Multi-Head Latent Attention (HMLA) for efficient,\nreduced-KV-cache training and inference. For both models, we develop essential\nhyperbolic equivalents of rotary positional encodings and RMS normalization. We\nare the first to train fully hyperbolic LLMs at billion-parameter scale, and\nevaluate them on well-known benchmarks such as MMLU and ARC, spanning STEM\nproblem-solving, general knowledge, and commonsense reasoning. Our results show\nconsistent gains from our HELM architectures -- up to 4% -- over popular\nEuclidean architectures used in LLaMA and DeepSeek, highlighting the efficacy\nand enhanced reasoning afforded by hyperbolic geometry in large-scale LM\npretraining."}
{"id": "2505.24721", "pdf": "https://arxiv.org/pdf/2505.24721", "abs": "https://arxiv.org/abs/2505.24721", "authors": ["Nick Rossenbach", "Benedikt Hilmes", "Leon Brackmann", "Moritz Gunz", "Ralf SchlÃ¼ter"], "title": "Running Conventional Automatic Speech Recognition on Memristor Hardware: A Simulated Approach", "categories": ["cs.LG", "cs.AR", "cs.ET"], "comment": "Accepted for the Blue Sky track at Interspeech 2025", "summary": "Memristor-based hardware offers new possibilities for energy-efficient\nmachine learning (ML) by providing analog in-memory matrix multiplication.\nCurrent hardware prototypes cannot fit large neural networks, and related\nliterature covers only small ML models for tasks like MNIST or single word\nrecognition. Simulation can be used to explore how hardware properties affect\nlarger models, but existing software assumes simplified hardware. We propose a\nPyTorch-based library based on \"Synaptogen\" to simulate neural network\nexecution with accurately captured memristor hardware properties. For the first\ntime, we show how an ML system with millions of parameters would behave on\nmemristor hardware, using a Conformer trained on the speech recognition task\nTED-LIUMv2 as example. With adjusted quantization-aware training, we limit the\nrelative degradation in word error rate to 25% when using a 3-bit weight\nprecision to execute linear operations via simulated analog computation."}
{"id": "2505.24635", "pdf": "https://arxiv.org/pdf/2505.24635", "abs": "https://arxiv.org/abs/2505.24635", "authors": ["Jiahao Ying", "Wei Tang", "Yiran Zhao", "Yixin Cao", "Yu Rong", "Wenxuan Zhang"], "title": "Disentangling Language and Culture for Evaluating Multilingual Large Language Models", "categories": ["cs.CL"], "comment": "Accepted to ACL 2025 (Main Conference)", "summary": "This paper introduces a Dual Evaluation Framework to comprehensively assess\nthe multilingual capabilities of LLMs. By decomposing the evaluation along the\ndimensions of linguistic medium and cultural context, this framework enables a\nnuanced analysis of LLMs' ability to process questions within both native and\ncross-cultural contexts cross-lingually. Extensive evaluations are conducted on\na wide range of models, revealing a notable \"CulturalLinguistic Synergy\"\nphenomenon, where models exhibit better performance when questions are\nculturally aligned with the language. This phenomenon is further explored\nthrough interpretability probing, which shows that a higher proportion of\nspecific neurons are activated in a language's cultural context. This\nactivation proportion could serve as a potential indicator for evaluating\nmultilingual performance during model training. Our findings challenge the\nprevailing notion that LLMs, primarily trained on English data, perform\nuniformly across languages and highlight the necessity of culturally and\nlinguistically model evaluations. Our code can be found at\nhttps://yingjiahao14. github.io/Dual-Evaluation/."}
{"id": "2505.24862", "pdf": "https://arxiv.org/pdf/2505.24862", "abs": "https://arxiv.org/abs/2505.24862", "authors": ["Cailin Zhuang", "Ailin Huang", "Wei Cheng", "Jingwei Wu", "Yaoqi Hu", "Jiaqi Liao", "Zhewei Huang", "Hongyuan Wang", "Xinyao Liao", "Weiwei Cai", "Hengyuan Xu", "Xuanyang Zhang", "Xianfang Zeng", "Gang Yu", "Chi Zhang"], "title": "ViStoryBench: Comprehensive Benchmark Suite for Story Visualization", "categories": ["cs.CV"], "comment": "33 Pages, Project Page: https://vistorybench.github.io/, Code:\n  https://github.com/vistorybench/vistorybench", "summary": "Story visualization, which aims to generate a sequence of visually coherent\nimages aligning with a given narrative and reference images, has seen\nsignificant progress with recent advancements in generative models. To further\nenhance the performance of story visualization frameworks in real-world\nscenarios, we introduce a comprehensive evaluation benchmark, ViStoryBench. We\ncollect a diverse dataset encompassing various story types and artistic styles,\nensuring models are evaluated across multiple dimensions such as different\nplots (e.g., comedy, horror) and visual aesthetics (e.g., anime, 3D\nrenderings). ViStoryBench is carefully curated to balance narrative structures\nand visual elements, featuring stories with single and multiple protagonists to\ntest models' ability to maintain character consistency. Additionally, it\nincludes complex plots and intricate world-building to challenge models in\ngenerating accurate visuals. To ensure comprehensive comparisons, our benchmark\nincorporates a wide range of evaluation metrics assessing critical aspects.\nThis structured and multifaceted framework enables researchers to thoroughly\nidentify both the strengths and weaknesses of different models, fostering\ntargeted improvements."}
{"id": "2505.24754", "pdf": "https://arxiv.org/pdf/2505.24754", "abs": "https://arxiv.org/abs/2505.24754", "authors": ["Yingchaojie Feng", "Yiqun Sun", "Yandong Sun", "Minfeng Zhu", "Qiang Huang", "Anthony K. H. Tung", "Wei Chen"], "title": "Don't Reinvent the Wheel: Efficient Instruction-Following Text Embedding based on Guided Space Transformation", "categories": ["cs.CL", "cs.AI", "cs.IR"], "comment": "Accepted to ACL 2025", "summary": "In this work, we investigate an important task named instruction-following\ntext embedding, which generates dynamic text embeddings that adapt to user\ninstructions, highlighting specific attributes of text. Despite recent\nadvancements, existing approaches suffer from significant computational\noverhead, as they require re-encoding the entire corpus for each new\ninstruction. To address this challenge, we propose GSTransform, a novel\ninstruction-following text embedding framework based on Guided Space\nTransformation. Our key observation is that instruction-relevant information is\ninherently encoded in generic embeddings but remains underutilized. Instead of\nrepeatedly encoding the corpus for each instruction, GSTransform is a\nlightweight transformation mechanism that adapts pre-computed embeddings in\nreal time to align with user instructions, guided by a small amount of text\ndata with instruction-focused label annotation. We conduct extensive\nexperiments on three instruction-awareness downstream tasks across nine\nreal-world datasets, demonstrating that GSTransform improves\ninstruction-following text embedding quality over state-of-the-art methods\nwhile achieving dramatic speedups of 6~300x in real-time processing on\nlarge-scale datasets. The source code is available at\nhttps://github.com/YingchaojieFeng/GSTransform."}
{"id": "2505.24728", "pdf": "https://arxiv.org/pdf/2505.24728", "abs": "https://arxiv.org/abs/2505.24728", "authors": ["Dongzi Jin", "Yong Xiao", "Yingyu Li"], "title": "Robust Federated Learning against Model Perturbation in Edge Networks", "categories": ["cs.LG", "cs.DC"], "comment": "Accepted by IEEE ICC 2025", "summary": "Federated Learning (FL) is a promising paradigm for realizing edge\nintelligence, allowing collaborative learning among distributed edge devices by\nsharing models instead of raw data. However, the shared models are often\nassumed to be ideal, which would be inevitably violated in practice due to\nvarious perturbations, leading to significant performance degradation. To\novercome this challenge, we propose a novel method, termed Sharpness-Aware\nMinimization-based Robust Federated Learning (SMRFL), which aims to improve\nmodel robustness against perturbations by exploring the geometrical property of\nthe model landscape. Specifically, SMRFL solves a min-max optimization problem\nthat promotes model convergence towards a flat minimum by minimizing the\nmaximum loss within a neighborhood of the model parameters. In this way, model\nsensitivity to perturbations is reduced, and robustness is enhanced since\nmodels in the neighborhood of the flat minimum also enjoy low loss values. The\ntheoretical result proves that SMRFL can converge at the same rate as FL\nwithout perturbations. Extensive experimental results show that SMRFL\nsignificantly enhances robustness against perturbations compared to three\nbaseline methods on two real-world datasets under three perturbation scenarios."}
{"id": "2505.24643", "pdf": "https://arxiv.org/pdf/2505.24643", "abs": "https://arxiv.org/abs/2505.24643", "authors": ["Juan Wisznia", "Cecilia BolaÃ±os", "Juan Tollo", "Giovanni Marraffini", "AgustÃ­n Gianolini", "Noe Hsueh", "Luciano Del Corro"], "title": "Are Optimal Algorithms Still Optimal? Rethinking Sorting in LLM-Based Pairwise Ranking with Batching and Caching", "categories": ["cs.CL"], "comment": null, "summary": "We introduce a novel framework for analyzing sorting algorithms in pairwise\nranking prompting (PRP), re-centering the cost model around LLM inferences\nrather than traditional pairwise comparisons. While classical metrics based on\ncomparison counts have traditionally been used to gauge efficiency, our\nanalysis reveals that expensive LLM inferences overturn these predictions;\naccordingly, our framework encourages strategies such as batching and caching\nto mitigate inference costs. We show that algorithms optimal in the classical\nsetting can lose efficiency when LLM inferences dominate the cost under certain\noptimizations."}
{"id": "2505.24866", "pdf": "https://arxiv.org/pdf/2505.24866", "abs": "https://arxiv.org/abs/2505.24866", "authors": ["Xinqi Xiong", "Prakrut Patel", "Qingyuan Fan", "Amisha Wadhwa", "Sarathy Selvam", "Xiao Guo", "Luchao Qi", "Xiaoming Liu", "Roni Sengupta"], "title": "TalkingHeadBench: A Multi-Modal Benchmark & Analysis of Talking-Head DeepFake Detection", "categories": ["cs.CV"], "comment": null, "summary": "The rapid advancement of talking-head deepfake generation fueled by advanced\ngenerative models has elevated the realism of synthetic videos to a level that\nposes substantial risks in domains such as media, politics, and finance.\nHowever, current benchmarks for deepfake talking-head detection fail to reflect\nthis progress, relying on outdated generators and offering limited insight into\nmodel robustness and generalization. We introduce TalkingHeadBench, a\ncomprehensive multi-model multi-generator benchmark and curated dataset\ndesigned to evaluate the performance of state-of-the-art detectors on the most\nadvanced generators. Our dataset includes deepfakes synthesized by leading\nacademic and commercial models and features carefully constructed protocols to\nassess generalization under distribution shifts in identity and generator\ncharacteristics. We benchmark a diverse set of existing detection methods,\nincluding CNNs, vision transformers, and temporal models, and analyze their\nrobustness and generalization capabilities. In addition, we provide error\nanalysis using Grad-CAM visualizations to expose common failure modes and\ndetector biases. TalkingHeadBench is hosted on\nhttps://huggingface.co/datasets/luchaoqi/TalkingHeadBench with open access to\nall data splits and protocols. Our benchmark aims to accelerate research\ntowards more robust and generalizable detection models in the face of rapidly\nevolving generative techniques."}
{"id": "2505.24759", "pdf": "https://arxiv.org/pdf/2505.24759", "abs": "https://arxiv.org/abs/2505.24759", "authors": ["Mu Qiao"], "title": "Unsupervised Evolutionary Cell Type Matching via Entropy-Minimized Optimal Transport", "categories": ["q-bio.QM", "cs.AI", "cs.LG"], "comment": null, "summary": "Identifying evolutionary correspondences between cell types across species is\na fundamental challenge in comparative genomics and evolutionary biology.\nExisting approaches often rely on either reference-based matching, which\nimposes asymmetry by designating one species as the reference, or\nprojection-based matching, which may increase computational complexity and\nobscure biological interpretability at the cell-type level. Here, we present\nOT-MESH, an unsupervised computational framework leveraging entropy-regularized\noptimal transport (OT) to systematically determine cross-species cell type\nhomologies. Our method uniquely integrates the Minimize Entropy of Sinkhorn\n(MESH) technique to refine the OT plan. It begins by selecting genes with high\nSignal-to-Noise Ratio (SNR) to capture the most informative features, from\nwhich a cost matrix is constructed using cosine distances between cell-type\ncentroids. Importantly, the MESH procedure iteratively refines the cost matrix,\nleading to a transport plan with significantly enhanced sparsity and\ninterpretability of the resulting correspondence matrices. Applied to retinal\nbipolar cells (BCs) and retinal ganglion cells (RGCs) from mouse and macaque,\nOT-MESH accurately recovers known evolutionary relationships and uncovers novel\ncorrespondences, one of which was independently validated experimentally. Thus,\nour framework offers a principled, scalable, symmetric, and interpretable\nsolution for evolutionary cell type mapping, facilitating deeper insights into\ncellular specialization and conservation across species."}
{"id": "2505.24729", "pdf": "https://arxiv.org/pdf/2505.24729", "abs": "https://arxiv.org/abs/2505.24729", "authors": ["Magamed Taimeskhanov", "Damien Garreau"], "title": "Feature Attribution from First Principles", "categories": ["cs.LG"], "comment": "30 pages, 3 figures", "summary": "Feature attribution methods are a popular approach to explain the behavior of\nmachine learning models. They assign importance scores to each input feature,\nquantifying their influence on the model's prediction. However, evaluating\nthese methods empirically remains a significant challenge. To bypass this\nshortcoming, several prior works have proposed axiomatic frameworks that any\nfeature attribution method should satisfy. In this work, we argue that such\naxioms are often too restrictive, and propose in response a new feature\nattribution framework, built from the ground up. Rather than imposing axioms,\nwe start by defining attributions for the simplest possible models, i.e.,\nindicator functions, and use these as building blocks for more complex models.\nWe then show that one recovers several existing attribution methods, depending\non the choice of atomic attribution. Subsequently, we derive closed-form\nexpressions for attribution of deep ReLU networks, and take a step toward the\noptimization of evaluation metrics with respect to feature attributions."}
{"id": "2505.24646", "pdf": "https://arxiv.org/pdf/2505.24646", "abs": "https://arxiv.org/abs/2505.24646", "authors": ["Yiqun Sun", "Qiang Huang", "Anthony K. H. Tung", "Jun Yu"], "title": "PRISM: A Framework for Producing Interpretable Political Bias Embeddings with Political-Aware Cross-Encoder", "categories": ["cs.CL"], "comment": "Accepted to ACL 2025", "summary": "Semantic Text Embedding is a fundamental NLP task that encodes textual\ncontent into vector representations, where proximity in the embedding space\nreflects semantic similarity. While existing embedding models excel at\ncapturing general meaning, they often overlook ideological nuances, limiting\ntheir effectiveness in tasks that require an understanding of political bias.\nTo address this gap, we introduce PRISM, the first framework designed to\nProduce inteRpretable polItical biaS eMbeddings. PRISM operates in two key\nstages: (1) Controversial Topic Bias Indicator Mining, which systematically\nextracts fine-grained political topics and their corresponding bias indicators\nfrom weakly labeled news data, and (2) Cross-Encoder Political Bias Embedding,\nwhich assigns structured bias scores to news articles based on their alignment\nwith these indicators. This approach ensures that embeddings are explicitly\ntied to bias-revealing dimensions, enhancing both interpretability and\npredictive power. Through extensive experiments on two large-scale datasets, we\ndemonstrate that PRISM outperforms state-of-the-art text embedding models in\npolitical bias classification while offering highly interpretable\nrepresentations that facilitate diversified retrieval and ideological analysis.\nThe source code is available at https://github.com/dukesun99/ACL-PRISM."}
{"id": "2505.24867", "pdf": "https://arxiv.org/pdf/2505.24867", "abs": "https://arxiv.org/abs/2505.24867", "authors": ["Ujjwal Upadhyay", "Mukul Ranjan", "Zhiqiang Shen", "Mohamed Elhoseiny"], "title": "Time Blindness: Why Video-Language Models Can't See What Humans Can?", "categories": ["cs.CV", "cs.AI"], "comment": "Project page at https://timeblindness.github.io/", "summary": "Recent advances in vision-language models (VLMs) have made impressive strides\nin understanding spatio-temporal relationships in videos. However, when spatial\ninformation is obscured, these models struggle to capture purely temporal\npatterns. We introduce $\\textbf{SpookyBench}$, a benchmark where information is\nencoded solely in temporal sequences of noise-like frames, mirroring natural\nphenomena from biological signaling to covert communication. Interestingly,\nwhile humans can recognize shapes, text, and patterns in these sequences with\nover 98% accuracy, state-of-the-art VLMs achieve 0% accuracy. This performance\ngap highlights a critical limitation: an over-reliance on frame-level spatial\nfeatures and an inability to extract meaning from temporal cues. Furthermore,\nwhen trained in data sets with low spatial signal-to-noise ratios (SNR),\ntemporal understanding of models degrades more rapidly than human perception,\nespecially in tasks requiring fine-grained temporal reasoning. Overcoming this\nlimitation will require novel architectures or training paradigms that decouple\nspatial dependencies from temporal processing. Our systematic analysis shows\nthat this issue persists across model scales and architectures. We release\nSpookyBench to catalyze research in temporal pattern recognition and bridge the\ngap between human and machine video understanding. Dataset and code has been\nmade available on our project website: https://timeblindness.github.io/."}
{"id": "2505.24760", "pdf": "https://arxiv.org/pdf/2505.24760", "abs": "https://arxiv.org/abs/2505.24760", "authors": ["Zafir Stojanovski", "Oliver Stanley", "Joe Sharratt", "Richard Jones", "Abdulhakeem Adefioye", "Jean Kaddour", "Andreas KÃ¶pf"], "title": "REASONING GYM: Reasoning Environments for Reinforcement Learning with Verifiable Rewards", "categories": ["cs.LG", "cs.AI", "cs.CL"], "comment": "For code, see https://github.com/open-thought/reasoning-gym", "summary": "We introduce Reasoning Gym (RG), a library of reasoning environments for\nreinforcement learning with verifiable rewards. It provides over 100 data\ngenerators and verifiers spanning multiple domains including algebra,\narithmetic, computation, cognition, geometry, graph theory, logic, and various\ncommon games. Its key innovation is the ability to generate virtually infinite\ntraining data with adjustable complexity, unlike most previous reasoning\ndatasets, which are typically fixed. This procedural generation approach allows\nfor continuous evaluation across varying difficulty levels. Our experimental\nresults demonstrate the efficacy of RG in both evaluating and reinforcement\nlearning of reasoning models."}
{"id": "2505.24737", "pdf": "https://arxiv.org/pdf/2505.24737", "abs": "https://arxiv.org/abs/2505.24737", "authors": ["Erchi Wang", "Yuqing Zhu", "Yu-Xiang Wang"], "title": "Adapting to Linear Separable Subsets with Large-Margin in Differentially Private Learning", "categories": ["cs.LG", "stat.ML"], "comment": null, "summary": "This paper studies the problem of differentially private empirical risk\nminimization (DP-ERM) for binary linear classification. We obtain an efficient\n$(\\varepsilon,\\delta)$-DP algorithm with an empirical zero-one risk bound of\n$\\tilde{O}\\left(\\frac{1}{\\gamma^2\\varepsilon n} +\n\\frac{|S_{\\mathrm{out}}|}{\\gamma n}\\right)$ where $n$ is the number of data\npoints, $S_{\\mathrm{out}}$ is an arbitrary subset of data one can remove and\n$\\gamma$ is the margin of linear separation of the remaining data points (after\n$S_{\\mathrm{out}}$ is removed). Here, $\\tilde{O}(\\cdot)$ hides only logarithmic\nterms. In the agnostic case, we improve the existing results when the number of\noutliers is small. Our algorithm is highly adaptive because it does not require\nknowing the margin parameter $\\gamma$ or outlier subset $S_{\\mathrm{out}}$. We\nalso derive a utility bound for the advanced private hyperparameter tuning\nalgorithm."}
{"id": "2505.24672", "pdf": "https://arxiv.org/pdf/2505.24672", "abs": "https://arxiv.org/abs/2505.24672", "authors": ["Xiaorui Wu", "Xiaofeng Mao", "Fei Li", "Xin Zhang", "Xuanhong Li", "Chong Teng", "Donghong Ji", "Zhuang Li"], "title": "TRIDENT: Enhancing Large Language Model Safety with Tri-Dimensional Diversified Red-Teaming Data Synthesis", "categories": ["cs.CL"], "comment": null, "summary": "Large Language Models (LLMs) excel in various natural language processing\ntasks but remain vulnerable to generating harmful content or being exploited\nfor malicious purposes. Although safety alignment datasets have been introduced\nto mitigate such risks through supervised fine-tuning (SFT), these datasets\noften lack comprehensive risk coverage. Most existing datasets focus primarily\non lexical diversity while neglecting other critical dimensions. To address\nthis limitation, we propose a novel analysis framework to systematically\nmeasure the risk coverage of alignment datasets across three essential\ndimensions: Lexical Diversity, Malicious Intent, and Jailbreak Tactics. We\nfurther introduce TRIDENT, an automated pipeline that leverages persona-based,\nzero-shot LLM generation to produce diverse and comprehensive instructions\nspanning these dimensions. Each harmful instruction is paired with an ethically\naligned response, resulting in two datasets: TRIDENT-Core, comprising 26,311\nexamples, and TRIDENT-Edge, with 18,773 examples. Fine-tuning Llama 3.1-8B on\nTRIDENT-Edge demonstrates substantial improvements, achieving an average 14.29%\nreduction in Harm Score, and a 20% decrease in Attack Success Rate compared to\nthe best-performing baseline model fine-tuned on the WildBreak dataset."}
{"id": "2505.24869", "pdf": "https://arxiv.org/pdf/2505.24869", "abs": "https://arxiv.org/abs/2505.24869", "authors": ["Ce Zhang", "Yan-Bo Lin", "Ziyang Wang", "Mohit Bansal", "Gedas Bertasius"], "title": "SiLVR: A Simple Language-based Video Reasoning Framework", "categories": ["cs.CV"], "comment": null, "summary": "Recent advances in test-time optimization have led to remarkable reasoning\ncapabilities in Large Language Models (LLMs), enabling them to solve highly\ncomplex problems in math and coding. However, the reasoning capabilities of\nmultimodal LLMs (MLLMs) still significantly lag, especially for complex\nvideo-language tasks. To address this issue, we present SiLVR, a Simple\nLanguage-based Video Reasoning framework that decomposes complex video\nunderstanding into two stages. In the first stage, SiLVR transforms raw video\ninto language-based representations using multisensory inputs, such as short\nclip captions and audio/speech subtitles. In the second stage, language\ndescriptions are fed into a powerful reasoning LLM to solve complex\nvideo-language understanding tasks. To handle long-context multisensory inputs,\nwe use an adaptive token reduction scheme, which dynamically determines the\ntemporal granularity with which to sample the tokens. Our simple, modular, and\ntraining-free video reasoning framework achieves the best-reported results on\nVideo-MME (long), Video-MMMU (comprehension), Video-MMLU, CGBench, and EgoLife.\nFurthermore, our empirical study focused on video reasoning capabilities shows\nthat, despite not being explicitly trained on video, strong reasoning LLMs can\neffectively aggregate multisensory input information from video, speech, and\naudio for complex temporal, causal, long-context, and knowledge acquisition\nreasoning tasks in video. Code is available at https://github.com/CeeZh/SILVR."}
{"id": "2505.24765", "pdf": "https://arxiv.org/pdf/2505.24765", "abs": "https://arxiv.org/abs/2505.24765", "authors": ["Srikanth Thudumu", "Jason Fisher", "Hung Du"], "title": "Supervised Quantum Machine Learning: A Future Outlook from Qubits to Enterprise Applications", "categories": ["quant-ph", "cs.AI"], "comment": "Future outlook and roadmap of QML with 7 pages and 1 figure", "summary": "Supervised Quantum Machine Learning (QML) represents an intersection of\nquantum computing and classical machine learning, aiming to use quantum\nresources to support model training and inference. This paper reviews recent\ndevelopments in supervised QML, focusing on methods such as variational quantum\ncircuits, quantum neural networks, and quantum kernel methods, along with\nhybrid quantum-classical workflows. We examine recent experimental studies that\nshow partial indications of quantum advantage and describe current limitations\nincluding noise, barren plateaus, scalability issues, and the lack of formal\nproofs of performance improvement over classical methods. The main contribution\nis a ten-year outlook (2025-2035) that outlines possible developments in\nsupervised QML, including a roadmap describing conditions under which QML may\nbe used in applied research and enterprise systems over the next decade."}
{"id": "2505.24749", "pdf": "https://arxiv.org/pdf/2505.24749", "abs": "https://arxiv.org/abs/2505.24749", "authors": ["Yehonathan Refael", "Guy Smorodinsky", "Tom Tirer", "Ofir Lindenbaum"], "title": "SUMO: Subspace-Aware Moment-Orthogonalization for Accelerating Memory-Efficient LLM Training", "categories": ["cs.LG", "cs.CL", "math.OC"], "comment": null, "summary": "Low-rank gradient-based optimization methods have significantly improved\nmemory efficiency during the training of large language models (LLMs), enabling\noperations within constrained hardware without sacrificing performance.\nHowever, these methods primarily emphasize memory savings, often overlooking\npotential acceleration in convergence due to their reliance on standard\nisotropic steepest descent techniques, which can perform suboptimally in the\nhighly anisotropic landscapes typical of deep networks, particularly LLMs. In\nthis paper, we propose SUMO (Subspace-Aware Moment-Orthogonalization), an\noptimizer that employs exact singular value decomposition (SVD) for moment\northogonalization within a dynamically adapted low-dimensional subspace,\nenabling norm-inducing steepest descent optimization steps. By explicitly\naligning optimization steps with the spectral characteristics of the loss\nlandscape, SUMO effectively mitigates approximation errors associated with\ncommonly used methods like Newton-Schulz orthogonalization approximation. We\ntheoretically establish an upper bound on these approximation errors, proving\ntheir dependence on the condition numbers of moments, conditions we\nanalytically demonstrate are encountered during LLM training. Furthermore, we\nboth theoretically and empirically illustrate that exact orthogonalization via\nSVD substantially improves convergence rates while reducing overall complexity.\nEmpirical evaluations confirm that SUMO accelerates convergence, enhances\nstability, improves performance, and reduces memory requirements by up to 20%\ncompared to state-of-the-art methods."}
{"id": "2505.24680", "pdf": "https://arxiv.org/pdf/2505.24680", "abs": "https://arxiv.org/abs/2505.24680", "authors": ["Xinrui Chen", "Haoli Bai", "Tao Yuan", "Ruikang Liu", "Kang Zhao", "Xianzhi Yu", "Lu Hou", "Tian Guan", "Yonghong He", "Chun Yuan"], "title": "A Simple Linear Patch Revives Layer-Pruned Large Language Models", "categories": ["cs.CL"], "comment": null, "summary": "Layer pruning has become a popular technique for compressing large language\nmodels (LLMs) due to its simplicity. However, existing layer pruning methods\noften suffer from significant performance drops. We identify that this\ndegradation stems from the mismatch of activation magnitudes across layers and\ntokens at the pruning interface. To address this, we propose LinearPatch, a\nsimple plug-and-play technique to revive the layer-pruned LLMs. The proposed\nmethod adopts Hadamard transformation to suppress massive outliers in\nparticular tokens, and channel-wise scaling to align the activation magnitudes.\nThese operations can be fused into a single matrix, which functions as a patch\nto bridge the pruning interface with negligible inference overhead. LinearPatch\nretains up to 94.15% performance of the original model when pruning 5 layers of\nLLaMA-3-8B on the question answering benchmark, surpassing existing\nstate-of-the-art methods by 4%. In addition, the patch matrix can be further\noptimized with memory efficient offline knowledge distillation. With only 5K\nsamples, the retained performance of LinearPatch can be further boosted to\n95.16% within 30 minutes on a single computing card."}
{"id": "2505.24870", "pdf": "https://arxiv.org/pdf/2505.24870", "abs": "https://arxiv.org/abs/2505.24870", "authors": ["Zehan Wang", "Jiayang Xu", "Ziang Zhang", "Tianyu Pan", "Chao Du", "Hengshuang Zhao", "Zhou Zhao"], "title": "GenSpace: Benchmarking Spatially-Aware Image Generation", "categories": ["cs.CV"], "comment": null, "summary": "Humans can intuitively compose and arrange scenes in the 3D space for\nphotography. However, can advanced AI image generators plan scenes with similar\n3D spatial awareness when creating images from text or image prompts? We\npresent GenSpace, a novel benchmark and evaluation pipeline to comprehensively\nassess the spatial awareness of current image generation models. Furthermore,\nstandard evaluations using general Vision-Language Models (VLMs) frequently\nfail to capture the detailed spatial errors. To handle this challenge, we\npropose a specialized evaluation pipeline and metric, which reconstructs 3D\nscene geometry using multiple visual foundation models and provides a more\naccurate and human-aligned metric of spatial faithfulness. Our findings show\nthat while AI models create visually appealing images and can follow general\ninstructions, they struggle with specific 3D details like object placement,\nrelationships, and measurements. We summarize three core limitations in the\nspatial perception of current state-of-the-art image generation models: 1)\nObject Perspective Understanding, 2) Egocentric-Allocentric Transformation and\n3) Metric Measurement Adherence, highlighting possible directions for improving\nspatial intelligence in image generation."}
{"id": "2505.24767", "pdf": "https://arxiv.org/pdf/2505.24767", "abs": "https://arxiv.org/abs/2505.24767", "authors": ["Nabasmita Talukdar", "Xiaodan Zhang", "Shreya Paithankar", "Hui Wang", "Bin Chen"], "title": "A survey of using EHR as real-world evidence for discovering and validating new drug indications", "categories": ["stat.AP", "cs.AI"], "comment": null, "summary": "Electronic Health Records (EHRs) have been increasingly used as real-world\nevidence (RWE) to support the discovery and validation of new drug indications.\nThis paper surveys current approaches to EHR-based drug repurposing, covering\ndata sources, processing methodologies, and representation techniques. It\ndiscusses study designs and statistical frameworks for evaluating drug\nefficacy. Key challenges in validation are discussed, with emphasis on the role\nof large language models (LLMs) and target trial emulation. By synthesizing\nrecent developments and methodological advances, this work provides a\nfoundational resource for researchers aiming to translate real-world data into\nactionable drug-repurposing evidence."}
{"id": "2505.24773", "pdf": "https://arxiv.org/pdf/2505.24773", "abs": "https://arxiv.org/abs/2505.24773", "authors": ["Yajie Zhou", "Xiaoyi Pang", "Zhibo Wang"], "title": "AFLoRA: Adaptive Federated Fine-Tuning of Large Language Models with Resource-Aware Low-Rank Adaption", "categories": ["cs.LG"], "comment": null, "summary": "Federated fine-tuning has emerged as a promising approach to adapt foundation\nmodels to downstream tasks using decentralized data. However, real-world\ndeployment remains challenging due to the high computational and communication\ndemands of fine-tuning Large Language Models (LLMs) on clients with data and\nsystem resources that are heterogeneous and constrained. In such settings, the\nglobal model's performance is often bottlenecked by the weakest clients and\nfurther degraded by the non-IID nature of local data. Although existing methods\nleverage parameter-efficient techniques such as Low-Rank Adaptation (LoRA) to\nreduce communication and computation overhead, they often fail to\nsimultaneously ensure accurate aggregation of low-rank updates and maintain low\nsystem costs, thereby hindering overall performance. To address these\nchallenges, we propose AFLoRA, an adaptive and lightweight federated\nfine-tuning framework for LLMs. AFLoRA decouples shared and client-specific\nupdates to reduce overhead and improve aggregation accuracy, incorporates\ndiagonal matrix-based rank pruning to better utilize local resources, and\nemploys rank-aware aggregation with public data refinement to strengthen\ngeneralization under data heterogeneity. Extensive experiments demonstrate that\nAFLoRA outperforms state-of-the-art methods in both accuracy and efficiency,\nproviding a practical solution for efficient LLM adaptation in heterogeneous\nenvironments in the real world."}
{"id": "2505.24688", "pdf": "https://arxiv.org/pdf/2505.24688", "abs": "https://arxiv.org/abs/2505.24688", "authors": ["Qinglin Zhu", "Runcong Zhao", "Hanqi Yan", "Yulan He", "Yudong Chen", "Lin Gui"], "title": "Soft Reasoning: Navigating Solution Spaces in Large Language Models through Controlled Embedding Exploration", "categories": ["cs.CL"], "comment": "Accepted by ICML 2025", "summary": "Large Language Models (LLMs) struggle with complex reasoning due to limited\ndiversity and inefficient search. We propose Soft Reasoning, an embedding-based\nsearch framework that optimises the embedding of the first token to guide\ngeneration. It combines (1) embedding perturbation for controlled exploration\nand (2) Bayesian optimisation to refine embeddings via a verifier-guided\nobjective, balancing exploration and exploitation. This approach improves\nreasoning accuracy and coherence while avoiding reliance on heuristic search.\nExperiments demonstrate superior correctness with minimal computation, making\nit a scalable, model-agnostic solution."}
{"id": "2505.24871", "pdf": "https://arxiv.org/pdf/2505.24871", "abs": "https://arxiv.org/abs/2505.24871", "authors": ["Yiqing Liang", "Jielin Qiu", "Wenhao Ding", "Zuxin Liu", "James Tompkin", "Mengdi Xu", "Mengzhou Xia", "Zhengzhong Tu", "Laixi Shi", "Jiacheng Zhu"], "title": "MoDoMoDo: Multi-Domain Data Mixtures for Multimodal LLM Reinforcement Learning", "categories": ["cs.CV", "cs.CL", "cs.LG"], "comment": "Project Webpage: https://modomodo-rl.github.io/", "summary": "Reinforcement Learning with Verifiable Rewards (RLVR) has recently emerged as\na powerful paradigm for post-training large language models (LLMs), achieving\nstate-of-the-art performance on tasks with structured, verifiable answers.\nApplying RLVR to Multimodal LLMs (MLLMs) presents significant opportunities but\nis complicated by the broader, heterogeneous nature of vision-language tasks\nthat demand nuanced visual, logical, and spatial capabilities. As such,\ntraining MLLMs using RLVR on multiple datasets could be beneficial but creates\nchallenges with conflicting objectives from interaction among diverse datasets,\nhighlighting the need for optimal dataset mixture strategies to improve\ngeneralization and reasoning. We introduce a systematic post-training framework\nfor Multimodal LLM RLVR, featuring a rigorous data mixture problem formulation\nand benchmark implementation. Specifically, (1) We developed a multimodal RLVR\nframework for multi-dataset post-training by curating a dataset that contains\ndifferent verifiable vision-language problems and enabling multi-domain online\nRL learning with different verifiable rewards; (2) We proposed a data mixture\nstrategy that learns to predict the RL fine-tuning outcome from the data\nmixture distribution, and consequently optimizes the best mixture.\nComprehensive experiments showcase that multi-domain RLVR training, when\ncombined with mixture prediction strategies, can significantly boost MLLM\ngeneral reasoning capacities. Our best mixture improves the post-trained\nmodel's accuracy on out-of-distribution benchmarks by an average of 5.24%\ncompared to the same model post-trained with uniform data mixture, and by a\ntotal of 20.74% compared to the pre-finetuning baseline."}
{"id": "2505.24786", "pdf": "https://arxiv.org/pdf/2505.24786", "abs": "https://arxiv.org/abs/2505.24786", "authors": ["Eran Bamani Beeri", "Eden Nissinman", "Avishai Sintov"], "title": "DiG-Net: Enhancing Quality of Life through Hyper-Range Dynamic Gesture Recognition in Assistive Robotics", "categories": ["cs.RO", "cs.AI", "cs.CV"], "comment": "arXiv admin note: substantial text overlap with arXiv:2411.18413", "summary": "Dynamic hand gestures play a pivotal role in assistive human-robot\ninteraction (HRI), facilitating intuitive, non-verbal communication,\nparticularly for individuals with mobility constraints or those operating\nrobots remotely. Current gesture recognition methods are mostly limited to\nshort-range interactions, reducing their utility in scenarios demanding robust\nassistive communication from afar. In this paper, we introduce a novel approach\ndesigned specifically for assistive robotics, enabling dynamic gesture\nrecognition at extended distances of up to 30 meters, thereby significantly\nimproving accessibility and quality of life. Our proposed Distance-aware\nGesture Network (DiG-Net) effectively combines Depth-Conditioned Deformable\nAlignment (DADA) blocks with Spatio-Temporal Graph modules, enabling robust\nprocessing and classification of gesture sequences captured under challenging\nconditions, including significant physical attenuation, reduced resolution, and\ndynamic gesture variations commonly experienced in real-world assistive\nenvironments. We further introduce the Radiometric Spatio-Temporal Depth\nAttenuation Loss (RSTDAL), shown to enhance learning and strengthen model\nrobustness across varying distances. Our model demonstrates significant\nperformance improvement over state-of-the-art gesture recognition frameworks,\nachieving a recognition accuracy of 97.3% on a diverse dataset with challenging\nhyper-range gestures. By effectively interpreting gestures from considerable\ndistances, DiG-Net significantly enhances the usability of assistive robots in\nhome healthcare, industrial safety, and remote assistance scenarios, enabling\nseamless and intuitive interactions for users regardless of physical\nlimitations"}
{"id": "2505.24776", "pdf": "https://arxiv.org/pdf/2505.24776", "abs": "https://arxiv.org/abs/2505.24776", "authors": ["Zachary Bastiani", "Robert M. Kirby", "Jacob Hochhalter", "Shandian Zhe"], "title": "Diffusion-Based Symbolic Regression", "categories": ["cs.LG"], "comment": null, "summary": "Diffusion has emerged as a powerful framework for generative modeling,\nachieving remarkable success in applications such as image and audio synthesis.\nEnlightened by this progress, we propose a novel diffusion-based approach for\nsymbolic regression. We construct a random mask-based diffusion and denoising\nprocess to generate diverse and high-quality equations. We integrate this\ngenerative processes with a token-wise Group Relative Policy Optimization\n(GRPO) method to conduct efficient reinforcement learning on the given\nmeasurement dataset. In addition, we introduce a long short-term risk-seeking\npolicy to expand the pool of top-performing candidates, further enhancing\nperformance. Extensive experiments and ablation studies have demonstrated the\neffectiveness of our approach."}
{"id": "2505.24689", "pdf": "https://arxiv.org/pdf/2505.24689", "abs": "https://arxiv.org/abs/2505.24689", "authors": ["Sander Land", "Catherine Arnett"], "title": "BPE Stays on SCRIPT: Structured Encoding for Robust Multilingual Pretokenization", "categories": ["cs.CL"], "comment": "9 pages, 2 figures. For associated code, see\n  https://github.com/sanderland/script_bpe", "summary": "Byte Pair Encoding (BPE) tokenizers, widely used in Large Language Models,\nface challenges in multilingual settings, including penalization of non-Western\nscripts and the creation of tokens with partial UTF-8 sequences.\nPretokenization, often reliant on complex regular expressions, can also\nintroduce fragility and unexpected edge cases. We propose SCRIPT (Script\nCategory Representation in PreTokenization), a novel encoding scheme that\nbypasses UTF-8 byte conversion by using initial tokens based on Unicode script\nand category properties. This approach enables a simple, rule-based\npretokenization strategy that respects script boundaries, offering a robust\nalternative to pretokenization strategies based on regular expressions. We also\nintroduce and validate a constrained BPE merging strategy that enforces\ncharacter integrity, applicable to both SCRIPT-BPE and byte-based BPE. Our\nexperiments demonstrate that SCRIPT-BPE achieves competitive compression while\neliminating encoding-based penalties for non-Latin-script languages."}
{"id": "2505.24872", "pdf": "https://arxiv.org/pdf/2505.24872", "abs": "https://arxiv.org/abs/2505.24872", "authors": ["Zilin Xiao", "Jaywon Koo", "Siru Ouyang", "Jefferson Hernandez", "Yu Meng", "Vicente Ordonez"], "title": "ProxyThinker: Test-Time Guidance through Small Visual Reasoners", "categories": ["cs.CV", "cs.AI", "cs.CL"], "comment": null, "summary": "Recent advancements in reinforcement learning with verifiable rewards have\npushed the boundaries of the visual reasoning capabilities in large\nvision-language models (LVLMs). However, training LVLMs with reinforcement\nfine-tuning (RFT) is computationally expensive, posing a significant challenge\nto scaling model size. In this work, we propose ProxyThinker, an inference-time\ntechnique that enables large models to inherit the visual reasoning\ncapabilities from small, slow-thinking visual reasoners without any training.\nBy subtracting the output distributions of base models from those of RFT\nreasoners, ProxyThinker modifies the decoding dynamics and successfully elicits\nthe slow-thinking reasoning demonstrated by the emerged sophisticated behaviors\nsuch as self-verification and self-correction. ProxyThinker consistently boosts\nperformance on challenging visual benchmarks on spatial, mathematical, and\nmulti-disciplinary reasoning, enabling untuned base models to compete with the\nperformance of their full-scale RFT counterparts. Furthermore, our\nimplementation efficiently coordinates multiple language models with\nparallelism techniques and achieves up to 38 $\\times$ faster inference compared\nto previous decoding-time methods, paving the way for the practical deployment\nof ProxyThinker. Code is available at\nhttps://github.com/MrZilinXiao/ProxyThinker."}
{"id": "2505.24788", "pdf": "https://arxiv.org/pdf/2505.24788", "abs": "https://arxiv.org/abs/2505.24788", "authors": ["Houjun Liu", "John Bauer", "Christopher D. Manning"], "title": "Drop Dropout on Single-Epoch Language Model Pretraining", "categories": ["cs.CL", "cs.AI"], "comment": "Accepted to ACL Findings; 5 pages, 2 figures, 4 pages of appendix", "summary": "Originally, dropout was seen as a breakthrough regularization technique that\nreduced overfitting and improved performance in almost all applications of deep\nlearning by reducing overfitting. Yet, single-epoch pretraining tasks common to\nmodern LLMs yield minimal overfitting, leading to dropout not being used for\nlarge LLMs. Nevertheless, no thorough empirical investigation has been done on\nthe role of dropout in LM pretraining. Through experiments in single-epoch\npretraining of both masked (BERT) and autoregressive (Pythia 160M and 1.4B) LMs\nwith varying levels of dropout, we find that downstream performance in language\nmodeling, morpho-syntax (BLiMP), question answering (SQuAD), and\nnatural-language inference (MNLI) improves when dropout is not applied during\npretraining. We additionally find that the recently-introduced \"early dropout\"\nalso degrades performance over applying no dropout at all. We further\ninvestigate the models' editability, and find that models trained without\ndropout are more successful in gradient-based model editing (MEND) and\nequivalent in representation-based model editing (ReFT). Therefore, we advocate\nto drop dropout during single-epoch pretraining."}
{"id": "2505.24779", "pdf": "https://arxiv.org/pdf/2505.24779", "abs": "https://arxiv.org/abs/2505.24779", "authors": ["Yidong Luo", "Chenguang Wang", "Jiahao Yang", "Fanzeng Xia", "Tianshu Yu"], "title": "EVA-MILP: Towards Standardized Evaluation of MILP Instance Generation", "categories": ["cs.LG"], "comment": "The code is available in\n  \\url{https://github.com/anonymous-neurips-submission-2025/EVA-MILP}", "summary": "Mixed-Integer Linear Programming (MILP) is fundamental to solving complex\ndecision-making problems. The proliferation of MILP instance generation\nmethods, driven by machine learning's demand for diverse optimization datasets\nand the limitations of static benchmarks, has significantly outpaced\nstandardized evaluation techniques. Consequently, assessing the fidelity and\nutility of synthetic MILP instances remains a critical, multifaceted challenge.\nThis paper introduces a comprehensive benchmark framework designed for the\nsystematic and objective evaluation of MILP instance generation methods. Our\nframework provides a unified and extensible methodology, assessing instance\nquality across crucial dimensions: mathematical validity, structural\nsimilarity, computational hardness, and utility in downstream machine learning\ntasks. A key innovation is its in-depth analysis of solver-internal features --\nparticularly by comparing distributions of key solver outputs including root\nnode gap, heuristic success rates, and cut plane usage -- leveraging the\nsolver's dynamic solution behavior as an `expert assessment' to reveal nuanced\ncomputational resemblances. By offering a structured approach with clearly\ndefined solver-independent and solver-dependent metrics, our benchmark aims to\nfacilitate robust comparisons among diverse generation techniques, spur the\ndevelopment of higher-quality instance generators, and ultimately enhance the\nreliability of research reliant on synthetic MILP data. The framework's\neffectiveness in systematically comparing the fidelity of instance sets is\ndemonstrated using contemporary generative models."}
{"id": "2505.24712", "pdf": "https://arxiv.org/pdf/2505.24712", "abs": "https://arxiv.org/abs/2505.24712", "authors": ["Guido Ivetta", "Marcos J. Gomez", "SofÃ­a Martinelli", "Pietro Palombini", "M. Emilia Echeveste", "Nair Carolina Mazzeo", "Beatriz Busaniche", "Luciana Benotti"], "title": "HESEIA: A community-based dataset for evaluating social biases in large language models, co-designed in real school settings in Latin America", "categories": ["cs.CL", "cs.CY"], "comment": null, "summary": "Most resources for evaluating social biases in Large Language Models are\ndeveloped without co-design from the communities affected by these biases, and\nrarely involve participatory approaches. We introduce HESEIA, a dataset of\n46,499 sentences created in a professional development course. The course\ninvolved 370 high-school teachers and 5,370 students from 189 Latin-American\nschools. Unlike existing benchmarks, HESEIA captures intersectional biases\nacross multiple demographic axes and school subjects. It reflects local\ncontexts through the lived experience and pedagogical expertise of educators.\nTeachers used minimal pairs to create sentences that express stereotypes\nrelevant to their school subjects and communities. We show the dataset\ndiversity in term of demographic axes represented and also in terms of the\nknowledge areas included. We demonstrate that the dataset contains more\nstereotypes unrecognized by current LLMs than previous datasets. HESEIA is\navailable to support bias assessments grounded in educational communities."}
{"id": "2505.24873", "pdf": "https://arxiv.org/pdf/2505.24873", "abs": "https://arxiv.org/abs/2505.24873", "authors": ["Bojia Zi", "Weixuan Peng", "Xianbiao Qi", "Jianan Wang", "Shihao Zhao", "Rong Xiao", "Kam-Fai Wong"], "title": "MiniMax-Remover: Taming Bad Noise Helps Video Object Removal", "categories": ["cs.CV"], "comment": null, "summary": "Recent advances in video diffusion models have driven rapid progress in video\nediting techniques. However, video object removal, a critical subtask of video\nediting, remains challenging due to issues such as hallucinated objects and\nvisual artifacts. Furthermore, existing methods often rely on computationally\nexpensive sampling procedures and classifier-free guidance (CFG), resulting in\nslow inference. To address these limitations, we propose MiniMax-Remover, a\nnovel two-stage video object removal approach. Motivated by the observation\nthat text condition is not best suited for this task, we simplify the\npretrained video generation model by removing textual input and cross-attention\nlayers, resulting in a more lightweight and efficient model architecture in the\nfirst stage. In the second stage, we distilled our remover on successful videos\nproduced by the stage-1 model and curated by human annotators, using a minimax\noptimization strategy to further improve editing quality and inference speed.\nSpecifically, the inner maximization identifies adversarial input noise (\"bad\nnoise\") that makes failure removals, while the outer minimization step trains\nthe model to generate high-quality removal results even under such challenging\nconditions. As a result, our method achieves a state-of-the-art video object\nremoval results with as few as 6 sampling steps and doesn't rely on CFG,\nsignificantly improving inference efficiency. Extensive experiments demonstrate\nthe effectiveness and superiority of MiniMax-Remover compared to existing\nmethods. Codes and Videos are available at: https://minimax-remover.github.io."}
{"id": "2505.24791", "pdf": "https://arxiv.org/pdf/2505.24791", "abs": "https://arxiv.org/abs/2505.24791", "authors": ["Jiaru Zhang", "Juanwu Lu", "Ziran Wang", "Ruqi Zhang"], "title": "Inference Acceleration of Autoregressive Normalizing Flows by Selective Jacobi Decoding", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Normalizing flows are promising generative models with advantages such as\ntheoretical rigor, analytical log-likelihood computation, and end-to-end\ntraining. However, the architectural constraints to ensure invertibility and\ntractable Jacobian computation limit their expressive power and practical\nusability. Recent advancements utilize autoregressive modeling, significantly\nenhancing expressive power and generation quality. However, such sequential\nmodeling inherently restricts parallel computation during inference, leading to\nslow generation that impedes practical deployment. In this paper, we first\nidentify that strict sequential dependency in inference is unnecessary to\ngenerate high-quality samples. We observe that patches in sequential modeling\ncan also be approximated without strictly conditioning on all preceding\npatches. Moreover, the models tend to exhibit low dependency redundancy in the\ninitial layer and higher redundancy in subsequent layers. Leveraging these\nobservations, we propose a selective Jacobi decoding (SeJD) strategy that\naccelerates autoregressive inference through parallel iterative optimization.\nTheoretical analyses demonstrate the method's superlinear convergence rate and\nguarantee that the number of iterations required is no greater than the\noriginal sequential approach. Empirical evaluations across multiple datasets\nvalidate the generality and effectiveness of our acceleration technique.\nExperiments demonstrate substantial speed improvements up to 4.7 times faster\ninference while keeping the generation quality and fidelity."}
{"id": "2505.24780", "pdf": "https://arxiv.org/pdf/2505.24780", "abs": "https://arxiv.org/abs/2505.24780", "authors": ["Run-Ze He", "Jun-Jian Su", "Su-Juan Qin", "Zheng-Ping Jin", "Fei Gao"], "title": "QGAN-based data augmentation for hybrid quantum-classical neural networks", "categories": ["cs.LG", "quant-ph"], "comment": null, "summary": "Quantum neural networks converge faster and achieve higher accuracy than\nclassical models. However, data augmentation in quantum machine learning\nremains underexplored. To tackle data scarcity, we integrate quantum generative\nadversarial networks (QGANs) with hybrid quantum-classical neural networks\n(HQCNNs) to develop an augmentation framework. We propose two strategies: a\ngeneral approach to enhance data processing and classification across HQCNNs,\nand a customized strategy that dynamically generates samples tailored to the\nHQCNN's performance on specific data categories, improving its ability to learn\nfrom complex datasets. Simulation experiments on the MNIST dataset demonstrate\nthat QGAN outperforms traditional data augmentation methods and classical GANs.\nCompared to baseline DCGAN, QGAN achieves comparable performance with half the\nparameters, balancing efficiency and effectiveness. This suggests that QGANs\ncan simplify models and generate high-quality data, enhancing HQCNN accuracy\nand performance. These findings pave the way for applying quantum data\naugmentation techniques in machine learning."}
{"id": "2505.24714", "pdf": "https://arxiv.org/pdf/2505.24714", "abs": "https://arxiv.org/abs/2505.24714", "authors": ["Junyu Luo", "Zhizhuo Kou", "Liming Yang", "Xiao Luo", "Jinsheng Huang", "Zhiping Xiao", "Jingshu Peng", "Chengzhong Liu", "Jiaming Ji", "Xuanzhe Liu", "Sirui Han", "Ming Zhang", "Yike Guo"], "title": "FinMME: Benchmark Dataset for Financial Multi-Modal Reasoning Evaluation", "categories": ["cs.CL"], "comment": "ACL 2025 Main Conference", "summary": "Multimodal Large Language Models (MLLMs) have experienced rapid development\nin recent years. However, in the financial domain, there is a notable lack of\neffective and specialized multimodal evaluation datasets. To advance the\ndevelopment of MLLMs in the finance domain, we introduce FinMME, encompassing\nmore than 11,000 high-quality financial research samples across 18 financial\ndomains and 6 asset classes, featuring 10 major chart types and 21 subtypes. We\nensure data quality through 20 annotators and carefully designed validation\nmechanisms. Additionally, we develop FinScore, an evaluation system\nincorporating hallucination penalties and multi-dimensional capability\nassessment to provide an unbiased evaluation. Extensive experimental results\ndemonstrate that even state-of-the-art models like GPT-4o exhibit\nunsatisfactory performance on FinMME, highlighting its challenging nature. The\nbenchmark exhibits high robustness with prediction variations under different\nprompts remaining below 1%, demonstrating superior reliability compared to\nexisting datasets. Our dataset and evaluation protocol are available at\nhttps://huggingface.co/datasets/luojunyu/FinMME and\nhttps://github.com/luo-junyu/FinMME."}
{"id": "2505.24875", "pdf": "https://arxiv.org/pdf/2505.24875", "abs": "https://arxiv.org/abs/2505.24875", "authors": ["Yu Zhang", "Yunqi Li", "Yifan Yang", "Rui Wang", "Yuqing Yang", "Dai Qi", "Jianmin Bao", "Dongdong Chen", "Chong Luo", "Lili Qiu"], "title": "ReasonGen-R1: CoT for Autoregressive Image generation models through SFT and RL", "categories": ["cs.CV", "cs.CL"], "comment": null, "summary": "Although chain-of-thought reasoning and reinforcement learning (RL) have\ndriven breakthroughs in NLP, their integration into generative vision models\nremains underexplored. We introduce ReasonGen-R1, a two-stage framework that\nfirst imbues an autoregressive image generator with explicit text-based\n\"thinking\" skills via supervised fine-tuning on a newly generated reasoning\ndataset of written rationales, and then refines its outputs using Group\nRelative Policy Optimization. To enable the model to reason through text before\ngenerating images, We automatically generate and release a corpus of model\ncrafted rationales paired with visual prompts, enabling controlled planning of\nobject layouts, styles, and scene compositions. Our GRPO algorithm uses reward\nsignals from a pretrained vision language model to assess overall visual\nquality, optimizing the policy in each update. Evaluations on GenEval, DPG, and\nthe T2I benchmark demonstrate that ReasonGen-R1 consistently outperforms strong\nbaselines and prior state-of-the-art models. More: aka.ms/reasongen."}
{"id": "2505.24808", "pdf": "https://arxiv.org/pdf/2505.24808", "abs": "https://arxiv.org/abs/2505.24808", "authors": ["Wenhao Ding", "Sushant Veer", "Yuxiao Chen", "Yulong Cao", "Chaowei Xiao", "Marco Pavone"], "title": "RealDrive: Retrieval-Augmented Driving with Diffusion Models", "categories": ["cs.RO", "cs.AI"], "comment": null, "summary": "Learning-based planners generate natural human-like driving behaviors by\nlearning to reason about nuanced interactions from data, overcoming the rigid\nbehaviors that arise from rule-based planners. Nonetheless, data-driven\napproaches often struggle with rare, safety-critical scenarios and offer\nlimited controllability over the generated trajectories. To address these\nchallenges, we propose RealDrive, a Retrieval-Augmented Generation (RAG)\nframework that initializes a diffusion-based planning policy by retrieving the\nmost relevant expert demonstrations from the training dataset. By interpolating\nbetween current observations and retrieved examples through a denoising\nprocess, our approach enables fine-grained control and safe behavior across\ndiverse scenarios, leveraging the strong prior provided by the retrieved\nscenario. Another key insight we produce is that a task-relevant retrieval\nmodel trained with planning-based objectives results in superior planning\nperformance in our framework compared to a task-agnostic retriever.\nExperimental results demonstrate improved generalization to long-tail events\nand enhanced trajectory diversity compared to standard learning-based planners\n-- we observe a 40% reduction in collision rate on the Waymo Open Motion\ndataset with RAG."}
{"id": "2505.24802", "pdf": "https://arxiv.org/pdf/2505.24802", "abs": "https://arxiv.org/abs/2505.24802", "authors": ["Marc GonzÃ¡lez", "Rachid Guerraoui", "Rafael Pinot", "Geovani Rizk", "John Stephan", "FranÃ§ois TaÃ¯ani"], "title": "ByzFL: Research Framework for Robust Federated Learning", "categories": ["cs.LG"], "comment": null, "summary": "We present ByzFL, an open-source Python library for developing and\nbenchmarking robust federated learning (FL) algorithms. ByzFL provides a\nunified and extensible framework that includes implementations of\nstate-of-the-art robust aggregators, a suite of configurable attacks, and tools\nfor simulating a variety of FL scenarios, including heterogeneous data\ndistributions, multiple training algorithms, and adversarial threat models. The\nlibrary enables systematic experimentation via a single JSON-based\nconfiguration file and includes built-in utilities for result visualization.\nCompatible with PyTorch tensors and NumPy arrays, ByzFL is designed to\nfacilitate reproducible research and rapid prototyping of robust FL solutions.\nByzFL is available at https://byzfl.epfl.ch/, with source code hosted on\nGitHub: https://github.com/LPD-EPFL/byzfl."}
{"id": "2505.24726", "pdf": "https://arxiv.org/pdf/2505.24726", "abs": "https://arxiv.org/abs/2505.24726", "authors": ["Shelly Bensal", "Umar Jamil", "Christopher Bryant", "Melisa Russak", "Kiran Kamble", "Dmytro Mozolevskyi", "Muayad Ali", "Waseem AlShikh"], "title": "Reflect, Retry, Reward: Self-Improving LLMs via Reinforcement Learning", "categories": ["cs.CL"], "comment": null, "summary": "We explore a method for improving the performance of large language models\nthrough self-reflection and reinforcement learning. By incentivizing the model\nto generate better self-reflections when it answers incorrectly, we demonstrate\nthat a model's ability to solve complex, verifiable tasks can be enhanced even\nwhen generating synthetic data is infeasible and only binary feedback is\navailable. Our framework operates in two stages: first, upon failing a given\ntask, the model generates a self-reflective commentary analyzing its previous\nattempt; second, the model is given another attempt at the task with the\nself-reflection in context. If the subsequent attempt succeeds, the tokens\ngenerated during the self-reflection phase are rewarded. Our experimental\nresults show substantial performance gains across a variety of model\narchitectures, as high as 34.7% improvement at math equation writing and 18.1%\nimprovement at function calling. Notably, smaller fine-tuned models (1.5\nbillion to 7 billion parameters) outperform models in the same family that are\n10 times larger. Our novel paradigm is thus an exciting pathway to more useful\nand reliable language models that can self-improve on challenging tasks with\nlimited external feedback."}
{"id": "2505.24876", "pdf": "https://arxiv.org/pdf/2505.24876", "abs": "https://arxiv.org/abs/2505.24876", "authors": ["Tajamul Ashraf", "Amal Saqib", "Hanan Ghani", "Muhra AlMahri", "Yuhao Li", "Noor Ahsan", "Umair Nawaz", "Jean Lahoud", "Hisham Cholakkal", "Mubarak Shah", "Philip Torr", "Fahad Shahbaz Khan", "Rao Muhammad Anwer", "Salman Khan"], "title": "Agent-X: Evaluating Deep Multimodal Reasoning in Vision-Centric Agentic Tasks", "categories": ["cs.CV", "cs.CL"], "comment": null, "summary": "Deep reasoning is fundamental for solving complex tasks, especially in\nvision-centric scenarios that demand sequential, multimodal understanding.\nHowever, existing benchmarks typically evaluate agents with fully synthetic,\nsingle-turn queries, limited visual modalities, and lack a framework to assess\nreasoning quality over multiple steps as required in real-world settings. To\naddress this, we introduce Agent-X, a large-scale benchmark for evaluating\nvision-centric agents multi-step and deep reasoning capabilities in real-world,\nmultimodal settings. Agent- X features 828 agentic tasks with authentic visual\ncontexts, including images, multi-image comparisons, videos, and instructional\ntext. These tasks span six major agentic environments: general visual\nreasoning, web browsing, security and surveillance, autonomous driving, sports,\nand math reasoning. Our benchmark requires agents to integrate tool use with\nexplicit, stepwise decision-making in these diverse settings. In addition, we\npropose a fine-grained, step-level evaluation framework that assesses the\ncorrectness and logical coherence of each reasoning step and the effectiveness\nof tool usage throughout the task. Our results reveal that even the\nbest-performing models, including GPT, Gemini, and Qwen families, struggle to\nsolve multi-step vision tasks, achieving less than 50% full-chain success.\nThese findings highlight key bottlenecks in current LMM reasoning and tool-use\ncapabilities and identify future research directions in vision-centric agentic\nreasoning models. Our data and code are publicly available at\nhttps://github.com/mbzuai-oryx/Agent-X"}
{"id": "2505.24823", "pdf": "https://arxiv.org/pdf/2505.24823", "abs": "https://arxiv.org/abs/2505.24823", "authors": ["Yinggan Xu", "Yue Liu", "Zhiqiang Gao", "Changnan Peng", "Di Luo"], "title": "PhySense: Principle-Based Physics Reasoning Benchmarking for Large Language Models", "categories": ["cs.LG", "cs.AI", "cs.CL"], "comment": null, "summary": "Large language models (LLMs) have rapidly advanced and are increasingly\ncapable of tackling complex scientific problems, including those in physics.\nDespite this progress, current LLMs often fail to emulate the concise,\nprinciple-based reasoning characteristic of human experts, instead generating\nlengthy and opaque solutions. This discrepancy highlights a crucial gap in\ntheir ability to apply core physical principles for efficient and interpretable\nproblem solving. To systematically investigate this limitation, we introduce\nPhySense, a novel principle-based physics reasoning benchmark designed to be\neasily solvable by experts using guiding principles, yet deceptively difficult\nfor LLMs without principle-first reasoning. Our evaluation across multiple\nstate-of-the-art LLMs and prompt types reveals a consistent failure to align\nwith expert-like reasoning paths, providing insights for developing AI systems\nwith efficient, robust and interpretable principle-based scientific reasoning."}
{"id": "2505.24835", "pdf": "https://arxiv.org/pdf/2505.24835", "abs": "https://arxiv.org/abs/2505.24835", "authors": ["Fuyuan Lyu", "Linfeng Du", "Yunpeng Weng", "Qiufang Ying", "Zhiyan Xu", "Wen Zou", "Haolun Wu", "Xiuqiang He", "Xing Tang"], "title": "Timing is important: Risk-aware Fund Allocation based on Time-Series Forecasting", "categories": ["cs.LG"], "comment": "Accepted by KDD 2025 ADS Track", "summary": "Fund allocation has been an increasingly important problem in the financial\ndomain. In reality, we aim to allocate the funds to buy certain assets within a\ncertain future period. Naive solutions such as prediction-only or\nPredict-then-Optimize approaches suffer from goal mismatch. Additionally, the\nintroduction of the SOTA time series forecasting model inevitably introduces\nadditional uncertainty in the predicted result. To solve both problems\nmentioned above, we introduce a Risk-aware Time-Series Predict-and-Allocate\n(RTS-PnO) framework, which holds no prior assumption on the forecasting models.\nSuch a framework contains three features: (i) end-to-end training with\nobjective alignment measurement, (ii) adaptive forecasting uncertainty\ncalibration, and (iii) agnostic towards forecasting models. The evaluation of\nRTS-PnO is conducted over both online and offline experiments. For offline\nexperiments, eight datasets from three categories of financial applications are\nused: Currency, Stock, and Cryptos. RTS-PnO consistently outperforms other\ncompetitive baselines. The online experiment is conducted on the Cross-Border\nPayment business at FiT, Tencent, and an 8.4\\% decrease in regret is witnessed\nwhen compared with the product-line approach. The code for the offline\nexperiment is available at https://github.com/fuyuanlyu/RTS-PnO."}
{"id": "2505.24731", "pdf": "https://arxiv.org/pdf/2505.24731", "abs": "https://arxiv.org/abs/2505.24731", "authors": ["Alan Sun"], "title": "Circuit Stability Characterizes Language Model Generalization", "categories": ["cs.CL"], "comment": "16 pages, 10 figures", "summary": "Extensively evaluating the capabilities of (large) language models is\ndifficult. Rapid development of state-of-the-art models induce benchmark\nsaturation, while creating more challenging datasets is labor-intensive.\nInspired by the recent developments in mechanistic interpretability, we\nintroduce circuit stability as a new way to assess model performance. Circuit\nstability refers to a model's ability to apply a consistent reasoning\nprocess-its circuit-across various inputs. We mathematically formalize circuit\nstability and circuit equivalence. Then, through three case studies, we\nempirically show that circuit stability and the lack thereof can characterize\nand predict different aspects of generalization. Our proposed methods offer a\nstep towards rigorously relating the generality of models to their\ninterpretability."}
{"id": "2505.24877", "pdf": "https://arxiv.org/pdf/2505.24877", "abs": "https://arxiv.org/abs/2505.24877", "authors": ["Yangyi Huang", "Ye Yuan", "Xueting Li", "Jan Kautz", "Umar Iqbal"], "title": "AdaHuman: Animatable Detailed 3D Human Generation with Compositional Multiview Diffusion", "categories": ["cs.CV"], "comment": "Website: https://nvlabs.github.io/AdaHuman", "summary": "Existing methods for image-to-3D avatar generation struggle to produce highly\ndetailed, animation-ready avatars suitable for real-world applications. We\nintroduce AdaHuman, a novel framework that generates high-fidelity animatable\n3D avatars from a single in-the-wild image. AdaHuman incorporates two key\ninnovations: (1) A pose-conditioned 3D joint diffusion model that synthesizes\nconsistent multi-view images in arbitrary poses alongside corresponding 3D\nGaussian Splats (3DGS) reconstruction at each diffusion step; (2) A\ncompositional 3DGS refinement module that enhances the details of local body\nparts through image-to-image refinement and seamlessly integrates them using a\nnovel crop-aware camera ray map, producing a cohesive detailed 3D avatar. These\ncomponents allow AdaHuman to generate highly realistic standardized A-pose\navatars with minimal self-occlusion, enabling rigging and animation with any\ninput motion. Extensive evaluation on public benchmarks and in-the-wild images\ndemonstrates that AdaHuman significantly outperforms state-of-the-art methods\nin both avatar reconstruction and reposing. Code and models will be publicly\navailable for research purposes."}
{"id": "2505.24830", "pdf": "https://arxiv.org/pdf/2505.24830", "abs": "https://arxiv.org/abs/2505.24830", "authors": ["Juraj Vladika", "Annika Domres", "Mai Nguyen", "Rebecca Moser", "Jana Nano", "Felix Busch", "Lisa C. Adams", "Keno K. Bressem", "Denise Bernhardt", "Stephanie E. Combs", "Kai J. Borm", "Florian Matthes", "Jan C. Peeken"], "title": "Improving Reliability and Explainability of Medical Question Answering through Atomic Fact Checking in Retrieval-Augmented LLMs", "categories": ["cs.CL", "cs.AI"], "comment": "11 pages, 4 figures", "summary": "Large language models (LLMs) exhibit extensive medical knowledge but are\nprone to hallucinations and inaccurate citations, which pose a challenge to\ntheir clinical adoption and regulatory compliance. Current methods, such as\nRetrieval Augmented Generation, partially address these issues by grounding\nanswers in source documents, but hallucinations and low fact-level\nexplainability persist. In this work, we introduce a novel atomic fact-checking\nframework designed to enhance the reliability and explainability of LLMs used\nin medical long-form question answering. This method decomposes LLM-generated\nresponses into discrete, verifiable units called atomic facts, each of which is\nindependently verified against an authoritative knowledge base of medical\nguidelines. This approach enables targeted correction of errors and direct\ntracing to source literature, thereby improving the factual accuracy and\nexplainability of medical Q&A. Extensive evaluation using multi-reader\nassessments by medical experts and an automated open Q&A benchmark demonstrated\nsignificant improvements in factual accuracy and explainability. Our framework\nachieved up to a 40% overall answer improvement and a 50% hallucination\ndetection rate. The ability to trace each atomic fact back to the most relevant\nchunks from the database provides a granular, transparent explanation of the\ngenerated responses, addressing a major gap in current medical AI applications.\nThis work represents a crucial step towards more trustworthy and reliable\nclinical applications of LLMs, addressing key prerequisites for clinical\napplication and fostering greater confidence in AI-assisted healthcare."}
{"id": "2505.24842", "pdf": "https://arxiv.org/pdf/2505.24842", "abs": "https://arxiv.org/abs/2505.24842", "authors": ["Harsh Chaudhari", "Jamie Hayes", "Matthew Jagielski", "Ilia Shumailov", "Milad Nasr", "Alina Oprea"], "title": "Cascading Adversarial Bias from Injection to Distillation in Language Models", "categories": ["cs.LG"], "comment": null, "summary": "Model distillation has become essential for creating smaller, deployable\nlanguage models that retain larger system capabilities. However, widespread\ndeployment raises concerns about resilience to adversarial manipulation. This\npaper investigates vulnerability of distilled models to adversarial injection\nof biased content during training. We demonstrate that adversaries can inject\nsubtle biases into teacher models through minimal data poisoning, which\npropagates to student models and becomes significantly amplified. We propose\ntwo propagation modes: Untargeted Propagation, where bias affects multiple\ntasks, and Targeted Propagation, focusing on specific tasks while maintaining\nnormal behavior elsewhere. With only 25 poisoned samples (0.25% poisoning\nrate), student models generate biased responses 76.9% of the time in targeted\nscenarios - higher than 69.4% in teacher models. For untargeted propagation,\nadversarial bias appears 6x-29x more frequently in student models on unseen\ntasks. We validate findings across six bias types (targeted advertisements,\nphishing links, narrative manipulations, insecure coding practices), various\ndistillation methods, and different modalities spanning text and code\ngeneration. Our evaluation reveals shortcomings in current defenses -\nperplexity filtering, bias detection systems, and LLM-based autorater\nframeworks - against these attacks. Results expose significant security\nvulnerabilities in distilled models, highlighting need for specialized\nsafeguards. We propose practical design principles for building effective\nadversarial bias mitigation strategies."}
{"id": "2505.24757", "pdf": "https://arxiv.org/pdf/2505.24757", "abs": "https://arxiv.org/abs/2505.24757", "authors": ["Christian Jaumann", "Andreas Wiedholz", "Annemarie Friedrich"], "title": "LGAR: Zero-Shot LLM-Guided Neural Ranking for Abstract Screening in Systematic Literature Reviews", "categories": ["cs.CL"], "comment": null, "summary": "The scientific literature is growing rapidly, making it hard to keep track of\nthe state-of-the-art. Systematic literature reviews (SLRs) aim to identify and\nevaluate all relevant papers on a topic. After retrieving a set of candidate\npapers, the abstract screening phase determines initial relevance. To date,\nabstract screening methods using large language models (LLMs) focus on binary\nclassification settings; existing question answering (QA) based ranking\napproaches suffer from error propagation. LLMs offer a unique opportunity to\nevaluate the SLR's inclusion and exclusion criteria, yet, existing benchmarks\ndo not provide them exhaustively. We manually extract these criteria as well as\nresearch questions for 57 SLRs, mostly in the medical domain, enabling\nprincipled comparisons between approaches. Moreover, we propose LGAR, a\nzero-shot LLM Guided Abstract Ranker composed of an LLM based graded relevance\nscorer and a dense re-ranker. Our extensive experiments show that LGAR\noutperforms existing QA-based methods by 5-10 pp. in mean average precision.\nOur code and data is publicly available."}
{"id": "2505.24038", "pdf": "https://arxiv.org/pdf/2505.24038", "abs": "https://arxiv.org/abs/2505.24038", "authors": ["LÃ©o AndÃ©ol", "Luca Mossina", "Adrien Mazoyer", "SÃ©bastien Gerchinovitz"], "title": "Conformal Object Detection by Sequential Risk Control", "categories": ["stat.ML", "cs.CV", "cs.LG"], "comment": "28 pages, 11 figures", "summary": "Recent advances in object detectors have led to their adoption for industrial\nuses. However, their deployment in critical applications is hindered by the\ninherent lack of reliability of neural networks and the complex structure of\nobject detection models. To address these challenges, we turn to Conformal\nPrediction, a post-hoc procedure which offers statistical guarantees that are\nvalid for any dataset size, without requiring prior knowledge on the model or\ndata distribution. Our contribution is manifold: first, we formally define the\nproblem of Conformal Object Detection (COD) and introduce a novel method,\nSequential Conformal Risk Control (SeqCRC), that extends the statistical\nguarantees of Conformal Risk Control (CRC) to two sequential tasks with two\nparameters, as required in the COD setting. Then, we propose loss functions and\nprediction sets suited to applying CRC to different applications and\ncertification requirements. Finally, we present a conformal toolkit, enabling\nreplication and further exploration of our methods. Using this toolkit, we\nperform extensive experiments, yielding a benchmark that validates the\ninvestigated methods and emphasizes trade-offs and other practical\nconsequences."}
{"id": "2505.24850", "pdf": "https://arxiv.org/pdf/2505.24850", "abs": "https://arxiv.org/abs/2505.24850", "authors": ["Shuyao Xu", "Cheng Peng", "Jiangxuan Long", "Weidi Xu", "Wei Chu", "Yuan Qi"], "title": "Harnessing Negative Signals: Reinforcement Distillation from Teacher Data for LLM Reasoning", "categories": ["cs.LG", "cs.AI", "cs.CL", "I.2.6; I.2.7"], "comment": "27 pages, 10 figures. Code available at\n  https://github.com/Tim-Siu/reinforcement-distillation", "summary": "Recent advances in model distillation demonstrate that data from advanced\nreasoning models (e.g., DeepSeek-R1, OpenAI's o1) can effectively transfer\ncomplex reasoning abilities to smaller, efficient student models. However,\nstandard practices employ rejection sampling, discarding incorrect reasoning\nexamples -- valuable, yet often underutilized data. This paper addresses the\ncritical question: How can both positive and negative distilled reasoning\ntraces be effectively leveraged to maximize LLM reasoning performance in an\noffline setting? To this end, We propose Reinforcement Distillation (REDI), a\ntwo-stage framework. Stage 1 learns from positive traces via Supervised\nFine-Tuning (SFT). Stage 2 further refines the model using both positive and\nnegative traces through our proposed REDI objective. This novel objective is a\nsimple, reference-free loss function that outperforms established methods like\nDPO and SimPO in this distillation context. Our empirical evaluations\ndemonstrate REDI's superiority over baseline Rejection Sampling SFT or SFT\ncombined with DPO/SimPO on mathematical reasoning tasks. Notably, the\nQwen-REDI-1.5B model, post-trained on just 131k positive and negative examples\nfrom the open Open-R1 dataset, achieves an 83.1% score on MATH-500 (pass@1).\nIts performance matches or surpasses that of DeepSeek-R1-Distill-Qwen-1.5B (a\nmodel post-trained on 800k proprietary data) across various mathematical\nreasoning benchmarks, establishing a new state-of-the-art for 1.5B models\npost-trained offline with openly available data."}
{"id": "2505.24843", "pdf": "https://arxiv.org/pdf/2505.24843", "abs": "https://arxiv.org/abs/2505.24843", "authors": ["Ruqi Bai", "Yao Ji", "Zeyu Zhou", "David I. Inouye"], "title": "From Invariant Representations to Invariant Data: Provable Robustness to Spurious Correlations via Noisy Counterfactual Matching", "categories": ["cs.LG"], "comment": null, "summary": "Spurious correlations can cause model performance to degrade in new\nenvironments. Prior causality-inspired works aim to learn invariant\nrepresentations (e.g., IRM) but typically underperform empirical risk\nminimization (ERM). Recent alternatives improve robustness by leveraging\ntest-time data, but such data may be unavailable in practice. To address these\nissues, we take a data-centric approach by leveraging invariant data pairs,\npairs of samples that would have the same prediction with the optimally robust\nclassifier. We prove that certain counterfactual pairs will naturally satisfy\nthis invariance property and introduce noisy counterfactual matching (NCM), a\nsimple constraint-based method for leveraging invariant pairs for enhanced\nrobustness, even with a small set of noisy pairs-in the ideal case, each pair\ncan eliminate one spurious feature. For linear causal models, we prove that the\ntest domain error can be upper bounded by the in-domain error and a term that\ndepends on the counterfactuals' diversity and quality. We validate on a\nsynthetic dataset and demonstrate on real-world benchmarks that linear probing\non a pretrained backbone improves robustness."}
{"id": "2505.24768", "pdf": "https://arxiv.org/pdf/2505.24768", "abs": "https://arxiv.org/abs/2505.24768", "authors": ["Haoyu Li", "Xuhong Li", "Yiming Dong", "Kun Liu"], "title": "From Macro to Micro: Probing Dataset Diversity in Language Model Fine-Tuning", "categories": ["cs.CL"], "comment": null, "summary": "Dataset diversity plays a pivotal role for the successful training of many\nmachine learning models, particularly in the supervised fine-tuning (SFT) stage\nof large language model (LLM) development. Despite increasing recognition of\nits importance, systematic analyses of dataset diversity still remain\nunderexplored. To address this gap, this work presents a systematic taxonomy of\nexisting diversity-control strategies, which primarily focus on the instruction\ncomponent, operating at either macroscopic (entire instruction semantics) or\nmesoscopic levels (instruction units), and furthermore introduces a novel\nanalysis of microscopic diversity within the response component, specifically\nanalyzing the statistical distribution of tokens in SFT training samples. In\nthe experimental evaluation, we construct fixed-size datasets (e.g., 10,000\nsamples each) from a corpus of 117,000 open-source SFT samples, incorporating\nsix distinct diversity-control strategies spanning macro-, meso-, and\nmicroscopic levels applied to both instructions and responses. We then\nfine-tune LLMs on these datasets to assess the six diversity-control\nstrategies. Results reveal that while macroscopic and mesoscopic strategies\nlead to higher performance with increasing diversity, the microscopic strategy\nin responses exhibits both a stronger correlation between model performance and\nthe degree of diversity and superior performance with maximum diversity across\nall strategies. These findings offer actionable insights for constructing\nhigh-performance SFT datasets."}
{"id": "2505.24053", "pdf": "https://arxiv.org/pdf/2505.24053", "abs": "https://arxiv.org/abs/2505.24053", "authors": ["Zixun Huang", "Cho-Ying Wu", "Yuliang Guo", "Xinyu Huang", "Liu Ren"], "title": "3DGEER: Exact and Efficient Volumetric Rendering with 3D Gaussians", "categories": ["cs.GR", "cs.CV"], "comment": null, "summary": "3D Gaussian Splatting (3DGS) marks a significant milestone in balancing the\nquality and efficiency of differentiable rendering. However, its high\nefficiency stems from an approximation of projecting 3D Gaussians onto the\nimage plane as 2D Gaussians, which inherently limits rendering\nquality--particularly under large Field-of-View (FoV) camera inputs. While\nseveral recent works have extended 3DGS to mitigate these approximation errors,\nnone have successfully achieved both exactness and high efficiency\nsimultaneously. In this work, we introduce 3DGEER, an Exact and Efficient\nVolumetric Gaussian Rendering method. Starting from first principles, we derive\na closed-form expression for the density integral along a ray traversing a 3D\nGaussian distribution. This formulation enables precise forward rendering with\narbitrary camera models and supports gradient-based optimization of 3D Gaussian\nparameters. To ensure both exactness and real-time performance, we propose an\nefficient method for computing a tight Particle Bounding Frustum (PBF) for each\n3D Gaussian, enabling accurate and efficient ray-Gaussian association. We also\nintroduce a novel Bipolar Equiangular Projection (BEAP) representation to\naccelerate ray association under generic camera models. BEAP further provides a\nmore uniform ray sampling strategy to apply supervision, which empirically\nimproves reconstruction quality. Experiments on multiple pinhole and fisheye\ndatasets show that our method consistently outperforms prior methods,\nestablishing a new state-of-the-art in real-time neural rendering."}
{"id": "2505.24853", "pdf": "https://arxiv.org/pdf/2505.24853", "abs": "https://arxiv.org/abs/2505.24853", "authors": ["Zhao Mandi", "Yifan Hou", "Dieter Fox", "Yashraj Narang", "Ajay Mandlekar", "Shuran Song"], "title": "DexMachina: Functional Retargeting for Bimanual Dexterous Manipulation", "categories": ["cs.RO", "cs.AI", "cs.LG"], "comment": null, "summary": "We study the problem of functional retargeting: learning dexterous\nmanipulation policies to track object states from human hand-object\ndemonstrations. We focus on long-horizon, bimanual tasks with articulated\nobjects, which is challenging due to large action space, spatiotemporal\ndiscontinuities, and embodiment gap between human and robot hands. We propose\nDexMachina, a novel curriculum-based algorithm: the key idea is to use virtual\nobject controllers with decaying strength: an object is first driven\nautomatically towards its target states, such that the policy can gradually\nlearn to take over under motion and contact guidance. We release a simulation\nbenchmark with a diverse set of tasks and dexterous hands, and show that\nDexMachina significantly outperforms baseline methods. Our algorithm and\nbenchmark enable a functional comparison for hardware designs, and we present\nkey findings informed by quantitative and qualitative results. With the recent\nsurge in dexterous hand development, we hope this work will provide a useful\nplatform for identifying desirable hardware capabilities and lower the barrier\nfor contributing to future research. Videos and more at\nhttps://project-dexmachina.github.io/"}
{"id": "2505.24844", "pdf": "https://arxiv.org/pdf/2505.24844", "abs": "https://arxiv.org/abs/2505.24844", "authors": ["Wanyun Xie", "Francesco Tonin", "Volkan Cevher"], "title": "Chameleon: A Flexible Data-mixing Framework for Language Model Pretraining and Finetuning", "categories": ["cs.LG", "cs.CL"], "comment": "ICML 2025", "summary": "Training data mixtures greatly impact the generalization performance of large\nlanguage models. Existing domain reweighting methods often rely on costly\nweight computations and require retraining when new data is introduced. To this\nend, we introduce a flexible and efficient data mixing framework, Chameleon,\nthat employs leverage scores to quantify domain importance within a learned\nembedding space. We first construct a domain affinity matrix over domain\nembeddings. The induced leverage scores determine a mixture that upweights\ndomains sharing common representations in embedding space. This formulation\nallows direct transfer to new data by computing the new domain embeddings. In\nexperiments, we demonstrate improvements over three key scenarios: (i) our\ncomputed weights improve performance on pretraining domains with a fraction of\nthe compute of existing methods; (ii) Chameleon can adapt to data changes\nwithout proxy retraining, boosting few-shot reasoning accuracies when\ntransferred to new data; (iii) our method enables efficient domain reweighting\nin finetuning, consistently improving test perplexity on all finetuning domains\nover uniform mixture. Our code is available at\nhttps://github.com/LIONS-EPFL/Chameleon."}
{"id": "2505.24778", "pdf": "https://arxiv.org/pdf/2505.24778", "abs": "https://arxiv.org/abs/2505.24778", "authors": ["Jiayu Liu", "Qing Zong", "Weiqi Wang", "Yangqiu Song"], "title": "Revisiting Epistemic Markers in Confidence Estimation: Can Markers Accurately Reflect Large Language Models' Uncertainty?", "categories": ["cs.CL"], "comment": "ACL2025", "summary": "As large language models (LLMs) are increasingly used in high-stakes domains,\naccurately assessing their confidence is crucial. Humans typically express\nconfidence through epistemic markers (e.g., \"fairly confident\") instead of\nnumerical values. However, it remains unclear whether LLMs consistently use\nthese markers to reflect their intrinsic confidence due to the difficulty of\nquantifying uncertainty associated with various markers. To address this gap,\nwe first define marker confidence as the observed accuracy when a model employs\nan epistemic marker. We evaluate its stability across multiple\nquestion-answering datasets in both in-distribution and out-of-distribution\nsettings for open-source and proprietary LLMs. Our results show that while\nmarkers generalize well within the same distribution, their confidence is\ninconsistent in out-of-distribution scenarios. These findings raise significant\nconcerns about the reliability of epistemic markers for confidence estimation,\nunderscoring the need for improved alignment between marker based confidence\nand actual model uncertainty. Our code is available at\nhttps://github.com/HKUST-KnowComp/MarCon."}
{"id": "2505.24062", "pdf": "https://arxiv.org/pdf/2505.24062", "abs": "https://arxiv.org/abs/2505.24062", "authors": ["Kamyar Barakati", "Yu Liu", "Hiroshi Funakubo", "Sergei V. Kalinin"], "title": "Exploring Domain Wall Pinning in Ferroelectrics via Automated High Throughput AFM", "categories": ["cond-mat.mtrl-sci", "cs.CV", "cs.LG", "physics.app-ph"], "comment": "17 pages, 6 figures", "summary": "Domain-wall dynamics in ferroelectric materials are strongly\nposition-dependent since each polar interface is locked into a unique local\nmicrostructure. This necessitates spatially resolved studies of the\nwall-pinning using scanning-probe microscopy techniques. The pinning centers\nand preexisting domain walls are usually sparse within image plane, precluding\nthe use of dense hyperspectral imaging modes and requiring time-consuming human\nexperimentation. Here, a large area epitaxial PbTiO$_3$ film on cubic KTaO$_3$\nwere investigated to quantify the electric field driven dynamics of the\npolar-strain domain structures using ML-controlled automated Piezoresponse\nForce Microscopy. Analysis of 1500 switching events reveals that domain wall\ndisplacement depends not only on field parameters but also on the local\nferroelectric-ferroelastic configuration. For example, twin boundaries in\npolydomains regions like a$_1^-$/$c^+$ $\\parallel$ a$_2^-$/$c^-$ stay pinned up\nto a certain level of bias magnitude and change only marginally as the bias\nincreases from 20V to 30V, whereas single variant boundaries like a$_2^+$/$c^+$\n$\\parallel$ a$_2^-$/$c^-$ stack are already activated at 20V. These statistics\non the possible ferroelectric and ferroelastic wall orientations, together with\nthe automated, high-throughput AFM workflow, can be distilled into a predictive\nmap that links domain configurations to pulse parameters. This\nmicrostructure-specific rule set forms the foundation for designing\nferroelectric memories."}
{"id": "2505.24864", "pdf": "https://arxiv.org/pdf/2505.24864", "abs": "https://arxiv.org/abs/2505.24864", "authors": ["Mingjie Liu", "Shizhe Diao", "Ximing Lu", "Jian Hu", "Xin Dong", "Yejin Choi", "Jan Kautz", "Yi Dong"], "title": "ProRL: Prolonged Reinforcement Learning Expands Reasoning Boundaries in Large Language Models", "categories": ["cs.CL", "cs.AI"], "comment": "26 pages, 17 figures", "summary": "Recent advances in reasoning-centric language models have highlighted\nreinforcement learning (RL) as a promising method for aligning models with\nverifiable rewards. However, it remains contentious whether RL truly expands a\nmodel's reasoning capabilities or merely amplifies high-reward outputs already\nlatent in the base model's distribution, and whether continually scaling up RL\ncompute reliably leads to improved reasoning performance. In this work, we\nchallenge prevailing assumptions by demonstrating that prolonged RL (ProRL)\ntraining can uncover novel reasoning strategies that are inaccessible to base\nmodels, even under extensive sampling. We introduce ProRL, a novel training\nmethodology that incorporates KL divergence control, reference policy\nresetting, and a diverse suite of tasks. Our empirical analysis reveals that\nRL-trained models consistently outperform base models across a wide range of\npass@k evaluations, including scenarios where base models fail entirely\nregardless of the number of attempts. We further show that reasoning boundary\nimprovements correlates strongly with task competence of base model and\ntraining duration, suggesting that RL can explore and populate new regions of\nsolution space over time. These findings offer new insights into the conditions\nunder which RL meaningfully expands reasoning boundaries in language models and\nestablish a foundation for future work on long-horizon RL for reasoning. We\nrelease model weights to support further research:\nhttps://huggingface.co/nvidia/Nemotron-Research-Reasoning-Qwen-1.5B"}
{"id": "2505.24857", "pdf": "https://arxiv.org/pdf/2505.24857", "abs": "https://arxiv.org/abs/2505.24857", "authors": ["Heli Ben-Hamu", "Itai Gat", "Daniel Severo", "Niklas Nolte", "Brian Karrer"], "title": "Accelerated Sampling from Masked Diffusion Models via Entropy Bounded Unmasking", "categories": ["cs.LG"], "comment": null, "summary": "Recent masked diffusion models (MDMs) have shown competitive performance\ncompared to autoregressive models (ARMs) for language modeling. While most\nliterature has focused on performance enhancing sampling procedures, efficient\nsampling from MDMs has been scarcely explored. We make the observation that\noften a given sequence of partially masked tokens determines the values of\nmultiple unknown tokens deterministically, meaning that a single prediction of\na masked model holds additional information unused by standard sampling\nprocedures. Based on this observation, we introduce EB-Sampler, a simple\ndrop-in replacement for existing samplers, utilizing an Entropy Bounded\nunmasking procedure that dynamically unmasks multiple tokens in one function\nevaluation with predefined approximate error tolerance. We formulate the\nEB-Sampler as part of a broad family of adaptive samplers for which we provide\nan error analysis that motivates our algorithmic choices. EB-Sampler\naccelerates sampling from current state of the art MDMs by roughly 2-3x on\nstandard coding and math reasoning benchmarks without loss in performance. We\nalso validate the same procedure works well on smaller reasoning tasks\nincluding maze navigation and Sudoku, tasks ARMs often struggle with."}
{"id": "2505.24803", "pdf": "https://arxiv.org/pdf/2505.24803", "abs": "https://arxiv.org/abs/2505.24803", "authors": ["Zhijun Pan", "Antonios Andronis", "Eva Hayek", "Oscar AP Wilkinson", "Ilya Lasy", "Annette Parry", "Guy Gadney", "Tim J. Smith", "Mick Grierson"], "title": "Guiding Generative Storytelling with Knowledge Graphs", "categories": ["cs.CL", "cs.HC"], "comment": "This manuscript was submitted for peer review in January 2025", "summary": "Large Language Models (LLMs) have shown great potential in automated story\ngeneration, but challenges remain in maintaining long-form coherence and\nproviding users with intuitive and effective control. Retrieval-Augmented\nGeneration (RAG) has proven effective in reducing hallucinations in text\ngeneration; however, the use of structured data to support generative\nstorytelling remains underexplored. This paper investigates how knowledge\ngraphs (KGs) can enhance LLM-based storytelling by improving narrative quality\nand enabling user-driven modifications. We propose a KG-assisted storytelling\npipeline and evaluate its effectiveness through a user study with 15\nparticipants. Participants created their own story prompts, generated stories,\nand edited knowledge graphs to shape their narratives. Through quantitative and\nqualitative analysis, our findings demonstrate that knowledge graphs\nsignificantly enhance story quality in action-oriented and structured\nnarratives within our system settings. Additionally, editing the knowledge\ngraph increases users' sense of control, making storytelling more engaging,\ninteractive, and playful."}
{"id": "2505.24134", "pdf": "https://arxiv.org/pdf/2505.24134", "abs": "https://arxiv.org/abs/2505.24134", "authors": ["Ricardo Baptista", "Andrew M. Stuart", "Son Tran"], "title": "A Mathematical Perspective On Contrastive Learning", "categories": ["stat.ML", "cs.CV", "cs.LG"], "comment": "44 pages, 15 figures", "summary": "Multimodal contrastive learning is a methodology for linking different data\nmodalities; the canonical example is linking image and text data. The\nmethodology is typically framed as the identification of a set of encoders, one\nfor each modality, that align representations within a common latent space. In\nthis work, we focus on the bimodal setting and interpret contrastive learning\nas the optimization of (parameterized) encoders that define conditional\nprobability distributions, for each modality conditioned on the other,\nconsistent with the available data. This provides a framework for multimodal\nalgorithms such as crossmodal retrieval, which identifies the mode of one of\nthese conditional distributions, and crossmodal classification, which is\nsimilar to retrieval but includes a fine-tuning step to make it task specific.\n  The framework we adopt also gives rise to crossmodal generative models. This\nprobabilistic perspective suggests two natural generalizations of contrastive\nlearning: the introduction of novel probabilistic loss functions, and the use\nof alternative metrics for measuring alignment in the common latent space. We\nstudy these generalizations of the classical approach in the multivariate\nGaussian setting. In this context we view the latent space identification as a\nlow-rank matrix approximation problem. This allows us to characterize the\ncapabilities of loss functions and alignment metrics to approximate natural\nstatistics, such as conditional means and covariances; doing so yields novel\nvariants on contrastive learning algorithms for specific mode-seeking and for\ngenerative tasks. The framework we introduce is also studied through numerical\nexperiments on multivariate Gaussians, the labeled MNIST dataset, and on a data\nassimilation application arising in oceanography."}
{"id": "2303.17707", "pdf": "https://arxiv.org/pdf/2303.17707", "abs": "https://arxiv.org/abs/2303.17707", "authors": ["Weina Jin", "Xiaoxiao Li", "Ghassan Hamarneh"], "title": "Why is plausibility surprisingly problematic as an XAI criterion?", "categories": ["cs.AI", "cs.HC"], "comment": null, "summary": "Explainable artificial intelligence (XAI) is motivated by the problem of\nmaking AI predictions understandable, transparent, and responsible, as AI\nbecomes increasingly impactful in society and high-stakes domains. The\nevaluation and optimization criteria of XAI are gatekeepers for XAI algorithms\nto achieve their expected goals and should withstand rigorous inspection. To\nimprove the scientific rigor of XAI, we conduct a critical examination of a\ncommon XAI criterion: plausibility. Plausibility assesses how convincing the AI\nexplanation is to humans, and is usually quantified by metrics of feature\nlocalization or feature correlation. Our examination shows that plausibility is\ninvalid to measure explainability, and human explanations are not the ground\ntruth for XAI, because doing so ignores the necessary assumptions underpinning\nan explanation. Our examination further reveals the consequences of using\nplausibility as an XAI criterion, including increasing misleading explanations\nthat manipulate users, deteriorating users' trust in the AI system, undermining\nhuman autonomy, being unable to achieve complementary human-AI task\nperformance, and abandoning other possible approaches of enhancing\nunderstandability. Due to the invalidity of measurements and the unethical\nissues, this position paper argues that the community should stop using\nplausibility as a criterion for the evaluation and optimization of XAI\nalgorithms. We also delineate new research approaches to improve XAI in\ntrustworthiness, understandability, and utility to users, including\ncomplementary human-AI task performance."}
{"id": "2505.24859", "pdf": "https://arxiv.org/pdf/2505.24859", "abs": "https://arxiv.org/abs/2505.24859", "authors": ["Joschka Braun", "Carsten Eickhoff", "Seyed Ali Bahrainian"], "title": "Beyond Multiple Choice: Evaluating Steering Vectors for Adaptive Free-Form Summarization", "categories": ["cs.LG", "cs.CL"], "comment": "29 pages, 21 figures, preprint", "summary": "Steering vectors are a lightweight method for controlling text properties by\nadding a learned bias to language model activations at inference time. So far,\nsteering vectors have predominantly been evaluated in multiple-choice settings,\nwhile their effectiveness in free-form generation tasks remains understudied.\nMoving \"Beyond Multiple Choice,\" we thoroughly evaluate the effectiveness of\nsteering vectors in adaptively controlling topical focus, sentiment, toxicity,\nand readability in abstractive summaries of the NEWTS dataset. We find that\nsteering effectively controls the targeted summary properties, but high\nsteering strengths consistently degrade both intrinsic and extrinsic text\nquality. Compared to steering, prompting offers weaker control, while\npreserving text quality. Combining steering and prompting yields the strongest\ncontrol over text properties and offers the most favorable efficacy-quality\ntrade-off at moderate steering strengths. Our results underscore the practical\ntrade-off between control strength and text quality preservation when applying\nsteering vectors to free-form generation tasks."}
{"id": "2505.24826", "pdf": "https://arxiv.org/pdf/2505.24826", "abs": "https://arxiv.org/abs/2505.24826", "authors": ["Li yunhan", "Wu gengshen"], "title": "LegalEval-Q: A New Benchmark for The Quality Evaluation of LLM-Generated Legal Text", "categories": ["cs.CL", "cs.CV"], "comment": "10 pages, 11 figures", "summary": "As large language models (LLMs) are increasingly used in legal applications,\ncurrent evaluation benchmarks tend to focus mainly on factual accuracy while\nlargely neglecting important linguistic quality aspects such as clarity,\ncoherence, and terminology. To address this gap, we propose three steps: First,\nwe develop a regression model to evaluate the quality of legal texts based on\nclarity, coherence, and terminology. Second, we create a specialized set of\nlegal questions. Third, we analyze 49 LLMs using this evaluation framework.\n  Our analysis identifies three key findings: First, model quality levels off\nat 14 billion parameters, with only a marginal improvement of $2.7\\%$ noted at\n72 billion parameters. Second, engineering choices such as quantization and\ncontext length have a negligible impact, as indicated by statistical\nsignificance thresholds above 0.016. Third, reasoning models consistently\noutperform base architectures. A significant outcome of our research is the\nrelease of a ranking list and Pareto analysis, which highlight the Qwen3 series\nas the optimal choice for cost-performance tradeoffs. This work not only\nestablishes standardized evaluation protocols for legal LLMs but also uncovers\nfundamental limitations in current training data refinement approaches. Code\nand models are available at: https://github.com/lyxx3rd/LegalEval-Q."}
{"id": "2505.24305", "pdf": "https://arxiv.org/pdf/2505.24305", "abs": "https://arxiv.org/abs/2505.24305", "authors": ["Mingxu Zhang", "Xiaoqi Li", "Jiahui Xu", "Kaichen Zhou", "Hojin Bae", "Yan Shen", "Chuyan Xiong", "Jiaming Liu", "Hao Dong"], "title": "SR3D: Unleashing Single-view 3D Reconstruction for Transparent and Specular Object Grasping", "categories": ["cs.RO", "cs.CV"], "comment": null, "summary": "Recent advancements in 3D robotic manipulation have improved grasping of\neveryday objects, but transparent and specular materials remain challenging due\nto depth sensing limitations. While several 3D reconstruction and depth\ncompletion approaches address these challenges, they suffer from setup\ncomplexity or limited observation information utilization. To address this,\nleveraging the power of single view 3D object reconstruction approaches, we\npropose a training free framework SR3D that enables robotic grasping of\ntransparent and specular objects from a single view observation. Specifically,\ngiven single view RGB and depth images, SR3D first uses the external visual\nmodels to generate 3D reconstructed object mesh based on RGB image. Then, the\nkey idea is to determine the 3D object's pose and scale to accurately localize\nthe reconstructed object back into its original depth corrupted 3D scene.\nTherefore, we propose view matching and keypoint matching mechanisms,which\nleverage both the 2D and 3D's inherent semantic and geometric information in\nthe observation to determine the object's 3D state within the scene, thereby\nreconstructing an accurate 3D depth map for effective grasp detection.\nExperiments in both simulation and real world show the reconstruction\neffectiveness of SR3D."}
{"id": "2310.15274", "pdf": "https://arxiv.org/pdf/2310.15274", "abs": "https://arxiv.org/abs/2310.15274", "authors": ["Eren Kurshan"], "title": "From the Pursuit of Universal AGI Architecture to Systematic Approach to Heterogenous AGI: Addressing Alignment, Energy, & AGI Grand Challenges", "categories": ["cs.AI", "cs.LG"], "comment": "Categories: Artificial Intelligence; AI; Artificial General\n  Intelligence; AGI; System Design; System Architecture Preprint International\n  Journal on Semantic Computing Vol. 18, No. 03, pp. 465-500", "summary": "Artificial intelligence (AI) faces a trifecta of grand challenges: the Energy\nWall, the Alignment Problem and the Leap from Narrow AI to AGI. We present\nSAGI, a Systematic Approach to AGI that utilizes system design principles to\novercome the energy wall and alignment challenges. This paper asserts that AGI\ncan be realized through multiplicity of design specific pathways and customized\nthrough system design rather than a singular overarching architecture. AGI\nsystems may exhibit diver architectural configurations and capabilities,\ncontingent upon their intended use cases. Alignment, a challenge broadly\nrecognized as AIs most formidable, is the one that depends most critically on\nsystem design and serves as its primary driving force as a foundational\ncriterion for AGI. Capturing the complexities of human morality for alignment\nrequires architectural support to represent the intricacies of moral\ndecision-making and the pervasive ethical processing at every level, with\nperformance reliability exceeding that of human moral judgment. Hence,\nrequiring a more robust architecture towards safety and alignment goals,\nwithout replicating or resembling the human brain.\n  We argue that system design (such as feedback loops, energy and performance\noptimization) on learning substrates (capable of learning its system\narchitecture) is more fundamental to achieving AGI goals and guarantees,\nsuperseding classical symbolic, emergentist and hybrid approaches. Through\nlearning of the system architecture itself, the resulting AGI is not a product\nof spontaneous emergence but of systematic design and deliberate engineering,\nwith core features, including an integrated moral architecture, deeply embedded\nwithin its architecture. The approach aims to guarantee design goals such as\nalignment, efficiency by self-learning system architecture."}
{"id": "2505.24874", "pdf": "https://arxiv.org/pdf/2505.24874", "abs": "https://arxiv.org/abs/2505.24874", "authors": ["Adam Stein", "Aaditya Naik", "Neelay Velingker", "Mayur Naik", "Eric Wong"], "title": "The Road to Generalizable Neuro-Symbolic Learning Should be Paved with Foundation Models", "categories": ["cs.LG"], "comment": "19 pages, 11 figures", "summary": "Neuro-symbolic learning was proposed to address challenges with training\nneural networks for complex reasoning tasks with the added benefits of\ninterpretability, reliability, and efficiency. Neuro-symbolic learning methods\ntraditionally train neural models in conjunction with symbolic programs, but\nthey face significant challenges that limit them to simplistic problems. On the\nother hand, purely-neural foundation models now reach state-of-the-art\nperformance through prompting rather than training, but they are often\nunreliable and lack interpretability. Supplementing foundation models with\nsymbolic programs, which we call neuro-symbolic prompting, provides a way to\nuse these models for complex reasoning tasks. Doing so raises the question:\nWhat role does specialized model training as part of neuro-symbolic learning\nhave in the age of foundation models? To explore this question, we highlight\nthree pitfalls of traditional neuro-symbolic learning with respect to the\ncompute, data, and programs leading to generalization problems. This position\npaper argues that foundation models enable generalizable neuro-symbolic\nsolutions, offering a path towards achieving the original goals of\nneuro-symbolic learning without the downsides of training from scratch."}
{"id": "2505.24832", "pdf": "https://arxiv.org/pdf/2505.24832", "abs": "https://arxiv.org/abs/2505.24832", "authors": ["John X. Morris", "Chawin Sitawarin", "Chuan Guo", "Narine Kokhlikyan", "G. Edward Suh", "Alexander M. Rush", "Kamalika Chaudhuri", "Saeed Mahloujifar"], "title": "How much do language models memorize?", "categories": ["cs.CL"], "comment": null, "summary": "We propose a new method for estimating how much a model ``knows'' about a\ndatapoint and use it to measure the capacity of modern language models. Prior\nstudies of language model memorization have struggled to disentangle\nmemorization from generalization. We formally separate memorization into two\ncomponents: \\textit{unintended memorization}, the information a model contains\nabout a specific dataset, and \\textit{generalization}, the information a model\ncontains about the true data-generation process. When we completely eliminate\ngeneralization, we can compute the total memorization, which provides an\nestimate of model capacity: our measurements estimate that GPT-style models\nhave a capacity of approximately 3.6 bits per parameter. We train language\nmodels on datasets of increasing size and observe that models memorize until\ntheir capacity fills, at which point ``grokking'' begins, and unintended\nmemorization decreases as models begin to generalize. We train hundreds of\ntransformer language models ranging from $500K$ to $1.5B$ parameters and\nproduce a series of scaling laws relating model capacity and data size to\nmembership inference."}
{"id": "2505.24514", "pdf": "https://arxiv.org/pdf/2505.24514", "abs": "https://arxiv.org/abs/2505.24514", "authors": ["Janek GrÃ¶hl", "Leonid Kunyansky", "Jenni Poimala", "Thomas R. Else", "Francesca Di Cecio", "Sarah E. Bohndiek", "Ben T. Cox", "Andreas Hauptmann"], "title": "Digital twins enable full-reference quality assessment of photoacoustic image reconstructions", "categories": ["physics.med-ph", "cs.CV", "eess.SP"], "comment": null, "summary": "Quantitative comparison of the quality of photoacoustic image reconstruction\nalgorithms remains a major challenge. No-reference image quality measures are\noften inadequate, but full-reference measures require access to an ideal\nreference image. While the ground truth is known in simulations, it is unknown\nin vivo, or in phantom studies, as the reference depends on both the phantom\nproperties and the imaging system. We tackle this problem by using numerical\ndigital twins of tissue-mimicking phantoms and the imaging system to perform a\nquantitative calibration to reduce the simulation gap. The contributions of\nthis paper are two-fold: First, we use this digital-twin framework to compare\nmultiple state-of-the-art reconstruction algorithms. Second, among these is a\nFourier transform-based reconstruction algorithm for circular detection\ngeometries, which we test on experimental data for the first time. Our results\ndemonstrate the usefulness of digital phantom twins by enabling assessment of\nthe accuracy of the numerical forward model and enabling comparison of image\nreconstruction schemes with full-reference image quality assessment. We show\nthat the Fourier transform-based algorithm yields results comparable to those\nof iterative time reversal, but at a lower computational cost. All data and\ncode are publicly available on Zenodo: https://doi.org/10.5281/zenodo.15388429."}
{"id": "2406.11317", "pdf": "https://arxiv.org/pdf/2406.11317", "abs": "https://arxiv.org/abs/2406.11317", "authors": ["Wentong Chen", "Junbo Cui", "Jinyi Hu", "Yujia Qin", "Junjie Fang", "Yue Zhao", "Chongyi Wang", "Jun Liu", "Guirong Chen", "Yupeng Huo", "Yuan Yao", "Yankai Lin", "Zhiyuan Liu", "Maosong Sun"], "title": "GUICourse: From General Vision Language Models to Versatile GUI Agents", "categories": ["cs.AI", "cs.CL", "cs.CV", "cs.HC"], "comment": null, "summary": "Utilizing Graphic User Interface (GUI) for human-computer interaction is\nessential for accessing a wide range of digital tools. Recent advancements in\nVision Language Models (VLMs) highlight the compelling potential to develop\nversatile agents to help humans finish GUI navigation tasks. However, current\nVLMs are challenged in terms of fundamental abilities (OCR and grounding) and\nGUI knowledge (the functions and control methods of GUI elements), preventing\nthem from becoming practical GUI agents. To solve these challenges, we\ncontribute GUICourse, a suite of datasets to train visual-based GUI agents from\ngeneral VLMs. First, we introduce the GUIEnv dataset to strengthen the OCR and\ngrounding capabilities of VLMs. Then, we introduce the GUIAct and GUIChat\ndatasets to enrich their knowledge of GUI components and interactions.\nExperiments demonstrate that our GUI agents have better performance on common\nGUI tasks than their baseline VLMs. Even the small-size GUI agent (with 3.1B\nparameters) can still work well on single-step and multi-step GUI tasks.\nFinally, we analyze the different varieties in the training stage of this agent\nby ablation study. Our source codes and datasets are released at\nhttps://github.com/yiye3/GUICourse."}
{"id": "2411.14207", "pdf": "https://arxiv.org/pdf/2411.14207", "abs": "https://arxiv.org/abs/2411.14207", "authors": ["Shivam Saini", "JÃ¼rgen Peissig"], "title": "HARP: A Large-Scale Higher-Order Ambisonic Room Impulse Response Dataset", "categories": ["cs.SD", "cs.AI", "cs.LG", "cs.MM", "eess.AS"], "comment": "Accepted at ICASSP 2025 Workshop. Code to generate uploaded at:\n  https://github.com/whojavumusic/HARP", "summary": "This contribution introduces a dataset of 7th-order Ambisonic Room Impulse\nResponses (HOA-RIRs), created using the Image Source Method. By employing\nhigher-order Ambisonics, our dataset enables precise spatial audio\nreproduction, a critical requirement for realistic immersive audio\napplications. Leveraging the virtual simulation, we present a unique microphone\nconfiguration, based on the superposition principle, designed to optimize sound\nfield coverage while addressing the limitations of traditional microphone\narrays. The presented 64-microphone configuration allows us to capture RIRs\ndirectly in the Spherical Harmonics domain. The dataset features a wide range\nof room configurations, encompassing variations in room geometry, acoustic\nabsorption materials, and source-receiver distances. A detailed description of\nthe simulation setup is provided alongside for an accurate reproduction. The\ndataset serves as a vital resource for researchers working on spatial audio,\nparticularly in applications involving machine learning to improve room\nacoustics modeling and sound field synthesis. It further provides a very high\nlevel of spatial resolution and realism crucial for tasks such as source\nlocalization, reverberation prediction, and immersive sound reproduction."}
{"id": "2505.24834", "pdf": "https://arxiv.org/pdf/2505.24834", "abs": "https://arxiv.org/abs/2505.24834", "authors": ["Roksana Goworek", "Haim Dubossarsky"], "title": "Multilinguality Does not Make Sense: Investigating Factors Behind Zero-Shot Transfer in Sense-Aware Tasks", "categories": ["cs.CL"], "comment": "8 pages, 8 figures", "summary": "Cross-lingual transfer allows models to perform tasks in languages unseen\nduring training and is often assumed to benefit from increased multilinguality.\nIn this work, we challenge this assumption in the context of two underexplored,\nsense-aware tasks: polysemy disambiguation and lexical semantic change. Through\na large-scale analysis across 28 languages, we show that multilingual training\nis neither necessary nor inherently beneficial for effective transfer. Instead,\nwe find that confounding factors - such as fine-tuning data composition and\nevaluation artifacts - better account for the perceived advantages of\nmultilinguality. Our findings call for more rigorous evaluations in\nmultilingual NLP. We release fine-tuned models and benchmarks to support\nfurther research, with implications extending to low-resource and typologically\ndiverse languages."}
{"id": "2505.24654", "pdf": "https://arxiv.org/pdf/2505.24654", "abs": "https://arxiv.org/abs/2505.24654", "authors": ["Maria Rafaela Gkeka", "Bowen Sun", "Evgenia Smirni", "Christos D. Antonopoulos", "Spyros Lalis", "Nikolaos Bellas"], "title": "Black-box Adversarial Attacks on CNN-based SLAM Algorithms", "categories": ["cs.RO", "cs.CV", "68T40, 68T45, 68M25,"], "comment": "9 pages, 8 figures", "summary": "Continuous advancements in deep learning have led to significant progress in\nfeature detection, resulting in enhanced accuracy in tasks like Simultaneous\nLocalization and Mapping (SLAM). Nevertheless, the vulnerability of deep neural\nnetworks to adversarial attacks remains a challenge for their reliable\ndeployment in applications, such as navigation of autonomous agents. Even\nthough CNN-based SLAM algorithms are a growing area of research there is a\nnotable absence of a comprehensive presentation and examination of adversarial\nattacks targeting CNN-based feature detectors, as part of a SLAM system. Our\nwork introduces black-box adversarial perturbations applied to the RGB images\nfed into the GCN-SLAM algorithm. Our findings on the TUM dataset [30] reveal\nthat even attacks of moderate scale can lead to tracking failure in as many as\n76% of the frames. Moreover, our experiments highlight the catastrophic impact\nof attacking depth instead of RGB input images on the SLAM system."}
{"id": "2408.10504", "pdf": "https://arxiv.org/pdf/2408.10504", "abs": "https://arxiv.org/abs/2408.10504", "authors": ["Yilun Kong", "Hangyu Mao", "Qi Zhao", "Bin Zhang", "Jingqing Ruan", "Li Shen", "Yongzhe Chang", "Xueqian Wang", "Rui Zhao", "Dacheng Tao"], "title": "QPO: Query-dependent Prompt Optimization via Multi-Loop Offline Reinforcement Learning", "categories": ["cs.AI"], "comment": "Transactions on Machine Learning Research (TMLR)", "summary": "Prompt engineering has demonstrated remarkable success in enhancing the\nperformance of large language models (LLMs) across diverse tasks. However, most\nexisting prompt optimization methods only focus on the task-level performance,\noverlooking the importance of query-preferred prompts, which leads to\nsuboptimal performances. Additionally, these methods rely heavily on frequent\ninteractions with LLMs to obtain feedback for guiding the optimization process,\nincurring substantial redundant interaction costs. In this paper, we introduce\nQuery-dependent Prompt Optimization (QPO), which leverages multi-loop offline\nreinforcement learning to iteratively fine-tune a small pretrained language\nmodel to generate optimal prompts tailored to the input queries, thus\nsignificantly improving the prompting effect on the large target LLM. We derive\ninsights from offline prompting demonstration data, which already exists in\nlarge quantities as a by-product of benchmarking diverse prompts on\nopen-sourced tasks, thereby circumventing the expenses of online interactions.\nFurthermore, we continuously augment the offline dataset with the generated\nprompts in each loop, as the prompts from the fine-tuned model are supposed to\noutperform the source prompts in the original dataset. These iterative loops\nbootstrap the model towards generating optimal prompts. Experiments on various\nLLM scales and diverse NLP and math tasks demonstrate the efficacy and\ncost-efficiency of our method in both zero-shot and few-shot scenarios."}
{"id": "2505.05880", "pdf": "https://arxiv.org/pdf/2505.05880", "abs": "https://arxiv.org/abs/2505.05880", "authors": ["Bettina Fazzinga", "Sergio Flesca", "Filippo Furfaro", "Luigi Pontieri", "Francesco Scala"], "title": "Combining Abstract Argumentation and Machine Learning for Efficiently Analyzing Low-Level Process Event Streams", "categories": ["cs.AI", "cs.LG"], "comment": null, "summary": "Monitoring and analyzing process traces is a critical task for modern\ncompanies and organizations. In scenarios where there is a gap between trace\nevents and reference business activities, this entails an interpretation\nproblem, amounting to translating each event of any ongoing trace into the\ncorresponding step of the activity instance. Building on a recent approach that\nframes the interpretation problem as an acceptance problem within an Abstract\nArgumentation Framework (AAF), one can elegantly analyze plausible event\ninterpretations (possibly in an aggregated form), as well as offer explanations\nfor those that conflict with prior process knowledge. Since, in settings where\nevent-to-activity mapping is highly uncertain (or simply under-specified) this\nreasoning-based approach may yield lowly-informative results and heavy\ncomputation, one can think of discovering a sequencetagging model, trained to\nsuggest highly-probable candidate event interpretations in a context-aware way.\nHowever, training such a model optimally may require using a large amount of\nmanually-annotated example traces. Considering the urgent need of developing\nGreen AI solutions enabling environmental and societal sustainability (with\nreduced labor/computational costs and carbon footprint), we propose a\ndata/computation-efficient neuro-symbolic approach to the problem, where the\ncandidate interpretations returned by the example-driven sequence tagger is\nrefined by the AAF-based reasoner. This allows us to also leverage prior\nknowledge to compensate for the scarcity of example data, as confirmed by\nexperimental results; clearly, this property is particularly useful in settings\nwhere data annotation and model optimization costs are subject to stringent\nconstraints."}
{"id": "2505.24858", "pdf": "https://arxiv.org/pdf/2505.24858", "abs": "https://arxiv.org/abs/2505.24858", "authors": ["Gabrielle Kaili-May Liu", "Gal Yona", "Avi Caciularu", "Idan Szpektor", "Tim G. J. Rudner", "Arman Cohan"], "title": "MetaFaith: Faithful Natural Language Uncertainty Expression in LLMs", "categories": ["cs.CL", "cs.LG"], "comment": null, "summary": "A critical component in the trustworthiness of LLMs is reliable uncertainty\ncommunication, yet LLMs often use assertive language when conveying false\nclaims, leading to over-reliance and eroded trust. We present the first\nsystematic study of $\\textit{faithful confidence calibration}$ of LLMs,\nbenchmarking models' ability to use linguistic expressions of uncertainty that\n$\\textit{faithfully reflect}$ their intrinsic uncertainty, across a\ncomprehensive array of models, datasets, and prompting strategies. Our results\ndemonstrate that LLMs largely fail at this task, and that existing\ninterventions are insufficient: standard prompt approaches provide only\nmarginal gains, and existing, factuality-based calibration techniques can even\nharm faithful calibration. To address this critical gap, we introduce\nMetaFaith, a novel prompt-based calibration approach inspired by human\nmetacognition. We show that MetaFaith robustly improves faithful calibration\nacross diverse models and task domains, enabling up to 61% improvement in\nfaithfulness and achieving an 83% win rate over original generations as judged\nby humans."}
{"id": "2505.24703", "pdf": "https://arxiv.org/pdf/2505.24703", "abs": "https://arxiv.org/abs/2505.24703", "authors": ["Dennis Jacob", "Chong Xiang", "Prateek Mittal"], "title": "PatchDEMUX: A Certifiably Robust Framework for Multi-label Classifiers Against Adversarial Patches", "categories": ["cs.CR", "cs.CV", "cs.LG"], "comment": "CVPR 2025", "summary": "Deep learning techniques have enabled vast improvements in computer vision\ntechnologies. Nevertheless, these models are vulnerable to adversarial patch\nattacks which catastrophically impair performance. The physically realizable\nnature of these attacks calls for certifiable defenses, which feature provable\nguarantees on robustness. While certifiable defenses have been successfully\napplied to single-label classification, limited work has been done for\nmulti-label classification. In this work, we present PatchDEMUX, a certifiably\nrobust framework for multi-label classifiers against adversarial patches. Our\napproach is a generalizable method which can extend any existing certifiable\ndefense for single-label classification; this is done by considering the\nmulti-label classification task as a series of isolated binary classification\nproblems to provably guarantee robustness. Furthermore, in the scenario where\nan attacker is limited to a single patch we propose an additional certification\nprocedure that can provide tighter robustness bounds. Using the current\nstate-of-the-art (SOTA) single-label certifiable defense PatchCleanser as a\nbackbone, we find that PatchDEMUX can achieve non-trivial robustness on the\nMS-COCO and PASCAL VOC datasets while maintaining high clean performance"}
{"id": "2409.14106", "pdf": "https://arxiv.org/pdf/2409.14106", "abs": "https://arxiv.org/abs/2409.14106", "authors": ["Yibo Li", "Yuan Fang", "Mengmei Zhang", "Chuan Shi"], "title": "Advancing Molecular Graph-Text Pre-training via Fine-grained Alignment", "categories": ["cs.AI"], "comment": "Accepted by KDD 2025", "summary": "Understanding molecular structure and related knowledge is crucial for\nscientific research. Recent studies integrate molecular graphs with their\ntextual descriptions to enhance molecular representation learning. However,\nthey focus on the whole molecular graph and neglect frequently occurring\nsubgraphs, known as motifs, which are essential for determining molecular\nproperties. Without such fine-grained knowledge, these models struggle to\ngeneralize to unseen molecules and tasks that require motif-level insights. To\nbridge this gap, we propose FineMolTex, a novel Fine-grained Molecular\ngraph-Text pre-training framework to jointly learn coarse-grained\nmolecule-level knowledge and fine-grained motif-level knowledge. Specifically,\nFineMolTex consists of two pre-training tasks: a contrastive alignment task for\ncoarse-grained matching and a masked multi-modal modeling task for fine-grained\nmatching. In particular, the latter predicts the labels of masked motifs and\nwords, which are selected based on their importance. By leveraging insights\nfrom both modalities, FineMolTex is able to understand the fine-grained\nmatching between motifs and words. Finally, we conduct extensive experiments\nacross three downstream tasks, achieving up to 230% improvement in the\ntext-based molecule editing task. Additionally, our case studies reveal that\nFineMolTex successfully captures fine-grained knowledge, potentially offering\nvaluable insights for drug discovery and catalyst design."}
{"id": "2505.23774", "pdf": "https://arxiv.org/pdf/2505.23774", "abs": "https://arxiv.org/abs/2505.23774", "authors": ["Zeki Doruk Erden", "Boi Faltings"], "title": "On the Parallels Between Evolutionary Theory and the State of AI", "categories": ["q-bio.NC", "cs.LG", "cs.NE", "nlin.AO"], "comment": "Published at the Evolving Self-Organization Workshop in GECCO 2025", "summary": "This article critically examines the foundational principles of contemporary\nAI methods, exploring the limitations that hinder its potential. We draw\nparallels between the modern AI landscape and the 20th-century Modern Synthesis\nin evolutionary biology, and highlight how advancements in evolutionary theory\nthat augmented the Modern Synthesis, particularly those of Evolutionary\nDevelopmental Biology, offer insights that can inform a new design paradigm for\nAI. By synthesizing findings across AI and evolutionary theory, we propose a\npathway to overcome existing limitations, enabling AI to achieve its\naspirational goals."}
{"id": "2505.24863", "pdf": "https://arxiv.org/pdf/2505.24863", "abs": "https://arxiv.org/abs/2505.24863", "authors": ["Junyu Zhang", "Runpei Dong", "Han Wang", "Xuying Ning", "Haoran Geng", "Peihao Li", "Xialin He", "Yutong Bai", "Jitendra Malik", "Saurabh Gupta", "Huan Zhang"], "title": "AlphaOne: Reasoning Models Thinking Slow and Fast at Test Time", "categories": ["cs.CL"], "comment": null, "summary": "This paper presents AlphaOne ($\\alpha$1), a universal framework for\nmodulating reasoning progress in large reasoning models (LRMs) at test time.\n$\\alpha$1 first introduces $\\alpha$ moment, which represents the scaled\nthinking phase with a universal parameter $\\alpha$. Within this scaled\npre-$\\alpha$ moment phase, it dynamically schedules slow thinking transitions\nby modeling the insertion of reasoning transition tokens as a Bernoulli\nstochastic process. After the $\\alpha$ moment, $\\alpha$1 deterministically\nterminates slow thinking with the end-of-thinking token, thereby fostering fast\nreasoning and efficient answer generation. This approach unifies and\ngeneralizes existing monotonic scaling methods by enabling flexible and dense\nslow-to-fast reasoning modulation. Extensive empirical studies on various\nchallenging benchmarks across mathematical, coding, and scientific domains\ndemonstrate $\\alpha$1's superior reasoning capability and efficiency. Project\npage: https://alphaone-project.github.io/"}
{"id": "2505.24781", "pdf": "https://arxiv.org/pdf/2505.24781", "abs": "https://arxiv.org/abs/2505.24781", "authors": ["Karim Abou-Moustafa"], "title": "Efficient Estimation of Regularized Tyler's M-Estimator Using Approximate LOOCV", "categories": ["stat.ML", "cs.CE", "cs.CV", "cs.LG", "eess.SP", "I.2.0; I.2.6"], "comment": "An extended version of a short article that appeared in 2023 IEEE\n  Workshop on Information Theory, Saint-Malo, France", "summary": "We consider the problem of estimating a regularization parameter, or a\nshrinkage coefficient $\\alpha \\in (0,1)$ for Regularized Tyler's M-estimator\n(RTME). In particular, we propose to estimate an optimal shrinkage coefficient\nby setting $\\alpha$ as the solution to a suitably chosen objective function;\nnamely the leave-one-out cross-validated (LOOCV) log-likelihood loss. Since\nLOOCV is computationally prohibitive even for moderate sample size $n$, we\npropose a computationally efficient approximation for the LOOCV log-likelihood\nloss that eliminates the need for invoking the RTME procedure $n$ times for\neach sample left out during the LOOCV procedure. This approximation yields an\n$O(n)$ reduction in the running time complexity for the LOOCV procedure, which\nresults in a significant speedup for computing the LOOCV estimate. We\ndemonstrate the efficiency and accuracy of the proposed approach on synthetic\nhigh-dimensional data sampled from heavy-tailed elliptical distributions, as\nwell as on real high-dimensional datasets for object recognition, face\nrecognition, and handwritten digit's recognition. Our experiments show that the\nproposed approach is efficient and consistently more accurate than other\nmethods in the literature for shrinkage coefficient estimation."}
{"id": "2410.16270", "pdf": "https://arxiv.org/pdf/2410.16270", "abs": "https://arxiv.org/abs/2410.16270", "authors": ["Lingyu Li", "Yixu Wang", "Haiquan Zhao", "Shuqi Kong", "Yan Teng", "Chunbo Li", "Yingchun Wang"], "title": "Reflection-Bench: Evaluating Epistemic Agency in Large Language Models", "categories": ["cs.AI"], "comment": "29 pages, 19 figures, 9 tables", "summary": "With large language models (LLMs) increasingly deployed as cognitive engines\nfor AI agents, the reliability and effectiveness critically hinge on their\nintrinsic epistemic agency, which remains understudied. Epistemic agency, the\nability to flexibly construct, adapt, and monitor beliefs about dynamic\nenvironments, represents a base-model-level capacity independent of specific\ntools, modules, or applications. We characterize the holistic process\nunderlying epistemic agency, which unfolds in seven interrelated dimensions:\nprediction, decision-making, perception, memory, counterfactual thinking,\nbelief updating, and meta-reflection. Correspondingly, we propose\nReflection-Bench, a cognitive-psychology-inspired benchmark consisting of seven\ntasks with long-term relevance and minimization of data leakage. Through a\ncomprehensive evaluation of 16 models using three prompting strategies, we\nidentify a clear three-tier performance hierarchy and significant limitations\nof current LLMs, particularly in meta-reflection capabilities. While\nstate-of-the-art LLMs demonstrate rudimentary signs of epistemic agency, our\nfindings suggest several promising research directions, including enhancing\ncore cognitive functions, improving cross-functional coordination, and\ndeveloping adaptive processing mechanisms. Our code and data are available at\nhttps://github.com/AI45Lab/ReflectionBench."}
{"id": "2505.23841", "pdf": "https://arxiv.org/pdf/2505.23841", "abs": "https://arxiv.org/abs/2505.23841", "authors": ["Hairu Wang", "Yuan Feng", "Yukun Cao", "Xike Xie", "S Kevin Zhou"], "title": "SkewRoute: Training-Free LLM Routing for Knowledge Graph Retrieval-Augmented Generation via Score Skewness of Retrieved Context", "categories": ["cs.IR", "cs.CL"], "comment": null, "summary": "Large language models excel at many tasks but often incur high inference\ncosts during deployment. To mitigate hallucination, many systems use a\nknowledge graph to enhance retrieval-augmented generation (KG-RAG). However,\nthe large amount of retrieved knowledge contexts increase these inference costs\nfurther. A promising solution to balance performance and cost is LLM routing,\nwhich directs simple queries to smaller LLMs and complex ones to larger LLMs.\nHowever, no dedicated routing methods currently exist for RAG, and existing\ntraining-based routers face challenges scaling to this domain due to the need\nfor extensive training data. We observe that the score distributions produced\nby the retrieval scorer strongly correlate with query difficulty. Based on\nthis, we propose a novel, training-free routing framework, the first tailored\nto KG-RAG that effectively balances performance and cost in a plug-and-play\nmanner. Experiments show our method reduces calls to larger LLMs by up to 50%\nwithout sacrificing response quality, demonstrating its potential for efficient\nand scalable LLM deployment."}
{"id": "2505.24796", "pdf": "https://arxiv.org/pdf/2505.24796", "abs": "https://arxiv.org/abs/2505.24796", "authors": ["Zimu Liao", "Jifeng Ding", "Rong Fu", "Siwei Cui", "Ruixuan Gong", "Li Wang", "Boni Hu", "Yi Wang", "Hengjie Li", "XIngcheng Zhang", "Hui Wang"], "title": "TC-GS: A Faster Gaussian Splatting Module Utilizing Tensor Cores", "categories": ["cs.GR", "cs.CV", "cs.DC", "I.3.6; I.3.2; D.1.3"], "comment": "15 pages, 6 figures", "summary": "3D Gaussian Splatting (3DGS) renders pixels by rasterizing Gaussian\nprimitives, where conditional alpha-blending dominates the time cost in the\nrendering pipeline. This paper proposes TC-GS, an algorithm-independent\nuniversal module that expands Tensor Core (TCU) applicability for 3DGS, leading\nto substantial speedups and seamless integration into existing 3DGS\noptimization frameworks. The key innovation lies in mapping alpha computation\nto matrix multiplication, fully utilizing otherwise idle TCUs in existing 3DGS\nimplementations. TC-GS provides plug-and-play acceleration for existing\ntop-tier acceleration algorithms tightly coupled with rendering pipeline\ndesigns, like Gaussian compression and redundancy elimination algorithms.\nAdditionally, we introduce a global-to-local coordinate transformation to\nmitigate rounding errors from quadratic terms of pixel coordinates caused by\nTensor Core half-precision computation. Extensive experiments demonstrate that\nour method maintains rendering quality while providing an additional 2.18x\nspeedup over existing Gaussian acceleration algorithms, thus reaching up to a\ntotal 5.6x acceleration. The code is currently available at anonymous\n\\href{https://github.com/TensorCore3DGS/3DGSTensorCore}"}
{"id": "2410.17885", "pdf": "https://arxiv.org/pdf/2410.17885", "abs": "https://arxiv.org/abs/2410.17885", "authors": ["Linger Deng", "Linghao Zhu", "Yuliang Liu", "Yu Wang", "Qunyi Xie", "Jingjing Wu", "Gang Zhang", "Yingying Zhu", "Xiang Bai"], "title": "Theorem-Validated Reverse Chain-of-Thought Problem Generation for Geometric Reasoning", "categories": ["cs.AI", "cs.CV"], "comment": null, "summary": "Large Multimodal Models (LMMs) face limitations in geometric reasoning due to\ninsufficient Chain of Thought (CoT) image-text training data. While existing\napproaches leverage template-based or LLM-assisted methods for geometric CoT\ndata creation, they often face challenges in achieving both diversity and\nprecision. To bridge this gap, we introduce a two-stage Theorem-Validated\nReverse Chain-of-Thought Reasoning Synthesis (TR-CoT) framework. The first\nstage, TR-Engine, synthesizes theorem-grounded geometric diagrams with\nstructured descriptions and properties. The second stage, TR-Reasoner, employs\nreverse reasoning to iteratively refine question-answer pairs by\ncross-validating geometric properties and description fragments. Our approach\nexpands theorem-type coverage, corrects long-standing misunderstandings, and\nenhances geometric reasoning. Fine-grained CoT improves theorem understanding\nand increases logical consistency by 24.5%. Our best models surpass the\nbaselines in MathVista and GeoQA by 10.1% and 4.7%, outperforming advanced\nclosed-source models like GPT-4o."}
{"id": "2505.24004", "pdf": "https://arxiv.org/pdf/2505.24004", "abs": "https://arxiv.org/abs/2505.24004", "authors": ["Amanda Chan", "Catherine Di", "Joseph Rupertus", "Gary Smith", "Varun Nagaraj Rao", "Manoel Horta Ribeiro", "AndrÃ©s Monroy-HernÃ¡ndez"], "title": "Redefining Research Crowdsourcing: Incorporating Human Feedback with LLM-Powered Digital Twins", "categories": ["cs.HC", "cs.CL", "cs.CY"], "comment": "Accepted as a CHI Late Breaking Work (2025), cite appropriately", "summary": "Crowd work platforms like Amazon Mechanical Turk and Prolific are vital for\nresearch, yet workers' growing use of generative AI tools poses challenges.\nResearchers face compromised data validity as AI responses replace authentic\nhuman behavior, while workers risk diminished roles as AI automates tasks. To\naddress this, we propose a hybrid framework using digital twins, personalized\nAI models that emulate workers' behaviors and preferences while keeping humans\nin the loop. We evaluate our system with an experiment (n=88 crowd workers) and\nin-depth interviews with crowd workers (n=5) and social science researchers\n(n=4). Our results suggest that digital twins may enhance productivity and\nreduce decision fatigue while maintaining response quality. Both researchers\nand workers emphasized the importance of transparency, ethical data use, and\nworker agency. By automating repetitive tasks and preserving human engagement\nfor nuanced ones, digital twins may help balance scalability with authenticity."}
{"id": "2505.24819", "pdf": "https://arxiv.org/pdf/2505.24819", "abs": "https://arxiv.org/abs/2505.24819", "authors": ["Haozhan Tang", "Tianyi Zhang", "Matthew Johnson-Roberson", "Weiming Zhi"], "title": "Bi-Manual Joint Camera Calibration and Scene Representation", "categories": ["cs.RO", "cs.CV", "cs.LG"], "comment": null, "summary": "Robot manipulation, especially bimanual manipulation, often requires setting\nup multiple cameras on multiple robot manipulators. Before robot manipulators\ncan generate motion or even build representations of their environments, the\ncameras rigidly mounted to the robot need to be calibrated. Camera calibration\nis a cumbersome process involving collecting a set of images, with each\ncapturing a pre-determined marker. In this work, we introduce the Bi-Manual\nJoint Calibration and Representation Framework (Bi-JCR). Bi-JCR enables\nmultiple robot manipulators, each with cameras mounted, to circumvent taking\nimages of calibration markers. By leveraging 3D foundation models for dense,\nmarker-free multi-view correspondence, Bi-JCR jointly estimates: (i) the\nextrinsic transformation from each camera to its end-effector, (ii) the\ninter-arm relative poses between manipulators, and (iii) a unified,\nscale-consistent 3D representation of the shared workspace, all from the same\ncaptured RGB image sets. The representation, jointly constructed from images\ncaptured by cameras on both manipulators, lives in a common coordinate frame\nand supports collision checking and semantic segmentation to facilitate\ndownstream bimanual coordination tasks. We empirically evaluate the robustness\nof Bi-JCR on a variety of tabletop environments, and demonstrate its\napplicability on a variety of downstream tasks."}
{"id": "2411.06824", "pdf": "https://arxiv.org/pdf/2411.06824", "abs": "https://arxiv.org/abs/2411.06824", "authors": ["Megh Thakkar", "Quentin Fournier", "Matthew Riemer", "Pin-Yu Chen", "Amal Zouaq", "Payel Das", "Sarath Chandar"], "title": "Combining Domain and Alignment Vectors to Achieve Better Knowledge-Safety Trade-offs in LLMs", "categories": ["cs.AI"], "comment": null, "summary": "There is a growing interest in training domain-expert LLMs that excel in\nspecific technical fields compared to their general-purpose instruction-tuned\ncounterparts. However, these expert models often experience a loss in their\nsafety abilities in the process, making them capable of generating harmful\ncontent. As a solution, we introduce an efficient and effective merging-based\nalignment method called \\textsc{MergeAlign} that interpolates the domain and\nalignment vectors, creating safer domain-specific models while preserving their\nutility. We apply \\textsc{MergeAlign} on Llama3 variants that are experts in\nmedicine and finance, obtaining substantial alignment improvements with minimal\nto no degradation on domain-specific benchmarks. We study the impact of model\nmerging through model similarity metrics and contributions of individual models\nbeing merged. We hope our findings open new research avenues and inspire more\nefficient development of safe expert LLMs."}
{"id": "2505.23858", "pdf": "https://arxiv.org/pdf/2505.23858", "abs": "https://arxiv.org/abs/2505.23858", "authors": ["Aashwin Mishra", "Matt Seaberg", "Ryan Roussel", "Fred Poitevin", "Jana Thayer", "Daniel Ratner", "Auralee Edelen", "Apurva Mehta"], "title": "A Start To End Machine Learning Approach To Maximize Scientific Throughput From The LCLS-II-HE", "categories": ["physics.ins-det", "cs.LG", "hep-ex"], "comment": null, "summary": "With the increasing brightness of Light sources, including the\nDiffraction-Limited brightness upgrade of APS and the high-repetition-rate\nupgrade of LCLS, the proposed experiments therein are becoming increasingly\ncomplex. For instance, experiments at LCLS-II-HE will require the X-ray beam to\nbe within a fraction of a micron in diameter, with pointing stability of a few\nnanoradians, at the end of a kilometer-long electron accelerator, a\nhundred-meter-long undulator section, and tens of meters long X-ray optics.\nThis enhancement of brightness will increase the data production rate to rival\nthe largest data generators in the world. Without real-time active feedback\ncontrol and an optimized pipeline to transform measurements to scientific\ninformation and insights, researchers will drown in a deluge of mostly useless\ndata, and fail to extract the highly sophisticated insights that the recent\nbrightness upgrades promise.\n  In this article, we outline the strategy we are developing at SLAC to\nimplement Machine Learning driven optimization, automation and real-time\nknowledge extraction from the electron-injector at the start of the electron\naccelerator, to the multidimensional X-ray optical systems, and till the\nexperimental endstations and the high readout rate, multi-megapixel detectors\nat LCLS to deliver the design performance to the users. This is illustrated via\nexamples from Accelerator, Optics and End User applications."}
{"id": "2505.24195", "pdf": "https://arxiv.org/pdf/2505.24195", "abs": "https://arxiv.org/abs/2505.24195", "authors": ["Zining Wang", "Yuxuan Zhang", "Dongwook Yoon", "Nicholas Vincent", "Farhan Samir", "Vered Shwartz"], "title": "WikiGap: Promoting Epistemic Equity by Surfacing Knowledge Gaps Between English Wikipedia and other Language Editions", "categories": ["cs.HC", "cs.CL"], "comment": null, "summary": "With more than 11 times as many pageviews as the next, English Wikipedia\ndominates global knowledge access relative to other language editions. Readers\nare prone to assuming English Wikipedia as a superset of all language editions,\nleading many to prefer it even when their primary language is not English.\nOther language editions, however, comprise complementary facts rooted in their\nrespective cultures and media environments, which are marginalized in English\nWikipedia. While Wikipedia's user interface enables switching between language\neditions through its Interlanguage Link (ILL) system, it does not reveal to\nreaders that other language editions contain valuable, complementary\ninformation. We present WikiGap, a system that surfaces complementary facts\nsourced from other Wikipedias within the English Wikipedia interface.\nSpecifically, by combining a recent multilingual information-gap discovery\nmethod with a user-centered design, WikiGap enables access to complementary\ninformation from French, Russian, and Chinese Wikipedia. In a mixed-methods\nstudy (n=21), WikiGap significantly improved fact-finding accuracy, reduced\ntask time, and received a 32-point higher usability score relative to\nWikipedia's current ILL-based navigation system. Participants reported\nincreased awareness of the availability of complementary information in\nnon-English editions and reconsidered the completeness of English Wikipedia.\nWikiGap thus paves the way for improved epistemic equity across language\neditions."}
{"id": "2309.15818", "pdf": "https://arxiv.org/pdf/2309.15818", "abs": "https://arxiv.org/abs/2309.15818", "authors": ["David Junhao Zhang", "Jay Zhangjie Wu", "Jia-Wei Liu", "Rui Zhao", "Lingmin Ran", "Yuchao Gu", "Difei Gao", "Mike Zheng Shou"], "title": "Show-1: Marrying Pixel and Latent Diffusion Models for Text-to-Video Generation", "categories": ["cs.CV"], "comment": "project page is https://showlab.github.io/Show-1", "summary": "Significant advancements have been achieved in the realm of large-scale\npre-trained text-to-video Diffusion Models (VDMs). However, previous methods\neither rely solely on pixel-based VDMs, which come with high computational\ncosts, or on latent-based VDMs, which often struggle with precise text-video\nalignment. In this paper, we are the first to propose a hybrid model, dubbed as\nShow-1, which marries pixel-based and latent-based VDMs for text-to-video\ngeneration. Our model first uses pixel-based VDMs to produce a low-resolution\nvideo of strong text-video correlation. After that, we propose a novel expert\ntranslation method that employs the latent-based VDMs to further upsample the\nlow-resolution video to high resolution, which can also remove potential\nartifacts and corruptions from low-resolution videos. Compared to latent VDMs,\nShow-1 can produce high-quality videos of precise text-video alignment;\nCompared to pixel VDMs, Show-1 is much more efficient (GPU memory usage during\ninference is 15G vs 72G). Furthermore, our Show-1 model can be readily adapted\nfor motion customization and video stylization applications through simple\ntemporal attention layer finetuning. Our model achieves state-of-the-art\nperformance on standard video generation benchmarks. Our code and model weights\nare publicly available at https://github.com/showlab/Show-1."}
{"id": "2411.09909", "pdf": "https://arxiv.org/pdf/2411.09909", "abs": "https://arxiv.org/abs/2411.09909", "authors": ["Janghwan Lee", "Jiwoong Park", "Jinseok Kim", "Yongjik Kim", "Jungju Oh", "Jinwook Oh", "Jungwook Choi"], "title": "AMXFP4: Taming Activation Outliers with Asymmetric Microscaling Floating-Point for 4-bit LLM Inference", "categories": ["cs.AI"], "comment": "Updated formatting", "summary": "As large language models (LLMs) grow in parameter size and context length,\ncomputation precision has been reduced from 16-bit to 4-bit to improve\ninference efficiency. However, this reduction causes accuracy degradation due\nto activation outliers. Rotation-based INT4 methods address this via matrix\ncalibration, but they introduce multi-hour overheads and leave key computations\nin full precision. Microscaling (MX) floating-point (FP) formats offer\nfine-grained representation with a shared scale, enabling fully quantized\nmatrix multiplications through direct casting without calibration. However,\nexisting research shows unsatisfactory empirical results for MXFP4 inference,\nand the robustness of MX formats remains largely unexplored. In this work, we\nuncover the fundamental tradeoffs of the MX format: while it effectively\nsuppresses activation outliers, it does so at the cost of increased group-wise\nasymmetry. To address this, we propose AMXFP4, a 4-bit asymmetric FP format\nthat handles both issues using asymmetric shared scales, without requiring\ncalibration. Our custom MAC engine adds negligible hardware cost while\nimproving accuracy: AMXFP4 outperforms MXFP4 by 3% on VQA and exceeds\nrotation-based methods by 1.6% on CSQA. It also surpasses recently deployed\ncommercial MXFP4 variants. Code: https://github.com/aiha-lab/MX-QLLM"}
{"id": "2505.23869", "pdf": "https://arxiv.org/pdf/2505.23869", "abs": "https://arxiv.org/abs/2505.23869", "authors": ["M. SÃ¼zen"], "title": "Gibbs randomness-compression proposition: An efficient deep learning", "categories": ["stat.ML", "cs.LG"], "comment": "5 pages, 5 figures", "summary": "A proposition that connects randomness and compression put forward via Gibbs\nentropy over set of measurement vectors associated with a compression process.\nThe proposition states that a lossy compression process is equivalent to {\\it\ndirected randomness} that preserves information content. The proposition\noriginated from the observed behaviour in newly proposed {\\it Dual Tomographic\nCompression} (DTC) compress-train framework. This is akin to tomographic\nreconstruction of layer weight matrices via building compressed sensed\nprojections, so called {\\it weight rays}. This tomographic approach is applied\nto previous and next layers in a dual fashion, that triggers neuronal-level\npruning. This novel model compress-train scheme appear in iterative fashion and\nact as smart neural architecture search, Experiments demonstrated utility of\nthis dual-tomography producing state-of-the-art performance with efficient\ncompression during training, accelerating and supporting lottery ticket\nhypothesis. However, random compress-train iterations having similar\nperformance demonstrated the connection between randomness and compression from\nstatistical physics perspective, we formulated so called {\\it Gibbs\nrandomness-compression proposition}, signifying randomness-compression\nrelationship via Gibbs entropy. Practically, DTC framework provides a promising\napproach for massively energy and resource efficient deep learning training\napproach."}
{"id": "2312.02219", "pdf": "https://arxiv.org/pdf/2312.02219", "abs": "https://arxiv.org/abs/2312.02219", "authors": ["AndrÃ©s Villa", "Juan Carlos LeÃ³n AlcÃ¡zar", "Alvaro Soto", "Bernard Ghanem"], "title": "Behind the Magic, MERLIM: Multi-modal Evaluation Benchmark for Large Image-Language Models", "categories": ["cs.CV", "cs.CL"], "comment": "18 pages, 10 figures, 6 tables", "summary": "Large Vision and Language Models have enabled significant advances in fully\nsupervised and zero-shot visual tasks. These large architectures serve as the\nbaseline to what is currently known as Instruction Tuning Large Vision and\nLanguage models (IT-LVLMs). IT-LVLMs are general-purpose multi-modal assistants\nwhose responses are modulated by natural language instructions and visual data.\nDespite this versatility, IT-LVLM effectiveness in fundamental computer vision\nproblems remains unclear, primarily due to the absence of a standardized\nevaluation benchmark. This paper introduces a Multi-modal Evaluation Benchmark\nnamed MERLIM, a scalable test-bed to assess the capabilities of IT-LVLMs on\nfundamental computer vision tasks. MERLIM contains over 300K image-question\npairs and has a strong focus on detecting cross-modal \"hallucination\" events in\nIT-LVLMs. Our results bring important insights on the performance of\nstate-of-the-art IT-LVLMs including limitations at identifying fine-grained\nvisual concepts, object hallucinations across tasks, and biases towards the\nlanguage query. Our findings also suggest that these models have weak visual\ngrounding, but manage to make adequate guesses from global visual patterns or\nlanguage biases contained in the LLM component. We name this phenomenon of\ncorrect answers with no visual grounding as hidden hallucinations."}
{"id": "2412.13682", "pdf": "https://arxiv.org/pdf/2412.13682", "abs": "https://arxiv.org/abs/2412.13682", "authors": ["Jie-Jing Shao", "Bo-Wen Zhang", "Xiao-Wen Yang", "Baizhi Chen", "Si-Yu Han", "Wen-Da Wei", "Guohao Cai", "Zhenhua Dong", "Lan-Zhe Guo", "Yu-feng Li"], "title": "ChinaTravel: An Open-Ended Benchmark for Language Agents in Chinese Travel Planning", "categories": ["cs.AI", "cs.CL"], "comment": "Webpage: https://www.lamda.nju.edu.cn/shaojj/chinatravel", "summary": "Recent advances in LLMs, particularly in language reasoning and tool\nintegration, have rapidly sparked the \\emph{Language Agents} for real-world\ndevelopment. Among these, travel planning represents a prominent domain,\ncombining complex multi-objective planning challenges with practical deployment\ndemands. However, existing benchmarks often oversimplify real-world\nrequirements by focusing on synthetic queries and limited constraints. We\naddress the gap of evaluating language agents in multi-day, multi-POI travel\nplanning scenarios with diverse and open human needs. Specifically, we\nintroduce \\emph{ChinaTravel}, the first open-ended benchmark grounded in\nauthentic Chinese travel requirements collected from 1,154 human participants.\nWe design a compositionally generalizable domain-specific language (DSL) for\nscalable evaluation, covering feasibility, constraint satisfaction, and\npreference comparison. Empirical studies reveal the potential of neuro-symbolic\nagents in travel planning, achieving a 37.0\\% constraint satisfaction rate on\nhuman queries, a 10\\times improvement over purely neural models. These findings\nhighlight ChinaTravel as a pivotal milestone for advancing language agents in\ncomplex, real-world planning scenarios."}
{"id": "2312.02980", "pdf": "https://arxiv.org/pdf/2312.02980", "abs": "https://arxiv.org/abs/2312.02980", "authors": ["Zhangyang Qi", "Ye Fang", "Zeyi Sun", "Xiaoyang Wu", "Tong Wu", "Jiaqi Wang", "Dahua Lin", "Hengshuang Zhao"], "title": "GPT4Point: A Unified Framework for Point-Language Understanding and Generation", "categories": ["cs.CV"], "comment": null, "summary": "Multimodal Large Language Models (MLLMs) have excelled in 2D image-text\ncomprehension and image generation, but their understanding of the 3D world is\nnotably deficient, limiting progress in 3D language understanding and\ngeneration. To solve this problem, we introduce GPT4Point, an innovative\ngroundbreaking point-language multimodal model designed specifically for\nunified 3D object understanding and generation within the MLLM framework.\nGPT4Point as a powerful 3D MLLM seamlessly can execute a variety of point-text\nreference tasks such as point-cloud captioning and Q&A. Additionally, GPT4Point\nis equipped with advanced capabilities for controllable 3D generation, it can\nget high-quality results through a low-quality point-text feature maintaining\nthe geometric shapes and colors. To support the expansive needs of 3D\nobject-text pairs, we develop Pyramid-XL, a point-language dataset annotation\nengine. It constructs a large-scale database over 1M objects of varied text\ngranularity levels from the Objaverse-XL dataset, essential for training\nGPT4Point. A comprehensive benchmark has been proposed to evaluate 3D\npoint-language understanding capabilities. In extensive evaluations, GPT4Point\nhas demonstrated superior performance in understanding and generation."}
{"id": "2502.04675", "pdf": "https://arxiv.org/pdf/2502.04675", "abs": "https://arxiv.org/abs/2502.04675", "authors": ["Xueru Wen", "Jie Lou", "Xinyu Lu", "Junjie Yang", "Yanjiang Liu", "Yaojie Lu", "Debing Zhang", "Xing Yu"], "title": "Scalable Oversight for Superhuman AI via Recursive Self-Critiquing", "categories": ["cs.AI", "cs.CL", "cs.LG"], "comment": null, "summary": "As AI capabilities increasingly surpass human proficiency in complex tasks,\ncurrent alignment techniques including SFT and RLHF face fundamental challenges\nin ensuring reliable oversight. These methods rely on direct human assessment\nand become untenable when AI outputs exceed human cognitive thresholds. In\nresponse to this challenge, we explore two hypotheses: (1) \\textit{Critique of\ncritique can be easier than critique itself}, extending the widely-accepted\nobservation that verification is easier than generation to the critique domain,\nas critique itself is a specialized form of generation; (2) \\textit{This\ndifficulty relationship is recursively held}, suggesting that when direct\nevaluation is infeasible, performing high-order critiques (e.g., critique of\ncritique of critique) offers a more tractable supervision pathway. We further\nconduct Human-AI and AI-AI experiments to investigate the potential of\nutilizing recursive self-critiquing for AI supervision. Our results highlight\nrecursive critique as a promising approach for scalable AI oversight."}
{"id": "2505.24006", "pdf": "https://arxiv.org/pdf/2505.24006", "abs": "https://arxiv.org/abs/2505.24006", "authors": ["Agnideep Aich", "Sameera Hewage", "Md Monzur Murshed", "Ashit Baran Aich", "Amanda Mayeaux", "Asim K. Dey", "Kumer P. Das", "Bruce Wade"], "title": "A2 Copula-Driven Spatial Bayesian Neural Network For Modeling Non-Gaussian Dependence: A Simulation Study", "categories": ["stat.ME", "cs.LG", "stat.ML", "62H12, 62P10, 65C20, 62F15, 68T07"], "comment": null, "summary": "In this paper, we introduce the A2 Copula Spatial Bayesian Neural Network\n(A2-SBNN), a predictive spatial model designed to map coordinates to continuous\nfields while capturing both typical spatial patterns and extreme dependencies.\nBy embedding the dual-tail novel Archimedean copula viz. A2 directly into the\nnetwork's weight initialization, A2-SBNN naturally models complex spatial\nrelationships, including rare co-movements in the data. The model is trained\nthrough a calibration-driven process combining Wasserstein loss, moment\nmatching, and correlation penalties to refine predictions and manage\nuncertainty. Simulation results show that A2-SBNN consistently delivers high\naccuracy across a wide range of dependency strengths, offering a new, effective\nsolution for spatial data modeling beyond traditional Gaussian-based\napproaches."}
{"id": "2403.05852", "pdf": "https://arxiv.org/pdf/2403.05852", "abs": "https://arxiv.org/abs/2403.05852", "authors": ["Hanzheng Wang", "Wei Li", "Xiang-Gen Xia", "Qian Du", "Jing Tian"], "title": "SSF-Net: Spatial-Spectral Fusion Network with Spectral Angle Awareness for Hyperspectral Object Tracking", "categories": ["cs.CV"], "comment": "IEEE Transactions on Image Processing 2025", "summary": "Hyperspectral video (HSV) offers valuable spatial, spectral, and temporal\ninformation simultaneously, making it highly suitable for handling challenges\nsuch as background clutter and visual similarity in object tracking. However,\nexisting methods primarily focus on band regrouping and rely on RGB trackers\nfor feature extraction, resulting in limited exploration of spectral\ninformation and difficulties in achieving complementary representations of\nobject features. In this paper, a spatial-spectral fusion network with spectral\nangle awareness (SST-Net) is proposed for hyperspectral (HS) object tracking.\nFirstly, to address the issue of insufficient spectral feature extraction in\nexisting networks, a spatial-spectral feature backbone ($S^2$FB) is designed.\nWith the spatial and spectral extraction branch, a joint representation of\ntexture and spectrum is obtained. Secondly, a spectral attention fusion module\n(SAFM) is presented to capture the intra- and inter-modality correlation to\nobtain the fused features from the HS and RGB modalities. It can incorporate\nthe visual information into the HS spectral context to form a robust\nrepresentation. Thirdly, to ensure a more accurate response of the tracker to\nthe object position, a spectral angle awareness module (SAAM) investigates the\nregion-level spectral similarity between the template and search images during\nthe prediction stage. Furthermore, we develop a novel spectral angle awareness\nloss (SAAL) to offer guidance for the SAAM based on similar regions. Finally,\nto obtain the robust tracking results, a weighted prediction method is\nconsidered to combine the HS and RGB predicted motions of objects to leverage\nthe strengths of each modality. Extensive experiments on the HOTC dataset\ndemonstrate the effectiveness of the proposed SSF-Net, compared with\nstate-of-the-art trackers."}
{"id": "2502.11357", "pdf": "https://arxiv.org/pdf/2502.11357", "abs": "https://arxiv.org/abs/2502.11357", "authors": ["Vardaan Pahuja", "Yadong Lu", "Corby Rosset", "Boyu Gou", "Arindam Mitra", "Spencer Whitehead", "Yu Su", "Ahmed Awadallah"], "title": "Explorer: Scaling Exploration-driven Web Trajectory Synthesis for Multimodal Web Agents", "categories": ["cs.AI", "cs.HC"], "comment": "ACL 2025 (Findings)", "summary": "Recent success in large multimodal models (LMMs) has sparked promising\napplications of agents capable of autonomously completing complex web tasks.\nWhile open-source LMM agents have made significant advances in offline\nevaluation benchmarks, their performance still falls substantially short of\nhuman-level capabilities in more realistic online settings. A key bottleneck is\nthe lack of diverse and large-scale trajectory-level datasets across various\ndomains, which are expensive to collect. In this paper, we address this\nchallenge by developing a scalable recipe to synthesize the largest and most\ndiverse trajectory-level dataset to date, containing over 94K successful\nmultimodal web trajectories, spanning 49K unique URLs, 720K screenshots, and\n33M web elements. In particular, we leverage extensive web exploration and\nrefinement to obtain diverse task intents. The average cost is 28 cents per\nsuccessful trajectory, making it affordable to a wide range of users in the\ncommunity. Leveraging this dataset, we train Explorer, a multimodal web agent,\nand demonstrate strong performance on both offline and online web agent\nbenchmarks such as Mind2Web-Live, Multimodal-Mind2Web, and MiniWob++.\nAdditionally, our experiments highlight data scaling as a key driver for\nimproving web agent capabilities. We hope this study makes state-of-the-art\nLMM-based agent research at a larger scale more accessible."}
{"id": "2505.24032", "pdf": "https://arxiv.org/pdf/2505.24032", "abs": "https://arxiv.org/abs/2505.24032", "authors": ["Sergei S. Kuzmin", "Ivan V. Dyakonov", "Stanislav S. Straupe"], "title": "Leveraging machine learning features for linear optical interferometer control", "categories": ["quant-ph", "cs.LG", "physics.optics"], "comment": "8 pages, 8 figures", "summary": "We have developed an algorithm that constructs a model of a reconfigurable\noptical interferometer, independent of specific architectural constraints. The\nprogramming of unitary transformations on the interferometer's optical modes\nrelies on either an analytical method for deriving the unitary matrix from a\nset of phase shifts or an optimization routine when such decomposition is not\navailable. Our algorithm employs a supervised learning approach, aligning the\ninterferometer model with a training set derived from the device being studied.\nA straightforward optimization procedure leverages this trained model to\ndetermine the phase shifts of the interferometer with a specific architecture,\nobtaining the required unitary transformation. This approach enables the\neffective tuning of interferometers without requiring a precise analytical\nsolution, paving the way for the exploration of new interferometric circuit\narchitectures."}
{"id": "2404.18919", "pdf": "https://arxiv.org/pdf/2404.18919", "abs": "https://arxiv.org/abs/2404.18919", "authors": ["Junhao Cheng", "Baiqiao Yin", "Kaixin Cai", "Minbin Huang", "Hanhui Li", "Yuxin He", "Xi Lu", "Yue Li", "Yifei Li", "Yuhao Cheng", "Yiqiang Yan", "Xiaodan Liang"], "title": "TheaterGen: Character Management with LLM for Consistent Multi-turn Image Generation", "categories": ["cs.CV"], "comment": null, "summary": "Recent advances in diffusion models can generate high-quality and stunning\nimages from text. However, multi-turn image generation, which is of high demand\nin real-world scenarios, still faces challenges in maintaining semantic\nconsistency between images and texts, as well as contextual consistency of the\nsame subject across multiple interactive turns. To address this issue, we\nintroduce TheaterGen, a training-free framework that integrates large language\nmodels (LLMs) and text-to-image (T2I) models to provide the capability of\nmulti-turn image generation. Within this framework, LLMs, acting as a\n\"Screenwriter\", engage in multi-turn interaction, generating and managing a\nstandardized prompt book that encompasses prompts and layout designs for each\ncharacter in the target image. Based on these, Theatergen generate a list of\ncharacter images and extract guidance information, akin to the \"Rehearsal\".\nSubsequently, through incorporating the prompt book and guidance information\ninto the reverse denoising process of T2I diffusion models, Theatergen generate\nthe final image, as conducting the \"Final Performance\". With the effective\nmanagement of prompt books and character images, TheaterGen significantly\nimproves semantic and contextual consistency in synthesized images.\nFurthermore, we introduce a dedicated benchmark, CMIGBench (Consistent\nMulti-turn Image Generation Benchmark) with 8000 multi-turn instructions.\nDifferent from previous multi-turn benchmarks, CMIGBench does not define\ncharacters in advance. Both the tasks of story generation and multi-turn\nediting are included on CMIGBench for comprehensive evaluation. Extensive\nexperimental results show that TheaterGen outperforms state-of-the-art methods\nsignificantly. It raises the performance bar of the cutting-edge Mini DALLE 3\nmodel by 21% in average character-character similarity and 19% in average\ntext-image similarity."}
{"id": "2502.13001", "pdf": "https://arxiv.org/pdf/2502.13001", "abs": "https://arxiv.org/abs/2502.13001", "authors": ["Frederic Kirstein", "Muneeb Khan", "Jan Philip Wahle", "Terry Ruas", "Bela Gipp"], "title": "You need to MIMIC to get FAME: Solving Meeting Transcript Scarcity with a Multi-Agent Conversations", "categories": ["cs.AI", "cs.CL"], "comment": "Accepted at ACL 2025 (Findings)", "summary": "Meeting summarization suffers from limited high-quality data, mainly due to\nprivacy restrictions and expensive collection processes. We address this gap\nwith FAME, a dataset of 500 meetings in English and 300 in German produced by\nMIMIC, our new multi-agent meeting synthesis framework that generates meeting\ntranscripts on a given knowledge source by defining psychologically grounded\nparticipant profiles, outlining the conversation, and orchestrating a large\nlanguage model (LLM) debate. A modular post-processing step refines these\noutputs, mitigating potential repetitiveness and overly formal tones, ensuring\ncoherent, credible dialogues at scale. We also propose a psychologically\ngrounded evaluation framework assessing naturalness, social behavior\nauthenticity, and transcript difficulties. Human assessments show that FAME\napproximates real-meeting spontaneity (4.5/5 in naturalness), preserves\nspeaker-centric challenges (3/5 in spoken language), and introduces richer\ninformation-oriented difficulty (4/5 in difficulty). These findings highlight\nthat FAME is a good and scalable proxy for real-world meeting conditions. It\nenables new test scenarios for meeting summarization research and other\nconversation-centric applications in tasks requiring conversation data or\nsimulating social scenarios under behavioral constraints."}
{"id": "2505.24065", "pdf": "https://arxiv.org/pdf/2505.24065", "abs": "https://arxiv.org/abs/2505.24065", "authors": ["Isaiah A. Moses", "Chen Chen", "Joan M. Redwing", "Wesley F. Reinhart"], "title": "Cross-Modal Characterization of Thin Film MoS$_2$ Using Generative Models", "categories": ["cond-mat.mtrl-sci", "cs.LG", "physics.app-ph"], "comment": "36 pages, 10 figures, 10 tables", "summary": "The growth and characterization of materials using empirical optimization\ntypically requires a significant amount of expert time, experience, and\nresources. Several complementary characterization methods are routinely\nperformed to determine the quality and properties of a grown sample. Machine\nlearning (ML) can support the conventional approaches by using historical data\nto guide and provide speed and efficiency to the growth and characterization of\nmaterials. Specifically, ML can provide quantitative information from\ncharacterization data that is typically obtained from a different modality. In\nthis study, we have investigated the feasibility of projecting the quantitative\nmetric from microscopy measurements, such as atomic force microscopy (AFM),\nusing data obtained from spectroscopy measurements, like Raman spectroscopy.\nGenerative models were also trained to generate the full and specific features\nof the Raman and photoluminescence spectra from each other and the AFM images\nof the thin film MoS$_2$. The results are promising and have provided a\nfoundational guide for the use of ML for the cross-modal characterization of\nmaterials for their accelerated, efficient, and cost-effective discovery."}
{"id": "2401.08491", "pdf": "https://arxiv.org/pdf/2401.08491", "abs": "https://arxiv.org/abs/2401.08491", "authors": ["Tassilo Klein", "Moin Nabi"], "title": "Contrastive Perplexity for Controlled Generation: An Application in Detoxifying Large Language Models", "categories": ["cs.CL", "cs.LG"], "comment": "Accepted to ACL 2025 (Main Track)", "summary": "The generation of toxic content by large language models (LLMs) remains a\ncritical challenge for the safe deployment of language technology. We propose a\nnovel framework for implicit knowledge editing and controlled text generation\nby fine-tuning LLMs with a prototype-based contrastive perplexity objective.\nCentral to our method is the construction of hard negatives - toxic outputs\nthat are generated through adversarial paraphrasing to be semantically similar\nand model probability to their non-toxic counterparts. By training on these\nchallenging and realistic pairs, our approach ensures robust and stable\ncontrastive optimization. Experimental results in the domain of detoxification\ndemonstrate that our method significantly reduces toxic generation while\nmaintaining strong performance on downstream tasks such as commonsense\nreasoning and reading comprehension. Our findings highlight the effectiveness\nof exploiting hard negatives for attribute-aware fine-tuning."}
{"id": "2405.14858", "pdf": "https://arxiv.org/pdf/2405.14858", "abs": "https://arxiv.org/abs/2405.14858", "authors": ["Feng Wang", "Jiahao Wang", "Sucheng Ren", "Guoyizhe Wei", "Jieru Mei", "Wei Shao", "Yuyin Zhou", "Alan Yuille", "Cihang Xie"], "title": "Mamba-R: Vision Mamba ALSO Needs Registers", "categories": ["cs.CV"], "comment": "Published in CVPR 2025", "summary": "Similar to Vision Transformers, this paper identifies artifacts also present\nwithin the feature maps of Vision Mamba. These artifacts, corresponding to\nhigh-norm tokens emerging in low-information background areas of images, appear\nmuch more severe in Vision Mamba -- they exist prevalently even with the\ntiny-sized model and activate extensively across background regions. To\nmitigate this issue, we follow the prior solution of introducing register\ntokens into Vision Mamba. To better cope with Mamba blocks' uni-directional\ninference paradigm, two key modifications are introduced: 1) evenly inserting\nregisters throughout the input token sequence, and 2) recycling registers for\nfinal decision predictions. We term this new architecture Mamba-R. Qualitative\nobservations suggest, compared to vanilla Vision Mamba, Mamba-R's feature maps\nappear cleaner and more focused on semantically meaningful regions.\nQuantitatively, Mamba-R attains stronger performance and scales better. For\nexample, on the ImageNet benchmark, our base-size Mamba-R attains 83.0%\naccuracy, significantly outperforming Vim-B's 81.8%; furthermore, we provide\nthe first successful scaling to the large model size (i.e., with 341M\nparameters), attaining a competitive accuracy of 83.6% (84.5% if finetuned with\n384x384 inputs). Additional validation on the downstream semantic segmentation\ntask also supports Mamba-R's efficacy. Code is available at\nhttps://github.com/wangf3014/Mamba-Reg."}
{"id": "2503.11718", "pdf": "https://arxiv.org/pdf/2503.11718", "abs": "https://arxiv.org/abs/2503.11718", "authors": ["Gabriele D'Acunto", "Claudio Battiloro"], "title": "The Relativity of Causal Knowledge", "categories": ["cs.AI", "cs.LG", "math.CT", "stat.ME"], "comment": "Accepted at UAI 2025. 19 pages, 2 figures", "summary": "Recent advances in artificial intelligence reveal the limits of purely\npredictive systems and call for a shift toward causal and collaborative\nreasoning. Drawing inspiration from the revolution of Grothendieck in\nmathematics, we introduce the relativity of causal knowledge, which posits\nstructural causal models (SCMs) are inherently imperfect, subjective\nrepresentations embedded within networks of relationships. By leveraging\ncategory theory, we arrange SCMs into a functor category and show that their\nobservational and interventional probability measures naturally form convex\nstructures. This result allows us to encode non-intervened SCMs with convex\nspaces of probability measures. Next, using sheaf theory, we construct the\nnetwork sheaf and cosheaf of causal knowledge. These structures enable the\ntransfer of causal knowledge across the network while incorporating\ninterventional consistency and the perspective of the subjects, ultimately\nleading to the formal, mathematical definition of relative causal knowledge."}
{"id": "2505.24097", "pdf": "https://arxiv.org/pdf/2505.24097", "abs": "https://arxiv.org/abs/2505.24097", "authors": ["Victor Li", "Baiting Chen", "Yuzhen Mao", "Qi Lei", "Zhun Deng"], "title": "Performative Risk Control: Calibrating Models for Reliable Deployment under Performativity", "categories": ["stat.ML", "cs.LG"], "comment": null, "summary": "Calibrating blackbox machine learning models to achieve risk control is\ncrucial to ensure reliable decision-making. A rich line of literature has been\nstudying how to calibrate a model so that its predictions satisfy explicit\nfinite-sample statistical guarantees under a fixed, static, and unknown\ndata-generating distribution. However, prediction-supported decisions may\ninfluence the outcome they aim to predict, a phenomenon named performativity of\npredictions, which is commonly seen in social science and economics. In this\npaper, we introduce Performative Risk Control, a framework to calibrate models\nto achieve risk control under performativity with provable theoretical\nguarantees. Specifically, we provide an iteratively refined calibration\nprocess, where we ensure the predictions are improved and risk-controlled\nthroughout the process. We also study different types of risk measures and\nchoices of tail bounds. Lastly, we demonstrate the effectiveness of our\nframework by numerical experiments on the task of predicting credit default\nrisk. To the best of our knowledge, this work is the first one to study\nstatistically rigorous risk control under performativity, which will serve as\nan important safeguard against a wide range of strategic manipulation in\ndecision-making processes."}
{"id": "2403.02839", "pdf": "https://arxiv.org/pdf/2403.02839", "abs": "https://arxiv.org/abs/2403.02839", "authors": ["Hui Huang", "Xingyuan Bu", "Hongli Zhou", "Yingqi Qu", "Jing Liu", "Muyun Yang", "Bing Xu", "Tiejun Zhao"], "title": "An Empirical Study of LLM-as-a-Judge for LLM Evaluation: Fine-tuned Judge Model is not a General Substitute for GPT-4", "categories": ["cs.CL"], "comment": "Accepted to Findings of ACL2025", "summary": "Recently, there has been a growing trend of utilizing Large Language Model\n(LLM) to evaluate the quality of other LLMs. Many studies have fine-tuned judge\nmodels based on open-source LLMs for evaluation. While the fine-tuned judge\nmodels are claimed to achieve comparable evaluation capability with GPT-4, in\nthis work, we conduct an empirical study of LLM-as-a-Judge. Our findings\nindicate that although the fine-tuned judge models achieve high performance on\nin-domain test sets, even surpassing GPT-4, they underperform GPT-4 across\nseveral dimensions, including generalizability, fairness and adaptability. We\nalso reveal that the fine-tuned judge model inherently operates as a\ntask-specific classifier, consequently imposing the limitations."}
{"id": "2405.21075", "pdf": "https://arxiv.org/pdf/2405.21075", "abs": "https://arxiv.org/abs/2405.21075", "authors": ["Chaoyou Fu", "Yuhan Dai", "Yongdong Luo", "Lei Li", "Shuhuai Ren", "Renrui Zhang", "Zihan Wang", "Chenyu Zhou", "Yunhang Shen", "Mengdan Zhang", "Peixian Chen", "Yanwei Li", "Shaohui Lin", "Sirui Zhao", "Ke Li", "Tong Xu", "Xiawu Zheng", "Enhong Chen", "Caifeng Shan", "Ran He", "Xing Sun"], "title": "Video-MME: The First-Ever Comprehensive Evaluation Benchmark of Multi-modal LLMs in Video Analysis", "categories": ["cs.CV", "cs.CL"], "comment": "Project Page: https://video-mme.github.io", "summary": "In the quest for artificial general intelligence, Multi-modal Large Language\nModels (MLLMs) have emerged as a focal point in recent advancements. However,\nthe predominant focus remains on developing their capabilities in static image\nunderstanding. The potential of MLLMs in processing sequential visual data is\nstill insufficiently explored, highlighting the absence of a comprehensive,\nhigh-quality assessment of their performance. In this paper, we introduce\nVideo-MME, the first-ever full-spectrum, Multi-Modal Evaluation benchmark of\nMLLMs in Video analysis. Our work distinguishes from existing benchmarks\nthrough four key features: 1) Diversity in video types, spanning 6 primary\nvisual domains with 30 subfields to ensure broad scenario generalizability; 2)\nDuration in temporal dimension, encompassing both short-, medium-, and\nlong-term videos, ranging from 11 seconds to 1 hour, for robust contextual\ndynamics; 3) Breadth in data modalities, integrating multi-modal inputs besides\nvideo frames, including subtitles and audios, to unveil the all-round\ncapabilities of MLLMs; 4) Quality in annotations, utilizing rigorous manual\nlabeling by expert annotators to facilitate precise and reliable model\nassessment. 900 videos with a total of 254 hours are manually selected and\nannotated by repeatedly viewing all the video content, resulting in 2,700\nquestion-answer pairs. With Video-MME, we extensively evaluate various\nstate-of-the-art MLLMs, including GPT-4 series and Gemini 1.5 Pro, as well as\nopen-source image models like InternVL-Chat-V1.5 and video models like\nLLaVA-NeXT-Video. Our experiments reveal that Gemini 1.5 Pro is the\nbest-performing commercial model, significantly outperforming the open-source\nmodels. Our dataset along with these findings underscores the need for further\nimprovements in handling longer sequences and multi-modal data. Project Page:\nhttps://video-mme.github.io"}
{"id": "2505.10844", "pdf": "https://arxiv.org/pdf/2505.10844", "abs": "https://arxiv.org/abs/2505.10844", "authors": ["Simeng Han", "Stephen Xia", "Grant Zhang", "Howard Dai", "Chen Liu", "Lichang Chen", "Hoang Huy Nguyen", "Hongyuan Mei", "Jiayuan Mao", "R. Thomas McCoy"], "title": "Creativity or Brute Force? Using Brainteasers as a Window into the Problem-Solving Abilities of Large Language Models", "categories": ["cs.AI", "cs.CL"], "comment": "13 Tables; 5 Figures", "summary": "Accuracy remains a standard metric for evaluating AI systems, but it offers\nlimited insight into how models arrive at their solutions. In this work, we\nintroduce a benchmark based on brainteasers written in long narrative form to\nprobe more deeply into the types of reasoning strategies that models use.\nBrainteasers are well-suited for this goal because they can be solved with\nmultiple approaches, such as a few-step solution that uses a creative insight\nor a longer solution that uses more brute force. We investigate large language\nmodels (LLMs) across multiple layers of reasoning, focusing not only on\ncorrectness but also on the quality and creativity of their solutions. We\ninvestigate many aspects of the reasoning process: (1) semantic parsing of the\nbrainteasers into precise mathematical competition style formats; (2)\ngenerating solutions from these mathematical forms; (3) self-correcting\nsolutions based on gold solutions; (4) producing step-by-step sketches of\nsolutions; and (5) making use of hints. We find that LLMs are in many cases\nable to find creative, insightful solutions to brainteasers, suggesting that\nthey capture some of the capacities needed to solve novel problems in creative\nways. Nonetheless, there also remain situations where they rely on brute force\ndespite the availability of more efficient, creative solutions, highlighting a\npotential direction for improvement in the reasoning abilities of LLMs."}
{"id": "2505.24117", "pdf": "https://arxiv.org/pdf/2505.24117", "abs": "https://arxiv.org/abs/2505.24117", "authors": ["Ananya Omanwar", "Fady Alajaji", "TamÃ¡s Linder"], "title": "Bounds on the Excess Minimum Risk via Generalized Information Divergence Measures", "categories": ["cs.IT", "cs.LG", "math.IT"], "comment": null, "summary": "Given finite-dimensional random vectors $Y$, $X$, and $Z$ that form a Markov\nchain in that order (i.e., $Y \\to X \\to Z$), we derive upper bounds on the\nexcess minimum risk using generalized information divergence measures. Here,\n$Y$ is a target vector to be estimated from an observed feature vector $X$ or\nits stochastically degraded version $Z$. The excess minimum risk is defined as\nthe difference between the minimum expected loss in estimating $Y$ from $X$ and\nfrom $Z$. We present a family of bounds that generalize the mutual information\nbased bound of Gy\\\"orfi et al. (2023), using the R\\'enyi and\n$\\alpha$-Jensen-Shannon divergences, as well as Sibson's mutual information.\nOur bounds are similar to those developed by Modak et al. (2021) and Aminian et\nal. (2024) for the generalization error of learning algorithms. However, unlike\nthese works, our bounds do not require the sub-Gaussian parameter to be\nconstant and therefore apply to a broader class of joint distributions over\n$Y$, $X$, and $Z$. We also provide numerical examples under both constant and\nnon-constant sub-Gaussianity assumptions, illustrating that our generalized\ndivergence based bounds can be tighter than the one based on mutual information\nfor certain regimes of the parameter $\\alpha$."}
{"id": "2405.14189", "pdf": "https://arxiv.org/pdf/2405.14189", "abs": "https://arxiv.org/abs/2405.14189", "authors": ["Yihao Huang", "Chong Wang", "Xiaojun Jia", "Qing Guo", "Felix Juefei-Xu", "Jian Zhang", "Geguang Pu", "Yang Liu"], "title": "Efficient Universal Goal Hijacking with Semantics-guided Prompt Organization", "categories": ["cs.CL", "cs.CV"], "comment": "accepted by ACL 2025", "summary": "Universal goal hijacking is a kind of prompt injection attack that forces\nLLMs to return a target malicious response for arbitrary normal user prompts.\nThe previous methods achieve high attack performance while being too cumbersome\nand time-consuming. Also, they have concentrated solely on optimization\nalgorithms, overlooking the crucial role of the prompt. To this end, we propose\na method called POUGH that incorporates an efficient optimization algorithm and\ntwo semantics-guided prompt organization strategies. Specifically, our method\nstarts with a sampling strategy to select representative prompts from a\ncandidate pool, followed by a ranking strategy that prioritizes them. Given the\nsequentially ranked prompts, our method employs an iterative optimization\nalgorithm to generate a fixed suffix that can concatenate to arbitrary user\nprompts for universal goal hijacking. Experiments conducted on four popular\nLLMs and ten types of target responses verified the effectiveness."}
{"id": "2406.01388", "pdf": "https://arxiv.org/pdf/2406.01388", "abs": "https://arxiv.org/abs/2406.01388", "authors": ["Junhao Cheng", "Xi Lu", "Hanhui Li", "Khun Loun Zai", "Baiqiao Yin", "Yuhao Cheng", "Yiqiang Yan", "Xiaodan Liang"], "title": "AutoStudio: Crafting Consistent Subjects in Multi-turn Interactive Image Generation", "categories": ["cs.CV"], "comment": "Multi-turn interactive image generation", "summary": "As cutting-edge Text-to-Image (T2I) generation models already excel at\nproducing remarkable single images, an even more challenging task, i.e.,\nmulti-turn interactive image generation begins to attract the attention of\nrelated research communities. This task requires models to interact with users\nover multiple turns to generate a coherent sequence of images. However, since\nusers may switch subjects frequently, current efforts struggle to maintain\nsubject consistency while generating diverse images. To address this issue, we\nintroduce a training-free multi-agent framework called AutoStudio. AutoStudio\nemploys three agents based on large language models (LLMs) to handle\ninteractions, along with a stable diffusion (SD) based agent for generating\nhigh-quality images. Specifically, AutoStudio consists of (i) a subject manager\nto interpret interaction dialogues and manage the context of each subject, (ii)\na layout generator to generate fine-grained bounding boxes to control subject\nlocations, (iii) a supervisor to provide suggestions for layout refinements,\nand (iv) a drawer to complete image generation. Furthermore, we introduce a\nParallel-UNet to replace the original UNet in the drawer, which employs two\nparallel cross-attention modules for exploiting subject-aware features. We also\nintroduce a subject-initialized generation method to better preserve small\nsubjects. Our AutoStudio hereby can generate a sequence of multi-subject images\ninteractively and consistently. Extensive experiments on the public CMIGBench\nbenchmark and human evaluations show that AutoStudio maintains multi-subject\nconsistency across multiple turns well, and it also raises the state-of-the-art\nperformance by 13.65% in average Frechet Inception Distance and 2.83% in\naverage character-character similarity."}
{"id": "2505.11942", "pdf": "https://arxiv.org/pdf/2505.11942", "abs": "https://arxiv.org/abs/2505.11942", "authors": ["Junhao Zheng", "Xidi Cai", "Qiuke Li", "Duzhen Zhang", "ZhongZhi Li", "Yingying Zhang", "Le Song", "Qianli Ma"], "title": "LifelongAgentBench: Evaluating LLM Agents as Lifelong Learners", "categories": ["cs.AI"], "comment": "Project Page: https://caixd-220529.github.io/LifelongAgentBench/", "summary": "Lifelong learning is essential for intelligent agents operating in dynamic\nenvironments. Current large language model (LLM)-based agents, however, remain\nstateless and unable to accumulate or transfer knowledge over time. Existing\nbenchmarks treat agents as static systems and fail to evaluate lifelong\nlearning capabilities. We present LifelongAgentBench, the first unified\nbenchmark designed to systematically assess the lifelong learning ability of\nLLM agents. It provides skill-grounded, interdependent tasks across three\ninteractive environments, Database, Operating System, and Knowledge Graph, with\nautomatic label verification, reproducibility, and modular extensibility.\nExtensive experiments reveal that conventional experience replay has limited\neffectiveness for LLM agents due to irrelevant information and context length\nconstraints. We further introduce a group self-consistency mechanism that\nsignificantly improves lifelong learning performance. We hope\nLifelongAgentBench will advance the development of adaptive, memory-capable LLM\nagents."}
{"id": "2505.24132", "pdf": "https://arxiv.org/pdf/2505.24132", "abs": "https://arxiv.org/abs/2505.24132", "authors": ["Kai Fukami", "Ryo Araki"], "title": "Information-theoretic machine learning for time-varying mode decomposition of separated airfoil wakes", "categories": ["physics.flu-dyn", "cs.LG", "physics.comp-ph"], "comment": null, "summary": "We perform an information-theoretic mode decomposition for separated wakes\naround a wing. The current data-driven approach based on a neural network\nreferred to as deep sigmoidal flow enables the extraction of an informative\ncomponent from a given flow field snapshot with respect to a target variable at\na future time stamp, thereby capturing the causality as a time-varying modal\nstructure. We consider three examples of separated flows around a NACA0012\nairfoil, namely, 1. laminar periodic wake at post-stall angles of attack, 2.\nstrong vortex-airfoil interactions, and 3. a turbulent wake in a\nspanwise-periodic domain. The present approach reveals informative vortical\nstructures associated with a time-varying lift response. For the periodic\nshedding cases, the informative structures vary in time corresponding to the\nfluctuation level from their mean values. With the second example of\nvortex-airfoil interactions, how the effect of vortex gust on a wing emerges in\nthe lift response over time is identified in an interpretable manner.\nFurthermore, for the case of turbulent wake, the present model highlights\nstructures near the wing and vortex cores as informative components based\nsolely on the information metric without any prior knowledge of aerodynamics\nand length scales. This study provides causality-based insights into a range of\nunsteady aerodynamic problems."}
{"id": "2406.10099", "pdf": "https://arxiv.org/pdf/2406.10099", "abs": "https://arxiv.org/abs/2406.10099", "authors": ["Jiaqi Li", "Yixuan Tang", "Yi Yang"], "title": "Know the Unknown: An Uncertainty-Sensitive Method for LLM Instruction Tuning", "categories": ["cs.CL"], "comment": null, "summary": "Large language models (LLMs) demonstrate remarkable capabilities but face\nchallenges from hallucinations, which typically arise from insufficient\nknowledge or context. While instructing LLMs to acknowledge knowledge\nlimitations by responding with \"I don't know\" appears promising, we find that\nmodels consistently struggle with admitting knowledge gaps. This challenge may\noriginate from current instruction datasets that emphasise answer generation\nover knowledge boundary awareness. To address this limitation, we introduce\nUncertainty-and-Sensitivity-Aware Tuning (US-Tuning), a novel two-stage\napproach for contextual question answering (QA). The first stage enhances LLMs'\nability to recognise their knowledge boundaries, while the second stage\nreinforces instruction adherence through carefully designed causal prompts. Our\nexperimental results demonstrate that US-Tuning not only significantly reduces\nincorrect answers in contextual QA but also improves models' faithfulness to\ntheir parametric knowledge, mitigating hallucinations in general QA tasks. Our\nfine-tuned Llama2-7B model achieves up to a 34.7% improvement in handling\nout-of-knowledge questions and outperforms GPT-4 by 4.2% in overall\nperformance."}
{"id": "2406.10973", "pdf": "https://arxiv.org/pdf/2406.10973", "abs": "https://arxiv.org/abs/2406.10973", "authors": ["Samar Khanna", "Medhanie Irgau", "David B. Lobell", "Stefano Ermon"], "title": "ExPLoRA: Parameter-Efficient Extended Pre-Training to Adapt Vision Transformers under Domain Shifts", "categories": ["cs.CV", "cs.AI"], "comment": "Published at ICML 2025", "summary": "Parameter-efficient fine-tuning (PEFT) techniques such as low-rank adaptation\n(LoRA) can effectively adapt large pre-trained foundation models to downstream\ntasks using only a small fraction (0.1%-10%) of the original trainable weights.\nAn under-explored question of PEFT is in extending the pre-training phase\nwithout supervised labels; that is, can we adapt a pre-trained foundation model\nto a new domain via efficient self-supervised pre-training on this domain? In\nthis work, we introduce ExPLoRA, a highly effective technique to improve\ntransfer learning of pre-trained vision transformers (ViTs) under domain\nshifts. Initializing a ViT with pre-trained weights on large, natural-image\ndatasets such as from DinoV2 or MAE, ExPLoRA continues the unsupervised\npre-training objective on a new domain, unfreezing 1-2 pre-trained ViT blocks\nand tuning all other layers with LoRA. We then fine-tune the resulting model\nonly with LoRA on this new domain for supervised learning. Our experiments\ndemonstrate state-of-the-art results on satellite imagery, even outperforming\nfully pre-training and fine-tuning ViTs. Using the DinoV2 training objective,\nwe demonstrate up to 8% improvement in linear probing top-1 accuracy on\ndownstream tasks while using <10% of the number of parameters that are used in\nprior fully-tuned state-of-the art approaches. Our ablation studies confirm the\nefficacy of our approach over other baselines such as PEFT. Code is available\non the project website: https://samar-khanna.github.io/ExPLoRA/"}
{"id": "2505.14970", "pdf": "https://arxiv.org/pdf/2505.14970", "abs": "https://arxiv.org/abs/2505.14970", "authors": ["Xiaoyin Chen", "Jiarui Lu", "Minsu Kim", "Dinghuai Zhang", "Jian Tang", "Alexandre PichÃ©", "Nicolas Gontier", "Yoshua Bengio", "Ehsan Kamalloo"], "title": "Self-Evolving Curriculum for LLM Reasoning", "categories": ["cs.AI", "cs.LG"], "comment": null, "summary": "Reinforcement learning (RL) has proven effective for fine-tuning large\nlanguage models (LLMs), significantly enhancing their reasoning abilities in\ndomains such as mathematics and code generation. A crucial factor influencing\nRL fine-tuning success is the training curriculum: the order in which training\nproblems are presented. While random curricula serve as common baselines, they\nremain suboptimal; manually designed curricula often rely heavily on\nheuristics, and online filtering methods can be computationally prohibitive. To\naddress these limitations, we propose Self-Evolving Curriculum (SEC), an\nautomatic curriculum learning method that learns a curriculum policy\nconcurrently with the RL fine-tuning process. Our approach formulates\ncurriculum selection as a non-stationary Multi-Armed Bandit problem, treating\neach problem category (e.g., difficulty level or problem type) as an individual\narm. We leverage the absolute advantage from policy gradient methods as a proxy\nmeasure for immediate learning gain. At each training step, the curriculum\npolicy selects categories to maximize this reward signal and is updated using\nthe TD(0) method. Across three distinct reasoning domains: planning, inductive\nreasoning, and mathematics, our experiments demonstrate that SEC significantly\nimproves models' reasoning capabilities, enabling better generalization to\nharder, out-of-distribution test problems. Additionally, our approach achieves\nbetter skill balance when fine-tuning simultaneously on multiple reasoning\ndomains. These findings highlight SEC as a promising strategy for RL\nfine-tuning of LLMs."}
{"id": "2505.24161", "pdf": "https://arxiv.org/pdf/2505.24161", "abs": "https://arxiv.org/abs/2505.24161", "authors": ["Zijie Xu", "Tong Bu", "Zecheng Hao", "Jianhao Ding", "Zhaofei Yu"], "title": "Proxy Target: Bridging the Gap Between Discrete Spiking Neural Networks and Continuous Control", "categories": ["cs.NE", "cs.LG"], "comment": null, "summary": "Spiking Neural Networks (SNNs) offer low-latency and energy-efficient\ndecision making through neuromorphic hardware, making them compelling for\nReinforcement Learning (RL) in resource-constrained edge devices. Recent\nstudies in this field directly replace Artificial Neural Networks (ANNs) by\nSNNs in existing RL frameworks, overlooking whether the RL algorithm is\nsuitable for SNNs. However, most RL algorithms in continuous control are\ndesigned tailored to ANNs, including the target network soft updates mechanism,\nwhich conflict with the discrete, non-differentiable dynamics of SNN spikes. We\nidentify that this mismatch destabilizes SNN training in continuous control\ntasks. To bridge this gap between discrete SNN and continuous control, we\npropose a novel proxy target framework. The continuous and differentiable\ndynamics of the proxy target enable smooth updates, bypassing the\nincompatibility of SNN spikes, stabilizing the RL algorithms. Since the proxy\nnetwork operates only during training, the SNN retains its energy efficiency\nduring deployment without inference overhead. Extensive experiments on\ncontinuous control benchmarks demonstrate that compared to vanilla SNNs, the\nproxy target framework enables SNNs to achieve up to 32% higher performance\nacross different spiking neurons. Notably, we are the first to surpass ANN\nperformance in continuous control with simple Leaky-Integrate-and-Fire (LIF)\nneurons. This work motivates a new class of SNN-friendly RL algorithms tailored\nto SNN's characteristics, paving the way for neuromorphic agents that combine\nhigh performance with low power consumption."}
{"id": "2406.16469", "pdf": "https://arxiv.org/pdf/2406.16469", "abs": "https://arxiv.org/abs/2406.16469", "authors": ["ChaeHun Park", "Yujin Baek", "Jaeseok Kim", "Yu-Jung Heo", "Du-Seong Chang", "Jaegul Choo"], "title": "Evaluating Visual and Cultural Interpretation: The K-Viscuit Benchmark with Human-VLM Collaboration", "categories": ["cs.CL", "cs.CV"], "comment": "ACL 2025 camera-ready", "summary": "To create culturally inclusive vision-language models (VLMs), developing a\nbenchmark that tests their ability to address culturally relevant questions is\nessential. Existing approaches typically rely on human annotators, making the\nprocess labor-intensive and creating a cognitive burden in generating diverse\nquestions. To address this, we propose a semi-automated framework for\nconstructing cultural VLM benchmarks, specifically targeting multiple-choice\nQA. This framework combines human-VLM collaboration, where VLMs generate\nquestions based on guidelines, a small set of annotated examples, and relevant\nknowledge, followed by a verification process by native speakers. We\ndemonstrate the effectiveness of this framework through the creation of\n\\texttt{K-Viscuit}, a dataset focused on Korean culture. Our experiments on\nthis dataset reveal that open-source models lag behind proprietary ones in\nunderstanding Korean culture, highlighting key areas for improvement. We also\npresent a series of further analyses, including human evaluation, augmenting\nVLMs with external knowledge, and the evaluation beyond multiple-choice QA. Our\ndataset is available at https://huggingface.co/datasets/ddehun/k-viscuit."}
{"id": "2408.05211", "pdf": "https://arxiv.org/pdf/2408.05211", "abs": "https://arxiv.org/abs/2408.05211", "authors": ["Chaoyou Fu", "Haojia Lin", "Zuwei Long", "Yunhang Shen", "Yuhang Dai", "Meng Zhao", "Yi-Fan Zhang", "Shaoqi Dong", "Yangze Li", "Xiong Wang", "Haoyu Cao", "Di Yin", "Long Ma", "Xiawu Zheng", "Rongrong Ji", "Yunsheng Wu", "Ran He", "Caifeng Shan", "Xing Sun"], "title": "VITA: Towards Open-Source Interactive Omni Multimodal LLM", "categories": ["cs.CV", "cs.AI", "cs.CL"], "comment": "Project Page: https://vita-home.github.io", "summary": "The remarkable multimodal capabilities and interactive experience of GPT-4o\nunderscore their necessity in practical applications, yet open-source models\nrarely excel in both areas. In this paper, we introduce VITA, the first-ever\nopen-source Multimodal Large Language Model (MLLM) adept at simultaneous\nprocessing and analysis of Video, Image, Text, and Audio modalities, and\nmeanwhile has an advanced multimodal interactive experience. Starting from\nMixtral 8x7B as a language foundation, we expand its Chinese vocabulary\nfollowed by bilingual instruction tuning. We further endow the language model\nwith visual and audio capabilities through two-stage multi-task learning of\nmultimodal alignment and instruction tuning. VITA demonstrates robust\nfoundational capabilities of multilingual, vision, and audio understanding, as\nevidenced by its strong performance across a range of both unimodal and\nmultimodal benchmarks. Beyond foundational capabilities, we have made\nconsiderable progress in enhancing the natural multimodal human-computer\ninteraction experience. VITA is the first step for the open-source community to\nexplore the seamless integration of multimodal understanding and interaction.\nWhile there is still lots of work to be done on VITA to get close to\nclose-source counterparts, we hope that its role as a pioneer can serve as a\ncornerstone for subsequent research. Project Page: https://vita-home.github.io."}
{"id": "2505.16223", "pdf": "https://arxiv.org/pdf/2505.16223", "abs": "https://arxiv.org/abs/2505.16223", "authors": ["Sangyong Lee", "Subo Hwang", "Dohoon Kim"], "title": "MADCluster: Model-agnostic Anomaly Detection with Self-supervised Clustering Network", "categories": ["cs.AI", "cs.LG"], "comment": "24 pages, 9 figures", "summary": "In this paper, we propose MADCluster, a novel model-agnostic anomaly\ndetection framework utilizing self-supervised clustering. MADCluster is\napplicable to various deep learning architectures and addresses the\n'hypersphere collapse' problem inherent in existing deep learning-based anomaly\ndetection methods. The core idea is to cluster normal pattern data into a\n'single cluster' while simultaneously learning the cluster center and mapping\ndata close to this center. Also, to improve expressiveness and enable effective\nsingle clustering, we propose a new 'One-directed Adaptive loss'. The\noptimization of this loss is mathematically proven. MADCluster consists of\nthree main components: Base Embedder capturing high-dimensional temporal\ndynamics, Cluster Distance Mapping, and Sequence-wise Clustering for continuous\ncenter updates. Its model-agnostic characteristics are achieved by applying\nvarious architectures to the Base Embedder. Experiments on four time series\nbenchmark datasets demonstrate that applying MADCluster improves the overall\nperformance of comparative models. In conclusion, the compatibility of\nMADCluster shows potential for enhancing model performance across various\narchitectures."}
{"id": "2505.24203", "pdf": "https://arxiv.org/pdf/2505.24203", "abs": "https://arxiv.org/abs/2505.24203", "authors": ["Jiarui Lu", "Xiaoyin Chen", "Stephen Zhewen Lu", "AurÃ©lie Lozano", "Vijil Chenthamarakshan", "Payel Das", "Jian Tang"], "title": "Aligning Protein Conformation Ensemble Generation with Physical Feedback", "categories": ["q-bio.BM", "cs.LG"], "comment": "Published as a conference paper at ICML 2025", "summary": "Protein dynamics play a crucial role in protein biological functions and\nproperties, and their traditional study typically relies on time-consuming\nmolecular dynamics (MD) simulations conducted in silico. Recent advances in\ngenerative modeling, particularly denoising diffusion models, have enabled\nefficient accurate protein structure prediction and conformation sampling by\nlearning distributions over crystallographic structures. However, effectively\nintegrating physical supervision into these data-driven approaches remains\nchallenging, as standard energy-based objectives often lead to intractable\noptimization. In this paper, we introduce Energy-based Alignment (EBA), a\nmethod that aligns generative models with feedback from physical models,\nefficiently calibrating them to appropriately balance conformational states\nbased on their energy differences. Experimental results on the MD ensemble\nbenchmark demonstrate that EBA achieves state-of-the-art performance in\ngenerating high-quality protein ensembles. By improving the physical\nplausibility of generated structures, our approach enhances model predictions\nand holds promise for applications in structural biology and drug discovery."}
{"id": "2407.09823", "pdf": "https://arxiv.org/pdf/2407.09823", "abs": "https://arxiv.org/abs/2407.09823", "authors": ["Md. Arid Hasan", "Maram Hasanain", "Fatema Ahmad", "Sahinur Rahman Laskar", "Sunaya Upadhyay", "Vrunda N Sukhadia", "Mucahid Kutlu", "Shammur Absar Chowdhury", "Firoj Alam"], "title": "NativQA: Multilingual Culturally-Aligned Natural Query for LLMs", "categories": ["cs.CL", "cs.AI", "68T50", "F.2.2; I.2.7"], "comment": "LLMs, Native, Multilingual, Language Diversity, Contextual\n  Understanding, Minority Languages, Culturally Informed, Foundation Models,\n  Large Language Models", "summary": "Natural Question Answering (QA) datasets play a crucial role in evaluating\nthe capabilities of large language models (LLMs), ensuring their effectiveness\nin real-world applications. Despite the numerous QA datasets that have been\ndeveloped and some work has been done in parallel, there is a notable lack of a\nframework and large scale region-specific datasets queried by native users in\ntheir own languages. This gap hinders the effective benchmarking and the\ndevelopment of fine-tuned models for regional and cultural specificities. In\nthis study, we propose a scalable, language-independent framework, NativQA, to\nseamlessly construct culturally and regionally aligned QA datasets in native\nlanguages, for LLM evaluation and tuning. We demonstrate the efficacy of the\nproposed framework by designing a multilingual natural QA dataset,\nMultiNativQA, consisting of ~64k manually annotated QA pairs in seven\nlanguages, ranging from high to extremely low resource, based on queries from\nnative speakers from 9 regions covering 18 topics. We benchmark open- and\nclosed-source LLMs with the MultiNativQA dataset. We made the MultiNativQA\ndataset(https://huggingface.co/datasets/QCRI/MultiNativQA), and other\nexperimental scripts(https://gitlab.com/nativqa/multinativqa) publicly\navailable for the community."}
{"id": "2408.09786", "pdf": "https://arxiv.org/pdf/2408.09786", "abs": "https://arxiv.org/abs/2408.09786", "authors": ["Yuxia Geng", "Runkai Zhu", "Jiaoyan Chen", "Jintai Chen", "Xiang Chen", "Zhuo Chen", "Shuofei Qiao", "Yuxiang Wang", "Xiaoliang Xu", "Sheng-Jun Huang"], "title": "Graph-guided Cross-composition Feature Disentanglement for Compositional Zero-shot Learning", "categories": ["cs.CV"], "comment": "Accepted in ACL 2025 findings", "summary": "Disentanglement of visual features of primitives (i.e., attributes and\nobjects) has shown exceptional results in Compositional Zero-shot Learning\n(CZSL). However, due to the feature divergence of an attribute (resp. object)\nwhen combined with different objects (resp. attributes), it is challenging to\nlearn disentangled primitive features that are general across different\ncompositions. To this end, we propose the solution of cross-composition feature\ndisentanglement, which takes multiple primitive-sharing compositions as inputs\nand constrains the disentangled primitive features to be general across these\ncompositions. More specifically, we leverage a compositional graph to define\nthe overall primitive-sharing relationships between compositions, and build a\ntask-specific architecture upon the recently successful large pre-trained\nvision-language model (VLM) CLIP, with dual cross-composition disentangling\nadapters (called L-Adapter and V-Adapter) inserted into CLIP's frozen text and\nimage encoders, respectively. Evaluation on three popular CZSL benchmarks shows\nthat our proposed solution significantly improves the performance of CZSL, and\nits components have been verified by solid ablation studies. Our code and data\nare available at:https://github.com/zhurunkai/DCDA."}
{"id": "2505.18134", "pdf": "https://arxiv.org/pdf/2505.18134", "abs": "https://arxiv.org/abs/2505.18134", "authors": ["Alex L. Zhang", "Thomas L. Griffiths", "Karthik R. Narasimhan", "Ofir Press"], "title": "VideoGameBench: Can Vision-Language Models complete popular video games?", "categories": ["cs.AI", "cs.CL", "cs.CV"], "comment": "9 pages, 33 pages including supplementary", "summary": "Vision-language models (VLMs) have achieved strong results on coding and math\nbenchmarks that are challenging for humans, yet their ability to perform tasks\nthat come naturally to humans--such as perception, spatial navigation, and\nmemory management--remains understudied. Real video games are crafted to be\nintuitive for humans to learn and master by leveraging innate inductive biases,\nmaking them an ideal testbed for evaluating such capabilities in VLMs. To this\nend, we introduce VideoGameBench, a benchmark consisting of 10 popular video\ngames from the 1990s that VLMs directly interact with in real-time.\nVideoGameBench challenges models to complete entire games with access to only\nraw visual inputs and a high-level description of objectives and controls, a\nsignificant departure from existing setups that rely on game-specific\nscaffolding and auxiliary information. We keep three of the games secret to\nencourage solutions that generalize to unseen environments. Our experiments\nshow that frontier vision-language models struggle to progress beyond the\nbeginning of each game. We find inference latency to be a major limitation of\nfrontier models in the real-time setting; therefore, we introduce\nVideoGameBench Lite, a setting where the game pauses while waiting for the LM's\nnext action. The best performing model, Gemini 2.5 Pro, completes only 0.48% of\nVideoGameBench and 1.6% of VideoGameBench Lite. We hope that the formalization\nof the human skills mentioned above into this benchmark motivates progress in\nthese research directions."}
{"id": "2505.24281", "pdf": "https://arxiv.org/pdf/2505.24281", "abs": "https://arxiv.org/abs/2505.24281", "authors": ["Yang Sui", "Qi Xu", "Yang Bai", "Annie Qu"], "title": "Multi-task Learning for Heterogeneous Data via Integrating Shared and Task-Specific Encodings", "categories": ["stat.ML", "cs.LG", "stat.ME"], "comment": null, "summary": "Multi-task learning (MTL) has become an essential machine learning tool for\naddressing multiple learning tasks simultaneously and has been effectively\napplied across fields such as healthcare, marketing, and biomedical research.\nHowever, to enable efficient information sharing across tasks, it is crucial to\nleverage both shared and heterogeneous information. Despite extensive research\non MTL, various forms of heterogeneity, including distribution and posterior\nheterogeneity, present significant challenges. Existing methods often fail to\naddress these forms of heterogeneity within a unified framework. In this paper,\nwe propose a dual-encoder framework to construct a heterogeneous latent factor\nspace for each task, incorporating a task-shared encoder to capture common\ninformation across tasks and a task-specific encoder to preserve unique task\ncharacteristics. Additionally, we explore the intrinsic similarity structure of\nthe coefficients corresponding to learned latent factors, allowing for adaptive\nintegration across tasks to manage posterior heterogeneity. We introduce a\nunified algorithm that alternately learns the task-specific and task-shared\nencoders and coefficients. In theory, we investigate the excess risk bound for\nthe proposed MTL method using local Rademacher complexity and apply it to a new\nbut related task. Through simulation studies, we demonstrate that the proposed\nmethod outperforms existing data integration methods across various settings.\nFurthermore, the proposed method achieves superior predictive performance for\ntime to tumor doubling across five distinct cancer types in PDX data."}
{"id": "2407.14878", "pdf": "https://arxiv.org/pdf/2407.14878", "abs": "https://arxiv.org/abs/2407.14878", "authors": ["Yongxin Huang", "Kexin Wang", "Goran GlavaÅ¡", "Iryna Gurevych"], "title": "Modular Sentence Encoders: Separating Language Specialization from Cross-Lingual Alignment", "categories": ["cs.CL"], "comment": "Accepted for ACL 2025 main conference", "summary": "Multilingual sentence encoders (MSEs) are commonly obtained by training\nmultilingual language models to map sentences from different languages into a\nshared semantic space. As such, they are subject to curse of multilinguality, a\nloss of monolingual representational accuracy due to parameter sharing. Another\nlimitation of MSEs is the trade-off between different task performance:\ncross-lingual alignment training distorts the optimal monolingual structure of\nsemantic spaces of individual languages, harming the utility of sentence\nembeddings in monolingual tasks; cross-lingual tasks, such as cross-lingual\nsemantic similarity and zero-shot transfer for sentence classification, may\nalso require conflicting cross-lingual alignment strategies. In this work, we\naddress both issues by means of modular training of sentence encoders. We first\ntrain language-specific monolingual modules to mitigate negative interference\nbetween languages (i.e., the curse). We then align all non-English sentence\nembeddings to the English by training cross-lingual alignment adapters,\npreventing interference with monolingual specialization from the first step. We\ntrain the cross-lingual adapters with two different types of data to resolve\nthe conflicting requirements of different cross-lingual tasks. Monolingual and\ncross-lingual results on semantic text similarity and relatedness, bitext\nmining and sentence classification show that our modular solution achieves\nbetter and more balanced performance across all the tasks compared to\nfull-parameter training of monolithic multilingual sentence encoders,\nespecially benefiting low-resource languages."}
{"id": "2408.13877", "pdf": "https://arxiv.org/pdf/2408.13877", "abs": "https://arxiv.org/abs/2408.13877", "authors": ["Xiaoyu Guo", "Pengzhi Zhong", "Hao Zhang", "Defeng Huang", "Huikai Shao", "Qijun Zhao", "Shuiwang Li"], "title": "Camouflaged Object Tracking: A Benchmark", "categories": ["cs.CV"], "comment": null, "summary": "Visual tracking has seen remarkable advancements, largely driven by the\navailability of large-scale training datasets that have enabled the development\nof highly accurate and robust algorithms. While significant progress has been\nmade in tracking general objects, research on more challenging scenarios, such\nas tracking camouflaged objects, remains limited. Camouflaged objects, which\nblend seamlessly with their surroundings or other objects, present unique\nchallenges for detection and tracking in complex environments. This challenge\nis particularly critical in applications such as military, security,\nagriculture, and marine monitoring, where precise tracking of camouflaged\nobjects is essential. To address this gap, we introduce the Camouflaged Object\nTracking Dataset (COTD), a specialized benchmark designed specifically for\nevaluating camouflaged object tracking methods. The COTD dataset comprises 200\nsequences and approximately 80,000 frames, each annotated with detailed\nbounding boxes. Our evaluation of 20 existing tracking algorithms reveals\nsignificant deficiencies in their performance with camouflaged objects. To\naddress these issues, we propose a novel tracking framework, HiPTrack-MLS,\nwhich demonstrates promising results in improving tracking performance for\ncamouflaged objects. COTD and code are avialable at\nhttps://github.com/openat25/HIPTrack-MLS."}
{"id": "2505.19550", "pdf": "https://arxiv.org/pdf/2505.19550", "abs": "https://arxiv.org/abs/2505.19550", "authors": ["Georgios Mappouras"], "title": "Turing Test 2.0: The General Intelligence Threshold", "categories": ["cs.AI"], "comment": null, "summary": "With the rise of artificial intelligence (A.I.) and large language models\nlike Chat-GPT, a new race for achieving artificial general intelligence (A.G.I)\nhas started. While many speculate how and when A.I. will achieve A.G.I., there\nis no clear agreement on how A.G.I. can be detected in A.I. models, even when\npopular tools like the Turing test (and its modern variations) are used to\nmeasure their intelligence. In this work, we discuss why traditional methods\nlike the Turing test do not suffice for measuring or detecting A.G.I. and\nprovide a new, practical method that can be used to decide if a system\n(computer or any other) has reached or surpassed A.G.I. To achieve this, we\nmake two new contributions. First, we present a clear definition for general\nintelligence (G.I.) and set a G.I. threshold (G.I.T.) that can be used to\ndistinguish between systems that achieve A.G.I. and systems that do not.\nSecond, we present a new framework on how to construct tests that can detect if\na system has achieved G.I. in a simple, comprehensive, and clear-cut fail/pass\nway. We call this novel framework the Turing test 2.0. We then demonstrate\nreal-life examples of applying tests that follow our Turing test 2.0 framework\non modern A.I. models."}
{"id": "2505.24296", "pdf": "https://arxiv.org/pdf/2505.24296", "abs": "https://arxiv.org/abs/2505.24296", "authors": ["Quinn Lanners", "Cynthia Rudin", "Alexander Volfovsky", "Harsh Parikh"], "title": "Data Fusion for Partial Identification of Causal Effects", "categories": ["stat.ME", "cs.LG", "econ.EM"], "comment": null, "summary": "Data fusion techniques integrate information from heterogeneous data sources\nto improve learning, generalization, and decision making across data sciences.\nIn causal inference, these methods leverage rich observational data to improve\ncausal effect estimation, while maintaining the trustworthiness of randomized\ncontrolled trials. Existing approaches often relax the strong no unobserved\nconfounding assumption by instead assuming exchangeability of counterfactual\noutcomes across data sources. However, when both assumptions simultaneously\nfail - a common scenario in practice - current methods cannot identify or\nestimate causal effects. We address this limitation by proposing a novel\npartial identification framework that enables researchers to answer key\nquestions such as: Is the causal effect positive or negative? and How severe\nmust assumption violations be to overturn this conclusion? Our approach\nintroduces interpretable sensitivity parameters that quantify assumption\nviolations and derives corresponding causal effect bounds. We develop doubly\nrobust estimators for these bounds and operationalize breakdown frontier\nanalysis to understand how causal conclusions change as assumption violations\nincrease. We apply our framework to the Project STAR study, which investigates\nthe effect of classroom size on students' third-grade standardized test\nperformance. Our analysis reveals that the Project STAR results are robust to\nsimultaneous violations of key assumptions, both on average and across various\nsubgroups of interest. This strengthens confidence in the study's conclusions\ndespite potential unmeasured biases in the data."}
{"id": "2408.08144", "pdf": "https://arxiv.org/pdf/2408.08144", "abs": "https://arxiv.org/abs/2408.08144", "authors": ["Yan Li", "So-Eon Kim", "Seong-Bae Park", "Soyeon Caren Han"], "title": "MIDAS: Multi-level Intent, Domain, And Slot Knowledge Distillation for Multi-turn NLU", "categories": ["cs.CL"], "comment": "Accepted by NAACL 2025", "summary": "Although Large Language Models (LLMs) can generate coherent text, they often\nstruggle to recognise user intent behind queries. In contrast, Natural Language\nUnderstanding (NLU) models interpret the purpose and key information of user\ninput for responsive interactions. Existing NLU models typically map utterances\nto a dual-level semantic frame, involving sentence-level intent (SI) and\nword-level slot (WS) labels. However, real-life conversations primarily consist\nof multi-turn dialogues, requiring the interpretation of complex and extended\nexchanges. Researchers encounter challenges in addressing all facets of\nmulti-turn dialogue using a unified NLU model. This paper introduces MIDAS, a\nnovel approach leveraging multi-level intent, domain, and slot knowledge\ndistillation for multi-turn NLU. We construct distinct teachers for SI\ndetection, WS filling, and conversation-level domain (CD) classification, each\nfine-tuned for specific knowledge. A multi-teacher loss is proposed to\nfacilitate the integration of these teachers, guiding a student model in\nmulti-turn dialogue tasks. Results demonstrate the efficacy of our model in\nimproving multi-turn conversation understanding, showcasing the potential for\nadvancements in NLU through multi-level dialogue knowledge distillation. Our\nimplementation is open-sourced on https://github.com/adlnlp/Midas."}
{"id": "2409.07541", "pdf": "https://arxiv.org/pdf/2409.07541", "abs": "https://arxiv.org/abs/2409.07541", "authors": ["Giorgos Savathrakis", "Antonis Argyros"], "title": "ENACT: Entropy-based Clustering of Attention Input for Reducing the Computational Needs of Object Detection Transformers", "categories": ["cs.CV"], "comment": null, "summary": "Transformers demonstrate competitive performance in terms of precision on the\nproblem of vision-based object detection. However, they require considerable\ncomputational resources due to the quadratic size of the attention weights. In\nthis work, we propose to cluster the transformer input on the basis of its\nentropy, due to its similarity between same object pixels. This is expected to\nreduce GPU usage during training, while maintaining reasonable accuracy. This\nidea is realized with an implemented module that is called ENtropy-based\nAttention Clustering for detection Transformers (ENACT), which serves as a\nplug-in to any multi-head self-attention based transformer network. Experiments\non the COCO object detection dataset and three detection transformers\ndemonstrate that the requirements on memory are reduced, while the detection\naccuracy is degraded only slightly. The code of the ENACT module is available\nat https://github.com/GSavathrakis/ENACT."}
{"id": "2505.19662", "pdf": "https://arxiv.org/pdf/2505.19662", "abs": "https://arxiv.org/abs/2505.19662", "authors": ["Atsunori Moteki", "Shoichi Masui", "Fan Yang", "Yueqi Song", "Yonatan Bisk", "Graham Neubig", "Ikuo Kusajima", "Yasuto Watanabe", "Hiroyuki Ishida", "Jun Takahashi", "Shan Jiang"], "title": "FieldWorkArena: Agentic AI Benchmark for Real Field Work Tasks", "categories": ["cs.AI", "cs.CV"], "comment": "6 pages, 2 figures, 4 tables", "summary": "This paper proposes FieldWorkArena, a benchmark for agentic AI targeting\nreal-world field work. With the recent increase in demand for agentic AI, they\nare required to monitor and report safety and health incidents, as well as\nmanufacturing-related incidents, that may occur in real-world work\nenvironments. Existing agentic AI benchmarks have been limited to evaluating\nweb tasks and are insufficient for evaluating agents in real-world work\nenvironments, where complexity increases significantly. In this paper, we\ndefine a new action space that agentic AI should possess for real world work\nenvironment benchmarks and improve the evaluation function from previous\nmethods to assess the performance of agentic AI in diverse real-world tasks.\nThe dataset consists of videos captured on-site and documents actually used in\nfactories and warehouses, and tasks were created based on interviews with\non-site workers and managers. Evaluation results confirmed that performance\nevaluation considering the characteristics of Multimodal LLM (MLLM) such as\nGPT-4o is feasible. Additionally, the effectiveness and limitations of the\nproposed new evaluation method were identified. The complete dataset\n(HuggingFace) and evaluation program (GitHub) can be downloaded from the\nfollowing website:\nhttps://en-documents.research.global.fujitsu.com/fieldworkarena/."}
{"id": "2505.24311", "pdf": "https://arxiv.org/pdf/2505.24311", "abs": "https://arxiv.org/abs/2505.24311", "authors": ["Yi Gu"], "title": "Equilibrium Distribution for t-Distributed Stochastic Neighbor Embedding with Generalized Kernels", "categories": ["stat.ML", "cs.LG", "math.PR", "math.ST", "stat.TH", "60"], "comment": null, "summary": "T-distributed stochastic neighbor embedding (t-SNE) is a well-known algorithm\nfor visualizing high-dimensional data by finding low-dimensional\nrepresentations. In this paper, we study the convergence of t-SNE with\ngeneralized kernels and extend the results of Auffinger and Fletcher in 2023.\nOur work starts by giving a concrete formulation of generalized input and\noutput kernels. Then we prove that under certain conditions, the t-SNE\nalgorithm converges to an equilibrium distribution for a wide range of input\nand output kernels as the number of data points diverges."}
{"id": "2408.12226", "pdf": "https://arxiv.org/pdf/2408.12226", "abs": "https://arxiv.org/abs/2408.12226", "authors": ["Nicy Scaria", "Silvester John Joseph Kennedy", "Thomas Latinovich", "Deepak Subramani"], "title": "EvalYaks: Instruction Tuning Datasets and LoRA Fine-tuned Models for Automated Scoring of CEFR B2 Speaking Assessment Transcripts", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Relying on human experts to evaluate CEFR speaking assessments in an\ne-learning environment creates scalability challenges, as it limits how quickly\nand widely assessments can be conducted. We aim to automate the evaluation of\nCEFR B2 English speaking assessments in e-learning environments from\nconversation transcripts. First, we evaluate the capability of leading open\nsource and commercial Large Language Models (LLMs) to score a candidate's\nperformance across various criteria in the CEFR B2 speaking exam in both global\nand India-specific contexts. Next, we create a new expert-validated,\nCEFR-aligned synthetic conversational dataset with transcripts that are rated\nat different assessment scores. In addition, new instruction-tuned datasets are\ndeveloped from the English Vocabulary Profile (up to CEFR B2 level) and the\nCEFR-SP WikiAuto datasets. Finally, using these new datasets, we perform\nparameter efficient instruction tuning of Mistral Instruct 7B v0.2 to develop a\nfamily of models called EvalYaks. Four models in this family are for assessing\nthe four sections of the CEFR B2 speaking exam, one for identifying the CEFR\nlevel of vocabulary and generating level-specific vocabulary, and another for\ndetecting the CEFR level of text and generating level-specific text. EvalYaks\nachieved an average acceptable accuracy of 96%, a degree of variation of 0.35\nlevels, and performed 3 times better than the next best model. This\ndemonstrates that a 7B parameter LLM instruction tuned with high-quality\nCEFR-aligned assessment data can effectively evaluate and score CEFR B2 English\nspeaking assessments, offering a promising solution for scalable, automated\nlanguage proficiency evaluation."}
{"id": "2410.02671", "pdf": "https://arxiv.org/pdf/2410.02671", "abs": "https://arxiv.org/abs/2410.02671", "authors": ["Taekyung Lee", "Jaemoo Choi", "Jaewoong Choi", "Myungjoo Kang"], "title": "Unsupervised Point Cloud Completion through Unbalanced Optimal Transport", "categories": ["cs.CV", "cs.AI"], "comment": "22 pages, 12 figures", "summary": "Unpaired point cloud completion is crucial for real-world applications, where\nground-truth data for complete point clouds are often unavailable. By learning\na completion map from unpaired incomplete and complete point cloud data, this\ntask avoids the reliance on paired datasets. In this paper, we propose the\n\\textit{Unbalanced Optimal Transport Map for Unpaired Point Cloud Completion\n(\\textbf{UOT-UPC})} model, which formulates the unpaired completion task as the\n(Unbalanced) Optimal Transport (OT) problem. Our method employs a Neural OT\nmodel learning the UOT map using neural networks. Our model is the first\nattempt to leverage UOT for unpaired point cloud completion, achieving\ncompetitive or superior performance on both single-category and multi-category\nbenchmarks. In particular, our approach is especially robust under the class\nimbalance problem, which is frequently encountered in real-world unpaired point\ncloud completion scenarios. The code is available at\nhttps://github.com/LEETK99/UOT-UPC."}
{"id": "2505.20662", "pdf": "https://arxiv.org/pdf/2505.20662", "abs": "https://arxiv.org/abs/2505.20662", "authors": ["Xuanle Zhao", "Zilin Sang", "Yuxuan Li", "Qi Shi", "Weilun Zhao", "Shuo Wang", "Duzhen Zhang", "Xu Han", "Zhiyuan Liu", "Maosong Sun"], "title": "AutoReproduce: Automatic AI Experiment Reproduction with Paper Lineage", "categories": ["cs.AI"], "comment": "20 pages, preprint version", "summary": "Efficient experiment reproduction is critical to accelerating progress in\nartificial intelligence. However, the inherent complexity of method design and\ntraining procedures presents substantial challenges for automation. Notably,\nreproducing experiments often requires implicit domain-specific knowledge not\nexplicitly documented in the original papers. To address this, we introduce the\npaper lineage algorithm, which identifies and extracts implicit knowledge from\nthe relevant references cited by the target paper. Building on this idea, we\npropose AutoReproduce, a multi-agent framework capable of automatically\nreproducing experiments described in research papers in an end-to-end manner.\nAutoReproduce enhances code executability by generating unit tests alongside\nthe reproduction process. To evaluate the reproduction capability, we construct\nReproduceBench, a benchmark annotated with verified implementations, and\nintroduce novel evaluation metrics to assess both the reproduction and\nexecution fidelity. Experimental results demonstrate that AutoReproduce\noutperforms the existing strong agent baselines on all five evaluation metrics\nby a peak margin of over $70\\%$. In particular, compared to the official\nimplementations, AutoReproduce achieves an average performance gap of $22.1\\%$\non $89.74\\%$ of the executable experiment runs. The code will be available at\nhttps://github.com/AI9Stars/AutoReproduce."}
{"id": "2505.24333", "pdf": "https://arxiv.org/pdf/2505.24333", "abs": "https://arxiv.org/abs/2505.24333", "authors": ["Alessio Giorlandino", "Sebastian Goldt"], "title": "Two failure modes of deep transformers and how to avoid them: a unified theory of signal propagation at initialisation", "categories": ["stat.ML", "cond-mat.dis-nn", "cond-mat.stat-mech", "cs.LG"], "comment": null, "summary": "Finding the right initialisation for neural networks is crucial to ensure\nsmooth training and good performance. In transformers, the wrong initialisation\ncan lead to one of two failure modes of self-attention layers: rank collapse,\nwhere all tokens collapse into similar representations, and entropy collapse,\nwhere highly concentrated attention scores lead to training instability. While\nthe right initialisation has been extensively studied in feed-forward networks,\nan exact description of signal propagation through a full transformer block has\nso far been lacking. Here, we provide an analytical theory of signal\npropagation through vanilla transformer blocks with self-attention layers,\nlayer normalisation, skip connections and ReLU MLP. To treat the self-attention\nlayer, we draw on a formal parallel with the Random Energy Model from\nstatistical physics. We identify and characterise two regimes governed by the\nvariance of the query and key initialisations: a low-variance regime, where we\nrecover the known rank collapse behaviour; and a previously unexplored\nhigh-variance regime, where signal is preserved but \\textit{entropy collapse}\noccurs. In the low-variance regime, we calculate the critical strength for the\nresidual connection to ensure signal propagation. Our theory yields\ntrainability diagrams that identify the correct choice of initialisation\nhyper-parameters for a given architecture. Experiments with BERT-style models\ntrained on TinyStories validate our predictions. Our theoretical framework\ngives a unified perspective on the two failure modes of self-attention and\ngives quantitative predictions on the scale of both weights and residual\nconnections that guarantees smooth training."}
{"id": "2409.11638", "pdf": "https://arxiv.org/pdf/2409.11638", "abs": "https://arxiv.org/abs/2409.11638", "authors": ["Mahammed Kamruzzaman", "Abdullah Al Monsur", "Shrabon Das", "Enamul Hassan", "Gene Louis Kim"], "title": "BanStereoSet: A Dataset to Measure Stereotypical Social Biases in LLMs for Bangla", "categories": ["cs.CL"], "comment": "Accepted at ACL-2025", "summary": "This study presents BanStereoSet, a dataset designed to evaluate\nstereotypical social biases in multilingual LLMs for the Bangla language. In an\neffort to extend the focus of bias research beyond English-centric datasets, we\nhave localized the content from the StereoSet, IndiBias, and Kamruzzaman et.\nal.'s datasets, producing a resource tailored to capture biases prevalent\nwithin the Bangla-speaking community. Our BanStereoSet dataset consists of\n1,194 sentences spanning 9 categories of bias: race, profession, gender,\nageism, beauty, beauty in profession, region, caste, and religion. This dataset\nnot only serves as a crucial tool for measuring bias in multilingual LLMs but\nalso facilitates the exploration of stereotypical bias across different social\ncategories, potentially guiding the development of more equitable language\ntechnologies in Bangladeshi contexts. Our analysis of several language models\nusing this dataset indicates significant biases, reinforcing the necessity for\nculturally and linguistically adapted datasets to develop more equitable\nlanguage technologies."}
{"id": "2410.03311", "pdf": "https://arxiv.org/pdf/2410.03311", "abs": "https://arxiv.org/abs/2410.03311", "authors": ["Ye Wang", "Sipeng Zheng", "Bin Cao", "Qianshan Wei", "Weishuai Zeng", "Qin Jin", "Zongqing Lu"], "title": "Scaling Large Motion Models with Million-Level Human Motions", "categories": ["cs.CV", "cs.LG"], "comment": "ICML 2025", "summary": "Inspired by the recent success of LLMs, the field of human motion\nunderstanding has increasingly shifted toward developing large motion models.\nDespite some progress, current efforts remain far from achieving truly\ngeneralist models, primarily due to the lack of massive high-quality data. To\naddress this gap, we present MotionLib, the first million-level dataset for\nmotion generation, which is at least 15$\\times$ larger than existing\ncounterparts and enriched with hierarchical text descriptions. Using MotionLib,\nwe train a large motion model named \\projname, demonstrating robust performance\nacross a wide range of human activities, including unseen ones. Through\nsystematic investigation, for the first time, we highlight the importance of\nscaling both data and model size for advancing motion generation, along with\nkey insights to achieve this goal. To better integrate the motion modality, we\npropose Motionbook, an innovative motion encoding approach including (1) a\ncompact yet lossless feature to represent motions; (2) a novel 2D lookup-free\nmotion tokenizer that preserves fine-grained motion details while expanding\ncodebook capacity, significantly enhancing the representational power of motion\ntokens. We believe this work lays the groundwork for developing more versatile\nand powerful motion generation models in the future. For further details, visit\nhttps://beingbeyond.github.io/Being-M0/."}
{"id": "2505.22092", "pdf": "https://arxiv.org/pdf/2505.22092", "abs": "https://arxiv.org/abs/2505.22092", "authors": ["Valentin Cuzin-Rambaud", "Emilien Komlenovic", "Alexandre Faure", "Bruno Yun"], "title": "VIRAL: Vision-grounded Integration for Reward design And Learning", "categories": ["cs.AI"], "comment": null, "summary": "The alignment between humans and machines is a critical challenge in\nartificial intelligence today. Reinforcement learning, which aims to maximize a\nreward function, is particularly vulnerable to the risks associated with poorly\ndesigned reward functions. Recent advancements has shown that Large Language\nModels (LLMs) for reward generation can outperform human performance in this\ncontext. We introduce VIRAL, a pipeline for generating and refining reward\nfunctions through the use of multi-modal LLMs. VIRAL autonomously creates and\ninteractively improves reward functions based on a given environment and a goal\nprompt or annotated image. The refinement process can incorporate human\nfeedback or be guided by a description generated by a video LLM, which explains\nthe agent's policy in video form. We evaluated VIRAL in five Gymnasium\nenvironments, demonstrating that it accelerates the learning of new behaviors\nwhile ensuring improved alignment with user intent. The source-code and demo\nvideo are available at: https://github.com/VIRAL-UCBL1/VIRAL and\nhttps://youtu.be/Hqo82CxVT38."}
{"id": "2505.24464", "pdf": "https://arxiv.org/pdf/2505.24464", "abs": "https://arxiv.org/abs/2505.24464", "authors": ["Manojlo Vukovic", "Dusan Jakovetic", "Dragana Bajovic", "Soummya Kar"], "title": "Distributed gradient methods under heavy-tailed communication noise", "categories": ["math.OC", "cs.LG", "90C25, 65K05"], "comment": "This work has been submitted to the IEEE for possible publication", "summary": "We consider a standard distributed optimization problem in which networked\nnodes collaboratively minimize the sum of their locally known convex costs. For\nthis setting, we address for the first time the fundamental problem of design\nand analysis of distributed methods to solve the above problem when inter-node\ncommunication is subject to \\emph{heavy-tailed} noise. Heavy-tailed noise is\nhighly relevant and frequently arises in densely deployed wireless sensor and\nInternet of Things (IoT) networks. Specifically, we design a distributed\ngradient-type method that features a carefully balanced mixed time-scale\ntime-varying consensus and gradient contribution step sizes and a bounded\nnonlinear operator on the consensus update to limit the effect of heavy-tailed\nnoise. Assuming heterogeneous strongly convex local costs with mutually\ndifferent minimizers that are arbitrarily far apart, we show that the proposed\nmethod converges to a neighborhood of the network-wide problem solution in the\nmean squared error (MSE) sense, and we also characterize the corresponding\nconvergence rate. We further show that the asymptotic MSE can be made\narbitrarily small through consensus step-size tuning, possibly at the cost of\nslowing down the transient error decay. Numerical experiments corroborate our\nfindings and demonstrate the resilience of the proposed method to heavy-tailed\n(and infinite variance) communication noise. They also show that existing\ndistributed methods, designed for finite-communication-noise-variance settings,\nfail in the presence of infinite variance noise."}
{"id": "2410.07523", "pdf": "https://arxiv.org/pdf/2410.07523", "abs": "https://arxiv.org/abs/2410.07523", "authors": ["Shan Xie", "Man Luo", "Chadly Daniel Stern", "Mengnan Du", "Lu Cheng"], "title": "DemoShapley: Valuation of Demonstrations for In-Context Learning", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": null, "summary": "Large language models (LLMs) using in-context learning (ICL) excel in many\ntasks without task-specific fine-tuning. However, demonstration selection and\nordering greatly impact ICL effectiveness. To address this, we propose\nDemoShapley and Beta-DemoShapley, inspired by Data Shapley and Beta Shapley, to\nassess the influence of individual demonstrations. DemoShapley captures how\neach example influences performance in different contexts, unlike other\ninfluence-based methods that rely on a fixed number of demonstrations.\nBeta-DemoShapley further enhances this framework by incorporating the Beta\ndistribution, allowing users to assign higher weights to smaller cardinalities,\nwhich aligns with ICL's prompt length and computational constraints. Our\nfindings show that the proposed algorithms improve model performance by\nselecting quality demonstrations, and enhancing generalization to\nout-of-distribution tasks. It also identifies noise-compromised data and\npromotes fairness in LLMs, protecting model performance and ensuring robustness\nacross various scenarios."}
{"id": "2410.07599", "pdf": "https://arxiv.org/pdf/2410.07599", "abs": "https://arxiv.org/abs/2410.07599", "authors": ["Feng Wang", "Timing Yang", "Yaodong Yu", "Sucheng Ren", "Guoyizhe Wei", "Angtian Wang", "Wei Shao", "Yuyin Zhou", "Alan Yuille", "Cihang Xie"], "title": "Adventurer: Optimizing Vision Mamba Architecture Designs for Efficiency", "categories": ["cs.CV"], "comment": "Published in CVPR 2025", "summary": "In this work, we introduce the Adventurer series models where we treat images\nas sequences of patch tokens and employ uni-directional language models to\nlearn visual representations. This modeling paradigm allows us to process\nimages in a recurrent formulation with linear complexity relative to the\nsequence length, which can effectively address the memory and computation\nexplosion issues posed by high-resolution and fine-grained images. In detail,\nwe introduce two simple designs that seamlessly integrate image inputs into the\ncausal inference framework: a global pooling token placed at the beginning of\nthe sequence and a flipping operation between every two layers. Extensive\nempirical studies highlight that compared with the existing plain architectures\nsuch as DeiT and Vim, Adventurer offers an optimal efficiency-accuracy\ntrade-off. For example, our Adventurer-Base attains a competitive test accuracy\nof 84.3% on the standard ImageNet-1k benchmark with 216 images/s training\nthroughput, which is 3.8 and 6.2 times faster than Vim and DeiT to achieve the\nsame result. As Adventurer offers great computation and memory efficiency and\nallows scaling with linear complexity, we hope this architecture can benefit\nfuture explorations in modeling long sequences for high-resolution or\nfine-grained images. Code is available at\nhttps://github.com/wangf3014/Adventurer."}
{"id": "2505.23432", "pdf": "https://arxiv.org/pdf/2505.23432", "abs": "https://arxiv.org/abs/2505.23432", "authors": ["L. Elisa Celis", "Lingxiao Huang", "Nisheeth K. Vishnoi"], "title": "A Mathematical Framework for AI-Human Integration in Work", "categories": ["cs.AI", "cs.CY", "econ.GN", "q-fin.EC"], "comment": "This paper will appear in ICML 2025", "summary": "The rapid rise of Generative AI (GenAI) tools has sparked debate over their\nrole in complementing or replacing human workers across job contexts. We\npresent a mathematical framework that models jobs, workers, and worker-job fit,\nintroducing a novel decomposition of skills into decision-level and\naction-level subskills to reflect the complementary strengths of humans and\nGenAI. We analyze how changes in subskill abilities affect job success,\nidentifying conditions for sharp transitions in success probability. We also\nestablish sufficient conditions under which combining workers with\ncomplementary subskills significantly outperforms relying on a single worker.\nThis explains phenomena such as productivity compression, where GenAI\nassistance yields larger gains for lower-skilled workers. We demonstrate the\nframework' s practicality using data from O*NET and Big-Bench Lite, aligning\nreal-world data with our model via subskill-division methods. Our results\nhighlight when and how GenAI complements human skills, rather than replacing\nthem."}
{"id": "2505.24487", "pdf": "https://arxiv.org/pdf/2505.24487", "abs": "https://arxiv.org/abs/2505.24487", "authors": ["Nicholas Cartocci", "Antonios E. Gkikakis", "Darwin G. Caldwell", "JesÃºs Ortiz"], "title": "Real-time Fall Prevention system for the Next-generation of Workers", "categories": ["eess.SP", "cs.LG"], "comment": null, "summary": "Developing a general-purpose wearable real-time fall-detection system is\nstill a challenging task, especially for healthy and strong subjects, such as\nindustrial workers that work in harsh environments. In this work, we present a\nhybrid approach for fall detection and prevention, which uses the dynamic model\nof an inverted pendulum to generate simulations of falling that are then fed to\na deep learning framework. The output is a signal to activate a fall mitigation\nmechanism when the subject is at risk of harm. The advantage of this approach\nis that abstracted models can be used to efficiently generate training data for\nthousands of different subjects with different falling initial conditions,\nsomething that is practically impossible with real experiments. This approach\nis suitable for a specific type of fall, where the subjects fall without\nchanging their initial configuration significantly, and it is the first step\ntoward a general-purpose wearable device, with the aim of reducing\nfall-associated injuries in industrial environments, which can improve the\nsafety of workers."}
{"id": "2410.11119", "pdf": "https://arxiv.org/pdf/2410.11119", "abs": "https://arxiv.org/abs/2410.11119", "authors": ["Yan Li", "Soyeon Caren Han", "Yue Dai", "Feiqi Cao"], "title": "ChuLo: Chunk-Level Key Information Representation for Long Document Processing", "categories": ["cs.CL"], "comment": "The paper has been accepted to ACL 2025", "summary": "Transformer-based models have achieved remarkable success in various Natural\nLanguage Processing (NLP) tasks, yet their ability to handle long documents is\nconstrained by computational limitations. Traditional approaches, such as\ntruncating inputs, sparse self-attention, and chunking, attempt to mitigate\nthese issues, but they often lead to information loss and hinder the model's\nability to capture long-range dependencies. In this paper, we introduce ChuLo,\na novel chunk representation method for long document understanding that\naddresses these limitations. Our ChuLo groups input tokens using unsupervised\nkeyphrase extraction, emphasizing semantically important keyphrase based chunks\nto retain core document content while reducing input length. This approach\nminimizes information loss and improves the efficiency of Transformer-based\nmodels. Preserving all tokens in long document understanding, especially token\nclassification tasks, is important to ensure that fine-grained annotations,\nwhich depend on the entire sequence context, are not lost. We evaluate our\nmethod on multiple long document classification tasks and long document token\nclassification tasks, demonstrating its effectiveness through comprehensive\nqualitative and quantitative analysis. Our implementation is open-sourced on\nhttps://github.com/adlnlp/Chulo."}
{"id": "2410.14138", "pdf": "https://arxiv.org/pdf/2410.14138", "abs": "https://arxiv.org/abs/2410.14138", "authors": ["Jingqi Zhou", "Sheng Wang", "Jingwei Dong", "Kai Liu", "Lei Li", "Jiahui Gao", "Jiyue Jiang", "Lingpeng Kong", "Chuan Wu"], "title": "ProReason: Multi-Modal Proactive Reasoning with Decoupled Eyesight and Wisdom", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Large vision-language models (LVLMs) have witnessed significant progress on\nvisual understanding tasks. However, they often prioritize language knowledge\nover image information on visual reasoning tasks, incurring performance\ndegradation. To tackle this issue, we first identify the drawbacks of existing\nsolutions (i.e., limited multi-modal reasoning capacities, and insufficient and\nirrelevant visual descriptions). We then decompose visual reasoning process\ninto two stages: proactive visual perception (i.e., eyesight) and textual\nreasoning (i.e., wisdom), and introduce a novel visual reasoning framework\nnamed ProReason. This framework features decoupled vision-reasoning\ncapabilities and multi-run proactive perception. Briefly, given a multi-modal\nquestion, ProReason iterates proactive information collection and reasoning\nuntil the answer can be concluded with necessary and sufficient visual\ndescriptions. Notably, the disassociation of capabilities allows seamless\nintegration of existing large language models (LLMs) to compensate for the\nreasoning deficits of LVLMs. Our extensive experiments demonstrate that\nProReason outperforms existing multi-step reasoning frameworks on various\nbenchmarks for both open-source and closed-source models, with the average\nperformance gain reaching 13.2%. Besides, the integration of LLMs allows\nProReason to produce high-quality visual reasoning data, which empowers\nProReason-distilled models (i.e., ProReason-VL and ProReason-Q3) to achieve\nsuperior performance in downstream tasks. Our insights into existing solutions\nand the decoupled perspective for feasible integration of LLMs illuminate\nfuture research on visual reasoning techniques, especially LLM-assisted ones."}
{"id": "2505.23473", "pdf": "https://arxiv.org/pdf/2505.23473", "abs": "https://arxiv.org/abs/2505.23473", "authors": ["Xiaorui Wu", "Xiaofeng Mao", "Xin Zhang", "Fei Li", "Chong Teng", "Yuxiang Peng", "Li Zheng", "Donghong Ji", "Zhuang Li"], "title": "EVOREFUSE: Evolutionary Prompt Optimization for Evaluation and Mitigation of LLM Over-Refusal to Pseudo-Malicious Instructions", "categories": ["cs.AI"], "comment": null, "summary": "Large language models (LLMs) frequently refuse to respond to pseudo-malicious\ninstructions: semantically harmless input queries triggering unnecessary LLM\nrefusals due to conservative safety alignment, significantly impairing user\nexperience. Collecting such instructions is crucial for evaluating and\nmitigating over-refusals, but existing instruction curation methods, like\nmanual creation or instruction rewriting, either lack scalability or fail to\nproduce sufficiently diverse and effective refusal-inducing prompts. To address\nthese limitations, we introduce EVOREFUSE, a prompt optimization approach that\ngenerates diverse pseudo-malicious instructions consistently eliciting\nconfident refusals across LLMs. EVOREFUSE employs an evolutionary algorithm\nexploring the instruction space in more diverse directions than existing\nmethods via mutation strategies and recombination, and iteratively evolves seed\ninstructions to maximize evidence lower bound on LLM refusal probability. Using\nEVOREFUSE, we create two novel datasets: EVOREFUSE-TEST, a benchmark of 582\npseudo-malicious instructions that outperforms the next-best benchmark with\n140.41% higher average refusal triggering rate across 9 LLMs, 34.86% greater\nlexical diversity, and 40.03% improved LLM response confidence scores; and\nEVOREFUSE-ALIGN, which provides 3,000 pseudo-malicious instructions with\nresponses for supervised and preference-based alignment training.\nLLAMA3.1-8B-INSTRUCT supervisedly fine-tuned on EVOREFUSE-ALIGN achieves up to\n14.31% fewer over-refusals than models trained on the second-best alignment\ndataset, without compromising safety. Our analysis with EVOREFUSE-TEST reveals\nmodels trigger over-refusals by overly focusing on sensitive keywords while\nignoring broader context."}
{"id": "2505.24556", "pdf": "https://arxiv.org/pdf/2505.24556", "abs": "https://arxiv.org/abs/2505.24556", "authors": ["Gabriel V Cardoso", "Mike Pereira"], "title": "Predictive posterior sampling from non-stationnary Gaussian process priors via Diffusion models with application to climate data", "categories": ["stat.ML", "cs.LG"], "comment": null, "summary": "Bayesian models based on Gaussian processes (GPs) offer a flexible framework\nto predict spatially distributed variables with uncertainty. But the use of\nnonstationary priors, often necessary for capturing complex spatial patterns,\nmakes sampling from the predictive posterior distribution (PPD) computationally\nintractable. In this paper, we propose a two-step approach based on diffusion\ngenerative models (DGMs) to mimic PPDs associated with non-stationary GP\npriors: we replace the GP prior by a DGM surrogate, and leverage recent\nadvances on training-free guidance algorithms for DGMs to sample from the\ndesired posterior distribution. We apply our approach to a rich non-stationary\nGP prior from which exact posterior sampling is untractable and validate that\nthe issuing distributions are close to their GP counterpart using several\nstatistical metrics. We also demonstrate how one can fine-tune the trained DGMs\nto target specific parts of the GP prior. Finally we apply the proposed\napproach to solve inverse problems arising in environmental sciences, thus\nyielding state-of-the-art predictions."}
{"id": "2410.13460", "pdf": "https://arxiv.org/pdf/2410.13460", "abs": "https://arxiv.org/abs/2410.13460", "authors": ["Ronja Stern", "Ken Kawamura", "Matthias StÃ¼rmer", "Ilias Chalkidis", "Joel Niklaus"], "title": "From Citations to Criticality: Predicting Legal Decision Influence in the Multilingual Swiss Jurisprudence", "categories": ["cs.CL", "cs.AI", "cs.LG", "68T50", "I.2; I.7"], "comment": "Accepted to ACL main 2025", "summary": "Many court systems are overwhelmed all over the world, leading to huge\nbacklogs of pending cases. Effective triage systems, like those in emergency\nrooms, could ensure proper prioritization of open cases, optimizing time and\nresource allocation in the court system. In this work, we introduce the\nCriticality Prediction dataset, a novel resource for evaluating case\nprioritization. Our dataset features a two-tier labeling system: (1) the binary\nLD-Label, identifying cases published as Leading Decisions (LD), and (2) the\nmore granular Citation-Label, ranking cases by their citation frequency and\nrecency, allowing for a more nuanced evaluation. Unlike existing approaches\nthat rely on resource-intensive manual annotations, we algorithmically derive\nlabels leading to a much larger dataset than otherwise possible. We evaluate\nseveral multilingual models, including both smaller fine-tuned models and large\nlanguage models in a zero-shot setting. Our results show that the fine-tuned\nmodels consistently outperform their larger counterparts, thanks to our large\ntraining set. Our results highlight that for highly domain-specific tasks like\nours, large training sets are still valuable."}
{"id": "2410.15432", "pdf": "https://arxiv.org/pdf/2410.15432", "abs": "https://arxiv.org/abs/2410.15432", "authors": ["Yongrui Yu", "Yannian Gu", "Shaoting Zhang", "Xiaofan Zhang"], "title": "MedDiff-FM: A Diffusion-based Foundation Model for Versatile Medical Image Applications", "categories": ["cs.CV"], "comment": null, "summary": "Diffusion models have achieved significant success in both natural image and\nmedical image domains, encompassing a wide range of applications. Previous\ninvestigations in medical images have often been constrained to specific\nanatomical regions, particular applications, and limited datasets, resulting in\nisolated diffusion models. This paper introduces a diffusion-based foundation\nmodel to address a diverse range of medical image tasks, namely MedDiff-FM.\nMedDiff-FM leverages 3D CT images from multiple publicly available datasets,\ncovering anatomical regions from head to abdomen, to pre-train a diffusion\nfoundation model, and explores the capabilities of the diffusion foundation\nmodel across a variety of application scenarios. The diffusion foundation model\nhandles multi-level integrated image processing both at the image-level and\npatch-level, utilizes position embedding to establish multi-level spatial\nrelationships, and leverages region classes and anatomical structures to\ncapture certain anatomical regions. MedDiff-FM manages several downstream tasks\nseamlessly, including image denoising, anomaly detection, and image synthesis.\nMedDiff-FM is also capable of performing super-resolution, lesion generation,\nand lesion inpainting by rapidly fine-tuning the diffusion foundation model\nusing ControlNet with task-specific conditions. The experimental results\ndemonstrate the effectiveness of MedDiff-FM in addressing diverse downstream\nmedical image tasks."}
{"id": "2306.04542", "pdf": "https://arxiv.org/pdf/2306.04542", "abs": "https://arxiv.org/abs/2306.04542", "authors": ["Ziyi Chang", "George Alex Koulieris", "Hyung Jin Chang", "Hubert P. H. Shum"], "title": "On the Design Fundamentals of Diffusion Models: A Survey", "categories": ["cs.LG", "cs.AI", "cs.CV"], "comment": "Accepted in Pattern Recognition", "summary": "Diffusion models are learning pattern-learning systems to model and sample\nfrom data distributions with three functional components namely the forward\nprocess, the reverse process, and the sampling process. The components of\ndiffusion models have gained significant attention with many design factors\nbeing considered in common practice. Existing reviews have primarily focused on\nhigher-level solutions, covering less on the design fundamentals of components.\nThis study seeks to address this gap by providing a comprehensive and coherent\nreview of seminal designable factors within each functional component of\ndiffusion models. This provides a finer-grained perspective of diffusion\nmodels, benefiting future studies in the analysis of individual components, the\ndesign factors for different purposes, and the implementation of diffusion\nmodels."}
{"id": "2505.24650", "pdf": "https://arxiv.org/pdf/2505.24650", "abs": "https://arxiv.org/abs/2505.24650", "authors": ["Hariom Tatsat", "Ariye Shater"], "title": "Beyond the Black Box: Interpretability of LLMs in Finance", "categories": ["cs.CE", "cs.LG", "q-fin.ST"], "comment": "28 pages, 15 figures", "summary": "Large Language Models (LLMs) exhibit remarkable capabilities across a\nspectrum of tasks in financial services, including report generation, chatbots,\nsentiment analysis, regulatory compliance, investment advisory, financial\nknowledge retrieval, and summarization. However, their intrinsic complexity and\nlack of transparency pose significant challenges, especially in the highly\nregulated financial sector, where interpretability, fairness, and\naccountability are critical. As far as we are aware, this paper presents the\nfirst application in the finance domain of understanding and utilizing the\ninner workings of LLMs through mechanistic interpretability, addressing the\npressing need for transparency and control in AI systems. Mechanistic\ninterpretability is the most intuitive and transparent way to understand LLM\nbehavior by reverse-engineering their internal workings. By dissecting the\nactivations and circuits within these models, it provides insights into how\nspecific features or components influence predictions - making it possible not\nonly to observe but also to modify model behavior. In this paper, we explore\nthe theoretical aspects of mechanistic interpretability and demonstrate its\npractical relevance through a range of financial use cases and experiments,\nincluding applications in trading strategies, sentiment analysis, bias, and\nhallucination detection. While not yet widely adopted, mechanistic\ninterpretability is expected to become increasingly vital as adoption of LLMs\nincreases. Advanced interpretability tools can ensure AI systems remain\nethical, transparent, and aligned with evolving financial regulations. In this\npaper, we have put special emphasis on how these techniques can help unlock\ninterpretability requirements for regulatory and compliance purposes -\naddressing both current needs and anticipating future expectations from\nfinancial regulators globally."}
{"id": "2410.14248", "pdf": "https://arxiv.org/pdf/2410.14248", "abs": "https://arxiv.org/abs/2410.14248", "authors": ["Olga Loginova", "Oleksandr Bezrukov", "Ravi Shekhar", "Alexey Kravets"], "title": "Addressing Blind Guessing: Calibration of Selection Bias in Multiple-Choice Question Answering by Video Language Models", "categories": ["cs.CL"], "comment": null, "summary": "Evaluating Video Language Models (VLMs) is a challenging task. Due to its\ntransparency, Multiple-Choice Question Answering (MCQA) is widely used to\nmeasure the performance of these models through accuracy. However, existing\nMCQA benchmarks fail to capture the full reasoning capabilities of VLMs due to\nselection bias, when models disproportionately favor certain answer options\nbased on positional patterns observed during training. In this work, we conduct\na comprehensive empirical analysis of several VLM architectures across major\ndatasets designed to assess complex video-focused reasoning. We identify where\nthe bias is most pronounced and demonstrate to what extent model responses\nreflect genuine understanding of video content and related questions, as\nopposed to reliance on arbitrary patterns or superficial cues, such as answer\nposition. By decomposing the MCQA task and adapting fairness bias metrics to\nVLMs, we introduce a post-processing calibration technique BOLD to balance this\nbias. Our results show that reducing selection bias improves not only debiasing\nmetrics but also overall model performance, including Accuracy and F1 Mean\nscore. Our method, by suppressing \"blind guessing\", offers a more cost- and\ntime-effective approach to mitigating selection bias compared to existing\ntechniques. This study represents the first focused investigation of selection\nbias in video-to-text LLM-powered models."}
{"id": "2411.14494", "pdf": "https://arxiv.org/pdf/2411.14494", "abs": "https://arxiv.org/abs/2411.14494", "authors": ["Nitish Shukla", "Arun Ross"], "title": "dc-GAN: Dual-Conditioned GAN for Face Demorphing From a Single Morph", "categories": ["cs.CV"], "comment": null, "summary": "A facial morph is an image strategically created by combining two face images\npertaining to two distinct identities. The goal is to create a face image that\ncan be matched to two different identities by a face matcher. Face demorphing\ninverts this process and attempts to recover the original images constituting a\nfacial morph. Existing demorphing techniques have two major limitations: (a)\nthey assume that some identities are common in the train and test sets; and (b)\nthey are prone to the morph replication problem, where the outputs are merely\nreplicates of the input morph. In this paper, we overcome these issues by\nproposing dc-GAN (dual-conditioned GAN), a novel demorphing method conditioned\non the morph image as well as the embedding extracted from the image. Our\nmethod overcomes the morph replication problem and produces high-fidelity\nreconstructions of the constituent images. Moreover, the proposed method is\nhighly generalizable and applicable to both reference-based and reference-free\ndemorphing methods. Experiments were conducted using the AMSL, FRLL-Morphs, and\nMorDiff datasets to demonstrate the efficacy of the method."}
{"id": "2306.17100", "pdf": "https://arxiv.org/pdf/2306.17100", "abs": "https://arxiv.org/abs/2306.17100", "authors": ["Federico Berto", "Chuanbo Hua", "Junyoung Park", "Laurin Luttmann", "Yining Ma", "Fanchen Bu", "Jiarui Wang", "Haoran Ye", "Minsu Kim", "Sanghyeok Choi", "Nayeli Gast Zepeda", "AndrÃ© Hottung", "Jianan Zhou", "Jieyi Bi", "Yu Hu", "Fei Liu", "Hyeonah Kim", "Jiwoo Son", "Haeyeon Kim", "Davide Angioni", "Wouter Kool", "Zhiguang Cao", "Qingfu Zhang", "Joungho Kim", "Jie Zhang", "Kijung Shin", "Cathy Wu", "Sungsoo Ahn", "Guojie Song", "Changhyun Kwon", "Kevin Tierney", "Lin Xie", "Jinkyoo Park"], "title": "RL4CO: an Extensive Reinforcement Learning for Combinatorial Optimization Benchmark", "categories": ["cs.LG", "cs.AI"], "comment": "KDD 2025 Oral", "summary": "Combinatorial optimization (CO) is fundamental to several real-world\napplications, from logistics and scheduling to hardware design and resource\nallocation. Deep reinforcement learning (RL) has recently shown significant\nbenefits in solving CO problems, reducing reliance on domain expertise and\nimproving computational efficiency. However, the absence of a unified\nbenchmarking framework leads to inconsistent evaluations, limits\nreproducibility, and increases engineering overhead, raising barriers to\nadoption for new researchers. To address these challenges, we introduce RL4CO,\na unified and extensive benchmark with in-depth library coverage of 27 CO\nproblem environments and 23 state-of-the-art baselines. Built on efficient\nsoftware libraries and best practices in implementation, RL4CO features\nmodularized implementation and flexible configurations of diverse environments,\npolicy architectures, RL algorithms, and utilities with extensive\ndocumentation. RL4CO helps researchers build on existing successes while\nexploring and developing their own designs, facilitating the entire research\nprocess by decoupling science from heavy engineering. We finally provide\nextensive benchmark studies to inspire new insights and future work. RL4CO has\nalready attracted numerous researchers in the community and is open-sourced at\nhttps://github.com/ai4co/rl4co."}
{"id": "2505.24668", "pdf": "https://arxiv.org/pdf/2505.24668", "abs": "https://arxiv.org/abs/2505.24668", "authors": ["Jonghyun Ham", "Maximilian Fleissner", "Debarghya Ghoshdastidar"], "title": "Impact of Bottleneck Layers and Skip Connections on the Generalization of Linear Denoising Autoencoders", "categories": ["stat.ML", "cs.LG"], "comment": null, "summary": "Modern deep neural networks exhibit strong generalization even in highly\noverparameterized regimes. Significant progress has been made to understand\nthis phenomenon in the context of supervised learning, but for unsupervised\ntasks such as denoising, several open questions remain. While some recent works\nhave successfully characterized the test error of the linear denoising problem,\nthey are limited to linear models (one-layer network). In this work, we focus\non two-layer linear denoising autoencoders trained under gradient flow,\nincorporating two key ingredients of modern deep learning architectures: A\nlow-dimensional bottleneck layer that effectively enforces a rank constraint on\nthe learned solution, as well as the possibility of a skip connection that\nbypasses the bottleneck. We derive closed-form expressions for all critical\npoints of this model under product regularization, and in particular describe\nits global minimizer under the minimum-norm principle. From there, we derive\nthe test risk formula in the overparameterized regime, both for models with and\nwithout skip connections. Our analysis reveals two interesting phenomena:\nFirstly, the bottleneck layer introduces an additional complexity measure akin\nto the classical bias-variance trade-off -- increasing the bottleneck width\nreduces bias but introduces variance, and vice versa. Secondly, skip connection\ncan mitigate the variance in denoising autoencoders -- especially when the\nmodel is mildly overparameterized. We further analyze the impact of skip\nconnections in denoising autoencoder using random matrix theory and support our\nclaims with numerical evidence."}
{"id": "2410.21728", "pdf": "https://arxiv.org/pdf/2410.21728", "abs": "https://arxiv.org/abs/2410.21728", "authors": ["Kangyang Luo", "Zichen Ding", "Zhenmin Weng", "Lingfeng Qiao", "Meng Zhao", "Xiang Li", "Di Yin", "Jinlong Shu"], "title": "Let's Be Self-generated via Step by Step: A Curriculum Learning Approach to Automated Reasoning with Large Language Models", "categories": ["cs.CL"], "comment": "Accepted by ACL2025(Findings)", "summary": "While Chain of Thought (CoT) prompting approaches have significantly\nconsolidated the reasoning capabilities of large language models (LLMs), they\nstill face limitations that require extensive human effort or have performance\nneeds to be improved. Existing endeavors have focused on bridging these gaps;\nhowever, these approaches either hinge on external data and cannot completely\neliminate manual effort, or they fall short in effectively directing LLMs to\ngenerate high-quality exemplary prompts. To address the said pitfalls, we\npropose a novel prompt approach for automatic reasoning named \\textbf{LBS3},\ninspired by curriculum learning which better reflects human learning habits.\nSpecifically, LBS3 initially steers LLMs to recall easy-to-hard proxy queries\nthat are pertinent to the target query. Following this, it invokes a\nprogressive strategy that utilizes exemplary prompts stemmed from easy-proxy\nqueries to direct LLMs in solving hard-proxy queries, enabling the high-quality\nof the proxy solutions. Finally, our extensive experiments in various\nreasoning-intensive tasks with varying open- and closed-source LLMs show that\nLBS3 achieves strongly competitive performance compared to the SOTA baselines."}
{"id": "2411.16725", "pdf": "https://arxiv.org/pdf/2411.16725", "abs": "https://arxiv.org/abs/2411.16725", "authors": ["Dahye Kim", "Xavier Thomas", "Deepti Ghadiyaram"], "title": "$\\textit{Revelio}$: Interpreting and leveraging semantic information in diffusion models", "categories": ["cs.CV"], "comment": "15 pages, 14 figures", "summary": "We study $\\textit{how}$ rich visual semantic information is represented\nwithin various layers and denoising timesteps of different diffusion\narchitectures. We uncover monosemantic interpretable features by leveraging\nk-sparse autoencoders (k-SAE). We substantiate our mechanistic interpretations\nvia transfer learning using light-weight classifiers on off-the-shelf diffusion\nmodels' features. On $4$ datasets, we demonstrate the effectiveness of\ndiffusion features for representation learning. We provide an in-depth analysis\nof how different diffusion architectures, pre-training datasets, and language\nmodel conditioning impacts visual representation granularity, inductive biases,\nand transfer learning capabilities. Our work is a critical step towards\ndeepening interpretability of black-box diffusion models. Code and\nvisualizations available at: https://github.com/revelio-diffusion/revelio"}
{"id": "2404.16792", "pdf": "https://arxiv.org/pdf/2404.16792", "abs": "https://arxiv.org/abs/2404.16792", "authors": ["Chujie Zheng", "Ziqi Wang", "Heng Ji", "Minlie Huang", "Nanyun Peng"], "title": "Model Extrapolation Expedites Alignment", "categories": ["cs.LG", "cs.AI", "cs.CL"], "comment": "ACL 2025", "summary": "Given the high computational cost of preference alignment training of large\nlanguage models (LLMs), exploring efficient methods to reduce the training\noverhead remains an important and compelling research problem. Motivated by the\nobservation that alignment training typically involves only small parameter\nchanges without injecting new knowledge into models, we propose a\nstraightforward method called ExPO (model extrapolation) to expedite LLMs'\nalignment with human preferences. Given a partially-trained model and its\ninitial SFT checkpoint, ExPO improves the implicit optimization objective of\nalignment training by simply amplifying the parameter change based on a\nfirst-order approximation, without any additional training overhead. Through\ncontrolled experiments, we demonstrate that ExPO boosts a DPO model trained\nwith only 20% steps to outperform the fully-trained one. Moreover, we show that\nExPO notably improves existing open-source LLMs (ranging from 1.8B to 70B\nparameters) on the leading AlpacaEval 2.0 and MT-Bench benchmarks, which\nhighlights ExPO's broader utility in efficiently enhancing LLM alignment."}
{"id": "2505.24704", "pdf": "https://arxiv.org/pdf/2505.24704", "abs": "https://arxiv.org/abs/2505.24704", "authors": ["Hideaki Kim", "Tomoharu Iwata", "Akinori Fujino"], "title": "K$^2$IE: Kernel Method-based Kernel Intensity Estimators for Inhomogeneous Poisson Processes", "categories": ["stat.ML", "cs.LG"], "comment": "Accepted to ICML 2025", "summary": "Kernel method-based intensity estimators, formulated within reproducing\nkernel Hilbert spaces (RKHSs), and classical kernel intensity estimators (KIEs)\nhave been among the most easy-to-implement and feasible methods for estimating\nthe intensity functions of inhomogeneous Poisson processes. While both\napproaches share the term \"kernel\", they are founded on distinct theoretical\nprinciples, each with its own strengths and limitations. In this paper, we\npropose a novel regularized kernel method for Poisson processes based on the\nleast squares loss and show that the resulting intensity estimator involves a\nspecialized variant of the representer theorem: it has the dual coefficient of\nunity and coincides with classical KIEs. This result provides new theoretical\ninsights into the connection between classical KIEs and kernel method-based\nintensity estimators, while enabling us to develop an efficient KIE by\nleveraging advanced techniques from RKHS theory. We refer to the proposed model\nas the kernel method-based kernel intensity estimator (K$^2$IE). Through\nexperiments on synthetic datasets, we show that K$^2$IE achieves comparable\npredictive performance while significantly surpassing the state-of-the-art\nkernel method-based estimator in computational efficiency."}
{"id": "2411.06160", "pdf": "https://arxiv.org/pdf/2411.06160", "abs": "https://arxiv.org/abs/2411.06160", "authors": ["Jingyi Zhou", "Senlin Luo", "Haofan Chen"], "title": "Expansion Quantization Network: An Efficient Micro-emotion Annotation and Detection Framework", "categories": ["cs.CL", "cs.AI", "cs.CV", "cs.HC", "cs.LG"], "comment": "3.1 There is a misstatement in the EQN Framework section", "summary": "Text emotion detection constitutes a crucial foundation for advancing\nartificial intelligence from basic comprehension to the exploration of\nemotional reasoning. Most existing emotion detection datasets rely on manual\nannotations, which are associated with high costs, substantial subjectivity,\nand severe label imbalances. This is particularly evident in the inadequate\nannotation of micro-emotions and the absence of emotional intensity\nrepresentation, which fail to capture the rich emotions embedded in sentences\nand adversely affect the quality of downstream task completion. By proposing an\nall-labels and training-set label regression method, we map label values to\nenergy intensity levels, thereby fully leveraging the learning capabilities of\nmachine models and the interdependencies among labels to uncover multiple\nemotions within samples. This led to the establishment of the Emotion\nQuantization Network (EQN) framework for micro-emotion detection and\nannotation. Using five commonly employed sentiment datasets, we conducted\ncomparative experiments with various models, validating the broad applicability\nof our framework within NLP machine learning models. Based on the EQN\nframework, emotion detection and annotation are conducted on the GoEmotions\ndataset. A comprehensive comparison with the results from Google literature\ndemonstrates that the EQN framework possesses a high capability for automatic\ndetection and annotation of micro-emotions. The EQN framework is the first to\nachieve automatic micro-emotion annotation with energy-level scores, providing\nstrong support for further emotion detection analysis and the quantitative\nresearch of emotion computing."}
{"id": "2411.17261", "pdf": "https://arxiv.org/pdf/2411.17261", "abs": "https://arxiv.org/abs/2411.17261", "authors": ["Fan Yang", "Ru Zhen", "Jianing Wang", "Yanhao Zhang", "Haoxiang Chen", "Haonan Lu", "Sicheng Zhao", "Guiguang Ding"], "title": "HEIE: MLLM-Based Hierarchical Explainable AIGC Image Implausibility Evaluator", "categories": ["cs.CV", "cs.AI"], "comment": "Accepted by CVPR 2025", "summary": "AIGC images are prevalent across various fields, yet they frequently suffer\nfrom quality issues like artifacts and unnatural textures. Specialized models\naim to predict defect region heatmaps but face two primary challenges: (1) lack\nof explainability, failing to provide reasons and analyses for subtle defects,\nand (2) inability to leverage common sense and logical reasoning, leading to\npoor generalization. Multimodal large language models (MLLMs) promise better\ncomprehension and reasoning but face their own challenges: (1) difficulty in\nfine-grained defect localization due to the limitations in capturing tiny\ndetails, and (2) constraints in providing pixel-wise outputs necessary for\nprecise heatmap generation. To address these challenges, we propose HEIE: a\nnovel MLLM-Based Hierarchical Explainable Image Implausibility Evaluator. We\nintroduce the CoT-Driven Explainable Trinity Evaluator, which integrates\nheatmaps, scores, and explanation outputs, using CoT to decompose complex tasks\ninto subtasks of increasing difficulty and enhance interpretability. Our\nAdaptive Hierarchical Implausibility Mapper synergizes low-level image features\nwith high-level mapper tokens from LLMs, enabling precise local-to-global\nhierarchical heatmap predictions through an uncertainty-based adaptive token\napproach. Moreover, we propose a new dataset: Expl-AIGI-Eval, designed to\nfacilitate interpretable implausibility evaluation of AIGC images. Our method\ndemonstrates state-of-the-art performance through extensive experiments. Our\nproject is at https://yfthu.github.io/HEIE/."}
{"id": "2405.02696", "pdf": "https://arxiv.org/pdf/2405.02696", "abs": "https://arxiv.org/abs/2405.02696", "authors": ["Liangqi Lei", "Keke Gai", "Jing Yu", "Liehuang Zhu"], "title": "DiffuseTrace: A Transparent and Flexible Watermarking Scheme for Latent Diffusion Model", "categories": ["cs.CR", "cs.AI"], "comment": null, "summary": "Latent Diffusion Models (LDMs) enable a wide range of applications but raise\nethical concerns regarding illegal utilization. Adding watermarks to generative\nmodel outputs is a vital technique employed for copyright tracking and\nmitigating potential risks associated with Artificial Intelligence\n(AI)-generated contents. However, post-processed watermarking methods are\nunable to withstand generative watermark attacks and there exists a trade-off\nbetween image fidelity and watermark strength. Therefore, we propose a novel\ntechnique called DiffuseTrace. DiffuseTrace does not rely on fine-tuning of the\ndiffusion model components. The multi-bit watermark is a embedded into the\nimage space semantically without compromising image quality. The watermark\ncomponent can be utilized as a plug-in in arbitrary diffusion models. We\nvalidate through experiments the effectiveness and flexibility of DiffuseTrace.\nUnder 8 types of image processing watermark attacks and 3 types of generative\nwatermark attacks, DiffuseTrace maintains watermark detection rate of 99% and\nattribution accuracy of over 94%."}
{"id": "2505.24727", "pdf": "https://arxiv.org/pdf/2505.24727", "abs": "https://arxiv.org/abs/2505.24727", "authors": ["Xiaochen Zhang", "Haoyi Xiong"], "title": "Knockoff-Guided Compressive Sensing: A Statistical Machine Learning Framework for Support-Assured Signal Recovery", "categories": ["stat.ML", "cs.LG", "eess.SP"], "comment": null, "summary": "This paper introduces a novel Knockoff-guided compressive sensing framework,\nreferred to as \\TheName{}, which enhances signal recovery by leveraging precise\nfalse discovery rate (FDR) control during the support identification phase.\nUnlike LASSO, which jointly performs support selection and signal estimation\nwithout explicit error control, our method guarantees FDR control in finite\nsamples, enabling more reliable identification of the true signal support. By\nseparating and controlling the support recovery process through statistical\nKnockoff filters, our framework achieves more accurate signal reconstruction,\nespecially in challenging scenarios where traditional methods fail. We\nestablish theoretical guarantees demonstrating how FDR control directly ensures\nrecovery performance under weaker conditions than traditional $\\ell_1$-based\ncompressive sensing methods, while maintaining accurate signal reconstruction.\nExtensive numerical experiments demonstrate that our proposed Knockoff-based\nmethod consistently outperforms LASSO-based and other state-of-the-art\ncompressive sensing techniques. In simulation studies, our method improves\nF1-score by up to 3.9x over baseline methods, attributed to principled false\ndiscovery rate (FDR) control and enhanced support recovery. The method also\nconsistently yields lower reconstruction and relative errors. We further\nvalidate the framework on real-world datasets, where it achieves top downstream\npredictive performance across both regression and classification tasks, often\nnarrowing or even surpassing the performance gap relative to uncompressed\nsignals. These results establish \\TheName{} as a robust and practical\nalternative to existing approaches, offering both theoretical guarantees and\nstrong empirical performance through statistically grounded support selection."}
{"id": "2411.07404", "pdf": "https://arxiv.org/pdf/2411.07404", "abs": "https://arxiv.org/abs/2411.07404", "authors": ["Julian Minder", "Kevin Du", "Niklas Stoehr", "Giovanni Monea", "Chris Wendler", "Robert West", "Ryan Cotterell"], "title": "Controllable Context Sensitivity and the Knob Behind It", "categories": ["cs.CL", "cs.AI"], "comment": "Published as a conference paper at ICLR 2025", "summary": "When making predictions, a language model must trade off how much it relies\non its context vs. its prior knowledge. Choosing how sensitive the model is to\nits context is a fundamental functionality, as it enables the model to excel at\ntasks like retrieval-augmented generation and question-answering. In this\npaper, we search for a knob which controls this sensitivity, determining\nwhether language models answer from the context or their prior knowledge. To\nguide this search, we design a task for controllable context sensitivity. In\nthis task, we first feed the model a context (Paris is in England) and a\nquestion (Where is Paris?); we then instruct the model to either use its prior\nor contextual knowledge and evaluate whether it generates the correct answer\nfor both intents (either France or England). When fine-tuned on this task,\ninstruction-tuned versions of Llama-3.1, Mistral-v0.3, and Gemma-2 can solve it\nwith high accuracy (85-95%). Analyzing these high-performing models, we narrow\ndown which layers may be important to context sensitivity using a novel linear\ntime algorithm. Then, in each model, we identify a 1-D subspace in a single\nlayer that encodes whether the model follows context or prior knowledge.\nInterestingly, while we identify this subspace in a fine-tuned model, we find\nthat the exact same subspace serves as an effective knob in not only that model\nbut also non-fine-tuned instruct and base models of that model family. Finally,\nwe show a strong correlation between a model's performance and how distinctly\nit separates context-agreeing from context-ignoring answers in this subspace.\nThese results suggest a single subspace facilitates how the model chooses\nbetween context and prior knowledge, hinting at a simple fundamental mechanism\nthat controls this behavior."}
{"id": "2411.17771", "pdf": "https://arxiv.org/pdf/2411.17771", "abs": "https://arxiv.org/abs/2411.17771", "authors": ["Xinyu Zhang", "Lingling Zhang", "Yanrui Wu", "Muye Huang", "Wenjun Wu", "Bo Li", "Shaowei Wang", "Basura Fernando", "Jun Liu"], "title": "Diagram-Driven Course Questions Generation", "categories": ["cs.CV"], "comment": null, "summary": "Visual Question Generation (VQG) research focuses predominantly on natural\nimages while neglecting the diagram, which is a critical component in\neducational materials. To meet the needs of pedagogical assessment, we propose\nthe Diagram-Driven Course Questions Generation (DDCQG) task and construct\nDiagramQG, a comprehensive dataset with 15,720 diagrams and 25,798 questions\nacross 37 subjects and 371 courses. Our approach employs course and input text\nconstraints to generate course-relevant questions about specific diagram\nelements. We reveal three challenges of DDCQG: domain-specific knowledge\nrequirements across courses, long-tail distribution in course coverage, and\nhigh information density in diagrams. To address these, we propose the\nHierarchical Knowledge Integration framework (HKI-DDCQG), which utilizes\ntrainable CLIP for identifying relevant diagram patches, leverages frozen\nvision-language models for knowledge extraction, and generates questions with\ntrainable T5. Experiments demonstrate that HKI-DDCQG outperforms existing\nmodels on DiagramQG while maintaining strong generalizability across natural\nimage datasets, establishing a strong baseline for DDCQG."}
{"id": "2406.06751", "pdf": "https://arxiv.org/pdf/2406.06751", "abs": "https://arxiv.org/abs/2406.06751", "authors": ["Zachary Bastiani", "Robert M. Kirby", "Jacob Hochhalter", "Shandian Zhe"], "title": "Complexity-Aware Deep Symbolic Regression with Robust Risk-Seeking Policy Gradients", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "We propose a novel deep symbolic regression approach to enhance the\nrobustness and interpretability of data-driven mathematical expression\ndiscovery. Our work is aligned with the popular DSR framework which focuses on\nlearning a data-specific expression generator, without relying on pretrained\nmodels or additional search or planning procedures. Despite the success of\nexisting DSR methods, they are built on recurrent neural networks, solely\nguided by data fitness, and potentially meet tail barriers that can zero out\nthe policy gradient, causing inefficient model updates. To overcome these\nlimitations, we design a decoder-only architecture that performs attention in\nthe frequency domain and introduce a dual-indexed position encoding to conduct\nlayer-wise generation. Second, we propose a Bayesian information criterion\n(BIC)-based reward function that can automatically adjust the trade-off between\nexpression complexity and data fitness, without the need for explicit manual\ntuning. Third, we develop a ranking-based weighted policy update method that\neliminates the tail barriers and enhances training effectiveness. Extensive\nbenchmarks and systematic experiments demonstrate the advantages of our\napproach."}
{"id": "2505.24769", "pdf": "https://arxiv.org/pdf/2505.24769", "abs": "https://arxiv.org/abs/2505.24769", "authors": ["Claudia Merger", "Sebastian Goldt"], "title": "Generalization Dynamics of Linear Diffusion Models", "categories": ["stat.ML", "cond-mat.dis-nn", "cs.LG", "math.ST", "stat.TH"], "comment": null, "summary": "Diffusion models trained on finite datasets with $N$ samples from a target\ndistribution exhibit a transition from memorisation, where the model reproduces\ntraining examples, to generalisation, where it produces novel samples that\nreflect the underlying data distribution. Understanding this transition is key\nto characterising the sample efficiency and reliability of generative models,\nbut our theoretical understanding of this transition is incomplete. Here, we\nanalytically study the memorisation-to-generalisation transition in a simple\nmodel using linear denoisers, which allow explicit computation of test errors,\nsampling distributions, and Kullback-Leibler divergences between samples and\ntarget distribution. Using these measures, we predict that this transition\noccurs roughly when $N \\asymp d$, the dimension of the inputs. When $N$ is\nsmaller than the dimension of the inputs $d$, so that only a fraction of\nrelevant directions of variation are present in the training data, we\ndemonstrate how both regularization and early stopping help to prevent\noverfitting. For $N > d$, we find that the sampling distributions of linear\ndiffusion models approach their optimum (measured by the Kullback-Leibler\ndivergence) linearly with $d/N$, independent of the specifics of the data\ndistribution. Our work clarifies how sample complexity governs generalisation\nin a simple model of diffusion-based generative models and provides insight\ninto the training dynamics of linear denoisers."}
{"id": "2411.17116", "pdf": "https://arxiv.org/pdf/2411.17116", "abs": "https://arxiv.org/abs/2411.17116", "authors": ["Shantanu Acharya", "Fei Jia", "Boris Ginsburg"], "title": "Star Attention: Efficient LLM Inference over Long Sequences", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "Accepted at ICML 2025", "summary": "Inference with Transformer-based Large Language Models (LLMs) on long\nsequences is both costly and slow due to the quadratic complexity of the\nself-attention mechanism. We introduce Star Attention, a two-phase block-sparse\napproximation that improves computational efficiency by sharding attention\nacross multiple hosts while minimizing communication overhead. In the first\nphase, the context is processed using blockwise-local attention across hosts,\nin parallel. In the second phase, query and response tokens attend to all prior\ncached tokens through sequence-global attention. Star Attention integrates\nseamlessly with most Transformer-based LLMs trained with global attention,\nreducing memory requirements and inference time by up to 11x while preserving\n97-100% of accuracy."}
{"id": "2411.18672", "pdf": "https://arxiv.org/pdf/2411.18672", "abs": "https://arxiv.org/abs/2411.18672", "authors": ["Alice Heiman", "Xiaoman Zhang", "Emma Chen", "Sung Eun Kim", "Pranav Rajpurkar"], "title": "FactCheXcker: Mitigating Measurement Hallucinations in Chest X-ray Report Generation Models", "categories": ["cs.CV"], "comment": null, "summary": "Medical vision-language model models often struggle with generating accurate\nquantitative measurements in radiology reports, leading to hallucinations that\nundermine clinical reliability. We introduce FactCheXcker, a modular framework\nthat de-hallucinates radiology report measurements by leveraging an improved\nquery-code-update paradigm. Specifically, FactCheXcker employs specialized\nmodules and the code generation capabilities of large language models to solve\nmeasurement queries generated based on the original report. After extracting\nmeasurable findings, the results are incorporated into an updated report. We\nevaluate FactCheXcker on endotracheal tube placement, which accounts for an\naverage of 78% of report measurements, using the MIMIC-CXR dataset and 11\nmedical report-generation models. Our results show that FactCheXcker\nsignificantly reduces hallucinations, improves measurement precision, and\nmaintains the quality of the original reports. Specifically, FactCheXcker\nimproves the performance of all 11 models and achieves an average improvement\nof 94.0% in reducing measurement hallucinations measured by mean absolute\nerror."}
{"id": "2406.18354", "pdf": "https://arxiv.org/pdf/2406.18354", "abs": "https://arxiv.org/abs/2406.18354", "authors": ["Gianluca De Carlo", "Andrea Mastropietro", "Aris Anagnostopoulos"], "title": "Kolmogorov-Arnold Graph Neural Networks", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Graph neural networks (GNNs) excel in learning from network-like data but\noften lack interpretability, making their application challenging in domains\nrequiring transparent decision-making. We propose the Graph Kolmogorov-Arnold\nNetwork (GKAN), a novel GNN model leveraging spline-based activation functions\non edges to enhance both accuracy and interpretability. Our experiments on five\nbenchmark datasets demonstrate that GKAN outperforms state-of-the-art GNN\nmodels in node classification, link prediction, and graph classification tasks.\nIn addition to the improved accuracy, GKAN's design inherently provides clear\ninsights into the model's decision-making process, eliminating the need for\npost-hoc explainability techniques. This paper discusses the methodology,\nperformance, and interpretability of GKAN, highlighting its potential for\napplications in domains where interpretability is crucial."}
{"id": "2505.24849", "pdf": "https://arxiv.org/pdf/2505.24849", "abs": "https://arxiv.org/abs/2505.24849", "authors": ["Jean Barbier", "Francesco Camilli", "Minh-Toan Nguyen", "Mauro Pastore", "Rudy Skerk"], "title": "Statistical mechanics of extensive-width Bayesian neural networks near interpolation", "categories": ["stat.ML", "cond-mat.dis-nn", "cond-mat.stat-mech", "cs.IT", "cs.LG", "math.IT"], "comment": "9 pages + appendices, 12 figures. This submission supersedes\n  arXiv:2501.18530", "summary": "For three decades statistical mechanics has been providing a framework to\nanalyse neural networks. However, the theoretically tractable models, e.g.,\nperceptrons, random features models and kernel machines, or multi-index models\nand committee machines with few neurons, remained simple compared to those used\nin applications. In this paper we help reducing the gap between practical\nnetworks and their theoretical understanding through a statistical physics\nanalysis of the supervised learning of a two-layer fully connected network with\ngeneric weight distribution and activation function, whose hidden layer is\nlarge but remains proportional to the inputs dimension. This makes it more\nrealistic than infinitely wide networks where no feature learning occurs, but\nalso more expressive than narrow ones or with fixed inner weights. We focus on\nthe Bayes-optimal learning in the teacher-student scenario, i.e., with a\ndataset generated by another network with the same architecture. We operate\naround interpolation, where the number of trainable parameters and of data are\ncomparable and feature learning emerges. Our analysis uncovers a rich\nphenomenology with various learning transitions as the number of data\nincreases. In particular, the more strongly the features (i.e., hidden neurons\nof the target) contribute to the observed responses, the less data is needed to\nlearn them. Moreover, when the data is scarce, the model only learns non-linear\ncombinations of the teacher weights, rather than \"specialising\" by aligning its\nweights with the teacher's. Specialisation occurs only when enough data becomes\navailable, but it can be hard to find for practical training algorithms,\npossibly due to statistical-to-computational~gaps."}
{"id": "2412.04905", "pdf": "https://arxiv.org/pdf/2412.04905", "abs": "https://arxiv.org/abs/2412.04905", "authors": ["Minzheng Wang", "Xinghua Zhang", "Kun Chen", "Nan Xu", "Haiyang Yu", "Fei Huang", "Wenji Mao", "Yongbin Li"], "title": "DEMO: Reframing Dialogue Interaction with Fine-grained Element Modeling", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "ACL 2025 Findings. We release the code and data at\n  https://github.com/MozerWang/DEMO", "summary": "Large language models (LLMs) enabled dialogue systems have become one of the\ncentral modes in human-machine interaction, which bring about vast amounts of\nconversation logs and increasing demand for dialogue generation. The dialogue's\nlife-cycle spans from $\\textit{Prelude}$ through $\\textit{Interlocution}$ to\n$\\textit{Epilogue}$, encompassing rich dialogue elements. Despite large volumes\nof dialogue-related studies, there is a lack of systematic investigation into\nthe dialogue stages to frame benchmark construction that covers comprehensive\ndialogue elements. This hinders the precise modeling, generation and assessment\nof LLMs-based dialogue systems. To bridge this gap, in this paper, we introduce\na new research task--$\\textbf{D}$ialogue $\\textbf{E}$lement\n$\\textbf{MO}$deling, including $\\textit{Element Awareness}$ and\n$\\textit{Dialogue Agent Interaction}$, and propose a novel benchmark,\n$\\textbf{DEMO}$, designed for a comprehensive dialogue modeling and assessment.\nOn this basis, we further build the DEMO agent with the adept ability to model\ndialogue elements via imitation learning. Extensive experiments on DEMO\nindicate that current representative LLMs still have considerable potential for\nenhancement, and our DEMO agent performs well in both dialogue element modeling\nand out-of-domain tasks."}
{"id": "2412.00686", "pdf": "https://arxiv.org/pdf/2412.00686", "abs": "https://arxiv.org/abs/2412.00686", "authors": ["Muhammad Fetrat Qharabagh", "Mohammadreza Ghofrani", "Kimon Fountoulakis"], "title": "LVLM-COUNT: Enhancing the Counting Ability of Large Vision-Language Models", "categories": ["cs.CV", "cs.AI"], "comment": "38 pages, 24 Figures, 19 Tables", "summary": "Counting is a fundamental operation for various real-world visual tasks,\nrequiring both object recognition and robust counting capabilities. Despite\ntheir advanced visual perception, large vision-language models (LVLMs) are\nknown to struggle with counting tasks. In this work, we evaluate the\nperformance of several recent LVLMs on visual counting tasks across multiple\ncounting and vision datasets. We observe that while their performance may be\nless prone to error for small numbers of objects, they exhibit significant\nweaknesses as the number of objects increases. To alleviate this issue, we\npropose a simple yet effective baseline method that enhances LVLMs' counting\nability for large numbers of objects using a divide-and-conquer approach. Our\nmethod decomposes counting problems into sub-tasks. Moreover, it incorporates a\nmechanism to prevent objects from being split during division, which could\notherwise lead to repetitive counting -- a common issue in a naive\ndivide-and-conquer implementation. We demonstrate the effectiveness of this\napproach across various datasets and benchmarks, establishing it as a valuable\nreference for evaluating future solutions."}
{"id": "2407.00066", "pdf": "https://arxiv.org/pdf/2407.00066", "abs": "https://arxiv.org/abs/2407.00066", "authors": ["Rickard BrÃ¼el-Gabrielsson", "Jiacheng Zhu", "Onkar Bhardwaj", "Leshem Choshen", "Kristjan Greenewald", "Mikhail Yurochkin", "Justin Solomon"], "title": "Compress then Serve: Serving Thousands of LoRA Adapters with Little Overhead", "categories": ["cs.DC", "cs.AI", "cs.CL", "cs.LG"], "comment": null, "summary": "Fine-tuning large language models (LLMs) with low-rank adaptations (LoRAs)\nhas become common practice, often yielding numerous copies of the same LLM\ndiffering only in their LoRA updates. This paradigm presents challenges for\nsystems that serve real-time responses to queries that each involve a different\nLoRA. Prior works optimize the design of such systems but still require\ncontinuous loading and offloading of LoRAs, as it is infeasible to store\nthousands of LoRAs in GPU memory. To mitigate this issue, we investigate the\nefficacy of compression when serving LoRAs. We propose a method for the joint\ncompression of LoRAs into a shared basis paired with LoRA-specific scaling\nmatrices. We extend our algorithm to learn clusters of LoRAs that are amenable\nto joint compression, allowing it to scale gracefully to large LoRA\ncollections. Our experiments with up to 1000 LoRAs demonstrate that compressed\nLoRAs preserve performance while offering major throughput gains in realistic\nserving scenarios with over a thousand LoRAs, maintaining 80% of the throughput\nof serving a single LoRA."}
{"id": "2505.24852", "pdf": "https://arxiv.org/pdf/2505.24852", "abs": "https://arxiv.org/abs/2505.24852", "authors": ["Douwe den Blanken", "Charlotte Frenkel"], "title": "Chameleon: A MatMul-Free Temporal Convolutional Network Accelerator for End-to-End Few-Shot and Continual Learning from Sequential Data", "categories": ["cs.AR", "cs.LG", "C.3; B.6.0; B.7.0; I.2.6; B.5.0"], "comment": "14 pages, 7 figures", "summary": "On-device learning at the edge enables low-latency, private personalization\nwith improved long-term robustness and reduced maintenance costs. Yet,\nachieving scalable, low-power end-to-end on-chip learning, especially from\nreal-world sequential data with a limited number of examples, is an open\nchallenge. Indeed, accelerators supporting error backpropagation optimize for\nlearning performance at the expense of inference efficiency, while simplified\nlearning algorithms often fail to reach acceptable accuracy targets. In this\nwork, we present Chameleon, leveraging three key contributions to solve these\nchallenges. (i) A unified learning and inference architecture supports few-shot\nlearning (FSL), continual learning (CL) and inference at only 0.5% area\noverhead to the inference logic. (ii) Long temporal dependencies are\nefficiently captured with temporal convolutional networks (TCNs), enabling the\nfirst demonstration of end-to-end on-chip FSL and CL on sequential data and\ninference on 16-kHz raw audio. (iii) A dual-mode, matrix-multiplication-free\ncompute array allows either matching the power consumption of state-of-the-art\ninference-only keyword spotting (KWS) accelerators or enabling $4.3\\times$\nhigher peak GOPS. Fabricated in 40-nm CMOS, Chameleon sets new accuracy records\non Omniglot for end-to-end on-chip FSL (96.8%, 5-way 1-shot, 98.8%, 5-way\n5-shot) and CL (82.2% final accuracy for learning 250 classes with 10 shots),\nwhile maintaining an inference accuracy of 93.3% on the 12-class Google Speech\nCommands dataset at an extreme-edge power budget of 3.1 $\\mu$W."}
{"id": "2412.08473", "pdf": "https://arxiv.org/pdf/2412.08473", "abs": "https://arxiv.org/abs/2412.08473", "authors": ["Huiyuan Lai", "Esther Ploeger", "Rik van Noord", "Antonio Toral"], "title": "Multi-perspective Alignment for Increasing Naturalness in Neural Machine Translation", "categories": ["cs.CL"], "comment": "Accepted to ACL 2025 main; 9 pages", "summary": "Neural machine translation (NMT) systems amplify lexical biases present in\ntheir training data, leading to artificially impoverished language in output\ntranslations. These language-level characteristics render automatic\ntranslations different from text originally written in a language and human\ntranslations, which hinders their usefulness in for example creating evaluation\ndatasets. Attempts to increase naturalness in NMT can fall short in terms of\ncontent preservation, where increased lexical diversity comes at the cost of\ntranslation accuracy. Inspired by the reinforcement learning from human\nfeedback framework, we introduce a novel method that rewards both naturalness\nand content preservation. We experiment with multiple perspectives to produce\nmore natural translations, aiming at reducing machine and human translationese.\nWe evaluate our method on English-to-Dutch literary translation, and find that\nour best model produces translations that are lexically richer and exhibit more\nproperties of human-written language, without loss in translation accuracy."}
{"id": "2412.01506", "pdf": "https://arxiv.org/pdf/2412.01506", "abs": "https://arxiv.org/abs/2412.01506", "authors": ["Jianfeng Xiang", "Zelong Lv", "Sicheng Xu", "Yu Deng", "Ruicheng Wang", "Bowen Zhang", "Dong Chen", "Xin Tong", "Jiaolong Yang"], "title": "Structured 3D Latents for Scalable and Versatile 3D Generation", "categories": ["cs.CV"], "comment": "Project Page: https://github.com/Microsoft/TRELLIS", "summary": "We introduce a novel 3D generation method for versatile and high-quality 3D\nasset creation. The cornerstone is a unified Structured LATent (SLAT)\nrepresentation which allows decoding to different output formats, such as\nRadiance Fields, 3D Gaussians, and meshes. This is achieved by integrating a\nsparsely-populated 3D grid with dense multiview visual features extracted from\na powerful vision foundation model, comprehensively capturing both structural\n(geometry) and textural (appearance) information while maintaining flexibility\nduring decoding. We employ rectified flow transformers tailored for SLAT as our\n3D generation models and train models with up to 2 billion parameters on a\nlarge 3D asset dataset of 500K diverse objects. Our model generates\nhigh-quality results with text or image conditions, significantly surpassing\nexisting methods, including recent ones at similar scales. We showcase flexible\noutput format selection and local 3D editing capabilities which were not\noffered by previous models. Code, model, and data will be released."}
{"id": "2407.15421", "pdf": "https://arxiv.org/pdf/2407.15421", "abs": "https://arxiv.org/abs/2407.15421", "authors": ["Mohammad Taufeeque", "Philip Quirke", "Maximilian Li", "Chris Cundy", "Aaron David Tucker", "Adam Gleave", "AdriÃ  Garriga-Alonso"], "title": "Planning in a recurrent neural network that plays Sokoban", "categories": ["cs.LG", "cs.AI"], "comment": "Mechanistic Interpretability workshop, ICML 2024", "summary": "Planning is essential for solving complex tasks, yet the internal mechanisms\nunderlying planning in neural networks remain poorly understood. Building on\nprior work, we analyze a recurrent neural network (RNN) trained on Sokoban, a\nchallenging puzzle requiring sequential, irreversible decisions. We find that\nthe RNN has a causal plan representation which predicts its future actions\nabout 50 steps in advance. The quality and length of the represented plan\nincreases over the first few steps. We uncover a surprising behavior: the RNN\n\"paces\" in cycles to give itself extra computation at the start of a level, and\nshow that this behavior is incentivized by training. Leveraging these insights,\nwe extend the trained RNN to significantly larger, out-of-distribution Sokoban\npuzzles, demonstrating robust representations beyond the training regime. We\nopen-source our model and code, and believe the neural network's interesting\nbehavior makes it an excellent model organism to deepen our understanding of\nlearned planning."}
{"id": "2209.06932", "pdf": "https://arxiv.org/pdf/2209.06932", "abs": "https://arxiv.org/abs/2209.06932", "authors": ["A. C. N. de Oliveira", "D. R. Figueiredo"], "title": "Optimizing Connectivity through Network Gradients for Restricted Boltzmann Machines", "categories": ["cs.LG"], "comment": null, "summary": "Leveraging sparse networks to connect successive layers in deep neural\nnetworks has recently been shown to provide benefits to large-scale\nstate-of-the-art models. However, network connectivity also plays a significant\nrole in the learning performance of shallow networks, such as the classic\nRestricted Boltzmann Machine (RBM). Efficiently finding sparse connectivity\npatterns that improve the learning performance of shallow networks is a\nfundamental problem. While recent principled approaches explicitly include\nnetwork connections as model parameters that must be optimized, they often rely\non explicit penalization or network sparsity as a hyperparameter. This work\npresents the Network Connectivity Gradients (NCG), an optimization method to\nfind optimal connectivity patterns for RBMs. NCG leverages the idea of network\ngradients: given a specific connection pattern, it determines the gradient of\nevery possible connection and uses the gradient to drive a continuous\nconnection strength parameter that in turn is used to determine the connection\npattern. Thus, learning RBM parameters and learning network connections is\ntruly jointly performed, albeit with different learning rates, and without\nchanges to the model's classic energy-based objective function. The proposed\nmethod is applied to the MNIST and other data sets showing that better RBM\nmodels are found for the benchmark tasks of sample generation and\nclassification. Results also show that NCG is robust to network initialization\nand is capable of both adding and removing network connections while learning."}
{"id": "2412.08972", "pdf": "https://arxiv.org/pdf/2412.08972", "abs": "https://arxiv.org/abs/2412.08972", "authors": ["Ruiwen Zhou", "Wenyue Hua", "Liangming Pan", "Sitao Cheng", "Xiaobao Wu", "En Yu", "William Yang Wang"], "title": "RuleArena: A Benchmark for Rule-Guided Reasoning with LLMs in Real-World Scenarios", "categories": ["cs.CL", "cs.AI"], "comment": "ACL 2025 Main Conference", "summary": "This paper introduces RuleArena, a novel and challenging benchmark designed\nto evaluate the ability of large language models (LLMs) to follow complex,\nreal-world rules in reasoning. Covering three practical domains -- airline\nbaggage fees, NBA transactions, and tax regulations -- RuleArena assesses LLMs'\nproficiency in handling intricate natural language instructions that demand\nlong-context understanding, logical reasoning, and accurate mathematical\ncomputation. Two key attributes distinguish RuleArena from traditional\nrule-based reasoning benchmarks: (1) it extends beyond standard first-order\nlogic representations, and (2) it is grounded in authentic, practical\nscenarios, providing insights into the suitability and reliability of LLMs for\nreal-world applications. Our findings reveal several notable limitations in\nLLMs: (1) they struggle to identify and apply the appropriate rules, frequently\nbecoming confused by similar but distinct regulations, (2) they cannot\nconsistently perform accurate mathematical computations, even when they\ncorrectly identify the relevant rules, and (3) in general, they perform poorly\nin the benchmark. We also observe a significant performance boost when LLMs are\nprovided with external tools for oracle math and logic operations. These\nresults highlight significant challenges and promising research directions in\nadvancing LLMs' rule-guided reasoning capabilities in real-life applications.\nOur codes and data are publicly available on\nhttps://github.com/skyriver-2000/RuleArena."}
{"id": "2412.06014", "pdf": "https://arxiv.org/pdf/2412.06014", "abs": "https://arxiv.org/abs/2412.06014", "authors": ["Anton Baumann", "Rui Li", "Marcus Klasson", "Santeri Mentu", "Shyamgopal Karthik", "Zeynep Akata", "Arno Solin", "Martin Trapp"], "title": "Post-hoc Probabilistic Vision-Language Models", "categories": ["cs.CV", "cs.LG"], "comment": "Project page: https://aaltoml.github.io/BayesVLM/", "summary": "Vision-language models (VLMs), such as CLIP and SigLIP, have found remarkable\nsuccess in classification, retrieval, and generative tasks. For this, VLMs\ndeterministically map images and text descriptions to a joint latent space in\nwhich their similarity is assessed using the cosine similarity. However, a\ndeterministic mapping of inputs fails to capture uncertainties over concepts\narising from domain shifts when used in downstream tasks. In this work, we\npropose post-hoc uncertainty estimation in VLMs that does not require\nadditional training. Our method leverages a Bayesian posterior approximation\nover the last layers in VLMs and analytically quantifies uncertainties over\ncosine similarities. We demonstrate its effectiveness for uncertainty\nquantification and support set selection in active learning. Compared to\nbaselines, we obtain improved and well-calibrated predictive uncertainties,\ninterpretable uncertainty estimates, and sample-efficient active learning. Our\nresults show promise for safety-critical applications of large-scale models."}
{"id": "2407.17963", "pdf": "https://arxiv.org/pdf/2407.17963", "abs": "https://arxiv.org/abs/2407.17963", "authors": ["Xingcheng Xu", "Zibo Zhao", "Haipeng Zhang", "Yanqing Yang"], "title": "Principled Understanding of Generalization for Generative Transformer Models in Arithmetic Reasoning Tasks", "categories": ["cs.LG", "cs.AI"], "comment": "Accepted by the 63rd Annual Meeting of the Association for\n  Computational Linguistics (ACL 2025), Main Conference", "summary": "Transformer-based models excel in various tasks but their generalization\ncapabilities, especially in arithmetic reasoning, remain incompletely\nunderstood. Arithmetic tasks provide a controlled framework to explore these\ncapabilities, yet performance anomalies persist, such as inconsistent\neffectiveness in multiplication and erratic generalization in modular addition\n(e.g., modulo 100 vs. 101). This paper develops a unified theoretical framework\nfor understanding the generalization behaviors of transformers in arithmetic\ntasks, focusing on length generalization. Through detailed analysis of\naddition, multiplication, and modular operations, we reveal that translation\ninvariance in addition aligns with relative positional encoding for robust\ngeneralization, while base mismatch in modular operations disrupts this\nalignment. Experiments across GPT-family models validate our framework,\nconfirming its ability to predict generalization behaviors. Our work highlights\nthe importance of task structure and training data distribution for achieving\ndata-efficient and structure-aware training, providing a systematic approach to\nunderstanding of length generalization in transformers."}
{"id": "2303.14537", "pdf": "https://arxiv.org/pdf/2303.14537", "abs": "https://arxiv.org/abs/2303.14537", "authors": ["Rickard BrÃ¼el-Gabrielsson", "Tongzhou Wang", "Manel Baradad", "Justin Solomon"], "title": "Deep Augmentation: Dropout as Augmentation for Self-Supervised Learning", "categories": ["cs.LG", "cs.CL", "cs.CV"], "comment": null, "summary": "Despite dropout's ubiquity in machine learning, its effectiveness as a form\nof data augmentation remains under-explored. We address two key questions: (i)\nWhen is dropout effective as an augmentation strategy? (ii) Is dropout uniquely\neffective under these conditions? To explore these questions, we propose Deep\nAugmentation, a network- and modality-agnostic method that applies dropout or\nPCA transformations to targeted layers in neural networks. Through extensive\nexperiments on contrastive learning tasks in NLP, computer vision, and graph\nlearning, we find that uniformly applying dropout across layers does not\nconsistently improve performance. Instead, dropout proves most beneficial in\ndeeper layers and can be matched by alternative augmentations (e.g., PCA). We\nalso show that a stop-gradient operation is critical for ensuring dropout\nfunctions effectively as an augmentation, and that performance trends invert\nwhen moving from contrastive tasks to supervised tasks. Our analysis suggests\nthat Deep Augmentation helps mitigate inter-layer co-adaptation -- a notable\nissue in self-supervised learning due to the absence of labeled data. Drawing\non these insights, we outline a procedure for selecting the optimal\naugmentation layer and demonstrate that Deep Augmentation can outperform\ntraditional input-level augmentations. This simple yet powerful approach can be\nseamlessly integrated into a wide range of architectures and modalities,\nyielding notable gains in both performance and generalization."}
{"id": "2412.12567", "pdf": "https://arxiv.org/pdf/2412.12567", "abs": "https://arxiv.org/abs/2412.12567", "authors": ["Seunghee Kim", "Changhyeon Kim", "Taeuk Kim"], "title": "FCMR: Robust Evaluation of Financial Cross-Modal Multi-Hop Reasoning", "categories": ["cs.CL"], "comment": "ACL 2025", "summary": "Real-world decision-making often requires integrating and reasoning over\ninformation from multiple modalities. While recent multimodal large language\nmodels (MLLMs) have shown promise in such tasks, their ability to perform\nmulti-hop reasoning across diverse sources remains insufficiently evaluated.\nExisting benchmarks, such as MMQA, face challenges due to (1) data\ncontamination and (2) a lack of complex queries that necessitate operations\nacross more than two modalities, hindering accurate performance assessment. To\naddress this, we present Financial Cross-Modal Multi-Hop Reasoning (FCMR), a\nbenchmark created to analyze the reasoning capabilities of MLLMs by urging them\nto combine information from textual reports, tables, and charts within the\nfinancial domain. FCMR is categorized into three difficulty levels-Easy,\nMedium, and Hard-facilitating a step-by-step evaluation. In particular,\nproblems at the Hard level require precise cross-modal three-hop reasoning and\nare designed to prevent the disregard of any modality. Experiments on this new\nbenchmark reveal that even state-of-the-art MLLMs struggle, with the\nbest-performing model (Claude 3.5 Sonnet) achieving only 30.4% accuracy on the\nmost challenging tier. We also conduct analysis to provide insights into the\ninner workings of the models, including the discovery of a critical bottleneck\nin the information retrieval phase."}
{"id": "2412.08922", "pdf": "https://arxiv.org/pdf/2412.08922", "abs": "https://arxiv.org/abs/2412.08922", "authors": ["Liyang He", "Yuren Zhang", "Rui Li", "Zhenya Huang", "Runze Wu", "Enhong Chen"], "title": "Nested Hash Layer: A Plug-and-play Module for Multiple-length Hash Code Learning", "categories": ["cs.CV", "cs.IR"], "comment": null, "summary": "Deep supervised hashing is essential for efficient storage and search in\nlarge-scale image retrieval. Traditional deep supervised hashing models\ngenerate single-length hash codes, but this creates a trade-off between\nefficiency and effectiveness for different code lengths. To find the optimal\nlength for a task, multiple models must be trained, increasing time and\ncomputation. Furthermore, relationships between hash codes of different lengths\nare often ignored. To address these issues, we propose the Nested Hash Layer\n(NHL), a plug-and-play module for deep supervised hashing models. NHL generates\nhash codes of multiple lengths simultaneously in a nested structure. To resolve\noptimization conflicts from multiple learning objectives, we introduce a\ndominance-aware dynamic weighting strategy to adjust gradients. Additionally,\nwe propose a long-short cascade self-distillation method, where long hash codes\nguide the learning of shorter ones, improving overall code quality. Experiments\nindicate that the NHL achieves an overall training speed improvement of\napproximately 5 to 8 times across various deep supervised hashing models and\nenhances the average performance of these models by about 3.4%."}
{"id": "2408.08713", "pdf": "https://arxiv.org/pdf/2408.08713", "abs": "https://arxiv.org/abs/2408.08713", "authors": ["Yunxiao Shi", "Wujiang Xu", "Haimin Zhang", "Qiang Wu", "Min Xu"], "title": "Beyond KAN: Introducing KarSein for Adaptive High-Order Feature Interaction Modeling in CTR Prediction", "categories": ["cs.LG", "cs.AI", "cs.IR"], "comment": "draft paper", "summary": "Modeling high-order feature interactions is crucial for click-through rate\n(CTR) prediction, and traditional approaches often predefine a maximum\ninteraction order and rely on exhaustive enumeration of feature combinations up\nto this predefined order. This framework heavily relies on prior domain\nknowledge to define interaction scope and entails high computational costs from\nenumeration. Conventional CTR models face a trade-off between improving\nrepresentation through complex high-order feature interactions and reducing\ncomputational inefficiencies associated with these processes. To address this\ndual challenge, this study introduces the Kolmogorov-Arnold Represented Sparse\nEfficient Interaction Network (KarSein). Drawing inspiration from the learnable\nactivation mechanism in the Kolmogorov-Arnold Network (KAN), KarSein leverages\nthis mechanism to adaptively transform low-order basic features into high-order\nfeature interactions, offering a novel approach to feature interaction\nmodeling. KarSein extends the capabilities of KAN by introducing a more\nefficient architecture that significantly reduces computational costs while\naccommodating two-dimensional embedding vectors as feature inputs. Furthermore,\nit overcomes the limitation of KAN's its inability to spontaneously capture\nmultiplicative relationships among features.\n  Extensive experiments highlight the superiority of KarSein, demonstrating its\nability to surpass not only the vanilla implementation of KAN in CTR predictio\nbut also other baseline methods. Remarkably, KarSein achieves exceptional\npredictive accuracy while maintaining a highly compact parameter size and\nminimal computational overhead. As the first attempt to apply KAN in the CTR\ndomain, this work introduces KarSein as a novel solution for modeling complex\nfeature interactions, underscoring its transformative potential in advancing\nCTR prediction task."}
{"id": "2310.00098", "pdf": "https://arxiv.org/pdf/2310.00098", "abs": "https://arxiv.org/abs/2310.00098", "authors": ["Martin Pelikan", "Sheikh Shams Azam", "Vitaly Feldman", "Jan \"Honza\" Silovsky", "Kunal Talwar", "Christopher G. Brinton", "Tatiana Likhomanenko"], "title": "Enabling Differentially Private Federated Learning for Speech Recognition: Benchmarks, Adaptive Optimizers and Gradient Clipping", "categories": ["cs.LG", "cs.CR", "stat.ML"], "comment": "Under review", "summary": "While federated learning (FL) and differential privacy (DP) have been\nextensively studied, their application to automatic speech recognition (ASR)\nremains largely unexplored due to the challenges in training large transformer\nmodels. Specifically, large models further exacerbate issues in FL as they are\nparticularly susceptible to gradient heterogeneity across layers, unlike the\nrelatively uniform gradient behavior observed in shallow models. As a result,\nprior works struggle to converge with standard optimization techniques, even in\nthe absence of DP mechanisms. To the best of our knowledge, no existing work\nestablishes a competitive, practical recipe for FL with DP in the context of\nASR. To address this gap, we establish \\textbf{the first benchmark for FL with\nDP in end-to-end ASR}. Our approach centers on per-layer clipping and\nlayer-wise gradient normalization: theoretical analysis reveals that these\ntechniques together mitigate clipping bias and gradient heterogeneity across\nlayers in deeper models. Consistent with these theoretical insights, our\nempirical results show that FL with DP is viable under strong privacy\nguarantees, provided a population of at least several million users.\nSpecifically, we achieve user-level (7.2, $10^{-9}$)-DP (resp. (4.5,\n$10^{-9}$)-DP) with only a 1.3% (resp. 4.6%) absolute drop in word error rate\nwhen extrapolating to high (resp. low) population scales for FL with DP in ASR.\nAlthough our experiments focus on ASR, the underlying principles we uncover -\nparticularly those concerning gradient heterogeneity and layer-wise gradient\nnormalization - offer broader guidance for designing scalable,\nprivacy-preserving FL algorithms for large models across domains."}
{"id": "2412.13942", "pdf": "https://arxiv.org/pdf/2412.13942", "abs": "https://arxiv.org/abs/2412.13942", "authors": ["Beiduo Chen", "Siyao Peng", "Anna Korhonen", "Barbara Plank"], "title": "A Rose by Any Other Name: LLM-Generated Explanations Are Good Proxies for Human Explanations to Collect Label Distributions on NLI", "categories": ["cs.CL"], "comment": "Accepted by ACL 2025 Findings, 25 pages, 21 figures", "summary": "Disagreement in human labeling is ubiquitous, and can be captured in human\njudgment distributions (HJDs). Recent research has shown that explanations\nprovide valuable information for understanding human label variation (HLV) and\nlarge language models (LLMs) can approximate HJD from a few human-provided\nlabel-explanation pairs. However, collecting explanations for every label is\nstill time-consuming. This paper examines whether LLMs can be used to replace\nhumans in generating explanations for approximating HJD. Specifically, we use\nLLMs as annotators to generate model explanations for a few given human labels.\nWe test ways to obtain and combine these label-explanations with the goal to\napproximate human judgment distributions. We further compare the resulting\nhuman with model-generated explanations, and test automatic and human\nexplanation selection. Our experiments show that LLM explanations are promising\nfor NLI: to estimate HJDs, generated explanations yield comparable results to\nhuman's when provided with human labels. Importantly, our results generalize\nfrom datasets with human explanations to i) datasets where they are not\navailable and ii) challenging out-of-distribution test sets."}
{"id": "2412.14465", "pdf": "https://arxiv.org/pdf/2412.14465", "abs": "https://arxiv.org/abs/2412.14465", "authors": ["Wengyi Zhan", "Mingbao Lin", "Shuicheng Yan", "Rongrong Ji"], "title": "DiffusionTrend: A Minimalist Approach to Virtual Fashion Try-On", "categories": ["cs.CV"], "comment": null, "summary": "We introduce DiffusionTrend for virtual fashion try-on, which forgoes the\nneed for retraining diffusion models. Using advanced diffusion models,\nDiffusionTrend harnesses latent information rich in prior information to\ncapture the nuances of garment details. Throughout the diffusion denoising\nprocess, these details are seamlessly integrated into the model image\ngeneration, expertly directed by a precise garment mask crafted by a\nlightweight and compact CNN. Although our DiffusionTrend model initially\ndemonstrates suboptimal metric performance, our exploratory approach offers\nsome important advantages: (1) It circumvents resource-intensive retraining of\ndiffusion models on large datasets. (2) It eliminates the necessity for various\ncomplex and user-unfriendly model inputs. (3) It delivers a visually compelling\ntry-on experience, underscoring the potential of training-free diffusion model.\nThis initial foray into the application of untrained diffusion models in\nvirtual try-on technology potentially paves the way for further exploration and\nrefinement in this industrially and academically valuable field."}
{"id": "2408.13467", "pdf": "https://arxiv.org/pdf/2408.13467", "abs": "https://arxiv.org/abs/2408.13467", "authors": ["Chansung Park", "Juyong Jiang", "Fan Wang", "Sayak Paul", "Jing Tang"], "title": "LlamaDuo: LLMOps Pipeline for Seamless Migration from Service LLMs to Small-Scale Local LLMs", "categories": ["cs.LG", "cs.AI", "cs.DC"], "comment": "The first three authors contributed equally to this work; Accepted by\n  ACL 2025 (Main)", "summary": "The widespread adoption of cloud-based proprietary large language models\n(LLMs) has introduced significant challenges, including operational\ndependencies, privacy concerns, and the necessity of continuous internet\nconnectivity. In this work, we introduce an LLMOps pipeline, \"LlamaDuo\", for\nthe seamless migration of knowledge and abilities from service-oriented LLMs to\nsmaller, locally manageable models. This pipeline is crucial for ensuring\nservice continuity in the presence of operational failures, strict privacy\npolicies, or offline requirements. Our LlamaDuo involves fine-tuning a small\nlanguage model against the service LLM using a synthetic dataset generated by\nthe latter. If the performance of the fine-tuned model falls short of\nexpectations, it is automatically improved through additional fine-tuning using\nextra similar data generated by the service LLM. This multi-turn process\nguarantees that the smaller model can eventually match or even surpass the\nservice LLM's capabilities in specific downstream tasks, offering a practical\nand scalable solution for managing AI deployments in constrained environments.\nExtensive experiments with leading-edge LLMs are conducted to demonstrate the\neffectiveness, adaptability, and affordability of LlamaDuo across various\ndownstream tasks. Our pipeline implementation is available at\nhttps://github.com/deep-diver/llamaduo."}
{"id": "2401.04139", "pdf": "https://arxiv.org/pdf/2401.04139", "abs": "https://arxiv.org/abs/2401.04139", "authors": ["Hanbeot Park", "Yunjeong Cho", "Hoon-Hee Kim"], "title": "CCNETS: A Modular Causal Learning Framework for Pattern Recognition in Imbalanced Datasets", "categories": ["cs.LG"], "comment": "52 pages, Hoon-Hee Kim is Corresponding Author", "summary": "Handling class imbalance remains a central challenge in machine learning,\nparticularly in pattern recognition tasks where rare but critical events-such\nas fraudulent transactions or medical anomalies-must be identified accurately.\nTraditional generative models offer a potential remedy through data\naugmentation but often treat generation and classification as independent\nprocesses, leading to distribution mismatch and limited classifier benefit. To\naddress these shortcomings, we propose Causal Cooperative Networks (CCNETS), a\nmodular learning framework that integrates generation, inference, and\nreconstruction within a unified causal paradigm. CCNETS comprises three\ncooperative modules: an Explainer for latent feature abstraction, a Reasoner\nfor label prediction, and a Producer for context-aware data generation. These\ncomponents interact through a causal feedback loop, where classification\nresults guide targeted sample synthesis. A key innovation, the Zoint mechanism,\nenables adaptive fusion of latent and observable features, enhancing semantic\nrichness and enabling robust decision-making under uncertainty. We evaluate\nCCNETS on a real-world credit card fraud detection dataset with extreme\nimbalance (fraud cases < 0.2%). Across three experimental setups-including\nsynthetic training, amplified generation, and direct classifier\ncomparison-CCNETS outperforms baseline methods, achieving higher F1 scores,\nprecision, and recall. Models trained on CCNETS-generated data also demonstrate\nsuperior generalization under limited data conditions. These results establish\nCCNETS as a scalable, interpretable, and hybrid soft computing framework. By\ncausally aligning synthetic data with classifier objectives, CCNETS advances\nimbalanced pattern recognition and opens new directions for robust, modular\nlearning in real-world applications."}
{"id": "2412.15268", "pdf": "https://arxiv.org/pdf/2412.15268", "abs": "https://arxiv.org/abs/2412.15268", "authors": ["Yibo Zhao", "Jiapeng Zhu", "Can Xu", "Yao Liu", "Xiang Li"], "title": "Enhancing LLM-based Hatred and Toxicity Detection with Meta-Toxic Knowledge Graph", "categories": ["cs.CL", "cs.AI"], "comment": "8 pages of content", "summary": "The rapid growth of social media platforms has raised significant concerns\nregarding online content toxicity. When Large Language Models (LLMs) are used\nfor toxicity detection, two key challenges emerge: 1) the absence of\ndomain-specific toxic knowledge leads to false negatives; 2) the excessive\nsensitivity of LLMs to toxic speech results in false positives, limiting\nfreedom of speech. To address these issues, we propose a novel method called\nMetaTox, leveraging graph search on a meta-toxic knowledge graph to enhance\nhatred and toxicity detection. First, we construct a comprehensive meta-toxic\nknowledge graph by utilizing LLMs to extract toxic information through a\nthree-step pipeline, with toxic benchmark datasets serving as corpora. Second,\nwe query the graph via retrieval and ranking processes to supplement accurate,\nrelevant toxic knowledge. Extensive experiments and in-depth case studies\nacross multiple datasets demonstrate that our MetaTox significantly decreases\nthe false positive rate while boosting overall toxicity detection performance.\nOur code is available at https://github.com/YiboZhao624/MetaTox."}
{"id": "2412.20662", "pdf": "https://arxiv.org/pdf/2412.20662", "abs": "https://arxiv.org/abs/2412.20662", "authors": ["Yitong Zhou", "Mingyue Cheng", "Qingyang Mao", "Feiyang Xu", "Xin Li"], "title": "Enhancing Table Recognition with Vision LLMs: A Benchmark and Neighbor-Guided Toolchain Reasoner", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Pre-trained foundation models have recently made significant progress in\ntable-related tasks such as table understanding and reasoning. However,\nrecognizing the structure and content of unstructured tables using Vision Large\nLanguage Models (VLLMs) remains under-explored. To bridge this gap, we propose\na benchmark based on a hierarchical design philosophy to evaluate the\nrecognition capabilities of VLLMs in training-free scenarios. Through in-depth\nevaluations, we find that low-quality image input is a significant bottleneck\nin the recognition process. Drawing inspiration from this, we propose the\nNeighbor-Guided Toolchain Reasoner (NGTR) framework, which is characterized by\nintegrating diverse lightweight tools for visual operations aimed at mitigating\nissues with low-quality images. Specifically, we transfer a tool selection\nexperience from a similar neighbor to the input and design a reflection module\nto supervise the tool invocation process. Extensive experiments on public\ndatasets demonstrate that our approach significantly enhances the recognition\ncapabilities of the vanilla VLLMs. We believe that the benchmark and framework\ncould provide an alternative solution to table recognition."}
{"id": "2409.17275", "pdf": "https://arxiv.org/pdf/2409.17275", "abs": "https://arxiv.org/abs/2409.17275", "authors": ["Xun Xian", "Ganghua Wang", "Xuan Bi", "Jayanth Srinivasa", "Ashish Kundu", "Charles Fleming", "Mingyi Hong", "Jie Ding"], "title": "On the Vulnerability of Applying Retrieval-Augmented Generation within Knowledge-Intensive Application Domains", "categories": ["cs.CR", "cs.AI", "cs.CL", "cs.DB", "cs.ET", "cs.IR", "cs.LG", "68T50, 68T05, 94A60", "I.2.7; K.6.5; H.3.3; I.2.6"], "comment": "Accepted by ICML 2025", "summary": "Retrieval-Augmented Generation (RAG) has been empirically shown to enhance\nthe performance of large language models (LLMs) in knowledge-intensive domains\nsuch as healthcare, finance, and legal contexts. Given a query, RAG retrieves\nrelevant documents from a corpus and integrates them into the LLMs' generation\nprocess. In this study, we investigate the adversarial robustness of RAG,\nfocusing specifically on examining the retrieval system. First, across 225\ndifferent setup combinations of corpus, retriever, query, and targeted\ninformation, we show that retrieval systems are vulnerable to universal\npoisoning attacks in medical Q\\&A. In such attacks, adversaries generate\npoisoned documents containing a broad spectrum of targeted information, such as\npersonally identifiable information. When these poisoned documents are inserted\ninto a corpus, they can be accurately retrieved by any users, as long as\nattacker-specified queries are used. To understand this vulnerability, we\ndiscovered that the deviation from the query's embedding to that of the\npoisoned document tends to follow a pattern in which the high similarity\nbetween the poisoned document and the query is retained, thereby enabling\nprecise retrieval. Based on these findings, we develop a new detection-based\ndefense to ensure the safe use of RAG. Through extensive experiments spanning\nvarious Q\\&A domains, we observed that our proposed method consistently\nachieves excellent detection rates in nearly all cases."}
{"id": "2402.03979", "pdf": "https://arxiv.org/pdf/2402.03979", "abs": "https://arxiv.org/abs/2402.03979", "authors": ["Li Guo", "George Andriopoulos", "Zifan Zhao", "Shuyang Ling", "Zixuan Dong", "Keith Ross"], "title": "Cross Entropy versus Label Smoothing: A Neural Collapse Perspective", "categories": ["cs.LG"], "comment": null, "summary": "Label smoothing loss is a widely adopted technique to mitigate overfitting in\ndeep neural networks. This paper studies label smoothing from the perspective\nof Neural Collapse (NC), a powerful empirical and theoretical framework which\ncharacterizes model behavior during the terminal phase of training. We first\nshow empirically that models trained with label smoothing converge faster to\nneural collapse solutions and attain a stronger level of neural collapse.\nAdditionally, we show that at the same level of NC1, models under label\nsmoothing loss exhibit intensified NC2. These findings provide valuable\ninsights into the performance benefits and enhanced model calibration under\nlabel smoothing loss. We then leverage the unconstrained feature model to\nderive closed-form solutions for the global minimizers for both loss functions\nand further demonstrate that models under label smoothing have a lower\nconditioning number and, therefore, theoretically converge faster. Our study,\ncombining empirical evidence and theoretical results, not only provides nuanced\ninsights into the differences between label smoothing and cross-entropy losses,\nbut also serves as an example of how the powerful neural collapse framework can\nbe used to improve our understanding of DNNs."}
{"id": "2412.15712", "pdf": "https://arxiv.org/pdf/2412.15712", "abs": "https://arxiv.org/abs/2412.15712", "authors": ["Maike ZÃ¼fle", "Jan Niehues"], "title": "Contrastive Learning for Task-Independent SpeechLLM-Pretraining", "categories": ["cs.CL", "cs.HC"], "comment": null, "summary": "Large language models (LLMs) excel in natural language processing but\nadapting these LLMs to speech processing tasks efficiently is not\nstraightforward. Direct task-specific fine-tuning is limited by overfitting\nrisks, data requirements, and computational costs. To address these challenges,\nwe propose a scalable, two-stage training approach: (1) A task-independent\nspeech pretraining stage using contrastive learning to align text and speech\nrepresentations over all layers, followed by (2) a task-specific fine-tuning\nstage requiring minimal data. This approach outperforms traditional ASR\npretraining and enables the model to surpass models specialized on speech\ntranslation and question answering while being trained on only 10% of the\ntask-specific data."}
{"id": "2501.19036", "pdf": "https://arxiv.org/pdf/2501.19036", "abs": "https://arxiv.org/abs/2501.19036", "authors": ["Hongliang Li", "Jiaxin Zhang", "Wenhui Liao", "Dezhi Peng", "Kai Ding", "Lianwen Jin"], "title": "RedundancyLens: Revealing and Exploiting Visual Token Processing Redundancy for Efficient Decoder-Only MLLMs", "categories": ["cs.CV"], "comment": "ACL 2025 Findings", "summary": "Current Multimodal Large Language Model (MLLM) architectures face a critical\ntradeoff between performance and efficiency: decoder-only architectures achieve\nhigher performance but lower efficiency, while cross-attention-based\narchitectures offer greater efficiency but lower performance. The key\ndistinction lies in how visual tokens are processed. Decoder-only architectures\napply self-attention and FFN operations on visual tokens, while cross-attention\narchitectures skip these computations. To investigate whether redundancy exists\nin this computationally expensive process, we propose a training-free framework\nfor analyzing trained MLLMs. It consists of Probe-Activated Dynamic FFN and\nHollow Attention, which enable adjustable reductions in computations for visual\ntokens, as well as a Layer Ranking Algorithm that prioritizes layers for these\nreductions. Extensive experiments demonstrate substantial, structured, and\nclustered redundancy unique to decoder-only MLLMs, offering valuable insights\nfor future MLLM architecture design. Furthermore, by leveraging our reduction\nframework as a training-free inference acceleration approach, we achieve\nperformance comparable to or better than state-of-the-art methods while\nremaining compatible with them. Code will be publicly available at\nhttps://github.com/L-Hugh/RedundancyLens."}
{"id": "2410.00059", "pdf": "https://arxiv.org/pdf/2410.00059", "abs": "https://arxiv.org/abs/2410.00059", "authors": ["Chaohui Xu", "Qi Cui", "Jinxin Dong", "Weiyang He", "Chip-Hong Chang"], "title": "IDEA: An Inverse Domain Expert Adaptation Based Active DNN IP Protection Method", "categories": ["cs.CR", "cs.AI", "cs.CV", "cs.LG"], "comment": null, "summary": "Illegitimate reproduction, distribution and derivation of Deep Neural Network\n(DNN) models can inflict economic loss, reputation damage and even privacy\ninfringement. Passive DNN intellectual property (IP) protection methods such as\nwatermarking and fingerprinting attempt to prove the ownership upon IP\nviolation, but they are often too late to stop catastrophic damage of IP abuse\nand too feeble against strong adversaries. In this paper, we propose IDEA, an\nInverse Domain Expert Adaptation based proactive DNN IP protection method\nfeaturing active authorization and source traceability. IDEA generalizes active\nauthorization as an inverse problem of domain adaptation. The multi-adaptive\noptimization is solved by a mixture-of-experts model with one real and two fake\nexperts. The real expert re-optimizes the source model to correctly classify\ntest images with a unique model user key steganographically embedded. The fake\nexperts are trained to output random prediction on test images without or with\nincorrect user key embedded by minimizing their mutual information (MI) with\nthe real expert. The MoE model is knowledge distilled into a unified protected\nmodel to avoid leaking the expert model features by maximizing their MI with\nadditional multi-layer attention and contrastive representation loss\noptimization. IDEA not only prevents unauthorized users without the valid key\nto access the functional model, but also enable the model owner to validate the\ndeployed model and trace the source of IP infringement. We extensively evaluate\nIDEA on five datasets and four DNN models to demonstrate its effectiveness in\nauthorization control, culprit tracing success rate, and robustness against\nvarious attacks."}
{"id": "2403.00485", "pdf": "https://arxiv.org/pdf/2403.00485", "abs": "https://arxiv.org/abs/2403.00485", "authors": ["Jiaqi Han", "Jiacheng Cen", "Liming Wu", "Zongzhao Li", "Xiangzhe Kong", "Rui Jiao", "Ziyang Yu", "Tingyang Xu", "Fandi Wu", "Zihe Wang", "Hongteng Xu", "Zhewei Wei", "Deli Zhao", "Yang Liu", "Yu Rong", "Wenbing Huang"], "title": "A Survey of Geometric Graph Neural Networks: Data Structures, Models and Applications", "categories": ["cs.LG"], "comment": "The article has been accepted by Frontiers of Computer Science (FCS),\n  with the DOI: {10.1007/s11704-025-41426-w}", "summary": "Geometric graphs are a special kind of graph with geometric features, which\nare vital to model many scientific problems. Unlike generic graphs, geometric\ngraphs often exhibit physical symmetries of translations, rotations, and\nreflections, making them ineffectively processed by current Graph Neural\nNetworks (GNNs). To address this issue, researchers proposed a variety of\ngeometric GNNs equipped with invariant/equivariant properties to better\ncharacterize the geometry and topology of geometric graphs. Given the current\nprogress in this field, it is imperative to conduct a comprehensive survey of\ndata structures, models, and applications related to geometric GNNs. In this\npaper, based on the necessary but concise mathematical preliminaries, we\nformalize geometric graph as the data structure, on top of which we provide a\nunified view of existing models from the geometric message passing perspective.\nAdditionally, we summarize the applications as well as the related datasets to\nfacilitate later research for methodology development and experimental\nevaluation. We also discuss the challenges and future potential directions of\ngeometric GNNs at the end of this survey."}
{"id": "2501.03191", "pdf": "https://arxiv.org/pdf/2501.03191", "abs": "https://arxiv.org/abs/2501.03191", "authors": ["Aaron Gluck", "Katharina von der Wense", "Maria Leonor Pacheco"], "title": "CLIX: Cross-Lingual Explanations of Idiomatic Expressions", "categories": ["cs.CL"], "comment": "Accepted to Findings of ACL 2025", "summary": "Automated definition generation systems have been proposed to support\nvocabulary expansion for language learners. The main barrier to the success of\nthese systems is that learners often struggle to understand definitions due to\nthe presence of potentially unfamiliar words and grammar, particularly when\nnon-standard language is involved. To address these challenges, we propose\nCLIX, the task of Cross-Lingual explanations of Idiomatic eXpressions. We\nexplore the capabilities of current NLP models for this task, and observe that\nwhile it remains challenging, large language models show promise. Finally, we\nperform a detailed error analysis to highlight the key challenges that need to\nbe addressed before we can reliably incorporate these systems into educational\ntools."}
{"id": "2502.01522", "pdf": "https://arxiv.org/pdf/2502.01522", "abs": "https://arxiv.org/abs/2502.01522", "authors": ["Junhao Cheng", "Wei-Ting Chen", "Xi Lu", "Ming-Hsuan Yang"], "title": "Unpaired Deblurring via Decoupled Diffusion Model", "categories": ["cs.CV"], "comment": "We propose UID-Diff to integrate generative diffusion model into\n  unpaired deblurring tasks", "summary": "Generative diffusion models trained on large-scale datasets have achieved\nremarkable progress in image synthesis. In favor of their ability to supplement\nmissing details and generate aesthetically pleasing contents, recent works have\napplied them to image deblurring via training an adapter on blurry-sharp image\npairs to provide structural conditions for restoration. However, acquiring\nsubstantial amounts of realistic paired data is challenging and costly in\nreal-world scenarios. On the other hand, relying solely on synthetic data often\nresults in overfitting, leading to unsatisfactory performance when confronted\nwith unseen blur patterns. To tackle this issue, we propose UID-Diff, a\ngenerative-diffusion-based model designed to enhance deblurring performance on\nunknown domains by decoupling structural features and blur patterns through\njoint training on three specially designed tasks. We employ two Q-Formers as\nstructural features and blur patterns extractors separately. The features\nextracted by them will be used for the supervised deblurring task on synthetic\ndata and the unsupervised blur-transfer task by leveraging unpaired blurred\nimages from the target domain simultaneously. We further introduce a\nreconstruction task to make the structural features and blur patterns\ncomplementary. This blur-decoupled learning process enhances the generalization\ncapabilities of UID-Diff when encountering unknown blur patterns. Experiments\non real-world datasets demonstrate that UID-Diff outperforms existing\nstate-of-the-art methods in blur removal and structural preservation in various\nchallenging scenarios."}
{"id": "2410.00425", "pdf": "https://arxiv.org/pdf/2410.00425", "abs": "https://arxiv.org/abs/2410.00425", "authors": ["Stone Tao", "Fanbo Xiang", "Arth Shukla", "Yuzhe Qin", "Xander Hinrichsen", "Xiaodi Yuan", "Chen Bao", "Xinsong Lin", "Yulin Liu", "Tse-kai Chan", "Yuan Gao", "Xuanlin Li", "Tongzhou Mu", "Nan Xiao", "Arnav Gurha", "Viswesh Nagaswamy Rajesh", "Yong Woo Choi", "Yen-Ru Chen", "Zhiao Huang", "Roberto Calandra", "Rui Chen", "Shan Luo", "Hao Su"], "title": "ManiSkill3: GPU Parallelized Robotics Simulation and Rendering for Generalizable Embodied AI", "categories": ["cs.RO", "cs.AI"], "comment": "Project website: http://maniskill.ai/", "summary": "Simulation has enabled unprecedented compute-scalable approaches to robot\nlearning. However, many existing simulation frameworks typically support a\nnarrow range of scenes/tasks and lack features critical for scaling\ngeneralizable robotics and sim2real. We introduce and open source ManiSkill3,\nthe fastest state-visual GPU parallelized robotics simulator with contact-rich\nphysics targeting generalizable manipulation. ManiSkill3 supports GPU\nparallelization of many aspects including simulation+rendering, heterogeneous\nsimulation, pointclouds/voxels visual input, and more. Simulation with\nrendering on ManiSkill3 can run 10-1000x faster with 2-3x less GPU memory usage\nthan other platforms, achieving up to 30,000+ FPS in benchmarked environments\ndue to minimal python/pytorch overhead in the system, simulation on the GPU,\nand the use of the SAPIEN parallel rendering system. Tasks that used to take\nhours to train can now take minutes. We further provide the most comprehensive\nrange of GPU parallelized environments/tasks spanning 12 distinct domains\nincluding but not limited to mobile manipulation for tasks such as drawing,\nhumanoids, and dextrous manipulation in realistic scenes designed by artists or\nreal-world digital twins. In addition, millions of demonstration frames are\nprovided from motion planning, RL, and teleoperation. ManiSkill3 also provides\na comprehensive set of baselines that span popular RL and\nlearning-from-demonstrations algorithms."}
{"id": "2405.07543", "pdf": "https://arxiv.org/pdf/2405.07543", "abs": "https://arxiv.org/abs/2405.07543", "authors": ["Jia Hu", "Mingyue Lei", "Haoran Wang", "Zeyu Liu", "Fan Yang"], "title": "Accelerating the Evolution of Personalized Automated Lane Change through Lesson Learning", "categories": ["cs.LG", "cs.RO"], "comment": null, "summary": "Personalization is crucial for the widespread adoption of advanced driver\nassistance system. To match up with each user's preference, the online\nevolution capability is a must. However, conventional evolution methods learn\nfrom naturalistic driving data, which requires a lot computing power and cannot\nbe applied online. To address this challenge, this paper proposes a lesson\nlearning approach: learning from driver's takeover interventions. By leveraging\nonline takeover data, the driving zone is generated to ensure perceived safety\nusing Gaussian discriminant analysis. Real-time corrections to trajectory\nplanning rewards are enacted through apprenticeship learning. Guided by the\nobjective of optimizing rewards within the constraints of the driving zone,\nthis approach employs model predictive control for trajectory planning. This\nlesson learning framework is highlighted for its faster evolution capability,\nadeptness at experience accumulating, assurance of perceived safety, and\ncomputational efficiency. Simulation results demonstrate that the proposed\nsystem consistently achieves a successful customization without further\ntakeover interventions. Accumulated experience yields a 24% enhancement in\nevolution efficiency. The average number of learning iterations is only 13.8.\nThe average computation time is 0.08 seconds."}
{"id": "2501.03884", "pdf": "https://arxiv.org/pdf/2501.03884", "abs": "https://arxiv.org/abs/2501.03884", "authors": ["Aman Gupta", "Shao Tang", "Qingquan Song", "Sirou Zhu", "Jiwoo Hong", "Ankan Saha", "Viral Gupta", "Noah Lee", "Eunki Kim", "Siyu Zhu", "Parag Agrawal", "Natesh Pillai", "S. Sathiya Keerthi"], "title": "AlphaPO: Reward Shape Matters for LLM Alignment", "categories": ["cs.CL"], "comment": "26 pages, 16 figures. Accepted to ICML 2025", "summary": "Reinforcement Learning with Human Feedback (RLHF) and its variants have made\nhuge strides toward the effective alignment of large language models (LLMs) to\nfollow instructions and reflect human values. More recently, Direct Alignment\nAlgorithms (DAAs) have emerged in which the reward modeling stage of RLHF is\nskipped by characterizing the reward directly as a function of the policy being\nlearned. Some popular examples of DAAs include Direct Preference Optimization\n(DPO) and Simple Preference Optimization (SimPO). These methods often suffer\nfrom likelihood displacement, a phenomenon by which the probabilities of\npreferred responses are often reduced undesirably. In this paper, we argue\nthat, for DAAs the reward (function) shape matters. We introduce\n\\textbf{AlphaPO}, a new DAA method that leverages an $\\alpha$-parameter to help\nchange the shape of the reward function beyond the standard log reward. AlphaPO\nhelps maintain fine-grained control over likelihood displacement and\nover-optimization. Compared to SimPO, one of the best performing DAAs, AlphaPO\nleads to about 7\\% to 10\\% relative improvement in alignment performance for\nthe instruct versions of Mistral-7B and Llama3-8B while achieving 15\\% to 50\\%\nrelative improvement over DPO on the same models. The analysis and results\npresented highlight the importance of the reward shape and how one can\nsystematically change it to affect training dynamics, as well as improve\nalignment performance."}
{"id": "2502.03444", "pdf": "https://arxiv.org/pdf/2502.03444", "abs": "https://arxiv.org/abs/2502.03444", "authors": ["Hao Chen", "Yujin Han", "Fangyi Chen", "Xiang Li", "Yidong Wang", "Jindong Wang", "Ze Wang", "Zicheng Liu", "Difan Zou", "Bhiksha Raj"], "title": "Masked Autoencoders Are Effective Tokenizers for Diffusion Models", "categories": ["cs.CV", "cs.AI", "cs.LG"], "comment": null, "summary": "Recent advances in latent diffusion models have demonstrated their\neffectiveness for high-resolution image synthesis. However, the properties of\nthe latent space from tokenizer for better learning and generation of diffusion\nmodels remain under-explored. Theoretically and empirically, we find that\nimproved generation quality is closely tied to the latent distributions with\nbetter structure, such as the ones with fewer Gaussian Mixture modes and more\ndiscriminative features. Motivated by these insights, we propose MAETok, an\nautoencoder (AE) leveraging mask modeling to learn semantically rich latent\nspace while maintaining reconstruction fidelity. Extensive experiments validate\nour analysis, demonstrating that the variational form of autoencoders is not\nnecessary, and a discriminative latent space from AE alone enables\nstate-of-the-art performance on ImageNet generation using only 128 tokens.\nMAETok achieves significant practical improvements, enabling a gFID of 1.69\nwith 76x faster training and 31x higher inference throughput for 512x512\ngeneration. Our findings show that the structure of the latent space, rather\nthan variational constraints, is crucial for effective diffusion models. Code\nand trained models are released."}
{"id": "2410.02644", "pdf": "https://arxiv.org/pdf/2410.02644", "abs": "https://arxiv.org/abs/2410.02644", "authors": ["Hanrong Zhang", "Jingyuan Huang", "Kai Mei", "Yifei Yao", "Zhenting Wang", "Chenlu Zhan", "Hongwei Wang", "Yongfeng Zhang"], "title": "Agent Security Bench (ASB): Formalizing and Benchmarking Attacks and Defenses in LLM-based Agents", "categories": ["cs.CR", "cs.AI"], "comment": "Accepted by ICLR 2025", "summary": "Although LLM-based agents, powered by Large Language Models (LLMs), can use\nexternal tools and memory mechanisms to solve complex real-world tasks, they\nmay also introduce critical security vulnerabilities. However, the existing\nliterature does not comprehensively evaluate attacks and defenses against\nLLM-based agents. To address this, we introduce Agent Security Bench (ASB), a\ncomprehensive framework designed to formalize, benchmark, and evaluate the\nattacks and defenses of LLM-based agents, including 10 scenarios (e.g.,\ne-commerce, autonomous driving, finance), 10 agents targeting the scenarios,\nover 400 tools, 27 different types of attack/defense methods, and 7 evaluation\nmetrics. Based on ASB, we benchmark 10 prompt injection attacks, a memory\npoisoning attack, a novel Plan-of-Thought backdoor attack, 4 mixed attacks, and\n11 corresponding defenses across 13 LLM backbones. Our benchmark results reveal\ncritical vulnerabilities in different stages of agent operation, including\nsystem prompt, user prompt handling, tool usage, and memory retrieval, with the\nhighest average attack success rate of 84.30\\%, but limited effectiveness shown\nin current defenses, unveiling important works to be done in terms of agent\nsecurity for the community. We also introduce a new metric to evaluate the\nagents' capability to balance utility and security. Our code can be found at\nhttps://github.com/agiresearch/ASB."}
{"id": "2405.12372", "pdf": "https://arxiv.org/pdf/2405.12372", "abs": "https://arxiv.org/abs/2405.12372", "authors": ["Jonathan Vasquez", "Carlotta Domeniconi", "Huzefa Rangwala"], "title": "DispaRisk: Auditing Fairness Through Usable Information", "categories": ["cs.LG"], "comment": null, "summary": "Machine Learning algorithms (ML) impact virtually every aspect of human lives\nand have found use across diverse sectors including healthcare, finance, and\neducation. Often, ML algorithms have been found to exacerbate societal biases\npresent in datasets leading to adversarial impacts on subsets/groups of\nindividuals and in many cases on minority groups. To effectively mitigate these\nuntoward effects, it is crucial that disparities/biases are identified early in\na ML pipeline. This proactive approach facilitates timely interventions to\nprevent bias amplification and reduce complexity at later stages of model\ndevelopment. In this paper, we leverage recent advancements in usable\ninformation theory to introduce DispaRisk, a novel framework designed to\nproactively assess the potential risks of disparities in datasets during the\ninitial stages of the ML pipeline. We evaluate DispaRisk's effectiveness by\nbenchmarking it against commonly used datasets in fairness research. Our\nfindings demonstrate DispaRisk's capabilities to identify datasets with a high\nrisk of discrimination, detect model families prone to biases within an ML\npipeline, and enhance the explainability of these bias risks. This work\ncontributes to the development of fairer ML systems by providing a robust tool\nfor early bias detection and mitigation."}
{"id": "2501.13074", "pdf": "https://arxiv.org/pdf/2501.13074", "abs": "https://arxiv.org/abs/2501.13074", "authors": ["Ang Lv", "Ruobing Xie", "Yining Qian", "Songhao Wu", "Xingwu Sun", "Zhanhui Kang", "Di Wang", "Rui Yan"], "title": "Autonomy-of-Experts Models", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "Accepted by ICML 2025", "summary": "Mixture-of-Experts (MoE) models mostly use a router to assign tokens to\nspecific expert modules, activating only partial parameters and often\noutperforming dense models. We argue that the separation between the router's\ndecision-making and the experts' execution is a critical yet overlooked issue,\nleading to suboptimal expert selection and ineffective learning. To address\nthis, we propose Autonomy-of-Experts (AoE), a novel MoE paradigm in which\nexperts autonomously select themselves to process inputs. AoE is based on the\ninsight that an expert is aware of its own capacity to effectively process a\ntoken, an awareness reflected in the scale of its internal activations. In AoE,\nrouters are removed; instead, experts pre-compute internal activations for\ninputs and are ranked based on their activation norms. Only the top-ranking\nexperts proceed with the forward pass, while the others abort. The overhead of\npre-computing activations is reduced through a low-rank weight factorization.\nThis self-evaluating-then-partner-comparing approach ensures improved expert\nselection and effective learning. We pre-train language models having 700M up\nto 4B parameters, demonstrating that AoE outperforms traditional MoE models\nwith comparable efficiency."}
{"id": "2502.03758", "pdf": "https://arxiv.org/pdf/2502.03758", "abs": "https://arxiv.org/abs/2502.03758", "authors": ["Yibo Xu", "Dawei Zhou", "Decheng Liu", "Nannan Wang"], "title": "Improving Adversarial Robustness via Phase and Amplitude-aware Prompting", "categories": ["cs.CV"], "comment": null, "summary": "Deep neural networks are found to be vulnerable to adversarial perturbations.\nThe prompt-based defense has been increasingly studied due to its high\nefficiency. However, existing prompt-based defenses mainly exploited mixed\nprompt patterns, where critical patterns closely related to object semantics\nlack sufficient focus. The phase and amplitude spectra have been proven to be\nhighly related to specific semantic patterns and crucial for robustness. To\nthis end, in this paper, we propose a Phase and Amplitude-aware Prompting (PAP)\ndefense. Specifically, we construct phase-level and amplitude-level prompts for\neach class, and adjust weights for prompting according to the model's robust\nperformance under these prompts during training. During testing, we select\nprompts for each image using its predicted label to obtain the prompted image,\nwhich is inputted to the model to get the final prediction. Experimental\nresults demonstrate the effectiveness of our method."}
{"id": "2410.03380", "pdf": "https://arxiv.org/pdf/2410.03380", "abs": "https://arxiv.org/abs/2410.03380", "authors": ["Menghua Wu", "Umesh Padia", "Sean H. Murphy", "Regina Barzilay", "Tommi Jaakkola"], "title": "Identifying biological perturbation targets through causal differential networks", "categories": ["cs.LG", "cs.AI", "q-bio.QM"], "comment": null, "summary": "Identifying variables responsible for changes to a biological system enables\napplications in drug target discovery and cell engineering. Given a pair of\nobservational and interventional datasets, the goal is to isolate the subset of\nobserved variables that were the targets of the intervention. Directly applying\ncausal discovery algorithms is challenging: the data may contain thousands of\nvariables with as few as tens of samples per intervention, and biological\nsystems do not adhere to classical causality assumptions. We propose a\ncausality-inspired approach to address this practical setting. First, we infer\nnoisy causal graphs from the observational and interventional data. Then, we\nlearn to map the differences between these graphs, along with additional\nstatistical features, to sets of variables that were intervened upon. Both\nmodules are jointly trained in a supervised framework, on simulated and real\ndata that reflect the nature of biological interventions. This approach\nconsistently outperforms baselines for perturbation modeling on seven\nsingle-cell transcriptomics datasets. We also demonstrate significant\nimprovements over current causal discovery methods for predicting soft and hard\nintervention targets across a variety of synthetic data."}
{"id": "2405.18674", "pdf": "https://arxiv.org/pdf/2405.18674", "abs": "https://arxiv.org/abs/2405.18674", "authors": ["Yuta Tarumi", "Keisuke Fukuda", "Shin-ichi Maeda"], "title": "Deep Bayesian Filter for Bayes-faithful Data Assimilation", "categories": ["cs.LG", "physics.ao-ph", "physics.data-an"], "comment": "Main text 9 pages, ICML2025", "summary": "State estimation for nonlinear state space models (SSMs) is a challenging\ntask. Existing assimilation methodologies predominantly assume Gaussian\nposteriors on physical space, where true posteriors become inevitably\nnon-Gaussian. We propose Deep Bayesian Filtering (DBF) for data assimilation on\nnonlinear SSMs. DBF constructs new latent variables $h_t$ in addition to the\noriginal physical variables $z_t$ and assimilates observations $o_t$. By (i)\nconstraining the state transition on the new latent space to be linear and (ii)\nlearning a Gaussian inverse observation operator $r(h_t|o_t)$, posteriors\nremain Gaussian. Notably, the structured design of test distributions enables\nan analytical formula for the recursive computation, eliminating the\naccumulation of Monte Carlo sampling errors across time steps. DBF trains the\nGaussian inverse observation operators $r(h_t|o_t)$ and other latent SSM\nparameters (e.g., dynamics matrix) by maximizing the evidence lower bound.\nExperiments demonstrate that DBF outperforms model-based approaches and latent\nassimilation methods in tasks where the true posterior distribution on physical\nspace is significantly non-Gaussian."}
{"id": "2501.18922", "pdf": "https://arxiv.org/pdf/2501.18922", "abs": "https://arxiv.org/abs/2501.18922", "authors": ["Haoran Luo", "Haihong E", "Yikai Guo", "Qika Lin", "Xiaobao Wu", "Xinyu Mu", "Wenhao Liu", "Meina Song", "Yifan Zhu", "Luu Anh Tuan"], "title": "KBQA-o1: Agentic Knowledge Base Question Answering with Monte Carlo Tree Search", "categories": ["cs.CL", "cs.AI", "cs.DB"], "comment": "Accepted by ICML 2025 main conference", "summary": "Knowledge Base Question Answering (KBQA) aims to answer natural language\nquestions with a large-scale structured knowledge base (KB). Despite\nadvancements with large language models (LLMs), KBQA still faces challenges in\nweak KB awareness, imbalance between effectiveness and efficiency, and high\nreliance on annotated data. To address these challenges, we propose KBQA-o1, a\nnovel agentic KBQA method with Monte Carlo Tree Search (MCTS). It introduces a\nReAct-based agent process for stepwise logical form generation with KB\nenvironment exploration. Moreover, it employs MCTS, a heuristic search method\ndriven by policy and reward models, to balance agentic exploration's\nperformance and search space. With heuristic exploration, KBQA-o1 generates\nhigh-quality annotations for further improvement by incremental fine-tuning.\nExperimental results show that KBQA-o1 outperforms previous low-resource KBQA\nmethods with limited annotated data, boosting Llama-3.1-8B model's GrailQA F1\nperformance to 78.5% compared to 48.5% of the previous sota method with\nGPT-3.5-turbo. Our code is publicly available."}
{"id": "2502.05173", "pdf": "https://arxiv.org/pdf/2502.05173", "abs": "https://arxiv.org/abs/2502.05173", "authors": ["Xilin Wei", "Xiaoran Liu", "Yuhang Zang", "Xiaoyi Dong", "Pan Zhang", "Yuhang Cao", "Jian Tong", "Haodong Duan", "Qipeng Guo", "Jiaqi Wang", "Xipeng Qiu", "Dahua Lin"], "title": "VideoRoPE: What Makes for Good Video Rotary Position Embedding?", "categories": ["cs.CV"], "comment": null, "summary": "While Rotary Position Embedding (RoPE) and its variants are widely adopted\nfor their long-context capabilities, the extension of the 1D RoPE to video,\nwith its complex spatio-temporal structure, remains an open challenge. This\nwork first introduces a comprehensive analysis that identifies four key\ncharacteristics essential for the effective adaptation of RoPE to video, which\nhave not been fully considered in prior work. As part of our analysis, we\nintroduce a challenging V-NIAH-D (Visual Needle-In-A-Haystack with Distractors)\ntask, which adds periodic distractors into V-NIAH. The V-NIAH-D task\ndemonstrates that previous RoPE variants, lacking appropriate temporal\ndimension allocation, are easily misled by distractors. Based on our analysis,\nwe introduce \\textbf{VideoRoPE}, with a \\textit{3D structure} designed to\npreserve spatio-temporal relationships. VideoRoPE features\n\\textit{low-frequency temporal allocation} to mitigate periodic oscillations, a\n\\textit{diagonal layout} to maintain spatial symmetry, and \\textit{adjustable\ntemporal spacing} to decouple temporal and spatial indexing. VideoRoPE\nconsistently surpasses previous RoPE variants, across diverse downstream tasks\nsuch as long video retrieval, video understanding, and video hallucination. Our\ncode will be available at\n\\href{https://github.com/Wiselnn570/VideoRoPE}{https://github.com/Wiselnn570/VideoRoPE}."}
{"id": "2410.15625", "pdf": "https://arxiv.org/pdf/2410.15625", "abs": "https://arxiv.org/abs/2410.15625", "authors": ["Anjiang Wei", "Allen Nie", "Thiago S. F. X. Teixeira", "Rohan Yadav", "Wonchan Lee", "Ke Wang", "Alex Aiken"], "title": "Improving Parallel Program Performance with LLM Optimizers via Agent-System Interfaces", "categories": ["cs.LG", "cs.AI", "cs.CL", "cs.DC"], "comment": null, "summary": "Modern scientific discovery increasingly relies on high-performance computing\nfor complex modeling and simulation. A key challenge in improving parallel\nprogram performance is efficiently mapping tasks to processors and data to\nmemory, a process dictated by intricate, low-level system code known as\nmappers. Developing high-performance mappers demands days of manual tuning,\nposing a significant barrier for domain scientists without systems expertise.\nWe introduce a framework that automates mapper development with generative\noptimization, leveraging richer feedback beyond scalar performance metrics. Our\napproach features the Agent-System Interface, which includes a Domain-Specific\nLanguage (DSL) to abstract away the low-level complexity of system code and\ndefine a structured search space, as well as AutoGuide, a mechanism that\ninterprets raw execution output into actionable feedback. Unlike traditional\nreinforcement learning methods such as OpenTuner, which rely solely on scalar\nfeedback, our method finds superior mappers in far fewer iterations. With just\n10 iterations, it outperforms OpenTuner even after 1000 iterations, achieving\n3.8X faster performance. Our approach finds mappers that surpass expert-written\nmappers by up to 1.34X speedup across nine benchmarks while reducing tuning\ntime from days to minutes."}
{"id": "2406.09291", "pdf": "https://arxiv.org/pdf/2406.09291", "abs": "https://arxiv.org/abs/2406.09291", "authors": ["Guy Bar-Shalom", "Yam Eitan", "Fabrizio Frasca", "Haggai Maron"], "title": "A Flexible, Equivariant Framework for Subgraph GNNs via Graph Products and Graph Coarsening", "categories": ["cs.LG"], "comment": "Published at NeurIPS 2024", "summary": "Subgraph GNNs enhance message-passing GNNs expressivity by representing\ngraphs as sets of subgraphs, demonstrating impressive performance across\nvarious tasks. However, their scalability is hindered by the need to process\nlarge numbers of subgraphs. While previous approaches attempted to generate\nsmaller subsets of subgraphs through random or learnable sampling, these\nmethods often yielded suboptimal selections or were limited to small subset\nsizes, ultimately compromising their effectiveness. This paper introduces a new\nSubgraph GNN framework to address these issues. Our approach diverges from most\nprevious methods by associating subgraphs with node clusters rather than with\nindividual nodes. We show that the resulting collection of subgraphs can be\nviewed as the product of coarsened and original graphs, unveiling a new\nconnectivity structure on which we perform generalized message passing.\n  Crucially, controlling the coarsening function enables meaningful selection\nof any number of subgraphs. In addition, we reveal novel permutation symmetries\nin the resulting node feature tensor, characterize associated linear\nequivariant layers, and integrate them into our Subgraph GNN. We also introduce\nnovel node marking strategies and provide a theoretical analysis of their\nexpressive power and other key aspects of our approach. Extensive experiments\non multiple graph learning benchmarks demonstrate that our method is\nsignificantly more flexible than previous approaches, as it can seamlessly\nhandle any number of subgraphs, while consistently outperforming baseline\napproaches. Our code is available at\nhttps://github.com/BarSGuy/Efficient-Subgraph-GNNs."}
{"id": "2502.00592", "pdf": "https://arxiv.org/pdf/2502.00592", "abs": "https://arxiv.org/abs/2502.00592", "authors": ["Yu Wang", "Dmitry Krotov", "Yuanzhe Hu", "Yifan Gao", "Wangchunshu Zhou", "Julian McAuley", "Dan Gutfreund", "Rogerio Feris", "Zexue He"], "title": "M+: Extending MemoryLLM with Scalable Long-Term Memory", "categories": ["cs.CL"], "comment": null, "summary": "Equipping large language models (LLMs) with latent-space memory has attracted\nincreasing attention as they can extend the context window of existing language\nmodels. However, retaining information from the distant past remains a\nchallenge. For example, MemoryLLM (Wang et al., 2024a), as a representative\nwork with latent-space memory, compresses past information into hidden states\nacross all layers, forming a memory pool of 1B parameters. While effective for\nsequence lengths up to 16k tokens, it struggles to retain knowledge beyond 20k\ntokens. In this work, we address this limitation by introducing M+, a\nmemory-augmented model based on MemoryLLM that significantly enhances long-term\ninformation retention. M+ integrates a long-term memory mechanism with a\nco-trained retriever, dynamically retrieving relevant information during text\ngeneration. We evaluate M+ on diverse benchmarks, including long-context\nunderstanding and knowledge retention tasks. Experimental results show that M+\nsignificantly outperforms MemoryLLM and recent strong baselines, extending\nknowledge retention from under 20k to over 160k tokens with similar GPU memory\noverhead. We open-source our code at https://github.com/wangyu-ustc/MemoryLLM"}
{"id": "2502.13859", "pdf": "https://arxiv.org/pdf/2502.13859", "abs": "https://arxiv.org/abs/2502.13859", "authors": ["Shuyong Gao", "Yu'ang Feng", "Qishan Wang", "Lingyi Hong", "Xinyu Zhou", "Liu Fei", "Yan Wang", "Wenqiang Zhang"], "title": "MSVCOD:A Large-Scale Multi-Scene Dataset for Video Camouflage Object Detection", "categories": ["cs.CV"], "comment": "10 pages", "summary": "Video Camouflaged Object Detection (VCOD) is a challenging task which aims to\nidentify objects that seamlessly concealed within the background in videos. The\ndynamic properties of video enable detection of camouflaged objects through\nmotion cues or varied perspectives. Previous VCOD datasets primarily contain\nanimal objects, limiting the scope of research to wildlife scenarios. However,\nthe applications of VCOD extend beyond wildlife and have significant\nimplications in security, art, and medical fields. Addressing this problem, we\nconstruct a new large-scale multi-domain VCOD dataset MSVCOD. To achieve\nhigh-quality annotations, we design a semi-automatic iterative annotation\npipeline that reduces costs while maintaining annotation accuracy. Our MSVCOD\nis the largest VCOD dataset to date, introducing multiple object categories\nincluding human, animal, medical, and vehicle objects for the first time, while\nalso expanding background diversity across various environments. This expanded\nscope increases the practical applicability of the VCOD task in camouflaged\nobject detection. Alongside this dataset, we introduce a one-steam video\ncamouflage object detection model that performs both feature extraction and\ninformation fusion without additional motion feature fusion modules. Our\nframework achieves state-of-the-art results on the existing VCOD animal dataset\nand the proposed MSVCOD. The dataset and code will be made publicly available."}
{"id": "2410.22307", "pdf": "https://arxiv.org/pdf/2410.22307", "abs": "https://arxiv.org/abs/2410.22307", "authors": ["Yifan Sun", "Yuhang Li", "Yue Zhang", "Yuchen Jin", "Huan Zhang"], "title": "SVIP: Towards Verifiable Inference of Open-source Large Language Models", "categories": ["cs.LG", "cs.AI", "cs.CL", "cs.CR"], "comment": "22 pages", "summary": "The ever-increasing size of open-source Large Language Models (LLMs) renders\nlocal deployment impractical for individual users. Decentralized computing has\nemerged as a cost-effective solution, allowing individuals and small companies\nto perform LLM inference for users using surplus computational power. However,\na computing provider may stealthily substitute the requested LLM with a\nsmaller, less capable model without consent from users, thereby benefiting from\ncost savings. We introduce SVIP, a secret-based verifiable LLM inference\nprotocol. Unlike existing solutions based on cryptographic or game-theoretic\ntechniques, our method is computationally effective and does not rest on strong\nassumptions. Our protocol requires the computing provider to return both the\ngenerated text and processed hidden representations from LLMs. We then train a\nproxy task on these representations, effectively transforming them into a\nunique model identifier. With our protocol, users can reliably verify whether\nthe computing provider is acting honestly. A carefully integrated secret\nmechanism further strengthens its security. We thoroughly analyze our protocol\nunder multiple strong and adaptive adversarial scenarios. Our extensive\nexperiments demonstrate that SVIP is accurate, generalizable, computationally\nefficient, and resistant to various attacks. Notably, SVIP achieves false\nnegative rates below 5% and false positive rates below 3%, while requiring less\nthan 0.01 seconds per prompt query for verification."}
{"id": "2407.09297", "pdf": "https://arxiv.org/pdf/2407.09297", "abs": "https://arxiv.org/abs/2407.09297", "authors": ["Peter Sorrenson", "Daniel Behrend-Uriarte", "Christoph SchnÃ¶rr", "Ullrich KÃ¶the"], "title": "Learning Distances from Data with Normalizing Flows and Score Matching", "categories": ["cs.LG", "stat.ML"], "comment": "ICML 2025", "summary": "Density-based distances (DBDs) provide a principled approach to metric\nlearning by defining distances in terms of the underlying data distribution. By\nemploying a Riemannian metric that increases in regions of low probability\ndensity, shortest paths naturally follow the data manifold. Fermat distances, a\nspecific type of DBD, have attractive properties, but existing estimators based\non nearest neighbor graphs suffer from poor convergence due to inaccurate\ndensity estimates. Moreover, graph-based methods scale poorly to high\ndimensions, as the proposed geodesics are often insufficiently smooth. We\naddress these challenges in two key ways. First, we learn densities using\nnormalizing flows. Second, we refine geodesics through relaxation, guided by a\nlearned score model. Additionally, we introduce a dimension-adapted Fermat\ndistance that scales intuitively to high dimensions and improves numerical\nstability. Our work paves the way for the practical use of density-based\ndistances, especially in high-dimensional spaces."}
{"id": "2502.01349", "pdf": "https://arxiv.org/pdf/2502.01349", "abs": "https://arxiv.org/abs/2502.01349", "authors": ["Giorgos Filandrianos", "Angeliki Dimitriou", "Maria Lymperaiou", "Konstantinos Thomas", "Giorgos Stamou"], "title": "Bias Beware: The Impact of Cognitive Biases on LLM-Driven Product Recommendations", "categories": ["cs.CL"], "comment": null, "summary": "The advent of Large Language Models (LLMs) has revolutionized product\nrecommenders, yet their susceptibility to adversarial manipulation poses\ncritical challenges, particularly in real-world commercial applications. Our\napproach is the first one to tap into human psychological principles,\nseamlessly modifying product descriptions, making such manipulations hard to\ndetect. In this work, we investigate cognitive biases as black-box adversarial\nstrategies, drawing parallels between their effects on LLMs and human\npurchasing behavior. Through extensive evaluation across models of varying\nscale, we find that certain biases, such as social proof, consistently boost\nproduct recommendation rate and ranking, while others, like scarcity and\nexclusivity, surprisingly reduce visibility. Our results demonstrate that\ncognitive biases are deeply embedded in state-of-the-art LLMs, leading to\nhighly unpredictable behavior in product recommendations and posing significant\nchallenges for effective mitigation."}
{"id": "2502.14799", "pdf": "https://arxiv.org/pdf/2502.14799", "abs": "https://arxiv.org/abs/2502.14799", "authors": ["Hai Wang", "Xiaoyu Xiang", "Weihao Xia", "Jing-Hao Xue"], "title": "A Survey on Text-Driven 360-Degree Panorama Generation", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "The advent of text-driven 360-degree panorama generation, enabling the\nsynthesis of 360-degree panoramic images directly from textual descriptions,\nmarks a transformative advancement in immersive visual content creation. This\ninnovation significantly simplifies the traditionally complex process of\nproducing such content. Recent progress in text-to-image diffusion models has\naccelerated the rapid development in this emerging field. This survey presents\na comprehensive review of text-driven 360-degree panorama generation, offering\nan in-depth analysis of state-of-the-art algorithms and their expanding\napplications in 360-degree 3D scene generation. Furthermore, we critically\nexamine current limitations and propose promising directions for future\nresearch. A curated project page with relevant resources and research papers is\navailable at https://littlewhitesea.github.io/Text-Driven-Pano-Gen/."}
{"id": "2410.22366", "pdf": "https://arxiv.org/pdf/2410.22366", "abs": "https://arxiv.org/abs/2410.22366", "authors": ["Viacheslav Surkov", "Chris Wendler", "Antonio Mari", "Mikhail Terekhov", "Justin Deschenaux", "Robert West", "Caglar Gulcehre", "David Bau"], "title": "One-Step is Enough: Sparse Autoencoders for Text-to-Image Diffusion Models", "categories": ["cs.LG", "cs.AI", "cs.CV"], "comment": null, "summary": "For large language models (LLMs), sparse autoencoders (SAEs) have been shown\nto decompose intermediate representations that often are not interpretable\ndirectly into sparse sums of interpretable features, facilitating better\ncontrol and subsequent analysis. However, similar analyses and approaches have\nbeen lacking for text-to-image models. We investigate the possibility of using\nSAEs to learn interpretable features for SDXL Turbo, a few-step text-to-image\ndiffusion model. To this end, we train SAEs on the updates performed by\ntransformer blocks within SDXL Turbo's denoising U-net in its 1-step setting.\nInterestingly, we find that they generalize to 4-step SDXL Turbo and even to\nthe multi-step SDXL base model (i.e., a different model) without additional\ntraining. In addition, we show that their learned features are interpretable,\ncausally influence the generation process, and reveal specialization among the\nblocks. We do so by creating RIEBench, a representation-based image editing\nbenchmark, for editing images while they are generated by turning on and off\nindividual SAE features. This allows us to track which transformer blocks'\nfeatures are the most impactful depending on the edit category. Our work is the\nfirst investigation of SAEs for interpretability in text-to-image diffusion\nmodels and our results establish SAEs as a promising approach for understanding\nand manipulating the internal mechanisms of text-to-image models."}
{"id": "2407.11867", "pdf": "https://arxiv.org/pdf/2407.11867", "abs": "https://arxiv.org/abs/2407.11867", "authors": ["Zikui Cai", "Yaoteng Tan", "M. Salman Asif"], "title": "Targeted Unlearning with Single Layer Unlearning Gradient", "categories": ["cs.LG"], "comment": "Accepted to ICML 2025", "summary": "Machine unlearning methods aim to remove sensitive or unwanted content from\ntrained models, but typically demand extensive model updates at significant\ncomputational cost while potentially degrading model performance on both\nrelated and unrelated tasks. We propose Single Layer Unlearning Gradient (SLUG)\nas an efficient method to unlearn targeted information by updating a single\ncritical layer using a one-time gradient computation. SLUG uses layer\nimportance and gradient alignment metrics to identify the optimal layer for\ntargeted information removal while preserving the model utility. We demonstrate\nthe effectiveness of SLUG for CLIP, Stable Diffusion, and vision-language\nmodels (VLMs) in removing concrete (e.g., identities and objects) and abstract\nconcepts (e.g., artistic styles). On the UnlearnCanvas benchmark, SLUG achieves\ncomparable unlearning performance to existing methods while requiring\nsignificantly less computational resources. Our proposed approach offers a\npractical solution for targeted unlearning that is computationally efficient\nand precise. Our code is available at https://github.com/CSIPlab/SLUG."}
{"id": "2502.02339", "pdf": "https://arxiv.org/pdf/2502.02339", "abs": "https://arxiv.org/abs/2502.02339", "authors": ["Jinyang Wu", "Mingkuan Feng", "Shuai Zhang", "Fangrui Lv", "Ruihan Jin", "Feihu Che", "Zengqi Wen", "Jianhua Tao"], "title": "Boosting Multimodal Reasoning with Automated Structured Thinking", "categories": ["cs.CL"], "comment": null, "summary": "Multimodal large language models excel across diverse domains but struggle\nwith complex visual reasoning tasks. Current approaches aim to incorporate\nstructured thinking via two strategies: explicit search methods and\npost-training techniques. However, both approaches face significant\nlimitations: Search-based methods suffer from computational inefficiency due to\nextensive solution space exploration, while post-training methods require\nsubstantial data, computational resources, and often encounter training\ninstability. To address these limitations, we propose AStar, an\n\\textbf{A}utomated \\textbf{S}tructured \\textbf{t}hinking paradigm for\nmultimod\\textbf{a}l \\textbf{r}easoning. Our method introduces \"thought cards\",\na lightweight library of high-level reasoning patterns abstracted from 500\nprior samples using Monte Carlo Tree Search. For each test problem, AStar\nadaptively retrieves the optimal thought cards and seamlessly integrates these\nexternal explicit guidelines with the model's internal implicit reasoning\ncapabilities. Extensive experiments demonstrate AStar's effectiveness and\nefficiency: using only 500 prior samples and a 7B backbone, our training-free\nframework achieves 53.9$\\%$ accuracy on MathVerse (surpassing GPT-4o's 50.2%)\nand 32.7% on MathVision (versus GPT-4o's 30.4%). Further analysis reveals that\nAStar generalizes beyond multimodal reasoning to visual perception and\nunderstanding domains, and serves as a plug-and-play test-time inference method\ncompatible with mainstream post-training techniques like GRPO."}
{"id": "2503.11101", "pdf": "https://arxiv.org/pdf/2503.11101", "abs": "https://arxiv.org/abs/2503.11101", "authors": ["Asifullah Khan", "Laiba Asmatullah", "Anza Malik", "Shahzaib Khan", "Hamna Asif"], "title": "A Survey on Self-supervised Contrastive Learning for Multimodal Text-Image Analysis", "categories": ["cs.CV", "cs.LG"], "comment": "38 pages, 8 figures, survey paper", "summary": "Self-supervised learning is a machine learning approach that generates\nimplicit labels by learning underlined patterns and extracting discriminative\nfeatures from unlabeled data without manual labelling. Contrastive learning\nintroduces the concept of \"positive\" and \"negative\" samples, where positive\npairs (e.g., variation of the same image/object) are brought together in the\nembedding space, and negative pairs (e.g., views from different images/objects)\nare pushed farther away. This methodology has shown significant improvements in\nimage understanding and image text analysis without much reliance on labeled\ndata. In this paper, we comprehensively discuss the terminologies, recent\ndevelopments and applications of contrastive learning with respect to\ntext-image models. Specifically, we provide an overview of the approaches of\ncontrastive learning in text-image models in recent years. Secondly, we\ncategorize the approaches based on different model structures. Thirdly, we\nfurther introduce and discuss the latest advances of the techniques used in the\nprocess such as pretext tasks for both images and text, architectural\nstructures, and key trends. Lastly, we discuss the recent state-of-art\napplications of self-supervised contrastive learning Text-Image based models."}
{"id": "2410.22815", "pdf": "https://arxiv.org/pdf/2410.22815", "abs": "https://arxiv.org/abs/2410.22815", "authors": ["Jabin Koo", "Minwoo Jang", "Jungseul Ok"], "title": "Towards Robust and Efficient Federated Low-Rank Adaptation with Heterogeneous Clients", "categories": ["cs.LG", "cs.AI", "cs.DC"], "comment": "Appears in ACL 2025", "summary": "Federated fine-tuning for Large Language Models (LLMs) faces significant\nchallenges due to the heavy communication overhead of transmitting large model\nupdates. Although Low Rank Adaptation (LoRA) has been proposed as a solution,\nyet its application in federated learning is complicated by discordance in\naggregation. Existing methods addressing this discordance often suffer from\nperformance degradation at low ranks in heterogeneous data settings. In\nresponse, we introduce LoRA-A$^2$ (Low Rank Adaptation with Alternating freeze\nand Adaptive rank selection), which demonstrates robustness in challenging\nsettings with low ranks and high data heterogeneity. Our experimental findings\nreveal that LoRA-A$^2$ maintains performance even under extreme heterogeneity\nand low rank conditions, achieving up to a significant reduction in uploaded\nparameters compared to full fine-tuning without compromising performance. This\nadaptive mechanism increases robustness and communication efficiency in\nfederated fine-tuning, enabling the practical deployment of LLMs in\nresource-constrained environments."}
{"id": "2407.19331", "pdf": "https://arxiv.org/pdf/2407.19331", "abs": "https://arxiv.org/abs/2407.19331", "authors": ["Yifan Yang", "Ali Payani", "Parinaz Naghizadeh"], "title": "Friends in Unexpected Places: Enhancing Local Fairness in Federated Learning through Clustering", "categories": ["cs.LG", "cs.CY"], "comment": null, "summary": "Federated Learning (FL) has been a pivotal paradigm for collaborative\ntraining of machine learning models across distributed datasets. In\nheterogeneous settings, it has been observed that a single shared FL model can\nlead to low local accuracy, motivating personalized FL algorithms. In parallel,\nfair FL algorithms have been proposed to enforce group fairness on the global\nmodels. Again, in heterogeneous settings, global and local fairness do not\nnecessarily align, motivating the recent literature on locally fair FL. In this\npaper, we propose new FL algorithms for heterogeneous settings, spanning the\nspace between personalized and locally fair FL. Building on existing\nclustering-based personalized FL methods, we incorporate a new fairness metric\ninto cluster assignment, enabling a tunable balance between local accuracy and\nfairness. Our methods match or exceed the performance of existing locally fair\nFL approaches, without explicit fairness intervention. We further demonstrate\n(numerically and analytically) that personalization alone can improve local\nfairness and that our methods exploit this alignment when present."}
{"id": "2502.02659", "pdf": "https://arxiv.org/pdf/2502.02659", "abs": "https://arxiv.org/abs/2502.02659", "authors": ["Yan Li", "Tianyi Zhang", "Zechuan Li", "Soyeon Caren Han"], "title": "A Training-Free Length Extrapolation Approach for LLMs: Greedy Attention Logit Interpolation (GALI)", "categories": ["cs.CL", "cs.AI"], "comment": "9 pages, under review in the conference", "summary": "Transformer-based Large Language Models (LLMs) struggle with inputs exceeding\ntheir training context window due to positional out-of-distribution (O.O.D.)\nissues that disrupt attention. Existing solutions, including fine-tuning and\ntraining-free methods, face challenges like inefficiency, redundant\ninterpolation, logit outliers, or loss of local positional information. We\npropose Greedy Attention Logit Interpolation (GALI), a training-free method\nthat improves length extrapolation by greedily reusing pretrained positional\nintervals and interpolating attention logit to eliminate outliers. GALI\nachieves stable and superior performance across a wide range of long-context\ntasks without requiring input-length-specific tuning. Our analysis further\nreveals that LLMs interpret positional intervals unevenly and that restricting\ninterpolation to narrower ranges improves performance, even on short-context\ntasks. GALI represents a step toward more robust and generalizable long-text\nprocessing in LLMs. Our implementation of GALI, along with the experiments from\nour paper, is open-sourced at https://github.com/adlnlp/Gali."}
{"id": "2503.14604", "pdf": "https://arxiv.org/pdf/2503.14604", "abs": "https://arxiv.org/abs/2503.14604", "authors": ["Sara Sarto", "Marcella Cornia", "Rita Cucchiara"], "title": "Image Captioning Evaluation in the Age of Multimodal LLMs: Challenges and Future Perspectives", "categories": ["cs.CV", "cs.AI", "cs.CL"], "comment": "IJCAI 2025. Repo GitHub:\n  https://github.com/aimagelab/awesome-captioning-evaluation", "summary": "The evaluation of machine-generated image captions is a complex and evolving\nchallenge. With the advent of Multimodal Large Language Models (MLLMs), image\ncaptioning has become a core task, increasing the need for robust and reliable\nevaluation metrics. This survey provides a comprehensive overview of\nadvancements in image captioning evaluation, analyzing the evolution,\nstrengths, and limitations of existing metrics. We assess these metrics across\nmultiple dimensions, including correlation with human judgment, ranking\naccuracy, and sensitivity to hallucinations. Additionally, we explore the\nchallenges posed by the longer and more detailed captions generated by MLLMs\nand examine the adaptability of current metrics to these stylistic variations.\nOur analysis highlights some limitations of standard evaluation approaches and\nsuggests promising directions for future research in image captioning\nassessment."}
{"id": "2411.01707", "pdf": "https://arxiv.org/pdf/2411.01707", "abs": "https://arxiv.org/abs/2411.01707", "authors": ["Jingtao Tang", "Zining Mao", "Hang Ma"], "title": "Large-Scale Multi-Robot Coverage Path Planning on Grids with Path Deconfliction", "categories": ["cs.RO", "cs.AI"], "comment": "accepted to T-RO", "summary": "We study Multi-Robot Coverage Path Planning (MCPP) on a 4-neighbor 2D grid G,\nwhich aims to compute paths for multiple robots to cover all cells of G.\nTraditional approaches are limited as they first compute coverage trees on a\nquadrant coarsened grid H and then employ the Spanning Tree Coverage (STC)\nparadigm to generate paths on G, making them inapplicable to grids with\npartially obstructed 2x2 blocks. To address this limitation, we reformulate the\nproblem directly on G, revolutionizing grid-based MCPP solving and establishing\nnew NP-hardness results. We introduce Extended-STC (ESTC), a novel paradigm\nthat extends STC to ensure complete coverage with bounded suboptimality, even\nwhen H includes partially obstructed blocks. Furthermore, we present LS-MCPP, a\nnew algorithmic framework that integrates ESTC with three novel types of\nneighborhood operators within a local search strategy to optimize coverage\npaths directly on G. Unlike prior grid-based MCPP work, our approach also\nincorporates a versatile post-processing procedure that applies Multi-Agent\nPath Finding (MAPF) techniques to MCPP for the first time, enabling a fusion of\nthese two important fields in multi-robot coordination. This procedure\neffectively resolves inter-robot conflicts and accommodates turning costs by\nsolving a MAPF variant, making our MCPP solutions more practical for real-world\napplications. Extensive experiments demonstrate that our approach significantly\nimproves solution quality and efficiency, managing up to 100 robots on grids as\nlarge as 256x256 within minutes of runtime. Validation with physical robots\nconfirms the feasibility of our solutions under real-world conditions."}
{"id": "2408.05178", "pdf": "https://arxiv.org/pdf/2408.05178", "abs": "https://arxiv.org/abs/2408.05178", "authors": ["Kaden McKeen", "Sameer Masood", "Augustin Toma", "Barry Rubin", "Bo Wang"], "title": "ECG-FM: An Open Electrocardiogram Foundation Model", "categories": ["cs.LG", "68T01", "I.2.0"], "comment": "23 pages, 10 figures, 8 tables", "summary": "Conventional task-specific electrocardiogram (ECG) analysis models require\nlarge annotated datasets to train. Foundation models mitigate this burden by\nleveraging self-supervised pretraining; however, the scarcity of open-weight\nECG foundation models hinders adoption and cross-study comparability. We\npresent ECG-FM, an open foundation model for ECG analysis, and conduct a study\nusing a dataset of 1.5 million ECGs. ECG-FM is a transformer-based model\npretrained using a hybrid contrastive and generative self-supervised learning\napproach. Our downstream tasks include predicting reduced left ventricular\nejection fraction (LVEF) and ECG interpretation labels, where we release a\nbenchmark task on the MIMIC-IV-ECG dataset. We affirm that ECG-FM is robust,\nlabel-efficient, and functionally discriminative by showcasing data scaling\nexperiments, performing a latent space analysis, and generating saliency maps.\nECG-FM markedly outperforms task-specific models in the small-to-medium-scale\ndata regime and demonstrates cross-dataset generalizability, achieving high\nAUROC on many clinically salient labels such as atrial fibrillation (0.996) and\nLVEF<=40% (0.929). We release our code, model weights, and benchmark task at\nhttps://github.com/bowang-lab/ECG-FM/."}
{"id": "2502.04037", "pdf": "https://arxiv.org/pdf/2502.04037", "abs": "https://arxiv.org/abs/2502.04037", "authors": ["Hongfu Gao", "Feipeng Zhang", "Hao Zeng", "Deyu Meng", "Bingyi Jing", "Hongxin Wei"], "title": "Exploring Imbalanced Annotations for Effective In-Context Learning", "categories": ["cs.CL", "cs.LG"], "comment": null, "summary": "Large language models (LLMs) have shown impressive performance on downstream\ntasks through in-context learning (ICL), which heavily relies on the\ndemonstrations selected from annotated datasets. However, these datasets often\nexhibit long-tailed class distributions in real-world scenarios, leading to\nbiased demonstration selection. In this work, we show that such class\nimbalances significantly degrade the ICL performance across various tasks,\nregardless of selection methods. Moreover, classical rebalancing methods, which\nfocus solely on class weights, yield poor performance due to neglecting\ncondition bias--skewed feature distributions within classes. To address this,\nwe propose Reweighting with Conditional Bias (dubbed RCB), a simple and\ncomplementary approach to enhance ICL performance under class imbalance. In\nparticular, RCB estimates conditional bias using a balanced subset and\nre-weights demonstration scores based on both class weight and conditional\nbias. In effect, RCB prevents over-selection from dominant classes while\npreserving the efficacy of current selection methods. Extensive experiments on\ncommon benchmarks demonstrate the effectiveness of our method, improving the\naverage accuracy of current selection methods by up to 5.42%."}
{"id": "2503.17794", "pdf": "https://arxiv.org/pdf/2503.17794", "abs": "https://arxiv.org/abs/2503.17794", "authors": ["Ketan Suhaas Saichandran", "Xavier Thomas", "Prakhar Kaushik", "Deepti Ghadiyaram"], "title": "Progressive Prompt Detailing for Improved Alignment in Text-to-Image Generative Models", "categories": ["cs.CV", "cs.AI"], "comment": "Accepted at CVPR 2025 workshops (AI4CC (oral) & GMCV (poster))", "summary": "Text-to-image generative models often struggle with long prompts detailing\ncomplex scenes, diverse objects with distinct visual characteristics and\nspatial relationships. In this work, we propose SCoPE (Scheduled interpolation\nof Coarse-to-fine Prompt Embeddings), a training-free method to improve\ntext-to-image alignment by progressively refining the input prompt in a\ncoarse-to-fine-grained manner. Given a detailed input prompt, we first\ndecompose it into multiple sub-prompts which evolve from describing broad scene\nlayout to highly intricate details. During inference, we interpolate between\nthese sub-prompts and thus progressively introduce finer-grained details into\nthe generated image. Our training-free plug-and-play approach significantly\nenhances prompt alignment, achieves an average improvement of more than +8 in\nVisual Question Answering (VQA) scores over the Stable Diffusion baselines on\n83% of the prompts from the GenAI-Bench dataset."}
{"id": "2411.02355", "pdf": "https://arxiv.org/pdf/2411.02355", "abs": "https://arxiv.org/abs/2411.02355", "authors": ["Eldar Kurtic", "Alexandre Marques", "Shubhra Pandit", "Mark Kurtz", "Dan Alistarh"], "title": "\"Give Me BF16 or Give Me Death\"? Accuracy-Performance Trade-Offs in LLM Quantization", "categories": ["cs.LG", "cs.AI"], "comment": "Accepted to ACL 2025", "summary": "Quantization is a powerful tool for accelerating large language model (LLM)\ninference, but the accuracy-performance trade-offs across different formats\nremain unclear. In this paper, we conduct the most comprehensive empirical\nstudy to date, evaluating FP8, INT8, and INT4 quantization across academic\nbenchmarks and real-world tasks on the entire Llama-3.1 model family. Through\nover 500,000 evaluations, our investigation yields several key findings: (1)\nFP8 (W8A8-FP) is effectively lossless across all model scales, (2) well-tuned\nINT8 (W8A8-INT) achieves surprisingly low (1-3\\%) accuracy degradation, and (3)\nINT4 weight-only (W4A16-INT) is more competitive than expected, rivaling 8-bit\nquantization. Further, we investigate the optimal quantization format for\ndifferent deployments by analyzing inference performance through the popular\nvLLM framework. Our analysis provides clear deployment recommendations: W4A16\nis the most cost-efficient for synchronous setups, while W8A8 dominates in\nasynchronous continuous batching. For mixed workloads, the optimal choice\ndepends on the specific use case. Our findings offer practical, data-driven\nguidelines for deploying quantized LLMs at scale -- ensuring the best balance\nbetween speed, efficiency, and accuracy."}
{"id": "2408.08868", "pdf": "https://arxiv.org/pdf/2408.08868", "abs": "https://arxiv.org/abs/2408.08868", "authors": ["H. Brendan McMahan", "Zheng Xu", "Yanxiang Zhang"], "title": "A Hassle-free Algorithm for Private Learning in Practice: Don't Use Tree Aggregation, Use BLTs", "categories": ["cs.LG"], "comment": "v2: EMNLP camera ready, minor error and typo fix, updated production\n  model launch info; v3: update production model launch info again to reflect\n  the success of \"upgrade all existing FL LMs that have previously been\n  launched without DP to be trained with DP\" in arxiv:2306.14793", "summary": "The state-of-the-art for training on-device language models for mobile\nkeyboard applications combines federated learning (FL) with differential\nprivacy (DP) via the DP-Follow-the-Regularized-Leader (DP-FTRL) algorithm. Two\nvariants of DP-FTRL are used in practice, tree aggregation and matrix\nfactorization. However, tree aggregation suffers from significantly suboptimal\nprivacy/utility tradeoffs, while matrix mechanisms require expensive\noptimization parameterized by hard-to-estimate-in-advance constants, and high\nruntime memory costs. This paper extends the recently introduced Buffered\nLinear Toeplitz (BLT) mechanism to multi-participation scenarios. Our\nBLT-DP-FTRL maintains the ease-of-use advantages of tree aggregation, while\nessentially matching matrix factorization in terms of utility and privacy. We\nevaluate BLT-DP-FTRL on the StackOverflow dataset, serving as a re-producible\nsimulation benchmark, and across four on-device language model tasks in a\nproduction FL system. Our empirical results highlight the advantages of the BLT\nmechanism and elevate the practicality and effectiveness of DP in real-world\nscenarios."}
{"id": "2502.09284", "pdf": "https://arxiv.org/pdf/2502.09284", "abs": "https://arxiv.org/abs/2502.09284", "authors": ["Amirbek Djanibekov", "Hanan Aldarmaki"], "title": "SparQLe: Speech Queries to Text Translation Through LLMs", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "With the growing influence of Large Language Models (LLMs), there is\nincreasing interest in integrating speech representations with them to enable\nmore seamless multi-modal processing and speech understanding. This study\nintroduces a novel approach that combines self-supervised speech\nrepresentations with instruction-tuned LLMs for speech-to-text translation. The\nproposed approach leverages a modality adapter to align extracted speech\nfeatures with instruction-tuned LLMs using English speech data. Our experiments\ndemonstrate that this method effectively preserves the semantic content of the\ninput speech and serves as an effective bridge between self-supervised speech\nmodels and instruction-tuned LLMs, offering a promising approach for various\nspeech understanding applications."}
{"id": "2503.18034", "pdf": "https://arxiv.org/pdf/2503.18034", "abs": "https://arxiv.org/abs/2503.18034", "authors": ["Qiao Liang", "Yanjiang Liu", "Weixiang Zhou", "Ben He", "Yaojie Lu", "Hongyu Lin", "Jia Zheng", "Xianpei Han", "Le Sun", "Yingfei Sun"], "title": "Expanding the Boundaries of Vision Prior Knowledge in Multi-modal Large Language Models", "categories": ["cs.CV", "cs.CL"], "comment": null, "summary": "Does the prior knowledge of the vision encoder constrain the capability\nboundary of Multi-modal Large Language Models (MLLMs)? While most existing\nresearch treats MLLMs as unified systems optimized through end-to-end training,\nthe impact of vision encoder's prior knowledge is seldom investigated. In this\nwork, we introduce a novel metric, $Rank_e$, to quantify the effect of prior\nknowledge of the vision encoder on MLLM performance. Our analysis reveals a\npositive correlation between prior knowledge and MLLM performance. Moreover, we\nfind that domain-specific fine-tuning using solely end-to-end visual question\nanswering (VQA) data is insufficient, particularly for entities with low\ninherent visual prior knowledge. To address this issue, we propose VisPRE\n(Vision Prior Remediation), a two-stage training framework that explicitly\nincorporates prior knowledge at the vision encoder level. Experimental results\ndemonstrate that augmenting vision encoder's prior knowledge substantially\nboosts the visual understanding capabilities of MLLMs, offering a novel and\neffective strategy for improving performance, especially in scenarios involving\nuncommon visual entities."}
{"id": "2411.10877", "pdf": "https://arxiv.org/pdf/2411.10877", "abs": "https://arxiv.org/abs/2411.10877", "authors": ["Trevor Stalnaker", "Nathan Wintersgill", "Oscar Chaparro", "Laura A. Heymann", "Massimiliano Di Penta", "Daniel M German", "Denys Poshyvanyk"], "title": "Developer Perspectives on Licensing and Copyright Issues Arising from Generative AI for Software Development", "categories": ["cs.SE", "cs.AI"], "comment": null, "summary": "Despite the utility that Generative AI (GenAI) tools provide for tasks such\nas writing code, the use of these tools raises important legal questions and\npotential risks, particularly those associated with copyright law. As lawmakers\nand regulators engage with those questions, the views of users can provide\nrelevant perspectives. In this paper, we provide: (1) a survey of 574\ndevelopers on the licensing and copyright aspects of GenAI for coding, as well\nas follow-up interviews; (2) a snapshot of developers' views at a time when\nGenAI and perceptions of it are rapidly evolving; and (3) an analysis of\ndevelopers' views, yielding insights and recommendations that can inform future\nregulatory decisions in this evolving field. Our results show the benefits\ndevelopers derive from GenAI, how they view the use of AI-generated code as\nsimilar to using other existing code, the varied opinions they have on who\nshould own or be compensated for such code, that they are concerned about data\nleakage via GenAI, and much more, providing organizations and policymakers with\nvaluable insights into how the technology is being used and what concerns\nstakeholders would like to see addressed."}
{"id": "2408.09429", "pdf": "https://arxiv.org/pdf/2408.09429", "abs": "https://arxiv.org/abs/2408.09429", "authors": ["Kening Zheng", "Junkai Chen", "Yibo Yan", "Xin Zou", "Xuming Hu"], "title": "Reefknot: A Comprehensive Benchmark for Relation Hallucination Evaluation, Analysis and Mitigation in Multimodal Large Language Models", "categories": ["cs.LG", "cs.CL", "cs.CV"], "comment": "Accepted by Findings of ACL 2025", "summary": "Hallucination issues continue to affect multimodal large language models\n(MLLMs), with existing research mainly addressing object-level or\nattribute-level hallucinations, neglecting the more complex relation\nhallucinations that require advanced reasoning. Current benchmarks for relation\nhallucinations lack detailed evaluation and effective mitigation, and their\ndatasets often suffer from biases due to systematic annotation processes. To\naddress these challenges, we introduce Reefknot, a comprehensive benchmark\ntargeting relation hallucinations, comprising over 20,000 real-world samples.\nWe provide a systematic definition of relation hallucinations, integrating\nperceptive and cognitive perspectives, and construct a relation-based corpus\nusing the Visual Genome scene graph dataset. Our comparative evaluation reveals\nsignificant limitations in current MLLMs' ability to handle relation\nhallucinations. Additionally, we propose a novel confidence-based mitigation\nstrategy, which reduces the hallucination rate by an average of 9.75% across\nthree datasets, including Reefknot. Our work offers valuable insights for\nachieving trustworthy multimodal intelligence."}
{"id": "2502.11084", "pdf": "https://arxiv.org/pdf/2502.11084", "abs": "https://arxiv.org/abs/2502.11084", "authors": ["Yuting Huang", "Chengyuan Liu", "Yifeng Feng", "Yiquan Wu", "Chao Wu", "Fei Wu", "Kun Kuang"], "title": "Rewrite to Jailbreak: Discover Learnable and Transferable Implicit Harmfulness Instruction", "categories": ["cs.CL"], "comment": "22 pages, 10 figures, accepted to ACL 2025 findings", "summary": "As Large Language Models (LLMs) are widely applied in various domains, the\nsafety of LLMs is increasingly attracting attention to avoid their powerful\ncapabilities being misused. Existing jailbreak methods create a forced\ninstruction-following scenario, or search adversarial prompts with prefix or\nsuffix tokens to achieve a specific representation manually or automatically.\nHowever, they suffer from low efficiency and explicit jailbreak patterns, far\nfrom the real deployment of mass attacks to LLMs. In this paper, we point out\nthat simply rewriting the original instruction can achieve a jailbreak, and we\nfind that this rewriting approach is learnable and transferable. We propose the\nRewrite to Jailbreak (R2J) approach, a transferable black-box jailbreak method\nto attack LLMs by iteratively exploring the weakness of the LLMs and\nautomatically improving the attacking strategy. The jailbreak is more efficient\nand hard to identify since no additional features are introduced. Extensive\nexperiments and analysis demonstrate the effectiveness of R2J, and we find that\nthe jailbreak is also transferable to multiple datasets and various types of\nmodels with only a few queries. We hope our work motivates further\ninvestigation of LLM safety. The code can be found at\nhttps://github.com/ythuang02/R2J/."}
{"id": "2503.18767", "pdf": "https://arxiv.org/pdf/2503.18767", "abs": "https://arxiv.org/abs/2503.18767", "authors": ["Konstantin Pakulev", "Alexander Vakhitov", "Gonzalo Ferrer"], "title": "Good Keypoints for the Two-View Geometry Estimation Problem", "categories": ["cs.CV"], "comment": "Preprint", "summary": "Local features are essential to many modern downstream applications.\nTherefore, it is of interest to determine the properties of local features that\ncontribute to the downstream performance for a better design of feature\ndetectors and descriptors. In our work, we propose a new theoretical model for\nscoring feature points (keypoints) in the context of the two-view geometry\nestimation problem. The model determines two properties that a good keypoint\nfor solving the homography estimation problem should have: be repeatable and\nhave a small expected measurement error. This result provides key insights into\nwhy maximizing the number of correspondences doesn't always lead to better\nhomography estimation accuracy. We use the developed model to design a method\nthat detects keypoints that benefit the homography estimation and introduce the\nBounded NeSS-ST (BoNeSS-ST) keypoint detector. The novelty of BoNeSS-ST comes\nfrom strong theoretical foundations, a more accurate keypoint scoring due to\nsubpixel refinement and a cost designed for superior robustness to low saliency\nkeypoints. As a result, BoNeSS-ST outperforms prior self-supervised local\nfeature detectors on the planar homography estimation task and is on par with\nthem on the epipolar geometry estimation task."}
{"id": "2411.12042", "pdf": "https://arxiv.org/pdf/2411.12042", "abs": "https://arxiv.org/abs/2411.12042", "authors": ["Reza Asad", "Reza Babanezhad", "Issam Laradji", "Nicolas Le Roux", "Sharan Vaswani"], "title": "Fast Convergence of Softmax Policy Mirror Ascent", "categories": ["cs.LG", "cs.AI", "cs.RO"], "comment": "AISTATS 2025", "summary": "Natural policy gradient (NPG) is a common policy optimization algorithm and\ncan be viewed as mirror ascent in the space of probabilities. Recently, Vaswani\net al. [2021] introduced a policy gradient method that corresponds to mirror\nascent in the dual space of logits. We refine this algorithm, removing its need\nfor a normalization across actions and analyze the resulting method (referred\nto as SPMA). For tabular MDPs, we prove that SPMA with a constant step-size\nmatches the linear convergence of NPG and achieves a faster convergence than\nconstant step-size (accelerated) softmax policy gradient. To handle large\nstate-action spaces, we extend SPMA to use a log-linear policy\nparameterization. Unlike that for NPG, generalizing SPMA to the linear function\napproximation (FA) setting does not require compatible function approximation.\nUnlike MDPO, a practical generalization of NPG, SPMA with linear FA only\nrequires solving convex softmax classification problems. We prove that SPMA\nachieves linear convergence to the neighbourhood of the optimal value function.\nWe extend SPMA to handle non-linear FA and evaluate its empirical performance\non the MuJoCo and Atari benchmarks. Our results demonstrate that SPMA\nconsistently achieves similar or better performance compared to MDPO, PPO and\nTRPO."}
{"id": "2408.13448", "pdf": "https://arxiv.org/pdf/2408.13448", "abs": "https://arxiv.org/abs/2408.13448", "authors": ["Bao Duong", "Hung Le", "Biwei Huang", "Thin Nguyen"], "title": "Reinforcement Learning for Causal Discovery without Acyclicity Constraints", "categories": ["cs.LG", "stat.ME", "stat.ML"], "comment": "Accepted at TMLR 04/2025", "summary": "Recently, reinforcement learning (RL) has proved a promising alternative for\nconventional local heuristics in score-based approaches to learning directed\nacyclic causal graphs (DAGs) from observational data. However, the intricate\nacyclicity constraint still challenges the efficient exploration of the vast\nspace of DAGs in existing methods. In this study, we introduce ALIAS\n(reinforced dAg Learning wIthout Acyclicity conStraints), a novel approach to\ncausal discovery powered by the RL machinery. Our method features an efficient\npolicy for generating DAGs in just a single step with an optimal quadratic\ncomplexity, fueled by a novel parametrization of DAGs that directly translates\na continuous space to the space of all DAGs, bypassing the need for explicitly\nenforcing acyclicity constraints. This approach enables us to navigate the\nsearch space more effectively by utilizing policy gradient methods and\nestablished scoring functions. In addition, we provide compelling empirical\nevidence for the strong performance of ALIAS in comparison with\nstate-of-the-arts in causal discovery over increasingly difficult experiment\nconditions on both synthetic and real datasets."}
{"id": "2502.11361", "pdf": "https://arxiv.org/pdf/2502.11361", "abs": "https://arxiv.org/abs/2502.11361", "authors": ["Shaina Raza", "Ashmal Vayani", "Aditya Jain", "Aravind Narayanan", "Vahid Reza Khazaie", "Syed Raza Bashir", "Elham Dolatabadi", "Gias Uddin", "Christos Emmanouilidis", "Rizwan Qureshi", "Mubarak Shah"], "title": "VLDBench Evaluating Multimodal Disinformation with Regulatory Alignment", "categories": ["cs.CL"], "comment": "under review", "summary": "Detecting disinformation that blends manipulated text and images has become\nincreasingly challenging, as AI tools make synthetic content easy to generate\nand disseminate. While most existing AI safety benchmarks focus on single\nmodality misinformation (i.e., false content shared without intent to deceive),\nintentional multimodal disinformation, such as propaganda or conspiracy\ntheories that imitate credible news, remains largely unaddressed. We introduce\nthe Vision-Language Disinformation Detection Benchmark (VLDBench), the first\nlarge-scale resource supporting both unimodal (text-only) and multimodal (text\n+ image) disinformation detection. VLDBench comprises approximately 62,000\nlabeled text-image pairs across 13 categories, curated from 58 news outlets.\nUsing a semi-automated pipeline followed by expert review, 22 domain experts\ninvested over 500 hours to produce high-quality annotations with substantial\ninter-annotator agreement. Evaluations of state-of-the-art Large Language\nModels (LLMs) and Vision-Language Models (VLMs) on VLDBench show that\nincorporating visual cues improves detection accuracy by 5 to 35 percentage\npoints over text-only models. VLDBench provides data and code for evaluation,\nfine-tuning, and robustness testing to support disinformation analysis.\nDeveloped in alignment with AI governance frameworks (e.g., the MIT AI Risk\nRepository), VLDBench offers a principled foundation for advancing trustworthy\ndisinformation detection in multimodal media.\n  Project: https://vectorinstitute.github.io/VLDBench/ Dataset:\nhttps://huggingface.co/datasets/vector-institute/VLDBench Code:\nhttps://github.com/VectorInstitute/VLDBench"}
{"id": "2503.20771", "pdf": "https://arxiv.org/pdf/2503.20771", "abs": "https://arxiv.org/abs/2503.20771", "authors": ["Masoumeh Sharafi", "Emma Ollivier", "Muhammad Osama Zeeshan", "Soufiane Belharbi", "Marco Pedersoli", "Alessandro Lameiras Koerich", "Simon Bacon", "Eric Granger"], "title": "Disentangled Source-Free Personalization for Facial Expression Recognition with Neutral Target Data", "categories": ["cs.CV"], "comment": "13 pages, 13 figures, FG 2025: IEEE Conf. on Automatic Face and\n  Gesture Recognition", "summary": "Facial Expression Recognition (FER) from videos is a crucial task in various\napplication areas, such as human-computer interaction and health diagnosis and\nmonitoring (e.g., assessing pain and depression). Beyond the challenges of\nrecognizing subtle emotional or health states, the effectiveness of deep FER\nmodels is often hindered by the considerable inter-subject variability in\nexpressions. Source-free (unsupervised) domain adaptation (SFDA) methods may be\nemployed to adapt a pre-trained source model using only unlabeled target domain\ndata, thereby avoiding data privacy, storage, and transmission issues.\nTypically, SFDA methods adapt to a target domain dataset corresponding to an\nentire population and assume it includes data from all recognition classes.\nHowever, collecting such comprehensive target data can be difficult or even\nimpossible for FER in healthcare applications. In many real-world scenarios, it\nmay be feasible to collect a short neutral control video (which displays only\nneutral expressions) from target subjects before deployment. These videos can\nbe used to adapt a model to better handle the variability of expressions among\nsubjects. This paper introduces the Disentangled SFDA (DSFDA) method to address\nthe challenge posed by adapting models with missing target expression data.\nDSFDA leverages data from a neutral target control video for end-to-end\ngeneration and adaptation of target data with missing non-neutral data. Our\nmethod learns to disentangle features related to expressions and identity while\ngenerating the missing non-neutral expression data for the target subject,\nthereby enhancing model accuracy. Additionally, our self-supervision strategy\nimproves model adaptation by reconstructing target images that maintain the\nsame identity and source expression."}
{"id": "2411.16746", "pdf": "https://arxiv.org/pdf/2411.16746", "abs": "https://arxiv.org/abs/2411.16746", "authors": ["Ming Yin", "Jingyang Zhang", "Jingwei Sun", "Minghong Fang", "Hai Li", "Yiran Chen"], "title": "LoBAM: LoRA-Based Backdoor Attack on Model Merging", "categories": ["cs.CR", "cs.AI", "cs.LG"], "comment": null, "summary": "Model merging is an emerging technique that integrates multiple models\nfine-tuned on different tasks to create a versatile model that excels in\nmultiple domains. This scheme, in the meantime, may open up backdoor attack\nopportunities where one single malicious model can jeopardize the integrity of\nthe merged model. Existing works try to demonstrate the risk of such attacks by\nassuming substantial computational resources, focusing on cases where the\nattacker can fully fine-tune the pre-trained model. Such an assumption,\nhowever, may not be feasible given the increasing size of machine learning\nmodels. In practice where resources are limited and the attacker can only\nemploy techniques like Low-Rank Adaptation (LoRA) to produce the malicious\nmodel, it remains unclear whether the attack can still work and pose threats.\nIn this work, we first identify that the attack efficacy is significantly\ndiminished when using LoRA for fine-tuning. Then, we propose LoBAM, a method\nthat yields high attack success rate with minimal training resources. The key\nidea of LoBAM is to amplify the malicious weights in an intelligent way that\neffectively enhances the attack efficacy. We demonstrate that our design can\nlead to improved attack success rate through extensive empirical experiments\nacross various model merging scenarios. Moreover, we show that our method is\nhighly stealthy and is difficult to detect and defend against."}
{"id": "2409.01249", "pdf": "https://arxiv.org/pdf/2409.01249", "abs": "https://arxiv.org/abs/2409.01249", "authors": ["Giorgio Piras", "Maura Pintor", "Ambra Demontis", "Battista Biggio", "Giorgio Giacinto", "Fabio Roli"], "title": "Adversarial Pruning: A Survey and Benchmark of Pruning Methods for Adversarial Robustness", "categories": ["cs.LG", "cs.CR", "cs.CV"], "comment": "Accepted at Pattern Recognition", "summary": "Recent work has proposed neural network pruning techniques to reduce the size\nof a network while preserving robustness against adversarial examples, i.e.,\nwell-crafted inputs inducing a misclassification. These methods, which we refer\nto as adversarial pruning methods, involve complex and articulated designs,\nmaking it difficult to analyze the differences and establish a fair and\naccurate comparison. In this work, we overcome these issues by surveying\ncurrent adversarial pruning methods and proposing a novel taxonomy to\ncategorize them based on two main dimensions: the pipeline, defining when to\nprune; and the specifics, defining how to prune. We then highlight the\nlimitations of current empirical analyses and propose a novel, fair evaluation\nbenchmark to address them. We finally conduct an empirical re-evaluation of\ncurrent adversarial pruning methods and discuss the results, highlighting the\nshared traits of top-performing adversarial pruning methods, as well as common\nissues. We welcome contributions in our publicly-available benchmark at\nhttps://github.com/pralab/AdversarialPruningBenchmark"}
{"id": "2502.11404", "pdf": "https://arxiv.org/pdf/2502.11404", "abs": "https://arxiv.org/abs/2502.11404", "authors": ["Hanxing Ding", "Shuchang Tao", "Liang Pang", "Zihao Wei", "Jinyang Gao", "Bolin Ding", "Huawei Shen", "Xueqi Cheng"], "title": "ToolCoder: A Systematic Code-Empowered Tool Learning Framework for Large Language Models", "categories": ["cs.CL"], "comment": "Accepted to ACL 2025", "summary": "Tool learning has emerged as a crucial capability for large language models\n(LLMs) to solve complex real-world tasks through interaction with external\ntools. Existing approaches face significant challenges, including reliance on\nhand-crafted prompts, difficulty in multi-step planning, and lack of precise\nerror diagnosis and reflection mechanisms. We propose ToolCoder, a novel\nframework that reformulates tool learning as a code generation task. Inspired\nby software engineering principles, ToolCoder transforms natural language\nqueries into structured Python function scaffold and systematically breaks down\ntasks with descriptive comments, enabling LLMs to leverage coding paradigms for\ncomplex reasoning and planning. It then generates and executes function\nimplementations to obtain final responses. Additionally, ToolCoder stores\nsuccessfully executed functions in a repository to promote code reuse, while\nleveraging error traceback mechanisms for systematic debugging, optimizing both\nexecution efficiency and robustness. Experiments demonstrate that ToolCoder\nachieves superior performance in task completion accuracy and execution\nreliability compared to existing approaches, establishing the effectiveness of\ncode-centric approaches in tool learning."}
{"id": "2503.23083", "pdf": "https://arxiv.org/pdf/2503.23083", "abs": "https://arxiv.org/abs/2503.23083", "authors": ["Hasan Moughnieh", "Mohamad Chalhoub", "Hasan Nasrallah", "Cristiano Nattero", "Paolo Campanella", "Giovanni Nico", "Ali J. Ghandour"], "title": "Efficient Adaptation For Remote Sensing Visual Grounding", "categories": ["cs.CV", "cs.AI", "cs.CL"], "comment": null, "summary": "Adapting pre-trained models has become an effective strategy in artificial\nintelligence, offering a scalable and efficient alternative to training models\nfrom scratch. In the context of remote sensing (RS), where visual grounding(VG)\nremains underexplored, this approach enables the deployment of powerful\nvision-language models to achieve robust cross-modal understanding while\nsignificantly reducing computational overhead. To address this, we applied\nParameter Efficient Fine Tuning (PEFT) techniques to adapt these models for\nRS-specific VG tasks. Specifically, we evaluated LoRA placement across\ndifferent modules in Grounding DINO and used BitFit and adapters to fine-tune\nthe OFA foundation model pre-trained on general-purpose VG datasets. This\napproach achieved performance comparable to or surpassing current State Of The\nArt (SOTA) models while significantly reducing computational costs. This study\nhighlights the potential of PEFT techniques to advance efficient and precise\nmulti-modal analysis in RS, offering a practical and cost-effective alternative\nto full model training."}
{"id": "2412.08556", "pdf": "https://arxiv.org/pdf/2412.08556", "abs": "https://arxiv.org/abs/2412.08556", "authors": ["Foivos Fioravantes", "DuÅ¡an Knop", "Jan MatyÃ¡Å¡ KÅiÅ¡Å¥an", "Nikolaos Melissinos", "Michal Opler"], "title": "Exact Algorithms for Multiagent Path Finding with Communication Constraints on Tree-Like Structures", "categories": ["cs.CC", "cs.AI"], "comment": null, "summary": "Consider the scenario where multiple agents have to move in an optimal way\nthrough a network, each one towards their ending position while avoiding\ncollisions. By optimal, we mean as fast as possible, which is evaluated by a\nmeasure known as the makespan of the proposed solution. This is the setting\nstudied in the Multiagent Path Finding problem. In this work, we additionally\nprovide the agents with a way to communicate with each other. Due to size\nconstraints, it is reasonable to assume that the range of communication of each\nagent will be limited. What should be the trajectories of the agents to,\nadditionally, maintain a backbone of communication? In this work, we study the\nMultiagent Path Finding with Communication Constraint problem under the\nparameterized complexity framework.\n  Our main contribution is three exact algorithms that are efficient when\nconsidering particular structures for the input network. We provide such\nalgorithms for the case when the communication range and the number of agents\n(the makespan resp.) are provided in the input and the network has a tree\ntopology, or bounded maximum degree (has a tree-like topology, i.e., bounded\ntreewidth resp.). We complement these results by showing that it is highly\nunlikely to construct efficient algorithms when considering the number of\nagents as part of the input, even if the makespan is $3$ and the communication\nrange is $1$."}
{"id": "2409.05030", "pdf": "https://arxiv.org/pdf/2409.05030", "abs": "https://arxiv.org/abs/2409.05030", "authors": ["Ronald Katende"], "title": "Unified theoretical guarantees for stability, consistency, and convergence in neural PDE solvers from non-IID data to physics-informed networks", "categories": ["cs.LG", "cs.NA", "math.NA"], "comment": null, "summary": "We establish a unified theoretical framework addressing the stability,\nconsistency, and convergence of neural networks under realistic training\nconditions, specifically, in the presence of non-IID data, geometric\nconstraints, and embedded physical laws. For standard supervised learning with\ndependent data, we derive uniform stability bounds for gradient-based methods\nusing mixing coefficients and dynamic learning rates. In federated learning\nwith heterogeneous data and non-Euclidean parameter spaces, we quantify model\ninconsistency via curvature-aware aggregation and information-theoretic\ndivergence. For Physics-Informed Neural Networks (PINNs), we rigorously prove\nperturbation stability, residual consistency, Sobolev convergence, energy\nstability for conservation laws, and convergence under adaptive multi-domain\nrefinements. Each result is grounded in variational analysis, compactness\narguments, and universal approximation theorems in Sobolev spaces. Our\ntheoretical guarantees are validated across parabolic, elliptic, and hyperbolic\nPDEs, confirming that residual minimization aligns with physical solution\naccuracy. This work offers a mathematically principled basis for designing\nrobust, generalizable, and physically coherent neural architectures across\ndiverse learning environments."}
{"id": "2502.11471", "pdf": "https://arxiv.org/pdf/2502.11471", "abs": "https://arxiv.org/abs/2502.11471", "authors": ["Kangyang Luo", "Yuzhuo Bai", "Cheng Gao", "Shuzheng Si", "Yingli Shen", "Zhu Liu", "Zhitong Wang", "Cunliang Kong", "Wenhao Li", "Yufei Huang", "Ye Tian", "Xuantang Xiong", "Lei Han", "Maosong Sun"], "title": "GLTW: Joint Improved Graph Transformer and LLM via Three-Word Language for Knowledge Graph Completion", "categories": ["cs.CL", "cs.IR"], "comment": "Accepted by ACL2025(Findings)", "summary": "Knowledge Graph Completion (KGC), which aims to infer missing or incomplete\nfacts, is a crucial task for KGs. However, integrating the vital structural\ninformation of KGs into Large Language Models (LLMs) and outputting predictions\ndeterministically remains challenging. To address this, we propose a new method\ncalled GLTW, which encodes the structural information of KGs and merges it with\nLLMs to enhance KGC performance. Specifically, we introduce an improved Graph\nTransformer (iGT) that effectively encodes subgraphs with both local and global\nstructural information and inherits the characteristics of language model,\nbypassing training from scratch. Also, we develop a subgraph-based\nmulti-classification training objective, using all entities within KG as\nclassification objects, to boost learning efficiency.Importantly, we combine\niGT with an LLM that takes KG language prompts as input.Our extensive\nexperiments on various KG datasets show that GLTW achieves significant\nperformance gains compared to SOTA baselines."}
{"id": "2504.01014", "pdf": "https://arxiv.org/pdf/2504.01014", "abs": "https://arxiv.org/abs/2504.01014", "authors": ["Junhao Cheng", "Yuying Ge", "Yixiao Ge", "Jing Liao", "Ying Shan"], "title": "AnimeGamer: Infinite Anime Life Simulation with Next Game State Prediction", "categories": ["cs.CV"], "comment": "Project released at: https://howe125.github.io/AnimeGamer.github.io/", "summary": "Recent advancements in image and video synthesis have opened up new promise\nin generative games. One particularly intriguing application is transforming\ncharacters from anime films into interactive, playable entities. This allows\nplayers to immerse themselves in the dynamic anime world as their favorite\ncharacters for life simulation through language instructions. Such games are\ndefined as infinite game since they eliminate predetermined boundaries and\nfixed gameplay rules, where players can interact with the game world through\nopen-ended language and experience ever-evolving storylines and environments.\nRecently, a pioneering approach for infinite anime life simulation employs\nlarge language models (LLMs) to translate multi-turn text dialogues into\nlanguage instructions for image generation. However, it neglects historical\nvisual context, leading to inconsistent gameplay. Furthermore, it only\ngenerates static images, failing to incorporate the dynamics necessary for an\nengaging gaming experience. In this work, we propose AnimeGamer, which is built\nupon Multimodal Large Language Models (MLLMs) to generate each game state,\nincluding dynamic animation shots that depict character movements and updates\nto character states, as illustrated in Figure 1. We introduce novel\naction-aware multimodal representations to represent animation shots, which can\nbe decoded into high-quality video clips using a video diffusion model. By\ntaking historical animation shot representations as context and predicting\nsubsequent representations, AnimeGamer can generate games with contextual\nconsistency and satisfactory dynamics. Extensive evaluations using both\nautomated metrics and human evaluations demonstrate that AnimeGamer outperforms\nexisting methods in various aspects of the gaming experience. Codes and\ncheckpoints are available at https://github.com/TencentARC/AnimeGamer."}
{"id": "2412.16772", "pdf": "https://arxiv.org/pdf/2412.16772", "abs": "https://arxiv.org/abs/2412.16772", "authors": ["Ivan Zakazov", "Mikolaj Boronski", "Lorenzo Drudi", "Robert West"], "title": "Assessing Social Alignment: Do Personality-Prompted Large Language Models Behave Like Humans?", "categories": ["cs.CY", "cs.AI", "cs.LG"], "comment": "Accepted to NeurIPS 2024 Workshop on Behavioral Machine Learning", "summary": "The ongoing revolution in language modeling has led to various novel\napplications, some of which rely on the emerging social abilities of large\nlanguage models (LLMs). Already, many turn to the new cyber friends for advice\nduring the pivotal moments of their lives and trust them with the deepest\nsecrets, implying that accurate shaping of the LLM's personality is paramount.\nTo this end, state-of-the-art approaches exploit a vast variety of training\ndata, and prompt the model to adopt a particular personality. We ask (i) if\npersonality-prompted models behave (i.e., make decisions when presented with a\nsocial situation) in line with the ascribed personality (ii) if their behavior\ncan be finely controlled. We use classic psychological experiments, the Milgram\nexperiment and the Ultimatum Game, as social interaction testbeds and apply\npersonality prompting to open- and closed-source LLMs from 4 different vendors.\nOur experiments reveal failure modes of the prompt-based modulation of the\nmodels' behavior that are shared across all models tested and persist under\nprompt perturbations. These findings challenge the optimistic sentiment toward\npersonality prompting generally held in the community."}
{"id": "2410.03782", "pdf": "https://arxiv.org/pdf/2410.03782", "abs": "https://arxiv.org/abs/2410.03782", "authors": ["Changdae Oh", "Yixuan Li", "Kyungwoo Song", "Sangdoo Yun", "Dongyoon Han"], "title": "DaWin: Training-free Dynamic Weight Interpolation for Robust Adaptation", "categories": ["cs.LG", "cs.CV"], "comment": "ICLR 2025 camera-ready; typo-fixed", "summary": "Adapting a pre-trained foundation model on downstream tasks should ensure\nrobustness against distribution shifts without the need to retrain the whole\nmodel. Although existing weight interpolation methods are simple yet effective,\nwe argue that their static nature limits downstream performance while achieving\nefficiency. In this work, we propose DaWin, a training-free dynamic weight\ninterpolation method that leverages the entropy of individual models over each\nunlabeled test sample to assess model expertise, and compute per-sample\ninterpolation coefficients dynamically. Unlike previous works that typically\nrely on additional training to learn such coefficients, our approach requires\nno training. Then, we propose a mixture modeling approach that greatly reduces\ninference overhead raised by dynamic interpolation. We validate DaWin on the\nlarge-scale visual recognition benchmarks, spanning 14 tasks across robust\nfine-tuning -- ImageNet and derived five distribution shift benchmarks -- and\nmulti-task learning with eight classification tasks. Results demonstrate that\nDaWin achieves significant performance gain in considered settings, with\nminimal computational overhead. We further discuss DaWin's analytic behavior to\nexplain its empirical success."}
{"id": "2502.11541", "pdf": "https://arxiv.org/pdf/2502.11541", "abs": "https://arxiv.org/abs/2502.11541", "authors": ["Hui Huang", "Jiaheng Liu", "Yancheng He", "Shilong Li", "Bing Xu", "Conghui Zhu", "Muyun Yang", "Tiejun Zhao"], "title": "MuSC: Improving Complex Instruction Following with Multi-granularity Self-Contrastive Training", "categories": ["cs.CL", "cs.AI"], "comment": "Accepted to ACL2025", "summary": "Complex instruction-following with elaborate constraints is imperative for\nLarge Language Models (LLMs). While existing methods have constructed data for\ncomplex instruction alignment, they all rely on a more advanced model,\nespecially GPT-4, limiting their application. In this paper, we propose a\nMulti-granularity Self-Contrastive Training (MuSC) framework, to improve the\ncomplex instruction alignment without relying on a stronger model. Our method\nis conducted on both coarse and fine granularity. On coarse-granularity, we\nconstruct constraint-aware preference data based on instruction decomposition\nand recombination. On fine-granularity, we perform token-aware preference\noptimization with dynamic token-level supervision. Our method is evaluated on\nopen-sourced models, and experiment results show our method achieves\nsignificant improvement on both complex and general instruction-following\nbenchmarks, surpassing previous self-alignment methods."}
{"id": "2504.03169", "pdf": "https://arxiv.org/pdf/2504.03169", "abs": "https://arxiv.org/abs/2504.03169", "authors": ["Shabnam Choudhury", "Yash Salunkhe", "Sarthak Mehrotra", "Biplab Banerjee"], "title": "REJEPA: A Novel Joint-Embedding Predictive Architecture for Efficient Remote Sensing Image Retrieval", "categories": ["cs.CV"], "comment": "14 pages", "summary": "The rapid expansion of remote sensing image archives demands the development\nof strong and efficient techniques for content-based image retrieval (RS-CBIR).\nThis paper presents REJEPA (Retrieval with Joint-Embedding Predictive\nArchitecture), an innovative self-supervised framework designed for unimodal\nRS-CBIR. REJEPA utilises spatially distributed context token encoding to\nforecast abstract representations of target tokens, effectively capturing\nhigh-level semantic features and eliminating unnecessary pixel-level details.\nIn contrast to generative methods that focus on pixel reconstruction or\ncontrastive techniques that depend on negative pairs, REJEPA functions within\nfeature space, achieving a reduction in computational complexity of 40-60% when\ncompared to pixel-reconstruction baselines like Masked Autoencoders (MAE). To\nguarantee strong and varied representations, REJEPA incorporates\nVariance-Invariance-Covariance Regularisation (VICReg), which prevents encoder\ncollapse by promoting feature diversity and reducing redundancy. The method\ndemonstrates an estimated enhancement in retrieval accuracy of 5.1% on BEN-14K\n(S1), 7.4% on BEN-14K (S2), 6.0% on FMoW-RGB, and 10.1% on FMoW-Sentinel\ncompared to prominent SSL techniques, including CSMAE-SESD, Mask-VLM, SatMAE,\nScaleMAE, and SatMAE++, on extensive RS benchmarks BEN-14K (multispectral and\nSAR data), FMoW-RGB and FMoW-Sentinel. Through effective generalisation across\nsensor modalities, REJEPA establishes itself as a sensor-agnostic benchmark for\nefficient, scalable, and precise RS-CBIR, addressing challenges like varying\nresolutions, high object density, and complex backgrounds with computational\nefficiency."}
{"id": "2412.18407", "pdf": "https://arxiv.org/pdf/2412.18407", "abs": "https://arxiv.org/abs/2412.18407", "authors": ["Siavash Ameli", "Siyuan Zhuang", "Ion Stoica", "Michael W. Mahoney"], "title": "A Statistical Framework for Ranking LLM-Based Chatbots", "categories": ["stat.ML", "cs.AI", "cs.LG"], "comment": null, "summary": "Large language models (LLMs) have transformed natural language processing,\nwith frameworks like Chatbot Arena providing pioneering platforms for\nevaluating these models. By facilitating millions of pairwise comparisons based\non human judgments, Chatbot Arena has become a cornerstone in LLM evaluation,\noffering rich datasets for ranking models in open-ended conversational tasks.\nBuilding upon this foundation, we propose a statistical framework that\nincorporates key advancements to address specific challenges in pairwise\ncomparison analysis. First, we introduce a factored tie model that enhances the\nability to handle ties -- an integral aspect of human-judged comparisons --\nsignificantly improving the model's fit to observed data. Second, we extend the\nframework to model covariance between competitors, enabling deeper insights\ninto performance relationships and facilitating intuitive groupings into\nperformance tiers. Third, we resolve optimization challenges arising from\nparameter non-uniqueness by introducing novel constraints, ensuring stable and\ninterpretable parameter estimation. Through rigorous evaluation and extensive\nexperimentation, our framework demonstrates substantial improvements over\nexisting methods in modeling pairwise comparison data. To support\nreproducibility and practical adoption, we release leaderbot, an open-source\nPython package implementing our models and analyses."}
{"id": "2410.07704", "pdf": "https://arxiv.org/pdf/2410.07704", "abs": "https://arxiv.org/abs/2410.07704", "authors": ["Michael Sucker", "Peter Ochs"], "title": "A Generalization Result for Convergence in Learning-to-Optimize", "categories": ["cs.LG", "math.OC", "math.PR"], "comment": "spotlight at ICML 2025", "summary": "Learning-to-optimize leverages machine learning to accelerate optimization\nalgorithms. While empirical results show tremendous improvements compared to\nclassical optimization algorithms, theoretical guarantees are mostly lacking,\nsuch that the outcome cannot be reliably assured. Especially, convergence is\nhardly studied in learning-to-optimize, because conventional convergence\nguarantees in optimization are based on geometric arguments, which cannot be\napplied easily to learned algorithms. Thus, we develop a probabilistic\nframework that resembles classical optimization and allows for transferring\ngeometric arguments into learning-to-optimize. Based on our new proof-strategy,\nour main theorem is a generalization result for parametric classes of\npotentially non-smooth, non-convex loss functions and establishes the\nconvergence of learned optimization algorithms to critical points with high\nprobability. This effectively generalizes the results of a worst-case analysis\ninto a probabilistic framework, and frees the design of the learned algorithm\nfrom using safeguards."}
{"id": "2502.11718", "pdf": "https://arxiv.org/pdf/2502.11718", "abs": "https://arxiv.org/abs/2502.11718", "authors": ["Jihao Gu", "Yingyao Wang", "Pi Bu", "Chen Wang", "Ziming Wang", "Tengtao Song", "Donglai Wei", "Jiale Yuan", "Yingxiu Zhao", "Yancheng He", "Shilong Li", "Jiaheng Liu", "Meng Cao", "Jun Song", "Yingshui Tan", "Xiang Li", "Wenbo Su", "Zhicheng Zheng", "Xiaoyong Zhu", "Bo Zheng"], "title": "\"See the World, Discover Knowledge\": A Chinese Factuality Evaluation for Large Vision Language Models", "categories": ["cs.CL", "cs.CV"], "comment": "26 pages, 21 figures", "summary": "The evaluation of factual accuracy in large vision language models (LVLMs)\nhas lagged behind their rapid development, making it challenging to fully\nreflect these models' knowledge capacity and reliability. In this paper, we\nintroduce the first factuality-based visual question-answering benchmark in\nChinese, named ChineseSimpleVQA, aimed at assessing the visual factuality of\nLVLMs across 8 major topics and 56 subtopics. The key features of this\nbenchmark include a focus on the Chinese language, diverse knowledge types, a\nmulti-hop question construction, high-quality data, static consistency, and\neasy-to-evaluate through short answers. Moreover, we contribute a rigorous data\nconstruction pipeline and decouple the visual factuality into two parts: seeing\nthe world (i.e., object recognition) and discovering knowledge. This decoupling\nallows us to analyze the capability boundaries and execution mechanisms of\nLVLMs. Subsequently, we evaluate 34 advanced open-source and closed-source\nmodels, revealing critical performance gaps within this field. Our\nevaluation-friendly code and data have already been open-sourced."}
{"id": "2504.07934", "pdf": "https://arxiv.org/pdf/2504.07934", "abs": "https://arxiv.org/abs/2504.07934", "authors": ["Xiyao Wang", "Zhengyuan Yang", "Chao Feng", "Hongjin Lu", "Linjie Li", "Chung-Ching Lin", "Kevin Lin", "Furong Huang", "Lijuan Wang"], "title": "SoTA with Less: MCTS-Guided Sample Selection for Data-Efficient Visual Reasoning Self-Improvement", "categories": ["cs.CV"], "comment": "27 pages, 5 figures", "summary": "We introduce ThinkLite-VL, a family of visual reasoning models that achieve\nstate-of-the-art (SoTA) performance using an order of magnitude fewer training\nsamples, relying purely on reinforcement fine-tuning (RFT) self-improvement\nwithout any knowledge distillation. Our central insight is that sample\ndifficulty critically influences RFT effectiveness: appropriately challenging\nexamples can drive substantial reasoning improvements, even in low-data\nregimes. However, quantifying sample difficulty in a reliable and scalable\nmanner remains non-trivial. To address this, we repurpose Monte Carlo Tree\nSearch (MCTS) to measure sample difficulty via the number of reasoning\niterations a vision-language model (VLM) requires to solve each instance. This\nMCTS-based selection procedure identifies samples that induce deeper reasoning\nwhile remaining solvable, allowing us to filter a high-quality subset from 70k\nopen-source examples spanning math, natural image understanding, and chart\ncomprehension. Using this approach, we select just 11k challenging samples for\nRFT on Qwen2.5-VL-7B-Instruct and 7.5k samples for Qwen2.5-VL-72B-Instruct. The\nresulting models, ThinkLite-VL-7B and ThinkLite-VL-72B, significantly\noutperform their respective base models across eight visual reasoning\nbenchmarks. In particular, ThinkLite-VL-7B improves the average performance of\nQwen2.5-VL-7B-Instruct by 7\\% and surpasses all existing 7B-level models, as\nwell as much larger models such as GPT-4o, O1 and Qwen2.5-VL-72B, achieving a\nnew SoTA score of 75.1 on MathVista. ThinkLite-VL-72B further advances the SoTA\nfrontier, achieving an accuracy of 79.7 on MathVista and an average benchmark\nimprovement of 4.42 over the open-source SOTA. These results demonstrate that\nMCTS-guided difficulty filtering provides a scalable and effective path toward\ndata-efficient self-improvement in multimodal reasoning."}
{"id": "2502.02017", "pdf": "https://arxiv.org/pdf/2502.02017", "abs": "https://arxiv.org/abs/2502.02017", "authors": ["Shuo Wang", "Bokui Wang", "Zhixiang Shen", "Boyan Deng", "Zhao Kang"], "title": "Multi-Domain Graph Foundation Models: Robust Knowledge Transfer via Topology Alignment", "categories": ["cs.SI", "cs.AI", "cs.LG"], "comment": null, "summary": "Recent advances in CV and NLP have inspired researchers to develop\ngeneral-purpose graph foundation models through pre-training across diverse\ndomains. However, a fundamental challenge arises from the substantial\ndifferences in graph topologies across domains. Additionally, real-world graphs\nare often sparse and prone to noisy connections and adversarial attacks. To\naddress these issues, we propose the Multi-Domain Graph Foundation Model\n(MDGFM), a unified framework that aligns and leverages cross-domain topological\ninformation to facilitate robust knowledge transfer. MDGFM bridges different\ndomains by adaptively balancing features and topology while refining original\ngraphs to eliminate noise and align topological structures. To further enhance\nknowledge transfer, we introduce an efficient prompt-tuning approach. By\naligning topologies, MDGFM not only improves multi-domain pre-training but also\nenables robust knowledge transfer to unseen domains. Theoretical analyses\nprovide guarantees of MDGFM's effectiveness and domain generalization\ncapabilities. Extensive experiments on both homophilic and heterophilic graph\ndatasets validate the robustness and efficacy of our method."}
{"id": "2410.16222", "pdf": "https://arxiv.org/pdf/2410.16222", "abs": "https://arxiv.org/abs/2410.16222", "authors": ["Valentyn Boreiko", "Alexander Panfilov", "Vaclav Voracek", "Matthias Hein", "Jonas Geiping"], "title": "An Interpretable N-gram Perplexity Threat Model for Large Language Model Jailbreaks", "categories": ["cs.LG"], "comment": null, "summary": "A plethora of jailbreaking attacks have been proposed to obtain harmful\nresponses from safety-tuned LLMs. These methods largely succeed in coercing the\ntarget output in their original settings, but their attacks vary substantially\nin fluency and computational effort. In this work, we propose a unified threat\nmodel for the principled comparison of these methods. Our threat model checks\nif a given jailbreak is likely to occur in the distribution of text. For this,\nwe build an N-gram language model on 1T tokens, which, unlike model-based\nperplexity, allows for an LLM-agnostic, nonparametric, and inherently\ninterpretable evaluation. We adapt popular attacks to this threat model, and,\nfor the first time, benchmark these attacks on equal footing with it. After an\nextensive comparison, we find attack success rates against safety-tuned modern\nmodels to be lower than previously presented and that attacks based on discrete\noptimization significantly outperform recent LLM-based attacks. Being\ninherently interpretable, our threat model allows for a comprehensive analysis\nand comparison of jailbreak attacks. We find that effective attacks exploit and\nabuse infrequent bigrams, either selecting the ones absent from real-world text\nor rare ones, e.g., specific to Reddit or code datasets."}
{"id": "2502.12476", "pdf": "https://arxiv.org/pdf/2502.12476", "abs": "https://arxiv.org/abs/2502.12476", "authors": ["Elnaz Rahmati", "Alireza S. Ziabari", "Morteza Dehghani"], "title": "CoCo-CoLa: Evaluating and Improving Language Adherence in Multilingual LLMs", "categories": ["cs.CL"], "comment": "26 pages, 7 figures", "summary": "Multilingual Large Language Models (LLMs) develop cross-lingual abilities\ndespite being trained on limited parallel data. However, they often struggle to\ngenerate responses in the intended language, favoring high-resource languages\nsuch as English. In this work, we introduce CoCo-CoLa (Correct Concept -\nCorrect Language), a novel metric to evaluate language adherence in\nmultilingual LLMs. Using fine-tuning experiments on a closed-book QA task\nacross seven languages, we analyze how training in one language affects others'\nperformance. Our findings reveal that multilingual models share task knowledge\nacross languages but exhibit biases in the selection of output language. We\nidentify language-specific layers, showing that final layers play a crucial\nrole in determining output language. Accordingly, we propose a partial training\nstrategy that selectively fine-tunes key layers, improving language adherence\nwhile significantly reducing computational cost. Our method achieves comparable\nor superior performance to full fine-tuning, particularly for low-resource\nlanguages, offering a more efficient multilingual adaptation."}
{"id": "2504.11366", "pdf": "https://arxiv.org/pdf/2504.11366", "abs": "https://arxiv.org/abs/2504.11366", "authors": ["Hasan Wehbi", "Hasan Nasrallah", "Mohamad Hasan Zahweh", "Zeinab Takach", "Veera Ganesh Yalla", "Ali J. Ghandour"], "title": "A Decade of Wheat Mapping for Lebanon", "categories": ["cs.CV"], "comment": null, "summary": "Wheat accounts for approximately 20% of the world's caloric intake, making it\na vital component of global food security. Given this importance, mapping wheat\nfields plays a crucial role in enabling various stakeholders, including policy\nmakers, researchers, and agricultural organizations, to make informed decisions\nregarding food security, supply chain management, and resource allocation. In\nthis paper, we tackle the problem of accurately mapping wheat fields out of\nsatellite images by introducing an improved pipeline for winter wheat\nsegmentation, as well as presenting a case study on a decade-long analysis of\nwheat mapping in Lebanon. We integrate a Temporal Spatial Vision Transformer\n(TSViT) with Parameter-Efficient Fine Tuning (PEFT) and a novel post-processing\npipeline based on the Fields of The World (FTW) framework. Our proposed\npipeline addresses key challenges encountered in existing approaches, such as\nthe clustering of small agricultural parcels in a single large field. By\nmerging wheat segmentation with precise field boundary extraction, our method\nproduces geometrically coherent and semantically rich maps that enable us to\nperform in-depth analysis such as tracking crop rotation pattern over years.\nExtensive evaluations demonstrate improved boundary delineation and field-level\nprecision, establishing the potential of the proposed framework in operational\nagricultural monitoring and historical trend analysis. By allowing for accurate\nmapping of wheat fields, this work lays the foundation for a range of critical\nstudies and future advances, including crop monitoring and yield estimation."}
{"id": "2502.03503", "pdf": "https://arxiv.org/pdf/2502.03503", "abs": "https://arxiv.org/abs/2502.03503", "authors": ["Omar Naim", "Nicholas Asher"], "title": "Analyzing limits for in-context learning", "categories": ["stat.ML", "cs.AI", "cs.LG"], "comment": null, "summary": "We examine limits of in-context learning (ICL) in transformer models trained\nfrom scratch, focusing on function approximation tasks as a controlled setting\nto uncover fundamental behaviors. While we show empirically that transformer\nmodels can generalize, approximating unseen classes of polynomial (non linear)\nfunctions, they cannot generalize beyond certain values. We provide both\nempirical and mathematical arguments explaining that these limitations stem\nfrom architectural components, namely layer normalization and the attention\nscoring function, softmax. Together, our findings reveal structural constraints\non ICL that are often masked in more complex NLP tasks but that need to be\nunderstood to improve robustness and interpretability in transformer-based\nmodels."}
{"id": "2501.16178", "pdf": "https://arxiv.org/pdf/2501.16178", "abs": "https://arxiv.org/abs/2501.16178", "authors": ["Wenxuan Xie", "Fanpu Cao"], "title": "SWIFT: Mapping Sub-series with Wavelet Decomposition Improves Time Series Forecasting", "categories": ["cs.LG", "stat.ML"], "comment": null, "summary": "In recent work on time-series prediction, Transformers and even large\nlanguage models have garnered significant attention due to their strong\ncapabilities in sequence modeling. However, in practical deployments,\ntime-series prediction often requires operation in resource-constrained\nenvironments, such as edge devices, which are unable to handle the\ncomputational overhead of large models. To address such scenarios, some\nlightweight models have been proposed, but they exhibit poor performance on\nnon-stationary sequences. In this paper, we propose $\\textit{SWIFT}$, a\nlightweight model that is not only powerful, but also efficient in deployment\nand inference for Long-term Time Series Forecasting (LTSF). Our model is based\non three key points: (i) Utilizing wavelet transform to perform lossless\ndownsampling of time series. (ii) Achieving cross-band information fusion with\na learnable filter. (iii) Using only one shared linear layer or one shallow MLP\nfor sub-series' mapping. We conduct comprehensive experiments, and the results\nshow that $\\textit{SWIFT}$ achieves state-of-the-art (SOTA) performance on\nmultiple datasets, offering a promising method for edge computing and\ndeployment in this task. Moreover, it is noteworthy that the number of\nparameters in $\\textit{SWIFT-Linear}$ is only 25\\% of what it would be with a\nsingle-layer linear model for time-domain prediction. Our code is available at\nhttps://github.com/LancelotXWX/SWIFT."}
{"id": "2502.14494", "pdf": "https://arxiv.org/pdf/2502.14494", "abs": "https://arxiv.org/abs/2502.14494", "authors": ["Jinnan Li", "Jinzhe Li", "Yue Wang", "Yi Chang", "Yuan Wu"], "title": "StructFlowBench: A Structured Flow Benchmark for Multi-turn Instruction Following", "categories": ["cs.CL"], "comment": "ACL 2025 Findings camera-ready version", "summary": "Multi-turn instruction following capability constitutes a core competency of\nlarge language models (LLMs) in real-world applications. Existing evaluation\nbenchmarks predominantly focus on fine-grained constraint satisfaction and\ndomain-specific capability assessment, yet overlook the crucial structural\ndependencies between dialogue turns that distinguish multi-turn from\nsingle-turn interactions. These structural dependencies not only reflect user\nintent but also establish an essential second dimension for the instruction\nfollowing evaluation beyond constraint satisfaction. To address this gap, we\npropose StructFlowBench, a multi-turn instruction following benchmark with\nstructural flow modeling. The benchmark defines an innovative structural flow\nframework with six fundamental inter-turn relationships. These relationships\nintroduce novel structural constraints for model evaluation and also serve as\ngeneration parameters for creating customized dialogue flows tailored to\nspecific scenarios. Adopting established LLM-based automatic evaluation\nmethodologies, we conduct systematic evaluations of 13 leading open-source and\nclosed-source LLMs. Experimental results reveal significant deficiencies in\ncurrent models' comprehension of multi-turn dialogue structures. The code is\navailable at https://github.com/MLGroupJLU/StructFlowBench."}
{"id": "2504.15085", "pdf": "https://arxiv.org/pdf/2504.15085", "abs": "https://arxiv.org/abs/2504.15085", "authors": ["Wangyu Wu", "Zhenhong Chen", "Siqi Song", "Xianglin Qiu", "Xiaowei Huang", "Fei Ma", "Jimin Xiao"], "title": "Hierarchical Attention Fusion of Visual and Textual Representations for Cross-Domain Sequential Recommendation", "categories": ["cs.CV"], "comment": "Accepted at CogSCI 2025. arXiv admin note: text overlap with\n  arXiv:2502.15694", "summary": "Cross-Domain Sequential Recommendation (CDSR) predicts user behavior by\nleveraging historical interactions across multiple domains, focusing on\nmodeling cross-domain preferences through intra- and inter-sequence item\nrelationships. Inspired by human cognitive processes, we propose Hierarchical\nAttention Fusion of Visual and Textual Representations (HAF-VT), a novel\napproach integrating visual and textual data to enhance cognitive modeling.\nUsing the frozen CLIP model, we generate image and text embeddings, enriching\nitem representations with multimodal data. A hierarchical attention mechanism\njointly learns single-domain and cross-domain preferences, mimicking human\ninformation integration. Evaluated on four e-commerce datasets, HAF-VT\noutperforms existing methods in capturing cross-domain user interests, bridging\ncognitive principles with computational models and highlighting the role of\nmultimodal data in sequential decision-making."}
{"id": "2502.03773", "pdf": "https://arxiv.org/pdf/2502.03773", "abs": "https://arxiv.org/abs/2502.03773", "authors": ["Chhavi Yadav", "Evan Monroe Laufer", "Dan Boneh", "Kamalika Chaudhuri"], "title": "ExpProof : Operationalizing Explanations for Confidential Models with ZKPs", "categories": ["cs.LG", "cs.AI", "cs.CR"], "comment": null, "summary": "In principle, explanations are intended as a way to increase trust in machine\nlearning models and are often obligated by regulations. However, many\ncircumstances where these are demanded are adversarial in nature, meaning the\ninvolved parties have misaligned interests and are incentivized to manipulate\nexplanations for their purpose. As a result, explainability methods fail to be\noperational in such settings despite the demand \\cite{bordt2022post}. In this\npaper, we take a step towards operationalizing explanations in adversarial\nscenarios with Zero-Knowledge Proofs (ZKPs), a cryptographic primitive.\nSpecifically we explore ZKP-amenable versions of the popular explainability\nalgorithm LIME and evaluate their performance on Neural Networks and Random\nForests. Our code is publicly available at\nhttps://github.com/emlaufer/ExpProof."}
{"id": "2501.19374", "pdf": "https://arxiv.org/pdf/2501.19374", "abs": "https://arxiv.org/abs/2501.19374", "authors": ["Christopher Subich", "Syed Zahid Husain", "Leo Separovic", "Jing Yang"], "title": "Fixing the Double Penalty in Data-Driven Weather Forecasting Through a Modified Spherical Harmonic Loss Function", "categories": ["cs.LG", "physics.ao-ph", "I.2.6; I.2.1; J.2"], "comment": "Accepted at ICML 2025", "summary": "Recent advancements in data-driven weather forecasting models have delivered\ndeterministic models that outperform the leading operational forecast systems\nbased on traditional, physics-based models. However, these data-driven models\nare typically trained with a mean squared error loss function, which causes\nsmoothing of fine scales through a \"double penalty\" effect. We develop a\nsimple, parameter-free modification to this loss function that avoids this\nproblem by separating the loss attributable to decorrelation from the loss\nattributable to spectral amplitude errors. Fine-tuning the GraphCast model with\nthis new loss function results in sharp deterministic weather forecasts, an\nincrease of the model's effective resolution from 1,250km to 160km,\nimprovements to ensemble spread, and improvements to predictions of tropical\ncyclone strength and surface wind extremes."}
{"id": "2502.14561", "pdf": "https://arxiv.org/pdf/2502.14561", "abs": "https://arxiv.org/abs/2502.14561", "authors": ["Paris Koloveas", "Serafeim Chatzopoulos", "Thanasis Vergoulis", "Christos Tryfonopoulos"], "title": "Can LLMs Predict Citation Intent? An Experimental Analysis of In-context Learning and Fine-tuning on Open LLMs", "categories": ["cs.CL", "cs.DL"], "comment": null, "summary": "This work investigates the ability of open Large Language Models (LLMs) to\npredict citation intent through in-context learning and fine-tuning. Unlike\ntraditional approaches relying on domain-specific pre-trained models like\nSciBERT, we demonstrate that general-purpose LLMs can be adapted to this task\nwith minimal task-specific data. We evaluate twelve model variations across\nfive prominent open LLM families using zero-, one-, few-, and many-shot\nprompting. Our experimental study identifies the top-performing model and\nprompting parameters through extensive in-context learning experiments. We then\ndemonstrate the significant impact of task-specific adaptation by fine-tuning\nthis model, achieving a relative F1-score improvement of 8% on the SciCite\ndataset and 4.3% on the ACL-ARC dataset compared to the instruction-tuned\nbaseline. These findings provide valuable insights for model selection and\nprompt engineering. Additionally, we make our end-to-end evaluation framework\nand models openly available for future use."}
{"id": "2504.16181", "pdf": "https://arxiv.org/pdf/2504.16181", "abs": "https://arxiv.org/abs/2504.16181", "authors": ["Banafsheh Karimian", "Giulia Avanzato", "Soufian Belharbi", "Luke McCaffrey", "Mohammadhadi Shateri", "Eric Granger"], "title": "CLIP-IT: CLIP-based Pairing for Histology Images Classification", "categories": ["cs.CV"], "comment": null, "summary": "Multimodal learning has shown significant promise for improving medical image\nanalysis by integrating information from complementary data sources. This is\nwidely employed for training vision-language models (VLMs) for cancer detection\nbased on histology images and text reports. However, one of the main\nlimitations in training these VLMs is the requirement for large paired\ndatasets, raising concerns over privacy, and data collection, annotation, and\nmaintenance costs. To address this challenge, we introduce CLIP-IT method to\ntrain a vision backbone model to classify histology images by pairing them with\nprivileged textual information from an external source. At first, the modality\npairing step relies on a CLIP-based model to match histology images with\nsemantically relevant textual report data from external sources, creating an\naugmented multimodal dataset without the need for manually paired samples.\nThen, we propose a multimodal training procedure that distills the knowledge\nfrom the paired text modality to the unimodal image classifier for enhanced\nperformance without the need for the textual data during inference. A\nparameter-efficient fine-tuning method is used to efficiently address the\nmisalignment between the main (image) and paired (text) modalities. During\ninference, the improved unimodal histology classifier is used, with only\nminimal additional computational complexity. Our experiments on challenging\nPCAM, CRC, and BACH histology image datasets show that CLIP-IT can provide a\ncost-effective approach to leverage privileged textual information and\noutperform unimodal classifiers for histology."}
{"id": "2502.04040", "pdf": "https://arxiv.org/pdf/2502.04040", "abs": "https://arxiv.org/abs/2502.04040", "authors": ["Haoyu Wang", "Zeyu Qin", "Li Shen", "Xueqian Wang", "Dacheng Tao", "Minhao Cheng"], "title": "Safety Reasoning with Guidelines", "categories": ["cs.LG", "cs.AI", "cs.CL"], "comment": "ICML 2025 paper. The first two authors contributed equally", "summary": "Training safe LLMs remains a critical challenge. The most widely used method,\nRefusal Training (RT), struggles to generalize against various\nOut-of-Distribution (OOD) jailbreaking attacks. Although various advanced\nmethods have been proposed to address this issue, we instead question whether\nOOD attacks inherently surpass the capability of vanilla RT. Evaluations using\nBest-of-N (BoN) reveal significant safety improvements as N increases,\nindicating models possess adequate latent safety knowledge but RT fails to\nconsistently elicit it under OOD scenarios. Further domain adaptation analysis\nreveals that direct RT causes reliance on superficial shortcuts, resulting in\nnon-generalizable representation mappings. Inspired by our findings, we propose\ntraining model to perform safety reasoning for each query. Specifically, we\nsynthesize reasoning supervision aligned with specified guidelines that reflect\ndiverse perspectives on safety knowledge. This encourages model to engage in\ndeeper reasoning, explicitly eliciting and utilizing latent safety knowledge\nfor each query. Extensive experiments show that our method significantly\nimproves model generalization against OOD attacks."}
{"id": "2502.00816", "pdf": "https://arxiv.org/pdf/2502.00816", "abs": "https://arxiv.org/abs/2502.00816", "authors": ["Yong Liu", "Guo Qin", "Zhiyuan Shi", "Zhi Chen", "Caiyin Yang", "Xiangdong Huang", "Jianmin Wang", "Mingsheng Long"], "title": "Sundial: A Family of Highly Capable Time Series Foundation Models", "categories": ["cs.LG"], "comment": null, "summary": "We introduce Sundial, a family of native, flexible, and scalable time series\nfoundation models. To predict the next-patch's distribution, we propose a\nTimeFlow Loss based on flow-matching, which facilitates native pre-training of\nTransformers on continuous-valued time series without discrete tokenization.\nConditioned on arbitrary-length time series, our models are pre-trained without\nspecifying any prior distribution and can generate multiple probable\npredictions, achieving more flexibility in representation learning than using\nparametric densities. Towards time series foundation models, we leverage\nminimal but crucial adaptations of Transformers and curate TimeBench with one\ntrillion time points, comprising mostly real-world datasets and synthetic data.\nBy mitigating mode collapse via TimeFlow Loss, we pre-train a family of Sundial\nmodels on TimeBench, which achieve unprecedented model capacity and\ngeneralization performance. In addition to excellent scalability, Sundial\nachieves state-of-the-art results on both point and probabilistic forecasting\nbenchmarks with a just-in-time inference speed, i.e., making zero-shot\npredictions within a few milliseconds. We believe that Sundial's pioneering\ngenerative forecasting capability can improve model reliability in real-world\ndecision-making. Code is available at: https://github.com/thuml/Sundial."}
{"id": "2502.14662", "pdf": "https://arxiv.org/pdf/2502.14662", "abs": "https://arxiv.org/abs/2502.14662", "authors": ["Wujiang Xu", "Yunxiao Shi", "Zujie Liang", "Xuying Ning", "Kai Mei", "Kun Wang", "Xi Zhu", "Min Xu", "Yongfeng Zhang"], "title": "iAgent: LLM Agent as a Shield between User and Recommender Systems", "categories": ["cs.CL", "cs.IR"], "comment": "Findings of ACL 2025 and WWW2025@HCRS", "summary": "Traditional recommender systems usually take the user-platform paradigm,\nwhere users are directly exposed under the control of the platform's\nrecommendation algorithms. However, the defect of recommendation algorithms may\nput users in very vulnerable positions under this paradigm. First, many\nsophisticated models are often designed with commercial objectives in mind,\nfocusing on the platform's benefits, which may hinder their ability to protect\nand capture users' true interests. Second, these models are typically optimized\nusing data from all users, which may overlook individual user's preferences.\nDue to these shortcomings, users may experience several disadvantages under the\ntraditional user-platform direct exposure paradigm, such as lack of control\nover the recommender system, potential manipulation by the platform, echo\nchamber effects, or lack of personalization for less active users due to the\ndominance of active users during collaborative learning. Therefore, there is an\nurgent need to develop a new paradigm to protect user interests and alleviate\nthese issues. Recently, some researchers have introduced LLM agents to simulate\nuser behaviors, these approaches primarily aim to optimize platform-side\nperformance, leaving core issues in recommender systems unresolved. To address\nthese limitations, we propose a new user-agent-platform paradigm, where agent\nserves as the protective shield between user and recommender system that\nenables indirect exposure."}
{"id": "2504.19581", "pdf": "https://arxiv.org/pdf/2504.19581", "abs": "https://arxiv.org/abs/2504.19581", "authors": ["Chengzhi Wu", "Yuxin Wan", "Hao Fu", "Julius Pfrommer", "Zeyun Zhong", "Junwei Zheng", "Jiaming Zhang", "JÃ¼rgen Beyerer"], "title": "SAMBLE: Shape-Specific Point Cloud Sampling for an Optimal Trade-Off Between Local Detail and Global Uniformity", "categories": ["cs.CV"], "comment": null, "summary": "Driven by the increasing demand for accurate and efficient representation of\n3D data in various domains, point cloud sampling has emerged as a pivotal\nresearch topic in 3D computer vision. Recently, learning-to-sample methods have\ngarnered growing interest from the community, particularly for their ability to\nbe jointly trained with downstream tasks. However, previous learning-based\nsampling methods either lead to unrecognizable sampling patterns by generating\na new point cloud or biased sampled results by focusing excessively on sharp\nedge details. Moreover, they all overlook the natural variations in point\ndistribution across different shapes, applying a similar sampling strategy to\nall point clouds. In this paper, we propose a Sparse Attention Map and\nBin-based Learning method (termed SAMBLE) to learn shape-specific sampling\nstrategies for point cloud shapes. SAMBLE effectively achieves an improved\nbalance between sampling edge points for local details and preserving\nuniformity in the global shape, resulting in superior performance across\nmultiple common point cloud downstream tasks, even in scenarios with few-point\nsampling."}
{"id": "2502.04563", "pdf": "https://arxiv.org/pdf/2502.04563", "abs": "https://arxiv.org/abs/2502.04563", "authors": ["Congjie He", "Yeqi Huang", "Pei Mu", "Ziming Miao", "Jilong Xue", "Lingxiao Ma", "Fan Yang", "Luo Mai"], "title": "WaferLLM: Large Language Model Inference at Wafer Scale", "categories": ["cs.LG", "cs.AI", "cs.AR", "cs.DC", "cs.ET"], "comment": null, "summary": "Emerging AI accelerators increasingly adopt wafer-scale manufacturing\ntechnologies, integrating hundreds of thousands of AI cores in a mesh\narchitecture with large distributed on-chip memory (tens of GB in total) and\nultra-high on-chip memory bandwidth (tens of PB/s). However, current LLM\ninference systems, optimized for shared memory architectures like GPUs, fail to\nexploit these accelerators fully.\n  We introduce WaferLLM, the first wafer-scale LLM inference system. WaferLLM\nis guided by a novel PLMR model (pronounced as \"Plummer\") that captures the\nunique hardware characteristics of wafer-scale architectures. Leveraging this\nmodel, WaferLLM pioneers wafer-scale LLM parallelism, optimizing the\nutilization of hundreds of thousands of on-chip cores. It also introduces\nMeshGEMM and MeshGEMV, the first GEMM and GEMV implementations designed to\nscale effectively on wafer-scale accelerators.\n  Evaluations show that WaferLLM achieves up to 200$\\times$ higher accelerator\nutilization than state-of-the-art methods. Leveraging a wafer-scale accelerator\n(Cerebras WSE2), WaferLLM delivers GEMV operations 606$\\times$ faster and\n16$\\times$ more energy-efficient than on an NVIDIA A100 GPU. For full LLM\ninference, WaferLLM achieves 10-20$\\times$ speedups over A100 GPU clusters\nrunning SGLang and vLLM. These advantages are expected to grow as wafer-scale\nAI models, software, and hardware continue to mature. WaferLLM is open-sourced\nat https://github.com/MeshInfra/WaferLLM."}
{"id": "2502.04959", "pdf": "https://arxiv.org/pdf/2502.04959", "abs": "https://arxiv.org/abs/2502.04959", "authors": ["Daniel Marczak", "Simone Magistri", "Sebastian Cygert", "BartÅomiej Twardowski", "Andrew D. Bagdanov", "Joost van de Weijer"], "title": "No Task Left Behind: Isotropic Model Merging with Common and Task-Specific Subspaces", "categories": ["cs.LG"], "comment": "Accepted at ICML 2025", "summary": "Model merging integrates the weights of multiple task-specific models into a\nsingle multi-task model. Despite recent interest in the problem, a significant\nperformance gap between the combined and single-task models remains. In this\npaper, we investigate the key characteristics of task matrices -- weight update\nmatrices applied to a pre-trained model -- that enable effective merging. We\nshow that alignment between singular components of task-specific and merged\nmatrices strongly correlates with performance improvement over the pre-trained\nmodel. Based on this, we propose an isotropic merging framework that flattens\nthe singular value spectrum of task matrices, enhances alignment, and reduces\nthe performance gap. Additionally, we incorporate both common and task-specific\nsubspaces to further improve alignment and performance. Our proposed approach\nachieves state-of-the-art performance on vision and language tasks across\nvarious sets of tasks and model scales. This work advances the understanding of\nmodel merging dynamics, offering an effective methodology to merge models\nwithout requiring additional training. Code is available at\nhttps://github.com/danielm1405/iso-merging ."}
{"id": "2502.14830", "pdf": "https://arxiv.org/pdf/2502.14830", "abs": "https://arxiv.org/abs/2502.14830", "authors": ["Danni Liu", "Jan Niehues"], "title": "Middle-Layer Representation Alignment for Cross-Lingual Transfer in Fine-Tuned LLMs", "categories": ["cs.CL", "cs.AI"], "comment": "ACL 2025", "summary": "While large language models demonstrate remarkable capabilities at\ntask-specific applications through fine-tuning, extending these benefits across\ndiverse languages is essential for broad accessibility. However, effective\ncross-lingual transfer is hindered by LLM performance gaps across languages and\nthe scarcity of fine-tuning data in many languages. Through analysis of LLM\ninternal representations from over 1,000+ language pairs, we discover that\nmiddle layers exhibit the strongest potential for cross-lingual alignment.\nBuilding on this finding, we propose a middle-layer alignment objective\nintegrated into task-specific training. Our experiments on slot filling,\nmachine translation, and structured text generation show consistent\nimprovements in cross-lingual transfer, especially to lower-resource languages.\nThe method is robust to the choice of alignment languages and generalizes to\nlanguages unseen during alignment. Furthermore, we show that separately trained\nalignment modules can be merged with existing task-specific modules, improving\ncross-lingual capabilities without full re-training. Our code is publicly\navailable (https://github.com/dannigt/mid-align)."}
{"id": "2505.02746", "pdf": "https://arxiv.org/pdf/2505.02746", "abs": "https://arxiv.org/abs/2505.02746", "authors": ["Simon Ging", "Sebastian Walter", "Jelena BratuliÄ", "Johannes Dienert", "Hannah Bast", "Thomas Brox"], "title": "Using Knowledge Graphs to harvest datasets for efficient CLIP model training", "categories": ["cs.CV", "cs.CL", "cs.IR", "cs.LG"], "comment": null, "summary": "Training high-quality CLIP models typically requires enormous datasets, which\nlimits the development of domain-specific models -- especially in areas that\neven the largest CLIP models do not cover well -- and drives up training costs.\nThis poses challenges for scientific research that needs fine-grained control\nover the training procedure of CLIP models. In this work, we show that by\nemploying smart web search strategies enhanced with knowledge graphs, a robust\nCLIP model can be trained from scratch with considerably less data.\nSpecifically, we demonstrate that an expert foundation model for living\norganisms can be built using just 10M images. Moreover, we introduce EntityNet,\na dataset comprising 33M images paired with 46M text descriptions, which\nenables the training of a generic CLIP model in significantly reduced time."}
{"id": "2502.06516", "pdf": "https://arxiv.org/pdf/2502.06516", "abs": "https://arxiv.org/abs/2502.06516", "authors": ["Soobin Um", "Beomsu Kim", "Jong Chul Ye"], "title": "Boost-and-Skip: A Simple Guidance-Free Diffusion for Minority Generation", "categories": ["cs.LG", "cs.AI", "cs.CV", "stat.ML"], "comment": "ICML 2025, 29 pages, 11 figures", "summary": "Minority samples are underrepresented instances located in low-density\nregions of a data manifold, and are valuable in many generative AI\napplications, such as data augmentation, creative content generation, etc.\nUnfortunately, existing diffusion-based minority generators often rely on\ncomputationally expensive guidance dedicated for minority generation. To\naddress this, here we present a simple yet powerful guidance-free approach\ncalled Boost-and-Skip for generating minority samples using diffusion models.\nThe key advantage of our framework requires only two minimal changes to\nstandard generative processes: (i) variance-boosted initialization and (ii)\ntimestep skipping. We highlight that these seemingly-trivial modifications are\nsupported by solid theoretical and empirical evidence, thereby effectively\npromoting emergence of underrepresented minority features. Our comprehensive\nexperiments demonstrate that Boost-and-Skip greatly enhances the capability of\ngenerating minority samples, even rivaling guidance-based state-of-the-art\napproaches while requiring significantly fewer computations. Code is available\nat https://github.com/soobin-um/BnS."}
{"id": "2502.06250", "pdf": "https://arxiv.org/pdf/2502.06250", "abs": "https://arxiv.org/abs/2502.06250", "authors": ["Yaohua Zang", "Phaedon-Stelios Koutsourelakis"], "title": "DGenNO: A Novel Physics-aware Neural Operator for Solving Forward and Inverse PDE Problems based on Deep, Generative Probabilistic Modeling", "categories": ["cs.LG", "math-ph", "math.MP"], "comment": null, "summary": "Solving parametric partial differential equations (PDEs) and associated\nPDE-based, inverse problems is a central task in engineering and physics, yet\nexisting neural operator methods struggle with high-dimensional, discontinuous\ninputs and require large amounts of {\\em labeled} training data. We propose the\nDeep Generative Neural Operator (DGenNO), a physics-aware framework that\naddresses these challenges by leveraging a deep, generative, probabilistic\nmodel in combination with a set of lower-dimensional, latent variables that\nsimultaneously encode PDE-inputs and PDE-outputs. This formulation can make use\nof unlabeled data and significantly improves inverse problem-solving,\nparticularly for discontinuous or discrete-valued input functions. DGenNO\nenforces physics constraints without labeled data by incorporating as virtual\nobservables, weak-form residuals based on compactly supported radial basis\nfunctions (CSRBFs). These relax regularity constraints and eliminate\nhigher-order derivatives from the objective function. We also introduce\nMultiONet, a novel neural operator architecture, which is a more expressive\ngeneralization of the popular DeepONet that significantly enhances the\napproximating power of the proposed model. These innovations make DGenNO\nparticularly effective for challenging forward and inverse, PDE-based problems,\nsuch as those involving multi-phase media. Numerical experiments demonstrate\nthat DGenNO achieves higher accuracy across multiple benchmarks while\nexhibiting robustness to noise and strong generalization to out-of-distribution\ncases. Its adaptability, and the ability to handle sparse, noisy data while\nproviding probabilistic estimates, make DGenNO a powerful tool for scientific\nand engineering applications."}
{"id": "2502.15197", "pdf": "https://arxiv.org/pdf/2502.15197", "abs": "https://arxiv.org/abs/2502.15197", "authors": ["Zhaoxuan Wu", "Zijian Zhou", "Arun Verma", "Alok Prakash", "Daniela Rus", "Bryan Kian Hsiang Low"], "title": "TETRIS: Optimal Draft Token Selection for Batch Speculative Decoding", "categories": ["cs.CL", "cs.AI"], "comment": "17 pages, 11 figures, 5 tables", "summary": "We propose TETRIS, a novel method that optimizes the total throughput of\nbatch speculative decoding in multi-request settings. Unlike existing methods\nthat optimize for a single request or a group of requests as a whole, TETRIS\nactively selects the most promising draft tokens (for every request in a batch)\nto be accepted when verified in parallel, resulting in fewer rejected tokens\nand hence less wasted computing resources. Such an effective resource\nutilization to achieve fast inference in large language models (LLMs) is\nespecially important to service providers with limited inference capacity.\nCompared to baseline speculative decoding, TETRIS yields a consistently higher\nacceptance rate and more effective utilization of the limited inference\ncapacity. We show theoretically and empirically that TETRIS outperforms\nbaseline speculative decoding and existing methods that dynamically select\ndraft tokens, leading to a more efficient batch inference in LLMs."}
{"id": "2505.04594", "pdf": "https://arxiv.org/pdf/2505.04594", "abs": "https://arxiv.org/abs/2505.04594", "authors": ["Zhihao Zhang", "Abhinav Kumar", "Girish Chandar Ganesan", "Xiaoming Liu"], "title": "MonoCoP: Chain-of-Prediction for Monocular 3D Object Detection", "categories": ["cs.CV"], "comment": null, "summary": "Accurately predicting 3D attributes is crucial for monocular 3D object\ndetection (Mono3D), with depth estimation posing the greatest challenge due to\nthe inherent ambiguity in mapping 2D images to 3D space. While existing methods\nleverage multiple depth cues (e.g., estimating depth uncertainty, modeling\ndepth error) to improve depth accuracy, they overlook that accurate depth\nprediction requires conditioning on other 3D attributes, as these attributes\nare intrinsically inter-correlated through the 3D to 2D projection, which\nultimately limits overall accuracy and stability. Inspired by Chain-of-Thought\n(CoT) in large language models (LLMs), this paper proposes MonoCoP, which\nleverages a Chain-of-Prediction (CoP) to predict attributes sequentially and\nconditionally via three key designs. First, it employs a lightweight\nAttributeNet (AN) for each 3D attribute to learn attribute-specific features.\nNext, MonoCoP constructs an explicit chain to propagate these learned features\nfrom one attribute to the next. Finally, MonoCoP uses a residual connection to\naggregate features for each attribute along the chain, ensuring that later\nattribute predictions are conditioned on all previously processed attributes\nwithout forgetting the features of earlier ones. Experimental results show that\nour MonoCoP achieves state-of-the-art (SoTA) performance on the KITTI\nleaderboard without requiring additional data and further surpasses existing\nmethods on the Waymo and nuScenes frontal datasets."}
{"id": "2502.08987", "pdf": "https://arxiv.org/pdf/2502.08987", "abs": "https://arxiv.org/abs/2502.08987", "authors": ["Shiqian Li", "Ruihong Shen", "Yaoyu Tao", "Chi Zhang", "Yixin Zhu"], "title": "Neural Force Field: Few-shot Learning of Generalized Physical Reasoning", "categories": ["cs.LG", "cs.AI"], "comment": "31 pages", "summary": "Physical reasoning is a remarkable human ability that enables rapid learning\nand generalization from limited experience. Current AI models, despite\nextensive training, still struggle to achieve similar generalization,\nespecially in Out-of-distribution (OOD) settings. This limitation stems from\ntheir inability to abstract core physical principles from observations. A key\nchallenge is developing representations that can efficiently learn and\ngeneralize physical dynamics from minimal data. Here we present Neural Force\nField (NFF), a framework extending Neural Ordinary Differential Equation (NODE)\nto learn complex object interactions through force field representations, which\ncan be efficiently integrated through an Ordinary Differential Equation (ODE)\nsolver to predict object trajectories. Unlike existing approaches that rely on\ndiscrete latent spaces, NFF captures fundamental physical concepts such as\ngravity, support, and collision in continuous explicit force fields.\nExperiments on three challenging physical reasoning tasks demonstrate that NFF,\ntrained with only a few examples, achieves strong generalization to unseen\nscenarios. This physics-grounded representation enables efficient\nforward-backward planning and rapid adaptation through interactive refinement.\nOur work suggests that incorporating physics-inspired representations into\nlearning systems can help bridge the gap between artificial and human physical\nreasoning capabilities."}
{"id": "2502.06658", "pdf": "https://arxiv.org/pdf/2502.06658", "abs": "https://arxiv.org/abs/2502.06658", "authors": ["Eren Mehmet KÄ±ral", "NurÅen AydÄ±n", "Å. Ä°lker Birbil"], "title": "Generating Samples to Question Trained Models", "categories": ["cs.LG"], "comment": null, "summary": "There is a growing need for investigating how machine learning models\noperate. With this work, we aim to understand trained machine learning models\nby questioning their data preferences. We propose a mathematical framework that\nallows us to probe trained models and identify their preferred samples in\nvarious scenarios including prediction-risky, parameter-sensitive, or\nmodel-contrastive samples. To showcase our framework, we pose these queries to\na range of models trained on a range of classification and regression tasks,\nand receive answers in the form of generated data."}
{"id": "2502.15401", "pdf": "https://arxiv.org/pdf/2502.15401", "abs": "https://arxiv.org/abs/2502.15401", "authors": ["Xuetao Ma", "Wenbin Jiang", "Hua Huang"], "title": "Problem-Solving Logic Guided Curriculum In-Context Learning for LLMs Complex Reasoning", "categories": ["cs.CL", "cs.AI"], "comment": "19 pages, 6 figures, ACL 2025 findings, camera-ready version", "summary": "In-context learning (ICL) can significantly enhance the complex reasoning\ncapabilities of large language models (LLMs), with the key lying in the\nselection and ordering of demonstration examples. Previous methods typically\nrelied on simple features to measure the relevance between examples. We argue\nthat these features are not sufficient to reflect the intrinsic connections\nbetween examples. In this study, we propose a curriculum ICL strategy guided by\nproblem-solving logic. We select demonstration examples by analyzing the\nproblem-solving logic and order them based on curriculum learning.\nSpecifically, we constructed a problem-solving logic instruction set based on\nthe BREAK dataset and fine-tuned a language model to analyze the\nproblem-solving logic of examples. Subsequently, we selected appropriate\ndemonstration examples based on problem-solving logic and assessed their\ndifficulty according to the number of problem-solving steps. In accordance with\nthe principles of curriculum learning, we ordered the examples from easy to\nhard to serve as contextual prompts. Experimental results on multiple\nbenchmarks indicate that our method outperforms previous ICL approaches in\nterms of performance and efficiency, effectively enhancing the complex\nreasoning capabilities of LLMs. Our project will be released at\nhttps://github.com/maxuetao/CurriculumICL"}
{"id": "2505.05528", "pdf": "https://arxiv.org/pdf/2505.05528", "abs": "https://arxiv.org/abs/2505.05528", "authors": ["Hanxun Huang", "Sarah Erfani", "Yige Li", "Xingjun Ma", "James Bailey"], "title": "X-Transfer Attacks: Towards Super Transferable Adversarial Attacks on CLIP", "categories": ["cs.CV", "cs.AI", "cs.CL", "cs.LG"], "comment": "ICML 2025", "summary": "As Contrastive Language-Image Pre-training (CLIP) models are increasingly\nadopted for diverse downstream tasks and integrated into large vision-language\nmodels (VLMs), their susceptibility to adversarial perturbations has emerged as\na critical concern. In this work, we introduce \\textbf{X-Transfer}, a novel\nattack method that exposes a universal adversarial vulnerability in CLIP.\nX-Transfer generates a Universal Adversarial Perturbation (UAP) capable of\ndeceiving various CLIP encoders and downstream VLMs across different samples,\ntasks, and domains. We refer to this property as \\textbf{super\ntransferability}--a single perturbation achieving cross-data, cross-domain,\ncross-model, and cross-task adversarial transferability simultaneously. This is\nachieved through \\textbf{surrogate scaling}, a key innovation of our approach.\nUnlike existing methods that rely on fixed surrogate models, which are\ncomputationally intensive to scale, X-Transfer employs an efficient surrogate\nscaling strategy that dynamically selects a small subset of suitable surrogates\nfrom a large search space. Extensive evaluations demonstrate that X-Transfer\nsignificantly outperforms previous state-of-the-art UAP methods, establishing a\nnew benchmark for adversarial transferability across CLIP models. The code is\npublicly available in our\n\\href{https://github.com/HanxunH/XTransferBench}{GitHub repository}."}
{"id": "2502.11147", "pdf": "https://arxiv.org/pdf/2502.11147", "abs": "https://arxiv.org/abs/2502.11147", "authors": ["Junhao Hu", "Wenrui Huang", "Weidong Wang", "Zhenwen Li", "Tiancheng Hu", "Zhixia Liu", "Xusheng Chen", "Tao Xie", "Yizhou Shan"], "title": "RaaS: Reasoning-Aware Attention Sparsity for Efficient LLM Reasoning", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Large Language Models (LLMs) have demonstrated strong capabilities across\nvarious domains, with recent advancements in challenging reasoning tasks such\nas mathematics and programming. However, solving reasoning tasks often requires\nan LLM to generate long sequences, incurring $O(N)$ time and memory\ncomplexities per token, where $N$ is the current sequence length. To reduce\ncomplexities, existing sparsity-based algorithms propose to retain Key-Value\n(KV) vectors, the intermediate representations of only the most critical\ntokens. However, these algorithms struggle with the \"impossible trinity\" of\naccuracy, time, and memory. For example, the state-of-the-art algorithm, Quest,\nachieves high accuracy with $O(L)$ time but $O(N)$ memory ($L$ is the cache\nbudget, $L \\ll N$). To address the \"impossible trinity\", in this paper, we\nidentify a new attention pattern during the decode stage of reasoning tasks,\nwhere milestone tokens (analogous to lemmas in mathematical proofs) emerge, are\nutilized, and then become unimportant afterward. Based on this pattern, we\npropose a new algorithm RaaS that identifies milestone tokens and retains their\nKV vectors until they are no longer needed, achieving high accuracy with $O(L)$\ntime and $O(L)$ memory complexities."}
{"id": "2502.07937", "pdf": "https://arxiv.org/pdf/2502.07937", "abs": "https://arxiv.org/abs/2502.07937", "authors": ["Xuefeng Liu", "Hung T. C. Le", "Siyu Chen", "Rick Stevens", "Zhuoran Yang", "Matthew R. Walter", "Yuxin Chen"], "title": "Active Advantage-Aligned Online Reinforcement Learning with Offline Data", "categories": ["cs.LG", "stat.ML"], "comment": null, "summary": "Online reinforcement learning (RL) enhances policies through direct\ninteractions with the environment, but faces challenges related to sample\nefficiency. In contrast, offline RL leverages extensive pre-collected data to\nlearn policies, but often produces suboptimal results due to limited data\ncoverage. Recent efforts integrate offline and online RL in order to harness\nthe advantages of both approaches. However, effectively combining online and\noffline RL remains challenging due to issues that include catastrophic\nforgetting, lack of robustness to data quality and limited sample efficiency in\ndata utilization. In an effort to address these challenges, we introduce A3RL,\nwhich incorporates a novel confidence aware Active Advantage Aligned (A3)\nsampling strategy that dynamically prioritizes data aligned with the policy's\nevolving needs from both online and offline sources, optimizing policy\nimprovement. Moreover, we provide theoretical insights into the effectiveness\nof our active sampling strategy and conduct diverse empirical experiments and\nablation studies, demonstrating that our method outperforms competing online RL\ntechniques that leverage offline data. Our code will be publicly available\nat:https://github.com/xuefeng-cs/A3RL."}
{"id": "2502.15434", "pdf": "https://arxiv.org/pdf/2502.15434", "abs": "https://arxiv.org/abs/2502.15434", "authors": ["Yue Zhou", "Yi Chang", "Yuan Wu"], "title": "Mixup Model Merge: Enhancing Model Merging Performance through Randomized Linear Interpolation", "categories": ["cs.CL", "I.2.7; I.2.6"], "comment": "15 pages", "summary": "Model merging aims to integrate multiple task-specific models into a unified\nmodel that inherits the capabilities of the task-specific models, without\nadditional training. Existing model merging methods often lack consideration of\nthe varying contribution ratios of different task-specific models to the final\nmerged model. In this paper, we propose Mixup Model Merge (M3), a simple yet\neffective method inspired by the randomized linear interpolation strategy from\nthe Mixup data augmentation technique. M3 performs randomized linear\ninterpolation in parameter space between two task-specific LLMs, where\ninterpolation coefficients are sampled from a Beta distribution to explore\ndiverse contribution ratios. This controllable randomness allows M3 to\noutperform standard equal-ratio merging by discovering better contribution\nratio combinations. Extensive experiments show that M3 significantly (1)\nimproves merged LLM performance across tasks, (2) enhances out-of-distribution\nand adversarial robustness, and (3) outperforms the positive effects of the\nsparsification method DARE on model merging and can be further combined with\nDARE to achieve superior results. By tuning the Beta distribution's shape\nparameters, (4) M3 balances exploration efficiency and diversity in\ncontribution ratios. The code is available at:\nhttps://github.com/MLGroupJLU/MixupModelMerge"}
{"id": "2505.10238", "pdf": "https://arxiv.org/pdf/2505.10238", "abs": "https://arxiv.org/abs/2505.10238", "authors": ["Yanbo Ding", "Xirui Hu", "Zhizhi Guo", "Chi Zhang", "Yali Wang"], "title": "MTVCrafter: 4D Motion Tokenization for Open-World Human Image Animation", "categories": ["cs.CV"], "comment": null, "summary": "Human image animation has gained increasing attention and developed rapidly\ndue to its broad applications in digital humans. However, existing methods rely\nlargely on 2D-rendered pose images for motion guidance, which limits\ngeneralization and discards essential 3D information for open-world animation.\nTo tackle this problem, we propose MTVCrafter (Motion Tokenization Video\nCrafter), the first framework that directly models raw 3D motion sequences\n(i.e., 4D motion) for human image animation. Specifically, we introduce 4DMoT\n(4D motion tokenizer) to quantize 3D motion sequences into 4D motion tokens.\nCompared to 2D-rendered pose images, 4D motion tokens offer more robust\nspatio-temporal cues and avoid strict pixel-level alignment between pose image\nand character, enabling more flexible and disentangled control. Then, we\nintroduce MV-DiT (Motion-aware Video DiT). By designing unique motion attention\nwith 4D positional encodings, MV-DiT can effectively leverage motion tokens as\n4D compact yet expressive context for human image animation in the complex 3D\nworld. Hence, it marks a significant step forward in this field and opens a new\ndirection for pose-guided human video generation. Experiments show that our\nMTVCrafter achieves state-of-the-art results with an FID-VID of 6.98,\nsurpassing the second-best by 65%. Powered by robust motion tokens, MTVCrafter\nalso generalizes well to diverse open-world characters (single/multiple,\nfull/half-body) across various styles and scenarios. Our video demos and code\nare on: https://github.com/DINGYANB/MTVCrafter."}
{"id": "2502.11844", "pdf": "https://arxiv.org/pdf/2502.11844", "abs": "https://arxiv.org/abs/2502.11844", "authors": ["Mark Vero", "Niels MÃ¼ndler", "Victor Chibotaru", "Veselin Raychev", "Maximilian Baader", "Nikola JovanoviÄ", "Jingxuan He", "Martin Vechev"], "title": "BaxBench: Can LLMs Generate Correct and Secure Backends?", "categories": ["cs.CR", "cs.AI", "cs.LG", "cs.PL"], "comment": null, "summary": "Automatic program generation has long been a fundamental challenge in\ncomputer science. Recent benchmarks have shown that large language models\n(LLMs) can effectively generate code at the function level, make code edits,\nand solve algorithmic coding tasks. However, to achieve full automation, LLMs\nshould be able to generate production-quality, self-contained application\nmodules. To evaluate the capabilities of LLMs in solving this challenge, we\nintroduce BaxBench, a novel evaluation benchmark consisting of 392 tasks for\nthe generation of backend applications. We focus on backends for three critical\nreasons: (i) they are practically relevant, building the core components of\nmost modern web and cloud software, (ii) they are difficult to get right,\nrequiring multiple functions and files to achieve the desired functionality,\nand (iii) they are security-critical, as they are exposed to untrusted\nthird-parties, making secure solutions that prevent deployment-time attacks an\nimperative. BaxBench validates the functionality of the generated applications\nwith comprehensive test cases, and assesses their security exposure by\nexecuting end-to-end exploits. Our experiments reveal key limitations of\ncurrent LLMs in both functionality and security: (i) even the best model,\nOpenAI o1, achieves a mere 62% on code correctness; (ii) on average, we could\nsuccessfully execute security exploits on around half of the correct programs\ngenerated by each LLM; and (iii) in less popular backend frameworks, models\nfurther struggle to generate correct and secure applications. Progress on\nBaxBench signifies important steps towards autonomous and secure software\ndevelopment with LLMs."}
{"id": "2502.09263", "pdf": "https://arxiv.org/pdf/2502.09263", "abs": "https://arxiv.org/abs/2502.09263", "authors": ["Yuankai Luo", "Lei Shi", "Xiao-Ming Wu"], "title": "Can Classic GNNs Be Strong Baselines for Graph-level Tasks? Simple Architectures Meet Excellence", "categories": ["cs.LG"], "comment": "ICML 2025", "summary": "Message-passing Graph Neural Networks (GNNs) are often criticized for their\nlimited expressiveness, issues like over-smoothing and over-squashing, and\nchallenges in capturing long-range dependencies. Conversely, Graph Transformers\n(GTs) are regarded as superior due to their employment of global attention\nmechanisms, which potentially mitigate these challenges. Literature frequently\nsuggests that GTs outperform GNNs in graph-level tasks, especially for graph\nclassification and regression on small molecular graphs. In this study, we\nexplore the untapped potential of GNNs through an enhanced framework, GNN+,\nwhich integrates six widely used techniques: edge feature integration,\nnormalization, dropout, residual connections, feed-forward networks, and\npositional encoding, to effectively tackle graph-level tasks. We conduct a\nsystematic re-evaluation of three classic GNNs (GCN, GIN, and GatedGCN)\nenhanced by the GNN+ framework across 14 well-known graph-level datasets. Our\nresults reveal that, contrary to prevailing beliefs, these classic GNNs\nconsistently match or surpass the performance of GTs, securing top-three\nrankings across all datasets and achieving first place in eight. Furthermore,\nthey demonstrate greater efficiency, running several times faster than GTs on\nmany datasets. This highlights the potential of simple GNN architectures,\nchallenging the notion that complex mechanisms in GTs are essential for\nsuperior graph-level performance. Our source code is available at\nhttps://github.com/LUOyk1999/GNNPlus."}
{"id": "2502.16487", "pdf": "https://arxiv.org/pdf/2502.16487", "abs": "https://arxiv.org/abs/2502.16487", "authors": ["Tarun Gupta", "Danish Pruthi"], "title": "All That Glitters is Not Novel: Plagiarism in AI Generated Research", "categories": ["cs.CL"], "comment": "Accepted to ACL 2025 (main) conference", "summary": "Automating scientific research is considered the final frontier of science.\nRecently, several papers claim autonomous research agents can generate novel\nresearch ideas. Amidst the prevailing optimism, we document a critical concern:\na considerable fraction of such research documents are smartly plagiarized.\nUnlike past efforts where experts evaluate the novelty and feasibility of\nresearch ideas, we request $13$ experts to operate under a different\nsituational logic: to identify similarities between LLM-generated research\ndocuments and existing work. Concerningly, the experts identify $24\\%$ of the\n$50$ evaluated research documents to be either paraphrased (with one-to-one\nmethodological mapping), or significantly borrowed from existing work. These\nreported instances are cross-verified by authors of the source papers. Experts\nfind an additional $32\\%$ ideas to partially overlap with prior work, and a\nsmall fraction to be completely original. Problematically, these LLM-generated\nresearch documents do not acknowledge original sources, and bypass inbuilt\nplagiarism detectors. Lastly, through controlled experiments we show that\nautomated plagiarism detectors are inadequate at catching plagiarized ideas\nfrom such systems. We recommend a careful assessment of LLM-generated research,\nand discuss the implications of our findings on academic publishing."}
{"id": "2505.11404", "pdf": "https://arxiv.org/pdf/2505.11404", "abs": "https://arxiv.org/abs/2505.11404", "authors": ["Wenchuan Zhang", "Penghao Zhang", "Jingru Guo", "Tao Cheng", "Jie Chen", "Shuwan Zhang", "Zhang Zhang", "Yuhao Yi", "Hong Bu"], "title": "Patho-R1: A Multimodal Reinforcement Learning-Based Pathology Expert Reasoner", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Recent advances in vision language models (VLMs) have enabled broad progress\nin the general medical field. However, pathology still remains a more\nchallenging subdomain, with current pathology specific VLMs exhibiting\nlimitations in both diagnostic accuracy and reasoning plausibility. Such\nshortcomings are largely attributable to the nature of current pathology\ndatasets, which are primarily composed of image description pairs that lack the\ndepth and structured diagnostic paradigms employed by real world pathologists.\nIn this study, we leverage pathology textbooks and real world pathology experts\nto construct high-quality, reasoning-oriented datasets. Building on this, we\nintroduce Patho-R1, a multimodal RL-based pathology Reasoner, trained through a\nthree-stage pipeline: (1) continued pretraining on 3.5 million image-text pairs\nfor knowledge infusion; (2) supervised fine-tuning on 500k high-quality\nChain-of-Thought samples for reasoning incentivizing; (3) reinforcement\nlearning using Group Relative Policy Optimization and Decoupled Clip and\nDynamic sAmpling Policy Optimization strategies for multimodal reasoning\nquality refinement. To further assess the alignment quality of our dataset, we\npropose Patho-CLIP, trained on the same figure-caption corpus used for\ncontinued pretraining. Comprehensive experimental results demonstrate that both\nPatho-CLIP and Patho-R1 achieve robust performance across a wide range of\npathology-related tasks, including zero-shot classification, cross-modal\nretrieval, Visual Question Answering, and Multiple Choice Question. Our project\nis available at the Patho-R1 repository:\nhttps://github.com/Wenchuan-Zhang/Patho-R1."}
{"id": "2502.12929", "pdf": "https://arxiv.org/pdf/2502.12929", "abs": "https://arxiv.org/abs/2502.12929", "authors": ["Lakshmi Nair", "Ian Trase", "Mark Kim"], "title": "Flow-of-Options: Diversified and Improved LLM Reasoning by Thinking Through Options", "categories": ["cs.LG", "cs.AI", "cs.CL"], "comment": "ICML 2025", "summary": "We present a novel reasoning approach called Flow-of-Options (FoO), designed\nto address intrinsic biases in Large Language Models (LLMs). Flow-of-Options\nenables LLMs to systematically explore a diverse range of possibilities in\ntheir reasoning, as demonstrated by an FoO-based agentic framework developed\nfor autonomously solving Machine Learning (ML) tasks. FoO enforces diversity in\nLLM solutions through compressed and interpretable task representations,\nresulting in improvements of 38.2% - 69.2% on standard data science tasks, and\n37.4% - 47.9% on therapeutic chemistry tasks, as compared to state-of-the-art\nbaselines. With an overall operation cost under $1 per task, our framework is\nwell-suited for cost-sensitive applications. Going beyond tabular\nclassification and regression, we show the broader applicability of our\nFoO-based agentic system to tasks such as reinforcement learning and image\ngeneration. Our code is open-sourced at:\nhttps://github.com/flagshippioneering/Flow-of-Options."}
{"id": "2502.11646", "pdf": "https://arxiv.org/pdf/2502.11646", "abs": "https://arxiv.org/abs/2502.11646", "authors": ["Yunzhe Hu", "Difan Zou", "Dong Xu"], "title": "Hyper-SET: Designing Transformers via Hyperspherical Energy Minimization", "categories": ["cs.LG"], "comment": "31 pages", "summary": "Transformer-based models have achieved remarkable success, but their core\ncomponents, Transformer layers, are largely heuristics-driven and engineered\nfrom the bottom up, calling for a prototypical model with high interpretability\nand practical competence. To this end, we conceptualize a principled, top-down\napproach grounded in energy-based interpretation. Specifically, we formalize\ntoken dynamics as a joint maximum likelihood estimation on the hypersphere,\nfeaturing two properties: semantic alignment in the high-dimensional space and\ndistributional uniformity in the low-dimensional space. By quantifying them\nwith extended Hopfield energy functions, we instantiate this idea as a\nconstrained energy minimization problem, which enables designs of symmetric\nattention and feedforward modules with RMS normalization. We further present\n\\textit{Hyper-Spherical Energy Transformer} (Hyper-SET), a recurrent-depth\nalternative to vanilla Transformers naturally emerging from iterative energy\noptimization on the hypersphere. With shared parameters across layers,\nHyper-SET can scale to arbitrary depth with fewer parameters. Theoretically\ngrounded and compact, it achieves competitive or superior performance across\ndiverse tasks, including Sudoku solving, image classification, and masked image\nmodeling. We also design novel variations under the proposed general principle,\nsuch as linear attention and gated feedforward layer. Moreover, we showcase its\nscalability with depth-wise LoRA. Our results highlight Hyper-SET as a step\ntoward interpretable and principled Transformer design."}
{"id": "2502.16989", "pdf": "https://arxiv.org/pdf/2502.16989", "abs": "https://arxiv.org/abs/2502.16989", "authors": ["Davide Testa", "Giovanni Bonetta", "Raffaella Bernardi", "Alessandro Bondielli", "Alessandro Lenci", "Alessio Miaschi", "Lucia Passaro", "Bernardo Magnini"], "title": "All-in-one: Understanding and Generation in Multimodal Reasoning with the MAIA Benchmark", "categories": ["cs.CL"], "comment": null, "summary": "We introduce MAIA (Multimodal AI Assessment), a native-Italian benchmark\ndesigned for fine-grained investigation of the reasoning abilities of visual\nlanguage models on videos. MAIA differs from other available video benchmarks\nfor its design, its reasoning categories, the metric it uses, and the language\nand culture of the videos. MAIA evaluates Vision Language Models (VLMs) on two\naligned tasks: a visual statement verification task and an open-ended visual\nquestion-answering task, both on the same set of video-related questions. It\nconsiders twelve reasoning categories that aim to disentangle language and\nvision relations by highlighting the role of the visual input. Thanks to its\ncarefully taught design, it evaluates VLMs' consistency and visually grounded\nnatural language comprehension and generation simultaneously through an\naggregated metric revealing low results that highlight models' fragility. Last\nbut not least, the video collection has been carefully selected to reflect the\nItalian culture, and the language data are produced by native-speakers."}
{"id": "2505.16192", "pdf": "https://arxiv.org/pdf/2505.16192", "abs": "https://arxiv.org/abs/2505.16192", "authors": ["Chaoya Jiang", "Yongrui Heng", "Wei Ye", "Han Yang", "Haiyang Xu", "Ming Yan", "Ji Zhang", "Fei Huang", "Shikun Zhang"], "title": "VLM-R$^3$: Region Recognition, Reasoning, and Refinement for Enhanced Multimodal Chain-of-Thought", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Recently, reasoning-based MLLMs have achieved a degree of success in\ngenerating long-form textual reasoning chains. However, they still struggle\nwith complex tasks that necessitate dynamic and iterative focusing on and\nrevisiting of visual regions to achieve precise grounding of textual reasoning\nin visual evidence. We introduce \\textbf{VLM-R$^3$} (\\textbf{V}isual\n\\textbf{L}anguage \\textbf{M}odel with \\textbf{R}egion \\textbf{R}ecognition and\n\\textbf{R}easoning), a framework that equips an MLLM with the ability to (i)\ndecide \\emph{when} additional visual evidence is needed, (ii) determine\n\\emph{where} to ground within the image, and (iii) seamlessly weave the\nrelevant sub-image content back into an interleaved chain-of-thought. The core\nof our method is \\textbf{Region-Conditioned Reinforcement Policy Optimization\n(R-GRPO)}, a training paradigm that rewards the model for selecting informative\nregions, formulating appropriate transformations (e.g.\\ crop, zoom), and\nintegrating the resulting visual context into subsequent reasoning steps. To\nbootstrap this policy, we compile a modest but carefully curated Visuo-Lingual\nInterleaved Rationale (VLIR) corpus that provides step-level supervision on\nregion selection and textual justification. Extensive experiments on MathVista,\nScienceQA, and other benchmarks show that VLM-R$^3$ sets a new state of the art\nin zero-shot and few-shot settings, with the largest gains appearing on\nquestions demanding subtle spatial reasoning or fine-grained visual cue\nextraction."}
{"id": "2502.13681", "pdf": "https://arxiv.org/pdf/2502.13681", "abs": "https://arxiv.org/abs/2502.13681", "authors": ["Ruida Hu", "Chao Peng", "Xinchen Wang", "Junjielong Xu", "Cuiyun Gao"], "title": "Repo2Run: Automated Building Executable Environment for Code Repository at Scale", "categories": ["cs.SE", "cs.AI", "cs.CL", "cs.LG"], "comment": null, "summary": "Scaling up executable code data is significant for improving language models'\nsoftware engineering capability. The intricate nature of the process makes it\nlabor-intensive, time-consuming and expert-knowledge-dependent to build a large\nnumber of executable code repositories, limiting the scalability of existing\nwork based on running tests. The primary bottleneck lies in the automated\nbuilding of test environments for different repositories, which is an essential\nyet underexplored task. To mitigate the gap, we introduce Repo2Run, the first\nLLM-based agent aiming at automating the building of executable test\nenvironments for any repositories at scale. Specifically, given a code\nrepository, Repo2Run iteratively builds the Docker image, runs unit tests based\non the feedback of the building, and synthesizes the Dockerfile until the\nentire pipeline is executed successfully. The resulting Dockerfile can then be\nused to create Docker container environments for running code and tests. We\ncreated a benchmark containing 420 Python repositories with unit tests for\nevaluation. The results illustrate that Repo2Run achieves an 86.0% success\nrate, outperforming SWE-agent by 77.0%. The resources of Repo2Run are available\nat https://github.com/bytedance/Repo2Run."}
{"id": "2502.12115", "pdf": "https://arxiv.org/pdf/2502.12115", "abs": "https://arxiv.org/abs/2502.12115", "authors": ["Samuel Miserendino", "Michele Wang", "Tejal Patwardhan", "Johannes Heidecke"], "title": "SWE-Lancer: Can Frontier LLMs Earn $1 Million from Real-World Freelance Software Engineering?", "categories": ["cs.LG", "cs.SE"], "comment": "9 pages, 30 pages appendix", "summary": "We introduce SWE-Lancer, a benchmark of over 1,400 freelance software\nengineering tasks from Upwork, valued at \\$1 million USD total in real-world\npayouts. SWE-Lancer encompasses both independent engineering tasks--ranging\nfrom \\$50 bug fixes to \\$32,000 feature implementations--and managerial tasks,\nwhere models choose between technical implementation proposals. Independent\ntasks are graded with end-to-end tests triple-verified by experienced software\nengineers, while managerial decisions are assessed against the choices of the\noriginal hired engineering managers. We evaluate model performance and find\nthat frontier models are still unable to solve the majority of tasks. To\nfacilitate future research, we open-source a unified Docker image and a public\nevaluation split, SWE-Lancer Diamond\n(https://github.com/openai/SWELancer-Benchmark). By mapping model performance\nto monetary value, we hope SWE-Lancer enables greater research into the\neconomic impact of AI model development."}
{"id": "2502.20864", "pdf": "https://arxiv.org/pdf/2502.20864", "abs": "https://arxiv.org/abs/2502.20864", "authors": ["Mohammad Rifqi Farhansyah", "Iwan Darmawan", "Adryan Kusumawardhana", "Genta Indra Winata", "Alham Fikri Aji", "Derry Tanti Wijaya"], "title": "Do Language Models Understand Honorific Systems in Javanese?", "categories": ["cs.CL"], "comment": "ACL 2025 - Main Conference", "summary": "The Javanese language features a complex system of honorifics that vary\naccording to the social status of the speaker, listener, and referent. Despite\nits cultural and linguistic significance, there has been limited progress in\ndeveloping a comprehensive corpus to capture these variations for natural\nlanguage processing (NLP) tasks. In this paper, we present Unggah-Ungguh, a\ncarefully curated dataset designed to encapsulate the nuances of Unggah-Ungguh\nBasa, the Javanese speech etiquette framework that dictates the choice of words\nand phrases based on social hierarchy and context. Using Unggah-Ungguh, we\nassess the ability of language models (LMs) to process various levels of\nJavanese honorifics through classification and machine translation tasks. To\nfurther evaluate cross-lingual LMs, we conduct machine translation experiments\nbetween Javanese (at specific honorific levels) and Indonesian. Additionally,\nwe explore whether LMs can generate contextually appropriate Javanese\nhonorifics in conversation tasks, where the honorific usage should align with\nthe social role and contextual cues. Our findings indicate that current LMs\nstruggle with most honorific levels, exhibitinga bias toward certain honorific\ntiers."}
{"id": "2505.17550", "pdf": "https://arxiv.org/pdf/2505.17550", "abs": "https://arxiv.org/abs/2505.17550", "authors": ["Xiaoyu Ye", "Songjie Cheng", "Yongtao Wang", "Yajiao Xiong", "Yishen Li"], "title": "T2VUnlearning: A Concept Erasing Method for Text-to-Video Diffusion Models", "categories": ["cs.CV"], "comment": null, "summary": "Recent advances in text-to-video (T2V) diffusion models have significantly\nenhanced the quality of generated videos. However, their ability to produce\nexplicit or harmful content raises concerns about misuse and potential rights\nviolations. Inspired by the success of unlearning techniques in erasing\nundesirable concepts from text-to-image (T2I) models, we extend unlearning to\nT2V models and propose a robust and precise unlearning method. Specifically, we\nadopt negatively-guided velocity prediction fine-tuning and enhance it with\nprompt augmentation to ensure robustness against LLM-refined prompts. To\nachieve precise unlearning, we incorporate a localization and a preservation\nregularization to preserve the model's ability to generate non-target concepts.\nExtensive experiments demonstrate that our method effectively erases a specific\nconcept while preserving the model's generation capability for all other\nconcepts, outperforming existing methods. We provide the unlearned models in\n\\href{https://github.com/VDIGPKU/T2VUnlearning.git}{https://github.com/VDIGPKU/T2VUnlearning.git}."}
{"id": "2502.15210", "pdf": "https://arxiv.org/pdf/2502.15210", "abs": "https://arxiv.org/abs/2502.15210", "authors": ["Aarash Feizi", "Sai Rajeswar", "Adriana Romero-Soriano", "Reihaneh Rabbany", "Valentina Zantedeschi", "Spandana Gella", "JoÃ£o Monteiro"], "title": "PairBench: Are Vision-Language Models Reliable at Comparing What They See?", "categories": ["cs.LG", "cs.AI", "cs.CL"], "comment": null, "summary": "Understanding how effectively large vision language models (VLMs) compare\nvisual inputs is crucial across numerous applications, yet this fundamental\ncapability remains insufficiently assessed. While VLMs are increasingly\ndeployed for tasks requiring comparative judgment, including automated\nevaluation, re-ranking, and retrieval-augmented generation, no systematic\nframework exists to measure their performance in these scenarios. We present\nPairBench, a simple framework that evaluates VLMs as customizable similarity\ntools using widely available image datasets. Our approach introduces four key\nmetrics for reliable comparison: alignment with human annotations, consistency\nacross pair ordering, distribution smoothness, and controllability through\nprompting. Our analysis reveals that no model consistently excels across all\nmetrics, with each demonstrating distinct strengths and weaknesses. Most\nconcerning is the widespread inability of VLMs to maintain symmetric similarity\nscores. Interestingly, we demonstrate that performance on our benchmark\nstrongly correlates with popular benchmarks used for more complex tasks, while\nproviding additional metrics into controllability, smoothness and ordering.\nThis makes PairBench a unique and comprehensive framework to evaluate the\nperformance of VLMs for automatic evaluation depending on the task."}
{"id": "2502.15678", "pdf": "https://arxiv.org/pdf/2502.15678", "abs": "https://arxiv.org/abs/2502.15678", "authors": ["Luca M. Schulze Buschoff", "Konstantinos Voudouris", "Elif Akata", "Matthias Bethge", "Joshua B. Tenenbaum", "Eric Schulz"], "title": "Testing the Limits of Fine-Tuning for Improving Visual Cognition in Vision Language Models", "categories": ["cs.LG"], "comment": null, "summary": "Pre-trained vision language models still fall short of human visual\ncognition. In an effort to improve visual cognition and align models with human\nbehavior, we introduce visual stimuli and human judgments on visual cognition\ntasks, allowing us to systematically evaluate performance across cognitive\ndomains under a consistent environment. We fine-tune models on ground truth\ndata for intuitive physics and causal reasoning and find that this improves\nmodel performance in the respective fine-tuning domain. Furthermore, it can\nimprove model alignment with human behavior. However, we find that\ntask-specific fine-tuning does not contribute to robust human-like\ngeneralization to data with other visual characteristics or to tasks in other\ncognitive domains."}
{"id": "2503.01372", "pdf": "https://arxiv.org/pdf/2503.01372", "abs": "https://arxiv.org/abs/2503.01372", "authors": ["Joel Niklaus", "Jakob Merane", "Luka Nenadic", "Sina Ahmadi", "Yingqiang Gao", "Cyrill A. H. Chevalley", "Claude Humbel", "Christophe GÃ¶sken", "Lorenzo Tanzi", "Thomas LÃ¼thi", "Stefan Palombo", "Spencer Poff", "Boling Yang", "Nan Wu", "Matthew Guillod", "Robin MamiÃ©", "Daniel Brunner", "Julio Pereyra", "Niko Grupen"], "title": "SwiLTra-Bench: The Swiss Legal Translation Benchmark", "categories": ["cs.CL", "cs.AI", "cs.LG", "68T50", "I.2"], "comment": "Accepted at ACL main 2025", "summary": "In Switzerland legal translation is uniquely important due to the country's\nfour official languages and requirements for multilingual legal documentation.\nHowever, this process traditionally relies on professionals who must be both\nlegal experts and skilled translators -- creating bottlenecks and impacting\neffective access to justice. To address this challenge, we introduce\nSwiLTra-Bench, a comprehensive multilingual benchmark of over 180K aligned\nSwiss legal translation pairs comprising laws, headnotes, and press releases\nacross all Swiss languages along with English, designed to evaluate LLM-based\ntranslation systems. Our systematic evaluation reveals that frontier models\nachieve superior translation performance across all document types, while\nspecialized translation systems excel specifically in laws but under-perform in\nheadnotes. Through rigorous testing and human expert validation, we demonstrate\nthat while fine-tuning open SLMs significantly improves their translation\nquality, they still lag behind the best zero-shot prompted frontier models such\nas Claude-3.5-Sonnet. Additionally, we present SwiLTra-Judge, a specialized LLM\nevaluation system that aligns best with human expert assessments."}
{"id": "2505.17677", "pdf": "https://arxiv.org/pdf/2505.17677", "abs": "https://arxiv.org/abs/2505.17677", "authors": ["Ming Hu", "Zhengdi Yu", "Feilong Tang", "Kaiwen Chen", "Yulong Li", "Imran Razzak", "Junjun He", "Tolga Birdal", "Kaijing Zhou", "Zongyuan Ge"], "title": "Towards Dynamic 3D Reconstruction of Hand-Instrument Interaction in Ophthalmic Surgery", "categories": ["cs.CV"], "comment": null, "summary": "Accurate 3D reconstruction of hands and instruments is critical for\nvision-based analysis of ophthalmic microsurgery, yet progress has been\nhampered by the lack of realistic, large-scale datasets and reliable annotation\ntools. In this work, we introduce OphNet-3D, the first extensive RGB-D dynamic\n3D reconstruction dataset for ophthalmic surgery, comprising 41 sequences from\n40 surgeons and totaling 7.1 million frames, with fine-grained annotations of\n12 surgical phases, 10 instrument categories, dense MANO hand meshes, and full\n6-DoF instrument poses. To scalably produce high-fidelity labels, we design a\nmulti-stage automatic annotation pipeline that integrates multi-view data\nobservation, data-driven motion prior with cross-view geometric consistency and\nbiomechanical constraints, along with a combination of collision-aware\ninteraction constraints for instrument interactions. Building upon OphNet-3D,\nwe establish two challenging benchmarks-bimanual hand pose estimation and\nhand-instrument interaction reconstruction-and propose two dedicated\narchitectures: H-Net for dual-hand mesh recovery and OH-Net for joint\nreconstruction of two-hand-two-instrument interactions. These models leverage a\nnovel spatial reasoning module with weak-perspective camera modeling and\ncollision-aware center-based representation. Both architectures outperform\nexisting methods by substantial margins, achieving improvements of over 2mm in\nMean Per Joint Position Error (MPJPE) and up to 23% in ADD-S metrics for hand\nand instrument reconstruction, respectively."}
{"id": "2502.16736", "pdf": "https://arxiv.org/pdf/2502.16736", "abs": "https://arxiv.org/abs/2502.16736", "authors": ["Rui Liu", "Peng Gao", "Yu Shen", "Ming Lin", "Pratap Tokekar"], "title": "Adaptive Conformal Guidance: A Framework for Multi-Domain Learning under Uncertainty", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Learning with guidance has proven effective across a wide range of machine\nlearning domains. Guidance may, for example, come from annotated datasets in\nsupervised learning, pseudo-labels in semi-supervised learning, and expert\ndemonstration policies in reinforcement learning. However, guidance signals can\nbe noisy due to domain shifts and limited data availability and may not\ngeneralize well. Blindly trusting such signals when they are noisy, incomplete,\nor misaligned with the target domain can lead to degraded performance. To\naddress these challenges, we propose $\\underline{Ada}$ptive\n$\\underline{Con}$formal $\\underline{G}$uidance (AdaConG), a universal,\nplug-and-play framework that dynamically modulates the influence of guidance\nsignals based on their associated uncertainty, quantified via split conformal\nprediction (CP). By adaptively adjusting to guidance uncertainty, AdaConG\nenables models to reduce reliance on potentially misleading signals and enhance\nlearning performance. We validate AdaConG across diverse domains and tasks,\nincluding knowledge distillation, semi-supervised image classification,\ngridworld navigation, and autonomous driving. Experimental results demonstrate\nthat AdaConG improves performance and robustness under imperfect guidance,\ne.g., in gridworld navigation, it accelerates convergence and achieves over\n$6\\times$ higher rewards than the best-performing baseline. These results\nhighlight AdaConG as a simple yet effective solution for multi-domain learning\nunder uncertainty."}
{"id": "2502.16772", "pdf": "https://arxiv.org/pdf/2502.16772", "abs": "https://arxiv.org/abs/2502.16772", "authors": ["Alireza Kazemipour", "Simone Parisi", "Matthew E. Taylor", "Michael Bowling"], "title": "Model-Based Exploration in Monitored Markov Decision Processes", "categories": ["cs.LG"], "comment": null, "summary": "A tenet of reinforcement learning is that the agent always observes rewards.\nHowever, this is not true in many realistic settings, e.g., a human observer\nmay not always be available to provide rewards, sensors may be limited or\nmalfunctioning, or rewards may be inaccessible during deployment. Monitored\nMarkov decision processes (Mon-MDPs) have recently been proposed to model such\nsettings. However, existing Mon-MDP algorithms have several limitations: they\ndo not fully exploit the problem structure, cannot leverage a known monitor,\nlack worst-case guarantees for 'unsolvable' Mon-MDPs without specific\ninitialization, and offer only asymptotic convergence proofs. This paper makes\nthree contributions. First, we introduce a model-based algorithm for Mon-MDPs\nthat addresses these shortcomings. The algorithm employs two instances of\nmodel-based interval estimation: one to ensure that observable rewards are\nreliably captured, and another to learn the minimax-optimal policy. Second, we\nempirically demonstrate the advantages. We show faster convergence than prior\nalgorithms in over four dozen benchmarks, and even more dramatic improvement\nwhen the monitoring process is known. Third, we present the first finite-sample\nbound on performance. We show convergence to a minimax-optimal policy even when\nsome rewards are never observable."}
{"id": "2503.01606", "pdf": "https://arxiv.org/pdf/2503.01606", "abs": "https://arxiv.org/abs/2503.01606", "authors": ["Zhanghao Hu", "Hanqi Yan", "Qinglin Zhu", "Zhenyi Shen", "Yulan He", "Lin Gui"], "title": "Beyond Prompting: An Efficient Embedding Framework for Open-Domain Question Answering", "categories": ["cs.CL", "cs.AI", "cs.CY", "cs.LG"], "comment": "Accepted in ACL 2025 Main", "summary": "Large language models have recently pushed open domain question answering\n(ODQA) to new frontiers. However, prevailing retriever-reader pipelines often\ndepend on multiple rounds of prompt level instructions, leading to high\ncomputational overhead, instability, and suboptimal retrieval coverage. In this\npaper, we propose EmbQA, an embedding-level framework that alleviates these\nshortcomings by enhancing both the retriever and the reader. Specifically, we\nrefine query representations via lightweight linear layers under an\nunsupervised contrastive learning objective, thereby reordering retrieved\npassages to highlight those most likely to contain correct answers.\nAdditionally, we introduce an exploratory embedding that broadens the model's\nlatent semantic space to diversify candidate generation and employs an\nentropy-based selection mechanism to choose the most confident answer\nautomatically. Extensive experiments across three open-source LLMs, three\nretrieval methods, and four ODQA benchmarks demonstrate that EmbQA\nsubstantially outperforms recent baselines in both accuracy and efficiency."}
{"id": "2505.17779", "pdf": "https://arxiv.org/pdf/2505.17779", "abs": "https://arxiv.org/abs/2505.17779", "authors": ["Anjie Le", "Henan Liu", "Yue Wang", "Zhenyu Liu", "Rongkun Zhu", "Taohan Weng", "Jinze Yu", "Boyang Wang", "Yalun Wu", "Kaiwen Yan", "Quanlin Sun", "Meirui Jiang", "Jialun Pei", "Siya Liu", "Haoyun Zheng", "Zhoujun Li", "Alison Noble", "Jacques Souquet", "Xiaoqing Guo", "Manxi Lin", "Hongcheng Guo"], "title": "U2-BENCH: Benchmarking Large Vision-Language Models on Ultrasound Understanding", "categories": ["cs.CV", "cs.LG"], "comment": null, "summary": "Ultrasound is a widely-used imaging modality critical to global healthcare,\nyet its interpretation remains challenging due to its varying image quality on\noperators, noises, and anatomical structures. Although large vision-language\nmodels (LVLMs) have demonstrated impressive multimodal capabilities across\nnatural and medical domains, their performance on ultrasound remains largely\nunexplored. We introduce U2-BENCH, the first comprehensive benchmark to\nevaluate LVLMs on ultrasound understanding across classification, detection,\nregression, and text generation tasks. U2-BENCH aggregates 7,241 cases spanning\n15 anatomical regions and defines 8 clinically inspired tasks, such as\ndiagnosis, view recognition, lesion localization, clinical value estimation,\nand report generation, across 50 ultrasound application scenarios. We evaluate\n20 state-of-the-art LVLMs, both open- and closed-source, general-purpose and\nmedical-specific. Our results reveal strong performance on image-level\nclassification, but persistent challenges in spatial reasoning and clinical\nlanguage generation. U2-BENCH establishes a rigorous and unified testbed to\nassess and accelerate LVLM research in the uniquely multimodal domain of\nmedical ultrasound imaging."}
{"id": "2502.17510", "pdf": "https://arxiv.org/pdf/2502.17510", "abs": "https://arxiv.org/abs/2502.17510", "authors": ["Yujie Feng", "Xujia Wang", "Zexin Lu", "Shenghong Fu", "Guangyuan Shi", "Yongxin Xu", "Yasha Wang", "Philip S. Yu", "Xu Chu", "Xiao-Ming Wu"], "title": "Recurrent Knowledge Identification and Fusion for Language Model Continual Learning", "categories": ["cs.LG", "cs.AI", "cs.CL"], "comment": "Accepted to ACL 2025 (Main Conference)", "summary": "Continual learning (CL) is crucial for deploying large language models (LLMs)\nin dynamic real-world environments without costly retraining. While recent\nmodel ensemble and model merging methods guided by parameter importance have\ngained popularity, they often struggle to balance knowledge transfer and\nforgetting, mainly due to the reliance on static importance estimates during\nsequential training. In this paper, we present Recurrent-KIF, a novel CL\nframework for Recurrent Knowledge Identification and Fusion, which enables\ndynamic estimation of parameter importance distributions to enhance knowledge\ntransfer. Inspired by human continual learning, Recurrent-KIF employs an inner\nloop that rapidly adapts to new tasks while identifying important parameters,\ncoupled with an outer loop that globally manages the fusion of new and\nhistorical knowledge through redundant knowledge pruning and key knowledge\nmerging. These inner-outer loops iteratively perform multiple rounds of fusion,\nallowing Recurrent-KIF to leverage intermediate training information and\nadaptively adjust fusion strategies based on evolving importance distributions.\nExtensive experiments on two CL benchmarks with various model sizes (from 770M\nto 13B) demonstrate that Recurrent-KIF effectively mitigates catastrophic\nforgetting and enhances knowledge transfer."}
{"id": "2502.17384", "pdf": "https://arxiv.org/pdf/2502.17384", "abs": "https://arxiv.org/abs/2502.17384", "authors": ["Sasha Voitovych", "Mahdi Haghifam", "Idan Attias", "Gintare Karolina Dziugaite", "Roi Livni", "Daniel M. Roy"], "title": "On Traceability in $\\ell_p$ Stochastic Convex Optimization", "categories": ["cs.LG"], "comment": "52 pages. Changes compared to v1: major rewrite of the introduction,\n  updated related work, and various presentation improvements", "summary": "In this paper, we investigate the necessity of traceability for accurate\nlearning in stochastic convex optimization (SCO) under $\\ell_p$ geometries.\nInformally, we say a learning algorithm is $m$-traceable if, by analyzing its\noutput, it is possible to identify at least $m$ of its training samples. Our\nmain results uncover a fundamental tradeoff between traceability and excess\nrisk in SCO. For every $p\\in [1,\\infty)$, we establish the existence of an\nexcess risk threshold below which every sample-efficient learner is traceable\nwith the number of samples which is a constant fraction of its training sample.\nFor $p\\in [1,2]$, this threshold coincides with the best excess risk of\ndifferentially private (DP) algorithms, i.e., above this threshold, there exist\nalgorithms that are not traceable, which corresponds to a sharp phase\ntransition. For $p \\in (2,\\infty)$, this threshold instead gives novel lower\nbounds for DP learning, partially closing an open problem in this setup. En\nroute to establishing these results, we prove a sparse variant of the\nfingerprinting lemma, which is of independent interest to the community."}
{"id": "2503.03854", "pdf": "https://arxiv.org/pdf/2503.03854", "abs": "https://arxiv.org/abs/2503.03854", "authors": ["IÃ±igo Alonso", "Gorka Azkune", "Ander Salaberria", "Jeremy Barnes", "Oier Lopez de Lacalle"], "title": "Vision-Language Models Struggle to Align Entities across Modalities", "categories": ["cs.CL"], "comment": "Accepted Findings ACL 2025", "summary": "Cross-modal entity linking refers to the ability to align entities and their\nattributes across different modalities. While cross-modal entity linking is a\nfundamental skill needed for real-world applications such as multimodal code\ngeneration, fake news detection, or scene understanding, it has not been\nthoroughly studied in the literature. In this paper, we introduce a new task\nand benchmark to address this gap. Our benchmark, MATE, consists of 5.5k\nevaluation instances featuring visual scenes aligned with their textual\nrepresentations. To evaluate cross-modal entity linking performance, we design\na question-answering task that involves retrieving one attribute of an object\nin one modality based on a unique attribute of that object in another modality.\nWe evaluate state-of-the-art Vision-Language Models (VLMs) and humans on this\ntask, and find that VLMs struggle significantly compared to humans,\nparticularly as the number of objects in the scene increases. Our analysis also\nshows that, while chain-of-thought prompting can improve VLM performance,\nmodels remain far from achieving human-level proficiency. These findings\nhighlight the need for further research in cross-modal entity linking and show\nthat MATE is a strong benchmark to support that progress."}
{"id": "2505.19610", "pdf": "https://arxiv.org/pdf/2505.19610", "abs": "https://arxiv.org/abs/2505.19610", "authors": ["Jiaxin Song", "Yixu Wang", "Jie Li", "Rui Yu", "Yan Teng", "Xingjun Ma", "Yingchun Wang"], "title": "JailBound: Jailbreaking Internal Safety Boundaries of Vision-Language Models", "categories": ["cs.CV"], "comment": null, "summary": "Vision-Language Models (VLMs) exhibit impressive performance, yet the\nintegration of powerful vision encoders has significantly broadened their\nattack surface, rendering them increasingly susceptible to jailbreak attacks.\nHowever, lacking well-defined attack objectives, existing jailbreak methods\noften struggle with gradient-based strategies prone to local optima and lacking\nprecise directional guidance, and typically decouple visual and textual\nmodalities, thereby limiting their effectiveness by neglecting crucial\ncross-modal interactions. Inspired by the Eliciting Latent Knowledge (ELK)\nframework, we posit that VLMs encode safety-relevant information within their\ninternal fusion-layer representations, revealing an implicit safety decision\nboundary in the latent space. This motivates exploiting boundary to steer model\nbehavior. Accordingly, we propose JailBound, a novel latent space jailbreak\nframework comprising two stages: (1) Safety Boundary Probing, which addresses\nthe guidance issue by approximating decision boundary within fusion layer's\nlatent space, thereby identifying optimal perturbation directions towards the\ntarget region; and (2) Safety Boundary Crossing, which overcomes the\nlimitations of decoupled approaches by jointly optimizing adversarial\nperturbations across both image and text inputs. This latter stage employs an\ninnovative mechanism to steer the model's internal state towards\npolicy-violating outputs while maintaining cross-modal semantic consistency.\nExtensive experiments on six diverse VLMs demonstrate JailBound's efficacy,\nachieves 94.32% white-box and 67.28% black-box attack success averagely, which\nare 6.17% and 21.13% higher than SOTA methods, respectively. Our findings\nexpose a overlooked safety risk in VLMs and highlight the urgent need for more\nrobust defenses. Warning: This paper contains potentially sensitive, harmful\nand offensive content."}
{"id": "2502.17821", "pdf": "https://arxiv.org/pdf/2502.17821", "abs": "https://arxiv.org/abs/2502.17821", "authors": ["Rui Liu", "Yu Shen", "Peng Gao", "Pratap Tokekar", "Ming Lin"], "title": "CAML: Collaborative Auxiliary Modality Learning for Multi-Agent Systems", "categories": ["cs.RO", "cs.AI", "cs.LG"], "comment": null, "summary": "Multi-modal learning has become a crucial technique for improving the\nperformance of machine learning applications across domains such as autonomous\ndriving, robotics, and perception systems. However, in certain scenarios,\nparticularly in resource-constrained environments, some modalities available\nduring training may be absent during inference. While existing frameworks\neffectively utilize multiple data sources during training and enable inference\nwith reduced modalities, they are primarily designed for single-agent settings.\nThis poses a critical limitation in dynamic environments such as connected\nautonomous vehicles (CAV), where incomplete data coverage can lead to\ndecision-making blind spots. Conversely, some works explore multi-agent\ncollaboration but without addressing missing modality at test time. To overcome\nthese limitations, we propose Collaborative Auxiliary Modality Learning (CAML),\na novel multi-modal multi-agent framework that enables agents to collaborate\nand share multi-modal data during training, while allowing inference with\nreduced modalities during testing. Experimental results in collaborative\ndecision-making for CAV in accident-prone scenarios demonstrate that CAML\nachieves up to a ${\\bf 58.1}\\%$ improvement in accident detection.\nAdditionally, we validate CAML on real-world aerial-ground robot data for\ncollaborative semantic segmentation, achieving up to a ${\\bf 10.6}\\%$\nimprovement in mIoU."}
{"id": "2502.18807", "pdf": "https://arxiv.org/pdf/2502.18807", "abs": "https://arxiv.org/abs/2502.18807", "authors": ["Ruifeng Tan", "Weixiang Hong", "Jiayue Tang", "Xibin Lu", "Ruijun Ma", "Xiang Zheng", "Jia Li", "Jiaqiang Huang", "Tong-Yi Zhang"], "title": "BatteryLife: A Comprehensive Dataset and Benchmark for Battery Life Prediction", "categories": ["cs.LG", "cs.AI", "cs.DL"], "comment": "Accepted by KDD 2025. Typos and data statistics mistakes are fixed", "summary": "Battery Life Prediction (BLP), which relies on time series data produced by\nbattery degradation tests, is crucial for battery utilization, optimization,\nand production. Despite impressive advancements, this research area faces three\nkey challenges. Firstly, the limited size of existing datasets impedes insights\ninto modern battery life data. Secondly, most datasets are restricted to\nsmall-capacity lithium-ion batteries tested under a narrow range of diversity\nin labs, raising concerns about the generalizability of findings. Thirdly,\ninconsistent and limited benchmarks across studies obscure the effectiveness of\nbaselines and leave it unclear if models popular in other time series fields\nare effective for BLP. To address these challenges, we propose BatteryLife, a\ncomprehensive dataset and benchmark for BLP. BatteryLife integrates 16\ndatasets, offering a 2.5 times sample size compared to the previous largest\ndataset, and provides the most diverse battery life resource with batteries\nfrom 8 formats, 59 chemical systems, 9 operating temperatures, and 421\ncharge/discharge protocols, including both laboratory and industrial tests.\nNotably, BatteryLife is the first to release battery life datasets of zinc-ion\nbatteries, sodium-ion batteries, and industry-tested large-capacity lithium-ion\nbatteries. With the comprehensive dataset, we revisit the effectiveness of\nbaselines popular in this and other time series fields. Furthermore, we propose\nCyclePatch, a plug-in technique that can be employed in various neural\nnetworks. Extensive benchmarking of 18 methods reveals that models popular in\nother time series fields can be unsuitable for BLP, and CyclePatch consistently\nimproves model performance establishing state-of-the-art benchmarks. Moreover,\nBatteryLife evaluates model performance across aging conditions and domains.\nBatteryLife is available at https://github.com/Ruifeng-Tan/BatteryLife."}
{"id": "2503.04378", "pdf": "https://arxiv.org/pdf/2503.04378", "abs": "https://arxiv.org/abs/2503.04378", "authors": ["Zhilin Wang", "Jiaqi Zeng", "Olivier Delalleau", "Daniel Egert", "Ellie Evans", "Hoo-Chang Shin", "Felipe Soares", "Yi Dong", "Oleksii Kuchaiev"], "title": "HelpSteer3: Human-Annotated Feedback and Edit Data to Empower Inference-Time Scaling in Open-Ended General-Domain Tasks", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "23 pages, 2 figures, Accepted to ACL 2025 Main", "summary": "Inference-Time Scaling has been critical to the success of recent models such\nas OpenAI o1 and DeepSeek R1. However, many techniques used to train models for\ninference-time scaling require tasks to have answers that can be verified,\nlimiting their application to domains such as math, coding and logical\nreasoning. We take inspiration from how humans make first attempts, ask for\ndetailed feedback from others and make improvements based on such feedback\nacross a wide spectrum of open-ended endeavors. To this end, we collect\nHelpSteer3 data to train dedicated Feedback and Edit Models that are capable of\nperforming inference-time scaling for open-ended general-domain tasks. In our\nsetup, one model generates an initial response, which are given feedback by a\nsecond model, that are then used by a third model to edit the response. We show\nthat performance on Arena Hard, a benchmark strongly predictive of Chatbot\nArena Elo can be boosted by scaling the number of initial response drafts,\neffective feedback and edited responses. When scaled optimally, our setup based\non 70B models from the Llama 3 family can reach SoTA performance on Arena Hard\nat 92.7 as of 5 Mar 2025, surpassing OpenAI o1-preview-2024-09-12 with 90.4 and\nDeepSeek R1 with 92.3."}
{"id": "2505.19883", "pdf": "https://arxiv.org/pdf/2505.19883", "abs": "https://arxiv.org/abs/2505.19883", "authors": ["Shintaro Ito", "Natsuki Takama", "Koichi Ito", "Hwann-Tzong Chen", "Takafumi Aoki"], "title": "ErpGS: Equirectangular Image Rendering enhanced with 3D Gaussian Regularization", "categories": ["cs.CV"], "comment": "Accepted to ICIP2025. Project page: https://gsisaoki.github.io/ERPGS/", "summary": "The use of multi-view images acquired by a 360-degree camera can reconstruct\na 3D space with a wide area. There are 3D reconstruction methods from\nequirectangular images based on NeRF and 3DGS, as well as Novel View Synthesis\n(NVS) methods. On the other hand, it is necessary to overcome the large\ndistortion caused by the projection model of a 360-degree camera when\nequirectangular images are used. In 3DGS-based methods, the large distortion of\nthe 360-degree camera model generates extremely large 3D Gaussians, resulting\nin poor rendering accuracy. We propose ErpGS, which is Omnidirectional GS based\non 3DGS to realize NVS addressing the problems. ErpGS introduce some rendering\naccuracy improvement techniques: geometric regularization, scale\nregularization, and distortion-aware weights and a mask to suppress the effects\nof obstacles in equirectangular images. Through experiments on public datasets,\nwe demonstrate that ErpGS can render novel view images more accurately than\nconventional methods."}
{"id": "2502.18891", "pdf": "https://arxiv.org/pdf/2502.18891", "abs": "https://arxiv.org/abs/2502.18891", "authors": ["Ziyuan Zhong", "Junyang Zhou"], "title": "Dynamic Classification: Leveraging Self-Supervised Classification to Enhance Prediction Performance", "categories": ["cs.LG", "cs.AI", "J.0; I.0"], "comment": "18 pages, 6 figures", "summary": "In this study, we propose an innovative dynamic classification algorithm\naimed at achieving zero missed detections and minimal false positives,acritical\nin safety-critical domains (e.g., medical diagnostics) where undetected cases\nrisk severe outcomes. The algorithm partitions data in a self-supervised\nlearning-generated way, which allows the model to learn from the training set\nto understand the data distribution and thereby divides training set and test\nset into N different subareas. The training and test subsets in the same\nsubarea will have nearly the same boundary. For each subarea, there will be the\nsame type of model, such as linear or random forest model, to predict the\nresults of that subareas. In addition, the algorithm uses subareas boundary to\nrefine predictions results and filter out substandard results without requiring\nadditional models. This approach allows each model to operate within a smaller\ndata range and remove the inaccurate prediction results, thereby improving\noverall accuracy. Experimental results show that, with minimal data\npartitioning errors, the algorithm achieves exceptional performance with zero\nmissed detections and minimal false positives, outperforming existing ensembles\nlike XGBoost or LGBM model. Even with larger classification errors, it remains\ncomparable to that of state-of-the-art models.\n  Key innovations include self-supervised classification learning, small-range\nsubset predictions, and optimizing the prediction results and eliminate the\nunqualified ones without the need for additional model support. Although the\nalgorithm still has room for improvement in automatic parameter tuning and\nefficiency, it demonstrates outstanding performance across multiple datasets.\nFuture work will focus on optimizing the classification components to enhance\nrobustness and adaptability."}
{"id": "2502.19305", "pdf": "https://arxiv.org/pdf/2502.19305", "abs": "https://arxiv.org/abs/2502.19305", "authors": ["Shiqi Wang", "Zhibo Zhang", "Libing Fang", "Cam-Tu Nguyen", "Wenzhong Li"], "title": "Corporate Fraud Detection in Rich-yet-Noisy Financial Graph", "categories": ["cs.LG", "cs.AI", "q-fin.RM", "q-fin.ST"], "comment": null, "summary": "Corporate fraud detection aims to automatically recognize companies that\nconduct wrongful activities such as fraudulent financial statements or illegal\ninsider trading. Previous learning-based methods fail to effectively integrate\nrich interactions in the company network. To close this gap, we collect 18-year\nfinancial records in China to form three graph datasets with fraud labels. We\nanalyze the characteristics of the financial graphs, highlighting two\npronounced issues: (1) information overload: the dominance of (noisy)\nnon-company nodes over company nodes hinders the message-passing process in\nGraph Convolution Networks (GCN); and (2) hidden fraud: there exists a large\npercentage of possible undetected violations in the collected data. The hidden\nfraud problem will introduce noisy labels in the training dataset and\ncompromise fraud detection results. To handle such challenges, we propose a\nnovel graph-based method, namely, Knowledge-enhanced GCN with Robust Two-stage\nLearning (${\\rm KeGCN}_{R}$), which leverages Knowledge Graph Embeddings to\nmitigate the information overload and effectively learns rich representations.\nThe proposed model adopts a two-stage learning method to enhance robustness\nagainst hidden frauds. Extensive experimental results not only confirm the\nimportance of interactions but also show the superiority of ${\\rm KeGCN}_{R}$\nover a number of strong baselines in terms of fraud detection effectiveness and\nrobustness."}
{"id": "2503.05268", "pdf": "https://arxiv.org/pdf/2503.05268", "abs": "https://arxiv.org/abs/2503.05268", "authors": ["Francesco Cazzaro", "Justin Kleindienst", "Sofia Marquez Gomez", "Ariadna Quattoni"], "title": "ZOGRASCOPE: A New Benchmark for Semantic Parsing over Property Graphs", "categories": ["cs.CL"], "comment": null, "summary": "In recent years, the need for natural language interfaces to knowledge graphs\nhas become increasingly important since they enable easy and efficient access\nto the information contained in them. In particular, property graphs (PGs) have\nseen increased adoption as a means of representing complex structured\ninformation. Despite their growing popularity in industry, PGs remain\nrelatively underrepresented in semantic parsing research with a lack of\nresources for evaluation. To address this gap, we introduce ZOGRASCOPE, a\nbenchmark designed specifically for PGs and queries written in Cypher. Our\nbenchmark includes a diverse set of manually annotated queries of varying\ncomplexity and is organized into three partitions: iid, compositional and\nlength. We complement this paper with a set of experiments that test the\nperformance of different LLMs in a variety of learning settings."}
{"id": "2505.21541", "pdf": "https://arxiv.org/pdf/2505.21541", "abs": "https://arxiv.org/abs/2505.21541", "authors": ["Zitong Wang", "Hang Zhao", "Qianyu Zhou", "Xuequan Lu", "Xiangtai Li", "Yiren Song"], "title": "DiffDecompose: Layer-Wise Decomposition of Alpha-Composited Images via Diffusion Transformers", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Diffusion models have recently motivated great success in many generation\ntasks like object removal. Nevertheless, existing image decomposition methods\nstruggle to disentangle semi-transparent or transparent layer occlusions due to\nmask prior dependencies, static object assumptions, and the lack of datasets.\nIn this paper, we delve into a novel task: Layer-Wise Decomposition of\nAlpha-Composited Images, aiming to recover constituent layers from single\noverlapped images under the condition of semi-transparent/transparent alpha\nlayer non-linear occlusion. To address challenges in layer ambiguity,\ngeneralization, and data scarcity, we first introduce AlphaBlend, the first\nlarge-scale and high-quality dataset for transparent and semi-transparent layer\ndecomposition, supporting six real-world subtasks (e.g., translucent flare\nremoval, semi-transparent cell decomposition, glassware decomposition).\nBuilding on this dataset, we present DiffDecompose, a diffusion\nTransformer-based framework that learns the posterior over possible layer\ndecompositions conditioned on the input image, semantic prompts, and blending\ntype. Rather than regressing alpha mattes directly, DiffDecompose performs\nIn-Context Decomposition, enabling the model to predict one or multiple layers\nwithout per-layer supervision, and introduces Layer Position Encoding Cloning\nto maintain pixel-level correspondence across layers. Extensive experiments on\nthe proposed AlphaBlend dataset and public LOGO dataset verify the\neffectiveness of DiffDecompose. The code and dataset will be available upon\npaper acceptance. Our code will be available at:\nhttps://github.com/Wangzt1121/DiffDecompose."}
{"id": "2502.19668", "pdf": "https://arxiv.org/pdf/2502.19668", "abs": "https://arxiv.org/abs/2502.19668", "authors": ["Mingsheng Cai", "Jiuming Jiang", "Wenhao Huang", "Che Liu", "Rossella Arcucci"], "title": "SuPreME: A Supervised Pre-training Framework for Multimodal ECG Representation Learning", "categories": ["eess.SP", "cs.AI", "cs.CL", "cs.LG"], "comment": null, "summary": "Cardiovascular diseases are a leading cause of death and disability\nworldwide. Electrocardiogram (ECG) is critical for diagnosing and monitoring\ncardiac health, but obtaining large-scale annotated ECG datasets is\nlabor-intensive and time-consuming. Recent ECG Self-Supervised Learning (eSSL)\nmethods mitigate this by learning features without extensive labels but fail to\ncapture fine-grained clinical semantics and require extensive task-specific\nfine-tuning. To address these challenges, we propose $\\textbf{SuPreME}$, a\n$\\textbf{Su}$pervised $\\textbf{Pre}$-training framework for\n$\\textbf{M}$ultimodal $\\textbf{E}$CG representation learning. SuPreME is\npre-trained using structured diagnostic labels derived from ECG report entities\nthrough a one-time offline extraction with Large Language Models (LLMs), which\nhelp denoise, standardize cardiac concepts, and improve clinical representation\nlearning. By fusing ECG signals with textual cardiac queries instead of fixed\nlabels, SuPreME enables zero-shot classification of unseen conditions without\nfurther fine-tuning. We evaluate SuPreME on six downstream datasets covering\n106 cardiac conditions, achieving superior zero-shot AUC performance of\n$77.20\\%$, surpassing state-of-the-art eSSLs by $4.98\\%$. Results demonstrate\nSuPreME's effectiveness in leveraging structured, clinically relevant knowledge\nfor high-quality ECG representations."}
{"id": "2502.19859", "pdf": "https://arxiv.org/pdf/2502.19859", "abs": "https://arxiv.org/abs/2502.19859", "authors": ["Stefano Viel", "Luca Viano", "Volkan Cevher"], "title": "IL-SOAR : Imitation Learning with Soft Optimistic Actor cRitic", "categories": ["cs.LG"], "comment": null, "summary": "This paper introduces the SOAR framework for imitation learning. SOAR is an\nalgorithmic template that learns a policy from expert demonstrations with a\nprimal dual style algorithm that alternates cost and policy updates. Within the\npolicy updates, the SOAR framework uses an actor critic method with multiple\ncritics to estimate the critic uncertainty and build an optimistic critic\nfundamental to drive exploration. When instantiated in the tabular setting, we\nget a provable algorithm with guarantees that matches the best known results in\n$\\epsilon$. Practically, the SOAR template is shown to boost consistently the\nperformance of imitation learning algorithms based on Soft Actor Critic such as\nf-IRL, ML-IRL and CSIL in several MuJoCo environments. Overall, thanks to SOAR,\nthe required number of episodes to achieve the same performance is reduced by\nhalf."}
{"id": "2503.07067", "pdf": "https://arxiv.org/pdf/2503.07067", "abs": "https://arxiv.org/abs/2503.07067", "authors": ["Jongwoo Ko", "Tianyi Chen", "Sungnyun Kim", "Tianyu Ding", "Luming Liang", "Ilya Zharkov", "Se-Young Yun"], "title": "DistiLLM-2: A Contrastive Approach Boosts the Distillation of LLMs", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "ICML2025 Spotlight", "summary": "Despite the success of distillation in large language models (LLMs), most\nprior work applies identical loss functions to both teacher- and\nstudent-generated data. These strategies overlook the synergy between loss\nformulations and data types, leading to a suboptimal performance boost in\nstudent models. To address this, we propose DistiLLM-2, a contrastive approach\nthat simultaneously increases the likelihood of teacher responses and decreases\nthat of student responses by harnessing this synergy. Our extensive experiments\nshow that DistiLLM-2 not only builds high-performing student models across a\nwide range of tasks, including instruction-following and code generation, but\nalso supports diverse applications, such as preference alignment and\nvision-language extensions. These findings highlight the potential of a\ncontrastive approach to enhance the efficacy of LLM distillation by effectively\naligning teacher and student models across varied data types."}
{"id": "2505.21649", "pdf": "https://arxiv.org/pdf/2505.21649", "abs": "https://arxiv.org/abs/2505.21649", "authors": ["Keanu Nichols", "Nazia Tasnim", "Yuting Yan", "Nicholas Ikechukwu", "Elva Zou", "Deepti Ghadiyaram", "Bryan A. Plummer"], "title": "Right Side Up? Disentangling Orientation Understanding in MLLMs with Fine-grained Multi-axis Perception Tasks", "categories": ["cs.CV"], "comment": null, "summary": "Object orientation understanding represents a fundamental challenge in visual\nperception critical for applications like robotic manipulation and augmented\nreality. Current vision-language benchmarks fail to isolate this capability,\noften conflating it with positional relationships and general scene\nunderstanding. We introduce DORI (Discriminative Orientation Reasoning\nIntelligence), a comprehensive benchmark establishing object orientation\nperception as a primary evaluation target. DORI assesses four dimensions of\norientation comprehension: frontal alignment, rotational transformations,\nrelative directional relationships, and canonical orientation understanding.\nThrough carefully curated tasks from 11 datasets spanning 67 object categories\nacross synthetic and real-world scenarios, DORI provides insights on how\nmulti-modal systems understand object orientations. Our evaluation of 15\nstate-of-the-art vision-language models reveals critical limitations: even the\nbest models achieve only 54.2% accuracy on coarse tasks and 33.0% on granular\norientation judgments, with performance deteriorating for tasks requiring\nreference frame shifts or compound rotations. These findings demonstrate the\nneed for dedicated orientation representation mechanisms, as models show\nsystematic inability to perform precise angular estimations, track orientation\nchanges across viewpoints, and understand compound rotations - suggesting\nlimitations in their internal 3D spatial representations. As the first\ndiagnostic framework specifically designed for orientation awareness in\nmultimodal systems, DORI offers implications for improving robotic control, 3D\nscene reconstruction, and human-AI interaction in physical environments. DORI\ndata: https://huggingface.co/datasets/appledora/DORI-Benchmark"}
{"id": "2503.08720", "pdf": "https://arxiv.org/pdf/2503.08720", "abs": "https://arxiv.org/abs/2503.08720", "authors": ["Weina Jin", "Nicholas Vincent", "Ghassan Hamarneh"], "title": "AI for Just Work: Constructing Diverse Imaginations of AI beyond \"Replacing Humans\"", "categories": ["cs.CY", "cs.AI"], "comment": null, "summary": "\"why\" we develop AI. Lacking critical reflections on the general visions and\npurposes of AI may make the community vulnerable to manipulation. In this\nposition paper, we explore the \"why\" question of AI. We denote answers to the\n\"why\" question the imaginations of AI, which depict our general visions,\nframes, and mindsets for the prospects of AI. We identify that the prevailing\nvision in the AI community is largely a monoculture that emphasizes objectives\nsuch as replacing humans and improving productivity. Our critical examination\nof this mainstream imagination highlights its underpinning and potentially\nunjust assumptions. We then call to diversify our collective imaginations of\nAI, embedding ethical assumptions from the outset in the imaginations of AI. To\nfacilitate the community's pursuit of diverse imaginations, we demonstrate one\nprocess for constructing a new imagination of \"AI for just work,\" and showcase\nits application in the medical image synthesis task to make it more ethical. We\nhope this work will help the AI community to open critical dialogues with civil\nsociety on the visions and purposes of AI, and inspire more technical works and\nadvocacy in pursuit of diverse and ethical imaginations to restore the value of\nAI for the public good."}
{"id": "2503.00812", "pdf": "https://arxiv.org/pdf/2503.00812", "abs": "https://arxiv.org/abs/2503.00812", "authors": ["Hongzhi Luan", "Changxin Tian", "Zhaoxin Huan", "Xiaolu Zhang", "Kunlong Chen", "Zhiqiang Zhang", "Jun Zhou"], "title": "BOSE: A Systematic Evaluation Method Optimized for Base Models", "categories": ["cs.LG"], "comment": null, "summary": "This paper poses two critical issues in evaluating base models (without\npost-training): (1) Unstable evaluation during training: in the early stages of\npre-training, the models lack the capability to answer questions as required,\nleading to unstable evaluation results. This instability makes it difficult to\nprovide solid conclusions to guide the training, especially for key experiments\nsuch as data ablation and scaling law. (2) Inconsistency between base and\ninstruct models: base models generally exhibit poorer evaluation performance\ncompared to corresponding instruct models. This gap poses a challenge for\nassessing whether a base model with better evaluation can truly lead to a\nbetter instruct model. To address these issues, we propose Base model Oriented\nSystematic Evaluation (BOSE), a method specifically designed to optimize the\nevaluation of base models. Specifically, BOSE introduces two key innovations:\nIn-Context Light-instruction Prompt (ICLiP) for open-ended tasks and Blank-ppl\nfor multi-choice tasks with candidate options, which transforms the standard\nperplexity (ppl) metric into a fill-in-the-blank format to mitigate early-stage\nevaluation fluctuations. Furthermore, we are the first to propose Kendall's\nrank correlation to quantitatively measure the evaluation stability and\nconsistency. Experimental results demonstrate that BOSE significantly enhances\nboth the stability of evaluations during pre-training and the consistency\nbetween base and instruct models, thereby providing more reliable guidance for\nthe LLMs' training."}
{"id": "2503.15289", "pdf": "https://arxiv.org/pdf/2503.15289", "abs": "https://arxiv.org/abs/2503.15289", "authors": ["Junnan Zhu", "Min Xiao", "Yining Wang", "Feifei Zhai", "Yu Zhou", "Chengqing Zong"], "title": "TROVE: A Challenge for Fine-Grained Text Provenance via Source Sentence Tracing and Relationship Classification", "categories": ["cs.CL"], "comment": "To appear in ACL 2025 (Main)", "summary": "LLMs have achieved remarkable fluency and coherence in text generation, yet\ntheir widespread adoption has raised concerns about content reliability and\naccountability. In high-stakes domains, it is crucial to understand where and\nhow the content is created. To address this, we introduce the Text pROVEnance\n(TROVE) challenge, designed to trace each sentence of a target text back to\nspecific source sentences within potentially lengthy or multi-document inputs.\nBeyond identifying sources, TROVE annotates the fine-grained relationships\n(quotation, compression, inference, and others), providing a deep understanding\nof how each target sentence is formed. To benchmark TROVE, we construct our\ndataset by leveraging three public datasets covering 11 diverse scenarios\n(e.g., QA and summarization) in English and Chinese, spanning source texts of\nvarying lengths (0-5k, 5-10k, 10k+), emphasizing the multi-document and\nlong-document settings essential for provenance. To ensure high-quality data,\nwe employ a three-stage annotation process: sentence retrieval, GPT-4o\nprovenance, and human provenance. We evaluate 11 LLMs under direct prompting\nand retrieval-augmented paradigms, revealing that retrieval is essential for\nrobust performance, larger models perform better in complex relationship\nclassification, and closed-source models often lead, yet open-source models\nshow significant promise, particularly with retrieval augmentation. We make our\ndataset available here: https://github.com/ZNLP/ZNLP-Dataset."}
{"id": "2505.21850", "pdf": "https://arxiv.org/pdf/2505.21850", "abs": "https://arxiv.org/abs/2505.21850", "authors": ["Yanbei Jiang", "Yihao Ding", "Chao Lei", "Jiayang Ao", "Jey Han Lau", "Krista A. Ehinger"], "title": "Beyond Perception: Evaluating Abstract Visual Reasoning through Multi-Stage Task", "categories": ["cs.CV", "cs.AI"], "comment": "Accepted at ACL Findings", "summary": "Current Multimodal Large Language Models (MLLMs) excel in general visual\nreasoning but remain underexplored in Abstract Visual Reasoning (AVR), which\ndemands higher-order reasoning to identify abstract rules beyond simple\nperception. Existing AVR benchmarks focus on single-step reasoning, emphasizing\nthe end result but neglecting the multi-stage nature of reasoning process. Past\nstudies found MLLMs struggle with these benchmarks, but it doesn't explain how\nthey fail. To address this gap, we introduce MultiStAR, a Multi-Stage AVR\nbenchmark, based on RAVEN, designed to assess reasoning across varying levels\nof complexity. Additionally, existing metrics like accuracy only focus on the\nfinal outcomes while do not account for the correctness of intermediate steps.\nTherefore, we propose a novel metric, MSEval, which considers the correctness\nof intermediate steps in addition to the final outcomes. We conduct\ncomprehensive experiments on MultiStAR using 17 representative close-source and\nopen-source MLLMs. The results reveal that while existing MLLMs perform\nadequately on basic perception tasks, they continue to face challenges in more\ncomplex rule detection stages."}
{"id": "2503.09257", "pdf": "https://arxiv.org/pdf/2503.09257", "abs": "https://arxiv.org/abs/2503.09257", "authors": ["Haixing Gong", "Hui Zou", "Xingzhou Liang", "Shiyuan Meng", "Pinlong Cai", "Xingcheng Xu", "Jingjing Qu"], "title": "A Global Dataset Mapping the AI Innovation from Academic Research to Industrial Patents", "categories": ["cs.DB", "cs.AI", "cs.DL"], "comment": "38 pages and 4 figures", "summary": "In the rapidly evolving field of artificial intelligence (AI), mapping\ninnovation patterns and understanding effective technology transfer from\nresearch to applications are essential for economic growth. However, existing\ndata infrastructures suffer from fragmentation, incomplete coverage, and\ninsufficient evaluative capacity. Here, we present DeepInnovationAI, a\ncomprehensive global dataset containing three structured files.\nDeepPatentAI.csv: Contains 2,356,204 patent records with 8 field-specific\nattributes. DeepDiveAI.csv: Encompasses 3,511,929 academic publications with 13\nmetadata fields. These two datasets leverage large language models,\nmultilingual text analysis and dual-layer BERT classifiers to accurately\nidentify AI-related content, while utilizing hypergraph analysis to create\nrobust innovation metrics. Additionally, DeepCosineAI.csv: By applying semantic\nvector proximity analysis, this file contains 3,511,929 most relevant\npaper-patent pairs, each described by 3 metadata fields, to facilitate the\nidentification of potential knowledge flows. DeepInnovationAI enables\nresearchers, policymakers, and industry leaders to anticipate trends and\nidentify collaboration opportunities. With extensive temporal and geographical\nscope, it supports detailed analysis of technological development patterns and\ninternational competition dynamics, establishing a foundation for modeling AI\ninnovation and technology transfer processes."}
{"id": "2503.05771", "pdf": "https://arxiv.org/pdf/2503.05771", "abs": "https://arxiv.org/abs/2503.05771", "authors": ["Keqiang Yan", "Montgomery Bohde", "Andrii Kryvenko", "Ziyu Xiang", "Kaiji Zhao", "Siya Zhu", "Saagar Kolachina", "DoÄuhan SarÄ±tÃ¼rk", "Jianwen Xie", "Raymundo Arroyave", "Xiaoning Qian", "Xiaofeng Qian", "Shuiwang Ji"], "title": "A Materials Foundation Model via Hybrid Invariant-Equivariant Architectures", "categories": ["cs.LG", "cond-mat.mtrl-sci", "physics.comp-ph"], "comment": "Preprint", "summary": "Machine learning interatomic potentials (MLIPs) can predict energy, force,\nand stress of materials and enable a wide range of downstream discovery tasks.\nA key design choice in MLIPs involves the trade-off between invariant and\nequivariant architectures. Invariant models offer computational efficiency but\nmay not perform as well, especially when predicting high-order outputs. In\ncontrast, equivariant models can capture high-order symmetries, but are\ncomputationally expensive. In this work, we propose HIENet, a hybrid\ninvariant-equivariant materials interatomic potential model that integrates\nboth invariant and equivariant message passing layers, while provably\nsatisfying key physical constraints. HIENet achieves state-of-the-art\nperformance with considerable computational speedups over prior models.\nExperimental results on both common benchmarks and downstream materials\ndiscovery tasks demonstrate the efficiency and effectiveness of HIENet."}
{"id": "2503.18491", "pdf": "https://arxiv.org/pdf/2503.18491", "abs": "https://arxiv.org/abs/2503.18491", "authors": ["Shuo Yang", "Siwen Luo", "Soyeon Caren Han", "Eduard Hovy"], "title": "MAGIC-VQA: Multimodal And Grounded Inference with Commonsense Knowledge for Visual Question Answering", "categories": ["cs.CL"], "comment": "Findings of ACL 2025", "summary": "Visual Question Answering (VQA) requires reasoning across visual and textual\nmodalities, yet Large Vision-Language Models (LVLMs) often lack integrated\ncommonsense knowledge, limiting their robustness in real-world scenarios. To\naddress this, we introduce MAGIC-VQA, a novel framework that enhances VQA by\nsystematically integrating commonsense knowledge with LVLMs. MAGIC-VQA employs\na three-stage process: (1) Explicit Knowledge Integration from external\nsources, (2) By-Type Post-Processing for contextual refinement, and (3)\nImplicit Knowledge Augmentation using a Graph Neural Network (GNN) for\nstructured reasoning. While GNNs bring greater depth to structured inference,\nthey enable superior relational inference beyond LVLMs. MAGIC-VQA bridges a key\ngap by unifying commonsensse knowledge with LVLM-driven reasoning, eliminating\nthe need for extensive pre-training or complex prompt tuning. Our framework\nachieves state-of-the-art performance on benchmark datasets, significantly\nimproving commonsense reasoning in VQA."}
{"id": "2505.22111", "pdf": "https://arxiv.org/pdf/2505.22111", "abs": "https://arxiv.org/abs/2505.22111", "authors": ["Woonho Ko", "Jin Bok Park", "Il Yong Chun"], "title": "Autoregression-free video prediction using diffusion model for mitigating error propagation", "categories": ["cs.CV"], "comment": "6 pages, 4 figures, 2 tables", "summary": "Existing long-term video prediction methods often rely on an autoregressive\nvideo prediction mechanism. However, this approach suffers from error\npropagation, particularly in distant future frames. To address this limitation,\nthis paper proposes the first AutoRegression-Free (ARFree) video prediction\nframework using diffusion models. Different from an autoregressive video\nprediction mechanism, ARFree directly predicts any future frame tuples from the\ncontext frame tuple. The proposed ARFree consists of two key components: 1) a\nmotion prediction module that predicts a future motion using motion feature\nextracted from the context frame tuple; 2) a training method that improves\nmotion continuity and contextual consistency between adjacent future frame\ntuples. Our experiments with two benchmark datasets show that the proposed\nARFree video prediction framework outperforms several state-of-the-art video\nprediction methods."}
{"id": "2503.17229", "pdf": "https://arxiv.org/pdf/2503.17229", "abs": "https://arxiv.org/abs/2503.17229", "authors": ["Albert Sawczyn", "Jakub Binkowski", "Denis Janiak", "Bogdan Gabrys", "Tomasz Kajdanowicz"], "title": "FactSelfCheck: Fact-Level Black-Box Hallucination Detection for LLMs", "categories": ["cs.LG", "cs.AI", "cs.CL"], "comment": "Preprint", "summary": "Large Language Models (LLMs) frequently generate hallucinated content, posing\nsignificant challenges for applications where factuality is crucial. While\nexisting hallucination detection methods typically operate at the sentence\nlevel or passage level, we propose FactSelfCheck, a novel black-box\nsampling-based method that enables fine-grained fact-level detection. Our\napproach represents text as knowledge graphs consisting of facts in the form of\ntriples. Through analyzing factual consistency across multiple LLM responses,\nwe compute fine-grained hallucination scores without requiring external\nresources or training data. Our evaluation demonstrates that FactSelfCheck\nperforms competitively with leading sentence-level sampling-based methods while\nproviding more detailed insights. Most notably, our fact-level approach\nsignificantly improves hallucination correction, achieving a 35.5% increase in\nfactual content compared to the baseline, while sentence-level SelfCheckGPT\nyields only a 10.6% improvement. The granular nature of our detection enables\nmore precise identification and correction of hallucinated content.\nAdditionally, we contribute a new dataset for evaluating sampling-based methods\n- FavaMultiSamples."}
{"id": "2503.10345", "pdf": "https://arxiv.org/pdf/2503.10345", "abs": "https://arxiv.org/abs/2503.10345", "authors": ["Bowen Wang", "Matteo Zecchin", "Osvaldo Simeone"], "title": "Mirror Online Conformal Prediction with Intermittent Feedback", "categories": ["cs.LG", "eess.SP"], "comment": null, "summary": "Online conformal prediction enables the runtime calibration of a pre-trained\nartificial intelligence model using feedback on its performance. Calibration is\nachieved through set predictions that are updated via online rules so as to\nensure long-term coverage guarantees. While recent research has demonstrated\nthe benefits of incorporating prior knowledge into the calibration process,\nthis has come at the cost of replacing coverage guarantees with less tangible\nregret guarantees based on the quantile loss. This work introduces intermittent\nmirror online conformal prediction (IM-OCP), a novel runtime calibration\nframework that integrates prior knowledge, while maintaining long-term coverage\nand achieving sub-linear regret. IM-OCP features closed-form updates with\nminimal memory complexity, and is designed to operate under potentially\nintermittent feedback."}
{"id": "2503.18991", "pdf": "https://arxiv.org/pdf/2503.18991", "abs": "https://arxiv.org/abs/2503.18991", "authors": ["Ruoxi Cheng", "Haoxuan Ma", "Weixin Wang", "Zhiqiang Wang", "Xiaoshuang Jia", "Simeng Qin", "Xiaochun Cao", "Yang Liu", "Xiaojun Jia"], "title": "Inverse Reinforcement Learning with Dynamic Reward Scaling for LLM Alignment", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "The first three authors contributed equally to this work", "summary": "Robust alignment is vital for safely deploying large language models (LLMs).\nExisting techniques are either reward-based -- training a reward model on\npreference pairs and optimizing with reinforcement learning (RL) -- or\nreward-free -- directly fine-tuning on ranked outputs. Recent research shows\nthat well-tuned reward-based pipelines remain the most robust, and\nsingle-response demonstrations can outperform pairwise preference data.\nHowever, two key challenges remain: (i) imbalanced safety datasets that\nover-represent common hazards while neglecting long-tail threats; and (ii)\nstatic reward models that ignore task difficulty, limiting optimization\nefficiency and attainable gains. To address these limitations, we propose\n\\textbf{DR-IRL}, which dynamically adjusts rewards through inverse\nreinforcement learning. We first construct a balanced safety dataset of seven\nharmful categories using Chain-of-Draft (CoD) template prompts, which reduce\ntoken usage and generation time compared to Chain-of-Thought (CoT). We then\ntrain category-specific reward models on this dataset via IRL. Finally, to\nalign the LLM, we introduce \\textbf{GRPO-S} (Group Relative Policy\nOptimization--Scaling), a variant of GRPO that scales the reward during\noptimization to task difficulty -- data-level hardness measured by CLIP\nsimilarity and model-level responsiveness measured by reward gaps. Extensive\nexperiments on multiple benchmarks and LLMs demonstrate that DR-IRL outperforms\nall baselines in safety alignment while maintaining usefulness."}
{"id": "2505.22465", "pdf": "https://arxiv.org/pdf/2505.22465", "abs": "https://arxiv.org/abs/2505.22465", "authors": ["Zobia Batool", "Huseyin Ozkan", "Erchan Aptoula"], "title": "Single Domain Generalization for Alzheimer's Detection from 3D MRIs with Pseudo-Morphological Augmentations and Contrastive Learning", "categories": ["cs.CV"], "comment": null, "summary": "Although Alzheimer's disease detection via MRIs has advanced significantly\nthanks to contemporary deep learning models, challenges such as class\nimbalance, protocol variations, and limited dataset diversity often hinder\ntheir generalization capacity. To address this issue, this article focuses on\nthe single domain generalization setting, where given the data of one domain, a\nmodel is designed and developed with maximal performance w.r.t. an unseen\ndomain of distinct distribution. Since brain morphology is known to play a\ncrucial role in Alzheimer's diagnosis, we propose the use of learnable\npseudo-morphological modules aimed at producing shape-aware, anatomically\nmeaningful class-specific augmentations in combination with a supervised\ncontrastive learning module to extract robust class-specific representations.\nExperiments conducted across three datasets show improved performance and\ngeneralization capacity, especially under class imbalance and imaging protocol\nvariations. The source code will be made available upon acceptance at\nhttps://github.com/zobia111/SDG-Alzheimer."}
{"id": "2503.17353", "pdf": "https://arxiv.org/pdf/2503.17353", "abs": "https://arxiv.org/abs/2503.17353", "authors": ["Alex Reneau", "Jerry Yao-Chieh Hu", "Zhongfang Zhuang", "Ting-Chun Liu", "Xiang He", "Judah Goldfeder", "Nadav Timor", "Allen G Roush", "Ravid Shwartz-Ziv"], "title": "NdLinear: Don't Flatten! Building Superior Neural Architectures by Preserving N-D Structure", "categories": ["cs.LG", "cs.AI"], "comment": "Code is available at https://github.com/ensemble-core/NdLinear", "summary": "Many high-impact machine learning tasks involve multi-dimensional data such\nas images, volumetric medical scans, and multivariate time-series. Yet, most\nneural architectures flatten these inputs, discarding critical cross-dimension\ninformation. We introduce $\\textbf{NdLinear}$, a novel linear transformation\nthat circumvents this destructive flattening by operating directly on tensors.\nNdLinear applies transformations separately along each data dimension, thereby\npreserving the native data structure. Extensive experiments demonstrate\nNdLinear's capacity to significantly enhance representational power, achieve\ndramatic parameter reductions (often by orders of magnitude), and maintain a\nfavorable computational profile. For instance, when applied to Large Language\nModel finetuning, our $\\textbf{NdLinear-LoRA}$ delivers comparable or improved\naccuracy on reasoning tasks using up to $9\\times$ fewer trainable parameters\nthan standard LoRA. These broad advantages of NdLinear are consistently\nvalidated across diverse neural architectures (CNNs, RNNs, Transformers, MLPs)\nand data domains, including vision, language, time-series, and tabular tasks.\nAs a versatile, drop-in replacement for standard linear layers, NdLinear\nprocesses data in its original N-dimensional form, offering a foundational\ncomponent for developing more efficient and powerful next-generation neural\narchitectures."}
{"id": "2503.17439", "pdf": "https://arxiv.org/pdf/2503.17439", "abs": "https://arxiv.org/abs/2503.17439", "authors": ["Zhuoshi Pan", "Yu Li", "Honglin Lin", "Qizhi Pei", "Zinan Tang", "Wei Wu", "Chenlin Ming", "H. Vicky Zhao", "Conghui He", "Lijun Wu"], "title": "LEMMA: Learning from Errors for MatheMatical Advancement in LLMs", "categories": ["cs.LG", "cs.AI"], "comment": "ACL'25 Findings, Code is available at https://github.com/pzs19/LEMMA", "summary": "Large language models (LLMs) have demonstrated remarkable reasoning\ncapability in solving mathematical problems. However, existing approaches\nprimarily focus on improving the quality of correct training data, e.g.,\ndistilling high-quality correct solutions from advanced models, neglecting the\nvalue contained in error data, potentially hindering the model's reflective\nability. Though some studies attempt to leverage error data, they often involve\ncomplex mechanisms, such as Monte Carlo Tree Search (MCTS) to explore error\nnodes. In this work, we propose to enhance LLMs' reasoning ability by Learning\nfrom Errors for Mathematical Advancement (LEMMA). LEMMA constructs data\nconsisting of an incorrect solution with an erroneous step and a reflection\nconnection to a correct solution for fine-tuning. Specifically, we\nsystematically analyze the model-generated error types and introduce an\nerror-type grounded mistake augmentation method to collect diverse and\nrepresentative errors. Correct solutions are either from fixing the errors or\ngenerating a fresh start. Through a model-aware smooth reflection connection,\nthe erroneous solution is transferred to the correct one. By fine-tuning on the\nconstructed dataset, the model is able to self-correct errors autonomously\nwithin the generation process without relying on external critique models.\nExperimental results demonstrate that LEMMA achieves significant performance\nimprovements over other strong baselines."}
{"id": "2503.22353", "pdf": "https://arxiv.org/pdf/2503.22353", "abs": "https://arxiv.org/abs/2503.22353", "authors": ["Yubo Li", "Yidi Miao", "Xueying Ding", "Ramayya Krishnan", "Rema Padman"], "title": "Firm or Fickle? Evaluating Large Language Models Consistency in Sequential Interactions", "categories": ["cs.CL", "cs.AI"], "comment": "8 pages, 5 figures", "summary": "Large Language Models (LLMs) have shown remarkable capabilities across\nvarious tasks, but their deployment in high-stake domains requires consistent\nand coherent behavior across multiple rounds of user interaction. This paper\nintroduces a comprehensive framework for evaluating and improving LLM response\nconsistency, making three key contributions. Code and data are available at:\nhttps://github.com/yubol-bobo/MT-Consistency. First, we introduce\nPosition-Weighted Consistency (PWC), a metric designed to capture both the\nimportance of early-stage stability and recovery patterns in multi-turn\ninteractions. Second, we present MT-Consistency, a carefully curated benchmark\ndataset spanning diverse domains and difficulty levels, specifically designed\nto evaluate LLM consistency under various challenging follow-up scenarios.\nThird, we introduce Confidence-Aware Response Generation (CARG), a framework\nthat significantly improves response stability by explicitly integrating\ninternal model confidence scores during the generation process. Experimental\nresults demonstrate that CARG significantly improves response stability without\nsacrificing accuracy, offering a practical path toward more dependable LLM\nbehavior in critical, real-world deployments."}
{"id": "2505.22604", "pdf": "https://arxiv.org/pdf/2505.22604", "abs": "https://arxiv.org/abs/2505.22604", "authors": ["Ruixuan Zhang", "He Wang", "Zhengyu Zhao", "Zhiqing Guo", "Xun Yang", "Yunfeng Diao", "Meng Wang"], "title": "Adversarially Robust AI-Generated Image Detection for Free: An Information Theoretic Perspective", "categories": ["cs.CV"], "comment": null, "summary": "Rapid advances in Artificial Intelligence Generated Images (AIGI) have\nfacilitated malicious use, such as forgery and misinformation. Therefore,\nnumerous methods have been proposed to detect fake images. Although such\ndetectors have been proven to be universally vulnerable to adversarial attacks,\ndefenses in this field are scarce. In this paper, we first identify that\nadversarial training (AT), widely regarded as the most effective defense,\nsuffers from performance collapse in AIGI detection. Through an\ninformation-theoretic lens, we further attribute the cause of collapse to\nfeature entanglement, which disrupts the preservation of feature-label mutual\ninformation. Instead, standard detectors show clear feature separation.\nMotivated by this difference, we propose Training-free Robust Detection via\nInformation-theoretic Measures (TRIM), the first training-free adversarial\ndefense for AIGI detection. TRIM builds on standard detectors and quantifies\nfeature shifts using prediction entropy and KL divergence. Extensive\nexperiments across multiple datasets and attacks validate the superiority of\nour TRIM, e.g., outperforming the state-of-the-art defense by 33.88% (28.91%)\non ProGAN (GenImage), while well maintaining original accuracy."}
{"id": "2503.19217", "pdf": "https://arxiv.org/pdf/2503.19217", "abs": "https://arxiv.org/abs/2503.19217", "authors": ["Patrick Diehl", "Nojoud Nader", "Maxim Moraru", "Steven R. Brandt"], "title": "LLM Benchmarking with LLaMA2: Evaluating Code Development Performance Across Multiple Programming Languages", "categories": ["cs.SE", "cs.AI", "cs.LG"], "comment": null, "summary": "The rapid evolution of large language models (LLMs) has opened new\npossibilities for automating various tasks in software development. This paper\nevaluates the capabilities of the Llama 2-70B model in automating these tasks\nfor scientific applications written in commonly used programming languages.\nUsing representative test problems, we assess the model's capacity to generate\ncode, documentation, and unit tests, as well as its ability to translate\nexisting code between commonly used programming languages. Our comprehensive\nanalysis evaluates the compilation, runtime behavior, and correctness of the\ngenerated and translated code. Additionally, we assess the quality of\nautomatically generated code, documentation and unit tests. Our results\nindicate that while Llama 2-70B frequently generates syntactically correct and\nfunctional code for simpler numerical tasks, it encounters substantial\ndifficulties with more complex, parallelized, or distributed computations,\nrequiring considerable manual corrections. We identify key limitations and\nsuggest areas for future improvements to better leverage AI-driven automation\nin scientific computing workflows."}
{"id": "2504.02922", "pdf": "https://arxiv.org/pdf/2504.02922", "abs": "https://arxiv.org/abs/2504.02922", "authors": ["Julian Minder", "ClÃ©ment Dumas", "Caden Juang", "Bilal Chugtai", "Neel Nanda"], "title": "Overcoming Sparsity Artifacts in Crosscoders to Interpret Chat-Tuning", "categories": ["cs.LG", "cs.AI", "cs.CL"], "comment": "42 pages, 31 figures", "summary": "Model diffing is the study of how fine-tuning changes a model's\nrepresentations and internal algorithms. Many behaviors of interest are\nintroduced during fine-tuning, and model diffing offers a promising lens to\ninterpret such behaviors. Crosscoders are a recent model diffing method that\nlearns a shared dictionary of interpretable concepts represented as latent\ndirections in both the base and fine-tuned models, allowing us to track how\nconcepts shift or emerge during fine-tuning. Notably, prior work has observed\nconcepts with no direction in the base model, and it was hypothesized that\nthese model-specific latents were concepts introduced during fine-tuning.\nHowever, we identify two issues which stem from the crosscoders L1 training\nloss that can misattribute concepts as unique to the fine-tuned model, when\nthey really exist in both models. We develop Latent Scaling to flag these\nissues by more accurately measuring each latent's presence across models. In\nexperiments comparing Gemma 2 2B base and chat models, we observe that the\nstandard crosscoder suffers heavily from these issues. Building on these\ninsights, we train a crosscoder with BatchTopK loss and show that it\nsubstantially mitigates these issues, finding more genuinely chat-specific and\nhighly interpretable concepts. We recommend practitioners adopt similar\ntechniques. Using the BatchTopK crosscoder, we successfully identify a set of\nchat-specific latents that are both interpretable and causally effective,\nrepresenting concepts such as $\\textit{false information}$ and\n$\\textit{personal question}$, along with multiple refusal-related latents that\nshow nuanced preferences for different refusal triggers. Overall, our work\nadvances best practices for the crosscoder-based methodology for model diffing\nand demonstrates that it can provide concrete insights into how chat-tuning\nmodifies model behavior."}
{"id": "2504.04042", "pdf": "https://arxiv.org/pdf/2504.04042", "abs": "https://arxiv.org/abs/2504.04042", "authors": ["Kepu Zhang", "Weijie Yu", "Zhongxiang Sun", "Jun Xu"], "title": "An Explicit Syllogistic Legal Reasoning Framework for Large Language Models", "categories": ["cs.CL"], "comment": null, "summary": "Syllogistic reasoning is crucial for sound legal decision-making, allowing\nlegal professionals to draw logical conclusions by applying general principles\nto specific case facts. While large language models (LLMs) can answer legal\nquestions, they often struggle with explicit syllogistic reasoning. Their\noutputs tend to be implicit, unstructured, and consequently, less explainable\nand trustworthy. To overcome these limitations, we introduce SyLeR, a novel\nframework designed to enable LLMs to perform explicit syllogistic legal\nreasoning. SyLeR employs a tree-structured hierarchical retrieval mechanism to\nsynthesize relevant legal statutes and precedents, thereby constructing\ncomprehensive major premises. This is followed by a two-stage fine-tuning\nprocess: an initial supervised fine-tuning warm-up establishes a foundational\nunderstanding of syllogistic reasoning, while reinforcement learning, guided by\na structure-aware reward mechanism, refines the model's capacity to generate\ndiverse, logically sound, and well-structured reasoning paths. We conducted\nextensive experiments to evaluate SyLeR's performance. Our evaluations spanned\ndiverse dimensions, including both in-domain and cross-domain user groups\n(legal laypersons and practitioners), multiple languages (Chinese and French),\nand various LLM backbones (legal-specific and open-domain LLMs). The results\nconsistently demonstrate that SyLeR significantly enhances response accuracy\nand reliably produces explicit, explainable, and trustworthy legal reasoning."}
{"id": "2505.22815", "pdf": "https://arxiv.org/pdf/2505.22815", "abs": "https://arxiv.org/abs/2505.22815", "authors": ["Zhangyi Hu", "Jiemin Wu", "Hua Xu", "Mingqian Liao", "Ninghui Feng", "Bo Gao", "Songning Lai", "Yutao Yue"], "title": "IMTS is Worth Time $\\times$ Channel Patches: Visual Masked Autoencoders for Irregular Multivariate Time Series Prediction", "categories": ["cs.CV", "cs.AI", "cs.LG"], "comment": "ICML 2025", "summary": "Irregular Multivariate Time Series (IMTS) forecasting is challenging due to\nthe unaligned nature of multi-channel signals and the prevalence of extensive\nmissing data. Existing methods struggle to capture reliable temporal patterns\nfrom such data due to significant missing values. While pre-trained foundation\nmodels show potential for addressing these challenges, they are typically\ndesigned for Regularly Sampled Time Series (RTS). Motivated by the visual Mask\nAutoEncoder's (MAE) powerful capability for modeling sparse multi-channel\ninformation and its success in RTS forecasting, we propose VIMTS, a framework\nadapting Visual MAE for IMTS forecasting. To mitigate the effect of missing\nvalues, VIMTS first processes IMTS along the timeline into feature patches at\nequal intervals. These patches are then complemented using learned\ncross-channel dependencies. Then it leverages visual MAE's capability in\nhandling sparse multichannel data for patch reconstruction, followed by a\ncoarse-to-fine technique to generate precise predictions from focused contexts.\nIn addition, we integrate self-supervised learning for improved IMTS modeling\nby adapting the visual MAE to IMTS data. Extensive experiments demonstrate\nVIMTS's superior performance and few-shot capability, advancing the application\nof visual foundation models in more general time series tasks. Our code is\navailable at https://github.com/WHU-HZY/VIMTS."}
{"id": "2504.01986", "pdf": "https://arxiv.org/pdf/2504.01986", "abs": "https://arxiv.org/abs/2504.01986", "authors": ["Dario Garcia-Gasulla", "Gokcen Kestor", "Emanuele Parisi", "Miquel AlbertÃ­-Binimelis", "Cristian Gutierrez", "Razine Moundir Ghorab", "Orlando Montenegro", "Bernat Homs", "Miquel Moreto"], "title": "TuRTLe: A Unified Evaluation of LLMs for RTL Generation", "categories": ["cs.AR", "cs.AI", "I.2.5; J.6"], "comment": null, "summary": "The rapid advancements in LLMs have driven the adoption of generative AI in\nvarious domains, including Electronic Design Automation (EDA). Unlike\ntraditional software development, EDA presents unique challenges, as generated\nRTL code must not only be syntactically correct and functionally accurate but\nalso synthesizable by hardware generators while meeting performance, power, and\narea constraints. These additional requirements introduce complexities that\nexisting code-generation benchmarks often fail to capture, limiting their\neffectiveness in evaluating LLMs for RTL generation. To address this gap, we\npropose TuRTLe, a unified evaluation framework designed to systematically\nassess LLMs across key RTL generation tasks. TuRTLe integrates multiple\nexisting benchmarks and automates the evaluation process, enabling a\ncomprehensive assessment of LLM performance in syntax correctness, functional\ncorrectness, synthesis, PPA optimization, and exact line completion. Using this\nframework, we benchmark a diverse set of open LLMs and analyze their strengths\nand weaknesses in EDA-specific tasks. Our results show that reasoning-based\nmodels, such as DeepSeek R1, consistently outperform others across multiple\nevaluation criteria, but at the cost of increased computational overhead and\ninference latency. Additionally, base models are better suited in module\ncompletion tasks, while instruct-tuned models perform better in\nspecification-to-RTL tasks."}
{"id": "2504.03743", "pdf": "https://arxiv.org/pdf/2504.03743", "abs": "https://arxiv.org/abs/2504.03743", "authors": ["Benjamin Patrick Evans", "Leo Ardon", "Sumitra Ganesh"], "title": "Modelling bounded rational decision-making through Wasserstein constraints", "categories": ["cs.LG", "cs.AI", "cs.GT", "econ.GN", "q-fin.EC"], "comment": "Accepted at RLDM 2025", "summary": "Modelling bounded rational decision-making through information constrained\nprocessing provides a principled approach for representing departures from\nrationality within a reinforcement learning framework, while still treating\ndecision-making as an optimization process. However, existing approaches are\ngenerally based on Entropy, Kullback-Leibler divergence, or Mutual Information.\nIn this work, we highlight issues with these approaches when dealing with\nordinal action spaces. Specifically, entropy assumes uniform prior beliefs,\nmissing the impact of a priori biases on decision-makings. KL-Divergence\naddresses this, however, has no notion of \"nearness\" of actions, and\nadditionally, has several well known potentially undesirable properties such as\nthe lack of symmetry, and furthermore, requires the distributions to have the\nsame support (e.g. positive probability for all actions). Mutual information is\noften difficult to estimate. Here, we propose an alternative approach for\nmodeling bounded rational RL agents utilising Wasserstein distances. This\napproach overcomes the aforementioned issues. Crucially, this approach accounts\nfor the nearness of ordinal actions, modeling \"stickiness\" in agent decisions\nand unlikeliness of rapidly switching to far away actions, while also\nsupporting low probability actions, zero-support prior distributions, and is\nsimple to calculate directly."}
{"id": "2504.07282", "pdf": "https://arxiv.org/pdf/2504.07282", "abs": "https://arxiv.org/abs/2504.07282", "authors": ["Lv Qingsong", "Yangning Li", "Zihua Lan", "Zishan Xu", "Jiwei Tang", "Yinghui Li", "Wenhao Jiang", "Hai-Tao Zheng", "Philip S. Yu"], "title": "RAISE: Reinforced Adaptive Instruction Selection For Large Language Models", "categories": ["cs.CL"], "comment": null, "summary": "In the instruction fine-tuning of large language models (LLMs), it is widely\nrecognized that a few high-quality instructions are superior to a large number\nof low-quality instructions. At present, many instruction selection methods\nhave been proposed, but most of these methods select instruction based on\nheuristic quality metrics, and only consider data selection before training.\nThese designs lead to insufficient optimization of instruction fine-tuning, and\nfixed heuristic indicators are often difficult to optimize for specific tasks.\nTherefore, we design a dynamic, task-objective-driven instruction selection\nframework RAISE(Reinforced Adaptive Instruction SElection), which incorporates\nthe entire instruction fine-tuning process into optimization, selecting\ninstructions at each step based on the expected impact of each instruction on\nmodel performance improvement. Our approach is well interpretable and has\nstrong task-specific optimization capabilities. By modeling dynamic instruction\nselection as a sequential decision-making process, we use RL to train our\nselection strategy. Extensive experiments and result analysis prove the\nsuperiority of our method compared with other instruction selection methods.\nNotably, RAISE achieves superior performance by updating only 1% of the\ntraining steps compared to full-data training, demonstrating its efficiency and\neffectiveness."}
{"id": "2505.22918", "pdf": "https://arxiv.org/pdf/2505.22918", "abs": "https://arxiv.org/abs/2505.22918", "authors": ["Ruichen Chen", "Keith G. Mills", "Liyao Jiang", "Chao Gao", "Di Niu"], "title": "Re-ttention: Ultra Sparse Visual Generation via Attention Statistical Reshape", "categories": ["cs.CV"], "comment": "Submitted before obtaining agreement of all authors", "summary": "Diffusion Transformers (DiT) have become the de-facto model for generating\nhigh-quality visual content like videos and images. A huge bottleneck is the\nattention mechanism where complexity scales quadratically with resolution and\nvideo length. One logical way to lessen this burden is sparse attention, where\nonly a subset of tokens or patches are included in the calculation. However,\nexisting techniques fail to preserve visual quality at extremely high sparsity\nlevels and might even incur non-negligible compute overheads. % To address this\nconcern, we propose Re-ttention, which implements very high sparse attention\nfor visual generation models by leveraging the temporal redundancy of Diffusion\nModels to overcome the probabilistic normalization shift within the attention\nmechanism. Specifically, Re-ttention reshapes attention scores based on the\nprior softmax distribution history in order to preserve the visual quality of\nthe full quadratic attention at very high sparsity levels. % Experimental\nresults on T2V/T2I models such as CogVideoX and the PixArt DiTs demonstrate\nthat Re-ttention requires as few as 3.1\\% of the tokens during inference,\noutperforming contemporary methods like FastDiTAttn, Sparse VideoGen and\nMInference. Further, we measure latency to show that our method can attain over\n45\\% end-to-end % and over 92\\% self-attention latency reduction on an H100 GPU\nat negligible overhead cost.\n  Code available online here:\n\\href{https://github.com/cccrrrccc/Re-ttention}{https://github.com/cccrrrccc/Re-ttention}"}
{"id": "2504.05258", "pdf": "https://arxiv.org/pdf/2504.05258", "abs": "https://arxiv.org/abs/2504.05258", "authors": ["AdriÃ¡n Bazaga", "Rexhina Blloshmi", "Bill Byrne", "AdriÃ  de Gispert"], "title": "Learning to Reason Over Time: Timeline Self-Reflection for Improved Temporal Reasoning in Language Models", "categories": ["cs.LG", "cs.AI", "cs.CL"], "comment": "ACL 2025 (Main)", "summary": "Large Language Models (LLMs) have emerged as powerful tools for generating\ncoherent text, understanding context, and performing reasoning tasks. However,\nthey struggle with temporal reasoning, which requires processing time-related\ninformation such as event sequencing, durations, and inter-temporal\nrelationships. These capabilities are critical for applications including\nquestion answering, scheduling, and historical analysis. In this paper, we\nintroduce TISER, a novel framework that enhances the temporal reasoning\nabilities of LLMs through a multi-stage process that combines timeline\nconstruction with iterative self-reflection. Our approach leverages test-time\nscaling to extend the length of reasoning traces, enabling models to capture\ncomplex temporal dependencies more effectively. This strategy not only boosts\nreasoning accuracy but also improves the traceability of the inference process.\nExperimental results demonstrate state-of-the-art performance across multiple\nbenchmarks, including out-of-distribution test sets, and reveal that TISER\nenables smaller open-source models to surpass larger closed-weight models on\nchallenging temporal reasoning tasks."}
{"id": "2504.11992", "pdf": "https://arxiv.org/pdf/2504.11992", "abs": "https://arxiv.org/abs/2504.11992", "authors": ["Pascal Schlachter", "Jonathan Fuss", "Bin Yang"], "title": "Analysis of Pseudo-Labeling for Online Source-Free Universal Domain Adaptation", "categories": ["cs.LG", "cs.CV"], "comment": "Accepted at the 33rd European Signal Processing Conference (EUSIPCO\n  2025)", "summary": "A domain (distribution) shift between training and test data often hinders\nthe real-world performance of deep neural networks, necessitating unsupervised\ndomain adaptation (UDA) to bridge this gap. Online source-free UDA has emerged\nas a solution for practical scenarios where access to source data is restricted\nand target data is received as a continuous stream. However, the open-world\nnature of many real-world applications additionally introduces category shifts\nmeaning that the source and target label spaces may differ. Online source-free\nuniversal domain adaptation (SF-UniDA) addresses this challenge. Existing\nmethods mainly rely on self-training with pseudo-labels, yet the relationship\nbetween pseudo-labeling and adaptation outcomes has not been studied yet. To\nbridge this gap, we conduct a systematic analysis through controlled\nexperiments with simulated pseudo-labeling, offering valuable insights into\npseudo-labeling for online SF-UniDA. Our findings reveal a substantial gap\nbetween the current state-of-the-art and the upper bound of adaptation achieved\nwith perfect pseudo-labeling. Moreover, we show that a contrastive loss enables\neffective adaptation even with moderate pseudo-label accuracy, while a\ncross-entropy (CE) loss, though less robust to pseudo-label errors, achieves\nsuperior results when pseudo-labeling approaches perfection. Lastly, our\nfindings indicate that pseudo-label accuracy is in general more crucial than\nquantity, suggesting that prioritizing fewer but high-confidence pseudo-labels\nis beneficial. Overall, our study highlights the critical role of\npseudo-labeling in (online) SF-UniDA and provides actionable insights to drive\nfuture advancements in the field. Our code is available at\nhttps://github.com/pascalschlachter/PLAnalysis."}
{"id": "2504.08120", "pdf": "https://arxiv.org/pdf/2504.08120", "abs": "https://arxiv.org/abs/2504.08120", "authors": ["Daniil Larionov", "Sotaro Takeshita", "Ran Zhang", "Yanran Chen", "Christoph Leiter", "Zhipin Wang", "Christian Greisinger", "Steffen Eger"], "title": "DeepSeek-R1 vs. o3-mini: How Well can Reasoning LLMs Evaluate MT and Summarization?", "categories": ["cs.CL"], "comment": null, "summary": "Reasoning-enabled large language models (LLMs) excel in logical tasks, yet\ntheir utility for evaluating natural language generation remains unexplored.\nThis study systematically compares reasoning LLMs with non-reasoning\ncounterparts across machine translation and text summarization evaluation\ntasks. We evaluate eight models spanning state-of-the-art reasoning models\n(DeepSeek-R1, OpenAI o3), their distilled variants (8B-70B parameters), and\nequivalent non-reasoning LLMs. Experiments on WMT23 and SummEval benchmarks\nreveal architecture and task-dependent benefits: OpenAI o3-mini models show\nimproved performance with increased reasoning on MT, while DeepSeek-R1 and\ngenerally underperforms compared to its non-reasoning variant except in\nsummarization consistency evaluation. Correlation analysis demonstrates that\nreasoning token usage correlates with evaluation quality only in specific\nmodels, while almost all models generally allocate more reasoning tokens when\nidentifying more quality issues. Distillation maintains reasonable performance\nup to 32B parameter models but degrades substantially at 8B scale. This work\nprovides the first assessment of reasoning LLMs for NLG evaluation and\ncomparison to non-reasoning models. We share our code to facilitate further\nresearch: https://github.com/NL2G/reasoning-eval."}
{"id": "2505.23201", "pdf": "https://arxiv.org/pdf/2505.23201", "abs": "https://arxiv.org/abs/2505.23201", "authors": ["Hao Wu", "Junzhou Chen", "Ronghui Zhang", "Nengchao Lyu", "Hongyu Hu", "Yanyong Guo", "Tony Z. Qiu"], "title": "WTEFNet: Real-Time Low-Light Object Detection for Advanced Driver Assistance Systems", "categories": ["cs.CV"], "comment": "This paper is expected to be submitted to IEEE Transactions on\n  Instrumentation and Measurement", "summary": "Object detection is a cornerstone of environmental perception in advanced\ndriver assistance systems(ADAS). However, most existing methods rely on RGB\ncameras, which suffer from significant performance degradation under low-light\nconditions due to poor image quality. To address this challenge, we proposes\nWTEFNet, a real-time object detection framework specifically designed for\nlow-light scenarios, with strong adaptability to mainstream detectors. WTEFNet\ncomprises three core modules: a Low-Light Enhancement (LLE) module, a\nWavelet-based Feature Extraction (WFE) module, and an Adaptive Fusion Detection\n(AFFD) module. The LLE enhances dark regions while suppressing overexposed\nareas; the WFE applies multi-level discrete wavelet transforms to isolate high-\nand low-frequency components, enabling effective denoising and structural\nfeature retention; the AFFD fuses semantic and illumination features for robust\ndetection. To support training and evaluation, we introduce GSN, a manually\nannotated dataset covering both clear and rainy night-time scenes. Extensive\nexperiments on BDD100K, SHIFT, nuScenes, and GSN demonstrate that WTEFNet\nachieves state-of-the-art accuracy under low-light conditions. Furthermore,\ndeployment on a embedded platform (NVIDIA Jetson AGX Orin) confirms the\nframework's suitability for real-time ADAS applications."}
{"id": "2504.08756", "pdf": "https://arxiv.org/pdf/2504.08756", "abs": "https://arxiv.org/abs/2504.08756", "authors": ["Jeongsoo Lee", "Daeyong Kwon", "Kyohoon Jin", "Junnyeong Jeong", "Minwoo Sim", "Minwoo Kim"], "title": "MHTS: Multi-Hop Tree Structure Framework for Generating Difficulty-Controllable QA Datasets for RAG Evaluation", "categories": ["cs.IR", "cs.AI"], "comment": null, "summary": "Existing RAG benchmarks often overlook query difficulty, leading to inflated\nperformance on simpler questions and unreliable evaluations. A robust benchmark\ndataset must satisfy three key criteria: quality, diversity, and difficulty,\nwhich capturing the complexity of reasoning based on hops and the distribution\nof supporting evidence. In this paper, we propose MHTS (Multi-Hop Tree\nStructure), a novel dataset synthesis framework that systematically controls\nmulti-hop reasoning complexity by leveraging a multi-hop tree structure to\ngenerate logically connected, multi-chunk queries. Our fine-grained difficulty\nestimation formula exhibits a strong correlation with the overall performance\nmetrics of a RAG system, validating its effectiveness in assessing both\nretrieval and answer generation capabilities. By ensuring high-quality,\ndiverse, and difficulty-controlled queries, our approach enhances RAG\nevaluation and benchmarking capabilities."}
{"id": "2504.12991", "pdf": "https://arxiv.org/pdf/2504.12991", "abs": "https://arxiv.org/abs/2504.12991", "authors": ["Yu Wang", "Fu-Chieh Chang", "Pei-Yuan Wu"], "title": "A Theoretical Framework for OOD Robustness in Transformers using Gevrey Classes", "categories": ["cs.LG"], "comment": null, "summary": "We study the robustness of Transformer language models under semantic\nout-of-distribution (OOD) shifts, where training and test data lie in disjoint\nlatent spaces. Using Wasserstein-1 distance and Gevrey-class smoothness, we\nderive sub-exponential upper bounds on prediction error. Our theoretical\nframework explains how smoothness governs generalization under distributional\ndrift. We validate these findings through controlled experiments on arithmetic\nand Chain-of-Thought tasks with latent permutations and scalings. Results show\nempirical degradation aligns with our bounds, highlighting the geometric and\nfunctional principles underlying OOD generalization in Transformers."}
{"id": "2504.10792", "pdf": "https://arxiv.org/pdf/2504.10792", "abs": "https://arxiv.org/abs/2504.10792", "authors": ["Jessica Lin", "Amir Zeldes"], "title": "GUM-SAGE: A Novel Dataset and Approach for Graded Entity Salience Prediction", "categories": ["cs.CL"], "comment": "Camera-ready for ACL Findings 2025", "summary": "Determining and ranking the most salient entities in a text is critical for\nuser-facing systems, especially as users increasingly rely on models to\ninterpret long documents they only partially read. Graded entity salience\naddresses this need by assigning entities scores that reflect their relative\nimportance in a text. Existing approaches fall into two main categories:\nsubjective judgments of salience, which allow for gradient scoring but lack\nconsistency, and summarization-based methods, which define salience as\nmention-worthiness in a summary, promoting explainability but limiting outputs\nto binary labels (entities are either summary-worthy or not). In this paper, we\nintroduce a novel approach for graded entity salience that combines the\nstrengths of both approaches. Using an English dataset spanning 12 spoken and\nwritten genres, we collect 5 summaries per document and calculate each entity's\nsalience score based on its presence across these summaries. Our approach shows\nstronger correlation with scores based on human summaries and alignments, and\noutperforms existing techniques, including LLMs. We release our data and code\nat https://github.com/jl908069/gum_sum_salience to support further research on\ngraded salient entity extraction."}
{"id": "2505.23272", "pdf": "https://arxiv.org/pdf/2505.23272", "abs": "https://arxiv.org/abs/2505.23272", "authors": ["Yazhou Zhang", "Chunwang Zou", "Qimeng Liu", "Lu Rong", "Ben Yao", "Zheng Lian", "Qiuchi Li", "Peng Zhang", "Jing Qin"], "title": "Are MLMs Trapped in the Visual Room?", "categories": ["cs.CV"], "comment": "19 pages", "summary": "Can multi-modal large models (MLMs) that can ``see'' an image be said to\n``understand'' it? Drawing inspiration from Searle's Chinese Room, we propose\nthe \\textbf{Visual Room} argument: a system may process and describe every\ndetail of visual inputs by following algorithmic rules, without genuinely\ncomprehending the underlying intention. This dilemma challenges the prevailing\nassumption that perceptual mastery implies genuine understanding. In\nimplementation, we introduce a two-tier evaluation framework spanning\nperception and cognition. The perception component evaluates whether MLMs can\naccurately capture the surface-level details of visual contents, where the\ncognitive component examines their ability to infer sarcasm polarity. To\nsupport this framework, We further introduce a high-quality multi-modal sarcasm\ndataset comprising both 924 static images and 100 dynamic videos. All sarcasm\nlabels are annotated by the original authors and verified by independent\nreviewers to ensure clarity and consistency. We evaluate eight state-of-the-art\n(SoTA) MLMs. Our results highlight three key findings: (1) MLMs demonstrate\nhigh accuracy in visual perception; (2) even with correct perception, MLMs\nexhibit an average error rate of ~17.1\\% in sarcasm understanding, revealing a\nsignificant gap between seeing and understanding; (3) this gap stems from\nweaknesses in context integration, emotional reasoning, and pragmatic\ninference. This work provides empirical grounding for the proposed Visual Room\nargument and offers a new evaluation paradigm for MLMs."}
{"id": "2505.02712", "pdf": "https://arxiv.org/pdf/2505.02712", "abs": "https://arxiv.org/abs/2505.02712", "authors": ["Andrzej Mizera", "Jakub Zarzycki"], "title": "Graph Neural Network-Based Reinforcement Learning for Controlling Biological Networks: The GATTACA Framework", "categories": ["cs.LG", "cs.AI", "q-bio.MN"], "comment": null, "summary": "Cellular reprogramming, the artificial transformation of one cell type into\nanother, has been attracting increasing research attention due to its\ntherapeutic potential for complex diseases. However, discovering reprogramming\nstrategies through classical wet-lab experiments is hindered by lengthy time\ncommitments and high costs. In this study, we explore the use of deep\nreinforcement learning (DRL) to control Boolean network models of complex\nbiological systems, such as gene regulatory networks and signalling pathway\nnetworks. We formulate a novel control problem for Boolean network models under\nthe asynchronous update mode in the context of cellular reprogramming. To\nfacilitate scalability, we consider our previously introduced concept of a\npseudo-attractor and we improve our procedure for effective identification of\npseudo-attractor states. Finally, we devise a computational framework to solve\nthe control problem. To leverage the structure of biological systems, we\nincorporate graph neural networks with graph convolutions into the artificial\nneural network approximator for the action-value function learned by the DRL\nagent. Experiments on a number of large real-world biological networks from\nliterature demonstrate the scalability and effectiveness of our approach."}
{"id": "2504.20966", "pdf": "https://arxiv.org/pdf/2504.20966", "abs": "https://arxiv.org/abs/2504.20966", "authors": ["Zayd M. K. Zuhri", "Erland Hilman Fuadi", "Alham Fikri Aji"], "title": "Softpick: No Attention Sink, No Massive Activations with Rectified Softmax", "categories": ["cs.LG"], "comment": "Updated to include experiments on 1.8B parameter models", "summary": "We introduce softpick, a rectified, not sum-to-one, drop-in replacement for\nsoftmax in transformer attention mechanisms that eliminates attention sink and\nmassive activations. Our experiments with 340M and 1.8B parameter models\ndemonstrate that softpick achieves 0\\% sink rate consistently. The softpick\ntransformers produce hidden states with significantly lower kurtosis and\ncreates sparse attention maps. Quantized models using softpick outperform\nsoftmax on standard benchmarks, with a particularly pronounced advantage at\nlower bit precisions. Our analysis and discussion shows how softpick has the\npotential to open new possibilities for quantization, low-precision training,\nsparsity optimization, pruning, and interpretability. Our code is available at\nhttps://github.com/zaydzuhri/softpick-attention"}
{"id": "2504.11277", "pdf": "https://arxiv.org/pdf/2504.11277", "abs": "https://arxiv.org/abs/2504.11277", "authors": ["Guocong Li", "Weize Liu", "Yihang Wu", "Ping Wang", "Shuaihan Huang", "Hongxia Xu", "Jian Wu"], "title": "From Misleading Queries to Accurate Answers: A Three-Stage Fine-Tuning Method for LLMs", "categories": ["cs.CL"], "comment": "Accepted to ACL 2025 (Findings)", "summary": "Large language models (LLMs) exhibit excellent performance in natural\nlanguage processing (NLP), but remain highly sensitive to the quality of input\nqueries, especially when these queries contain misleading or inaccurate\ninformation. Existing methods focus on correcting the output, but they often\noverlook the potential of improving the ability of LLMs to detect and correct\nmisleading content in the input itself. In this paper, we propose a novel\nthree-stage fine-tuning method that enhances the ability of LLMs to detect and\ncorrect misleading information in the input, further improving response\naccuracy and reducing hallucinations. Specifically, the three stages include\n(1) training LLMs to identify misleading information, (2) training LLMs to\ncorrect the misleading information using built-in or external knowledge, and\n(3) training LLMs to generate accurate answers based on the corrected queries.\nTo evaluate our method, we conducted experiments on three datasets for the\nhallucination detection task and the question answering~(QA) task, as well as\ntwo datasets containing misleading information that we constructed. The\nexperimental results demonstrate that our method significantly improves the\naccuracy and factuality of LLM responses, while also enhancing the ability to\ndetect hallucinations and reducing the generation of hallucinations in the\noutput, particularly when the query contains misleading information."}
{"id": "2505.23558", "pdf": "https://arxiv.org/pdf/2505.23558", "abs": "https://arxiv.org/abs/2505.23558", "authors": ["Xu Chu", "Xinrong Chen", "Guanyu Wang", "Zhijie Tan", "Kui Huang", "Wenyu Lv", "Tong Mo", "Weiping Li"], "title": "Qwen Look Again: Guiding Vision-Language Reasoning Models to Re-attention Visual Information", "categories": ["cs.CV", "cs.LG"], "comment": null, "summary": "Inference time scaling drives extended reasoning to enhance the performance\nof Vision-Language Models (VLMs), thus forming powerful Vision-Language\nReasoning Models (VLRMs). However, long reasoning dilutes visual tokens,\ncausing visual information to receive less attention and may trigger\nhallucinations. Although introducing text-only reflection processes shows\npromise in language models, we demonstrate that it is insufficient to suppress\nhallucinations in VLMs. To address this issue, we introduce Qwen-LookAgain\n(Qwen-LA), a novel VLRM designed to mitigate hallucinations by incorporating a\nvision-text reflection process that guides the model to re-attention visual\ninformation during reasoning. We first propose a reinforcement learning method\nBalanced Reflective Policy Optimization (BRPO), which guides the model to\ndecide when to generate vision-text reflection on its own and balance the\nnumber and length of reflections. Then, we formally prove that VLRMs lose\nattention to visual tokens as reasoning progresses, and demonstrate that\nsupplementing visual information during reflection enhances visual attention.\nTherefore, during training and inference, Visual Token COPY and Visual Token\nROUTE are introduced to force the model to re-attention visual information at\nthe visual level, addressing the limitations of text-only reflection.\nExperiments on multiple visual QA datasets and hallucination metrics indicate\nthat Qwen-LA achieves leading accuracy performance while reducing\nhallucinations. Our code is available at: https://github.com/Liar406/Look_Again"}
{"id": "2505.06186", "pdf": "https://arxiv.org/pdf/2505.06186", "abs": "https://arxiv.org/abs/2505.06186", "authors": ["Massimiliano Pronesti", "Joao Bettencourt-Silva", "Paul Flanagan", "Alessandra Pascale", "Oisin Redmond", "Anya Belz", "Yufang Hou"], "title": "Query-driven Document-level Scientific Evidence Extraction from Biomedical Studies", "categories": ["cs.CL", "cs.AI"], "comment": "Accepted at ACL 2025 Main Conference", "summary": "Extracting scientific evidence from biomedical studies for clinical research\nquestions (e.g., Does stem cell transplantation improve quality of life in\npatients with medically refractory Crohn's disease compared to placebo?) is a\ncrucial step in synthesising biomedical evidence. In this paper, we focus on\nthe task of document-level scientific evidence extraction for clinical\nquestions with conflicting evidence. To support this task, we create a dataset\ncalled CochraneForest, leveraging forest plots from Cochrane systematic\nreviews. It comprises 202 annotated forest plots, associated clinical research\nquestions, full texts of studies, and study-specific conclusions. Building on\nCochraneForest, we propose URCA (Uniform Retrieval Clustered Augmentation), a\nretrieval-augmented generation framework designed to tackle the unique\nchallenges of evidence extraction. Our experiments show that URCA outperforms\nthe best existing methods by up to 10.3% in F1 score on this task. However, the\nresults also underscore the complexity of CochraneForest, establishing it as a\nchallenging testbed for advancing automated evidence synthesis systems."}
{"id": "2505.05355", "pdf": "https://arxiv.org/pdf/2505.05355", "abs": "https://arxiv.org/abs/2505.05355", "authors": ["Robert Busa-Fekete", "Travis Dick", "Claudio Gentile", "Haim Kaplan", "Tomer Koren", "Uri Stemmer"], "title": "Nearly Optimal Sample Complexity for Learning with Label Proportions", "categories": ["cs.LG"], "comment": "To appear in ICML 2025", "summary": "We investigate Learning from Label Proportions (LLP), a partial information\nsetting where examples in a training set are grouped into bags, and only\naggregate label values in each bag are available. Despite the partial\nobservability, the goal is still to achieve small regret at the level of\nindividual examples. We give results on the sample complexity of LLP under\nsquare loss, showing that our sample complexity is essentially optimal. From an\nalgorithmic viewpoint, we rely on carefully designed variants of Empirical Risk\nMinimization, and Stochastic Gradient Descent algorithms, combined with ad hoc\nvariance reduction techniques. On one hand, our theoretical results improve in\nimportant ways on the existing literature on LLP, specifically in the way the\nsample complexity depends on the bag size. On the other hand, we validate our\nalgorithmic solutions on several datasets, demonstrating improved empirical\nperformance (better accuracy for less samples) against recent baselines."}
{"id": "2504.15815", "pdf": "https://arxiv.org/pdf/2504.15815", "abs": "https://arxiv.org/abs/2504.15815", "authors": ["Michael A. Hedderich", "Anyi Wang", "Raoyuan Zhao", "Florian Eichin", "Jonas Fischer", "Barbara Plank"], "title": "What's the Difference? Supporting Users in Identifying the Effects of Prompt and Model Changes Through Token Patterns", "categories": ["cs.CL", "cs.HC", "cs.LG"], "comment": "Accepted at ACL'25", "summary": "Prompt engineering for large language models is challenging, as even small\nprompt perturbations or model changes can significantly impact the generated\noutput texts. Existing evaluation methods of LLM outputs, either automated\nmetrics or human evaluation, have limitations, such as providing limited\ninsights or being labor-intensive. We propose Spotlight, a new approach that\ncombines both automation and human analysis. Based on data mining techniques,\nwe automatically distinguish between random (decoding) variations and\nsystematic differences in language model outputs. This process provides token\npatterns that describe the systematic differences and guide the user in\nmanually analyzing the effects of their prompts and changes in models\nefficiently. We create three benchmarks to quantitatively test the reliability\nof token pattern extraction methods and demonstrate that our approach provides\nnew insights into established prompt data. From a human-centric perspective,\nthrough demonstration studies and a user study, we show that our token pattern\napproach helps users understand the systematic differences of language model\noutputs. We are further able to discover relevant differences caused by prompt\nand model changes (e.g. related to gender or culture), thus supporting the\nprompt engineering process and human-centric model behavior research."}
{"id": "2505.23661", "pdf": "https://arxiv.org/pdf/2505.23661", "abs": "https://arxiv.org/abs/2505.23661", "authors": ["Size Wu", "Zhonghua Wu", "Zerui Gong", "Qingyi Tao", "Sheng Jin", "Qinyue Li", "Wei Li", "Chen Change Loy"], "title": "OpenUni: A Simple Baseline for Unified Multimodal Understanding and Generation", "categories": ["cs.CV"], "comment": null, "summary": "In this report, we present OpenUni, a simple, lightweight, and fully\nopen-source baseline for unifying multimodal understanding and generation.\nInspired by prevailing practices in unified model learning, we adopt an\nefficient training strategy that minimizes the training complexity and overhead\nby bridging the off-the-shelf multimodal large language models (LLMs) and\ndiffusion models through a set of learnable queries and a light-weight\ntransformer-based connector. With a minimalist choice of architecture, we\ndemonstrate that OpenUni can: 1) generate high-quality and instruction-aligned\nimages, and 2) achieve exceptional performance on standard benchmarks such as\nGenEval, DPG- Bench, and WISE, with only 1.1B and 3.1B activated parameters. To\nsupport open research and community advancement, we release all model weights,\ntraining code, and our curated training datasets (including 23M image-text\npairs) at https://github.com/wusize/OpenUni."}
{"id": "2505.08052", "pdf": "https://arxiv.org/pdf/2505.08052", "abs": "https://arxiv.org/abs/2505.08052", "authors": ["Kourosh Shahnazari", "Seyed Moein Ayyoubzadeh", "Mohammadamin Fazli", "Mohammadali Keshtparvar"], "title": "NAZM: Network Analysis of Zonal Metrics in Persian Poetic Tradition", "categories": ["cs.SI", "cs.AI", "cs.CL", "cs.LG"], "comment": null, "summary": "This study formalizes a computational model to simulate classical Persian\npoets' dynamics of influence through constructing a multi-dimensional\nsimilarity network. Using a rigorously curated dataset based on Ganjoor's\ncorpus, we draw upon semantic, lexical, stylistic, thematic, and metrical\nfeatures to demarcate each poet's corpus. Each is contained within weighted\nsimilarity matrices, which are then appended to generate an aggregate graph\nshowing poet-to-poet influence. Further network investigation is carried out to\nidentify key poets, style hubs, and bridging poets by calculating degree,\ncloseness, betweenness, eigenvector, and Katz centrality measures. Further, for\ntypological insight, we use the Louvain community detection algorithm to\ndemarcate clusters of poets sharing both style and theme coherence, which\ncorrespond closely to acknowledged schools of literature like Sabk-e Hindi,\nSabk-e Khorasani, and the Bazgasht-e Adabi phenomenon. Our findings provide a\nnew data-driven view of Persian literature distinguished between canonical\nsignificance and interextual influence, thus highlighting relatively\nlesser-known figures who hold great structural significance. Combining\ncomputational linguistics with literary study, this paper produces an\ninterpretable and scalable model for poetic tradition, enabling retrospective\nreflection as well as forward-looking research within digital humanities."}
{"id": "2505.11578", "pdf": "https://arxiv.org/pdf/2505.11578", "abs": "https://arxiv.org/abs/2505.11578", "authors": ["Peimian Du", "Jiabin Liu", "Xiaowei Jin", "Wangmeng Zuo", "Hui Li"], "title": "Spatiotemporal Field Generation Based on Hybrid Mamba-Transformer with Physics-informed Fine-tuning", "categories": ["cs.LG", "cs.AI", "physics.comp-ph"], "comment": null, "summary": "This research confronts the challenge of substantial physical equation\ndiscrepancies encountered in the generation of spatiotemporal physical fields\nthrough data-driven trained models. A spatiotemporal physical field generation\nmodel, named HMT-PF, is developed based on the hybrid Mamba-Transformer\narchitecture, incorporating unstructured grid information as input. A\nfine-tuning block, enhanced with physical information, is introduced to\neffectively reduce the physical equation discrepancies. The physical equation\nresiduals are computed through a point query mechanism for efficient gradient\nevaluation, then encoded into latent space for refinement. The fine-tuning\nprocess employs a self-supervised learning approach to achieve physical\nconsistency while maintaining essential field characteristics. Results show\nthat the hybrid Mamba-Transformer model achieves good performance in generating\nspatiotemporal fields, while the physics-informed fine-tuning mechanism further\nreduces significant physical errors effectively. A MSE-R evaluation method is\ndeveloped to assess the accuracy and realism of physical field generation."}
{"id": "2505.13772", "pdf": "https://arxiv.org/pdf/2505.13772", "abs": "https://arxiv.org/abs/2505.13772", "authors": ["Dimitris Roussis", "Leon Voukoutis", "Georgios Paraskevopoulos", "Sokratis Sofianopoulos", "Prokopis Prokopidis", "Vassilis Papavasileiou", "Athanasios Katsamanis", "Stelios Piperidis", "Vassilis Katsouros"], "title": "Krikri: Advancing Open Large Language Models for Greek", "categories": ["cs.CL"], "comment": null, "summary": "We introduce Llama-Krikri-8B, a cutting-edge Large Language Model tailored\nfor the Greek language, built on Meta's Llama 3.1-8B. Llama-Krikri-8B has been\nextensively trained on high-quality Greek data to ensure superior adaptation to\nlinguistic nuances. With 8 billion parameters, it offers advanced capabilities\nwhile maintaining efficient computational performance. Llama-Krikri-8B supports\nboth Modern Greek and English, and is also equipped to handle polytonic text\nand Ancient Greek. The chat version of Llama-Krikri-8B features a multi-stage\npost-training pipeline, utilizing both human and synthetic instruction and\npreference data, by applying techniques such as MAGPIE. In addition, for\nevaluation, we propose three novel public benchmarks for Greek. Our evaluation\non existing as well as the proposed benchmarks shows notable improvements over\ncomparable Greek and multilingual LLMs in both natural language understanding\nand generation as well as code generation."}
{"id": "2505.23734", "pdf": "https://arxiv.org/pdf/2505.23734", "abs": "https://arxiv.org/abs/2505.23734", "authors": ["Weijie Wang", "Donny Y. Chen", "Zeyu Zhang", "Duochao Shi", "Akide Liu", "Bohan Zhuang"], "title": "ZPressor: Bottleneck-Aware Compression for Scalable Feed-Forward 3DGS", "categories": ["cs.CV"], "comment": "Project Page: https://lhmd.top/zpressor, Code:\n  https://github.com/ziplab/ZPressor", "summary": "Feed-forward 3D Gaussian Splatting (3DGS) models have recently emerged as a\npromising solution for novel view synthesis, enabling one-pass inference\nwithout the need for per-scene 3DGS optimization. However, their scalability is\nfundamentally constrained by the limited capacity of their encoders, leading to\ndegraded performance or excessive memory consumption as the number of input\nviews increases. In this work, we analyze feed-forward 3DGS frameworks through\nthe lens of the Information Bottleneck principle and introduce ZPressor, a\nlightweight architecture-agnostic module that enables efficient compression of\nmulti-view inputs into a compact latent state $Z$ that retains essential scene\ninformation while discarding redundancy. Concretely, ZPressor enables existing\nfeed-forward 3DGS models to scale to over 100 input views at 480P resolution on\nan 80GB GPU, by partitioning the views into anchor and support sets and using\ncross attention to compress the information from the support views into anchor\nviews, forming the compressed latent state $Z$. We show that integrating\nZPressor into several state-of-the-art feed-forward 3DGS models consistently\nimproves performance under moderate input views and enhances robustness under\ndense view settings on two large-scale benchmarks DL3DV-10K and RealEstate10K.\nThe video results, code and trained models are available on our project page:\nhttps://lhmd.top/zpressor."}
{"id": "2505.12304", "pdf": "https://arxiv.org/pdf/2505.12304", "abs": "https://arxiv.org/abs/2505.12304", "authors": ["Li Ni", "Hengkai Xu", "Lin Mu", "Yiwen Zhang", "Wenjian Luo"], "title": "Pre-trained Prompt-driven Semi-supervised Local Community Detection", "categories": ["cs.SI", "cs.AI"], "comment": "11 pages, 3 figures", "summary": "Semi-supervised local community detection aims to leverage known communities\nto detect the community containing a given node. Although existing\nsemi-supervised local community detection studies yield promising results, they\nsuffer from time-consuming issues, highlighting the need for more efficient\nalgorithms. Therefore, we apply the \"pre-train, prompt\" paradigm to\nsemi-supervised local community detection and propose the Pre-trained\nPrompt-driven Semi-supervised Local community detection method (PPSL). PPSL\nconsists of three main components: node encoding, sample generation, and\nprompt-driven fine-tuning. Specifically, the node encoding component employs\ngraph neural networks to learn the representations of nodes and communities.\nBased on representations of nodes and communities, the sample generation\ncomponent selects known communities that are structurally similar to the local\nstructure of the given node as training samples. Finally, the prompt-driven\nfine-tuning component leverages these training samples as prompts to guide the\nfinal community prediction. Experimental results on five real-world datasets\ndemonstrate that PPSL outperforms baselines in both community quality and\nefficiency."}
{"id": "2505.13898", "pdf": "https://arxiv.org/pdf/2505.13898", "abs": "https://arxiv.org/abs/2505.13898", "authors": ["RÃ³bert CsordÃ¡s", "Christopher D. Manning", "Christopher Potts"], "title": "Do Language Models Use Their Depth Efficiently?", "categories": ["cs.LG", "cs.AI", "cs.NE"], "comment": null, "summary": "Modern LLMs are increasingly deep, and depth correlates with performance,\nalbeit with diminishing returns. However, do these models use their depth\nefficiently? Do they compose more features to create higher-order computations\nthat are impossible in shallow models, or do they merely spread the same kinds\nof computation out over more layers? To address these questions, we analyze the\nresidual stream of the Llama 3.1 and Qwen 3 family of models. We find: First,\ncomparing the output of the sublayers to the residual stream reveals that\nlayers in the second half contribute much less than those in the first half,\nwith a clear phase transition between the two halves. Second, skipping layers\nin the second half has a much smaller effect on future computations and output\npredictions. Third, for multihop tasks, we are unable to find evidence that\nmodels are using increased depth to compose subresults in examples involving\nmany hops. Fourth, we seek to directly address whether deeper models are using\ntheir additional layers to perform new kinds of computation. To do this, we\ntrain linear maps from the residual stream of a shallow model to a deeper one.\nWe find that layers with the same relative depth map best to each other,\nsuggesting that the larger model simply spreads the same computations out over\nits many layers. All this evidence suggests that deeper models are not using\ntheir depth to learn new kinds of computation, but only using the greater depth\nto perform more fine-grained adjustments to the residual. This may help explain\nwhy increasing scale leads to diminishing returns for stacked Transformer\narchitectures."}
{"id": "2505.14079", "pdf": "https://arxiv.org/pdf/2505.14079", "abs": "https://arxiv.org/abs/2505.14079", "authors": ["Weihong Du", "Wenrui Liao", "Binyu Yan", "Hongru Liang", "Anthony G. Cohn", "Wenqiang Lei"], "title": "BAR: A Backward Reasoning based Agent for Complex Minecraft Tasks", "categories": ["cs.CL"], "comment": null, "summary": "Large language model (LLM) based agents have shown great potential in\nfollowing human instructions and automatically completing various tasks. To\ncomplete a task, the agent needs to decompose it into easily executed steps by\nplanning. Existing studies mainly conduct the planning by inferring what steps\nshould be executed next starting from the agent's initial state. However, this\nforward reasoning paradigm doesn't work well for complex tasks. We propose to\nstudy this issue in Minecraft, a virtual environment that simulates complex\ntasks based on real-world scenarios. We believe that the failure of forward\nreasoning is caused by the big perception gap between the agent's initial state\nand task goal. To this end, we leverage backward reasoning and make the\nplanning starting from the terminal state, which can directly achieve the task\ngoal in one step. Specifically, we design a BAckward Reasoning based agent\n(BAR). It is equipped with a recursive goal decomposition module, a state\nconsistency maintaining module and a stage memory module to make robust,\nconsistent, and efficient planning starting from the terminal state.\nExperimental results demonstrate the superiority of BAR over existing methods\nand the effectiveness of proposed modules."}
{"id": "2405.14979", "pdf": "https://arxiv.org/pdf/2405.14979", "abs": "https://arxiv.org/abs/2405.14979", "authors": ["Weiyu Li", "Jiarui Liu", "Hongyu Yan", "Rui Chen", "Yixun Liang", "Xuelin Chen", "Ping Tan", "Xiaoxiao Long"], "title": "CraftsMan3D: High-fidelity Mesh Generation with 3D Native Generation and Interactive Geometry Refiner", "categories": ["cs.GR", "cs.CV"], "comment": "HomePage: https://craftsman3d.github.io/, Code:\n  https://github.com/wyysf-98/CraftsMan3D", "summary": "We present a novel generative 3D modeling system, coined CraftsMan, which can\ngenerate high-fidelity 3D geometries with highly varied shapes, regular mesh\ntopologies, and detailed surfaces, and, notably, allows for refining the\ngeometry in an interactive manner. Despite the significant advancements in 3D\ngeneration, existing methods still struggle with lengthy optimization\nprocesses, irregular mesh topologies, noisy surfaces, and difficulties in\naccommodating user edits, consequently impeding their widespread adoption and\nimplementation in 3D modeling software. Our work is inspired by the craftsman,\nwho usually roughs out the holistic figure of the work first and elaborates the\nsurface details subsequently. Specifically, we employ a 3D native diffusion\nmodel, which operates on latent space learned from latent set-based 3D\nrepresentations, to generate coarse geometries with regular mesh topology in\nseconds. In particular, this process takes as input a text prompt or a\nreference image and leverages a powerful multi-view (MV) diffusion model to\ngenerate multiple views of the coarse geometry, which are fed into our\nMV-conditioned 3D diffusion model for generating the 3D geometry, significantly\nimproving robustness and generalizability. Following that, a normal-based\ngeometry refiner is used to significantly enhance the surface details. This\nrefinement can be performed automatically, or interactively with user-supplied\nedits. Extensive experiments demonstrate that our method achieves high efficacy\nin producing superior-quality 3D assets compared to existing methods. HomePage:\nhttps://craftsman3d.github.io/, Code: https://github.com/wyysf-98/CraftsMan"}
{"id": "2505.12583", "pdf": "https://arxiv.org/pdf/2505.12583", "abs": "https://arxiv.org/abs/2505.12583", "authors": ["Takeshi Kojima", "Yaonan Zhu", "Yusuke Iwasawa", "Toshinori Kitamura", "Gang Yan", "Shu Morikuni", "Ryosuke Takanami", "Alfredo Solano", "Tatsuya Matsushima", "Akiko Murakami", "Yutaka Matsuo"], "title": "A Comprehensive Survey on Physical Risk Control in the Era of Foundation Model-enabled Robotics", "categories": ["cs.RO", "cs.AI", "cs.LG"], "comment": "Accepted to IJCAI 2025 Survey Track", "summary": "Recent Foundation Model-enabled robotics (FMRs) display greatly improved\ngeneral-purpose skills, enabling more adaptable automation than conventional\nrobotics. Their ability to handle diverse tasks thus creates new opportunities\nto replace human labor. However, unlike general foundation models, FMRs\ninteract with the physical world, where their actions directly affect the\nsafety of humans and surrounding objects, requiring careful deployment and\ncontrol. Based on this proposition, our survey comprehensively summarizes robot\ncontrol approaches to mitigate physical risks by covering all the lifespan of\nFMRs ranging from pre-deployment to post-accident stage. Specifically, we\nbroadly divide the timeline into the following three phases: (1) pre-deployment\nphase, (2) pre-incident phase, and (3) post-incident phase. Throughout this\nsurvey, we find that there is much room to study (i) pre-incident risk\nmitigation strategies, (ii) research that assumes physical interaction with\nhumans, and (iii) essential issues of foundation models themselves. We hope\nthat this survey will be a milestone in providing a high-resolution analysis of\nthe physical risks of FMRs and their control, contributing to the realization\nof a good human-robot relationship."}
{"id": "2505.14742", "pdf": "https://arxiv.org/pdf/2505.14742", "abs": "https://arxiv.org/abs/2505.14742", "authors": ["Hong Huang", "Dapeng Wu"], "title": "Quaff: Quantized Parameter-Efficient Fine-Tuning under Outlier Spatial Stability Hypothesis", "categories": ["cs.LG", "cs.AI"], "comment": "Accepted by ACL 2025", "summary": "Large language models (LLMs) have made exciting achievements across various\ndomains, yet their deployment on resource-constrained personal devices remains\nhindered by the prohibitive computational and memory demands of task-specific\nfine-tuning. While quantization offers a pathway to efficiency, existing\nmethods struggle to balance performance and overhead, either incurring high\ncomputational/memory costs or failing to address activation outliers, a\ncritical bottleneck in quantized fine-tuning. To address these challenges, we\npropose the Outlier Spatial Stability Hypothesis (OSSH): During fine-tuning,\ncertain activation outlier channels retain stable spatial positions across\ntraining iterations. Building on OSSH, we propose Quaff, a Quantized\nparameter-efficient fine-tuning framework for LLMs, optimizing low-precision\nactivation representations through targeted momentum scaling. Quaff dynamically\nsuppresses outliers exclusively in invariant channels using lightweight\noperations, eliminating full-precision weight storage and global rescaling\nwhile reducing quantization errors. Extensive experiments across ten benchmarks\nvalidate OSSH and demonstrate Quaff's efficacy. Specifically, on the GPQA\nreasoning benchmark, Quaff achieves a 1.73x latency reduction and 30% memory\nsavings over full-precision fine-tuning while improving accuracy by 0.6% on the\nPhi-3 model, reconciling the triple trade-off between efficiency, performance,\nand deployability. By enabling consumer-grade GPU fine-tuning (e.g., RTX 2080\nSuper) without sacrificing model utility, Quaff democratizes personalized LLM\ndeployment. The code is available at https://github.com/Little0o0/Quaff.git."}
{"id": "2505.14590", "pdf": "https://arxiv.org/pdf/2505.14590", "abs": "https://arxiv.org/abs/2505.14590", "authors": ["Huihao Jing", "Haoran Li", "Wenbin Hu", "Qi Hu", "Heli Xu", "Tianshu Chu", "Peizhao Hu", "Yangqiu Song"], "title": "MCIP: Protecting MCP Safety via Model Contextual Integrity Protocol", "categories": ["cs.CL"], "comment": "17 pages", "summary": "As Model Context Protocol (MCP) introduces an easy-to-use ecosystem for users\nand developers, it also brings underexplored safety risks. Its decentralized\narchitecture, which separates clients and servers, poses unique challenges for\nsystematic safety analysis. This paper proposes a novel framework to enhance\nMCP safety. Guided by the MAESTRO framework, we first analyze the missing\nsafety mechanisms in MCP, and based on this analysis, we propose the Model\nContextual Integrity Protocol (MCIP), a refined version of MCP that addresses\nthese gaps. Next, we develop a fine-grained taxonomy that captures a diverse\nrange of unsafe behaviors observed in MCP scenarios. Building on this taxonomy,\nwe develop benchmark and training data that support the evaluation and\nimprovement of LLMs' capabilities in identifying safety risks within MCP\ninteractions. Leveraging the proposed benchmark and training data, we conduct\nextensive experiments on state-of-the-art LLMs. The results highlight LLMs'\nvulnerabilities in MCP interactions and demonstrate that our approach\nsubstantially improves their safety performance."}
{"id": "2505.13182", "pdf": "https://arxiv.org/pdf/2505.13182", "abs": "https://arxiv.org/abs/2505.13182", "authors": ["Jianfeng Xu"], "title": "Information Science Principles of Machine Learning: A Causal Chain Meta-Framework Based on Formalized Information Mapping", "categories": ["cs.LO", "cs.AI"], "comment": null, "summary": "[Objective] This study focuses on addressing the current lack of a unified\nformal theoretical framework in machine learning, as well as the deficiencies\nin interpretability and ethical safety assurance. [Methods] A formal\ninformation model is first constructed, utilizing sets of well-formed formulas\nto explicitly define the ontological states and carrier mappings of typical\ncomponents in machine learning. Learnable and processable predicates, along\nwith learning and processing functions, are introduced to analyze the logical\ndeduction and constraint rules of the causal chains within models. [Results] A\nmeta-framework for machine learning theory (MLT-MF) is established. Based on\nthis framework, universal definitions for model interpretability and ethical\nsafety are proposed. Furthermore, three key theorems are proved: the\nequivalence of model interpretability and information recoverability, the\nassurance of ethical safety, and the estimation of generalization error.\n[Limitations] The current framework assumes ideal conditions with noiseless\ninformation-enabling mappings and primarily targets model learning and\nprocessing logic in static scenarios. It does not yet address information\nfusion and conflict resolution across ontological spaces in multimodal or\nmulti-agent systems. [Conclusions] This work overcomes the limitations of\nfragmented research and provides a unified theoretical foundation for\nsystematically addressing the critical challenges currently faced in machine\nlearning."}
{"id": "2505.15072", "pdf": "https://arxiv.org/pdf/2505.15072", "abs": "https://arxiv.org/abs/2505.15072", "authors": ["Xin Zhou", "Weiqing Wang", "Francisco J. BaldÃ¡n", "Wray Buntine", "Christoph Bergmeir"], "title": "MoTime: A Dataset Suite for Multimodal Time Series Forecasting", "categories": ["cs.LG", "cs.CL", "cs.DB", "cs.IR"], "comment": null, "summary": "While multimodal data sources are increasingly available from real-world\nforecasting, most existing research remains on unimodal time series. In this\nwork, we present MoTime, a suite of multimodal time series forecasting datasets\nthat pair temporal signals with external modalities such as text, metadata, and\nimages. Covering diverse domains, MoTime supports structured evaluation of\nmodality utility under two scenarios: 1) the common forecasting task, where\nvarying-length history is available, and 2) cold-start forecasting, where no\nhistorical data is available. Experiments show that external modalities can\nimprove forecasting performance in both scenarios, with particularly strong\nbenefits for short series in some datasets, though the impact varies depending\non data characteristics. By making datasets and findings publicly available, we\naim to support more comprehensive and realistic benchmarks in future multimodal\ntime series forecasting research."}
{"id": "2505.16491", "pdf": "https://arxiv.org/pdf/2505.16491", "abs": "https://arxiv.org/abs/2505.16491", "authors": ["Dario Di Palma", "Alessandro De Bellis", "Giovanni Servedio", "Vito Walter Anelli", "Fedelucio Narducci", "Tommaso Di Noia"], "title": "LLaMAs Have Feelings Too: Unveiling Sentiment and Emotion Representations in LLaMA Models Through Probing", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Large Language Models (LLMs) have rapidly become central to NLP,\ndemonstrating their ability to adapt to various tasks through prompting\ntechniques, including sentiment analysis. However, we still have a limited\nunderstanding of how these models capture sentiment-related information. This\nstudy probes the hidden layers of Llama models to pinpoint where sentiment\nfeatures are most represented and to assess how this affects sentiment\nanalysis.\n  Using probe classifiers, we analyze sentiment encoding across layers and\nscales, identifying the layers and pooling methods that best capture sentiment\nsignals. Our results show that sentiment information is most concentrated in\nmid-layers for binary polarity tasks, with detection accuracy increasing up to\n14% over prompting techniques. Additionally, we find that in decoder-only\nmodels, the last token is not consistently the most informative for sentiment\nencoding. Finally, this approach enables sentiment tasks to be performed with\nmemory requirements reduced by an average of 57%.\n  These insights contribute to a broader understanding of sentiment in LLMs,\nsuggesting layer-specific probing as an effective approach for sentiment tasks\nbeyond prompting, with potential to enhance model utility and reduce memory\nrequirements."}
{"id": "2505.17013", "pdf": "https://arxiv.org/pdf/2505.17013", "abs": "https://arxiv.org/abs/2505.17013", "authors": ["Kevin Lu", "Nicky Kriplani", "Rohit Gandikota", "Minh Pham", "David Bau", "Chinmay Hegde", "Niv Cohen"], "title": "When Are Concepts Erased From Diffusion Models?", "categories": ["cs.LG", "cs.CV"], "comment": "Project Page:\n  https://nyu-dice-lab.github.io/when-are-concepts-erased/", "summary": "Concept erasure, the ability to selectively prevent a model from generating\nspecific concepts, has attracted growing interest, with various approaches\nemerging to address the challenge. However, it remains unclear how thoroughly\nthese methods erase the target concept. We begin by proposing two conceptual\nmodels for the erasure mechanism in diffusion models: (i) reducing the\nlikelihood of generating the target concept, and (ii) interfering with the\nmodel's internal guidance mechanisms. To thoroughly assess whether a concept\nhas been truly erased from the model, we introduce a suite of independent\nevaluations. Our evaluation framework includes adversarial attacks, novel\nprobing techniques, and analysis of the model's alternative generations in\nplace of the erased concept. Our results shed light on the tension between\nminimizing side effects and maintaining robustness to adversarial prompts.\nBroadly, our work underlines the importance of comprehensive evaluation for\nerasure in diffusion models."}
{"id": "2505.16400", "pdf": "https://arxiv.org/pdf/2505.16400", "abs": "https://arxiv.org/abs/2505.16400", "authors": ["Yang Chen", "Zhuolin Yang", "Zihan Liu", "Chankyu Lee", "Peng Xu", "Mohammad Shoeybi", "Bryan Catanzaro", "Wei Ping"], "title": "AceReason-Nemotron: Advancing Math and Code Reasoning through Reinforcement Learning", "categories": ["cs.LG", "cs.AI", "cs.CL"], "comment": "We release the models at:\n  https://huggingface.co/collections/nvidia/acereason-682f4e1261dc22f697fd1485", "summary": "Despite recent progress in large-scale reinforcement learning (RL) for\nreasoning, the training recipe for building high-performing reasoning models\nremains elusive. Key implementation details of frontier models, such as\nDeepSeek-R1, including data curation strategies and RL training recipe, are\noften omitted. Moreover, recent research indicates distillation remains more\neffective than RL for smaller models. In this work, we demonstrate that\nlarge-scale RL can significantly enhance the reasoning capabilities of strong,\nsmall- and mid-sized models, achieving results that surpass those of\nstate-of-the-art distillation-based models. We systematically study the RL\ntraining process through extensive ablations and propose a simple yet effective\napproach: first training on math-only prompts, then on code-only prompts.\nNotably, we find that math-only RL not only significantly enhances the\nperformance of strong distilled models on math benchmarks (e.g., +14.6% /\n+17.2% on AIME 2025 for the 7B / 14B models), but also code reasoning tasks\n(e.g., +6.8% / +5.8% on LiveCodeBench for the 7B / 14B models). In addition,\nextended code-only RL iterations further improve performance on code benchmarks\nwith minimal or no degradation in math results. We develop a robust data\ncuration pipeline to collect challenging prompts with high-quality, verifiable\nanswers and test cases to enable verification-based RL across both domains.\nFinally, we identify key experimental insights, including curriculum learning\nwith progressively increasing response lengths and the stabilizing effect of\non-policy parameter updates. We find that RL not only elicits the foundational\nreasoning capabilities acquired during pretraining and supervised fine-tuning\n(e.g., distillation), but also pushes the limits of the model's reasoning\nability, enabling it to solve problems that were previously unsolvable."}
{"id": "2505.17552", "pdf": "https://arxiv.org/pdf/2505.17552", "abs": "https://arxiv.org/abs/2505.17552", "authors": ["Zijie Qiu", "Jiaqi Wei", "Xiang Zhang", "Sheng Xu", "Kai Zou", "Zhi Jin", "Zhiqiang Gao", "Nanqing Dong", "Siqi Sun"], "title": "Universal Biological Sequence Reranking for Improved De Novo Peptide Sequencing", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "De novo peptide sequencing is a critical task in proteomics. However, the\nperformance of current deep learning-based methods is limited by the inherent\ncomplexity of mass spectrometry data and the heterogeneous distribution of\nnoise signals, leading to data-specific biases. We present RankNovo, the first\ndeep reranking framework that enhances de novo peptide sequencing by leveraging\nthe complementary strengths of multiple sequencing models. RankNovo employs a\nlist-wise reranking approach, modeling candidate peptides as multiple sequence\nalignments and utilizing axial attention to extract informative features across\ncandidates. Additionally, we introduce two new metrics, PMD (Peptide Mass\nDeviation) and RMD (residual Mass Deviation), which offer delicate supervision\nby quantifying mass differences between peptides at both the sequence and\nresidue levels. Extensive experiments demonstrate that RankNovo not only\nsurpasses its base models used to generate training candidates for reranking\npre-training, but also sets a new state-of-the-art benchmark. Moreover,\nRankNovo exhibits strong zero-shot generalization to unseen models whose\ngenerations were not exposed during training, highlighting its robustness and\npotential as a universal reranking framework for peptide sequencing. Our work\npresents a novel reranking strategy that fundamentally challenges existing\nsingle-model paradigms and advances the frontier of accurate de novo\nsequencing. Our source code is provided on GitHub."}
{"id": "2505.16520", "pdf": "https://arxiv.org/pdf/2505.16520", "abs": "https://arxiv.org/abs/2505.16520", "authors": ["Giovanni Servedio", "Alessandro De Bellis", "Dario Di Palma", "Vito Walter Anelli", "Tommaso Di Noia"], "title": "Are the Hidden States Hiding Something? Testing the Limits of Factuality-Encoding Capabilities in LLMs", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Factual hallucinations are a major challenge for Large Language Models\n(LLMs). They undermine reliability and user trust by generating inaccurate or\nfabricated content. Recent studies suggest that when generating false\nstatements, the internal states of LLMs encode information about truthfulness.\nHowever, these studies often rely on synthetic datasets that lack realism,\nwhich limits generalization when evaluating the factual accuracy of text\ngenerated by the model itself. In this paper, we challenge the findings of\nprevious work by investigating truthfulness encoding capabilities, leading to\nthe generation of a more realistic and challenging dataset. Specifically, we\nextend previous work by introducing: (1) a strategy for sampling plausible\ntrue-false factoid sentences from tabular data and (2) a procedure for\ngenerating realistic, LLM-dependent true-false datasets from Question Answering\ncollections. Our analysis of two open-source LLMs reveals that while the\nfindings from previous studies are partially validated, generalization to\nLLM-generated datasets remains challenging. This study lays the groundwork for\nfuture research on factuality in LLMs and offers practical guidelines for more\neffective evaluation."}
{"id": "2505.18194", "pdf": "https://arxiv.org/pdf/2505.18194", "abs": "https://arxiv.org/abs/2505.18194", "authors": ["Yubo Peng", "Luping Xiang", "Bingxin Zhang", "Kun Yang"], "title": "Large Language Model-Driven Distributed Integrated Multimodal Sensing and Semantic Communications", "categories": ["eess.SP", "cs.AI", "cs.CV"], "comment": null, "summary": "Traditional single-modal sensing systems-based solely on either radio\nfrequency (RF) or visual data-struggle to cope with the demands of complex and\ndynamic environments. Furthermore, single-device systems are constrained by\nlimited perspectives and insufficient spatial coverage, which impairs their\neffectiveness in urban or non-line-of-sight scenarios. To overcome these\nchallenges, we propose a novel large language model (LLM)-driven distributed\nintegrated multimodal sensing and semantic communication (LLM-DiSAC) framework.\nSpecifically, our system consists of multiple collaborative sensing devices\nequipped with RF and camera modules, working together with an aggregation\ncenter to enhance sensing accuracy. First, on sensing devices, LLM-DiSAC\ndevelops an RF-vision fusion network (RVFN), which employs specialized feature\nextractors for RF and visual data, followed by a cross-attention module for\neffective multimodal integration. Second, a LLM-based semantic transmission\nnetwork (LSTN) is proposed to enhance communication efficiency, where the\nLLM-based decoder leverages known channel parameters, such as transceiver\ndistance and signal-to-noise ratio (SNR), to mitigate semantic distortion.\nThird, at the aggregation center, a transformer-based aggregation model (TRAM)\nwith an adaptive aggregation attention mechanism is developed to fuse\ndistributed features and enhance sensing accuracy. To preserve data privacy, a\ntwo-stage distributed learning strategy is introduced, allowing local model\ntraining at the device level and centralized aggregation model training using\nintermediate features. Finally, evaluations on a synthetic multi-view RF-visual\ndataset generated by the Genesis simulation engine show that LLM-DiSAC achieves\na good performance."}
{"id": "2505.17072", "pdf": "https://arxiv.org/pdf/2505.17072", "abs": "https://arxiv.org/abs/2505.17072", "authors": ["Jianwei Li", "Jung-Eun Kim"], "title": "Safety Alignment Can Be Not Superficial With Explicit Safety Signals", "categories": ["cs.CR", "cs.AI", "cs.CL", "cs.LG"], "comment": "ICML 2025", "summary": "Recent studies on the safety alignment of large language models (LLMs) have\nrevealed that existing approaches often operate superficially, leaving models\nvulnerable to various adversarial attacks. Despite their significance, these\nstudies generally fail to offer actionable solutions beyond data augmentation\nfor achieving more robust safety mechanisms. This paper identifies a\nfundamental cause of this superficiality: existing alignment approaches often\npresume that models can implicitly learn a safety-related reasoning task during\nthe alignment process, enabling them to refuse harmful requests. However, the\nlearned safety signals are often diluted by other competing objectives, leading\nmodels to struggle with drawing a firm safety-conscious decision boundary when\nconfronted with adversarial attacks. Based on this observation, by explicitly\nintroducing a safety-related binary classification task and integrating its\nsignals with our attention and decoding strategies, we eliminate this ambiguity\nand allow models to respond more responsibly to malicious queries. We emphasize\nthat, with less than 0.2x overhead cost, our approach enables LLMs to assess\nthe safety of both the query and the previously generated tokens at each\nnecessary generating step. Extensive experiments demonstrate that our method\nsignificantly improves the resilience of LLMs against various adversarial\nattacks, offering a promising pathway toward more robust generative AI systems."}
{"id": "2505.18176", "pdf": "https://arxiv.org/pdf/2505.18176", "abs": "https://arxiv.org/abs/2505.18176", "authors": ["Jonathan Tammer Eweis-Labolle", "Tyler Johnson", "Xiangyu Sun", "Ramin Bostanabad"], "title": "Should We Simultaneously Calibrate Multiple Computer Models?", "categories": ["cs.LG", "stat.ML"], "comment": null, "summary": "In an increasing number of applications designers have access to multiple\ncomputer models which typically have different levels of fidelity and cost.\nTraditionally, designers calibrate these models one at a time against some\nhigh-fidelity data (e.g., experiments). In this paper, we question this\ntradition and assess the potential of calibrating multiple computer models at\nthe same time. To this end, we develop a probabilistic framework that is\nfounded on customized neural networks (NNs) that are designed to calibrate an\narbitrary number of computer models. In our approach, we (1) consider the fact\nthat most computer models are multi-response and that the number and nature of\ncalibration parameters may change across the models, and (2) learn a unique\nprobability distribution for each calibration parameter of each computer model,\n(3) develop a loss function that enables our NN to emulate all data sources\nwhile calibrating the computer models, and (4) aim to learn a visualizable\nlatent space where model-form errors can be identified. We test the performance\nof our approach on analytic and engineering problems to understand the\npotential advantages and pitfalls in simultaneous calibration of multiple\ncomputer models. Our method can improve predictive accuracy, however, it is\nprone to non-identifiability issues in higher-dimensional input spaces that are\nnormally constrained by underlying physics."}
{"id": "2505.17139", "pdf": "https://arxiv.org/pdf/2505.17139", "abs": "https://arxiv.org/abs/2505.17139", "authors": ["Wanghan Xu", "Xiangyu Zhao", "Yuhao Zhou", "Xiaoyu Yue", "Ben Fei", "Fenghua Ling", "Wenlong Zhang", "Lei Bai"], "title": "EarthSE: A Benchmark for Evaluating Earth Scientific Exploration Capability of LLMs", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Advancements in Large Language Models (LLMs) drive interest in scientific\napplications, necessitating specialized benchmarks such as Earth science.\nExisting benchmarks either present a general science focus devoid of Earth\nscience specificity or cover isolated subdomains, lacking holistic evaluation.\nFurthermore, current benchmarks typically neglect the assessment of LLMs'\ncapabilities in open-ended scientific exploration. In this paper, we present a\ncomprehensive and professional benchmark for the Earth sciences, designed to\nevaluate the capabilities of LLMs in scientific exploration within this domain,\nspanning from fundamental to advanced levels. Leveraging a corpus of 100,000\nresearch papers, we first construct two Question Answering (QA) datasets:\nEarth-Iron, which offers extensive question coverage for broad assessment, and\nEarth-Silver, which features a higher level of difficulty to evaluate\nprofessional depth. These datasets encompass five Earth spheres, 114\ndisciplines, and 11 task categories, assessing foundational knowledge crucial\nfor scientific exploration. Most notably, we introduce Earth-Gold with new\nmetrics, a dataset comprising open-ended multi-turn dialogues specifically\ndesigned to evaluate the advanced capabilities of LLMs in scientific\nexploration, including methodology induction, limitation analysis, and concept\nproposal. Extensive experiments reveal limitations in 11 leading LLMs across\ndifferent domains and tasks, highlighting considerable room for improvement in\ntheir scientific exploration capabilities. The benchmark is available on\nhttps://huggingface.co/ai-earth ."}
{"id": "2505.20485", "pdf": "https://arxiv.org/pdf/2505.20485", "abs": "https://arxiv.org/abs/2505.20485", "authors": ["Abhijit Chunduru", "Majid Morafah", "Mahdi Morafah", "Vishnu Pandi Chellapandi", "Ang Li"], "title": "Avoid Forgetting by Preserving Global Knowledge Gradients in Federated Learning with Non-IID Data", "categories": ["cs.LG", "cs.AI", "cs.CV", "cs.DC", "cs.PF"], "comment": null, "summary": "The inevitable presence of data heterogeneity has made federated learning\nvery challenging. There are numerous methods to deal with this issue, such as\nlocal regularization, better model fusion techniques, and data sharing. Though\neffective, they lack a deep understanding of how data heterogeneity can affect\nthe global decision boundary. In this paper, we bridge this gap by performing\nan experimental analysis of the learned decision boundary using a toy example.\nOur observations are surprising: (1) we find that the existing methods suffer\nfrom forgetting and clients forget the global decision boundary and only learn\nthe perfect local one, and (2) this happens regardless of the initial weights,\nand clients forget the global decision boundary even starting from pre-trained\noptimal weights. In this paper, we present FedProj, a federated learning\nframework that robustly learns the global decision boundary and avoids its\nforgetting during local training. To achieve better ensemble knowledge fusion,\nwe design a novel server-side ensemble knowledge transfer loss to further\ncalibrate the learned global decision boundary. To alleviate the issue of\nlearned global decision boundary forgetting, we further propose leveraging an\nepisodic memory of average ensemble logits on a public unlabeled dataset to\nregulate the gradient updates at each step of local training. Experimental\nresults demonstrate that FedProj outperforms state-of-the-art methods by a\nlarge margin."}
{"id": "2505.17873", "pdf": "https://arxiv.org/pdf/2505.17873", "abs": "https://arxiv.org/abs/2505.17873", "authors": ["Wanhao Liu", "Zonglin Yang", "Jue Wang", "Lidong Bing", "Di Zhang", "Dongzhan Zhou", "Yuqiang Li", "Houqiang Li", "Erik Cambria", "Wanli Ouyang"], "title": "MOOSE-Chem3: Toward Experiment-Guided Hypothesis Ranking via Simulated Experimental Feedback", "categories": ["cs.CL", "cs.AI", "cs.CE"], "comment": null, "summary": "Hypothesis ranking is a crucial component of automated scientific discovery,\nparticularly in natural sciences where wet-lab experiments are costly and\nthroughput-limited. Existing approaches focus on pre-experiment ranking,\nrelying solely on large language model's internal reasoning without\nincorporating empirical outcomes from experiments. We introduce the task of\nexperiment-guided ranking, which aims to prioritize candidate hypotheses based\non the results of previously tested ones. However, developing such strategies\nis challenging due to the impracticality of repeatedly conducting real\nexperiments in natural science domains. To address this, we propose a simulator\ngrounded in three domain-informed assumptions, modeling hypothesis performance\nas a function of similarity to a known ground truth hypothesis, perturbed by\nnoise. We curate a dataset of 124 chemistry hypotheses with experimentally\nreported outcomes to validate the simulator. Building on this simulator, we\ndevelop a pseudo experiment-guided ranking method that clusters hypotheses by\nshared functional characteristics and prioritizes candidates based on insights\nderived from simulated experimental feedback. Experiments show that our method\noutperforms pre-experiment baselines and strong ablations."}
{"id": "2505.18777", "pdf": "https://arxiv.org/pdf/2505.18777", "abs": "https://arxiv.org/abs/2505.18777", "authors": ["Yiding Wang", "Fauxu Meng", "Xuefeng Zhang", "Fan Jiang", "Pingzhi Tang", "Muhan Zhang"], "title": "HD-PiSSA: High-Rank Distributed Orthogonal Adaptation", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Existing parameter-efficient fine-tuning (PEFT) methods for large language\nmodels (LLMs), such as LoRA and PiSSA, constrain model updates to low-rank\nsubspaces, limiting their expressiveness and leading to suboptimal performance\non complex tasks. To address this, we introduce High-rank Distributed PiSSA\n(HD-PiSSA), a distributed PEFT approach that initializes orthogonal adapters\nacross different devices and aggregates their delta updates collectively on W\nfor fine-tuning. Unlike Data Parallel LoRA or PiSSA, which maintain identical\nadapters across all devices, HD-PiSSA assigns different principal components of\nthe pre-trained weights to each GPU, significantly expanding the range of\nupdate directions. This results in over 16x higher effective updated ranks than\ndata-parallel LoRA or PiSSA when fine-tuning on 8 GPUs with the same per-device\nadapter rank. Empirically, we evaluate HD-PiSSA across various challenging\ndownstream tasks, including mathematics, code generation, and multi-task\nlearning. In the multi-task setting, HD-PiSSA achieves average gains of 10.0\nabsolute points (14.63%) over LoRA and 4.98 points (6.60%) over PiSSA across 12\nbenchmarks, demonstrating its benefits from the extra optimization flexibility."}
{"id": "2505.18799", "pdf": "https://arxiv.org/pdf/2505.18799", "abs": "https://arxiv.org/abs/2505.18799", "authors": ["Hao Chen", "Haoze Li", "Zhiqing Xiao", "Lirong Gao", "Qi Zhang", "Xiaomeng Hu", "Ningtao Wang", "Xing Fu", "Junbo Zhao"], "title": "ALPS: Attention Localization and Pruning Strategy for Efficient Alignment of Large Language Models", "categories": ["cs.CL", "cs.AI"], "comment": "17 pages, 8 figures, 14 tables", "summary": "Aligning general-purpose large language models (LLMs) to downstream tasks\noften incurs significant training adjustment costs. Prior research has explored\nvarious avenues to enhance alignment efficiency, primarily through minimal-data\ntraining or data-driven activations to identify key attention heads. However,\nthese approaches inherently introduce data dependency, which hinders\ngeneralization and reusability. To address this issue and enhance model\nalignment efficiency, we propose the \\textit{\\textbf{A}ttention\n\\textbf{L}ocalization and \\textbf{P}runing \\textbf{S}trategy (\\textbf{ALPS})},\nan efficient algorithm that localizes the most task-sensitive attention heads\nand prunes by restricting attention training updates to these heads, thereby\nreducing alignment costs. Experimental results demonstrate that our method\nactivates only \\textbf{10\\%} of attention parameters during fine-tuning while\nachieving a \\textbf{2\\%} performance improvement over baselines on three tasks.\nMoreover, the identified task-specific heads are transferable across datasets\nand mitigate knowledge forgetting. Our work and findings provide a novel\nperspective on efficient LLM alignment. The code is available at\nhttps://github.com/VoiceBeer/ALPS."}
{"id": "2505.21906", "pdf": "https://arxiv.org/pdf/2505.21906", "abs": "https://arxiv.org/abs/2505.21906", "authors": ["Zhongyi Zhou", "Yichen Zhu", "Junjie Wen", "Chaomin Shen", "Yi Xu"], "title": "ChatVLA-2: Vision-Language-Action Model with Open-World Embodied Reasoning from Pretrained Knowledge", "categories": ["cs.RO", "cs.AI", "cs.CV"], "comment": "Project page: https://chatvla-2.github.io/", "summary": "Vision-language-action (VLA) models have emerged as the next generation of\nmodels in robotics. However, despite leveraging powerful pre-trained\nVision-Language Models (VLMs), existing end-to-end VLA systems often lose key\ncapabilities during fine-tuning as the model adapts to specific robotic tasks.\nWe argue that a generalizable VLA model should retain and expand upon the VLM's\ncore competencies: 1) Open-world embodied reasoning - the VLA should inherit\nthe knowledge from VLM, i.e., recognize anything that the VLM can recognize, be\ncapable of solving math problems, and possess visual-spatial intelligence, 2)\nReasoning following - effectively translating the open-world reasoning into\nactionable steps for the robot. In this work, we introduce ChatVLA-2, a novel\nmixture-of-expert VLA model coupled with a specialized two-stage training\npipeline designed to preserve the VLM's original strengths while enabling\nactionable reasoning. To validate our approach, we design a math-matching task\nwherein a robot interprets math problems written on a whiteboard and picks\ncorresponding number cards from a table to solve equations. Remarkably, our\nmethod exhibits exceptional mathematical reasoning and OCR capabilities,\ndespite these abilities not being explicitly trained within the VLA.\nFurthermore, we demonstrate that the VLA possesses strong spatial reasoning\nskills, enabling it to interpret novel directional instructions involving\npreviously unseen objects. Overall, our method showcases reasoning and\ncomprehension abilities that significantly surpass state-of-the-art imitation\nlearning methods such as OpenVLA, DexVLA, and pi-zero. This work represents a\nsubstantial advancement toward developing truly generalizable robotic\nfoundation models endowed with robust reasoning capacities."}
{"id": "2505.18893", "pdf": "https://arxiv.org/pdf/2505.18893", "abs": "https://arxiv.org/abs/2505.18893", "authors": ["Reva Schwartz", "Rumman Chowdhury", "Akash Kundu", "Heather Frase", "Marzieh Fadaee", "Tom David", "Gabriella Waters", "Afaf Taik", "Morgan Briggs", "Patrick Hall", "Shomik Jain", "Kyra Yee", "Spencer Thomas", "Sundeep Bhandari", "Paul Duncan", "Andrew Thompson", "Maya Carlyle", "Qinghua Lu", "Matthew Holmes", "Theodora Skeadas"], "title": "Reality Check: A New Evaluation Ecosystem Is Necessary to Understand AI's Real World Effects", "categories": ["cs.CY", "cs.AI"], "comment": "9 pages", "summary": "Conventional AI evaluation approaches concentrated within the AI stack\nexhibit systemic limitations for exploring, navigating and resolving the human\nand societal factors that play out in real world deployment such as in\neducation, finance, healthcare, and employment sectors. AI capability\nevaluations can capture detail about first-order effects, such as whether\nimmediate system outputs are accurate, or contain toxic, biased or\nstereotypical content, but AI's second-order effects, i.e. any long-term\noutcomes and consequences that may result from AI use in the real world, have\nbecome a significant area of interest as the technology becomes embedded in our\ndaily lives. These secondary effects can include shifts in user behavior,\nsocietal, cultural and economic ramifications, workforce transformations, and\nlong-term downstream impacts that may result from a broad and growing set of\nrisks. This position paper argues that measuring the indirect and secondary\neffects of AI will require expansion beyond static, single-turn approaches\nconducted in silico to include testing paradigms that can capture what actually\nmaterializes when people use AI technology in context. Specifically, we\ndescribe the need for data and methods that can facilitate contextual awareness\nand enable downstream interpretation and decision making about AI's secondary\neffects, and recommend requirements for a new ecosystem."}
{"id": "2505.19458", "pdf": "https://arxiv.org/pdf/2505.19458", "abs": "https://arxiv.org/abs/2505.19458", "authors": ["Akiyoshi Tomihari", "Ryo Karakida"], "title": "Recurrent Self-Attention Dynamics: An Energy-Agnostic Perspective from Jacobians", "categories": ["cs.LG", "cond-mat.dis-nn", "cs.NE", "stat.ML"], "comment": null, "summary": "The theoretical understanding of self-attention (SA) has been steadily\nprogressing. A prominent line of work studies a class of SA layers that admit\nan energy function decreased by state updates. While it provides valuable\ninsights into inherent biases in signal propagation, it often relies on\nidealized assumptions or additional constraints not necessarily present in\nstandard SA. Thus, to broaden our understanding, this work aims to relax these\nenergy constraints and provide an energy-agnostic characterization of inference\ndynamics by dynamical systems analysis. In more detail, we first consider\nrelaxing the symmetry and single-head constraints traditionally required in\nenergy-based formulations. Next, to investigate more general SA architectures\ncapable of oscillatory dynamics without necessarily admitting an energy\nfunction, we analyze the Jacobian matrix of the state. We reveal that\nnormalization layers effectively normalize the Jacobian's complex eigenvalues,\nforcing the dynamics close to a critical state. This significantly enhances\ninference performance. Furthermore, we utilize the Jacobian perspective to\ndevelop regularization methods for training and a pseudo-energy for monitoring\ninference dynamics."}
{"id": "2505.19240", "pdf": "https://arxiv.org/pdf/2505.19240", "abs": "https://arxiv.org/abs/2505.19240", "authors": ["Aida Kostikova", "Zhipin Wang", "Deidamea Bajri", "Ole PÃ¼tz", "Benjamin PaaÃen", "Steffen Eger"], "title": "LLLMs: A Data-Driven Survey of Evolving Research on Limitations of Large Language Models", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": null, "summary": "Large language model (LLM) research has grown rapidly, along with increasing\nconcern about their limitations such as failures in reasoning, hallucinations,\nand limited multilingual capability. While prior reviews have addressed these\nissues, they often focus on individual limitations or consider them within the\nbroader context of evaluating overall model performance. This survey addresses\nthe gap by presenting a data-driven, semi-automated review of research on\nlimitations of LLMs (LLLMs) from 2022 to 2025, using a bottom-up approach. From\na corpus of 250,000 ACL and arXiv papers, we extract 14,648 relevant limitation\npapers using keyword filtering and LLM-based classification, validated against\nexpert labels. Using topic clustering (via two approaches, HDBSCAN+BERTopic and\nLlooM), we identify between 7 and 15 prominent types of limitations discussed\nin recent LLM research across the ACL and arXiv datasets. We find that\nLLM-related research increases nearly sixfold in ACL and nearly fifteenfold in\narXiv between 2022 and 2025, while LLLMs research grows even faster, by a\nfactor of over 12 in ACL and nearly 28 in arXiv. Reasoning remains the most\nstudied limitation, followed by generalization, hallucination, bias, and\nsecurity. The distribution of topics in the ACL dataset stays relatively stable\nover time, while arXiv shifts toward safety and controllability (with topics\nlike security risks, alignment, hallucinations, knowledge editing), and\nmultimodality between 2022 and 2025. We offer a quantitative view of trends in\nLLM limitations research and release a dataset of annotated abstracts and a\nvalidated methodology, available at:\nhttps://github.com/a-kostikova/LLLMs-Survey."}
{"id": "2505.20112", "pdf": "https://arxiv.org/pdf/2505.20112", "abs": "https://arxiv.org/abs/2505.20112", "authors": ["Haolei Bai", "Siyong Jian", "Tuo Liang", "Yu Yin", "Huan Wang"], "title": "ResSVD: Residual Compensated SVD for Large Language Model Compression", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Large language models (LLMs) have demonstrated impressive capabilities in a\nwide range of downstream natural language processing tasks. Nevertheless, their\nconsiderable sizes and memory demands hinder practical deployment, underscoring\nthe importance of developing efficient compression strategies. Singular value\ndecomposition (SVD) decomposes a matrix into orthogonal components, enabling\nefficient low-rank approximation. This is particularly suitable for LLM\ncompression, where weight matrices often exhibit significant redundancy.\nHowever, current SVD-based methods neglect the residual matrix from truncation,\nresulting in significant truncation loss. Additionally, compressing all layers\nof the model results in severe performance degradation. To overcome these\nlimitations, we propose ResSVD, a new post-training SVD-based LLM compression\nmethod. Specifically, we leverage the residual matrix generated during the\ntruncation process to reduce truncation loss. Moreover, under a fixed overall\ncompression ratio, we selectively compress the last few layers of the model,\nwhich mitigates error propagation and significantly improves the performance of\ncompressed models. Comprehensive evaluations of ResSVD on diverse LLM families\nand multiple benchmark datasets indicate that ResSVD consistently achieves\nsuperior performance over existing counterpart methods, demonstrating its\npractical effectiveness."}
{"id": "2505.20646", "pdf": "https://arxiv.org/pdf/2505.20646", "abs": "https://arxiv.org/abs/2505.20646", "authors": ["Eduardo Y. Sakabe", "Felipe S. AbrahÃ£o", "Alexandre SimÃµes", "Esther Colombini", "Paula Costa", "Ricardo Gudwin", "Hector Zenil"], "title": "Binarized Neural Networks Converge Toward Algorithmic Simplicity: Empirical Support for the Learning-as-Compression Hypothesis", "categories": ["cs.LG", "cs.AI", "cs.IT", "math.IT", "68T07, 68Q30, 68Q32", "I.2.6; F.1.1; F.1.3"], "comment": "10 pages total, 1 figure. Submitted to NeurIPS 2025", "summary": "Understanding and controlling the informational complexity of neural networks\nis a central challenge in machine learning, with implications for\ngeneralization, optimization, and model capacity. While most approaches rely on\nentropy-based loss functions and statistical metrics, these measures often fail\nto capture deeper, causally relevant algorithmic regularities embedded in\nnetwork structure. We propose a shift toward algorithmic information theory,\nusing Binarized Neural Networks (BNNs) as a first proxy. Grounded in\nalgorithmic probability (AP) and the universal distribution it defines, our\napproach characterizes learning dynamics through a formal, causally grounded\nlens. We apply the Block Decomposition Method (BDM) -- a scalable approximation\nof algorithmic complexity based on AP -- and demonstrate that it more closely\ntracks structural changes during training than entropy, consistently exhibiting\nstronger correlations with training loss across varying model sizes and\nrandomized training runs. These results support the view of training as a\nprocess of algorithmic compression, where learning corresponds to the\nprogressive internalization of structured regularities. In doing so, our work\noffers a principled estimate of learning progression and suggests a framework\nfor complexity-aware learning and regularization, grounded in first principles\nfrom information theory, complexity, and computability."}
{"id": "2505.19439", "pdf": "https://arxiv.org/pdf/2505.19439", "abs": "https://arxiv.org/abs/2505.19439", "authors": ["Rihui Xin", "Han Liu", "Zecheng Wang", "Yupeng Zhang", "Dianbo Sui", "Xiaolin Hu", "Bingning Wang"], "title": "Surrogate Signals from Format and Length: Reinforcement Learning for Solving Mathematical Problems without Ground Truth Answers", "categories": ["cs.CL"], "comment": null, "summary": "Large Language Models have achieved remarkable success in natural language\nprocessing tasks, with Reinforcement Learning playing a key role in adapting\nthem to specific applications. However, obtaining ground truth answers for\ntraining LLMs in mathematical problem-solving is often challenging, costly, and\nsometimes unfeasible. This research delves into the utilization of format and\nlength as surrogate signals to train LLMs for mathematical problem-solving,\nbypassing the need for traditional ground truth answers.Our study shows that a\nreward function centered on format correctness alone can yield performance\nimprovements comparable to the standard GRPO algorithm in early phases.\nRecognizing the limitations of format-only rewards in the later phases, we\nincorporate length-based rewards. The resulting GRPO approach, leveraging\nformat-length surrogate signals, not only matches but surpasses the performance\nof the standard GRPO algorithm relying on ground truth answers in certain\nscenarios, achieving 40.0% accuracy on AIME2024 with a 7B base model. Through\nsystematic exploration and experimentation, this research not only offers a\npractical solution for training LLMs to solve mathematical problems and\nreducing the dependence on extensive ground truth data collection, but also\nreveals the essence of why our label-free approach succeeds: the powerful base\nmodel is like an excellent student who has already mastered mathematical and\nlogical reasoning skills, but performs poorly on the test paper, it simply\nneeds to develop good answering habits to achieve outstanding results in exams\n, to unlock the capabilities it already possesses."}
{"id": "2505.20751", "pdf": "https://arxiv.org/pdf/2505.20751", "abs": "https://arxiv.org/abs/2505.20751", "authors": ["Zongcai Tan", "Dandan Zhang"], "title": "Interactive OT Gym: A Reinforcement Learning-Based Interactive Optical tweezer (OT)-Driven Microrobotics Simulation Platform", "categories": ["cs.RO", "cs.AI"], "comment": "ICRA 2025", "summary": "Optical tweezers (OT) offer unparalleled capabilities for micromanipulation\nwith submicron precision in biomedical applications. However, controlling\nconventional multi-trap OT to achieve cooperative manipulation of multiple\ncomplex-shaped microrobots in dynamic environments poses a significant\nchallenge. To address this, we introduce Interactive OT Gym, a reinforcement\nlearning (RL)-based simulation platform designed for OT-driven microrobotics.\nOur platform supports complex physical field simulations and integrates haptic\nfeedback interfaces, RL modules, and context-aware shared control strategies\ntailored for OT-driven microrobot in cooperative biological object manipulation\ntasks. This integration allows for an adaptive blend of manual and autonomous\ncontrol, enabling seamless transitions between human input and autonomous\noperation. We evaluated the effectiveness of our platform using a cell\nmanipulation task. Experimental results show that our shared control system\nsignificantly improves micromanipulation performance, reducing task completion\ntime by approximately 67% compared to using pure human or RL control alone and\nachieving a 100% success rate. With its high fidelity, interactivity, low cost,\nand high-speed simulation capabilities, Interactive OT Gym serves as a\nuser-friendly training and testing environment for the development of advanced\ninteractive OT-driven micromanipulation systems and control algorithms. For\nmore details on the project, please see our website\nhttps://sites.google.com/view/otgym"}
{"id": "2505.21680", "pdf": "https://arxiv.org/pdf/2505.21680", "abs": "https://arxiv.org/abs/2505.21680", "authors": ["Andrew J. Loza", "Jun Yup Kim", "Shangzheng Song", "Yihang Liu", "Joseph J. Y. Sung", "R Andrew Taylor", "Dennis L. Shung"], "title": "multivariateGPT: a decoder-only transformer for multivariate categorical and numeric data", "categories": ["cs.LG", "cs.AI", "68T07", "I.2.6; I.5.1"], "comment": "15 pages, 5 figures", "summary": "Real-world processes often generate data that are a mix of categorical and\nnumeric values that are recorded at irregular and informative intervals.\nDiscrete token-based approaches are limited in numeric representation capacity\nwhile methods like neural ordinary differential equations are not well suited\nfor categorical data or informative sampling and require augmentation to handle\ncertain classes of trajectories. Here, we present multivariateGPT, a single\narchitecture for modeling sequences of mixed categorical (including tokenized\ntext) and numeric data. This is accomplished with an autoregressive sequence\ndecomposition, embedding scheme, and loss function that extend the next token\nprediction task to likelihood estimation of the joint distribution of next\ntoken class and value. We demonstrate how this approach can efficiently learn\nto generalize patterns in simple physical systems and model complex time series\nincluding electrocardiograms and multivariate electronic health record data.\nThis work extends the utility of transformer based models to additional classes\nof data."}
{"id": "2505.20767", "pdf": "https://arxiv.org/pdf/2505.20767", "abs": "https://arxiv.org/abs/2505.20767", "authors": ["Xiaqiang Tang", "Jian Li", "Keyu Hu", "Du Nan", "Xiaolong Li", "Xi Zhang", "Weigao Sun", "Sihong Xie"], "title": "CogniBench: A Legal-inspired Framework and Dataset for Assessing Cognitive Faithfulness of Large Language Models", "categories": ["cs.CL", "cs.AI"], "comment": "ACL 2025", "summary": "Faithfulness hallucinations are claims generated by a Large Language Model\n(LLM) not supported by contexts provided to the LLM. Lacking assessment\nstandards, existing benchmarks focus on \"factual statements\" that rephrase\nsource materials while overlooking \"cognitive statements\" that involve making\ninferences from the given context. Consequently, evaluating and detecting the\nhallucination of cognitive statements remains challenging. Inspired by how\nevidence is assessed in the legal domain, we design a rigorous framework to\nassess different levels of faithfulness of cognitive statements and introduce\nthe CogniBench dataset where we reveal insightful statistics. To keep pace with\nrapidly evolving LLMs, we further develop an automatic annotation pipeline that\nscales easily across different models. This results in a large-scale\nCogniBench-L dataset, which facilitates training accurate detectors for both\nfactual and cognitive hallucinations. We release our model and datasets at:\nhttps://github.com/FUTUREEEEEE/CogniBench"}
{"id": "2505.22442", "pdf": "https://arxiv.org/pdf/2505.22442", "abs": "https://arxiv.org/abs/2505.22442", "authors": ["Mattie Fellows", "Clarisse Wibault", "Uljad Berdica", "Johannes Forkel", "Michael A. Osborne", "Jakob N. Foerster"], "title": "SOReL and TOReL: Two Methods for Fully Offline Reinforcement Learning", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Sample efficiency remains a major obstacle for real world adoption of\nreinforcement learning (RL): success has been limited to settings where\nsimulators provide access to essentially unlimited environment interactions,\nwhich in reality are typically costly or dangerous to obtain. Offline RL in\nprinciple offers a solution by exploiting offline data to learn a near-optimal\npolicy before deployment. In practice, however, current offline RL methods rely\non extensive online interactions for hyperparameter tuning, and have no\nreliable bound on their initial online performance. To address these two\nissues, we introduce two algorithms. Firstly, SOReL: an algorithm for safe\noffline reinforcement learning. Using only offline data, our Bayesian approach\ninfers a posterior over environment dynamics to obtain a reliable estimate of\nthe online performance via the posterior predictive uncertainty. Crucially, all\nhyperparameters are also tuned fully offline. Secondly, we introduce TOReL: a\ntuning for offline reinforcement learning algorithm that extends our\ninformation rate based offline hyperparameter tuning methods to general offline\nRL approaches. Our empirical evaluation confirms SOReL's ability to accurately\nestimate regret in the Bayesian setting whilst TOReL's offline hyperparameter\ntuning achieves competitive performance with the best online hyperparameter\ntuning methods using only offline data. Thus, SOReL and TOReL make a\nsignificant step towards safe and reliable offline RL, unlocking the potential\nfor RL in the real world. Our implementations are publicly available:\nhttps://github.com/CWibault/sorel\\_torel."}
{"id": "2505.21785", "pdf": "https://arxiv.org/pdf/2505.21785", "abs": "https://arxiv.org/abs/2505.21785", "authors": ["Yana Veitsman", "Mayank Jobanputra", "Yash Sarrof", "Aleksandra Bakalova", "Vera Demberg", "Ellie Pavlick", "Michael Hahn"], "title": "Born a Transformer -- Always a Transformer?", "categories": ["cs.LG", "cs.CL"], "comment": null, "summary": "Transformers have theoretical limitations in modeling certain\nsequence-to-sequence tasks, yet it remains largely unclear if these limitations\nplay a role in large-scale pretrained LLMs, or whether LLMs might effectively\novercome these constraints in practice due to the scale of both the models\nthemselves and their pretraining data. We explore how these architectural\nconstraints manifest after pretraining, by studying a family of\n$\\textit{retrieval}$ and $\\textit{copying}$ tasks inspired by Liu et al.\n[2024a]. We use a recently proposed framework for studying length\ngeneralization [Huang et al., 2025] to provide guarantees for each of our\nsettings. Empirically, we observe an $\\textit{induction-versus-anti-induction}$\nasymmetry, where pretrained models are better at retrieving tokens to the right\n(induction) rather than the left (anti-induction) of a query token. This\nasymmetry disappears upon targeted fine-tuning if length-generalization is\nguaranteed by theory. Mechanistic analysis reveals that this asymmetry is\nconnected to the differences in the strength of induction versus anti-induction\ncircuits within pretrained transformers. We validate our findings through\npractical experiments on real-world tasks demonstrating reliability risks. Our\nresults highlight that pretraining selectively enhances certain transformer\ncapabilities, but does not overcome fundamental length-generalization limits."}
{"id": "2505.21693", "pdf": "https://arxiv.org/pdf/2505.21693", "abs": "https://arxiv.org/abs/2505.21693", "authors": ["Raoyuan Zhao", "Beiduo Chen", "Barbara Plank", "Michael A. Hedderich"], "title": "MAKIEval: A Multilingual Automatic WiKidata-based Framework for Cultural Awareness Evaluation for LLMs", "categories": ["cs.CL"], "comment": null, "summary": "Large language models (LLMs) are used globally across many languages, but\ntheir English-centric pretraining raises concerns about cross-lingual\ndisparities for cultural awareness, often resulting in biased outputs. However,\ncomprehensive multilingual evaluation remains challenging due to limited\nbenchmarks and questionable translation quality. To better assess these\ndisparities, we introduce MAKIEval, an automatic multilingual framework for\nevaluating cultural awareness in LLMs across languages, regions, and topics.\nMAKIEval evaluates open-ended text generation, capturing how models express\nculturally grounded knowledge in natural language. Leveraging Wikidata's\nmultilingual structure as a cross-lingual anchor, it automatically identifies\ncultural entities in model outputs and links them to structured knowledge,\nenabling scalable, language-agnostic evaluation without manual annotation or\ntranslation. We then introduce four metrics that capture complementary\ndimensions of cultural awareness: granularity, diversity, cultural specificity,\nand consensus across languages. We assess 7 LLMs developed from different parts\nof the world, encompassing both open-source and proprietary systems, across 13\nlanguages, 19 countries and regions, and 6 culturally salient topics (e.g.,\nfood, clothing). Notably, we find that models tend to exhibit stronger cultural\nawareness in English, suggesting that English prompts more effectively activate\nculturally grounded knowledge. We publicly release our code and data."}
{"id": "2505.22571", "pdf": "https://arxiv.org/pdf/2505.22571", "abs": "https://arxiv.org/abs/2505.22571", "authors": ["Hoang Pham", "Thuy-Duong Nguyen", "Khac-Hoai Nam Bui"], "title": "Agent-UniRAG: A Trainable Open-Source LLM Agent Framework for Unified Retrieval-Augmented Generation Systems", "categories": ["cs.CL", "cs.AI", "cs.DB", "cs.IR"], "comment": null, "summary": "This paper presents a novel approach for unified retrieval-augmented\ngeneration (RAG) systems using the recent emerging large language model (LLM)\nagent concept. Specifically, Agent LLM, which utilizes LLM as fundamental\ncontrollers, has become a promising approach to enable the interpretability of\nRAG tasks, especially for complex reasoning question-answering systems (e.g.,\nmulti-hop queries). Nonetheless, previous works mainly focus on solving RAG\nsystems with either single-hop or multi-hop approaches separately, which limits\nthe application of those approaches to real-world applications. In this study,\nwe propose a trainable agent framework called Agent-UniRAG for unified\nretrieval-augmented LLM systems, which enhances the effectiveness and\ninterpretability of RAG systems. The main idea is to design an LLM agent\nframework to solve RAG tasks step-by-step based on the complexity of the\ninputs, simultaneously including single-hop and multi-hop queries in an\nend-to-end manner. Furthermore, we introduce SynAgent-RAG, a synthetic dataset\nto enable the proposed agent framework for small open-source LLMs (e.g.,\nLlama-3-8B). The results show comparable performances with closed-source and\nlarger open-source LLMs across various RAG benchmarks. Our source code and\ndataset are publicly available for further exploitation."}
{"id": "2505.22257", "pdf": "https://arxiv.org/pdf/2505.22257", "abs": "https://arxiv.org/abs/2505.22257", "authors": ["Youssef Mroueh", "Nicolas Dupuis", "Brian Belgodere", "Apoorva Nitsure", "Mattia Rigotti", "Kristjan Greenewald", "Jiri Navratil", "Jerret Ross", "Jesus Rios"], "title": "Revisiting Group Relative Policy Optimization: Insights into On-Policy and Off-Policy Training", "categories": ["cs.LG", "stat.ML"], "comment": null, "summary": "We revisit Group Relative Policy Optimization (GRPO) in both on-policy and\noff-policy optimization regimes. Our motivation comes from recent work on\noff-policy Proximal Policy Optimization (PPO), which improves training\nstability, sampling efficiency, and memory usage. In addition, a recent\nanalysis of GRPO suggests that estimating the advantage function with\noff-policy samples could be beneficial. Building on these observations, we\nadapt GRPO to the off-policy setting. We show that both on-policy and\noff-policy GRPO objectives yield an improvement in the reward. This result\nmotivates the use of clipped surrogate objectives in the off-policy version of\nGRPO. We then compare the empirical performance of reinforcement learning with\nverifiable rewards in post-training using both GRPO variants. Our results show\nthat off-policy GRPO either significantly outperforms or performs on par with\nits on-policy counterpart."}
{"id": "2505.21701", "pdf": "https://arxiv.org/pdf/2505.21701", "abs": "https://arxiv.org/abs/2505.21701", "authors": ["Raoyuan Zhao", "Abdullatif KÃ¶ksal", "Ali Modarressi", "Michael A. Hedderich", "Hinrich SchÃ¼tze"], "title": "Do We Know What LLMs Don't Know? A Study of Consistency in Knowledge Probing", "categories": ["cs.CL"], "comment": null, "summary": "The reliability of large language models (LLMs) is greatly compromised by\ntheir tendency to hallucinate, underscoring the need for precise identification\nof knowledge gaps within LLMs. Various methods for probing such gaps exist,\nranging from calibration-based to prompting-based methods. To evaluate these\nprobing methods, in this paper, we propose a new process based on using input\nvariations and quantitative metrics. Through this, we expose two dimensions of\ninconsistency in knowledge gap probing. (1) Intra-method inconsistency: Minimal\nnon-semantic perturbations in prompts lead to considerable variance in detected\nknowledge gaps within the same probing method; e.g., the simple variation of\nshuffling answer options can decrease agreement to around 40%. (2) Cross-method\ninconsistency: Probing methods contradict each other on whether a model knows\nthe answer. Methods are highly inconsistent -- with decision consistency across\nmethods being as low as 7% -- even though the model, dataset, and prompt are\nall the same. These findings challenge existing probing methods and highlight\nthe urgent need for perturbation-robust probing frameworks."}
{"id": "2505.22767", "pdf": "https://arxiv.org/pdf/2505.22767", "abs": "https://arxiv.org/abs/2505.22767", "authors": ["Eleni Vasilaki"], "title": "In Dialogue with Intelligence: Rethinking Large Language Models as Collective Knowledge", "categories": ["cs.HC", "cs.AI"], "comment": "6 pages, 1 table", "summary": "Large Language Models (LLMs) are typically analysed through architectural,\nbehavioural, or training-data lenses. This article offers a theoretical and\nexperiential re-framing: LLMs as dynamic instantiations of Collective human\nKnowledge (CK), where intelligence is evoked through dialogue rather than\nstored statically. Drawing on concepts from neuroscience and AI, and grounded\nin sustained interaction with ChatGPT-4, I examine emergent dialogue patterns,\nthe implications of fine-tuning, and the notion of co-augmentation: mutual\nenhancement between human and machine cognition. This perspective offers a new\nlens for understanding interaction, representation, and agency in contemporary\nAI systems."}
{"id": "2505.22362", "pdf": "https://arxiv.org/pdf/2505.22362", "abs": "https://arxiv.org/abs/2505.22362", "authors": ["Aihu Zhang", "Jiaxing Xu", "Mengcheng Lan", "Shili Xiang", "Yiping Ke"], "title": "Directed Homophily-Aware Graph Neural Network", "categories": ["cs.LG"], "comment": null, "summary": "Graph Neural Networks (GNNs) have achieved significant success in various\nlearning tasks on graph-structured data. Nevertheless, most GNNs struggle to\ngeneralize to heterophilic neighborhoods. Additionally, many GNNs ignore the\ndirectional nature of real-world graphs, resulting in suboptimal performance on\ndirected graphs with asymmetric structures. In this work, we propose Directed\nHomophily-aware Graph Neural Network (DHGNN), a novel framework that addresses\nthese limitations by incorporating homophily-aware and direction-sensitive\ncomponents. DHGNN employs a resettable gating mechanism to adaptively modulate\nmessage contributions based on homophily levels and informativeness, and a\nstructure-aware noise-tolerant fusion module to effectively integrate node\nrepresentations from the original and reverse directions. Extensive experiments\non both homophilic and heterophilic directed graph datasets demonstrate that\nDHGNN outperforms state-of-the-art methods in node classification and link\nprediction. In particular, DHGNN improves over the best baseline by up to\n15.07% in link prediction. Our analysis further shows that the gating mechanism\ncaptures directional homophily gaps and fluctuating homophily across layers,\nproviding deeper insights into message-passing behavior on complex graph\nstructures."}
{"id": "2505.22630", "pdf": "https://arxiv.org/pdf/2505.22630", "abs": "https://arxiv.org/abs/2505.22630", "authors": ["Ziling Cheng", "Meng Cao", "Marc-Antoine Rondeau", "Jackie Chi Kit Cheung"], "title": "Stochastic Chameleons: Irrelevant Context Hallucinations Reveal Class-Based (Mis)Generalization in LLMs", "categories": ["cs.CL"], "comment": "Accepted to ACL 2025 (Main Conference)", "summary": "The widespread success of large language models (LLMs) on NLP benchmarks has\nbeen accompanied by concerns that LLMs function primarily as stochastic parrots\nthat reproduce texts similar to what they saw during pre-training, often\nerroneously. But what is the nature of their errors, and do these errors\nexhibit any regularities? In this work, we examine irrelevant context\nhallucinations, in which models integrate misleading contextual cues into their\npredictions. Through behavioral analysis, we show that these errors result from\na structured yet flawed mechanism that we term class-based (mis)generalization,\nin which models combine abstract class cues with features extracted from the\nquery or context to derive answers. Furthermore, mechanistic interpretability\nexperiments on Llama-3, Mistral, and Pythia across 39 factual recall relation\ntypes reveal that this behavior is reflected in the model's internal\ncomputations: (i) abstract class representations are constructed in lower\nlayers before being refined into specific answers in higher layers, (ii)\nfeature selection is governed by two competing circuits -- one prioritizing\ndirect query-based reasoning, the other incorporating contextual cues -- whose\nrelative influences determine the final output. Our findings provide a more\nnuanced perspective on the stochastic parrot argument: through form-based\ntraining, LLMs can exhibit generalization leveraging abstractions, albeit in\nunreliable ways based on contextual cues -- what we term stochastic chameleons."}
{"id": "2505.22904", "pdf": "https://arxiv.org/pdf/2505.22904", "abs": "https://arxiv.org/abs/2505.22904", "authors": ["Youngsoo Choi", "Siu Wun Cheung", "Youngkyu Kim", "Ping-Hsuan Tsai", "Alejandro N. Diaz", "Ivan Zanardi", "Seung Whan Chung", "Dylan Matthew Copeland", "Coleman Kendrick", "William Anderson", "Traian Iliescu", "Matthias Heinkenschloss"], "title": "Defining Foundation Models for Computational Science: A Call for Clarity and Rigor", "categories": ["cs.LG", "cs.AI", "cs.NA", "math.NA"], "comment": "26 pages, 2 tables, 7 figures", "summary": "The widespread success of foundation models in natural language processing\nand computer vision has inspired researchers to extend the concept to\nscientific machine learning and computational science. However, this position\npaper argues that as the term \"foundation model\" is an evolving concept, its\napplication in computational science is increasingly used without a universally\naccepted definition, potentially creating confusion and diluting its precise\nscientific meaning. In this paper, we address this gap by proposing a formal\ndefinition of foundation models in computational science, grounded in the core\nvalues of generality, reusability, and scalability. We articulate a set of\nessential and desirable characteristics that such models must exhibit, drawing\nparallels with traditional foundational methods, like the finite element and\nfinite volume methods. Furthermore, we introduce the Data-Driven Finite Element\nMethod (DD-FEM), a framework that fuses the modular structure of classical FEM\nwith the representational power of data-driven learning. We demonstrate how\nDD-FEM addresses many of the key challenges in realizing foundation models for\ncomputational science, including scalability, adaptability, and physics\nconsistency. By bridging traditional numerical methods with modern AI\nparadigms, this work provides a rigorous foundation for evaluating and\ndeveloping novel approaches toward future foundation models in computational\nscience."}
{"id": "2505.22861", "pdf": "https://arxiv.org/pdf/2505.22861", "abs": "https://arxiv.org/abs/2505.22861", "authors": ["Carlota ParÃ©s-Morlans", "Michelle Yi", "Claire Chen", "Sarah A. Wu", "Rika Antonova", "Tobias Gerstenberg", "Jeannette Bohg"], "title": "Causal-PIK: Causality-based Physical Reasoning with a Physics-Informed Kernel", "categories": ["cs.LG"], "comment": "Accepted to ICML 2025", "summary": "Tasks that involve complex interactions between objects with unknown dynamics\nmake planning before execution difficult. These tasks require agents to\niteratively improve their actions after actively exploring causes and effects\nin the environment. For these type of tasks, we propose Causal-PIK, a method\nthat leverages Bayesian optimization to reason about causal interactions via a\nPhysics-Informed Kernel to help guide efficient search for the best next\naction. Experimental results on Virtual Tools and PHYRE physical reasoning\nbenchmarks show that Causal-PIK outperforms state-of-the-art results, requiring\nfewer actions to reach the goal. We also compare Causal-PIK to human studies,\nincluding results from a new user study we conducted on the PHYRE benchmark. We\nfind that Causal-PIK remains competitive on tasks that are very challenging,\neven for human problem-solvers."}
{"id": "2505.23017", "pdf": "https://arxiv.org/pdf/2505.23017", "abs": "https://arxiv.org/abs/2505.23017", "authors": ["Xingjian Wu", "Xiangfei Qiu", "Hongfan Gao", "Jilin Hu", "Bin Yang", "Chenjuan Guo"], "title": "$K^2$VAE: A Koopman-Kalman Enhanced Variational AutoEncoder for Probabilistic Time Series Forecasting", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Probabilistic Time Series Forecasting (PTSF) plays a crucial role in\ndecision-making across various fields, including economics, energy, and\ntransportation. Most existing methods excell at short-term forecasting, while\noverlooking the hurdles of Long-term Probabilistic Time Series Forecasting\n(LPTSF). As the forecast horizon extends, the inherent nonlinear dynamics have\na significant adverse effect on prediction accuracy, and make generative models\ninefficient by increasing the cost of each iteration. To overcome these\nlimitations, we introduce $K^2$VAE, an efficient VAE-based generative model\nthat leverages a KoopmanNet to transform nonlinear time series into a linear\ndynamical system, and devises a KalmanNet to refine predictions and model\nuncertainty in such linear system, which reduces error accumulation in\nlong-term forecasting. Extensive experiments demonstrate that $K^2$VAE\noutperforms state-of-the-art methods in both short- and long-term PTSF,\nproviding a more efficient and accurate solution."}
{"id": "2505.23593", "pdf": "https://arxiv.org/pdf/2505.23593", "abs": "https://arxiv.org/abs/2505.23593", "authors": ["Nikita Agrawal", "Simon Mertel", "Ruben Mayer"], "title": "Position: Federated Foundation Language Model Post-Training Should Focus on Open-Source Models", "categories": ["cs.LG"], "comment": null, "summary": "Post-training of foundation language models has emerged as a promising\nresearch domain in federated learning (FL) with the goal to enable\nprivacy-preserving model improvements and adaptations to user's downstream\ntasks. Recent advances in this area adopt centralized post-training approaches\nthat build upon black-box foundation language models where there is no access\nto model weights and architecture details. Although the use of black-box models\nhas been successful in centralized post-training, their blind replication in FL\nraises several concerns. Our position is that using black-box models in FL\ncontradicts the core principles of federation such as data privacy and\nautonomy. In this position paper, we critically analyze the usage of black-box\nmodels in federated post-training, and provide a detailed account of various\naspects of openness and their implications for FL."}
{"id": "2501.02772", "pdf": "https://arxiv.org/pdf/2501.02772", "abs": "https://arxiv.org/abs/2501.02772", "authors": ["Haoyu Liu", "Shaohan Huang", "Jianfeng Liu", "Yuefeng Zhan", "Hao Sun", "Weiwei Deng", "Feng Sun", "Furu Wei", "Qi Zhang"], "title": "GeAR: Generation Augmented Retrieval", "categories": ["cs.IR", "cs.CL"], "comment": "In ACL 2025", "summary": "Document retrieval techniques are essential for developing large-scale\ninformation systems. The common approach involves using a bi-encoder to compute\nthe semantic similarity between a query and documents. However, the scalar\nsimilarity often fail to reflect enough information, hindering the\ninterpretation of retrieval results. In addition, this process primarily\nfocuses on global semantics, overlooking the finer-grained semantic\nrelationships between the query and the document's content. In this paper, we\nintroduce a novel method, $\\textbf{Ge}$neration $\\textbf{A}$ugmented\n$\\textbf{R}$etrieval ($\\textbf{GeAR}$), which not only improves the global\ndocument-query similarity through contrastive learning, but also integrates\nwell-designed fusion and decoding modules. This enables GeAR to generate\nrelevant context within the documents based on a given query, facilitating\nlearning to retrieve local fine-grained information. Furthermore, when used as\na retriever, GeAR does not incur any additional computational cost over\nbi-encoders. GeAR exhibits competitive retrieval performance across diverse\nscenarios and tasks. Moreover, qualitative analysis and the results generated\nby GeAR provide novel insights into the interpretation of retrieval results.\nThe code, data, and models will be released at\n\\href{https://github.com/microsoft/LMOps}{https://github.com/microsoft/LMOps}."}
{"id": "2505.23655", "pdf": "https://arxiv.org/pdf/2505.23655", "abs": "https://arxiv.org/abs/2505.23655", "authors": ["Peter David Fagan"], "title": "Keyed Chaotic Masking: A Functional Privacy Framework for Neural Inference", "categories": ["cs.CR", "cs.AI", "94A60, 37N25, 68T05", "D.4.6"], "comment": "8 pages", "summary": "This work introduces a lightweight framework for privacy-preserving neural\nnetwork inference based on keyed chaotic masking a deterministic, user-specific\nobfuscation method derived from cryptographically seeded chaotic dynamical\nsystems. The approach applies masks to input and output tensors using\nkey-conditioned graph dynamics, enabling authenticated inference, user\nattribution, and soft output watermarking without modifying model\narchitectures. While the underlying chaotic system used to generate each mask\nis not analytically invertible, the masking operation itself is algebraically\nreversible by authorized key holders, offering functional privacy without\nformal cryptographic guarantees. Unlike traditional encryption or secure\nmulti-party computation, this method operates in continuous space and imposes\nminimal computational overhead. We describe the construction of the masking\nsystem, including graph sampling, dynamical rule selection, and chaos\ndiagnostics. Applications include privacy-preserving inference, secure data\ncontribution, and per-user watermarking in shared model pipelines. This\nframework offers a practical and modular building block for user-controlled\nprivacy in modern AI systems."}
{"id": "2112.14249", "pdf": "https://arxiv.org/pdf/2112.14249", "abs": "https://arxiv.org/abs/2112.14249", "authors": ["Isaac Meza", "Rahul Singh"], "title": "Nested Nonparametric Instrumental Variable Regression", "categories": ["stat.ML", "cs.LG", "econ.EM", "math.ST", "stat.TH"], "comment": null, "summary": "Several causal parameters in short panel data models are functionals of a\nnested nonparametric instrumental variable regression (nested NPIV). Recent\nexamples include mediated, time varying, and long term treatment effects\nidentified using proxy variables. In econometrics, examples arise in triangular\nsimultaneous equations and hedonic price systems. However, it appears that\nexplicit mean square convergence rates for nested NPIV are unknown, preventing\ninference on some of these parameters with generic machine learning. A major\nchallenge is compounding ill posedness due to the nested inverse problems. To\nlimit how ill posedness compounds, we introduce two techniques: relative well\nposedness, and multiple robustness to ill posedness. With these techniques, we\nprovide explicit mean square rates for nested NPIV and efficient inference for\nrecently identified causal parameters. Our nonasymptotic analysis accommodates\nneural networks, random forests, and reproducing kernel Hilbert spaces. It\nextends to causal functions, e.g. heterogeneous long term treatment effects."}
{"id": "2501.10639", "pdf": "https://arxiv.org/pdf/2501.10639", "abs": "https://arxiv.org/abs/2501.10639", "authors": ["Xin Yi", "Yue Li", "Dongsheng Shi", "Linlin Wang", "Xiaoling Wang", "Liang He"], "title": "Latent-space adversarial training with post-aware calibration for defending large language models against jailbreak attacks", "categories": ["cs.CR", "cs.CL"], "comment": null, "summary": "Ensuring safety alignment is a critical requirement for large language models\n(LLMs), particularly given increasing deployment in real-world applications.\nDespite considerable advancements, LLMs remain susceptible to jailbreak\nattacks, which exploit system vulnerabilities to circumvent safety measures and\nelicit harmful or inappropriate outputs. Furthermore, while adversarial\ntraining-based defense methods have shown promise, a prevalent issue is the\nunintended over-defense behavior, wherein models excessively reject benign\nqueries, significantly undermining their practical utility. To address these\nlimitations, we introduce LATPC, a Latent-space Adversarial Training with\nPost-aware Calibration framework. LATPC dynamically identifies safety-critical\nlatent dimensions by contrasting harmful and benign inputs, enabling the\nadaptive construction of targeted refusal feature removal attacks. This\nmechanism allows adversarial training to concentrate on real-world jailbreak\ntactics that disguise harmful queries as benign ones. During inference, LATPC\nemploys an efficient embedding-level calibration mechanism to minimize\nover-defense behaviors with negligible computational overhead. Experimental\nresults across five types of disguise-based jailbreak attacks demonstrate that\nLATPC achieves a superior balance between safety and utility compared to\nexisting defense frameworks. Further analysis demonstrates the effectiveness of\nleveraging safety-critical dimensions in developing robust defense methods\nagainst jailbreak attacks."}
{"id": "2303.16750", "pdf": "https://arxiv.org/pdf/2303.16750", "abs": "https://arxiv.org/abs/2303.16750", "authors": ["Ivan Stelmakh", "John Wieting", "Sarina Xi", "Graham Neubig", "Nihar B. Shah"], "title": "A Gold Standard Dataset for the Reviewer Assignment Problem", "categories": ["cs.IR", "cs.DL", "cs.LG"], "comment": null, "summary": "Many peer-review venues are using algorithms to assign submissions to\nreviewers. The crux of such automated approaches is the notion of the\n\"similarity score\" -- a numerical estimate of the expertise of a reviewer in\nreviewing a paper -- and many algorithms have been proposed to compute these\nscores. However, these algorithms have not been subjected to a principled\ncomparison, making it difficult for stakeholders to choose the algorithm in an\nevidence-based manner. The key challenge in comparing existing algorithms and\ndeveloping better algorithms is the lack of publicly available gold-standard\ndata. We address this challenge by collecting a novel dataset of similarity\nscores that we release to the research community. Our dataset consists of 477\nself-reported expertise scores provided by 58 researchers who evaluated their\nexpertise in reviewing papers they have read previously.\n  Using our dataset, we compare several widely used similarity algorithms and\noffer key insights. First, all algorithms exhibit significant error, with\nmisranking rates between 12%-30% in easier cases and 36%-43% in harder ones.\nSecond, most specialized algorithms are designed to work with titles and\nabstracts of papers, and in this regime the SPECTER2 algorithm performs best.\nInterestingly, classical TF-IDF matches SPECTER2 in accuracy when given access\nto full submission texts. In contrast, off-the-shelf LLMs lag behind\nspecialized approaches."}
{"id": "2306.14853", "pdf": "https://arxiv.org/pdf/2306.14853", "abs": "https://arxiv.org/abs/2306.14853", "authors": ["Lesi Chen", "Yaohua Ma", "Jingzhao Zhang"], "title": "Near-Optimal Nonconvex-Strongly-Convex Bilevel Optimization with Fully First-Order Oracles", "categories": ["math.OC", "cs.LG"], "comment": "JMLR 2025", "summary": "In this work, we consider bilevel optimization when the lower-level problem\nis strongly convex. Recent works show that with a Hessian-vector product (HVP)\noracle, one can provably find an $\\epsilon$-stationary point within\n${\\mathcal{O}}(\\epsilon^{-2})$ oracle calls. However, the HVP oracle may be\ninaccessible or expensive in practice. Kwon et al. (ICML 2023) addressed this\nissue by proposing a first-order method that can achieve the same goal at a\nslower rate of $\\tilde{\\mathcal{O}}(\\epsilon^{-3})$. In this paper, we\nincorporate a two-time-scale update to improve their method to achieve the\nnear-optimal $\\tilde {\\mathcal{O}}(\\epsilon^{-2})$ first-order oracle\ncomplexity. Our analysis is highly extensible. In the stochastic setting, our\nalgorithm can achieve the stochastic first-order oracle complexity of $\\tilde\n{\\mathcal{O}}(\\epsilon^{-4})$ and $\\tilde {\\mathcal{O}}(\\epsilon^{-6})$ when\nthe stochastic noises are only in the upper-level objective and in both level\nobjectives, respectively. When the objectives have higher-order smoothness\nconditions, our deterministic method can escape saddle points by injecting\nnoise, and can be accelerated to achieve a faster rate of $\\tilde\n{\\mathcal{O}}(\\epsilon^{-1.75})$ using Nesterov's momentum."}
{"id": "2308.13135", "pdf": "https://arxiv.org/pdf/2308.13135", "abs": "https://arxiv.org/abs/2308.13135", "authors": ["Patrick Emedom-Nnamdi", "Timothy R. Smith", "Jukka-Pekka Onnela", "Junwei Lu"], "title": "Nonparametric Additive Value Functions: Interpretable Reinforcement Learning with an Application to Surgical Recovery", "categories": ["stat.ML", "cs.LG"], "comment": "28 pages, 13 figures", "summary": "We propose a nonparametric additive model for estimating interpretable value\nfunctions in reinforcement learning, with an application in optimizing\npostoperative recovery through personalized, adaptive recommendations. While\nreinforcement learning has achieved significant success in various domains,\nrecent methods often rely on black-box approaches such as neural networks,\nwhich hinder the examination of individual feature contributions to a\ndecision-making policy. Our novel method offers a flexible technique for\nestimating action-value functions without explicit parametric assumptions,\novercoming the limitations of the linearity assumption of classical algorithms.\nBy incorporating local kernel regression and basis expansion, we obtain a\nsparse, additive representation of the action-value function, enabling local\napproximation and retrieval of nonlinear, independent contributions of select\nstate features and the interactions between joint feature pairs. We validate\nour approach through a simulation study and apply it to spine disease recovery,\nuncovering recommendations aligned with clinical knowledge. This method bridges\nthe gap between flexible machine learning techniques and the interpretability\nrequired in healthcare applications, paving the way for more personalized\ninterventions."}
{"id": "2504.21578", "pdf": "https://arxiv.org/pdf/2504.21578", "abs": "https://arxiv.org/abs/2504.21578", "authors": ["Kamila Barylska", "Franck Delaplace", "Anna GogoliÅska", "Ewa PaÅkowska"], "title": "Glucagon and insulin production in pancreatic cells modeled using Petri nets and Boolean networks", "categories": ["q-bio.CB", "cs.CL", "03", "F.2; G.0"], "comment": null, "summary": "Diabetes is a civilization chronic disease characterized by a constant\nelevated concentration of glucose in the blood. Many processes are involved in\nthe glucose regulation, and their interactions are very complex. To better\nunderstand those processes we set ourselves a goal to create a Petri net model\nof the glucose regulation in the whole body. So far we have managed to create a\nmodel of glycolysis and synthesis of glucose in the liver, and the general\noverview models of the glucose regulation in a healthy and diabetic person. In\nthis paper we introduce Petri nets models of insulin secretion in beta cell of\nthe pancreas, and glucagon in the pancreas alpha cells. Those two hormones have\nmutually opposite effects: insulin preventing hyperglycemia, and glucagon\npreventing hypoglycemia. Understanding the mechanisms of insulin and glucagon\nsecretion constitutes the basis for understanding diabetes. We also present a\nmodel in which both processes occur together, depending on the blood glucose\nlevel. The dynamics of each model is analysed. Additionally, we transform the\noverall insulin and glucagon secretion system to a Boolean network, following\nstandard transformation rules."}
{"id": "2311.16139", "pdf": "https://arxiv.org/pdf/2311.16139", "abs": "https://arxiv.org/abs/2311.16139", "authors": ["Zeyu Song", "Ehsanul Kabir", "Shagufta Mehnaz"], "title": "GNNBleed: Inference Attacks to Unveil Private Edges in Graphs with Realistic Access to GNN Models", "categories": ["cs.CR", "cs.LG"], "comment": "The paper has been accepted to the PoPETs 2025", "summary": "Graph Neural Networks (GNNs) have become indispensable tools for learning\nfrom graph structured data, catering to various applications such as social\nnetwork analysis and fraud detection for financial services. At the heart of\nthese networks are the edges, which are crucial in guiding GNN models'\npredictions. In many scenarios, these edges represent sensitive information,\nsuch as personal associations or financial dealings, which require privacy\nassurance. However, their contributions to GNN model predictions may, in turn,\nbe exploited by the adversary to compromise their privacy. Motivated by these\nconflicting requirements, this paper investigates edge privacy in contexts\nwhere adversaries possess only black-box access to the target GNN model,\nrestricted further by access controls, preventing direct insights into\narbitrary node outputs. Moreover, we are the first to extensively examine\nsituations where the target graph continuously evolves, a common trait of many\nreal-world graphs. In this setting, we present a range of attacks that leverage\nthe message-passing mechanism of GNNs. We evaluated the effectiveness of our\nattacks using nine real-world datasets, encompassing both static and dynamic\ngraphs, across four different GNN architectures. The results demonstrate that\nour attack outperforms existing methods across various GNN architectures,\nconsistently achieving an F1 score of at least 0.8 in static scenarios.\nFurthermore, our attack retains robustness in dynamic graph scenarios,\nmaintaining F1 scores up to 0.8, unlike previous methods that only achieve F1\nscores around 0.2."}
{"id": "2312.02849", "pdf": "https://arxiv.org/pdf/2312.02849", "abs": "https://arxiv.org/abs/2312.02849", "authors": ["Yiheng Jiang", "Sinho Chewi", "Aram-Alexandre Pooladian"], "title": "Algorithms for mean-field variational inference via polyhedral optimization in the Wasserstein space", "categories": ["math.ST", "cs.LG", "math.OC", "stat.TH"], "comment": "49 pages", "summary": "We develop a theory of finite-dimensional polyhedral subsets over the\nWasserstein space and optimization of functionals over them via first-order\nmethods. Our main application is to the problem of mean-field variational\ninference, which seeks to approximate a distribution $\\pi$ over $\\mathbb{R}^d$\nby a product measure $\\pi^\\star$. When $\\pi$ is strongly log-concave and\nlog-smooth, we provide (1) approximation rates certifying that $\\pi^\\star$ is\nclose to the minimizer $\\pi^\\star_\\diamond$ of the KL divergence over a\n\\emph{polyhedral} set $\\mathcal{P}_\\diamond$, and (2) an algorithm for\nminimizing $\\text{KL}(\\cdot\\|\\pi)$ over $\\mathcal{P}_\\diamond$ based on\naccelerated gradient descent over $\\R^d$. As a byproduct of our analysis, we\nobtain the first end-to-end analysis for gradient-based algorithms for MFVI."}
{"id": "2401.06738", "pdf": "https://arxiv.org/pdf/2401.06738", "abs": "https://arxiv.org/abs/2401.06738", "authors": ["Anh Dang", "Reza Babanezhad", "Sharan Vaswani"], "title": "(Accelerated) Noise-adaptive Stochastic Heavy-Ball Momentum", "categories": ["math.OC", "cs.LG", "stat.ML"], "comment": "TMLR 2025", "summary": "Stochastic heavy ball momentum (SHB) is commonly used to train machine\nlearning models, and often provides empirical improvements over stochastic\ngradient descent. By primarily focusing on strongly-convex quadratics, we aim\nto better understand the theoretical advantage of SHB and subsequently improve\nthe method. For strongly-convex quadratics, Kidambi et al. (2018) show that SHB\n(with a mini-batch of size $1$) cannot attain accelerated convergence, and\nhence has no theoretical benefit over SGD. They conjecture that the practical\ngain of SHB is a by-product of using larger mini-batches. We first substantiate\nthis claim by showing that SHB can attain an accelerated rate when the\nmini-batch size is larger than a threshold $b^*$ that depends on the condition\nnumber $\\kappa$. Specifically, we prove that with the same step-size and\nmomentum parameters as in the deterministic setting, SHB with a sufficiently\nlarge mini-batch size results in an $O\\left(\\exp(-\\frac{T}{\\sqrt{\\kappa}}) +\n\\sigma \\right)$ convergence when measuring the distance to the optimal solution\nin the $\\ell_2$ norm, where $T$ is the number of iterations and $\\sigma^2$ is\nthe variance in the stochastic gradients. We prove a lower-bound which\ndemonstrates that a $\\kappa$ dependence in $b^*$ is necessary. To ensure\nconvergence to the minimizer, we design a noise-adaptive multi-stage algorithm\nthat results in an $O\\left(\\exp\\left(-\\frac{T}{\\sqrt{\\kappa}}\\right) +\n\\frac{\\sigma}{\\sqrt{T}}\\right)$ rate when measuring the distance to the optimal\nsolution in the $\\ell_2$ norm. We also consider the general smooth,\nstrongly-convex setting and propose the first noise-adaptive SHB variant that\nconverges to the minimizer at an $O(\\exp(-\\frac{T}{\\kappa}) +\n\\frac{\\sigma^2}{T})$ rate when measuring the distance to the optimal solution\nin the squared $\\ell_2$ norm. We empirically demonstrate the effectiveness of\nthe proposed algorithms."}
{"id": "2401.14283", "pdf": "https://arxiv.org/pdf/2401.14283", "abs": "https://arxiv.org/abs/2401.14283", "authors": ["Pritha Gupta", "Marcel Wever", "Eyke HÃ¼llermeier"], "title": "Information Leakage Detection through Approximate Bayes-optimal Prediction", "categories": ["stat.ML", "cs.LG", "94A15, 62H30, 94A60", "I.5.1; G.3; E.3"], "comment": "Under submission in Information Sciences", "summary": "In today's data-driven world, the proliferation of publicly available\ninformation raises security concerns due to the information leakage (IL)\nproblem. IL involves unintentionally exposing sensitive information to\nunauthorized parties via observable system information. Conventional\nstatistical approaches rely on estimating mutual information (MI) between\nobservable and secret information for detecting ILs, face challenges of the\ncurse of dimensionality, convergence, computational complexity, and MI\nmisestimation. Though effective, emerging supervised machine learning based\napproaches to detect ILs are limited to binary system sensitive information and\nlack a comprehensive framework. To address these limitations, we establish a\ntheoretical framework using statistical learning theory and information theory\nto quantify and detect IL accurately. Using automated machine learning, we\ndemonstrate that MI can be accurately estimated by approximating the typically\nunknown Bayes predictor's log-loss and accuracy. Based on this, we show how MI\ncan effectively be estimated to detect ILs. Our method performs superior to\nstate-of-the-art baselines in an empirical study considering synthetic and\nreal-world OpenSSL TLS server datasets."}
{"id": "2405.06003", "pdf": "https://arxiv.org/pdf/2405.06003", "abs": "https://arxiv.org/abs/2405.06003", "authors": ["Yuzhou Gu", "Zhao Song", "Junze Yin"], "title": "Binary Hypothesis Testing for Softmax Models and Leverage Score Models", "categories": ["stat.ML", "cs.LG"], "comment": "ICML 2025", "summary": "Softmax distributions are widely used in machine learning, including Large\nLanguage Models (LLMs), where the attention unit uses softmax distributions. We\nabstract the attention unit as the softmax model, where given a vector input,\nthe model produces an output drawn from the softmax distribution (which depends\non the vector input). We consider the fundamental problem of binary hypothesis\ntesting in the setting of softmax models. That is, given an unknown softmax\nmodel, which is known to be one of the two given softmax models, how many\nqueries are needed to determine which one is the truth? We show that the sample\ncomplexity is asymptotically $O(\\epsilon^{-2})$ where $\\epsilon$ is a certain\ndistance between the parameters of the models. Furthermore, we draw an analogy\nbetween the softmax model and the leverage score model, an important tool for\nalgorithm design in linear algebra and graph theory. The leverage score model,\non a high level, is a model which, given a vector input, produces an output\ndrawn from a distribution dependent on the input. We obtain similar results for\nthe binary hypothesis testing problem for leverage score models."}
{"id": "2405.08719", "pdf": "https://arxiv.org/pdf/2405.08719", "abs": "https://arxiv.org/abs/2405.08719", "authors": ["Antoine Wehenkel", "Juan L. Gamella", "Ozan Sener", "Jens Behrmann", "Guillermo Sapiro", "JÃ¶rn-Henrik Jacobsen", "Marco Cuturi"], "title": "Addressing Misspecification in Simulation-based Inference through Data-driven Calibration", "categories": ["stat.ML", "cs.LG", "stat.ME"], "comment": null, "summary": "Driven by steady progress in deep generative modeling, simulation-based\ninference (SBI) has emerged as the workhorse for inferring the parameters of\nstochastic simulators. However, recent work has demonstrated that model\nmisspecification can compromise the reliability of SBI, preventing its adoption\nin important applications where only misspecified simulators are available.\nThis work introduces robust posterior estimation~(RoPE), a framework that\novercomes model misspecification with a small real-world calibration set of\nground-truth parameter measurements. We formalize the misspecification gap as\nthe solution of an optimal transport~(OT) problem between learned\nrepresentations of real-world and simulated observations, allowing RoPE to\nlearn a model of the misspecification without placing additional assumptions on\nits nature. RoPE demonstrates how OT and a calibration set provide a\ncontrollable balance between calibrated uncertainty and informative inference,\neven under severely misspecified simulators. Results on four synthetic tasks\nand two real-world problems with ground-truth labels demonstrate that RoPE\noutperforms baselines and consistently returns informative and calibrated\ncredible intervals."}
{"id": "2407.18929", "pdf": "https://arxiv.org/pdf/2407.18929", "abs": "https://arxiv.org/abs/2407.18929", "authors": ["Alan J. X. Guo", "Mengyi Wei", "Yufan Dai", "Yali Wei", "Pengchen Zhang"], "title": "Disturbance-based Discretization, Differentiable IDS Channel, and an IDS-Correcting Code for DNA Storage", "categories": ["cs.IT", "cs.ET", "cs.LG", "math.IT"], "comment": null, "summary": "Insertion, deletion, and substitution (IDS) error-correcting codes have\ngarnered increased attention with recent advancements in DNA storage\ntechnology. However, a universal method for designing tailored IDS-correcting\ncodes across varying channel settings remains underexplored. We present an\nautoencoder-based approach, THEA-code, aimed at efficiently generating\nIDS-correcting codes for complex IDS channels. In the work, a disturbance-based\ndiscretization is proposed to discretize the features of the autoencoder, and a\nsimulated differentiable IDS channel is developed as a differentiable\nalternative for IDS operations. These innovations facilitate the successful\nconvergence of the autoencoder, producing channel-customized IDS-correcting\ncodes that demonstrate commendable performance across complex IDS channels,\nparticularly in the realistic DNA storage channel."}
{"id": "2410.00665", "pdf": "https://arxiv.org/pdf/2410.00665", "abs": "https://arxiv.org/abs/2410.00665", "authors": ["Moein Khajehnejad", "Forough Habibollahi", "Ahmad Khajehnejad", "Chris French", "Brett J. Kagan", "Adeel Razi"], "title": "Graph-Based Representation Learning of Neuronal Dynamics and Behavior", "categories": ["q-bio.NC", "cs.LG", "cs.NE"], "comment": "31 pages, 6 figures, 4 supplemental figures, 4 tables, 8 supplemental\n  tables", "summary": "Understanding how neuronal networks reorganize in response to external\nstimuli and give rise to behavior is a central challenge in neuroscience and\nartificial intelligence. However, existing methods often fail to capture the\nevolving structure of neural connectivity in ways that capture its relationship\nto behavior, especially in dynamic, uncertain, or high-dimensional settings\nwith sufficient resolution or interpretability. We introduce the Temporal\nAttention-enhanced Variational Graph Recurrent Neural Network (TAVRNN), a novel\nframework that models time-varying neuronal connectivity by integrating\nprobabilistic graph learning with temporal attention mechanisms. TAVRNN learns\nlatent dynamics at the single-unit level while maintaining interpretable\npopulation-level representations, to identify key connectivity patterns linked\nto behavior. TAVRNN generalizes across diverse neural systems and modalities,\ndemonstrating state-of-the-art classification and clustering performance. We\nvalidate TAVRNN on three diverse datasets: (1) electrophysiological data from a\nfreely behaving rat, (2) primate somatosensory cortex recordings during a\nreaching task, and (3) biological neurons in the DishBrain platform interacting\nwith a virtual game environment. Our method outperforms state-of-the-art\ndynamic embedding techniques, revealing previously unreported relationships\nbetween adaptive behavior and the evolving topological organization of neural\nnetworks. These findings demonstrate that TAVRNN offers a powerful and\ngeneralizable approach for modeling neural dynamics across experimental and\nsynthetic biological systems. Its architecture is modality-agnostic and\nscalable, making it applicable across a wide range of neural recording\nplatforms and behavioral paradigms."}
{"id": "2410.19990", "pdf": "https://arxiv.org/pdf/2410.19990", "abs": "https://arxiv.org/abs/2410.19990", "authors": ["Ricardo Baptista", "Michael Brennan", "Youssef Marzouk"], "title": "Dimension reduction via score ratio matching", "categories": ["stat.CO", "cs.LG", "stat.ML", "65C60, 62F15", "G.3"], "comment": "23 pages, 9 figures, 1 table", "summary": "Gradient-based dimension reduction decreases the cost of Bayesian inference\nand probabilistic modeling by identifying maximally informative (and informed)\nlow-dimensional projections of the data and parameters, allowing\nhigh-dimensional problems to be reformulated as cheaper low-dimensional\nproblems. A broad family of such techniques identify these projections and\nprovide error bounds on the resulting posterior approximations, via\neigendecompositions of certain diagnostic matrices. Yet these matrices require\ngradients or even Hessians of the log-likelihood, excluding the purely\ndata-driven setting and many problems of simulation-based inference. We propose\na framework, derived from score-matching, to extend gradient-based dimension\nreduction to problems where gradients are unavailable. Specifically, we\nformulate an objective function to directly learn the score ratio function\nneeded to compute the diagnostic matrices, propose a tailored parameterization\nfor the score ratio network, and introduce regularization methods that\ncapitalize on the hypothesized low-dimensional structure. We also introduce a\nnovel algorithm to iteratively identify the low-dimensional reduced basis\nvectors more accurately with limited data based on eigenvalue deflation\nmethods. We show that our approach outperforms standard score-matching for\nproblems with low-dimensional structure, and demonstrate its effectiveness for\nPDE-constrained Bayesian inverse problems and conditional generative modeling."}
{"id": "2411.03328", "pdf": "https://arxiv.org/pdf/2411.03328", "abs": "https://arxiv.org/abs/2411.03328", "authors": ["Alec Farid", "Peter Schleede", "Aaron Huang", "Christoffer Heckman"], "title": "Foundation Models for Rapid Autonomy Validation", "categories": ["cs.RO", "cs.LG"], "comment": null, "summary": "We are motivated by the problem of autonomous vehicle performance validation.\nA key challenge is that an autonomous vehicle requires testing in every kind of\ndriving scenario it could encounter, including rare events, to provide a strong\ncase for safety and show there is no edge-case pathological behavior.\nAutonomous vehicle companies rely on potentially millions of miles driven in\nrealistic simulation to expose the driving stack to enough miles to estimate\nrates and severity of collisions. To address scalability and coverage, we\npropose the use of a behavior foundation model, specifically a masked\nautoencoder (MAE), trained to reconstruct driving scenarios. We leverage the\nfoundation model in two complementary ways: we (i) use the learned embedding\nspace to group qualitatively similar scenarios together and (ii) fine-tune the\nmodel to label scenario difficulty based on the likelihood of a collision upon\nsimulation. We use the difficulty scoring as importance weighting for the\ngroups of scenarios. The result is an approach which can more rapidly estimate\nthe rates and severity of collisions by prioritizing hard scenarios while\nensuring exposure to every kind of driving scenario."}
{"id": "2412.07223", "pdf": "https://arxiv.org/pdf/2412.07223", "abs": "https://arxiv.org/abs/2412.07223", "authors": ["Zong Ke", "Jingyu Xu", "Zizhou Zhang", "Yu Cheng", "Wenjun Wu"], "title": "A Consolidated Volatility Prediction with Back Propagation Neural Network and Genetic Algorithm", "categories": ["q-fin.CP", "cs.LG", "cs.NE"], "comment": "6 pages, 7 figures, 1 table, The paper will be published by IEEE on\n  conference: 2024 3rd International Conference on Image Processing, Computer\n  Vision and Machine Learning (ICICML 2024) (V4)", "summary": "This paper provides a unique approach with AI algorithms to predict emerging\nstock markets volatility. Traditionally, stock volatility is derived from\nhistorical volatility,Monte Carlo simulation and implied volatility as well. In\nthis paper, the writer designs a consolidated model with back-propagation\nneural network and genetic algorithm to predict future volatility of emerging\nstock markets and found that the results are quite accurate with low errors."}
{"id": "2412.16457", "pdf": "https://arxiv.org/pdf/2412.16457", "abs": "https://arxiv.org/abs/2412.16457", "authors": ["Zhangsong Li"], "title": "Robust random graph matching in Gaussian models via vector approximate message passing", "categories": ["stat.ML", "cs.DS", "cs.LG", "math.PR", "math.ST", "stat.TH", "68Q87, 90C35"], "comment": "41 pages; revised according to reviewer comments and changed the\n  title; an extended abstract of this paper will be presented at COLT 2025", "summary": "In this paper, we focus on the matching recovery problem between a pair of\ncorrelated Gaussian Wigner matrices with a latent vertex correspondence. We are\nparticularly interested in a robust version of this problem such that our\nobservation is a perturbed input $(A+E,B+F)$ where $(A,B)$ is a pair of\ncorrelated Gaussian Wigner matrices and $E,F$ are adversarially chosen matrices\nsupported on an unknown $\\epsilon n * \\epsilon n$ principle minor of $A,B$,\nrespectively. We propose a vector approximate message passing (vector AMP)\nalgorithm that succeeds in polynomial time as long as the correlation $\\rho$\nbetween $(A,B)$ is a non-vanishing constant and $\\epsilon = o\\big(\n\\tfrac{1}{(\\log n)^{20}} \\big)$.\n  The main methodological inputs for our result are the iterative random graph\nmatching algorithm proposed in \\cite{DL22+, DL23+} and the spectral cleaning\nprocedure proposed in \\cite{IS24+}. To the best of our knowledge, our algorithm\nis the first efficient random graph matching type algorithm that is robust\nunder any adversarial perturbations of $n^{1-o(1)}$ size."}
{"id": "2501.19277", "pdf": "https://arxiv.org/pdf/2501.19277", "abs": "https://arxiv.org/abs/2501.19277", "authors": ["Jierui Zuo", "Hanzhang Qin"], "title": "On Pareto Optimality for the Multinomial Logistic Bandit", "categories": ["stat.ML", "cs.LG"], "comment": null, "summary": "We provide a new online learning algorithm for tackling the Multinomial Logit\nBandit (MNL-Bandit) problem. Despite the challenges posed by the combinatorial\nnature of the MNL model, we develop a novel Upper Confidence Bound (UCB)-based\nmethod that achieves Pareto optimality by balancing regret minimization and\nestimation error of the assortment revenues and the MNL parameters. We develop\ntheoretical guarantees characterizing the tradeoff between regret and\nestimation error for the MNL-Bandit problem through information-theoretic\nbounds, and propose a modified UCB algorithm that incorporates forced\nexploration to improve parameter estimation accuracy while maintaining low\nregret. Our analysis sheds critical insights into how to optimally balance the\ncollected revenues and the treatment estimation in dynamic assortment\noptimization."}
{"id": "2502.00168", "pdf": "https://arxiv.org/pdf/2502.00168", "abs": "https://arxiv.org/abs/2502.00168", "authors": ["Daniel Herrera-Esposito", "Johannes Burge"], "title": "Supervised Quadratic Feature Analysis: Information Geometry Approach for Dimensionality Reduction", "categories": ["stat.ML", "cs.LG", "math.DG", "math.ST", "stat.TH"], "comment": "27 pages, 13 figures", "summary": "Supervised dimensionality reduction aims to map labeled data to a\nlow-dimensional feature space while maximizing class discriminability. Directly\ncomputing discriminability is often impractical, so an alternative approach is\nto learn features that maximize a distance or dissimilarity measure between\nclasses. The Fisher-Rao distance is an important information geometry distance\nin statistical manifolds. It is induced by the Fisher information metric, a\ntool widely used for understanding neural representations. Despite its\ntheoretical and pratical appeal, Fisher-Rao distances between classes have not\nbeen used as a maximization objective in supervised feature learning. Here, we\npresent Supervised Quadratic Feature Analysis (SQFA), a linear dimensionality\nreduction method that maximizes Fisher-Rao distances between class\ndistributions, by exploiting the information geometry of the symmetric positive\ndefinite manifold. SQFA maximizes distances using first- and second-order\nstatistics, and its features allow for quadratic discriminability (i.e. QDA\nperformance) matching or surpassing state-of-the-art methods on real-world\ndatasets. We theoretically motivate Fisher-Rao distances as a proxy for\nquadratic discriminability, and compare its performance to other popular\ndistances (e.g. Wasserstein distances). SQFA provides a flexible\nstate-of-the-art method for dimensionality reduction. Its successful use of\nFisher-Rao distances between classes motivates future research directions."}
{"id": "2502.05368", "pdf": "https://arxiv.org/pdf/2502.05368", "abs": "https://arxiv.org/abs/2502.05368", "authors": ["Toufique Ahmed", "Jatin Ganhotra", "Rangeet Pan", "Avraham Shinnar", "Saurabh Sinha", "Martin Hirzel"], "title": "Otter: Generating Tests from Issues to Validate SWE Patches", "categories": ["cs.SE", "cs.LG"], "comment": "Accepted to the main technical track of the International Conference\n  on Machine Learning (ICML), 2025", "summary": "While there has been plenty of work on generating tests from existing code,\nthere has been limited work on generating tests from issues. A correct test\nmust validate the code patch that resolves the issue. This paper focuses on the\nscenario where that code patch does not yet exist. Doing so supports two major\nuse-cases. First, it supports TDD (test-driven development), the discipline of\n\"test first, write code later\" that has well-documented benefits for human\nsoftware engineers. Second, it also validates SWE (software engineering)\nagents, which generate code patches for resolving issues. This paper introduces\nTDD-Bench-Verified, a benchmark for generating tests from issues, and Otter, an\nLLM-based solution for this task. Otter augments LLMs with rule-based analysis\nto check and repair their outputs, and introduces a novel self-reflective\naction planner. Experiments show Otter outperforming state-of-the-art systems\nfor generating tests from issues, in addition to enhancing systems that\ngenerate patches from issues. We hope that Otter helps make developers more\nproductive at resolving issues and leads to more robust, well-tested code."}
{"id": "2502.13757", "pdf": "https://arxiv.org/pdf/2502.13757", "abs": "https://arxiv.org/abs/2502.13757", "authors": ["Stas Syrota", "Yevgen Zainchkovskyy", "Johnny Xi", "Benjamin Bloem-Reddy", "SÃ¸ren Hauberg"], "title": "Identifying Metric Structures of Deep Latent Variable Models", "categories": ["stat.ML", "cs.LG"], "comment": null, "summary": "Deep latent variable models learn condensed representations of data that,\nhopefully, reflect the inner workings of the studied phenomena. Unfortunately,\nthese latent representations are not statistically identifiable, meaning they\ncannot be uniquely determined. Domain experts, therefore, need to tread\ncarefully when interpreting these. Current solutions limit the lack of\nidentifiability through additional constraints on the latent variable model,\ne.g. by requiring labeled training data, or by restricting the expressivity of\nthe model. We change the goal: instead of identifying the latent variables, we\nidentify relationships between them such as meaningful distances, angles, and\nvolumes. We prove this is feasible under very mild model conditions and without\nadditional labeled data. We empirically demonstrate that our theory results in\nmore reliable latent distances, offering a principled path forward in\nextracting trustworthy conclusions from deep latent variable models."}
{"id": "2503.00131", "pdf": "https://arxiv.org/pdf/2503.00131", "abs": "https://arxiv.org/abs/2503.00131", "authors": ["Farouk Mokhtar", "Joosep Pata", "Dolores Garcia", "Eric Wulff", "Mengke Zhang", "Michael Kagan", "Javier Duarte"], "title": "Fine-tuning machine-learned particle-flow reconstruction for new detector geometries in future colliders", "categories": ["hep-ex", "cs.LG", "hep-ph", "physics.data-an", "physics.ins-det"], "comment": "20 pages, 13 figures", "summary": "We demonstrate transfer learning capabilities in a machine-learned algorithm\ntrained for particle-flow reconstruction in high energy particle colliders.\nThis paper presents a cross-detector fine-tuning study, where we initially\npretrain the model on a large full simulation dataset from one detector design,\nand subsequently fine-tune the model on a sample with a different collider and\ndetector design. Specifically, we use the Compact Linear Collider detector\n(CLICdet) model for the initial training set and demonstrate successful\nknowledge transfer to the CLIC-like detector (CLD) proposed for the Future\nCircular Collider in electron-positron mode. We show that with an order of\nmagnitude less samples from the second dataset, we can achieve the same\nperformance as a costly training from scratch, across particle-level and\nevent-level performance metrics, including jet and missing transverse momentum\nresolution. Furthermore, we find that the fine-tuned model achieves comparable\nperformance to the traditional rule-based particle-flow approach on event-level\nmetrics after training on 100,000 CLD events, whereas a model trained from\nscratch requires at least 1 million CLD events to achieve similar\nreconstruction performance. To our knowledge, this represents the first\nfull-simulation cross-detector transfer learning study for particle-flow\nreconstruction. These findings offer valuable insights towards building large\nfoundation models that can be fine-tuned across different detector designs and\ngeometries, helping to accelerate the development cycle for new detectors and\nopening the door to rapid detector design and optimization using machine\nlearning."}
{"id": "2503.00140", "pdf": "https://arxiv.org/pdf/2503.00140", "abs": "https://arxiv.org/abs/2503.00140", "authors": ["Abdessamad El-Kabid", "El-Mahdi El-Mhamdi"], "title": "Approaching the Harm of Gradient Attacks While Only Flipping Labels", "categories": ["cs.CR", "cs.LG", "stat.ML"], "comment": null, "summary": "Machine learning systems deployed in distributed or federated environments\nare highly susceptible to adversarial manipulations, particularly availability\nattacks -adding imperceptible perturbations to training data, thereby rendering\nthe trained model unavailable. Prior research in distributed machine learning\nhas demonstrated such adversarial effects through the injection of gradients or\ndata poisoning. In this study, we aim to enhance comprehension of the potential\nof weaker (and more probable) adversaries by posing the following inquiry: Can\navailability attacks be inflicted solely through the flipping of a subset of\ntraining labels, without altering features, and under a strict flipping budget?\nWe analyze the extent of damage caused by constrained label flipping attacks.\nFocusing on a distributed classification problem, (1) we propose a novel\nformalization of label flipping attacks on logistic regression models and\nderive a greedy algorithm that is provably optimal at each training step. (2)\nTo demonstrate that availability attacks can be approached by label flipping\nalone, we show that a budget of only $0.1\\%$ of labels at each training step\ncan reduce the accuracy of the model by $6\\%$, and that some models can perform\nworse than random guessing when up to $25\\%$ of labels are flipped. (3) We shed\nlight on an interesting interplay between what the attacker gains from more\nwrite-access versus what they gain from more flipping budget. (4) we define and\ncompare the power of targeted label flipping attack to that of an untargeted\nlabel flipping attack."}
{"id": "2504.10545", "pdf": "https://arxiv.org/pdf/2504.10545", "abs": "https://arxiv.org/abs/2504.10545", "authors": ["Yijun Liu"], "title": "HSTU-BLaIR: Lightweight Contrastive Text Embedding for Generative Recommender", "categories": ["cs.IR", "cs.LG"], "comment": "Code available at https://www.github.com/snapfinger/HSTU-BLaIR", "summary": "Recent advances in recommender systems have underscored the complementary\nstrengths of generative modeling and pretrained language models. We propose\nHSTU-BLaIR, a hybrid framework that augments the Hierarchical Sequential\nTransduction Unit (HSTU)-based generative recommender with BLaIR, a lightweight\ncontrastive text embedding model. This integration enriches item\nrepresentations with semantic signals from textual metadata while preserving\nHSTU's powerful sequence modeling capabilities.\n  We evaluate HSTU-BLaIR on two e-commerce datasets: three subsets from the\nAmazon Reviews 2023 dataset and the Steam dataset. We compare its performance\nagainst both the original HSTU-based recommender and a variant augmented with\nembeddings from OpenAI's state-of-the-art \\texttt{text-embedding-3-large}\nmodel. Despite the latter being trained on a substantially larger corpus with\nsignificantly more parameters, our lightweight BLaIR-enhanced approach --\npretrained on domain-specific data -- achieves better performance in nearly all\ncases. Specifically, HSTU-BLaIR outperforms the OpenAI embedding-based variant\non all but one metric, where it is marginally lower, and matches it on another.\nThese findings highlight the effectiveness of contrastive text embeddings in\ncompute-efficient recommendation settings."}
{"id": "2504.13883", "pdf": "https://arxiv.org/pdf/2504.13883", "abs": "https://arxiv.org/abs/2504.13883", "authors": ["Shayla Sharmin", "Roghayeh Leila Barmaki"], "title": "Hybrid Deep Learning Model to Estimate Cognitive Effort from fNIRS Signals in Educational Game Playing", "categories": ["cs.HC", "cs.LG"], "comment": null, "summary": "This study estimates cognitive effort (CE) based on functional near-infrared\nspectroscopy (fNIRS) data and performance scores using a hybrid deep learning\nmodel. The estimation of CE enables educators to modify material to enhance\nlearning effectiveness and student engagement. Relative neural efficiency (RNE)\nand relative neural involvement (RNI) are two metrics that have been used to\nrepresent CE. To estimate RNE and RNI we need hemodynamic response in the brain\nand the performance score of a task.We collected oxygenated hemoglobin ($\\Delta\n\\mathrm{HbO}$). Sixteen participants answered 16 questions in a unity-based\neducational game, each with a 30-second response time. We used deep learning\nmodels to predict the performance score and estimate RNE and RNI to understand\nCE. The study compares traditional machine learning techniques with deep\nlearning models such as CNN, LSTM, BiLSTM, and a hybrid CNN-GRU to determine\nwhich approach provides better accuracy in predicting performance scores. The\nresult shows that the hybrid CNN-GRU gives better performance with 78.36\\%\ntraining accuracy and 73.08\\% test accuracy than other models. We performed\nXGBoost on the extracted GRU feature and got the highest accuracy (69.23\\%).\nThis suggests that the features learned from this hybrid model generalize\nbetter even in traditional machine learning algorithms. We used the $\\Delta\n\\mathrm{HbO}$ and predicted score to calculate RNE and RNI to observe cognitive\neffort in our four test cases. Our result shows that even with moderate\naccuracy, the predicted RNE and RNI closely follows the actual trends. we also\nobserved that when participants were in a state of high CE, introducing rest\nled decrease of CE. These findings can be helpful to design and improve\nlearning environments and provide valuable insights in learning materials."}
{"id": "2505.14314", "pdf": "https://arxiv.org/pdf/2505.14314", "abs": "https://arxiv.org/abs/2505.14314", "authors": ["Kosmas Alexandridis", "Vasileios Titopoulos", "Giorgos Dimitrakopoulos"], "title": "Low-Cost FlashAttention with Fused Exponential and Multiplication Hardware Operators", "categories": ["cs.AR", "cs.LG"], "comment": "IEEE Computer Society Annual Symposium on VLSI (ISVLSI 2025)", "summary": "Attention mechanisms, particularly within Transformer architectures and large\nlanguage models (LLMs), have revolutionized sequence modeling in machine\nlearning and artificial intelligence applications. To compute attention for\nincreasingly long sequences, specialized accelerators have been proposed to\nexecute key attention steps directly in hardware. Among the various recently\nproposed architectures, those based on variants of the FlashAttention\nalgorithm, originally designed for GPUs, stand out due to their optimized\ncomputation, tiling capabilities, and reduced memory traffic. In this work, we\nfocus on optimizing the kernel of floating-point-based FlashAttention using new\nhardware operators that fuse the computation of exponentials and vector\nmultiplications, e.g., e^x, V. The proposed ExpMul hardware operators\nsignificantly reduce the area and power costs of FlashAttention-based hardware\naccelerators. When implemented in a 28nm ASIC technology, they achieve\nimprovements of 28.8% in area and 17.6% in power, on average, compared to\nstate-of-the-art hardware architectures with separate exponentials and vector\nmultiplications hardware operators."}
{"id": "2505.17836", "pdf": "https://arxiv.org/pdf/2505.17836", "abs": "https://arxiv.org/abs/2505.17836", "authors": ["Anna Van Elst", "Igor Colin", "Stephan ClÃ©menÃ§on"], "title": "Robust Distributed Estimation: Extending Gossip Algorithms to Ranking and Trimmed Means", "categories": ["stat.ML", "cs.LG", "stat.AP"], "comment": null, "summary": "This paper addresses the problem of robust estimation in gossip algorithms\nover arbitrary communication graphs. Gossip algorithms are fully decentralized,\nrelying only on local neighbor-to-neighbor communication, making them\nwell-suited for situations where communication is constrained. A fundamental\nchallenge in existing mean-based gossip algorithms is their vulnerability to\nmalicious or corrupted nodes. In this paper, we show that an outlier-robust\nmean can be computed by globally estimating a robust statistic. More\nspecifically, we propose a novel gossip algorithm for rank estimation, referred\nto as \\textsc{GoRank}, and leverage it to design a gossip procedure dedicated\nto trimmed mean estimation, coined \\textsc{GoTrim}. In addition to a detailed\ndescription of the proposed methods, a key contribution of our work is a\nprecise convergence analysis: we establish an $\\mathcal{O}(1/t)$ rate for rank\nestimation and an $\\mathcal{O}(\\log(t)/t)$ rate for trimmed mean estimation,\nwhere by $t$ is meant the number of iterations. Moreover, we provide a\nbreakdown point analysis of \\textsc{GoTrim}. We empirically validate our\ntheoretical results through experiments on diverse network topologies, data\ndistributions and contamination schemes."}
{"id": "2505.17917", "pdf": "https://arxiv.org/pdf/2505.17917", "abs": "https://arxiv.org/abs/2505.17917", "authors": ["Xingyu Li", "Qing Liu", "Tony Jiang", "Hong Amy Xia", "Brian P. Hobbs", "Peng Wei"], "title": "M-learner:A Flexible And Powerful Framework To Study Heterogeneous Treatment Effect In Mediation Model", "categories": ["stat.ML", "cs.LG", "stat.ME"], "comment": null, "summary": "We propose a novel method, termed the M-learner, for estimating heterogeneous\nindirect and total treatment effects and identifying relevant subgroups within\na mediation framework. The procedure comprises four key steps. First, we\ncompute individual-level conditional average indirect/total treatment effect\nSecond, we construct a distance matrix based on pairwise differences. Third, we\napply tSNE to project this matrix into a low-dimensional Euclidean space,\nfollowed by K-means clustering to identify subgroup structures. Finally, we\ncalibrate and refine the clusters using a threshold-based procedure to\ndetermine the optimal configuration. To the best of our knowledge, this is the\nfirst approach specifically designed to capture treatment effect heterogeneity\nin the presence of mediation. Experimental results validate the robustness and\neffectiveness of the proposed framework. Application to the real-world Jobs II\ndataset highlights the broad adaptability and potential applicability of our\nmethod.Code is available at https: //anonymous.4open.science/r/M-learner-C4BB."}
{"id": "2505.19521", "pdf": "https://arxiv.org/pdf/2505.19521", "abs": "https://arxiv.org/abs/2505.19521", "authors": ["Dongzhe Zheng", "Wenjie Mei"], "title": "Learning Dynamics under Environmental Constraints via Measurement-Induced Bundle Structures", "categories": ["cs.RO", "cs.LG", "cs.SY", "eess.SY"], "comment": "Accepted by ICML 2025", "summary": "Learning unknown dynamics under environmental (or external) constraints is\nfundamental to many fields (e.g., modern robotics), particularly challenging\nwhen constraint information is only locally available and uncertain. Existing\napproaches requiring global constraints or using probabilistic filtering fail\nto fully exploit the geometric structure inherent in local measurements (by\nusing, e.g., sensors) and constraints. This paper presents a geometric\nframework unifying measurements, constraints, and dynamics learning through a\nfiber bundle structure over the state space. This naturally induced geometric\nstructure enables measurement-aware Control Barrier Functions that adapt to\nlocal sensing (or measurement) conditions. By integrating Neural ODEs, our\nframework learns continuous-time dynamics while preserving geometric\nconstraints, with theoretical guarantees of learning convergence and constraint\nsatisfaction dependent on sensing quality. The geometric framework not only\nenables efficient dynamics learning but also suggests promising directions for\nintegration with reinforcement learning approaches. Extensive simulations\ndemonstrate significant improvements in both learning efficiency and constraint\nsatisfaction over traditional methods, especially under limited and uncertain\nsensing conditions."}
{"id": "2505.22688", "pdf": "https://arxiv.org/pdf/2505.22688", "abs": "https://arxiv.org/abs/2505.22688", "authors": ["Palur Venkata Raghuvamsi", "Siyuan Brandon Loh", "Prasanta Bhattacharya", "Joses Ho", "Raphael Lee Tze Chuen", "Alvin X. Han", "Sebastian Maurer-Stroh"], "title": "Investigating the effectiveness of multimodal data in forecasting SARS-COV-2 case surges", "categories": ["q-bio.QM", "cs.LG", "stat.ML"], "comment": null, "summary": "The COVID-19 pandemic response relied heavily on statistical and machine\nlearning models to predict key outcomes such as case prevalence and fatality\nrates. These predictions were instrumental in enabling timely public health\ninterventions that helped break transmission cycles. While most existing models\nare grounded in traditional epidemiological data, the potential of alternative\ndatasets, such as those derived from genomic information and human behavior,\nremains underexplored. In the current study, we investigated the usefulness of\ndiverse modalities of feature sets in predicting case surges. Our results\nhighlight the relative effectiveness of biological (e.g., mutations), public\nhealth (e.g., case counts, policy interventions) and human behavioral features\n(e.g., mobility and social media conversations) in predicting country-level\ncase surges. Importantly, we uncover considerable heterogeneity in predictive\nperformance across countries and feature modalities, suggesting that surge\nprediction models may need to be tailored to specific national contexts and\npandemic phases. Overall, our work highlights the value of integrating\nalternative data sources into existing disease surveillance frameworks to\nenhance the prediction of pandemic dynamics."}
