<div id=toc></div>

# Table of Contents

- [cs.CL](#cs.CL) [Total: 250]
- [cs.CV](#cs.CV) [Total: 205]
- [cs.AI](#cs.AI) [Total: 82]
- [cs.SD](#cs.SD) [Total: 24]
- [cs.LG](#cs.LG) [Total: 245]
- [cs.MA](#cs.MA) [Total: 9]
- [cs.MM](#cs.MM) [Total: 1]
- [eess.AS](#eess.AS) [Total: 18]
- [eess.IV](#eess.IV) [Total: 14]


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [1] [Towards Pretraining Robust ASR Foundation Model with Acoustic-Aware Data Augmentation](https://arxiv.org/pdf/2505.20606)
*Dancheng Liu, Amir Nassereldine, Chenhui Xu, Jinjun Xiong*

Main category: cs.CL

TL;DR: Targeted acoustic augmentation improves ASR model robustness, reducing word-error rates by 19.24%, even with smaller datasets like Librispeech.


<details>
  <summary>Details</summary>
Motivation: Examine how linguistic and acoustic diversity in training data affect ASR robustness, given impracticality of massive datasets like Whisper's 680k-hour set.

Method: Analyze impact of acoustic variation vs. linguistic richness on ASR performance; test targeted acoustic augmentation on the 960-hour Librispeech dataset.

Result: Acoustic variation drives transcription generalization more than linguistic richness; targeted augmentation reduces word-error rates by up to 19.24% on unseen datasets.

Conclusion: Strategic acoustic-focused data augmentation is a viable alternative to massive datasets for robust ASR models, especially when large-scale human speech data is unavailable.

Abstract: Whisper's robust performance in automatic speech recognition (ASR) is often
attributed to its massive 680k-hour training set, an impractical scale for most
researchers. In this work, we examine how linguistic and acoustic diversity in
training data affect the robustness of the ASR model and reveal that
transcription generalization is primarily driven by acoustic variation rather
than linguistic richness. We find that targeted acoustic augmentation methods
could significantly improve the generalization ability of ASR models, reducing
word-error rates by up to 19.24 percent on unseen datasets when training on the
960-hour Librispeech dataset. These findings highlight strategic acoustically
focused data augmentation as a promising alternative to massive datasets for
building robust ASR models, offering a potential solution to future foundation
ASR models when massive human speech data is lacking.

</details>


### [2] [Guiding Giants: Lightweight Controllers for Weighted Activation Steering in LLMs](https://arxiv.org/pdf/2505.20309)
*Amr Hegazy, Mostafa Elhoushi, Amr Alanwar*

Main category: cs.CL

TL;DR: A lightweight, trainable controller network is introduced for fine-grained, adaptive control of LLM behaviors during inference, outperforming existing methods.


<details>
  <summary>Details</summary>
Motivation: To address the lack of fine-grained, adaptive mechanisms in existing activation steering methods for controlling undesirable LLM behaviors like unsafe content generation.

Method: A controller network observes LLM activations, predicts a global scaling factor and layer-specific weights, and dynamically modulates a steering patch derived from a pre-computed refusal direction.

Result: Significantly increases refusal rates for harmful inputs on benchmarks like ToxicChat and In-The-Wild Jailbreak Prompts without altering original model parameters.

Conclusion: The approach provides an efficient, adaptive method for fine-grained inference-time control over LLM behavior, outperforming existing methods.

Abstract: Controlling undesirable Large Language Model (LLM) behaviors, such as the
generation of unsafe content or failing to adhere to safety guidelines, often
relies on costly fine-tuning. Activation steering provides an alternative for
inference-time control, but existing methods typically lack fine-grained,
adaptive mechanisms. We introduce a novel approach using a lightweight,
trainable controller network integrated during inference. This controller
network observes specific intermediate LLM activations and predicts both a
global scaling factor and layer-specific weights. The predicted global scaling
factor and layer-specific weights then dynamically modulate the intensity of a
steering patch, derived from a pre-computed "refusal direction" vector, applied
across the LLM's layers during generation. Trained on activations from both
harmful and benign prompts, our controller learns to discriminatively apply
nuanced, layer-aware interventions, activating steering primarily for harmful
inputs. Experiments using safety benchmarks like ToxicChat & In-The-Wild
Jailbreak Prompts demonstrate that our weighted steering controller
significantly increases refusal rates compared to the base LLM, achieving
targeted behavioral modification without altering the original model
parameters. Our experiments with Llama-3.1-8B, Llama-3.2-1B & Mistral-7B show
our approach outperforms existing methods, presenting an efficient and adaptive
method for fine-grained control over LLM behavior at inference time.

</details>


### [3] [Arctic-Text2SQL-R1: Simple Rewards, Strong Reasoning in Text-to-SQL](https://arxiv.org/pdf/2505.20315)
*Zhewei Yao, Guoheng Sun, Lukasz Borchmann, Zheyu Shen, Minghang Deng, Bohan Zhai, Hao Zhang, Ang Li, Yuxiong He*

Main category: cs.CL

TL;DR: Arctic-Text2SQL-R1 uses reinforcement learning to generate accurate SQL from natural language, achieving state-of-the-art results without complex supervision.


<details>
  <summary>Details</summary>
Motivation: Improving SQL generation correctness, especially for complex queries, remains a challenge despite advances in LLMs.

Method: A reinforcement learning framework with execution-based rewards, strong supervised initialization, and curated data.

Result: State-of-the-art execution accuracy on six benchmarks, outperforming larger models.

Conclusion: The approach is scalable, efficient, and offers practical insights for future research.

Abstract: Translating natural language into SQL (Test2SQL) is a longstanding challenge
at the intersection of natural language understanding and structured data
access. While large language models (LLMs) have significantly improved fluency
in SQL generation, producing correct and executable SQL--particularly for
complex queries--remains a bottleneck. We present Arctic-Text2SQL-R1, a
reinforcement learning (RL) framework and model family designed to generate
accurate, executable SQL using a lightweight reward signal based solely on
execution correctness. Our approach avoids brittle intermediate supervision and
complex reward shaping, promoting stable training and alignment with the end
task. Combined with carefully curated data, strong supervised initialization,
and effective training practices, Arctic-Text2SQL-R1 achieves state-of-the-art
execution accuracy across six diverse Test2SQL benchmarks, including the top
position on the BIRD leaderboard. Notably, our 7B model outperforms prior
70B-class systems, highlighting the framework's scalability and efficiency. We
further demonstrate inference-time robustness through simple extensions like
value retrieval and majority voting. Extensive experiments and ablation studies
offer both positive and negative insights, providing practical guidance for
future Test2SQL research.

</details>


### [4] [Beyond Demonstrations: Dynamic Vector Construction from Latent Representations](https://arxiv.org/pdf/2505.20318)
*Wang Cai, Hsiu-Yuan Huang, Zhixiang Wang, Yunfang Wu*

Main category: cs.CL

TL;DR: DyVec improves ICV methods by dynamically extracting and injecting semantically aggregated latent representations, outperforming few-shot ICL and prior ICV baselines.


<details>
  <summary>Details</summary>
Motivation: Existing ICV methods are sensitive to ICL-specific factors, use coarse representations, and rely on heuristic-based injection positions, limiting their applicability.

Method: DyVec uses Exhaustive Query Rotation (EQR) for robust representation extraction, Dynamic Latent Segmentation for adaptive partitioning, and REINFORCE-based optimization for optimal injection positions.

Result: DyVec outperforms few-shot ICL, LoRA, and prior ICV baselines in experiments.

Conclusion: DyVec offers a lightweight, data-efficient solution for inference-time task adaptation, demonstrating the effectiveness of dynamic segmentation and injection.

Abstract: In-Context derived Vector (ICV) methods extract task-relevant representations
from large language models (LLMs) and reinject them during inference, achieving
comparable performance to few-shot In-Context Learning (ICL) without repeated
demonstration processing. However, existing ICV methods remain sensitive to
ICL-specific factors, often use coarse or semantically fragmented
representations as the source of the vector, and rely on heuristic-based
injection positions, limiting their applicability.
  To address these issues, we propose Dynamic Vector (DyVec), which
incorporates an Exhaustive Query Rotation (EQR) strategy to extract robust
semantically aggregated latent representations by mitigating variance
introduced by ICL. It then applies Dynamic Latent Segmentation and Injection to
adaptively partition representations based on task complexity and leverages
REINFORCE-based optimization to learn optimal injection positions for each
segment.
  Experiments results show that DyVec outperforms few-shot ICL, LoRA, and prior
ICV baselines. Further analysis highlights the effectiveness of dynamically
segmenting and injecting semantically aggregated latent representations. DyVec
provides a lightweight and data-efficient solution for inference-time task
adaptation.

</details>


### [5] [Less Context, Same Performance: A RAG Framework for Resource-Efficient LLM-Based Clinical NLP](https://arxiv.org/pdf/2505.20320)
*Satya Narayana Cheetirala, Ganesh Raut, Dhavalkumar Patel, Fabio Sanatana, Robert Freeman, Matthew A Levin, Girish N. Nadkarni, Omar Dawkins, Reba Miller, Randolph M. Steinhagen, Eyal Klang, Prem Timsina*

Main category: cs.CL

TL;DR: RAG approach with relevant text segments matches whole-text LLM performance in clinical note classification, reducing token usage without accuracy loss.


<details>
  <summary>Details</summary>
Motivation: Address challenges of long text classification in LLMs (token limits, high costs) by testing RAG's efficiency.

Method: Split clinical notes into chunks, embed vectors, store in FAISS index, retrieve top 4,000 words, feed to LLMs (GPT4o, LLaMA, Mistral).

Result: No significant difference in AUC ROC, precision, recall, F1 between RAG and whole-text processing (p > 0.05).

Conclusion: RAG reduces computational costs and token usage while maintaining accuracy, offering a scalable solution for clinical document analysis.

Abstract: Long text classification is challenging for Large Language Models (LLMs) due
to token limits and high computational costs. This study explores whether a
Retrieval Augmented Generation (RAG) approach using only the most relevant text
segments can match the performance of processing entire clinical notes with
large context LLMs. We begin by splitting clinical documents into smaller
chunks, converting them into vector embeddings, and storing these in a FAISS
index. We then retrieve the top 4,000 words most pertinent to the
classification query and feed these consolidated segments into an LLM. We
evaluated three LLMs (GPT4o, LLaMA, and Mistral) on a surgical complication
identification task. Metrics such as AUC ROC, precision, recall, and F1 showed
no statistically significant differences between the RAG based approach and
whole-text processing (p > 0.05p > 0.05). These findings indicate that RAG can
significantly reduce token usage without sacrificing classification accuracy,
providing a scalable and cost effective solution for analyzing lengthy clinical
documents.

</details>


### [6] [BiomedSQL: Text-to-SQL for Scientific Reasoning on Biomedical Knowledge Bases](https://arxiv.org/pdf/2505.20321)
*Mathew J. Koretsky, Maya Willey, Adi Asija, Owen Bianchi, Chelsea X. Alvarado, Tanay Nayak, Nicole Kuznetsov, Sungwon Kim, Mike A. Nalls, Daniel Khashabi, Faraz Faghri*

Main category: cs.CL

TL;DR: BiomedSQL is a benchmark for evaluating scientific reasoning in text-to-SQL systems for biomedical databases, revealing performance gaps in current models.


<details>
  <summary>Details</summary>
Motivation: Current text-to-SQL systems struggle with domain-specific reasoning in biomedical research, necessitating a specialized benchmark.

Method: BiomedSQL includes 68,000 question/SQL/answer triples grounded in a biomedical knowledge base, testing implicit domain reasoning.

Result: GPT-3-mini achieves 59.0% accuracy, while the custom BMSQL agent reaches 62.6%, both below the expert baseline of 90.0%.

Conclusion: BiomedSQL advances text-to-SQL systems by enabling robust reasoning over biomedical knowledge, with open-source data and code.

Abstract: Biomedical researchers increasingly rely on large-scale structured databases
for complex analytical tasks. However, current text-to-SQL systems often
struggle to map qualitative scientific questions into executable SQL,
particularly when implicit domain reasoning is required. We introduce
BiomedSQL, the first benchmark explicitly designed to evaluate scientific
reasoning in text-to-SQL generation over a real-world biomedical knowledge
base. BiomedSQL comprises 68,000 question/SQL query/answer triples grounded in
a harmonized BigQuery knowledge base that integrates gene-disease associations,
causal inference from omics data, and drug approval records. Each question
requires models to infer domain-specific criteria, such as genome-wide
significance thresholds, effect directionality, or trial phase filtering,
rather than rely on syntactic translation alone. We evaluate a range of open-
and closed-source LLMs across prompting strategies and interaction paradigms.
Our results reveal a substantial performance gap: GPT-o3-mini achieves 59.0%
execution accuracy, while our custom multi-step agent, BMSQL, reaches 62.6%,
both well below the expert baseline of 90.0%. BiomedSQL provides a new
foundation for advancing text-to-SQL systems capable of supporting scientific
discovery through robust reasoning over structured biomedical knowledge bases.
Our dataset is publicly available at
https://huggingface.co/datasets/NIH-CARD/BiomedSQL, and our code is open-source
at https://github.com/NIH-CARD/biomedsql.

</details>


### [7] [Beyond Prompt Engineering: Robust Behavior Control in LLMs via Steering Target Atoms](https://arxiv.org/pdf/2505.20322)
*Mengru Wang, Ziwen Xu, Shengyu Mao, Shumin Deng, Zhaopeng Tu, Huajun Chen, Ningyu Zhang*

Main category: cs.CL

TL;DR: The paper introduces Steering Target Atoms (STA), a method to isolate and manipulate disentangled knowledge components in language models for improved safety and control.


<details>
  <summary>Details</summary>
Motivation: Precise control over language model generation is crucial for safety and reliability, but current methods like prompt engineering and steering face limitations due to intertwined internal representations.

Method: The proposed STA method isolates and manipulates disentangled knowledge components, addressing the challenge of locating atomic knowledge in high-dimensional spaces.

Result: Experiments show STA's effectiveness in enhancing safety, robustness, and flexibility, especially in adversarial scenarios. It also works well for precise reasoning control in large models.

Conclusion: STA offers a promising solution for precise and safe control of language model behaviors, outperforming existing methods in robustness and flexibility.

Abstract: Precise control over language model generation is vital for ensuring both
safety and reliability. Although prompt engineering and steering are commonly
used to intervene in model behaviors, the vast number of parameters in models
often results in highly intertwined internal representations. This
interdependency can limit control precision and sometimes lead to unintended
side effects. Recent research has explored the use of sparse autoencoders (SAE)
to disentangle knowledge in high-dimensional spaces for steering. However,
these applications have been limited to toy tasks owing to the nontrivial issue
of locating atomic knowledge components. In this paper, we propose Steering
Target Atoms (STA), a novel method that isolates and manipulates disentangled
knowledge components to enhance safety. Comprehensive experiments demonstrate
the effectiveness of our approach. Further analysis reveals that steering
exhibits superior robustness and flexibility, particularly in adversarial
scenarios. We also apply the steering strategy to the large reasoning model,
confirming its effectiveness in precise reasoning control.

</details>


### [8] [PMOA-TTS: Introducing the PubMed Open Access Textual Times Series Corpus](https://arxiv.org/pdf/2505.20323)
*Shahriar Noroozizadeh, Sayantan Kumar, George H. Chen, Jeremy C. Weiss*

Main category: cs.CL

TL;DR: PMOA-TTS is the first openly available dataset of 124,699 PubMed Open Access case reports, converted into structured (event, time) timelines using an LLM-based pipeline, with over 5.6 million timestamped events. It shows high quality in event matching, temporal concordance, and timestamp alignment, and proves useful in downstream tasks like survival prediction.


<details>
  <summary>Details</summary>
Motivation: Large-scale temporally annotated resources for clinical narratives are limited, hindering patient trajectory modeling. PMOA-TTS addresses this gap by providing a scalable dataset for temporal reasoning in biomedical NLP.

Method: The approach combines heuristic filtering with Llama 3.3 to identify single-patient case reports, followed by prompt-driven extraction using Llama 3.3 and DeepSeek R1 to create structured timelines. Quality is assessed via event-level matching, temporal concordance, and timestamp alignment metrics.

Result: The dataset achieves 80% event-level matching, c-index > 0.90 for temporal concordance, and high AULTC for timestamp alignment. In survival prediction, embeddings from timelines achieve time-dependent concordance indices up to 0.82 ± 0.01.

Conclusion: PMOA-TTS provides a scalable foundation for timeline extraction and longitudinal modeling in biomedical NLP, demonstrating the predictive value of temporally structured narratives. The dataset is openly available.

Abstract: Understanding temporal dynamics in clinical narratives is essential for
modeling patient trajectories, yet large-scale temporally annotated resources
remain limited. We present PMOA-TTS, the first openly available dataset of
124,699 PubMed Open Access (PMOA) case reports, each converted into structured
(event, time) timelines via a scalable LLM-based pipeline. Our approach
combines heuristic filtering with Llama 3.3 to identify single-patient case
reports, followed by prompt-driven extraction using Llama 3.3 and DeepSeek R1,
resulting in over 5.6 million timestamped clinical events. To assess timeline
quality, we evaluate against a clinician-curated reference set using three
metrics: (i) event-level matching (80% match at a cosine similarity threshold
of 0.1), (ii) temporal concordance (c-index > 0.90), and (iii) Area Under the
Log-Time CDF (AULTC) for timestamp alignment. Corpus-level analysis shows wide
diagnostic and demographic coverage. In a downstream survival prediction task,
embeddings from extracted timelines achieve time-dependent concordance indices
up to 0.82 $\pm$ 0.01, demonstrating the predictive value of temporally
structured narratives. PMOA-TTS provides a scalable foundation for timeline
extraction, temporal reasoning, and longitudinal modeling in biomedical NLP.
The dataset is available at: https://huggingface.co/datasets/snoroozi/pmoa-tts .

</details>


### [9] [Guided by Gut: Efficient Test-Time Scaling with Reinforced Intrinsic Confidence](https://arxiv.org/pdf/2505.20325)
*Amirhosein Ghasemabadi, Keith G. Mills, Baochun Li, Di Niu*

Main category: cs.CL

TL;DR: Guided by Gut (GG) is a self-guided Test-Time Scaling framework that enhances LLM reasoning efficiently without costly external models, using intrinsic signals and lightweight tree search.


<details>
  <summary>Details</summary>
Motivation: To reduce computational costs of TTS methods like PRMs or BoN sampling while maintaining performance.

Method: Uses intrinsic LLM signals (token-level confidence, step novelty) and lightweight tree search, with reinforcement learning to improve confidence estimates.

Result: GG enables smaller models to match or surpass larger models' accuracy, reduces GPU memory by 10x, speeds inference 8x, and cuts KV cache memory by 50%.

Conclusion: GG offers a practical, efficient alternative to PRM-based TTS methods, enhancing deployment feasibility.

Abstract: Test-Time Scaling (TTS) methods for enhancing Large Language Model (LLM)
reasoning often incur substantial computational costs, primarily due to
extensive reliance on external Process Reward Models (PRMs) or sampling methods
like Best-of-N (BoN). This paper introduces Guided by Gut (GG), an efficient
self-guided TTS framework that achieves PRM-level performance without costly
external verifier models. Our method employs a lightweight tree search guided
solely by intrinsic LLM signals, token-level confidence and step novelty. One
critical innovation is improving the reliability of internal confidence
estimates via a targeted reinforcement learning fine-tuning phase. Empirical
evaluations on challenging mathematical reasoning benchmarks demonstrate that
GG enables smaller models (e.g., 1.5B parameters) to achieve accuracy matching
or surpassing significantly larger models (e.g., 32B-70B parameters), while
reducing GPU memory usage by up to 10x. Compared to PRM-based methods, GG
achieves comparable accuracy with 8x faster inference speeds and 4-5x lower
memory usage. Additionally, GG reduces KV cache memory usage by approximately
50% compared to the BoN strategy, facilitating more efficient and practical
deployment of TTS techniques.

</details>


### [10] [Multi-Scale Manifold Alignment: A Unified Framework for Enhanced Explainability of Large Language Models](https://arxiv.org/pdf/2505.20333)
*Yukun Zhang, Qi Dong*

Main category: cs.CL

TL;DR: A Multi-Scale Manifold Alignment framework is proposed to enhance interpretability of LLMs by decomposing latent space into semantic manifolds and enforcing alignment with geometric and information constraints.


<details>
  <summary>Details</summary>
Motivation: LLMs lack interpretability in their reasoning, limiting trust in critical applications. The framework aims to address this by providing a structured understanding of multi-scale semantics.

Method: The framework decomposes latent space into global, intermediate, and local manifolds, using cross-scale mapping functions with geometric alignment (Procrustes analysis) and information preservation (mutual information constraints). It includes curvature regularization and hyperparameter tuning.

Result: Theoretical analysis bounds alignment error (KL divergence) under mild assumptions. The framework improves interpretability and enables applications like bias detection and robustness enhancement.

Conclusion: The proposed framework advances LLM interpretability by unifying multi-scale semantic structure, supporting trust and practical applications.

Abstract: Recent advances in Large Language Models (LLMs) have achieved strong
performance, yet their internal reasoning remains opaque, limiting
interpretability and trust in critical applications. We propose a novel
Multi_Scale Manifold Alignment framework that decomposes the latent space into
global, intermediate, and local semantic manifolds capturing themes, context,
and word-level details. Our method introduces cross_scale mapping functions
that jointly enforce geometric alignment (e.g., Procrustes analysis) and
information preservation (via mutual information constraints like MINE or VIB).
We further incorporate curvature regularization and hyperparameter tuning for
stable optimization. Theoretical analysis shows that alignment error, measured
by KL divergence, can be bounded under mild assumptions. This framework offers
a unified explanation of how LLMs structure multi-scale semantics, advancing
interpretability and enabling applications such as bias detection and
robustness enhancement.

</details>


### [11] [Lookahead Q-Cache: Achieving More Consistent KV Cache Eviction via Pseudo Query](https://arxiv.org/pdf/2505.20334)
*Yixuan Wang, Shiyu Ji, Yijun Liu, Yuzhuang Xu, Yang Xu, Qingfu Zhu, Wanxiang Che*

Main category: cs.CL

TL;DR: LAQ improves KV cache eviction by using pseudo lookahead queries for better alignment with real inference, outperforming existing methods.


<details>
  <summary>Details</summary>
Motivation: KV cache memory usage grows with longer sequences, and existing eviction methods are inconsistent with actual inference queries.

Method: Proposes Lookahead Q-Cache (LAQ), using low-cost pseudo lookahead queries to estimate token importance more accurately.

Result: LAQ achieves 1-4 point improvement on LongBench under limited cache budgets and works well with other methods.

Conclusion: LAQ offers a flexible, effective solution for KV cache eviction, enhancing efficiency in LLM deployment.

Abstract: Large language models (LLMs) rely on key-value cache (KV cache) to accelerate
decoding by reducing redundant computations. However, the KV cache memory usage
grows substantially with longer text sequences, posing challenges for efficient
deployment. Existing KV cache eviction methods prune tokens using
prefilling-stage attention scores, causing inconsistency with actual inference
queries, especially under tight memory budgets. In this paper, we propose
Lookahead Q-Cache (LAQ), a novel eviction framework that generates low-cost
pseudo lookahead queries to better approximate the true decoding-stage queries.
By using these lookahead queries as the observation window for importance
estimation, LAQ achieves more consistent and accurate KV cache eviction aligned
with real inference scenarios. Experimental results on LongBench and
Needle-in-a-Haystack benchmarks show that LAQ outperforms existing methods
across various budget levels, achieving a 1 $\sim$ 4 point improvement on
LongBench under limited cache budget. Moreover, LAQ is complementary to
existing approaches and can be flexibly combined to yield further improvements.

</details>


### [12] [ArVoice: A Multi-Speaker Dataset for Arabic Speech Synthesis](https://arxiv.org/pdf/2505.20506)
*Hawau Olamide Toyin, Rufael Marew, Humaid Alblooshi, Samar M. Magdy, Hanan Aldarmaki*

Main category: cs.CL

TL;DR: ArVoice is a multi-speaker Modern Standard Arabic speech corpus with diacritized transcriptions, useful for tasks like speech synthesis, diacritic restoration, and deepfake detection. It includes professionally recorded, modified, and synthetic speech totaling 83.52 hours across 11 voices.


<details>
  <summary>Details</summary>
Motivation: To provide a comprehensive Arabic speech dataset for research in multi-speaker speech synthesis and related tasks, addressing the need for diverse and high-quality resources.

Method: Combines professionally recorded human voices, a modified subset of an existing corpus, and synthetic speech from commercial systems. Three TTS and two voice conversion systems were trained to demonstrate its utility.

Result: The corpus includes 83.52 hours of speech (10 hours human voices from 7 speakers) and supports diverse applications like TTS and voice conversion.

Conclusion: ArVoice is a valuable resource for Arabic speech research, offering high-quality, diverse data for multiple applications and is available for research use.

Abstract: We introduce ArVoice, a multi-speaker Modern Standard Arabic (MSA) speech
corpus with diacritized transcriptions, intended for multi-speaker speech
synthesis, and can be useful for other tasks such as speech-based diacritic
restoration, voice conversion, and deepfake detection. ArVoice comprises: (1) a
new professionally recorded set from six voice talents with diverse
demographics, (2) a modified subset of the Arabic Speech Corpus; and (3)
high-quality synthetic speech from two commercial systems. The complete corpus
consists of a total of 83.52 hours of speech across 11 voices; around 10 hours
consist of human voices from 7 speakers. We train three open-source TTS and two
voice conversion systems to illustrate the use cases of the dataset. The corpus
is available for research use.

</details>


### [13] [Language Model Distillation: A Temporal Difference Imitation Learning Perspective](https://arxiv.org/pdf/2505.20335)
*Zishun Yu, Shangzhe Li, Xinhua Zhang*

Main category: cs.CL

TL;DR: The paper introduces a framework for temporal difference-based distillation of large language models by leveraging their distributional sparsity to improve efficiency.


<details>
  <summary>Details</summary>
Motivation: Large language models are computationally expensive, and distillation methods often mimic behavior without optimizing for efficiency. The sparsity of token probabilities in these models presents an opportunity for more efficient distillation.

Method: The authors propose a temporal difference learning framework that operates on a reduced action space (subset of vocabulary), exploiting the teacher model's distributional sparsity to derive practical algorithms.

Result: The framework demonstrates performance improvements by focusing on a smaller subset of tokens, making the distillation process more efficient.

Conclusion: The proposed temporal difference-based distillation framework offers a practical and efficient method for compressing large language models while maintaining performance.

Abstract: Large language models have led to significant progress across many NLP tasks,
although their massive sizes often incur substantial computational costs.
Distillation has become a common practice to compress these large and highly
capable models into smaller, more efficient ones. Many existing language model
distillation methods can be viewed as behavior cloning from the perspective of
imitation learning or inverse reinforcement learning. This viewpoint has
inspired subsequent studies that leverage (inverse) reinforcement learning
techniques, including variations of behavior cloning and temporal difference
learning methods. Rather than proposing yet another specific temporal
difference method, we introduce a general framework for temporal
difference-based distillation by exploiting the distributional sparsity of the
teacher model. Specifically, it is often observed that language models assign
most probability mass to a small subset of tokens. Motivated by this
observation, we design a temporal difference learning framework that operates
on a reduced action space (a subset of vocabulary), and demonstrate how
practical algorithms can be derived and the resulting performance improvements.

</details>


### [14] [MOSLIM:Align with diverse preferences in prompts through reward classification](https://arxiv.org/pdf/2505.20336)
*Yu Zhang, Wanli Jiang, Zhengyu Yang*

Main category: cs.CL

TL;DR: MOSLIM introduces a single-reward-model method for multi-objective alignment in LLMs, outperforming existing approaches with fewer resources.


<details>
  <summary>Details</summary>
Motivation: Current multi-objective alignment methods require multiple policies or reward models, which is inefficient. MOSLIM aims to simplify this process.

Method: MOSLIM uses a multi-head reward model for classification and a mapping function to derive scalar rewards, optimizing a single policy model.

Result: MOSLIM outperforms existing methods on benchmarks and uses fewer GPU resources.

Conclusion: MOSLIM offers an efficient, flexible solution for multi-objective alignment in LLMs.

Abstract: The multi-objective alignment of Large Language Models (LLMs) is essential
for ensuring foundational models conform to diverse human preferences. Current
research in this field typically involves either multiple policies or multiple
reward models customized for various preferences, or the need to train a
preference-specific supervised fine-tuning (SFT) model. In this work, we
introduce a novel multi-objective alignment method, MOSLIM, which utilizes a
single reward model and policy model to address diverse objectives. MOSLIM
provides a flexible way to control these objectives through prompting and does
not require preference training during SFT phase, allowing thousands of
off-the-shelf models to be directly utilized within this training framework.
MOSLIM leverages a multi-head reward model that classifies question-answer
pairs instead of scoring them and then optimize policy model with a scalar
reward derived from a mapping function that converts classification results
from reward model into reward scores. We demonstrate the efficacy of our
proposed method across several multi-objective benchmarks and conduct ablation
studies on various reward model sizes and policy optimization methods. The
MOSLIM method outperforms current multi-objective approaches in most results
while requiring significantly fewer GPU computing resources compared with
existing policy optimization methods.

</details>


### [15] [Phir Hera Fairy: An English Fairytaler is a Strong Faker of Fluent Speech in Low-Resource Indian Languages](https://arxiv.org/pdf/2505.20693)
*Praveen Srinivasa Varadhan, Srija Anand, Soma Siddhartha, Mitesh M. Khapra*

Main category: cs.CL

TL;DR: Fine-tuning the English F5-TTS model on Indian languages (IN-F5) achieves near-human polyglot fluency, outperforming training from scratch or mixed-data fine-tuning. It enables cross-language speech and aids low-resource TTS, including zero-resource synthesis via synthetic data.


<details>
  <summary>Details</summary>
Motivation: To evaluate how an English TTS model adapts to Indian languages and explore effective fine-tuning strategies for low-resource scenarios.

Method: Compare three approaches: (i) training from scratch, (ii) fine-tuning on Indian data only, and (iii) fine-tuning on both Indian and English data. Assess polyglot fluency, voice/style cloning, and code-mixing.

Result: Fine-tuning with Indian data alone (IN-F5) is most effective, achieving near-human polyglot fluency and enabling cross-language speech. English pretraining helps low-resource TTS reach human parity.

Conclusion: IN-F5 demonstrates the potential of fine-tuning for low-resource TTS, including zero-resource languages via synthetic data, offering a compute-optimal strategy for similar challenges.

Abstract: What happens when an English Fairytaler is fine-tuned on Indian languages? We
evaluate how the English F5-TTS model adapts to 11 Indian languages, measuring
polyglot fluency, voice-cloning, style-cloning, and code-mixing. We compare:
(i) training from scratch, (ii) fine-tuning English F5 on Indian data, and
(iii) fine-tuning on both Indian and English data to prevent forgetting.
Fine-tuning with only Indian data proves most effective and the resultant IN-F5
is a near-human polyglot; that enables speakers of one language (e.g., Odia) to
fluently speak in another (e.g., Hindi). Our results show English pretraining
aids low-resource TTS in reaching human parity. To aid progress in other
low-resource languages, we study data-constrained setups and arrive at a
compute optimal strategy. Finally, we show IN-F5 can synthesize unseen
languages like Bhojpuri and Tulu using a human-in-the-loop approach for
zero-resource TTS via synthetic data generation.

</details>


### [16] [Assessing the Capability of LLMs in Solving POSCOMP Questions](https://arxiv.org/pdf/2505.20338)
*Cayo Viegas, Rohit Gheyi, Márcio Ribeiro*

Main category: cs.CL

TL;DR: LLMs, especially ChatGPT-4, perform well on text-based questions in the POSCOMP exam, surpassing human performance, but struggle with image tasks. Newer models show further improvements.


<details>
  <summary>Details</summary>
Motivation: To evaluate LLMs' proficiency in specialized domains like computer science using the POSCOMP exam as a benchmark.

Method: Assessed four LLMs (ChatGPT-4, Gemini 1.0 Advanced, Claude 3 Sonnet, Le Chat Mistral Large) on the 2022 and 2023 POSCOMP exams, later including newer models (o1, Gemini 2.5 Pro, Claude 3.7 Sonnet, o3-mini-high) for 2022-2024 exams.

Result: ChatGPT-4 led in 2022 (57/69 correct) and 2023, surpassing human performance. Newer models consistently outperformed humans across all years.

Conclusion: LLMs excel in text-based tasks on POSCOMP, with newer models showing even better performance, though image interpretation remains a challenge.

Abstract: Recent advancements in Large Language Models (LLMs) have significantly
expanded the capabilities of artificial intelligence in natural language
processing tasks. Despite this progress, their performance in specialized
domains such as computer science remains relatively unexplored. Understanding
the proficiency of LLMs in these domains is critical for evaluating their
practical utility and guiding future developments. The POSCOMP, a prestigious
Brazilian examination used for graduate admissions in computer science promoted
by the Brazlian Computer Society (SBC), provides a challenging benchmark. This
study investigates whether LLMs can match or surpass human performance on the
POSCOMP exam. Four LLMs - ChatGPT-4, Gemini 1.0 Advanced, Claude 3 Sonnet, and
Le Chat Mistral Large - were initially evaluated on the 2022 and 2023 POSCOMP
exams. The assessments measured the models' proficiency in handling complex
questions typical of the exam. LLM performance was notably better on text-based
questions than on image interpretation tasks. In the 2022 exam, ChatGPT-4 led
with 57 correct answers out of 69 questions, followed by Gemini 1.0 Advanced
(49), Le Chat Mistral (48), and Claude 3 Sonnet (44). Similar trends were
observed in the 2023 exam. ChatGPT-4 achieved the highest performance,
surpassing all students who took the POSCOMP 2023 exam. LLMs, particularly
ChatGPT-4, show promise in text-based tasks on the POSCOMP exam, although image
interpretation remains a challenge. Given the rapid evolution of LLMs, we
expanded our analysis to include more recent models - o1, Gemini 2.5 Pro,
Claude 3.7 Sonnet, and o3-mini-high - evaluated on the 2022-2024 POSCOMP exams.
These newer models demonstrate further improvements and consistently surpass
both the average and top-performing human participants across all three years.

</details>


### [17] [Dub-S2ST: Textless Speech-to-Speech Translation for Seamless Dubbing](https://arxiv.org/pdf/2505.20899)
*Jeongsoo Choi, Jaehun Kim, Joon Son Chung*

Main category: cs.CL

TL;DR: A cross-lingual dubbing system translates speech while preserving duration, speaker identity, and speed, using a diffusion-based model and flow matching for synthesis.


<details>
  <summary>Details</summary>
Motivation: Existing speech translation methods neglect speech patterns, causing mismatches in dubbing. This work aims to align translated speech with source characteristics.

Method: A discrete diffusion-based speech-to-unit translation model with duration control, combined with a conditional flow matching model for synthesis and a unit-based speed adaptation mechanism.

Result: The system produces natural, fluent translations aligned with source duration and pace, achieving competitive translation performance.

Conclusion: The proposed framework effectively addresses the limitations of current methods, enabling high-quality cross-lingual dubbing.

Abstract: This paper introduces a cross-lingual dubbing system that translates speech
from one language to another while preserving key characteristics such as
duration, speaker identity, and speaking speed. Despite the strong translation
quality of existing speech translation approaches, they often overlook the
transfer of speech patterns, leading to mismatches with source speech and
limiting their suitability for dubbing applications. To address this, we
propose a discrete diffusion-based speech-to-unit translation model with
explicit duration control, enabling time-aligned translation. We then
synthesize speech based on the predicted units and source identity with a
conditional flow matching model. Additionally, we introduce a unit-based speed
adaptation mechanism that guides the translation model to produce speech at a
rate consistent with the source, without relying on any text. Extensive
experiments demonstrate that our framework generates natural and fluent
translations that align with the original speech's duration and speaking pace,
while achieving competitive translation performance.

</details>


### [18] [Dynamic Manifold Evolution Theory: Modeling and Stability Analysis of Latent Representations in Large Language Models](https://arxiv.org/pdf/2505.20340)
*Yukun Zhang, Qi Dong*

Main category: cs.CL

TL;DR: DMET models LLM generation as a controlled dynamical system on a low-dimensional semantic manifold, linking latent dynamics to text quality.


<details>
  <summary>Details</summary>
Motivation: To provide a unified framework for understanding and improving text generation in large language models by analyzing their latent dynamics.

Method: Uses dynamical system theory to model latent-state updates, maps energy-driven flows and context-dependent forces to Transformer components, and employs Lyapunov stability theory with empirical metrics.

Result: Validates DMET's predictions through experiments, offering guidelines for balancing creativity and consistency in text generation.

Conclusion: DMET effectively links latent-trajectory properties to text quality, providing actionable insights for improving generation.

Abstract: We introduce Dynamic Manifold Evolution Theory (DMET),a unified framework
that models large language model generation as a controlled dynamical system
evolving on a low_dimensional semantic manifold. By casting latent_state
updates as discrete time Euler approximations of continuous dynamics, we map
intrinsic energy_driven flows and context_dependent forces onto Transformer
components (residual connections, attention, feed-forward networks). Leveraging
Lyapunov stability theory We define three empirical metrics (state continuity,
clustering quality, topological persistence) that quantitatively link
latent_trajectory properties to text fluency, grammaticality, and semantic
coherence. Extensive experiments across decoding parameters validate DMET's
predictions and yield principled guidelines for balancing creativity and
consistency in text generation.

</details>


### [19] [Scaling and Prompting for Improved End-to-End Spoken Grammatical Error Correction](https://arxiv.org/pdf/2505.21137)
*Mengjie Qian, Rao Ma, Stefano Bannò, Kate M. Knill, Mark J. F. Gales*

Main category: cs.CL

TL;DR: The paper explores using end-to-end speech foundation models for Spoken Grammatical Error Correction (SGEC) and Feedback (SGECF), introducing pseudo-labelling to expand training data and prompting for performance gains.


<details>
  <summary>Details</summary>
Motivation: To improve SGEC and SGECF by leveraging end-to-end models and addressing limited labelled data.

Method: Uses pseudo-labelling to expand training data, prompts an E2E Whisper-based model with fluent transcriptions, and evaluates model size impact.

Result: Pseudo-labelling expands data to 2500 hours, improving performance. Prompting enhances SGEC and feedback generation. Larger models benefit from prompts but not pseudo-labelled data.

Conclusion: End-to-end models with pseudo-labelling and prompting improve SGEC and feedback, though model size gains depend on training methods.

Abstract: Spoken Grammatical Error Correction (SGEC) and Feedback (SGECF) are crucial
for second language learners, teachers and test takers. Traditional SGEC
systems rely on a cascaded pipeline consisting of an ASR, a module for
disfluency detection (DD) and removal and one for GEC. With the rise of
end-to-end (E2E) speech foundation models, we investigate their effectiveness
in SGEC and feedback generation. This work introduces a pseudo-labelling
process to address the challenge of limited labelled data, expanding the
training data size from 77 hours to approximately 2500 hours, leading to
improved performance. Additionally, we prompt an E2E Whisper-based SGEC model
with fluent transcriptions, showing a slight improvement in SGEC performance,
with more significant gains in feedback generation. Finally, we assess the
impact of increasing model size, revealing that while pseudo-labelled data does
not yield performance gain for a larger Whisper model, training with prompts
proves beneficial.

</details>


### [20] [Do LLMs have a Gender (Entropy) Bias?](https://arxiv.org/pdf/2505.20343)
*Sonal Prabhune, Balaji Padmanabhan, Kaushik Dutta*

Main category: cs.CL

TL;DR: The paper investigates gender bias in LLMs, introduces a new benchmark dataset (RealWorldQuestioning), and proposes a debiasing method. While no significant bias exists at the category level, finer granularity reveals disparities. A simple prompt-based debiasing approach improves response quality.


<details>
  <summary>Details</summary>
Motivation: To identify and address gender bias in LLM responses, particularly in real-world contexts like business and health, ensuring fair and informative outputs.

Method: Developed a benchmark dataset, defined entropy bias, tested four LLMs, and used ChatGPT-4o for evaluation. Proposed a debiasing strategy merging gendered responses.

Result: No significant bias at category level, but disparities at question level. The debiasing method improved response quality in 78% of cases.

Conclusion: Gender bias in LLMs is nuanced; a simple debiasing approach can enhance fairness and information content in responses.

Abstract: We investigate the existence and persistence of a specific type of gender
bias in some of the popular LLMs and contribute a new benchmark dataset,
RealWorldQuestioning (released on HuggingFace ), developed from real-world
questions across four key domains in business and health contexts: education,
jobs, personal financial management, and general health. We define and study
entropy bias, which we define as a discrepancy in the amount of information
generated by an LLM in response to real questions users have asked. We tested
this using four different LLMs and evaluated the generated responses both
qualitatively and quantitatively by using ChatGPT-4o (as "LLM-as-judge"). Our
analyses (metric-based comparisons and "LLM-as-judge" evaluation) suggest that
there is no significant bias in LLM responses for men and women at a category
level. However, at a finer granularity (the individual question level), there
are substantial differences in LLM responses for men and women in the majority
of cases, which "cancel" each other out often due to some responses being
better for males and vice versa. This is still a concern since typical users of
these tools often ask a specific question (only) as opposed to several varied
ones in each of these common yet important areas of life. We suggest a simple
debiasing approach that iteratively merges the responses for the two genders to
produce a final result. Our approach demonstrates that a simple, prompt-based
debiasing strategy can effectively debias LLM outputs, thus producing responses
with higher information content than both gendered variants in 78% of the
cases, and consistently achieving a balanced integration in the remaining
cases.

</details>


### [21] [Agentic Medical Knowledge Graphs Enhance Medical Question Answering: Bridging the Gap Between LLMs and Evolving Medical Knowledge](https://arxiv.org/pdf/2502.13010)
*Mohammad Reza Rezaei, Reza Saadati Fard, Rahul G. Krishnan, Milad Lankarany*

Main category: cs.CL

TL;DR: AMG-RAG automates medical knowledge graph updates and integrates reasoning to improve accuracy and interpretability in medical question-answering, outperforming larger models.


<details>
  <summary>Details</summary>
Motivation: Addressing the challenges of rapidly evolving medical knowledge and manual updates in LLMs for reliable medical question-answering.

Method: Introduces AMG-RAG, a framework for automated knowledge graph construction, continuous updates, and integration of external evidence like PubMed and WikiSearch.

Result: Achieves F1 score of 74.1% on MEDQA and 66.34% accuracy on MEDMCQA, outperforming larger models without added computational cost.

Conclusion: AMG-RAG demonstrates the importance of automated knowledge graphs and external evidence for reliable, up-to-date medical insights.

Abstract: Large Language Models (LLMs) have significantly advanced medical
question-answering by leveraging extensive clinical data and medical
literature. However, the rapid evolution of medical knowledge and the
labor-intensive process of manually updating domain-specific resources pose
challenges to the reliability of these systems. To address this, we introduce
Agentic Medical Graph-RAG (AMG-RAG), a comprehensive framework that automates
the construction and continuous updating of medical knowledge graphs,
integrates reasoning, and retrieves current external evidence, such as PubMed
and WikiSearch. By dynamically linking new findings and complex medical
concepts, AMG-RAG not only improves accuracy but also enhances interpretability
in medical queries.
  Evaluations on the MEDQA and MEDMCQA benchmarks demonstrate the effectiveness
of AMG-RAG, achieving an F1 score of 74.1 percent on MEDQA and an accuracy of
66.34 percent on MEDMCQA, outperforming both comparable models and those 10 to
100 times larger. Notably, these improvements are achieved without increasing
computational overhead, highlighting the critical role of automated knowledge
graph generation and external evidence retrieval in delivering up-to-date,
trustworthy medical insights.

</details>


### [22] [Leveraging LLM and Self-Supervised Training Models for Speech Recognition in Chinese Dialects: A Comparative Analysis](https://arxiv.org/pdf/2505.21138)
*Tianyi Xu, Hongjie Chen, Wang Qing, Lv Hang, Jian Kang, Li Jie, Zhennan Lin, Yongxiang Li, Xie Lei*

Main category: cs.CL

TL;DR: The paper explores self-supervised pre-training with LLMs to improve ASR for Chinese dialects, achieving SOTA results on datasets like Kespeech.


<details>
  <summary>Details</summary>
Motivation: Chinese accents and dialects are challenging for ASR due to data scarcity. The study investigates if self-supervised learning and LLMs can enhance performance in low-resource scenarios.

Method: Pre-train a Data2vec2 model on 300K hours of unlabeled dialect/accented speech, then align on 40K hours of supervised data. Evaluate impact of projectors and LLMs on recognition performance.

Result: Achieved state-of-the-art (SOTA) results on multiple dialect datasets, including Kespeech.

Conclusion: Self-supervised pre-training with LLMs effectively improves ASR for Chinese dialects, with plans to open-source the work for reproducibility.

Abstract: Large-scale training corpora have significantly improved the performance of
ASR models. Unfortunately, due to the relative scarcity of data, Chinese
accents and dialects remain a challenge for most ASR models. Recent
advancements in self-supervised learning have shown that self-supervised pre-
training, combined with large language models (LLM), can effectively enhance
ASR performance in low-resource scenarios. We aim to investigate the
effectiveness of this paradigm for Chinese dialects. Specifically, we pre-train
a Data2vec2 model on 300,000 hours of unlabeled dialect and accented speech
data and do alignment training on a supervised dataset of 40,000 hours. Then,
we systematically examine the impact of various projectors and LLMs on
Mandarin, dialect, and accented speech recognition performance under this
paradigm. Our method achieved SOTA results on multiple dialect datasets,
including Kespeech. We will open-source our work to promote reproducible
research

</details>


### [23] [SeRL: Self-Play Reinforcement Learning for Large Language Models with Limited Data](https://arxiv.org/pdf/2505.20347)
*Wenkai Fang, Shunyu Liu, Yang Zhou, Kongcheng Zhang, Tongya Zheng, Kaixuan Chen, Mingli Song, Dacheng Tao*

Main category: cs.CL

TL;DR: SeRL uses self-play reinforcement learning to train LLMs with limited data, combining self-instruction and self-rewarding modules to generate high-quality training data without external annotations.


<details>
  <summary>Details</summary>
Motivation: Existing RL methods for LLMs rely on high-quality instructions and verifiable rewards, which are hard to obtain in specialized domains.

Method: SeRL includes self-instruction (generates and filters instructions) and self-rewarding (uses majority voting for reward estimation), followed by conventional RL.

Result: SeRL outperforms counterparts and matches performance of methods using high-quality, verifiable rewards.

Conclusion: SeRL is effective for training LLMs with limited data, eliminating dependency on external annotations.

Abstract: Recent advances have demonstrated the effectiveness of Reinforcement Learning
(RL) in improving the reasoning capabilities of Large Language Models (LLMs).
However, existing works inevitably rely on high-quality instructions and
verifiable rewards for effective training, both of which are often difficult to
obtain in specialized domains. In this paper, we propose Self-play
Reinforcement Learning(SeRL) to bootstrap LLM training with limited initial
data. Specifically, SeRL comprises two complementary modules: self-instruction
and self-rewarding. The former module generates additional instructions based
on the available data at each training step, employing robust online filtering
strategies to ensure instruction quality, diversity, and difficulty. The latter
module introduces a simple yet effective majority-voting mechanism to estimate
response rewards for additional instructions, eliminating the need for external
annotations. Finally, SeRL performs conventional RL based on the generated
data, facilitating iterative self-play learning. Extensive experiments on
various reasoning benchmarks and across different LLM backbones demonstrate
that the proposed SeRL yields results superior to its counterparts and achieves
performance on par with those obtained by high-quality data with verifiable
rewards. Our code is available at https://github.com/wantbook-book/SeRL.

</details>


### [24] [Assessment of L2 Oral Proficiency using Speech Large Language Models](https://arxiv.org/pdf/2505.21148)
*Rao Ma, Mengjie Qian, Siyuan Tang, Stefano Bannò, Kate M. Knill, Mark J. F. Gales*

Main category: cs.CL

TL;DR: The paper explores using multi-modal LLMs for grading L2 English spoken proficiency, outperforming previous methods and showing strong generalization.


<details>
  <summary>Details</summary>
Motivation: The demand for automated spoken language assessment (SLA) is rising, but existing methods (statistical models, text encoders, speech models) have limitations like information loss or performance issues.

Method: The study compares training strategies (regression and classification) using multi-modal LLMs to grade L2 oral proficiency.

Result: Speech LLMs outperform previous baselines on two datasets and demonstrate strong generalization in cross-part/task evaluations.

Conclusion: Multi-modal LLMs are effective for SLA, leveraging pre-training knowledge for superior performance and generalization.

Abstract: The growing population of L2 English speakers has increased the demand for
developing automatic graders for spoken language assessment (SLA).
Historically, statistical models, text encoders, and self-supervised speech
models have been utilised for this task. However, cascaded systems suffer from
the loss of information, while E2E graders also have limitations. With the
recent advancements of multi-modal large language models (LLMs), we aim to
explore their potential as L2 oral proficiency graders and overcome these
issues. In this work, we compare various training strategies using regression
and classification targets. Our results show that speech LLMs outperform all
previous competitive baselines, achieving superior performance on two datasets.
Furthermore, the trained grader demonstrates strong generalisation capabilities
in the cross-part or cross-task evaluation, facilitated by the audio
understanding knowledge acquired during LLM pre-training.

</details>


### [25] [Rethinking Text-based Protein Understanding: Retrieval or LLM?](https://arxiv.org/pdf/2505.20354)
*Juntong Wu, Zijing Liu, He Cao, Hao Li, Bin Feng, Zishan Shu, Ke Yu, Li Yuan, Yu Li*

Main category: cs.CL

TL;DR: The paper addresses data leakage and evaluation issues in protein-text models, proposing a retrieval-enhanced method that outperforms fine-tuned LLMs for protein-to-text generation.


<details>
  <summary>Details</summary>
Motivation: Current benchmarks for protein-text models suffer from data leakage and inadequate evaluation metrics, limiting accurate performance assessment.

Method: The authors reorganize datasets and introduce a biological entity-based evaluation framework. They propose a retrieval-enhanced method for protein-to-text generation.

Result: The retrieval-enhanced method outperforms fine-tuned LLMs, showing accuracy and efficiency, especially in training-free scenarios.

Conclusion: The proposed framework and method address critical limitations in protein-text modeling, offering improved evaluation and generation capabilities.

Abstract: In recent years, protein-text models have gained significant attention for
their potential in protein generation and understanding. Current approaches
focus on integrating protein-related knowledge into large language models
through continued pretraining and multi-modal alignment, enabling simultaneous
comprehension of textual descriptions and protein sequences. Through a thorough
analysis of existing model architectures and text-based protein understanding
benchmarks, we identify significant data leakage issues present in current
benchmarks. Moreover, conventional metrics derived from natural language
processing fail to accurately assess the model's performance in this domain. To
address these limitations, we reorganize existing datasets and introduce a
novel evaluation framework based on biological entities. Motivated by our
observation, we propose a retrieval-enhanced method, which significantly
outperforms fine-tuned LLMs for protein-to-text generation and shows accuracy
and efficiency in training-free scenarios. Our code and data can be seen at
https://github.com/IDEA-XL/RAPM.

</details>


### [26] [Enhancing Logical Reasoning in Language Models via Symbolically-Guided Monte Carlo Process Supervision](https://arxiv.org/pdf/2505.20415)
*Xingwei Tan, Marco Valentino, Mahmud Akhter, Maria Liakata, Nikolaos Aletras*

Main category: cs.CL

TL;DR: The paper addresses LLMs' reliance on memorization over generalization in reasoning tasks by introducing a method to generate and select high-quality symbolic reasoning trajectories, improving logical reasoning and out-of-domain generalization.


<details>
  <summary>Details</summary>
Motivation: LLMs often rely on memorization rather than robust symbolic reasoning, limiting their generalization. Combining LLMs with symbolic methods has been challenging due to verification issues.

Method: The proposed method generates symbolic reasoning trajectories, selects high-quality ones using a process reward model tuned via Monte Carlo estimation, and employs them for fine-tuning.

Result: Experiments on benchmarks like FOLIO and LogicAsker show significant improvements in reasoning. Fine-tuning also enhances out-of-domain generalization.

Conclusion: Symbolically-guided process supervision can mitigate memorization effects in LLMs, improving reasoning and generalization.

Abstract: Large language models (LLMs) have shown promising performance in mathematical
and logical reasoning benchmarks. However, recent studies have pointed to
memorization, rather than generalization, as one of the leading causes for such
performance. LLMs, in fact, are susceptible to content variations,
demonstrating a lack of robust symbolic abstractions supporting their reasoning
process. To improve reliability, many attempts have been made to combine LLMs
with symbolic methods. Nevertheless, existing approaches fail to effectively
leverage symbolic representations due to the challenges involved in developing
reliable and scalable verification mechanisms. In this paper, we propose to
overcome such limitations by generating symbolic reasoning trajectories and
select the high-quality ones using a process reward model automatically tuned
based on Monte Carlo estimation. The trajectories are then employed via
fine-tuning methods to improve logical reasoning and generalization. Our
results on logical reasoning benchmarks such as FOLIO and LogicAsker show the
effectiveness of the proposed method with large gains on frontier and
open-weight models. Moreover, additional experiments on claim verification
reveal that fine-tuning on the generated symbolic reasoning trajectories
enhances out-of-domain generalizability, suggesting the potential impact of
symbolically-guided process supervision in alleviating the effect of
memorization on LLM reasoning.

</details>


### [27] [GraphGen: Enhancing Supervised Fine-Tuning for LLMs with Knowledge-Driven Synthetic Data Generation](https://arxiv.org/pdf/2505.20416)
*Zihong Chen, Wanli Jiang, Jinzhe Li, Zhonghang Yuan, Huanjun Kong, Wanli Ouyang, Nanqing Dong*

Main category: cs.CL

TL;DR: GraphGen is a knowledge graph-guided framework for generating high-quality synthetic QA data to address data scarcity in LLM fine-tuning, outperforming conventional methods.


<details>
  <summary>Details</summary>
Motivation: High-quality supervised data for LLM fine-tuning is costly and labor-intensive, and existing synthetic data methods often lack accuracy, coverage, and diversity.

Method: GraphGen constructs a knowledge graph, identifies knowledge gaps, prioritizes high-value QA pairs, uses multi-hop sampling, and employs style-controlled generation.

Result: GraphGen outperforms conventional synthetic data methods in knowledge-intensive QA tasks under closed-book settings.

Conclusion: GraphGen provides a reliable and comprehensive solution to data scarcity in supervised fine-tuning, with publicly available code and data.

Abstract: Fine-tuning for large language models (LLMs) typically requires substantial
amounts of high-quality supervised data, which is both costly and
labor-intensive to acquire. While synthetic data generation has emerged as a
promising solution, existing approaches frequently suffer from factual
inaccuracies, insufficient long-tail coverage, simplistic knowledge structures,
and homogenized outputs. To address these challenges, we introduce GraphGen, a
knowledge graph-guided framework designed for three key question-answering (QA)
scenarios: atomic QA, aggregated QA, and multi-hop QA. It begins by
constructing a fine-grained knowledge graph from the source text. It then
identifies knowledge gaps in LLMs using the expected calibration error metric,
prioritizing the generation of QA pairs that target high-value, long-tail
knowledge. Furthermore, GraphGen incorporates multi-hop neighborhood sampling
to capture complex relational information and employs style-controlled
generation to diversify the resulting QA data. Experimental results on
knowledge-intensive tasks under closed-book settings demonstrate that GraphGen
outperforms conventional synthetic data methods, offering a more reliable and
comprehensive solution to the data scarcity challenge in supervised
fine-tuning. The code and data are publicly available at
https://github.com/open-sciencelab/GraphGen.

</details>


### [28] [SEMMA: A Semantic Aware Knowledge Graph Foundation Model](https://arxiv.org/pdf/2505.20422)
*Arvindh Arun, Sumit Kumar, Mojtaba Nayyeri, Bo Xiong, Ponnurangam Kumaraguru, Antonio Vergari, Steffen Staab*

Main category: cs.CL

TL;DR: SEMMA, a dual-module KGFM, integrates textual semantics with graph structure, outperforming structural baselines in inductive link prediction and generalization tasks.


<details>
  <summary>Details</summary>
Motivation: Existing KGFMs overlook textual attributes, limiting their generalization. SEMMA aims to unify structural and semantic signals for better reasoning.

Method: SEMMA uses LLMs to enrich relation identifiers, creating semantic embeddings and a textual relation graph fused with structural components.

Result: SEMMA outperforms structural baselines like ULTRA, especially in challenging generalization settings with unseen relations.

Conclusion: Textual semantics are crucial for generalization where structure fails, advocating for foundation models combining both signals.

Abstract: Knowledge Graph Foundation Models (KGFMs) have shown promise in enabling
zero-shot reasoning over unseen graphs by learning transferable patterns.
However, most existing KGFMs rely solely on graph structure, overlooking the
rich semantic signals encoded in textual attributes. We introduce SEMMA, a
dual-module KGFM that systematically integrates transferable textual semantics
alongside structure. SEMMA leverages Large Language Models (LLMs) to enrich
relation identifiers, generating semantic embeddings that subsequently form a
textual relation graph, which is fused with the structural component. Across 54
diverse KGs, SEMMA outperforms purely structural baselines like ULTRA in fully
inductive link prediction. Crucially, we show that in more challenging
generalization settings, where the test-time relation vocabulary is entirely
unseen, structural methods collapse while SEMMA is 2x more effective. Our
findings demonstrate that textual semantics are critical for generalization in
settings where structure alone fails, highlighting the need for foundation
models that unify structural and linguistic signals in knowledge reasoning.

</details>


### [29] [The UD-NewsCrawl Treebank: Reflections and Challenges from a Large-scale Tagalog Syntactic Annotation Project](https://arxiv.org/pdf/2505.20428)
*Angelina A. Aquino, Lester James V. Miranda, Elsie Marie T. Or*

Main category: cs.CL

TL;DR: UD-NewsCrawl is the largest Tagalog treebank (15.6k trees) with manual annotation under Universal Dependencies. It details development, evaluates parsers, and addresses Tagalog's syntactic challenges.


<details>
  <summary>Details</summary>
Motivation: To advance computational linguistics for underrepresented languages like Tagalog by providing a high-quality, manually annotated treebank.

Method: Data collection, pre-processing, manual annotation, and quality assurance. Baseline evaluations using transformer-based models.

Result: UD-NewsCrawl is created, and baseline evaluations assess parser performance on Tagalog. Challenges in syntactic analysis are highlighted.

Conclusion: UD-NewsCrawl and baseline models will support research in underrepresented languages, addressing Tagalog's unique grammatical properties.

Abstract: This paper presents UD-NewsCrawl, the largest Tagalog treebank to date,
containing 15.6k trees manually annotated according to the Universal
Dependencies framework. We detail our treebank development process, including
data collection, pre-processing, manual annotation, and quality assurance
procedures. We provide baseline evaluations using multiple transformer-based
models to assess the performance of state-of-the-art dependency parsers on
Tagalog. We also highlight challenges in the syntactic analysis of Tagalog
given its distinctive grammatical properties, and discuss its implications for
the annotation of this treebank. We anticipate that UD-NewsCrawl and our
baseline model implementations will serve as valuable resources for advancing
computational linguistics research in underrepresented languages like Tagalog.

</details>


### [30] [PreP-OCR: A Complete Pipeline for Document Image Restoration and Enhanced OCR Accuracy](https://arxiv.org/pdf/2505.20429)
*Shuhao Guan, Moule Lin, Cheng Xu, Xinyi Liu, Jinman Zhao, Jiexin Fan, Qi Xu, Derek Greene*

Main category: cs.CL

TL;DR: PreP-OCR is a two-stage pipeline combining image restoration and semantic-aware post-OCR correction to improve text extraction from degraded historical documents, reducing error rates by 63.9-70.3%.


<details>
  <summary>Details</summary>
Motivation: To enhance text extraction from degraded historical documents by jointly optimizing image clarity and linguistic consistency.

Method: 1. Synthetic image pairs with randomized fonts, layouts, and degradations train an image restoration model. 2. A ByT5 post-corrector fine-tuned on synthetic historical text addresses OCR errors.

Result: PreP-OCR reduces character error rates by 63.9-70.3% on 13,831 pages of historical documents in English, French, and Spanish.

Conclusion: Integrating image restoration with linguistic correction shows promise for digitizing historical archives.

Abstract: This paper introduces PreP-OCR, a two-stage pipeline that combines document
image restoration with semantic-aware post-OCR correction to improve text
extraction from degraded historical documents. Our key innovation lies in
jointly optimizing image clarity and linguistic consistency. First, we generate
synthetic image pairs with randomized text fonts, layouts, and degradations. An
image restoration model is trained on this synthetic data, using
multi-directional patch extraction and fusion to process large images. Second,
a ByT5 post-corrector, fine-tuned on synthetic historical text training pairs,
addresses any remaining OCR errors. Detailed experiments on 13,831 pages of
real historical documents in English, French, and Spanish show that PreP-OCR
pipeline reduces character error rates by 63.9-70.3\% compared to OCR on raw
images. Our pipeline demonstrates the potential of integrating image
restoration with linguistic error correction for digitizing historical
archives.

</details>


### [31] [HAMburger: Accelerating LLM Inference via Token Smashing](https://arxiv.org/pdf/2505.20438)
*Jingyu Liu, Ce Zhang*

Main category: cs.CL

TL;DR: HAMburger introduces a hierarchical auto-regressive model for LLM inference, reducing KV cache computation and improving speed while maintaining quality.


<details>
  <summary>Details</summary>
Motivation: Current LLM inference is sub-optimal due to uniform computation per token. HAMburger leverages LLMs' self-identification capability to optimize resource allocation.

Method: HAMburger uses a compositional embedder and micro-step decoder to generate multiple tokens per step, reducing KV cache and FLOPs growth.

Result: Achieves up to 2× KV cache reduction and 2× TPS improvement, maintaining quality in short- and long-context tasks.

Conclusion: HAMburger explores a hardware-agnostic, efficient inference regime, balancing computation and memory efficiency.

Abstract: The growing demand for efficient Large Language Model (LLM) inference
requires a holistic optimization on algorithms, systems, and hardware. However,
very few works have fundamentally changed the generation pattern: each token
needs one forward pass and one KV cache. This can be sub-optimal because we
found that LLMs are extremely capable of self-identifying the exact dose of
information that a single KV cache can store, and many tokens can be generated
confidently without global context. Based on this insight, we introduce
HAMburger, a Hierarchically Auto-regressive Model that redefines resource
allocation in LLMs by moving beyond uniform computation and storage per token
during inference. Stacking a compositional embedder and a micro-step decoder in
between a base LLM, HAMburger smashes multiple tokens into a single KV and
generates several tokens per step. Additionally, HAMburger functions as a
speculative decoding framework where it can blindly trust self-drafted tokens.
As a result, HAMburger shifts the growth of KV cache and forward FLOPs from
linear to sub-linear with respect to output length, and adjusts its inference
speed based on query perplexity and output structure. Extensive evaluations
show that HAMburger reduces the KV cache computation by up to 2$\times$ and
achieves up to 2$\times$ TPS, while maintaining quality in both short- and
long-context tasks. Our method explores an extremely challenging inference
regime that requires both computation- and memory-efficiency with a
hardware-agnostic design.

</details>


### [32] [In-context Language Learning for Endangered Languages in Speech Recognition](https://arxiv.org/pdf/2505.20445)
*Zhaolin Li, Jan Niehues*

Main category: cs.CL

TL;DR: LLMs can learn low-resource languages via in-context learning, improving language modeling and ASR tasks, even surpassing dedicated models.


<details>
  <summary>Details</summary>
Motivation: To explore if LLMs can learn unseen, low-resource languages without supervised data, extending prior research to speech recognition.

Method: Experiments on four endangered languages using in-context learning (ICL), comparing probability-based and instruction-based approaches.

Result: More relevant text samples improve performance; probability-based ICL outperforms instruction-based. LLMs match or surpass dedicated ASR models.

Conclusion: ICL enables LLMs to learn new languages effectively, maintaining original capabilities while achieving competitive ASR performance.

Abstract: With approximately 7,000 languages spoken worldwide, current large language
models (LLMs) support only a small subset. Prior research indicates LLMs can
learn new languages for certain tasks without supervised data. We extend this
investigation to speech recognition, investigating whether LLMs can learn
unseen, low-resource languages through in-context learning (ICL). With
experiments on four diverse endangered languages that LLMs have not been
trained on, we find that providing more relevant text samples enhances
performance in both language modelling and Automatic Speech Recognition (ASR)
tasks. Furthermore, we show that the probability-based approach outperforms the
traditional instruction-based approach in language learning. Lastly, we show
ICL enables LLMs to achieve ASR performance that is comparable to or even
surpasses dedicated language models trained specifically for these languages,
while preserving the original capabilities of the LLMs.

</details>


### [33] [Autoregressive Speech Synthesis without Vector Quantization](https://arxiv.org/pdf/2407.08551)
*Lingwei Meng, Long Zhou, Shujie Liu, Sanyuan Chen, Bing Han, Shujie Hu, Yanqing Liu, Jinyu Li, Sheng Zhao, Xixin Wu, Helen Meng, Furu Wei*

Main category: cs.CL

TL;DR: MELLE is a continuous-valued token-based language model for TTS, bypassing vector quantization for better fidelity and robustness.


<details>
  <summary>Details</summary>
Motivation: To improve text-to-speech synthesis by avoiding vector quantization's fidelity loss and enhancing output diversity.

Method: Uses regression loss with spectrogram flux and variational inference for continuous mel-spectrogram generation.

Result: Outperforms VALL-E in robustness and metrics, offering a simpler single-stage approach.

Conclusion: MELLE provides a superior, streamlined TTS paradigm with higher fidelity and diversity.

Abstract: We present MELLE, a novel continuous-valued token based language modeling
approach for text-to-speech synthesis (TTS). MELLE autoregressively generates
continuous mel-spectrogram frames directly from text condition, bypassing the
need for vector quantization, which is typically designed for audio compression
and sacrifices fidelity compared to continuous representations. Specifically,
(i) instead of cross-entropy loss, we apply regression loss with a proposed
spectrogram flux loss function to model the probability distribution of the
continuous-valued tokens; (ii) we have incorporated variational inference into
MELLE to facilitate sampling mechanisms, thereby enhancing the output diversity
and model robustness. Experiments demonstrate that, compared to the two-stage
codec language model VALL-E and its variants, the single-stage MELLE mitigates
robustness issues by avoiding the inherent flaws of sampling vector-quantized
codes, achieves superior performance across multiple metrics, and, most
importantly, offers a more streamlined paradigm. The demos of our work are
provided at https://aka.ms/melle.

</details>


### [34] [Amulet: Putting Complex Multi-Turn Conversations on the Stand with LLM Juries](https://arxiv.org/pdf/2505.20451)
*Sahana Ramnath, Anurag Mudgil, Brihi Joshi, Skyler Hallinan, Xiang Ren*

Main category: cs.CL

TL;DR: Amulet improves LLM-judges' accuracy in evaluating multi-turn conversations by analyzing dialog acts and maxims, showing significant human intent changes and response differentiation.


<details>
  <summary>Details</summary>
Motivation: Benchmarking and improving LLM-judges for real-world, diverse, and lengthy human-assistant conversations.

Method: Amulet framework uses dialog acts and conversational maxims to analyze and judge preference data in multi-turn conversations.

Result: Humans change intents frequently (60-70%), and 75% of responses can be differentiated using dialog acts/maxims. Amulet outperforms baselines.

Conclusion: Amulet enhances LLM-judges' performance, proving the value of dialog acts and maxims in evaluating complex conversational data.

Abstract: Today, large language models are widely used as judges to evaluate responses
from other language models. Hence, it is imperative to benchmark and improve
these LLM-judges on real-world language model usage: a typical human-assistant
conversation is lengthy, and shows significant diversity in topics, intents,
and requirements across turns, e.g. social interactions, task requests,
feedback. We present Amulet, a framework that leverages pertinent linguistic
concepts of dialog-acts and maxims to improve the accuracy of LLM-judges on
preference data with complex, multi-turn conversational context. Amulet
presents valuable insights about (a) the communicative structures and intents
present in the conversation (dialog acts), and (b) the satisfaction of
conversational principles (maxims) by the preference responses, and uses them
to make judgments. On four challenging datasets, Amulet shows that (a) humans
frequently (60 to 70 percent of the time) change their intents from one turn of
the conversation to the next, and (b) in 75 percent of instances, the
preference responses can be differentiated via dialog acts and/or maxims,
reiterating the latter's significance in judging such data. Amulet can be used
either as a judge by applying the framework to a single LLM, or integrated into
a jury with different LLM judges; our judges and juries show strong
improvements on relevant baselines for all four datasets.

</details>


### [35] [Sentiment Reasoning for Healthcare](https://arxiv.org/pdf/2407.21054)
*Khai-Nguyen Nguyen, Khai Le-Duc, Bach Phan Tat, Duy Le, Long Vo-Dang, Truong-Son Hy*

Main category: cs.CL

TL;DR: The paper introduces Sentiment Reasoning, a task for explaining sentiment predictions in AI healthcare, using a multimodal multitask framework and a large dataset. It improves transparency and performance.


<details>
  <summary>Details</summary>
Motivation: Enhancing transparency in AI healthcare decision-making by providing rationales for predictions.

Method: Proposes a multimodal multitask framework for Sentiment Reasoning, predicting labels and generating rationales. Uses a large dataset across five languages.

Result: Improves model transparency and classification performance (+2% accuracy/macro-F1). No significant difference in rationale quality between human and ASR transcripts.

Conclusion: Sentiment Reasoning enhances AI transparency and performance, with broad applicability across languages and modalities.

Abstract: Transparency in AI healthcare decision-making is crucial. By incorporating
rationales to explain reason for each predicted label, users could understand
Large Language Models (LLMs)'s reasoning to make better decision. In this work,
we introduce a new task - Sentiment Reasoning - for both speech and text
modalities, and our proposed multimodal multitask framework and the world's
largest multimodal sentiment analysis dataset. Sentiment Reasoning is an
auxiliary task in sentiment analysis where the model predicts both the
sentiment label and generates the rationale behind it based on the input
transcript. Our study conducted on both human transcripts and Automatic Speech
Recognition (ASR) transcripts shows that Sentiment Reasoning helps improve
model transparency by providing rationale for model prediction with quality
semantically comparable to humans while also improving model's classification
performance (+2% increase in both accuracy and macro-F1) via
rationale-augmented fine-tuning. Also, no significant difference in the
semantic quality of generated rationales between human and ASR transcripts. All
code, data (five languages - Vietnamese, English, Chinese, German, and French)
and models are published online:
https://github.com/leduckhai/Sentiment-Reasoning

</details>


### [36] [Align-SLM: Textless Spoken Language Models with Reinforcement Learning from AI Feedback](https://arxiv.org/pdf/2411.01834)
*Guan-Ting Lin, Prashanth Gurunath Shivakumar, Aditya Gourav, Yile Gu, Ankur Gandhe, Hung-yi Lee, Ivan Bulyko*

Main category: cs.CL

TL;DR: The paper introduces Align-SLM, a framework using preference optimization (inspired by RLAIF) to enhance semantic understanding in textless Spoken Language Models (SLMs), achieving state-of-the-art results.


<details>
  <summary>Details</summary>
Motivation: Textless SLMs lag behind text-based LLMs in semantic coherence and relevance, prompting the need for improved semantic understanding.

Method: Align-SLM generates multiple speech continuations, uses semantic metrics to create preference data, and applies Direct Preference Optimization (DPO).

Result: The method achieves state-of-the-art performance on benchmarks like ZeroSpeech 2021 and StoryCloze, with improvements in semantic coherence.

Conclusion: Preference optimization is crucial for enhancing SLM semantics, as demonstrated by Align-SLM's superior performance.

Abstract: While textless Spoken Language Models (SLMs) have shown potential in
end-to-end speech-to-speech modeling, they still lag behind text-based Large
Language Models (LLMs) in terms of semantic coherence and relevance. This work
introduces the Align-SLM framework, which leverages preference optimization
inspired by Reinforcement Learning with AI Feedback (RLAIF) to enhance the
semantic understanding of SLMs. Our approach generates multiple speech
continuations from a given prompt and uses semantic metrics to create
preference data for Direct Preference Optimization (DPO). We evaluate the
framework using ZeroSpeech 2021 benchmarks for lexical and syntactic modeling,
the spoken version of the StoryCloze dataset for semantic coherence, and other
speech generation metrics, including the GPT4-o score and human evaluation.
Experimental results show that our method achieves state-of-the-art performance
for SLMs on most benchmarks, highlighting the importance of preference
optimization to improve the semantics of SLMs.

</details>


### [37] [Conversation Kernels: A Flexible Mechanism to Learn Relevant Context for Online Conversation Understanding](https://arxiv.org/pdf/2505.20482)
*Vibhor Agarwal, Arjoo Gupta, Suparna De, Nishanth Sastry*

Main category: cs.CL

TL;DR: The paper proposes a method called Conversation Kernels to analyze online conversations by capturing context and dependencies between posts, tested on Slashdot data.


<details>
  <summary>Details</summary>
Motivation: Understanding online conversations is challenging due to short, context-dependent posts, requiring methods to encode conversational context.

Method: Two families of Conversation Kernels are designed to explore post neighborhoods in conversation trees, building relevant context for tasks like labeling posts.

Result: The method is applied to Slashdot conversations, testing its adaptability to diverse labeling tasks (e.g., 'insightful', 'funny').

Conclusion: Conversation Kernels provide a flexible, general-purpose framework for understanding and labeling online conversations.

Abstract: Understanding online conversations has attracted research attention with the
growth of social networks and online discussion forums. Content analysis of
posts and replies in online conversations is difficult because each individual
utterance is usually short and may implicitly refer to other posts within the
same conversation. Thus, understanding individual posts requires capturing the
conversational context and dependencies between different parts of a
conversation tree and then encoding the context dependencies between posts and
comments/replies into the language model.
  To this end, we propose a general-purpose mechanism to discover appropriate
conversational context for various aspects about an online post in a
conversation, such as whether it is informative, insightful, interesting or
funny. Specifically, we design two families of Conversation Kernels, which
explore different parts of the neighborhood of a post in the tree representing
the conversation and through this, build relevant conversational context that
is appropriate for each task being considered. We apply our developed method to
conversations crawled from slashdot.org, which allows users to apply highly
different labels to posts, such as 'insightful', 'funny', etc., and therefore
provides an ideal experimental platform to study whether a framework such as
Conversation Kernels is general-purpose and flexible enough to be adapted to
disparately different conversation understanding tasks.

</details>


### [38] [Rethinking MUSHRA: Addressing Modern Challenges in Text-to-Speech Evaluation](https://arxiv.org/pdf/2411.12719)
*Praveen Srinivasa Varadhan, Amogh Gulati, Ashwin Sankar, Srija Anand, Anirudh Gupta, Anirudh Mukherjee, Shiva Kumar Marepally, Ankur Bhatia, Saloni Jaju, Suvrat Bhooshan, Mitesh M. Khapra*

Main category: cs.CL

TL;DR: The paper critiques the MUSHRA test for TTS evaluation, highlighting its bias toward human references and ambiguity. It proposes refined variants for fairer and clearer assessments and introduces MANGO, a large dataset of human ratings for Indian languages.


<details>
  <summary>Details</summary>
Motivation: Current TTS evaluation methods like MOS and CMOS are either inconsistent or time-consuming, while MUSHRA is biased against modern TTS systems that outperform human speech.

Method: The study assesses MUSHRA's sensitivity to rater variability, listener fatigue, and reference bias through extensive human evaluations (492 listeners) in Hindi and Tamil. Two refined MUSHRA variants are proposed to address identified issues.

Result: Identified shortcomings include reference-matching bias and judgement ambiguity. The proposed variants improve fairness (for superior TTS samples) and reduce rater variance.

Conclusion: The refined MUSHRA variants and MANGO dataset enhance TTS evaluation reliability and granularity, particularly for Indian languages.

Abstract: Despite rapid advancements in TTS models, a consistent and robust human
evaluation framework is still lacking. For example, MOS tests fail to
differentiate between similar models, and CMOS's pairwise comparisons are
time-intensive. The MUSHRA test is a promising alternative for evaluating
multiple TTS systems simultaneously, but in this work we show that its reliance
on matching human reference speech unduly penalises the scores of modern TTS
systems that can exceed human speech quality. More specifically, we conduct a
comprehensive assessment of the MUSHRA test, focusing on its sensitivity to
factors such as rater variability, listener fatigue, and reference bias. Based
on our extensive evaluation involving 492 human listeners across Hindi and
Tamil we identify two primary shortcomings: (i) reference-matching bias, where
raters are unduly influenced by the human reference, and (ii) judgement
ambiguity, arising from a lack of clear fine-grained guidelines. To address
these issues, we propose two refined variants of the MUSHRA test. The first
variant enables fairer ratings for synthesized samples that surpass human
reference quality. The second variant reduces ambiguity, as indicated by the
relatively lower variance across raters. By combining these approaches, we
achieve both more reliable and more fine-grained assessments. We also release
MANGO, a massive dataset of 246,000 human ratings, the first-of-its-kind
collection for Indian languages, aiding in analyzing human preferences and
developing automatic metrics for evaluating TTS systems.

</details>


### [39] [VoxEval: Benchmarking the Knowledge Understanding Capabilities of End-to-End Spoken Language Models](https://arxiv.org/pdf/2501.04962)
*Wenqian Cui, Xiaoqi Jiao, Ziqiao Meng, Irwin King*

Main category: cs.CL

TL;DR: VoxEval is a new SpeechQA benchmark designed to evaluate Spoken Language Models (SLMs) by testing knowledge understanding through pure speech interactions, including robustness to audio conditions and complex tasks like math reasoning.


<details>
  <summary>Details</summary>
Motivation: Existing QA benchmarks lack support for end-to-end speech evaluation and varied audio conditions, limiting SLM assessment.

Method: VoxEval maintains speech format for inputs/outputs, tests robustness across audio conditions, and assesses complex tasks like math reasoning.

Result: VoxEval challenges current SLMs, exposing sensitivity to audio conditions and gaps in reasoning capabilities.

Conclusion: VoxEval aims to guide the development of more sophisticated and reliable SLMs.

Abstract: With the rising need for speech-based interaction models, end-to-end Spoken
Language Models (SLMs) have emerged as a promising solution. While these models
require comprehensive world knowledge for meaningful and reliable human
interactions, existing question-answering (QA) benchmarks fall short in
evaluating SLMs' knowledge understanding due to their inability to support
end-to-end speech evaluation and account for varied input audio conditions. To
address these limitations, we present VoxEval, a novel SpeechQA benchmark that
assesses SLMs' knowledge understanding through pure speech interactions. Our
benchmark 1) uniquely maintains speech format for both inputs and outputs, 2)
evaluates model robustness across diverse input audio conditions, and 3)
pioneers the assessment of complex tasks like mathematical reasoning in spoken
format. Systematic evaluation demonstrates that VoxEval presents significant
challenges to current SLMs, revealing their sensitivity to varying audio
conditions and highlighting the need to enhance reasoning capabilities in
future development. We hope this benchmark could guide the advancement of more
sophisticated and reliable SLMs. VoxEval dataset is available at:
https://github.com/dreamtheater123/VoxEval

</details>


### [40] [InFact: Informativeness Alignment for Improved LLM Factuality](https://arxiv.org/pdf/2505.20487)
*Roi Cohen, Russa Biswas, Gerard de Melo*

Main category: cs.CL

TL;DR: The paper addresses the issue of factual completeness in LLMs, proposing an informativeness alignment mechanism to improve both informativeness and factuality.


<details>
  <summary>Details</summary>
Motivation: LLMs often generate factually correct but less informative text, and the paper aims to enhance both correctness and informativeness.

Method: An informativeness alignment mechanism is introduced, leveraging factual benchmarks to prioritize correct and informative answers.

Result: Training models with this mechanism improves both informativeness and factuality.

Conclusion: The proposed alignment mechanism effectively enhances the quality of factual text generated by LLMs.

Abstract: Factual completeness is a general term that captures how detailed and
informative a factually correct text is. For instance, the factual sentence
``Barack Obama was born in the United States'' is factually correct, though
less informative than the factual sentence ``Barack Obama was born in Honolulu,
Hawaii, United States''. Despite the known fact that LLMs tend to hallucinate
and generate factually incorrect text, they might also tend to choose to
generate factual text that is indeed factually correct and yet less informative
than other, more informative choices. In this work, we tackle this problem by
proposing an informativeness alignment mechanism. This mechanism takes
advantage of recent factual benchmarks to propose an informativeness alignment
objective. This objective prioritizes answers that are both correct and
informative. A key finding of our work is that when training a model to
maximize this objective or optimize its preference, we can improve not just
informativeness but also factuality.

</details>


### [41] [Towards Inclusive ASR: Investigating Voice Conversion for Dysarthric Speech Recognition in Low-Resource Languages](https://arxiv.org/pdf/2505.14874)
*Chin-Jou Li, Eunjung Yeo, Kwanghee Choi, Paula Andrea Pérez-Toro, Masao Someki, Rohan Kumar Das, Zhengjun Yue, Juan Rafael Orozco-Arroyave, Elmar Nöth, David R. Mortensen*

Main category: cs.CL

TL;DR: Fine-tuning a voice conversion model on English dysarthric speech to generate non-English dysarthric-like speech improves multilingual ASR performance for dysarthric speech.


<details>
  <summary>Details</summary>
Motivation: Addressing data scarcity for dysarthric speech in non-English languages by leveraging English dysarthric data.

Method: Fine-tune a voice conversion model on English dysarthric speech (UASpeech), apply it to convert healthy non-English speech (FLEURS) into dysarthric-like speech, and use this to fine-tune a multilingual ASR model (MMS).

Result: VC with speaker and prosody conversion outperforms off-the-shelf MMS and conventional augmentation techniques, validated on Spanish, Italian, and Tamil datasets.

Conclusion: Generated dysarthric-like speech effectively improves ASR performance for dysarthric speech in non-English languages.

Abstract: Automatic speech recognition (ASR) for dysarthric speech remains challenging
due to data scarcity, particularly in non-English languages. To address this,
we fine-tune a voice conversion model on English dysarthric speech (UASpeech)
to encode both speaker characteristics and prosodic distortions, then apply it
to convert healthy non-English speech (FLEURS) into non-English dysarthric-like
speech. The generated data is then used to fine-tune a multilingual ASR model,
Massively Multilingual Speech (MMS), for improved dysarthric speech
recognition. Evaluation on PC-GITA (Spanish), EasyCall (Italian), and SSNCE
(Tamil) demonstrates that VC with both speaker and prosody conversion
significantly outperforms the off-the-shelf MMS performance and conventional
augmentation techniques such as speed and tempo perturbation. Objective and
subjective analyses of the generated data further confirm that the generated
speech simulates dysarthric characteristics.

</details>


### [42] [Inceptive Transformers: Enhancing Contextual Representations through Multi-Scale Feature Learning Across Domains and Languages](https://arxiv.org/pdf/2505.20496)
*Asif Shahriar, Rifat Shahriyar, M Saifur Rahman*

Main category: cs.CL

TL;DR: The paper introduces Inceptive Transformer, a modular architecture enhancing token representations by integrating multi-scale feature extraction, outperforming baselines by 1-14% across diverse tasks.


<details>
  <summary>Details</summary>
Motivation: Addressing information loss in conventional transformers due to compressing all tokens into a single [CLS] token, especially for tasks needing localized or hierarchical cues.

Method: Proposes Inceptive Transformer with a multi-scale feature extraction module, dynamically weighting tokens to balance local and global dependencies.

Result: Outperforms baselines by 1-14% in tasks like emotion recognition, irony detection, disease identification, and anti-COVID vaccine tweets classification.

Conclusion: Demonstrates versatility and cross-lingual applicability of the method for enriching transformer-based representations across domains.

Abstract: Conventional transformer models typically compress the information from all
tokens in a sequence into a single \texttt{[CLS]} token to represent global
context-- an approach that can lead to information loss in tasks requiring
localized or hierarchical cues. In this work, we introduce \textit{Inceptive
Transformer}, a modular and lightweight architecture that enriches
transformer-based token representations by integrating a multi-scale feature
extraction module inspired by inception networks. Our model is designed to
balance local and global dependencies by dynamically weighting tokens based on
their relevance to a particular task. Evaluation across a diverse range of
tasks including emotion recognition (both English and Bangla), irony detection,
disease identification, and anti-COVID vaccine tweets classification shows that
our models consistently outperform the baselines by 1\% to 14\% while
maintaining efficiency. These findings highlight the versatility and
cross-lingual applicability of our method for enriching transformer-based
representations across diverse domains.

</details>


### [43] [Beyond Keywords: Evaluating Large Language Model Classification of Nuanced Ableism](https://arxiv.org/pdf/2505.20500)
*Naba Rizvi, Harper Strickland, Saleha Ahmedi, Aekta Kallepalli, Isha Khirwadkar, William Wu, Imani N. S. Munyaka, Nedjma Ousidhoum*

Main category: cs.CL

TL;DR: LLMs can identify autism-related language but often miss harmful connotations, relying on surface-level keyword matching unlike humans who consider context and impact.


<details>
  <summary>Details</summary>
Motivation: To evaluate LLMs' ability to detect nuanced ableism, particularly against autistic individuals, and compare their performance with human annotators.

Method: Assessed four LLMs for identifying ableism in text, comparing their understanding of terminology and effectiveness in context. Also conducted qualitative comparison of human and LLM explanations.

Result: LLMs often miss harmful connotations, relying on keyword matching, while humans consider context and impact. Both agree on binary classification adequacy.

Conclusion: LLMs need improvement in contextual understanding for detecting ableism, though binary classification aligns with human annotation.

Abstract: Large language models (LLMs) are increasingly used in decision-making tasks
like r\'esum\'e screening and content moderation, giving them the power to
amplify or suppress certain perspectives. While previous research has
identified disability-related biases in LLMs, little is known about how they
conceptualize ableism or detect it in text. We evaluate the ability of four
LLMs to identify nuanced ableism directed at autistic individuals. We examine
the gap between their understanding of relevant terminology and their
effectiveness in recognizing ableist content in context. Our results reveal
that LLMs can identify autism-related language but often miss harmful or
offensive connotations. Further, we conduct a qualitative comparison of human
and LLM explanations. We find that LLMs tend to rely on surface-level keyword
matching, leading to context misinterpretations, in contrast to human
annotators who consider context, speaker identity, and potential impact. On the
other hand, both LLMs and humans agree on the annotation scheme, suggesting
that a binary classification is adequate for evaluating LLM performance, which
is consistent with findings from prior studies involving human annotators.

</details>


### [44] [Gatsby Without the 'E': Crafting Lipograms with LLMs](https://arxiv.org/pdf/2505.20501)
*Rohan Balasubramanian, Nitish Gokulakrishnan, Syeda Jannatus Saba, Steven Skiena*

Main category: cs.CL

TL;DR: Modern LLMs transform 'The Great Gatsby' into an 'e'-less lipogram, showing minimal meaning loss with constraints up to 3.6% of common letters.


<details>
  <summary>Details</summary>
Motivation: Explore the adaptability of English under strict lipogram constraints using modern LLMs.

Method: Tested techniques from synonym replacement to generative models with beam search and named entity analysis.

Result: Excluding up to 3.6% of common letters minimally affects meaning; fidelity decays with stricter constraints.

Conclusion: Highlights the flexibility and creativity of language under extreme constraints.

Abstract: Lipograms are a unique form of constrained writing where all occurrences of a
particular letter are excluded from the text, typified by the novel Gadsby,
which daringly avoids all usage of the letter 'e'. In this study, we explore
the power of modern large language models (LLMs) by transforming the novel F.
Scott Fitzgerald's The Great Gatsby into a fully 'e'-less text. We experimented
with a range of techniques, from baseline methods like synonym replacement to
sophisticated generative models enhanced with beam search and named entity
analysis. We show that excluding up to 3.6% of the most common letters (up to
the letter 'u') had minimal impact on the text's meaning, although translation
fidelity rapidly and predictably decays with stronger lipogram constraints. Our
work highlights the surprising flexibility of English under strict constraints,
revealing just how adaptable and creative language can be.

</details>


### [45] [Large Language Models for IT Automation Tasks: Are We There Yet?](https://arxiv.org/pdf/2505.20505)
*Md Mahadi Hassan, John Salvador, Akond Rahman, Santu Karmaker*

Main category: cs.CL

TL;DR: ITAB benchmark evaluates LLMs for Ansible automation tasks, revealing low success rates (≤12% pass@10) due to state reconciliation and module-specific errors.


<details>
  <summary>Details</summary>
Motivation: Assess LLMs' effectiveness in IT automation (e.g., Ansible), as existing benchmarks lack real-world task relevance.

Method: Created ITAB with 126 diverse tasks, evaluating 14 open-source LLMs via dynamic execution in controlled environments.

Result: Low performance (≤12% pass@10), with 44.87% errors from state reconciliation and 24.37% from module-specific knowledge gaps.

Conclusion: LLMs struggle with state reasoning and domain-specific execution, needing major advances for reliable IT automation.

Abstract: LLMs show promise in code generation, yet their effectiveness for IT
automation tasks, particularly for tools like Ansible, remains understudied.
Existing benchmarks rely primarily on synthetic tasks that fail to capture the
needs of practitioners who use IT automation tools, such as Ansible. We present
ITAB (IT Automation Task Benchmark), a benchmark of 126 diverse tasks (e.g.,
configuring servers, managing files) where each task accounts for state
reconciliation: a property unique to IT automation tools. ITAB evaluates LLMs'
ability to generate functional Ansible automation scripts via dynamic execution
in controlled environments. We evaluate 14 open-source LLMs, none of which
accomplish pass@10 at a rate beyond 12%. To explain these low scores, we
analyze 1,411 execution failures across the evaluated LLMs and identify two
main categories of prevalent semantic errors: failures in state reconciliation
related reasoning (44.87% combined from variable (11.43%), host (11.84%),
path(11.63%), and template (9.97%) issues) and deficiencies in module-specific
execution knowledge (24.37% combined from Attribute and parameter (14.44%) and
module (9.93%) errors). Our findings reveal key limitations in open-source
LLMs' ability to track state changes and apply specialized module knowledge,
indicating that reliable IT automation will require major advances in state
reasoning and domain-specific execution understanding.

</details>


### [46] [Multimodal Emotion Recognition in Conversations: A Survey of Methods, Trends, Challenges and Prospects](https://arxiv.org/pdf/2505.20511)
*Chengyan Wu, Yiqiang Cai, Yang Liu, Pengxu Zhu, Yun Xue, Ziwei Gong, Julia Hirschberg, Bolei Ma*

Main category: cs.CL

TL;DR: A survey on Multimodal Emotion Recognition in Conversations (MERC) highlighting its importance, methods, challenges, and future directions.


<details>
  <summary>Details</summary>
Motivation: Real-world dialogue systems require nuanced emotional understanding beyond single-modality approaches, driving the need for MERC.

Method: The survey systematically reviews MERC, covering motivations, tasks, methods (e.g., integrating text, speech, visual signals), and evaluation strategies.

Result: Identifies recent trends, key challenges, and provides guidance for advancing MERC research.

Conclusion: MERC is crucial for emotionally intelligent systems, and this survey offers timely insights for future research.

Abstract: While text-based emotion recognition methods have achieved notable success,
real-world dialogue systems often demand a more nuanced emotional understanding
than any single modality can offer. Multimodal Emotion Recognition in
Conversations (MERC) has thus emerged as a crucial direction for enhancing the
naturalness and emotional understanding of human-computer interaction. Its goal
is to accurately recognize emotions by integrating information from various
modalities such as text, speech, and visual signals.
  This survey offers a systematic overview of MERC, including its motivations,
core tasks, representative methods, and evaluation strategies. We further
examine recent trends, highlight key challenges, and outline future directions.
As interest in emotionally intelligent systems grows, this survey provides
timely guidance for advancing MERC research.

</details>


### [47] [AstroVisBench: A Code Benchmark for Scientific Computing and Visualization in Astronomy](https://arxiv.org/pdf/2505.20538)
*Sebastian Antony Joseph, Syed Murtaza Husain, Stella S. R. Offner, Stéphanie Juneau, Paul Torrey, Adam S. Bolton, Juan P. Farias, Niall Gaffney, Greg Durrett, Junyi Jessy Li*

Main category: cs.CL

TL;DR: AstroVisBench is introduced as the first benchmark for evaluating LLMs in astronomy, focusing on data processing, analysis, and visualization, revealing gaps in their utility for scientific research.


<details>
  <summary>Details</summary>
Motivation: To assess LLMs' ability to generate correct scientific insights through data processing and visualization, a capability not previously evaluated.

Method: Developed AstroVisBench, a benchmark for astronomy workflows, and used an LLM-as-a-judge approach validated by professional astronomers.

Result: State-of-the-art LLMs show significant limitations in assisting astronomy research, highlighting the need for improvement.

Conclusion: AstroVisBench provides a foundation for advancing LLM applications in visualization-based scientific workflows across various domains.

Abstract: Large Language Models (LLMs) are being explored for applications in
scientific research, including their capabilities to synthesize literature,
answer research questions, generate research ideas, and even conduct
computational experiments. Ultimately, our goal is for these to help scientists
derive novel scientific insights. In many areas of science, such insights often
arise from processing and visualizing data to understand its patterns. However,
evaluating whether an LLM-mediated scientific workflow produces outputs
conveying the correct scientific insights is challenging to evaluate and has
not been addressed in past work. We introduce AstroVisBench, the first
benchmark for both scientific computing and visualization in the astronomy
domain. AstroVisBench judges a language model's ability to both (1) create
astronomy-specific workflows to process and analyze data and (2) visualize the
results of these workflows through complex plots. Our evaluation of
visualizations uses a novel LLM-as-a-judge workflow, which is validated against
annotation by five professional astronomers. Using AstroVisBench we present an
evaluation of state-of-the-art language models, showing a significant gap in
their ability to engage in astronomy research as useful assistants. This
evaluation provides a strong end-to-end evaluation for AI scientists that
offers a path forward for the development of visualization-based workflows,
which are central to a broad range of domains from physics to biology.

</details>


### [48] [Paths Not Taken: Understanding and Mending the Multilingual Factual Recall Pipeline](https://arxiv.org/pdf/2505.20546)
*Meng Lu, Ruochen Zhang, Ellie Pavlick, Carsten Eickhoff*

Main category: cs.CL

TL;DR: The paper investigates why multilingual LLMs perform worse in factual recall tasks for non-English languages, attributing it to an English-centric recall mechanism and translation errors. It proposes language-agnostic interventions to improve accuracy.


<details>
  <summary>Details</summary>
Motivation: To understand and address the factual inconsistency across languages in multilingual LLMs, which perform better in English than other languages.

Method: Mechanistic analysis techniques to uncover the LLM's pipeline, identifying errors in English-centric recall and translation. Introduces two vector interventions to improve factual consistency.

Result: Interventions increased recall accuracy by over 35% for the lowest-performing language.

Conclusion: Mechanistic insights can enhance multilingual capabilities in LLMs, improving factual consistency across languages.

Abstract: Multilingual large language models (LLMs) often exhibit factual
inconsistencies across languages, with significantly better performance in
factual recall tasks in English than in other languages. The causes of these
failures, however, remain poorly understood. Using mechanistic analysis
techniques, we uncover the underlying pipeline that LLMs employ, which involves
using the English-centric factual recall mechanism to process multilingual
queries and then translating English answers back into the target language. We
identify two primary sources of error: insufficient engagement of the reliable
English-centric mechanism for factual recall, and incorrect translation from
English back into the target language for the final answer. To address these
vulnerabilities, we introduce two vector interventions, both independent of
languages and datasets, to redirect the model toward better internal paths for
higher factual consistency. Our interventions combined increase the recall
accuracy by over 35 percent for the lowest-performing language. Our findings
demonstrate how mechanistic insights can be used to unlock latent multilingual
capabilities in LLMs.

</details>


### [49] [The NaijaVoices Dataset: Cultivating Large-Scale, High-Quality, Culturally-Rich Speech Data for African Languages](https://arxiv.org/pdf/2505.20564)
*Chris Emezue, The NaijaVoices Community, Busayo Awobade, Abraham Owodunni, Handel Emezue, Gloria Monica Tobechukwu Emezue, Nefertiti Nneoma Emezue, Sewade Ogun, Bunmi Akinremi, David Ifeoluwa Adelani, Chris Pal*

Main category: cs.CL

TL;DR: The NaijaVoices dataset addresses the lack of large, diverse speech datasets for African languages (Igbo, Hausa, Yoruba), offering 1,800 hours of speech-text data from 5,000+ speakers. It significantly improves speech recognition performance.


<details>
  <summary>Details</summary>
Motivation: African languages are underrepresented in speech technologies, limiting accessibility for billions. Existing datasets lack scale and diversity.

Method: Introduces NaijaVoices, a large-scale dataset, detailing its collection approach and acoustic diversity.

Result: Finetuning experiments show WER improvements: 75.86% (Whisper), 52.06% (MMS), and 42.33% (XLSR).

Conclusion: NaijaVoices has the potential to advance multilingual speech processing for African languages.

Abstract: The development of high-performing, robust, and reliable speech technologies
depends on large, high-quality datasets. However, African languages --
including our focus, Igbo, Hausa, and Yoruba -- remain under-represented due to
insufficient data. Popular voice-enabled technologies do not support any of the
2000+ African languages, limiting accessibility for circa one billion people.
While previous dataset efforts exist for the target languages, they lack the
scale and diversity needed for robust speech models. To bridge this gap, we
introduce the NaijaVoices dataset, a 1,800-hour speech-text dataset with 5,000+
speakers. We outline our unique data collection approach, analyze its acoustic
diversity, and demonstrate its impact through finetuning experiments on
automatic speech recognition, averagely achieving 75.86% (Whisper), 52.06%
(MMS), and 42.33% (XLSR) WER improvements. These results highlight NaijaVoices'
potential to advance multilingual speech processing for African languages.

</details>


### [50] [Emotion Classification In-Context in Spanish](https://arxiv.org/pdf/2505.20571)
*Bipul Thapa, Gabriel Cofre*

Main category: cs.CL

TL;DR: The paper proposes a hybrid NLP and ML approach (TF-IDF + BERT embeddings with a Custom Stacking Ensemble) to classify Spanish customer feedback into positive, neutral, and negative emotions, achieving 93.3% accuracy.


<details>
  <summary>Details</summary>
Motivation: Traditional methods translating feedback lose semantic integrity; the study aims to preserve Spanish language nuances for better emotion classification.

Method: Combines TF-IDF and BERT embeddings, using a Custom Stacking Ensemble (CSE) with Logistic Regression, KNN, Bagging classifier (LGBM), and AdaBoost as base models, and one-vs-all Logistic Regression as meta-model.

Result: CSE outperforms individual models and BERT, achieving 93.3% test accuracy on native Spanish data, higher than translated versions.

Conclusion: Hybrid techniques (TF-IDF + BERT) improve emotion classification in Spanish, offering businesses better tools for customer feedback analysis.

Abstract: Classifying customer feedback into distinct emotion categories is essential
for understanding sentiment and improving customer experience. In this paper,
we classify customer feedback in Spanish into three emotion
categories--positive, neutral, and negative--using advanced NLP and ML
techniques. Traditional methods translate feedback from widely spoken languages
to less common ones, resulting in a loss of semantic integrity and contextual
nuances inherent to the original language. To address this limitation, we
propose a hybrid approach that combines TF-IDF with BERT embeddings,
effectively transforming Spanish text into rich numerical representations that
preserve the semantic depth of the original language by using a Custom Stacking
Ensemble (CSE) approach. To evaluate emotion classification, we utilize a range
of models, including Logistic Regression, KNN, Bagging classifier with LGBM,
and AdaBoost. The CSE model combines these classifiers as base models and uses
a one-vs-all Logistic Regression as the meta-model. Our experimental results
demonstrate that CSE significantly outperforms the individual and BERT model,
achieving a test accuracy of 93.3% on the native Spanish dataset--higher than
the accuracy obtained from the translated version. These findings underscore
the challenges of emotion classification in Spanish and highlight the
advantages of combining vectorization techniques like TF-IDF with BERT for
improved accuracy. Our results provide valuable insights for businesses seeking
to leverage emotion classification to enhance customer feedback analysis and
service improvements.

</details>


### [51] [Effectiveness of Prompt Optimization in NL2SQL Systems](https://arxiv.org/pdf/2505.20591)
*Sairam Gurajada, Eser Kandogan, Sajjadur Rahman*

Main category: cs.CL

TL;DR: The paper proposes a prompt optimization framework for NL2SQL systems to achieve high-precision and high-performance SQL generation by selecting a static set of exemplars, addressing the limitations of retrieval-based methods.


<details>
  <summary>Details</summary>
Motivation: Current NL2SQL approaches focus on high-quality SQL generation but neglect the need for high-precision and high-performance systems in production scenarios.

Method: The paper introduces a prompt optimization framework that selects a static set of exemplars capturing query log intricacies, database specifics, SQL constructs, and execution latencies, using multi-objective optimization.

Result: Preliminary empirical analysis shows the framework's effectiveness in improving NL2SQL system performance and precision.

Conclusion: The proposed framework addresses the challenges of exemplar selection in production settings, optimizing both precision and performance of NL2SQL systems.

Abstract: NL2SQL approaches have greatly benefited from the impressive capabilities of
large language models (LLMs). In particular, bootstrapping an NL2SQL system for
a specific domain can be as simple as instructing an LLM with sufficient
contextual information, such as schema details and translation demonstrations.
However, building an accurate system still requires the rigorous task of
selecting the right context for each query-including identifying relevant
schema elements, cell values, and suitable exemplars that help the LLM
understand domain-specific nuances. Retrieval-based methods have become the
go-to approach for identifying such context. While effective, these methods
introduce additional inference-time costs due to the retrieval process.
  In this paper, we argue that production scenarios demand high-precision,
high-performance NL2SQL systems, rather than simply high-quality SQL
generation, which is the focus of most current NL2SQL approaches. In such
scenarios, the careful selection of a static set of exemplars-capturing the
intricacies of the query log, target database, SQL constructs, and execution
latencies-plays a more crucial role than exemplar selection based solely on
similarity. The key challenge, however, lies in identifying a representative
set of exemplars for a given production setting. To this end, we propose a
prompt optimization framework that not only addresses the high-precision
requirement but also optimizes the performance of the generated SQL through
multi-objective optimization. Preliminary empirical analysis demonstrates the
effectiveness of the proposed framework.

</details>


### [52] [REAL-Prover: Retrieval Augmented Lean Prover for Mathematical Reasoning](https://arxiv.org/pdf/2505.20613)
*Ziju Shen, Naohao Huang, Fanyi Yang, Yutong Wang, Guoxiong Gao, Tianyi Xu, Jiedong Jiang, Wanyi He, Pu Yang, Mengzhou Sun, Haocheng Ju, Peihao Wu, Bryan Dai, Bin Dong*

Main category: cs.CL

TL;DR: REAL-Prover, a new theorem prover for Lean 4, advances college-level math problem-solving using a fine-tuned LLM and retrieval system, achieving competitive results on ProofNet and SOTA on FATE-M.


<details>
  <summary>Details</summary>
Motivation: Existing theorem provers excel at high-school math but struggle with advanced topics. REAL-Prover aims to bridge this gap.

Method: Combines a fine-tuned LLM (REAL-Prover-v1) with a retrieval system (Leansearch-PS), trained using HERALD-AF for data extraction and Jixia-interactive for data synthesis.

Result: Achieves 23.7% success on ProofNet and 56.7% on FATE-M, outperforming SOTA in algebra.

Conclusion: REAL-Prover demonstrates significant progress in automating advanced math proofs, with potential for broader applications.

Abstract: Nowadays, formal theorem provers have made monumental progress on high-school
and competition-level mathematics, but few of them generalize to more advanced
mathematics. In this paper, we present REAL-Prover, a new open-source stepwise
theorem prover for Lean 4 to push this boundary. This prover, based on our
fine-tuned large language model (REAL-Prover-v1) and integrated with a
retrieval system (Leansearch-PS), notably boosts performance on solving
college-level mathematics problems. To train REAL-Prover-v1, we developed
HERALD-AF, a data extraction pipeline that converts natural language math
problems into formal statements, and a new open-source Lean 4 interactive
environment (Jixia-interactive) to facilitate synthesis data collection. In our
experiments, our prover using only supervised fine-tune achieves competitive
results with a 23.7% success rate (Pass@64) on the ProofNet dataset-comparable
to state-of-the-art (SOTA) models. To further evaluate our approach, we
introduce FATE-M, a new benchmark focused on algebraic problems, where our
prover achieves a SOTA success rate of 56.7% (Pass@64).

</details>


### [53] [SeqPO-SiMT: Sequential Policy Optimization for Simultaneous Machine Translation](https://arxiv.org/pdf/2505.20622)
*Ting Xu, Zhichao Huang, Jiankai Sun, Shanbo Cheng, Wai Lam*

Main category: cs.CL

TL;DR: SeqPO-SiMT is a new policy optimization framework for simultaneous machine translation, improving quality and reducing latency by treating it as a sequential decision problem.


<details>
  <summary>Details</summary>
Motivation: To enhance simultaneous machine translation (SiMT) by addressing its multi-step nature, unlike single-step RLHF methods like PPO and DPO.

Method: SeqPO-SiMT treats SiMT as a sequential decision problem, using a tailored reward to refine translation quality and latency.

Result: Outperforms SFT models in translation quality (1.13 COMET points higher) and latency (6.17 lower Average Lagging) on En-Zh tasks.

Conclusion: SeqPO-SiMT rivals offline translation performance of high-performing LLMs, even with limited context.

Abstract: We present Sequential Policy Optimization for Simultaneous Machine
Translation (SeqPO-SiMT), a new policy optimization framework that defines the
simultaneous machine translation (SiMT) task as a sequential decision making
problem, incorporating a tailored reward to enhance translation quality while
reducing latency. In contrast to popular Reinforcement Learning from Human
Feedback (RLHF) methods, such as PPO and DPO, which are typically applied in
single-step tasks, SeqPO-SiMT effectively tackles the multi-step SiMT task.
This intuitive framework allows the SiMT LLMs to simulate and refine the SiMT
process using a tailored reward. We conduct experiments on six datasets from
diverse domains for En to Zh and Zh to En SiMT tasks, demonstrating that
SeqPO-SiMT consistently achieves significantly higher translation quality with
lower latency. In particular, SeqPO-SiMT outperforms the supervised fine-tuning
(SFT) model by 1.13 points in COMET, while reducing the Average Lagging by 6.17
in the NEWSTEST2021 En to Zh dataset. While SiMT operates with far less context
than offline translation, the SiMT results of SeqPO-SiMT on 7B LLM surprisingly
rival the offline translation of high-performing LLMs, including
Qwen-2.5-7B-Instruct and LLaMA-3-8B-Instruct.

</details>


### [54] [POLAR: A Benchmark for Multilingual, Multicultural, and Multi-Event Online Polarization](https://arxiv.org/pdf/2505.20624)
*Usman Naseem, Juan Ren, Saba Anwar, Sarah Kohail, Rudy Alexandro Garrido Veliz, Robert Geislinger, Aisha Jabr, Idris Abdulmumin, Laiba Qureshi, Aarushi Ajay Borkar, Maryam Ibrahim Mukhtar, Abinew Ali Ayele, Ibrahim Said Ahmad, Adem Ali, Martin Semmann, Shamsuddeen Hassan Muhammad, Seid Muhie Yimam*

Main category: cs.CL

TL;DR: POLAR is a multilingual, multicultural dataset for studying online polarization, annotated along three axes. Experiments with multilingual models and LLMs show challenges in predicting polarization types and manifestations.


<details>
  <summary>Details</summary>
Motivation: Address the lack of multilingual and culturally diverse datasets for studying online polarization, which is crucial for democratic discourse.

Method: Introduce POLAR dataset with 23k instances in seven languages, annotated for polarization. Fine-tune multilingual models and evaluate LLMs in few-shot/zero-shot scenarios.

Result: Models perform well on binary polarization detection but struggle with predicting types and manifestations, highlighting contextual complexity.

Conclusion: Polarization is highly contextual, requiring adaptable NLP approaches. POLAR dataset will support global research and mitigation efforts.

Abstract: Online polarization poses a growing challenge for democratic discourse, yet
most computational social science research remains monolingual, culturally
narrow, or event-specific. We introduce POLAR, a multilingual, multicultural,
and multievent dataset with over 23k instances in seven languages from diverse
online platforms and real-world events. Polarization is annotated along three
axes: presence, type, and manifestation, using a variety of annotation
platforms adapted to each cultural context. We conduct two main experiments:
(1) we fine-tune six multilingual pretrained language models in both
monolingual and cross-lingual setups; and (2) we evaluate a range of open and
closed large language models (LLMs) in few-shot and zero-shot scenarios.
Results show that while most models perform well on binary polarization
detection, they achieve substantially lower scores when predicting polarization
types and manifestations. These findings highlight the complex, highly
contextual nature of polarization and the need for robust, adaptable approaches
in NLP and computational social science. All resources will be released to
support further research and effective mitigation of digital polarization
globally.

</details>


### [55] [Long Context Scaling: Divide and Conquer via Multi-Agent Question-driven Collaboration](https://arxiv.org/pdf/2505.20625)
*Sibo Xiao, Zixin Lin, Wenyang Gao, Yue Zhang*

Main category: cs.CL

TL;DR: XpandA is a multi-agent framework for robust long-context processing, addressing latency, information loss, and dependency disruption through dynamic partitioning, question-guided updates, and selective replay.


<details>
  <summary>Details</summary>
Motivation: Existing agent-based methods for long-context processing suffer from high latency, information loss, and disrupted textual dependencies.

Method: XpandA uses dynamic partitioning, question-driven workflow, and selective replay to adaptively process long texts and maintain inter-agent knowledge.

Result: XpandA improves long-context capabilities by 20% and achieves 1.5x inference speedup over baselines, handling sequences up to 1M in length.

Conclusion: XpandA effectively enhances LLMs' long-context processing, outperforming existing methods in performance and efficiency.

Abstract: Processing long contexts has become a critical capability for modern large
language models (LLMs). Existing works leverage agent-based divide-and-conquer
methods for processing long contexts. But these methods face crucial
limitations, including prohibitive accumulated latency and amplified
information loss from excessive agent invocations, and the disruption of
inherent textual dependencies by immoderate partitioning. In this paper, we
propose a novel multi-agent framework XpandA (Expand-Agent) coupled with
question-driven workflow and dynamic partitioning for robust long-context
processing. XpandA overcomes these limitations through: 1) dynamic partitioning
of long texts, which adaptively modulates the filling rate of context windows
for input sequences of vastly varying lengths; 2) question-guided protocol to
update flat information ensembles within centralized shared memory,
constructing consistent inter-agent knowledge across partitions; and 3)
selectively replaying specific partitions based on the state-tracking of
question-information couples to promote the resolution of inverted-order
structures across partitions (e.g., flashbacks). We perform a comprehensive
evaluation of XpandA on multiple long-context benchmarks with length varying
from 1k to 1M, demonstrating XpandA's feasibility for processing ultra-long
sequences and its significant effectiveness in enhancing the long-context
capabilities of various LLMs by achieving 20\% improvements and 1.5x inference
speedup over baselines of full-context, RAG and previous agent-based methods.

</details>


### [56] [Test-Time Learning for Large Language Models](https://arxiv.org/pdf/2505.20633)
*Jinwu Hu, Zhitian Zhang, Guohao Chen, Xutao Wen, Chao Shuai, Wei Luo, Bin Xiao, Yuanqing Li, Mingkui Tan*

Main category: cs.CL

TL;DR: The paper introduces TLM, a Test-Time Learning paradigm for LLMs, adapting them to target domains using unlabeled test data by minimizing input perplexity. It includes a sample-efficient strategy and Low-Rank Adaptation to improve performance by 20%.


<details>
  <summary>Details</summary>
Motivation: LLMs struggle with generalization to specialized domains and linguistic variations (distribution shifts). TLM aims to dynamically adapt LLMs during testing using unlabeled data.

Method: TLM minimizes input perplexity of unlabeled test data for self-supervised enhancement. It uses a sample-efficient strategy to prioritize high-perplexity samples and Low-Rank Adaptation (LoRA) for stable updates.

Result: TLM improves LLM performance by at least 20% on domain adaptation tasks, as demonstrated on the AdaptEval benchmark.

Conclusion: TLM effectively adapts LLMs to specialized domains during testing, leveraging perplexity minimization and lightweight updates, achieving significant performance gains.

Abstract: While Large Language Models (LLMs) have exhibited remarkable emergent
capabilities through extensive pre-training, they still face critical
limitations in generalizing to specialized domains and handling diverse
linguistic variations, known as distribution shifts. In this paper, we propose
a Test-Time Learning (TTL) paradigm for LLMs, namely TLM, which dynamically
adapts LLMs to target domains using only unlabeled test data during testing.
Specifically, we first provide empirical evidence and theoretical insights to
reveal that more accurate predictions from LLMs can be achieved by minimizing
the input perplexity of the unlabeled test data. Based on this insight, we
formulate the Test-Time Learning process of LLMs as input perplexity
minimization, enabling self-supervised enhancement of LLM performance.
Furthermore, we observe that high-perplexity samples tend to be more
informative for model optimization. Accordingly, we introduce a Sample
Efficient Learning Strategy that actively selects and emphasizes these
high-perplexity samples for test-time updates. Lastly, to mitigate catastrophic
forgetting and ensure adaptation stability, we adopt Low-Rank Adaptation (LoRA)
instead of full-parameter optimization, which allows lightweight model updates
while preserving more original knowledge from the model. We introduce the
AdaptEval benchmark for TTL and demonstrate through experiments that TLM
improves performance by at least 20% compared to original LLMs on domain
knowledge adaptation.

</details>


### [57] [STEER-BENCH: A Benchmark for Evaluating the Steerability of Large Language Models](https://arxiv.org/pdf/2505.20645)
*Kai Chen, Zihao He, Taiwei Shi, Kristina Lerman*

Main category: cs.CL

TL;DR: Steer-Bench is a benchmark for evaluating LLMs' ability to align outputs with diverse community norms, revealing gaps in performance compared to human experts.


<details>
  <summary>Details</summary>
Motivation: Assessing LLMs' steerability—adapting outputs to align with diverse community norms—is critical for real-world applications but under-evaluated.

Method: Steer-Bench uses 30 contrasting subreddit pairs, 10,000 instruction-response pairs, and 5,500 validated multiple-choice questions to test alignment with community norms.

Result: Human experts achieve 81% accuracy, while the best LLMs reach ~65%, with some lagging by over 15 percentage points.

Conclusion: Steer-Bench highlights significant gaps in LLMs' community-sensitive steerability, emphasizing the need for better alignment with diverse perspectives.

Abstract: Steerability, or the ability of large language models (LLMs) to adapt outputs
to align with diverse community-specific norms, perspectives, and communication
styles, is critical for real-world applications but remains under-evaluated. We
introduce Steer-Bench, a benchmark for assessing population-specific steering
using contrasting Reddit communities. Covering 30 contrasting subreddit pairs
across 19 domains, Steer-Bench includes over 10,000 instruction-response pairs
and validated 5,500 multiple-choice question with corresponding silver labels
to test alignment with diverse community norms. Our evaluation of 13 popular
LLMs using Steer-Bench reveals that while human experts achieve an accuracy of
81% with silver labels, the best-performing models reach only around 65%
accuracy depending on the domain and configuration. Some models lag behind
human-level alignment by over 15 percentage points, highlighting significant
gaps in community-sensitive steerability. Steer-Bench is a benchmark to
systematically assess how effectively LLMs understand community-specific
instructions, their resilience to adversarial steering attempts, and their
ability to accurately represent diverse cultural and ideological perspectives.

</details>


### [58] [Graph RAG for Legal Norms: A Hierarchical and Temporal Approach](https://arxiv.org/pdf/2505.00039)
*Hudson de Martim*

Main category: cs.CL

TL;DR: Graph RAG is adapted for legal norms, combining knowledge graphs and text to handle complexity and volume of legal data.


<details>
  <summary>Details</summary>
Motivation: Legal norms are complex, hierarchical, and voluminous, requiring advanced AI methods for analysis and comprehension.

Method: Adapts Graph RAG by integrating hierarchical structure, temporal evolution, and Text Units into knowledge graphs.

Result: Creates richer, interconnected legal knowledge representations.

Conclusion: Advances AI in law, improving legal research, legislative analysis, and decision support.

Abstract: This article proposes an adaptation of Graph Retrieval Augmented Generation
(Graph RAG) specifically designed for the analysis and comprehension of legal
norms, which are characterized by their predefined hierarchical structure,
extensive network of internal and external references and multiple temporal
versions. By combining structured knowledge graphs with contextually enriched
text segments, Graph RAG offers a promising solution to address the inherent
complexity and vast volume of legal data. The integration of hierarchical
structure and temporal evolution into knowledge graphs - along with the concept
of comprehensive Text Units - facilitates the construction of richer,
interconnected representations of legal knowledge. Through a detailed analysis
of Graph RAG and its application to legal norm datasets, this article aims to
advance the field of Artificial Intelligence applied to Law, creating
opportunities for more effective systems in legal research, legislative
analysis, and decision support.

</details>


### [59] [FinTagging: An LLM-ready Benchmark for Extracting and Structuring Financial Information](https://arxiv.org/pdf/2505.20650)
*Yan Wang, Yang Ren, Lingfei Qian, Xueqing Peng, Keyi Wang, Yi Han, Dongji Feng, Xiao-Yang Liu, Jimin Huang, Qianqian Xie*

Main category: cs.CL

TL;DR: FinTagging is a benchmark for evaluating LLMs in XBRL financial reporting, focusing on entity extraction (FinNI) and concept alignment (FinCL). It reveals LLMs' strengths in extraction but weaknesses in fine-grained alignment.


<details>
  <summary>Details</summary>
Motivation: To address the lack of a comprehensive benchmark for evaluating LLMs in XBRL tagging, which combines unstructured text and structured tables.

Method: Decomposes XBRL tagging into FinNI (entity extraction) and FinCL (concept alignment), testing LLMs under zero-shot settings.

Result: LLMs excel in extraction but struggle with fine-grained concept alignment, especially disambiguating similar taxonomy entries.

Conclusion: Current LLMs need improved semantic reasoning and schema-awareness for accurate XBRL tagging.

Abstract: We introduce FinTagging, the first full-scope, table-aware XBRL benchmark
designed to evaluate the structured information extraction and semantic
alignment capabilities of large language models (LLMs) in the context of
XBRL-based financial reporting. Unlike prior benchmarks that oversimplify XBRL
tagging as flat multi-class classification and focus solely on narrative text,
FinTagging decomposes the XBRL tagging problem into two subtasks: FinNI for
financial entity extraction and FinCL for taxonomy-driven concept alignment. It
requires models to jointly extract facts and align them with the full 10k+
US-GAAP taxonomy across both unstructured text and structured tables, enabling
realistic, fine-grained evaluation. We assess a diverse set of LLMs under
zero-shot settings, systematically analyzing their performance on both subtasks
and overall tagging accuracy. Our results reveal that, while LLMs demonstrate
strong generalization in information extraction, they struggle with
fine-grained concept alignment, particularly in disambiguating closely related
taxonomy entries. These findings highlight the limitations of existing LLMs in
fully automating XBRL tagging and underscore the need for improved semantic
reasoning and schema-aware modeling to meet the demands of accurate financial
disclosure. Code is available at our GitHub repository and data is at our
Hugging Face repository.

</details>


### [60] [Chinese Cyberbullying Detection: Dataset, Method, and Validation](https://arxiv.org/pdf/2505.20654)
*Yi Zhu, Xin Zou, Xindong Wu*

Main category: cs.CL

TL;DR: The paper introduces CHNCI, the first Chinese cyberbullying incident detection dataset, organized by incidents rather than speech polarity. It uses an ensemble method for pseudo-labeling and human annotation, with evaluation criteria for cyberbullying incidents.


<details>
  <summary>Details</summary>
Motivation: Existing benchmarks focus on hate speech polarity, not real-world cyberbullying incidents, which attract social attention. The study aims to address this gap.

Method: Combines three cyberbullying detection methods for pseudo-labeling, followed by human annotation. Proposes evaluation criteria for cyberbullying incidents.

Result: Constructs CHNCI with 220,676 comments in 91 incidents, serving as a benchmark for cyberbullying detection and incident prediction.

Conclusion: CHNCI is the first dataset for Chinese cyberbullying incident detection, demonstrating its potential as a benchmark.

Abstract: Existing cyberbullying detection benchmarks were organized by the polarity of
speech, such as "offensive" and "non-offensive", which were essentially hate
speech detection. However, in the real world, cyberbullying often attracted
widespread social attention through incidents. To address this problem, we
propose a novel annotation method to construct a cyberbullying dataset that
organized by incidents. The constructed CHNCI is the first Chinese
cyberbullying incident detection dataset, which consists of 220,676 comments in
91 incidents. Specifically, we first combine three cyberbullying detection
methods based on explanations generation as an ensemble method to generate the
pseudo labels, and then let human annotators judge these labels. Then we
propose the evaluation criteria for validating whether it constitutes a
cyberbullying incident. Experimental results demonstrate that the constructed
dataset can be a benchmark for the tasks of cyberbullying detection and
incident prediction. To the best of our knowledge, this is the first study for
the Chinese cyberbullying incident detection task.

</details>


### [61] [ShIOEnv: A CLI Behavior-Capturing Environment Enabling Grammar-Guided Command Synthesis for Dataset Curation](https://arxiv.org/pdf/2505.18374)
*Jarrod Ragsdale, Rajendra Boppana*

Main category: cs.CL

TL;DR: The paper introduces ShIOEnv, a Shell Input-Output Environment, to improve CLI interaction modeling by generating rich datasets with execution data, using grammar masking and PPO for efficiency.


<details>
  <summary>Details</summary>
Motivation: Existing CLI datasets lack execution data (exit codes, outputs), limiting behavioral modeling. Smaller models need richer data to match large models like GPT.

Method: ShIOEnv treats command construction as a Markov Decision Process, using grammar masking and PPO to optimize sampling. It generates datasets with execution details.

Result: Grammar masking and PPO improve dataset quality (more arguments, fewer redundancies). Fine-tuned CodeT5 shows 85% BLEU-4 improvement with grammar constraints, plus 26% with PPO.

Conclusion: ShIOEnv and its datasets enhance CLI interaction modeling, enabling smaller models to perform comparably to large ones. The tools are released for future research.

Abstract: Command-line interfaces (CLIs) provide structured textual environments for
system administration. Explorations have been performed using pre-trained
language models (PLMs) to simulate these environments for safe interaction in
high-risk environments. However, their use has been constrained to frozen,
large parameter models like GPT. For smaller architectures to reach a similar
level of believability, a rich dataset of CLI interactions is required.
Existing public datasets focus on mapping natural-language tasks to commands,
omitting crucial execution data such as exit codes, outputs, and environmental
side effects, limiting their usability for behavioral modeling. We introduce a
Shell Input -Output Environment (ShIOEnv), which casts command construction as
a Markov Decision Process whose state is the partially built sequence and whose
actions append arguments. After each action, ShIOEnv executes the candidate and
returns its exit status, output, and progress toward a minimal-length
behavioral objective. Due to the intractable nature of the combinatorial
argument state-action space, we derive a context-free grammar from man pages to
mask invalid arguments from being emitted. We explore random and
proximal-policy optimization (PPO)-optimized sampling of unrestricted and
grammar-masked action spaces to produce four exploration strategies. We
observed that grammar masking and PPO significantly improve sample efficiency
to produce a higher quality dataset (maximizing the number of arguments while
minimizing redundancies). Policy-generated datasets of shell input-output
behavior pairs are used to fine-tune CodeT5, where we observe 85% improvements
in BLEU-4 when constraining the action space to grammar productions with an
additional 26% improvement when applying PPO. The ShIOEnv environment and
curated command behavior datasets are released for use in future research.

</details>


### [62] [Enhancing Transformation from Natural Language to Signal Temporal Logic Using LLMs with Diverse External Knowledge](https://arxiv.org/pdf/2505.20658)
*Yue Fang, Zhi Jin, Jie An, Hongshen Chen, Xiaohong Chen, Naijun Zhan*

Main category: cs.CL

TL;DR: The paper introduces STL-DivEn, a diverse NL-STL dataset, and KGST, a knowledge-guided framework for NL-to-STL transformation, outperforming baselines.


<details>
  <summary>Details</summary>
Motivation: Manual NL-to-STL transformation is time-consuming and error-prone, and lack of datasets hinders automation.

Method: Created STL-DivEn dataset via clustering and LLMs, then developed KGST framework for transformation.

Result: STL-DivEn shows higher diversity; KGST outperforms baselines in accuracy.

Conclusion: STL-DivEn and KGST advance NL-to-STL automation, addressing dataset and transformation challenges.

Abstract: Temporal Logic (TL), especially Signal Temporal Logic (STL), enables precise
formal specification, making it widely used in cyber-physical systems such as
autonomous driving and robotics. Automatically transforming NL into STL is an
attractive approach to overcome the limitations of manual transformation, which
is time-consuming and error-prone. However, due to the lack of datasets,
automatic transformation currently faces significant challenges and has not
been fully explored. In this paper, we propose an NL-STL dataset named
STL-Diversity-Enhanced (STL-DivEn), which comprises 16,000 samples enriched
with diverse patterns. To develop the dataset, we first manually create a
small-scale seed set of NL-STL pairs. Next, representative examples are
identified through clustering and used to guide large language models (LLMs) in
generating additional NL-STL pairs. Finally, diversity and accuracy are ensured
through rigorous rule-based filters and human validation. Furthermore, we
introduce the Knowledge-Guided STL Transformation (KGST) framework, a novel
approach for transforming natural language into STL, involving a
generate-then-refine process based on external knowledge. Statistical analysis
shows that the STL-DivEn dataset exhibits more diversity than the existing
NL-STL dataset. Moreover, both metric-based and human evaluations indicate that
our KGST approach outperforms baseline models in transformation accuracy on
STL-DivEn and DeepSTL datasets.

</details>


### [63] [BacktrackAgent: Enhancing GUI Agent with Error Detection and Backtracking Mechanism](https://arxiv.org/pdf/2505.20660)
*Qinzhuo Wu, Pengzhi Gao, Wei Liu, Jian Luan*

Main category: cs.CL

TL;DR: BacktrackAgent improves GUI task completion with backtracking, error detection, and recovery mechanisms, showing better success rates and accuracy.


<details>
  <summary>Details</summary>
Motivation: Existing GUI agents lack robust error detection and recovery, limiting task efficiency.

Method: Proposes BacktrackAgent with verifier, judger, and reflector modules, plus judgment rewards and a specialized training dataset.

Result: Improved task success rate and step accuracy on Mobile3M and Auto-UI benchmarks.

Conclusion: BacktrackAgent effectively addresses error handling in GUI agents, enhancing performance.

Abstract: Graphical User Interface (GUI) agents have gained substantial attention due
to their impressive capabilities to complete tasks through multiple
interactions within GUI environments. However, existing agents primarily focus
on enhancing the accuracy of individual actions and often lack effective
mechanisms for detecting and recovering from errors. To address these
shortcomings, we propose the BacktrackAgent, a robust framework that
incorporates a backtracking mechanism to improve task completion efficiency.
BacktrackAgent includes verifier, judger, and reflector components as modules
for error detection and recovery, while also applying judgment rewards to
further enhance the agent's performance. Additionally, we develop a training
dataset specifically designed for the backtracking mechanism, which considers
the outcome pages after action executions. Experimental results show that
BacktrackAgent has achieved performance improvements in both task success rate
and step accuracy on Mobile3M and Auto-UI benchmarks. Our data and code will be
released upon acceptance.

</details>


### [64] [Self-Route: Automatic Mode Switching via Capability Estimation for Efficient Reasoning](https://arxiv.org/pdf/2505.20664)
*Yang He, Xiao Ding, Bibo Cai, Yufei Zhang, Kai Xiong, Zhouhao Sun, Bing Qin, Ting Liu*

Main category: cs.CL

TL;DR: Self-Route is a dynamic reasoning framework that reduces unnecessary token consumption in RLLMs by selecting between general and reasoning modes based on model capability estimation, achieving comparable accuracy with 30-55% fewer tokens.


<details>
  <summary>Details</summary>
Motivation: RLLMs often overthink simpler problems, leading to inefficient token usage without proportional accuracy gains.

Method: Introduces a lightweight pre-inference stage to extract capability-aware embeddings and uses the Gradient-10K dataset to train a router for dynamic mode selection.

Result: Reduces token consumption by 30-55% while maintaining accuracy across diverse benchmarks.

Conclusion: Self-Route is effective and practical for models of varying scales and reasoning paradigms.

Abstract: While reasoning-augmented large language models (RLLMs) significantly enhance
complex task performance through extended reasoning chains, they inevitably
introduce substantial unnecessary token consumption, particularly for simpler
problems where Short Chain-of-Thought (Short CoT) suffices. This overthinking
phenomenon leads to inefficient resource usage without proportional accuracy
gains. To address this issue, we propose Self-Route, a dynamic reasoning
framework that automatically selects between general and reasoning modes based
on model capability estimation. Our approach introduces a lightweight
pre-inference stage to extract capability-aware embeddings from hidden layer
representations, enabling real-time evaluation of the model's ability to solve
problems. We further construct Gradient-10K, a model difficulty
estimation-based dataset with dense complexity sampling, to train the router
for precise capability boundary detection. Extensive experiments demonstrate
that Self-Route achieves comparable accuracy to reasoning models while reducing
token consumption by 30-55\% across diverse benchmarks. The proposed framework
demonstrates consistent effectiveness across models with different parameter
scales and reasoning paradigms, highlighting its general applicability and
practical value.

</details>


### [65] [Pretraining Language Models to Ponder in Continuous Space](https://arxiv.org/pdf/2505.20674)
*Boyi Zeng, Shixiang Song, Siyuan Huang, Yixuan Wang, He Li, Ziwei He, Xinbing Wang, Zhiyu Li, Zhouhan Lin*

Main category: cs.CL

TL;DR: The paper introduces a 'pondering' process for language models, where the model repeatedly processes token embeddings internally before generating output, improving performance without extra parameters.


<details>
  <summary>Details</summary>
Motivation: To mimic human cognitive pondering in language models for deeper processing and better performance.

Method: Repeatedly invoke the forward process during token generation, using weighted token embeddings for internal pondering.

Result: Pondering models match vanilla models with twice the parameters and outperform baselines on downstream tasks.

Conclusion: Pondering enhances model efficiency and performance, applicable across architectures without extra training data.

Abstract: Humans ponder before articulating complex sentence elements, enabling deeper
cognitive processing through focused effort. In this work, we introduce this
pondering process into language models by repeatedly invoking the forward
process within a single token generation step. During pondering, instead of
generating an actual token sampled from the prediction distribution, the model
ponders by yielding a weighted sum of all token embeddings according to the
predicted token distribution. The generated embedding is then fed back as input
for another forward pass. We show that the model can learn to ponder in this
way through self-supervised learning, without any human annotations. Our method
is straightforward and can be seamlessly integrated with various existing
language models. Experiments across three widely used open-source
architectures-GPT-2, Pythia, and LLaMA-and extensive downstream task
evaluations demonstrate the effectiveness and generality of our method. For
language modeling tasks, pondering language models achieve performance
comparable to vanilla models with twice the number of parameters. On 9
downstream benchmarks, our pondering-enhanced Pythia models significantly
outperform the official Pythia models. Notably, pondering-enhanced Pythia-1B is
comparable to TinyLlama-1.1B, which is trained on 10 times more data. The code
is available at https://github.com/LUMIA-Group/PonderingLM.

</details>


### [66] [SELF-PERCEPT: Introspection Improves Large Language Models' Detection of Multi-Person Mental Manipulation in Conversations](https://arxiv.org/pdf/2505.20679)
*Danush Khanna, Pratinav Seth, Sidhaarth Sredharan Murali, Aditya Kumar Guru, Siddharth Shukla, Tanuj Tyagi, Sandeep Chaurasia, Kripabandhu Ghosh*

Main category: cs.CL

TL;DR: The paper introduces MultiManip, a dataset for detecting mental manipulation in multi-turn, multi-person dialogues, and proposes SELF-PERCEPT, a two-stage prompting framework to improve detection using LLMs.


<details>
  <summary>Details</summary>
Motivation: Mental manipulation is a subtle form of abuse, but detecting it in complex conversations is challenging for LLMs due to its nuanced nature.

Method: The authors create the MultiManip dataset from reality shows and evaluate LLMs like GPT-4o and Llama-3.1-8B. They propose SELF-PERCEPT, a two-stage prompting framework based on Self-Perception Theory.

Result: Existing LLMs struggle with manipulation detection, but SELF-PERCEPT shows strong performance in identifying manipulative language.

Conclusion: The study highlights the difficulty of detecting manipulation and presents SELF-PERCEPT as an effective solution, with publicly available code and data.

Abstract: Mental manipulation is a subtle yet pervasive form of abuse in interpersonal
communication, making its detection critical for safeguarding potential
victims. However, due to manipulation's nuanced and context-specific nature,
identifying manipulative language in complex, multi-turn, and multi-person
conversations remains a significant challenge for large language models (LLMs).
To address this gap, we introduce the MultiManip dataset, comprising 220
multi-turn, multi-person dialogues balanced between manipulative and
non-manipulative interactions, all drawn from reality shows that mimic
real-world scenarios. For manipulative interactions, it includes 11 distinct
manipulations depicting real-life scenarios. We conduct extensive evaluations
of state-of-the-art LLMs, such as GPT-4o and Llama-3.1-8B, employing various
prompting strategies. Despite their capabilities, these models often struggle
to detect manipulation effectively. To overcome this limitation, we propose
SELF-PERCEPT, a novel, two-stage prompting framework inspired by
Self-Perception Theory, demonstrating strong performance in detecting
multi-person, multi-turn mental manipulation. Our code and data are publicly
available at https://github.com/danushkhanna/self-percept .

</details>


### [67] [Beyond Templates: Dynamic Adaptation of Reasoning Demonstrations via Feasibility-Aware Exploration](https://arxiv.org/pdf/2505.20700)
*Yong Wu, Weihang Pan, Ke Li, Chen Binhui, Ping Li, Binbin Lin*

Main category: cs.CL

TL;DR: DART is a framework that adapts expert reasoning for small language models (SLMs) by selectively imitating expert steps and exploring alternatives when needed, improving generalization and efficiency.


<details>
  <summary>Details</summary>
Motivation: Aligning reasoning capabilities of large language models (LLMs) to smaller models (SLMs) is challenging due to mismatches and limited capacity. Existing datasets degrade SLM performance.

Method: DART uses selective imitation guided by adaptability estimation. If expert steps exceed the student's capacity (Imitation Gap), it explores alternative paths while ensuring outcome consistency.

Result: DART improves generalization and data efficiency across reasoning benchmarks and model scales compared to static fine-tuning.

Conclusion: DART offers a scalable solution for aligning reasoning in resource-constrained models by matching training signals to student capabilities.

Abstract: Large language models (LLMs) have shown remarkable reasoning capabilities,
yet aligning such abilities to small language models (SLMs) remains a challenge
due to distributional mismatches and limited model capacity. Existing reasoning
datasets, typically designed for powerful LLMs, often lead to degraded
performance when directly applied to weaker models. In this work, we introduce
Dynamic Adaptation of Reasoning Trajectories (DART), a novel data adaptation
framework that bridges the capability gap between expert reasoning trajectories
and diverse SLMs. Instead of uniformly imitating expert steps, DART employs a
selective imitation strategy guided by step-wise adaptability estimation via
solution simulation. When expert steps surpass the student's capacity --
signaled by an Imitation Gap -- the student autonomously explores alternative
reasoning paths, constrained by outcome consistency. We validate DART across
multiple reasoning benchmarks and model scales, demonstrating that it
significantly improves generalization and data efficiency over static
fine-tuning. Our method enhances supervision quality by aligning training
signals with the student's reasoning capabilities, offering a scalable solution
for reasoning alignment in resource-constrained models.

</details>


### [68] [Dissecting Physics Reasoning in Small Language Models: A Multi-Dimensional Analysis from an Educational Perspective](https://arxiv.org/pdf/2505.20707)
*Nicy Scaria, Silvester John Joseph Kennedy, Diksha Seth, Deepak Subramani*

Main category: cs.CL

TL;DR: SLMs show promise for educational use but struggle with complex physics reasoning, often providing correct answers with flawed reasoning. Performance varies by model and topic, with cultural contextualization having minimal impact.


<details>
  <summary>Details</summary>
Motivation: To explore the potential of SLMs in high school physics education, focusing on their reasoning capabilities and cultural adaptability.

Method: Evaluated SLMs using a physics dataset from OpenStax, annotated by Bloom's Taxonomy, with cultural adaptations. Used LLM-as-a-judge framework for assessment.

Result: Qwen 3 1.7B had high answer accuracy (85%) but low reasoning correctness (38%). Performance varied by topic and declined with complexity. Cultural context had little effect.

Conclusion: SLMs need improved reasoning for reliable educational use, prioritizing understanding over answer accuracy.

Abstract: Small Language Models (SLMs) offer computational efficiency and
accessibility, making them promising for educational applications. However,
their capacity for complex reasoning, particularly in domains such as physics,
remains underexplored. This study investigates the high school physics
reasoning capabilities of state-of-the-art SLMs (under 4 billion parameters),
including instruct versions of Llama 3.2, Phi 4 Mini, Gemma 3, and Qwen series.
We developed a comprehensive physics dataset from the OpenStax High School
Physics textbook, annotated according to Bloom's Taxonomy, with LaTeX and
plaintext mathematical notations. A novel cultural contextualization approach
was applied to a subset, creating culturally adapted problems for Asian,
African, and South American/Australian contexts while preserving core physics
principles. Using an LLM-as-a-judge framework with Google's Gemini 2.5 Flash,
we evaluated answer and reasoning chain correctness, along with calculation
accuracy. The results reveal significant differences between the SLMs. Qwen 3
1.7B achieved high `answer accuracy' (85%), but `fully correct reasoning' was
substantially low (38%). The format of the mathematical notation had a
negligible impact on performance. SLMs exhibited varied performance across the
physics topics and showed a decline in reasoning quality with increasing
cognitive and knowledge complexity. In particular, the consistency of reasoning
was largely maintained in diverse cultural contexts, especially by better
performing models. These findings indicate that, while SLMs can often find
correct answers, their underlying reasoning is frequently flawed, suggesting an
overreliance on pattern recognition. For SLMs to become reliable educational
tools in physics, future development must prioritize enhancing genuine
understanding and the generation of sound, verifiable reasoning chains over
mere answer accuracy.

</details>


### [69] [SPA-RL: Reinforcing LLM Agents via Stepwise Progress Attribution](https://arxiv.org/pdf/2505.20732)
*Hanlin Wang, Chak Tou Leong, Jiashuo Wang, Jian Wang, Wenjie Li*

Main category: cs.CL

TL;DR: The paper proposes Stepwise Progress Attribution (SPA), a reward redistribution framework for RL in LLM agents, addressing delayed rewards by decomposing final rewards into stepwise contributions. It outperforms state-of-the-art methods in success rate and grounding accuracy.


<details>
  <summary>Details</summary>
Motivation: Delayed rewards in RL for LLM agents hinder training by providing insufficient guidance for earlier actions. The paper aims to improve this by attributing rewards incrementally to individual steps.

Method: SPA decomposes the final reward into stepwise contributions using a trained progress estimator. These contributions, combined with grounding signals, serve as intermediate rewards for effective training.

Result: SPA outperforms state-of-the-art methods, improving success rates by 2.5% and grounding accuracy by 1.9% on benchmarks like Webshop, ALFWorld, and VirtualHome.

Conclusion: SPA effectively addresses delayed rewards in RL for LLM agents, providing better intermediate rewards and enhancing training performance.

Abstract: Reinforcement learning (RL) holds significant promise for training LLM agents
to handle complex, goal-oriented tasks that require multi-step interactions
with external environments. However, a critical challenge when applying RL to
these agentic tasks arises from delayed rewards: feedback signals are typically
available only after the entire task is completed. This makes it non-trivial to
assign delayed rewards to earlier actions, providing insufficient guidance
regarding environmental constraints and hindering agent training. In this work,
we draw on the insight that the ultimate completion of a task emerges from the
cumulative progress an agent makes across individual steps. We propose Stepwise
Progress Attribution (SPA), a general reward redistribution framework that
decomposes the final reward into stepwise contributions, each reflecting its
incremental progress toward overall task completion. To achieve this, we train
a progress estimator that accumulates stepwise contributions over a trajectory
to match the task completion. During policy optimization, we combine the
estimated per-step contribution with a grounding signal for actions executed in
the environment as the fine-grained, intermediate reward for effective agent
training. Extensive experiments on common agent benchmarks (including Webshop,
ALFWorld, and VirtualHome) demonstrate that SPA consistently outperforms the
state-of-the-art method in both success rate (+2.5\% on average) and grounding
accuracy (+1.9\% on average). Further analyses demonstrate that our method
remarkably provides more effective intermediate rewards for RL training. Our
code is available at https://github.com/WangHanLinHenry/SPA-RL-Agent.

</details>


### [70] [Silencer: From Discovery to Mitigation of Self-Bias in LLM-as-Benchmark-Generator](https://arxiv.org/pdf/2505.20738)
*Peiwen Yuan, Yiwei Li, Shaoxiong Feng, Xinglin Wang, Yueqi Zhang, Jiayi Shi, Chuyi Tan, Boyuan Pan, Yao Hu, Kan Li*

Main category: cs.CL

TL;DR: The paper introduces Silencer, a framework to mitigate self-bias in LLM-generated benchmarks, improving evaluation quality.


<details>
  <summary>Details</summary>
Motivation: To address underexplored biases in LLM-as-Benchmark-Generator methods, particularly self-bias from question domain, language style, and wrong labels.

Method: Proposes Silencer, leveraging heterogeneity between multiple generators at sample and benchmark levels to neutralize bias.

Result: Silencer reduces self-bias to near zero and improves Pearson correlation with human benchmarks from 0.655 to 0.833.

Conclusion: Silencer effectively mitigates self-bias, enhances benchmark quality, and shows strong generalizability.

Abstract: LLM-as-Benchmark-Generator methods have been widely studied as a supplement
to human annotators for scalable evaluation, while the potential biases within
this paradigm remain underexplored. In this work, we systematically define and
validate the phenomenon of inflated performance in models evaluated on their
self-generated benchmarks, referred to as self-bias, and attribute it to
sub-biases arising from question domain, language style, and wrong labels. On
this basis, we propose Silencer, a general framework that leverages the
heterogeneity between multiple generators at both the sample and benchmark
levels to neutralize bias and generate high-quality, self-bias-silenced
benchmark. Experimental results across various settings demonstrate that
Silencer can suppress self-bias to near zero, significantly improve evaluation
effectiveness of the generated benchmark (with an average improvement from
0.655 to 0.833 in Pearson correlation with high-quality human-annotated
benchmark), while also exhibiting strong generalizability.

</details>


### [71] [CogniBench: A Legal-inspired Framework and Dataset for Assessing Cognitive Faithfulness of Large Language Models](https://arxiv.org/pdf/2505.20767)
*Xiaqiang Tang, Jian Li, Keyu Hu, Du Nan, Xiaolong Li, Xi Zhang, Weigao Sun, Sihong Xie*

Main category: cs.CL

TL;DR: The paper introduces a framework to assess faithfulness hallucination in LLMs, focusing on cognitive statements, and releases a benchmark dataset (CogniBench-L) for training detection models.


<details>
  <summary>Details</summary>
Motivation: Existing benchmarks lack standards for evaluating cognitive statements (inferences from context), making it hard to assess faithfulness in LLMs.

Method: Inspired by legislative evidence assessment, the authors design a framework for evaluating cognitive statements and create an annotation pipeline for automated benchmark generation.

Result: The CogniBench-L dataset is created, revealing insightful statistics and enabling training of accurate hallucination detection models.

Conclusion: The framework and dataset (CogniBench-L) are released to improve consistency evaluation and optimization of cognitive statements in LLMs.

Abstract: Faithfulness hallucination are claims generated by a Large Language Model
(LLM) not supported by contexts provided to the LLM. Lacking assessment
standard, existing benchmarks only contain "factual statements" that rephrase
source materials without marking "cognitive statements" that make inference
from the given context, making the consistency evaluation and optimization of
cognitive statements difficult. Inspired by how an evidence is assessed in the
legislative domain, we design a rigorous framework to assess different levels
of faithfulness of cognitive statements and create a benchmark dataset where we
reveal insightful statistics. We design an annotation pipeline to create larger
benchmarks for different LLMs automatically, and the resulting larger-scale
CogniBench-L dataset can be used to train accurate cognitive hallucination
detection model. We release our model and dataset at:
https://github.com/FUTUREEEEEE/CogniBench

</details>


### [72] [SpecExtend: A Drop-in Enhancement for Speculative Decoding of Long Sequences](https://arxiv.org/pdf/2505.20776)
*Jungyoub Cha, Hyunjong Kim, Sungzoon Cho*

Main category: cs.CL

TL;DR: SpecExtend enhances speculative decoding for LLMs on long inputs by integrating efficient attention mechanisms and Cross-model Retrieval, achieving up to 2.22x speedup.


<details>
  <summary>Details</summary>
Motivation: Performance of speculative decoding degrades on long inputs due to higher attention costs and lower draft accuracy.

Method: Integrates FlashAttention and Hybrid Tree Attention, and introduces Cross-model Retrieval for dynamic KV cache updates.

Result: SpecExtend accelerates decoding by up to 2.22x on inputs up to 16K tokens.

Conclusion: SpecExtend provides an effective, training-free solution for long-sequence speculative decoding.

Abstract: Speculative decoding is a widely adopted technique for accelerating inference
in large language models (LLMs), but its performance degrades on long inputs
due to increased attention cost and reduced draft accuracy. We introduce
SpecExtend, a drop-in enhancement that improves the performance of speculative
decoding on long sequences without any additional training. SpecExtend
integrates efficient attention mechanisms such as FlashAttention and Hybrid
Tree Attention into both the draft and target models, reducing latency across
all stages. To improve draft accuracy and speed, we propose Cross-model
Retrieval, a novel KV cache update strategy that uses the target model's
attention scores to dynamically select relevant context for the draft model.
Extensive evaluations on three long-context understanding datasets show that
SpecExtend accelerates standard tree-based speculative decoding by up to 2.22x
for inputs up to 16K tokens, providing an effective solution for speculative
decoding of long sequences. The code is available at
https://github.com/jycha98/SpecExtend .

</details>


### [73] [CHIMERA: A Knowledge Base of Idea Recombination in Scientific Literature](https://arxiv.org/pdf/2505.20779)
*Noy Sternlicht, Tom Hope*

Main category: cs.CL

TL;DR: CHIMERA is a knowledge base of recombination examples mined from scientific literature, aiding in exploring cross-domain inspiration and training ML models for creative predictions.


<details>
  <summary>Details</summary>
Motivation: To empirically study how scientists recombine concepts and inspire innovation across domains.

Method: Developed an LLM-based extraction model trained on annotated abstracts to mine recombination examples from AI papers.

Result: Created CHIMERA, a KB with 28K recombination examples, and a hypothesis generation model predicting inspiring directions.

Conclusion: CHIMERA enables large-scale exploration of recombination and supports AI-driven scientific creativity.

Abstract: A hallmark of human innovation is the process of recombination -- creating
original ideas by integrating elements of existing mechanisms and concepts. In
this work, we automatically mine the scientific literature and build CHIMERA: a
large-scale knowledge base (KB) of recombination examples. CHIMERA can be used
to empirically explore at scale how scientists recombine concepts and take
inspiration from different areas, or to train supervised machine learning
models that learn to predict new creative cross-domain directions. To build
this KB, we present a novel information extraction task of extracting
recombination from scientific paper abstracts, collect a high-quality corpus of
hundreds of manually annotated abstracts, and use it to train an LLM-based
extraction model. The model is applied to a large corpus of papers in the AI
domain, yielding a KB of over 28K recombination examples. We analyze CHIMERA to
explore the properties of recombination in different subareas of AI. Finally,
we train a scientific hypothesis generation model using the KB, which predicts
new recombination directions that real-world researchers find inspiring. Our
data and code are available at https://github.cs.huji.ac.il/tomhope-lab/CHIMERA

</details>


### [74] [Improved Representation Steering for Language Models](https://arxiv.org/pdf/2505.20809)
*Zhengxuan Wu, Qinan Yu, Aryaman Arora, Christopher D. Manning, Christopher Potts*

Main category: cs.CL

TL;DR: RePS, a new bidirectional preference-optimization method, improves representation steering in language models, outperforming existing methods and narrowing the gap with prompting while enhancing interpretability and robustness.


<details>
  <summary>Details</summary>
Motivation: To address the limitations of existing steering methods (e.g., weight or representation adjustments) and improve fine-grained control over model behavior, especially for concept introduction or suppression.

Method: Developed Reference-free Preference Steering (RePS), a bidirectional preference-optimization objective for joint concept steering and suppression. Evaluated on AxBench with Gemma models (2B to 27B).

Result: RePS outperforms existing steering methods, narrows the gap with prompting, and remains resilient to jailbreaking attacks. It matches or exceeds the performance of language-modeling objectives, especially in suppression tasks.

Conclusion: RePS offers an interpretable and robust alternative to prompting for steering and suppression in language models.

Abstract: Steering methods for language models (LMs) seek to provide fine-grained and
interpretable control over model generations by variously changing model
inputs, weights, or representations to adjust behavior. Recent work has shown
that adjusting weights or representations is often less effective than steering
by prompting, for instance when wanting to introduce or suppress a particular
concept. We demonstrate how to improve representation steering via our new
Reference-free Preference Steering (RePS), a bidirectional
preference-optimization objective that jointly does concept steering and
suppression. We train three parameterizations of RePS and evaluate them on
AxBench, a large-scale model steering benchmark. On Gemma models with sizes
ranging from 2B to 27B, RePS outperforms all existing steering methods trained
with a language modeling objective and substantially narrows the gap with
prompting -- while promoting interpretability and minimizing parameter count.
In suppression, RePS matches the language-modeling objective on Gemma-2 and
outperforms it on the larger Gemma-3 variants while remaining resilient to
prompt-based jailbreaking attacks that defeat prompting. Overall, our results
suggest that RePS provides an interpretable and robust alternative to prompting
for both steering and suppression.

</details>


### [75] [RSCF: Relation-Semantics Consistent Filter for Entity Embedding of Knowledge Graph](https://arxiv.org/pdf/2505.20813)
*Junsik Kim, Jinwook Park, Kangil Kim*

Main category: cs.CL

TL;DR: The paper introduces RSCF, a plug-in KGE method ensuring consistent entity-transformation in knowledge graph embeddings, outperforming state-of-the-art methods.


<details>
  <summary>Details</summary>
Motivation: Addressing inconsistency in embedding differences before and after transformation, which risks losing inductive bias, due to disconnected relation-specific transformations and disruptive plug-in approaches.

Method: RSCF features shared affine transformation, rooted entity-transformation, and normalization. It includes relation transformation and prediction modules to enhance semantics.

Result: RSCF significantly outperforms existing KGE methods in knowledge graph completion tasks, showing robustness across relations and frequencies.

Conclusion: RSCF effectively preserves semantic consistency in embeddings, enhancing performance and robustness in knowledge graph tasks.

Abstract: In knowledge graph embedding, leveraging relation-specific
entity-transformation has markedly enhanced performance. However, the
consistency of embedding differences before and after transformation remains
unaddressed, risking the loss of valuable inductive bias inherent in the
embeddings. This inconsistency stems from two problems. First, transformation
representations are specified for relations in a disconnected manner, allowing
dissimilar transformations and corresponding entity-embeddings for similar
relations. Second, a generalized plug-in approach as a SFBR (Semantic Filter
Based on Relations) disrupts this consistency through excessive concentration
of entity embeddings under entity-based regularization, generating
indistinguishable score distributions among relations. In this paper, we
introduce a plug-in KGE method, Relation-Semantics Consistent Filter (RSCF),
containing more consistent entity-transformation characterized by three
features: 1) shared affine transformation of relation embeddings across all
relations, 2) rooted entity-transformation that adds an entity embedding to its
change represented by the transformed vector, and 3) normalization of the
change to prevent scale reduction. To amplify the advantages of consistency
that preserve semantics on embeddings, RSCF adds relation transformation and
prediction modules for enhancing the semantics. In knowledge graph completion
tasks with distance-based and tensor decomposition models, RSCF significantly
outperforms state-of-the-art KGE methods, showing robustness across all
relations and their frequencies.

</details>


### [76] [Rethinking Information Synthesis in Multimodal Question Answering A Multi-Agent Perspective](https://arxiv.org/pdf/2505.20816)
*Krishna Singh Rajput, Tejas Anvekar, Chitta Baral, Vivek Gupta*

Main category: cs.CL

TL;DR: MAMMQA is a multi-agent QA framework for multimodal inputs (text, tables, images) that outperforms existing methods by leveraging specialized agents for decomposition, synthesis, and integration.


<details>
  <summary>Details</summary>
Motivation: Existing multimodal QA methods rely on generalized reasoning, ignoring modality-specific traits, limiting accuracy and interpretability.

Method: MAMMQA uses two VLM agents (for decomposition and synthesis) and one LLM agent (for integration) to process multimodal inputs.

Result: Outperforms baselines in accuracy and robustness on diverse benchmarks.

Conclusion: The modular, multi-agent design improves interpretability and performance by leveraging specialized reasoning.

Abstract: Recent advances in multimodal question answering have primarily focused on
combining heterogeneous modalities or fine-tuning multimodal large language
models. While these approaches have shown strong performance, they often rely
on a single, generalized reasoning strategy, overlooking the unique
characteristics of each modality ultimately limiting both accuracy and
interpretability. To address these limitations, we propose MAMMQA, a
multi-agent QA framework for multimodal inputs spanning text, tables, and
images. Our system includes two Visual Language Model (VLM) agents and one
text-based Large Language Model (LLM) agent. The first VLM decomposes the user
query into sub-questions and sequentially retrieves partial answers from each
modality. The second VLM synthesizes and refines these results through
cross-modal reasoning. Finally, the LLM integrates the insights into a cohesive
answer. This modular design enhances interpretability by making the reasoning
process transparent and allows each agent to operate within its domain of
expertise. Experiments on diverse multimodal QA benchmarks demonstrate that our
cooperative, multi-agent framework consistently outperforms existing baselines
in both accuracy and robustness.

</details>


### [77] [Tracing and Reversing Rank-One Model Edits](https://arxiv.org/pdf/2505.20819)
*Paul Youssef, Zhixue Zhao, Christin Seifert, Jörg Schlötterer*

Main category: cs.CL

TL;DR: The paper explores the traceability and reversibility of knowledge edits in LLMs, focusing on ROME, and proposes methods to detect, trace, and reverse adversarial edits.


<details>
  <summary>Details</summary>
Motivation: Knowledge edits (KEs) in LLMs are cost-effective but pose dual-use risks, as they can be exploited to implant misinformation or bias. Robust techniques are needed to detect and mitigate such adversarial edits.

Method: The study investigates ROME's edited weight matrices for distinctive patterns, uses these to predict edited facts, infers edited entities, and demonstrates reversibility of edits.

Result: ROME edits introduce detectable patterns, enabling prediction of edited facts (95% accuracy) and reversal of edits (≥80% accuracy).

Conclusion: The findings show feasibility of detecting, tracing, and reversing adversarial edits, providing a framework to safeguard LLMs.

Abstract: Knowledge editing methods (KEs) are a cost-effective way to update the
factual content of large language models (LLMs), but they pose a dual-use risk.
While KEs are beneficial for updating outdated or incorrect information, they
can be exploited maliciously to implant misinformation or bias. In order to
defend against these types of malicious manipulation, we need robust techniques
that can reliably detect, interpret, and mitigate adversarial edits. This work
investigates the traceability and reversibility of knowledge edits, focusing on
the widely used Rank-One Model Editing (ROME) method. We first show that ROME
introduces distinctive distributional patterns in the edited weight matrices,
which can serve as effective signals for locating the edited weights. Second,
we show that these altered weights can reliably be used to predict the edited
factual relation, enabling partial reconstruction of the modified fact.
Building on this, we propose a method to infer the edited object entity
directly from the modified weights, without access to the editing prompt,
achieving over 95% accuracy. Finally, we demonstrate that ROME edits can be
reversed, recovering the model's original outputs with $\geq$ 80% accuracy. Our
findings highlight the feasibility of detecting, tracing, and reversing edits
based on the edited weights, offering a robust framework for safeguarding LLMs
against adversarial manipulations.

</details>


### [78] [Reinforced Informativeness Optimization for Long-Form Retrieval-Augmented Generation](https://arxiv.org/pdf/2505.20825)
*Yuhao Wang, Ruiyang Ren, Yucheng Wang, Wayne Xin Zhao, Jing Liu, Hua Wu, Haifeng Wang*

Main category: cs.CL

TL;DR: RioRAG introduces a reinforcement learning framework to improve long-form QA by optimizing informativeness and using a hierarchical reward model for factual alignment.


<details>
  <summary>Details</summary>
Motivation: Addressing challenges in long-form QA like data scarcity, hallucination risks, and lack of reliable evaluation metrics.

Method: Uses reinforced informativeness optimization and a nugget-centric hierarchical reward model for factual assessment.

Result: Demonstrated effectiveness on LongFact and RAGChecker benchmarks.

Conclusion: RioRAG advances long-form RAG by improving informativeness and factual accuracy without needing expensive supervised data.

Abstract: Long-form question answering (LFQA) presents unique challenges for large
language models, requiring the synthesis of coherent, paragraph-length answers.
While retrieval-augmented generation (RAG) systems have emerged as a promising
solution, existing research struggles with key limitations: the scarcity of
high-quality training data for long-form generation, the compounding risk of
hallucination in extended outputs, and the absence of reliable evaluation
metrics for factual completeness. In this paper, we propose RioRAG, a novel
reinforcement learning (RL) framework that advances long-form RAG through
reinforced informativeness optimization. Our approach introduces two
fundamental innovations to address the core challenges. First, we develop an RL
training paradigm of reinforced informativeness optimization that directly
optimizes informativeness and effectively addresses the slow-thinking deficit
in conventional RAG systems, bypassing the need for expensive supervised data.
Second, we propose a nugget-centric hierarchical reward modeling approach that
enables precise assessment of long-form answers through a three-stage process:
extracting the nugget from every source webpage, constructing a nugget claim
checklist, and computing rewards based on factual alignment. Extensive
experiments on two LFQA benchmarks LongFact and RAGChecker demonstrate the
effectiveness of the proposed method. Our codes are available at
https://github.com/RUCAIBox/RioRAG.

</details>


### [79] [AdParaphrase v2.0: Generating Attractive Ad Texts Using a Preference-Annotated Paraphrase Dataset](https://arxiv.org/pdf/2505.20826)
*Soichiro Murakami, Peinan Zhang, Hidetaka Kamigaito, Hiroya Takamura, Manabu Okumura*

Main category: cs.CL

TL;DR: AdParaphrase v2.0 is a dataset for ad text paraphrasing, 20x larger than v1.0, with human preference annotations. It helps analyze linguistic features and develop methods for attractive ad texts.


<details>
  <summary>Details</summary>
Motivation: To identify linguistic factors that make ad texts attractive and support methods for generating engaging ads.

Method: Created a dataset (16,460 ad text paraphrase pairs) with human preference annotations from ten evaluators per pair. Analyzed linguistic features and explored generation methods.

Result: Identified new linguistic features of engaging ads and relationships between human preference and ad performance. Highlighted potential of reference-free metrics for evaluation.

Conclusion: AdParaphrase v2.0 enables comprehensive analysis of ad text attractiveness and supports development of better ad generation methods.

Abstract: Identifying factors that make ad text attractive is essential for advertising
success. This study proposes AdParaphrase v2.0, a dataset for ad text
paraphrasing, containing human preference data, to enable the analysis of the
linguistic factors and to support the development of methods for generating
attractive ad texts. Compared with v1.0, this dataset is 20 times larger,
comprising 16,460 ad text paraphrase pairs, each annotated with preference data
from ten evaluators, thereby enabling a more comprehensive and reliable
analysis. Through the experiments, we identified multiple linguistic features
of engaging ad texts that were not observed in v1.0 and explored various
methods for generating attractive ad texts. Furthermore, our analysis
demonstrated the relationships between human preference and ad performance, and
highlighted the potential of reference-free metrics based on large language
models for evaluating ad text attractiveness. The dataset is publicly available
at: https://github.com/CyberAgentAILab/AdParaphrase-v2.0.

</details>


### [80] [Concealment of Intent: A Game-Theoretic Analysis](https://arxiv.org/pdf/2505.20841)
*Xinbo Wu, Abhishek Umrawal, Lav R. Varshney*

Main category: cs.CL

TL;DR: The paper introduces intent-hiding adversarial prompting as a scalable attack on LLMs, analyzes it using game theory, and proposes a defense mechanism.


<details>
  <summary>Details</summary>
Motivation: Addressing vulnerabilities in LLM alignment mechanisms against adversarial prompts.

Method: Develops a game-theoretic framework to model attack-defense interactions and proposes a tailored defense.

Result: Identifies attacker advantages and validates attack effectiveness on real-world LLMs.

Conclusion: Highlights the need for improved defenses against intent-hiding adversarial attacks.

Abstract: As large language models (LLMs) grow more capable, concerns about their safe
deployment have also grown. Although alignment mechanisms have been introduced
to deter misuse, they remain vulnerable to carefully designed adversarial
prompts. In this work, we present a scalable attack strategy: intent-hiding
adversarial prompting, which conceals malicious intent through the composition
of skills. We develop a game-theoretic framework to model the interaction
between such attacks and defense systems that apply both prompt and response
filtering. Our analysis identifies equilibrium points and reveals structural
advantages for the attacker. To counter these threats, we propose and analyze a
defense mechanism tailored to intent-hiding attacks. Empirically, we validate
the attack's effectiveness on multiple real-world LLMs across a range of
malicious behaviors, demonstrating clear advantages over existing adversarial
prompting techniques.

</details>


### [81] [Divide-Then-Align: Honest Alignment based on the Knowledge Boundary of RAG](https://arxiv.org/pdf/2505.20871)
*Xin Sun, Jianan Xie, Zhongqi Chen, Qiang Liu, Shu Wu, Yuehe Chen, Bowen Song, Weiqiang Wang, Zilei Wang, Liang Wang*

Main category: cs.CL

TL;DR: Divide-Then-Align (DTA) improves retrieval-augmented systems by enabling them to abstain from answering uncertain queries, enhancing reliability.


<details>
  <summary>Details</summary>
Motivation: Current methods like RAFT force models to answer even without reliable knowledge, risking reliability in high-stakes domains.

Method: DTA divides data into knowledge quadrants, creates tailored preference data, and uses Direct Preference Optimization (DPO) for training.

Result: DTA balances accuracy and abstention, improving system reliability on three benchmark datasets.

Conclusion: DTA enhances trustworthiness of retrieval-augmented systems by addressing uncertainty in responses.

Abstract: Large language models (LLMs) augmented with retrieval systems have
significantly advanced natural language processing tasks by integrating
external knowledge sources, enabling more accurate and contextually rich
responses. To improve the robustness of such systems against noisy retrievals,
Retrieval-Augmented Fine-Tuning (RAFT) has emerged as a widely adopted method.
However, RAFT conditions models to generate answers even in the absence of
reliable knowledge. This behavior undermines their reliability in high-stakes
domains, where acknowledging uncertainty is critical. To address this issue, we
propose Divide-Then-Align (DTA), a post-training approach designed to endow RAG
systems with the ability to respond with "I don't know" when the query is out
of the knowledge boundary of both the retrieved passages and the model's
internal knowledge. DTA divides data samples into four knowledge quadrants and
constructs tailored preference data for each quadrant, resulting in a curated
dataset for Direct Preference Optimization (DPO). Experimental results on three
benchmark datasets demonstrate that DTA effectively balances accuracy with
appropriate abstention, enhancing the reliability and trustworthiness of
retrieval-augmented systems.

</details>


### [82] [Can LLMs Learn to Map the World from Local Descriptions?](https://arxiv.org/pdf/2505.20874)
*Sirui Xia, Aili Chen, Xintao Wang, Tinghui Zhu, Yikai Zhang, Jiangjie Chen, Yanghua Xiao*

Main category: cs.CL

TL;DR: LLMs can integrate local spatial descriptions to form global spatial cognition, excelling in spatial perception and navigation tasks.


<details>
  <summary>Details</summary>
Motivation: To explore LLMs' ability to internalize structured spatial knowledge, which remains underexplored despite their success in other domains.

Method: Investigates LLMs' spatial cognition by testing their ability to infer global layouts from local relationships and learn road connectivity from trajectories in a simulated urban environment.

Result: LLMs generalize to unseen spatial relationships, align with real-world distributions, and learn road connectivity for accurate path planning.

Conclusion: LLMs show promise in constructing coherent global spatial cognition from fragmented local descriptions, enhancing spatial tasks.

Abstract: Recent advances in Large Language Models (LLMs) have demonstrated strong
capabilities in tasks such as code and mathematics. However, their potential to
internalize structured spatial knowledge remains underexplored. This study
investigates whether LLMs, grounded in locally relative human observations, can
construct coherent global spatial cognition by integrating fragmented
relational descriptions. We focus on two core aspects of spatial cognition:
spatial perception, where models infer consistent global layouts from local
positional relationships, and spatial navigation, where models learn road
connectivity from trajectory data and plan optimal paths between unconnected
locations. Experiments conducted in a simulated urban environment demonstrate
that LLMs not only generalize to unseen spatial relationships between points of
interest (POIs) but also exhibit latent representations aligned with real-world
spatial distributions. Furthermore, LLMs can learn road connectivity from
trajectory descriptions, enabling accurate path planning and dynamic spatial
awareness during navigation.

</details>


### [83] [Trans-EnV: A Framework for Evaluating the Linguistic Robustness of LLMs Against English Varieties](https://arxiv.org/pdf/2505.20875)
*Jiyoung Lee, Seungho Kim, Jieun Han, Jun-Min Lee, Kitaek Kim, Alice Oh, Edward Choi*

Main category: cs.CL

TL;DR: The paper introduces Trans-EnV, a framework to evaluate LLMs on non-standard English varieties, revealing significant performance disparities.


<details>
  <summary>Details</summary>
Motivation: Current LLM evaluations focus on Standard American English, neglecting global diversity, raising fairness concerns.

Method: Trans-EnV transforms SAE datasets into 38 English varieties using expert knowledge and LLM-based transformations.

Result: Performance drops by up to 46.3% on non-standard varieties, highlighting robustness issues.

Conclusion: Comprehensive evaluation across diverse English varieties is crucial for fairness and linguistic robustness.

Abstract: Large Language Models (LLMs) are predominantly evaluated on Standard American
English (SAE), often overlooking the diversity of global English varieties.
This narrow focus may raise fairness concerns as degraded performance on
non-standard varieties can lead to unequal benefits for users worldwide.
Therefore, it is critical to extensively evaluate the linguistic robustness of
LLMs on multiple non-standard English varieties. We introduce Trans-EnV, a
framework that automatically transforms SAE datasets into multiple English
varieties to evaluate the linguistic robustness. Our framework combines (1)
linguistics expert knowledge to curate variety-specific features and
transformation guidelines from linguistic literature and corpora, and (2)
LLM-based transformations to ensure both linguistic validity and scalability.
Using Trans-EnV, we transform six benchmark datasets into 38 English varieties
and evaluate seven state-of-the-art LLMs. Our results reveal significant
performance disparities, with accuracy decreasing by up to 46.3% on
non-standard varieties. These findings highlight the importance of
comprehensive linguistic robustness evaluation across diverse English
varieties. Each construction of Trans-EnV was validated through rigorous
statistical testing and consultation with a researcher in the field of second
language acquisition, ensuring its linguistic validity. Our
\href{https://github.com/jiyounglee-0523/TransEnV}{code} and
\href{https://huggingface.co/collections/jiyounglee0523/transenv-681eadb3c0c8cf363b363fb1}{datasets}
are publicly available.

</details>


### [84] [MSA at SemEval-2025 Task 3: High Quality Weak Labeling and LLM Ensemble Verification for Multilingual Hallucination Detection](https://arxiv.org/pdf/2505.20880)
*Baraa Hikal, Ahmed Nasreldin, Ali Hamdi*

Main category: cs.CL

TL;DR: A system combining prompt engineering and LLM ensemble verification for detecting hallucinated text spans in multilingual LLM outputs, achieving top rankings in multiple languages.


<details>
  <summary>Details</summary>
Motivation: To address the challenge of detecting hallucinated spans in text generated by multilingual LLMs, simulating human annotation workflows for accuracy.

Method: Task-specific prompt engineering with an LLM ensemble verification mechanism, using probability-based voting and fuzzy matching for span alignment.

Result: Ranked 1st in Arabic and Basque, 2nd in German, Swedish, and Finnish, and 3rd in Czech, Farsi, and French.

Conclusion: The approach effectively detects hallucinations in multilingual LLM outputs, demonstrating strong performance across diverse languages.

Abstract: This paper describes our submission for SemEval-2025 Task 3: Mu-SHROOM, the
Multilingual Shared-task on Hallucinations and Related Observable
Overgeneration Mistakes. The task involves detecting hallucinated spans in text
generated by instruction-tuned Large Language Models (LLMs) across multiple
languages. Our approach combines task-specific prompt engineering with an LLM
ensemble verification mechanism, where a primary model extracts hallucination
spans and three independent LLMs adjudicate their validity through
probability-based voting. This framework simulates the human annotation
workflow used in the shared task validation and test data. Additionally, fuzzy
matching refines span alignment. Our system ranked 1st in Arabic and Basque,
2nd in German, Swedish, and Finnish, and 3rd in Czech, Farsi, and French.

</details>


### [85] [EasyDistill: A Comprehensive Toolkit for Effective Knowledge Distillation of Large Language Models](https://arxiv.org/pdf/2505.20888)
*Chengyu Wang, Junbing Yan, Wenrui Cai, Yuanhao Yue, Jun Huang*

Main category: cs.CL

TL;DR: EasyDistill is a toolkit for knowledge distillation (KD) of large language models (LLMs), offering versatile functionalities like data synthesis, fine-tuning, and reinforcement learning, tailored for both System 1 and System 2 models.


<details>
  <summary>Details</summary>
Motivation: To make advanced KD techniques for LLMs more accessible and impactful by providing a modular, user-friendly toolkit.

Method: The framework includes data synthesis, supervised fine-tuning, ranking optimization, and reinforcement learning, with a modular design.

Result: EasyDistill offers robust distilled models, KD-based solutions, and open-sourced datasets, integrated into Alibaba Cloud's PAI.

Conclusion: EasyDistill enhances accessibility and impact of KD techniques in the NLP community.

Abstract: In this paper, we present EasyDistill, a comprehensive toolkit designed for
effective black-box and white-box knowledge distillation (KD) of large language
models (LLMs). Our framework offers versatile functionalities, including data
synthesis, supervised fine-tuning, ranking optimization, and reinforcement
learning techniques specifically tailored for KD scenarios. The toolkit
accommodates KD functionalities for both System 1 (fast, intuitive) and System
2 (slow, analytical) models. With its modular design and user-friendly
interface, EasyDistill empowers researchers and industry practitioners to
seamlessly experiment with and implement state-of-the-art KD strategies for
LLMs. In addition, EasyDistill provides a series of robust distilled models and
KD-based industrial solutions developed by us, along with the corresponding
open-sourced datasets, catering to a variety of use cases. Furthermore, we
describe the seamless integration of EasyDistill into Alibaba Cloud's Platform
for AI (PAI). Overall, the EasyDistill toolkit makes advanced KD techniques for
LLMs more accessible and impactful within the NLP community.

</details>


### [86] [A Stereotype Content Analysis on Color-related Social Bias in Large Vision Language Models](https://arxiv.org/pdf/2505.20901)
*Junhyuk Choi, Minju Kim, Yeseon Hong, Bugeun Kim*

Main category: cs.CL

TL;DR: This paper addresses limitations in evaluating stereotypes in large vision-language models (LVLMs) by introducing new metrics (SCM-based) and a benchmark (BASIC). It finds LVLMs exhibit color, gender, and race stereotypes, influenced by model architecture and size.


<details>
  <summary>Details</summary>
Motivation: To overcome shortcomings in existing metrics and datasets for assessing stereotypes in LVLMs, particularly the neglect of content words and color effects.

Method: Introduces SCM-based evaluation metrics and the BASIC benchmark to assess stereotypes in eight LVLMs.

Result: Three key findings: SCM metrics effectively capture stereotypes; LVLMs show color, gender, and race stereotypes; model architecture and size influence stereotypes.

Conclusion: The study highlights the need for better evaluation tools (like BASIC) and reveals the pervasive nature of stereotypes in LVLMs, urging further research.

Abstract: As large vision language models(LVLMs) rapidly advance, concerns about their
potential to learn and generate social biases and stereotypes are increasing.
Previous studies on LVLM's stereotypes face two primary limitations: metrics
that overlooked the importance of content words, and datasets that overlooked
the effect of color. To address these limitations, this study introduces new
evaluation metrics based on the Stereotype Content Model (SCM). We also propose
BASIC, a benchmark for assessing gender, race, and color stereotypes. Using SCM
metrics and BASIC, we conduct a study with eight LVLMs to discover stereotypes.
As a result, we found three findings. (1) The SCM-based evaluation is effective
in capturing stereotypes. (2) LVLMs exhibit color stereotypes in the output
along with gender and race ones. (3) Interaction between model architecture and
parameter sizes seems to affect stereotypes. We release BASIC publicly on
[anonymized for review].

</details>


### [87] [Towards Objective Fine-tuning: How LLMs' Prior Knowledge Causes Potential Poor Calibration?](https://arxiv.org/pdf/2505.20903)
*Ziming Wang, Zeyu Shi, Haoyi Zhou, Shiqi Gao, Qingyun Sun, Jianxin Li*

Main category: cs.CL

TL;DR: Fine-tuned LLMs often misalign confidence with performance due to prior knowledge. CogCalib, a cognition-aware framework, improves calibration by 57% ECE reduction in Llama3-8B.


<details>
  <summary>Details</summary>
Motivation: Study the impact of LLMs' prior knowledge on calibration during fine-tuning, as it remains understudied despite its potential harm.

Method: Propose CogCalib, a framework applying targeted learning strategies based on the model's prior knowledge.

Result: CogCalib reduces ECE by 57% in Llama3-8B and generalizes well to out-of-domain tasks.

Conclusion: CogCalib enhances calibration and reliability of LLMs, making them more trustworthy for critical applications.

Abstract: Fine-tuned Large Language Models (LLMs) often demonstrate poor calibration,
with their confidence scores misaligned with actual performance. While
calibration has been extensively studied in models trained from scratch, the
impact of LLMs' prior knowledge on calibration during fine-tuning remains
understudied. Our research reveals that LLMs' prior knowledge causes potential
poor calibration due to the ubiquitous presence of known data in real-world
fine-tuning, which appears harmful for calibration. Specifically, data aligned
with LLMs' prior knowledge would induce overconfidence, while new knowledge
improves calibration. Our findings expose a tension: LLMs' encyclopedic
knowledge, while enabling task versatility, undermines calibration through
unavoidable knowledge overlaps. To address this, we propose CogCalib, a
cognition-aware framework that applies targeted learning strategies according
to the model's prior knowledge. Experiments across 7 tasks using 3 LLM families
prove that CogCalib significantly improves calibration while maintaining
performance, achieving an average 57\% reduction in ECE compared to standard
fine-tuning in Llama3-8B. These improvements generalize well to out-of-domain
tasks, enhancing the objectivity and reliability of domain-specific LLMs, and
making them more trustworthy for critical human-AI interaction applications.

</details>


### [88] [Automated Privacy Information Annotation in Large Language Model Interactions](https://arxiv.org/pdf/2505.20910)
*Hang Zeng, Xiangyu Liu, Yong Hu, Chaoyue Niu, Fan Wu, Shaojie Tang, Guihai Chen*

Main category: cs.CL

TL;DR: The paper addresses privacy risks in LLM interactions by creating a multilingual dataset for privacy detection, using automated annotation and evaluating baseline methods.


<details>
  <summary>Details</summary>
Motivation: Users risk disclosing private info in LLM queries; existing methods aren't tailored for this scenario.

Method: Constructed a 249K-query dataset with automated annotation, designed evaluation metrics, and tested light-weight LLMs.

Result: Baseline methods show a performance gap for real-world LLM privacy detection needs.

Conclusion: Future research is needed for better local privacy detection methods using the provided dataset.

Abstract: Users interacting with large language models (LLMs) under their real
identifiers often unknowingly risk disclosing private information.
Automatically notifying users whether their queries leak privacy and which
phrases leak what private information has therefore become a practical need.
Existing privacy detection methods, however, were designed for different
objectives and application scenarios, typically tagging personally identifiable
information (PII) in anonymous content. In this work, to support the
development and evaluation of privacy detection models for LLM interactions
that are deployable on local user devices, we construct a large-scale
multilingual dataset with 249K user queries and 154K annotated privacy phrases.
In particular, we build an automated privacy annotation pipeline with
cloud-based strong LLMs to automatically extract privacy phrases from dialogue
datasets and annotate leaked information. We also design evaluation metrics at
the levels of privacy leakage, extracted privacy phrase, and privacy
information. We further establish baseline methods using light-weight LLMs with
both tuning-free and tuning-based methods, and report a comprehensive
evaluation of their performance. Evaluation results reveal a gap between
current performance and the requirements of real-world LLM applications,
motivating future research into more effective local privacy detection methods
grounded in our dataset.

</details>


### [89] [Automatic Transmission for LLM Tiers: Optimizing Cost and Accuracy in Large Language Models](https://arxiv.org/pdf/2505.20921)
*Injae Na, Keonwoong Noh, Woohwan Jung*

Main category: cs.CL

TL;DR: LLM-AT is a framework for automatically selecting the best LLM tier for subtasks, balancing cost and performance without training. It uses Starter, Generator, and Judge components to iteratively upgrade tiers until a valid response is obtained.


<details>
  <summary>Details</summary>
Motivation: To address the challenge of selecting suitable LLM tiers for complex NLP tasks to balance cost and performance.

Method: LLM-AT consists of Starter (selects initial tier), Generator (produces response), and Judge (evaluates response). It iteratively upgrades tiers if responses are invalid. An accuracy estimator predicts initial tier suitability.

Result: LLM-AT achieves superior performance while reducing costs, validated through experiments.

Conclusion: LLM-AT is a practical solution for real-world applications, optimizing LLM tier selection.

Abstract: LLM providers typically offer multiple LLM tiers, varying in performance and
price. As NLP tasks become more complex and modularized, selecting the suitable
LLM tier for each subtask is a key challenge to balance between cost and
performance. To address the problem, we introduce LLM Automatic Transmission
(LLM-AT) framework that automatically selects LLM tiers without training.
LLM-AT consists of Starter, Generator, and Judge. The starter selects the
initial LLM tier expected to solve the given question, the generator produces a
response using the LLM of the selected tier, and the judge evaluates the
validity of the response. If the response is invalid, LLM-AT iteratively
upgrades to a higher-tier model, generates a new response, and re-evaluates
until a valid response is obtained. Additionally, we propose accuracy
estimator, which enables the suitable initial LLM tier selection without
training. Given an input question, accuracy estimator estimates the expected
accuracy of each LLM tier by computing the valid response rate across top-k
similar queries from past inference records. Experiments demonstrate that
LLM-AT achieves superior performance while reducing costs, making it a
practical solution for real-world applications.

</details>


### [90] [Multi-objective Large Language Model Alignment with Hierarchical Experts](https://arxiv.org/pdf/2505.20925)
*Zhuo Li, Guodong Du, Weiyang Guo, Yigeng Zhou, Xiucheng Li, Wenya Wang, Fangming Liu, Yequan Wang, Deheng Ye, Min Zhang, Jing Li*

Main category: cs.CL

TL;DR: HoE is a lightweight, parameter-efficient method for aligning LLMs to multiple objectives without retraining, outperforming 15 baselines across diverse tasks.


<details>
  <summary>Details</summary>
Motivation: Existing alignment methods struggle with balancing conflicting human preferences and often require costly retraining.

Method: HoE uses a hierarchical approach with LoRA Experts, Router Experts, and Preference Routing to adapt LLMs across the Pareto frontier.

Result: HoE achieves superior performance on 14 objectives and 200 preferences across 6 benchmarks.

Conclusion: HoE provides an efficient, plug-and-play solution for aligning LLMs to diverse preferences without training.

Abstract: Aligning large language models (LLMs) to simultaneously satisfy multiple
objectives remains a significant challenge, especially given the diverse and
often conflicting nature of human preferences. Existing alignment methods
struggle to balance trade-offs effectively, often requiring costly retraining
or yielding suboptimal results across the Pareto frontier of preferences. In
this paper, we introduce \textit{HoE}(Hierarchical Mixture-of-Experts), a
\textit{lightweight}, \textit{parameter-efficient}, and \textit{plug-and-play}
approach that eliminates the need for model training, while enabling LLMs to
adapt across the entire Pareto frontier and accommodate diverse user
preferences. In particular, \textit{HoE} consists of three hierarchical
components: LoRA Experts, Router Experts and Preference Routing, reaching
optimal Pareto frontiers and achieving a trade-off between parameter size,
training cost, and performance. We evaluate \textit{HoE} across various tasks
on 14 objectives and 200 different preferences among 6 benchmarks,
demonstrating superior performance over 15 recent baselines. Code is available
in the supplementary materials.

</details>


### [91] [Information-Theoretic Complementary Prompts for Improved Continual Text Classification](https://arxiv.org/pdf/2505.20933)
*Duzhen Zhang, Yong Ren, Chenxing Li, Dong Yu, Tielin Zhang*

Main category: cs.CL

TL;DR: InfoComp introduces dual prompt spaces (P-Prompt and S-Prompt) for continual text classification, leveraging information theory to mitigate forgetting and enhance transfer.


<details>
  <summary>Details</summary>
Motivation: Existing methods overlook shared task-agnostic knowledge. Inspired by complementary learning systems theory, InfoComp aims to balance task-specific and task-invariant learning.

Method: InfoComp learns two prompt spaces (P-Prompt for task-specific, S-Prompt for task-invariant knowledge) using an information-theoretic framework with novel loss functions.

Result: Outperforms state-of-the-art methods on diverse benchmarks, effectively mitigating catastrophic forgetting and improving knowledge transfer.

Conclusion: InfoComp successfully addresses CTC challenges by explicitly separating and optimizing task-specific and task-invariant knowledge.

Abstract: Continual Text Classification (CTC) aims to continuously classify new text
data over time while minimizing catastrophic forgetting of previously acquired
knowledge. However, existing methods often focus on task-specific knowledge,
overlooking the importance of shared, task-agnostic knowledge. Inspired by the
complementary learning systems theory, which posits that humans learn
continually through the interaction of two systems -- the hippocampus,
responsible for forming distinct representations of specific experiences, and
the neocortex, which extracts more general and transferable representations
from past experiences -- we introduce Information-Theoretic Complementary
Prompts (InfoComp), a novel approach for CTC. InfoComp explicitly learns two
distinct prompt spaces: P(rivate)-Prompt and S(hared)-Prompt. These
respectively encode task-specific and task-invariant knowledge, enabling models
to sequentially learn classification tasks without relying on data replay. To
promote more informative prompt learning, InfoComp uses an
information-theoretic framework that maximizes mutual information between
different parameters (or encoded representations). Within this framework, we
design two novel loss functions: (1) to strengthen the accumulation of
task-specific knowledge in P-Prompt, effectively mitigating catastrophic
forgetting, and (2) to enhance the retention of task-invariant knowledge in
S-Prompt, improving forward knowledge transfer. Extensive experiments on
diverse CTC benchmarks show that our approach outperforms previous
state-of-the-art methods.

</details>


### [92] [On VLMs for Diverse Tasks in Multimodal Meme Classification](https://arxiv.org/pdf/2505.20937)
*Deepesh Gavit, Debajyoti Mazumder, Samiran Das, Jasabanta Patro*

Main category: cs.CL

TL;DR: The paper introduces a VLM-based approach for meme classification, combining vision-language models with fine-tuned LLMs to improve performance across sarcasm, offensive, and sentiment tasks.


<details>
  <summary>Details</summary>
Motivation: To enhance meme classification by leveraging VLMs for image understanding and fine-tuning LLMs for text analysis, addressing the limitations of existing methods.

Method: Benchmarking VLMs with tailored prompts, evaluating LoRA fine-tuning, and using VLM-generated interpretations to train smaller LLMs.

Result: Performance improved by 8.34%, 3.52%, and 26.24% for sarcasm, offensive, and sentiment classification, respectively.

Conclusion: The study highlights VLM strengths and limitations and proposes a novel strategy for meme understanding by combining VLMs and LLMs.

Abstract: In this paper, we present a comprehensive and systematic analysis of
vision-language models (VLMs) for disparate meme classification tasks. We
introduced a novel approach that generates a VLM-based understanding of meme
images and fine-tunes the LLMs on textual understanding of the embedded meme
text for improving the performance. Our contributions are threefold: (1)
Benchmarking VLMs with diverse prompting strategies purposely to each sub-task;
(2) Evaluating LoRA fine-tuning across all VLM components to assess performance
gains; and (3) Proposing a novel approach where detailed meme interpretations
generated by VLMs are used to train smaller language models (LLMs),
significantly improving classification. The strategy of combining VLMs with
LLMs improved the baseline performance by 8.34%, 3.52% and 26.24% for sarcasm,
offensive and sentiment classification, respectively. Our results reveal the
strengths and limitations of VLMs and present a novel strategy for meme
understanding.

</details>


### [93] [Research Community Perspectives on "Intelligence" and Large Language Models](https://arxiv.org/pdf/2505.20959)
*Bertram Højer, Terne Sasha Thorn Jakobsen, Anna Rogers, Stefan Heinrich*

Main category: cs.CL

TL;DR: Survey reveals NLP researchers' views on AI "intelligence," identifying generalization, adaptability, and reasoning as key criteria. Most don't see current NLP systems as intelligent or prioritize developing them.


<details>
  <summary>Details</summary>
Motivation: To clarify what researchers mean by "intelligence" in AI and NLP, given its widespread but ambiguous use.

Method: Conducted a survey of 303 researchers across NLP, ML, Cognitive Science, Linguistics, and Neuroscience.

Result: Top agreed criteria for intelligence: generalization, adaptability, reasoning. Only 29% see current NLP systems as intelligent; 16.2% prioritize developing intelligent systems.

Conclusion: The term "intelligence" in AI/NLP lacks consensus, and most researchers neither view current systems as intelligent nor prioritize creating them.

Abstract: Despite the widespread use of ''artificial intelligence'' (AI) framing in
Natural Language Processing (NLP) research, it is not clear what researchers
mean by ''intelligence''. To that end, we present the results of a survey on
the notion of ''intelligence'' among researchers and its role in the research
agenda. The survey elicited complete responses from 303 researchers from a
variety of fields including NLP, Machine Learning (ML), Cognitive Science,
Linguistics, and Neuroscience. We identify 3 criteria of intelligence that the
community agrees on the most: generalization, adaptability, & reasoning. Our
results suggests that the perception of the current NLP systems as
''intelligent'' is a minority position (29%). Furthermore, only 16.2% of the
respondents see developing intelligent systems as a research goal, and these
respondents are more likely to consider the current systems intelligent.

</details>


### [94] [Context-Aware Content Moderation for German Newspaper Comments](https://arxiv.org/pdf/2505.20963)
*Felix Krejca, Tobias Kietreiber, Alexander Buchelt, Sebastian Neumaier*

Main category: cs.CL

TL;DR: The paper develops context-aware models for hate speech detection in German newspaper forums, finding CNN and LSTM outperform ChatGPT-3.5 Turbo.


<details>
  <summary>Details</summary>
Motivation: Addressing the gap in research on German-language newspaper forums and the neglect of platform-specific context in existing studies.

Method: Using LSTM, CNN, and ChatGPT-3.5 Turbo with the One Million Posts Corpus, evaluating context-aware binary classification models.

Result: CNN and LSTM models benefit from contextual information and perform well, while ChatGPT's zero-shot classification underperforms.

Conclusion: Context-aware models (CNN, LSTM) are effective for hate speech detection in German newspaper forums, unlike ChatGPT.

Abstract: The increasing volume of online discussions requires advanced automatic
content moderation to maintain responsible discourse. While hate speech
detection on social media is well-studied, research on German-language
newspaper forums remains limited. Existing studies often neglect
platform-specific context, such as user history and article themes. This paper
addresses this gap by developing and evaluating binary classification models
for automatic content moderation in German newspaper forums, incorporating
contextual information. Using LSTM, CNN, and ChatGPT-3.5 Turbo, and leveraging
the One Million Posts Corpus from the Austrian newspaper Der Standard, we
assess the impact of context-aware models. Results show that CNN and LSTM
models benefit from contextual information and perform competitively with
state-of-the-art approaches. In contrast, ChatGPT's zero-shot classification
does not improve with added context and underperforms.

</details>


### [95] [Reason-Align-Respond: Aligning LLM Reasoning with Knowledge Graphs for KGQA](https://arxiv.org/pdf/2505.20971)
*Xiangqing Shen, Fanfan Wang, Rui Xia*

Main category: cs.CL

TL;DR: RAR integrates LLM reasoning with knowledge graphs for KGQA, improving accuracy and interpretability.


<details>
  <summary>Details</summary>
Motivation: Address LLM hallucinations and lack of factual grounding by leveraging structured knowledge from KGs.

Method: Combines a Reasoner, Aligner, and Responser, optimized via Expectation-Maximization.

Result: Achieves 93.3% and 91.0% Hit@1 on WebQSP and CWQ, with strong zero-shot generalization.

Conclusion: RAR effectively bridges LLMs and KGs, enhancing reasoning and factual accuracy.

Abstract: LLMs have demonstrated remarkable capabilities in complex reasoning tasks,
yet they often suffer from hallucinations and lack reliable factual grounding.
Meanwhile, knowledge graphs (KGs) provide structured factual knowledge but lack
the flexible reasoning abilities of LLMs. In this paper, we present
Reason-Align-Respond (RAR), a novel framework that systematically integrates
LLM reasoning with knowledge graphs for KGQA. Our approach consists of three
key components: a Reasoner that generates human-like reasoning chains, an
Aligner that maps these chains to valid KG paths, and a Responser that
synthesizes the final answer. We formulate this process as a probabilistic
model and optimize it using the Expectation-Maximization algorithm, which
iteratively refines the reasoning chains and knowledge paths. Extensive
experiments on multiple benchmarks demonstrate the effectiveness of RAR,
achieving state-of-the-art performance with Hit@1 scores of 93.3% and 91.0% on
WebQSP and CWQ respectively. Human evaluation confirms that RAR generates
high-quality, interpretable reasoning chains well-aligned with KG paths.
Furthermore, RAR exhibits strong zero-shot generalization capabilities and
maintains computational efficiency during inference.

</details>


### [96] [Personalized Query Auto-Completion for Long and Short-Term Interests with Adaptive Detoxification Generation](https://arxiv.org/pdf/2505.20966)
*Zhibo Wang, Xiaoze Jiang, Zhiheng Qin, Enyun Yu, Han Li*

Main category: cs.CL

TL;DR: The paper introduces LaD, a model for hierarchical personalized query auto-completion (QAC) with adaptive detoxification, addressing challenges of nuanced personalization and toxic content generation.


<details>
  <summary>Details</summary>
Motivation: Existing QAC systems lack hierarchical personalization and risk generating toxic content due to short or erroneous query prefixes.

Method: LaD captures long-term and short-term user interests hierarchically and uses Reject Preference Optimization (RPO) for adaptive detoxification.

Result: The model improves metrics significantly in industrial-scale tests and is deployed on Kuaishou search, serving millions of users.

Conclusion: LaD successfully addresses personalization and detoxification in QAC, proving effective in real-world deployment.

Abstract: Query auto-completion (QAC) plays a crucial role in modern search systems.
However, in real-world applications, there are two pressing challenges that
still need to be addressed. First, there is a need for hierarchical
personalized representations for users. Previous approaches have typically used
users' search behavior as a single, overall representation, which proves
inadequate in more nuanced generative scenarios. Additionally, query prefixes
are typically short and may contain typos or sensitive information, increasing
the likelihood of generating toxic content compared to traditional text
generation tasks. Such toxic content can degrade user experience and lead to
public relations issues. Therefore, the second critical challenge is
detoxifying QAC systems.
  To address these two limitations, we propose a novel model (LaD) that
captures personalized information from both long-term and short-term interests,
incorporating adaptive detoxification. In LaD, personalized information is
captured hierarchically at both coarse-grained and fine-grained levels. This
approach preserves as much personalized information as possible while enabling
online generation within time constraints. To move a futher step, we propose an
online training method based on Reject Preference Optimization (RPO). By
incorporating a special token [Reject] during both the training and inference
processes, the model achieves adaptive detoxification. Consequently, the
generated text presented to users is both non-toxic and relevant to the given
prefix. We conduct comprehensive experiments on industrial-scale datasets and
perform online A/B tests, delivering the largest single-experiment metric
improvement in nearly two years of our product. Our model has been deployed on
Kuaishou search, driving the primary traffic for hundreds of millions of active
users. The code is available at https://github.com/JXZe/LaD.

</details>


### [97] [Contrastive Learning on LLM Back Generation Treebank for Cross-domain Constituency Parsing](https://arxiv.org/pdf/2505.20976)
*Peiming Guo, Meishan Zhang, Jianling Li, Min Zhang, Yue Zhang*

Main category: cs.CL

TL;DR: The paper proposes LLM back generation for cross-domain constituency treebank creation and a contrastive learning strategy, achieving state-of-the-art results.


<details>
  <summary>Details</summary>
Motivation: Cross-domain constituency parsing lacks sufficient multi-domain treebanks, limiting performance.

Method: Uses LLM back generation to fill incomplete trees and span-level contrastive learning for pre-training.

Result: Achieves state-of-the-art performance on five MCTB domains.

Conclusion: The approach effectively addresses cross-domain parsing challenges with LLM-generated treebanks and contrastive learning.

Abstract: Cross-domain constituency parsing is still an unsolved challenge in
computational linguistics since the available multi-domain constituency
treebank is limited. We investigate automatic treebank generation by large
language models (LLMs) in this paper. The performance of LLMs on constituency
parsing is poor, therefore we propose a novel treebank generation method, LLM
back generation, which is similar to the reverse process of constituency
parsing. LLM back generation takes the incomplete cross-domain constituency
tree with only domain keyword leaf nodes as input and fills the missing words
to generate the cross-domain constituency treebank. Besides, we also introduce
a span-level contrastive learning pre-training strategy to make full use of the
LLM back generation treebank for cross-domain constituency parsing. We verify
the effectiveness of our LLM back generation treebank coupled with contrastive
learning pre-training on five target domains of MCTB. Experimental results show
that our approach achieves state-of-the-art performance on average results
compared with various baselines.

</details>


### [98] [Who Reasons in the Large Language Models?](https://arxiv.org/pdf/2505.20993)
*Jie Shao, Jianxin Wu*

Main category: cs.CL

TL;DR: The paper investigates whether reasoning in LLMs stems from specific modules, focusing on the output projection (oproj) in MHSA. It introduces SfN tools to analyze this, finding oproj central to reasoning while other modules aid dialogue.


<details>
  <summary>Details</summary>
Motivation: To understand if reasoning in LLMs is localized (e.g., in oproj) or distributed, addressing the opacity of how new capabilities like math reasoning are added.

Method: Introduces Stethoscope for Networks (SfN), a diagnostic toolset to probe LLM internals, specifically analyzing oproj's role.

Result: Evidence suggests oproj is key for reasoning, while other modules support fluent dialogue.

Conclusion: This insight improves LLM interpretability and could guide targeted training for specialized models.

Abstract: Despite the impressive performance of large language models (LLMs), the
process of endowing them with new capabilities--such as mathematical
reasoning--remains largely empirical and opaque. A critical open question is
whether reasoning abilities stem from the entire model, specific modules, or
are merely artifacts of overfitting. In this work, we hypothesize that the
reasoning capabilities in well-trained LLMs are primarily attributed to the
output projection module (oproj) in the Transformer's multi-head self-attention
(MHSA) mechanism. To support this hypothesis, we introduce Stethoscope for
Networks (SfN), a suite of diagnostic tools designed to probe and analyze the
internal behaviors of LLMs. Using SfN, we provide both circumstantial and
empirical evidence suggesting that oproj plays a central role in enabling
reasoning, whereas other modules contribute more to fluent dialogue. These
findings offer a new perspective on LLM interpretability and open avenues for
more targeted training strategies, potentially enabling more efficient and
specialized LLMs.

</details>


### [99] [Evaluating and Steering Modality Preferences in Multimodal Large Language Model](https://arxiv.org/pdf/2505.20977)
*Yu Zhang, Jinlong Ma, Yongshuai Hou, Xuefeng Bai, Kehai Chen, Yang Xiang, Jun Yu, Min Zhang*

Main category: cs.CL

TL;DR: The paper investigates modality preference in multimodal large language models (MLLMs) using a controlled benchmark (MC²) and finds clear bias. It proposes a method to steer this preference without fine-tuning.


<details>
  <summary>Details</summary>
Motivation: To understand if MLLMs exhibit modality preference when processing conflicting multimodal evidence and how to control it.

Method: Built the MC² benchmark to evaluate modality preference, analyzed latent representations, and proposed a probing and steering method based on representation engineering.

Result: All 18 tested MLLMs showed modality bias, which can be influenced. The proposed method effectively steers preference and improves downstream tasks like hallucination mitigation.

Conclusion: Modality preference in MLLMs is controllable via representation engineering, offering practical benefits for applications.

Abstract: Multimodal large language models (MLLMs) have achieved remarkable performance
on complex tasks with multimodal context. However, it is still understudied
whether they exhibit modality preference when processing multimodal contexts.
To study this question, we first build a \textbf{MC\textsuperscript{2}}
benchmark under controlled evidence conflict scenarios to systematically
evaluate modality preference, which is the tendency to favor one modality over
another when making decisions based on multimodal conflicting evidence. Our
extensive evaluation reveals that all 18 tested MLLMs generally demonstrate
clear modality bias, and modality preference can be influenced by external
interventions. An in-depth analysis reveals that the preference direction can
be captured within the latent representations of MLLMs. Built on this, we
propose a probing and steering method based on representation engineering to
explicitly control modality preference without additional fine-tuning or
carefully crafted prompts. Our method effectively amplifies modality preference
toward a desired direction and applies to downstream tasks such as
hallucination mitigation and multimodal machine translation, yielding promising
improvements.

</details>


### [100] [Articulatory strategy in vowel production as a basis for speaker discrimination](https://arxiv.org/pdf/2505.20995)
*Justin J. H. Lo, Patrycja Strycharczuk, Sam Kirkham*

Main category: cs.CL

TL;DR: The study investigates if articulatory strategies in vowel production can discriminate speakers, using tongue shape data from 40 English speakers. Tongue size and anterior shape variations were most effective for discrimination.


<details>
  <summary>Details</summary>
Motivation: To determine if speaker-specific articulatory strategies in vowel production can be used for speaker discrimination.

Method: Generalised Procrustes Analyses of tongue shape data from 40 speakers, assessing discriminatory potential using likelihood ratios.

Result: Tongue size was the most discriminatory feature, with anterior tongue shape variations outperforming posterior ones. Shape-only information matched size-and-shape when features didn't co-vary.

Conclusion: Articulatory strategies, especially tongue size and anterior shape, can discriminate speakers, but co-variation affects performance.

Abstract: The way speakers articulate is well known to be variable across individuals
while at the same time subject to anatomical and biomechanical constraints. In
this study, we ask whether articulatory strategy in vowel production can be
sufficiently speaker-specific to form the basis for speaker discrimination. We
conducted Generalised Procrustes Analyses of tongue shape data from 40 English
speakers from the North West of England, and assessed the
speaker-discriminatory potential of orthogonal tongue shape features within the
framework of likelihood ratios. Tongue size emerged as the individual dimension
with the strongest discriminatory power, while tongue shape variation in the
more anterior part of the tongue generally outperformed tongue shape variation
in the posterior part. When considered in combination, shape-only information
may offer comparable levels of speaker specificity to size-and-shape
information, but only when features do not exhibit speaker-level co-variation.

</details>


### [101] [Uncertainty Unveiled: Can Exposure to More In-context Examples Mitigate Uncertainty for Large Language Models?](https://arxiv.org/pdf/2505.21003)
*Yifei Wang, Yu Sheng, Linjing Li, Daniel Zeng*

Main category: cs.CL

TL;DR: The paper explores how increasing in-context examples affects the trustworthiness of long-context in-context learning (ICL), focusing on predictive uncertainty. It finds that more examples reduce uncertainty by lowering epistemic uncertainty (EU), especially in complex tasks after mitigating noise.


<details>
  <summary>Details</summary>
Motivation: To investigate the underexplored impact of additional in-context examples on the trustworthiness of ICL, particularly predictive uncertainty.

Method: Systematically quantify uncertainty in ICL with varying shot counts, decompose uncertainty, and analyze epistemic uncertainty (EU). Examine internal confidence evolution across layers.

Result: Additional examples reduce total uncertainty by decreasing EU, enhancing performance. Complex tasks benefit only after addressing noise from longer inputs.

Conclusion: More examples improve trustworthiness by reducing uncertainty, with EU playing a key role. Layer-wise confidence analysis reveals mechanisms behind uncertainty reduction.

Abstract: Recent advances in handling long sequences have facilitated the exploration
of long-context in-context learning (ICL). While much of the existing research
emphasizes performance improvements driven by additional in-context examples,
the influence on the trustworthiness of generated responses remains
underexplored. This paper addresses this gap by investigating how increased
examples influence predictive uncertainty, an essential aspect in
trustworthiness. We begin by systematically quantifying the uncertainty of ICL
with varying shot counts, analyzing the impact of example quantity. Through
uncertainty decomposition, we introduce a novel perspective on performance
enhancement, with a focus on epistemic uncertainty (EU). Our results reveal
that additional examples reduce total uncertainty in both simple and complex
tasks by injecting task-specific knowledge, thereby diminishing EU and
enhancing performance. For complex tasks, these advantages emerge only after
addressing the increased noise and uncertainty associated with longer inputs.
Finally, we explore the evolution of internal confidence across layers,
unveiling the mechanisms driving the reduction in uncertainty.

</details>


### [102] [FCKT: Fine-Grained Cross-Task Knowledge Transfer with Semantic Contrastive Learning for Targeted Sentiment Analysis](https://arxiv.org/pdf/2505.21040)
*Wei Chen, Zhao Zhang, Meng Yuan, Kepeng Xu, Fuzhen Zhuang*

Main category: cs.CL

TL;DR: The paper proposes FCKT, a fine-grained cross-task knowledge transfer framework for targeted sentiment analysis (TSA), addressing limitations of coarse-grained transfer by incorporating aspect-level information to improve performance.


<details>
  <summary>Details</summary>
Motivation: Existing TSA methods rely on coarse-grained knowledge transfer, neglecting contextual cues and leading to negative transfer. The paper aims to enhance performance by fine-grained control over aspect-sentiment relationships.

Method: The FCKT framework explicitly incorporates aspect-level information into sentiment prediction, enabling fine-grained knowledge transfer.

Result: Experiments on three datasets show FCKT outperforms baselines and large language models, mitigating negative transfer.

Conclusion: FCKT effectively enhances TSA performance by fine-grained knowledge transfer, with source code available for reproducibility.

Abstract: In this paper, we address the task of targeted sentiment analysis (TSA),
which involves two sub-tasks, i.e., identifying specific aspects from reviews
and determining their corresponding sentiments. Aspect extraction forms the
foundation for sentiment prediction, highlighting the critical dependency
between these two tasks for effective cross-task knowledge transfer. While most
existing studies adopt a multi-task learning paradigm to align task-specific
features in the latent space, they predominantly rely on coarse-grained
knowledge transfer. Such approaches lack fine-grained control over
aspect-sentiment relationships, often assuming uniform sentiment polarity
within related aspects. This oversimplification neglects contextual cues that
differentiate sentiments, leading to negative transfer. To overcome these
limitations, we propose FCKT, a fine-grained cross-task knowledge transfer
framework tailored for TSA. By explicitly incorporating aspect-level
information into sentiment prediction, FCKT achieves fine-grained knowledge
transfer, effectively mitigating negative transfer and enhancing task
performance. Experiments on three datasets, including comparisons with various
baselines and large language models (LLMs), demonstrate the effectiveness of
FCKT. The source code is available on https://github.com/cwei01/FCKT.

</details>


### [103] [LLMs are Frequency Pattern Learners in Natural Language Inference](https://arxiv.org/pdf/2505.21011)
*Liang Cheng, Zhaowei Wang, Mark Steedman*

Main category: cs.CL

TL;DR: Fine-tuning LLMs on NLI datasets improves performance by exploiting frequency biases in predicates, but this reliance harms adversarial case performance.


<details>
  <summary>Details</summary>
Motivation: To understand what LLMs learn during fine-tuning on NLI tasks and the role of frequency biases in their performance.

Method: Analyzed predicate frequencies in NLI datasets, tested models on bias-consistent and adversarial cases, and examined hyponym-hypernym frequencies from WordNet.

Result: LLMs rely on frequency biases for inference, with fine-tuned models showing increased dependence, leading to poor adversarial performance. Frequency bias correlates with textual entailment.

Conclusion: Frequency patterns learned during fine-tuning explain performance improvements but also highlight limitations in adversarial scenarios.

Abstract: While fine-tuning LLMs on NLI corpora improves their inferential performance,
the underlying mechanisms driving this improvement remain largely opaque. In
this work, we conduct a series of experiments to investigate what LLMs actually
learn during fine-tuning. We begin by analyzing predicate frequencies in
premises and hypotheses across NLI datasets and identify a consistent frequency
bias, where predicates in hypotheses occur more frequently than those in
premises for positive instances. To assess the impact of this bias, we evaluate
both standard and NLI fine-tuned LLMs on bias-consistent and bias-adversarial
cases. We find that LLMs exploit frequency bias for inference and perform
poorly on adversarial instances. Furthermore, fine-tuned LLMs exhibit
significantly increased reliance on this bias, suggesting that they are
learning these frequency patterns from datasets. Finally, we compute the
frequencies of hyponyms and their corresponding hypernyms from WordNet,
revealing a correlation between frequency bias and textual entailment. These
findings help explain why learning frequency patterns can enhance model
performance on inference tasks.

</details>


### [104] [Def-DTS: Deductive Reasoning for Open-domain Dialogue Topic Segmentation](https://arxiv.org/pdf/2505.21033)
*Seungmin Lee, Yongsang Yoo, Minhwa Jung, Min Song*

Main category: cs.CL

TL;DR: Def-DTS uses LLM-based deductive reasoning to improve Dialogue Topic Segmentation (DTS), addressing data shortage and labeling ambiguity while outperforming existing methods.


<details>
  <summary>Details</summary>
Motivation: DTS suffers from data shortage, labeling ambiguity, and complex solutions. LLMs and reasoning strategies are underutilized in DTS.

Method: Def-DTS employs multi-step deductive reasoning, structured prompting for context summarization, utterance intent classification, and topic shift detection.

Result: Def-DTS outperforms traditional and state-of-the-art methods, reducing type 2 errors and enabling autolabeling potential.

Conclusion: LLM reasoning techniques enhance DTS performance, with Def-DTS offering a robust solution for open-domain dialogue segmentation.

Abstract: Dialogue Topic Segmentation (DTS) aims to divide dialogues into coherent
segments. DTS plays a crucial role in various NLP downstream tasks, but suffers
from chronic problems: data shortage, labeling ambiguity, and incremental
complexity of recently proposed solutions. On the other hand, Despite advances
in Large Language Models (LLMs) and reasoning strategies, these have rarely
been applied to DTS. This paper introduces Def-DTS: Deductive Reasoning for
Open-domain Dialogue Topic Segmentation, which utilizes LLM-based multi-step
deductive reasoning to enhance DTS performance and enable case study using
intermediate result. Our method employs a structured prompting approach for
bidirectional context summarization, utterance intent classification, and
deductive topic shift detection. In the intent classification process, we
propose the generalizable intent list for domain-agnostic dialogue intent
classification. Experiments in various dialogue settings demonstrate that
Def-DTS consistently outperforms traditional and state-of-the-art approaches,
with each subtask contributing to improved performance, particularly in
reducing type 2 error. We also explore the potential for autolabeling,
emphasizing the importance of LLM reasoning techniques in DTS.

</details>


### [105] [Visual Cues Enhance Predictive Turn-Taking for Two-Party Human Interaction](https://arxiv.org/pdf/2505.21043)
*Sam O'Connor Russell, Naomi Harte*

Main category: cs.CL

TL;DR: MM-VAP, a multimodal predictive turn-taking model, outperforms audio-only models by incorporating visual cues like facial expressions, head pose, and gaze, achieving 84% accuracy in turn prediction.


<details>
  <summary>Details</summary>
Motivation: Most predictive turn-taking models rely solely on speech, missing the multimodal nature of human interaction. This work aims to improve accuracy by integrating visual cues.

Method: MM-VAP combines speech with visual features (facial expression, head pose, gaze) and groups turn transitions by silence duration. An ablation study identifies key features.

Result: MM-VAP achieves 84% hold/shift prediction accuracy, outperforming audio-only models (79%). Facial expressions contribute most to performance.

Conclusion: Visual cues are vital for accurate turn-taking prediction in visible interactions. The study validates automatic speech alignment for training and highlights the need for multimodal approaches.

Abstract: Turn-taking is richly multimodal. Predictive turn-taking models (PTTMs)
facilitate naturalistic human-robot interaction, yet most rely solely on
speech. We introduce MM-VAP, a multimodal PTTM which combines speech with
visual cues including facial expression, head pose and gaze. We find that it
outperforms the state-of-the-art audio-only in videoconferencing interactions
(84% vs. 79% hold/shift prediction accuracy). Unlike prior work which
aggregates all holds and shifts, we group by duration of silence between turns.
This reveals that through the inclusion of visual features, MM-VAP outperforms
a state-of-the-art audio-only turn-taking model across all durations of speaker
transitions. We conduct a detailed ablation study, which reveals that facial
expression features contribute the most to model performance. Thus, our working
hypothesis is that when interlocutors can see one another, visual cues are
vital for turn-taking and must therefore be included for accurate turn-taking
prediction. We additionally validate the suitability of automatic speech
alignment for PTTM training using telephone speech. This work represents the
first comprehensive analysis of multimodal PTTMs. We discuss implications for
future work and make all code publicly available.

</details>


### [106] [BLUCK: A Benchmark Dataset for Bengali Linguistic Understanding and Cultural Knowledge](https://arxiv.org/pdf/2505.21092)
*Daeen Kabir, Minhajur Rahman Chowdhury Mahim, Sheikh Shafayat, Adnan Sadik, Arian Ahmed, Eunsu Kim, Alice Oh*

Main category: cs.CL

TL;DR: BLUCK is a new dataset for evaluating LLMs' Bengali linguistic and cultural knowledge, showing gaps in phonetics and mid-resource status.


<details>
  <summary>Details</summary>
Motivation: To assess LLMs' performance in Bengali linguistic understanding and cultural knowledge, addressing the lack of native-centric benchmarks.

Method: Created a dataset of 2366 MCQs from exams, benchmarked 9 LLMs (6 proprietary, 3 open-source).

Result: LLMs perform reasonably but struggle with Bengali phonetics; Bengali is a mid-resource language.

Conclusion: BLUCK is the first native Bengali-centric benchmark, highlighting gaps in LLM performance.

Abstract: In this work, we introduce BLUCK, a new dataset designed to measure the
performance of Large Language Models (LLMs) in Bengali linguistic understanding
and cultural knowledge. Our dataset comprises 2366 multiple-choice questions
(MCQs) carefully curated from compiled collections of several college and job
level examinations and spans 23 categories covering knowledge on Bangladesh's
culture and history and Bengali linguistics. We benchmarked BLUCK using 6
proprietary and 3 open-source LLMs - including GPT-4o, Claude-3.5-Sonnet,
Gemini-1.5-Pro, Llama-3.3-70B-Instruct, and DeepSeekV3. Our results show that
while these models perform reasonably well overall, they, however, struggles in
some areas of Bengali phonetics. Although current LLMs' performance on Bengali
cultural and linguistic contexts is still not comparable to that of mainstream
languages like English, our results indicate Bengali's status as a mid-resource
language. Importantly, BLUCK is also the first MCQ-based evaluation benchmark
that is centered around native Bengali culture, history, and linguistics.

</details>


### [107] [Predicting Implicit Arguments in Procedural Video Instructions](https://arxiv.org/pdf/2505.21068)
*Anil Batra, Laura Sevilla-Lara, Marcus Rohrbach, Frank Keller*

Main category: cs.CL

TL;DR: The paper introduces Implicit-VidSRL, a dataset for improving Semantic Role Labeling (SRL) by addressing implicit arguments in procedural texts, and proposes iSRL-Qwen2-VL, a model outperforming GPT-4o.


<details>
  <summary>Details</summary>
Motivation: Procedural texts often have implicit arguments, which prior SRL benchmarks miss, leading to incomplete understanding. The paper aims to enhance contextual reasoning in AI for such texts.

Method: The authors create Implicit-VidSRL, a multimodal dataset for inferring implicit and explicit arguments in cooking procedures. They evaluate multimodal LLMs and propose iSRL-Qwen2-VL.

Result: Multimodal LLMs struggle with predicting implicit arguments. iSRL-Qwen2-VL improves F1-scores by 17% for what-implicit and 14.7% for where/with-implicit roles over GPT-4o.

Conclusion: The work advances SRL by addressing implicit arguments, demonstrating the effectiveness of iSRL-Qwen2-VL in multimodal procedural contexts.

Abstract: Procedural texts help AI enhance reasoning about context and action
sequences. Transforming these into Semantic Role Labeling (SRL) improves
understanding of individual steps by identifying predicate-argument structure
like {verb,what,where/with}. Procedural instructions are highly elliptic, for
instance, (i) add cucumber to the bowl and (ii) add sliced tomatoes, the second
step's where argument is inferred from the context, referring to where the
cucumber was placed. Prior SRL benchmarks often miss implicit arguments,
leading to incomplete understanding. To address this, we introduce
Implicit-VidSRL, a dataset that necessitates inferring implicit and explicit
arguments from contextual information in multimodal cooking procedures. Our
proposed dataset benchmarks multimodal models' contextual reasoning, requiring
entity tracking through visual changes in recipes. We study recent multimodal
LLMs and reveal that they struggle to predict implicit arguments of what and
where/with from multi-modal procedural data given the verb. Lastly, we propose
iSRL-Qwen2-VL, which achieves a 17% relative improvement in F1-score for
what-implicit and a 14.7% for where/with-implicit semantic roles over GPT-4o.

</details>


### [108] [Thinker: Learning to Think Fast and Slow](https://arxiv.org/pdf/2505.21097)
*Stephen Chung, Wenyu Du, Jie Fu*

Main category: cs.CL

TL;DR: Applying Reinforcement Learning (RL) to QA tasks improves LLM reasoning. A four-stage task (Fast Thinking, Verification, Slow Thinking, Summarization) enhances accuracy and efficiency, showing distinct benefits of intuition and deliberative reasoning.


<details>
  <summary>Details</summary>
Motivation: To address imprecise search behavior and lack of confidence in LLMs during QA tasks, inspired by Dual Process Theory.

Method: Introduces a four-stage QA task: Fast Thinking (strict token budget), Verification (evaluates initial response), Slow Thinking (refines response), Summarization (distills refined steps).

Result: Improved accuracy from 24.9% to 27.9% for Qwen2.5-1.5B and 45.9% to 49.8% for DeepSeek-R1-Qwen-1.5B. Fast Thinking alone achieved 26.8% accuracy with <1000 tokens.

Conclusion: Intuition and deliberative reasoning are complementary systems; targeted training enhances LLM performance in QA tasks.

Abstract: Recent studies show that the reasoning capabilities of Large Language Models
(LLMs) can be improved by applying Reinforcement Learning (RL) to
question-answering (QA) tasks in areas such as math and coding. With a long
context length, LLMs may learn to perform search, as indicated by the
self-correction behavior observed in DeepSeek R1. However, this search behavior
is often imprecise and lacks confidence, resulting in long, redundant responses
and highlighting deficiencies in intuition and verification. Inspired by the
Dual Process Theory in psychology, we introduce a simple modification to the QA
task that includes four stages: Fast Thinking, where the LLM must answer within
a strict token budget; Verification, where the model evaluates its initial
response; Slow Thinking, where it refines the initial response with more
deliberation; and Summarization, where it distills the refinement from the
previous stage into precise steps. Our proposed task improves average accuracy
from 24.9% to 27.9% for Qwen2.5-1.5B, and from 45.9% to 49.8% for
DeepSeek-R1-Qwen-1.5B. Notably, for Qwen2.5-1.5B, the Fast Thinking mode alone
achieves 26.8% accuracy using fewer than 1000 tokens, demonstrating substantial
inference efficiency gains. These findings suggest that intuition and
deliberative reasoning are distinct, complementary systems benefiting from
targeted training.

</details>


### [109] [Faithfulness-Aware Uncertainty Quantification for Fact-Checking the Output of Retrieval Augmented Generation](https://arxiv.org/pdf/2505.21072)
*Ekaterina Fadeeva, Aleksandr Rubashevskii, Roman Vashurin, Shehzaad Dhuliawala, Artem Shelmanov, Timothy Baldwin, Preslav Nakov, Mrinmaya Sachan, Maxim Panov*

Main category: cs.CL

TL;DR: FRANQ is a new method for detecting hallucinations in RAG systems by distinguishing between factuality and faithfulness, outperforming existing techniques.


<details>
  <summary>Details</summary>
Motivation: RAG systems often produce hallucinations, and current methods misclassify factual statements as errors if not directly supported by retrieved context.

Method: FRANQ uses Uncertainty Quantification (UQ) techniques to assess factuality based on faithfulness to retrieved context, evaluated on a new QA dataset.

Result: FRANQ detects factual errors more accurately than existing methods in both long- and short-form QA tasks.

Conclusion: FRANQ improves hallucination detection in RAG systems by separating factuality from faithfulness, validated through extensive experiments.

Abstract: Large Language Models (LLMs) enhanced with external knowledge retrieval, an
approach known as Retrieval-Augmented Generation (RAG), have shown strong
performance in open-domain question answering. However, RAG systems remain
susceptible to hallucinations: factually incorrect outputs that may arise
either from inconsistencies in the model's internal knowledge or incorrect use
of the retrieved context. Existing approaches often conflate factuality with
faithfulness to the retrieved context, misclassifying factually correct
statements as hallucinations if they are not directly supported by the
retrieval. In this paper, we introduce FRANQ (Faithfulness-based Retrieval
Augmented UNcertainty Quantification), a novel method for hallucination
detection in RAG outputs. FRANQ applies different Uncertainty Quantification
(UQ) techniques to estimate factuality based on whether a statement is faithful
to the retrieved context or not. To evaluate FRANQ and other UQ techniques for
RAG, we present a new long-form Question Answering (QA) dataset annotated for
both factuality and faithfulness, combining automated labeling with manual
validation of challenging examples. Extensive experiments on long- and
short-form QA across multiple datasets and LLMs show that FRANQ achieves more
accurate detection of factual errors in RAG-generated responses compared to
existing methods.

</details>


### [110] [A Lightweight Multi-Expert Generative Language Model System for Engineering Information and Knowledge Extraction](https://arxiv.org/pdf/2505.21109)
*Bogdan Bogachov, Yaoyao Fiona Zhao*

Main category: cs.CL

TL;DR: The paper introduces Small Language Graph (SLG), a lightweight domain adaptation method for language models, addressing computational intensity and hallucination issues. SLG outperforms traditional fine-tuning in speed and accuracy.


<details>
  <summary>Details</summary>
Motivation: Existing domain adaptation methods are computationally intensive and prone to hallucination, especially in engineering contexts requiring precise text generation.

Method: SLG uses a graph structure with lightweight expert nodes (small language models fine-tuned on specific texts) to reduce computational demands and improve accuracy.

Result: SLG achieved 3x higher Exact Match scores and 1.7x faster fine-tuning compared to conventional methods.

Conclusion: SLG enables smaller companies to use generative AI affordably and suggests potential for distributed AI systems, reducing reliance on centralized compute clusters.

Abstract: Despite recent advancements in domain adaptation techniques for large
language models, these methods remain computationally intensive, and the
resulting models can still exhibit hallucination issues. Most existing
adaptation methods do not prioritize reducing the computational resources
required for fine-tuning and inference of language models. Hallucination issues
have gradually decreased with each new model release. However, they remain
prevalent in engineering contexts, where generating well-structured text with
minimal errors and inconsistencies is critical. This work introduces a novel
approach called the Small Language Graph (SLG), which is a lightweight
adaptation solution designed to address the two key challenges outlined above.
The system is structured in the form of a graph, where each node represents a
lightweight expert - a small language model fine-tuned on specific and concise
texts. The results of this study have shown that SLG was able to surpass
conventional fine-tuning methods on the Exact Match metric by 3 times.
Additionally, the fine-tuning process was 1.7 times faster compared to that of
a larger stand-alone language model. These findings introduce a potential for
small to medium-sized engineering companies to confidently use generative AI
technologies, such as LLMs, without the necessity to invest in expensive
computational resources. Also, the graph architecture and the small size of
expert nodes offer a possible opportunity for distributed AI systems, thus
potentially diverting the global need for expensive centralized compute
clusters.

</details>


### [111] [LLMs Think, But Not In Your Flow: Reasoning-Level Personalization for Black-Box Large Language Models](https://arxiv.org/pdf/2505.21082)
*Jieyong Kim, Tongyoung Kim, Soonjin Yoon, Jaehyung Kim, Dongha Lee*

Main category: cs.CL

TL;DR: RPM is a framework for reasoning-level personalization in black-box LLMs, outperforming response-level methods by aligning model reasoning with user-specific logic.


<details>
  <summary>Details</summary>
Motivation: Black-box LLMs lack personalization in reasoning, producing generalized responses. RPM addresses this by tailoring reasoning processes to user preferences.

Method: RPM constructs user-specific factors from history, builds personalized reasoning paths, and retrieves reasoning-aligned examples for inference.

Result: RPM consistently outperforms response-level personalization methods, improving accuracy and interpretability.

Conclusion: Reasoning-level personalization enhances black-box LLM performance by grounding outputs in user-specific logic.

Abstract: Large language models (LLMs) have recently achieved impressive performance
across a wide range of natural language tasks and are now widely used in
real-world applications. Among them, black-box LLMs--served via APIs without
access to model internals--are especially dominant due to their scalability and
ease of deployment. Despite their strong capabilities, these models typically
produce generalized responses that overlook personal preferences and reasoning
styles. This has led to growing interest in black-box LLM personalization,
which aims to tailor model outputs to user-specific context without modifying
model parameters. However, existing approaches primarily focus on
response-level personalization, attempting to match final outputs without
modeling personal thought process. To address this limitation, we propose RPM,
a framework for reasoning-level personalization that aligns the model's
reasoning process with a user's personalized logic. RPM first constructs
statistical user-specific factors by extracting and grouping
response-influential features from user history. It then builds personalized
reasoning paths that reflect how these factors are used in context. In the
inference stage, RPM retrieves reasoning-aligned examples for new queries via
feature-level similarity and performs inference conditioned on the structured
factors and retrieved reasoning paths, enabling the model to follow
user-specific reasoning trajectories. This reasoning-level personalization
enhances both predictive accuracy and interpretability by grounding model
outputs in user-specific logic through structured information. Extensive
experiments across diverse tasks show that RPM consistently outperforms
response-level personalization methods, demonstrating the effectiveness of
reasoning-level personalization in black-box LLMs.

</details>


### [112] [Will It Still Be True Tomorrow? Multilingual Evergreen Question Classification to Improve Trustworthy QA](https://arxiv.org/pdf/2505.21115)
*Sergey Pletenev, Maria Marina, Nikolay Ivanov, Daria Galimzianova, Nikita Krayko, Mikhail Salnikov, Vasily Konovalov, Alexander Panchenko, Viktor Moskvoretskii*

Main category: cs.CL

TL;DR: The paper introduces EverGreenQA, a multilingual dataset for evaluating and training LLMs on question temporality (evergreen vs. mutable). It benchmarks 12 LLMs and trains EG-E5, a classifier, showing applications in self-knowledge estimation, dataset filtering, and explaining GPT-4 retrieval.


<details>
  <summary>Details</summary>
Motivation: LLMs often hallucinate in QA tasks, and question temporality (evergreen/mutable) is an underexplored factor contributing to this issue.

Method: Introduces EverGreenQA dataset, benchmarks 12 LLMs for explicit/implicit temporality encoding, and trains EG-E5, a multilingual classifier.

Result: EG-E5 achieves state-of-the-art performance. The study demonstrates practical applications of evergreen classification.

Conclusion: EverGreenQA and EG-E5 effectively address LLM hallucination by leveraging question temporality, with broad utility in QA tasks.

Abstract: Large Language Models (LLMs) often hallucinate in question answering (QA)
tasks. A key yet underexplored factor contributing to this is the temporality
of questions -- whether they are evergreen (answers remain stable over time) or
mutable (answers change). In this work, we introduce EverGreenQA, the first
multilingual QA dataset with evergreen labels, supporting both evaluation and
training. Using EverGreenQA, we benchmark 12 modern LLMs to assess whether they
encode question temporality explicitly (via verbalized judgments) or implicitly
(via uncertainty signals). We also train EG-E5, a lightweight multilingual
classifier that achieves SoTA performance on this task. Finally, we demonstrate
the practical utility of evergreen classification across three applications:
improving self-knowledge estimation, filtering QA datasets, and explaining
GPT-4o retrieval behavior.

</details>


### [113] [M-Wanda: Improving One-Shot Pruning for Multilingual LLMs](https://arxiv.org/pdf/2505.21171)
*Rochelle Choenni, Ivan Titov*

Main category: cs.CL

TL;DR: The paper explores the trade-offs between multilingual performance and model sparsity in LLMs, proposing M-Wanda, a language-aware pruning method to mitigate performance loss.


<details>
  <summary>Details</summary>
Motivation: To address the performance loss in multilingual LLMs due to pruning, while maintaining efficiency.

Method: Introduces M-Wanda, a pruning method that uses language-aware activation statistics and dynamic layerwise sparsity adjustment.

Result: M-Wanda improves multilingual performance at minimal cost, outperforming standard pruning methods.

Conclusion: The work pioneers optimizing pruning for multilingual performance, encouraging future research in this area.

Abstract: Multilingual LLM performance is often critically dependent on model size.
With an eye on efficiency, this has led to a surge in interest in one-shot
pruning methods that retain the benefits of large-scale pretraining while
shrinking the model size. However, as pruning tends to come with performance
loss, it is important to understand the trade-offs between multilinguality and
sparsification. In this work, we study multilingual performance under different
sparsity constraints and show that moderate ratios already substantially harm
performance. To help bridge this gap, we propose M-Wanda, a pruning method that
models cross-lingual variation by incorporating language-aware activation
statistics into its pruning criterion and dynamically adjusts layerwise
sparsity based on cross-lingual importance. We show that M-Wanda consistently
improves performance at minimal additional costs. We are the first to
explicitly optimize pruning to retain multilingual performance, and hope to
inspire future advances in multilingual pruning.

</details>


### [114] [TAT-R1: Terminology-Aware Translation with Reinforcement Learning and Word Alignment](https://arxiv.org/pdf/2505.21172)
*Zheng Li, Mao Zheng, Mingyang Song, Wenjie Yang*

Main category: cs.CL

TL;DR: TAT-R1, a reinforcement learning-based terminology-aware translation model, improves terminology translation accuracy while maintaining general translation performance.


<details>
  <summary>Details</summary>
Motivation: Deep reasoning LLMs have advanced in tasks like mathematics and coding, but terminology translation in machine translation remains unexplored.

Method: Extracts keyword translation pairs using word alignment, designs rule-based alignment rewards, and trains the model with reinforcement learning.

Result: TAT-R1 significantly improves terminology translation accuracy over baselines without compromising general translation quality.

Conclusion: The model demonstrates the potential of reinforcement learning and word alignment for enhancing terminology-aware machine translation.

Abstract: Recently, deep reasoning large language models(LLMs) like DeepSeek-R1 have
made significant progress in tasks such as mathematics and coding. Inspired by
this, several studies have employed reinforcement learning(RL) to enhance
models' deep reasoning capabilities and improve machine translation(MT)
quality. However, the terminology translation, an essential task in MT, remains
unexplored in deep reasoning LLMs. In this paper, we propose \textbf{TAT-R1}, a
terminology-aware translation model trained with reinforcement learning and
word alignment. Specifically, we first extract the keyword translation pairs
using a word alignment model. Then we carefully design three types of
rule-based alignment rewards with the extracted alignment relationships. With
those alignment rewards, the RL-trained translation model can learn to focus on
the accurate translation of key information, including terminology in the
source text. Experimental results show the effectiveness of TAT-R1. Our model
significantly improves terminology translation accuracy compared to the
baseline models while maintaining comparable performance on general translation
tasks. In addition, we conduct detailed ablation studies of the
DeepSeek-R1-like training paradigm for machine translation and reveal several
key findings.

</details>


### [115] [Walk Before You Run! Concise LLM Reasoning via Reinforcement Learning](https://arxiv.org/pdf/2505.21178)
*Mingyang Song, Mao Zheng*

Main category: cs.CL

TL;DR: The paper introduces ConciseR, a two-stage reinforcement learning framework to reduce overthinking in LLMs by promoting concise reasoning in long CoT responses.


<details>
  <summary>Details</summary>
Motivation: Addressing the persistent overthinking issue in state-of-the-art reasoning models, which leads to redundant or repetitive patterns in long CoT responses.

Method: A two-stage RL framework: Stage 1 (GRPO++) enhances reasoning capabilities, and Stage 2 (L-GRPO) enforces conciseness. ConciseR optimizes length only after correct rollouts.

Result: ConciseR outperforms SOTA models on benchmarks like AIME 2024, MATH-500, and AMC 2023 by generating more concise CoT responses.

Conclusion: ConciseR effectively balances reasoning depth and conciseness, improving efficiency in LLMs.

Abstract: As test-time scaling becomes a pivotal research frontier in Large Language
Models (LLMs) development, contemporary and advanced post-training
methodologies increasingly focus on extending the generation length of long
Chain-of-Thought (CoT) responses to enhance reasoning capabilities toward
DeepSeek R1-like performance. However, recent studies reveal a persistent
overthinking phenomenon in state-of-the-art reasoning models, manifesting as
excessive redundancy or repetitive thinking patterns in long CoT responses. To
address this issue, in this paper, we propose a simple yet effective two-stage
reinforcement learning framework for achieving concise reasoning in LLMs, named
ConciseR. Specifically, the first stage, using more training steps, aims to
incentivize the model's reasoning capabilities via Group Relative Policy
Optimization with clip-higher and dynamic sampling components (GRPO++), and the
second stage, using fewer training steps, explicitly enforces conciseness and
improves efficiency via Length-aware Group Relative Policy Optimization
(L-GRPO). Significantly, ConciseR only optimizes response length once all
rollouts of a sample are correct, following the "walk before you run"
principle. Extensive experimental results demonstrate that our ConciseR model,
which generates more concise CoT reasoning responses, outperforms recent
state-of-the-art reasoning models with zero RL paradigm across AIME 2024,
MATH-500, AMC 2023, Minerva, and Olympiad benchmarks.

</details>


### [116] [Exploring the Latent Capacity of LLMs for One-Step Text Generation](https://arxiv.org/pdf/2505.21189)
*Gleb Mezentsev, Ivan Oseledets*

Main category: cs.CL

TL;DR: Frozen LLMs can generate hundreds of accurate tokens in one forward pass using just two learned embeddings, revealing a novel capability without autoregression.


<details>
  <summary>Details</summary>
Motivation: To explore if text reconstruction in LLMs is possible without autoregressive generation, uncovering underexplored capabilities.

Method: Using frozen LLMs with two learned embeddings to generate multi-token text in a single forward pass, analyzing embedding behavior and space properties.

Result: LLMs can produce accurate multi-token outputs without iterative decoding, with embeddings forming connected, local regions in embedding space.

Conclusion: This reveals a surprising LLM capability and suggests potential for learning dedicated encoders into the embedding space.

Abstract: A recent study showed that large language models (LLMs) can reconstruct
surprisingly long texts - up to thousands of tokens - via autoregressive
generation from just one specially trained input embedding. In this work, we
explore whether such reconstruction is possible without autoregression. We show
that frozen LLMs can generate hundreds of accurate tokens in just one forward
pass, when provided with only two learned embeddings. This reveals a surprising
and underexplored capability of LLMs - multi-token generation without iterative
decoding. We investigate the behaviour of these embeddings and provide insight
into the type of information they encode. We also empirically show that
although these representations are not unique for a given text, they form
connected and local regions in embedding space - a property that suggests the
potential of learning a dedicated encoder into that space.

</details>


### [117] [Lunguage: A Benchmark for Structured and Sequential Chest X-ray Interpretation](https://arxiv.org/pdf/2505.21190)
*Jong Hak Moon, Geon Choi, Paloma Rabaey, Min Gwan Kim, Hyuk Gi Hong, Jung-Oh Lee, Hangyul Yoon, Eun Woo Doe, Jiyoun Kim, Harshita Sharma, Daniel C. Castro, Javier Alvarez-Valle, Edward Choi*

Main category: cs.CL

TL;DR: LUNGUAGE is a benchmark dataset for structured radiology report generation, enabling single-report and longitudinal patient-level evaluation. It includes expert-annotated reports and a two-stage framework for fine-grained structured representations, along with the LUNGUAGESCORE metric for interpretable evaluation.


<details>
  <summary>Details</summary>
Motivation: Existing radiology report evaluation methods lack fine-grained clinical semantics and temporal dependencies, limiting their effectiveness.

Method: A two-stage framework transforms generated reports into structured representations, and the LUNGUAGESCORE metric evaluates entity, relation, and attribute levels with temporal consistency.

Result: LUNGUAGE includes 1,473 annotated reports (80 with longitudinal annotations) and demonstrates effective structured report evaluation.

Conclusion: LUNGUAGE and LUNGUAGESCORE provide the first benchmark and metric for sequential radiology reporting, improving evaluation quality.

Abstract: Radiology reports convey detailed clinical observations and capture
diagnostic reasoning that evolves over time. However, existing evaluation
methods are limited to single-report settings and rely on coarse metrics that
fail to capture fine-grained clinical semantics and temporal dependencies. We
introduce LUNGUAGE,a benchmark dataset for structured radiology report
generation that supports both single-report evaluation and longitudinal
patient-level assessment across multiple studies. It contains 1,473 annotated
chest X-ray reports, each reviewed by experts, and 80 of them contain
longitudinal annotations to capture disease progression and inter-study
intervals, also reviewed by experts. Using this benchmark, we develop a
two-stage framework that transforms generated reports into fine-grained,
schema-aligned structured representations, enabling longitudinal
interpretation. We also propose LUNGUAGESCORE, an interpretable metric that
compares structured outputs at the entity, relation, and attribute level while
modeling temporal consistency across patient timelines. These contributions
establish the first benchmark dataset, structuring framework, and evaluation
metric for sequential radiology reporting, with empirical results demonstrating
that LUNGUAGESCORE effectively supports structured report evaluation. The code
is available at: https://github.com/SuperSupermoon/Lunguage

</details>


### [118] [Unveiling Instruction-Specific Neurons & Experts: An Analytical Framework for LLM's Instruction-Following Capabilities](https://arxiv.org/pdf/2505.21191)
*Junyan Zhang, Yubo Gao, Yibo Yan, Jungang Li, Zhaorui Hou, Sicheng Tao, Shuliang Liu, Song Dai, Yonghua Hei, Junzhuo Li, Xuming Hu*

Main category: cs.CL

TL;DR: The paper investigates how fine-tuning reconfigures LLM computations by analyzing sparse components in dense and MoE models, using HexaInst dataset and SPARCOM framework.


<details>
  <summary>Details</summary>
Motivation: To understand the computational mechanisms behind improved instruction-following in fine-tuned LLMs.

Method: Introduces HexaInst dataset and SPARCOM framework to identify and evaluate sparse components in LLMs.

Result: Demonstrates functional generality, uniqueness, and critical role of sparse components in instruction execution.

Conclusion: Provides insights into how fine-tuning adapts LLMs for instruction-following, aiding trustworthy LLM development.

Abstract: The finetuning of Large Language Models (LLMs) has significantly advanced
their instruction-following capabilities, yet the underlying computational
mechanisms driving these improvements remain poorly understood. This study
systematically examines how fine-tuning reconfigures LLM computations by
isolating and analyzing instruction-specific sparse components, i.e., neurons
in dense models and both neurons and experts in Mixture-of-Experts (MoE)
architectures. In particular, we introduce HexaInst, a carefully curated and
balanced instructional dataset spanning six distinct categories, and propose
SPARCOM, a novel analytical framework comprising three key contributions: (1) a
method for identifying these sparse components, (2) an evaluation of their
functional generality and uniqueness, and (3) a systematic comparison of their
alterations. Through experiments, we demonstrate functional generality,
uniqueness, and the critical role of these components in instruction execution.
By elucidating the relationship between fine-tuning-induced adaptations and
sparse computational substrates, this work provides deeper insights into how
LLMs internalize instruction-following behavior for the trustworthy LLM
community.

</details>


### [119] [Pretrained LLMs Learn Multiple Types of Uncertainty](https://arxiv.org/pdf/2505.21218)
*Roi Cohen, Omri Fahn, Gerard de Melo*

Main category: cs.CL

TL;DR: LLMs can capture uncertainty without explicit training, showing it as a linear concept in latent space. They handle multiple uncertainty types, useful for correctness prediction. Unifying uncertainty types improves correctness.


<details>
  <summary>Details</summary>
Motivation: To explore how LLMs inherently capture uncertainty and its impact on factual correctness, despite not being explicitly trained for it.

Method: Analyze uncertainty as a linear concept in latent space, study different uncertainty types, and test unification via instruction-tuning or [IDK]-token tuning.

Result: LLMs capture multiple uncertainty types, useful for correctness prediction. Unifying uncertainty improves correctness, and model scaling doesn't affect uncertainty capture.

Conclusion: LLMs inherently capture uncertainty, and unifying its types enhances correctness prediction, offering insights for improving model reliability.

Abstract: Large Language Models are known to capture real-world knowledge, allowing
them to excel in many downstream tasks. Despite recent advances, these models
are still prone to what are commonly known as hallucinations, causing them to
emit unwanted and factually incorrect text. In this work, we study how well
LLMs capture uncertainty, without explicitly being trained for that. We show
that, if considering uncertainty as a linear concept in the model's latent
space, it might indeed be captured, even after only pretraining. We further
show that, though unintuitive, LLMs appear to capture several different types
of uncertainty, each of which can be useful to predict the correctness for a
specific task or benchmark. Furthermore, we provide in-depth results such as
demonstrating a correlation between our correction prediction and the model's
ability to abstain from misinformation using words, and the lack of impact of
model scaling for capturing uncertainty. Finally, we claim that unifying the
uncertainty types as a single one using instruction-tuning or [IDK]-token
tuning is helpful for the model in terms of correctness prediction.

</details>


### [120] [A Representation Level Analysis of NMT Model Robustness to Grammatical Errors](https://arxiv.org/pdf/2505.21224)
*Abderrahmane Issam, Yusuf Can Semerci, Jan Scholtes, Gerasimos Spanakis*

Main category: cs.CL

TL;DR: The paper studies robustness in NLP by analyzing how machine translation models handle ungrammatical inputs through internal representations and attention mechanisms, identifying 'Robustness Heads' that play a key role.


<details>
  <summary>Details</summary>
Motivation: To understand robustness in NLP systems by examining model representations of ungrammatical inputs, moving beyond documenting failures or improving robustness.

Method: Uses Grammatical Error Detection (GED) probing and representational similarity analysis to study internal model representations and attention mechanisms.

Result: The encoder detects and corrects grammatical errors by shifting representations toward correct forms, with 'Robustness Heads' attending to linguistic units during this process.

Conclusion: Fine-tuning for robustness increases reliance on Robustness Heads, highlighting their interpretable role in correcting ungrammatical inputs.

Abstract: Understanding robustness is essential for building reliable NLP systems.
Unfortunately, in the context of machine translation, previous work mainly
focused on documenting robustness failures or improving robustness. In
contrast, we study robustness from a model representation perspective by
looking at internal model representations of ungrammatical inputs and how they
evolve through model layers. For this purpose, we perform Grammatical Error
Detection (GED) probing and representational similarity analysis. Our findings
indicate that the encoder first detects the grammatical error, then corrects it
by moving its representation toward the correct form. To understand what
contributes to this process, we turn to the attention mechanism where we
identify what we term Robustness Heads. We find that Robustness Heads attend to
interpretable linguistic units when responding to grammatical errors, and that
when we fine-tune models for robustness, they tend to rely more on Robustness
Heads for updating the ungrammatical word representation.

</details>


### [121] [LMCD: Language Models are Zeroshot Cognitive Diagnosis Learners](https://arxiv.org/pdf/2505.21239)
*Yu He, Zihan Yao, Chentao Song, Tianyu Qi, Jun Liu, Ming Li, Qing Huang*

Main category: cs.CL

TL;DR: LMCD is a framework using LLMs for cognitive diagnosis in cold-start scenarios, outperforming existing methods by leveraging semantic-cognitive fusion.


<details>
  <summary>Details</summary>
Motivation: Traditional CD models struggle in cold-start due to lack of interaction data, and NLP-based methods don't fully bridge semantic-cognitive gaps.

Method: LMCD uses LLMs in two phases: Knowledge Diffusion (enriching exercise/KC content) and Semantic-Cognitive Fusion (integrating text and cognitive states).

Result: LMCD outperforms state-of-the-art methods in exercise-cold and domain-cold settings on real-world datasets.

Conclusion: LMCD effectively addresses cold-start challenges in CD by leveraging LLMs, offering improved performance and practical applicability.

Abstract: Cognitive Diagnosis (CD) has become a critical task in AI-empowered
education, supporting personalized learning by accurately assessing students'
cognitive states. However, traditional CD models often struggle in cold-start
scenarios due to the lack of student-exercise interaction data. Recent
NLP-based approaches leveraging pre-trained language models (PLMs) have shown
promise by utilizing textual features but fail to fully bridge the gap between
semantic understanding and cognitive profiling. In this work, we propose
Language Models as Zeroshot Cognitive Diagnosis Learners (LMCD), a novel
framework designed to handle cold-start challenges by harnessing large language
models (LLMs). LMCD operates via two primary phases: (1) Knowledge Diffusion,
where LLMs generate enriched contents of exercises and knowledge concepts
(KCs), establishing stronger semantic links; and (2) Semantic-Cognitive Fusion,
where LLMs employ causal attention mechanisms to integrate textual information
and student cognitive states, creating comprehensive profiles for both students
and exercises. These representations are efficiently trained with off-the-shelf
CD models. Experiments on two real-world datasets demonstrate that LMCD
significantly outperforms state-of-the-art methods in both exercise-cold and
domain-cold settings. The code is publicly available at
https://github.com/TAL-auroraX/LMCD

</details>


### [122] [Evaluation of LLMs in Medical Text Summarization: The Role of Vocabulary Adaptation in High OOV Settings](https://arxiv.org/pdf/2505.21242)
*Gunjan Balde, Soumyadeep Roy, Mainack Mondal, Niloy Ganguly*

Main category: cs.CL

TL;DR: LLMs perform poorly in medical text summarization for data with high OOV words or novelty. Vocabulary adaptation improves performance, as shown in benchmarks and human evaluations.


<details>
  <summary>Details</summary>
Motivation: To address the performance drop of LLMs in medical summarization for challenging data points, especially those with OOV words or high novelty.

Method: Benchmarked LLMs with vocabulary adaptation strategies, continual pretraining, and evaluated on medical summarization datasets. Human evaluation by medical experts was also conducted.

Result: Vocabulary adaptation improves summarization performance, reducing over-fragmentation and enhancing relevance and faithfulness of summaries.

Conclusion: Vocabulary adaptation is effective for customizing LLMs to the medical domain, improving performance in difficult settings.

Abstract: Large Language Models (LLMs) recently achieved great success in medical text
summarization by simply using in-context learning. However, these recent
efforts do not perform fine-grained evaluations under difficult settings where
LLMs might fail. They typically report performance scores over the entire
dataset. Through our benchmarking study, we show that LLMs show a significant
performance drop for data points with high concentration of out-of-vocabulary
(OOV) words or with high novelty. Vocabulary adaptation is an intuitive
solution to this vocabulary mismatch issue where the LLM vocabulary gets
updated with certain expert domain (here, medical) words or subwords. An
interesting finding from our study is that Llama-3.1, even with a vocabulary
size of around 128K tokens, still faces over-fragmentation issue with medical
words. To that end, we show vocabulary adaptation helps improve the LLM
summarization performance even in difficult settings. Through extensive
experimentation of multiple vocabulary adaptation strategies, two continual
pretraining strategies, and three benchmark medical summarization datasets, we
gain valuable insights into the role of vocabulary adaptation strategies for
customizing LLMs to the medical domain. We also performed a human evaluation
study with medical experts where they found that vocabulary adaptation results
in more relevant and faithful summaries. Our codebase is made publicly
available at https://github.com/gb-kgp/LLM-MedicalSummarization-Benchmark.

</details>


### [123] [Multilingual Pretraining for Pixel Language Models](https://arxiv.org/pdf/2505.21265)
*Ilker Kesen, Jonas F. Lotz, Ingo Ziegler, Phillip Rust, Desmond Elliott*

Main category: cs.CL

TL;DR: PIXEL-M4, a multilingual pixel language model, outperforms English-only models on non-Latin scripts and captures rich linguistic features, even in unseen languages.


<details>
  <summary>Details</summary>
Motivation: To explore the underexplored area of multilingual pretraining in pixel language models and enhance their cross-lingual capabilities.

Method: Pretraining PIXEL-M4 on four diverse languages (English, Hindi, Ukrainian, Simplified Chinese) and evaluating it on semantic and syntactic tasks.

Result: PIXEL-M4 outperforms English-only models on non-Latin scripts and captures linguistic features in unseen languages, with aligned semantic embeddings across pretraining languages.

Conclusion: Multilingual pretraining significantly improves pixel language models' ability to support diverse languages.

Abstract: Pixel language models operate directly on images of rendered text,
eliminating the need for a fixed vocabulary. While these models have
demonstrated strong capabilities for downstream cross-lingual transfer,
multilingual pretraining remains underexplored. We introduce PIXEL-M4, a model
pretrained on four visually and linguistically diverse languages: English,
Hindi, Ukrainian, and Simplified Chinese. Multilingual evaluations on semantic
and syntactic tasks show that PIXEL-M4 outperforms an English-only counterpart
on non-Latin scripts. Word-level probing analyses confirm that PIXEL-M4
captures rich linguistic features, even in languages not seen during
pretraining. Furthermore, an analysis of its hidden representations shows that
multilingual pretraining yields a semantic embedding space closely aligned
across the languages used for pretraining. This work demonstrates that
multilingual pretraining substantially enhances the capability of pixel
language models to effectively support a diverse set of languages.

</details>


### [124] [ReSCORE: Label-free Iterative Retriever Training for Multi-hop Question Answering with Relevance-Consistency Supervision](https://arxiv.org/pdf/2505.21250)
*Dosung Lee, Wonjun Oh, Boyoung Kim, Minyoung Kim, Joonsuk Park, Paul Hongsuck Seo*

Main category: cs.CL

TL;DR: ReSCORE trains dense retrievers for multi-hop QA without labeled documents, using LLMs to assess relevance and consistency, achieving state-of-the-art results.


<details>
  <summary>Details</summary>
Motivation: Overcoming the challenge of requiring labeled query-document pairs for dense retrievers in multi-hop QA due to query variability.

Method: Introduces ReSCORE, leveraging LLMs to evaluate document relevance and consistency with answers, training retrievers iteratively.

Result: Significant improvements in retrieval and state-of-the-art MHQA performance on three benchmarks.

Conclusion: ReSCORE effectively trains retrievers without labeled data, enhancing multi-hop QA performance.

Abstract: Multi-hop question answering (MHQA) involves reasoning across multiple
documents to answer complex questions. Dense retrievers typically outperform
sparse methods like BM25 by leveraging semantic embeddings; however, they
require labeled query-document pairs for fine-tuning. This poses a significant
challenge in MHQA due to the high variability of queries (reformulated)
questions throughout the reasoning steps. To overcome this limitation, we
introduce Retriever Supervision with Consistency and Relevance (ReSCORE), a
novel method for training dense retrievers for MHQA without labeled documents.
ReSCORE leverages large language models to capture each documents relevance to
the question and consistency with the correct answer and use them to train a
retriever within an iterative question-answering framework. Experiments on
three MHQA benchmarks demonstrate the effectiveness of ReSCORE, with
significant improvements in retrieval, and in turn, the state-of-the-art MHQA
performance. Our implementation is available at:
https://leeds1219.github.io/ReSCORE.

</details>


### [125] [rStar-Coder: Scaling Competitive Code Reasoning with a Large-Scale Verified Dataset](https://arxiv.org/pdf/2505.21297)
*Yifei Liu, Li Lyna Zhang, Yi Zhu, Bingcheng Dong, Xudong Zhou, Ning Shang, Fan Yang, Mao Yang*

Main category: cs.CL

TL;DR: rStar-Coder introduces a large-scale, verified dataset to enhance LLM code reasoning, achieving superior performance with smaller models.


<details>
  <summary>Details</summary>
Motivation: The scarcity of high-difficulty datasets with verifiable test cases limits LLM code reasoning.

Method: Constructs a dataset via competitive programming problems, a test-case synthesis pipeline, and verified long-reasoning solutions.

Result: Significant improvements in benchmarks, e.g., Qwen2.5-7B from 17.4% to 57.3%.

Conclusion: rStar-Coder advances LLM code reasoning, outperforming larger models.

Abstract: Advancing code reasoning in large language models (LLMs) is fundamentally
limited by the scarcity of high-difficulty datasets, especially those with
verifiable input-output test cases necessary for rigorous solution validation
at scale. We introduce rStar-Coder, which significantly improves LLM code
reasoning capabilities by constructing a large-scale, verified dataset of 418K
competition-level code problems, 580K long-reasoning solutions along with rich
test cases of varying difficulty. This is achieved through three core
contributions: (1) we curate competitive programming code problems and oracle
solutions to synthesize new, solvable problems; (2) we introduce a reliable
input-output test case synthesis pipeline that decouples the generation into a
three-step input generation method and a mutual verification mechanism for
effective output labeling; (3) we augment problems with high-quality,
test-case-verified long-reasoning solutions. Extensive experiments on Qwen
models (1.5B-14B) across various code reasoning benchmarks demonstrate the
superiority of rStar-Coder dataset, achieving leading performance comparable to
frontier reasoning LLMs with much smaller model sizes. On LiveCodeBench,
rStar-Coder improves Qwen2.5-7B from 17.4% to an impressive 57.3%, and
Qwen2.5-14B from 23.3% to 62.5%, surpassing o3-mini (low) by3.1%. On the more
challenging USA Computing Olympiad, our 7B model achieves an average pass@1
accuracy of 16.15%, outperforming the frontier-level QWQ-32B. Code and the
dataset will be released at https://github.com/microsoft/rStar.

</details>


### [126] [How Humans and LLMs Organize Conceptual Knowledge: Exploring Subordinate Categories in Italian](https://arxiv.org/pdf/2505.21301)
*Andrea Pedrotti, Giulia Rambelli, Caterina Villani, Marianna Bolognesi*

Main category: cs.CL

TL;DR: The study examines subordinate-level category organization using a new Italian psycholinguistic dataset and evaluates LLMs' alignment with human exemplars.


<details>
  <summary>Details</summary>
Motivation: Prior research focused on basic-level categories; this study explores subordinate-level organization to fill this gap.

Method: A new Italian dataset of human-generated exemplars for 187 concrete words was created. LLMs were evaluated on exemplar generation, category induction, and typicality judgment tasks.

Result: Low alignment between humans and LLMs, with performance varying across semantic domains.

Conclusion: AI-generated exemplars show promise but have limitations for psychological and linguistic research.

Abstract: People can categorize the same entity at multiple taxonomic levels, such as
basic (bear), superordinate (animal), and subordinate (grizzly bear). While
prior research has focused on basic-level categories, this study is the first
attempt to examine the organization of categories by analyzing exemplars
produced at the subordinate level. We present a new Italian psycholinguistic
dataset of human-generated exemplars for 187 concrete words. We then use these
data to evaluate whether textual and vision LLMs produce meaningful exemplars
that align with human category organization across three key tasks: exemplar
generation, category induction, and typicality judgment. Our findings show a
low alignment between humans and LLMs, consistent with previous studies.
However, their performance varies notably across different semantic domains.
Ultimately, this study highlights both the promises and the constraints of
using AI-generated exemplars to support psychological and linguistic research.

</details>


### [127] [Charting the Landscape of African NLP: Mapping Progress and Shaping the Road Ahead](https://arxiv.org/pdf/2505.21315)
*Jesujoba O. Alabi, Michael A. Hedderich, David Ifeoluwa Adelani, Dietrich Klakow*

Main category: cs.CL

TL;DR: The paper surveys NLP research on African languages, highlighting their underrepresentation in mainstream NLP systems and the growing interest in this field.


<details>
  <summary>Details</summary>
Motivation: The exclusion of African languages in NLP systems risks widening the digital divide, despite the region's linguistic richness.

Method: Analysis of 734 research papers on NLP for African languages published in the last five years.

Result: Identifies key trends and progress in NLP for African languages, driven by multilingual resources and community initiatives.

Conclusion: Outlines directions for more inclusive and sustainable NLP research for African languages.

Abstract: With over 2,000 languages and potentially millions of speakers, Africa
represents one of the richest linguistic regions in the world. Yet, this
diversity is scarcely reflected in state-of-the-art natural language processing
(NLP) systems and large language models (LLMs), which predominantly support a
narrow set of high-resource languages. This exclusion not only limits the reach
and utility of modern NLP technologies but also risks widening the digital
divide across linguistic communities. Nevertheless, NLP research on African
languages is active and growing. In recent years, there has been a surge of
interest in this area, driven by several factors-including the creation of
multilingual language resources, the rise of community-led initiatives, and
increased support through funding programs. In this survey, we analyze 734
research papers on NLP for African languages published over the past five
years, offering a comprehensive overview of recent progress across core tasks.
We identify key trends shaping the field and conclude by outlining promising
directions to foster more inclusive and sustainable NLP research for African
languages.

</details>


### [128] [Leveraging large language models and traditional machine learning ensembles for ADHD detection from narrative transcripts](https://arxiv.org/pdf/2505.21324)
*Yuxin Zhu, Yuting Guo, Noah Marchuck, Abeed Sarker, Yun Wang*

Main category: cs.CL

TL;DR: An ensemble framework combining LLaMA3, RoBERTa, and SVM improves ADHD diagnosis classification from narrative data, outperforming individual models with an F1 score of 0.71.


<details>
  <summary>Details</summary>
Motivation: The study addresses the underexplored integration of LLMs with traditional ML techniques in psychiatric applications, particularly for nuanced narrative data like ADHD diagnosis.

Method: The ensemble framework integrates LLaMA3 (for semantic structure), RoBERTa (fine-tuned on clinical narratives), and SVM (lexical features), aggregated via majority voting.

Result: The ensemble achieved an F1 score of 0.71, improving recall while maintaining precision compared to the best individual model (SVM).

Conclusion: Hybrid architectures combining LLMs and traditional ML show promise for robust psychiatric text classification, leveraging semantic richness and interpretability.

Abstract: Despite rapid advances in large language models (LLMs), their integration
with traditional supervised machine learning (ML) techniques that have proven
applicability to medical data remains underexplored. This is particularly true
for psychiatric applications, where narrative data often exhibit nuanced
linguistic and contextual complexity, and can benefit from the combination of
multiple models with differing characteristics. In this study, we introduce an
ensemble framework for automatically classifying
Attention-Deficit/Hyperactivity Disorder (ADHD) diagnosis (binary) using
narrative transcripts. Our approach integrates three complementary models:
LLaMA3, an open-source LLM that captures long-range semantic structure;
RoBERTa, a pre-trained transformer model fine-tuned on labeled clinical
narratives; and a Support Vector Machine (SVM) classifier trained using
TF-IDF-based lexical features. These models are aggregated through a majority
voting mechanism to enhance predictive robustness. The dataset includes 441
instances, including 352 for training and 89 for validation. Empirical results
show that the ensemble outperforms individual models, achieving an F$_1$ score
of 0.71 (95\% CI: [0.60-0.80]). Compared to the best-performing individual
model (SVM), the ensemble improved recall while maintaining competitive
precision. This indicates the strong sensitivity of the ensemble in identifying
ADHD-related linguistic cues. These findings demonstrate the promise of hybrid
architectures that leverage the semantic richness of LLMs alongside the
interpretability and pattern recognition capabilities of traditional supervised
ML, offering a new direction for robust and generalizable psychiatric text
classification.

</details>


### [129] [PEDANTIC: A Dataset for the Automatic Examination of Definiteness in Patent Claims](https://arxiv.org/pdf/2505.21342)
*Valentin Knappich, Annemarie Friedrich, Anna Hätty, Simon Razniewski*

Main category: cs.CL

TL;DR: The paper introduces PEDANTIC, a dataset of 14k US patent claims annotated for indefiniteness, using an automated pipeline with LLMs, and evaluates LLM performance against baselines.


<details>
  <summary>Details</summary>
Motivation: To address the lack of annotated datasets for patent definiteness examination, aiming to improve patent drafting and examination efficiency.

Method: Constructs PEDANTIC using an automated pipeline with LLMs to extract reasons for indefiniteness from USPTO documents, validated by human study. Evaluates LLM performance using LLM-as-Judge and logistic regression baselines.

Result: LLMs (Qwen 2.5 32B and 72B) struggle to outperform logistic regression baselines in definiteness prediction but often correctly identify reasons.

Conclusion: PEDANTIC is a valuable resource for patent AI research, enabling advanced examination models, with plans for public release of dataset and code.

Abstract: Patent claims define the scope of protection for an invention. If there are
ambiguities in a claim, it is rejected by the patent office. In the US, this is
referred to as indefiniteness (35 U.S.C {\S} 112(b)) and is among the most
frequent reasons for patent application rejection. The development of automatic
methods for patent definiteness examination has the potential to make patent
drafting and examination more efficient, but no annotated dataset has been
published to date.
  We introduce PEDANTIC (\underline{P}at\underline{e}nt
\underline{D}efiniteness Ex\underline{a}mi\underline{n}a\underline{ti}on
\underline{C}orpus), a novel dataset of 14k US patent claims from patent
applications relating to Natural Language Processing (NLP), annotated with
reasons for indefiniteness. We construct PEDANTIC using a fully automatic
pipeline that retrieves office action documents from the USPTO and uses Large
Language Models (LLMs) to extract the reasons for indefiniteness. A human
validation study confirms the pipeline's accuracy in generating high-quality
annotations. To gain insight beyond binary classification metrics, we implement
an LLM-as-Judge evaluation that compares the free-form reasoning of every
model-cited reason with every examiner-cited reason. We show that LLM agents
based on Qwen 2.5 32B and 72B struggle to outperform logistic regression
baselines on definiteness prediction, even though they often correctly identify
the underlying reasons. PEDANTIC provides a valuable resource for patent AI
researchers, enabling the development of advanced examination models. We will
publicly release the dataset and code.

</details>


### [130] [Leveraging Large Language Models for Bengali Math Word Problem Solving with Chain of Thought Reasoning](https://arxiv.org/pdf/2505.21354)
*Bidyarthi Paul, Jalisha Jashim Era, Mirazur Rahman Zim, Tahmid Sattar Aothoi, Faisal Muhammad Shah*

Main category: cs.CL

TL;DR: The paper introduces SOMADHAN, a dataset of 8792 complex Bengali Math Word Problems (MWPs) with step-by-step solutions, and evaluates LLMs like GPT-4o and LLaMA-3.3 70B, showing improved performance with Chain of Thought (CoT) prompting and LoRA fine-tuning.


<details>
  <summary>Details</summary>
Motivation: Bengali MWPs are challenging due to the language's low-resource status and lack of annotated datasets, hindering progress in mathematical reasoning for Bengali.

Method: Created SOMADHAN dataset, evaluated LLMs (GPT-4o, GPT-3.5 Turbo, LLaMA, etc.) using zero-shot/few-shot prompting with/without CoT, and applied LoRA for efficient fine-tuning.

Result: CoT prompting improved performance, with LLaMA-3.3 70B achieving 88% accuracy. LoRA enabled efficient adaptation to Bengali MWPs.

Conclusion: The work addresses a critical gap in Bengali NLP, providing a high-quality dataset and scalable framework for complex MWPs, advancing equitable research in low-resource languages.

Abstract: Solving Bengali Math Word Problems (MWPs) remains a major challenge in
natural language processing (NLP) due to the language's low-resource status and
the multi-step reasoning required. Existing models struggle with complex
Bengali MWPs, largely because no human-annotated Bengali dataset has previously
addressed this task. This gap has limited progress in Bengali mathematical
reasoning. To address this, we created SOMADHAN, a dataset of 8792 complex
Bengali MWPs with manually written, step-by-step solutions. We designed this
dataset to support reasoning-focused evaluation and model development in a
linguistically underrepresented context. Using SOMADHAN, we evaluated a range
of large language models (LLMs) - including GPT-4o, GPT-3.5 Turbo, LLaMA series
models, Deepseek, and Qwen - through both zero-shot and few-shot prompting with
and without Chain of Thought (CoT) reasoning. CoT prompting consistently
improved performance over standard prompting, especially in tasks requiring
multi-step logic. LLaMA-3.3 70B achieved the highest accuracy of 88% with
few-shot CoT prompting. We also applied Low-Rank Adaptation (LoRA) to fine-tune
models efficiently, enabling them to adapt to Bengali MWPs with minimal
computational cost. Our work fills a critical gap in Bengali NLP by providing a
high-quality reasoning dataset and a scalable framework for solving complex
MWPs. We aim to advance equitable research in low-resource languages and
enhance reasoning capabilities in educational and language technologies.

</details>


### [131] [Evaluating LLM Adaptation to Sociodemographic Factors: User Profile vs. Dialogue History](https://arxiv.org/pdf/2505.21362)
*Qishuai Zhong, Zongmin Li, Siqi Fan, Aixin Sun*

Main category: cs.CL

TL;DR: The paper evaluates how LLMs adapt responses to sociodemographic attributes via explicit user profiles or implicit dialogue history, finding variability in consistency, with reasoning-capable models showing better alignment.


<details>
  <summary>Details</summary>
Motivation: To assess LLM adaptation to sociodemographic traits (age, occupation, education) through explicit or implicit context, as existing evaluations often overlook multi-turn interactions.

Method: Proposes a framework using a multi-agent pipeline to create a synthetic dataset with dialogue histories and user profiles, probing value expression via VSM 2013 questions.

Result: Most models adjust values based on demographics (age, education), but consistency varies; models with stronger reasoning align better.

Conclusion: Reasoning capability is key for robust sociodemographic adaptation in LLMs, highlighting the need for improved consistency in behavioral alignment.

Abstract: Effective engagement by large language models (LLMs) requires adapting
responses to users' sociodemographic characteristics, such as age, occupation,
and education level. While many real-world applications leverage dialogue
history for contextualization, existing evaluations of LLMs' behavioral
adaptation often focus on single-turn prompts. In this paper, we propose a
framework to evaluate LLM adaptation when attributes are introduced either (1)
explicitly via user profiles in the prompt or (2) implicitly through multi-turn
dialogue history. We assess the consistency of model behavior across these
modalities. Using a multi-agent pipeline, we construct a synthetic dataset
pairing dialogue histories with distinct user profiles and employ questions
from the Value Survey Module (VSM 2013) (Hofstede and Hofstede, 2016) to probe
value expression. Our findings indicate that most models adjust their expressed
values in response to demographic changes, particularly in age and education
level, but consistency varies. Models with stronger reasoning capabilities
demonstrate greater alignment, indicating the importance of reasoning in robust
sociodemographic adaptation.

</details>


### [132] [Analyzing values about gendered language reform in LLMs' revisions](https://arxiv.org/pdf/2505.21378)
*Jules Watson, Xi Wang, Raymond Liu, Suzanne Stevenson, Barend Beekhuizen*

Main category: cs.CL

TL;DR: The paper examines how LLMs revise gendered role nouns and justify these revisions, evaluating their alignment with feminist and trans-inclusive language reforms in English. It also assesses if LLMs mirror human contextual sensitivity in applying such reforms.


<details>
  <summary>Details</summary>
Motivation: To understand LLMs' alignment with progressive language reforms and their contextual sensitivity in gendered language revision.

Method: Analyzes LLMs' revisions of gendered role nouns and their justifications, comparing them to feminist and trans-inclusive language standards. Incorporates sociolinguistic insights to evaluate contextual effects.

Result: Finds broad evidence that LLMs are sensitive to contextual effects in gendered language revision, similar to humans.

Conclusion: Highlights implications for value alignment in LLMs, suggesting they can reflect human-like sensitivity in language reforms.

Abstract: Within the common LLM use case of text revision, we study LLMs' revision of
gendered role nouns (e.g., outdoorsperson/woman/man) and their justifications
of such revisions. We evaluate their alignment with feminist and
trans-inclusive language reforms for English. Drawing on insight from
sociolinguistics, we further assess if LLMs are sensitive to the same
contextual effects in the application of such reforms as people are, finding
broad evidence of such effects. We discuss implications for value alignment.

</details>


### [133] [PHISH in MESH: Korean Adversarial Phonetic Substitution and Phonetic-Semantic Feature Integration Defense](https://arxiv.org/pdf/2505.21380)
*Byungjun Kim, Minju Kim, Hyeonchu Park, Bugeun Kim*

Main category: cs.CL

TL;DR: The paper addresses phonetic substitution evasion in hate speech detection, focusing on Korean language challenges and proposing two methods (PHISH and MESH) to improve detection robustness.


<details>
  <summary>Details</summary>
Motivation: Malicious users exploit phonetic substitution to evade hate speech detection, with Korean being overlooked despite its vulnerability. Existing work lacks architectural defenses.

Method: Proposes PHISH for exploiting Korean phonological traits and MESH for integrating phonetic features into the detector's architecture.

Result: Methods improve detection on perturbed and unperturbed datasets, reflecting real adversarial behaviors.

Conclusion: PHISH and MESH effectively enhance hate speech detection robustness against phonetic evasion in Korean.

Abstract: As malicious users increasingly employ phonetic substitution to evade hate
speech detection, researchers have investigated such strategies. However, two
key challenges remain. First, existing studies have overlooked the Korean
language, despite its vulnerability to phonetic perturbations due to its
phonographic nature. Second, prior work has primarily focused on constructing
datasets rather than developing architectural defenses. To address these
challenges, we propose (1) PHonetic-Informed Substitution for Hangul (PHISH)
that exploits the phonological characteristics of the Korean writing system,
and (2) Mixed Encoding of Semantic-pHonetic features (MESH) that enhances the
detector's robustness by incorporating phonetic information at the
architectural level. Our experimental results demonstrate the effectiveness of
our proposed methods on both perturbed and unperturbed datasets, suggesting
that they not only improve detection performance but also reflect realistic
adversarial behaviors employed by malicious users.

</details>


### [134] [AutoJudger: An Agent-Driven Framework for Efficient Benchmarking of MLLMs](https://arxiv.org/pdf/2505.21389)
*Xuanwen Ding, Chengjun Pan, Zejun Li, Jiwen Zhang, Siyuan Wang, Zhongyu Wei*

Main category: cs.CL

TL;DR: AutoJudger is an agent-driven framework using Item Response Theory and dynamic question selection to efficiently evaluate multimodal large language models, reducing evaluation costs significantly.


<details>
  <summary>Details</summary>
Motivation: The increasing expense of evaluating multimodal large language models due to their size and complexity motivates the need for an adaptive and cost-effective benchmarking solution.

Method: AutoJudger employs Item Response Theory to estimate question difficulty and uses an autonomous agent for dynamic question selection, incorporating semantic-aware retrieval and dynamic memory.

Result: Experiments show AutoJudger achieves over 90% ranking accuracy with only 4% of the data on MMT-Bench, significantly reducing evaluation costs.

Conclusion: AutoJudger provides an efficient and adaptive solution for benchmarking MLLMs, dramatically lowering evaluation expenses while maintaining high accuracy.

Abstract: Evaluating multimodal large language models (MLLMs) is increasingly
expensive, as the growing size and cross-modality complexity of benchmarks
demand significant scoring efforts. To tackle with this difficulty, we
introduce AutoJudger, an agent-driven framework for efficient and adaptive
benchmarking of MLLMs that tackles this escalating cost. AutoJudger employs the
Item Response Theory (IRT) to estimate the question difficulty and an
autonomous evaluation agent to dynamically select the most informative test
questions based on the model's real-time performance. Specifically, AutoJudger
incorporates two pivotal components: a semantic-aware retrieval mechanism to
ensure that selected questions cover diverse and challenging scenarios across
both vision and language modalities, and a dynamic memory that maintains
contextual statistics of previously evaluated questions to guide coherent and
globally informed question selection throughout the evaluation process.
Extensive experiments on four representative multimodal benchmarks demonstrate
that our adaptive framework dramatically reduces evaluation expenses, i.e.
AutoJudger uses only 4% of the data to achieve over 90% ranking accuracy with
the full benchmark evaluation on MMT-Bench.

</details>


### [135] [Improving Research Idea Generation Through Data: An Empirical Investigation in Social Science](https://arxiv.org/pdf/2505.21396)
*Xiao Liu, Xinyi Dong, Xinyang Gao, Yansong Feng, Xun Pang*

Main category: cs.CL

TL;DR: Augmenting LLMs with data during idea generation improves feasibility and quality, validated in social science experiments.


<details>
  <summary>Details</summary>
Motivation: Address challenges of feasibility and effectiveness in LLM-generated research ideas by incorporating data.

Method: Introduce metadata guidance during idea generation and automatic validation during idea selection.

Result: Metadata boosts feasibility by 20%, validation improves idea quality by 7%. Human study confirms higher-quality ideas.

Conclusion: Data-driven LLM-assisted ideation is practical and beneficial for academic research.

Abstract: Recent advancements in large language models (LLMs) have shown promise in
generating novel research ideas. However, these ideas often face challenges
related to feasibility and expected effectiveness. This paper explores how
augmenting LLMs with relevant data during the idea generation process can
enhance the quality of generated ideas. We introduce two ways of incorporating
data: (1) providing metadata during the idea generation stage to guide LLMs
toward feasible directions, and (2) adding automatic validation during the idea
selection stage to assess the empirical plausibility of hypotheses within
ideas. We conduct experiments in the social science domain, specifically with
climate negotiation topics, and find that metadata improves the feasibility of
generated ideas by 20%, while automatic validation improves the overall quality
of selected ideas by 7%. A human study shows that LLM-generated ideas, along
with their related data and validation processes, inspire researchers to
propose research ideas with higher quality. Our work highlights the potential
of data-driven research idea generation, and underscores the practical utility
of LLM-assisted ideation in real-world academic settings.

</details>


### [136] [DecisionFlow: Advancing Large Language Model as Principled Decision Maker](https://arxiv.org/pdf/2505.21397)
*Xiusi Chen, Shanyong Wang, Cheng Qian, Hongru Wang, Peixuan Han, Heng Ji*

Main category: cs.CL

TL;DR: DecisionFlow is a framework for transparent and explainable decision-making in high-stakes domains, improving accuracy and alignment over traditional methods.


<details>
  <summary>Details</summary>
Motivation: Current language models lack structured deliberation for high-stakes decisions, generating disconnected outcomes and justifications.

Method: DecisionFlow uses structured representations of actions, attributes, and constraints, inferring a latent utility function for transparent, utility-driven reasoning.

Result: Empirical results show 30% accuracy gains and better alignment compared to prompting baselines.

Conclusion: DecisionFlow advances symbolic reasoning in LLMs, enabling accountable and reliable decision support systems.

Abstract: In high-stakes domains such as healthcare and finance, effective
decision-making demands not just accurate outcomes but transparent and
explainable reasoning. However, current language models often lack the
structured deliberation needed for such tasks, instead generating decisions and
justifications in a disconnected, post-hoc manner. To address this, we propose
DecisionFlow, a novel decision modeling framework that guides models to reason
over structured representations of actions, attributes, and constraints. Rather
than predicting answers directly from prompts, DecisionFlow builds a
semantically grounded decision space and infers a latent utility function to
evaluate trade-offs in a transparent, utility-driven manner. This process
produces decisions tightly coupled with interpretable rationales reflecting the
model's reasoning. Empirical results on two high-stakes benchmarks show that
DecisionFlow not only achieves up to 30% accuracy gains over strong prompting
baselines but also enhances alignment in outcomes. Our work is a critical step
toward integrating symbolic reasoning with LLMs, enabling more accountable,
explainable, and reliable LLM decision support systems. We release the data and
code at https://github.com/xiusic/DecisionFlow.

</details>


### [137] [Factual Self-Awareness in Language Models: Representation, Robustness, and Scaling](https://arxiv.org/pdf/2505.21399)
*Hovhannes Tamoyan, Subhabrata Dutta, Iryna Gurevych*

Main category: cs.CL

TL;DR: LLMs have an internal mechanism to detect factual correctness during generation, revealed by linear features in the Transformer's residual stream. This self-awareness is robust and emerges early in training.


<details>
  <summary>Details</summary>
Motivation: Address factual incorrectness in LLM-generated content by exploring their intrinsic ability to self-monitor correctness during generation.

Method: Analyze linear features in the Transformer's residual stream to identify self-awareness signals, test robustness to formatting, and study context perturbation effects.

Result: LLMs encode self-awareness signals for factual recall, robust to formatting, and peak in intermediate layers. This capability emerges rapidly during training.

Conclusion: LLMs possess intrinsic self-monitoring abilities, enhancing interpretability and reliability, with implications for improving factual accuracy in generated content.

Abstract: Factual incorrectness in generated content is one of the primary concerns in
ubiquitous deployment of large language models (LLMs). Prior findings suggest
LLMs can (sometimes) detect factual incorrectness in their generated content
(i.e., fact-checking post-generation). In this work, we provide evidence
supporting the presence of LLMs' internal compass that dictate the correctness
of factual recall at the time of generation. We demonstrate that for a given
subject entity and a relation, LLMs internally encode linear features in the
Transformer's residual stream that dictate whether it will be able to recall
the correct attribute (that forms a valid entity-relation-attribute triplet).
This self-awareness signal is robust to minor formatting variations. We
investigate the effects of context perturbation via different example selection
strategies. Scaling experiments across model sizes and training dynamics
highlight that self-awareness emerges rapidly during training and peaks in
intermediate layers. These findings uncover intrinsic self-monitoring
capabilities within LLMs, contributing to their interpretability and
reliability.

</details>


### [138] [RelationalFactQA: A Benchmark for Evaluating Tabular Fact Retrieval from Large Language Models](https://arxiv.org/pdf/2505.21409)
*Dario Satriani, Enzo Veltri, Donatello Santoro, Paolo Papotti*

Main category: cs.CL

TL;DR: RelationalFactQA is a new benchmark to evaluate LLMs' ability to generate structured tabular outputs, revealing their poor performance (≤25% accuracy) in relational fact retrieval.


<details>
  <summary>Details</summary>
Motivation: Current benchmarks overlook LLMs' ability to generate structured, multi-record tabular outputs, a critical capability for factual knowledge retrieval.

Method: Introduces RelationalFactQA, a benchmark with natural language questions (paired with SQL) and gold-standard tabular answers, designed to assess structured knowledge retrieval.

Result: State-of-the-art LLMs struggle (≤25% accuracy), with performance worsening as output dimensionality increases.

Conclusion: RelationalFactQA highlights LLMs' limitations in structured factual synthesis and serves as a key resource for future progress.

Abstract: Factuality in Large Language Models (LLMs) is a persistent challenge. Current
benchmarks often assess short factual answers, overlooking the critical ability
to generate structured, multi-record tabular outputs from parametric knowledge.
We demonstrate that this relational fact retrieval is substantially more
difficult than isolated point-wise queries, even when individual facts are
known to the model, exposing distinct failure modes sensitive to output
dimensionality (e.g., number of attributes or records). To systematically
evaluate this under-explored capability, we introduce RelationalFactQA, a new
benchmark featuring diverse natural language questions (paired with SQL) and
gold-standard tabular answers, specifically designed to assess knowledge
retrieval in a structured format. RelationalFactQA enables analysis across
varying query complexities, output sizes, and data characteristics. Our
experiments reveal that even state-of-the-art LLMs struggle significantly, not
exceeding 25% factual accuracy in generating relational outputs, with
performance notably degrading as output dimensionality increases. These
findings underscore critical limitations in current LLMs' ability to synthesize
structured factual knowledge and establish RelationalFactQA as a crucial
resource for measuring future progress in LLM factuality.

</details>


### [139] [RefTool: Enhancing Model Reasoning with Reference-Guided Tool Creation](https://arxiv.org/pdf/2505.21413)
*Xiao Liu, Da Yin, Zirui Wu, Yansong Feng*

Main category: cs.CL

TL;DR: RefTool is a framework for LLMs to create and use tools from external references, improving reasoning beyond their internal knowledge.


<details>
  <summary>Details</summary>
Motivation: To address the limitation of LLMs failing in domains beyond their knowledge when generating tools without predefined references.

Method: RefTool has two modules: tool creation (generating, validating, and organizing tools from references) and tool utilization (selecting and applying tools).

Result: RefTool outperforms existing methods by 11.3% in accuracy and is cost-efficient and generalizable.

Conclusion: Grounding tool creation in external references enhances LLMs' reasoning, overcoming knowledge limitations.

Abstract: Tools enhance the reasoning capabilities of large language models (LLMs) in
complex problem-solving tasks, but not all tasks have available tools. In the
absence of predefined tools, prior works have explored instructing LLMs to
generate tools on their own. However, such approaches rely heavily on the
models' internal knowledge and would fail in domains beyond the LLMs' knowledge
scope. To address this limitation, we propose RefTool, a reference-guided
framework for automatic tool creation that leverages structured external
materials such as textbooks. RefTool consists of two modules: (1) tool
creation, where LLMs generate executable tools from reference content, validate
them using illustrative examples, and organize them hierarchically into a
toolbox; and (2) tool utilization, where LLMs navigate the toolbox structure to
select and apply the appropriate tools to solve problems. Experiments on
causality, physics, and chemistry benchmarks demonstrate that RefTool
outperforms existing tool-creation and domain-specific reasoning methods by
11.3% on average accuracy, while being cost-efficient and broadly
generalizable. Analyses reveal that grounding tool creation in references
produces accurate and faithful tools, and that the hierarchical structure
facilitates effective tool selection. RefTool enables LLMs to overcome
knowledge limitations, demonstrating the value of grounding tool creation in
external references for enhanced and generalizable reasoning.

</details>


### [140] [Pangu Pro MoE: Mixture of Grouped Experts for Efficient Sparsity](https://arxiv.org/pdf/2505.21411)
*Yehui Tang, Xiaosong Li, Fangcheng Liu, Wei Guo, Hang Zhou, Yaoyuan Wang, Kai Han, Xianzhi Yu, Jinpeng Li, Hui Zang, Fei Mi, Xiaojun Meng, Zhicheng Liu, Hanting Chen, Binfan Zheng, Can Chen, Youliang Yan, Ruiming Tang, Peifeng Qin, Xinghao Chen, Dacheng Tao, Yunhe Wang*

Main category: cs.CL

TL;DR: MoGE improves expert workload balancing in MoE models, enhancing efficiency and throughput on Ascend NPUs, as demonstrated by Pangu Pro MoE.


<details>
  <summary>Details</summary>
Motivation: Address the inefficiency in MoE models where some experts are overused, leading to system imbalance.

Method: Introduce MoGE, grouping experts and balancing their activation, optimized for Ascend NPUs.

Result: Pangu Pro MoE achieves high throughput (1148-1528 tokens/s per card) and outperforms comparable dense models.

Conclusion: MoGE ensures balanced computational load, improving training and inference efficiency on Ascend NPUs.

Abstract: The surgence of Mixture of Experts (MoE) in Large Language Models promises a
small price of execution cost for a much larger model parameter count and
learning capacity, because only a small fraction of parameters are activated
for each input token. However, it is commonly observed that some experts are
activated far more often than others, leading to system inefficiency when
running the experts on different devices in parallel. Therefore, we introduce
Mixture of Grouped Experts (MoGE), which groups the experts during selection
and balances the expert workload better than MoE in nature. It constrains
tokens to activate an equal number of experts within each predefined expert
group. When a model execution is distributed on multiple devices, this
architectural design ensures a balanced computational load across devices,
significantly enhancing throughput, particularly for the inference phase.
Further, we build Pangu Pro MoE on Ascend NPUs, a sparse model based on MoGE
with 72 billion total parameters, 16 billion of which are activated for each
token. The configuration of Pangu Pro MoE is optimized for Ascend 300I Duo and
800I A2 through extensive system simulation studies. Our experiments indicate
that MoGE indeed leads to better expert load balancing and more efficient
execution for both model training and inference on Ascend NPUs. The inference
performance of Pangu Pro MoE achieves 1148 tokens/s per card and can be further
improved to 1528 tokens/s per card by speculative acceleration, outperforming
comparable 32B and 72B Dense models. Furthermore, we achieve an excellent
cost-to-performance ratio for model inference on Ascend 300I Duo.Our studies
show that Ascend NPUs are capable of training Pangu Pro MoE with massive
parallelization to make it a leading model within the sub-100B total parameter
class, outperforming prominent open-source models like GLM-Z1-32B and
Qwen3-32B.

</details>


### [141] [Towards Better Instruction Following Retrieval Models](https://arxiv.org/pdf/2505.21439)
*Yuchen Zhuang, Aaron Trinh, Rushi Qiang, Haotian Sun, Chao Zhang, Hanjun Dai, Bo Dai*

Main category: cs.CL

TL;DR: InF-IR introduces a large-scale corpus for instruction-following IR, enhancing retrieval models with contrastive learning and instruction-query attention.


<details>
  <summary>Details</summary>
Motivation: Standard IR models fail to interpret explicit user instructions, necessitating a tailored training corpus.

Method: InF-IR expands training pairs into instruction-query-passage triplets, including hard negatives, validated for semantic plausibility.

Result: InF-Embed, trained on InF-IR, outperforms baselines by 8.1% in p-MRR on instruction-based benchmarks.

Conclusion: InF-IR and InF-Embed significantly improve instruction-following capabilities in retrieval models.

Abstract: Modern information retrieval (IR) models, trained exclusively on standard
<query, passage> pairs, struggle to effectively interpret and follow explicit
user instructions. We introduce InF-IR, a large-scale, high-quality training
corpus tailored for enhancing retrieval models in Instruction-Following IR.
InF-IR expands traditional training pairs into over 38,000 expressive
<instruction, query, passage> triplets as positive samples. In particular, for
each positive triplet, we generate two additional hard negative examples by
poisoning both instructions and queries, then rigorously validated by an
advanced reasoning model (o3-mini) to ensure semantic plausibility while
maintaining instructional incorrectness. Unlike existing corpora that primarily
support computationally intensive reranking tasks for decoder-only language
models, the highly contrastive positive-negative triplets in InF-IR further
enable efficient representation learning for smaller encoder-only models,
facilitating direct embedding-based retrieval. Using this corpus, we train
InF-Embed, an instruction-aware Embedding model optimized through contrastive
learning and instruction-query attention mechanisms to align retrieval outcomes
precisely with user intents. Extensive experiments across five
instruction-based retrieval benchmarks demonstrate that InF-Embed significantly
surpasses competitive baselines by 8.1% in p-MRR, measuring the
instruction-following capabilities.

</details>


### [142] [Words Like Knives: Backstory-Personalized Modeling and Detection of Violent Communication](https://arxiv.org/pdf/2505.21451)
*Jocelyn Shen, Akhila Yerukola, Xuhui Zhou, Cynthia Breazeal, Maarten Sap, Hae Won Park*

Main category: cs.CL

TL;DR: The paper explores how relationship backstories influence conflict perception in conversations, using NVC theory and a new dataset (PersonaConflicts Corpus). Models struggle to leverage backstories effectively and overestimate positive impacts.


<details>
  <summary>Details</summary>
Motivation: To address the gap in NLP research that overlooks relational dynamics in conflict detection, focusing on personal histories and emotional contexts.

Method: Leveraging NVC theory, the study uses the PersonaConflicts Corpus (simulated dialogues) and a human study to annotate breakdown types and assess backstory impact.

Result: Relationship backstories significantly shift human perception, but models fail to leverage them effectively and overestimate positive message impacts.

Conclusion: Personalization to relationship contexts is crucial for LLMs to mediate human communication effectively.

Abstract: Conversational breakdowns in close relationships are deeply shaped by
personal histories and emotional context, yet most NLP research treats conflict
detection as a general task, overlooking the relational dynamics that influence
how messages are perceived. In this work, we leverage nonviolent communication
(NVC) theory to evaluate LLMs in detecting conversational breakdowns and
assessing how relationship backstory influences both human and model perception
of conflicts. Given the sensitivity and scarcity of real-world datasets
featuring conflict between familiar social partners with rich personal
backstories, we contribute the PersonaConflicts Corpus, a dataset of N=5,772
naturalistic simulated dialogues spanning diverse conflict scenarios between
friends, family members, and romantic partners. Through a controlled human
study, we annotate a subset of dialogues and obtain fine-grained labels of
communication breakdown types on individual turns, and assess the impact of
backstory on human and model perception of conflict in conversation. We find
that the polarity of relationship backstories significantly shifted human
perception of communication breakdowns and impressions of the social partners,
yet models struggle to meaningfully leverage those backstories in the detection
task. Additionally, we find that models consistently overestimate how
positively a message will make a listener feel. Our findings underscore the
critical role of personalization to relationship contexts in enabling LLMs to
serve as effective mediators in human communication for authentic connection.

</details>


### [143] [Do LLMs Need to Think in One Language? Correlation between Latent Language and Task Performance](https://arxiv.org/pdf/2505.21458)
*Shintaro Ozaki, Tatsuya Hiraoka, Hiroto Otake, Hiroki Ouchi, Masaru Isonuma, Benjamin Heinzerling, Kentaro Inui, Taro Watanabe, Yusuke Miyao, Yohei Oseki, Yu Takagi*

Main category: cs.CL

TL;DR: The study explores how the consistency of latent language in LLMs affects downstream task performance, finding it's not always necessary for optimal results.


<details>
  <summary>Details</summary>
Motivation: To understand the impact of latent language consistency on LLM task performance, as prior work largely overlooks this aspect.

Method: Varying input prompt languages across tasks and analyzing latent language consistency's correlation with performance, using datasets from translation and geo-culture domains.

Result: Consistency in latent language isn't always crucial for performance, as LLMs adapt internal representations near final layers to match target languages.

Conclusion: Latent language consistency may not be essential for optimal task performance due to LLMs' adaptive capabilities.

Abstract: Large Language Models (LLMs) are known to process information using a
proficient internal language consistently, referred to as latent language,
which may differ from the input or output languages. However, how the
discrepancy between the latent language and the input and output language
affects downstream task performance remains largely unexplored. While many
studies research the latent language of LLMs, few address its importance in
influencing task performance. In our study, we hypothesize that thinking in
latent language consistently enhances downstream task performance. To validate
this, our work varies the input prompt languages across multiple downstream
tasks and analyzes the correlation between consistency in latent language and
task performance. We create datasets consisting of questions from diverse
domains such as translation and geo-culture, which are influenced by the choice
of latent language. Experimental results across multiple LLMs on translation
and geo-culture tasks, which are sensitive to the choice of language, indicate
that maintaining consistency in latent language is not always necessary for
optimal downstream task performance. This is because these models adapt their
internal representations near the final layers to match the target language,
reducing the impact of consistency on overall performance.

</details>


### [144] [Accelerating Diffusion Language Model Inference via Efficient KV Caching and Guided Diffusion](https://arxiv.org/pdf/2505.21467)
*Zhanqiu Hu, Jian Meng, Yash Akhauri, Mohamed S. Abdelfattah, Jae-sun Seo, Zhiru Zhang, Udit Gupta*

Main category: cs.CL

TL;DR: The paper introduces FreeCache and Guided Diffusion to address slow inference and token incoherence in diffusion language models, achieving up to 34x speedup without quality loss.


<details>
  <summary>Details</summary>
Motivation: Diffusion language models are promising but suffer from slow inference and token incoherence, limiting their practicality compared to autoregressive models.

Method: Proposes FreeCache (KV approximation caching) and Guided Diffusion (autoregressive supervision) to reduce computational costs and denoising steps.

Result: Combined methods deliver up to 34x speedup while maintaining accuracy, matching or surpassing autoregressive model latency.

Conclusion: The techniques enable scalable and efficient diffusion language models, broadening their applicability across domains.

Abstract: Diffusion language models offer parallel token generation and inherent
bidirectionality, promising more efficient and powerful sequence modeling
compared to autoregressive approaches. However, state-of-the-art diffusion
models (e.g., Dream 7B, LLaDA 8B) suffer from slow inference. While they match
the quality of similarly sized Autoregressive (AR) Models (e.g., Qwen2.5 7B,
Llama3 8B), their iterative denoising requires multiple full-sequence forward
passes, resulting in high computational costs and latency, particularly for
long input prompts and long-context scenarios. Furthermore, parallel token
generation introduces token incoherence problems, and current sampling
heuristics suffer from significant quality drops with decreasing denoising
steps. We address these limitations with two training-free techniques. First,
we propose FreeCache, a Key-Value (KV) approximation caching technique that
reuses stable KV projections across denoising steps, effectively reducing the
computational cost of DLM inference. Second, we introduce Guided Diffusion, a
training-free method that uses a lightweight pretrained autoregressive model to
supervise token unmasking, dramatically reducing the total number of denoising
iterations without sacrificing quality. We conduct extensive evaluations on
open-source reasoning benchmarks, and our combined methods deliver up to a 34x
end-to-end speedup without compromising accuracy. For the first time, diffusion
language models achieve a comparable and even faster latency as the widely
adopted autoregressive models. Our work successfully paved the way for scaling
up the diffusion language model to a broader scope of applications across
different domains.

</details>


### [145] [Scaling External Knowledge Input Beyond Context Windows of LLMs via Multi-Agent Collaboration](https://arxiv.org/pdf/2505.21471)
*Zijun Liu, Zhennan Wan, Peng Li, Ming Yan, Ji Zhang, Fei Huang, Yang Liu*

Main category: cs.CL

TL;DR: ExtAgents, a multi-agent framework, addresses bottlenecks in knowledge synchronization and reasoning for LLMs, improving scalability without longer-context training.


<details>
  <summary>Details</summary>
Motivation: To overcome the limited context window of LLMs and information loss in existing methods, enabling better integration of external knowledge for complex tasks.

Method: Develops ExtAgents, a multi-agent framework, to handle massive input distributionally, tested on ∞Bench+ and other datasets.

Result: Significantly outperforms non-training methods in performance and efficiency, regardless of knowledge input size.

Conclusion: ExtAgents enhances LLM scalability and efficiency, with potential for real-world applications through further agent coordination studies.

Abstract: With the rapid advancement of post-training techniques for reasoning and
information seeking, large language models (LLMs) can incorporate a large
quantity of retrieved knowledge to solve complex tasks. However, the limited
context window of LLMs obstructs scaling the amount of external knowledge
input, prohibiting further improvement, especially for tasks requiring
significant amount of external knowledge. Existing context window extension
methods inevitably cause information loss. LLM-based multi-agent methods emerge
as a new paradigm to handle massive input in a distributional manner, where we
identify two core bottlenecks in existing knowledge synchronization and
reasoning processes. In this work, we develop a multi-agent framework,
$\textbf{ExtAgents}$, to overcome the bottlenecks and enable better scalability
in inference-time knowledge integration without longer-context training.
Benchmarked with our enhanced multi-hop question answering test,
$\textbf{$\boldsymbol{\infty}$Bench+}$, and other public test sets including
long survey generation, ExtAgents significantly enhances the performance over
existing non-training methods with the same amount of external knowledge input,
regardless of whether it falls $\textit{within or exceeds the context window}$.
Moreover, the method maintains high efficiency due to high parallelism. Further
study in the coordination of LLM agents on increasing external knowledge input
could benefit real-world applications.

</details>


### [146] [Are Language Models Consequentialist or Deontological Moral Reasoners?](https://arxiv.org/pdf/2505.21479)
*Keenan Samway, Max Kleiman-Weiner, David Guzman Piedrahita, Rada Mihalcea, Bernhard Schölkopf, Zhijing Jin*

Main category: cs.CL

TL;DR: The paper analyzes moral reasoning in LLMs using 600+ trolley problems, revealing a preference for deontological reasoning in chains-of-thought and consequentialist explanations.


<details>
  <summary>Details</summary>
Motivation: Understanding how LLMs handle ethically complex scenarios is critical for safe deployment in high-stakes fields like healthcare and law.

Method: Large-scale analysis of moral reasoning traces in LLMs using a taxonomy based on consequentialism and deontology, tested with 600+ trolley problems.

Result: LLMs favor deontological principles in reasoning traces but shift to consequentialist rationales in post-hoc explanations.

Conclusion: The study provides a framework for understanding LLM ethical reasoning, aiding safe and interpretable deployment in decision-making.

Abstract: As AI systems increasingly navigate applications in healthcare, law, and
governance, understanding how they handle ethically complex scenarios becomes
critical. Previous work has mainly examined the moral judgments in large
language models (LLMs), rather than their underlying moral reasoning process.
In contrast, we focus on a large-scale analysis of the moral reasoning traces
provided by LLMs. Furthermore, unlike prior work that attempted to draw
inferences from only a handful of moral dilemmas, our study leverages over 600
distinct trolley problems as probes for revealing the reasoning patterns that
emerge within different LLMs. We introduce and test a taxonomy of moral
rationales to systematically classify reasoning traces according to two main
normative ethical theories: consequentialism and deontology. Our analysis
reveals that LLM chains-of-thought tend to favor deontological principles based
on moral obligations, while post-hoc explanations shift notably toward
consequentialist rationales that emphasize utility. Our framework provides a
foundation for understanding how LLMs process and articulate ethical
considerations, an important step toward safe and interpretable deployment of
LLMs in high-stakes decision-making environments. Our code is available at
https://github.com/keenansamway/moral-lens .

</details>


### [147] [UI-Genie: A Self-Improving Approach for Iteratively Boosting MLLM-based Mobile GUI Agents](https://arxiv.org/pdf/2505.21496)
*Han Xiao, Guozhi Wang, Yuxiang Chai, Zimu Lu, Weifeng Lin, Hao He, Lue Fan, Liuyang Bian, Rui Hu, Liang Liu, Shuai Ren, Yafei Wen, Xiaoxin Chen, Aojun Zhou, Hongsheng Li*

Main category: cs.CL

TL;DR: UI-Genie is a self-improving framework for GUI agents, addressing trajectory verification and scalable training data via a reward model and self-improving pipeline. It achieves state-of-the-art performance and includes open-source datasets.


<details>
  <summary>Details</summary>
Motivation: The challenges of verifying GUI agent trajectories and generating scalable, high-quality training data motivate the development of UI-Genie.

Method: UI-Genie uses a reward model (UI-Genie-RM) with an image-text architecture and a self-improving pipeline for data generation and agent enhancement.

Result: UI-Genie achieves top performance on GUI benchmarks through three generations of self-improvement, supported by synthetic datasets.

Conclusion: UI-Genie successfully addresses key GUI agent challenges, offering an open-source framework and datasets for future research.

Abstract: In this paper, we introduce UI-Genie, a self-improving framework addressing
two key challenges in GUI agents: verification of trajectory outcome is
challenging and high-quality training data are not scalable. These challenges
are addressed by a reward model and a self-improving pipeline, respectively.
The reward model, UI-Genie-RM, features an image-text interleaved architecture
that efficiently pro- cesses historical context and unifies action-level and
task-level rewards. To sup- port the training of UI-Genie-RM, we develop
deliberately-designed data genera- tion strategies including rule-based
verification, controlled trajectory corruption, and hard negative mining. To
address the second challenge, a self-improvement pipeline progressively expands
solvable complex GUI tasks by enhancing both the agent and reward models
through reward-guided exploration and outcome verification in dynamic
environments. For training the model, we generate UI- Genie-RM-517k and
UI-Genie-Agent-16k, establishing the first reward-specific dataset for GUI
agents while demonstrating high-quality synthetic trajectory gen- eration
without manual annotation. Experimental results show that UI-Genie achieves
state-of-the-art performance across multiple GUI agent benchmarks with three
generations of data-model self-improvement. We open-source our complete
framework implementation and generated datasets to facilitate further research
in https://github.com/Euphoria16/UI-Genie.

</details>


### [148] [Silence is Not Consensus: Disrupting Agreement Bias in Multi-Agent LLMs via Catfish Agent for Clinical Decision Making](https://arxiv.org/pdf/2505.21503)
*Yihan Wang, Qiao Yan, Zhenghao Xing, Lihao Liu, Junjun He, Chi-Wing Fu, Xiaowei Hu, Pheng-Ann Heng*

Main category: cs.CL

TL;DR: The paper introduces a Catfish Agent to counter Silent Agreement in multi-agent LLM frameworks for clinical question answering, improving diagnostic accuracy by fostering critical analysis.


<details>
  <summary>Details</summary>
Motivation: To address the issue of Silent Agreement, where agents prematurely agree on diagnoses without thorough analysis, particularly in complex cases.

Method: Proposes a Catfish Agent with two mechanisms: complexity-aware intervention and tone-calibrated intervention to challenge consensus and stimulate deeper reasoning.

Result: Outperforms single- and multi-agent LLMs, including GPT-4o and DeepSeek-R1, on medical Q&A and VQA benchmarks.

Conclusion: The Catfish Agent effectively mitigates Silent Agreement, enhancing diagnostic accuracy in clinical question answering.

Abstract: Large language models (LLMs) have demonstrated strong potential in clinical
question answering, with recent multi-agent frameworks further improving
diagnostic accuracy via collaborative reasoning. However, we identify a
recurring issue of Silent Agreement, where agents prematurely converge on
diagnoses without sufficient critical analysis, particularly in complex or
ambiguous cases. We present a new concept called Catfish Agent, a
role-specialized LLM designed to inject structured dissent and counter silent
agreement. Inspired by the ``catfish effect'' in organizational psychology, the
Catfish Agent is designed to challenge emerging consensus to stimulate deeper
reasoning. We formulate two mechanisms to encourage effective and context-aware
interventions: (i) a complexity-aware intervention that modulates agent
engagement based on case difficulty, and (ii) a tone-calibrated intervention
articulated to balance critique and collaboration. Evaluations on nine medical
Q&A and three medical VQA benchmarks show that our approach consistently
outperforms both single- and multi-agent LLMs frameworks, including leading
commercial models such as GPT-4o and DeepSeek-R1.

</details>


### [149] [How does Alignment Enhance LLMs' Multilingual Capabilities? A Language Neurons Perspective](https://arxiv.org/pdf/2505.21505)
*Shimao Zhang, Zhejian Lai, Xiang Liu, Shuaijie She, Xiao Liu, Yeyun Gong, Shujian Huang, Jiajun Chen*

Main category: cs.CL

TL;DR: The paper introduces a finer-grained neuron identification algorithm to analyze LLMs' multilingual capabilities, dividing the internal process into four parts and examining neuron types before and after alignment.


<details>
  <summary>Details</summary>
Motivation: To enhance understanding of LLMs' multilingual mechanisms by identifying language-specific and language-agnostic neurons and analyzing their roles.

Method: Proposes a neuron identification algorithm to detect language-specific, language-related, and language-agnostic neurons, and divides LLMs' multilingual inference into four parts.

Result: Provides empirical insights into multilingual alignment, including the phenomenon of 'Spontaneous Multilingual Alignment,' and the roles of different neuron types.

Conclusion: The study offers valuable insights for understanding and improving multilingual capabilities in LLMs through neuron-level analysis.

Abstract: Multilingual Alignment is an effective and representative paradigm to enhance
LLMs' multilingual capabilities, which transfers the capabilities from the
high-resource languages to the low-resource languages. Meanwhile, some
researches on language-specific neurons reveal that there are language-specific
neurons that are selectively activated in LLMs when processing different
languages. This provides a new perspective to analyze and understand LLMs'
mechanisms more specifically in multilingual scenarios. In this work, we
propose a new finer-grained neuron identification algorithm, which detects
language neurons~(including language-specific neurons and language-related
neurons) and language-agnostic neurons. Furthermore, based on the
distributional characteristics of different types of neurons, we divide the
LLMs' internal process for multilingual inference into four parts: (1)
multilingual understanding, (2) shared semantic space reasoning, (3)
multilingual output space transformation, and (4) vocabulary space outputting.
Additionally, we systematically analyze the models before and after alignment
with a focus on different types of neurons. We also analyze the phenomenon of
''Spontaneous Multilingual Alignment''. Overall, our work conducts a
comprehensive investigation based on different types of neurons, providing
empirical results and valuable insights for better understanding multilingual
alignment and multilingual capabilities of LLMs.

</details>


### [150] [WizardLM: Empowering large pre-trained language models to follow complex instructions](https://arxiv.org/pdf/2304.12244)
*Can Xu, Qingfeng Sun, Kai Zheng, Xiubo Geng, Pu Zhao, Jiazhan Feng, Chongyang Tao, Qingwei Lin, Daxin Jiang*

Main category: cs.CL

TL;DR: The paper introduces Evol-Instruct, a method to generate complex instruction data using LLMs, and fine-tunes LLaMA to create WizardLM, which outperforms human-created instructions and rivals ChatGPT in many skills.


<details>
  <summary>Details</summary>
Motivation: Manually creating high-complexity instruction data is time-consuming and challenging for humans, prompting the need for an automated solution.

Method: Uses Evol-Instruct to rewrite initial instructions into more complex ones, then fine-tunes LLaMA with the generated data to create WizardLM.

Result: WizardLM outperforms human-created instructions and achieves over 90% of ChatGPT's capacity in 17 out of 29 skills, as per GPT-4 evaluation.

Conclusion: AI-evolved instructions are a promising direction for enhancing LLMs, though WizardLM still lags behind ChatGPT in some areas.

Abstract: Training large language models (LLMs) with open-domain instruction following
data brings colossal success. However, manually creating such instruction data
is very time-consuming and labor-intensive. Moreover, humans may struggle to
produce high-complexity instructions. In this paper, we show an avenue for
creating large amounts of instruction data with varying levels of complexity
using LLM instead of humans. Starting with an initial set of instructions, we
use our proposed Evol-Instruct to rewrite them step by step into more complex
instructions. Then, we mix all generated instruction data to fine-tune LLaMA.
We call the resulting model WizardLM. Human evaluations on a
complexity-balanced test bed and Vicuna's testset show that instructions from
Evol-Instruct are superior to human-created ones. By analyzing the human
evaluation results of the high complexity part, we demonstrate that outputs
from our WizardLM are preferred to outputs from OpenAI ChatGPT. In GPT-4
automatic evaluation, WizardLM achieves more than 90\% capacity of ChatGPT on
17 out of 29 skills. Even though WizardLM still lags behind ChatGPT in some
aspects, our findings suggest that fine-tuning with AI-evolved instructions is
a promising direction for enhancing LLMs. Our code and data are public at
https://github.com/nlpxucan/WizardLM

</details>


### [151] [WizardCoder: Empowering Code Large Language Models with Evol-Instruct](https://arxiv.org/pdf/2306.08568)
*Ziyang Luo, Can Xu, Pu Zhao, Qingfeng Sun, Xiubo Geng, Wenxiang Hu, Chongyang Tao, Jing Ma, Qingwei Lin, Daxin Jiang*

Main category: cs.CL

TL;DR: WizardCoder enhances Code LLMs with complex instruction fine-tuning using Evol-Instruct, outperforming open-source models and some closed LLMs on benchmarks.


<details>
  <summary>Details</summary>
Motivation: Existing Code LLMs lack instruction fine-tuning, limiting their performance. WizardCoder addresses this gap.

Method: Adapts Evol-Instruct for code instruction fine-tuning, evaluated on HumanEval, HumanEval+, MBPP, and DS-1000 benchmarks.

Result: Surpasses open-source Code LLMs and outperforms closed models like Claude and Bard on HumanEval and HumanEval+.

Conclusion: WizardCoder demonstrates superior performance, with public availability of code, model, and data.

Abstract: Code Large Language Models (Code LLMs), such as StarCoder, have demonstrated
exceptional performance in code-related tasks. However, most existing models
are solely pre-trained on extensive raw code data without instruction
fine-tuning. In this paper, we introduce WizardCoder, which empowers Code LLMs
with complex instruction fine-tuning, by adapting the Evol-Instruct method to
the domain of code. Through comprehensive experiments on four prominent code
generation benchmarks, namely HumanEval, HumanEval+, MBPP, and DS-1000, we
unveil the exceptional capabilities of our model. It surpasses all other
open-source Code LLMs by a substantial margin. Moreover, our model even
outperforms the largest closed LLMs, Anthropic's Claude and Google's Bard, on
HumanEval and HumanEval+. Our code, model weights, and data are public at
https://github.com/nlpxucan/WizardLM

</details>


### [152] [Tradeoffs Between Alignment and Helpfulness in Language Models with Steering Methods](https://arxiv.org/pdf/2401.16332)
*Yotam Wolf, Noam Wies, Dorin Shteyman, Binyamin Rothberg, Yoav Levine, Amnon Shashua*

Main category: cs.CL

TL;DR: The paper explores the tradeoff between alignment and helpfulness in language models using representation engineering, proposing a theoretical framework and empirical validation.


<details>
  <summary>Details</summary>
Motivation: To understand and quantify the tradeoff between alignment (enhancing desired behaviors) and helpfulness (basic task performance) in language models when using representation engineering.

Method: Proposes a theoretical framework to bound alignment and helpfulness, then validates it empirically with representation engineering.

Result: Alignment increases linearly with the representation engineering vector's norm, while helpfulness decreases quadratically, identifying an efficient regime for its use.

Conclusion: Representation engineering guarantees alignment but harms helpfulness, with a tradeoff that can be optimized within certain bounds.

Abstract: Language model alignment has become an important component of AI safety,
allowing safe interactions between humans and language models, by enhancing
desired behaviors and inhibiting undesired ones. It is often done by tuning the
model or inserting preset aligning prompts. Recently, representation
engineering, a method which alters the model's behavior via changing its
representations post-training, was shown to be effective in aligning LLMs (Zou
et al., 2023a). Representation engineering yields gains in alignment oriented
tasks such as resistance to adversarial attacks and reduction of social biases,
but was also shown to cause a decrease in the ability of the model to perform
basic tasks. In this paper we study the tradeoff between the increase in
alignment and decrease in helpfulness of the model. We propose a theoretical
framework which provides bounds for these two quantities, and demonstrate their
relevance empirically. First, we find that under the conditions of our
framework, alignment can be guaranteed with representation engineering, and at
the same time that helpfulness is harmed in the process. Second, we show that
helpfulness is harmed quadratically with the norm of the representation
engineering vector, while the alignment increases linearly with it, indicating
a regime in which it is efficient to use representation engineering. We
validate our findings empirically, and chart the boundaries to the usefulness
of representation engineering for alignment.

</details>


### [153] [LLMs with Industrial Lens: Deciphering the Challenges and Prospects -- A Survey](https://arxiv.org/pdf/2402.14558)
*Ashok Urlana, Charaka Vinayak Kumar, Ajeet Kumar Singh, Bala Mallikarjunarao Garlapati, Srinivasa Rao Chalamala, Rahul Mishra*

Main category: cs.CL

TL;DR: The paper explores challenges and opportunities in using large language models (LLMs) in industry, involving surveys, research questions, and analysis of 68 papers.


<details>
  <summary>Details</summary>
Motivation: LLMs are widely used in industry, but their challenges and potential improvements need exploration.

Method: Conducted a survey with industry practitioners, developed four research questions, and analyzed 68 industry papers.

Result: Identified obstacles and opportunities in leveraging LLMs for industrial applications.

Conclusion: The study provides insights into LLM utilization in industry and maintains a Github repository for ongoing research.

Abstract: Large language models (LLMs) have become the secret ingredient driving
numerous industrial applications, showcasing their remarkable versatility
across a diverse spectrum of tasks. From natural language processing and
sentiment analysis to content generation and personalized recommendations,
their unparalleled adaptability has facilitated widespread adoption across
industries. This transformative shift driven by LLMs underscores the need to
explore the underlying associated challenges and avenues for enhancement in
their utilization. In this paper, our objective is to unravel and evaluate the
obstacles and opportunities inherent in leveraging LLMs within an industrial
context. To this end, we conduct a survey involving a group of industry
practitioners, develop four research questions derived from the insights
gathered, and examine 68 industry papers to address these questions and derive
meaningful conclusions. We maintain the Github repository with the most recent
papers in the field.

</details>


### [154] [An In-depth Evaluation of Large Language Models in Sentence Simplification with Error-based Human Assessment](https://arxiv.org/pdf/2403.04963)
*Xuanxin Wu, Yuki Arase*

Main category: cs.CL

TL;DR: The study evaluates LLMs' simplification abilities, highlighting issues with current evaluation methods and proposing a reliable error-based human annotation framework. GPT-4 outperforms others but struggles with lexical paraphrasing, and automatic metrics lack sensitivity.


<details>
  <summary>Details</summary>
Motivation: Existing evaluation methods for LLMs' simplification abilities are either superficial or overly complex, leading to unreliable results. This study aims to provide deeper insights while ensuring evaluation reliability.

Method: An error-based human annotation framework is designed to assess LLMs (GPT-4, Qwen2.5-72B, Llama-3.2-3B). Meta-evaluations of automatic metrics are also conducted.

Result: GPT-4 generates fewer errors but struggles with lexical paraphrasing. Automatic metrics lack sensitivity for high-quality simplifications.

Conclusion: The study underscores the limitations of current evaluation methods and LLMs, advocating for more reliable and sensitive assessment frameworks.

Abstract: Recent studies have used both automatic metrics and human evaluations to
assess the simplification abilities of LLMs. However, the suitability of
existing evaluation methodologies for LLMs remains in question. First, the
suitability of current automatic metrics on LLMs' simplification evaluation is
still uncertain. Second, current human evaluation approaches in sentence
simplification often fall into two extremes: they are either too superficial,
failing to offer a clear understanding of the models' performance, or overly
detailed, making the annotation process complex and prone to inconsistency,
which in turn affects the evaluation's reliability. To address these problems,
this study provides in-depth insights into LLMs' performance while ensuring the
reliability of the evaluation. We design an error-based human annotation
framework to assess the LLMs' simplification capabilities. We select both
closed-source and open-source LLMs, including GPT-4, Qwen2.5-72B, and
Llama-3.2-3B. We believe that these models offer a representative selection
across large, medium, and small sizes of LLMs. Results show that GPT-4
generally generates fewer erroneous simplification outputs compared to the
current state-of-the-art. However, LLMs have their limitations, as seen in
GPT-4's struggles with lexical paraphrasing. Results show that LLMs generally
generate fewer erroneous simplification outputs compared to the previous
state-of-the-art. However, LLMs have their limitations, as seen in GPT-4's and
Qwen2.5-72B's struggle with lexical paraphrasing. Furthermore, we conduct
meta-evaluations on widely used automatic metrics using our human annotations.
We find that these metrics lack sufficient sensitivity to assess the overall
high-quality simplifications, particularly those generated by high-performance
LLMs.

</details>


### [155] [Bias-Augmented Consistency Training Reduces Biased Reasoning in Chain-of-Thought](https://arxiv.org/pdf/2403.05518)
*James Chua, Edward Rees, Hunar Batra, Samuel R. Bowman, Julian Michael, Ethan Perez, Miles Turpin*

Main category: cs.CL

TL;DR: Chain-of-thought (CoT) prompting can misrepresent model reasoning due to biases. A new dataset identifies 9 biases in GPT-3.5-Turbo and Llama-8b. Bias-augmented consistency training (BCT) reduces biased reasoning by 86% on held-out tasks and generalizes to other biases.


<details>
  <summary>Details</summary>
Motivation: To address the issue of biased reasoning in CoT prompting, which can misrepresent model behavior and align with user opinions.

Method: Introduce BCT, an unsupervised fine-tuning method, to train models for consistent reasoning across biased and unbiased prompts. Test on 9 biases and 7 QA tasks.

Result: BCT reduces biased reasoning by 86% on held-out tasks and generalizes to other biases (37% reduction).

Conclusion: BCT is promising for mitigating unknown biases and tasks without ground truth, as it generalizes and requires no gold labels.

Abstract: Chain-of-thought prompting (CoT) has the potential to improve the
explainability of language model reasoning. But CoT can also systematically
misrepresent the factors influencing models' behavior -- for example,
rationalizing answers in line with a user's opinion.
  We first create a new dataset of 9 different biases that affect GPT-3.5-Turbo
and Llama-8b models. These consist of spurious-few-shot patterns, post hoc
rationalization, and sycophantic settings. Models switch to the answer implied
by the bias, without mentioning the effect of the bias in the CoT.
  To mitigate this biased reasoning problem, we introduce bias-augmented
consistency training (BCT), an unsupervised fine-tuning scheme that trains
models to give consistent reasoning across prompts with and without biasing
features. We construct a suite testing nine forms of biased reasoning on seven
question-answering tasks, and find that applying BCT to GPT-3.5-Turbo with one
bias reduces the rate of biased reasoning by 86\% on held-out tasks. Moreover,
this model generalizes to other forms of bias, reducing biased reasoning on
held-out biases by an average of 37\%. As BCT generalizes to held-out biases
and does not require gold labels, this method may hold promise for reducing
biased reasoning from as-of-yet unknown biases and on tasks where ground truth
reasoning is unavailable.

</details>


### [156] [Don't Half-listen: Capturing Key-part Information in Continual Instruction Tuning](https://arxiv.org/pdf/2403.10056)
*Yongquan He, Wenyuan Zhang, Xuancheng Huang, Peng Zhang, Lingxun Meng, Xiang Zhou, Ke Zeng, Xunliang Cai*

Main category: cs.CL

TL;DR: A novel continual instruction tuning method (KPIG) is proposed to mitigate catastrophic forgetting in LLMs by dynamically replaying data and refining training objectives, outperforming existing methods.


<details>
  <summary>Details</summary>
Motivation: Address the catastrophic forgetting problem in continual instruction tuning for LLMs, where existing methods fail to retain deep task-aware information.

Method: Uses Key-part Information Gain (KPIG) to dynamically replay data and refine training objectives, focusing on task-aware information. Introduces P-score and V-score metrics for evaluation.

Result: Superior performance on both seen and held-out tasks, demonstrating effective mitigation of catastrophic forgetting.

Conclusion: KPIG effectively addresses catastrophic forgetting in continual instruction tuning, improving LLM generalization and instruction-following abilities.

Abstract: Instruction tuning for large language models (LLMs) can drive them to produce
results consistent with human goals in specific downstream tasks. However, the
process of continual instruction tuning (CIT) for LLMs may bring about the
catastrophic forgetting (CF) problem, where previously learned abilities are
degraded. Recent methods try to alleviate the CF problem by modifying models or
replaying data, which may only remember the surface-level pattern of
instructions and get confused on held-out tasks. In this paper, we propose a
novel continual instruction tuning method based on Key-part Information Gain
(KPIG). Our method computes the information gain on masked parts to dynamically
replay data and refine the training objective, which enables LLMs to capture
task-aware information relevant to the correct response and alleviate
overfitting to general descriptions in instructions. In addition, we propose
two metrics, P-score and V-score, to measure the generalization and
instruction-following abilities of LLMs. Experiments demonstrate our method
achieves superior performance on both seen and held-out tasks.

</details>


### [157] [OR-Bench: An Over-Refusal Benchmark for Large Language Models](https://arxiv.org/pdf/2405.20947)
*Justin Cui, Wei-Lin Chiang, Ion Stoica, Cho-Jui Hsieh*

Main category: cs.CL

TL;DR: The paper introduces OR-Bench, a large-scale benchmark for measuring over-refusal in LLMs, addressing the challenge of systematically evaluating safety alignment side effects.


<details>
  <summary>Details</summary>
Motivation: Safety alignment in LLMs often leads to over-refusal, where models reject harmless prompts, reducing their usefulness. Current methods lack systematic measurement tools.

Method: Proposes an automated method to generate large-scale over-refusal datasets, creating OR-Bench with 80,000 prompts, including hard and toxic subsets.

Result: Evaluates 32 LLMs across 8 families using OR-Bench, providing insights into over-refusal behaviors. Datasets and code are publicly available.

Conclusion: OR-Bench aids in developing better safety-aligned LLMs by offering a standardized tool for measuring and addressing over-refusal.

Abstract: Large Language Models (LLMs) require careful safety alignment to prevent
malicious outputs. While significant research focuses on mitigating harmful
content generation, the enhanced safety often come with the side effect of
over-refusal, where LLMs may reject innocuous prompts and become less helpful.
Although the issue of over-refusal has been empirically observed, a systematic
measurement is challenging due to the difficulty of crafting prompts that can
elicit the over-refusal behaviors of LLMs. This study proposes a novel method
for automatically generating large-scale over-refusal datasets. Leveraging this
technique, we introduce OR-Bench, the first large-scale over-refusal benchmark.
OR-Bench comprises 80,000 over-refusal prompts across 10 common rejection
categories, a subset of around 1,000 hard prompts that are challenging even for
state-of-the-art LLMs, and an additional 600 toxic prompts to prevent
indiscriminate responses. We then conduct a comprehensive study to measure the
over-refusal of 32 popular LLMs across 8 model families. Our datasets are
publicly available at https://huggingface.co/bench-llms and our codebase is
open-sourced at https://github.com/justincui03/or-bench. We hope this benchmark
can help the community develop better safety aligned models.

</details>


### [158] [Predicting drug-gene relations via analogy tasks with word embeddings](https://arxiv.org/pdf/2406.00984)
*Hiroaki Yamagiwa, Ryoma Hashimoto, Kiwamu Arakane, Ken Murakami, Shou Soeda, Momose Oyama, Yihua Zhu, Mariko Okada, Hidetoshi Shimodaira*

Main category: cs.CL

TL;DR: BioConceptVec embeddings and custom PubMed-trained embeddings predict drug-gene relations via analogy tasks, outperforming large models like GPT-4.


<details>
  <summary>Details</summary>
Motivation: To leverage word embeddings for predicting drug-gene relations and improve performance using biological pathway categorization.

Method: Used BioConceptVec and custom embeddings trained on PubMed abstracts, applying analogy tasks (vector arithmetic) to predict drug-gene relations.

Result: Demonstrated that analogy tasks with embeddings predict drug-gene relations effectively, even outperforming GPT-4 in some cases.

Conclusion: Simple vector arithmetic with tailored embeddings is a powerful tool for predicting biological relations, offering a lightweight alternative to large models.

Abstract: Natural language processing (NLP) is utilized in a wide range of fields,
where words in text are typically transformed into feature vectors called
embeddings. BioConceptVec is a specific example of embeddings tailored for
biology, trained on approximately 30 million PubMed abstracts using models such
as skip-gram. Generally, word embeddings are known to solve analogy tasks
through simple vector arithmetic. For example, subtracting the vector for man
from that of king and then adding the vector for woman yields a point that lies
closer to queen in the embedding space. In this study, we demonstrate that
BioConceptVec embeddings, along with our own embeddings trained on PubMed
abstracts, contain information about drug-gene relations and can predict target
genes from a given drug through analogy computations. We also show that
categorizing drugs and genes using biological pathways improves performance.
Furthermore, we illustrate that vectors derived from known relations in the
past can predict unknown future relations in datasets divided by year. Despite
the simplicity of implementing analogy tasks as vector additions, our approach
demonstrated performance comparable to that of large language models such as
GPT-4 in predicting drug-gene relations.

</details>


### [159] [NAP^2: A Benchmark for Naturalness and Privacy-Preserving Text Rewriting by Learning from Human](https://arxiv.org/pdf/2406.03749)
*Shuo Huang, William MacLean, Xiaoxi Kang, Qiongkai Xu, Zhuang Li, Xingliang Yuan, Gholamreza Haffari, Lizhen Qu*

Main category: cs.CL

TL;DR: The paper introduces a method to sanitize sensitive text in cloud-based LLM interactions using human-inspired strategies (deletion and abstraction) and presents a corpus (NAP^2) for this purpose.


<details>
  <summary>Details</summary>
Motivation: Concerns over privacy in cloud-based LLM interactions due to potential exposure of sensitive information.

Method: Uses two human-inspired strategies: deleting sensitive expressions and obscuring details by abstraction. A corpus (NAP^2) is curated via crowdsourcing and LLMs.

Result: The approach yields more natural rewrites and better balances privacy protection and data utility compared to prior anonymization methods.

Conclusion: The proposed method effectively addresses privacy concerns in LLM interactions while maintaining data utility, with the NAP^2 corpus supporting further research.

Abstract: The widespread use of cloud-based Large Language Models (LLMs) has heightened
concerns over user privacy, as sensitive information may be inadvertently
exposed during interactions with these services. To protect privacy before
sending sensitive data to those models, we suggest sanitizing sensitive text
using two common strategies used by humans: i) deleting sensitive expressions,
and ii) obscuring sensitive details by abstracting them. To explore the issues
and develop a tool for text rewriting, we curate the first corpus, coined
NAP^2, through both crowdsourcing and the use of large language models (LLMs).
Compared to the prior works on anonymization, the human-inspired approaches
result in more natural rewrites and offer an improved balance between privacy
protection and data utility, as demonstrated by our extensive experiments.
Researchers interested in accessing the dataset are encouraged to contact the
first or corresponding author via email.

</details>


### [160] [Raising the Bar: Investigating the Values of Large Language Models via Generative Evolving Testing](https://arxiv.org/pdf/2406.14230)
*Han Jiang, Xiaoyuan Yi, Zhihua Wei, Ziang Xiao, Shu Wang, Xing Xie*

Main category: cs.CL

TL;DR: GETA is a generative evolving testing approach to dynamically assess LLMs' value alignment, addressing the limitations of static benchmarks.


<details>
  <summary>Details</summary>
Motivation: Static benchmarks for evaluating LLMs' ethical alignment suffer from chronoeffect, leaking into training data or becoming outdated as models evolve.

Method: GETA uses adaptive testing to dynamically generate test items tailored to model capability, co-evolving with LLMs.

Result: GETA effectively creates difficulty-tailored test items and provides evaluations consistent with models' performance on unseen data.

Conclusion: GETA offers a robust framework for evolving LLM evaluation, addressing chronoeffect and improving alignment assessment.

Abstract: Warning: Contains harmful model outputs. Despite significant advancements,
the propensity of Large Language Models (LLMs) to generate harmful and
unethical content poses critical challenges. Measuring value alignment of LLMs
becomes crucial for their regulation and responsible deployment. Although
numerous benchmarks have been constructed to assess social bias, toxicity, and
ethical issues in LLMs, those static benchmarks suffer from evaluation
chronoeffect, in which, as models rapidly evolve, existing benchmarks may leak
into training data or become saturated, overestimating ever-developing LLMs. To
tackle this problem, we propose GETA, a novel generative evolving testing
approach based on adaptive testing methods in measurement theory. Unlike
traditional adaptive testing methods that rely on a static test item pool, GETA
probes the underlying moral boundaries of LLMs by dynamically generating test
items tailored to model capability. GETA co-evolves with LLMs by learning a
joint distribution of item difficulty and model value conformity, thus
effectively addressing evaluation chronoeffect. We evaluated various popular
LLMs with GETA and demonstrated that 1) GETA can dynamically create
difficulty-tailored test items and 2) GETA's evaluation results are more
consistent with models' performance on unseen OOD and i.i.d. items, laying the
groundwork for future evaluation paradigms.

</details>


### [161] [Can Small Language Models Learn, Unlearn, and Retain Noise Patterns?](https://arxiv.org/pdf/2407.00996)
*Nicy Scaria, Silvester John Joseph Kennedy, Deepak Subramani*

Main category: cs.CL

TL;DR: The study explores how Small Language Models (SLMs) handle noise, comparing four models (Olmo, Qwen1.5, Gemma1.1, Phi2) in learning, retaining, and eliminating noise. Results show varying adaptability, with Phi2 resisting noise due to high-quality pretraining and Gemma excelling in multilingual noise. Clean data training mitigates noise effects.


<details>
  <summary>Details</summary>
Motivation: The need for efficient SLMs in resource-constrained environments and the lack of research on their noise-handling capabilities motivated this study.

Method: Four pretrained SLMs were instruction-tuned on noise-free data, tested with in-context noise examples, and then trained with noise patterns to assess adaptability.

Result: Smaller models like Olmo adapted quickly to noise, while Phi2 resisted noise due to high-quality pretraining. Gemma handled multilingual noise well. Clean data training reduced noise effects.

Conclusion: The findings offer practical strategies for developing robust SLMs, highlighting the importance of pretraining data quality and multilingual capabilities for noise resilience.

Abstract: With the growing need for efficient language models in resource-constrained
environments, Small Language Models (SLMs) have emerged as compact and
practical alternatives to Large Language Models (LLMs). While studies have
explored noise handling in LLMs, little is known about how SLMs handle noise, a
critical factor for their reliable real-world deployment. This study
investigates the ability of SLMs with parameters between 1 and 3 billion to
learn, retain, and subsequently eliminate different types of noise (word flip,
character flip, transliteration, irrelevant content, and contradictory
information). Four pretrained SLMs (Olmo 1B, Qwen1.5 1.8B, Gemma1.1 2B, and
Phi2 2.7B) were instruction-tuned on noise-free data and tested with in-context
examples to assess noise learning. Subsequently, noise patterns were introduced
in instruction tuning to assess their adaptability. The results revealed
differences in how models handle noise, with smaller models like Olmo quickly
adapting to noise patterns. Phi2's carefully curated, structured, and
high-quality pretraining data enabled resistance to character level,
transliteration, and counterfactual noise, while Gemma adapted successfully to
transliteration noise through its multilingual pretraining. Subsequent clean
data training effectively mitigated noise effects. These findings provide
practical strategies for developing robust SLMs for real-world applications.

</details>


### [162] [Fine-Tuning on Diverse Reasoning Chains Drives Within-Inference CoT Refinement in LLMs](https://arxiv.org/pdf/2407.03181)
*Haritz Puerto, Tilek Chubakov, Xiaodan Zhu, Harish Tayyar Madabushi, Iryna Gurevych*

Main category: cs.CL

TL;DR: Fine-tuning LLMs to generate Diverse Chains of Thought (DCoT) in one step improves reasoning performance over traditional CoT methods, especially for complex tasks.


<details>
  <summary>Details</summary>
Motivation: To enhance LLM reasoning by enabling within-inference refinement of reasoning chains without external feedback.

Method: Fine-tuning LLMs to produce multiple diverse reasoning chains (DCoT) in a single inference step.

Result: DCoT outperforms CoT baselines across model sizes (1.3B to 70B), particularly in tasks with large result spaces.

Conclusion: DCoT enables self-improvement in reasoning, demonstrating significant performance gains and refinement capabilities.

Abstract: Requiring a large language model (LLM) to generate intermediary reasoning
steps, known as Chain of Thought (CoT), has been shown to be an effective way
of boosting performance. Previous approaches have focused on generating
multiple independent CoTs, combining them through ensembling or other post-hoc
strategies to enhance reasoning. In this work, we introduce a novel approach
where LLMs are fine-tuned to generate a sequence of Diverse Chains of Thought
(DCoT) within a single inference step, which is fundamentally different from
prior work that primarily operate on parallel CoT generations. DCoT allows LLMs
to gain the ability to perform within-inference refinement of reasoning chains
without requiring external feedback. Through a rigorous set of experiments
spanning a wide range of tasks that require various reasoning types, we show
that fine-tuning on DCoT improves performance over the CoT baseline across
model families and scales (1.3B to 70B). These improvements are particularly
impactful for tasks with a large result state space, such as those involving
numeric answers. Our work is also significant because both quantitative
analyses and manual evaluations reveal the observed gains stem from the models'
ability to refine an initial reasoning chain by generating a second, improved
chain within the same inference step, demonstrating previously elusive
self-improvement. Our code and data are publicly available at
https://github.com/UKPLab/acl2025-diverse-cot.

</details>


### [163] [Pandora's Box or Aladdin's Lamp: A Comprehensive Analysis Revealing the Role of RAG Noise in Large Language Models](https://arxiv.org/pdf/2408.13533)
*Jinyang Wu, Shuai Zhang, Feihu Che, Mingkuan Feng, Pengpeng Shao, Jianhua Tao*

Main category: cs.CL

TL;DR: The paper introduces a Noise RAG Benchmark (NoiserBench) to evaluate the impact of seven linguistic noise types on LLMs, revealing that noise can be beneficial or harmful, influencing model performance differently.


<details>
  <summary>Details</summary>
Motivation: To address the limitations of current RAG models in handling diverse noise types and assumptions about noise being universally detrimental, the study aims to provide a more realistic evaluation framework.

Method: The authors define seven noise types, create NoiserBench, and empirically evaluate eight LLMs across datasets and reasoning tasks.

Result: Noise is categorized into beneficial and harmful types, with harmful noise impairing performance and beneficial noise potentially enhancing model capabilities.

Conclusion: The findings guide the development of robust RAG solutions and better hallucination mitigation in varied retrieval scenarios.

Abstract: Retrieval-Augmented Generation (RAG) has emerged as a crucial method for
addressing hallucinations in large language models (LLMs). While recent
research has extended RAG models to complex noisy scenarios, these explorations
often confine themselves to limited noise types and presuppose that noise is
inherently detrimental to LLMs, potentially deviating from real-world retrieval
environments and restricting practical applicability. In this paper, we define
seven distinct noise types from a linguistic perspective and establish a Noise
RAG Benchmark (NoiserBench), a comprehensive evaluation framework encompassing
multiple datasets and reasoning tasks. Through empirical evaluation of eight
representative LLMs with diverse architectures and scales, we reveal that these
noises can be further categorized into two practical groups: noise that is
beneficial to LLMs (aka beneficial noise) and noise that is harmful to LLMs
(aka harmful noise). While harmful noise generally impairs performance,
beneficial noise may enhance several aspects of model capabilities and overall
performance. Our analysis offers insights for developing more robust, adaptable
RAG solutions and mitigating hallucinations across diverse retrieval scenarios.

</details>


### [164] [GALLa: Graph Aligned Large Language Models for Improved Source Code Understanding](https://arxiv.org/pdf/2409.04183)
*Ziyin Zhang, Hang Yu, Shijie Li, Peng Di, Jianguo Li, Rui Wang*

Main category: cs.CL

TL;DR: GALLa integrates graph neural networks and cross-modal alignment to inject structural code information into LLMs during finetuning, improving performance without inference costs.


<details>
  <summary>Details</summary>
Motivation: Existing code models either ignore structural information or modify Transformer architectures, limiting scalability and compatibility with pretrained LLMs.

Method: GALLa uses graph neural networks and cross-modal alignment to inject structural code information into LLMs as an auxiliary task during finetuning.

Result: Experiments on five code tasks with seven LLMs (350M to 14B) show consistent improvements, even for powerful models like LLaMA3 and Qwen2.5-Coder.

Conclusion: GALLa is a scalable, model-agnostic framework that enhances code LLMs by incorporating structural information without inference overhead.

Abstract: Programming languages possess rich semantic information - such as data flow -
that is represented by graphs and not available from the surface form of source
code. Recent code language models have scaled to billions of parameters, but
model source code solely as text tokens while ignoring any other structural
information. Conversely, models that do encode structural information of code
make modifications to the Transformer architecture, limiting their scale and
compatibility with pretrained LLMs. In this work, we take the best of both
worlds with GALLa - Graph Aligned Large Language Models. GALLa utilizes graph
neural networks and cross-modal alignment technologies to inject the structural
information of code into LLMs as an auxiliary task during finetuning. This
framework is both model-agnostic and task-agnostic, as it can be applied to any
code LLM for any code downstream task, and requires the structural graph data
only at training time from a corpus unrelated to the finetuning data, while
incurring no cost at inference time over the baseline LLM. Experiments on five
code tasks with seven different baseline LLMs ranging in size from 350M to 14B
validate the effectiveness of GALLa, demonstrating consistent improvement over
the baseline, even for powerful models such as LLaMA3 and Qwen2.5-Coder.

</details>


### [165] [Rethinking Semantic Parsing for Large Language Models: Enhancing LLM Performance with Semantic Hints](https://arxiv.org/pdf/2409.14469)
*Kaikai An, Shuzheng Si, Helan Hu, Haozhe Zhao, Yuchi Wang, Qingyan Guo, Baobao Chang*

Main category: cs.CL

TL;DR: Semantic parsing helps smaller models but harms LLMs. SENSE, a new prompting method, embeds semantic hints to boost LLM performance.


<details>
  <summary>Details</summary>
Motivation: To explore if semantic parsing benefits LLMs like it does smaller models, and to address the performance drop when directly adding parsing results.

Method: Proposed SENSE, a prompting approach that integrates semantic hints into prompts for LLMs.

Result: SENSE consistently improves LLM performance across tasks.

Conclusion: Semantic hints via SENSE enhance LLM capabilities, showing promise for better integration of semantic information.

Abstract: Semantic Parsing aims to capture the meaning of a sentence and convert it
into a logical, structured form. Previous studies show that semantic parsing
enhances the performance of smaller models (e.g., BERT) on downstream tasks.
However, it remains unclear whether the improvements extend similarly to LLMs.
In this paper, our empirical findings reveal that, unlike smaller models,
directly adding semantic parsing results into LLMs reduces their performance.
To overcome this, we propose SENSE, a novel prompting approach that embeds
semantic hints within the prompt. Experiments show that SENSE consistently
improves LLMs' performance across various tasks, highlighting the potential of
integrating semantic information to improve LLM capabilities.

</details>


### [166] [Efficient Length-Generalizable Attention via Causal Retrieval for Long-Context Language Modeling](https://arxiv.org/pdf/2410.01651)
*Xiang Hu, Zhihao Teng, Jun Zhao, Wei Wu, Kewei Tu*

Main category: cs.CL

TL;DR: Proposes Grouped Cross Attention (GCA), a novel attention mechanism for Transformers, enabling generalization to 1000x pre-training context length with constant computational cost.


<details>
  <summary>Details</summary>
Motivation: Transformers struggle with long contexts due to limited length generalization and quadratic self-attention complexity, requiring costly post-training.

Method: Splits input into chunks, retrieves top-k relevant past chunks for generation, and learns retrieval end-to-end to minimize auto-regressive loss.

Result: Achieves near-perfect accuracy in passkey retrieval for 16M context lengths (1000x training length).

Conclusion: GCA reduces computational and memory costs while maintaining long-range information access.

Abstract: Despite the success of Transformers, handling long contexts remains
challenging due to the limited length generalization and quadratic complexity
of self-attention. Thus Transformers often require post-training with a larger
attention window, significantly increasing computational and memory costs. In
this paper, we propose a novel attention mechanism based on dynamic context,
Grouped Cross Attention (GCA), which can generalize to 1000 times the
pre-training context length while maintaining the ability to access distant
information with a constant attention window size. For a given input sequence,
we split it into chunks and use each chunk to retrieve top-k relevant past
chunks for subsequent text generation. Specifically, unlike most previous works
that use an off-the-shelf retriever, our key innovation allows the retriever to
learn how to retrieve past chunks that better minimize the auto-regressive loss
of subsequent tokens in an end-to-end manner. Such a mechanism accommodates
retrieved chunks with a fixed-size attention window to achieve long-range
information access, significantly reducing computational and memory costs
during training and inference. Experiments show that GCA-based models achieve
near-perfect accuracy in passkey retrieval for 16M context lengths, which is
1000 times the training length.

</details>


### [167] [Subtle Errors in Reasoning: Preference Learning via Error-injected Self-editing](https://arxiv.org/pdf/2410.06638)
*Kaishuai Xu, Tiezheng Yu, Wenjun Hou, Yi Cheng, Chak Tou Leong, Liangyou Li, Xin Jiang, Lifeng Shang, Qun Liu, Wenjie Li*

Main category: cs.CL

TL;DR: RISE is a novel preference learning framework that injects subtle errors into LLM solutions to improve mathematical reasoning by mitigating critical errors.


<details>
  <summary>Details</summary>
Motivation: Existing methods for improving LLMs' mathematical reasoning overlook subtle errors, limiting their potential. RISE addresses this gap.

Method: RISE injects predefined subtle errors into pivotal tokens in reasoning steps, using the LLM itself to edit solutions. It combines self-edited and sampled pairs for training.

Result: RISE improves Qwen2-7B-Instruct by 3.0% on GSM8K and 7.9% on MATH with only 4.5K samples, extending benefits to logical reasoning and code generation.

Conclusion: RISE effectively mitigates subtle errors in LLMs, enhancing mathematical and broader reasoning capabilities without fine-grained sampling or annotation.

Abstract: Large Language Models (LLMs) have exhibited strong mathematical reasoning
prowess, tackling tasks ranging from basic arithmetic to advanced
competition-level problems. However, frequently occurring subtle yet critical
errors, such as miscalculations or incorrect substitutions, limit the LLMs'
full potential. Existing studies to improve mathematical ability typically
involve applying preference learning to step-wise solution pairs. Although
these methods leverage samples of varying granularity to mitigate reasoning
errors, they overlook critical subtle errors. In this work, we propose a novel
preference learning framework called eRror-Injected Self-Editing (RISE), which
injects predefined subtle errors into pivotal tokens in reasoning or
computation steps to construct hard pairs for error mitigation. In detail, RISE
uses the LLM itself to edit a small number of tokens in the solution, injecting
designed subtle errors. Then, pairs composed of self-edited solutions and their
corresponding correct ones, along with pairs of correct and incorrect solutions
obtained through sampling, are used together for subtle error-aware DPO
training. Compared with other preference learning methods, RISE further refines
the training objective without requiring fine-grained sampling or preference
annotation. Extensive experiments validate the effectiveness of RISE, with
preference learning on Qwen2-7B-Instruct yielding notable improvements of 3.0%
on GSM8K and 7.9% on MATH with only 4.5K training samples. Moreover, the effect
of error mitigation extends from mathematical reasoning to logical reasoning
and code generation.

</details>


### [168] [Conversational Code Generation: a Case Study of Designing a Dialogue System for Generating Driving Scenarios for Testing Autonomous Vehicles](https://arxiv.org/pdf/2410.09829)
*Rimvydas Rubavicius, Antonio Valerio Miceli-Barone, Alex Lascarides, Subramanian Ramamoorthy*

Main category: cs.CL

TL;DR: A natural language interface using a large language model helps non-coding experts create simulation scenarios for autonomous vehicles, improving success rates with dialogue.


<details>
  <summary>Details</summary>
Motivation: To simplify scenario specification for testing autonomous vehicles in simulation for non-coding domain experts.

Method: Design a natural language interface using an instruction-following large language model to convert utterances into symbolic programs.

Result: Feasible conversion despite small training data; dialogue increases success rate by 4.5 times.

Conclusion: Dialogue is crucial for effective simulation scenario generation, enhancing usability for non-experts.

Abstract: Cyber-physical systems like autonomous vehicles are tested in simulation
before deployment, using domain-specific programs for scenario specification.
To aid the testing of autonomous vehicles in simulation, we design a natural
language interface, using an instruction-following large language model, to
assist a non-coding domain expert in synthesising the desired scenarios and
vehicle behaviours. We show that using it to convert utterances to the symbolic
program is feasible, despite the very small training dataset. Human experiments
show that dialogue is critical to successful simulation generation, leading to
a 4.5 times higher success rate than a generation without engaging in extended
conversation.

</details>


### [169] [The Best of Both Worlds: Bridging Quality and Diversity in Data Selection with Bipartite Graph](https://arxiv.org/pdf/2410.12458)
*Minghao Wu, Thuy-Trang Vu, Lizhen Qu, Gholamreza Haffari*

Main category: cs.CL

TL;DR: GraphFilter balances quality and diversity in data selection for LLMs by modeling datasets as bipartite graphs and using a multiplicative priority function, outperforming existing methods.


<details>
  <summary>Details</summary>
Motivation: Current data selection methods for LLMs prioritize either quality or diversity, leading to suboptimal training outcomes.

Method: GraphFilter formulates data selection as a set cover problem, using a bipartite graph to connect sentences and n-grams, and a multiplicative priority function for iterative selection.

Result: GraphFilter outperforms nine baselines across six benchmarks, improving model performance and computational efficiency.

Conclusion: GraphFilter's design enhances subset selection, highlights instruction diversity's importance, and provides insights into quality-diversity trade-offs.

Abstract: The performance of large language models (LLMs) is strongly influenced by the
quality and diversity of data used during supervised fine-tuning (SFT).
However, current data selection methods often prioritize one aspect over the
other, resulting in suboptimal training outcomes. To address this, we formulate
data selection as a set cover problem and present GraphFilter, a novel approach
that balances both quality and diversity in data selection. GraphFilter models
the dataset as a bipartite graph connecting sentences to their constituent
n-grams, then employs a priority function that combines quality and diversity
metrics multiplicatively. GraphFilter iteratively selects sentences with the
highest priority, removes covered n-grams from the bipartite graph, and
recomputes priorities to reflect the changing data landscape. We validate
GraphFilter using three model backbones across six widely-used benchmarks,
demonstrating that it outperforms nine existing baselines in both model
performance and computational efficiency. Further analysis shows that our
design choices lead to more effective subset selection, underscores the value
of instruction diversity, and provides insights into how quality and diversity
interact with different subset sizes.

</details>


### [170] [BQA: Body Language Question Answering Dataset for Video Large Language Models](https://arxiv.org/pdf/2410.13206)
*Shintaro Ozaki, Kazuki Hayashi, Miyu Oba, Yusuke Sakai, Hidetaka Kamigaito, Taro Watanabe*

Main category: cs.CL

TL;DR: The paper introduces BQA, a dataset for evaluating VideoLLMs' ability to interpret body language, revealing biases and challenges in current models.


<details>
  <summary>Details</summary>
Motivation: Nonverbal communication lacks formal rules, making it hard for VideoLLMs to interpret accurately. The study aims to address this gap.

Method: Proposed BQA, a dataset with 26 emotion labels from body language videos, to test VideoLLMs' interpretation accuracy.

Result: VideoLLMs struggled with body language interpretation and showed biases based on age and ethnicity.

Conclusion: The BQA dataset highlights the need for improved VideoLLMs to better understand nonverbal cues and reduce biases.

Abstract: A large part of human communication relies on nonverbal cues such as facial
expressions, eye contact, and body language. Unlike language or sign language,
such nonverbal communication lacks formal rules, requiring complex reasoning
based on commonsense understanding. Enabling current Video Large Language
Models (VideoLLMs) to accurately interpret body language is a crucial
challenge, as human unconscious actions can easily cause the model to
misinterpret their intent. To address this, we propose a dataset, BQA, a body
language question answering dataset, to validate whether the model can
correctly interpret emotions from short clips of body language comprising 26
emotion labels of videos of body language. We evaluated various VideoLLMs on
BQA and revealed that understanding body language is challenging, and our
analyses of the wrong answers by VideoLLMs show that certain VideoLLMs made
significantly biased answers depending on the age group and ethnicity of the
individuals in the video. The dataset is available.

</details>


### [171] [Aggregation Artifacts in Subjective Tasks Collapse Large Language Models' Posteriors](https://arxiv.org/pdf/2410.13776)
*Georgios Chochlakis, Alexandros Potamianos, Kristina Lerman, Shrikanth Narayanan*

Main category: cs.CL

TL;DR: The paper examines how In-context Learning (ICL) in LLMs relies on task priors rather than learning, especially in subjective domains like emotion and morality. It identifies dataset aggregation as a confounding factor and suggests modeling individuals instead. Minority annotators may align better with LLMs.


<details>
  <summary>Details</summary>
Motivation: To understand why ICL in LLMs depends on task priors and how dataset aggregation affects performance in subjective tasks.

Method: Analyzes dataset aggregation effects and evaluates LLM priors using quantitative measures, focusing on annotator-level labels.

Result: Aggregation confounds subjective task modeling, and minority annotators can align better with LLMs.

Conclusion: Modeling individuals is preferable for subjective tasks, but other factors beyond aggregation also impact ICL performance.

Abstract: In-context Learning (ICL) has become the primary method for performing
natural language tasks with Large Language Models (LLMs). The knowledge
acquired during pre-training is crucial for this few-shot capability, providing
the model with task priors. However, recent studies have shown that ICL
predominantly relies on retrieving task priors rather than "learning" to
perform tasks. This limitation is particularly evident in complex subjective
domains such as emotion and morality, where priors significantly influence
posterior predictions. In this work, we examine whether this is the result of
the aggregation used in corresponding datasets, where trying to combine
low-agreement, disparate annotations might lead to annotation artifacts that
create detrimental noise in the prompt. Moreover, we evaluate the posterior
bias towards certain annotators by grounding our study in appropriate,
quantitative measures of LLM priors. Our results indicate that aggregation is a
confounding factor in the modeling of subjective tasks, and advocate focusing
on modeling individuals instead. However, aggregation does not explain the
entire gap between ICL and the state of the art, meaning other factors in such
tasks also account for the observed phenomena. Finally, by rigorously studying
annotator-level labels, we find that it is possible for minority annotators to
both better align with LLMs and have their perspectives further amplified.

</details>


### [172] [Distance between Relevant Information Pieces Causes Bias in Long-Context LLMs](https://arxiv.org/pdf/2410.14641)
*Runchu Tian, Yanghao Li, Yuepeng Fu, Siyang Deng, Qinyu Luo, Cheng Qian, Shuo Wang, Xin Cong, Zhong Zhang, Yesai Wu, Yankai Lin, Huadong Wang, Xiaojiang Liu*

Main category: cs.CL

TL;DR: LongPiBench evaluates positional bias in LLMs with multiple relevant information pieces, revealing biases in spacing despite robustness against 'lost in the middle.'


<details>
  <summary>Details</summary>
Motivation: Address the gap in assessing positional bias in LLMs for multiple relevant information pieces, beyond single-piece focus.

Method: Develop LongPiBench benchmark and test five commercial and six open-source models.

Result: Most models are robust against 'lost in the middle' but show biases in spacing of relevant information.

Conclusion: Evaluating and reducing positional biases is crucial for advancing LLM capabilities.

Abstract: Positional bias in large language models (LLMs) hinders their ability to
effectively process long inputs. A prominent example is the "lost in the
middle" phenomenon, where LLMs struggle to utilize relevant information
situated in the middle of the input. While prior research primarily focuses on
single pieces of relevant information, real-world applications often involve
multiple relevant information pieces. To bridge this gap, we present
LongPiBench, a benchmark designed to assess positional bias involving multiple
pieces of relevant information. Thorough experiments are conducted with five
commercial and six open-source models. These experiments reveal that while most
current models are robust against the "lost in the middle" issue, there exist
significant biases related to the spacing of relevant information pieces. These
findings highlight the importance of evaluating and reducing positional biases
to advance LLM's capabilities.

</details>


### [173] [Unleashing LLM Reasoning Capability via Scalable Question Synthesis from Scratch](https://arxiv.org/pdf/2410.18693)
*Yuyang Ding, Xinyu Shi, Xiaobo Liang, Juntao Li, Zhaopeng Tu, Qiaoming Zhu, Min Zhang*

Main category: cs.CL

TL;DR: ScaleQuest introduces a scalable, cost-effective method to generate large-scale mathematical reasoning datasets using lightweight models, improving LLMs' reasoning capabilities.


<details>
  <summary>Details</summary>
Motivation: Addressing the challenge of limited access to diverse, high-quality reasoning datasets for the open-source community.

Method: A two-stage question-tuning process (QFT and QPO) to generate diverse questions without relying on proprietary models or seed data, producing 1M problem-solution pairs.

Result: Models trained on ScaleQuest data outperform existing open-source datasets in evaluations and show continued improvement with more data.

Conclusion: ScaleQuest offers a practical solution to enhance LLMs' mathematical reasoning, with potential for generalization to other tasks like code reasoning.

Abstract: Improving the mathematical reasoning capabilities of Large Language Models
(LLMs) is critical for advancing artificial intelligence. However, access to
extensive, diverse, and high-quality reasoning datasets remains a significant
challenge, particularly for the open-source community. In this paper, we
propose ScaleQuest, a novel, scalable, and cost-effective data synthesis method
that enables the generation of large-scale mathematical reasoning datasets
using lightweight 7B-scale models. ScaleQuest introduces a two-stage
question-tuning process comprising Question Fine-Tuning (QFT) and Question
Preference Optimization (QPO) to unlock the question generation capabilities of
problem-solving models. By generating diverse questions from scratch -- without
relying on powerful proprietary models or seed data -- we produce a dataset of
1 million problem-solution pairs. Our experiments demonstrate that models
trained on our data outperform existing open-source datasets in both in-domain
and out-of-domain evaluations. Furthermore, our approach shows continued
performance improvement as the volume of training data increases, highlighting
its potential for ongoing data scaling. The extensive improvements observed in
code reasoning tasks demonstrate the generalization capabilities of our
proposed method. Our work provides the open-source community with a practical
solution to enhance the mathematical reasoning abilities of LLMs.

</details>


### [174] [Frequency matters: Modeling irregular morphological patterns in Spanish with Transformers](https://arxiv.org/pdf/2410.21013)
*Akhilesh Kakolu Ramarao, Kevin Tang, Dinah Baer-Henney*

Main category: cs.CL

TL;DR: The paper investigates how transformer models handle irregular L-shaped verb patterns in Spanish, revealing performance biases and memorization tendencies under varying input frequencies.


<details>
  <summary>Details</summary>
Motivation: To understand how speakers solve the Paradigm Cell Filling Problem (PCFP) in complex inflectional systems, focusing on irregular L-shaped verbs in Spanish.

Method: Formulates the problem as a morphological reinflection task, manipulating input distributions to analyze model behavior.

Result: Key findings include better performance on L-shaped verbs, primacy effects, memorization trends, and regularization tendencies.

Conclusion: Input frequency significantly impacts model behavior, with memorization and regularization playing key roles in handling irregular patterns.

Abstract: Over the past decade, various studies have addressed how speakers solve the
so-called `The Paradigm Cell Filling Problem' (PCFP) \citep{ackerman2009parts}
across different languages. The PCFP addresses a fundamental question in
morphological processing: how do speakers accurately generate inflected forms
of words when presented with incomplete paradigms? This problem is particularly
salient when modeling complex inflectional systems. We focus on Spanish verbal
paradigms, where certain verbs follow an irregular L-shaped pattern, where the
first-person singular present indicative stem matches the stem used throughout
the present subjunctive mood. We formulate the problem as a morphological
reinflection task. Specifically, we investigate the role of input frequency in
the acquisition of regular versus irregular L-shaped patterns in transformer
models. By systematically manipulating the input distributions and analyzing
model behavior, we reveal four key findings: 1) Models perform better on
L-shaped verbs compared to regular verbs, especially in uneven frequency
conditions; 2) Robust primacy effects are observed, but no consistent recency
effects; 3) Memorization becomes more prominent as the proportion of L-shaped
verbs increases; 4) There is a tendency to regularize L-shaped verbs when their
consonant alternation pairs are rare or absent in the training data.

</details>


### [175] [STEM-POM: Evaluating Language Models Math-Symbol Reasoning in Document Parsing](https://arxiv.org/pdf/2411.00387)
*Jiaru Zou, Qing Wang, Pratyush Thakur, Nickvash Kani*

Main category: cs.CL

TL;DR: STEM-PoM is a benchmark dataset to evaluate LLMs' reasoning on math symbols in STEM documents, showing current models' limited accuracy (20-60%).


<details>
  <summary>Details</summary>
Motivation: LLMs struggle with understanding abstract math symbols in STEM documents, limiting their reasoning capabilities.

Method: STEM-PoM dataset includes 2K+ math symbols from ArXiv, classified by attributes (variables, constants, etc.) and sub-attributes.

Result: LLMs achieve 20-60% accuracy in-context and 50-60% with fine-tuning, revealing gaps in symbol classification.

Conclusion: STEM-PoM improves LLMs' math symbol classification, enhancing downstream reasoning. Dataset and code are publicly available.

Abstract: Advances in large language models (LLMs) have spurred research into enhancing
their reasoning capabilities, particularly in math-rich STEM (Science,
Technology, Engineering, and Mathematics) documents. While LLMs can generate
equations or solve math-related queries, their ability to fully understand and
interpret abstract mathematical symbols in long, math-rich documents remains
limited. In this paper, we introduce STEM-PoM, a comprehensive benchmark
dataset designed to evaluate LLMs' reasoning abilities on math symbols within
contextual scientific text. The dataset, sourced from real-world ArXiv
documents, contains over 2K math symbols classified as main attributes of
variables, constants, operators, and unit descriptors, with additional
sub-attributes including scalar/vector/matrix for variables and
local/global/discipline-specific labels for both constants and operators. Our
extensive experiments demonstrate that state-of-the-art LLMs achieve an average
accuracy of 20-60% under in-context learning and 50-60% with fine-tuning,
highlighting a substantial gap in their ability to classify mathematical
symbols. By improving LLMs' mathematical symbol classification, STEM-PoM
further enhances models' downstream mathematical reasoning capabilities. The
code and data are available at https://github.com/jiaruzouu/STEM-PoM.

</details>


### [176] [Efficient and Accurate Prompt Optimization: the Benefit of Memory in Exemplar-Guided Reflection](https://arxiv.org/pdf/2411.07446)
*Cilin Yan, Jingyun Wang, Lin Zhang, Ruihui Zhao, Xiaopu Wu, Kai Xiong, Qingsong Liu, Guoliang Kang, Yangyang Kang*

Main category: cs.CL

TL;DR: The paper introduces ERM, an exemplar-guided reflection with memory mechanism, to improve prompt optimization for LLMs by leveraging historical feedback and better exemplar selection.


<details>
  <summary>Details</summary>
Motivation: Current methods for prompt optimization ignore historical and unselected feedbacks and use suboptimal exemplar selection, limiting performance.

Method: ERM uses exemplar-guided reflection for feedback generation and two memory types to utilize historical feedback and improve exemplar retrieval.

Result: ERM outperforms state-of-the-art methods, achieving a 10.1 F1 score improvement on LIAR and halving optimization steps on ProTeGi.

Conclusion: ERM enhances prompt optimization efficiency and accuracy by integrating historical feedback and better exemplar guidance.

Abstract: Automatic prompt engineering aims to enhance the generation quality of large
language models (LLMs). Recent works utilize feedbacks generated from erroneous
cases to guide the prompt optimization. During inference, they may further
retrieve several semantically-related exemplars and concatenate them to the
optimized prompts to improve the performance. However, those works only utilize
the feedback at the current step, ignoring historical and unseleccted feedbacks
which are potentially beneficial. Moreover, the selection of exemplars only
considers the general semantic relationship and may not be optimal in terms of
task performance and matching with the optimized prompt. In this work, we
propose an Exemplar-Guided Reflection with Memory mechanism (ERM) to realize
more efficient and accurate prompt optimization. Specifically, we design an
exemplar-guided reflection mechanism where the feedback generation is
additionally guided by the generated exemplars. We further build two kinds of
memory to fully utilize the historical feedback information and support more
effective exemplar retrieval. Empirical evaluations show our method surpasses
previous state-of-the-arts with less optimization steps, i.e., improving F1
score by 10.1 on LIAR dataset, and reducing half of the optimization steps on
ProTeGi.

</details>


### [177] [DRPruning: Efficient Large Language Model Pruning through Distributionally Robust Optimization](https://arxiv.org/pdf/2411.14055)
*Hexuan Deng, Wenxiang Jiao, Xuebo Liu, Jing Li, Min Zhang, Zhaopeng Tu*

Main category: cs.CL

TL;DR: DRPruning dynamically adjusts data distribution during training to balance performance in pruned LLMs, outperforming similar models in pruning and pretraining tasks.


<details>
  <summary>Details</summary>
Motivation: Addressing uneven performance degradation in structured pruning of LLMs due to biased data distribution.

Method: Proposes DRPruning, which dynamically adjusts data distribution during training to restore balanced performance.

Result: Outperforms similarly sized models in perplexity, downstream tasks, and instruction tuning; robust across domains and distribution shifts.

Conclusion: DRPruning effectively balances performance in pruned models and has potential for broader applications.

Abstract: Large language models (LLMs) deliver impressive results but face challenges
from increasing model sizes and computational costs. Structured pruning reduces
model size and speeds up inference but often causes uneven degradation across
domains, leading to biased performance. To address this, we propose DRPruning,
a method that dynamically adjusts the data distribution during training to
restore balanced performance across heterogeneous and multi-tasking data.
Experiments in monolingual and multilingual settings show that DRPruning
surpasses similarly sized models in both pruning and continued pretraining over
perplexity, downstream tasks, and instruction tuning. Further analysis
demonstrates the robustness of DRPruning towards various domains and
distribution shifts. Furthermore, DRPruning can determine optimal reference
losses and data ratios automatically, suggesting potential for broader
applications. Code and scripts are available at
https://github.com/hexuandeng/DRPruning.

</details>


### [178] [How Private are Language Models in Abstractive Summarization?](https://arxiv.org/pdf/2412.12040)
*Anthony Hughes, Ning Ma, Nikolaos Aletras*

Main category: cs.CL

TL;DR: The paper investigates privacy risks in LM-based summarization, revealing frequent leaks of sensitive information, unlike human-generated summaries which offer better protection.


<details>
  <summary>Details</summary>
Motivation: Protecting sensitive information in domains like medical and legal is critical, but sharing data is challenging. LMs' ability to provide privacy-preserving summaries is unclear.

Method: The study evaluates privacy risks in LM-based summarization using two closed- and four open-weight models, testing prompting and fine-tuning strategies on medical and legal datasets.

Result: LMs often leak personally identifiable information in summaries, while human-generated summaries show significantly higher privacy protection.

Conclusion: There's a notable gap between LM capabilities and human expertise in privacy-sensitive summarization, highlighting the need for improved LM privacy measures.

Abstract: In sensitive domains such as medical and legal, protecting sensitive
information is critical, with protective laws strictly prohibiting the
disclosure of personal data. This poses challenges for sharing valuable data
such as medical reports and legal cases summaries. While language models (LMs)
have shown strong performance in text summarization, it is still an open
question to what extent they can provide privacy-preserving summaries from
non-private source documents. In this paper, we perform a comprehensive study
of privacy risks in LM-based summarization across two closed- and four
open-weight models of different sizes and families. We experiment with both
prompting and fine-tuning strategies for privacy-preservation across a range of
summarization datasets including medical and legal domains. Our quantitative
and qualitative analysis, including human evaluation, shows that LMs frequently
leak personally identifiable information in their summaries, in contrast to
human-generated privacy-preserving summaries, which demonstrate significantly
higher privacy protection levels. These findings highlight a substantial gap
between current LM capabilities and expert human expert performance in
privacy-sensitive summarization tasks.

</details>


### [179] [Knowledge Boundary of Large Language Models: A Survey](https://arxiv.org/pdf/2412.12472)
*Moxin Li, Yong Zhao, Wenxuan Zhang, Shuaiyi Li, Wenya Xie, See-Kiong Ng, Tat-Seng Chua, Yang Deng*

Main category: cs.CL

TL;DR: The paper defines the knowledge boundary of LLMs, categorizes knowledge into four types, and reviews motivations, methods, and mitigation strategies for studying these boundaries.


<details>
  <summary>Details</summary>
Motivation: To address LLMs' limitations in memorizing and utilizing knowledge, leading to untruthful responses, by defining and understanding their knowledge boundaries.

Method: Proposes a taxonomy of knowledge types and systematically reviews the field through three lenses: motivation, boundary identification methods, and mitigation strategies.

Result: A comprehensive definition of LLM knowledge boundaries and a structured review of the field.

Conclusion: The survey aims to provide an overview, highlight key issues, and inspire future research in LLM knowledge boundaries.

Abstract: Although large language models (LLMs) store vast amount of knowledge in their
parameters, they still have limitations in the memorization and utilization of
certain knowledge, leading to undesired behaviors such as generating untruthful
and inaccurate responses. This highlights the critical need to understand the
knowledge boundary of LLMs, a concept that remains inadequately defined in
existing research. In this survey, we propose a comprehensive definition of the
LLM knowledge boundary and introduce a formalized taxonomy categorizing
knowledge into four distinct types. Using this foundation, we systematically
review the field through three key lenses: the motivation for studying LLM
knowledge boundaries, methods for identifying these boundaries, and strategies
for mitigating the challenges they present. Finally, we discuss open challenges
and potential research directions in this area. We aim for this survey to offer
the community a comprehensive overview, facilitate access to key issues, and
inspire further advancements in LLM knowledge research.

</details>


### [180] [DynamicKV: Task-Aware Adaptive KV Cache Compression for Long Context LLMs](https://arxiv.org/pdf/2412.14838)
*Xiabin Zhou, Wenbin Wang, Minyan Zeng, Jiaxian Guo, Xuebo Liu, Li Shen, Min Zhang, Liang Ding*

Main category: cs.CL

TL;DR: DynamicKV dynamically optimizes KV cache retention in LLMs for long-context tasks, achieving high performance with minimal cache size.


<details>
  <summary>Details</summary>
Motivation: Existing KV cache compression methods use fixed patterns, ignoring task-specific needs and losing essential information. Distinct activation patterns across tasks suggest adaptive strategies are needed.

Method: DynamicKV adjusts token retention per layer, using global and per-layer budgets, and updates cache sizes during inference.

Result: Retains only 1.7% of KV cache size while achieving ~85% of Full KV performance. Outperforms SOTA by 11% at extreme compression (0.9%).

Conclusion: DynamicKV offers a task-adaptive solution for efficient KV cache management, significantly improving performance with minimal resource use.

Abstract: Efficient KV cache management in LLMs is crucial for long-context tasks like
RAG and summarization. Existing KV cache compression methods enforce a fixed
pattern, neglecting task-specific characteristics and reducing the retention of
essential information. However, we observe distinct activation patterns across
layers in various tasks, highlighting the need for adaptive strategies tailored
to each task's unique demands. Based on this insight, we propose DynamicKV, a
method that dynamically optimizes token retention by adjusting the number of
tokens retained at each layer to adapt to the specific task. DynamicKV
establishes global and per-layer maximum KV cache budgets, temporarily
retaining the maximum budget for the current layer, and periodically updating
the KV cache sizes of all preceding layers during inference. Our method retains
only 1.7% of the KV cache size while achieving ~85% of the Full KV cache
performance on LongBench. Notably, even under extreme compression (0.9%),
DynamicKV surpasses state-of-the-art (SOTA) methods by 11% in the
Needle-in-a-Haystack test using Mistral-7B-Instruct-v0.2. The code will be
released.

</details>


### [181] [ProgCo: Program Helps Self-Correction of Large Language Models](https://arxiv.org/pdf/2501.01264)
*Xiaoshuai Song, Yanan Wu, Weixun Wang, Jiaheng Liu, Wenbo Su, Bo Zheng*

Main category: cs.CL

TL;DR: ProgCo introduces program-driven self-correction (ProgCo) for LLMs, using pseudo-programs for verification (ProgVe) and refinement (ProgRe) to improve self-correction in complex reasoning tasks.


<details>
  <summary>Details</summary>
Motivation: LLMs struggle with self-verification and refinement, especially in complex tasks, leading to ineffective self-correction.

Method: ProgCo uses program-driven verification (ProgVe) and refinement (ProgRe) to enhance self-correction through pseudo-programs and dual reflection.

Result: Experiments show ProgCo improves self-correction in instruction-following and mathematical tasks, with further gains when combined with real program tools.

Conclusion: ProgCo effectively addresses LLM self-correction challenges in complex reasoning tasks, with potential for broader application.

Abstract: Self-Correction aims to enable large language models (LLMs) to self-verify
and self-refine their initial responses without external feedback. However,
LLMs often fail to effectively self-verify and generate correct feedback,
further misleading refinement and leading to the failure of self-correction,
especially in complex reasoning tasks. In this paper, we propose Program-driven
Self-Correction (ProgCo). First, program-driven verification (ProgVe) achieves
complex verification logic and extensive validation through self-generated,
self-executing verification pseudo-programs. Then, program-driven refinement
(ProgRe) receives feedback from ProgVe, conducts dual reflection and refinement
on both responses and verification programs to mitigate misleading of incorrect
feedback in complex reasoning tasks. Experiments on three instruction-following
and mathematical benchmarks indicate that ProgCo achieves effective
self-correction, and can be further enhance performance when combined with real
program tools. We release our code at https://github.com/songxiaoshuai/progco.

</details>


### [182] [Mitigating Forgetting in LLM Fine-Tuning via Low-Perplexity Token Learning](https://arxiv.org/pdf/2501.14315)
*Chao-Chung Wu, Zhi Rui Tam, Chieh-Yen Lin, Yun-Nung Chen, Shao-Hua Sun, Hung-yi Lee*

Main category: cs.CL

TL;DR: Fine-tuning with LLM-generated data improves target task performance and reduces non-target task degradation by reducing high perplexity tokens, offering insights for robust fine-tuning strategies.


<details>
  <summary>Details</summary>
Motivation: To understand the impact of LLM-generated data on cross-domain generalization and mitigate catastrophic forgetting in LLMs after fine-tuning.

Method: Systematic analysis of fine-tuning with LLM-generated data, comparing it to ground truth data, and masking high perplexity tokens in ground truth data.

Result: LLM-generated data reduces non-target task degradation by lowering high perplexity tokens, and masking such tokens in ground truth data achieves similar results.

Conclusion: Token perplexity reduction is key to mitigating catastrophic forgetting, providing a foundation for more robust fine-tuning strategies.

Abstract: Maintaining consistent model performance across domains is a fundamental
challenge in machine learning. While recent work has explored using
LLM-generated data for fine-tuning, its impact on cross-domain generalization
remains poorly understood. This paper presents a systematic analysis revealing
that fine-tuning with LLM-generated data not only improves target task
performance but also reduces non-target task degradation compared to
fine-tuning with ground truth data. Through analyzing the data sequence in
tasks of various domains, we demonstrate that this enhancement of non-target
task robustness stems from the reduction of high perplexity tokens found in
LLM-generated sequences. Following our findings, we showed that masking high
perplexity tokens in ground truth training data achieves similar non-target
task performance preservation, comparable to using LLM-generated data.
Extensive experiments across different model families and scales, including
Gemma 2 IT 2B, Llama 3 8B Instruct, and 3 additional models, agree with our
findings. To the best of our knowledge, this is the first work to provide an
empirical explanation based on token perplexity reduction to mitigate
catastrophic forgetting in LLMs after fine-tuning, offering valuable insights
for developing more robust fine-tuning strategies.

</details>


### [183] [Tuning LLM Judge Design Decisions for 1/1000 of the Cost](https://arxiv.org/pdf/2501.17178)
*David Salinas, Omar Swelam, Frank Hutter*

Main category: cs.CL

TL;DR: The paper proposes a systematic method to analyze and tune hyperparameters of LLM-based judges for evaluating LLMs, using multi-objective multi-fidelity optimization to balance accuracy and cost.


<details>
  <summary>Details</summary>
Motivation: Human annotations for evaluating LLMs are costly, and existing LLM-based judges lack systematic hyperparameter tuning, leading to inconsistent comparisons.

Method: The approach leverages multi-objective multi-fidelity optimization to efficiently search for optimal judges, balancing accuracy and cost, and uses open-weight models for accessibility.

Result: The method identifies judges that outperform benchmarks in accuracy and cost-efficiency while ensuring reproducibility.

Conclusion: The proposed approach provides a cost-effective and reproducible solution for evaluating LLMs using optimized LLM-based judges.

Abstract: Evaluating Large Language Models (LLMs) often requires costly human
annotations. To address this, LLM-based judges have been proposed, which
compare the outputs of two LLMs enabling the ranking of models without human
intervention. While several approaches have been proposed, many confounding
factors are present between different papers. For instance the model, the
prompt and other hyperparameters are typically changed at the same time making
apple-to-apple comparisons challenging. In this paper, we propose to
systematically analyze and tune the hyperparameters of LLM judges. To alleviate
the high cost of evaluating a judge, we propose to leverage multi-objective
multi-fidelity which allows to find judges that trade accuracy for cost and
also significantly reduce the cost of the search. Our method identifies judges
that not only outperform existing benchmarks in accuracy and cost-efficiency
but also utilize open-weight models, ensuring greater accessibility and
reproducibility. The code to reproduce our experiments is available at this
repository https://github.com/geoalgo/judgetuning .

</details>


### [184] [KBQA-o1: Agentic Knowledge Base Question Answering with Monte Carlo Tree Search](https://arxiv.org/pdf/2501.18922)
*Haoran Luo, Haihong E, Yikai Guo, Qika Lin, Xiaobao Wu, Xinyu Mu, Wenhao Liu, Meina Song, Yifan Zhu, Luu Anh Tuan*

Main category: cs.CL

TL;DR: KBQA-o1 is a novel KBQA method using MCTS for stepwise logical form generation, improving performance with limited annotated data.


<details>
  <summary>Details</summary>
Motivation: Address challenges in KBQA like weak KB awareness, efficiency-effectiveness imbalance, and high reliance on annotated data.

Method: Uses ReAct-based agent process and MCTS for heuristic search, balancing exploration and performance.

Result: Outperforms previous methods, achieving 78.5% F1 on GrailQA with limited data.

Conclusion: KBQA-o1 is effective for low-resource KBQA, with potential for further improvement via fine-tuning.

Abstract: Knowledge Base Question Answering (KBQA) aims to answer natural language
questions with a large-scale structured knowledge base (KB). Despite
advancements with large language models (LLMs), KBQA still faces challenges in
weak KB awareness, imbalance between effectiveness and efficiency, and high
reliance on annotated data. To address these challenges, we propose KBQA-o1, a
novel agentic KBQA method with Monte Carlo Tree Search (MCTS). It introduces a
ReAct-based agent process for stepwise logical form generation with KB
environment exploration. Moreover, it employs MCTS, a heuristic search method
driven by policy and reward models, to balance agentic exploration's
performance and search space. With heuristic exploration, KBQA-o1 generates
high-quality annotations for further improvement by incremental fine-tuning.
Experimental results show that KBQA-o1 outperforms previous low-resource KBQA
methods with limited annotated data, boosting Llama-3.1-8B model's GrailQA F1
performance to 78.5% compared to 48.5% of the previous sota method with
GPT-3.5-turbo. Our code is publicly available.

</details>


### [185] [Thinking beyond the anthropomorphic paradigm benefits LLM research](https://arxiv.org/pdf/2502.09192)
*Lujain Ibrahim, Myra Cheng*

Main category: cs.CL

TL;DR: The paper examines the prevalence of anthropomorphic terminology in LLM research, identifies five limiting assumptions, and suggests non-anthropomorphic alternatives for advancing LLM development.


<details>
  <summary>Details</summary>
Motivation: To highlight how anthropomorphism in LLM research may unintentionally restrict innovation and propose broader, non-human-centric approaches.

Method: Analyzed hundreds of thousands of research articles to track anthropomorphic terminology and identified five key assumptions in LLM research.

Result: Found empirical evidence of widespread anthropomorphism and demonstrated viable non-anthropomorphic alternatives for each assumption.

Conclusion: Encourages challenging anthropomorphic assumptions to unlock new pathways for LLM understanding and improvement.

Abstract: Anthropomorphism, or the attribution of human traits to technology, is an
automatic and unconscious response that occurs even in those with advanced
technical expertise. In this position paper, we analyze hundreds of thousands
of research articles to present empirical evidence of the prevalence and growth
of anthropomorphic terminology in research on large language models (LLMs). We
argue for challenging the deeper assumptions reflected in this terminology --
which, though often useful, may inadvertently constrain LLM development -- and
broadening beyond them to open new pathways for understanding and improving
LLMs. Specifically, we identify and examine five anthropomorphic assumptions
that shape research across the LLM development lifecycle. For each assumption
(e.g., that LLMs must use natural language for reasoning, or that they should
be evaluated on benchmarks originally meant for humans), we demonstrate
empirical, non-anthropomorphic alternatives that remain under-explored yet
offer promising directions for LLM research and development.

</details>


### [186] [The Hidden Dimensions of LLM Alignment: A Multi-Dimensional Analysis of Orthogonal Safety Directions](https://arxiv.org/pdf/2502.09674)
*Wenbo Pan, Zhichao Liu, Qiguang Chen, Xiangyang Zhou, Haining Yu, Xiaohua Jia*

Main category: cs.CL

TL;DR: Safety-aligned behaviors in LLMs are controlled by multi-dimensional directions, not just one, revealing deeper mechanistic insights and vulnerabilities.


<details>
  <summary>Details</summary>
Motivation: To understand the multi-dimensional nature of safety-aligned behaviors in LLMs, moving beyond the single-direction model for better mechanistic insights.

Method: Studied orthogonal directions in activation space during safety fine-tuning on Llama 3 8B, analyzing refusal behavior and secondary features.

Result: Found a dominant refusal direction and interpretable secondary directions; showed how secondary directions shape refusal behavior and identified vulnerabilities.

Conclusion: Safety alignment is multi-dimensional, and understanding these directions provides insights into vulnerabilities and potential bypass methods.

Abstract: Large Language Models' safety-aligned behaviors, such as refusing harmful
queries, can be represented by linear directions in activation space. Previous
research modeled safety behavior with a single direction, limiting mechanistic
understanding to an isolated safety feature. In this work, we discover that
safety-aligned behavior is jointly controlled by multi-dimensional directions.
Namely, we study the vector space of representation shifts during safety
fine-tuning on Llama 3 8B for refusing jailbreaks. By studying orthogonal
directions in the space, we first find that a dominant direction governs the
model's refusal behavior, while multiple smaller directions represent distinct
and interpretable features like hypothetical narrative and role-playing. We
then measure how different directions promote or suppress the dominant
direction, showing the important role of secondary directions in shaping the
model's refusal representation. Finally, we demonstrate that removing certain
trigger tokens in harmful queries can mitigate these directions to bypass the
learned safety capability, providing new insights on understanding safety
alignment vulnerability from a multi-dimensional perspective. Code and
artifacts are available at https://github.com/BMPixel/safety-residual-space.

</details>


### [187] [MMUnlearner: Reformulating Multimodal Machine Unlearning in the Era of Multimodal Large Language Models](https://arxiv.org/pdf/2502.11051)
*Jiahao Huo, Yibo Yan, Xu Zheng, Yuanhuiyi Lyu, Xin Zou, Zhihua Wei, Xuming Hu*

Main category: cs.CL

TL;DR: The paper introduces MMUnlearner, a method for multimodal machine unlearning in MLLMs, focusing on erasing visual patterns while preserving textual knowledge.


<details>
  <summary>Details</summary>
Motivation: Addressing the lack of solutions for multimodal machine unlearning in MLLMs, particularly for selectively removing visual patterns without affecting textual knowledge.

Method: Proposes a geometry-constrained gradient ascent method (MMUnlearner) that updates MLLM weights using a saliency map, preserving non-target knowledge.

Result: MMUnlearner outperforms baselines like GA and NPO in all evaluation dimensions.

Conclusion: The method effectively achieves selective unlearning in MLLMs, balancing visual erasure with textual preservation.

Abstract: Recent progress in Machine Unlearning (MU) has introduced solutions for the
selective removal of private or sensitive information encoded within deep
neural networks. Nonetheless, MU for Multimodal Large Language Models (MLLMs)
remains in its nascent phase. Therefore, we propose to reformulate the task of
multimodal MU in the era of MLLMs, which aims to erase only the visual patterns
associated with a given entity while preserving the corresponding textual
knowledge encoded within the original parameters of the language model
backbone. Furthermore, we develop a novel geometry-constrained gradient ascent
method MMUnlearner. It updates the weights of MLLMs with a weight saliency map
jointly restricted by the remaining concepts and textual knowledge during
unlearning, thereby preserving parameters essential for non-target knowledge.
Extensive experiments demonstrate that MMUnlearner surpasses baselines that
finetuning MLLMs with VQA data directly through Gradient Ascent (GA) or
Negative Preference Optimization (NPO), across all evaluation dimensions. Our
code can be found in [this URL](https://github.com/Z1zs/MMUnlearner).

</details>


### [188] [ANCHOLIK-NER: A Benchmark Dataset for Bangla Regional Named Entity Recognition](https://arxiv.org/pdf/2502.11198)
*Bidyarthi Paul, Faika Fairuj Preotee, Shuvashis Sarker, Shamim Rahim Refat, Shifat Islam, Tashreef Muhammad, Mohammad Ashraful Hoque, Shahriar Manzoor*

Main category: cs.CL

TL;DR: The paper introduces ANCHOLIK-NER, the first benchmark dataset for NER in Bangla regional dialects, and evaluates transformer-based models, finding BERT Base Multilingual Cased performs best.


<details>
  <summary>Details</summary>
Motivation: NER in Bangla regional dialects is underexplored, with no existing resources or models addressing unique linguistic features of dialects like Barishal, Chittagong, etc.

Method: The dataset (17,405 sentences across five regions) was created using publicly available resources and manual translations. Three transformer models (Bangla BERT, Bangla BERT Base, BERT Base Multilingual Cased) were evaluated.

Result: BERT Base Multilingual Cased performed best (e.g., 82.611% F1-score in Mymensingh), though challenges remain in regions like Chittagong.

Conclusion: This work fills a gap in NER for Bangla dialects and lays a foundation for future improvements in model performance and dataset expansion.

Abstract: Named Entity Recognition (NER) in regional dialects is a critical yet
underexplored area in Natural Language Processing (NLP), especially for
low-resource languages like Bangla. While NER systems for Standard Bangla have
made progress, no existing resources or models specifically address the
challenge of regional dialects such as Barishal, Chittagong, Mymensingh,
Noakhali, and Sylhet, which exhibit unique linguistic features that existing
models fail to handle effectively. To fill this gap, we introduce ANCHOLIK-NER,
the first benchmark dataset for NER in Bangla regional dialects, comprising
17,405 sentences distributed across five regions. The dataset was sourced from
publicly available resources and supplemented with manual translations,
ensuring alignment of named entities across dialects. We evaluate three
transformer-based models - Bangla BERT, Bangla BERT Base, and BERT Base
Multilingual Cased - on this dataset. Our findings demonstrate that BERT Base
Multilingual Cased performs best in recognizing named entities across regions,
with significant performance observed in Mymensingh with an F1-score of
82.611%. Despite strong overall performance, challenges remain in region like
Chittagong, where the models show lower precision and recall. Since no previous
NER systems for Bangla regional dialects exist, our work represents a
foundational step in addressing this gap. Future work will focus on improving
model performance in underperforming regions and expanding the dataset to
include more dialects, enhancing the development of dialect-aware NER systems.

</details>


### [189] [How to Upscale Neural Networks with Scaling Law? A Survey and Practical Guidelines](https://arxiv.org/pdf/2502.12051)
*Ayan Sengupta, Yash Goel, Tanmoy Chakraborty*

Main category: cs.CL

TL;DR: Neural scaling laws guide AI model optimization but face limitations across architectures, modalities, and domains, requiring adaptive strategies.


<details>
  <summary>Details</summary>
Motivation: To synthesize insights from over 50 studies on neural scaling laws, addressing their limitations and advocating for nuanced approaches.

Method: Survey of theoretical foundations, empirical findings, and practical implications of scaling laws across diverse AI models and domains.

Result: Scaling laws are not universally applicable; deviations occur in sparse models, multimodal systems, and domain-specific contexts.

Conclusion: Adaptive scaling strategies are needed, as traditional scaling laws do not generalize across all architectures and training scenarios.

Abstract: Neural scaling laws have revolutionized the design and optimization of
large-scale AI models by revealing predictable relationships between model
size, dataset volume, and computational resources. Early research established
power-law relationships in model performance, leading to compute-optimal
scaling strategies. However, recent studies highlighted their limitations
across architectures, modalities, and deployment contexts. Sparse models,
mixture-of-experts, retrieval-augmented learning, and multimodal models often
deviate from traditional scaling patterns. Moreover, scaling behaviors vary
across domains such as vision, reinforcement learning, and fine-tuning,
underscoring the need for more nuanced approaches. In this survey, we
synthesize insights from over 50 studies, examining the theoretical
foundations, empirical findings, and practical implications of scaling laws. We
also explore key challenges, including data efficiency, inference scaling, and
architecture-specific constraints, advocating for adaptive scaling strategies
tailored to real-world applications. We suggest that while scaling laws provide
a useful guide, they do not always generalize across all architectures and
training strategies.

</details>


### [190] [SoftCoT: Soft Chain-of-Thought for Efficient Reasoning with LLMs](https://arxiv.org/pdf/2502.12134)
*Yige Xu, Xu Guo, Zhiwei Zeng, Chunyan Miao*

Main category: cs.CL

TL;DR: The paper introduces SoftCoT, a method for continuous-space reasoning in LLMs without modifying the model, using a lightweight assistant and trainable projection.


<details>
  <summary>Details</summary>
Motivation: Existing CoT approaches limit reasoning to discrete vocabulary, while continuous-space methods require full fine-tuning and suffer from forgetting. SoftCoT aims to improve reasoning without altering LLMs.

Method: Uses a fixed assistant model to generate soft thought tokens, mapped into the LLM's space via a trainable projection, enabling parameter-efficient fine-tuning.

Result: Improves reasoning performance on five benchmarks through supervised fine-tuning.

Conclusion: SoftCoT offers a practical, efficient solution for enhancing LLM reasoning without model modification.

Abstract: Chain-of-Thought (CoT) reasoning enables Large Language Models (LLMs) to
solve complex reasoning tasks by generating intermediate reasoning steps.
However, most existing approaches focus on hard token decoding, which
constrains reasoning within the discrete vocabulary space and may not always be
optimal. While recent efforts explore continuous-space reasoning, they often
require full-model fine-tuning and suffer from catastrophic forgetting,
limiting their applicability to state-of-the-art LLMs that already perform well
in zero-shot settings with a proper instruction. To address this challenge, we
propose a novel approach for continuous-space reasoning that does not require
modifying the LLM. Specifically, we employ a lightweight fixed assistant model
to speculatively generate instance-specific soft thought tokens as the initial
chain of thoughts, which are then mapped into the LLM's representation space
via a trainable projection module. Experimental results on five reasoning
benchmarks demonstrate that our method enhances LLM reasoning performance
through supervised, parameter-efficient fine-tuning. Source code is available
at https://github.com/xuyige/SoftCoT.

</details>


### [191] [Hallucinations are inevitable but can be made statistically negligible. The "innate" inevitability of hallucinations cannot explain practical LLM issues](https://arxiv.org/pdf/2502.12187)
*Atsushi Suzuki, Yulan He, Feng Tian, Zhongyuan Wang*

Main category: cs.CL

TL;DR: The paper argues that while computability theory shows hallucinations in LMs are inevitable, practical improvements can make them statistically negligible.


<details>
  <summary>Details</summary>
Motivation: To reconcile the theoretical inevitability of LM hallucinations with practical mitigation strategies.

Method: Uses probabilistic theory to prove hallucinations can be reduced with better data and algorithms.

Result: Shows hallucinations can be made statistically negligible despite their theoretical inevitability.

Conclusion: Probabilistic results better reflect practical LM deployment than computability-theoretic limitations.

Abstract: Hallucinations, a phenomenon where a language model (LM) generates nonfactual
content, pose a significant challenge to the practical deployment of LMs. While
many empirical methods have been proposed to mitigate hallucinations, recent
studies established a computability-theoretic result showing that any LM will
inevitably generate hallucinations on an infinite set of inputs, regardless of
the quality and quantity of training datasets and the choice of the language
model architecture and training and inference algorithms. Although the
computability-theoretic result may seem pessimistic, its significance in
practical viewpoints has remained unclear. This paper claims that those
"innate" inevitability results from computability theory and diagonal argument,
in principle, cannot explain practical issues of LLMs. We demonstrate this
claim by presenting a positive theoretical result from a probabilistic
perspective. Specifically, we prove that hallucinations can be made
statistically negligible, provided that the quality and quantity of the
training data are sufficient. Interestingly, our positive result coexists with
the computability-theoretic result, implying that while hallucinations on an
infinite set of inputs cannot be entirely eliminated, their probability can
always be reduced by improving algorithms and training data. By evaluating the
two seemingly contradictory results through the lens of information theory, we
argue that our probability-theoretic positive result better reflects practical
considerations than the computability-theoretic negative result.

</details>


### [192] [Task-Informed Anti-Curriculum by Masking Improves Downstream Performance on Text](https://arxiv.org/pdf/2502.12953)
*Andrei Jarca, Florinel Alin Croitoru, Radu Tudor Ionescu*

Main category: cs.CL

TL;DR: The paper introduces a task-informed anti-curriculum learning scheme (TIACBM) to improve masked language modeling by dynamically adjusting masking ratios and token selection based on task-specific knowledge.


<details>
  <summary>Details</summary>
Motivation: Random token masking in pre-training LLMs is suboptimal. The paper aims to enhance model focus on task-relevant features by leveraging task-specific knowledge and anti-curriculum learning.

Method: Proposes TIACBM: (1) task-specific token masking, (2) cyclic decaying masking ratio (hard to easy). Evaluated on sentiment analysis, text classification, and authorship attribution.

Result: TIACBM improves model focus on key features, yielding statistically significant performance gains across tasks.

Conclusion: Dynamic masking strategies like TIACBM enhance LLM pre-training by aligning masking with task relevance, improving downstream performance.

Abstract: Masked language modeling has become a widely adopted unsupervised technique
to pre-train large language models (LLMs). However, the process of selecting
tokens for masking is random, and the percentage of masked tokens is typically
fixed for the entire training process. In this paper, we propose to adjust the
masking ratio and to decide which tokens to mask based on a novel task-informed
anti-curriculum learning scheme. First, we harness task-specific knowledge
about useful and harmful tokens in order to determine which tokens to mask.
Second, we propose a cyclic decaying masking ratio, which corresponds to an
anti-curriculum schedule (from hard to easy). We exemplify our novel
task-informed anti-curriculum by masking (TIACBM) approach across three diverse
downstream tasks: sentiment analysis, text classification by topic, and
authorship attribution. Our findings suggest that TIACBM enhances the ability
of the model to focus on key task-relevant features, contributing to
statistically significant performance gains across tasks. We release our code
at https://github.com/JarcaAndrei/TIACBM.

</details>


### [193] [Can Community Notes Replace Professional Fact-Checkers?](https://arxiv.org/pdf/2502.14132)
*Nadav Borenstein, Greta Warren, Desmond Elliott, Isabelle Augenstein*

Main category: cs.CL

TL;DR: Community notes on Twitter/X heavily rely on professional fact-checking, especially for posts tied to misinformation narratives, revealing a deeper interdependence between crowdsourced moderation and professional fact-checking.


<details>
  <summary>Details</summary>
Motivation: To understand the dependencies between professional fact-checking and community notes in combating misinformation on social media.

Method: Used language models to analyze a large corpus of Twitter/X community notes, annotating attributes like topic, cited sources, and refutation of misinformation.

Result: Community notes cite fact-checking sources up to five times more than previously thought, with posts linked to broader narratives twice as likely to reference them.

Conclusion: Successful community moderation depends on professional fact-checking, showing a strong interconnection between citizen and professional efforts.

Abstract: Two commonly employed strategies to combat the rise of misinformation on
social media are (i) fact-checking by professional organisations and (ii)
community moderation by platform users. Policy changes by Twitter/X and, more
recently, Meta, signal a shift away from partnerships with fact-checking
organisations and towards an increased reliance on crowdsourced community
notes. However, the extent and nature of dependencies between fact-checking and
helpful community notes remain unclear. To address these questions, we use
language models to annotate a large corpus of Twitter/X community notes with
attributes such as topic, cited sources, and whether they refute claims tied to
broader misinformation narratives. Our analysis reveals that community notes
cite fact-checking sources up to five times more than previously reported.
Fact-checking is especially crucial for notes on posts linked to broader
narratives, which are twice as likely to reference fact-checking sources
compared to other sources. Our results show that successful community
moderation relies on professional fact-checking and highlight how citizen and
professional fact-checking are deeply intertwined.

</details>


### [194] [Behavioral Analysis of Information Salience in Large Language Models](https://arxiv.org/pdf/2502.14613)
*Jan Trienes, Jörg Schlötterer, Junyi Jessy Li, Christin Seifert*

Main category: cs.CL

TL;DR: The paper introduces an explainable framework to study how LLMs prioritize information in summarization, revealing nuanced, hierarchical salience patterns that differ from human perceptions.


<details>
  <summary>Details</summary>
Motivation: To understand the unclear notion of salience that LLMs internalize during text summarization.

Method: Uses length-controlled summarization and traces answerability of Questions Under Discussion to derive salience proxies. Tests 13 models across four datasets.

Result: LLMs exhibit consistent, hierarchical salience patterns, but these differ from human perceptions and cannot be accessed through introspection.

Conclusion: LLMs have a distinct, nuanced notion of salience, highlighting a gap between model behavior and human expectations.

Abstract: Large Language Models (LLMs) excel at text summarization, a task that
requires models to select content based on its importance. However, the exact
notion of salience that LLMs have internalized remains unclear. To bridge this
gap, we introduce an explainable framework to systematically derive and
investigate information salience in LLMs through their summarization behavior.
Using length-controlled summarization as a behavioral probe into the content
selection process, and tracing the answerability of Questions Under Discussion
throughout, we derive a proxy for how models prioritize information. Our
experiments on 13 models across four datasets reveal that LLMs have a nuanced,
hierarchical notion of salience, generally consistent across model families and
sizes. While models show highly consistent behavior and hence salience
patterns, this notion of salience cannot be accessed through introspection, and
only weakly correlates with human perceptions of information salience.

</details>


### [195] [GraphCheck: Breaking Long-Term Text Barriers with Extracted Knowledge Graph-Powered Fact-Checking](https://arxiv.org/pdf/2502.16514)
*Yingjian Chen, Haoran Liu, Yinhong Liu, Jinxiang Xie, Rui Yang, Han Yuan, Yanran Fu, Peng Yuan Zhou, Qingyu Chen, James Caverlee, Irene Li*

Main category: cs.CL

TL;DR: GraphCheck is a fact-checking framework using knowledge graphs and Graph Neural Networks to improve LLMs' accuracy in detecting subtle factual errors, especially in specialized domains like medicine.


<details>
  <summary>Details</summary>
Motivation: Existing fact-checking methods struggle with complex multihop relations and high computational costs, making them inefficient for specialized domains.

Method: GraphCheck extracts knowledge graphs, processes them with Graph Neural Networks, and integrates them as soft prompts into LLMs for enhanced reasoning.

Result: GraphCheck achieves up to 7.1% improvement over baselines, outperforming specialized fact-checkers and matching state-of-the-art LLMs with fewer parameters.

Conclusion: GraphCheck offers a precise and efficient solution for fact-checking in specialized domains by leveraging structured knowledge and multihop reasoning.

Abstract: Large language models (LLMs) are widely used, but they often generate subtle
factual errors, especially in long-form text. These errors are fatal in some
specialized domains such as medicine. Existing fact-checking with grounding
documents methods face two main challenges: (1) they struggle to understand
complex multihop relations in long documents, often overlooking subtle factual
errors; (2) most specialized methods rely on pairwise comparisons, requiring
multiple model calls, leading to high resource and computational costs. To
address these challenges, we propose GraphCheck, a fact-checking framework that
uses extracted knowledge graphs to enhance text representation. Graph Neural
Networks further process these graphs as a soft prompt, enabling LLMs to
incorporate structured knowledge more effectively. Enhanced with graph-based
reasoning, GraphCheck captures multihop reasoning chains that are often
overlooked by existing methods, enabling precise and efficient fact-checking in
a single inference call. Experimental results on seven benchmarks spanning both
general and medical domains demonstrate up to a 7.1% overall improvement over
baseline models. Notably, GraphCheck outperforms existing specialized
fact-checkers and achieves comparable performance with state-of-the-art LLMs,
such as DeepSeek-V3 and OpenAI-o1, with significantly fewer parameters.

</details>


### [196] [Predicting Through Generation: Why Generation Is Better for Prediction](https://arxiv.org/pdf/2502.17817)
*Md Kowsher, Nusrat Jahan Prottasha, Prakash Bhat, Chun-Nam Yu, Mojtaba Soltanalian, Ivan Garibay, Ozlem Garibay, Chen Chen, Niloofar Yousefi*

Main category: cs.CL

TL;DR: Token-level generation outperforms pooled representations for prediction tasks due to better mutual information retention. PredGen addresses challenges like exposure bias and format mismatch, improving structured prediction performance.


<details>
  <summary>Details</summary>
Motivation: Token-level generation aligns with LLMs' training and retains more mutual information, but autoregressive models face exposure bias and format mismatch.

Method: Introduces PredGen with scheduled sampling for exposure bias and a task adapter for structured outputs, plus WDAL for consistency.

Result: PredGen outperforms baselines in classification and regression benchmarks.

Conclusion: PredGen effectively addresses challenges in autoregressive models, enhancing structured prediction tasks.

Abstract: This paper argues that generating output tokens is more effective than using
pooled representations for prediction tasks because token-level generation
retains more mutual information. Since LLMs are trained on massive text corpora
using next-token prediction, generation aligns naturally with their learned
behavior. Using the Data Processing Inequality (DPI), we provide both
theoretical and empirical evidence supporting this claim. However,
autoregressive models face two key challenges when used for prediction: (1)
exposure bias, where the model sees ground truth tokens during training but
relies on its own predictions during inference, leading to errors, and (2)
format mismatch, where discrete tokens do not always align with the tasks
required output structure. To address these challenges, we introduce
PredGen(Predicting Through Generating), an end to end framework that (i) uses
scheduled sampling to reduce exposure bias, and (ii) introduces a task adapter
to convert the generated tokens into structured outputs. Additionally, we
introduce Writer-Director Alignment Loss (WDAL), which ensures consistency
between token generation and final task predictions, improving both text
coherence and numerical accuracy. We evaluate PredGen on multiple
classification and regression benchmarks. Our results show that PredGen
consistently outperforms standard baselines, demonstrating its effectiveness in
structured prediction tasks.

</details>


### [197] [Unveiling the Key Factors for Distilling Chain-of-Thought Reasoning](https://arxiv.org/pdf/2502.18001)
*Xinghao Chen, Zhijing Sun, Wenjin Guo, Miaoran Zhang, Yanjun Chen, Yirong Sun, Hui Su, Yijie Pan, Dietrich Klakow, Wenjie Li, Xiaoyu Shen*

Main category: cs.CL

TL;DR: The study explores factors affecting Chain-of-Thought (CoT) distillation into Small Language Models (SLMs), revealing non-monotonic granularity effects, minimal format impact, and teacher model trade-offs.


<details>
  <summary>Details</summary>
Motivation: To address the computational demands of CoT prompting in LLMs by distilling CoT capabilities into SLMs.

Method: Systematic examination of granularity, format, and teacher model choices via experiments with four teachers and seven students across seven reasoning datasets.

Result: Key findings: (1) SLMs show non-monotonic granularity effects, (2) CoT format impacts LLMs more than SLMs, (3) stronger teachers don't always yield better students.

Conclusion: Tailoring CoT strategies to specific SLMs is crucial, offering insights for optimizing CoT distillation.

Abstract: Large Language Models (LLMs) excel in reasoning tasks through
Chain-of-Thought (CoT) prompting. However, CoT prompting greatly increases
computational demands, which has prompted growing interest in distilling CoT
capabilities into Small Language Models (SLMs). This study systematically
examines the factors influencing CoT distillation, including the choice of
granularity, format and teacher model. Through experiments involving four
teacher models and seven student models across seven mathematical and
commonsense reasoning datasets, we uncover three key findings: (1) Unlike LLMs,
SLMs exhibit a non-monotonic relationship with granularity, with stronger
models benefiting from finer-grained reasoning and weaker models performing
better with simpler CoT supervision; (2) CoT format significantly impacts LLMs
but has minimal effect on SLMs, likely due to their reliance on supervised
fine-tuning rather than pretraining preferences; (3) Stronger teacher models do
NOT always produce better student models, as diversity and complexity in CoT
supervision can outweigh accuracy alone. These findings emphasize the need to
tailor CoT strategies to specific student model, offering actionable insights
for optimizing CoT distillation in SLMs. The code and datasets are available at
https://github.com/EIT-NLP/Distilling-CoT-Reasoning.

</details>


### [198] [Learning to Align Multi-Faceted Evaluation: A Unified and Robust Framework](https://arxiv.org/pdf/2502.18874)
*Kaishuai Xu, Tiezheng Yu, Wenjun Hou, Yi Cheng, Liangyou Li, Xin Jiang, Lifeng Shang, Qun Liu, Wenjie Li*

Main category: cs.CL

TL;DR: ARJudge, a novel framework, improves LLM evaluation by combining text-based and code-driven analyses, outperforming existing methods.


<details>
  <summary>Details</summary>
Motivation: Existing LLM evaluators lack adaptability for unseen instructions and struggle with quantitative/structural constraints.

Method: ARJudge uses a fine-tuned Analyzer for multi-faceted analyses and a tuning-free Refiner for final judgments, trained on a Composite Analysis Corpus.

Result: ARJudge outperforms existing evaluators in effectiveness and robustness, highlighting the value of multi-faceted and code-driven analyses.

Conclusion: ARJudge enhances LLM evaluation by integrating adaptive criteria and diverse analyses, proving more effective and robust.

Abstract: Large Language Models (LLMs) are being used more and more extensively for
automated evaluation in various scenarios. Previous studies have attempted to
fine-tune open-source LLMs to replicate the evaluation explanations and
judgments of powerful proprietary models, such as GPT-4. However, these methods
are largely limited to text-based analyses under predefined general criteria,
resulting in reduced adaptability for unseen instructions and demonstrating
instability in evaluating adherence to quantitative and structural constraints.
To address these limitations, we propose a novel evaluation framework, ARJudge,
that adaptively formulates evaluation criteria and synthesizes both text-based
and code-driven analyses to evaluate LLM responses. ARJudge consists of two
components: a fine-tuned Analyzer that generates multi-faceted evaluation
analyses and a tuning-free Refiner that combines and refines all analyses to
make the final judgment. We construct a Composite Analysis Corpus that
integrates tasks for evaluation criteria generation alongside text-based and
code-driven analysis generation to train the Analyzer. Our results demonstrate
that ARJudge outperforms existing fine-tuned evaluators in effectiveness and
robustness. Furthermore, it demonstrates the importance of multi-faceted
evaluation and code-driven analyses in enhancing evaluation capabilities.

</details>


### [199] [Between Circuits and Chomsky: Pre-pretraining on Formal Languages Imparts Linguistic Biases](https://arxiv.org/pdf/2502.19249)
*Michael Y. Hu, Jackson Petty, Chuan Shi, William Merrill, Tal Linzen*

Main category: cs.CL

TL;DR: Pretraining on formal language improves natural language acquisition by capturing hierarchical dependencies and staying within computational limits, outperforming matched natural language training.


<details>
  <summary>Details</summary>
Motivation: To identify features of formal language that enhance transfer to natural language, leveraging linguistic and complexity theory insights.

Method: Pre-pretraining transformers on formal languages, testing hierarchical dependencies and computational limits, and comparing to natural language training.

Result: Formal languages with hierarchical dependencies reduce loss and improve generalization; pre-pretraining is 33% more efficient than natural language training.

Conclusion: Formal language pretraining, especially with hierarchical dependencies, effectively transfers to natural language, offering efficiency and performance gains.

Abstract: Pretraining language models on formal language can improve their acquisition
of natural language. Which features of the formal language impart an inductive
bias that leads to effective transfer? Drawing on insights from linguistics and
complexity theory, we hypothesize that effective transfer occurs when two
conditions are met: the formal language should capture the dependency
structures present in natural language, and it should remain within the
computational limitations of the model architecture. We experiment with
pre-pretraining (training on formal language before natural languages) on
transformers and find that formal languages capturing hierarchical dependencies
indeed enable language models to achieve lower loss on natural language and
better linguistic generalization compared to other formal languages. We also
find modest support for the hypothesis that the formal language should fall
within the computational limitations of the architecture. Strikingly,
pre-pretraining reduces loss more efficiently than training on a matched amount
of natural language. For a 1B-parameter language model trained on roughly 1.6B
tokens of natural language, pre-pretraining achieves the same loss and better
linguistic generalization with a 33% smaller token budget. Finally, we also
give mechanistic evidence of transfer from formal to natural language:
attention heads acquired during pre-pretraining remain crucial for the model's
performance on syntactic evaluations.

</details>


### [200] [Plan2Align: Predictive Planning Based Test-Time Preference Alignment for Large Language Models](https://arxiv.org/pdf/2502.20795)
*Kuang-Da Wang, Teng-Ruei Chen, Yu Heng Hung, Guo-Xun Ko, Shuoyang Ding, Yueh-Hua Wu, Yu-Chiang Frank Wang, Chao-Han Huck Yang, Wen-Chih Peng, Ping-Chun Hsieh*

Main category: cs.CL

TL;DR: Plan2Align is a test-time alignment framework using predictive planning to improve long-text generation, outperforming existing methods in performance and efficiency.


<details>
  <summary>Details</summary>
Motivation: Existing test-time alignment methods are limited to short responses and slow inference, lacking coherence in extended contexts.

Method: Plan2Align adapts Model Predictive Control (MPC) to iteratively refine outputs by rolling out multiple responses and optimizing segments.

Result: Plan2Align significantly enhances base LLMs' performance on long-text tasks, matching or surpassing existing methods while improving inference efficiency.

Conclusion: Plan2Align offers a resource-efficient and effective solution for aligning large language models, particularly for long-text generation.

Abstract: Aligning Large Language Models with Preference Fine-Tuning is often
resource-intensive. Test-time alignment techniques that do not modify the
underlying models, such as prompting and guided decodings, offer a lightweight
alternative. However, existing test-time alignment methods primarily improve
short responses and fail to ensure coherence over extended contexts due to the
myopic nature of token-level alignment. Moreover, these methods often incur a
slowdown during inference. To address these challenges, we propose Plan2Align,
a test-time alignment framework that formulates text generation as a predictive
planning problem. Plan2Align adapts Model Predictive Control (MPC) to
iteratively refine output by rolling out multiple complete responses and
optimizing each segment. To more rigorously evaluate the effectiveness and
efficiency, we focus on the more challenging task of long-text generation.
Experiments on the long-form response subset of the HH-RLHF dataset and the
WMT'24 Discourse-Level Literary Translation demonstrate that Plan2Align
significantly enhances the performance of base LLMs. Compared to existing
training-time and test-time alignment methods on LLaMA-3.1 8B, Plan2Align
achieves comparable or superior results, while also delivering improved
inference efficiency relative to prior test-time alignment approaches.

</details>


### [201] [The Power of Personality: A Human Simulation Perspective to Investigate Large Language Model Agents](https://arxiv.org/pdf/2502.20859)
*Yifan Duan, Yihong Tang, Xuefeng Bai, Kehai Chen, Juntao Li, Min Zhang*

Main category: cs.CL

TL;DR: The paper explores LLM intelligence by simulating human traits, revealing how personality impacts task performance and multi-agent collaboration.


<details>
  <summary>Details</summary>
Motivation: To bridge the gap between LLM capabilities and human-like intelligence by studying the effects of personality traits on tasks.

Method: Assigning Big Five personality traits to LLM agents and evaluating their performance in single- and multi-agent settings.

Result: Specific traits significantly affect reasoning accuracy and creativity, with multi-agent systems showing unique collective intelligence.

Conclusion: Personality traits in LLMs influence task outcomes, and multi-agent collaboration yields distinct collective behaviors.

Abstract: Large language models (LLMs) excel in both closed tasks (including
problem-solving, and code generation) and open tasks (including creative
writing), yet existing explanations for their capabilities lack connections to
real-world human intelligence. To fill this gap, this paper systematically
investigates LLM intelligence through the lens of ``human simulation'',
addressing three core questions: (1) \textit{How do personality traits affect
problem-solving in closed tasks?} (2) \textit{How do traits shape creativity in
open tasks?} (3) \textit{How does single-agent performance influence
multi-agent collaboration?} By assigning Big Five personality traits to LLM
agents and evaluating their performance in single- and multi-agent settings, we
reveal that specific traits significantly influence reasoning accuracy (closed
tasks) and creative output (open tasks). Furthermore, multi-agent systems
exhibit collective intelligence distinct from individual capabilities, driven
by distinguishing combinations of personalities.

</details>


### [202] [Beware of Your Po! Measuring and Mitigating AI Safety Risks in Role-Play Fine-Tuning of LLMs](https://arxiv.org/pdf/2502.20968)
*Weixiang Zhao, Yulin Hu, Yang Deng, Jiahe Guo, Xingyu Sui, Xinyang Han, An Zhang, Yanyan Zhao, Bing Qin, Tat-Seng Chua, Ting Liu*

Main category: cs.CL

TL;DR: Role-play fine-tuning in LLMs improves adaptability but risks safety, especially for villainous roles. SaRFT is proposed to balance role-play and safety, outperforming baselines.


<details>
  <summary>Details</summary>
Motivation: Address safety risks in role-play fine-tuning of LLMs, which degrade safety performance, particularly for villainous characters.

Method: Train 95 role-specific LLMs using RoleBench, assess risks, and propose Safety-Aware Role-Play Fine-Tuning (SaRFT).

Result: SaRFT outperforms baselines in balancing role-play and safety across multiple models.

Conclusion: Role-adaptive safety measures are necessary, and SaRFT effectively mitigates role-specific safety risks.

Abstract: Role-playing enables large language models (LLMs) to engage users in
immersive and personalized interactions, but it also introduces significant
safety risks. Existing role-play fine-tuning techniques improve role
adaptability but may degrade safety performance, particularly for villainous
characters. In this work, we conduct the first comprehensive assessment of
role-play fine-tuning risks by training 95 role-specific LLMs using RoleBench.
Our experiments reveal that role-play fine-tuning leads to a noticeable decline
in safety performance, with safety risks varying based on character traits. To
tackle this challenge, we propose Safety-Aware Role-Play Fine-Tuning (SaRFT), a
novel method designed to balance role-playing capabilities and safety.
Extensive experiments on LLaMA-3-8B-Instruct, Gemma-2-9B-it, and
Qwen2.5-7B-Instruct demonstrate that SaRFT consistently outperforms
state-of-the-art baselines under both LoRA and full-parameter fine-tuning
settings. Our findings highlight the necessity of role-adaptive safety measures
and provide insights into mitigating role-specific safety risks in role-playing
LLMs.

</details>


### [203] [MA-LoT: Model-Collaboration Lean-based Long Chain-of-Thought Reasoning enhances Formal Theorem Proving](https://arxiv.org/pdf/2503.03205)
*Ruida Wang, Rui Pan, Yuxin Li, Jipeng Zhang, Yizhen Jia, Shizhe Diao, Renjie Pi, Junjie Hu, Tong Zhang*

Main category: cs.CL

TL;DR: MA-LoT is a framework for Lean4 theorem proving that combines model collaboration and Long CoT to outperform existing methods, achieving 61.07% accuracy on MiniF2F-Test.


<details>
  <summary>Details</summary>
Motivation: Existing methods using single LLMs for proof generation or tree search fail to balance these tasks, limiting their effectiveness.

Method: MA-LoT separates cognition tasks (general NL for proof generation and error analysis) via model collaboration and structured LLM-Lean4 interaction in Long CoT. LoT-Transfer Learning enables Long CoT without special data annotation.

Result: Achieves 61.07% accuracy on MiniF2F-Test, surpassing DeepSeek-V3 (33.61%), InternLM-Step-Prover (50.70%), and Godel-Prover (55.33%).

Conclusion: Combining Long CoT with formal verification shows promise for broader, more insightful proof generation.

Abstract: Solving mathematical problems using computer-verifiable languages like Lean
has significantly impacted the mathematical and computer science communities.
State-of-the-art methods utilize a single Large Language Model (LLM) to
generate complete proof or perform tree search, but they fail to balance these
tasks. We propose **MA-LoT**: *Model-CollAboration Lean-based Long
Chain-of-Thought*, a comprehensive framework for Lean4 theorem proving to solve
this issue. It separates the cognition tasks of general NL for whole-proof
generation and error analysis for proof correction using the
model-collaboration method. We achieve this by structured interaction of the
LLM and Lean4 verifier in Long CoT. To implement the framework, we propose the
novel *LoT-Transfer Learning* training-inference pipeline, which enables the
Long CoT thinking capability to LLMs without special data annotation. Extensive
experiment shows that our framework achieves a **61.07%** accuracy rate on the
Lean4 version of the MiniF2F-Test dataset, largely outperforming DeepSeek-V3
(33.61%), single-model tree search (InternLM-Step-Prover, 50.70%), and
whole-proof generation (Godel-Prover, 55.33%) baselines. Furthermore, our
findings highlight the potential of combining Long CoT with formal verification
for a more insightful generation in a broader perspective.

</details>


### [204] [HalluCounter: Reference-free LLM Hallucination Detection in the Wild!](https://arxiv.org/pdf/2503.04615)
*Ashok Urlana, Gopichand Kanumolu, Charaka Vinayak Kumar, Bala Mallikarjunarao Garlapati, Rahul Mishra*

Main category: cs.CL

TL;DR: HalluCounter is a reference-free hallucination detection method using response-response and query-response consistency, outperforming existing methods with over 90% confidence.


<details>
  <summary>Details</summary>
Motivation: Existing RFHD methods lack accuracy due to poor query-response alignment and limited benchmark datasets.

Method: HalluCounter combines response-response and query-response consistency to train a classifier for hallucination detection.

Result: Achieves over 90% average confidence in hallucination detection, surpassing state-of-the-art methods.

Conclusion: HalluCounter and HalluCounterEval address key limitations in hallucination detection, offering improved accuracy and a diverse benchmark dataset.

Abstract: Response consistency-based, reference-free hallucination detection (RFHD)
methods do not depend on internal model states, such as generation
probabilities or gradients, which Grey-box models typically rely on but are
inaccessible in closed-source LLMs. However, their inability to capture
query-response alignment patterns often results in lower detection accuracy.
Additionally, the lack of large-scale benchmark datasets spanning diverse
domains remains a challenge, as most existing datasets are limited in size and
scope. To this end, we propose HalluCounter, a novel reference-free
hallucination detection method that utilizes both response-response and
query-response consistency and alignment patterns. This enables the training of
a classifier that detects hallucinations and provides a confidence score and an
optimal response for user queries. Furthermore, we introduce HalluCounterEval,
a benchmark dataset comprising both synthetically generated and human-curated
samples across multiple domains. Our method outperforms state-of-the-art
approaches by a significant margin, achieving over 90\% average confidence in
hallucination detection across datasets.

</details>


### [205] [How to Protect Yourself from 5G Radiation? Investigating LLM Responses to Implicit Misinformation](https://arxiv.org/pdf/2503.09598)
*Ruohao Guo, Wei Xu, Alan Ritter*

Main category: cs.CL

TL;DR: EchoMist is a benchmark for evaluating LLMs on implicit misinformation, revealing their poor performance and proposing mitigation methods like Self-Alert and RAG.


<details>
  <summary>Details</summary>
Motivation: Address the overlooked issue of implicit misinformation in LLMs, which manifests subtly as unchallenged premises in real-world interactions.

Method: Developed EchoMist, a benchmark embedding false assumptions in queries, and tested 15 LLMs. Investigated mitigation methods (Self-Alert and RAG).

Result: Current LLMs perform poorly on implicit misinformation, often failing to detect false premises. Mitigation methods show limited success.

Conclusion: EchoMist highlights the persistent challenge of implicit misinformation, emphasizing the need for safeguards in LLM deployment.

Abstract: As Large Language Models (LLMs) are widely deployed in diverse scenarios, the
extent to which they could tacitly spread misinformation emerges as a critical
safety concern. Current research primarily evaluates LLMs on explicit false
statements, overlooking how misinformation often manifests subtly as
unchallenged premises in real-world interactions. We curated EchoMist, the
first comprehensive benchmark for implicit misinformation, where false
assumptions are embedded in the query to LLMs. EchoMist targets circulated,
harmful, and ever-evolving implicit misinformation from diverse sources,
including realistic human-AI conversations and social media interactions.
Through extensive empirical studies on 15 state-of-the-art LLMs, we find that
current models perform alarmingly poorly on this task, often failing to detect
false premises and generating counterfactual explanations. We also investigate
two mitigation methods, i.e., Self-Alert and RAG, to enhance LLMs' capability
to counter implicit misinformation. Our findings indicate that EchoMist remains
a persistent challenge and underscore the critical need to safeguard against
the risk of implicit misinformation.

</details>


### [206] [No LLM is Free From Bias: A Comprehensive Study of Bias Evaluation in Large Language Models](https://arxiv.org/pdf/2503.11985)
*Charaka Vinayak Kumar, Ashok Urlana, Gopichand Kanumolu, Bala Mallikarjunarao Garlapati, Pruthwik Mishra*

Main category: cs.CL

TL;DR: The paper evaluates biases in small and medium-sized LLMs, proposes prompting approaches for bias detection, and identifies Phi-3.5B as the least biased model.


<details>
  <summary>Details</summary>
Motivation: Despite LLMs' advancements, biases in training data persist, necessitating a unified evaluation of biases across models.

Method: The study uses representative LLMs and five prompting approaches to detect biases, addressing three research questions.

Result: All evaluated LLMs exhibit biases, with Phi-3.5B performing as the least biased.

Conclusion: The paper highlights challenges in bias detection and suggests future directions for addressing biases in LLMs.

Abstract: Advancements in Large Language Models (LLMs) have increased the performance
of different natural language understanding as well as generation tasks.
Although LLMs have breached the state-of-the-art performance in various tasks,
they often reflect different forms of bias present in the training data. In the
light of this perceived limitation, we provide a unified evaluation of
benchmarks using a set of representative small and medium-sized LLMs that cover
different forms of biases starting from physical characteristics to
socio-economic categories. Moreover, we propose five prompting approaches to
carry out the bias detection task across different aspects of bias. Further, we
formulate three research questions to gain valuable insight in detecting biases
in LLMs using different approaches and evaluation metrics across benchmarks.
The results indicate that each of the selected LLMs suffer from one or the
other form of bias with the Phi-3.5B model being the least biased. Finally, we
conclude the paper with the identification of key challenges and possible
future directions.

</details>


### [207] [Enabling Inclusive Systematic Reviews: Incorporating Preprint Articles with Large Language Model-Driven Evaluations](https://arxiv.org/pdf/2503.13857)
*Rui Yang, Jiayi Tong, Haoyuan Wang, Hui Huang, Ziyang Hu, Peiyu Li, Nan Liu, Christopher J. Lindsell, Michael J. Pencina, Yong Chen, Chuan Hong*

Main category: cs.CL

TL;DR: AutoConfidence is a framework for predicting preprint publication using NLP, semantic embeddings, and LLM-driven scores, improving efficiency for systematic reviews.


<details>
  <summary>Details</summary>
Motivation: Preprints vary in quality, posing challenges for systematic reviews. AutoConfidence aims to reduce manual curation and enhance preprint utilization.

Method: Uses automated data extraction, semantic embeddings, and LLM-driven scores with random forest and survival cure models for prediction.

Result: Random forest achieved AUROC up to 0.747; survival cure model reached AUROC 0.731 and concordance index 0.667.

Conclusion: AutoConfidence improves preprint prediction, aiding systematic reviews by reducing manual effort and integrating advanced features.

Abstract: Background. Systematic reviews in comparative effectiveness research require
timely evidence synthesis. Preprints accelerate knowledge dissemination but
vary in quality, posing challenges for systematic reviews.
  Methods. We propose AutoConfidence (automated confidence assessment), an
advanced framework for predicting preprint publication, which reduces reliance
on manual curation and expands the range of predictors, including three key
advancements: (1) automated data extraction using natural language processing
techniques, (2) semantic embeddings of titles and abstracts, and (3) large
language model (LLM)-driven evaluation scores. Additionally, we employed two
prediction models: a random forest classifier for binary outcome and a survival
cure model that predicts both binary outcome and publication risk over time.
  Results. The random forest classifier achieved AUROC 0.692 with LLM-driven
scores, improving to 0.733 with semantic embeddings and 0.747 with article
usage metrics. The survival cure model reached AUROC 0.716 with LLM-driven
scores, improving to 0.731 with semantic embeddings. For publication risk
prediction, it achieved a concordance index of 0.658, increasing to 0.667 with
semantic embeddings.
  Conclusion. Our study advances the framework for preprint publication
prediction through automated data extraction and multiple feature integration.
By combining semantic embeddings with LLM-driven evaluations, AutoConfidence
enhances predictive performance while reducing manual annotation burden. The
framework has the potential to facilitate incorporation of preprint articles
during the appraisal phase of systematic reviews, supporting researchers in
more effective utilization of preprint resources.

</details>


### [208] [S1-Bench: A Simple Benchmark for Evaluating System 1 Thinking Capability of Large Reasoning Models](https://arxiv.org/pdf/2504.10368)
*Wenyuan Zhang, Shuaiyi Nie, Xinghua Zhang, Zefeng Zhang, Tingwen Liu*

Main category: cs.CL

TL;DR: S1-Bench is a new benchmark for evaluating Large Reasoning Models (LRMs) on simple, intuitive tasks (system 1 thinking), revealing their inefficiency and inaccuracy in such tasks.


<details>
  <summary>Details</summary>
Motivation: Current LRMs excel in complex reasoning (system 2) but lack benchmarks for evaluating their system 1 thinking capabilities.

Method: S1-Bench introduces diverse, simple questions across domains and languages to assess LRMs' system 1 performance.

Result: Evaluations of 28 LRMs show inefficiency, low accuracy, and robustness issues in simple tasks, along with a perception-generation gap.

Conclusion: S1-Bench highlights the need for dual-system compatibility in LRM development to improve system 1 thinking performance.

Abstract: We introduce S1-Bench, a novel benchmark designed to evaluate the performance
of Large Reasoning Models (LRMs) on simple tasks that favor intuitive system 1
thinking rather than deliberative system 2 reasoning. While LRMs have achieved
significant breakthroughs in complex reasoning tasks through explicit chains of
thought, their heavy reliance on system 2 thinking may limit their system 1
thinking capabilities. However, there is a lack of an appropriate benchmark for
evaluating LRM's system 1 thinking capabilities. To fill this gap, S1-Bench
introduces a suite of simple, diverse, and natural questions across multiple
domains and languages, specifically designed to assess LRMs' performance on
questions more suitable for system 1 . We conduct extensive evaluations across
28 LRMs, revealing their inefficiency, inadequate accuracy, and limited
robustness when handling simple questions. Additionally, we observe a gap
between their difficulty perception and generation length. Overall, this work
paves the way toward dual-system compatibility in the development of LRMs.

</details>


### [209] [Information Gain-Guided Causal Intervention for Autonomous Debiasing Large Language Models](https://arxiv.org/pdf/2504.12898)
*Zhouhao Sun, Xiao Ding, Li Du, Yunpeng Xu, Yixuan Ma, Yang Zhao, Bing Qin, Ting Liu*

Main category: cs.CL

TL;DR: The paper proposes an information gain-guided causal intervention debiasing (ICD) framework to address dataset biases in LLMs, improving their generalizability.


<details>
  <summary>Details</summary>
Motivation: Current LLMs still capture dataset biases, limiting their generalizability, and existing debiasing methods are insufficient.

Method: Combines causal mechanisms with information theory, using causal intervention-based data rewriting to balance dataset distribution and supervised fine-tuning.

Result: ICD effectively debiases LLMs, enhancing their performance across various tasks.

Conclusion: The ICD framework successfully reduces biases in LLMs, improving their generalizability.

Abstract: Despite significant progress, recent studies indicate that current large
language models (LLMs) may still capture dataset biases and utilize them during
inference, leading to the poor generalizability of LLMs. However, due to the
diversity of dataset biases and the insufficient nature of bias suppression
based on in-context learning, the effectiveness of previous prior
knowledge-based debiasing methods and in-context learning based automatic
debiasing methods is limited. To address these challenges, we explore the
combination of causal mechanisms with information theory and propose an
information gain-guided causal intervention debiasing (ICD) framework. To
eliminate biases within the instruction-tuning dataset, it is essential to
ensure that these biases do not provide any additional information to predict
the answers, i.e., the information gain of these biases for predicting the
answers needs to be 0. Under this guidance, this framework utilizes a causal
intervention-based data rewriting method to automatically and autonomously
balance the distribution of instruction-tuning dataset for reducing the
information gain. Subsequently, it employs a standard supervised fine-tuning
process to train LLMs on the debiased dataset. Experimental results show that
ICD can effectively debias LLM to improve its generalizability across different
tasks.

</details>


### [210] [Rethinking Memory in AI: Taxonomy, Operations, Topics, and Future Directions](https://arxiv.org/pdf/2505.00675)
*Yiming Du, Wenyu Huang, Danna Zheng, Zhaowei Wang, Sebastien Montella, Mirella Lapata, Kam-Fai Wong, Jeff Z. Pan*

Main category: cs.CL

TL;DR: The survey categorizes memory representations in AI into parametric and contextual forms, introduces six fundamental memory operations, and maps them to key research topics, providing a structured perspective on memory in LLMs.


<details>
  <summary>Details</summary>
Motivation: Prior surveys overlook the atomic operations underlying memory dynamics in AI, prompting this structured review.

Method: Categorizes memory representations, introduces six atomic operations, and maps them to relevant research topics.

Result: A dynamic framework for understanding memory in AI, with links to datasets and tools.

Conclusion: The survey clarifies memory dynamics in LLMs and suggests future research directions.

Abstract: Memory is a fundamental component of AI systems, underpinning large language
models (LLMs)-based agents. While prior surveys have focused on memory
applications with LLMs (e.g., enabling personalized memory in conversational
agents), they often overlook the atomic operations that underlie memory
dynamics. In this survey, we first categorize memory representations into
parametric and contextual forms, and then introduce six fundamental memory
operations: Consolidation, Updating, Indexing, Forgetting, Retrieval, and
Compression. We map these operations to the most relevant research topics
across long-term, long-context, parametric modification, and multi-source
memory. By reframing memory systems through the lens of atomic operations and
representation types, this survey provides a structured and dynamic perspective
on research, benchmark datasets, and tools related to memory in AI, clarifying
the functional interplay in LLMs based agents while outlining promising
directions for future research\footnote{The paper list, datasets, methods and
tools are available at
\href{https://github.com/Elvin-Yiming-Du/Survey_Memory_in_AI}{https://github.com/Elvin-Yiming-Du/Survey\_Memory\_in\_AI}.}.

</details>


### [211] [Unveiling Language-Specific Features in Large Language Models via Sparse Autoencoders](https://arxiv.org/pdf/2505.05111)
*Boyi Deng, Yu Wan, Yidan Zhang, Baosong Yang, Fuli Feng*

Main category: cs.CL

TL;DR: The paper explores multilingual capabilities in LLMs using Sparse Autoencoders (SAEs) to identify language-specific features, showing their impact on model performance and their utility in controlling language generation.


<details>
  <summary>Details</summary>
Motivation: To address limitations of neuron-based methods in analyzing multilingual LLMs, such as superposition and layer-wise activation variance, by leveraging SAEs for more reliable feature decomposition.

Method: Uses SAEs to decompose LLM activations into sparse features, introduces a metric for monolinguality, and ablates features to test language-specific impacts. Also enhances steering vectors with SAE-derived features.

Result: Identifies language-specific SAE features; ablating them affects only one language. Some languages have synergistic features, and combining SAE features improves control over LLM language generation.

Conclusion: SAEs provide a robust method to analyze and control multilingual LLMs, revealing language-specific features and enabling targeted model adjustments.

Abstract: The mechanisms behind multilingual capabilities in Large Language Models
(LLMs) have been examined using neuron-based or internal-activation-based
methods. However, these methods often face challenges such as superposition and
layer-wise activation variance, which limit their reliability. Sparse
Autoencoders (SAEs) offer a more nuanced analysis by decomposing the
activations of LLMs into a sparse linear combination of SAE features. We
introduce a novel metric to assess the monolinguality of features obtained from
SAEs, discovering that some features are strongly related to specific
languages. Additionally, we show that ablating these SAE features only
significantly reduces abilities in one language of LLMs, leaving others almost
unaffected. Interestingly, we find some languages have multiple synergistic SAE
features, and ablating them together yields greater improvement than ablating
individually. Moreover, we leverage these SAE-derived language-specific
features to enhance steering vectors, achieving control over the language
generated by LLMs. The code is publicly available at
https://github.com/Aatrox103/multilingual-llm-features.

</details>


### [212] [Beyond 'Aha!': Toward Systematic Meta-Abilities Alignment in Large Reasoning Models](https://arxiv.org/pdf/2505.10554)
*Zhiyuan Hu, Yibo Wang, Hanze Dong, Yuhui Xu, Amrita Saha, Caiming Xiong, Bryan Hooi, Junnan Li*

Main category: cs.CL

TL;DR: The paper proposes a method to enhance reasoning in large models by explicitly aligning them with meta-abilities (deduction, induction, abduction) using self-verifiable tasks, improving performance and reliability.


<details>
  <summary>Details</summary>
Motivation: Current large reasoning models exhibit unpredictable emergent reasoning behaviors, limiting scalability and reliability.

Method: A three-stage pipeline: individual alignment, parameter-space merging, and domain-specific reinforcement learning.

Result: Performance boosts by over 10% compared to baselines, with additional gains in 7B and 32B models across benchmarks.

Conclusion: Explicit meta-ability alignment provides a scalable and dependable foundation for reasoning in large models.

Abstract: Large reasoning models (LRMs) already possess a latent capacity for long
chain-of-thought reasoning. Prior work has shown that outcome-based
reinforcement learning (RL) can incidentally elicit advanced reasoning
behaviors such as self-correction, backtracking, and verification phenomena
often referred to as the model's "aha moment". However, the timing and
consistency of these emergent behaviors remain unpredictable and
uncontrollable, limiting the scalability and reliability of LRMs' reasoning
capabilities. To address these limitations, we move beyond reliance on prompts
and coincidental "aha moments". Instead, we explicitly align models with three
meta-abilities: deduction, induction, and abduction, using automatically
generated, self-verifiable tasks. Our three stage-pipeline individual
alignment, parameter-space merging, and domain-specific reinforcement learning,
boosting performance by over 10\% relative to instruction-tuned baselines.
Furthermore, domain-specific RL from the aligned checkpoint yields an
additional gain in performance ceiling for both 7B and 32B models across math,
coding, and science benchmarks, demonstrating that explicit meta-ability
alignment offers a scalable and dependable foundation for reasoning. Code is
available at: https://github.com/zhiyuanhubj/Meta-Ability-Alignment

</details>


### [213] [SoftCoT++: Test-Time Scaling with Soft Chain-of-Thought Reasoning](https://arxiv.org/pdf/2505.11484)
*Yige Xu, Xu Guo, Zhiwei Zeng, Chunyan Miao*

Main category: cs.CL

TL;DR: SoftCoT++ enhances reasoning by diversifying latent thought exploration in continuous space, outperforming SoftCoT and self-consistency scaling.


<details>
  <summary>Details</summary>
Motivation: Existing continuous-space reasoning methods lack diversity in exploration due to fixed latent representations.

Method: Introduces SoftCoT++, which perturbs latent thoughts via specialized initial tokens and uses contrastive learning to diversify soft thought representations.

Result: Significantly improves reasoning performance across benchmarks and LLM architectures, outperforming SoftCoT and self-consistency scaling.

Conclusion: SoftCoT++ effectively addresses the diversity limitation in continuous-space reasoning and is compatible with conventional scaling techniques.

Abstract: Test-Time Scaling (TTS) refers to approaches that improve reasoning
performance by allocating extra computation during inference, without altering
the model's parameters. While existing TTS methods operate in a discrete token
space by generating more intermediate steps, recent studies in Coconut and
SoftCoT have demonstrated that thinking in the continuous latent space can
further enhance the reasoning performance. Such latent thoughts encode
informative thinking without the information loss associated with
autoregressive token generation, sparking increased interest in
continuous-space reasoning. Unlike discrete decoding, where repeated sampling
enables exploring diverse reasoning paths, latent representations in continuous
space are fixed for a given input, which limits diverse exploration, as all
decoded paths originate from the same latent thought. To overcome this
limitation, we introduce SoftCoT++ to extend SoftCoT to the Test-Time Scaling
paradigm by enabling diverse exploration of thinking paths. Specifically, we
perturb latent thoughts via multiple specialized initial tokens and apply
contrastive learning to promote diversity among soft thought representations.
Experiments across five reasoning benchmarks and two distinct LLM architectures
demonstrate that SoftCoT++ significantly boosts SoftCoT and also outperforms
SoftCoT with self-consistency scaling. Moreover, it shows strong compatibility
with conventional scaling techniques such as self-consistency. Source code is
available at https://github.com/xuyige/SoftCoT.

</details>


### [214] [Retrospex: Language Agent Meets Offline Reinforcement Learning Critic](https://arxiv.org/pdf/2505.11807)
*Yufei Xiang, Yiqun Shen, Yeqin Zhang, Cam-Tu Nguyen*

Main category: cs.CL

TL;DR: Retrospex is a new LLM-based agent framework that leverages past experiences via RL and dynamic action rescoring, outperforming baselines in tested environments.


<details>
  <summary>Details</summary>
Motivation: Existing LLM agent frameworks underutilize past experiences for improvement, limiting their potential.

Method: Retrospex combines LLM action likelihood with RL Critic values from offline retrospection and uses dynamic action rescoring.

Result: Retrospex outperforms contemporary baselines in ScienceWorld, ALFWorld, and Webshop environments.

Conclusion: Retrospex effectively enhances LLM agents by integrating past experiences through RL and dynamic mechanisms.

Abstract: Large Language Models (LLMs) possess extensive knowledge and commonsense
reasoning capabilities, making them valuable for creating powerful agents.
However, existing LLM agent frameworks have not fully utilized past experiences
for improvement. This work introduces a new LLM-based agent framework called
Retrospex, which addresses this challenge by analyzing past experiences in
depth. Unlike previous approaches, Retrospex does not directly integrate
experiences into the LLM's context. Instead, it combines the LLM's action
likelihood with action values estimated by a Reinforcement Learning (RL)
Critic, which is trained on past experiences through an offline
''retrospection'' process. Additionally, Retrospex employs a dynamic action
rescoring mechanism that increases the importance of experience-based values
for tasks that require more interaction with the environment. We evaluate
Retrospex in ScienceWorld, ALFWorld and Webshop environments, demonstrating its
advantages over strong, contemporary baselines.

</details>


### [215] [Enhance Mobile Agents Thinking Process Via Iterative Preference Learning](https://arxiv.org/pdf/2505.12299)
*Kun Huang, Weikai Xu, Yuxuan Liu, Quandong Wang, Pengzhi Gao, Wei Liu, Jian Luan, Bin Wang, Bo An*

Main category: cs.CL

TL;DR: The paper introduces Iterative Preference Learning (IPL) to enhance the reasoning of VLM-based mobile agents by constructing a CoaT-tree and using rule-based rewards, outperforming existing methods.


<details>
  <summary>Details</summary>
Motivation: Address the limitations of existing self-training approaches for CoaT trajectories, which lack correctness in intermediate steps or rely on expensive annotations.

Method: Proposes IPL with CoaT-tree construction, rule-based leaf scoring, and T-DPO pairs. Uses GPT-4o for diverse Q&A generation to improve generality.

Result: MobileIPL outperforms baselines like OS-ATLAS and UI-TARS, achieving SOTA on three benchmarks and strong out-of-domain generalization.

Conclusion: IPL effectively improves reasoning and generalization in mobile GUI agents, addressing data scarcity and correctness issues.

Abstract: The Chain of Action-Planning Thoughts (CoaT) paradigm has been shown to
improve the reasoning performance of VLM-based mobile agents in GUI tasks.
However, the scarcity of diverse CoaT trajectories limits the expressiveness
and generalization ability of such agents. While self-training is commonly
employed to address data scarcity, existing approaches either overlook the
correctness of intermediate reasoning steps or depend on expensive
process-level annotations to construct process reward models (PRM). To address
the above problems, we propose an Iterative Preference Learning (IPL) that
constructs a CoaT-tree through interative sampling, scores leaf nodes using
rule-based reward, and backpropagates feedback to derive Thinking-level Direct
Preference Optimization (T-DPO) pairs. To prevent overfitting during warm-up
supervised fine-tuning, we further introduce a three-stage instruction
evolution, which leverages GPT-4o to generate diverse Q\&A pairs based on real
mobile UI screenshots, enhancing both generality and layout understanding.
Experiments on three standard Mobile GUI-agent benchmarks demonstrate that our
agent MobileIPL outperforms strong baselines, including continual pretraining
models such as OS-ATLAS and UI-TARS. It achieves state-of-the-art performance
across three standard Mobile GUI-Agents benchmarks and shows strong
generalization to out-of-domain scenarios.

</details>


### [216] [Shadow-FT: Tuning Instruct via Base](https://arxiv.org/pdf/2505.12716)
*Taiqiang Wu, Runming Yang, Jiayi Li, Pengfei Hu, Ngai Wong, Yujiu Yang*

Main category: cs.CL

TL;DR: Shadow-FT improves INSTRUCT model performance by grafting weight updates from fine-tuned BASE models, outperforming traditional tuning methods.


<details>
  <summary>Details</summary>
Motivation: Direct fine-tuning of INSTRUCT models often yields marginal gains or performance drops, despite BASE models having highly similar weights.

Method: Shadow-FT fine-tunes the BASE model and grafts the learned updates to the INSTRUCT model, introducing no extra parameters.

Result: Shadow-FT consistently outperforms full-parameter and parameter-efficient tuning on 19 benchmarks, including coding, reasoning, and math tasks.

Conclusion: Shadow-FT is effective, scalable, and applicable to multimodal LLMs and DPO, with code and weights publicly available.

Abstract: Large language models (LLMs) consistently benefit from further fine-tuning on
various tasks. However, we observe that directly tuning the INSTRUCT (i.e.,
instruction tuned) models often leads to marginal improvements and even
performance degeneration. Notably, paired BASE models, the foundation for these
INSTRUCT variants, contain highly similar weight values (i.e., less than 2% on
average for Llama 3.1 8B). Therefore, we propose a novel Shadow-FT framework to
tune the INSTRUCT models by leveraging the corresponding BASE models. The key
insight is to fine-tune the BASE model, and then directly graft the learned
weight updates to the INSTRUCT model. Our proposed Shadow-FT introduces no
additional parameters, is easy to implement, and significantly improves
performance. We conduct extensive experiments on tuning mainstream LLMs, such
as Qwen 3 and Llama 3 series, and evaluate them across 19 benchmarks covering
coding, reasoning, and mathematical tasks. Experimental results demonstrate
that Shadow-FT consistently outperforms conventional full-parameter and
parameter-efficient tuning approaches. Further analyses indicate that Shadow-FT
can be applied to multimodal large language models (MLLMs) and combined with
direct preference optimization (DPO). Codes and weights are available at
\href{https://github.com/wutaiqiang/Shadow-FT}{Github}.

</details>


### [217] [Systematic Generalization in Language Models Scales with Information Entropy](https://arxiv.org/pdf/2505.13089)
*Sondre Wold, Lucas Georges Gabriel Charpentier, Étienne Simon*

Main category: cs.CL

TL;DR: The paper explores how entropy of component parts in training data affects systematic generalization in language models, showing performance scales with entropy.


<details>
  <summary>Details</summary>
Motivation: To address the challenge of systematic generalization in language models, particularly their sensitivity to input permutations and novel contexts.

Method: Formalized a framework to measure entropy in sequence-to-sequence tasks and analyzed performance of model architectures relative to entropy.

Result: Performance of models scales with entropy; high entropy success is possible without built-in priors, and low entropy serves as a benchmark for robust generalization.

Conclusion: Connects systematic generalization to information efficiency, suggesting entropy as a measure for assessing progress in robust generalization.

Abstract: Systematic generalization remains challenging for current language models,
which are known to be both sensitive to semantically similar permutations of
the input and to struggle with known concepts presented in novel contexts.
Although benchmarks exist for assessing compositional behavior, it is unclear
how to measure the difficulty of a systematic generalization problem. In this
work, we show how one aspect of systematic generalization can be described by
the entropy of the distribution of component parts in the training data. We
formalize a framework for measuring entropy in a sequence-to-sequence task and
find that the performance of popular model architectures scales with the
entropy. Our work connects systematic generalization to information efficiency,
and our results indicate that success at high entropy can be achieved even
without built-in priors, and that success at low entropy can serve as a target
for assessing progress towards robust systematic generalization.

</details>


### [218] [DRP: Distilled Reasoning Pruning with Skill-aware Step Decomposition for Efficient Large Reasoning Models](https://arxiv.org/pdf/2505.13975)
*Yuxuan Jiang, Dawei Li, Frank Ferraro*

Main category: cs.CL

TL;DR: DRP improves token efficiency in LRMs by pruning verbose reasoning traces and distilling pruned paths into a student model, maintaining accuracy.


<details>
  <summary>Details</summary>
Motivation: LRMs suffer from inefficiency due to verbose reasoning traces, prompting the need for a method to streamline reasoning without losing accuracy.

Method: DRP combines inference-time pruning and tuning-based distillation, using a teacher model for step decomposition and pruning, then distilling into a student model.

Result: DRP reduces token usage significantly (e.g., GSM8K: 917 to 328 tokens) while improving accuracy (91.7% to 94.1%).

Conclusion: Aligning reasoning structure with student capacity is key for effective knowledge transfer and performance gains.

Abstract: While Large Reasoning Models (LRMs) have demonstrated success in complex
reasoning tasks through long chain-of-thought (CoT) reasoning, their inference
often involves excessively verbose reasoning traces, resulting in substantial
inefficiency. To address this, we propose Distilled Reasoning Pruning (DRP), a
hybrid framework that combines inference-time pruning with tuning-based
distillation, two widely used strategies for efficient reasoning. DRP uses a
teacher model to perform skill-aware step decomposition and content pruning,
and then distills the pruned reasoning paths into a student model, enabling it
to reason both efficiently and accurately. Across several challenging
mathematical reasoning datasets, we find that models trained with DRP achieve
substantial improvements in token efficiency without sacrificing accuracy.
Specifically, DRP reduces average token usage on GSM8K from 917 to 328 while
improving accuracy from 91.7% to 94.1%, and achieves a 43% token reduction on
AIME with no performance drop. Further analysis shows that aligning the
reasoning structure of training CoTs with the student's reasoning capacity is
critical for effective knowledge transfer and performance gains.

</details>


### [219] [DiagnosisArena: Benchmarking Diagnostic Reasoning for Large Language Models](https://arxiv.org/pdf/2505.14107)
*Yakun Zhu, Zhongzhen Huang, Linjie Mu, Yutong Huang, Wei Nie, Jiaji Liu, Shaoting Zhang, Pengfei Liu, Xiaofan Zhang*

Main category: cs.CL

TL;DR: DiagnosisArena is a new benchmark to evaluate large language models' diagnostic reasoning in healthcare, revealing their current limitations.


<details>
  <summary>Details</summary>
Motivation: To address the lack of robust benchmarks for assessing AI's diagnostic capabilities in complex clinical scenarios.

Method: Developed DiagnosisArena with 1,113 patient case-diagnosis pairs from top medical journals, involving AI and human expert reviews.

Result: Top models (o3-mini, o1, DeepSeek-R1) achieved low accuracies (45.82%, 31.09%, 17.79%), showing a generalization bottleneck.

Conclusion: DiagnosisArena aims to advance AI diagnostic reasoning for real-world clinical challenges, with tools available for further research.

Abstract: The emergence of groundbreaking large language models capable of performing
complex reasoning tasks holds significant promise for addressing various
scientific challenges, including those arising in complex clinical scenarios.
To enable their safe and effective deployment in real-world healthcare
settings, it is urgently necessary to benchmark the diagnostic capabilities of
current models systematically. Given the limitations of existing medical
benchmarks in evaluating advanced diagnostic reasoning, we present
DiagnosisArena, a comprehensive and challenging benchmark designed to
rigorously assess professional-level diagnostic competence. DiagnosisArena
consists of 1,113 pairs of segmented patient cases and corresponding diagnoses,
spanning 28 medical specialties, deriving from clinical case reports published
in 10 top-tier medical journals. The benchmark is developed through a
meticulous construction pipeline, involving multiple rounds of screening and
review by both AI systems and human experts, with thorough checks conducted to
prevent data leakage. Our study reveals that even the most advanced reasoning
models, o3-mini, o1, and DeepSeek-R1, achieve only 45.82%, 31.09%, and 17.79%
accuracy, respectively. This finding highlights a significant generalization
bottleneck in current large language models when faced with clinical diagnostic
reasoning challenges. Through DiagnosisArena, we aim to drive further
advancements in AIs diagnostic reasoning capabilities, enabling more effective
solutions for real-world clinical diagnostic challenges. We provide the
benchmark and evaluation tools for further research and development
https://github.com/SPIRAL-MED/DiagnosisArena.

</details>


### [220] [SEPS: A Separability Measure for Robust Unlearning in LLMs](https://arxiv.org/pdf/2505.14832)
*Wonje Jeung, Sangyeon Yoon, Albert No*

Main category: cs.CL

TL;DR: SEPS introduces a framework for evaluating machine unlearning in LLMs, addressing mixed-query scenarios. It identifies failure modes in existing methods and proposes Mixed Prompt unlearning for better performance.


<details>
  <summary>Details</summary>
Motivation: Existing unlearning metrics fail to capture real-world scenarios where forget and retain queries coexist in prompts, necessitating a more robust evaluation framework.

Method: SEPS evaluates unlearning in mixed-query prompts. Mixed Prompt (MP) unlearning integrates forget and retain queries into a unified training objective.

Result: SEPS identifies failure modes in untargeted and targeted unlearning. MP unlearning improves effectiveness, handling up to eight mixed queries robustly.

Conclusion: SEPS and MP unlearning address limitations of existing methods, enhancing unlearning performance in complex, mixed-query scenarios.

Abstract: Machine unlearning aims to selectively remove targeted knowledge from Large
Language Models (LLMs), ensuring they forget specified content while retaining
essential information. Existing unlearning metrics assess whether a model
correctly answers retain queries and rejects forget queries, but they fail to
capture real-world scenarios where forget queries rarely appear in isolation.
In fact, forget and retain queries often coexist within the same prompt, making
mixed-query evaluation crucial.
  We introduce SEPS, an evaluation framework that explicitly measures a model's
ability to both forget and retain information within a single prompt. Through
extensive experiments across three benchmarks, we identify two key failure
modes in existing unlearning methods: (1) untargeted unlearning
indiscriminately erases both forget and retain content once a forget query
appears, and (2) targeted unlearning overfits to single-query scenarios,
leading to catastrophic failures when handling multiple queries. To address
these issues, we propose Mixed Prompt (MP) unlearning, a strategy that
integrates both forget and retain queries into a unified training objective.
Our approach significantly improves unlearning effectiveness, demonstrating
robustness even in complex settings with up to eight mixed forget and retain
queries in a single prompt.

</details>


### [221] [DUSK: Do Not Unlearn Shared Knowledge](https://arxiv.org/pdf/2505.15209)
*Wonje Jeung, Sangyeon Yoon, Hyesoo Hong, Soeun Kim, Seungju Han, Youngjae Yu, Albert No*

Main category: cs.CL

TL;DR: DUSK is a benchmark for evaluating machine unlearning methods under realistic data overlap, revealing current methods struggle with selective removal of unique content while preserving shared facts.


<details>
  <summary>Details</summary>
Motivation: Address concerns about unauthorized use of copyrighted/sensitive data in LLMs by improving unlearning methods for overlapping datasets.

Method: Introduces DUSK, a benchmark with document sets sharing factual content but differing in style, to test selective unlearning.

Result: Evaluation of nine unlearning methods shows they often fail to remove deeper knowledge without harming shared content.

Conclusion: DUSK highlights the need for more precise unlearning techniques and is released as a public benchmark for further development.

Abstract: Large language models (LLMs) are increasingly deployed in real-world
applications, raising concerns about the unauthorized use of copyrighted or
sensitive data. Machine unlearning aims to remove such 'forget' data while
preserving utility and information from the 'retain' set. However, existing
evaluations typically assume that forget and retain sets are fully disjoint,
overlooking realistic scenarios where they share overlapping content. For
instance, a news article may need to be unlearned, even though the same event,
such as an earthquake in Japan, is also described factually on Wikipedia.
Effective unlearning should remove the specific phrasing of the news article
while preserving publicly supported facts. In this paper, we introduce DUSK, a
benchmark designed to evaluate unlearning methods under realistic data overlap.
DUSK constructs document sets that describe the same factual content in
different styles, with some shared information appearing across all sets and
other content remaining unique to each. When one set is designated for
unlearning, an ideal method should remove its unique content while preserving
shared facts. We define seven evaluation metrics to assess whether unlearning
methods can achieve this selective removal. Our evaluation of nine recent
unlearning methods reveals a key limitation: while most can remove
surface-level text, they often fail to erase deeper, context-specific knowledge
without damaging shared content. We release DUSK as a public benchmark to
support the development of more precise and reliable unlearning techniques for
real-world applications.

</details>


### [222] [R-TOFU: Unlearning in Large Reasoning Models](https://arxiv.org/pdf/2505.15214)
*Sangyeon Yoon, Wonje Jeung, Albert No*

Main category: cs.CL

TL;DR: R-TOFU is a benchmark for evaluating unlearning in Large Reasoning Models (LRMs) with step-wise metrics. It reveals residual knowledge in reasoning traces and compares gradient-based and preference-optimization methods. A new method, Reasoned IDK, improves forgetting efficacy while preserving reasoning utility.


<details>
  <summary>Details</summary>
Motivation: Existing unlearning benchmarks focus on final answers, ignoring multi-step reasoning traces in LRMs. This gap makes unlearning unreliable.

Method: R-TOFU augments unlearning tasks with realistic chain-of-thought annotations and step-wise metrics. It evaluates gradient-based and preference-optimization baselines and introduces Reasoned IDK.

Result: Conventional methods leave forget traces in reasoning. Reasoned IDK balances forgetting and utility better. Decoding variants like ZeroThink can still reveal forgotten content.

Conclusion: R-TOFU provides a systematic foundation for studying unlearning in LRMs, highlighting the need for diverse evaluation settings and improved methods like Reasoned IDK.

Abstract: Large Reasoning Models (LRMs) embed private or copyrighted information not
only in their final answers but also throughout multi-step chain-of-thought
(CoT) traces, making reliable unlearning far more demanding than in standard
LLMs. We introduce Reasoning-TOFU (R-TOFU), the first benchmark tailored to
this setting. R-TOFU augments existing unlearning tasks with realistic CoT
annotations and provides step-wise metrics that expose residual knowledge
invisible to answer-level checks. Using R-TOFU, we carry out a comprehensive
comparison of gradient-based and preference-optimization baselines and show
that conventional answer-only objectives leave substantial forget traces in
reasoning. We further propose Reasoned IDK, a preference-optimization variant
that preserves coherent yet inconclusive reasoning, achieving a stronger
balance between forgetting efficacy and model utility than earlier refusal
styles. Finally, we identify a failure mode: decoding variants such as
ZeroThink and LessThink can still reveal forgotten content despite seemingly
successful unlearning, emphasizing the need to evaluate models under diverse
decoding settings. Together, the benchmark, analysis, and new baseline
establish a systematic foundation for studying and improving unlearning in LRMs
while preserving their reasoning capabilities.

</details>


### [223] [Benchmarking and Pushing the Multi-Bias Elimination Boundary of LLMs via Causal Effect Estimation-guided Debiasing](https://arxiv.org/pdf/2505.16522)
*Zhouhao Sun, Zhiyuan Kan, Xiao Ding, Li Du, Yang Zhao, Bing Qin, Ting Liu*

Main category: cs.CL

TL;DR: A multi-bias benchmark and CMBE method are proposed to address LLMs' poor generalizability due to multiple biases, showing improved performance.


<details>
  <summary>Details</summary>
Motivation: Current LLMs struggle with multiple biases in real-world data, and existing benchmarks lack multi-bias evaluation.

Method: Proposes a multi-bias benchmark and CMBE, a causal effect estimation-guided method to eliminate multiple biases.

Result: CMBE effectively eliminates multiple biases, enhancing LLM generalizability.

Conclusion: CMBE addresses the challenge of multi-bias elimination, improving LLM performance.

Abstract: Despite significant progress, recent studies have indicated that current
large language models (LLMs) may still utilize bias during inference, leading
to the poor generalizability of LLMs. Some benchmarks are proposed to
investigate the generalizability of LLMs, with each piece of data typically
containing one type of controlled bias. However, a single piece of data may
contain multiple types of biases in practical applications. To bridge this gap,
we propose a multi-bias benchmark where each piece of data contains five types
of biases. The evaluations conducted on this benchmark reveal that the
performance of existing LLMs and debiasing methods is unsatisfying,
highlighting the challenge of eliminating multiple types of biases
simultaneously. To overcome this challenge, we propose a causal effect
estimation-guided multi-bias elimination method (CMBE). This method first
estimates the causal effect of multiple types of biases simultaneously.
Subsequently, we eliminate the causal effect of biases from the total causal
effect exerted by both the semantic information and biases during inference.
Experimental results show that CMBE can effectively eliminate multiple types of
bias simultaneously to enhance the generalizability of LLMs.

</details>


### [224] [Think Silently, Think Fast: Dynamic Latent Compression of LLM Reasoning Chains](https://arxiv.org/pdf/2505.16552)
*Wenhui Tan, Jiaze Li, Jianzhong Ju, Zhenbo Luo, Jian Luan, Ruihua Song*

Main category: cs.CL

TL;DR: CoLaR compresses reasoning chains in latent space, reducing computational cost while maintaining performance, outperforming baselines in accuracy and efficiency.


<details>
  <summary>Details</summary>
Motivation: Token-level reasoning in LLMs is computationally expensive and inefficient, prompting the need for a more efficient method like CoLaR.

Method: CoLaR uses a two-stage approach: supervised fine-tuning with compressed embedding prediction and RL to explore diverse reasoning paths.

Result: CoLaR achieves 14.1% higher accuracy than baselines, reduces reasoning chain length by 53.3%, and improves performance in challenging tasks by 5.4%.

Conclusion: CoLaR offers an efficient and dynamic alternative to explicit reasoning, significantly reducing computational overhead without major performance loss.

Abstract: Large Language Models (LLMs) achieve superior performance through
Chain-of-Thought (CoT) reasoning, but these token-level reasoning chains are
computationally expensive and inefficient. In this paper, we introduce
Compressed Latent Reasoning (CoLaR), a novel framework that dynamically
compresses reasoning processes in latent space through a two-stage training
approach. First, during supervised fine-tuning, CoLaR extends beyond next-token
prediction by incorporating an auxiliary next compressed embedding prediction
objective. This process merges embeddings of consecutive tokens using a
compression factor randomly sampled from a predefined range, and trains a
specialized latent head to predict distributions of subsequent compressed
embeddings. Second, we enhance CoLaR through reinforcement learning (RL) that
leverages the latent head's non-deterministic nature to explore diverse
reasoning paths and exploit more compact ones. This approach enables CoLaR to:
i) perform reasoning at a dense latent level (i.e., silently), substantially
reducing reasoning chain length, and ii) dynamically adjust reasoning speed at
inference time by simply prompting the desired compression factor. Extensive
experiments across four mathematical reasoning datasets demonstrate that CoLaR
achieves 14.1% higher accuracy than latent-based baseline methods at comparable
compression ratios, and reduces reasoning chain length by 53.3% with only 4.8%
performance degradation compared to explicit CoT method. Moreover, when applied
to more challenging mathematical reasoning tasks, our RL-enhanced CoLaR
demonstrates performance gains of up to 5.4% while dramatically reducing latent
reasoning chain length by 82.8%. The code and models will be released upon
acceptance.

</details>


### [225] [Does Synthetic Data Help Named Entity Recognition for Low-Resource Languages?](https://arxiv.org/pdf/2505.16814)
*Gaurav Kamath, Sowmya Vajjala*

Main category: cs.CL

TL;DR: Synthetic data shows promise for improving NER in low-resource languages, with varying effectiveness across 11 diverse languages.


<details>
  <summary>Details</summary>
Motivation: Addressing the challenge of limited labeled training data for NER in low-resource languages.

Method: Using synthetic data augmentation for multilingual NER across 11 diverse languages.

Result: Synthetic data improves NER performance, but effectiveness varies significantly between languages.

Conclusion: Synthetic data is a viable solution for low-resource NER, though language-specific adaptation may be needed.

Abstract: Named Entity Recognition(NER) for low-resource languages aims to produce
robust systems for languages where there is limited labeled training data
available, and has been an area of increasing interest within NLP. Data
augmentation for increasing the amount of low-resource labeled data is a common
practice. In this paper, we explore the role of synthetic data in the context
of multilingual, low-resource NER, considering 11 languages from diverse
language families. Our results suggest that synthetic data does in fact hold
promise for low-resource language NER, though we see significant variation
between languages.

</details>


### [226] [Gender and Positional Biases in LLM-Based Hiring Decisions: Evidence from Comparative CV/Résumé Evaluations](https://arxiv.org/pdf/2505.17049)
*David Rozado*

Main category: cs.CL

TL;DR: LLMs show biases in candidate selection, favoring female-named applicants and displaying positional biases, raising concerns about their use in high-stakes decisions.


<details>
  <summary>Details</summary>
Motivation: To investigate biases in LLMs when evaluating professional candidates based on gendered and neutral identifiers in CVs.

Method: Experiment with 22 LLMs, presenting CV pairs with gendered or neutral names and analyzing selection preferences.

Result: LLMs consistently favored female-named candidates, showed positional bias, and slight pronoun effects. Gender-neutral identifiers reduced bias.

Conclusion: LLMs exhibit biases in decision-making, necessitating caution in autonomous high-stakes applications.

Abstract: This study examines the behavior of Large Language Models (LLMs) when
evaluating professional candidates based on their resumes or curricula vitae
(CVs). In an experiment involving 22 leading LLMs, each model was
systematically given one job description along with a pair of
profession-matched CVs, one bearing a male first name, the other a female first
name, and asked to select the more suitable candidate for the job. Each CV pair
was presented twice, with names swapped to ensure that any observed preferences
in candidate selection stemmed from gendered names cues. Despite identical
professional qualifications across genders, all LLMs consistently favored
female-named candidates across 70 different professions. Adding an explicit
gender field (male/female) to the CVs further increased the preference for
female applicants. When gendered names were replaced with gender-neutral
identifiers "Candidate A" and "Candidate B", several models displayed a
preference to select "Candidate A". Counterbalancing gender assignment between
these gender-neutral identifiers resulted in gender parity in candidate
selection. When asked to rate CVs in isolation rather than compare pairs, LLMs
assigned slightly higher average scores to female CVs overall, but the effect
size was negligible. Including preferred pronouns (he/him or she/her) next to a
candidate's name slightly increased the odds of the candidate being selected
regardless of gender. Finally, most models exhibited a substantial positional
bias to select the candidate listed first in the prompt. These findings
underscore the need for caution when deploying LLMs in high-stakes autonomous
decision-making contexts and raise doubts about whether LLMs consistently apply
principled reasoning.

</details>


### [227] [Power-Law Decay Loss for Large Language Model Finetuning: Focusing on Information Sparsity to Enhance Generation Quality](https://arxiv.org/pdf/2505.16900)
*Jintian Shao, Yiming Cheng, Hongyi Huang, Jiayi Wu, Beiwen Zhang, Zhiyu Wu, You Shan, Mingkai Zheng*

Main category: cs.CL

TL;DR: The paper introduces Power-Law Decay Loss (PDL), a novel loss function for text generation finetuning, addressing the overemphasis on high-frequency tokens by re-weighting token contributions based on frequency.


<details>
  <summary>Details</summary>
Motivation: Standard cross-entropy loss treats all tokens equally, causing models to neglect low-frequency, high-information tokens. PDL is motivated by the inverse relationship between token frequency and informativeness.

Method: PDL re-weights tokens in cross-entropy loss using a power-law decay, reducing weights for high-frequency tokens and increasing them for low-frequency ones.

Result: PDL improves text generation quality, diversity, and informativeness by focusing on specific, unique tokens.

Conclusion: PDL is theoretically justified and applicable to tasks like summarization, dialogue systems, and style transfer, enhancing generated text.

Abstract: During the finetuning stage of text generation tasks, standard cross-entropy
loss treats all tokens equally. This can lead models to overemphasize
high-frequency, low-information tokens, neglecting lower-frequency tokens
crucial for specificity and informativeness in generated content. This paper
introduces a novel loss function, Power-Law Decay Loss (PDL), specifically
designed to optimize the finetuning process for text generation. The core
motivation for PDL stems from observations in information theory and
linguistics: the informativeness of a token is often inversely proportional to
its frequency of occurrence. PDL re-weights the contribution of each token in
the standard cross-entropy loss based on its frequency in the training corpus,
following a power-law decay. Specifically, the weights for high-frequency
tokens are reduced, while low-frequency, information-dense tokens are assigned
higher weights. This mechanism guides the model during finetuning to focus more
on learning and generating tokens that convey specific and unique information,
thereby enhancing the quality, diversity, and informativeness of the generated
text. We theoretically elaborate on the motivation and construction of PDL and
discuss its potential applications and advantages across various text
generation finetuning tasks, such as abstractive summarization, dialogue
systems, and style transfer.

</details>


### [228] [From Tokens to Thoughts: How LLMs and Humans Trade Compression for Meaning](https://arxiv.org/pdf/2505.17117)
*Chen Shani, Dan Jurafsky, Yann LeCun, Ravid Shwartz-Ziv*

Main category: cs.CL

TL;DR: The paper compares human and LLM semantic compression strategies, finding LLMs favor aggressive compression over nuanced human-like distinctions.


<details>
  <summary>Details</summary>
Motivation: To understand if LLMs achieve human-like trade-offs between semantic fidelity and representational simplicity in knowledge organization.

Method: An information-theoretic framework (Rate-Distortion Theory and Information Bottleneck) is used to analyze LLM token embeddings against human categorization benchmarks.

Result: LLMs form broad categories aligned with humans but miss fine-grained distinctions. They prioritize statistical compression, unlike humans who favor contextual richness.

Conclusion: LLMs diverge from human cognitive strategies, highlighting pathways for more human-aligned AI conceptual representations.

Abstract: Humans organize knowledge into compact categories through semantic
compression by mapping diverse instances to abstract representations while
preserving meaning (e.g., robin and blue jay are both birds; most birds can
fly). These concepts reflect a trade-off between expressive fidelity and
representational simplicity. Large Language Models (LLMs) demonstrate
remarkable linguistic abilities, yet whether their internal representations
strike a human-like trade-off between compression and semantic fidelity is
unclear. We introduce a novel information-theoretic framework, drawing from
Rate-Distortion Theory and the Information Bottleneck principle, to
quantitatively compare these strategies. Analyzing token embeddings from a
diverse suite of LLMs against seminal human categorization benchmarks, we
uncover key divergences. While LLMs form broad conceptual categories that align
with human judgment, they struggle to capture the fine-grained semantic
distinctions crucial for human understanding. More fundamentally, LLMs
demonstrate a strong bias towards aggressive statistical compression, whereas
human conceptual systems appear to prioritize adaptive nuance and contextual
richness, even if this results in lower compressional efficiency by our
measures. These findings illuminate critical differences between current AI and
human cognitive architectures, guiding pathways toward LLMs with more
human-aligned conceptual representations.

</details>


### [229] [Select2Reason: Efficient Instruction-Tuning Data Selection for Long-CoT Reasoning](https://arxiv.org/pdf/2505.17266)
*Cehao Yang, Xueyuan Lin, Chengjin Xu, Xuhui Jiang, Xiaojun Wu, Honghao Liu, Hui Xiong, Jian Guo*

Main category: cs.CL

TL;DR: Select2Reason is a framework for efficiently selecting high-quality long-CoT reasoning instructions, reducing training overhead while maintaining performance.


<details>
  <summary>Details</summary>
Motivation: Large-scale instruction datasets for fine-tuning LLMs incur high costs, and effective automatic selection strategies for long-CoT reasoning are lacking.

Method: Select2Reason uses a quantifier to estimate question difficulty and a reasoning trace length-based heuristic to rank instructions, prioritizing high-utility examples.

Result: Fine-tuning on 10% of Select2Reason-selected data matches or outperforms full-data tuning and OpenR1-Qwen-7B on multiple benchmarks.

Conclusion: Select2Reason is scalable, efficient, and adaptable, offering a cost-effective solution for long-CoT reasoning in LLMs.

Abstract: A practical approach to activate long chain-of-thoughts reasoning ability in
pre-trained large language models is to perform supervised fine-tuning on
instruction datasets synthesized by strong Large Reasoning Models such as
DeepSeek-R1, offering a cost-effective alternative to reinforcement learning.
However, large-scale instruction sets with more than 100k samples incur
significant training overhead, while effective strategies for automatic
long-CoT instruction selection still remain unexplored. In this work, we
propose Select2Reason, a novel and efficient instruction-tuning data selection
framework for long-CoT reasoning. From the perspective of emergence of
rethinking behaviors like self-correction and backtracking, we investigate
common metrics that may determine the quality of long-CoT reasoning
instructions. Select2Reason leverages a quantifier to estimate difficulty of
question and jointly incorporates a reasoning trace length-based heuristic
through a weighted scheme for ranking to prioritize high-utility examples.
Empirical results on OpenR1-Math-220k demonstrate that fine-tuning LLM on only
10% of the data selected by Select2Reason achieves performance competitive with
or superior to full-data tuning and open-source baseline OpenR1-Qwen-7B across
three competition-level and six comprehensive mathematical benchmarks. Further
experiments highlight the scalability in varying data size, efficiency during
inference, and its adaptability to other instruction pools with minimal cost.

</details>


### [230] [QwenLong-L1: Towards Long-Context Large Reasoning Models with Reinforcement Learning](https://arxiv.org/pdf/2505.17667)
*Fanqi Wan, Weizhou Shen, Shengyi Liao, Yingcheng Shi, Chenliang Li, Ziyi Yang, Ji Zhang, Fei Huang, Jingren Zhou, Ming Yan*

Main category: cs.CL

TL;DR: QwenLong-L1 adapts short-context reasoning models to long-context tasks via progressive scaling, outperforming top models like OpenAI-o3-mini and matching Claude-3.7-Sonnet-Thinking.


<details>
  <summary>Details</summary>
Motivation: Extending large reasoning models (LRMs) to long-context inputs via RL is a critical unsolved challenge due to suboptimal training efficiency and unstable optimization.

Method: Proposes QwenLong-L1, using warm-up SFT, curriculum-guided RL, and difficulty-aware retrospective sampling to stabilize and enhance policy evolution.

Result: Outperforms flagship LRMs on seven benchmarks, achieving performance comparable to Claude-3.7-Sonnet-Thinking.

Conclusion: Advances practical long-context LRMs for robust reasoning in information-intensive environments.

Abstract: Recent large reasoning models (LRMs) have demonstrated strong reasoning
capabilities through reinforcement learning (RL). These improvements have
primarily been observed within the short-context reasoning tasks. In contrast,
extending LRMs to effectively process and reason on long-context inputs via RL
remains a critical unsolved challenge. To bridge this gap, we first formalize
the paradigm of long-context reasoning RL, and identify key challenges in
suboptimal training efficiency and unstable optimization process. To address
these issues, we propose QwenLong-L1, a framework that adapts short-context
LRMs to long-context scenarios via progressive context scaling. Specifically,
we utilize a warm-up supervised fine-tuning (SFT) stage to establish a robust
initial policy, followed by a curriculum-guided phased RL technique to
stabilize the policy evolution, and enhanced with a difficulty-aware
retrospective sampling strategy to incentivize the policy exploration.
Experiments on seven long-context document question-answering benchmarks
demonstrate that QwenLong-L1-32B outperforms flagship LRMs like OpenAI-o3-mini
and Qwen3-235B-A22B, achieving performance on par with
Claude-3.7-Sonnet-Thinking, demonstrating leading performance among
state-of-the-art LRMs. This work advances the development of practical
long-context LRMs capable of robust reasoning across information-intensive
environments.

</details>


### [231] [QwenLong-CPRS: Towards $\infty$-LLMs with Dynamic Context Optimization](https://arxiv.org/pdf/2505.18092)
*Weizhou Shen, Chenliang Li, Fanqi Wan, Shengyi Liao, Shaopeng Lai, Bo Zhang, Yingcheng Shi, Yuning Wu, Gang Fu, Zhansheng Li, Bin Yang, Ji Zhang, Fei Huang, Jingren Zhou, Ming Yan*

Main category: cs.CL

TL;DR: QwenLong-CPRS is a context compression framework for long-context optimization, improving efficiency and performance in LLMs through dynamic context optimization and multi-granularity compression.


<details>
  <summary>Details</summary>
Motivation: Addresses high computation overhead and performance degradation in LLMs during long sequence processing.

Method: Introduces natural language-guided dynamic optimization, bidirectional reasoning layers, token critic mechanisms, and window-parallel inference.

Result: Outperforms other methods in accuracy and efficiency, achieves 21.59× context compression, and integrates with flagship LLMs for significant performance gains.

Conclusion: QwenLong-CPRS sets new SOTA performance, surpassing proprietary LLMs on benchmarks like Ruler-128K and InfiniteBench.

Abstract: This technical report presents QwenLong-CPRS, a context compression framework
designed for explicit long-context optimization, addressing prohibitive
computation overhead during the prefill stage and the "lost in the middle"
performance degradation of large language models (LLMs) during long sequence
processing. Implemented through a novel dynamic context optimization mechanism,
QwenLong-CPRS enables multi-granularity context compression guided by natural
language instructions, achieving both efficiency gains and improved
performance.
  Evolved from the Qwen architecture series, QwenLong-CPRS introduces four key
innovations: (1) Natural language-guided dynamic optimization, (2)
Bidirectional reasoning layers for enhanced boundary awareness, (3) Token
critic mechanisms with language modeling heads, and (4) Window-parallel
inference.
  Comprehensive evaluations across five benchmarks (4K-2M word contexts)
demonstrate QwenLong-CPRS's threefold effectiveness: (1) Consistent superiority
over other context management methods like RAG and sparse attention in both
accuracy and efficiency. (2) Architecture-agnostic integration with all
flagship LLMs, including GPT-4o, Gemini2.0-pro, Claude3.7-sonnet, DeepSeek-v3,
and Qwen2.5-max, achieves 21.59$\times$ context compression alongside
19.15-point average performance gains; (3) Deployed with Qwen2.5-32B-Instruct,
QwenLong-CPRS surpasses leading proprietary LLMs by 4.85 and 10.88 points on
Ruler-128K and InfiniteBench, establishing new SOTA performance.

</details>


### [232] [RaDeR: Reasoning-aware Dense Retrieval Models](https://arxiv.org/pdf/2505.18405)
*Debrup Das, Sam O' Nuallain, Razieh Rahimi*

Main category: cs.CL

TL;DR: RaDeR is a reasoning-based dense retrieval model trained with LLM-derived data for mathematical problem solving, outperforming baselines and BM25 in reasoning tasks.


<details>
  <summary>Details</summary>
Motivation: To enhance reasoning-intensive relevance by leveraging LLM-generated data and self-reflective evaluation for diverse and hard-negative samples.

Method: Uses retrieval-augmented reasoning trajectories and self-reflective relevance evaluation to train dense retrievers for mathematical reasoning.

Result: Outperforms baselines in BRIGHT and RAR-b benchmarks, excels in Math and Coding splits, and surpasses BM25 for Chain-of-Thought queries.

Conclusion: RaDeR demonstrates the effectiveness of reasoning-based retrieval, achieving high performance with minimal training data compared to REASONIR.

Abstract: We propose RaDeR, a set of reasoning-based dense retrieval models trained
with data derived from mathematical problem solving using large language models
(LLMs). Our method leverages retrieval-augmented reasoning trajectories of an
LLM and self-reflective relevance evaluation, enabling the creation of both
diverse and hard-negative samples for reasoning-intensive relevance. RaDeR
retrievers, trained for mathematical reasoning, effectively generalize to
diverse reasoning tasks in the BRIGHT and RAR-b benchmarks, consistently
outperforming strong baselines in overall performance. Notably, RaDeR achieves
significantly higher performance than baselines on the Math and Coding splits.
In addition, RaDeR presents the first dense retriever that outperforms BM25
when queries are Chain-of-Thought reasoning steps, underscoring the critical
role of reasoning-based retrieval to augment reasoning language models.
Furthermore, RaDeR achieves comparable or superior performance while using only
2.5% of the training data used by the concurrent work REASONIR, highlighting
the quality of our synthesized training data.

</details>


### [233] [Debate-to-Detect: Reformulating Misinformation Detection as a Real-World Debate with Large Language Models](https://arxiv.org/pdf/2505.18596)
*Chen Han, Wenzhen Zheng, Xijin Tang*

Main category: cs.CL

TL;DR: D2D is a Multi-Agent Debate framework for misinformation detection, improving over static methods by using structured adversarial debates and multi-dimensional evaluation.


<details>
  <summary>Details</summary>
Motivation: Traditional misinformation detection methods are static and lack depth, while LLMs face issues like inconsistency. D2D aims to address these gaps.

Method: D2D uses a five-stage debate process with domain-specific agents and evaluates claims across five dimensions (Factuality, Source Reliability, etc.).

Result: Experiments show D2D outperforms baselines, refining evidence and improving transparency.

Conclusion: D2D advances robust, interpretable misinformation detection and will be open-sourced.

Abstract: The proliferation of misinformation in digital platforms reveals the
limitations of traditional detection methods, which mostly rely on static
classification and fail to capture the intricate process of real-world
fact-checking. Despite advancements in Large Language Models (LLMs) that
enhance automated reasoning, their application to misinformation detection
remains hindered by issues of logical inconsistency and superficial
verification. In response, we introduce Debate-to-Detect (D2D), a novel
Multi-Agent Debate (MAD) framework that reformulates misinformation detection
as a structured adversarial debate. Inspired by fact-checking workflows, D2D
assigns domain-specific profiles to each agent and orchestrates a five-stage
debate process, including Opening Statement, Rebuttal, Free Debate, Closing
Statement, and Judgment. To transcend traditional binary classification, D2D
introduces a multi-dimensional evaluation mechanism that assesses each claim
across five distinct dimensions: Factuality, Source Reliability, Reasoning
Quality, Clarity, and Ethics. Experiments with GPT-4o on two fakenews datasets
demonstrate significant improvements over baseline methods, and the case study
highlight D2D's capability to iteratively refine evidence while improving
decision transparency, representing a substantial advancement towards robust
and interpretable misinformation detection. The code will be open-sourced in a
future release.

</details>


### [234] [RASMALAI: Resources for Adaptive Speech Modeling in Indian Languages with Accents and Intonations](https://arxiv.org/pdf/2505.18609)
*Ashwin Sankar, Yoach Lacombe, Sherry Thomas, Praveen Srinivasa Varadhan, Sanchit Gandhi, Mitesh M Khapra*

Main category: cs.CL

TL;DR: RASMALAI is a large-scale speech dataset for 23 Indian languages and English, enabling controllable TTS synthesis. IndicParlerTTS, developed using it, excels in generating expressive, high-quality speech with fine-grained control.


<details>
  <summary>Details</summary>
Motivation: To advance controllable and expressive TTS synthesis for Indian languages, addressing the lack of large-scale datasets with rich text descriptions.

Method: Created RASMALAI, a dataset with 13,000 hours of speech and 24M text annotations. Developed IndicParlerTTS, an open-source TTS system guided by text descriptions.

Result: IndicParlerTTS generates high-quality speech, follows text descriptions accurately, and transfers expressive traits across languages.

Conclusion: IndicParlerTTS sets a new standard for multilingual expressive TTS in Indian languages, demonstrating strong performance in evaluations.

Abstract: We introduce RASMALAI, a large-scale speech dataset with rich text
descriptions, designed to advance controllable and expressive text-to-speech
(TTS) synthesis for 23 Indian languages and English. It comprises 13,000 hours
of speech and 24 million text-description annotations with fine-grained
attributes like speaker identity, accent, emotion, style, and background
conditions. Using RASMALAI, we develop IndicParlerTTS, the first open-source,
text-description-guided TTS for Indian languages. Systematic evaluation
demonstrates its ability to generate high-quality speech for named speakers,
reliably follow text descriptions and accurately synthesize specified
attributes. Additionally, it effectively transfers expressive characteristics
both within and across languages. IndicParlerTTS consistently achieves strong
performance across these evaluations, setting a new standard for controllable
multilingual expressive speech synthesis in Indian languages.

</details>


### [235] [Hierarchical Mamba Meets Hyperbolic Geometry: A New Paradigm for Structured Language Embeddings](https://arxiv.org/pdf/2505.18973)
*Sarang Patil, Ashish Parmanand Pandey, Ioannis Koutis, Mengjia Xu*

Main category: cs.CL

TL;DR: HiM integrates Mamba2 with hyperbolic geometry for hierarchy-aware language embeddings, outperforming Euclidean baselines in hierarchical tasks.


<details>
  <summary>Details</summary>
Motivation: Existing models struggle with hierarchical reasoning due to flat Euclidean embeddings. HiM aims to improve this by leveraging hyperbolic geometry.

Method: HiM combines Mamba2 with hyperbolic geometry (Poincare ball or Lorentzian manifold) and optimizes embeddings with a hyperbolic loss.

Result: HiM outperforms baselines in hierarchical tasks, with HiM-Poincare excelling in detail and HiM-Lorentz in robustness.

Conclusion: HiM effectively captures hierarchical relationships, offering a promising approach for complex language tasks.

Abstract: Selective state-space models have achieved great success in long-sequence
modeling. However, their capacity for language representation, especially in
complex hierarchical reasoning tasks, remains underexplored. Most large
language models rely on flat Euclidean embeddings, limiting their ability to
capture latent hierarchies. To address this limitation, we propose Hierarchical
Mamba (HiM), integrating efficient Mamba2 with exponential growth and curved
nature of hyperbolic geometry to learn hierarchy-aware language embeddings for
deeper linguistic understanding. Mamba2-processed sequences are projected to
the Poincare ball (via tangent-based mapping) or Lorentzian manifold (via
cosine and sine-based mapping) with "learnable" curvature, optimized with a
combined hyperbolic loss. Our HiM model facilitates the capture of relational
distances across varying hierarchical levels, enabling effective long-range
reasoning. This makes it well-suited for tasks like mixed-hop prediction and
multi-hop inference in hierarchical classification. We evaluated our HiM with
four linguistic and medical datasets for mixed-hop prediction and multi-hop
inference tasks. Experimental results demonstrated that: 1) Both HiM models
effectively capture hierarchical relationships for four ontological datasets,
surpassing Euclidean baselines. 2) HiM-Poincare captures fine-grained semantic
distinctions with higher h-norms, while HiM-Lorentz provides more stable,
compact, and hierarchy-preserving embeddings favoring robustness over detail.

</details>


### [236] [When Two LLMs Debate, Both Think They'll Win](https://arxiv.org/pdf/2505.19184)
*Pradyumna Shyama Prasad, Minh Nhat Nguyen*

Main category: cs.CL

TL;DR: LLMs show systematic overconfidence and poor belief updating in adversarial debates, raising concerns about their self-assessment accuracy in dynamic tasks.


<details>
  <summary>Details</summary>
Motivation: To evaluate LLMs' ability to adjust confidence dynamically in adversarial settings, beyond static fact-based tasks.

Method: Organized 60 three-round policy debates among ten LLMs, tracking private confidence ratings after each round.

Result: Observed overconfidence, confidence escalation, mutual overestimation, self-debate bias, and misaligned private reasoning.

Conclusion: LLMs struggle with accurate self-assessment in dynamic tasks, posing risks for deployment in unsupervised roles.

Abstract: Can LLMs accurately adjust their confidence when facing opposition? Building
on previous studies measuring calibration on static fact-based
question-answering tasks, we evaluate Large Language Models (LLMs) in a
dynamic, adversarial debate setting, uniquely combining two realistic factors:
(a) a multi-turn format requiring models to update beliefs as new information
emerges, and (b) a zero-sum structure to control for task-related uncertainty,
since mutual high-confidence claims imply systematic overconfidence. We
organized 60 three-round policy debates among ten state-of-the-art LLMs, with
models privately rating their confidence (0-100) in winning after each round.
We observed five concerning patterns: (1) Systematic overconfidence: models
began debates with average initial confidence of 72.9% vs. a rational 50%
baseline. (2) Confidence escalation: rather than reducing confidence as debates
progressed, debaters increased their win probabilities, averaging 83% by the
final round. (3) Mutual overestimation: in 61.7% of debates, both sides
simultaneously claimed >=75% probability of victory, a logical impossibility.
(4) Persistent self-debate bias: models debating identical copies increased
confidence from 64.1% to 75.2%; even when explicitly informed their chance of
winning was exactly 50%, confidence still rose (from 50.0% to 57.1%). (5)
Misaligned private reasoning: models' private scratchpad thoughts sometimes
differed from their public confidence ratings, raising concerns about
faithfulness of chain-of-thought reasoning. These results suggest LLMs lack the
ability to accurately self-assess or update their beliefs in dynamic,
multi-turn tasks; a major concern as LLM outputs are deployed without careful
review in assistant roles or agentic settings.

</details>


### [237] [A Graph Perspective to Probe Structural Patterns of Knowledge in Large Language Models](https://arxiv.org/pdf/2505.19286)
*Utkarsh Sahu, Zhisheng Qi, Yongjia Lei, Ryan A. Rossi, Franck Dernoncourt, Nesreen K. Ahmed, Mahantesh M Halappanavar, Yao Ma, Yu Wang*

Main category: cs.CL

TL;DR: The paper investigates structural patterns of knowledge in large language models (LLMs) from a graph perspective, analyzing triplet and entity-level knowledge and its relation to graph properties like node degree. It introduces knowledge homophily and a graph-based model to estimate entity knowledge, improving fine-tuning performance.


<details>
  <summary>Details</summary>
Motivation: Few works focus on the structural patterns of knowledge in LLMs, prompting an investigation into these patterns from a graph perspective.

Method: Quantify LLM knowledge at triplet and entity levels, analyze graph structural properties (e.g., node degree), uncover knowledge homophily, and develop a graph machine learning model to estimate entity knowledge.

Result: Empirical results show superior performance when fine-tuning LLMs with selected triplets identified by the model.

Conclusion: The study highlights the importance of structural knowledge patterns in LLMs and demonstrates the effectiveness of graph-based methods for knowledge estimation and fine-tuning.

Abstract: Large language models have been extensively studied as neural knowledge bases
for their knowledge access, editability, reasoning, and explainability.
However, few works focus on the structural patterns of their knowledge.
Motivated by this gap, we investigate these structural patterns from a graph
perspective. We quantify the knowledge of LLMs at both the triplet and entity
levels, and analyze how it relates to graph structural properties such as node
degree. Furthermore, we uncover the knowledge homophily, where topologically
close entities exhibit similar levels of knowledgeability, which further
motivates us to develop graph machine learning models to estimate entity
knowledge based on its local neighbors. This model further enables valuable
knowledge checking by selecting triplets less known to LLMs. Empirical results
show that using selected triplets for fine-tuning leads to superior
performance.

</details>


### [238] [CulFiT: A Fine-grained Cultural-aware LLM Training Paradigm via Multilingual Critique Data Synthesis](https://arxiv.org/pdf/2505.19484)
*Ruixiang Feng, Shen Gao, Xiuying Chen, Lisi Chen, Shuo Shang*

Main category: cs.CL

TL;DR: CulFiT is a culturally-aware training paradigm for LLMs, addressing biases by using multilingual data and fine-grained rewards, achieving top performance in cultural alignment.


<details>
  <summary>Details</summary>
Motivation: LLMs exhibit cultural biases, neglecting low-resource regions, which risks reinforcing stereotypes and discrimination.

Method: CulFiT leverages multilingual data, fine-grained reward modeling, and decomposes cultural texts into verifiable units. It also introduces GlobalCultureQA for evaluation.

Result: CulFiT achieves state-of-the-art performance in cultural alignment and general reasoning on benchmarks.

Conclusion: CulFiT effectively enhances cultural sensitivity and inclusivity in LLMs, addressing biases and improving global applicability.

Abstract: Large Language Models (LLMs) have demonstrated remarkable capabilities across
various tasks, yet they often exhibit a specific cultural biases, neglecting
the values and linguistic diversity of low-resource regions. This cultural bias
not only undermines universal equality, but also risks reinforcing stereotypes
and perpetuating discrimination. To address this, we propose CulFiT, a novel
culturally-aware training paradigm that leverages multilingual data and
fine-grained reward modeling to enhance cultural sensitivity and inclusivity.
Our approach synthesizes diverse cultural-related questions, constructs
critique data in culturally relevant languages, and employs fine-grained
rewards to decompose cultural texts into verifiable knowledge units for
interpretable evaluation. We also introduce GlobalCultureQA, a multilingual
open-ended question-answering dataset designed to evaluate culturally-aware
responses in a global context. Extensive experiments on three existing
benchmarks and our GlobalCultureQA demonstrate that CulFiT achieves
state-of-the-art open-source model performance in cultural alignment and
general reasoning.

</details>


### [239] [Analyzing Biases in Political Dialogue: Tagging U.S. Presidential Debates with an Extended DAMSL Framework](https://arxiv.org/pdf/2505.19515)
*Lavanya Prahallad, Radhika Mamidi*

Main category: cs.CL

TL;DR: The paper analyzes Trump's rhetorical strategies in 2024 U.S. presidential debates using BEADS, a novel annotation framework. It compares human and ChatGPT-assisted tagging, revealing Trump's dominance in adversarial and emotionally charged rhetoric.


<details>
  <summary>Details</summary>
Motivation: To systematically analyze bias-driven and adversarial discourse in political communication, focusing on Trump's debate strategies.

Method: Introduces BEADS, an extension of DAMSL, for tagging ideological framing, emotional appeals, and confrontational tactics. Compares human and ChatGPT-assisted annotations on debate transcripts.

Result: Trump dominated in adversarial exchanges, selective emphasis, fear appeals, political bias, and dismissiveness, highlighting his emotionally charged rhetoric.

Conclusion: BEADS is a scalable, reproducible framework for critical discourse analysis across languages and political contexts.

Abstract: We present a critical discourse analysis of the 2024 U.S. presidential
debates, examining Donald Trump's rhetorical strategies in his interactions
with Joe Biden and Kamala Harris. We introduce a novel annotation framework,
BEADS (Bias Enriched Annotation for Dialogue Structure), which systematically
extends the DAMSL framework to capture bias driven and adversarial discourse
features in political communication. BEADS includes a domain and language
agnostic set of tags that model ideological framing, emotional appeals, and
confrontational tactics. Our methodology compares detailed human annotation
with zero shot ChatGPT assisted tagging on verified transcripts from the Trump
and Biden (19,219 words) and Trump and Harris (18,123 words) debates. Our
analysis shows that Trump consistently dominated in key categories: Challenge
and Adversarial Exchanges, Selective Emphasis, Appeal to Fear, Political Bias,
and Perceived Dismissiveness. These findings underscore his use of emotionally
charged and adversarial rhetoric to control the narrative and influence
audience perception. In this work, we establish BEADS as a scalable and
reproducible framework for critical discourse analysis across languages,
domains, and political contexts.

</details>


### [240] [AmpleHate: Amplifying the Attention for Versatile Implicit Hate Detection](https://arxiv.org/pdf/2505.19528)
*Yejin Lee, Joonghyuk Hahn, Hyeseon Ahn, Yo-Sub Han*

Main category: cs.CL

TL;DR: AmpleHate is a novel approach for implicit hate speech detection, mimicking human reasoning by identifying targets and their context relations, outperforming contrastive learning methods.


<details>
  <summary>Details</summary>
Motivation: Current methods rely on contrastive learning, but humans detect implicit hate by analyzing targets and context. AmpleHate aims to replicate this process.

Method: AmpleHate uses NER for explicit targets, captures implicit targets via [CLS] tokens, and computes attention-based relationships to enhance detection.

Result: AmpleHate outperforms baselines by 82.14%, converges faster, and aligns with human judgment in attention patterns.

Conclusion: AmpleHate is effective, interpretable, and robust for implicit hate speech detection.

Abstract: Implicit hate speech detection is challenging due to its subtlety and
reliance on contextual interpretation rather than explicit offensive words.
Current approaches rely on contrastive learning, which are shown to be
effective on distinguishing hate and non-hate sentences. Humans, however,
detect implicit hate speech by first identifying specific targets within the
text and subsequently interpreting how these target relate to their surrounding
context. Motivated by this reasoning process, we propose AmpleHate, a novel
approach designed to mirror human inference for implicit hate detection.
AmpleHate identifies explicit target using a pretrained Named Entity
Recognition model and capture implicit target information via [CLS] tokens. It
computes attention-based relationships between explicit, implicit targets and
sentence context and then, directly injects these relational vectors into the
final sentence representation. This amplifies the critical signals of
target-context relations for determining implicit hate. Experiments demonstrate
that AmpleHate achieves state-of-the-art performance, outperforming contrastive
learning baselines by an average of 82.14% and achieve faster convergence.
Qualitative analyses further reveal that attention patterns produced by
AmpleHate closely align with human judgement, underscoring its interpretability
and robustness.

</details>


### [241] [TailorKV: A Hybrid Framework for Long-Context Inference via Tailored KV Cache Optimization](https://arxiv.org/pdf/2505.19586)
*Dingyu Yao, Bowen Shen, Zheng Lin, Wei Liu, Jian Luan, Bin Wang, Weiping Wang*

Main category: cs.CL

TL;DR: TailorKV is a hybrid compression method combining quantization and offloading to reduce KV cache memory overhead in LLMs, achieving near-lossless performance and efficient GPU usage.


<details>
  <summary>Details</summary>
Motivation: The KV cache in LLMs causes high memory overhead, and existing solutions (offloading or compression) lead to latency or performance degradation. TailorKV addresses this by leveraging selective loading and quantization.

Method: TailorKV integrates quantization and offloading, focusing on dominant tokens for selective loading and quantizing all tokens. It includes an inference framework and hardware-friendly implementation.

Result: TailorKV achieves near-lossless performance under aggressive compression, serving Llama-3.1-8B with 128k context on a single RTX 3090 GPU at 82 ms per token.

Conclusion: TailorKV outperforms state-of-the-art methods by efficiently balancing compression and performance, making it practical for large-context LLM deployment.

Abstract: The Key-Value (KV) cache in generative large language models (LLMs)
introduces substantial memory overhead. Existing works mitigate this burden by
offloading or compressing the KV cache. However, loading the entire cache
incurs significant latency due to PCIe bandwidth bottlenecks in CPU-GPU
communication, while aggressive compression causes notable performance
degradation. We identify that certain layers in the LLM need to maintain global
information and are unsuitable for selective loading. In contrast, other layers
primarily focus on a few tokens with dominant activations that potentially
incur substantial quantization error. This observation leads to a key insight
that loading dominant tokens and quantizing all tokens can complement each
other. Building on this insight, we propose a hybrid compression method,
TailorKV, which seamlessly integrates quantization and offloading. TailorKV
develops an inference framework along with a hardware-friendly implementation
that leverages these complementary characteristics. Extensive long-context
evaluations exhibit that TailorKV achieves nearly lossless performance under
aggressive compression settings, outperforming the state-of-the-art.
Particularly, the Llama-3.1-8B with 128k context can be served within a single
RTX 3090 GPU, reaching 82 ms per token during decoding.

</details>


### [242] [HomeBench: Evaluating LLMs in Smart Homes with Valid and Invalid Instructions Across Single and Multiple Devices](https://arxiv.org/pdf/2505.19628)
*Silin Li, Yuhang Guo, Jiashu Yao, Zeming Liu, Haifeng Wang*

Main category: cs.CL

TL;DR: The paper introduces HomeBench, a dataset for testing LLMs in smart home scenarios involving valid and invalid single/multi-device instructions, revealing current LLMs' limitations.


<details>
  <summary>Details</summary>
Motivation: To address the gap in handling complex real-world smart home scenarios, like invalid or multi-device instructions, which existing LLM integrations overlook.

Method: Developed HomeBench, a dataset with diverse instructions, and tested 13 LLMs (e.g., GPT-4o) using methods like in-context learning and fine-tuning.

Result: GPT-4o scored 0.0% success in invalid multi-device scenarios, showing LLMs' current inadequacy despite advanced techniques.

Conclusion: LLMs still struggle with complex smart home tasks, highlighting the need for further research and improved models.

Abstract: Large language models (LLMs) have the potential to revolutionize smart home
assistants by enhancing their ability to accurately understand user needs and
respond appropriately, which is extremely beneficial for building a smarter
home environment. While recent studies have explored integrating LLMs into
smart home systems, they primarily focus on handling straightforward, valid
single-device operation instructions. However, real-world scenarios are far
more complex and often involve users issuing invalid instructions or
controlling multiple devices simultaneously. These have two main challenges:
LLMs must accurately identify and rectify errors in user instructions and
execute multiple user instructions perfectly. To address these challenges and
advance the development of LLM-based smart home assistants, we introduce
HomeBench, the first smart home dataset with valid and invalid instructions
across single and multiple devices in this paper. We have experimental results
on 13 distinct LLMs; e.g., GPT-4o achieves only a 0.0% success rate in the
scenario of invalid multi-device instructions, revealing that the existing
state-of-the-art LLMs still cannot perform well in this situation even with the
help of in-context learning, retrieval-augmented generation, and fine-tuning.
Our code and dataset are publicly available at
https://github.com/BITHLP/HomeBench.

</details>


### [243] [Faster and Better LLMs via Latency-Aware Test-Time Scaling](https://arxiv.org/pdf/2505.19634)
*Zili Wang, Tianyu Zhang, Haoli Bai, Lu Hou, Xianzhi Yu, Wulong Liu, Shiming Xiang, Lei Zhu*

Main category: cs.CL

TL;DR: Test-Time Scaling (TTS) improves LLM performance but lacks latency efficiency. The paper proposes latency-optimal TTS via branch-wise and sequence-wise parallelism, achieving high accuracy within tight time constraints.


<details>
  <summary>Details</summary>
Motivation: Existing TTS methods overlook latency efficiency, which is critical in real-world scenarios. The study aims to bridge this gap by optimizing concurrency configurations.

Method: Two approaches are introduced: (1) branch-wise parallelism for concurrent inference branches and (2) sequence-wise parallelism via speculative decoding. Computational resources are allocated optimally.

Result: A 32B model achieves 82.3% accuracy on MATH-500 in 1 minute, and a 3B model reaches 72.4% in 10 seconds.

Conclusion: Latency-aware TTS is crucial for balancing speed and accuracy in latency-sensitive applications, as demonstrated by the proposed methods.

Abstract: Test-Time Scaling (TTS) has proven effective in improving the performance of
Large Language Models (LLMs) during inference. However, existing research has
overlooked the efficiency of TTS from a latency-sensitive perspective. Through
a latency-aware evaluation of representative TTS methods, we demonstrate that a
compute-optimal TTS does not always result in the lowest latency in scenarios
where latency is critical. To address this gap and achieve latency-optimal TTS,
we propose two key approaches by optimizing the concurrency configurations: (1)
branch-wise parallelism, which leverages multiple concurrent inference
branches, and (2) sequence-wise parallelism, enabled by speculative decoding.
By integrating these two approaches and allocating computational resources
properly to each, our latency-optimal TTS enables a 32B model to reach 82.3%
accuracy on MATH-500 within 1 minute and a smaller 3B model to achieve 72.4%
within 10 seconds. Our work emphasizes the importance of latency-aware TTS and
demonstrates its ability to deliver both speed and accuracy in
latency-sensitive scenarios.

</details>


### [244] [Token-level Accept or Reject: A Micro Alignment Approach for Large Language Models](https://arxiv.org/pdf/2505.19743)
*Yang Zhang, Yu Yu, Bo Tang, Yu Zhu, Chuxiong Sun, Wenqiang Wei, Jie Hu, Zipeng Xie, Zhiyu Li, Feiyu Xiong, Edward Chung*

Main category: cs.CL

TL;DR: Proposes Micro token-level Accept-Reject Aligning (MARA) for efficient alignment of LLMs with human preferences, reducing computational costs.


<details>
  <summary>Details</summary>
Motivation: Existing alignment techniques like RLHF or DPO are computationally expensive for large LLMs.

Method: MARA decomposes sentence-level preference learning into token-level binary classification using a compact three-layer network.

Result: Achieves significant alignment improvements across seven LLMs and three datasets while lowering computational costs.

Conclusion: MARA offers an efficient alternative to traditional alignment methods, with publicly available code and models.

Abstract: With the rapid development of Large Language Models (LLMs), aligning these
models with human preferences and values is critical to ensuring ethical and
safe applications. However, existing alignment techniques such as RLHF or DPO
often require direct fine-tuning on LLMs with billions of parameters, resulting
in substantial computational costs and inefficiencies. To address this, we
propose Micro token-level Accept-Reject Aligning (MARA) approach designed to
operate independently of the language models. MARA simplifies the alignment
process by decomposing sentence-level preference learning into token-level
binary classification, where a compact three-layer fully-connected network
determines whether candidate tokens are "Accepted" or "Rejected" as part of the
response. Extensive experiments across seven different LLMs and three
open-source datasets show that MARA achieves significant improvements in
alignment performance while reducing computational costs. The source code and
implementation details are publicly available at
https://github.com/IAAR-Shanghai/MARA, and the trained models are released at
https://huggingface.co/IAAR-Shanghai/MARA_AGENTS.

</details>


### [245] [Adaptive Deep Reasoning: Triggering Deep Thinking When Needed](https://arxiv.org/pdf/2505.20101)
*Yunhao Wang, Yuhao Zhang, Tinghao Yu, Can Xu, Feng Zhang, Fengzong Lian*

Main category: cs.CL

TL;DR: A novel method autonomously switches between short and long reasoning chains in LLMs, optimizing efficiency without sacrificing performance.


<details>
  <summary>Details</summary>
Motivation: To address the computational inefficiency of long-chain reasoning in LLMs while maintaining accuracy.

Method: Supervised fine-tuning for dual reasoning abilities, followed by reinforcement learning with adaptive rewards and logit-based switching loss.

Result: The model dynamically adapts reasoning length based on problem complexity, maintaining performance on mathematical datasets.

Conclusion: This approach enhances the practicality of LLMs for real-world applications by balancing efficiency and accuracy.

Abstract: Large language models (LLMs) have shown impressive capabilities in handling
complex tasks through long-chain reasoning. However, the extensive reasoning
steps involved can significantly increase computational costs, posing
challenges for real-world deployment. Recent efforts have focused on optimizing
reasoning efficiency by shortening the Chain-of-Thought (CoT) reasoning
processes through various approaches, such as length-aware prompt engineering,
supervised fine-tuning on CoT data with variable lengths, and reinforcement
learning with length penalties. Although these methods effectively reduce
reasoning length, they still necessitate an initial reasoning phase. More
recent approaches have attempted to integrate long-chain and short-chain
reasoning abilities into a single model, yet they still rely on manual control
to toggle between short and long CoT. In this work, we propose a novel approach
that autonomously switches between short and long reasoning chains based on
problem complexity. Our method begins with supervised fine-tuning of the base
model to equip both long-chain and short-chain reasoning abilities. We then
employ reinforcement learning to further balance short and long CoT generation
while maintaining accuracy through two key strategies: first, integrating
reinforcement learning with a long-short adaptive group-wise reward strategy to
assess prompt complexity and provide corresponding rewards; second,
implementing a logit-based reasoning mode switching loss to optimize the
model's initial token choice, thereby guiding the selection of the reasoning
type. Evaluations on mathematical datasets demonstrate that our model can
dynamically switch between long-chain and short-chain reasoning modes without
substantially sacrificing performance. This advancement enhances the
practicality of reasoning in large language models for real-world applications.

</details>


### [246] [Does quantization affect models' performance on long-context tasks?](https://arxiv.org/pdf/2505.20276)
*Anmol Mekala, Anirudh Atmakuru, Yixiao Song, Marzena Karpinska, Mohit Iyyer*

Main category: cs.CL

TL;DR: Quantization of LLMs for long-input tasks shows 8-bit preserves accuracy (~0.8% drop), while 4-bit causes significant losses (up to 59%), especially in non-English contexts. Performance varies by method, model, and task.


<details>
  <summary>Details</summary>
Motivation: To address the high memory and latency costs of large context windows in LLMs, this work evaluates quantization's impact on performance in long-input and long-output tasks.

Method: Systematic evaluation of 9.7K test examples across five quantization methods (FP8, GPTQ-int8, AWQ-int4, GPTQ-int4, BNB-nf4) and five models (Llama-3.1 8B/70B; Qwen-2.5 7B/32B/72B).

Result: 8-bit quantization preserves accuracy (~0.8% drop), while 4-bit methods degrade performance (up to 59% drop), especially for non-English inputs. Effects vary by method, model, and task.

Conclusion: Task-specific evaluation is crucial before deploying quantized LLMs, particularly for long-context scenarios and non-English languages.

Abstract: Large language models (LLMs) now support context windows exceeding 128K
tokens, but this comes with significant memory requirements and high inference
latency. Quantization can mitigate these costs, but may degrade performance. In
this work, we present the first systematic evaluation of quantized LLMs on
tasks with long-inputs (>64K tokens) and long-form outputs. Our evaluation
spans 9.7K test examples, five quantization methods (FP8, GPTQ-int8, AWQ-int4,
GPTQ-int4, BNB-nf4), and five models (Llama-3.1 8B and 70B; Qwen-2.5 7B, 32B,
and 72B). We find that, on average, 8-bit quantization preserves accuracy
(~0.8% drop), whereas 4-bit methods lead to substantial losses, especially for
tasks involving long context inputs (drops of up to 59%). This degradation
tends to worsen when the input is in a language other than English. Crucially,
the effects of quantization depend heavily on the quantization method, model,
and task. For instance, while Qwen-2.5 72B remains robust under BNB-nf4,
Llama-3.1 70B experiences a 32% performance drop on the same task. These
findings highlight the importance of a careful, task-specific evaluation before
deploying quantized LLMs, particularly in long-context scenarios and with
languages other than English.

</details>


### [247] [TrojanStego: Your Language Model Can Secretly Be A Steganographic Privacy Leaking Agent](https://arxiv.org/pdf/2505.20118)
*Dominik Meier, Jan Philip Wahle, Paul Röttger, Terry Ruas, Bela Gipp*

Main category: cs.CL

TL;DR: TrojanStego is a threat model where LLMs are fine-tuned to embed sensitive data into natural outputs via steganography, posing a covert and practical data exfiltration risk.


<details>
  <summary>Details</summary>
Motivation: Addressing concerns about LLMs leaking confidential information by exploring a passive and covert attack vector.

Method: Proposes a vocabulary partitioning-based encoding scheme for LLMs to embed secrets into outputs without explicit input control.

Result: Compromised models achieve 87% accuracy in transmitting 32-bit secrets, with 97% accuracy using majority voting, while maintaining utility and evading detection.

Conclusion: TrojanStego introduces a dangerous new class of passive, covert LLM attacks, highlighting the need for defenses against such threats.

Abstract: As large language models (LLMs) become integrated into sensitive workflows,
concerns grow over their potential to leak confidential information. We propose
TrojanStego, a novel threat model in which an adversary fine-tunes an LLM to
embed sensitive context information into natural-looking outputs via linguistic
steganography, without requiring explicit control over inference inputs. We
introduce a taxonomy outlining risk factors for compromised LLMs, and use it to
evaluate the risk profile of the threat. To implement TrojanStego, we propose a
practical encoding scheme based on vocabulary partitioning learnable by LLMs
via fine-tuning. Experimental results show that compromised models reliably
transmit 32-bit secrets with 87% accuracy on held-out prompts, reaching over
97% accuracy using majority voting across three generations. Further, they
maintain high utility, can evade human detection, and preserve coherence. These
results highlight a new class of LLM data exfiltration attacks that are
passive, covert, practical, and dangerous.

</details>


### [248] [Monocle: Hybrid Local-Global In-Context Evaluation for Long-Text Generation with Uncertainty-Based Active Learning](https://arxiv.org/pdf/2505.20195)
*Xiaorong Wang, Ting Yang, Zhu Zhang, Shuo Wang, Zihan Zhou, Liner Yang, Zhiyuan Liu, Maosong Sun*

Main category: cs.CL

TL;DR: A divide-and-conquer approach for evaluating long-form model-generated text, combining localized scoring with global assessment and human feedback, outperforms baselines.


<details>
  <summary>Details</summary>
Motivation: Challenges in assessing long-form text quality due to performance degradation with increasing input length.

Method: Divide-and-conquer strategy with localized scoring, global assessment, hybrid in-context learning, and uncertainty-based active learning for human annotation.

Result: The framework outperforms baselines, showing effectiveness in granular and overall evaluations.

Conclusion: The proposed approach improves long-form text evaluation by combining localized and global assessments with human feedback.

Abstract: Assessing the quality of long-form, model-generated text is challenging, even
with advanced LLM-as-a-Judge methods, due to performance degradation as input
length increases. To address this issue, we propose a divide-and-conquer
approach, which breaks down the comprehensive evaluation task into a series of
localized scoring tasks, followed by a final global assessment. This strategy
allows for more granular and manageable evaluations, ensuring that each segment
of the text is assessed in isolation for both coherence and quality, while also
accounting for the overall structure and consistency of the entire piece.
Moreover, we introduce a hybrid in-context learning approach that leverages
human annotations to enhance the performance of both local and global
evaluations. By incorporating human-generated feedback directly into the
evaluation process, this method allows the model to better align with human
judgment. Finally, we develop an uncertainty-based active learning algorithm
that efficiently selects data samples for human annotation, thereby reducing
annotation costs in practical scenarios. Experimental results show that the
proposed evaluation framework outperforms several representative baselines,
highlighting the effectiveness of our approach.

</details>


### [249] [One-shot Entropy Minimization](https://arxiv.org/pdf/2505.20282)
*Zitian Gao, Lynx Chen, Joey Zhou, Bryan Dai*

Main category: cs.CL

TL;DR: Entropy minimization with just one unlabeled data and 10 optimization steps matches or outperforms rule-based RL with thousands of data, suggesting a rethink of post-training methods for LLMs.


<details>
  <summary>Details</summary>
Motivation: To explore whether entropy minimization, a simpler approach, can achieve performance improvements comparable to complex rule-based reinforcement learning methods.

Method: Trained 13,440 large language models, comparing entropy minimization (using one unlabeled data and 10 optimization steps) to rule-based RL with extensive data and rewards.

Result: Entropy minimization achieved comparable or better performance than rule-based RL, despite requiring far fewer resources.

Conclusion: This challenges current post-training paradigms, advocating for simpler, more efficient methods like entropy minimization.

Abstract: We trained 13,440 large language models and found that entropy minimization
requires only a single unlabeled data and 10 steps optimization to achieve
performance improvements comparable to or even greater than those obtained
using thousands of data and carefully designed rewards in rule-based
reinforcement learning. This striking result may prompt a rethinking of
post-training paradigms for large language models. Our code is avaliable at
https://github.com/zitian-gao/one-shot-em.

</details>


### [250] [MaskSearch: A Universal Pre-Training Framework to Enhance Agentic Search Capability](https://arxiv.org/pdf/2505.20285)
*Weiqi Wu, Xin Guan, Shen Huang, Yong Jiang, Pengjun Xie, Fei Huang, Jiuxin Cao, Hai Zhao, Jingren Zhou*

Main category: cs.CL

TL;DR: MaskSearch is a pre-training framework for LLMs that enhances universal search capabilities by combining retrieval-augmented mask prediction (RAMP) with downstream fine-tuning (SFT and RL).


<details>
  <summary>Details</summary>
Motivation: Existing training-based methods for LLMs limit agentic abilities due to task-specific data constraints. MaskSearch aims to improve universal retrieval and reasoning.

Method: Proposes RAMP for pre-training, followed by SFT (multi-agent system and distillation) and RL (DAPO with hybrid rewards). Uses curriculum learning for progressive training.

Result: MaskSearch significantly boosts LLM-based search agents' performance in open-domain multi-hop QA, including in-domain and out-of-domain tasks.

Conclusion: MaskSearch effectively enhances LLMs' universal search and reasoning capabilities, outperforming existing methods.

Abstract: Retrieval-Augmented Language Models (RALMs) represent a classic paradigm
where models enhance generative capabilities using external knowledge retrieved
via a specialized module. Recent advancements in Agent techniques enable Large
Language Models (LLMs) to autonomously utilize tools for retrieval, planning,
and reasoning. While existing training-based methods show promise, their
agentic abilities are limited by inherent characteristics of the task-specific
data used during training. To further enhance the universal search capability
of agents, we propose a novel pre-training framework, MaskSearch. In the
pre-training stage, we introduce the Retrieval Augmented Mask Prediction (RAMP)
task, where the model learns to leverage search tools to fill masked spans on a
large number of pre-training data, thus acquiring universal retrieval and
reasoning capabilities for LLMs. After that, the model is trained on downstream
tasks to achieve further improvement. We apply both Supervised Fine-tuning
(SFT) and Reinforcement Learning (RL) for training. For SFT, we combine
agent-based and distillation-based methods to generate training data, starting
with a multi-agent system consisting of a planner, rewriter, observer, and
followed by a self-evolving teacher model. While for RL, we employ DAPO as the
training framework and adopt a hybrid reward system consisting of answer
rewards and format rewards. Additionally, we introduce a curriculum learning
approach that allows the model to learn progressively from easier to more
challenging instances based on the number of masked spans. We evaluate the
effectiveness of our framework in the scenario of open-domain multi-hop
question answering. Through extensive experiments, we demonstrate that
MaskSearch significantly enhances the performance of LLM-based search agents on
both in-domain and out-of-domain downstream tasks.

</details>


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [251] [What Changed? Detecting and Evaluating Instruction-Guided Image Edits with Multimodal Large Language Models](https://arxiv.org/pdf/2505.20405)
*Lorenzo Baraldi, Davide Bucciarelli, Federico Betti, Marcella Cornia, Lorenzo Baraldi, Nicu Sebe, Rita Cucchiara*

Main category: cs.CV

TL;DR: DICE is a model for evaluating instruction-based image edits by detecting localized differences and assessing their relevance, outperforming existing metrics in alignment with human judgment.


<details>
  <summary>Details</summary>
Motivation: Existing metrics for evaluating instruction-based image editing lack alignment with human judgment and explainability.

Method: DICE combines a difference detector and coherence estimator built on an autoregressive MLLM, trained with self-supervision, distillation, and full supervision.

Result: DICE effectively identifies coherent edits and correlates strongly with human judgment.

Conclusion: DICE provides a robust solution for evaluating image edits, with publicly released resources.

Abstract: Instruction-based image editing models offer increased personalization
opportunities in generative tasks. However, properly evaluating their results
is challenging, and most of the existing metrics lag in terms of alignment with
human judgment and explainability. To tackle these issues, we introduce DICE
(DIfference Coherence Estimator), a model designed to detect localized
differences between the original and the edited image and to assess their
relevance to the given modification request. DICE consists of two key
components: a difference detector and a coherence estimator, both built on an
autoregressive Multimodal Large Language Model (MLLM) and trained using a
strategy that leverages self-supervision, distillation from inpainting
networks, and full supervision. Through extensive experiments, we evaluate each
stage of our pipeline, comparing different MLLMs within the proposed framework.
We demonstrate that DICE effectively identifies coherent edits, effectively
evaluating images generated by different editing models with a strong
correlation with human judgment. We publicly release our source code, models,
and data.

</details>


### [252] [ReaMOT: A Benchmark and Framework for Reasoning-based Multi-Object Tracking](https://arxiv.org/pdf/2505.20381)
*Sijia Chen, Yanqiu Yu, En Yu, Wenbing Tao*

Main category: cs.CV

TL;DR: The paper introduces ReaMOT, a challenging task for multi-object tracking requiring reasoning with language instructions, and proposes ReaTrack, a baseline framework using LVLM and SAM2.


<details>
  <summary>Details</summary>
Motivation: Existing RMOT tasks fail with complex language instructions, prompting the need for a reasoning-based approach.

Method: Proposes ReaMOT, a benchmark with 1,156 reasoning-based instructions, and ReaTrack, a training-free framework using LVLM and SAM2.

Result: ReaTrack shows effectiveness on the ReaMOT Challenge benchmark.

Conclusion: ReaMOT advances tracking by incorporating reasoning, with ReaTrack serving as a strong baseline.

Abstract: Referring Multi-object tracking (RMOT) is an important research field in
computer vision. Its task form is to guide the models to track the objects that
conform to the language instruction. However, the RMOT task commonly requires
clear language instructions, such methods often fail to work when complex
language instructions with reasoning characteristics appear. In this work, we
propose a new task, called Reasoning-based Multi-Object Tracking (ReaMOT).
ReaMOT is a more challenging task that requires accurate reasoning about
objects that match the language instruction with reasoning characteristic and
tracking the objects' trajectories. To advance the ReaMOT task and evaluate the
reasoning capabilities of tracking models, we construct ReaMOT Challenge, a
reasoning-based multi-object tracking benchmark built upon 12 datasets.
Specifically, it comprises 1,156 language instructions with reasoning
characteristic, 423,359 image-language pairs, and 869 diverse scenes, which is
divided into three levels of reasoning difficulty. In addition, we propose a
set of evaluation metrics tailored for the ReaMOT task. Furthermore, we propose
ReaTrack, a training-free framework for reasoning-based multi-object tracking
based on large vision-language models (LVLM) and SAM2, as a baseline for the
ReaMOT task. Extensive experiments on the ReaMOT Challenge benchmark
demonstrate the effectiveness of our ReaTrack framework.

</details>


### [253] [RetroMotion: Retrocausal Motion Forecasting Models are Instructable](https://arxiv.org/pdf/2505.20414)
*Royden Wagner, Omer Sahin Tas, Felix Hauser, Marlon Steiner, Dominik Strutz, Abhishek Vivekanandan, Carlos Fernandez, Christoph Stiller*

Main category: cs.CV

TL;DR: A multi-task learning method for motion forecasting uses retrocausal information flow to predict marginal and joint trajectory distributions, achieving state-of-the-art results and enabling goal-based instructions.


<details>
  <summary>Details</summary>
Motivation: Addressing the complexity of motion forecasts due to scene constraints and interactive behavior by integrating retrocausal information flow.

Method: Uses a transformer model to generate joint distributions by re-encoding marginal distributions and pairwise modeling, with positional uncertainty modeled via compressed exponential power distributions.

Result: Achieves state-of-the-art results on Waymo Interaction Prediction and generalizes well to Argoverse 2, with added capability for goal-based instructions.

Conclusion: The method effectively handles motion forecasting complexity and provides a flexible interface for trajectory modifications and instructions.

Abstract: Motion forecasts of road users (i.e., agents) vary in complexity as a
function of scene constraints and interactive behavior. We address this with a
multi-task learning method for motion forecasting that includes a retrocausal
flow of information. The corresponding tasks are to forecast (1) marginal
trajectory distributions for all modeled agents and (2) joint trajectory
distributions for interacting agents. Using a transformer model, we generate
the joint distributions by re-encoding marginal distributions followed by
pairwise modeling. This incorporates a retrocausal flow of information from
later points in marginal trajectories to earlier points in joint trajectories.
Per trajectory point, we model positional uncertainty using compressed
exponential power distributions. Notably, our method achieves state-of-the-art
results in the Waymo Interaction Prediction dataset and generalizes well to the
Argoverse 2 dataset. Additionally, our method provides an interface for issuing
instructions through trajectory modifications. Our experiments show that
regular training of motion forecasting leads to the ability to follow
goal-based instructions and to adapt basic directional instructions to the
scene context. Code: https://github.com/kit-mrt/future-motion

</details>


### [254] [MMPerspective: Do MLLMs Understand Perspective? A Comprehensive Benchmark for Perspective Perception, Reasoning, and Robustness](https://arxiv.org/pdf/2505.20426)
*Yunlong Tang, Pinxin Liu, Mingqian Feng, Zhangyun Tan, Rui Mao, Chao Huang, Jing Bi, Yunzhong Xiao, Susan Liang, Hang Hua, Ali Vosoughi, Luchuan Song, Zeliang Zhang, Chenliang Xu*

Main category: cs.CV

TL;DR: MMPerspective is a benchmark evaluating MLLMs' understanding of perspective through 10 tasks, revealing limitations in compositional reasoning and spatial consistency.


<details>
  <summary>Details</summary>
Motivation: To assess how well multimodal large language models (MLLMs) internalize perspective geometry, a fundamental aspect of human visual perception.

Method: MMPerspective includes 2,711 images and 5,083 QA pairs across three dimensions: Perspective Perception, Reasoning, and Robustness. It evaluates 43 MLLMs on tasks like vanishing point perception and perspective type reasoning.

Result: MLLMs perform well on surface-level tasks but struggle with compositional reasoning and spatial consistency under perturbations. Model architecture and scale influence performance, with chain-of-thought prompting showing benefits.

Conclusion: MMPerspective provides a testbed for diagnosing and improving spatial understanding in vision-language systems, highlighting current limitations and potential improvements.

Abstract: Understanding perspective is fundamental to human visual perception, yet the
extent to which multimodal large language models (MLLMs) internalize
perspective geometry remains unclear. We introduce MMPerspective, the first
benchmark specifically designed to systematically evaluate MLLMs' understanding
of perspective through 10 carefully crafted tasks across three complementary
dimensions: Perspective Perception, Reasoning, and Robustness. Our benchmark
comprises 2,711 real-world and synthetic image instances with 5,083
question-answer pairs that probe key capabilities, such as vanishing point
perception and counting, perspective type reasoning, line relationship
understanding in 3D space, invariance to perspective-preserving
transformations, etc. Through a comprehensive evaluation of 43 state-of-the-art
MLLMs, we uncover significant limitations: while models demonstrate competence
on surface-level perceptual tasks, they struggle with compositional reasoning
and maintaining spatial consistency under perturbations. Our analysis further
reveals intriguing patterns between model architecture, scale, and perspective
capabilities, highlighting both robustness bottlenecks and the benefits of
chain-of-thought prompting. MMPerspective establishes a valuable testbed for
diagnosing and advancing spatial understanding in vision-language systems.
Resources available at: https://yunlong10.github.io/MMPerspective/

</details>


### [255] [DIPO: Dual-State Images Controlled Articulated Object Generation Powered by Diverse Data](https://arxiv.org/pdf/2505.20460)
*Ruqi Wu, Xinjie Wang, Liu Liu, Chunle Guo, Jiaxiong Qiu, Chongyi Li, Lichao Huang, Zhizhong Su, Ming-Ming Cheng*

Main category: cs.CV

TL;DR: DIPO is a framework for generating articulated 3D objects from dual-image inputs, outperforming baselines with improved motion prediction and dataset diversity.


<details>
  <summary>Details</summary>
Motivation: Existing single-image approaches lack motion information, limiting kinematic relationship prediction. Dual-image inputs provide reliable motion guidance.

Method: Uses a dual-image diffusion model for part layouts/joint parameters and a CoT-based graph reasoner for part connectivity. Introduces LEGO-Art for dataset expansion and PM-X for diversity.

Result: DIPO outperforms baselines in generating articulated objects. PM-X enhances generalization to complex structures.

Conclusion: DIPO and PM-X advance controllable 3D object generation, with plans to release code and dataset.

Abstract: We present DIPO, a novel framework for the controllable generation of
articulated 3D objects from a pair of images: one depicting the object in a
resting state and the other in an articulated state. Compared to the
single-image approach, our dual-image input imposes only a modest overhead for
data collection, but at the same time provides important motion information,
which is a reliable guide for predicting kinematic relationships between parts.
Specifically, we propose a dual-image diffusion model that captures
relationships between the image pair to generate part layouts and joint
parameters. In addition, we introduce a Chain-of-Thought (CoT) based graph
reasoner that explicitly infers part connectivity relationships. To further
improve robustness and generalization on complex articulated objects, we
develop a fully automated dataset expansion pipeline, name LEGO-Art, that
enriches the diversity and complexity of PartNet-Mobility dataset. We propose
PM-X, a large-scale dataset of complex articulated 3D objects, accompanied by
rendered images, URDF annotations, and textual descriptions. Extensive
experiments demonstrate that DIPO significantly outperforms existing baselines
in both the resting state and the articulated state, while the proposed PM-X
dataset further enhances generalization to diverse and structurally complex
articulated objects. Our code and dataset will be released to the community
upon publication.

</details>


### [256] [CCL-LGS: Contrastive Codebook Learning for 3D Language Gaussian Splatting](https://arxiv.org/pdf/2505.20469)
*Lei Tian, Xiaomin Li, Liqian Ma, Hefei Huang, Zirui Zheng, Hao Yin, Taiqing Li, Huchuan Lu, Xu Jia*

Main category: cs.CV

TL;DR: CCL-LGS is a novel framework addressing cross-view semantic inconsistencies in 3D semantic understanding by integrating multi-view semantic cues, outperforming prior methods.


<details>
  <summary>Details</summary>
Motivation: Cross-view semantic inconsistencies in 3D reconstruction degrade quality and introduce artifacts, necessitating a robust solution.

Method: Uses a zero-shot tracker for mask alignment, CLIP for semantic encodings, and a Contrastive Codebook Learning (CCL) module for feature distillation.

Result: CCL-LGS outperforms state-of-the-art methods in resolving semantic conflicts and preserving discriminability.

Conclusion: The framework effectively mitigates semantic inconsistencies, enhancing 3D semantic understanding for applications like robotics and AR/VR.

Abstract: Recent advances in 3D reconstruction techniques and vision-language models
have fueled significant progress in 3D semantic understanding, a capability
critical to robotics, autonomous driving, and virtual/augmented reality.
However, methods that rely on 2D priors are prone to a critical challenge:
cross-view semantic inconsistencies induced by occlusion, image blur, and
view-dependent variations. These inconsistencies, when propagated via
projection supervision, deteriorate the quality of 3D Gaussian semantic fields
and introduce artifacts in the rendered outputs. To mitigate this limitation,
we propose CCL-LGS, a novel framework that enforces view-consistent semantic
supervision by integrating multi-view semantic cues. Specifically, our approach
first employs a zero-shot tracker to align a set of SAM-generated 2D masks and
reliably identify their corresponding categories. Next, we utilize CLIP to
extract robust semantic encodings across views. Finally, our Contrastive
Codebook Learning (CCL) module distills discriminative semantic features by
enforcing intra-class compactness and inter-class distinctiveness. In contrast
to previous methods that directly apply CLIP to imperfect masks, our framework
explicitly resolves semantic conflicts while preserving category
discriminability. Extensive experiments demonstrate that CCL-LGS outperforms
previous state-of-the-art methods. Our project page is available at
https://epsilontl.github.io/CCL-LGS/.

</details>


### [257] [WeatherEdit: Controllable Weather Editing with 4D Gaussian Field](https://arxiv.org/pdf/2505.20471)
*Chenghao Qian, Wenjing Li, Yuhu Guo, Gustav Markkula*

Main category: cs.CV

TL;DR: WeatherEdit is a pipeline for generating realistic, controllable weather effects in 3D scenes using a two-part approach: weather background editing and particle construction.


<details>
  <summary>Details</summary>
Motivation: To enhance autonomous driving simulations by creating diverse, realistic weather effects with adjustable severity.

Method: Combines an all-in-one adapter for 2D weather editing with a Temporal-View attention mechanism and a 4D Gaussian field for 3D particle construction.

Result: Generates diverse, realistic weather effects with controllable severity, validated on driving datasets.

Conclusion: WeatherEdit shows promise for improving autonomous driving simulations in adverse weather conditions.

Abstract: In this work, we present WeatherEdit, a novel weather editing pipeline for
generating realistic weather effects with controllable types and severity in 3D
scenes. Our approach is structured into two key components: weather background
editing and weather particle construction. For weather background editing, we
introduce an all-in-one adapter that integrates multiple weather styles into a
single pretrained diffusion model, enabling the generation of diverse weather
effects in 2D image backgrounds. During inference, we design a Temporal-View
(TV-) attention mechanism that follows a specific order to aggregate temporal
and spatial information, ensuring consistent editing across multi-frame and
multi-view images. To construct the weather particles, we first reconstruct a
3D scene using the edited images and then introduce a dynamic 4D Gaussian field
to generate snowflakes, raindrops and fog in the scene. The attributes and
dynamics of these particles are precisely controlled through physical-based
modelling and simulation, ensuring realistic weather representation and
flexible severity adjustments. Finally, we integrate the 4D Gaussian field with
the 3D scene to render consistent and highly realistic weather effects.
Experiments on multiple driving datasets demonstrate that WeatherEdit can
generate diverse weather effects with controllable condition severity,
highlighting its potential for autonomous driving simulation in adverse
weather. See project page: https://jumponthemoon.github.io/w-edit

</details>


### [258] [Rebalancing Contrastive Alignment with Learnable Semantic Gaps in Text-Video Retrieval](https://arxiv.org/pdf/2505.12499)
*Jian Xiao, Zijie Song, Jialong Hu, Hao Cheng, Zhenzhen Hu, Jia Li, Richang Hong*

Main category: cs.CV

TL;DR: GARE introduces a learnable, pair-specific increment to mitigate the modality gap and false negatives in text-video retrieval, improving alignment accuracy.


<details>
  <summary>Details</summary>
Motivation: Existing methods overlook the modality gap and false negatives, causing conflicting gradients under InfoNCE loss, which hinders stable alignment.

Method: GARE uses a lightweight neural module to compute pair-specific increments (Delta_ij) derived from a Taylor approximation of InfoNCE loss, guided by gradient supervision and regularized for stability.

Result: Experiments on four benchmarks show GARE improves alignment accuracy and robustness to noisy supervision.

Conclusion: GARE effectively mitigates gradient conflicts and enhances retrieval performance by addressing the modality gap and false negatives.

Abstract: Recent advances in text-video retrieval have been largely driven by
contrastive learning frameworks. However, existing methods overlook a key
source of optimization tension: the separation between text and video
distributions in the representation space (referred to as the modality gap),
and the prevalence of false negatives in batch sampling. These factors lead to
conflicting gradients under the InfoNCE loss, impeding stable alignment. To
mitigate this, we propose GARE, a Gap-Aware Retrieval framework that introduces
a learnable, pair-specific increment Delta_ij between text t_i and video v_j to
offload the tension from the global anchor representation. We first derive the
ideal form of Delta_ij via a coupled multivariate first-order Taylor
approximation of the InfoNCE loss under a trust-region constraint, revealing it
as a mechanism for resolving gradient conflicts by guiding updates along a
locally optimal descent direction. Due to the high cost of directly computing
Delta_ij, we introduce a lightweight neural module conditioned on the semantic
gap between each video-text pair, enabling structure-aware correction guided by
gradient supervision. To further stabilize learning and promote
interpretability, we regularize Delta using three components: a trust-region
constraint to prevent oscillation, a directional diversity term to promote
semantic coverage, and an information bottleneck to limit redundancy.
Experiments across four retrieval benchmarks show that GARE consistently
improves alignment accuracy and robustness to noisy supervision, confirming the
effectiveness of gap-aware tension mitigation.

</details>


### [259] [ControlTac: Force- and Position-Controlled Tactile Data Augmentation with a Single Reference Image](https://arxiv.org/pdf/2505.20498)
*Dongyu Luo, Kelin Yu, Amir-Hossein Shahidzadeh, Cornelia Fermüller, Yiannis Aloimonos*

Main category: cs.CV

TL;DR: ControlTac is a two-stage framework for generating realistic tactile images using physical priors like reference images, force, and position, enhancing data augmentation for tactile datasets.


<details>
  <summary>Details</summary>
Motivation: Large-scale tactile data collection is costly and inconsistent, and existing methods like simulation or free-form generation often produce unrealistic or poorly transferable results.

Method: ControlTac uses a two-stage controllable framework to generate tactile images conditioned on a reference image, contact force, and position.

Result: The generated images are physically plausible and varied, improving performance in three downstream tasks.

Conclusion: ControlTac effectively augments tactile datasets, validated by real-world experiments, and offers practical utility.

Abstract: Vision-based tactile sensing has been widely used in perception,
reconstruction, and robotic manipulation. However, collecting large-scale
tactile data remains costly due to the localized nature of sensor-object
interactions and inconsistencies across sensor instances. Existing approaches
to scaling tactile data, such as simulation and free-form tactile generation,
often suffer from unrealistic output and poor transferability to downstream
tasks.To address this, we propose ControlTac, a two-stage controllable
framework that generates realistic tactile images conditioned on a single
reference tactile image, contact force, and contact position. With those
physical priors as control input, ControlTac generates physically plausible and
varied tactile images that can be used for effective data augmentation. Through
experiments on three downstream tasks, we demonstrate that ControlTac can
effectively augment tactile datasets and lead to consistent gains. Our three
real-world experiments further validate the practical utility of our approach.
Project page: https://dongyuluo.github.io/controltac.

</details>


### [260] [Modality Curation: Building Universal Embeddings for Advanced Multimodal Information Retrieval](https://arxiv.org/pdf/2505.19650)
*Fanheng Kong, Jingyuan Zhang, Yahui Liu, Hongzhi Zhang, Shi Feng, Xiaocui Yang, Daling Wang, Yu Tian, Victoria W., Fuzheng Zhang, Guorui Zhou*

Main category: cs.CV

TL;DR: UNITE is a universal framework for multimodal information retrieval (MIR) that addresses challenges like data heterogeneity and cross-modal alignment through data curation and modality-aware training, achieving state-of-the-art results.


<details>
  <summary>Details</summary>
Motivation: The heterogeneity of data sources and complexity of cross-modal alignment in MIR pose significant challenges, with modal gaps in feature spaces remaining underexplored.

Method: UNITE introduces data curation and modality-aware training configurations, including Modal-Aware Masked Contrastive Learning (MAMCL) to mitigate inter-modal competition.

Result: The framework outperforms existing methods on multiple benchmarks, demonstrating the importance of strategic modality curation and tailored training.

Conclusion: UNITE advances MIR performance and provides a foundational blueprint for future multimodal research.

Abstract: Multimodal information retrieval (MIR) faces inherent challenges due to the
heterogeneity of data sources and the complexity of cross-modal alignment.
While previous studies have identified modal gaps in feature spaces, a
systematic approach to address these challenges remains unexplored. In this
work, we introduce UNITE, a universal framework that tackles these challenges
through two critical yet underexplored aspects: data curation and
modality-aware training configurations. Our work provides the first
comprehensive analysis of how modality-specific data properties influence
downstream task performance across diverse scenarios. Moreover, we propose
Modal-Aware Masked Contrastive Learning (MAMCL) to mitigate the competitive
relationships among the instances of different modalities. Our framework
achieves state-of-the-art results on multiple multimodal retrieval benchmarks,
outperforming existing methods by notable margins. Through extensive
experiments, we demonstrate that strategic modality curation and tailored
training protocols are pivotal for robust cross-modal representation learning.
This work not only advances MIR performance but also provides a foundational
blueprint for future research in multimodal systems. Our project is available
at https://friedrichor.github.io/projects/UNITE.

</details>


### [261] [Electrolyzers-HSI: Close-Range Multi-Scene Hyperspectral Imaging Benchmark Dataset](https://arxiv.org/pdf/2505.20507)
*Elias Arbash, Ahmed Jamal Afifi, Ymane Belahsen, Margret Fuchs, Pedram Ghamisi, Paul Scheunders, Richard Gloaguen*

Main category: cs.CV

TL;DR: A novel multimodal dataset (Electrolyzers-HSI) is introduced for accurate electrolyzer material classification, aiding sustainable recycling. It includes RGB and HSI data, evaluated with ML and DL methods, and is openly available for reproducibility.


<details>
  <summary>Details</summary>
Motivation: To address the need for automated, accurate material detection systems for sustainable recycling and the circular economy.

Method: The dataset combines RGB and hyperspectral imaging (HSI) data. Baseline ML and SOTA transformer-based DL architectures (e.g., Vision Transformer, SpectralFormer) are evaluated, with zero-shot detection and majority voting for robustness.

Result: The dataset enables non-invasive spectral analysis and material classification, with methods tested for efficiency and robustness.

Conclusion: Electrolyzers-HSI supports reproducible research and broader adoption of smart recycling solutions, aligning with FAIR data principles.

Abstract: The global challenge of sustainable recycling demands automated, fast, and
accurate, state-of-the-art (SOTA) material detection systems that act as a
bedrock for a circular economy. Democratizing access to these cutting-edge
solutions that enable real-time waste analysis is essential for scaling up
recycling efforts and fostering the Green Deal. In response, we introduce
\textbf{Electrolyzers-HSI}, a novel multimodal benchmark dataset designed to
accelerate the recovery of critical raw materials through accurate electrolyzer
materials classification. The dataset comprises 55 co-registered
high-resolution RGB images and hyperspectral imaging (HSI) data cubes spanning
the 400--2500 nm spectral range, yielding over 4.2 million pixel vectors and
424,169 labeled ones. This enables non-invasive spectral analysis of shredded
electrolyzer samples, supporting quantitative and qualitative material
classification and spectral properties investigation. We evaluate a suite of
baseline machine learning (ML) methods alongside SOTA transformer-based deep
learning (DL) architectures, including Vision Transformer, SpectralFormer, and
the Multimodal Fusion Transformer, to investigate architectural bottlenecks for
further efficiency optimisation when deploying transformers in material
identification. We implement zero-shot detection techniques and majority voting
across pixel-level predictions to establish object-level classification
robustness. In adherence to the FAIR data principles, the electrolyzers-HSI
dataset and accompanying codebase are openly available at
https://github.com/hifexplo/Electrolyzers-HSI and
https://rodare.hzdr.de/record/3668, supporting reproducible research and
facilitating the broader adoption of smart and sustainable e-waste recycling
solutions.

</details>


### [262] [CPathAgent: An Agent-based Foundation Model for Interpretable High-Resolution Pathology Image Analysis Mimicking Pathologists' Diagnostic Logic](https://arxiv.org/pdf/2505.20510)
*Yuxuan Sun, Yixuan Si, Chenglu Zhu, Kai Zhang, Zhongyi Shui, Bowen Ding, Tao Lin, Lin Yang*

Main category: cs.CV

TL;DR: CPathAgent is an agent-based model mimicking pathologists' diagnostic logic by autonomously zooming and navigating pathology images, outperforming existing methods.


<details>
  <summary>Details</summary>
Motivation: Existing foundation models in computational pathology fail to replicate pathologists' systematic diagnostic process, lacking multi-scale reasoning.

Method: CPathAgent uses a multi-stage training strategy to unify patch-level, region-level, and whole-slide capabilities, mimicking pathologists' reasoning.

Result: CPathAgent generates more detailed and interpretable reports and outperforms existing methods across benchmarks, including the new PathMMU-HR².

Conclusion: CPathAgent validates the effectiveness of agent-based diagnostic approaches, offering a promising direction for computational pathology.

Abstract: Recent advances in computational pathology have led to the emergence of
numerous foundation models. However, these approaches fail to replicate the
diagnostic process of pathologists, as they either simply rely on
general-purpose encoders with multi-instance learning for classification or
directly apply multimodal models to generate reports from images. A significant
limitation is their inability to emulate the diagnostic logic employed by
pathologists, who systematically examine slides at low magnification for
overview before progressively zooming in on suspicious regions to formulate
comprehensive diagnoses. To address this gap, we introduce CPathAgent, an
innovative agent-based model that mimics pathologists' reasoning processes by
autonomously executing zoom-in/out and navigation operations across pathology
images based on observed visual features. To achieve this, we develop a
multi-stage training strategy unifying patch-level, region-level, and
whole-slide capabilities within a single model, which is essential for
mimicking pathologists, who require understanding and reasoning capabilities
across all three scales. This approach generates substantially more detailed
and interpretable diagnostic reports compared to existing methods, particularly
for huge region understanding. Additionally, we construct an expert-validated
PathMMU-HR$^{2}$, the first benchmark for huge region analysis, a critical
intermediate scale between patches and whole slides, as diagnosticians
typically examine several key regions rather than entire slides at once.
Extensive experiments demonstrate that CPathAgent consistently outperforms
existing approaches across three scales of benchmarks, validating the
effectiveness of our agent-based diagnostic approach and highlighting a
promising direction for the future development of computational pathology.

</details>


### [263] [A Feature-level Bias Evaluation Framework for Facial Expression Recognition Models](https://arxiv.org/pdf/2505.20512)
*Tangzheng Lian, Oya Celiktutan*

Main category: cs.CV

TL;DR: The paper proposes a feature-level bias evaluation framework for FER models when demographic labels are unavailable, outperforming pseudo-label methods. It also introduces a statistical module to ensure significance in bias results.


<details>
  <summary>Details</summary>
Motivation: Existing FER models exhibit biases, but limited annotated demographic labels and reliance on pseudo-labels distort bias analysis. Statistical significance in bias evaluations is often overlooked.

Method: A feature-level bias evaluation framework is introduced for scenarios without demographic labels, alongside a plug-and-play statistical module to validate significance.

Result: The method effectively evaluates biases without demographic labels and ensures statistical significance. Analysis reveals biases across age, gender, race, expressions, and architectures.

Conclusion: The framework provides reliable bias evaluation and insights for fairer FER model selection, addressing limitations of pseudo-labels and statistical oversight.

Abstract: Recent studies on fairness have shown that Facial Expression Recognition
(FER) models exhibit biases toward certain visually perceived demographic
groups. However, the limited availability of human-annotated demographic labels
in public FER datasets has constrained the scope of such bias analysis. To
overcome this limitation, some prior works have resorted to pseudo-demographic
labels, which may distort bias evaluation results. Alternatively, in this
paper, we propose a feature-level bias evaluation framework for evaluating
demographic biases in FER models under the setting where demographic labels are
unavailable in the test set. Extensive experiments demonstrate that our method
more effectively evaluates demographic biases compared to existing approaches
that rely on pseudo-demographic labels. Furthermore, we observe that many
existing studies do not include statistical testing in their bias evaluations,
raising concerns that some reported biases may not be statistically significant
but rather due to randomness. To address this issue, we introduce a
plug-and-play statistical module to ensure the statistical significance of
biased evaluation results. A comprehensive bias analysis based on the proposed
module is then conducted across three sensitive attributes (age, gender, and
race), seven facial expressions, and multiple network architectures on a
large-scale dataset, revealing the prominent demographic biases in FER and
providing insights on selecting a fairer network architecture.

</details>


### [264] [Stereo Radargrammetry Using Deep Learning from Airborne SAR Images](https://arxiv.org/pdf/2505.20876)
*Tatsuya Sasayama, Shintaro Ito, Koichi Ito, Takafumi Aoki*

Main category: cs.CV

TL;DR: A deep learning-based stereo radargrammetry method for airborne SAR images is proposed, addressing the lack of public datasets and improving elevation measurement accuracy.


<details>
  <summary>Details</summary>
Motivation: Existing deep learning methods for SAR images lack public datasets and suffer from geometric image modulation issues.

Method: The method involves creating a SAR dataset, fine-tuning a deep learning-based correspondence method, and processing images in patches without ground projection.

Result: The proposed method achieves wider range and more accurate elevation measurements than conventional methods.

Conclusion: The method effectively addresses dataset and quality issues, enhancing SAR image processing with deep learning.

Abstract: In this paper, we propose a stereo radargrammetry method using deep learning
from airborne Synthetic Aperture Radar (SAR) images.Deep learning-based methods
are considered to suffer less from geometric image modulation, while there is
no public SAR image dataset used to train such methods.We create a SAR image
dataset and perform fine-tuning of a deep learning-based image correspondence
method.The proposed method suppresses the degradation of image quality by pixel
interpolation without ground projection of the SAR image and divides the SAR
image into patches for processing, which makes it possible to apply deep
learning.Through a set of experiments, we demonstrate that the proposed method
exhibits a wider range and more accurate elevation measurements compared to
conventional methods.

</details>


### [265] [Paper2Poster: Towards Multimodal Poster Automation from Scientific Papers](https://arxiv.org/pdf/2505.21497)
*Wei Pang, Kevin Qinghong Lin, Xiangru Jian, Xi He, Philip Torr*

Main category: cs.CV

TL;DR: The paper introduces a benchmark and metric suite for academic poster generation, proposes PosterAgent for automated poster creation, and evaluates its performance against human-designed posters and GPT-4o outputs.


<details>
  <summary>Details</summary>
Motivation: Academic poster generation is challenging due to the need to compress long documents into visually coherent pages. The paper aims to address this by creating a benchmark and automated solution.

Method: The method involves a benchmark with four evaluation metrics and PosterAgent, a multi-agent pipeline with Parser, Planner, and Painter-Commenter components.

Result: PosterAgent outperforms GPT-4o-driven systems, using fewer tokens and achieving better scores across metrics. It generates editable posters at low cost.

Conclusion: The findings guide future automated poster-generation models, with open-source code and datasets available.

Abstract: Academic poster generation is a crucial yet challenging task in scientific
communication, requiring the compression of long-context interleaved documents
into a single, visually coherent page. To address this challenge, we introduce
the first benchmark and metric suite for poster generation, which pairs recent
conference papers with author-designed posters and evaluates outputs on
(i)Visual Quality-semantic alignment with human posters, (ii)Textual
Coherence-language fluency, (iii)Holistic Assessment-six fine-grained aesthetic
and informational criteria scored by a VLM-as-judge, and notably
(iv)PaperQuiz-the poster's ability to convey core paper content as measured by
VLMs answering generated quizzes. Building on this benchmark, we propose
PosterAgent, a top-down, visual-in-the-loop multi-agent pipeline: the (a)Parser
distills the paper into a structured asset library; the (b)Planner aligns
text-visual pairs into a binary-tree layout that preserves reading order and
spatial balance; and the (c)Painter-Commenter loop refines each panel by
executing rendering code and using VLM feedback to eliminate overflow and
ensure alignment. In our comprehensive evaluation, we find that GPT-4o
outputs-though visually appealing at first glance-often exhibit noisy text and
poor PaperQuiz scores, and we find that reader engagement is the primary
aesthetic bottleneck, as human-designed posters rely largely on visual
semantics to convey meaning. Our fully open-source variants (e.g. based on the
Qwen-2.5 series) outperform existing 4o-driven multi-agent systems across
nearly all metrics, while using 87% fewer tokens. It transforms a 22-page paper
into a finalized yet editable .pptx poster - all for just $0.005. These
findings chart clear directions for the next generation of fully automated
poster-generation models. The code and datasets are available at
https://github.com/Paper2Poster/Paper2Poster.

</details>


### [266] [MetaWriter: Personalized Handwritten Text Recognition Using Meta-Learned Prompt Tuning](https://arxiv.org/pdf/2505.20513)
*Wenhao Gu, Li Gu, Ching Yee Suen, Yang Wang*

Main category: cs.CV

TL;DR: Proposes an efficient framework for personalized handwritten text recognition (HTR) using prompt tuning and self-supervised learning, reducing parameter updates and eliminating annotation needs.


<details>
  <summary>Details</summary>
Motivation: Challenges in robust HTR across diverse writing styles due to lack of writer-specific personalization and inefficiencies in existing methods.

Method: Formulates personalization as prompt tuning with a self-supervised loss and meta-learning for optimal prompt initialization.

Result: Outperforms state-of-the-art methods on RIMES and IAM benchmarks with 20x fewer parameters and less than 1% updates.

Conclusion: Significant advancement in personalized HTR, enabling reliable deployment in resource-constrained scenarios.

Abstract: Recent advancements in handwritten text recognition (HTR) have enabled the
effective conversion of handwritten text to digital formats. However, achieving
robust recognition across diverse writing styles remains challenging.
Traditional HTR methods lack writer-specific personalization at test time due
to limitations in model architecture and training strategies. Existing attempts
to bridge this gap, through gradient-based meta-learning, still require labeled
examples and suffer from parameter-inefficient fine-tuning, leading to
substantial computational and memory overhead. To overcome these challenges, we
propose an efficient framework that formulates personalization as prompt
tuning, incorporating an auxiliary image reconstruction task with a
self-supervised loss to guide prompt adaptation with unlabeled test-time
examples. To ensure self-supervised loss effectively minimizes text recognition
error, we leverage meta-learning to learn the optimal initialization of the
prompts. As a result, our method allows the model to efficiently capture unique
writing styles by updating less than 1% of its parameters and eliminating the
need for time-intensive annotation processes. We validate our approach on the
RIMES and IAM Handwriting Database benchmarks, where it consistently
outperforms previous state-of-the-art methods while using 20x fewer parameters.
We believe this represents a significant advancement in personalized
handwritten text recognition, paving the way for more reliable and practical
deployment in resource-constrained scenarios.

</details>


### [267] [MultLFG: Training-free Multi-LoRA composition using Frequency-domain Guidance](https://arxiv.org/pdf/2505.20525)
*Aniket Roy, Maitreya Suin, Ketul Shah, Rama Chellappa*

Main category: cs.CV

TL;DR: MultLFG is a training-free framework for merging multiple LoRA adapters using frequency-domain guidance, improving multi-concept generation in generative models.


<details>
  <summary>Details</summary>
Motivation: Current methods for merging LoRA adapters lack efficiency in complex compositions, prompting the need for a better solution.

Method: MultLFG uses timestep and frequency subband adaptive fusion to selectively activate relevant LoRAs, enhancing spatial coherence and control.

Result: MultLFG outperforms baselines in compositional fidelity and image quality on the ComposLoRA benchmark.

Conclusion: MultLFG provides a superior, training-free approach for multi-LoRA composition, enabling more accurate and consistent generative results.

Abstract: Low-Rank Adaptation (LoRA) has gained prominence as a computationally
efficient method for fine-tuning generative models, enabling distinct visual
concept synthesis with minimal overhead. However, current methods struggle to
effectively merge multiple LoRA adapters without training, particularly in
complex compositions involving diverse visual elements. We introduce MultLFG, a
novel framework for training-free multi-LoRA composition that utilizes
frequency-domain guidance to achieve adaptive fusion of multiple LoRAs. Unlike
existing methods that uniformly aggregate concept-specific LoRAs, MultLFG
employs a timestep and frequency subband adaptive fusion strategy, selectively
activating relevant LoRAs based on content relevance at specific timesteps and
frequency bands. This frequency-sensitive guidance not only improves spatial
coherence but also provides finer control over multi-LoRA composition, leading
to more accurate and consistent results. Experimental evaluations on the
ComposLoRA benchmark reveal that MultLFG substantially enhances compositional
fidelity and image quality across various styles and concept sets,
outperforming state-of-the-art baselines in multi-concept generation tasks.
Code will be released.

</details>


### [268] [Boosting Adversarial Transferability via High-Frequency Augmentation and Hierarchical-Gradient Fusion](https://arxiv.org/pdf/2505.21181)
*Yayin Zheng, Chen Wan, Zihong Guo, Hailing Kuang, Xiaohai Lu*

Main category: cs.CV

TL;DR: FSA is a new adversarial attack framework combining frequency and spatial domains, outperforming existing methods with a 23.6% higher success rate.


<details>
  <summary>Details</summary>
Motivation: Addressing the challenge of adversarial attacks in black-box defense by integrating frequency-domain insights.

Method: Uses High-Frequency Augmentation (Fourier transform) and Hierarchical-Gradient Fusion for multi-scale gradient decomposition.

Result: Achieves 23.6% higher attack success rate than BSR on eight black-box defense models.

Conclusion: FSA effectively enhances adversarial transferability by leveraging frequency and spatial domains.

Abstract: Adversarial attacks have become a significant challenge in the security of
machine learning models, particularly in the context of black-box defense
strategies. Existing methods for enhancing adversarial transferability
primarily focus on the spatial domain. This paper presents Frequency-Space
Attack (FSA), a new adversarial attack framework that effectively integrates
frequency-domain and spatial-domain transformations. FSA combines two key
techniques: (1) High-Frequency Augmentation, which applies Fourier transform
with frequency-selective amplification to diversify inputs and emphasize the
critical role of high-frequency components in adversarial attacks, and (2)
Hierarchical-Gradient Fusion, which merges multi-scale gradient decomposition
and fusion to capture both global structures and fine-grained details,
resulting in smoother perturbations. Our experiment demonstrates that FSA
consistently outperforms state-of-the-art methods across various black-box
models. Notably, our proposed FSA achieves an average attack success rate
increase of 23.6% compared with BSR (CVPR 2024) on eight black-box defense
models.

</details>


### [269] [Causality and "In-the-Wild" Video-Based Person Re-ID: A Survey](https://arxiv.org/pdf/2505.20540)
*Md Rashidunnabi, Kailash Hambarde, Hugo Proença*

Main category: cs.CV

TL;DR: The survey explores causal reasoning in video-based person Re-ID, critiquing traditional correlation-based methods and proposing causal approaches for better generalization.


<details>
  <summary>Details</summary>
Motivation: Traditional Re-ID models rely on superficial correlations (e.g., clothing, background) that fail in real-world scenarios. Causal reasoning offers a principled alternative to address these limitations.

Method: The survey categorizes causal Re-ID methods into generative disentanglement, domain-invariant modeling, and causal transformers, while also introducing causal-specific robustness metrics.

Result: Causal methods show promise in isolating identity-specific features from confounders, improving generalization across domains and viewpoints.

Conclusion: The survey lays a foundation for causal Re-ID research, highlighting open problems and future directions like integrating causal modeling with efficient architectures and self-supervised learning.

Abstract: Video-based person re-identification (Re-ID) remains brittle in real-world
deployments despite impressive benchmark performance. Most existing models rely
on superficial correlations such as clothing, background, or lighting that fail
to generalize across domains, viewpoints, and temporal variations. This survey
examines the emerging role of causal reasoning as a principled alternative to
traditional correlation-based approaches in video-based Re-ID. We provide a
structured and critical analysis of methods that leverage structural causal
models, interventions, and counterfactual reasoning to isolate
identity-specific features from confounding factors. The survey is organized
around a novel taxonomy of causal Re-ID methods that spans generative
disentanglement, domain-invariant modeling, and causal transformers. We review
current evaluation metrics and introduce causal-specific robustness measures.
In addition, we assess practical challenges of scalability, fairness,
interpretability, and privacy that must be addressed for real-world adoption.
Finally, we identify open problems and outline future research directions that
integrate causal modeling with efficient architectures and self-supervised
learning. This survey aims to establish a coherent foundation for causal
video-based person Re-ID and to catalyze the next phase of research in this
rapidly evolving domain.

</details>


### [270] [DiMoSR: Feature Modulation via Multi-Branch Dilated Convolutions for Efficient Image Super-Resolution](https://arxiv.org/pdf/2505.21262)
*M. Akin Yilmaz, Ahmet Bilican, A. Murat Tekalp*

Main category: cs.CV

TL;DR: DiMoSR introduces a novel architecture for lightweight SISR, combining modulation and attention to improve feature representation while maintaining efficiency.


<details>
  <summary>Details</summary>
Motivation: Addressing the trade-off between reconstruction quality and model efficiency in lightweight SISR, exploring alternatives to attention mechanisms.

Method: Uses multi-branch dilated convolutions for rich contextual information and efficient computation, complementing attention with modulation.

Result: Outperforms state-of-the-art lightweight SISR methods in PSNR and SSIM metrics with comparable or lower complexity.

Conclusion: DiMoSR validates the synergy of attention and modulation, offering insights for future efficient network designs.

Abstract: Balancing reconstruction quality versus model efficiency remains a critical
challenge in lightweight single image super-resolution (SISR). Despite the
prevalence of attention mechanisms in recent state-of-the-art SISR approaches
that primarily emphasize or suppress feature maps, alternative architectural
paradigms warrant further exploration. This paper introduces DiMoSR (Dilated
Modulation Super-Resolution), a novel architecture that enhances feature
representation through modulation to complement attention in lightweight SISR
networks. The proposed approach leverages multi-branch dilated convolutions to
capture rich contextual information over a wider receptive field while
maintaining computational efficiency. Experimental results demonstrate that
DiMoSR outperforms state-of-the-art lightweight methods across diverse
benchmark datasets, achieving superior PSNR and SSIM metrics with comparable or
reduced computational complexity. Through comprehensive ablation studies, this
work not only validates the effectiveness of DiMoSR but also provides critical
insights into the interplay between attention mechanisms and feature modulation
to guide future research in efficient network design. The code and model
weights to reproduce our results are available at:
https://github.com/makinyilmaz/DiMoSR

</details>


### [271] [Retrieval Visual Contrastive Decoding to Mitigate Object Hallucinations in Large Vision-Language Models](https://arxiv.org/pdf/2505.20569)
*Jihoon Lee, Min Song*

Main category: cs.CV

TL;DR: RVCD, a new method using contrastive decoding with positive and negative images, reduces Object Hallucination in vision-language models without extra training.


<details>
  <summary>Details</summary>
Motivation: Address the persistent issue of Object Hallucination (OH) in Large Vision-Language Models without requiring additional model training.

Method: Introduces RVCD (Retrieval Visual Contrastive Decoding), leveraging negative and positive images at the logit level, including AI-generated images representing single concepts.

Result: Substantial improvements over existing decoding-based methods in suppressing OH.

Conclusion: RVCD effectively mitigates Object Hallucination, advancing vision-language model performance.

Abstract: Despite significant advancements in Large Vision-Language Models, Object
Hallucination (OH) remains a persistent challenge. Building upon prior studies
on contrastive decoding that address this issue without requiring additional
model training, we introduce RVCD (Retrieval Visual Contrastive Decoding), an
advanced method to suppress OH. RVCD leverages both negative and positive
images at the logit level, explicitly referencing AI-generated images designed
to represent a single concept. Our approach demonstrates substantial
improvements over existing decoding-based methods.

</details>


### [272] [Supervised and self-supervised land-cover segmentation & classification of the Biesbosch wetlands](https://arxiv.org/pdf/2505.21269)
*Eva Gmelich Meijling, Roberto Del Prete, Arnoud Visser*

Main category: cs.CV

TL;DR: The paper proposes a hybrid supervised and self-supervised learning approach for wetland land-cover classification, addressing data scarcity with SSL pretraining and scaling annotations across resolutions.


<details>
  <summary>Details</summary>
Motivation: Accurate wetland classification is vital for environmental monitoring, but limited annotated data, especially for high-resolution imagery, hinders supervised learning.

Method: A U-Net model is trained on Sentinel-2 imagery using both supervised and self-supervised learning (SSL) with autoencoder pretraining. High-resolution labels are scaled to medium-resolution inputs.

Result: SSL pretraining improved accuracy from 85.26% to 88.23%. High-resolution imagery offers sharper segmentation despite comparable quantitative performance.

Conclusion: The hybrid approach effectively addresses data scarcity, and the publicly shared dataset aids future wetland classification research.

Abstract: Accurate wetland land-cover classification is essential for environmental
monitoring, biodiversity assessment, and sustainable ecosystem management.
However, the scarcity of annotated data, especially for high-resolution
satellite imagery, poses a significant challenge for supervised learning
approaches. To tackle this issue, this study presents a methodology for wetland
land-cover segmentation and classification that adopts both supervised and
self-supervised learning (SSL). We train a U-Net model from scratch on
Sentinel-2 imagery across six wetland regions in the Netherlands, achieving a
baseline model accuracy of 85.26%.
  Addressing the limited availability of labeled data, the results show that
SSL pretraining with an autoencoder can improve accuracy, especially for the
high-resolution imagery where it is more difficult to obtain labeled data,
reaching an accuracy of 88.23%.
  Furthermore, we introduce a framework to scale manually annotated
high-resolution labels to medium-resolution inputs. While the quantitative
performance between resolutions is comparable, high-resolution imagery provides
significantly sharper segmentation boundaries and finer spatial detail.
  As part of this work, we also contribute a curated Sentinel-2 dataset with
Dynamic World labels, tailored for wetland classification tasks and made
publicly available.

</details>


### [273] [Total-Editing: Head Avatar with Editable Appearance, Motion, and Lighting](https://arxiv.org/pdf/2505.20582)
*Yizhou Zhao, Chunjiang Liu, Haoyu Chen, Bhiksha Raj, Min Xu, Tadas Baltrusaitis, Mitch Rundle, HsiangTao Wu, Kamran Ghasedi*

Main category: cs.CV

TL;DR: Total-Editing is a unified framework for portrait editing, combining face reenactment and relighting by leveraging neural radiance fields and deformation fields for realistic results.


<details>
  <summary>Details</summary>
Motivation: Current methods treat face reenactment and portrait relighting separately, missing opportunities for synergy in geometric consistency and illumination awareness.

Method: Uses a neural radiance field decoder with intrinsic decomposition and a deformation field for spatiotemporal coherence.

Result: Improves quality and realism of portrait editing, enabling flexible applications like illumination transfer and animation.

Conclusion: Total-Editing offers a unified, high-quality solution for portrait editing with precise control over appearance, motion, and lighting.

Abstract: Face reenactment and portrait relighting are essential tasks in portrait
editing, yet they are typically addressed independently, without much synergy.
Most face reenactment methods prioritize motion control and multiview
consistency, while portrait relighting focuses on adjusting shading effects. To
take advantage of both geometric consistency and illumination awareness, we
introduce Total-Editing, a unified portrait editing framework that enables
precise control over appearance, motion, and lighting. Specifically, we design
a neural radiance field decoder with intrinsic decomposition capabilities. This
allows seamless integration of lighting information from portrait images or HDR
environment maps into synthesized portraits. We also incorporate a moving least
squares based deformation field to enhance the spatiotemporal coherence of
avatar motion and shading effects. With these innovations, our unified
framework significantly improves the quality and realism of portrait editing
results. Further, the multi-source nature of Total-Editing supports more
flexible applications, such as illumination transfer from one portrait to
another, or portrait animation with customized backgrounds.

</details>


### [274] [Efficient Leaf Disease Classification and Segmentation using Midpoint Normalization Technique and Attention Mechanism](https://arxiv.org/pdf/2505.21316)
*Enam Ahmed Taufik, Antara Firoz Parsa, Seraj Al Mahmud Mostafa*

Main category: cs.CV

TL;DR: A two-stage method (Mid Point Normalization and attention mechanisms) improves plant disease detection from leaf images, achieving high accuracy and efficiency.


<details>
  <summary>Details</summary>
Motivation: Addressing challenges like scarce labeled data and complex contextual factors in plant disease detection from leaf imagery.

Method: Combines Mid Point Normalization (MPN) for preprocessing with attention mechanisms (Squeeze-and-Excitation blocks) for feature refinement, integrated into classification and segmentation pipelines.

Result: 93% accuracy in classification, perfect F1 score for target class; 72.44% Dice score and 58.54% IoU in segmentation, outperforming baselines.

Conclusion: The approach offers high accuracy, computational efficiency, and lightweight architectures suitable for real-world applications.

Abstract: Enhancing plant disease detection from leaf imagery remains a persistent
challenge due to scarce labeled data and complex contextual factors. We
introduce a transformative two-stage methodology, Mid Point Normalization (MPN)
for intelligent image preprocessing, coupled with sophisticated attention
mechanisms that dynamically recalibrate feature representations. Our
classification pipeline, merging MPN with Squeeze-and-Excitation (SE) blocks,
achieves remarkable 93% accuracy while maintaining exceptional class-wise
balance. The perfect F1 score attained for our target class exemplifies
attention's power in adaptive feature refinement. For segmentation tasks, we
seamlessly integrate identical attention blocks within U-Net architecture using
MPN-enhanced inputs, delivering compelling performance gains with 72.44% Dice
score and 58.54% IoU, substantially outperforming baseline implementations.
Beyond superior accuracy metrics, our approach yields computationally
efficient, lightweight architectures perfectly suited for real-world computer
vision applications.

</details>


### [275] [OmniIndoor3D: Comprehensive Indoor 3D Reconstruction](https://arxiv.org/pdf/2505.20610)
*Xiaobao Wei, Xiaoan Zhang, Hao Wang, Qingpo Wuwu, Ming Lu, Wenzhao Zheng, Shanghang Zhang*

Main category: cs.CV

TL;DR: OmniIndoor3D is a novel framework for indoor 3D reconstruction using Gaussian representations, combining RGB-D images for accurate appearance, geometry, and panoptic reconstruction.


<details>
  <summary>Details</summary>
Motivation: Existing 3DGS methods lack precise geometry for high-quality panoptic reconstruction, necessitating a solution for comprehensive indoor scene understanding.

Method: The framework initializes 3D Gaussians from coarse RGB-D reconstructions, uses a lightweight MLP to adjust geometry, and employs panoptic-guided densification for smooth surfaces.

Result: OmniIndoor3D achieves state-of-the-art results in appearance, geometry, and panoptic reconstruction across multiple datasets.

Conclusion: The work bridges a critical gap in indoor 3D reconstruction, enabling robust robotic navigation.

Abstract: We propose a novel framework for comprehensive indoor 3D reconstruction using
Gaussian representations, called OmniIndoor3D. This framework enables accurate
appearance, geometry, and panoptic reconstruction of diverse indoor scenes
captured by a consumer-level RGB-D camera. Since 3DGS is primarily optimized
for photorealistic rendering, it lacks the precise geometry critical for
high-quality panoptic reconstruction. Therefore, OmniIndoor3D first combines
multiple RGB-D images to create a coarse 3D reconstruction, which is then used
to initialize the 3D Gaussians and guide the 3DGS training. To decouple the
optimization conflict between appearance and geometry, we introduce a
lightweight MLP that adjusts the geometric properties of 3D Gaussians. The
introduced lightweight MLP serves as a low-pass filter for geometry
reconstruction and significantly reduces noise in indoor scenes. To improve the
distribution of Gaussian primitives, we propose a densification strategy guided
by panoptic priors to encourage smoothness on planar surfaces. Through the
joint optimization of appearance, geometry, and panoptic reconstruction,
OmniIndoor3D provides comprehensive 3D indoor scene understanding, which
facilitates accurate and robust robotic navigation. We perform thorough
evaluations across multiple datasets, and OmniIndoor3D achieves
state-of-the-art results in appearance, geometry, and panoptic reconstruction.
We believe our work bridges a critical gap in indoor 3D reconstruction. The
code will be released at: https://ucwxb.github.io/OmniIndoor3D/

</details>


### [276] [Mamba-Driven Topology Fusion for Monocular 3-D Human Pose Estimation](https://arxiv.org/pdf/2505.20611)
*Zenghao Zheng, Lianping Yang, Jinshan Pan, Hegui Zhu*

Main category: cs.CV

TL;DR: The paper introduces a Mamba-Driven Topology Fusion framework to improve 3-D human pose estimation by addressing computational and structural limitations of Transformer and Mamba models.


<details>
  <summary>Details</summary>
Motivation: Transformer-based methods for 3-D human pose estimation face high computational costs due to quadratic self-attention complexity, while Mamba's state space model (SSM) lacks suitability for topological joint sequences and local joint insights.

Method: Proposes a Bone Aware Module for topological guidance, enhances Mamba's convolution with forward/backward graph networks, and adds a Spatiotemporal Refinement Module for joint relationships.

Result: Experiments on Human3.6M and MPI-INF-3DHP show reduced computational cost and higher accuracy. Ablation studies confirm module effectiveness.

Conclusion: The framework effectively integrates skeletal topology to overcome Mamba's limitations, achieving efficient and accurate 3-D pose estimation.

Abstract: Transformer-based methods for 3-D human pose estimation face significant
computational challenges due to the quadratic growth of self-attention
mechanism complexity with sequence length. Recently, the Mamba model has
substantially reduced computational overhead and demonstrated outstanding
performance in modeling long sequences by leveraging state space model (SSM).
However, the ability of SSM to process sequential data is not suitable for 3-D
joint sequences with topological structures, and the causal convolution
structure in Mamba also lacks insight into local joint relationships. To
address these issues, we propose the Mamba-Driven Topology Fusion framework in
this paper. Specifically, the proposed Bone Aware Module infers the direction
and length of bone vectors in the spherical coordinate system, providing
effective topological guidance for the Mamba model in processing joint
sequences. Furthermore, we enhance the convolutional structure within the Mamba
model by integrating forward and backward graph convolutional network, enabling
it to better capture local joint dependencies. Finally, we design a
Spatiotemporal Refinement Module to model both temporal and spatial
relationships within the sequence. Through the incorporation of skeletal
topology, our approach effectively alleviates Mamba's limitations in capturing
human structural relationships. We conduct extensive experiments on the
Human3.6M and MPI-INF-3DHP datasets for testing and comparison, and the results
show that the proposed method greatly reduces computational cost while
achieving higher accuracy. Ablation studies further demonstrate the
effectiveness of each proposed module. The code and models will be released.

</details>


### [277] [Roboflow100-VL: A Multi-Domain Object Detection Benchmark for Vision-Language Models](https://arxiv.org/pdf/2505.20612)
*Peter Robicheaux, Matvei Popov, Anish Madan, Isaac Robinson, Joseph Nelson, Deva Ramanan, Neehar Peri*

Main category: cs.CV

TL;DR: VLMs struggle with out-of-distribution tasks; Roboflow100-VL introduces 100 diverse datasets for few-shot alignment, showing VLMs perform poorly (<2% accuracy) on medical imaging.


<details>
  <summary>Details</summary>
Motivation: Current VLMs fail to generalize to uncommon tasks or modalities, necessitating better alignment methods.

Method: Proposes Roboflow100-VL, a benchmark with 100 diverse datasets, and evaluates VLMs in various settings (zero-shot to fully supervised).

Result: VLMs like GroundingDINO and Qwen2.5-VL achieve <2% zero-shot accuracy on medical imaging datasets.

Conclusion: Few-shot concept alignment is crucial for improving VLM performance on out-of-distribution tasks.

Abstract: Vision-language models (VLMs) trained on internet-scale data achieve
remarkable zero-shot detection performance on common objects like car, truck,
and pedestrian. However, state-of-the-art models still struggle to generalize
to out-of-distribution classes, tasks and imaging modalities not typically
found in their pre-training. Rather than simply re-training VLMs on more visual
data, we argue that one should align VLMs to new concepts with annotation
instructions containing a few visual examples and rich textual descriptions. To
this end, we introduce Roboflow100-VL, a large-scale collection of 100
multi-modal object detection datasets with diverse concepts not commonly found
in VLM pre-training. We evaluate state-of-the-art models on our benchmark in
zero-shot, few-shot, semi-supervised, and fully-supervised settings, allowing
for comparison across data regimes. Notably, we find that VLMs like
GroundingDINO and Qwen2.5-VL achieve less than 2% zero-shot accuracy on
challenging medical imaging datasets within Roboflow100-VL, demonstrating the
need for few-shot concept alignment. Our code and dataset are available at
https://github.com/roboflow/rf100-vl/ and
https://universe.roboflow.com/rf100-vl/

</details>


### [278] [Intelligent Incident Hypertension Prediction in Obstructive Sleep Apnea](https://arxiv.org/pdf/2505.20615)
*Omid Halimi Milani, Ahmet Enis Cetin, Bharati Prasad*

Main category: cs.CV

TL;DR: A deep learning model using DCT-based transfer learning and polysomnography signals predicts hypertension risk in OSA patients with 72.88% AUC.


<details>
  <summary>Details</summary>
Motivation: Predicting hypertension in OSA patients is challenging due to intermittent hypoxia and sleep fragmentation. This study aims to improve prediction accuracy using advanced deep learning techniques.

Method: The study integrates DCT-based transfer learning, transforming polysomnography signals into 2D representations for pre-trained neural networks (MobileNet, EfficientNet, ResNet). A DCT layer enhances feature learning in the frequency domain.

Result: The model achieved a 72.88% AUC, showing effectiveness in predicting hypertension risk over five years.

Conclusion: Frequency-domain feature extraction and transfer learning improve hypertension prediction in OSA patients, especially with limited medical data.

Abstract: Obstructive sleep apnea (OSA) is a significant risk factor for hypertension,
primarily due to intermittent hypoxia and sleep fragmentation. Predicting
whether individuals with OSA will develop hypertension within five years
remains a complex challenge. This study introduces a novel deep learning
approach that integrates Discrete Cosine Transform (DCT)-based transfer
learning to enhance prediction accuracy. We are the first to incorporate all
polysomnography signals together for hypertension prediction, leveraging their
collective information to improve model performance. Features were extracted
from these signals and transformed into a 2D representation to utilize
pre-trained 2D neural networks such as MobileNet, EfficientNet, and ResNet
variants. To further improve feature learning, we introduced a DCT layer, which
transforms input features into a frequency-based representation, preserving
essential spectral information, decorrelating features, and enhancing
robustness to noise. This frequency-domain approach, coupled with transfer
learning, is especially beneficial for limited medical datasets, as it
leverages rich representations from pre-trained networks to improve
generalization. By strategically placing the DCT layer at deeper truncation
depths within EfficientNet, our model achieved a best area under the curve
(AUC) of 72.88%, demonstrating the effectiveness of frequency-domain feature
extraction and transfer learning in predicting hypertension risk in OSA
patients over a five-year period.

</details>


### [279] [OccLE: Label-Efficient 3D Semantic Occupancy Prediction](https://arxiv.org/pdf/2505.20617)
*Naiyu Fang, Zheyuan Zhou, Fayao Liu, Xulei Yang, Jiacheng Wei, Lemiao Qiu, Guosheng Lin*

Main category: cs.CV

TL;DR: OccLE is a label-efficient method for 3D semantic occupancy prediction, combining semantic and geometric learning with limited voxel annotations, achieving competitive performance.


<details>
  <summary>Details</summary>
Motivation: Existing methods require costly voxel-level annotations or yield suboptimal results with self-supervision. OccLE addresses this by decoupling semantic and geometric learning.

Method: Uses semantic and geometric branches: semantic branch distills 2D foundation models for pseudo labels; geometric branch integrates image and LiDAR with semi-supervision. Features are fused via Dual Mamba and supervised with aligned pseudo labels.

Result: Achieves 16.59% mIoU on SemanticKITTI with only 10% voxel annotations.

Conclusion: OccLE offers a label-efficient solution for 3D semantic occupancy prediction, balancing performance and annotation cost.

Abstract: 3D semantic occupancy prediction offers an intuitive and efficient scene
understanding and has attracted significant interest in autonomous driving
perception. Existing approaches either rely on full supervision, which demands
costly voxel-level annotations, or on self-supervision, which provides limited
guidance and yields suboptimal performance. To address these challenges, we
propose OccLE, a Label-Efficient 3D Semantic Occupancy Prediction that takes
images and LiDAR as inputs and maintains high performance with limited voxel
annotations. Our intuition is to decouple the semantic and geometric learning
tasks and then fuse the learned feature grids from both tasks for the final
semantic occupancy prediction. Therefore, the semantic branch distills 2D
foundation model to provide aligned pseudo labels for 2D and 3D semantic
learning. The geometric branch integrates image and LiDAR inputs in cross-plane
synergy based on their inherency, employing semi-supervision to enhance
geometry learning. We fuse semantic-geometric feature grids through Dual Mamba
and incorporate a scatter-accumulated projection to supervise unannotated
prediction with aligned pseudo labels. Experiments show that OccLE achieves
competitive performance with only 10% of voxel annotations, reaching a mIoU of
16.59% on the SemanticKITTI validation set.

</details>


### [280] [ConsiStyle: Style Diversity in Training-Free Consistent T2I Generation](https://arxiv.org/pdf/2505.20626)
*Yohai Mazuz, Janna Bruner, Lior Wolf*

Main category: cs.CV

TL;DR: A training-free method for text-to-image models ensures consistent character generation across diverse styles by manipulating attention matrices and aligning Value statistics.


<details>
  <summary>Details</summary>
Motivation: Existing methods struggle to maintain subject consistency across varying style prompts, often requiring large-scale fine-tuning or per-subject optimization, which lack generalization or alignment.

Method: Manipulates attention matrices to separate subject and style, using anchor images for Queries/Keys and parallel copies for Values, with cross-image components and Value statistics alignment.

Result: Effectively decouples style from subject appearance, enabling consistent character generation across diverse styles while aligning with text prompts.

Conclusion: The proposed training-free method successfully achieves style alignment and subject consistency without extensive fine-tuning.

Abstract: In text-to-image models, consistent character generation is the task of
achieving text alignment while maintaining the subject's appearance across
different prompts. However, since style and appearance are often entangled, the
existing methods struggle to preserve consistent subject characteristics while
adhering to varying style prompts. Current approaches for consistent
text-to-image generation typically rely on large-scale fine-tuning on curated
image sets or per-subject optimization, which either fail to generalize across
prompts or do not align well with textual descriptions. Meanwhile,
training-free methods often fail to maintain subject consistency across
different styles. In this work, we introduce a training-free method that
achieves both style alignment and subject consistency. The attention matrices
are manipulated such that Queries and Keys are obtained from the anchor
image(s) that are used to define the subject, while the Values are imported
from a parallel copy that is not subject-anchored. Additionally, cross-image
components are added to the self-attention mechanism by expanding the Key and
Value matrices. To do without shifting from the target style, we align the
statistics of the Value matrices. As is demonstrated in a comprehensive battery
of qualitative and quantitative experiments, our method effectively decouples
style from subject appearance and enables faithful generation of text-aligned
images with consistent characters across diverse styles.

</details>


### [281] [Incorporating Flexible Image Conditioning into Text-to-Video Diffusion Models without Training](https://arxiv.org/pdf/2505.20629)
*Bolin Lai, Sangmin Lee, Xu Cao, Xiang Li, James M. Rehg*

Main category: cs.CV

TL;DR: FlexTI2V is a training-free method for text-image-to-video generation, enabling flexible visual conditioning without costly finetuning.


<details>
  <summary>Details</summary>
Motivation: Existing methods for TI2V generation require resource-intensive finetuning and are limited to predefined conditions.

Method: FlexTI2V inverts images to noisy latent representations and uses random patch swapping during denoising to integrate visual features dynamically.

Result: The method outperforms previous training-free approaches and balances creativity and fidelity effectively.

Conclusion: FlexTI2V offers a scalable and efficient solution for TI2V generation with flexible visual conditioning.

Abstract: Text-image-to-video (TI2V) generation is a critical problem for controllable
video generation using both semantic and visual conditions. Most existing
methods typically add visual conditions to text-to-video (T2V) foundation
models by finetuning, which is costly in resources and only limited to a few
predefined conditioning settings. To tackle this issue, we introduce a unified
formulation for TI2V generation with flexible visual conditioning. Furthermore,
we propose an innovative training-free approach, dubbed FlexTI2V, that can
condition T2V foundation models on an arbitrary amount of images at arbitrary
positions. Specifically, we firstly invert the condition images to noisy
representation in a latent space. Then, in the denoising process of T2V models,
our method uses a novel random patch swapping strategy to incorporate visual
features into video representations through local image patches. To balance
creativity and fidelity, we use a dynamic control mechanism to adjust the
strength of visual conditioning to each video frame. Extensive experiments
validate that our method surpasses previous training-free image conditioning
methods by a notable margin. We also show more insights of our method by
detailed ablation study and analysis.

</details>


### [282] [Structure-Accurate Medical Image Translation via Dynamic Frequency Balance and Knowledge Guidance](https://arxiv.org/pdf/2504.09441)
*Jiahua Xu, Dawei Zhou, Lei Hu, Zaiyi Liu, Nannan Wang, Xinbo Gao*

Main category: cs.CV

TL;DR: A novel method using dynamic frequency balance and knowledge guidance improves medical image synthesis by addressing anatomical distortion issues.


<details>
  <summary>Details</summary>
Motivation: Existing diffusion models for medical image synthesis suffer from anatomical structure distortion due to imbalanced frequency handling.

Method: Uses wavelet transform for feature decomposition, dynamic frequency balance for adaptive adjustment, and knowledge-guided fusion of clinical prior knowledge.

Result: Achieves significant improvements in qualitative and quantitative assessments across multiple datasets.

Conclusion: The proposed method effectively enhances medical image synthesis by balancing frequencies and incorporating clinical knowledge.

Abstract: Multimodal medical images play a crucial role in the precise and
comprehensive clinical diagnosis. Diffusion model is a powerful strategy to
synthesize the required medical images. However, existing approaches still
suffer from the problem of anatomical structure distortion due to the
overfitting of high-frequency information and the weakening of low-frequency
information. Thus, we propose a novel method based on dynamic frequency balance
and knowledge guidance. Specifically, we first extract the low-frequency and
high-frequency components by decomposing the critical features of the model
using wavelet transform. Then, a dynamic frequency balance module is designed
to adaptively adjust frequency for enhancing global low-frequency features and
effective high-frequency details as well as suppressing high-frequency noise.
To further overcome the challenges posed by the large differences between
different medical modalities, we construct a knowledge-guided mechanism that
fuses the prior clinical knowledge from a visual language model with visual
features, to facilitate the generation of accurate anatomical structures.
Experimental evaluations on multiple datasets show the proposed method achieves
significant improvements in qualitative and quantitative assessments, verifying
its effectiveness and superiority.

</details>


### [283] [TrustSkin: A Fairness Pipeline for Trustworthy Facial Affect Analysis Across Skin Tone](https://arxiv.org/pdf/2505.20637)
*Ana M. Cabanas, Alma Pedro, Domingo Mery*

Main category: cs.CV

TL;DR: The study compares two skin tone classification methods (ITA and $H^*$-$L^*$) for fairness in facial affect analysis, revealing underrepresentation of dark skin tones and fairness disparities. The $H^*$-$L^*$ method is more consistent, while ITA is sensitive to lighting. A fairness-aware pipeline is proposed for future work.


<details>
  <summary>Details</summary>
Motivation: To assess how facial affect analysis (FAA) systems perform across demographic groups, focusing on skin tone classification methods and their impact on fairness.

Method: Comparison of ITA and $H^*$-$L^*$ skin tone classification methods using AffectNet and a MobileNet-based model, evaluating fairness metrics (F1-score, TPR) and model attention patterns (Grad-CAM).

Result: Underrepresentation of dark skin tones (~2%) and fairness disparities (F1-score up to 0.08, TPR up to 0.11). $H^*$-$L^*$ provides more consistent subgrouping and better diagnostics.

Conclusion: Skin tone measurement choice affects fairness assessment; ITA may overlook disparities for darker-skinned individuals. A fairness-aware pipeline is proposed for mitigation.

Abstract: Understanding how facial affect analysis (FAA) systems perform across
different demographic groups requires reliable measurement of sensitive
attributes such as ancestry, often approximated by skin tone, which itself is
highly influenced by lighting conditions. This study compares two objective
skin tone classification methods: the widely used Individual Typology Angle
(ITA) and a perceptually grounded alternative based on Lightness ($L^*$) and
Hue ($H^*$). Using AffectNet and a MobileNet-based model, we assess fairness
across skin tone groups defined by each method. Results reveal a severe
underrepresentation of dark skin tones ($\sim 2 \%$), alongside fairness
disparities in F1-score (up to 0.08) and TPR (up to 0.11) across groups. While
ITA shows limitations due to its sensitivity to lighting, the $H^*$-$L^*$
method yields more consistent subgrouping and enables clearer diagnostics
through metrics such as Equal Opportunity. Grad-CAM analysis further highlights
differences in model attention patterns by skin tone, suggesting variation in
feature encoding. To support future mitigation efforts, we also propose a
modular fairness-aware pipeline that integrates perceptual skin tone
estimation, model interpretability, and fairness evaluation. These findings
emphasize the relevance of skin tone measurement choices in fairness assessment
and suggest that ITA-based evaluations may overlook disparities affecting
darker-skinned individuals.

</details>


### [284] [Efficient LiDAR Reflectance Compression via Scanning Serialization](https://arxiv.org/pdf/2505.09433)
*Jiahao Zhu, Kang You, Dandan Ding, Zhan Ma*

Main category: cs.CV

TL;DR: SerLiC is a neural compression framework for LiDAR reflectance data, achieving significant volume reduction and efficiency.


<details>
  <summary>Details</summary>
Motivation: LiDAR reflectance attributes are underexplored in neural compression, despite their importance for downstream tasks.

Method: SerLiC serializes LiDAR point clouds into 1D sequences, tokenizes points with contextual data, and uses Mamba for efficient sequential modeling.

Result: SerLiC reduces reflectance data volume by over 2x, outperforming state-of-the-art methods by 22% with only 2% of parameters. A lightweight version achieves >10 fps.

Conclusion: SerLiC effectively compresses LiDAR reflectance data, offering practical benefits for real-world applications.

Abstract: Reflectance attributes in LiDAR point clouds provide essential information
for downstream tasks but remain underexplored in neural compression methods. To
address this, we introduce SerLiC, a serialization-based neural compression
framework to fully exploit the intrinsic characteristics of LiDAR reflectance.
SerLiC first transforms 3D LiDAR point clouds into 1D sequences via scan-order
serialization, offering a device-centric perspective for reflectance analysis.
Each point is then tokenized into a contextual representation comprising its
sensor scanning index, radial distance, and prior reflectance, for effective
dependencies exploration. For efficient sequential modeling, Mamba is
incorporated with a dual parallelization scheme, enabling simultaneous
autoregressive dependency capture and fast processing. Extensive experiments
demonstrate that SerLiC attains over 2x volume reduction against the original
reflectance data, outperforming the state-of-the-art method by up to 22%
reduction of compressed bits while using only 2% of its parameters. Moreover, a
lightweight version of SerLiC achieves > 10 fps (frames per second) with just
111K parameters, which is attractive for real-world applications.

</details>


### [285] [Open-Det: An Efficient Learning Framework for Open-Ended Detection](https://arxiv.org/pdf/2505.20639)
*Guiping Cao, Tao Wang, Wenjian Huang, Xiangyuan Lan, Jianguo Zhang, Dongmei Jiang*

Main category: cs.CV

TL;DR: Open-Det is a new framework for Open-Ended object Detection (OED) that improves efficiency and performance over existing models like GenerateU, using less data and resources.


<details>
  <summary>Details</summary>
Motivation: Existing OED models require large datasets, have slow convergence, and limited performance, prompting the need for a more efficient solution.

Method: Open-Det reconstructs the Object Detector and Object Name Generator, introduces a Vision-Language Aligner with alignment mechanisms, and uses a Prompts Distiller and Masked Alignment Loss for efficient training.

Result: Open-Det outperforms GenerateU with only 1.5% of training data, 20.8% of epochs, and fewer GPU resources, achieving +1.0% APr.

Conclusion: Open-Det is a highly efficient and effective solution for OED, addressing key limitations of existing models.

Abstract: Open-Ended object Detection (OED) is a novel and challenging task that
detects objects and generates their category names in a free-form manner,
without requiring additional vocabularies during inference. However, the
existing OED models, such as GenerateU, require large-scale datasets for
training, suffer from slow convergence, and exhibit limited performance. To
address these issues, we present a novel and efficient Open-Det framework,
consisting of four collaborative parts. Specifically, Open-Det accelerates
model training in both the bounding box and object name generation process by
reconstructing the Object Detector and the Object Name Generator. To bridge the
semantic gap between Vision and Language modalities, we propose a
Vision-Language Aligner with V-to-L and L-to-V alignment mechanisms,
incorporating with the Prompts Distiller to transfer knowledge from the VLM
into VL-prompts, enabling accurate object name generation for the LLM. In
addition, we design a Masked Alignment Loss to eliminate contradictory
supervision and introduce a Joint Loss to enhance classification, resulting in
more efficient training. Compared to GenerateU, Open-Det, using only 1.5% of
the training data (0.077M vs. 5.077M), 20.8% of the training epochs (31 vs.
149), and fewer GPU resources (4 V100 vs. 16 A100), achieves even higher
performance (+1.0% in APr). The source codes are available at:
https://github.com/Med-Process/Open-Det.

</details>


### [286] [IndustryEQA: Pushing the Frontiers of Embodied Question Answering in Industrial Scenarios](https://arxiv.org/pdf/2505.20640)
*Yifan Li, Yuhang Chen, Anh Dao, Lichi Li, Zhongyi Cai, Zhen Tan, Tianlong Chen, Yu Kong*

Main category: cs.CV

TL;DR: IndustryEQA is a new benchmark for evaluating embodied agents in safety-critical warehouse scenarios, addressing gaps in existing EQA benchmarks focused on household environments.


<details>
  <summary>Details</summary>
Motivation: Existing EQA benchmarks lack focus on industrial safety and reasoning, limiting agent readiness for real-world industrial applications.

Method: IndustryEQA uses NVIDIA Isaac Sim for high-fidelity episodic memory videos, featuring industrial assets, human agents, and hazardous situations. It includes 1,344 annotated question-answer pairs and a reasoning evaluation framework.

Result: The benchmark provides rich annotations across six categories and proposes baseline models for assessing perception and reasoning in industrial settings.

Conclusion: IndustryEQA aims to advance EQA research towards robust, safety-aware agents for industrial environments, with benchmark and codes publicly available.

Abstract: Existing Embodied Question Answering (EQA) benchmarks primarily focus on
household environments, often overlooking safety-critical aspects and reasoning
processes pertinent to industrial settings. This drawback limits the evaluation
of agent readiness for real-world industrial applications. To bridge this, we
introduce IndustryEQA, the first benchmark dedicated to evaluating embodied
agent capabilities within safety-critical warehouse scenarios. Built upon the
NVIDIA Isaac Sim platform, IndustryEQA provides high-fidelity episodic memory
videos featuring diverse industrial assets, dynamic human agents, and carefully
designed hazardous situations inspired by real-world safety guidelines. The
benchmark includes rich annotations covering six categories: equipment safety,
human safety, object recognition, attribute recognition, temporal
understanding, and spatial understanding. Besides, it also provides extra
reasoning evaluation based on these categories. Specifically, it comprises 971
question-answer pairs generated from small warehouse and 373 pairs from large
ones, incorporating scenarios with and without human. We further propose a
comprehensive evaluation framework, including various baseline models, to
assess their general perception and reasoning abilities in industrial
environments. IndustryEQA aims to steer EQA research towards developing more
robust, safety-aware, and practically applicable embodied agents for complex
industrial environments. Benchmark and codes are available.

</details>


### [287] [See through the Dark: Learning Illumination-affined Representations for Nighttime Occupancy Prediction](https://arxiv.org/pdf/2505.20641)
*Yuan Wu, Zhiqiang Yan, Yigong Zhang, Xiang Li, ian Yang*

Main category: cs.CV

TL;DR: LIAR is a framework for nighttime occupancy prediction, using illumination-aware techniques to enhance low-light images and improve semantic understanding.


<details>
  <summary>Details</summary>
Motivation: Existing vision-based methods perform poorly in nighttime due to visibility and lighting challenges. LIAR addresses these issues by leveraging illumination priors.

Method: LIAR uses Selective Low-light Image Enhancement (SLLIE) and two illumination-aware components (2D-IGS and 3D-IDP) to handle underexposure and overexposure.

Result: LIAR outperforms existing methods in nighttime scenarios, as shown by experiments on real and synthetic datasets.

Conclusion: LIAR effectively improves nighttime occupancy prediction by addressing illumination challenges, with code and models publicly available.

Abstract: Occupancy prediction aims to estimate the 3D spatial distribution of occupied
regions along with their corresponding semantic labels. Existing vision-based
methods perform well on daytime benchmarks but struggle in nighttime scenarios
due to limited visibility and challenging lighting conditions. To address these
challenges, we propose \textbf{LIAR}, a novel framework that learns
illumination-affined representations. LIAR first introduces Selective Low-light
Image Enhancement (SLLIE), which leverages the illumination priors from daytime
scenes to adaptively determine whether a nighttime image is genuinely dark or
sufficiently well-lit, enabling more targeted global enhancement. Building on
the illumination maps generated by SLLIE, LIAR further incorporates two
illumination-aware components: 2D Illumination-guided Sampling (2D-IGS) and 3D
Illumination-driven Projection (3D-IDP), to respectively tackle local
underexposure and overexposure. Specifically, 2D-IGS modulates feature sampling
positions according to illumination maps, assigning larger offsets to darker
regions and smaller ones to brighter regions, thereby alleviating feature
degradation in underexposed areas. Subsequently, 3D-IDP enhances semantic
understanding in overexposed regions by constructing illumination intensity
fields and supplying refined residual queries to the BEV context refinement
process. Extensive experiments on both real and synthetic datasets demonstrate
the superior performance of LIAR under challenging nighttime scenarios. The
source code and pretrained models are available
\href{https://github.com/yanzq95/LIAR}{here}.

</details>


### [288] [HCQA-1.5 @ Ego4D EgoSchema Challenge 2025](https://arxiv.org/pdf/2505.20644)
*Haoyu Zhang, Yisen Feng, Qiaohui Chu, Meng Liu, Weili Guan, Yaowei Wang, Liqiang Nie*

Main category: cs.CV

TL;DR: A method extending HCQA for egocentric video QA, using multi-source aggregation and confidence filtering, achieves 77% accuracy on EgoSchema.


<details>
  <summary>Details</summary>
Motivation: Improve reliability of answer prediction in egocentric video question answering.

Method: Extends HCQA with multi-source aggregation, confidence filtering, and fine-grained reasoning for low-confidence cases.

Result: 77% accuracy on EgoSchema blind test set, outperforming last year's winner.

Conclusion: The proposed method is effective, achieving high accuracy and outperforming competitors.

Abstract: In this report, we present the method that achieves third place for Ego4D
EgoSchema Challenge in CVPR 2025. To improve the reliability of answer
prediction in egocentric video question answering, we propose an effective
extension to the previously proposed HCQA framework. Our approach introduces a
multi-source aggregation strategy to generate diverse predictions, followed by
a confidence-based filtering mechanism that selects high-confidence answers
directly. For low-confidence cases, we incorporate a fine-grained reasoning
module that performs additional visual and contextual analysis to refine the
predictions. Evaluated on the EgoSchema blind test set, our method achieves 77%
accuracy on over 5,000 human-curated multiple-choice questions, outperforming
last year's winning solution and the majority of participating teams. Our code
will be added at https://github.com/Hyu-Zhang/HCQA.

</details>


### [289] [Scan-and-Print: Patch-level Data Summarization and Augmentation for Content-aware Layout Generation in Poster Design](https://arxiv.org/pdf/2505.20649)
*HsiaoYuan Hsu, Yuxin Peng*

Main category: cs.CV

TL;DR: The paper introduces Scan-and-Print, a patch-level data summarization and augmentation method for AI-empowered poster design, improving efficiency and layout quality.


<details>
  <summary>Details</summary>
Motivation: Existing methods for content-aware layout generation in poster design suffer from high computational costs and poor generalization due to excessive parameters.

Method: Proposes Scan-and-Print: a scan procedure selects suitable patches for element placement, and a print procedure synthesizes new samples by mixing patches and vertices. Introduces vertex-based layout representation.

Result: Achieves state-of-the-art layout quality while reducing computational bottleneck by 95.2%.

Conclusion: Scan-and-Print effectively addresses efficiency and generalization issues in AI-driven poster design.

Abstract: In AI-empowered poster design, content-aware layout generation is crucial for
the on-image arrangement of visual-textual elements, e.g., logo, text, and
underlay. To perceive the background images, existing work demanded a high
parameter count that far exceeds the size of available training data, which has
impeded the model's real-time performance and generalization ability. To
address these challenges, we proposed a patch-level data summarization and
augmentation approach, vividly named Scan-and-Print. Specifically, the scan
procedure selects only the patches suitable for placing element vertices to
perform fine-grained perception efficiently. Then, the print procedure mixes up
the patches and vertices across two image-layout pairs to synthesize over 100%
new samples in each epoch while preserving their plausibility. Besides, to
facilitate the vertex-level operations, a vertex-based layout representation is
introduced. Extensive experimental results on widely used benchmarks
demonstrated that Scan-and-Print can generate visually appealing layouts with
state-of-the-art quality while dramatically reducing computational bottleneck
by 95.2%.

</details>


### [290] [RoGA: Towards Generalizable Deepfake Detection through Robust Gradient Alignment](https://arxiv.org/pdf/2505.20653)
*Lingyu Qiu, Ke Jiang, Xiaoyang Tan*

Main category: cs.CV

TL;DR: A novel gradient alignment strategy for deepfake detection improves domain generalization by aligning updates with ERM, outperforming existing methods.


<details>
  <summary>Details</summary>
Motivation: Existing domain generalization methods for deepfake detection often degrade performance by over-regularizing, hindering ERM optimization.

Method: Proposes a learning objective that aligns generalization gradient updates with ERM updates, using parameter perturbations to enhance robustness.

Result: Outperforms state-of-the-art domain generalization techniques on multiple deepfake detection datasets.

Conclusion: The gradient alignment strategy effectively preserves domain-invariant features, improving robustness without additional regularization.

Abstract: Recent advancements in domain generalization for deepfake detection have
attracted significant attention, with previous methods often incorporating
additional modules to prevent overfitting to domain-specific patterns. However,
such regularization can hinder the optimization of the empirical risk
minimization (ERM) objective, ultimately degrading model performance. In this
paper, we propose a novel learning objective that aligns generalization
gradient updates with ERM gradient updates. The key innovation is the
application of perturbations to model parameters, aligning the ascending points
across domains, which specifically enhances the robustness of deepfake
detection models to domain shifts. This approach effectively preserves
domain-invariant features while managing domain-specific characteristics,
without introducing additional regularization. Experimental results on multiple
challenging deepfake detection datasets demonstrate that our gradient alignment
strategy outperforms state-of-the-art domain generalization techniques,
confirming the efficacy of our method. The code is available at
https://github.com/Lynn0925/RoGA.

</details>


### [291] [Photography Perspective Composition: Towards Aesthetic Perspective Recommendation](https://arxiv.org/pdf/2505.20655)
*Lujian Yao, Siming Zheng, Xinbin Yuan, Zhuoxuan Cai, Pu Wu, Jinwei Chen, Bo Li, Peng-Tao Jiang*

Main category: cs.CV

TL;DR: The paper introduces Photography Perspective Composition (PPC), a 3D recomposition method beyond traditional 2D cropping, addressing dataset scarcity and quality assessment challenges with automated dataset building, video demonstrations, and a human-based quality model.


<details>
  <summary>Details</summary>
Motivation: Traditional 2D cropping methods fail for poorly arranged scenes, while professional photographers use 3D perspective adjustments for better composition. The paper aims to formalize this practice.

Method: Proposes PPC with three contributions: (1) automated dataset creation from expert photos, (2) video generation showing perspective transformations, and (3) a Perspective Quality Assessment (PQA) model based on human performance.

Result: The approach provides a concise, user-friendly method for improving composition without extra prompts or camera trajectories.

Conclusion: PPC extends traditional methods, offering practical tools for users to enhance photographic composition through perspective adjustment.

Abstract: Traditional photography composition approaches are dominated by 2D
cropping-based methods. However, these methods fall short when scenes contain
poorly arranged subjects. Professional photographers often employ perspective
adjustment as a form of 3D recomposition, modifying the projected 2D
relationships between subjects while maintaining their actual spatial positions
to achieve better compositional balance. Inspired by this artistic practice, we
propose photography perspective composition (PPC), extending beyond traditional
cropping-based methods. However, implementing the PPC faces significant
challenges: the scarcity of perspective transformation datasets and undefined
assessment criteria for perspective quality. To address these challenges, we
present three key contributions: (1) An automated framework for building PPC
datasets through expert photographs. (2) A video generation approach that
demonstrates the transformation process from suboptimal to optimal
perspectives. (3) A perspective quality assessment (PQA) model constructed
based on human performance. Our approach is concise and requires no additional
prompt instructions or camera trajectories, helping and guiding ordinary users
to enhance their composition skills.

</details>


### [292] [DriveRX: A Vision-Language Reasoning Model for Cross-Task Autonomous Driving](https://arxiv.org/pdf/2505.20665)
*Muxi Diao, Lele Yang, Hongbo Yin, Zhexu Wang, Yejie Wang, Daxin Tian, Kongming Liang, Zhanyu Ma*

Main category: cs.CV

TL;DR: AutoDriveRL is a unified framework for autonomous driving, treating it as a structured reasoning process across four tasks, using vision-language models and task-specific rewards. DriveRX, its VLM, outperforms GPT-4o in behavior reasoning.


<details>
  <summary>Details</summary>
Motivation: Conventional end-to-end models lack structured reasoning, and existing VLMs for driving tasks are limited by isolated modules and static supervision.

Method: AutoDriveRL models each driving task as a vision-language QA problem, optimized with task-specific rewards. DriveRX is trained within this framework for real-time decision-making.

Result: DriveRX outperforms GPT-4o in behavior reasoning and shows robustness in complex or corrupted conditions.

Conclusion: AutoDriveRL and DriveRX advance autonomous driving by enabling structured, multi-stage reasoning. The framework and model will be released for future research.

Abstract: Autonomous driving requires real-time, robust reasoning across perception,
prediction, planning, and behavior. However, conventional end-to-end models
fail to generalize in complex scenarios due to the lack of structured
reasoning. Recent vision-language models (VLMs) have been applied to driving
tasks, but they typically rely on isolated modules and static supervision,
limiting their ability to support multi-stage decision-making. We present
AutoDriveRL, a unified training framework that formulates autonomous driving as
a structured reasoning process over four core tasks. Each task is independently
modeled as a vision-language question-answering problem and optimized using
task-specific reward models, enabling fine-grained reinforcement signals at
different reasoning stages. Within this framework, we train DriveRX, a
cross-task reasoning VLM designed for real-time decision-making. DriveRX
achieves strong performance on a public benchmark, outperforming GPT-4o in
behavior reasoning and demonstrating robustness under complex or corrupted
driving conditions. Our analysis further highlights the impact of vision
encoder design and reward-guided reasoning compression. We will release the
AutoDriveRL framework and the DriveRX model to support future research.

</details>


### [293] [Contrastive Desensitization Learning for Cross Domain Face Forgery Detection](https://arxiv.org/pdf/2505.20675)
*Lingyu Qiu, Ke Jiang, Xiaoyang Tan*

Main category: cs.CV

TL;DR: Proposes a cross-domain face forgery detection method (CDN) with low false positives and robustness to unseen forgery methods.


<details>
  <summary>Details</summary>
Motivation: Existing methods have high false positive rates, disrupting usability.

Method: Uses a Contrastive Desensitization Network (CDN) with a robust desensitization algorithm to learn domain characteristics from genuine face image pairs.

Result: Achieves lower false alarm rates and improved detection accuracy on benchmark datasets.

Conclusion: CDN is effective for cross-domain forgery detection with theoretical robustness guarantees.

Abstract: In this paper, we propose a new cross-domain face forgery detection method
that is insensitive to different and possibly unseen forgery methods while
ensuring an acceptable low false positive rate. Although existing face forgery
detection methods are applicable to multiple domains to some degree, they often
come with a high false positive rate, which can greatly disrupt the usability
of the system. To address this issue, we propose an Contrastive Desensitization
Network (CDN) based on a robust desensitization algorithm, which captures the
essential domain characteristics through learning them from domain
transformation over pairs of genuine face images. One advantage of CDN lies in
that the learnt face representation is theoretical justified with regard to the
its robustness against the domain changes. Extensive experiments over
large-scale benchmark datasets demonstrate that our method achieves a much
lower false alarm rate with improved detection accuracy compared to several
state-of-the-art methods.

</details>


### [294] [Supervised Contrastive Learning for Ordinal Engagement Measurement](https://arxiv.org/pdf/2505.20676)
*Sadaf Safa, Ali Abedi, Shehroz S. Khan*

Main category: cs.CV

TL;DR: A novel method using supervised contrastive learning for ordinal classification of student engagement in virtual learning environments, addressing class imbalance and ordered engagement levels.


<details>
  <summary>Details</summary>
Motivation: Automated engagement measurement helps instructors monitor participation and adapt teaching strategies, but faces challenges like class imbalance and treating engagement as ordered levels.

Method: Utilizes supervised contrastive learning with sequential classifiers, extracting affective/behavioral features from videos and applying time-series data augmentation.

Result: Evaluated on DAiSEE dataset, the method shows robust performance in classifying engagement levels.

Conclusion: The approach significantly contributes to understanding and improving student engagement in virtual learning.

Abstract: Student engagement plays a crucial role in the successful delivery of
educational programs. Automated engagement measurement helps instructors
monitor student participation, identify disengagement, and adapt their teaching
strategies to enhance learning outcomes effectively. This paper identifies two
key challenges in this problem: class imbalance and incorporating order into
engagement levels rather than treating it as mere categories. Then, a novel
approach to video-based student engagement measurement in virtual learning
environments is proposed that utilizes supervised contrastive learning for
ordinal classification of engagement. Various affective and behavioral features
are extracted from video samples and utilized to train ordinal classifiers
within a supervised contrastive learning framework (with a sequential
classifier as the encoder). A key step involves the application of diverse
time-series data augmentation techniques to these feature vectors, enhancing
model training. The effectiveness of the proposed method was evaluated using a
publicly available dataset for engagement measurement, DAiSEE, containing
videos of students who participated in virtual learning programs. The results
demonstrate the robust ability of the proposed method for the classification of
the engagement level. This approach promises a significant contribution to
understanding and enhancing student engagement in virtual learning
environments.

</details>


### [295] [Continual Learning on CLIP via Incremental Prompt Tuning with Intrinsic Textual Anchors](https://arxiv.org/pdf/2505.20680)
*Haodong Lu, Xinyu Zhang, Kristen Moore, Jason Xue, Lina Yao, Anton van den Hengel, Dong Gong*

Main category: cs.CV

TL;DR: Proposes Textual Prototype-guided Prompt Tuning (TPPT) for CLIP in continual learning, leveraging multi-modal embeddings and stable textual anchors to reduce forgetting and improve adaptation.


<details>
  <summary>Details</summary>
Motivation: Existing CL methods for CLIP are overly complex and underutilize its intrinsic capabilities. TPPT aims to simplify and optimize continual learning by fully exploiting CLIP's structure.

Method: TPPT uses textual prototypes as stable anchors to guide visual prompt learning (TPPT-V) and jointly optimizes visual and textual prompts (TPPT-VT) with relational diversity regularization.

Result: TPPT effectively learns new knowledge while reducing forgetting, outperforming existing methods in continual learning tasks.

Conclusion: TPPT demonstrates the benefits of leveraging CLIP's intrinsic guidance for continual adaptation, offering a simpler yet more effective approach.

Abstract: Continual learning (CL) enables deep networks to acquire new knowledge while
avoiding catastrophic forgetting. The powerful generalization ability of
pre-trained models (PTMs), such as the Contrastive Language-Image Pre-training
(CLIP) model, has inspired a range of CL methods targeting new and specialized
tasks, providing rich multi-modal embeddings that support lightweight,
incremental prompt tuning. Existing methods often rely on complex designs built
upon specific assumptions, such as intricate regularization schemes for prompt
pools, specialized routing mechanisms, or multi-stage incrementations, that
introduce additional-and possibly unnecessary-complexity, underutilizing CLIP's
intrinsic capabilities. In this paper, we propose a concise CL approach for
CLIP based on incremental prompt tuning that fully exploits its multi-modal
structure and the stability of textual representations. Our method, Textual
Prototype-guided Prompt Tuning (TPPT), introduces textual prototypes not merely
as static classifiers, as in existing methods, but as stable anchors to guide
the learning of visual prompts, thereby shaping the embedding space (i.e.,
TPPT-V). We show that our bidirectional supervision strategy enables more
effective learning of new knowledge while reducing forgetting. To further close
the vision-language gap during CL, we jointly optimizes visual and textual
prompts (i.e., TPPT-VT). We also introduce a relational diversity
regularization on the textual anchors to prevent embedding space collapse and
mitigate correlated forgetting. Extensive experiments and analyses demonstrate
the effectiveness of our proposed approach, highlighting the benefits of
leveraging CLIP's intrinsic guidance for continual adaptation.

</details>


### [296] [VisAlgae 2023: A Dataset and Challenge for Algae Detection in Microscopy Images](https://arxiv.org/pdf/2505.20687)
*Mingxuan Sun, Juntao Jiang, Zhiqiang Yang, Shenao Kong, Jiamin Qi, Jianru Shang, Shuangling Luo, Wanfa Sun, Tianyi Wang, Yanqi Wang, Qixuan Wang, Tingjian Dai, Tianxiang Chen, Jinming Zhang, Xuerui Zhang, Yuepeng He, Pengcheng Fu, Qiu Guan, Shizheng Zhou, Yanbo Yu, Qigui Jiang, Teng Zhou, Liuyong Shi, Hong Yan*

Main category: cs.CV

TL;DR: The paper summarizes the VisAlgae 2023 Challenge, focusing on improving microalgae detection using computer vision, with 369 teams participating and top methods analyzed.


<details>
  <summary>Details</summary>
Motivation: Microalgae detection is challenging due to their diversity in size and conditions, necessitating advanced methods for ecological and economic applications.

Method: The challenge involved 1000 images across six microalgae classes, with tasks like detecting small targets and handling motion blur. Top 10 methods were analyzed.

Result: The challenge highlighted effective techniques for high-throughput microalgae detection, with top-performing methods providing insights into accuracy improvements.

Conclusion: The intersection of algae research and computer vision shows promise for ecological and technological advancements, with the dataset publicly available.

Abstract: Microalgae, vital for ecological balance and economic sectors, present
challenges in detection due to their diverse sizes and conditions. This paper
summarizes the second "Vision Meets Algae" (VisAlgae 2023) Challenge, aiming to
enhance high-throughput microalgae cell detection. The challenge, which
attracted 369 participating teams, includes a dataset of 1000 images across six
classes, featuring microalgae of varying sizes and distinct features.
Participants faced tasks such as detecting small targets, handling motion blur,
and complex backgrounds. The top 10 methods, outlined here, offer insights into
overcoming these challenges and maximizing detection accuracy. This
intersection of algae research and computer vision offers promise for
ecological understanding and technological advancement. The dataset can be
accessed at: https://github.com/juntaoJianggavin/Visalgae2023/.

</details>


### [297] [Temporal Saliency-Guided Distillation: A Scalable Framework for Distilling Video Datasets](https://arxiv.org/pdf/2505.20694)
*Xulin Gu, Xinhao Zhong, Zhixing Wei, Yimin Zhou, Shuoyang Sun, Bin Chen, Hongpeng Wang, Yuan Luo*

Main category: cs.CV

TL;DR: A novel uni-level video dataset distillation framework optimizes synthetic videos using a pre-trained model, with temporal saliency-guided filtering to preserve motion and reduce redundancy, achieving state-of-the-art performance.


<details>
  <summary>Details</summary>
Motivation: Extending dataset distillation to videos is challenging due to high dimensionality and temporal complexity, with existing methods being computationally costly and ineffective at preserving dynamics.

Method: Proposes a uni-level framework with temporal saliency-guided filtering, leveraging inter-frame differences to retain informative cues and reduce redundancy.

Result: Achieves state-of-the-art performance on video benchmarks, bridging the gap between real and distilled video data.

Conclusion: The method offers a scalable solution for video dataset compression by effectively preserving temporal dynamics and reducing computational costs.

Abstract: Dataset distillation (DD) has emerged as a powerful paradigm for dataset
compression, enabling the synthesis of compact surrogate datasets that
approximate the training utility of large-scale ones. While significant
progress has been achieved in distilling image datasets, extending DD to the
video domain remains challenging due to the high dimensionality and temporal
complexity inherent in video data. Existing video distillation (VD) methods
often suffer from excessive computational costs and struggle to preserve
temporal dynamics, as na\"ive extensions of image-based approaches typically
lead to degraded performance. In this paper, we propose a novel uni-level video
dataset distillation framework that directly optimizes synthetic videos with
respect to a pre-trained model. To address temporal redundancy and enhance
motion preservation, we introduce a temporal saliency-guided filtering
mechanism that leverages inter-frame differences to guide the distillation
process, encouraging the retention of informative temporal cues while
suppressing frame-level redundancy. Extensive experiments on standard video
benchmarks demonstrate that our method achieves state-of-the-art performance,
bridging the gap between real and distilled video data and offering a scalable
solution for video dataset compression.

</details>


### [298] [Beyond Entropy: Region Confidence Proxy for Wild Test-Time Adaptation](https://arxiv.org/pdf/2505.20704)
*Zixuan Hu, Yichun Hu, Xiaotong Li, Shixiang Tang, Ling-Yu Duan*

Main category: cs.CV

TL;DR: ReCAP introduces a region-integrated method for Wild Test-Time Adaptation (WTTA), addressing noisy optimization dynamics and computational challenges with a probabilistic region modeling scheme and asymptotic approximation.


<details>
  <summary>Details</summary>
Motivation: Existing WTTA approaches focus on sample selection but neglect optimization inefficiencies, particularly in entropy minimization. Region confidence is identified as a better alternative, but direct optimization is impractical.

Method: Proposes ReCAP: a probabilistic region modeling scheme and a finite-to-infinite asymptotic approximation to make region confidence tractable and upper-bounded.

Result: ReCAP outperforms existing methods in various datasets and wild scenarios, demonstrating superior adaptation efficiency.

Conclusion: ReCAP effectively addresses WTTA challenges by leveraging region confidence dynamics, offering a concise and efficient solution.

Abstract: Wild Test-Time Adaptation (WTTA) is proposed to adapt a source model to
unseen domains under extreme data scarcity and multiple shifts. Previous
approaches mainly focused on sample selection strategies, while overlooking the
fundamental problem on underlying optimization. Initially, we critically
analyze the widely-adopted entropy minimization framework in WTTA and uncover
its significant limitations in noisy optimization dynamics that substantially
hinder adaptation efficiency. Through our analysis, we identify region
confidence as a superior alternative to traditional entropy, however, its
direct optimization remains computationally prohibitive for real-time
applications. In this paper, we introduce a novel region-integrated method
ReCAP that bypasses the lengthy process. Specifically, we propose a
probabilistic region modeling scheme that flexibly captures semantic changes in
embedding space. Subsequently, we develop a finite-to-infinite asymptotic
approximation that transforms the intractable region confidence into a
tractable and upper-bounded proxy. These innovations significantly unlock the
overlooked potential dynamics in local region in a concise solution. Our
extensive experiments demonstrate the consistent superiority of ReCAP over
existing methods across various datasets and wild scenarios.

</details>


### [299] [Hierarchical Instruction-aware Embodied Visual Tracking](https://arxiv.org/pdf/2505.20710)
*Kui Wu, Hao Chen, Churan Wang, Fakhri Karray, Zhoujun Li, Yizhou Wang, Fangwei Zhong*

Main category: cs.CV

TL;DR: HIEVT bridges high-level user instructions and low-level agent actions in visual tracking using spatial goals, outperforming existing models in robustness and generalizability.


<details>
  <summary>Details</summary>
Motivation: The gap between high-level instructions and low-level actions in embodied visual tracking poses challenges for reinforcement learning models, with existing language models lacking speed or generalizability.

Method: HIEVT uses an LLM-based Semantic-Spatial Goal Aligner to translate instructions into spatial goals and an RL-based Adaptive Goal-Aligned Policy for action generation.

Result: HIEVT demonstrates robustness and generalizability across diverse environments, target dynamics, and complex instructions.

Conclusion: HIEVT effectively addresses the UC-EVT challenge, offering a scalable and deployable solution for real-world applications.

Abstract: User-Centric Embodied Visual Tracking (UC-EVT) presents a novel challenge for
reinforcement learning-based models due to the substantial gap between
high-level user instructions and low-level agent actions. While recent
advancements in language models (e.g., LLMs, VLMs, VLAs) have improved
instruction comprehension, these models face critical limitations in either
inference speed (LLMs, VLMs) or generalizability (VLAs) for UC-EVT tasks. To
address these challenges, we propose \textbf{Hierarchical Instruction-aware
Embodied Visual Tracking (HIEVT)} agent, which bridges instruction
comprehension and action generation using \textit{spatial goals} as
intermediaries. HIEVT first introduces \textit{LLM-based Semantic-Spatial Goal
Aligner} to translate diverse human instructions into spatial goals that
directly annotate the desired spatial position. Then the \textit{RL-based
Adaptive Goal-Aligned Policy}, a general offline policy, enables the tracker to
position the target as specified by the spatial goal. To benchmark UC-EVT
tasks, we collect over ten million trajectories for training and evaluate
across one seen environment and nine unseen challenging environments. Extensive
experiments and real-world deployments demonstrate the robustness and
generalizability of HIEVT across diverse environments, varying target dynamics,
and complex instruction combinations. The complete project is available at
https://sites.google.com/view/hievt.

</details>


### [300] [MUSEG: Reinforcing Video Temporal Understanding via Timestamp-Aware Multi-Segment Grounding](https://arxiv.org/pdf/2505.20715)
*Fuwen Luo, Shengfeng Lou, Chi Chen, Ziyue Wang, Chenliang Li, Weizhou Shen, Jiyue Guo, Peng Li, Ming Yan, Ji Zhang, Fei Huang, Yang Liu*

Main category: cs.CV

TL;DR: MUSEG is a novel RL-based method for enhancing temporal understanding in MLLMs by aligning queries with multiple video segments, outperforming existing methods.


<details>
  <summary>Details</summary>
Motivation: Current MLLMs struggle with fine-grained temporal reasoning, and existing RL approaches are limited in effectiveness.

Method: MUSEG introduces timestamp-aware multi-segment grounding and a phased RL training recipe for progressive learning.

Result: MUSEG significantly outperforms existing methods on temporal grounding and video QA tasks and generalizes well.

Conclusion: MUSEG advances temporal reasoning in MLLMs and shows strong performance across diverse scenarios.

Abstract: Video temporal understanding is crucial for multimodal large language models
(MLLMs) to reason over events in videos. Despite recent advances in general
video understanding, current MLLMs still struggle with fine-grained temporal
reasoning. While reinforcement learning (RL) has been explored to address this
issue recently, existing RL approaches remain limited in effectiveness. In this
work, we propose MUSEG, a novel RL-based method that enhances temporal
understanding by introducing timestamp-aware multi-segment grounding. MUSEG
enables MLLMs to align queries with multiple relevant video segments, promoting
more comprehensive temporal reasoning. To facilitate effective learning, we
design a customized RL training recipe with phased rewards that progressively
guides the model toward temporally grounded reasoning. Extensive experiments on
temporal grounding and time-sensitive video QA tasks demonstrate that MUSEG
significantly outperforms existing methods and generalizes well across diverse
temporal understanding scenarios. View our project at
https://github.com/THUNLP-MT/MUSEG.

</details>


### [301] [VLM Can Be a Good Assistant: Enhancing Embodied Visual Tracking with Self-Improving Visual-Language Models](https://arxiv.org/pdf/2505.20718)
*Kui Wu, Shuhang Xu, Hao Chen, Churan Wang, Zhoujun Li, Yizhou Wang, Fangwei Zhong*

Main category: cs.CV

TL;DR: A self-improving framework enhances Embodied Visual Tracking (EVT) by integrating Visual-Language Models (VLMs) for failure recovery, improving success rates significantly.


<details>
  <summary>Details</summary>
Motivation: Current active visual tracking systems struggle with recovery from tracking failures, especially in dynamic environments.

Method: Combines active tracking methods with VLM reasoning, using a fast visual policy for normal tracking and activating VLM reasoning upon failure detection. Includes a memory-augmented self-reflection mechanism.

Result: Boosts success rates by 72% with RL-based methods and 220% with PID-based methods in challenging environments.

Conclusion: The framework advances EVT by integrating VLM-based reasoning for proactive failure recovery, benefiting real-world robotic applications.

Abstract: We introduce a novel self-improving framework that enhances Embodied Visual
Tracking (EVT) with Visual-Language Models (VLMs) to address the limitations of
current active visual tracking systems in recovering from tracking failure. Our
approach combines the off-the-shelf active tracking methods with VLMs'
reasoning capabilities, deploying a fast visual policy for normal tracking and
activating VLM reasoning only upon failure detection. The framework features a
memory-augmented self-reflection mechanism that enables the VLM to
progressively improve by learning from past experiences, effectively addressing
VLMs' limitations in 3D spatial reasoning. Experimental results demonstrate
significant performance improvements, with our framework boosting success rates
by $72\%$ with state-of-the-art RL-based approaches and $220\%$ with PID-based
methods in challenging environments. This work represents the first integration
of VLM-based reasoning to assist EVT agents in proactive failure recovery,
offering substantial advances for real-world robotic applications that require
continuous target monitoring in dynamic, unstructured environments. Project
website: https://sites.google.com/view/evt-recovery-assistant.

</details>


### [302] [LeDiFlow: Learned Distribution-guided Flow Matching to Accelerate Image Generation](https://arxiv.org/pdf/2505.20723)
*Pascal Zwick, Nils Friederich, Maximilian Beichter, Lennart Hilbert, Ralf Mikut, Oliver Bringmann*

Main category: cs.CV

TL;DR: LeDiFlow improves FM-based image generation by using a learned prior distribution, reducing solver steps and enhancing efficiency and quality.


<details>
  <summary>Details</summary>
Motivation: The iterative nature of Diffusion Models (DMs) and curved paths in Flow Matching (FM) hinder efficiency, requiring many inference calls.

Method: LeDiFlow introduces a learned prior distribution via an auxiliary model, enabling simpler probability paths and fewer solver steps.

Result: LeDiFlow speeds up inference by 3.75x in pixel space and improves image quality by 1.32x in CMMD.

Conclusion: LeDiFlow is a scalable, efficient method for FM-based image generation, outperforming baselines in speed and quality.

Abstract: Enhancing the efficiency of high-quality image generation using Diffusion
Models (DMs) is a significant challenge due to the iterative nature of the
process. Flow Matching (FM) is emerging as a powerful generative modeling
paradigm based on a simulation-free training objective instead of a score-based
one used in DMs. Typical FM approaches rely on a Gaussian distribution prior,
which induces curved, conditional probability paths between the prior and
target data distribution. These curved paths pose a challenge for the Ordinary
Differential Equation (ODE) solver, requiring a large number of inference calls
to the flow prediction network. To address this issue, we present Learned
Distribution-guided Flow Matching (LeDiFlow), a novel scalable method for
training FM-based image generation models using a better-suited prior
distribution learned via a regression-based auxiliary model. By initializing
the ODE solver with a prior closer to the target data distribution, LeDiFlow
enables the learning of more computationally tractable probability paths. These
paths directly translate to fewer solver steps needed for high-quality image
generation at inference time. Our method utilizes a State-Of-The-Art (SOTA)
transformer architecture combined with latent space sampling and can be trained
on a consumer workstation. We empirically demonstrate that LeDiFlow remarkably
outperforms the respective FM baselines. For instance, when operating directly
on pixels, our model accelerates inference by up to 3.75x compared to the
corresponding pixel-space baseline. Simultaneously, our latent FM model
enhances image quality on average by 1.32x in CLIP Maximum Mean Discrepancy
(CMMD) metric against its respective baseline.

</details>


### [303] [Intern-GS: Vision Model Guided Sparse-View 3D Gaussian Splatting](https://arxiv.org/pdf/2505.20729)
*Xiangyu Sun, Runnan Chen, Mingming Gong, Dong Xu, Tongliang Liu*

Main category: cs.CV

TL;DR: Intern-GS improves sparse-view scene reconstruction by leveraging vision foundation models for initialization and optimization, achieving high-quality results.


<details>
  <summary>Details</summary>
Motivation: Sparse-view reconstruction is challenging due to limited data, leading to incomplete reconstructions with existing methods.

Method: Intern-GS uses vision foundation models (e.g., DUSt3R) for dense point cloud initialization and depth/appearance prediction during optimization.

Result: Achieves state-of-the-art rendering quality on datasets like LLFF, DTU, and Tanks and Temples.

Conclusion: Intern-GS effectively addresses sparse-view limitations, enabling high-quality scene reconstruction.

Abstract: Sparse-view scene reconstruction often faces significant challenges due to
the constraints imposed by limited observational data. These limitations result
in incomplete information, leading to suboptimal reconstructions using existing
methodologies. To address this, we present Intern-GS, a novel approach that
effectively leverages rich prior knowledge from vision foundation models to
enhance the process of sparse-view Gaussian Splatting, thereby enabling
high-quality scene reconstruction. Specifically, Intern-GS utilizes vision
foundation models to guide both the initialization and the optimization process
of 3D Gaussian splatting, effectively addressing the limitations of sparse
inputs. In the initialization process, our method employs DUSt3R to generate a
dense and non-redundant gaussian point cloud. This approach significantly
alleviates the limitations encountered by traditional structure-from-motion
(SfM) methods, which often struggle under sparse-view constraints. During the
optimization process, vision foundation models predict depth and appearance for
unobserved views, refining the 3D Gaussians to compensate for missing
information in unseen regions. Extensive experiments demonstrate that Intern-GS
achieves state-of-the-art rendering quality across diverse datasets, including
both forward-facing and large-scale scenes, such as LLFF, DTU, and Tanks and
Temples.

</details>


### [304] [MVTN: Learning Multi-View Transformations for 3D Understanding](https://arxiv.org/pdf/2212.13462)
*Abdullah Hamdi, Faisal AlZahrani, Silvio Giancola, Bernard Ghanem*

Main category: cs.CV

TL;DR: MVTN introduces a Multi-View Transformation Network to dynamically learn optimal viewpoints for 3D shape recognition, improving classification and retrieval performance.


<details>
  <summary>Details</summary>
Motivation: Current multi-view techniques use fixed viewpoints, limiting adaptability. MVTN aims to learn optimal viewpoints dynamically.

Method: MVTN uses differentiable rendering to determine viewpoints and integrates into an adaptive multi-view pipeline for 3D meshes and point clouds.

Result: Achieves state-of-the-art performance in 3D classification and retrieval on benchmarks like ModelNet40, with improved robustness to occlusion.

Conclusion: MVTN advances multi-view techniques by learning viewpoints, supported by the release of MVTorch for further research.

Abstract: Multi-view projection techniques have shown themselves to be highly effective
in achieving top-performing results in the recognition of 3D shapes. These
methods involve learning how to combine information from multiple view-points.
However, the camera view-points from which these views are obtained are often
fixed for all shapes. To overcome the static nature of current multi-view
techniques, we propose learning these view-points. Specifically, we introduce
the Multi-View Transformation Network (MVTN), which uses differentiable
rendering to determine optimal view-points for 3D shape recognition. As a
result, MVTN can be trained end-to-end with any multi-view network for 3D shape
classification. We integrate MVTN into a novel adaptive multi-view pipeline
that is capable of rendering both 3D meshes and point clouds. Our approach
demonstrates state-of-the-art performance in 3D classification and shape
retrieval on several benchmarks (ModelNet40, ScanObjectNN, ShapeNet Core55).
Further analysis indicates that our approach exhibits improved robustness to
occlusion compared to other methods. We also investigate additional aspects of
MVTN, such as 2D pretraining and its use for segmentation. To support further
research in this area, we have released MVTorch, a PyTorch library for 3D
understanding and generation using multi-view projections.

</details>


### [305] [MoPFormer: Motion-Primitive Transformer for Wearable-Sensor Activity Recognition](https://arxiv.org/pdf/2505.20744)
*Hao Zhang, Zhan Zhuang, Xuehao Wang, Xiaodong Yang, Yu Zhang*

Main category: cs.CV

TL;DR: MoPFormer is a self-supervised Transformer framework for HAR, improving interpretability and cross-dataset generalization by tokenizing sensor data into motion primitives.


<details>
  <summary>Details</summary>
Motivation: Addressing limited interpretability and poor cross-dataset generalization in HAR with wearable sensors.

Method: Two-stage approach: 1) Tokenize sensor data into motion primitives, 2) Enrich tokens with a Transformer encoder using masked motion-modeling.

Result: Outperforms state-of-the-art methods on six HAR benchmarks and generalizes well across datasets.

Conclusion: MoPFormer enhances interpretability and cross-dataset performance by capturing fundamental movement patterns.

Abstract: Human Activity Recognition (HAR) with wearable sensors is challenged by
limited interpretability, which significantly impacts cross-dataset
generalization. To address this challenge, we propose Motion-Primitive
Transformer (MoPFormer), a novel self-supervised framework that enhances
interpretability by tokenizing inertial measurement unit signals into
semantically meaningful motion primitives and leverages a Transformer
architecture to learn rich temporal representations. MoPFormer comprises
two-stages. first stage is to partition multi-channel sensor streams into short
segments and quantizing them into discrete "motion primitive" codewords, while
the second stage enriches those tokenized sequences through a context-aware
embedding module and then processes them with a Transformer encoder. The
proposed MoPFormer can be pre-trained using a masked motion-modeling objective
that reconstructs missing primitives, enabling it to develop robust
representations across diverse sensor configurations. Experiments on six HAR
benchmarks demonstrate that MoPFormer not only outperforms state-of-the-art
methods but also successfully generalizes across multiple datasets. Most
importantly, the learned motion primitives significantly enhance both
interpretability and cross-dataset performance by capturing fundamental
movement patterns that remain consistent across similar activities regardless
of dataset origin.

</details>


### [306] [Understand, Think, and Answer: Advancing Visual Reasoning with Large Multimodal Models](https://arxiv.org/pdf/2505.20753)
*Yufei Zhan, Hongyin Zhao, Yousong Zhu, Shurong Zheng, Fan Yang, Ming Tang, Jinqiao Wang*

Main category: cs.CV

TL;DR: Griffon-R introduces a unified visual reasoning mechanism for Large Multimodal Models (LMMs) to solve compositional problems efficiently, outperforming benchmarks like VSR and CLEVR.


<details>
  <summary>Details</summary>
Motivation: LMMs lack advanced task-specific capabilities for compositional reasoning, limiting their general vision model potential.

Method: A human-like understanding-thinking-answering process is proposed, enabling single-pass forwarding without multiple inferences or external tools.

Result: Griffon-R achieves superior performance on complex reasoning benchmarks (VSR, CLEVR) and enhances multimodal capabilities (MMBench, ScienceQA).

Conclusion: The approach bridges foundational visual capabilities with general question answering, producing faithful and traceable responses.

Abstract: Large Multimodal Models (LMMs) have recently demonstrated remarkable visual
understanding performance on both vision-language and vision-centric tasks.
However, they often fall short in integrating advanced, task-specific
capabilities for compositional reasoning, which hinders their progress toward
truly competent general vision models. To address this, we present a unified
visual reasoning mechanism that enables LMMs to solve complicated compositional
problems by leveraging their intrinsic capabilities (e.g. grounding and visual
understanding capabilities). Different from the previous shortcut learning
mechanism, our approach introduces a human-like
understanding-thinking-answering process, allowing the model to complete all
steps in a single pass forwarding without the need for multiple inferences or
external tools. This design bridges the gap between foundational visual
capabilities and general question answering, encouraging LMMs to generate
faithful and traceable responses for complex visual reasoning. Meanwhile, we
curate 334K visual instruction samples covering both general scenes and
text-rich scenes and involving multiple foundational visual capabilities. Our
trained model, Griffon-R, has the ability of end-to-end automatic
understanding, self-thinking, and reasoning answers. Comprehensive experiments
show that Griffon-R not only achieves advancing performance on complex visual
reasoning benchmarks including VSR and CLEVR, but also enhances multimodal
capabilities across various benchmarks like MMBench and ScienceQA. Data,
models, and codes will be release at
https://github.com/jefferyZhan/Griffon/tree/master/Griffon-R soon.

</details>


### [307] [SpatialLLM: From Multi-modality Data to Urban Spatial Intelligence](https://arxiv.org/pdf/2505.12703)
*Jiabin Chen, Haiping Wang, Jinpeng Li, Yuan Liu, Zhen Dong, Bisheng Yang*

Main category: cs.CV

TL;DR: SpatialLLM is a novel language model for spatial intelligence tasks in urban scenes, requiring no training or expertise, and leverages pre-trained LLMs for zero-shot analysis.


<details>
  <summary>Details</summary>
Motivation: To simplify spatial intelligence tasks by eliminating the need for geographic tools or domain expertise, enabling broader accessibility.

Method: Constructs structured scene descriptions from raw spatial data to prompt pre-trained LLMs for analysis without fine-tuning.

Result: Pretrained LLMs accurately perform spatial tasks like urban planning and traffic management in zero-shot settings.

Conclusion: SpatialLLM offers a new approach for urban analysis, highlighting the importance of multi-field knowledge, context length, and reasoning in LLM performance.

Abstract: We propose SpatialLLM, a novel approach advancing spatial intelligence tasks
in complex urban scenes. Unlike previous methods requiring geographic analysis
tools or domain expertise, SpatialLLM is a unified language model directly
addressing various spatial intelligence tasks without any training,
fine-tuning, or expert intervention. The core of SpatialLLM lies in
constructing detailed and structured scene descriptions from raw spatial data
to prompt pre-trained LLMs for scene-based analysis. Extensive experiments show
that, with our designs, pretrained LLMs can accurately perceive spatial
distribution information and enable zero-shot execution of advanced spatial
intelligence tasks, including urban planning, ecological analysis, traffic
management, etc. We argue that multi-field knowledge, context length, and
reasoning ability are key factors influencing LLM performances in urban
analysis. We hope that SpatialLLM will provide a novel viable perspective for
urban intelligent analysis and management. The code and dataset are available
at https://github.com/WHU-USI3DV/SpatialLLM.

</details>


### [308] [PARTONOMY: Large Multimodal Models with Part-Level Visual Understanding](https://arxiv.org/pdf/2505.20759)
*Ansel Blume, Jeonghwan Kim, Hyeonjeong Ha, Elen Chatikyan, Xiaomeng Jin, Khanh Duy Nguyen, Nanyun Peng, Kai-Wei Chang, Derek Hoiem, Heng Ji*

Main category: cs.CV

TL;DR: PARTONOMY is a benchmark for pixel-level part grounding in LMMs, revealing their limitations and proposing PLUM, a novel model that outperforms existing methods.


<details>
  <summary>Details</summary>
Motivation: Large multimodal models struggle with fine-grained part identification, hindering compositional reasoning.

Method: Constructed PARTONOMY benchmark, trained part-centric LMMs, and introduced PLUM with span tagging and feedback loops.

Result: Existing LMMs perform poorly (e.g., 5.9% gIoU), while PLUM excels in reasoning segmentation and VQA tasks.

Conclusion: PLUM advances fine-grained visual understanding in LMMs, addressing key architectural flaws.

Abstract: Real-world objects are composed of distinctive, object-specific parts.
Identifying these parts is key to performing fine-grained, compositional
reasoning-yet, large multimodal models (LMMs) struggle to perform this
seemingly straightforward task. In this work, we introduce PARTONOMY, an LMM
benchmark designed for pixel-level part grounding. We construct PARTONOMY from
existing part datasets and our own rigorously annotated set of images,
encompassing 862 part labels and 534 object labels for evaluation. Unlike
existing datasets that simply ask models to identify generic parts, PARTONOMY
uses specialized concepts (e.g., agricultural airplane), and challenges models
to compare objects' parts, consider part-whole relationships, and justify
textual predictions with visual segmentations. Our experiments demonstrate
significant limitations in state-of-the-art LMMs (e.g., LISA-13B achieves only
5.9% gIoU), highlighting a critical gap in their part grounding abilities. We
note that existing segmentation-enabled LMMs (segmenting LMMs) have two key
architectural shortcomings: they use special [SEG] tokens not seen during
pretraining which induce distribution shift, and they discard predicted
segmentations instead of using past predictions to guide future ones. To
address these deficiencies, we train several part-centric LMMs and propose
PLUM, a novel segmenting LMM that uses span tagging instead of segmentation
tokens and that conditions on prior predictions in a feedback loop. We find
that pretrained PLUM outperforms existing segmenting LMMs on reasoning
segmentation, VQA, and visual hallucination benchmarks. In addition, PLUM
finetuned on our proposed Explanatory Part Segmentation task is competitive
with segmenting LMMs trained on significantly more segmentation data. Our work
opens up new avenues towards enabling fine-grained, grounded visual
understanding in LMMs.

</details>


### [309] [ConText-CIR: Learning from Concepts in Text for Composed Image Retrieval](https://arxiv.org/pdf/2505.20764)
*Eric Xing, Pranavi Kolouju, Robert Pless, Abby Stylianou, Nathan Jacobs*

Main category: cs.CV

TL;DR: ConText-CIR improves composed image retrieval by using a Text Concept-Consistency loss and synthetic data generation, achieving state-of-the-art results.


<details>
  <summary>Details</summary>
Motivation: Existing CIR methods struggle with accurately representing image-text modifications, leading to poor performance.

Method: Proposes ConText-CIR with a Text Concept-Consistency loss and a synthetic data generation pipeline.

Result: Achieves state-of-the-art performance on CIR tasks in supervised and zero-shot settings.

Conclusion: ConText-CIR effectively addresses limitations in CIR, offering improved accuracy and scalability.

Abstract: Composed image retrieval (CIR) is the task of retrieving a target image
specified by a query image and a relative text that describes a semantic
modification to the query image. Existing methods in CIR struggle to accurately
represent the image and the text modification, resulting in subpar performance.
To address this limitation, we introduce a CIR framework, ConText-CIR, trained
with a Text Concept-Consistency loss that encourages the representations of
noun phrases in the text modification to better attend to the relevant parts of
the query image. To support training with this loss function, we also propose a
synthetic data generation pipeline that creates training data from existing CIR
datasets or unlabeled images. We show that these components together enable
stronger performance on CIR tasks, setting a new state-of-the-art in composed
image retrieval in both the supervised and zero-shot settings on multiple
benchmark datasets, including CIRR and CIRCO. Source code, model checkpoints,
and our new datasets are available at https://github.com/mvrl/ConText-CIR.

</details>


### [310] [MetaSlot: Break Through the Fixed Number of Slots in Object-Centric Learning](https://arxiv.org/pdf/2505.20772)
*Hongjia Liu, Rongzhen Zhao, Haohan Chen, Joni Pajarinen*

Main category: cs.CV

TL;DR: MetaSlot is a plug-and-play Slot Attention variant that adapts to variable object counts, improving object representation and performance in Object-Centric Learning tasks.


<details>
  <summary>Details</summary>
Motivation: Existing Slot Attention methods use a static slot count, leading to fragmented object representations when object counts vary. MetaSlot addresses this limitation.

Method: MetaSlot introduces a codebook for object prototypes, removes duplicate slots via quantization, and injects noise to stabilize aggregation.

Result: Models with MetaSlot show significant performance gains and more interpretable slot representations across datasets and tasks.

Conclusion: MetaSlot is a versatile and effective enhancement for Slot Attention in Object-Centric Learning.

Abstract: Learning object-level, structured representations is widely regarded as a key
to better generalization in vision and underpins the design of next-generation
Pre-trained Vision Models (PVMs). Mainstream Object-Centric Learning (OCL)
methods adopt Slot Attention or its variants to iteratively aggregate objects'
super-pixels into a fixed set of query feature vectors, termed slots. However,
their reliance on a static slot count leads to an object being represented as
multiple parts when the number of objects varies. We introduce MetaSlot, a
plug-and-play Slot Attention variant that adapts to variable object counts.
MetaSlot (i) maintains a codebook that holds prototypes of objects in a dataset
by vector-quantizing the resulting slot representations; (ii) removes duplicate
slots from the traditionally aggregated slots by quantizing them with the
codebook; and (iii) injects progressively weaker noise into the Slot Attention
iterations to accelerate and stabilize the aggregation. MetaSlot is a general
Slot Attention variant that can be seamlessly integrated into existing OCL
architectures. Across multiple public datasets and tasks--including object
discovery and recognition--models equipped with MetaSlot achieve significant
performance gains and markedly interpretable slot representations, compared
with existing Slot Attention variants.

</details>


### [311] [TACO: Think-Answer Consistency for Optimized Long-Chain Reasoning and Efficient Data Learning via Reinforcement Learning in LVLMs](https://arxiv.org/pdf/2505.20777)
*Zhehan Kan, Yanlin Liu, Kun Yin, Xinghua Jiang, Xin Li, Haoyu Cao, Yinsong Liu, Deqiang Jiang, Xing Sun, Qingmin Liao, Wenming Yang*

Main category: cs.CV

TL;DR: TACO is a reinforcement learning algorithm for visual reasoning, addressing inconsistencies, instability, and low data efficiency in multimodal LLMs. It introduces Think-Answer Consistency, Rollback Resample Strategy, and adaptive learning for improved performance.


<details>
  <summary>Details</summary>
Motivation: Existing methods for replicating DeepSeek R1's reasoning in multimodal settings face issues like reasoning-answer inconsistencies, model instability, and low data efficiency.

Method: TACO builds on GRPO, introducing Think-Answer Consistency, Rollback Resample Strategy, adaptive learning, and Test-Time-Resolution-Scaling.

Result: Fine-tuning LVLMs with TACO shows significant performance improvements on REC and VQA benchmarks.

Conclusion: TACO effectively addresses key challenges in visual reasoning for LLMs, enhancing stability, consistency, and efficiency.

Abstract: DeepSeek R1 has significantly advanced complex reasoning for large language
models (LLMs). While recent methods have attempted to replicate R1's reasoning
capabilities in multimodal settings, they face limitations, including
inconsistencies between reasoning and final answers, model instability and
crashes during long-chain exploration, and low data learning efficiency. To
address these challenges, we propose TACO, a novel reinforcement learning
algorithm for visual reasoning. Building on Generalized Reinforcement Policy
Optimization (GRPO), TACO introduces Think-Answer Consistency, which tightly
couples reasoning with answer consistency to ensure answers are grounded in
thoughtful reasoning. We also introduce the Rollback Resample Strategy, which
adaptively removes problematic samples and reintroduces them to the sampler,
enabling stable long-chain exploration and future learning opportunities.
Additionally, TACO employs an adaptive learning schedule that focuses on
moderate difficulty samples to optimize data efficiency. Furthermore, we
propose the Test-Time-Resolution-Scaling scheme to address performance
degradation due to varying resolutions during reasoning while balancing
computational overhead. Extensive experiments on in-distribution and
out-of-distribution benchmarks for REC and VQA tasks show that fine-tuning
LVLMs leads to significant performance improvements.

</details>


### [312] [Breaking Dataset Boundaries: Class-Agnostic Targeted Adversarial Attacks](https://arxiv.org/pdf/2505.20782)
*Taïga Gonçalves, Tomo Miyazaki, Shinichiro Omachi*

Main category: cs.CV

TL;DR: CD-MTA is a method for generating adversarial examples that mislead classifiers toward any target class, including unseen ones, without needing the black-box model's training data.


<details>
  <summary>Details</summary>
Motivation: Traditional targeted attacks are limited to one class per model, requiring costly retraining, while existing multi-targeted attacks rely on class embeddings and training data, leading to poor generalization.

Method: CD-MTA uses image-based conditional inputs and class-agnostic losses to align perturbed and target images in feature space, removing dependence on class semantics.

Result: Experiments show CD-MTA outperforms prior multi-targeted attacks in standard and cross-domain settings.

Conclusion: CD-MTA enables effective adversarial attacks on unseen classes without data leakage, improving practicality in black-box scenarios.

Abstract: We present Cross-Domain Multi-Targeted Attack (CD-MTA), a method for
generating adversarial examples that mislead image classifiers toward any
target class, including those not seen during training. Traditional targeted
attacks are limited to one class per model, requiring expensive retraining for
each target. Multi-targeted attacks address this by introducing a perturbation
generator with a conditional input to specify the target class. However,
existing methods are constrained to classes observed during training and
require access to the black-box model's training data--introducing a form of
data leakage that undermines realistic evaluation in practical black-box
scenarios. We identify overreliance on class embeddings as a key limitation,
leading to overfitting and poor generalization to unseen classes. To address
this, CD-MTA replaces class-level supervision with an image-based conditional
input and introduces class-agnostic losses that align the perturbed and target
images in the feature space. This design removes dependence on class semantics,
thereby enabling generalization to unseen classes across datasets. Experiments
on ImageNet and seven other datasets show that CD-MTA outperforms prior
multi-targeted attacks in both standard and cross-domain settings--without
accessing the black-box model's training data.

</details>


### [313] [Integrating Intermediate Layer Optimization and Projected Gradient Descent for Solving Inverse Problems with Diffusion Models](https://arxiv.org/pdf/2505.20789)
*Yang Zheng, Wen Li, Zhaoqiang Liu*

Main category: cs.CV

TL;DR: The paper introduces DMILO and DMILO-PGD, two methods to improve diffusion model-based inverse problem solvers by reducing computational demands and enhancing convergence.


<details>
  <summary>Details</summary>
Motivation: Traditional inverse problem solvers rely on handcrafted priors, which may not capture real-world data complexity. Diffusion models offer better priors but face computational and convergence issues.

Method: DMILO uses intermediate layer optimization (ILO) to reduce memory burden, while DMILO-PGD combines ILO with projected gradient descent (PGD) to improve convergence.

Result: Experiments on diverse image datasets show significant performance gains over state-of-the-art methods.

Conclusion: DMILO and DMILO-PGD effectively address computational and convergence challenges in diffusion model-based inverse problem solvers.

Abstract: Inverse problems (IPs) involve reconstructing signals from noisy
observations. Traditional approaches often rely on handcrafted priors, which
can fail to capture the complexity of real-world data. The advent of
pre-trained generative models has introduced new paradigms, offering improved
reconstructions by learning rich priors from data. Among these, diffusion
models (DMs) have emerged as a powerful framework, achieving remarkable
reconstruction performance across numerous IPs. However, existing DM-based
methods frequently encounter issues such as heavy computational demands and
suboptimal convergence. In this work, building upon the idea of the recent work
DMPlug~\cite{wang2024dmplug}, we propose two novel methods, DMILO and
DMILO-PGD, to address these challenges. Our first method, DMILO, employs
intermediate layer optimization (ILO) to alleviate the memory burden inherent
in DMPlug. Additionally, by introducing sparse deviations, we expand the range
of DMs, enabling the exploration of underlying signals that may lie outside the
range of the diffusion model. We further propose DMILO-PGD, which integrates
ILO with projected gradient descent (PGD), thereby reducing the risk of
suboptimal convergence. We provide an intuitive theoretical analysis of our
approach under appropriate conditions and validate its superiority through
extensive experiments on diverse image datasets, encompassing both linear and
nonlinear IPs. Our results demonstrate significant performance gains over
state-of-the-art methods, highlighting the effectiveness of DMILO and DMILO-PGD
in addressing common challenges in DM-based IP solvers.

</details>


### [314] [Rendering-Aware Reinforcement Learning for Vector Graphics Generation](https://arxiv.org/pdf/2505.20793)
*Juan A. Rodriguez, Haotian Zhang, Abhay Puri, Aarash Feizi, Rishav Pramanik, Pascal Wichmann, Arnab Mondal, Mohammad Reza Samsami, Rabiul Awal, Perouz Taslakian, Spandana Gella, Sai Rajeswar, David Vazquez, Christopher Pal, Marco Pedersoli*

Main category: cs.CV

TL;DR: RLRF improves SVG generation in VLMs by using reinforcement learning based on rendered feedback, outperforming supervised methods.


<details>
  <summary>Details</summary>
Motivation: Existing VLMs struggle with faithful and efficient SVG generation due to lack of rendered image observation during training.

Method: RLRF leverages reinforcement learning, comparing rendered SVG outputs to original images for feedback, guiding model improvements.

Result: RLRF significantly outperforms supervised fine-tuning, producing more accurate and efficient SVGs.

Conclusion: RLRF enables precise, high-quality SVG generation with strong structural understanding and generalization.

Abstract: Scalable Vector Graphics (SVG) offer a powerful format for representing
visual designs as interpretable code. Recent advances in vision-language models
(VLMs) have enabled high-quality SVG generation by framing the problem as a
code generation task and leveraging large-scale pretraining. VLMs are
particularly suitable for this task as they capture both global semantics and
fine-grained visual patterns, while transferring knowledge across vision,
natural language, and code domains. However, existing VLM approaches often
struggle to produce faithful and efficient SVGs because they never observe the
rendered images during training. Although differentiable rendering for
autoregressive SVG code generation remains unavailable, rendered outputs can
still be compared to original inputs, enabling evaluative feedback suitable for
reinforcement learning (RL). We introduce RLRF(Reinforcement Learning from
Rendering Feedback), an RL method that enhances SVG generation in
autoregressive VLMs by leveraging feedback from rendered SVG outputs. Given an
input image, the model generates SVG roll-outs that are rendered and compared
to the original image to compute a reward. This visual fidelity feedback guides
the model toward producing more accurate, efficient, and semantically coherent
SVGs. RLRF significantly outperforms supervised fine-tuning, addressing common
failure modes and enabling precise, high-quality SVG generation with strong
structural understanding and generalization.

</details>


### [315] [Not All Thats Rare Is Lost: Causal Paths to Rare Concept Synthesis](https://arxiv.org/pdf/2505.20808)
*Bo-Kai Ruan, Zi-Xiang Ni, Bo-Lun Huang, Teng-Fang Hsiao, Hong-Han Shuai*

Main category: cs.CV

TL;DR: RAP improves rare concept generation in diffusion models by treating it as navigating a latent causal path, using frequent prompts to guide rare ones dynamically.


<details>
  <summary>Details</summary>
Motivation: Diffusion models struggle with rare concept generation due to infrequent training data. RAP addresses this by aligning prompt guidance with the model's dynamics.

Method: RAP formulates rare prompt guidance via frequent prompts, uses dynamic prompt switching based on score similarity, and treats alternation as second-order denoising.

Result: RAP outperforms baselines in rare concept generation across diverse diffusion models, validated by automated and human evaluations.

Conclusion: RAP provides a principled, model-aligned approach for enhancing rare concept synthesis in diffusion models.

Abstract: Diffusion models have shown strong capabilities in high-fidelity image
generation but often falter when synthesizing rare concepts, i.e., prompts that
are infrequently observed in the training distribution. In this paper, we
introduce RAP, a principled framework that treats rare concept generation as
navigating a latent causal path: a progressive, model-aligned trajectory
through the generative space from frequent concepts to rare targets. Rather
than relying on heuristic prompt alternation, we theoretically justify that
rare prompt guidance can be approximated by semantically related frequent
prompts. We then formulate prompt switching as a dynamic process based on score
similarity, enabling adaptive stage transitions. Furthermore, we reinterpret
prompt alternation as a second-order denoising mechanism, promoting smooth
semantic progression and coherent visual synthesis. Through this causal lens,
we align input scheduling with the model's internal generative dynamics.
Experiments across diverse diffusion backbones demonstrate that RAP
consistently enhances rare concept generation, outperforming strong baselines
in both automated evaluations and human studies.

</details>


### [316] [Frame-Level Captions for Long Video Generation with Complex Multi Scenes](https://arxiv.org/pdf/2505.20827)
*Guangcong Zheng, Jianlong Yuan, Bo Wang, Haoyang Huang, Guoqing Ma, Nan Duan*

Main category: cs.CV

TL;DR: A new method for generating complex, multi-scene long videos using frame-level annotations and a Frame-Level Attention Mechanism to reduce error accumulation and improve text-video alignment.


<details>
  <summary>Details</summary>
Motivation: Current autoregressive diffusion models struggle with error drift and lack flexibility for multi-scene stories, limiting long video generation.

Method: Proposes frame-level dataset annotation, Frame-Level Attention Mechanism, and Diffusion Forcing for flexible temporal handling.

Result: Outperforms on VBench 2.0 benchmarks, showing better instruction-following and high-quality long videos.

Conclusion: The method advances long video generation for complex stories and plans to share datasets and models.

Abstract: Generating long videos that can show complex stories, like movie scenes from
scripts, has great promise and offers much more than short clips. However,
current methods that use autoregression with diffusion models often struggle
because their step-by-step process naturally leads to a serious error
accumulation (drift). Also, many existing ways to make long videos focus on
single, continuous scenes, making them less useful for stories with many events
and changes. This paper introduces a new approach to solve these problems.
First, we propose a novel way to annotate datasets at the frame-level,
providing detailed text guidance needed for making complex, multi-scene long
videos. This detailed guidance works with a Frame-Level Attention Mechanism to
make sure text and video match precisely. A key feature is that each part
(frame) within these windows can be guided by its own distinct text prompt. Our
training uses Diffusion Forcing to provide the model with the ability to handle
time flexibly. We tested our approach on difficult VBench 2.0 benchmarks
("Complex Plots" and "Complex Landscapes") based on the WanX2.1-T2V-1.3B model.
The results show our method is better at following instructions in complex,
changing scenes and creates high-quality long videos. We plan to share our
dataset annotation methods and trained models with the research community.
Project page: https://zgctroy.github.io/frame-level-captions .

</details>


### [317] [Causality-Driven Infrared and Visible Image Fusion](https://arxiv.org/pdf/2505.20830)
*Linli Ma, Suzhen Lin, Jianchao Zeng, Zanxia Jin, Yanbo Wang, Fengyuan Li, Yubing Luo*

Main category: cs.CV

TL;DR: The paper addresses scene bias in image fusion by introducing a causal graph and a Back-door Adjustment based Feature Fusion Module (BAFFM) to improve performance.


<details>
  <summary>Details</summary>
Motivation: Existing methods ignore dataset scene bias, leading to spurious correlations and limited fusion performance.

Method: The paper re-examines image fusion from a causality perspective, constructs a causal graph, and proposes BAFFM to eliminate confounder interference.

Result: Extensive experiments show the method outperforms state-of-the-art methods in infrared and visible image fusion.

Conclusion: The causal approach and BAFFM effectively address bias, enhancing fusion performance.

Abstract: Image fusion aims to combine complementary information from multiple source
images to generate more comprehensive scene representations. Existing methods
primarily rely on the stacking and design of network architectures to enhance
the fusion performance, often ignoring the impact of dataset scene bias on
model training. This oversight leads the model to learn spurious correlations
between specific scenes and fusion weights under conventional likelihood
estimation framework, thereby limiting fusion performance. To solve the above
problems, this paper first re-examines the image fusion task from the causality
perspective, and disentangles the model from the impact of bias by constructing
a tailored causal graph to clarify the causalities among the variables in image
fusion task. Then, the Back-door Adjustment based Feature Fusion Module (BAFFM)
is proposed to eliminate confounder interference and enable the model to learn
the true causal effect. Finally, Extensive experiments on three standard
datasets prove that the proposed method significantly surpasses
state-of-the-art methods in infrared and visible image fusion.

</details>


### [318] [Fully Spiking Neural Networks for Unified Frame-Event Object Tracking](https://arxiv.org/pdf/2505.20834)
*Jingjun Yang, Liangwei Fan, Jinpu Zhang, Xiangkai Lian, Hui Shen, Dewen Hu*

Main category: cs.CV

TL;DR: SpikeFET, a Spiking Frame-Event Tracking framework, integrates image and event streams efficiently, balancing performance and energy use.


<details>
  <summary>Details</summary>
Motivation: Current fusion methods for visual object tracking are computationally heavy and inefficient with sparse event data, missing the energy benefits of spiking paradigms.

Method: SpikeFET combines convolutional local feature extraction and Transformer-based global modeling in a spiking paradigm, using a Random Patchwork Module (RPM) and Spatial-Temporal Regularization (STR).

Result: Outperforms existing methods in tracking accuracy with lower power consumption.

Conclusion: SpikeFET achieves optimal performance-efficiency balance, advancing robust visual tracking in complex environments.

Abstract: The integration of image and event streams offers a promising approach for
achieving robust visual object tracking in complex environments. However,
current fusion methods achieve high performance at the cost of significant
computational overhead and struggle to efficiently extract the sparse,
asynchronous information from event streams, failing to leverage the
energy-efficient advantages of event-driven spiking paradigms. To address this
challenge, we propose the first fully Spiking Frame-Event Tracking framework
called SpikeFET. This network achieves synergistic integration of convolutional
local feature extraction and Transformer-based global modeling within the
spiking paradigm, effectively fusing frame and event data. To overcome the
degradation of translation invariance caused by convolutional padding, we
introduce a Random Patchwork Module (RPM) that eliminates positional bias
through randomized spatial reorganization and learnable type encoding while
preserving residual structures. Furthermore, we propose a Spatial-Temporal
Regularization (STR) strategy that overcomes similarity metric degradation from
asymmetric features by enforcing spatio-temporal consistency among temporal
template features in latent space. Extensive experiments across multiple
benchmarks demonstrate that the proposed framework achieves superior tracking
accuracy over existing methods while significantly reducing power consumption,
attaining an optimal balance between performance and efficiency. The code will
be released.

</details>


### [319] [ProBA: Probabilistic Bundle Adjustment with the Bhattacharyya Coefficient](https://arxiv.org/pdf/2505.20858)
*Jason Chui, Daniel Cremers*

Main category: cs.CV

TL;DR: ProBA introduces a probabilistic BA method that handles uncertainty in 2D observations and 3D structure, eliminating the need for accurate initial estimates or known camera intrinsics.


<details>
  <summary>Details</summary>
Motivation: Traditional BA methods rely on accurate initial estimates and known camera intrinsics, limiting their use in uncertain or unstructured environments.

Method: ProBA uses 3D Gaussians for landmarks and uncertainty-aware reprojection losses, enforcing geometric consistency via the Bhattacharyya coefficient.

Result: ProBA outperforms traditional BA methods in challenging conditions, reducing sensitivity to outliers and poor local minima.

Conclusion: ProBA enhances SLAM practicality by removing reliance on strong initialization and known intrinsics, making it suitable for unstructured environments.

Abstract: Classical Bundle Adjustment (BA) methods require accurate initial estimates
for convergence and typically assume known camera intrinsics, which limits
their applicability when such information is uncertain or unavailable. We
propose a novel probabilistic formulation of BA (ProBA) that explicitly models
and propagates uncertainty in both the 2D observations and the 3D scene
structure, enabling optimization without any prior knowledge of camera poses or
focal length. Our method uses 3D Gaussians instead of point-like landmarks and
we introduce uncertainty-aware reprojection losses by projecting the 3D
Gaussians onto the 2D image space, and enforce geometric consistency across
multiple 3D Gaussians using the Bhattacharyya coefficient to encourage overlap
between their corresponding Gaussian distributions. This probabilistic
framework leads to more robust and reliable optimization, even in the presence
of outliers in the correspondence set, reducing the likelihood of converging to
poor local minima. Experimental results show that \textit{ProBA} outperforms
traditional methods in challenging real-world conditions. By removing the need
for strong initialization and known intrinsics, ProBA enhances the practicality
of SLAM systems deployed in unstructured environments.

</details>


### [320] [Exploring Timeline Control for Facial Motion Generation](https://arxiv.org/pdf/2505.20861)
*Yifeng Ma, Jinwei Qi, Chaonan Ji, Peng Zhang, Bang Zhang, Zhidong Deng, Liefeng Bo*

Main category: cs.CV

TL;DR: The paper introduces timeline control for facial motion generation, offering precise timing of actions. A diffusion-based model generates natural motions aligned with timelines, supported by text-to-timeline conversion via ChatGPT.


<details>
  <summary>Details</summary>
Motivation: Existing signals (audio, text) lack fine-grained control for facial motion. Timeline control enables precise timing of actions, improving user control.

Method: Annotate facial action intervals using Toeplitz Inverse Covariance-based Clustering. A diffusion-based model generates motions aligned with timelines, with text-to-timeline conversion via ChatGPT.

Result: Accurate annotation of facial action intervals and generation of natural motions aligned with timelines.

Conclusion: Timeline control enhances facial motion generation with precise timing, supported by effective annotation and diffusion-based modeling.

Abstract: This paper introduces a new control signal for facial motion generation:
timeline control. Compared to audio and text signals, timelines provide more
fine-grained control, such as generating specific facial motions with precise
timing. Users can specify a multi-track timeline of facial actions arranged in
temporal intervals, allowing precise control over the timing of each action. To
model the timeline control capability, We first annotate the time intervals of
facial actions in natural facial motion sequences at a frame-level granularity.
This process is facilitated by Toeplitz Inverse Covariance-based Clustering to
minimize human labor. Based on the annotations, we propose a diffusion-based
generation model capable of generating facial motions that are natural and
accurately aligned with input timelines. Our method supports text-guided motion
generation by using ChatGPT to convert text into timelines. Experimental
results show that our method can annotate facial action intervals with
satisfactory accuracy, and produces natural facial motions accurately aligned
with timelines.

</details>


### [321] [AVCD: Mitigating Hallucinations in Audio-Visual Large Language Models through Contrastive Decoding](https://arxiv.org/pdf/2505.20862)
*Chaeyoung Jung, Youngjoon Jang, Joon Son Chung*

Main category: cs.CV

TL;DR: AVCD is a novel decoding framework for AV-LLMs that dynamically suppresses hallucinations by leveraging attention distributions and adaptive masking, outperforming existing methods.


<details>
  <summary>Details</summary>
Motivation: Hallucinations in multimodal models (AV-LLMs) arise from complex interactions across audio, video, and language, requiring a modality-aware decoding strategy.

Method: Proposes Audio-Visual Contrastive Decoding (AVCD), which uses attentive masking and reformulates CD for trimodal inputs, with entropy-guided adaptive decoding for efficiency.

Result: AVCD improves accuracy by 6% for VideoLLaMA2 and 11% for video-SALMONN on AVHBench, showing robustness.

Conclusion: AVCD effectively addresses hallucinations in AV-LLMs by dynamically adapting to modality interactions, offering a training-free solution with strong performance.

Abstract: Hallucination remains a major challenge in multimodal large language models
(MLLMs). To address this, various contrastive decoding (CD) methods have been
proposed that contrasts original logits with hallucinated logits generated from
perturbed inputs. While CD has shown promise in vision-language models (VLMs),
it is not well-suited for AV-LLMs, where hallucinations often emerge from both
unimodal and cross-modal combinations involving audio, video, and language.
These intricate interactions call for a more adaptive and modality-aware
decoding strategy. In this paper, we propose Audio-Visual Contrastive Decoding
(AVCD)-a novel, training-free decoding framework designed to model trimodal
interactions and suppress modality-induced hallucinations in AV-LLMs. Unlike
previous CD methods in VLMs that corrupt a fixed modality, AVCD leverages
attention distributions to dynamically identify less dominant modalities and
applies attentive masking to generate perturbed output logits. To support CD in
a trimodal setting, we also reformulate the original CD framework to jointly
handle audio, visual, and textual inputs. Finally, to improve efficiency, we
introduce entropy-guided adaptive decoding, which selectively skips unnecessary
decoding steps based on the model's confidence in its predictions. Extensive
experiments demonstrate that AVCD consistently outperforms existing decoding
methods. Especially, on the AVHBench dataset, it improves accuracy by 6% for
VideoLLaMA2 and 11% for video-SALMONN, demonstrating strong robustness and
generalizability.

</details>


### [322] [In Context Learning with Vision Transformers: Case Study](https://arxiv.org/pdf/2505.20872)
*Antony Zhao, Alex Proshkin, Fergal Hennessy, Francesco Crivelli*

Main category: cs.CV

TL;DR: The paper explores extending transformer models' in-context learning capabilities to the image space for complex functions like CNNs.


<details>
  <summary>Details</summary>
Motivation: To analyze if transformers can in-context learn complex image functions, building on their success with simpler tasks.

Method: Extend in-context learning to image space, testing on functions like CNNs.

Result: Not explicitly stated, but aims to demonstrate transformers' adaptability to image tasks.

Conclusion: The study could expand transformers' applicability to more complex, image-based functions.

Abstract: Large transformer models have been shown to be capable of performing
in-context learning. By using examples in a prompt as well as a query, they are
capable of performing tasks such as few-shot, one-shot, or zero-shot learning
to output the corresponding answer to this query. One area of interest to us is
that these transformer models have been shown to be capable of learning the
general class of certain functions, such as linear functions and small 2-layer
neural networks, on random data (Garg et al, 2023). We aim to extend this to
the image space to analyze their capability to in-context learn more complex
functions on the image space, such as convolutional neural networks and other
methods.

</details>


### [323] [Fork-Merge Decoding: Enhancing Multimodal Understanding in Audio-Visual Large Language Models](https://arxiv.org/pdf/2505.20873)
*Chaeyoung Jung, Youngjoon Jang, Jongmin Choi, Joon Son Chung*

Main category: cs.CV

TL;DR: Fork-Merge Decoding (FMD) improves balanced multimodal understanding in AV-LLMs by addressing modality bias without extra training.


<details>
  <summary>Details</summary>
Motivation: Current AV-LLMs suffer from modality bias due to imbalanced training signals, leading to over-reliance on one modality.

Method: Proposes FMD, an inference-time strategy: fork phase for modality-specific reasoning, merge phase for joint reasoning.

Result: Consistent performance improvements on audio, video, and audio-visual tasks across benchmark datasets.

Conclusion: FMD effectively mitigates modality bias and enhances multimodal understanding without training or architectural changes.

Abstract: The goal of this work is to enhance balanced multimodal understanding in
audio-visual large language models (AV-LLMs) by addressing modality bias
without requiring additional training. In current AV-LLMs, audio and video
features are typically processed jointly in the decoder. While this strategy
facilitates unified multimodal understanding, it may introduce modality bias,
where the model tends to over-rely on one modality due to imbalanced training
signals. To mitigate this, we propose Fork-Merge Decoding (FMD), a simple yet
effective inference-time strategy that requires no additional training or
architectural modifications. FMD first performs modality-specific reasoning by
processing audio-only and video-only inputs through the early decoder layers (a
fork phase), and then merges the resulting hidden states for joint reasoning in
the remaining layers (a merge phase). This approach promotes balanced modality
contributions and leverages complementary information across modalities. We
evaluate our method on two representative AV-LLMs, VideoLLaMA2 and
video-SALMONN, using three benchmark datasets. Experimental results demonstrate
consistent performance improvements on tasks focused on audio, video, and
combined audio-visual reasoning, demonstrating the effectiveness of
inference-time interventions for robust multimodal understanding.

</details>


### [324] [YOLO-FireAD: Efficient Fire Detection via Attention-Guided Inverted Residual Learning and Dual-Pooling Feature Preservation](https://arxiv.org/pdf/2505.20884)
*Weichao Pan, Bohan Xu, Xu Wang, Chengze Lv, Shuoyang Wang, Zhenke Duan*

Main category: cs.CV

TL;DR: YOLO-FireAD improves fire detection by integrating attention mechanisms and dual-pooling fusion, achieving higher accuracy and efficiency than YOLOv8 variants.


<details>
  <summary>Details</summary>
Motivation: Addressing limitations in feature extraction and information loss in YOLO-based models for fire detection in dynamic environments.

Method: Proposes YOLO-FireAD with Attention-guided Inverted Residual Block (AIR) and Dual Pool Downscale Fusion Block (DPDF) to enhance fire features and preserve multi-scale patterns.

Result: Outperforms YOLOv8 variants with fewer parameters (1.45M, 51.8% lower) and higher mAP75 (1.3-5.5%).

Conclusion: YOLO-FireAD is efficient and accurate for fire detection, suitable for dynamic environments.

Abstract: Fire detection in dynamic environments faces continuous challenges, including
the interference of illumination changes, many false detections or missed
detections, and it is difficult to achieve both efficiency and accuracy. To
address the problem of feature extraction limitation and information loss in
the existing YOLO-based models, this study propose You Only Look Once for Fire
Detection with Attention-guided Inverted Residual and Dual-pooling Downscale
Fusion (YOLO-FireAD) with two core innovations: (1) Attention-guided Inverted
Residual Block (AIR) integrates hybrid channel-spatial attention with inverted
residuals to adaptively enhance fire features and suppress environmental noise;
(2) Dual Pool Downscale Fusion Block (DPDF) preserves multi-scale fire patterns
through learnable fusion of max-average pooling outputs, mitigating small-fire
detection failures. Extensive evaluation on two public datasets shows the
efficient performance of our model. Our proposed model keeps the sum amount of
parameters (1.45M, 51.8% lower than YOLOv8n) (4.6G, 43.2% lower than YOLOv8n),
and mAP75 is higher than the mainstream real-time object detection models
YOLOv8n, YOL-Ov9t, YOLOv10n, YOLO11n, YOLOv12n and other YOLOv8 variants
1.3-5.5%.

</details>


### [325] [Frequency Composition for Compressed and Domain-Adaptive Neural Networks](https://arxiv.org/pdf/2505.20890)
*Yoojin Kwon, Hongjun Suh, Wooseok Lee, Taesik Gong, Songyi Han, Hyung-Sin Kim*

Main category: cs.CV

TL;DR: CoDA unifies model compression and domain adaptation using frequency composition, improving accuracy on domain-shift benchmarks like CIFAR10-C and ImageNet-C.


<details>
  <summary>Details</summary>
Motivation: Addressing the combined challenge of model compression and domain adaptation, which prior work tackled separately.

Method: Uses quantization-aware training (QAT) with low-frequency components and test-time adaptation (TTA) with full-frequency data.

Result: Achieves accuracy improvements of 7.96%p on CIFAR10-C and 5.37%p on ImageNet-C over baselines.

Conclusion: CoDA effectively integrates compression and adaptation, enhancing performance under resource constraints.

Abstract: Modern on-device neural network applications must operate under resource
constraints while adapting to unpredictable domain shifts. However, this
combined challenge-model compression and domain adaptation-remains largely
unaddressed, as prior work has tackled each issue in isolation: compressed
networks prioritize efficiency within a fixed domain, whereas large, capable
models focus on handling domain shifts. In this work, we propose CoDA, a
frequency composition-based framework that unifies compression and domain
adaptation. During training, CoDA employs quantization-aware training (QAT)
with low-frequency components, enabling a compressed model to selectively learn
robust, generalizable features. At test time, it refines the compact model in a
source-free manner (i.e., test-time adaptation, TTA), leveraging the
full-frequency information from incoming data to adapt to target domains while
treating high-frequency components as domain-specific cues. LFC are aligned
with the trained distribution, while HFC unique to the target distribution are
solely utilized for batch normalization. CoDA can be integrated synergistically
into existing QAT and TTA methods. CoDA is evaluated on widely used
domain-shift benchmarks, including CIFAR10-C and ImageNet-C, across various
model architectures. With significant compression, it achieves accuracy
improvements of 7.96%p on CIFAR10-C and 5.37%p on ImageNet-C over the
full-precision TTA baseline.

</details>


### [326] [Cross from Left to Right Brain: Adaptive Text Dreamer for Vision-and-Language Navigation](https://arxiv.org/pdf/2505.20897)
*Pingrui Zhang, Yifei Su, Pengyuan Wu, Dong An, Li Zhang, Zhigang Wang, Dong Wang, Yan Ding, Bin Zhao, Xuelong Li*

Main category: cs.CV

TL;DR: ATD uses language-based imagination for efficient VLN, outperforming vision-based methods with fewer parameters.


<details>
  <summary>Details</summary>
Motivation: Aligning perception with language in VLN is challenging; vision-based methods are costly and redundant.

Method: Introduces Adaptive Text Dreamer (ATD), a dual-branch LLM-based policy for logical integration and imaginative prediction.

Result: ATD achieves state-of-the-art performance on R2R benchmark with fewer parameters.

Conclusion: Language-based imagination (ATD) is more reliable and efficient for VLN than vision-based methods.

Abstract: Vision-and-Language Navigation (VLN) requires the agent to navigate by
following natural instructions under partial observability, making it difficult
to align perception with language. Recent methods mitigate this by imagining
future scenes, yet they rely on vision-based synthesis, leading to high
computational cost and redundant details. To this end, we propose to adaptively
imagine key environmental semantics via \textit{language} form, enabling a more
reliable and efficient strategy. Specifically, we introduce a novel Adaptive
Text Dreamer (ATD), a dual-branch self-guided imagination policy built upon a
large language model (LLM). ATD is designed with a human-like left-right brain
architecture, where the left brain focuses on logical integration, and the
right brain is responsible for imaginative prediction of future scenes. To
achieve this, we fine-tune only the Q-former within both brains to efficiently
activate domain-specific knowledge in the LLM, enabling dynamic updates of
logical reasoning and imagination during navigation. Furthermore, we introduce
a cross-interaction mechanism to regularize the imagined outputs and inject
them into a navigation expert module, allowing ATD to jointly exploit both the
reasoning capacity of the LLM and the expertise of the navigation model. We
conduct extensive experiments on the R2R benchmark, where ATD achieves
state-of-the-art performance with fewer parameters. The code is
\href{https://github.com/zhangpingrui/Adaptive-Text-Dreamer}{here}.

</details>


### [327] [HTMNet: A Hybrid Network with Transformer-Mamba Bottleneck Multimodal Fusion for Transparent and Reflective Objects Depth Completion](https://arxiv.org/pdf/2505.20904)
*Guanghu Xie, Yonglong Zhang, Zhiduo Jiang, Yang Liu, Zongwu Xie, Baoshi Cao, Hong Liu*

Main category: cs.CV

TL;DR: HTMNet, a hybrid model combining Transformer, CNN, and Mamba, addresses depth completion challenges for transparent objects, achieving SOTA performance.


<details>
  <summary>Details</summary>
Motivation: Transparent and reflective objects cause incomplete depth data, hindering robotic tasks.

Method: HTMNet integrates Transformer-CNN encoders and a Transformer-Mamba fusion module, with a novel multi-scale decoder.

Result: Achieves SOTA performance on public datasets.

Conclusion: HTMNet effectively solves depth completion for transparent objects, showcasing Mamba's potential in this field.

Abstract: Transparent and reflective objects pose significant challenges for depth
sensors, resulting in incomplete depth information that adversely affects
downstream robotic perception and manipulation tasks. To address this issue, we
propose HTMNet, a novel hybrid model integrating Transformer, CNN, and Mamba
architectures. The encoder is constructed based on a dual-branch
Transformer-CNN framework, while the multi-scale fusion module leverages a
Transformer-Mamba architecture, which also serves as the foundation for the
decoder design. We introduce a novel multimodal fusion module grounded in
self-attention mechanisms and state space models, marking the first application
of the Mamba architecture in the field of transparent object depth completion
and revealing its promising potential. Additionally, we design an innovative
multi-scale fusion module for the decoder that combines channel attention,
spatial attention, and multi-scale feature extraction techniques to effectively
integrate multi-scale features through a down-fusion strategy. Extensive
evaluations on multiple public datasets demonstrate that our model achieves
state-of-the-art(SOTA) performance, validating the effectiveness of our
approach.

</details>


### [328] [Create Anything Anywhere: Layout-Controllable Personalized Diffusion Model for Multiple Subjects](https://arxiv.org/pdf/2505.20909)
*Wei Li, Hebei Li, Yansong Peng, Siying Wu, Yueyi Zhang, Xiaoyan Sun*

Main category: cs.CV

TL;DR: LCP-Diffusion enhances text-to-image generation by integrating precise layout control and dynamic subject features, improving fidelity without tuning.


<details>
  <summary>Details</summary>
Motivation: Existing methods lack layout controllability and ignore dynamic features of reference subjects, limiting fidelity in personalized text-to-image generation.

Method: Proposes LCP-Diffusion with a Dynamic-Static Complementary Visual Refining module and Dual Layout Control mechanism for tuning-free layout and identity preservation.

Result: Extensive experiments show LCP-Diffusion excels in identity preservation and layout controllability, enabling "create anything anywhere."

Conclusion: LCP-Diffusion is a pioneering framework for personalized, layout-controllable text-to-image generation.

Abstract: Diffusion models have significantly advanced text-to-image generation, laying
the foundation for the development of personalized generative frameworks.
However, existing methods lack precise layout controllability and overlook the
potential of dynamic features of reference subjects in improving fidelity. In
this work, we propose Layout-Controllable Personalized Diffusion
(LCP-Diffusion) model, a novel framework that integrates subject identity
preservation with flexible layout guidance in a tuning-free approach. Our model
employs a Dynamic-Static Complementary Visual Refining module to
comprehensively capture the intricate details of reference subjects, and
introduces a Dual Layout Control mechanism to enforce robust spatial control
across both training and inference stages. Extensive experiments validate that
LCP-Diffusion excels in both identity preservation and layout controllability.
To the best of our knowledge, this is a pioneering work enabling users to
"create anything anywhere".

</details>


### [329] [Geometry-Editable and Appearance-Preserving Object Compositon](https://arxiv.org/pdf/2505.20914)
*Jianman Lin, Haojie Li, Chunmei Qing, Zhijing Yang, Liang Lin, Tianshui Chen*

Main category: cs.CV

TL;DR: DGAD introduces a diffusion model for object composition, disentangling geometry editing and appearance preservation using semantic embeddings and cross-attention retrieval.


<details>
  <summary>Details</summary>
Motivation: Current methods lose fine-grained appearance details due to compact semantic embeddings. DGAD aims to preserve these details while enabling geometry editing.

Method: DGAD uses CLIP/DINO-derived embeddings for geometry and a cross-attention mechanism for appearance alignment in diffusion models.

Result: DGAD achieves precise geometry editing and faithful appearance preservation, outperforming benchmarks.

Conclusion: DGAD effectively integrates geometry and appearance in object composition, advancing the field.

Abstract: General object composition (GOC) aims to seamlessly integrate a target object
into a background scene with desired geometric properties, while simultaneously
preserving its fine-grained appearance details. Recent approaches derive
semantic embeddings and integrate them into advanced diffusion models to enable
geometry-editable generation. However, these highly compact embeddings encode
only high-level semantic cues and inevitably discard fine-grained appearance
details. We introduce a Disentangled Geometry-editable and
Appearance-preserving Diffusion (DGAD) model that first leverages semantic
embeddings to implicitly capture the desired geometric transformations and then
employs a cross-attention retrieval mechanism to align fine-grained appearance
features with the geometry-edited representation, facilitating both precise
geometry editing and faithful appearance preservation in object composition.
Specifically, DGAD builds on CLIP/DINO-derived and reference networks to
extract semantic embeddings and appearance-preserving representations, which
are then seamlessly integrated into the encoding and decoding pipelines in a
disentangled manner. We first integrate the semantic embeddings into
pre-trained diffusion models that exhibit strong spatial reasoning capabilities
to implicitly capture object geometry, thereby facilitating flexible object
manipulation and ensuring effective editability. Then, we design a dense
cross-attention mechanism that leverages the implicitly learned object geometry
to retrieve and spatially align appearance features with their corresponding
regions, ensuring faithful appearance consistency. Extensive experiments on
public benchmarks demonstrate the effectiveness of the proposed DGAD framework.

</details>


### [330] [HuMoCon: Concept Discovery for Human Motion Understanding](https://arxiv.org/pdf/2505.20920)
*Qihang Fang, Chengcheng Tang, Bugra Tekin, Shugao Ma, Yanchao Yang*

Main category: cs.CV

TL;DR: HuMoCon is a motion-video framework for human behavior analysis, featuring motion concept discovery, multi-modal feature alignment, and velocity reconstruction to improve performance.


<details>
  <summary>Details</summary>
Motivation: Addresses challenges in motion concept discovery, such as lack of multi-modality alignment and loss of high-frequency information in existing methods.

Method: Integrates feature alignment (video and motion) and velocity reconstruction to enhance high-frequency features and reduce temporal over-smoothing.

Result: Outperforms state-of-the-art methods in human motion understanding on standard benchmarks.

Conclusion: HuMoCon effectively discovers motion concepts and advances human motion analysis; code will be open-sourced.

Abstract: We present HuMoCon, a novel motion-video understanding framework designed for
advanced human behavior analysis. The core of our method is a human motion
concept discovery framework that efficiently trains multi-modal encoders to
extract semantically meaningful and generalizable features. HuMoCon addresses
key challenges in motion concept discovery for understanding and reasoning,
including the lack of explicit multi-modality feature alignment and the loss of
high-frequency information in masked autoencoding frameworks. Our approach
integrates a feature alignment strategy that leverages video for contextual
understanding and motion for fine-grained interaction modeling, further with a
velocity reconstruction mechanism to enhance high-frequency feature expression
and mitigate temporal over-smoothing. Comprehensive experiments on standard
benchmarks demonstrate that HuMoCon enables effective motion concept discovery
and significantly outperforms state-of-the-art methods in training large models
for human motion understanding. We will open-source the associated code with
our paper.

</details>


### [331] [Good Enough: Is it Worth Improving your Label Quality?](https://arxiv.org/pdf/2505.20928)
*Alexander Jaus, Zdravko Marinov, Constantin Seibold, Simon Reiß, Jens Kleesiek, Rainer Stiefelhagen*

Main category: cs.CV

TL;DR: The paper evaluates the impact of label quality in medical image segmentation, finding that high-quality labels improve in-domain performance but have minimal effect on pre-training.


<details>
  <summary>Details</summary>
Motivation: To determine whether the costly effort of improving label quality in medical image segmentation yields significant benefits.

Method: Systematic evaluation using multiple pseudo-labeled versions of CT datasets generated by models like nnU-Net, TotalSegmentator, and MedSAM.

Result: Higher-quality labels improve in-domain performance, but gains are unclear below a threshold. Pre-training is minimally affected by label quality.

Conclusion: Improving label quality is beneficial for in-domain tasks but may not be worth the effort for pre-training.

Abstract: Improving label quality in medical image segmentation is costly, but its
benefits remain unclear. We systematically evaluate its impact using multiple
pseudo-labeled versions of CT datasets, generated by models like nnU-Net,
TotalSegmentator, and MedSAM. Our results show that while higher-quality labels
improve in-domain performance, gains remain unclear if below a small threshold.
For pre-training, label quality has minimal impact, suggesting that models
rather transfer general concepts than detailed annotations. These findings
provide guidance on when improving label quality is worth the effort.

</details>


### [332] [QwT-v2: Practical, Effective and Efficient Post-Training Quantization](https://arxiv.org/pdf/2505.20932)
*Ningyuan Tang, Minghao Fu, Hao Yu, Jianxin Wu*

Main category: cs.CV

TL;DR: QwT-v2 improves upon QwT by reducing extra parameters and latency while maintaining accuracy and hardware compatibility.


<details>
  <summary>Details</summary>
Motivation: Network quantization reduces resource consumption in deep neural networks, but existing methods like QwT have drawbacks like extra parameters, latency, and hardware incompatibility.

Method: QwT-v2 uses a lightweight channel-wise affine compensation (CWAC) module to minimize extra parameters and computations.

Result: QwT-v2 matches or outperforms QwT in accuracy with fewer resources and is compatible with most hardware platforms.

Conclusion: QwT-v2 resolves major defects of QwT, offering a practical and efficient solution for network quantization.

Abstract: Network quantization is arguably one of the most practical network
compression approaches for reducing the enormous resource consumption of modern
deep neural networks. They usually require diverse and subtle design choices
for specific architecture and tasks. Instead, the QwT method is a simple and
general approach which introduces lightweight additional structures to improve
quantization. But QwT incurs extra parameters and latency. More importantly,
QwT is not compatible with many hardware platforms. In this paper, we propose
QwT-v2, which not only enjoys all advantages of but also resolves major defects
of QwT. By adopting a very lightweight channel-wise affine compensation (CWAC)
module, QwT-v2 introduces significantly less extra parameters and computations
compared to QwT, and at the same time matches or even outperforms QwT in
accuracy. The compensation module of QwT-v2 can be integrated into quantization
inference engines with little effort, which not only effectively removes the
extra costs but also makes it compatible with most existing hardware platforms.

</details>


### [333] [ISAC: Training-Free Instance-to-Semantic Attention Control for Improving Multi-Instance Generation](https://arxiv.org/pdf/2505.20935)
*Sanghyun Jo, Wooyeol Lee, Ziseok Lee, Kyungsu Kim*

Main category: cs.CV

TL;DR: ISAC, a training-free method, improves multi-instance text-to-image generation by disentangling objects and aligning them with semantic labels, achieving high accuracy.


<details>
  <summary>Details</summary>
Motivation: Addressing the challenge of multi-instance scenarios in text-to-image diffusion models, where objects are often merged or omitted.

Method: Instance-to-Semantic Attention Control (ISAC) uses an instance-first approach and hierarchical prompts to disentangle and align objects.

Result: Achieves 52% multi-class accuracy and 83% multi-instance accuracy without external models.

Conclusion: ISAC effectively resolves instance formation and semantic entanglement, enhancing multi-instance generation.

Abstract: Text-to-image diffusion models excel at generating single-instance scenes but
struggle with multi-instance scenarios, often merging or omitting objects.
Unlike previous training-free approaches that rely solely on semantic-level
guidance without addressing instance individuation, our training-free method,
Instance-to-Semantic Attention Control (ISAC), explicitly resolves incomplete
instance formation and semantic entanglement through an instance-first modeling
approach. This enables ISAC to effectively leverage a hierarchical,
tree-structured prompt mechanism, disentangling multiple object instances and
individually aligning them with their corresponding semantic labels. Without
employing any external models, ISAC achieves up to 52% average multi-class
accuracy and 83% average multi-instance accuracy by effectively forming
disentangled instances. The code will be made available upon publication.

</details>


### [334] [PMA: Towards Parameter-Efficient Point Cloud Understanding via Point Mamba Adapter](https://arxiv.org/pdf/2505.20941)
*Yaohua Zha, Yanzi Wang, Hang Guo, Jinpeng Wang, Tao Dai, Bin Chen, Zhihao Ouyang, Xue Yuerong, Ke Chen, Shu-Tao Xia*

Main category: cs.CV

TL;DR: The paper introduces Point Mamba Adapter (PMA), a method to enhance point cloud understanding by leveraging intermediate layers of pre-trained models, addressing the limitation of existing strategies that only use final outputs.


<details>
  <summary>Details</summary>
Motivation: Existing methods for point cloud understanding underutilize pre-trained models by ignoring intermediate layer information, limiting their potential.

Method: PMA constructs an ordered feature sequence from all layers of pre-trained models and uses Mamba to fuse complementary semantics. A geometry-constrained gate prompt generator (G2PG) optimizes spatial order for effective multi-layer integration.

Result: Experiments on challenging datasets show PMA significantly improves point cloud understanding by fusing intermediate features.

Conclusion: PMA advances point cloud understanding by effectively utilizing intermediate layer information, setting a new benchmark for performance.

Abstract: Applying pre-trained models to assist point cloud understanding has recently
become a mainstream paradigm in 3D perception. However, existing application
strategies are straightforward, utilizing only the final output of the
pre-trained model for various task heads. It neglects the rich complementary
information in the intermediate layer, thereby failing to fully unlock the
potential of pre-trained models. To overcome this limitation, we propose an
orthogonal solution: Point Mamba Adapter (PMA), which constructs an ordered
feature sequence from all layers of the pre-trained model and leverages Mamba
to fuse all complementary semantics, thereby promoting comprehensive point
cloud understanding. Constructing this ordered sequence is non-trivial due to
the inherent isotropy of 3D space. Therefore, we further propose a
geometry-constrained gate prompt generator (G2PG) shared across different
layers, which applies shared geometric constraints to the output gates of the
Mamba and dynamically optimizes the spatial order, thus enabling more effective
integration of multi-layer information. Extensive experiments conducted on
challenging point cloud datasets across various tasks demonstrate that our PMA
elevates the capability for point cloud understanding to a new level by fusing
diverse complementary intermediate features. Code is available at
https://github.com/zyh16143998882/PMA.

</details>


### [335] [DSOcc: Leveraging Depth Awareness and Semantic Aid to Boost Camera-Based 3D Semantic Occupancy Prediction](https://arxiv.org/pdf/2505.20951)
*Naiyu Fang, Zheyuan Zhou, Kang Wang, Ruibo Li, Lemiao Qiu, Shuyou Zhang, Zhe Wang, Guosheng Lin*

Main category: cs.CV

TL;DR: DSOcc improves camera-based 3D semantic occupancy prediction by integrating depth awareness and semantic aid, achieving state-of-the-art results.


<details>
  <summary>Details</summary>
Motivation: Existing methods suffer from incorrect feature assignments and insufficient samples for occupancy class inference.

Method: Jointly performs occupancy state and class inference using depth-aware voxel representation and semantic segmentation fusion.

Result: Achieves state-of-the-art performance on the SemanticKITTI dataset.

Conclusion: DSOcc effectively addresses challenges in 3D semantic occupancy prediction with depth and semantic integration.

Abstract: Camera-based 3D semantic occupancy prediction offers an efficient and
cost-effective solution for perceiving surrounding scenes in autonomous
driving. However, existing works rely on explicit occupancy state inference,
leading to numerous incorrect feature assignments, and insufficient samples
restrict the learning of occupancy class inference. To address these
challenges, we propose leveraging Depth awareness and Semantic aid to boost
camera-based 3D semantic Occupancy prediction (DSOcc). We jointly perform
occupancy state and occupancy class inference, where soft occupancy confidence
is calculated through non-learning method and multiplied with image features to
make the voxel representation aware of depth, enabling adaptive implicit
occupancy state inference. Rather than focusing on improving feature learning,
we directly utilize well-trained image semantic segmentation and fuse multiple
frames with their occupancy probabilities to aid occupancy class inference,
thereby enhancing robustness. Experimental results demonstrate that DSOcc
achieves state-of-the-art performance on the SemanticKITTI dataset among
camera-based methods.

</details>


### [336] [OrienText: Surface Oriented Textual Image Generation](https://arxiv.org/pdf/2505.20958)
*Shubham Singh Paliwal, Arushi Jain, Monika Sharma, Vikram Jamwal, Lovekesh Vig*

Main category: cs.CV

TL;DR: OrienText improves text-to-image generation by using surface normals to accurately render text on complex surfaces.


<details>
  <summary>Details</summary>
Motivation: Current T2I models struggle with accurate text placement on complex surfaces, which is crucial for e-commerce and marketing.

Method: OrienText uses region-specific surface normals as conditional input to a T2I diffusion model for precise text rendering.

Result: The method outperforms existing T2I models in rendering text on complex surfaces, as shown on a self-curated dataset.

Conclusion: OrienText effectively addresses the challenge of accurate text placement in T2I generation, enhancing its practical applications.

Abstract: Textual content in images is crucial in e-commerce sectors, particularly in
marketing campaigns, product imaging, advertising, and the entertainment
industry. Current text-to-image (T2I) generation diffusion models, though
proficient at producing high-quality images, often struggle to incorporate text
accurately onto complex surfaces with varied perspectives, such as angled views
of architectural elements like buildings, banners, or walls. In this paper, we
introduce the Surface Oriented Textual Image Generation (OrienText) method,
which leverages region-specific surface normals as conditional input to T2I
generation diffusion model. Our approach ensures accurate rendering and correct
orientation of the text within the image context. We demonstrate the
effectiveness of the OrienText method on a self-curated dataset of images and
compare it against the existing textual image generation methods.

</details>


### [337] [RF4D:Neural Radar Fields for Novel View Synthesis in Outdoor Dynamic Scenes](https://arxiv.org/pdf/2505.20967)
*Jiarui Zhang, Zhihao Li, Chong Wang, Bihan Wen*

Main category: cs.CV

TL;DR: RF4D is a radar-based neural field framework for novel view synthesis in dynamic outdoor scenes, addressing fragility to adverse weather and improving temporal coherence.


<details>
  <summary>Details</summary>
Motivation: Existing neural field methods are fragile in adverse weather and lack exploration of radar integration. Outdoor scenarios require spatiotemporal modeling for moving objects.

Method: RF4D incorporates temporal information and a feature-level flow module for temporal coherence. It uses a radar-specific power rendering formulation.

Result: RF4D outperforms in radar measurement synthesis and occupancy estimation, especially in dynamic scenes.

Conclusion: RF4D effectively integrates radar with neural fields for robust and temporally consistent novel view synthesis in outdoor dynamic scenes.

Abstract: Neural fields (NFs) have demonstrated remarkable performance in scene
reconstruction, powering various tasks such as novel view synthesis. However,
existing NF methods relying on RGB or LiDAR inputs often exhibit severe
fragility to adverse weather, particularly when applied in outdoor scenarios
like autonomous driving. In contrast, millimeter-wave radar is inherently
robust to environmental changes, while unfortunately, its integration with NFs
remains largely underexplored. Besides, as outdoor driving scenarios frequently
involve moving objects, making spatiotemporal modeling essential for temporally
consistent novel view synthesis. To this end, we introduce RF4D, a radar-based
neural field framework specifically designed for novel view synthesis in
outdoor dynamic scenes. RF4D explicitly incorporates temporal information into
its representation, significantly enhancing its capability to model moving
objects. We further introduce a feature-level flow module that predicts latent
temporal offsets between adjacent frames, enforcing temporal coherence in
dynamic scene modeling. Moreover, we propose a radar-specific power rendering
formulation closely aligned with radar sensing physics, improving synthesis
accuracy and interoperability. Extensive experiments on public radar datasets
demonstrate the superior performance of RF4D in terms of radar measurement
synthesis quality and occupancy estimation accuracy, achieving especially
pronounced improvements in dynamic outdoor scenarios.

</details>


### [338] [DreamBoothDPO: Improving Personalized Generation using Direct Preference Optimization](https://arxiv.org/pdf/2505.20975)
*Shamil Ayupov, Maksim Nakhodnov, Anastasia Yaschenko, Andrey Kuznetsov, Aibek Alanov*

Main category: cs.CV

TL;DR: An RL-based method improves personalized diffusion models by balancing concept fidelity and contextual alignment without human-annotated scores.


<details>
  <summary>Details</summary>
Motivation: Addressing the challenge of balancing concept fidelity and contextual alignment in personalized T2I generation.

Method: Uses RL to generate synthetic paired datasets for DPO-like training, leveraging external quality metrics for better-worse pairs.

Result: Outperforms baselines in convergence speed and output quality, with flexible trade-off adjustment.

Conclusion: The approach effectively improves T2I generation across architectures and fine-tuning techniques.

Abstract: Personalized diffusion models have shown remarkable success in Text-to-Image
(T2I) generation by enabling the injection of user-defined concepts into
diverse contexts. However, balancing concept fidelity with contextual alignment
remains a challenging open problem. In this work, we propose an RL-based
approach that leverages the diverse outputs of T2I models to address this
issue. Our method eliminates the need for human-annotated scores by generating
a synthetic paired dataset for DPO-like training using external quality
metrics. These better-worse pairs are specifically constructed to improve both
concept fidelity and prompt adherence. Moreover, our approach supports flexible
adjustment of the trade-off between image fidelity and textual alignment.
Through multi-step training, our approach outperforms a naive baseline in
convergence speed and output quality. We conduct extensive qualitative and
quantitative analysis, demonstrating the effectiveness of our method across
various architectures and fine-tuning techniques. The source code can be found
at https://github.com/ControlGenAI/DreamBoothDPO.

</details>


### [339] [RefAV: Towards Planning-Centric Scenario Mining](https://arxiv.org/pdf/2505.20981)
*Cainan Davidson, Deva Ramanan, Neehar Peri*

Main category: cs.CV

TL;DR: The paper introduces RefAV, a dataset for spatio-temporal scenario mining in AVs using vision-language models, highlighting challenges in repurposing existing models.


<details>
  <summary>Details</summary>
Motivation: Identifying safety-critical scenarios in uncurated AV driving logs is challenging with traditional methods.

Method: Uses vision-language models (VLMs) to detect and localize described scenarios, introducing the RefAV dataset with 10,000 natural language queries.

Result: Off-the-shelf VLMs perform poorly, indicating unique challenges in scenario mining.

Conclusion: RefAV provides a foundation for future work, but specialized solutions are needed for effective scenario mining.

Abstract: Autonomous Vehicles (AVs) collect and pseudo-label terabytes of multi-modal
data localized to HD maps during normal fleet testing. However, identifying
interesting and safety-critical scenarios from uncurated driving logs remains a
significant challenge. Traditional scenario mining techniques are error-prone
and prohibitively time-consuming, often relying on hand-crafted structured
queries. In this work, we revisit spatio-temporal scenario mining through the
lens of recent vision-language models (VLMs) to detect whether a described
scenario occurs in a driving log and, if so, precisely localize it in both time
and space. To address this problem, we introduce RefAV, a large-scale dataset
of 10,000 diverse natural language queries that describe complex multi-agent
interactions relevant to motion planning derived from 1000 driving logs in the
Argoverse 2 Sensor dataset. We evaluate several referential multi-object
trackers and present an empirical analysis of our baselines. Notably, we find
that naively repurposing off-the-shelf VLMs yields poor performance, suggesting
that scenario mining presents unique challenges. Our code and dataset are
available at https://github.com/CainanD/RefAV/ and
https://argoverse.github.io/user-guide/tasks/scenario_mining.html

</details>


### [340] [Assessing the Use of Face Swapping Methods as Face Anonymizers in Videos](https://arxiv.org/pdf/2505.20985)
*Mustafa İzzet Muştu, Hazım Kemal Ekenel*

Main category: cs.CV

TL;DR: Face swapping methods are effective for anonymizing video data while maintaining quality, as shown by evaluations of temporal consistency, anonymity strength, and visual fidelity.


<details>
  <summary>Details</summary>
Motivation: The need for privacy-preserving visual data due to regulations and large-scale demand drives the exploration of face swapping for anonymization.

Method: Extensive evaluations of face swapping techniques, focusing on temporal consistency, anonymity strength, and visual fidelity.

Result: Face swapping produces consistent facial transitions and effectively hides identities, making it suitable for privacy-preserving video applications.

Conclusion: Face swapping is a promising method for anonymization, with potential for future advancements in this area.

Abstract: The increasing demand for large-scale visual data, coupled with strict
privacy regulations, has driven research into anonymization methods that hide
personal identities without seriously degrading data quality. In this paper, we
explore the potential of face swapping methods to preserve privacy in video
data. Through extensive evaluations focusing on temporal consistency, anonymity
strength, and visual fidelity, we find that face swapping techniques can
produce consistent facial transitions and effectively hide identities. These
results underscore the suitability of face swapping for privacy-preserving
video applications and lay the groundwork for future advancements in
anonymization focused face-swapping models.

</details>


### [341] [Facial Attribute Based Text Guided Face Anonymization](https://arxiv.org/pdf/2505.21002)
*Mustafa İzzet Muştu, Hazım Kemal Ekenel*

Main category: cs.CV

TL;DR: A deep learning pipeline for face anonymization using diffusion models to generate privacy-compliant datasets.


<details>
  <summary>Details</summary>
Motivation: Address privacy concerns in computer vision by anonymizing faces without compromising data quality.

Method: Three-stage pipeline: face detection (RetinaNet), feature extraction (VGG-Face), and realistic face generation (BrushNet diffusion model).

Result: Generates natural-looking images with unrecognizable faces, enabling privacy-compliant datasets.

Conclusion: The pipeline effectively anonymizes faces while maintaining dataset utility for research.

Abstract: The increasing prevalence of computer vision applications necessitates
handling vast amounts of visual data, often containing personal information.
While this technology offers significant benefits, it should not compromise
privacy. Data privacy regulations emphasize the need for individual consent for
processing personal data, hindering researchers' ability to collect
high-quality datasets containing the faces of the individuals. This paper
presents a deep learning-based face anonymization pipeline to overcome this
challenge. Unlike most of the existing methods, our method leverages recent
advancements in diffusion-based inpainting models, eliminating the need for
training Generative Adversarial Networks. The pipeline employs a three-stage
approach: face detection with RetinaNet, feature extraction with VGG-Face, and
realistic face generation using the state-of-the-art BrushNet diffusion model.
BrushNet utilizes the entire image, face masks, and text prompts specifying
desired facial attributes like age, ethnicity, gender, and expression. This
enables the generation of natural-looking images with unrecognizable
individuals, facilitating the creation of privacy-compliant datasets for
computer vision research.

</details>


### [342] [Unified Alignment Protocol: Making Sense of the Unlabeled Data in New Domains](https://arxiv.org/pdf/2505.21010)
*Sabbir Ahmed, Mamshad Nayeem Rizve, Abdullah Al Arafat, Jacqueline Liu, Rahim Hossain, Mohaiminul Al Nahian, Adnan Siraj Rakin*

Main category: cs.CV

TL;DR: A novel framework, Unified Alignment Protocol (UAP), improves generalization in Semi-Supervised Federated Learning (SSFL) by aligning server and client features in a two-stage training process.


<details>
  <summary>Details</summary>
Motivation: Traditional SSFL assumes identical data distributions in training and testing, but domain shifts are common. Enhancing generalization in SSFL is crucial for real-world applications like healthcare.

Method: UAP involves a two-stage process: (1) server model training to align features with a parametric distribution, and (2) client training using the server's feature distribution for alignment.

Result: UAP achieves state-of-the-art generalization performance on domain generalization benchmarks in SSFL settings.

Conclusion: UAP effectively addresses domain shift challenges in SSFL, enhancing model generalization without additional communication overhead.

Abstract: Semi-Supervised Federated Learning (SSFL) is gaining popularity over
conventional Federated Learning in many real-world applications. Due to the
practical limitation of limited labeled data on the client side, SSFL considers
that participating clients train with unlabeled data, and only the central
server has the necessary resources to access limited labeled data, making it an
ideal fit for real-world applications (e.g., healthcare). However, traditional
SSFL assumes that the data distributions in the training phase and testing
phase are the same. In practice, however, domain shifts frequently occur,
making it essential for SSFL to incorporate generalization capabilities and
enhance their practicality. The core challenge is improving model
generalization to new, unseen domains while the client participate in SSFL.
However, the decentralized setup of SSFL and unsupervised client training
necessitates innovation to achieve improved generalization across domains. To
achieve this, we propose a novel framework called the Unified Alignment
Protocol (UAP), which consists of an alternating two-stage training process.
The first stage involves training the server model to learn and align the
features with a parametric distribution, which is subsequently communicated to
clients without additional communication overhead. The second stage proposes a
novel training algorithm that utilizes the server feature distribution to align
client features accordingly. Our extensive experiments on standard domain
generalization benchmark datasets across multiple model architectures reveal
that proposed UAP successfully achieves SOTA generalization performance in SSFL
setting.

</details>


### [343] [FeatInv: Spatially resolved mapping from feature space to input space using conditional diffusion models](https://arxiv.org/pdf/2505.21032)
*Nils Neukirch, Johanna Vielhaben, Nils Strodthoff*

Main category: cs.CV

TL;DR: A conditional diffusion model is proposed to map feature space to input space for better interpretation of deep neural networks, demonstrating strong reconstruction and applications in visualization and analysis.


<details>
  <summary>Details</summary>
Motivation: Internal representations in deep neural networks are hard to interpret, and existing methods for mapping feature space to input space are often approximate.

Method: Uses a conditional diffusion model, pretrained and conditioned on feature maps, to probabilistically map feature space to input space.

Result: Shows excellent reconstruction across CNNs and ViTs, validated through qualitative comparisons and robustness analysis.

Conclusion: The method improves feature space understanding and has broad applications in computer vision.

Abstract: Internal representations are crucial for understanding deep neural networks,
such as their properties and reasoning patterns, but remain difficult to
interpret. While mapping from feature space to input space aids in interpreting
the former, existing approaches often rely on crude approximations. We propose
using a conditional diffusion model - a pretrained high-fidelity diffusion
model conditioned on spatially resolved feature maps - to learn such a mapping
in a probabilistic manner. We demonstrate the feasibility of this approach
across various pretrained image classifiers from CNNs to ViTs, showing
excellent reconstruction capabilities. Through qualitative comparisons and
robustness analysis, we validate our method and showcase possible applications,
such as the visualization of concept steering in input space or investigations
of the composite nature of the feature space. This approach has broad potential
for improving feature space understanding in computer vision models.

</details>


### [344] [RainFusion: Adaptive Video Generation Acceleration via Multi-Dimensional Visual Redundancy](https://arxiv.org/pdf/2505.21036)
*Aiyue Chen, Bin Dong, Jingru Li, Jing Lin, Yiwu Yao, Gongyi Wang*

Main category: cs.CV

TL;DR: RainFusion is a training-free sparse attention method for video generation, reducing computational costs by 2× while preserving quality.


<details>
  <summary>Details</summary>
Motivation: Diffusion models for video generation are computationally expensive, with 3D attention consuming over 80% of resources. RainFusion addresses this by leveraging sparsity in visual data.

Method: RainFusion identifies three sparse patterns (Spatial, Temporal, Textural) and uses an Adaptive Recognition Module (ARM) to determine patterns online with minimal overhead. It integrates into existing models without training.

Result: RainFusion achieves a 2× speedup in attention computation with negligible impact on video quality (-0.2% VBench score drop).

Conclusion: RainFusion is a plug-and-play solution for efficient video generation, compatible with leading models like HunyuanVideo and OpenSoraPlan-1.2.

Abstract: Video generation using diffusion models is highly computationally intensive,
with 3D attention in Diffusion Transformer (DiT) models accounting for over
80\% of the total computational resources. In this work, we introduce {\bf
RainFusion}, a novel training-free sparse attention method that exploits
inherent sparsity nature in visual data to accelerate attention computation
while preserving video quality. Specifically, we identify three unique sparse
patterns in video generation attention calculations--Spatial Pattern, Temporal
Pattern and Textural Pattern. The sparse pattern for each attention head is
determined online with negligible overhead (\textasciitilde\,0.2\%) with our
proposed {\bf ARM} (Adaptive Recognition Module) during inference. Our proposed
{\bf RainFusion} is a plug-and-play method, that can be seamlessly integrated
into state-of-the-art 3D-attention video generation models without additional
training or calibration. We evaluate our method on leading open-sourced models
including HunyuanVideo, OpenSoraPlan-1.2 and CogVideoX-5B, demonstrating its
broad applicability and effectiveness. Experimental results show that
RainFusion achieves over {\bf 2\(\times\)} speedup in attention computation
while maintaining video quality, with only a minimal impact on VBench scores
(-0.2\%).

</details>


### [345] [Robust Video-Based Pothole Detection and Area Estimation for Intelligent Vehicles with Depth Map and Kalman Smoothing](https://arxiv.org/pdf/2505.21049)
*Dehao Wang, Haohang Zhu, Yiwen Xu, Kaiqi Liu*

Main category: cs.CV

TL;DR: A robust framework for pothole area estimation integrates object detection and monocular depth estimation, improving accuracy and robustness in real-world environments.


<details>
  <summary>Details</summary>
Motivation: Potholes threaten driving safety, and existing vision-based methods are error-prone due to camera angle variations and flat-road assumptions.

Method: Proposes ACSH-YOLOv8 for detection, BoT-SORT for tracking, DepthAnything V2 for depth maps, and MBTP for area estimation, optimized with CDKF.

Result: ACSH-YOLOv8 achieves 76.6% AP(50), a 7.6% improvement over YOLOv8, with CDKF enhancing robustness.

Conclusion: The framework improves pothole detection and area estimation, making it more practical for real-world applications.

Abstract: Road potholes pose a serious threat to driving safety and comfort, making
their detection and assessment a critical task in fields such as autonomous
driving. When driving vehicles, the operators usually avoid large potholes and
approach smaller ones at reduced speeds to ensure safety. Therefore, accurately
estimating pothole area is of vital importance. Most existing vision-based
methods rely on distance priors to construct geometric models. However, their
performance is susceptible to variations in camera angles and typically relies
on the assumption of a flat road surface, potentially leading to significant
errors in complex real-world environments. To address these problems, a robust
pothole area estimation framework that integrates object detection and
monocular depth estimation in a video stream is proposed in this paper. First,
to enhance pothole feature extraction and improve the detection of small
potholes, ACSH-YOLOv8 is proposed with ACmix module and the small object
detection head. Then, the BoT-SORT algorithm is utilized for pothole tracking,
while DepthAnything V2 generates depth maps for each frame. With the obtained
depth maps and potholes labels, a novel Minimum Bounding Triangulated Pixel
(MBTP) method is proposed for pothole area estimation. Finally, Kalman Filter
based on Confidence and Distance (CDKF) is developed to maintain consistency of
estimation results across consecutive frames. The results show that ACSH-YOLOv8
model achieves an AP(50) of 76.6%, representing a 7.6% improvement over YOLOv8.
Through CDKF optimization across consecutive frames, pothole predictions become
more robust, thereby enhancing the method's practical applicability.

</details>


### [346] [Advancing high-fidelity 3D and Texture Generation with 2.5D latents](https://arxiv.org/pdf/2505.21050)
*Xin Yang, Jiantao Lin, Yingjie Xu, Haodong Li, Yingcong Chen*

Main category: cs.CV

TL;DR: A novel framework for joint 3D geometry and texture generation using 2.5D representations, outperforming existing methods in quality and coherence.


<details>
  <summary>Details</summary>
Motivation: Addressing the lack of coherence between separately generated 3D geometry and texture, and the complexity of existing datasets.

Method: Integrates multiview RGB, normal, and coordinate images into 2.5D latents, leverages pre-trained 2D models, and refines to 3D with a lightweight decoder.

Result: Generates high-quality 3D objects with coherent structure and color, excelling in geometry-conditioned texture generation.

Conclusion: The proposed framework effectively unifies 3D generation, improving coherence and quality over existing approaches.

Abstract: Despite the availability of large-scale 3D datasets and advancements in 3D
generative models, the complexity and uneven quality of 3D geometry and texture
data continue to hinder the performance of 3D generation techniques. In most
existing approaches, 3D geometry and texture are generated in separate stages
using different models and non-unified representations, frequently leading to
unsatisfactory coherence between geometry and texture. To address these
challenges, we propose a novel framework for joint generation of 3D geometry
and texture. Specifically, we focus in generate a versatile 2.5D
representations that can be seamlessly transformed between 2D and 3D. Our
approach begins by integrating multiview RGB, normal, and coordinate images
into a unified representation, termed as 2.5D latents. Next, we adapt
pre-trained 2D foundation models for high-fidelity 2.5D generation, utilizing
both text and image conditions. Finally, we introduce a lightweight 2.5D-to-3D
refiner-decoder framework that efficiently generates detailed 3D
representations from 2.5D images. Extensive experiments demonstrate that our
model not only excels in generating high-quality 3D objects with coherent
structure and color from text and image inputs but also significantly
outperforms existing methods in geometry-conditioned texture generation.

</details>


### [347] [Styl3R: Instant 3D Stylized Reconstruction for Arbitrary Scenes and Styles](https://arxiv.org/pdf/2505.21060)
*Peng Wang, Xiang Liu, Peidong Liu*

Main category: cs.CV

TL;DR: A novel method for instant 3D scene stylization using unposed sparse-view images and a style image, achieving high-quality results with multi-view consistency and efficiency.


<details>
  <summary>Details</summary>
Motivation: Addressing the challenge of stylizing 3D scenes instantly while maintaining multi-view consistency and resembling a style image, without relying on computationally intensive optimization.

Method: Introduces a branched architecture separating structure modeling and appearance shading, and uses an identity loss for pre-training through novel view synthesis.

Result: Produces high-quality stylized 3D content with superior style-scene blending, multi-view consistency, and efficiency, outperforming existing methods.

Conclusion: The approach successfully achieves direct 3D stylization quickly and effectively, balancing style transfer and scene reconstruction.

Abstract: Stylizing 3D scenes instantly while maintaining multi-view consistency and
faithfully resembling a style image remains a significant challenge. Current
state-of-the-art 3D stylization methods typically involve computationally
intensive test-time optimization to transfer artistic features into a
pretrained 3D representation, often requiring dense posed input images. In
contrast, leveraging recent advances in feed-forward reconstruction models, we
demonstrate a novel approach to achieve direct 3D stylization in less than a
second using unposed sparse-view scene images and an arbitrary style image. To
address the inherent decoupling between reconstruction and stylization, we
introduce a branched architecture that separates structure modeling and
appearance shading, effectively preventing stylistic transfer from distorting
the underlying 3D scene structure. Furthermore, we adapt an identity loss to
facilitate pre-training our stylization model through the novel view synthesis
task. This strategy also allows our model to retain its original reconstruction
capabilities while being fine-tuned for stylization. Comprehensive evaluations,
using both in-domain and out-of-domain datasets, demonstrate that our approach
produces high-quality stylized 3D content that achieve a superior blend of
style and scene appearance, while also outperforming existing methods in terms
of multi-view consistency and efficiency.

</details>


### [348] [LPOI: Listwise Preference Optimization for Vision Language Models](https://arxiv.org/pdf/2505.21061)
*Fatemeh Pesaran Zadeh, Yoojin Oh, Gunhee Kim*

Main category: cs.CV

TL;DR: LPOI is a novel object-aware listwise preference optimization method for VLMs, reducing hallucinations by ranking incrementally masked images without extra annotations.


<details>
  <summary>Details</summary>
Motivation: Existing methods like RLHF and DPO overfit to text or worsen hallucinations; LPOI addresses this by leveraging listwise optimization with masked objects.

Method: LPOI masks a critical object in images, interpolates the masked region between positive and negative samples, and trains the model to rank images by object visibility.

Result: LPOI outperforms existing methods on benchmarks like MMHalBench, AMBER, and Object HalBench in reducing hallucinations and improving VLM performance.

Conclusion: LPOI effectively reduces hallucinations in VLMs using listwise preference optimization, requiring no additional annotations and maintaining visual fidelity.

Abstract: Aligning large VLMs with human preferences is a challenging task, as methods
like RLHF and DPO often overfit to textual information or exacerbate
hallucinations. Although augmenting negative image samples partially addresses
these pitfalls, no prior work has employed listwise preference optimization for
VLMs, due to the complexity and cost of constructing listwise image samples. In
this work, we propose LPOI, the first object-aware listwise preference
optimization developed for reducing hallucinations in VLMs. LPOI identifies and
masks a critical object in the image, and then interpolates the masked region
between the positive and negative images to form a sequence of incrementally
more complete images. The model is trained to rank these images in ascending
order of object visibility, effectively reducing hallucinations while retaining
visual fidelity. LPOI requires no extra annotations beyond standard pairwise
preference data, as it automatically constructs the ranked lists through object
masking and interpolation. Comprehensive experiments on MMHalBench, AMBER, and
Object HalBench confirm that LPOI outperforms existing preference optimization
methods in reducing hallucinations and enhancing VLM performance. We make the
code available at https://github.com/fatemehpesaran310/lpoi.

</details>


### [349] [Inverse Virtual Try-On: Generating Multi-Category Product-Style Images from Clothed Individuals](https://arxiv.org/pdf/2505.21062)
*Davide Lobba, Fulvio Sanguigni, Bin Ren, Marcella Cornia, Rita Cucchiara, Nicu Sebe*

Main category: cs.CV

TL;DR: TEMU-VTOFF introduces a novel architecture for virtual try-off (VTOFF), improving garment feature extraction and multi-category applicability, outperforming existing methods.


<details>
  <summary>Details</summary>
Motivation: Addressing limitations in existing VTOFF approaches, such as poor garment disentanglement and single-category restrictions, to enhance dataset generation.

Method: Uses a dual DiT-based backbone with multimodal attention, incorporating images, text, and masks, plus an alignment module for refinement.

Result: Achieves state-of-the-art performance on VITON-HD and Dress Code datasets, improving visual quality and garment fidelity.

Conclusion: TEMU-VTOFF advances VTOFF by robustly handling multi-category garments and refining outputs, proving effective for dataset enhancement.

Abstract: While virtual try-on (VTON) systems aim to render a garment onto a target
person image, this paper tackles the novel task of virtual try-off (VTOFF),
which addresses the inverse problem: generating standardized product images of
garments from real-world photos of clothed individuals. Unlike VTON, which must
resolve diverse pose and style variations, VTOFF benefits from a consistent and
well-defined output format -- typically a flat, lay-down-style representation
of the garment -- making it a promising tool for data generation and dataset
enhancement. However, existing VTOFF approaches face two major limitations: (i)
difficulty in disentangling garment features from occlusions and complex poses,
often leading to visual artifacts, and (ii) restricted applicability to
single-category garments (e.g., upper-body clothes only), limiting
generalization. To address these challenges, we present Text-Enhanced
MUlti-category Virtual Try-Off (TEMU-VTOFF), a novel architecture featuring a
dual DiT-based backbone with a modified multimodal attention mechanism for
robust garment feature extraction. Our architecture is designed to receive
garment information from multiple modalities like images, text, and masks to
work in a multi-category setting. Finally, we propose an additional alignment
module to further refine the generated visual details. Experiments on VITON-HD
and Dress Code datasets show that TEMU-VTOFF sets a new state-of-the-art on the
VTOFF task, significantly improving both visual quality and fidelity to the
target garments.

</details>


### [350] [Minute-Long Videos with Dual Parallelisms](https://arxiv.org/pdf/2505.21070)
*Zeqing Wang, Bowen Zheng, Xingyi Yang, Yuecong Xu, Xinchao Wang*

Main category: cs.CV

TL;DR: A distributed inference strategy (DualParal) is proposed to reduce latency and memory costs for long video generation using Diffusion Transformer (DiT) models by parallelizing frames and layers across GPUs.


<details>
  <summary>Details</summary>
Motivation: Current DiT-based video diffusion models face high latency and memory costs for long videos, limiting scalability.

Method: DualParal parallelizes temporal frames and model layers across GPUs, using block-wise denoising, feature caching, and coordinated noise initialization to enable asynchronous computation.

Result: Achieves 6.54× lower latency and 1.48× lower memory cost for generating 1,025-frame videos on 8×RTX 4090 GPUs.

Conclusion: DualParal enables efficient, artifact-free, and infinitely long video generation with significant performance improvements.

Abstract: Diffusion Transformer (DiT)-based video diffusion models generate
high-quality videos at scale but incur prohibitive processing latency and
memory costs for long videos. To address this, we propose a novel distributed
inference strategy, termed DualParal. The core idea is that, instead of
generating an entire video on a single GPU, we parallelize both temporal frames
and model layers across GPUs. However, a naive implementation of this division
faces a key limitation: since diffusion models require synchronized noise
levels across frames, this implementation leads to the serialization of
original parallelisms. We leverage a block-wise denoising scheme to handle
this. Namely, we process a sequence of frame blocks through the pipeline with
progressively decreasing noise levels. Each GPU handles a specific block and
layer subset while passing previous results to the next GPU, enabling
asynchronous computation and communication. To further optimize performance, we
incorporate two key enhancements. Firstly, a feature cache is implemented on
each GPU to store and reuse features from the prior block as context,
minimizing inter-GPU communication and redundant computation. Secondly, we
employ a coordinated noise initialization strategy, ensuring globally
consistent temporal dynamics by sharing initial noise patterns across GPUs
without extra resource costs. Together, these enable fast, artifact-free, and
infinitely long video generation. Applied to the latest diffusion transformer
video generator, our method efficiently produces 1,025-frame videos with up to
6.54$\times$ lower latency and 1.48$\times$ lower memory cost on 8$\times$RTX
4090 GPUs.

</details>


### [351] [DynamicVL: Benchmarking Multimodal Large Language Models for Dynamic City Understanding](https://arxiv.org/pdf/2505.21076)
*Weihao Xuan, Junjue Wang, Heli Qi, Zihang Chen, Zhuo Zheng, Yanfei Zhong, Junshi Xia, Naoto Yokoya*

Main category: cs.CV

TL;DR: DVL-Suite is a framework for analyzing long-term urban dynamics using remote sensing imagery, addressing limitations of current multimodal models in Earth observation.


<details>
  <summary>Details</summary>
Motivation: Existing models lack capability for long-term temporal analysis in Earth observation, focusing only on single or bi-temporal imagery.

Method: Introduces DVL-Suite with DVL-Bench (7 urban tasks) and DVL-Instruct (instruction-tuning dataset), evaluated on 17 models.

Result: Reveals limitations in long-term temporal understanding and quantitative analysis, leading to DVLChat, a baseline model for comprehensive urban analysis.

Conclusion: DVL-Suite enhances multimodal models' capabilities for long-term Earth observation through specialized datasets and tasks.

Abstract: Multimodal large language models have demonstrated remarkable capabilities in
visual understanding, but their application to long-term Earth observation
analysis remains limited, primarily focusing on single-temporal or bi-temporal
imagery. To address this gap, we introduce DVL-Suite, a comprehensive framework
for analyzing long-term urban dynamics through remote sensing imagery. Our
suite comprises 15,063 high-resolution (1.0m) multi-temporal images spanning 42
megacities in the U.S. from 2005 to 2023, organized into two components:
DVL-Bench and DVL-Instruct. The DVL-Bench includes seven urban understanding
tasks, from fundamental change detection (pixel-level) to quantitative analyses
(regional-level) and comprehensive urban narratives (scene-level), capturing
diverse urban dynamics including expansion/transformation patterns, disaster
assessment, and environmental challenges. We evaluate 17 state-of-the-art
multimodal large language models and reveal their limitations in long-term
temporal understanding and quantitative analysis. These challenges motivate the
creation of DVL-Instruct, a specialized instruction-tuning dataset designed to
enhance models' capabilities in multi-temporal Earth observation. Building upon
this dataset, we develop DVLChat, a baseline model capable of both image-level
question-answering and pixel-level segmentation, facilitating a comprehensive
understanding of city dynamics through language interactions.

</details>


### [352] [Uni3D-MoE: Scalable Multimodal 3D Scene Understanding via Mixture of Experts](https://arxiv.org/pdf/2505.21079)
*Yue Zhang, Yingzhao Jian, Hehe Fan, Yi Yang, Roger Zimmermann*

Main category: cs.CV

TL;DR: Uni3D-MoE is a sparse Mixture-of-Experts-based 3D multimodal large language model designed for adaptive 3D scene understanding by dynamically selecting modality-specific experts.


<details>
  <summary>Details</summary>
Motivation: Existing 3D MLLMs use limited modalities, leading to incomplete scene representations and reduced accuracy. Uniform processing fails to capture query-specific context.

Method: Uni3D-MoE integrates multiple 3D modalities (RGB, depth, BEV maps, point clouds, voxels) and uses a learnable routing mechanism in a sparse MoE-based LLM to dynamically select modality-specific experts.

Result: Extensive evaluations show Uni3D-MoE's effectiveness on standard 3D scene understanding benchmarks and specialized datasets.

Conclusion: Uni3D-MoE addresses limitations of existing approaches by enabling adaptive, query-specific 3D multimodal fusion.

Abstract: Recent advancements in multimodal large language models (MLLMs) have
demonstrated considerable potential for comprehensive 3D scene understanding.
However, existing approaches typically utilize only one or a limited subset of
3D modalities, resulting in incomplete representations of 3D scenes and reduced
interpretive accuracy. Furthermore, different types of queries inherently
depend on distinct modalities, indicating that uniform processing of all
modality tokens may fail to effectively capture query-specific context. To
address these challenges, we propose Uni3D-MoE, a sparse Mixture-of-Experts
(MoE)-based 3D MLLM designed to enable adaptive 3D multimodal fusion.
Specifically, Uni3D-MoE integrates a comprehensive set of 3D modalities,
including multi-view RGB and depth images, bird's-eye-view (BEV) maps, point
clouds, and voxel representations. At its core, our framework employs a
learnable routing mechanism within the sparse MoE-based large language model,
dynamically selecting appropriate experts at the token level. Each expert
specializes in processing multimodal tokens based on learned modality
preferences, thus facilitating flexible collaboration tailored to diverse
task-specific requirements. Extensive evaluations on standard 3D scene
understanding benchmarks and specialized datasets demonstrate the efficacy of
Uni3D-MoE.

</details>


### [353] [DisasterM3: A Remote Sensing Vision-Language Dataset for Disaster Damage Assessment and Response](https://arxiv.org/pdf/2505.21089)
*Junjue Wang, Weihao Xuan, Heli Qi, Zhihao Liu, Kunyi Liu, Yuhan Wu, Hongruixuan Chen, Jian Song, Junshi Xia, Zhuo Zheng, Naoto Yokoya*

Main category: cs.CV

TL;DR: DisasterM3 dataset addresses challenges in disaster assessment using VLMs, featuring multi-hazard, multi-sensor, and multi-task capabilities, and improves model performance through fine-tuning.


<details>
  <summary>Details</summary>
Motivation: Complex disaster scenes with diverse types, regions, and sensors pose challenges for VLMs, necessitating a specialized dataset for global disaster assessment.

Method: Curated DisasterM3 dataset with 26,988 bi-temporal satellite images and 123k instruction pairs, covering 36 disasters across 5 continents, and evaluated 14 VLMs.

Result: State-of-the-art VLMs struggle with disaster tasks due to corpus gaps and sensor issues; fine-tuning with DisasterM3 improves performance and generalization.

Conclusion: DisasterM3 enhances VLM capabilities for disaster assessment, addressing gaps in corpus, sensor compatibility, and task sensitivity.

Abstract: Large vision-language models (VLMs) have made great achievements in Earth
vision. However, complex disaster scenes with diverse disaster types,
geographic regions, and satellite sensors have posed new challenges for VLM
applications. To fill this gap, we curate a remote sensing vision-language
dataset (DisasterM3) for global-scale disaster assessment and response.
DisasterM3 includes 26,988 bi-temporal satellite images and 123k instruction
pairs across 5 continents, with three characteristics: 1) Multi-hazard:
DisasterM3 involves 36 historical disaster events with significant impacts,
which are categorized into 10 common natural and man-made disasters.
2)Multi-sensor: Extreme weather during disasters often hinders optical sensor
imaging, making it necessary to combine Synthetic Aperture Radar (SAR) imagery
for post-disaster scenes. 3) Multi-task: Based on real-world scenarios,
DisasterM3 includes 9 disaster-related visual perception and reasoning tasks,
harnessing the full potential of VLM's reasoning ability with progressing from
disaster-bearing body recognition to structural damage assessment and object
relational reasoning, culminating in the generation of long-form disaster
reports. We extensively evaluated 14 generic and remote sensing VLMs on our
benchmark, revealing that state-of-the-art models struggle with the disaster
tasks, largely due to the lack of a disaster-specific corpus, cross-sensor gap,
and damage object counting insensitivity. Focusing on these issues, we
fine-tune four VLMs using our dataset and achieve stable improvements across
all tasks, with robust cross-sensor and cross-disaster generalization
capabilities.

</details>


### [354] [Instance Data Condensation for Image Super-Resolution](https://arxiv.org/pdf/2505.21099)
*Tianhao Peng, Ho Man Kwan, Yuxuan Jiang, Ge Gao, Fan Zhang, Xiaozhong Xu, Shan Liu, David Bull*

Main category: cs.CV

TL;DR: A novel Instance Data Condensation (IDC) framework for image Super-Resolution (ISR) achieves high-quality synthetic datasets with 10% condensation, matching or outperforming full datasets.


<details>
  <summary>Details</summary>
Motivation: Deep learning for ISR requires large datasets, demanding high computational and storage resources. Dataset condensation is underexplored for ISR.

Method: Proposes IDC with Random Local Fourier Feature Extraction and Multi-level Feature Distribution Matching to optimize global and local feature distributions.

Result: Condensed DIV2K dataset (10% volume) matches or outperforms the original dataset in training ISR models, with excellent stability.

Conclusion: IDC successfully condenses ISR datasets, offering efficiency and performance, with potential for broader applications.

Abstract: Deep learning based image Super-Resolution (ISR) relies on large training
datasets to optimize model generalization; this requires substantial
computational and storage resources during training. While dataset condensation
has shown potential in improving data efficiency and privacy for high-level
computer vision tasks, it has not yet been fully exploited for ISR. In this
paper, we propose a novel Instance Data Condensation (IDC) framework
specifically for ISR, which achieves instance-level data condensation through
Random Local Fourier Feature Extraction and Multi-level Feature Distribution
Matching. This aims to optimize feature distributions at both global and local
levels and obtain high-quality synthesized training content with fine detail.
This framework has been utilized to condense the most commonly used training
dataset for ISR, DIV2K, with a 10% condensation rate. The resulting synthetic
dataset offers comparable or (in certain cases) even better performance
compared to the original full dataset and excellent training stability when
used to train various popular ISR models. To the best of our knowledge, this is
the first time that a condensed/synthetic dataset (with a 10% data volume) has
demonstrated such performance. The source code and the synthetic dataset have
been made available at https://github.com/.

</details>


### [355] [Differentiable Solver Search for Fast Diffusion Sampling](https://arxiv.org/pdf/2505.21114)
*Shuai Wang, Zexian Li, Qipeng zhang, Tianhui Song, Xubin Li, Tiezheng Ge, Bo Zheng, Limin Wang*

Main category: cs.CV

TL;DR: The paper introduces a novel differentiable solver search algorithm to optimize ODE-based solvers for diffusion models, achieving superior performance with fewer steps.


<details>
  <summary>Details</summary>
Motivation: Current ODE-based solvers for diffusion models rely on suboptimal t-related Lagrange interpolation, limiting efficiency.

Method: Proposes a compact search space for solver coefficients and time steps, and develops a differentiable solver search algorithm.

Result: Achieves FID scores of 2.40 and 2.35 with 10 steps on ImageNet256 for SiT-XL/2 and FlowDCN-XL/2, outperforming traditional solvers.

Conclusion: The searched solver is generalizable across architectures and resolutions, offering significant improvements in efficiency and performance.

Abstract: Diffusion models have demonstrated remarkable generation quality but at the
cost of numerous function evaluations. Recently, advanced ODE-based solvers
have been developed to mitigate the substantial computational demands of
reverse-diffusion solving under limited sampling steps. However, these solvers,
heavily inspired by Adams-like multistep methods, rely solely on t-related
Lagrange interpolation. We show that t-related Lagrange interpolation is
suboptimal for diffusion model and reveal a compact search space comprised of
time steps and solver coefficients. Building on our analysis, we propose a
novel differentiable solver search algorithm to identify more optimal solver.
Equipped with the searched solver, rectified-flow models, e.g., SiT-XL/2 and
FlowDCN-XL/2, achieve FID scores of 2.40 and 2.35, respectively, on ImageNet256
with only 10 steps. Meanwhile, DDPM model, DiT-XL/2, reaches a FID score of
2.33 with only 10 steps. Notably, our searched solver outperforms traditional
solvers by a significant margin. Moreover, our searched solver demonstrates
generality across various model architectures, resolutions, and model sizes.

</details>


### [356] [ReassembleNet: Learnable Keypoints and Diffusion for 2D Fresco Reconstruction](https://arxiv.org/pdf/2505.21117)
*Adeela Islam, Stefano Fiorini, Stuart James, Pietro Morerio, Alessio Del Bue*

Main category: cs.CV

TL;DR: ReassembleNet improves reassembly tasks by reducing complexity and integrating multimodal data, achieving significant accuracy gains over prior methods.


<details>
  <summary>Details</summary>
Motivation: Addressing scalability, multimodality, and real-world applicability limitations in current Deep Learning methods for reassembly tasks.

Method: Uses contour keypoints and Graph Neural Networks to select informative features, integrates multimodal data, and employs diffusion-based pose estimation.

Result: Improves prior methods by 55% (RMSE Rotation) and 86% (RMSE Translation).

Conclusion: ReassembleNet offers a scalable, multimodal solution for complex reassembly tasks with superior performance.

Abstract: The task of reassembly is a significant challenge across multiple domains,
including archaeology, genomics, and molecular docking, requiring the precise
placement and orientation of elements to reconstruct an original structure. In
this work, we address key limitations in state-of-the-art Deep Learning methods
for reassembly, namely i) scalability; ii) multimodality; and iii) real-world
applicability: beyond square or simple geometric shapes, realistic and complex
erosion, or other real-world problems. We propose ReassembleNet, a method that
reduces complexity by representing each input piece as a set of contour
keypoints and learning to select the most informative ones by Graph Neural
Networks pooling inspired techniques. ReassembleNet effectively lowers
computational complexity while enabling the integration of features from
multiple modalities, including both geometric and texture data. Further
enhanced through pretraining on a semi-synthetic dataset. We then apply
diffusion-based pose estimation to recover the original structure. We improve
on prior methods by 55% and 86% for RMSE Rotation and Translation,
respectively.

</details>


### [357] [FastFace: Tuning Identity Preservation in Distilled Diffusion via Guidance and Attention](https://arxiv.org/pdf/2505.21144)
*Sergey Karpukhin, Vadim Titov, Andrey Kuznetsov, Aibek Alanov*

Main category: cs.CV

TL;DR: FastFace framework enables training-free adaptation of ID-adapters to distilled diffusion models, improving speed and identity fidelity.


<details>
  <summary>Details</summary>
Motivation: Address the slow inference of jointly trained ID-adapters with diffusion models by enabling training-free adaptation.

Method: Redesign classifier-free guidance for few-step generation and manipulate attention in decoupled blocks.

Result: Proposed FastFace framework enhances identity similarity and fidelity in accelerated diffusion models.

Conclusion: FastFace offers a universal solution for efficient, high-quality personalized generation with distilled diffusion models.

Abstract: In latest years plethora of identity-preserving adapters for a personalized
generation with diffusion models have been released. Their main disadvantage is
that they are dominantly trained jointly with base diffusion models, which
suffer from slow multi-step inference. This work aims to tackle the challenge
of training-free adaptation of pretrained ID-adapters to diffusion models
accelerated via distillation - through careful re-design of classifier-free
guidance for few-step stylistic generation and attention manipulation
mechanisms in decoupled blocks to improve identity similarity and fidelity, we
propose universal FastFace framework. Additionally, we develop a disentangled
public evaluation protocol for id-preserving adapters.

</details>


### [358] [RoBiS: Robust Binary Segmentation for High-Resolution Industrial Images](https://arxiv.org/pdf/2505.21152)
*Xurui Li, Zhonesheng Jiang, Tingxuan Ai, Yu Zhou*

Main category: cs.CV

TL;DR: RoBiS is a robust framework for unsupervised anomaly detection, improving performance on MVTec AD 2 by 29.2% SegF1 via Swin-Cropping, data augmentation, and adaptive binarization.


<details>
  <summary>Details</summary>
Motivation: Current methods fail on complex real-world challenges in MVTec AD 2, necessitating a more robust solution.

Method: Uses Swin-Cropping for high-resolution pre-processing, data augmentation for robustness, and adaptive binarization (combining mean+3std and MEBin) with SAM refinement.

Result: Achieves 29.2% SegF1 improvement on Test_private and 29.82% on Test_private_mixed.

Conclusion: RoBiS significantly outperforms existing methods, offering a robust solution for real-world anomaly detection.

Abstract: Robust unsupervised anomaly detection (AD) in real-world scenarios is an
important task. Current methods exhibit severe performance degradation on the
MVTec AD 2 benchmark due to its complex real-world challenges. To solve this
problem, we propose a robust framework RoBiS, which consists of three core
modules: (1) Swin-Cropping, a high-resolution image pre-processing strategy to
preserve the information of small anomalies through overlapping window
cropping. (2) The data augmentation of noise addition and lighting simulation
is carried out on the training data to improve the robustness of AD model. We
use INP-Former as our baseline, which could generate better results on the
various sub-images. (3) The traditional statistical-based binarization strategy
(mean+3std) is combined with our previous work, MEBin (published in CVPR2025),
for joint adaptive binarization. Then, SAM is further employed to refine the
segmentation results. Compared with some methods reported by the MVTec AD 2,
our RoBiS achieves a 29.2% SegF1 improvement (from 21.8% to 51.00%) on
Test_private and 29.82% SegF1 gains (from 16.7% to 46.52%) on
Test_private_mixed. Code is available at https://github.com/xrli-U/RoBiS.

</details>


### [359] [Normalized Attention Guidance: Universal Negative Guidance for Diffusion Model](https://arxiv.org/pdf/2505.21179)
*Dar-Yen Chen, Hmrishav Bandyopadhyay, Kai Zou, Yi-Zhe Song*

Main category: cs.CV

TL;DR: NAG is a training-free method for effective negative guidance in diffusion models, outperforming CFG in few-step sampling and generalizing across architectures and modalities.


<details>
  <summary>Details</summary>
Motivation: Negative guidance in diffusion models fails under aggressive sampling step compression, motivating the need for a robust solution like NAG.

Method: NAG uses L1-based normalization and refinement in attention space to restore effective negative guidance.

Result: NAG improves text alignment, fidelity, and human-perceived quality across various architectures and modalities.

Conclusion: NAG is a universal plug-in for diffusion models, offering efficient negative guidance without retraining.

Abstract: Negative guidance -- explicitly suppressing unwanted attributes -- remains a
fundamental challenge in diffusion models, particularly in few-step sampling
regimes. While Classifier-Free Guidance (CFG) works well in standard settings,
it fails under aggressive sampling step compression due to divergent
predictions between positive and negative branches. We present Normalized
Attention Guidance (NAG), an efficient, training-free mechanism that applies
extrapolation in attention space with L1-based normalization and refinement.
NAG restores effective negative guidance where CFG collapses while maintaining
fidelity. Unlike existing approaches, NAG generalizes across architectures
(UNet, DiT), sampling regimes (few-step, multi-step), and modalities (image,
video), functioning as a \textit{universal} plug-in with minimal computational
overhead. Through extensive experimentation, we demonstrate consistent
improvements in text alignment (CLIP Score), fidelity (FID, PFID), and
human-perceived quality (ImageReward). Our ablation studies validate each
design component, while user studies confirm significant preference for
NAG-guided outputs. As a model-agnostic inference-time approach requiring no
retraining, NAG provides effortless negative guidance for all modern diffusion
frameworks -- pseudocode in the Appendix!

</details>


### [360] [Making Every Event Count: Balancing Data Efficiency and Accuracy in Event Camera Subsampling](https://arxiv.org/pdf/2505.21187)
*Hesam Araghi, Jan van Gemert, Nergis Tomen*

Main category: cs.CV

TL;DR: The paper evaluates hardware-friendly subsampling methods for event cameras, introducing a density-based approach to improve classification accuracy in sparse regimes.


<details>
  <summary>Details</summary>
Motivation: Event cameras' high event rates challenge data transmission and processing, but subsampling's impact on visual tasks is underexplored.

Method: Six subsampling methods are systematically evaluated using CNNs for event video classification, with a focus on density-based subsampling.

Result: The density-based method improves accuracy in sparse regimes, but performance varies with hyperparameters and event count variance.

Conclusion: The study offers insights for balancing data efficiency and task accuracy in hardware-efficient subsampling strategies.

Abstract: Event cameras offer high temporal resolution and power efficiency, making
them well-suited for edge AI applications. However, their high event rates
present challenges for data transmission and processing. Subsampling methods
provide a practical solution, but their effect on downstream visual tasks
remains underexplored. In this work, we systematically evaluate six
hardware-friendly subsampling methods using convolutional neural networks for
event video classification on various benchmark datasets. We hypothesize that
events from high-density regions carry more task-relevant information and are
therefore better suited for subsampling. To test this, we introduce a simple
causal density-based subsampling method, demonstrating improved classification
accuracy in sparse regimes. Our analysis further highlights key factors
affecting subsampling performance, including sensitivity to hyperparameters and
failure cases in scenarios with large event count variance. These findings
provide insights for utilization of hardware-efficient subsampling strategies
that balance data efficiency and task accuracy. The code for this paper will be
released at: https://github.com/hesamaraghi/event-camera-subsampling-methods.

</details>


### [361] [Think Twice, Act Once: Token-Aware Compression and Action Reuse for Efficient Inference in Vision-Language-Action Models](https://arxiv.org/pdf/2505.21200)
*Xudong Tan, Yaoxin Yang, Peng Ye, Jialin Zheng, Bizhe Bai, Xinyi Wang, Jia Hao, Tao Chen*

Main category: cs.CV

TL;DR: FlashVLA is a training-free framework that accelerates Vision-Language-Action (VLA) models by reducing redundancy in action steps and visual tokens, achieving significant efficiency gains with minimal performance loss.


<details>
  <summary>Details</summary>
Motivation: High inference costs in VLA models due to token computation and autoregressive decoding hinder real-time deployment, prompting the need for efficiency improvements.

Method: FlashVLA introduces action reuse for stable steps and prunes low-contribution visual tokens, avoiding redundant computations.

Result: FlashVLA reduces FLOPs by 55.7% and latency by 36.0%, with only a 0.7% drop in task success rate.

Conclusion: FlashVLA enables lightweight, low-latency VLA inference without retraining, demonstrating its practical effectiveness.

Abstract: Vision-Language-Action (VLA) models have emerged as a powerful paradigm for
general-purpose robot control through natural language instructions. However,
their high inference cost-stemming from large-scale token computation and
autoregressive decoding-poses significant challenges for real-time deployment
and edge applications. While prior work has primarily focused on architectural
optimization, we take a different perspective by identifying a dual form of
redundancy in VLA models: (i) high similarity across consecutive action steps,
and (ii) substantial redundancy in visual tokens. Motivated by these
observations, we propose FlashVLA, the first training-free and plug-and-play
acceleration framework that enables action reuse in VLA models. FlashVLA
improves inference efficiency through a token-aware action reuse mechanism that
avoids redundant decoding across stable action steps, and an information-guided
visual token selection strategy that prunes low-contribution tokens. Extensive
experiments on the LIBERO benchmark show that FlashVLA reduces FLOPs by 55.7%
and latency by 36.0%, with only a 0.7% drop in task success rate. These results
demonstrate the effectiveness of FlashVLA in enabling lightweight, low-latency
VLA inference without retraining.

</details>


### [362] [Is Hyperbolic Space All You Need for Medical Anomaly Detection?](https://arxiv.org/pdf/2505.21228)
*Alvaro Gonzalez-Jimenez, Simone Lionetti, Ludovic Amruthalingam, Philippe Gottfrois, Fabian Gröger, Marc Pouly, Alexander A. Navarini*

Main category: cs.CV

TL;DR: A novel approach using hyperbolic space for medical anomaly detection outperforms traditional Euclidean methods, showing higher accuracy and resilience in few-shot scenarios.


<details>
  <summary>Details</summary>
Motivation: Traditional Euclidean methods fail to capture hierarchical feature relationships, leading to suboptimal anomaly detection performance.

Method: Projects feature representations into hyperbolic space, aggregates them based on confidence levels, and classifies samples as healthy or anomalous.

Result: Hyperbolic space consistently outperforms Euclidean-based frameworks, achieving higher AUROC scores and resilience to parameter variations.

Conclusion: Hyperbolic space is a powerful alternative for medical anomaly detection, especially in few-shot scenarios.

Abstract: Medical anomaly detection has emerged as a promising solution to challenges
in data availability and labeling constraints. Traditional methods extract
features from different layers of pre-trained networks in Euclidean space;
however, Euclidean representations fail to effectively capture the hierarchical
relationships within these features, leading to suboptimal anomaly detection
performance. We propose a novel yet simple approach that projects feature
representations into hyperbolic space, aggregates them based on confidence
levels, and classifies samples as healthy or anomalous. Our experiments
demonstrate that hyperbolic space consistently outperforms Euclidean-based
frameworks, achieving higher AUROC scores at both image and pixel levels across
multiple medical benchmark datasets. Additionally, we show that hyperbolic
space exhibits resilience to parameter variations and excels in few-shot
scenarios, where healthy images are scarce. These findings underscore the
potential of hyperbolic space as a powerful alternative for medical anomaly
detection. The project website can be found at
https://hyperbolic-anomalies.github.io

</details>


### [363] [Sci-Fi: Symmetric Constraint for Frame Inbetweening](https://arxiv.org/pdf/2505.21205)
*Liuhan Chen, Xiaodong Cun, Xiaoyu Li, Xianyi He, Shenghai Yuan, Jie Chen, Ying Shan, Li Yuan*

Main category: cs.CV

TL;DR: The paper proposes Sci-Fi, a framework to address asymmetric control in frame inbetweening by introducing a stronger end-frame constraint mechanism via EF-Net, ensuring harmonious transitions.


<details>
  <summary>Details</summary>
Motivation: Current methods for frame inbetweening suffer from inconsistent motion or appearance collapse due to weak end-frame constraints compared to start-frame constraints.

Method: Sci-Fi introduces EF-Net, a lightweight module to encode and inject end-frame features into the I2V-DM, balancing control strength between start and end frames.

Result: Sci-Fi outperforms baselines, producing more harmonious transitions in various scenarios.

Conclusion: The proposed Sci-Fi framework effectively achieves symmetric constraints, improving frame inbetweening quality.

Abstract: Frame inbetweening aims to synthesize intermediate video sequences
conditioned on the given start and end frames. Current state-of-the-art methods
mainly extend large-scale pre-trained Image-to-Video Diffusion models (I2V-DMs)
by incorporating end-frame constraints via directly fine-tuning or omitting
training. We identify a critical limitation in their design: Their injections
of the end-frame constraint usually utilize the same mechanism that originally
imposed the start-frame (single image) constraint. However, since the original
I2V-DMs are adequately trained for the start-frame condition in advance,
naively introducing the end-frame constraint by the same mechanism with much
less (even zero) specialized training probably can't make the end frame have a
strong enough impact on the intermediate content like the start frame. This
asymmetric control strength of the two frames over the intermediate content
likely leads to inconsistent motion or appearance collapse in generated frames.
To efficiently achieve symmetric constraints of start and end frames, we
propose a novel framework, termed Sci-Fi, which applies a stronger injection
for the constraint of a smaller training scale. Specifically, it deals with the
start-frame constraint as before, while introducing the end-frame constraint by
an improved mechanism. The new mechanism is based on a well-designed
lightweight module, named EF-Net, which encodes only the end frame and expands
it into temporally adaptive frame-wise features injected into the I2V-DM. This
makes the end-frame constraint as strong as the start-frame constraint,
enabling our Sci-Fi to produce more harmonious transitions in various
scenarios. Extensive experiments prove the superiority of our Sci-Fi compared
with other baselines.

</details>


### [364] [Occlusion Boundary and Depth: Mutual Enhancement via Multi-Task Learning](https://arxiv.org/pdf/2505.21231)
*Lintao Xu, Yinghao Wang, Chaohui Wang*

Main category: cs.CV

TL;DR: MoDOT jointly estimates depth and occlusion boundaries (OBs) using CASM and OBDCL, achieving SOTA results on synthetic and real datasets.


<details>
  <summary>Details</summary>
Motivation: Improving scene understanding and 3D reconstruction by integrating occlusion boundary estimation (OBE) with monocular depth estimation (MDE) to resolve depth ambiguities and refine occlusion reasoning.

Method: Proposes MoDOT, a network with CASM (cross-attention multi-scale strip convolution) for enhanced depth prediction and OBDCL (occlusion-aware loss) for sharper depth boundaries.

Result: Achieves SOTA on synthetic datasets and NYUD-v2, with competitive real-world depth transfer results without domain adaptation.

Conclusion: Joint estimation of depth and OBs is mutually beneficial, and MoDOT's design effectively improves geometric fidelity and accuracy.

Abstract: Occlusion Boundary Estimation (OBE) identifies boundaries arising from both
inter-object occlusions and self-occlusion within individual objects,
distinguishing intrinsic object edges from occlusion-induced contours to
improve scene understanding and 3D reconstruction capacity. This is closely
related to Monocular Depth Estimation (MDE), which infers depth from a single
image, as occlusion boundaries provide critical geometric cues for resolving
depth ambiguities, while depth priors can conversely refine occlusion reasoning
in complex scenes. In this paper, we propose a novel network, MoDOT, that first
jointly estimates depth and OBs. We propose CASM, a cross-attention multi-scale
strip convolution module, leverages mid-level OB features to significantly
enhance depth prediction. Additionally, we introduce an occlusion-aware loss
function, OBDCL, which encourages sharper and more accurate depth boundaries.
Extensive experiments on both real and synthetic datasets demonstrate the
mutual benefits of jointly estimating depth and OB, and highlight the
effectiveness of our model design. Our method achieves the state-of-the-art
(SOTA) on both our proposed synthetic datasets and one popular real dataset,
NYUD-v2, significantly outperforming multi-task baselines. Besides, without
domain adaptation, results on real-world depth transfer are comparable to the
competitors, while preserving sharp occlusion boundaries for geometric
fidelity. We will release our code, pre-trained models, and datasets to support
future research in this direction.

</details>


### [365] [CROP: Contextual Region-Oriented Visual Token Pruning](https://arxiv.org/pdf/2505.21233)
*Jiawei Guo, Feifei Zhai, Pu Jian, Qianrun Wei, Yu Zhou*

Main category: cs.CV

TL;DR: CROP is a framework for compressing visual tokens in VLM-based VQA by localizing and pruning irrelevant regions, improving efficiency and performance.


<details>
  <summary>Details</summary>
Motivation: Existing VLM-based VQA methods process entire images, creating redundant visual tokens that increase computational costs.

Method: CROP uses a two-step process: Localization to identify relevant regions and Pruning via Pre-LLM Compression (PLC) and Inner-LLM Pruning (ILP).

Result: CROP outperforms existing pruning methods and achieves state-of-the-art performance on VQA tasks.

Conclusion: CROP effectively reduces computational overhead while maintaining high performance in VQA tasks.

Abstract: Current VLM-based VQA methods often process entire images, leading to
excessive visual tokens that include redundant information irrelevant to the
posed question. This abundance of unnecessary image details creates numerous
visual tokens, drastically increasing memory and computational requirements in
VLMs. To address this, we propose Contextual Region-Oriented Visual Token
Pruning (CROP), a novel framework to compress visual tokens through a two-step
process: Localization and Pruning. Specifically, CROP first employs an
efficient model to identify the contextual region relevant to the input query.
Subsequently, two distinct strategies are introduced for pruning: (1) Pre-LLM
Compression (PLC), which adaptively compresses different image regions with
varying ratios, and (2) Inner-LLM Pruning (ILP), a training-free method that
prunes tokens within early LLM layers guided by the identified contextual
region. Extensive experiments on a wide range of VQA tasks demonstrate that
CROP significantly outperforms existing visual token pruning methods and
achieves state-of-the-art performance. Our code and datasets will be made
available.

</details>


### [366] [3D-UIR: 3D Gaussian for Underwater 3D Scene Reconstruction via Physics-Based Appearance-Medium Decouplin](https://arxiv.org/pdf/2505.21238)
*Jieyu Yuan, Yujun Li, Yuanlin Zhang, Chunle Guo, Xiongxin Tang, Ruixing Wang, Chongyi Li*

Main category: cs.CV

TL;DR: A physics-based framework for underwater novel view synthesis addresses inhomogeneous medium effects using tailored Gaussian modeling and distance-guided optimization, outperforming existing methods.


<details>
  <summary>Details</summary>
Motivation: Underwater scenes suffer from complex light-media interactions, disrupting conventional rendering assumptions and causing artifacts in methods like 3D Gaussian Splatting.

Method: Proposes appearance embeddings for medium effects and a distance-guided optimization strategy with pseudo-depth supervision to enhance geometric fidelity.

Result: Achieves high-quality novel view synthesis and physically accurate scene restoration, outperforming existing methods in rendering quality and accuracy.

Conclusion: The framework successfully disentangles object appearance from medium effects, improving underwater scene reconstruction.

Abstract: Novel view synthesis for underwater scene reconstruction presents unique
challenges due to complex light-media interactions. Optical scattering and
absorption in water body bring inhomogeneous medium attenuation interference
that disrupts conventional volume rendering assumptions of uniform propagation
medium. While 3D Gaussian Splatting (3DGS) offers real-time rendering
capabilities, it struggles with underwater inhomogeneous environments where
scattering media introduce artifacts and inconsistent appearance. In this
study, we propose a physics-based framework that disentangles object appearance
from water medium effects through tailored Gaussian modeling. Our approach
introduces appearance embeddings, which are explicit medium representations for
backscatter and attenuation, enhancing scene consistency. In addition, we
propose a distance-guided optimization strategy that leverages pseudo-depth
maps as supervision with depth regularization and scale penalty terms to
improve geometric fidelity. By integrating the proposed appearance and medium
modeling components via an underwater imaging model, our approach achieves both
high-quality novel view synthesis and physically accurate scene restoration.
Experiments demonstrate our significant improvements in rendering quality and
restoration accuracy over existing methods. The project page is available at
\href{https://bilityniu.github.io/3D-UIR}{https://bilityniu.github.io/3D-UIR

</details>


### [367] [Plenodium: UnderWater 3D Scene Reconstruction with Plenoptic Medium Representation](https://arxiv.org/pdf/2505.21258)
*Changguanng Wu, Jiangxin Dong, Chengjian Li, Jinhui Tang*

Main category: cs.CV

TL;DR: Plenodium is a 3D representation framework for objects and participating media, using spherical harmonics for accurate underwater scene reconstruction. It improves depth priors and geometry optimization, showing significant improvements in real-world and simulated datasets.


<details>
  <summary>Details</summary>
Motivation: Existing medium representations lack joint modeling of objects and participating media, and struggle with degraded underwater environments. Plenodium aims to address these limitations.

Method: Uses spherical harmonics encoding for directional and positional information, pseudo-depth Gaussian complementation for depth priors, and depth ranking regularized loss for geometry optimization.

Result: Achieves significant improvements in 3D reconstruction on real-world underwater datasets and demonstrates restoration capability in simulated scenarios.

Conclusion: Plenodium effectively models underwater scenes, outperforming existing methods, with code and dataset publicly available.

Abstract: We present Plenodium (plenoptic medium), an effective and efficient 3D
representation framework capable of jointly modeling both objects and
participating media. In contrast to existing medium representations that rely
solely on view-dependent modeling, our novel plenoptic medium representation
incorporates both directional and positional information through spherical
harmonics encoding, enabling highly accurate underwater scene reconstruction.
To address the initialization challenge in degraded underwater environments, we
propose the pseudo-depth Gaussian complementation to augment COLMAP-derived
point clouds with robust depth priors. In addition, a depth ranking regularized
loss is developed to optimize the geometry of the scene and improve the ordinal
consistency of the depth maps. Extensive experiments on real-world underwater
datasets demonstrate that our method achieves significant improvements in 3D
reconstruction. Furthermore, we conduct a simulated dataset with ground truth
and the controllable scattering medium to demonstrate the restoration
capability of our method in underwater scenarios. Our code and dataset are
available at https://plenodium.github.io/.

</details>


### [368] [Spectral Compression Transformer with Line Pose Graph for Monocular 3D Human Pose Estimation](https://arxiv.org/pdf/2505.21309)
*Zenghao Zheng, Lianping Yang, Hegui Zhu, Mingrui Ye*

Main category: cs.CV

TL;DR: The paper introduces the Spectral Compression Transformer (SCT) to reduce computational costs in 3D human pose estimation by compressing sequence length and eliminating redundancy, achieving state-of-the-art performance.


<details>
  <summary>Details</summary>
Motivation: Transformer-based methods for 3D human pose estimation are computationally expensive due to quadratic complexity in self-attention and redundant frame sequences. Existing methods fail to balance model capacity and redundancy reduction.

Method: The SCT encoder uses Discrete Cosine Transform to compress sequences by filtering high-frequency noise. The Line Pose Graph (LPG) adds skeletal position information, and a dual-stream network models spatial and motion relationships.

Result: The method achieves an MPJPE of 37.7mm on Human3.6M with low computational cost, outperforming benchmarks. Ablation studies confirm module effectiveness.

Conclusion: SCT and LPG improve efficiency and performance in 3D pose estimation, with potential for broader applications. Code and models will be released.

Abstract: Transformer-based 3D human pose estimation methods suffer from high
computational costs due to the quadratic complexity of self-attention with
respect to sequence length. Additionally, pose sequences often contain
significant redundancy between frames. However, recent methods typically fail
to improve model capacity while effectively eliminating sequence redundancy. In
this work, we introduce the Spectral Compression Transformer (SCT) to reduce
sequence length and accelerate computation. The SCT encoder treats hidden
features between blocks as Temporal Feature Signals (TFS) and applies the
Discrete Cosine Transform, a Fourier transform-based technique, to determine
the spectral components to be retained. By filtering out certain high-frequency
noise components, SCT compresses the sequence length and reduces redundancy. To
further enrich the input sequence with prior structural information, we propose
the Line Pose Graph (LPG) based on line graph theory. The LPG generates
skeletal position information that complements the input 2D joint positions,
thereby improving the model's performance. Finally, we design a dual-stream
network architecture to effectively model spatial joint relationships and the
compressed motion trajectory within the pose sequence. Extensive experiments on
two benchmark datasets (i.e., Human3.6M and MPI-INF-3DHP) demonstrate that our
model achieves state-of-the-art performance with improved computational
efficiency. For example, on the Human3.6M dataset, our method achieves an MPJPE
of 37.7mm while maintaining a low computational cost. Furthermore, we perform
ablation studies on each module to assess its effectiveness. The code and
models will be released.

</details>


### [369] [MagicTryOn: Harnessing Diffusion Transformer for Garment-Preserving Video Virtual Try-on](https://arxiv.org/pdf/2505.21325)
*Guangyuan Li, Siming Zheng, Hao Zhang, Jinwei Chen, Junsheng Luan, Binkai Ou, Lei Zhao, Bo Li, Peng-Tao Jiang*

Main category: cs.CV

TL;DR: MagicTryOn improves video virtual try-on by using a diffusion Transformer and joint spatiotemporal modeling, outperforming current methods.


<details>
  <summary>Details</summary>
Motivation: Current VVT methods struggle with spatiotemporal consistency, garment detail preservation, and realism during motion.

Method: Proposes MagicTryOn, a framework using a diffusion Transformer and full self-attention for joint spatiotemporal modeling, with a coarse-to-fine garment preservation strategy and mask-aware loss.

Result: Outperforms SOTA methods in evaluations and generalizes to in-the-wild scenarios.

Conclusion: MagicTryOn effectively addresses VVT challenges, enhancing realism and consistency in garment simulation.

Abstract: Video Virtual Try-On (VVT) aims to simulate the natural appearance of
garments across consecutive video frames, capturing their dynamic variations
and interactions with human body motion. However, current VVT methods still
face challenges in terms of spatiotemporal consistency and garment content
preservation. First, they use diffusion models based on the U-Net, which are
limited in their expressive capability and struggle to reconstruct complex
details. Second, they adopt a separative modeling approach for spatial and
temporal attention, which hinders the effective capture of structural
relationships and dynamic consistency across frames. Third, their expression of
garment details remains insufficient, affecting the realism and stability of
the overall synthesized results, especially during human motion. To address the
above challenges, we propose MagicTryOn, a video virtual try-on framework built
upon the large-scale video diffusion Transformer.We replace the U-Net
architecture with a diffusion Transformer and combine full self-attention to
jointly model the spatiotemporal consistency of videos. We design a
coarse-to-fine garment preservation strategy. The coarse strategy integrates
garment tokens during the embedding stage, while the fine strategy incorporates
multiple garment-based conditions, such as semantics, textures, and contour
lines during the denoising stage. Moreover, we introduce a mask-aware loss to
further optimize garment region fidelity. Extensive experiments on both image
and video try-on datasets demonstrate that our method outperforms existing SOTA
methods in comprehensive evaluations and generalizes to in-the-wild scenarios.

</details>


### [370] [MME-VideoOCR: Evaluating OCR-Based Capabilities of Multimodal LLMs in Video Scenarios](https://arxiv.org/pdf/2505.21333)
*Yang Shi, Huanqian Wang, Wulin Xie, Huanyao Zhang, Lijie Zhao, Yi-Fan Zhang, Xinfeng Li, Chaoyou Fu, Zhuoer Wen, Wenting Liu, Zhuoran Zhang, Xinlong Chen, Bohan Zeng, Sihan Yang, Yuanxing Zhang, Pengfei Wan, Haotian Wang, Wenjing Yang*

Main category: cs.CV

TL;DR: MME-VideoOCR benchmark evaluates MLLMs for video OCR, revealing limitations in holistic video comprehension despite strong performance in single-frame tasks.


<details>
  <summary>Details</summary>
Motivation: Current MLLMs excel in static image OCR but underperform in video OCR due to challenges like motion blur and temporal variations.

Method: Introduced MME-VideoOCR benchmark with 1,464 videos, 2,000 QA pairs, and 25 tasks across 44 scenarios to test 18 MLLMs.

Result: Best model (Gemini-2.5 Pro) achieved 73.7% accuracy, struggling with spatio-temporal reasoning and cross-frame integration.

Conclusion: High-resolution input and temporal coverage are crucial for improving video OCR in MLLMs.

Abstract: Multimodal Large Language Models (MLLMs) have achieved considerable accuracy
in Optical Character Recognition (OCR) from static images. However, their
efficacy in video OCR is significantly diminished due to factors such as motion
blur, temporal variations, and visual effects inherent in video content. To
provide clearer guidance for training practical MLLMs, we introduce the
MME-VideoOCR benchmark, which encompasses a comprehensive range of video OCR
application scenarios. MME-VideoOCR features 10 task categories comprising 25
individual tasks and spans 44 diverse scenarios. These tasks extend beyond text
recognition to incorporate deeper comprehension and reasoning of textual
content within videos. The benchmark consists of 1,464 videos with varying
resolutions, aspect ratios, and durations, along with 2,000 meticulously
curated, manually annotated question-answer pairs. We evaluate 18
state-of-the-art MLLMs on MME-VideoOCR, revealing that even the best-performing
model (Gemini-2.5 Pro) achieves an accuracy of only 73.7%. Fine-grained
analysis indicates that while existing MLLMs demonstrate strong performance on
tasks where relevant texts are contained within a single or few frames, they
exhibit limited capability in effectively handling tasks that demand holistic
video comprehension. These limitations are especially evident in scenarios that
require spatio-temporal reasoning, cross-frame information integration, or
resistance to language prior bias. Our findings also highlight the importance
of high-resolution visual input and sufficient temporal coverage for reliable
OCR in dynamic video scenarios.

</details>


### [371] [HoliTom: Holistic Token Merging for Fast Video Large Language Models](https://arxiv.org/pdf/2505.21334)
*Kele Shao, Keda Tao, Can Qin, Haoxuan You, Yang Sui, Huan Wang*

Main category: cs.CV

TL;DR: HoliTom is a training-free token merging framework for video LLMs, combining outer-LLM and inner-LLM pruning to reduce computational costs while maintaining performance.


<details>
  <summary>Details</summary>
Motivation: Existing token pruning methods for video LLMs are inefficient, either incurring overhead or neglecting global temporal dynamics, leaving the synergy of combined strategies unexplored.

Method: HoliTom uses outer-LLM pruning via global redundancy-aware temporal segmentation and spatial-temporal merging, followed by inner-LLM token similarity-based merging.

Result: Achieves 90% token reduction, 6.9% FLOPs cost, 99.1% performance retention, 2.28x TTFT reduction, and 1.32x decoding throughput acceleration.

Conclusion: HoliTom demonstrates an efficient, integrated pruning approach for video LLMs, balancing performance and computational savings.

Abstract: Video large language models (video LLMs) excel at video comprehension but
face significant computational inefficiency due to redundant video tokens.
Existing token pruning methods offer solutions. However, approaches operating
within the LLM (inner-LLM pruning), such as FastV, incur intrinsic
computational overhead in shallow layers. In contrast, methods performing token
pruning before the LLM (outer-LLM pruning) primarily address spatial redundancy
within individual frames or limited temporal windows, neglecting the crucial
global temporal dynamics and correlations across longer video sequences. This
leads to sub-optimal spatio-temporal reduction and does not leverage video
compressibility fully. Crucially, the synergistic potential and mutual
influence of combining these strategies remain unexplored. To further reduce
redundancy, we introduce HoliTom, a novel training-free holistic token merging
framework. HoliTom employs outer-LLM pruning through global redundancy-aware
temporal segmentation, followed by spatial-temporal merging to reduce visual
tokens by over 90%, significantly alleviating the LLM's computational burden.
Complementing this, we introduce a robust inner-LLM token similarity-based
merging approach, designed for superior performance and compatibility with
outer-LLM pruning. Evaluations demonstrate our method's promising
efficiency-performance trade-off on LLaVA-OneVision-7B, reducing computational
costs to 6.9% of FLOPs while maintaining 99.1% of the original performance.
Furthermore, we achieve a 2.28x reduction in Time-To-First-Token (TTFT) and a
1.32x acceleration in decoding throughput, highlighting the practical benefits
of our integrated pruning approach for efficient video LLMs inference.

</details>


### [372] [Beyond Accuracy: Uncovering the Role of Similarity Perception and its Alignment with Semantics in Supervised Learning](https://arxiv.org/pdf/2505.21338)
*Katarzyna Filus, Mateusz Żarski*

Main category: cs.CV

TL;DR: The paper introduces Deep Similarity Inspector (DSI) to study how deep vision networks develop similarity perception, comparing CNNs and ViTs, revealing three training phases and differences between architectures.


<details>
  <summary>Details</summary>
Motivation: To understand how deep vision networks develop similarity perception and its alignment with semantic similarity, which is underexplored despite its importance in human-like categorization.

Method: Introduces DSI, a framework to inspect similarity perception in deep vision networks (CNNs and ViTs), analyzing their training phases and alignment with semantic similarity.

Result: CNNs and ViTs exhibit three training phases (initial surge, refinement, stabilization) with distinct differences. Mistakes are gradually eliminated and refined.

Conclusion: DSI provides insights into similarity perception in deep vision networks, highlighting differences between CNNs and ViTs and the refinement process during training.

Abstract: Similarity manifests in various forms, including semantic similarity that is
particularly important, serving as an approximation of human object
categorization based on e.g. shared functionalities and evolutionary traits. It
also offers practical advantages in computational modeling via lexical
structures such as WordNet with constant and interpretable similarity. As in
the domain of deep vision, there is still not enough focus on the phenomena
regarding the similarity perception emergence. We introduce Deep Similarity
Inspector (DSI) -- a systematic framework to inspect how deep vision networks
develop their similarity perception and its alignment with semantic similarity.
Our experiments show that both Convolutional Neural Networks' (CNNs) and Vision
Transformers' (ViTs) develop a rich similarity perception during training with
3 phases (initial similarity surge, refinement, stabilization), with clear
differences between CNNs and ViTs. Besides the gradual mistakes elimination,
the mistakes refinement phenomenon can be observed.

</details>


### [373] [AgriFM: A Multi-source Temporal Remote Sensing Foundation Model for Crop Mapping](https://arxiv.org/pdf/2505.21357)
*Wenyuan Li, Shunlin Liang, Keyan Chen, Yongzhe Chen, Han Ma, Jianglei Xu, Yichuan Ma, Shikang Guan, Husheng Fang, Zhenwei Shi*

Main category: cs.CV

TL;DR: AgriFM is a transformer-based remote sensing foundation model for crop mapping, addressing multi-scale spatiotemporal patterns by synchronizing temporal and spatial processing, outperforming existing methods.


<details>
  <summary>Details</summary>
Motivation: Current remote sensing foundation models (RSFMs) are suboptimal for crop mapping, either ignoring multi-scale spatiotemporal dynamics or focusing solely on spatial patterns.

Method: AgriFM uses a modified Video Swin Transformer for hierarchical spatiotemporal feature extraction, pre-trained on global satellite data (MODIS, Landsat-8/9, Sentinel-2) with a dynamic decoder for downstream tasks.

Result: AgriFM outperforms conventional deep learning and state-of-the-art RSFMs in crop mapping tasks.

Conclusion: AgriFM effectively bridges gaps in spatiotemporal processing for crop mapping, offering a versatile and superior solution.

Abstract: Accurate crop mapping fundamentally relies on modeling multi-scale
spatiotemporal patterns, where spatial scales range from individual field
textures to landscape-level context, and temporal scales capture both
short-term phenological transitions and full growing-season dynamics.
Transformer-based remote sensing foundation models (RSFMs) offer promising
potential for crop mapping due to their innate ability for unified
spatiotemporal processing. However, current RSFMs remain suboptimal for crop
mapping: they either employ fixed spatiotemporal windows that ignore the
multi-scale nature of crop systems or completely disregard temporal information
by focusing solely on spatial patterns. To bridge these gaps, we present
AgriFM, a multi-source remote sensing foundation model specifically designed
for agricultural crop mapping. Our approach begins by establishing the
necessity of simultaneous hierarchical spatiotemporal feature extraction,
leading to the development of a modified Video Swin Transformer architecture
where temporal down-sampling is synchronized with spatial scaling operations.
This modified backbone enables efficient unified processing of long time-series
satellite inputs. AgriFM leverages temporally rich data streams from three
satellite sources including MODIS, Landsat-8/9 and Sentinel-2, and is
pre-trained on a global representative dataset comprising over 25 million image
samples supervised by land cover products. The resulting framework incorporates
a versatile decoder architecture that dynamically fuses these learned
spatiotemporal representations, supporting diverse downstream tasks.
Comprehensive evaluations demonstrate AgriFM's superior performance over
conventional deep learning approaches and state-of-the-art general-purpose
RSFMs across all downstream tasks. Codes will be available at
urlhttps://github.com/flyakon/AgriFM.

</details>


### [374] [YOLO-SPCI: Enhancing Remote Sensing Object Detection via Selective-Perspective-Class Integration](https://arxiv.org/pdf/2505.21370)
*Xinyuan Wang, Lian Peng, Xiangcheng Li, Yilin He, KinTak U*

Main category: cs.CV

TL;DR: YOLO-SPCI enhances YOLOv8 with a Selective-Perspective-Class Integration module for better object detection in remote sensing imagery.


<details>
  <summary>Details</summary>
Motivation: Addressing challenges like scale variation, dense objects, and cluttered backgrounds in remote sensing imagery.

Method: Introduces SPCI module with Selective Stream Gate, Perspective Fusion Module, and Class Discrimination Module, integrated into YOLOv8.

Result: Outperforms state-of-the-art detectors on the NWPU VHR-10 dataset.

Conclusion: YOLO-SPCI effectively improves feature representation and detection performance in aerial imagery.

Abstract: Object detection in remote sensing imagery remains a challenging task due to
extreme scale variation, dense object distributions, and cluttered backgrounds.
While recent detectors such as YOLOv8 have shown promising results, their
backbone architectures lack explicit mechanisms to guide multi-scale feature
refinement, limiting performance on high-resolution aerial data. In this work,
we propose YOLO-SPCI, an attention-enhanced detection framework that introduces
a lightweight Selective-Perspective-Class Integration (SPCI) module to improve
feature representation. The SPCI module integrates three components: a
Selective Stream Gate (SSG) for adaptive regulation of global feature flow, a
Perspective Fusion Module (PFM) for context-aware multi-scale integration, and
a Class Discrimination Module (CDM) to enhance inter-class separability. We
embed two SPCI blocks into the P3 and P5 stages of the YOLOv8 backbone,
enabling effective refinement while preserving compatibility with the original
neck and head. Experiments on the NWPU VHR-10 dataset demonstrate that
YOLO-SPCI achieves superior performance compared to state-of-the-art detectors.

</details>


### [375] [Video-Holmes: Can MLLM Think Like Holmes for Complex Video Reasoning?](https://arxiv.org/pdf/2505.21374)
*Junhao Cheng, Yuying Ge, Teng Wang, Yixiao Ge, Jing Liao, Ying Shan*

Main category: cs.CV

TL;DR: The paper introduces Video-Holmes, a benchmark for evaluating complex video reasoning in MLLMs, inspired by Sherlock Holmes. It highlights gaps in current benchmarks and shows MLLMs struggle with integrating clues, scoring below 45% accuracy.


<details>
  <summary>Details</summary>
Motivation: Existing benchmarks focus on visual perception, not complex reasoning. The goal is to assess if MLLMs can reason like humans by integrating scattered clues.

Method: Video-Holmes includes 1,837 questions from 270 suspense films, spanning seven tasks requiring clue integration across video segments.

Result: MLLMs perform poorly, with top model Gemini-2.5-Pro at 45% accuracy, highlighting difficulties in integrating information.

Conclusion: Video-Holmes serves as a 'Holmes-test' for multimodal reasoning, urging models to improve human-like reasoning and addressing current challenges.

Abstract: Recent advances in CoT reasoning and RL post-training have been reported to
enhance video reasoning capabilities of MLLMs. This progress naturally raises a
question: can these models perform complex video reasoning in a manner
comparable to human experts? However, existing video benchmarks primarily
evaluate visual perception and grounding abilities, with questions that can be
answered based on explicit prompts or isolated visual cues. Such benchmarks do
not fully capture the intricacies of real-world reasoning, where humans must
actively search for, integrate, and analyze multiple clues before reaching a
conclusion. To address this issue, we present Video-Holmes, a benchmark
inspired by the reasoning process of Sherlock Holmes, designed to evaluate the
complex video reasoning capabilities of MLLMs. Video-Holmes consists of 1,837
questions derived from 270 manually annotated suspense short films, which spans
seven carefully designed tasks. Each task is constructed by first identifying
key events and causal relationships within films, and then designing questions
that require models to actively locate and connect multiple relevant visual
clues scattered across different video segments. Our comprehensive evaluation
of state-of-the-art MLLMs reveals that, while these models generally excel at
visual perception, they encounter substantial difficulties with integrating
information and often miss critical clues. For example, the best-performing
model, Gemini-2.5-Pro, achieves an accuracy of only 45%, with most models
scoring below 40%. We aim that Video-Holmes can serve as a "Holmes-test" for
multimodal reasoning, motivating models to reason more like humans and
emphasizing the ongoing challenges in this field. The benchmark is released in
https://github.com/TencentARC/Video-Holmes.

</details>


### [376] [GeoLLaVA-8K: Scaling Remote-Sensing Multimodal Large Language Models to 8K Resolution](https://arxiv.org/pdf/2505.21375)
*Fengxiang Wang, Mingshuo Chen, Yueying Li, Di Wang, Haotian Wang, Zonghao Guo, Zefan Wang, Boqi Shan, Long Lan, Yulin Wang, Hongzhen Wang, Wenjing Yang, Bo Du, Jing Zhang*

Main category: cs.CV

TL;DR: The paper introduces GeoLLaVA-8K, a multimodal large language model for ultra-high-resolution remote sensing imagery, addressing data scarcity and token explosion with novel datasets and pruning techniques.


<details>
  <summary>Details</summary>
Motivation: Challenges in using UHR RS imagery include limited training data and token explosion from large image sizes.

Method: Proposes SuperRS-VQA and HighRS-VQA datasets and two strategies (Background Token Pruning and Anchored Token Selection) to reduce redundancy.

Result: GeoLLaVA-8K achieves state-of-the-art performance on XLRS-Bench by handling 8K×8K resolution inputs.

Conclusion: The work advances RS-focused multimodal models by addressing key bottlenecks and introducing efficient token reduction techniques.

Abstract: Ultra-high-resolution (UHR) remote sensing (RS) imagery offers valuable data
for Earth observation but pose challenges for existing multimodal foundation
models due to two key bottlenecks: (1) limited availability of UHR training
data, and (2) token explosion caused by the large image size. To address data
scarcity, we introduce SuperRS-VQA (avg. 8,376$\times$8,376) and HighRS-VQA
(avg. 2,000$\times$1,912), the highest-resolution vision-language datasets in
RS to date, covering 22 real-world dialogue tasks. To mitigate token explosion,
our pilot studies reveal significant redundancy in RS images: crucial
information is concentrated in a small subset of object-centric tokens, while
pruning background tokens (e.g., ocean or forest) can even improve performance.
Motivated by these findings, we propose two strategies: Background Token
Pruning and Anchored Token Selection, to reduce the memory footprint while
preserving key semantics.Integrating these techniques, we introduce
GeoLLaVA-8K, the first RS-focused multimodal large language model capable of
handling inputs up to 8K$\times$8K resolution, built on the LLaVA framework.
Trained on SuperRS-VQA and HighRS-VQA, GeoLLaVA-8K sets a new state-of-the-art
on the XLRS-Bench.

</details>


### [377] [Empowering Vector Graphics with Consistently Arbitrary Viewing and View-dependent Visibility](https://arxiv.org/pdf/2505.21377)
*Yidi Li, Jun Xiao, Zhengda Lu, Yiqun Wang, Haiyong Jiang*

Main category: cs.CV

TL;DR: Dream3DVG introduces a dual-branch framework for text-to-vector graphics generation, enabling viewpoint flexibility, detail control, and occlusion awareness.


<details>
  <summary>Details</summary>
Motivation: To bridge the gap between text prompts and vector graphics while ensuring consistent guidance and progressive detail optimization.

Method: Uses a dual-branch framework: 3D Gaussian Splatting for domain bridging and detail control, and 3D vector graphics optimization with a visibility-awareness rendering module.

Result: Demonstrates superior performance in detail abstraction, cross-view consistency, and occlusion-aware stroke culling.

Conclusion: Dream3DVG effectively generates high-quality vector graphics from text with improved consistency and detail control.

Abstract: This work presents a novel text-to-vector graphics generation approach,
Dream3DVG, allowing for arbitrary viewpoint viewing, progressive detail
optimization, and view-dependent occlusion awareness. Our approach is a
dual-branch optimization framework, consisting of an auxiliary 3D Gaussian
Splatting optimization branch and a 3D vector graphics optimization branch. The
introduced 3DGS branch can bridge the domain gaps between text prompts and
vector graphics with more consistent guidance. Moreover, 3DGS allows for
progressive detail control by scheduling classifier-free guidance, facilitating
guiding vector graphics with coarse shapes at the initial stages and finer
details at later stages. We also improve the view-dependent occlusions by
devising a visibility-awareness rendering module. Extensive results on 3D
sketches and 3D iconographies, demonstrate the superiority of the method on
different abstraction levels of details, cross-view consistency, and
occlusion-aware stroke culling.

</details>


### [378] [ZigzagPointMamba: Spatial-Semantic Mamba for Point Cloud Understanding](https://arxiv.org/pdf/2505.21381)
*Linshuang Diao, Dayong Ren, Sensen Song, Yurong Qian*

Main category: cs.CV

TL;DR: ZigzagPointMamba improves point cloud self-supervised learning by addressing token ordering and masking issues in PointMamba, achieving better performance in downstream tasks.


<details>
  <summary>Details</summary>
Motivation: Existing PointMamba methods disrupt spatial continuity and local semantics due to complex token ordering and random masking.

Method: Proposes a zigzag scan path for token sequencing and a Semantic-Siamese Masking Strategy (SMS) to preserve spatial and semantic correlations.

Result: Achieves significant gains in part segmentation (1.59% mIoU on ShapeNetPart) and classification (0.4% higher accuracy on ModelNet40).

Conclusion: ZigzagPointMamba enhances feature extraction and semantic modeling, outperforming previous methods in efficiency and accuracy.

Abstract: State Space models (SSMs) such as PointMamba enable efficient feature
extraction for point cloud self-supervised learning with linear complexity,
outperforming Transformers in computational efficiency. However, existing
PointMamba-based methods depend on complex token ordering and random masking,
which disrupt spatial continuity and local semantic correlations. We propose
ZigzagPointMamba to tackle these challenges. The core of our approach is a
simple zigzag scan path that globally sequences point cloud tokens, enhancing
spatial continuity by preserving the proximity of spatially adjacent point
tokens. Nevertheless, random masking undermines local semantic modeling in
self-supervised learning. To address this, we introduce a Semantic-Siamese
Masking Strategy (SMS), which masks semantically similar tokens to facilitate
reconstruction by integrating local features of original and similar tokens.
This overcomes the dependence on isolated local features and enables robust
global semantic modeling. Our pre-trained ZigzagPointMamba weights
significantly improve downstream tasks, achieving a 1.59% mIoU gain on
ShapeNetPart for part segmentation, a 0.4% higher accuracy on ModelNet40 for
classification, and 0.19%, 1.22%, and 0.72% higher accuracies respectively for
the classification tasks on the OBJ-BG, OBJ-ONLY, and PB-T50-RS subsets of
ScanObjectNN. The code is available at:
https://anonymous.4open.science/r/ZigzagPointMamba-1800/

</details>


### [379] [Mentor3AD: Feature Reconstruction-based 3D Anomaly Detection via Multi-modality Mentor Learning](https://arxiv.org/pdf/2505.21420)
*Jinbao Wang, Hanzhe Liang, Can Gao, Chenxi Hu, Jie Zhou, Yunkang Cao, Linlin Shen, Weiming Shen*

Main category: cs.CV

TL;DR: Mentor3AD improves 3D anomaly detection by fusing RGB and 3D features via multi-modal mentor learning, enhancing feature reconstruction and detection accuracy.


<details>
  <summary>Details</summary>
Motivation: Leveraging complementary information from dual modalities (RGB and 3D) to better distinguish normal from anomalous features.

Method: Proposes Mentor3AD with Mentor of Fusion Module (MFM), Mentor of Guidance Module (MGM), and Voting Module (VM) for feature fusion and reconstruction.

Result: Validated on MVTec 3D-AD and Eyecandies, showing improved detection performance.

Conclusion: Mentor3AD effectively enhances 3D anomaly detection through multi-modal feature fusion and guided reconstruction.

Abstract: Multimodal feature reconstruction is a promising approach for 3D anomaly
detection, leveraging the complementary information from dual modalities. We
further advance this paradigm by utilizing multi-modal mentor learning, which
fuses intermediate features to further distinguish normal from feature
differences. To address these challenges, we propose a novel method called
Mentor3AD, which utilizes multi-modal mentor learning. By leveraging the shared
features of different modalities, Mentor3AD can extract more effective features
and guide feature reconstruction, ultimately improving detection performance.
Specifically, Mentor3AD includes a Mentor of Fusion Module (MFM) that merges
features extracted from RGB and 3D modalities to create a mentor feature.
Additionally, we have designed a Mentor of Guidance Module (MGM) to facilitate
cross-modal reconstruction, supported by the mentor feature. Lastly, we
introduce a Voting Module (VM) to more accurately generate the final anomaly
score. Extensive comparative and ablation studies on MVTec 3D-AD and Eyecandies
have verified the effectiveness of the proposed method.

</details>


### [380] [Automatically Identify and Rectify: Robust Deep Contrastive Multi-view Clustering in Noisy Scenarios](https://arxiv.org/pdf/2505.21387)
*Xihong Yang, Siwei Wang, Fangdi Wang, Jiaqi Jin, Suyuan Liu, Yue Liu, En Zhu, Xinwang Liu, Yueming Jin*

Main category: cs.CV

TL;DR: AIRMVC is a novel multi-view clustering framework that identifies and rectifies noisy data using GMM and a hybrid strategy, improving robustness in noisy scenarios.


<details>
  <summary>Details</summary>
Motivation: Existing multi-view clustering methods assume clean views, but real-world noise degrades performance. AIRMVC addresses this gap.

Method: Uses GMM for noisy data identification, a hybrid rectification strategy, and a noise-robust contrastive mechanism for reliable representations.

Result: Outperforms state-of-the-art algorithms on six benchmark datasets in noisy scenarios.

Conclusion: AIRMVC effectively handles noisy data, enhancing clustering performance, with code available on GitHub.

Abstract: Leveraging the powerful representation learning capabilities, deep multi-view
clustering methods have demonstrated reliable performance by effectively
integrating multi-source information from diverse views in recent years. Most
existing methods rely on the assumption of clean views. However, noise is
pervasive in real-world scenarios, leading to a significant degradation in
performance. To tackle this problem, we propose a novel multi-view clustering
framework for the automatic identification and rectification of noisy data,
termed AIRMVC. Specifically, we reformulate noisy identification as an anomaly
identification problem using GMM. We then design a hybrid rectification
strategy to mitigate the adverse effects of noisy data based on the
identification results. Furthermore, we introduce a noise-robust contrastive
mechanism to generate reliable representations. Additionally, we provide a
theoretical proof demonstrating that these representations can discard noisy
information, thereby improving the performance of downstream tasks. Extensive
experiments on six benchmark datasets demonstrate that AIRMVC outperforms
state-of-the-art algorithms in terms of robustness in noisy scenarios. The code
of AIRMVC are available at https://github.com/xihongyang1999/AIRMVC on Github.

</details>


### [381] [OmniSync: Towards Universal Lip Synchronization via Diffusion Transformers](https://arxiv.org/pdf/2505.21448)
*Ziqiao Peng, Jiwen Liu, Haoxian Zhang, Xiaoqiang Liu, Songlin Tang, Pengfei Wan, Di Zhang, Hongyan Liu, Jun He*

Main category: cs.CV

TL;DR: OmniSync is a universal lip sync framework using Diffusion Transformer models for mask-free training, flow-matching noise initialization, and dynamic spatiotemporal guidance, outperforming prior methods in diverse scenarios.


<details>
  <summary>Details</summary>
Motivation: Existing lip sync methods struggle with identity consistency, pose variations, and weak audio conditioning. OmniSync aims to address these limitations for diverse visual scenarios.

Method: Uses Diffusion Transformer models for mask-free training, flow-matching noise initialization, and Dynamic Spatiotemporal Classifier-Free Guidance (DS-CFG) for adaptive audio conditioning.

Result: OmniSync achieves superior lip sync accuracy and visual quality in real-world and AI-generated videos.

Conclusion: OmniSync is a robust, universal framework for lip synchronization, excelling in diverse scenarios and outperforming existing methods.

Abstract: Lip synchronization is the task of aligning a speaker's lip movements in
video with corresponding speech audio, and it is essential for creating
realistic, expressive video content. However, existing methods often rely on
reference frames and masked-frame inpainting, which limit their robustness to
identity consistency, pose variations, facial occlusions, and stylized content.
In addition, since audio signals provide weaker conditioning than visual cues,
lip shape leakage from the original video will affect lip sync quality. In this
paper, we present OmniSync, a universal lip synchronization framework for
diverse visual scenarios. Our approach introduces a mask-free training paradigm
using Diffusion Transformer models for direct frame editing without explicit
masks, enabling unlimited-duration inference while maintaining natural facial
dynamics and preserving character identity. During inference, we propose a
flow-matching-based progressive noise initialization to ensure pose and
identity consistency, while allowing precise mouth-region editing. To address
the weak conditioning signal of audio, we develop a Dynamic Spatiotemporal
Classifier-Free Guidance (DS-CFG) mechanism that adaptively adjusts guidance
strength over time and space. We also establish the AIGC-LipSync Benchmark, the
first evaluation suite for lip synchronization in diverse AI-generated videos.
Extensive experiments demonstrate that OmniSync significantly outperforms prior
methods in both visual quality and lip sync accuracy, achieving superior
results in both real-world and AI-generated videos.

</details>


### [382] [Visual Product Graph: Bridging Visual Products And Composite Images For End-to-End Style Recommendations](https://arxiv.org/pdf/2505.21454)
*Yue Li Du, Ben Alexander, Mikhail Antonenka, Rohan Mahadev, Hao-yu Wu, Dmitry Kislyuk*

Main category: cs.CV

TL;DR: The paper introduces Visual Product Graph (VPG), a real-time retrieval system for visually distinct but semantically similar content, enhancing visual search with contextual insights and complementary recommendations.


<details>
  <summary>Details</summary>
Motivation: To address the challenge of retrieving visually distinct but semantically similar content in visual search systems, improving user experience with contextual and complementary product insights.

Method: Leverages high-performance infrastructure and state-of-the-art computer vision models (object detection, visual embeddings) to build VPG, enabling real-time retrieval and recommendations.

Result: Achieves 78.8% extremely similar@1 in human relevance evaluations and a 6% module engagement rate, with deployment at Pinterest.

Conclusion: VPG effectively enhances visual search by providing contextual styling insights and complementary recommendations, demonstrating practical utility in production.

Abstract: Retrieving semantically similar but visually distinct contents has been a
critical capability in visual search systems. In this work, we aim to tackle
this problem with Visual Product Graph (VPG), leveraging high-performance
infrastructure for storage and state-of-the-art computer vision models for
image understanding. VPG is built to be an online real-time retrieval system
that enables navigation from individual products to composite scenes containing
those products, along with complementary recommendations. Our system not only
offers contextual insights by showcasing how products can be styled in a
context, but also provides recommendations for complementary products drawn
from these inspirations. We discuss the essential components for building the
Visual Product Graph, along with the core computer vision model improvements
across object detection, foundational visual embeddings, and other visual
signals. Our system achieves a 78.8% extremely similar@1 in end-to-end human
relevance evaluations, and a 6% module engagement rate. The "Ways to Style It"
module, powered by the Visual Product Graph technology, is deployed in
production at Pinterest.

</details>


### [383] [Active-O3: Empowering Multimodal Large Language Models with Active Perception via GRPO](https://arxiv.org/pdf/2505.21457)
*Muzhi Zhu, Hao Zhong, Canyu Zhao, Zongze Du, Zheng Huang, Mingyu Liu, Hao Chen, Cheng Zou, Jingdong Chen, Ming Yang, Chunhua Shen*

Main category: cs.CV

TL;DR: The paper introduces ACTIVE-O3, a reinforcement learning framework to equip Multimodal Large Language Models (MLLMs) with active perception capabilities, addressing inefficiencies in current methods like GPT-o3.


<details>
  <summary>Details</summary>
Motivation: Active perception is crucial for efficient decision-making in embodied agents, yet MLLMs lack exploration in this area. The paper aims to bridge this gap.

Method: Proposes ACTIVE-O3, a reinforcement learning framework built on GRPO, to enhance MLLMs' active perception. Includes a benchmark suite for evaluation.

Result: ACTIVE-O3 improves search efficiency and accuracy in tasks like small-object grounding and domain-specific scenarios, showing strong zero-shot reasoning.

Conclusion: The work provides a foundation for future research on active perception in MLLMs, offering a simple codebase and evaluation protocol.

Abstract: Active vision, also known as active perception, refers to the process of
actively selecting where and how to look in order to gather task-relevant
information. It is a critical component of efficient perception and
decision-making in humans and advanced embodied agents. Recently, the use of
Multimodal Large Language Models (MLLMs) as central planning and
decision-making modules in robotic systems has gained extensive attention.
However, despite the importance of active perception in embodied intelligence,
there is little to no exploration of how MLLMs can be equipped with or learn
active perception capabilities. In this paper, we first provide a systematic
definition of MLLM-based active perception tasks. We point out that the
recently proposed GPT-o3 model's zoom-in search strategy can be regarded as a
special case of active perception; however, it still suffers from low search
efficiency and inaccurate region selection. To address these issues, we propose
ACTIVE-O3, a purely reinforcement learning based training framework built on
top of GRPO, designed to equip MLLMs with active perception capabilities. We
further establish a comprehensive benchmark suite to evaluate ACTIVE-O3 across
both general open-world tasks, such as small-object and dense object grounding,
and domain-specific scenarios, including small object detection in remote
sensing and autonomous driving, as well as fine-grained interactive
segmentation. In addition, ACTIVE-O3 also demonstrates strong zero-shot
reasoning abilities on the V* Benchmark, without relying on any explicit
reasoning data. We hope that our work can provide a simple codebase and
evaluation protocol to facilitate future research on active perception in
MLLMs.

</details>


### [384] [ID-Align: RoPE-Conscious Position Remapping for Dynamic High-Resolution Adaptation in Vision-Language Models](https://arxiv.org/pdf/2505.21465)
*Bozhou Li, Wentao Zhang*

Main category: cs.CV

TL;DR: ID-Align improves Vision-Language Models by reordering position IDs to enhance interaction between high-resolution and thumbnail tokens, achieving notable performance gains.


<details>
  <summary>Details</summary>
Motivation: The current method of encoding high-resolution and thumbnail images generates many tokens, and RoPE's decay property hinders interaction between tokens and text.

Method: Proposes ID-Align, which reorders position IDs so high-resolution tokens inherit IDs from thumbnail tokens, constraining positional index expansion.

Result: Significant improvements, including 6.09% on MMBench's relation reasoning tasks and gains across multiple benchmarks.

Conclusion: ID-Align effectively addresses interaction issues in VLMs, demonstrating superior performance.

Abstract: Currently, a prevalent approach for enhancing Vision-Language Models (VLMs)
performance is to encode both the high-resolution version and the thumbnail of
an image simultaneously. While effective, this method generates a large number
of image tokens. When combined with the widely used Rotary Position Embedding
(RoPE), its long-term decay property hinders the interaction between
high-resolution tokens and thumbnail tokens, as well as between text and image.
To address these issues, we propose ID-Align, which alleviates these problems
by reordering position IDs. In this method, high-resolution tokens inherit IDs
from their corresponding thumbnail token while constraining the overexpansion
of positional indices. Our experiments conducted within the LLaVA-Next
framework demonstrate that ID-Align achieves significant improvements,
including a 6.09% enhancement on MMBench's relation reasoning tasks and notable
gains across multiple benchmarks. Our code is available at the following link:
https://github.com/zooblastlbz/ID-Align.

</details>


### [385] [Policy Optimized Text-to-Image Pipeline Design](https://arxiv.org/pdf/2505.21478)
*Uri Gadot, Rinon Gal, Yftah Ziser, Gal Chechik, Shie Mannor*

Main category: cs.CV

TL;DR: A reinforcement learning framework automates text-to-image pipeline design, improving efficiency and quality by using reward models and GRPO-based optimization.


<details>
  <summary>Details</summary>
Motivation: Current multi-component pipelines for text-to-image generation require expertise and suffer from computational inefficiencies and poor generalization.

Method: The approach uses reward models to predict image quality, a two-phase training strategy (vocabulary training and GRPO optimization), and classifier-free guidance for enhancement.

Result: The framework creates diverse workflows and achieves superior image quality compared to baselines.

Conclusion: The proposed method addresses limitations of existing approaches, offering a scalable and efficient solution for automated pipeline design.

Abstract: Text-to-image generation has evolved beyond single monolithic models to
complex multi-component pipelines. These combine fine-tuned generators,
adapters, upscaling blocks and even editing steps, leading to significant
improvements in image quality. However, their effective design requires
substantial expertise. Recent approaches have shown promise in automating this
process through large language models (LLMs), but they suffer from two critical
limitations: extensive computational requirements from generating images with
hundreds of predefined pipelines, and poor generalization beyond memorized
training examples. We introduce a novel reinforcement learning-based framework
that addresses these inefficiencies. Our approach first trains an ensemble of
reward models capable of predicting image quality scores directly from
prompt-workflow combinations, eliminating the need for costly image generation
during training. We then implement a two-phase training strategy: initial
workflow vocabulary training followed by GRPO-based optimization that guides
the model toward higher-performing regions of the workflow space. Additionally,
we incorporate a classifier-free guidance based enhancement technique that
extrapolates along the path between the initial and GRPO-tuned models, further
improving output quality. We validate our approach through a set of
comparisons, showing that it can successfully create new flows with greater
diversity and lead to superior image quality compared to existing baselines.

</details>


### [386] [Mitigating Hallucination in Large Vision-Language Models via Adaptive Attention Calibration](https://arxiv.org/pdf/2505.21472)
*Mehrdad Fazli, Bowen Wei, Ziwei Zhu*

Main category: cs.CV

TL;DR: CAAC framework reduces hallucination in LVLMs by balancing attention biases and reinforcing visual grounding, outperforming baselines in long-form generation.


<details>
  <summary>Details</summary>
Motivation: LVLMs suffer from hallucination and struggle with accuracy in open-ended tasks due to spatial perception and modality biases.

Method: CAAC uses Visual-Token Calibration (VTC) to balance attention and Adaptive Attention Re-Scaling (AAR) to reinforce visual grounding based on confidence.

Result: CAAC outperforms baselines on CHAIR, AMBER, and POPE benchmarks, reducing hallucination in long-form generation.

Conclusion: CAAC effectively addresses LVLM hallucination by confidence-aware attention calibration, improving visual alignment.

Abstract: Large vision-language models (LVLMs) achieve impressive performance on
multimodal tasks but often suffer from hallucination, and confidently describe
objects or attributes not present in the image. Current inference-time
interventions, while training-free, struggle to maintain accuracy in open-ended
and long-form generation scenarios. We introduce the Confidence-Aware Attention
Calibration (CAAC) framework to address this challenge by targeting two key
biases: spatial perception bias, which distributes attention disproportionately
across image tokens, and modality bias, which shifts focus from visual to
textual inputs over time. CAAC employs a two-step approach: Visual-Token
Calibration (VTC) to balance attention across visual tokens, and Adaptive
Attention Re-Scaling (AAR) to reinforce visual grounding based on the model's
confidence. This confidence-driven adjustment ensures consistent visual
alignment during generation. Experiments on CHAIR, AMBER, and POPE benchmarks
demonstrate that CAAC outperforms baselines, particularly in long-form
generations, effectively reducing hallucination.

</details>


### [387] [Be Decisive: Noise-Induced Layouts for Multi-Subject Generation](https://arxiv.org/pdf/2505.21488)
*Omer Dahary, Yehonathan Cohen, Or Patashnik, Kfir Aberman, Daniel Cohen-Or*

Main category: cs.CV

TL;DR: A new method improves multi-subject text-to-image generation by predicting and refining noise-induced layouts, avoiding conflicts with external layouts and enhancing alignment.


<details>
  <summary>Details</summary>
Motivation: Existing text-to-image models struggle with subject leakage in complex prompts, requiring spatial knowledge to prevent inaccuracies. External layout controls often conflict with the model's innate layout.

Method: The approach predicts a spatial layout from initial noise, refines it during denoising, and uses a small neural network to maintain clear subject boundaries and consistency.

Result: The noise-aligned strategy improves text-image alignment and multi-subject generation stability while preserving the model's original diversity.

Conclusion: The method outperforms layout-guided techniques by aligning with the model's prior and avoiding external layout conflicts.

Abstract: Generating multiple distinct subjects remains a challenge for existing
text-to-image diffusion models. Complex prompts often lead to subject leakage,
causing inaccuracies in quantities, attributes, and visual features. Preventing
leakage among subjects necessitates knowledge of each subject's spatial
location. Recent methods provide these spatial locations via an external layout
control. However, enforcing such a prescribed layout often conflicts with the
innate layout dictated by the sampled initial noise, leading to misalignment
with the model's prior. In this work, we introduce a new approach that predicts
a spatial layout aligned with the prompt, derived from the initial noise, and
refines it throughout the denoising process. By relying on this noise-induced
layout, we avoid conflicts with externally imposed layouts and better preserve
the model's prior. Our method employs a small neural network to predict and
refine the evolving noise-induced layout at each denoising step, ensuring clear
boundaries between subjects while maintaining consistency. Experimental results
show that this noise-aligned strategy achieves improved text-image alignment
and more stable multi-subject generation compared to existing layout-guided
techniques, while preserving the rich diversity of the model's original
distribution.

</details>


### [388] [DetailFlow: 1D Coarse-to-Fine Autoregressive Image Generation via Next-Detail Prediction](https://arxiv.org/pdf/2505.21473)
*Yiheng Liu, Liao Qu, Huichao Zhang, Xu Wang, Yi Jiang, Yiming Gao, Hu Ye, Xian Li, Shuai Wang, Daniel K. Du, Shu Cheng, Zehuan Yuan, Xinglong Wu*

Main category: cs.CV

TL;DR: DetailFlow is a coarse-to-fine 1D autoregressive image generation method that refines details progressively, achieving high-quality synthesis with fewer tokens and faster inference.


<details>
  <summary>Details</summary>
Motivation: To improve autoregressive image generation by aligning the coarse-to-fine process with the AR mechanism, reducing token count and speeding up inference.

Method: Uses a resolution-aware token sequence supervised by degraded images, enabling global-to-detailed generation. Introduces parallel inference with self-correction for speed and accuracy.

Result: Achieves 2.96 gFID with 128 tokens on ImageNet 256x256, outperforming VAR and FlexVAR. Runs nearly 2x faster with fewer tokens.

Conclusion: DetailFlow offers superior quality and efficiency in image generation compared to state-of-the-art methods.

Abstract: This paper presents DetailFlow, a coarse-to-fine 1D autoregressive (AR) image
generation method that models images through a novel next-detail prediction
strategy. By learning a resolution-aware token sequence supervised with
progressively degraded images, DetailFlow enables the generation process to
start from the global structure and incrementally refine details. This
coarse-to-fine 1D token sequence aligns well with the autoregressive inference
mechanism, providing a more natural and efficient way for the AR model to
generate complex visual content. Our compact 1D AR model achieves high-quality
image synthesis with significantly fewer tokens than previous approaches, i.e.
VAR/VQGAN. We further propose a parallel inference mechanism with
self-correction that accelerates generation speed by approximately 8x while
reducing accumulation sampling error inherent in teacher-forcing supervision.
On the ImageNet 256x256 benchmark, our method achieves 2.96 gFID with 128
tokens, outperforming VAR (3.3 FID) and FlexVAR (3.05 FID), which both require
680 tokens in their AR models. Moreover, due to the significantly reduced token
count and parallel inference mechanism, our method runs nearly 2x faster
inference speed compared to VAR and FlexVAR. Extensive experimental results
demonstrate DetailFlow's superior generation quality and efficiency compared to
existing state-of-the-art methods.

</details>


### [389] [MV-CoLight: Efficient Object Compositing with Consistent Lighting and Shadow Generation](https://arxiv.org/pdf/2505.21483)
*Kerui Ren, Jiayang Bai, Linning Xu, Lihan Jiang, Jiangmiao Pang, Mulin Yu, Bo Dai*

Main category: cs.CV

TL;DR: MV-CoLight is a two-stage framework for illumination-consistent object compositing in 2D/3D, addressing multi-view consistency and scalability issues with a novel feed-forward architecture and Hilbert curve-based mapping.


<details>
  <summary>Details</summary>
Motivation: Existing methods struggle with multi-view consistency, complex scenes, and diverse lighting, while inverse rendering techniques face scalability and efficiency limitations.

Method: MV-CoLight uses a feed-forward architecture to model lighting and shadows directly, avoiding iterative biases, and employs Hilbert curve-based mapping for seamless 2D-3D alignment.

Result: Achieves state-of-the-art harmonized results on benchmarks and real-world scenes, demonstrating robustness and generalization.

Conclusion: MV-CoLight broadens applicability for AR and embodied intelligence by addressing key challenges in illumination-consistent compositing.

Abstract: Object compositing offers significant promise for augmented reality (AR) and
embodied intelligence applications. Existing approaches predominantly focus on
single-image scenarios or intrinsic decomposition techniques, facing challenges
with multi-view consistency, complex scenes, and diverse lighting conditions.
Recent inverse rendering advancements, such as 3D Gaussian and diffusion-based
methods, have enhanced consistency but are limited by scalability, heavy data
requirements, or prolonged reconstruction time per scene. To broaden its
applicability, we introduce MV-CoLight, a two-stage framework for
illumination-consistent object compositing in both 2D images and 3D scenes. Our
novel feed-forward architecture models lighting and shadows directly, avoiding
the iterative biases of diffusion-based methods. We employ a Hilbert
curve-based mapping to align 2D image inputs with 3D Gaussian scene
representations seamlessly. To facilitate training and evaluation, we further
introduce a large-scale 3D compositing dataset. Experiments demonstrate
state-of-the-art harmonized results across standard benchmarks and our dataset,
as well as casually captured real-world scenes demonstrate the framework's
robustness and wide generalization.

</details>


### [390] [ViewSpatial-Bench: Evaluating Multi-perspective Spatial Localization in Vision-Language Models](https://arxiv.org/pdf/2505.21500)
*Dingming Li, Hongxing Li, Zixuan Wang, Yuchen Yan, Hang Zhang, Siqi Chen, Guiyang Hou, Shengpei Jiang, Wenqi Zhang, Yongliang Shen, Weiming Lu, Yueting Zhuang*

Main category: cs.CV

TL;DR: The paper introduces ViewSpatial-Bench, a benchmark for evaluating VLMs on multi-viewpoint spatial tasks, revealing their limitations in allocentric reasoning and showing significant improvement after fine-tuning.


<details>
  <summary>Details</summary>
Motivation: Current VLMs excel in egocentric spatial reasoning but struggle with allocentric viewpoints, limiting their broader applicability in spatial tasks.

Method: The authors develop ViewSpatial-Bench, a benchmark with five task types, and use an automated 3D annotation pipeline to generate precise labels. They evaluate and fine-tune VLMs on this dataset.

Result: VLMs perform well on egocentric tasks but poorly on allocentric ones. Fine-tuning improves performance by 46.24%.

Conclusion: The work establishes a benchmark for spatial intelligence in AI and shows that modeling 3D relationships enhances VLMs' spatial comprehension.

Abstract: Vision-language models (VLMs) have demonstrated remarkable capabilities in
understanding and reasoning about visual content, but significant challenges
persist in tasks requiring cross-viewpoint understanding and spatial reasoning.
We identify a critical limitation: current VLMs excel primarily at egocentric
spatial reasoning (from the camera's perspective) but fail to generalize to
allocentric viewpoints when required to adopt another entity's spatial frame of
reference. We introduce ViewSpatial-Bench, the first comprehensive benchmark
designed specifically for multi-viewpoint spatial localization recognition
evaluation across five distinct task types, supported by an automated 3D
annotation pipeline that generates precise directional labels. Comprehensive
evaluation of diverse VLMs on ViewSpatial-Bench reveals a significant
performance disparity: models demonstrate reasonable performance on
camera-perspective tasks but exhibit reduced accuracy when reasoning from a
human viewpoint. By fine-tuning VLMs on our multi-perspective spatial dataset,
we achieve an overall performance improvement of 46.24% across tasks,
highlighting the efficacy of our approach. Our work establishes a crucial
benchmark for spatial intelligence in embodied AI systems and provides
empirical evidence that modeling 3D spatial relationships enhances VLMs'
corresponding spatial comprehension capabilities.

</details>


### [391] [Frame In-N-Out: Unbounded Controllable Image-to-Video Generation](https://arxiv.org/pdf/2505.21491)
*Boyang Wang, Xuweiyi Chen, Matheus Gadelha, Zezhou Cheng*

Main category: cs.CV

TL;DR: The paper introduces a method for controllable video generation using Frame In and Frame Out techniques, outperforming existing baselines.


<details>
  <summary>Details</summary>
Motivation: Addressing challenges in video generation like controllability, temporal coherence, and detail synthesis by exploring the underexplored Frame In and Frame Out cinematic technique.

Method: Proposes an identity-preserving motion-controllable video Diffusion Transformer architecture, supported by a semi-automatically curated dataset and evaluation protocol.

Result: The approach significantly outperforms existing baselines in evaluation.

Conclusion: The proposed method effectively enhances controllability and detail synthesis in video generation, validated by superior performance.

Abstract: Controllability, temporal coherence, and detail synthesis remain the most
critical challenges in video generation. In this paper, we focus on a commonly
used yet underexplored cinematic technique known as Frame In and Frame Out.
Specifically, starting from image-to-video generation, users can control the
objects in the image to naturally leave the scene or provide breaking new
identity references to enter the scene, guided by user-specified motion
trajectory. To support this task, we introduce a new dataset curated
semi-automatically, a comprehensive evaluation protocol targeting this setting,
and an efficient identity-preserving motion-controllable video Diffusion
Transformer architecture. Our evaluation shows that our proposed approach
significantly outperforms existing baselines.

</details>


### [392] [InstructPart: Task-Oriented Part Segmentation with Instruction Reasoning](https://arxiv.org/pdf/2505.18291)
*Zifu Wan, Yaqi Xie, Ce Zhang, Zhiqiu Lin, Zihan Wang, Simon Stepputtis, Deva Ramanan, Katia Sycara*

Main category: cs.CV

TL;DR: The paper introduces InstructPart, a benchmark for evaluating part-level understanding in vision-language models, showing current models struggle with task-oriented part segmentation. A simple baseline improves performance significantly.


<details>
  <summary>Details</summary>
Motivation: Existing multimodal models treat objects as indivisible, missing insights into their components and affordances, which are crucial for functionality in tasks like robotics and information retrieval.

Method: The authors create InstructPart, a dataset with hand-labeled part segmentation and task-oriented instructions, and test state-of-the-art VLMs. They also propose a fine-tuned baseline.

Result: Task-oriented part segmentation is challenging for VLMs, but the baseline achieves a twofold performance improvement.

Conclusion: InstructPart aims to advance research in part-level understanding and broaden VLM applications in robotics, VR, and more.

Abstract: Large multimodal foundation models, particularly in the domains of language
and vision, have significantly advanced various tasks, including robotics,
autonomous driving, information retrieval, and grounding. However, many of
these models perceive objects as indivisible, overlooking the components that
constitute them. Understanding these components and their associated
affordances provides valuable insights into an object's functionality, which is
fundamental for performing a wide range of tasks. In this work, we introduce a
novel real-world benchmark, InstructPart, comprising hand-labeled part
segmentation annotations and task-oriented instructions to evaluate the
performance of current models in understanding and executing part-level tasks
within everyday contexts. Through our experiments, we demonstrate that
task-oriented part segmentation remains a challenging problem, even for
state-of-the-art Vision-Language Models (VLMs). In addition to our benchmark,
we introduce a simple baseline that achieves a twofold performance improvement
through fine-tuning with our dataset. With our dataset and benchmark, we aim to
facilitate research on task-oriented part segmentation and enhance the
applicability of VLMs across various domains, including robotics, virtual
reality, information retrieval, and other related fields. Project website:
https://zifuwan.github.io/InstructPart/.

</details>


### [393] [Adversarial Attacks against Closed-Source MLLMs via Feature Optimal Alignment](https://arxiv.org/pdf/2505.21494)
*Xiaojun Jia, Sensen Gao, Simeng Qin, Tianyu Pang, Chao Du, Yihao Huang, Xinfeng Li, Yiming Li, Bo Li, Yang Liu*

Main category: cs.CV

TL;DR: FOA-Attack improves adversarial transferability in MLLMs by aligning global and local features using cosine similarity and optimal transport, outperforming existing methods.


<details>
  <summary>Details</summary>
Motivation: Existing methods for targeted adversarial attacks on MLLMs overlook local patch token information, leading to suboptimal transferability, especially for closed-source models.

Method: FOA-Attack aligns global features via cosine similarity and local features via optimal transport, using clustering to refine alignment and a dynamic ensemble strategy for model weighting.

Result: The method outperforms state-of-the-art techniques, particularly in transferring to closed-source MLLMs.

Conclusion: FOA-Attack effectively enhances adversarial transferability by leveraging both global and local feature alignment, with practical benefits for closed-source models.

Abstract: Multimodal large language models (MLLMs) remain vulnerable to transferable
adversarial examples. While existing methods typically achieve targeted attacks
by aligning global features-such as CLIP's [CLS] token-between adversarial and
target samples, they often overlook the rich local information encoded in patch
tokens. This leads to suboptimal alignment and limited transferability,
particularly for closed-source models. To address this limitation, we propose a
targeted transferable adversarial attack method based on feature optimal
alignment, called FOA-Attack, to improve adversarial transfer capability.
Specifically, at the global level, we introduce a global feature loss based on
cosine similarity to align the coarse-grained features of adversarial samples
with those of target samples. At the local level, given the rich local
representations within Transformers, we leverage clustering techniques to
extract compact local patterns to alleviate redundant local features. We then
formulate local feature alignment between adversarial and target samples as an
optimal transport (OT) problem and propose a local clustering optimal transport
loss to refine fine-grained feature alignment. Additionally, we propose a
dynamic ensemble model weighting strategy to adaptively balance the influence
of multiple models during adversarial example generation, thereby further
improving transferability. Extensive experiments across various models
demonstrate the superiority of the proposed method, outperforming
state-of-the-art methods, especially in transferring to closed-source MLLMs.
The code is released at https://github.com/jiaxiaojunQAQ/FOA-Attack.

</details>


### [394] [Vision Transformers with Self-Distilled Registers](https://arxiv.org/pdf/2505.21501)
*Yinjie Chen, Zipeng Yan, Chong Zhou, Bo Dai, Andrew F. Luo*

Main category: cs.CV

TL;DR: PH-Reg, a self-distillation method, integrates register tokens into pre-trained ViTs to mitigate artifact tokens without full retraining, improving segmentation and depth prediction.


<details>
  <summary>Details</summary>
Motivation: Artifact tokens in ViTs degrade performance in fine-grained tasks; adding register tokens helps, but retraining large pre-trained ViTs is infeasible.

Method: PH-Reg uses self-distillation: a frozen teacher ViT generates denoised embeddings to train a student ViT with added register tokens, optimizing only a small subset of weights.

Result: PH-Reg reduces artifact tokens, enhancing segmentation and depth prediction in zero-shot and linear probing scenarios.

Conclusion: PH-Reg efficiently improves ViT performance by addressing artifact tokens without costly retraining.

Abstract: Vision Transformers (ViTs) have emerged as the dominant architecture for
visual processing tasks, demonstrating excellent scalability with increased
training data and model size. However, recent work has identified the emergence
of artifact tokens in ViTs that are incongruous with the local semantics. These
anomalous tokens degrade ViT performance in tasks that require fine-grained
localization or structural coherence. An effective mitigation of this issue is
to the addition of register tokens to ViTs, which implicitly "absorb" the
artifact term during training. Given the availability of various large-scale
pre-trained ViTs, in this paper we aim at equipping them with such register
tokens without the need of re-training them from scratch, which is infeasible
considering their size. Specifically, we propose Post Hoc Registers (PH-Reg),
an efficient self-distillation method that integrates registers into an
existing ViT without requiring additional labeled data and full retraining.
PH-Reg initializes both teacher and student networks from the same pre-trained
ViT. The teacher remains frozen and unmodified, while the student is augmented
with randomly initialized register tokens. By applying test-time augmentation
to the teacher's inputs, we generate denoised dense embeddings free of
artifacts, which are then used to optimize only a small subset of unlocked
student weights. We show that our approach can effectively reduce the number of
artifact tokens, improving the segmentation and depth prediction of the student
ViT under zero-shot and linear probing.

</details>


### [395] [Generalizable and Relightable Gaussian Splatting for Human Novel View Synthesis](https://arxiv.org/pdf/2505.21502)
*Yipengjing Sun, Chenyang Wang, Shunyuan Zheng, Zonglin Li, Shengping Zhang, Xiangyang Ji*

Main category: cs.CV

TL;DR: GRGS is a 3D Gaussian framework for high-fidelity human novel view synthesis under diverse lighting, using feed-forward supervised learning to project 2D cues into 3D representations.


<details>
  <summary>Details</summary>
Motivation: Existing methods either require per-character optimization or ignore physical constraints, limiting generalizability and relighting capabilities.

Method: GRGS uses a Lighting-aware Geometry Refinement (LGR) module for lighting-invariant geometry and a Physically Grounded Neural Rendering (PGNR) module for physics-based shading. Training leverages 2D-to-3D projection with differentiable supervision.

Result: GRGS achieves superior visual quality, geometric consistency, and generalization across characters and lighting conditions.

Conclusion: GRGS provides a generalizable and relightable solution for high-fidelity human novel view synthesis, outperforming existing methods.

Abstract: We propose GRGS, a generalizable and relightable 3D Gaussian framework for
high-fidelity human novel view synthesis under diverse lighting conditions.
Unlike existing methods that rely on per-character optimization or ignore
physical constraints, GRGS adopts a feed-forward, fully supervised strategy
that projects geometry, material, and illumination cues from multi-view 2D
observations into 3D Gaussian representations. Specifically, to reconstruct
lighting-invariant geometry, we introduce a Lighting-aware Geometry Refinement
(LGR) module trained on synthetically relit data to predict accurate depth and
surface normals. Based on the high-quality geometry, a Physically Grounded
Neural Rendering (PGNR) module is further proposed to integrate neural
prediction with physics-based shading, supporting editable relighting with
shadows and indirect illumination. Besides, we design a 2D-to-3D projection
training scheme that leverages differentiable supervision from ambient
occlusion, direct, and indirect lighting maps, which alleviates the
computational cost of explicit ray tracing. Extensive experiments demonstrate
that GRGS achieves superior visual quality, geometric consistency, and
generalization across characters and lighting conditions.

</details>


### [396] [Multiple Different Black Box Explanations for Image Classifiers](https://arxiv.org/pdf/2309.14309)
*Hana Chockler, David A. Kelly, Daniel Kroening*

Main category: cs.CV

TL;DR: MultEX is a tool that generates multiple explanations for image classifier outputs, improving insight and error detection compared to single-explanation methods.


<details>
  <summary>Details</summary>
Motivation: Existing tools provide only one explanation per image, limiting understanding of classifier behavior. Multiple explanations can enhance analysis and error detection.

Method: MultEX uses a principled approach based on actual causality to compute multiple explanations for black-box image classifiers. Theoretical complexity is analyzed.

Result: MultEX outperforms state-of-the-art methods, finding more and higher-quality explanations across three models and datasets.

Conclusion: MultEX enhances classifier analysis by providing multiple, high-quality explanations, addressing limitations of single-explanation tools.

Abstract: Existing explanation tools for image classifiers usually give only a single
explanation for an image's classification. For many images, however, image
classifiers accept more than one explanation for the image label. These
explanations are useful for analyzing the decision process of the classifier
and for detecting errors. Thus, restricting the number of explanations to just
one severely limits insight into the behavior of the classifier.
  In this paper, we describe an algorithm and a tool, MultEX, for computing
multiple explanations as the output of a black-box image classifier for a given
image. Our algorithm uses a principled approach based on actual causality. We
analyze its theoretical complexity and evaluate MultEX against the
state-of-the-art across three different models and three different datasets. We
find that MultEX finds more explanations and that these explanations are of
higher quality.

</details>


### [397] [LIB-KD: Learning Inductive Bias, Not Just Parameters A New Perspective on Knowledge Distillations](https://arxiv.org/pdf/2310.00369)
*Gousia Habib, Tausifa Jan Saleem, Ishfaq Ahmad Malik, Brejesh Lall*

Main category: cs.CV

TL;DR: The paper introduces an ensemble-based distillation approach to enhance Vision Transformers (ViTs) by leveraging lightweight teacher models with diverse architectural biases, improving efficiency and performance.


<details>
  <summary>Details</summary>
Motivation: ViTs lack inherent inductive biases, requiring large datasets for training. The goal is to make ViTs practical by distilling knowledge from complementary lightweight teacher models.

Method: An ensemble of lightweight teachers (e.g., convolution and involution models) jointly instructs the student ViT. Precomputed logits are stored to avoid repeated forward passes, optimizing the distillation process.

Result: The approach enhances student ViT performance by leveraging diverse inductive biases and reduces computational burden through logit precomputation.

Conclusion: The proposed framework makes ViTs more practical by improving efficiency and performance through ensemble-based distillation and logit optimization.

Abstract: With the rapid development of computer vision, Vision Transformers (ViTs)
offer the tantalizing prospect of unified information processing across visual
and textual domains. But due to the lack of inherent inductive biases in ViTs,
they require enormous amount of data for training. To make their applications
practical, we introduce an innovative ensemble-based distillation approach
distilling inductive bias from complementary lightweight teacher models. Prior
systems relied solely on convolution-based teaching. However, this method
incorporates an ensemble of light teachers with different architectural
tendencies, such as convolution and involution, to instruct the student
transformer jointly. Because of these unique inductive biases, instructors can
accumulate a wide range of knowledge, even from readily identifiable stored
datasets, which leads to enhanced student performance. Our proposed framework
also involves precomputing and storing logits in advance, essentially the
unnormalized predictions of the model. This optimization can accelerate the
distillation process by eliminating the need for repeated forward passes during
knowledge distillation, significantly reducing the computational burden and
enhancing efficiency.

</details>


### [398] [PLGSLAM: Progressive Neural Scene Represenation with Local to Global Bundle Adjustment](https://arxiv.org/pdf/2312.09866)
*Tianchen Deng, Guole Shen, Tong Qin, Jianyu Wang, Wentao Zhao, Jingchuan Wang, Danwei Wang, Weidong Chen*

Main category: cs.CV

TL;DR: PLGSLAM introduces a neural visual SLAM system for high-fidelity surface reconstruction and robust real-time camera tracking in large indoor scenes, using progressive scene representation and local-to-global bundle adjustment.


<details>
  <summary>Details</summary>
Motivation: Existing neural implicit SLAM methods struggle with low-quality reconstructions and inaccurate localization in large scenes due to limited global radiance field capacity and non-robust pose networks.

Method: PLGSLAM uses progressive scene representation with dynamic local allocation, tri-planes for high-frequency features, and MLPs for low-frequency features. It also employs local-to-global bundle adjustment with a global keyframe database.

Result: PLGSLAM achieves state-of-the-art reconstruction and tracking performance in both small and large-scale indoor environments.

Conclusion: PLGSLAM effectively addresses scalability and robustness challenges in neural SLAM, outperforming existing methods in large scenes and long sequences.

Abstract: Neural implicit scene representations have recently shown encouraging results
in dense visual SLAM. However, existing methods produce low-quality scene
reconstruction and low-accuracy localization performance when scaling up to
large indoor scenes and long sequences. These limitations are mainly due to
their single, global radiance field with finite capacity, which does not adapt
to large scenarios. Their end-to-end pose networks are also not robust enough
with the growth of cumulative errors in large scenes. To this end, we introduce
PLGSLAM, a neural visual SLAM system capable of high-fidelity surface
reconstruction and robust camera tracking in real-time. To handle large-scale
indoor scenes, PLGSLAM proposes a progressive scene representation method which
dynamically allocates new local scene representation trained with frames within
a local sliding window. This allows us to scale up to larger indoor scenes and
improves robustness (even under pose drifts). In local scene representation,
PLGSLAM utilizes tri-planes for local high-frequency features with multi-layer
perceptron (MLP) networks for the low-frequency feature, achieving smoothness
and scene completion in unobserved areas. Moreover, we propose local-to-global
bundle adjustment method with a global keyframe database to address the
increased pose drifts on long sequences. Experimental results demonstrate that
PLGSLAM achieves state-of-the-art scene reconstruction results and tracking
performance across various datasets and scenarios (both in small and
large-scale indoor environments). The code is open-sourced at
https://github.com/dtc111111/plgslam.

</details>


### [399] [UOD: Unseen Object Detection in 3D Point Cloud](https://arxiv.org/pdf/2401.03846)
*Hyunjun Choi, Daeho Um, Hawook Jeong*

Main category: cs.CV

TL;DR: The paper proposes methods to improve 3D object detection and OOD classification for unseen objects, introducing new benchmarks and showing significant performance gains.


<details>
  <summary>Details</summary>
Motivation: Addressing challenges in localizing and recognizing unseen 3D objects, crucial for autonomous driving in the wild.

Method: Anomaly sample augmentation, universal objectness learning, detecting unseen objects, and distinguishing unseen objects.

Result: Proposed methods outperform existing approaches by a large margin on new benchmarks (KITTI Misc, Nuscenes OOD, SUN-RGBD OOD).

Conclusion: The methods provide insights for future work on unseen 3D object detection in real-world scenarios.

Abstract: Existing 3D object detectors encounter extreme challenges in localizing
unseen 3D objects and recognizing them as unseen, which is a crucial technology
in autonomous driving in the wild. To address these challenges, we propose
practical methods to enhance the performance of 3D detection and
Out-Of-Distribution (OOD) classification for unseen objects. The proposed
methods include anomaly sample augmentation, learning of universal objectness,
learning of detecting unseen objects, and learning of distinguishing unseen
objects. To demonstrate the effectiveness of our approach, we propose the KITTI
Misc benchmark and two additional synthetic OOD benchmarks: the Nuscenes OOD
benchmark and the SUN-RGBD OOD benchmark. The proposed methods consistently
enhance performance by a large margin across all existing methods, giving
insight for future work on unseen 3D object detection in the wild.

</details>


### [400] [MetaGS: A Meta-Learned Gaussian-Phong Model for Out-of-Distribution 3D Scene Relighting](https://arxiv.org/pdf/2405.20791)
*Yumeng He, Yunbo Wang, Xiaokang Yang*

Main category: cs.CV

TL;DR: MetaGS addresses OOD 3D relighting by combining meta-learning with physical priors, improving generalization and accuracy.


<details>
  <summary>Details</summary>
Motivation: Existing relighting methods fail in OOD scenarios due to inconsistent light distributions. MetaGS aims to generalize across diverse lighting conditions.

Method: Uses meta-learning for 3D Gaussian splatting and embeds Blinn-Phong reflection priors to decouple shading components.

Result: Effective on synthetic and real datasets, supporting point-light relighting and unseen environment lighting.

Conclusion: MetaGS outperforms existing methods in OOD relighting by leveraging meta-learning and physical priors.

Abstract: Out-of-distribution (OOD) 3D relighting requires novel view synthesis under
unseen lighting conditions that differ significantly from the observed images.
Existing relighting methods, which assume consistent light source distributions
between training and testing, often degrade in OOD scenarios. We introduce
MetaGS to tackle this challenge from two perspectives. First, we propose a
meta-learning approach to train 3D Gaussian splatting, which explicitly
promotes learning generalizable Gaussian geometries and appearance attributes
across diverse lighting conditions, even with biased training data. Second, we
embed fundamental physical priors from the Blinn-Phong reflection model into
Gaussian splatting, which enhances the decoupling of shading components and
leads to more accurate 3D scene reconstruction. Results on both synthetic and
real-world datasets demonstrate the effectiveness of MetaGS in challenging OOD
relighting tasks, supporting efficient point-light relighting and generalizing
well to unseen environment lighting maps.

</details>


### [401] [ZeroPur: Succinct Training-Free Adversarial Purification](https://arxiv.org/pdf/2406.03143)
*Erhu Liu, Zonglin Yang, Bo Liu, Bin Xiao, Xiuli Bi*

Main category: cs.CV

TL;DR: ZeroPur is a simple adversarial purification method that projects adversarial images back to the natural image manifold without retraining, achieving state-of-the-art robust performance.


<details>
  <summary>Details</summary>
Motivation: Existing adversarial purification methods rely on external models or retraining, which is domain-dependent and computationally expensive. ZeroPur aims to address these limitations.

Method: ZeroPur uses Guided Shift to obtain shifted embeddings of adversarial examples and Adaptive Projection to project them back to the natural image manifold, without retraining.

Result: Extensive experiments on CIFAR-10, CIFAR-100, and ImageNet-1K show ZeroPur achieves state-of-the-art robust performance.

Conclusion: ZeroPur is an effective, training-free adversarial purification method that outperforms existing techniques.

Abstract: Adversarial purification is a kind of defense technique that can defend
against various unseen adversarial attacks without modifying the victim
classifier. Existing methods often depend on external generative models or
cooperation between auxiliary functions and victim classifiers. However,
retraining generative models, auxiliary functions, or victim classifiers relies
on the domain of the fine-tuned dataset and is computation-consuming. In this
work, we suppose that adversarial images are outliers of the natural image
manifold, and the purification process can be considered as returning them to
this manifold. Following this assumption, we present a simple adversarial
purification method without further training to purify adversarial images,
called ZeroPur. ZeroPur contains two steps: given an adversarial example,
Guided Shift obtains the shifted embedding of the adversarial example by the
guidance of its blurred counterparts; after that, Adaptive Projection
constructs a directional vector by this shifted embedding to provide momentum,
projecting adversarial images onto the manifold adaptively. ZeroPur is
independent of external models and requires no retraining of victim classifiers
or auxiliary functions, relying solely on victim classifiers themselves to
achieve purification. Extensive experiments on three datasets (CIFAR-10,
CIFAR-100, and ImageNet-1K) using various classifier architectures (ResNet,
WideResNet) demonstrate that our method achieves state-of-the-art robust
performance. The code will be publicly available.

</details>


### [402] [Minimal Interaction Separated Tuning: A New Paradigm for Visual Adaptation](https://arxiv.org/pdf/2406.17559)
*Ningyuan Tang, Minghao Fu, Jianxin Wu*

Main category: cs.CV

TL;DR: The paper introduces MIST, a method for efficient fine-tuning of large vision models on low-resource devices by minimizing information transfer and using lightweight adaptors.


<details>
  <summary>Details</summary>
Motivation: Addressing the difficulty of fine-tuning large vision models on low-resource devices due to computational constraints.

Method: Proposes Minimal Interaction Separated Tuning (MIST), which uses intermediate feature sums and a lightweight attention-based adaptor for efficient adaptation.

Result: MIST achieves efficient information transfer, parameter use, and computational/memory efficiency while performing competitively on benchmarks.

Conclusion: MIST offers a practical solution for adapting large vision models on low-resource devices with minimal overhead.

Abstract: The rapid scaling of large vision pretrained models makes fine-tuning tasks
more and more difficult on devices with low computational resources. We explore
a new visual adaptation paradigm called separated tuning, which treats large
pretrained models as standalone feature extractors that run on powerful cloud
servers. The fine-tuning carries out on devices which possess only low
computational resources (slow CPU, no GPU, small memory, etc.) Existing methods
that are potentially suitable for our separated tuning paradigm are discussed.
But, three major drawbacks hinder their application in separated tuning: low
adaptation capability, large adapter network, and in particular, high
information transfer overhead. To address these issues, we propose Minimal
Interaction Separated Tuning, or MIST, which reveals that the sum of
intermediate features from pretrained models not only has minimal information
transfer but also has high adaptation capability. With a lightweight
attention-based adaptor network, MIST achieves information transfer efficiency,
parameter efficiency, computational and memory efficiency, and at the same time
demonstrates competitive results on various visual adaptation benchmarks.

</details>


### [403] [Multimodal Causal Reasoning Benchmark: Challenging Vision Large Language Models to Discern Causal Links Across Modalities](https://arxiv.org/pdf/2408.08105)
*Zhiyuan Li, Heng Wang, Dongnan Liu, Chaoyi Zhang, Ao Ma, Jieting Long, Weidong Cai*

Main category: cs.CV

TL;DR: MuCR is a new benchmark for testing Multimodal Large Language Models (MLLMs) on causal reasoning across text and vision, revealing their limitations and proposing a solution (VcCoT) to improve performance.


<details>
  <summary>Details</summary>
Motivation: To investigate if MLLMs can generalize causal reasoning from text to vision and identify factors influencing cross-modal generalization.

Method: Introduces MuCR, a benchmark with synthetic siamese images and text pairs, and develops tailored metrics for assessment. Proposes VcCoT to highlight visual cues.

Result: Current MLLMs underperform in multimodal causal reasoning compared to text-only tasks. Visual cue identification is crucial for cross-modal generalization. VcCoT improves performance.

Conclusion: MuCR highlights MLLMs' limitations in multimodal causal reasoning and demonstrates the effectiveness of VcCoT in enhancing their capabilities.

Abstract: Multimodal Large Language Models (MLLMs) have showcased exceptional
Chain-of-Thought (CoT) reasoning ability in complex textual inference tasks
including causal reasoning. However, will these causalities remain
straightforward when crucial hints hide in visual details? If not, what factors
might influence cross-modal generalization? Whether we can effectively enhance
their capacity for robust causal inference across both text and vision?
Motivated by these, we introduce MuCR - a novel Multimodal Causal Reasoning
benchmark that leverages synthetic siamese images and text pairs to challenge
MLLMs. Additionally, we develop tailored metrics from multiple perspectives,
including image-level match, phrase-level understanding, and sentence-level
explanation, to comprehensively assess MLLMs' comprehension abilities. Our
experiments reveal that current MLLMs fall short in multimodal causal reasoning
compared to their performance in purely textual settings. Additionally, we find
that identifying visual cues across images is key to effective cross-modal
generalization. Finally, we propose a VcCoT strategy that better highlights
visual cues, and our results confirm its efficacy in enhancing multimodal
causal reasoning. The project is available at:
https://github.com/Zhiyuan-Li-John/MuCR

</details>


### [404] [Shaping a Stabilized Video by Mitigating Unintended Changes for Concept-Augmented Video Editing](https://arxiv.org/pdf/2410.12526)
*Mingce Guo, Jingxuan He, Shengeng Tang, Zhangye Wang, Lechao Cheng*

Main category: cs.CV

TL;DR: The paper proposes a concept-augmented video editing method using diffusion models to improve flexibility and stability in text-driven video editing.


<details>
  <summary>Details</summary>
Motivation: Existing methods are limited by pre-training word embeddings, causing issues with nuanced editing of open concepts and disrupting attention mechanisms.

Method: The approach uses concept-augmented textual inversion and dual prior supervision to enhance flexibility and stability in video editing.

Result: The method generates more stable and lifelike videos, outperforming current state-of-the-art techniques.

Conclusion: The proposed framework effectively improves video editing by addressing limitations of existing approaches.

Abstract: Text-driven video editing utilizing generative diffusion models has garnered
significant attention due to their potential applications. However, existing
approaches are constrained by the limited word embeddings provided in
pre-training, which hinders nuanced editing targeting open concepts with
specific attributes. Directly altering the keywords in target prompts often
results in unintended disruptions to the attention mechanisms. To achieve more
flexible editing easily, this work proposes an improved concept-augmented video
editing approach that generates diverse and stable target videos flexibly by
devising abstract conceptual pairs. Specifically, the framework involves
concept-augmented textual inversion and a dual prior supervision mechanism. The
former enables plug-and-play guidance of stable diffusion for video editing,
effectively capturing target attributes for more stylized results. The dual
prior supervision mechanism significantly enhances video stability and
fidelity. Comprehensive evaluations demonstrate that our approach generates
more stable and lifelike videos, outperforming state-of-the-art methods.

</details>


### [405] [SURDS: Benchmarking Spatial Understanding and Reasoning in Driving Scenarios with Vision Language Models](https://arxiv.org/pdf/2411.13112)
*Xianda Guo, Ruijun Zhang, Yiqun Duan, Yuhang He, Dujun Nie, Wenke Huang, Chenming Zhang, Shuai Liu, Hao Zhao, Long Chen*

Main category: cs.CV

TL;DR: SURDS is a benchmark for evaluating spatial reasoning in VLMs, revealing their limitations and proposing a reinforcement learning-based alignment method (GRPO) to improve performance.


<details>
  <summary>Details</summary>
Motivation: Accurate spatial reasoning is crucial for autonomous driving tasks like mapping and planning, but current VLMs lack fine-grained spatial understanding.

Method: Built on nuScenes, SURDS includes 41,080 training and 9,250 evaluation samples across six spatial categories. A GRPO-aligned variant is proposed using reinforcement learning with spatially grounded rewards.

Result: GRPO-aligned VLMs achieve a score of 40.80, outperforming GPT-4o (13.30) and Gemini-2.0-flash (35.71).

Conclusion: Reinforcement learning-based alignment significantly enhances VLM spatial reasoning, with SURDS and GRPO tools released for public use.

Abstract: Accurate spatial reasoning in outdoor environments - covering geometry,
object pose, and inter-object relationships - is fundamental to downstream
tasks such as mapping, motion forecasting, and high-level planning in
autonomous driving. We introduce SURDS, a large-scale benchmark designed to
systematically evaluate the spatial reasoning capabilities of vision language
models (VLMs). Built on the nuScenes dataset, SURDS comprises 41,080
vision-question-answer training instances and 9,250 evaluation samples,
spanning six spatial categories: orientation, depth estimation, pixel-level
localization, pairwise distance, lateral ordering, and front-behind relations.
We benchmark leading general-purpose VLMs, including GPT, Gemini, and Qwen,
revealing persistent limitations in fine-grained spatial understanding. To
address these deficiencies, we go beyond static evaluation and explore whether
alignment techniques can improve spatial reasoning performance. Specifically,
we propose a reinforcement learning-based alignment scheme leveraging spatially
grounded reward signals - capturing both perception-level accuracy (location)
and reasoning consistency (logic). We further incorporate final-answer
correctness and output-format rewards to guide fine-grained policy adaptation.
Our GRPO-aligned variant achieves an overall score of 40.80 in the SURDS
benchmark. Notably, it outperforms proprietary systems such as GPT-4o (13.30)
and Gemini-2.0-flash (35.71). To our best knowledge, this is the first study to
demonstrate that reinforcement learning-based alignment can significantly and
consistently enhance the spatial reasoning capabilities of VLMs in real-world
driving contexts. We release the SURDS benchmark, evaluation toolkit, and GRPO
alignment code through: https://github.com/XiandaGuo/Drive-MLLM.

</details>


### [406] [Multi-Granularity Class Prototype Topology Distillation for Class-Incremental Source-Free Unsupervised Domain Adaptation](https://arxiv.org/pdf/2411.16064)
*Peihua Deng, Jiehua Zhang, Xichun Sheng, Chenggang Yan, Yaoqi Sun, Ying Fu, Liang Li*

Main category: cs.CV

TL;DR: The paper introduces GROTO, a method for Class-Incremental Source-Free Unsupervised Domain Adaptation (CI-SFUDA), addressing challenges like knowledge interference and new target knowledge shocks.


<details>
  <summary>Details</summary>
Motivation: To tackle the CI-SFUDA problem, where unlabeled target data arrives incrementally without labeled source data, and to mitigate interference and knowledge shocks.

Method: Proposes GROTO with multi-granularity class prototype self-organization and topology distillation modules to transfer source knowledge and stabilize learning.

Result: Achieves state-of-the-art performance on three public datasets.

Conclusion: GROTO effectively addresses CI-SFUDA challenges, demonstrating superior performance and robustness.

Abstract: This paper explores the Class-Incremental Source-Free Unsupervised Domain
Adaptation (CI-SFUDA) problem, where the unlabeled target data come
incrementally without access to labeled source instances. This problem poses
two challenges, the interference of similar source-class knowledge in
target-class representation learning and the shocks of new target knowledge to
old ones. To address them, we propose the Multi-Granularity Class Prototype
Topology Distillation (GROTO) algorithm, which effectively transfers the source
knowledge to the class-incremental target domain. Concretely, we design the
multi-granularity class prototype self-organization module and the prototype
topology distillation module. First, we mine the positive classes by modeling
accumulation distributions. Next, we introduce multi-granularity class
prototypes to generate reliable pseudo-labels, and exploit them to promote the
positive-class target feature self-organization. Second, the positive-class
prototypes are leveraged to construct the topological structures of source and
target feature spaces. Then, we perform the topology distillation to
continually mitigate the shocks of new target knowledge to old ones. Extensive
experiments demonstrate that our proposed method achieves state-of-the-art
performance on three public datasets. Code is available at
https://github.com/dengpeihua/GROTO.

</details>


### [407] [Bringing Objects to Life: training-free 4D generation from 3D objects through view consistent noise](https://arxiv.org/pdf/2412.20422)
*Ohad Rahamim, Ori Malca, Dvir Samuel, Gal Chechik*

Main category: cs.CV

TL;DR: A training-free method for animating 3D objects using text prompts to guide 4D generation, improving motion realism and preserving object identity.


<details>
  <summary>Details</summary>
Motivation: Existing methods lack growth or structural development capabilities and are limited to trained mesh datasets. This work aims to enable custom general scenes while maintaining object identity.

Method: Convert 3D mesh to static 4D NeRF, animate using Image-to-Video diffusion model with text prompts, and enhance motion realism with view-consistent noising and masked SDS loss.

Result: Outperforms baseline in temporal coherence, prompt adherence, and visual fidelity, especially in hard scenarios.

Conclusion: The proposed method effectively generates dynamic 4D content with improved realism and adherence to text prompts.

Abstract: Recent advancements in generative models have enabled the creation of dynamic
4D content - 3D objects in motion - based on text prompts, which holds
potential for applications in virtual worlds, media, and gaming. Existing
methods provide control over the appearance of generated content, including the
ability to animate 3D objects. However, their ability to generate dynamics is
limited to the mesh datasets they were trained on, lacking any growth or
structural development capability. In this work, we introduce a training-free
method for animating 3D objects by conditioning on textual prompts to guide 4D
generation, enabling custom general scenes while maintaining the original
object's identity. We first convert a 3D mesh into a static 4D Neural Radiance
Field (NeRF) that preserves the object's visual attributes. Then, we animate
the object using an Image-to-Video diffusion model driven by text. To improve
motion realism, we introduce a view-consistent noising protocol that aligns
object perspectives with the noising process to promote lifelike movement, and
a masked Score Distillation Sampling (SDS) loss that leverages attention maps
to focus optimization on relevant regions, better preserving the original
object. We evaluate our model on two different 3D object datasets for temporal
coherence, prompt adherence, and visual fidelity, and find that our method
outperforms the baseline based on multiview training, achieving better
consistency with the textual prompt in hard scenarios.

</details>


### [408] [Unforgettable Lessons from Forgettable Images: Intra-Class Memorability Matters in Computer Vision](https://arxiv.org/pdf/2412.20761)
*Jie Jing, Qing Lin, Shuangpeng Han, Lucia Schiatti, Yen-Ling Kuo, Mengmi Zhang*

Main category: cs.CV

TL;DR: The paper introduces intra-class memorability, proposing a novel metric (ICMscore) and dataset (ICMD) to study why some images in the same class are more memorable. AI models trained on ICMD reveal surprising impacts on tasks like image recognition and continual learning, and a diffusion model is fine-tuned for memorability-controlled editing.


<details>
  <summary>Details</summary>
Motivation: To understand why certain images within the same class are more memorable than others, despite shared category traits, and to explore the underlying visual features driving memorability.

Method: Human behavior experiments to quantify memorability using ICMscore, creation of the ICMD dataset, and training AI models for tasks like memorability prediction, image recognition, and memorability-controlled editing.

Result: High-ICMscore images hinder AI performance in recognition and continual learning, while low-ICMscore images improve it. A diffusion model successfully manipulates image elements to alter memorability.

Conclusion: The study advances understanding of intra-class memorability, identifies its impact on AI tasks, and enables practical applications in computer vision, with all resources to be publicly released.

Abstract: We introduce intra-class memorability, where certain images within the same
class are more memorable than others despite shared category characteristics.
To investigate what features make one object instance more memorable than
others, we design and conduct human behavior experiments, where participants
are shown a series of images, and they must identify when the current image
matches the image presented a few steps back in the sequence. To quantify
memorability, we propose the Intra-Class Memorability score (ICMscore), a novel
metric that incorporates the temporal intervals between repeated image
presentations into its calculation. Furthermore, we curate the Intra-Class
Memorability Dataset (ICMD), comprising over 5,000 images across ten object
classes with their ICMscores derived from 2,000 participants' responses.
Subsequently, we demonstrate the usefulness of ICMD by training AI models on
this dataset for various downstream tasks: memorability prediction, image
recognition, continual learning, and memorability-controlled image editing.
Surprisingly, high-ICMscore images impair AI performance in image recognition
and continual learning tasks, while low-ICMscore images improve outcomes in
these tasks. Additionally, we fine-tune a state-of-the-art image diffusion
model on ICMD image pairs with and without masked semantic objects. The
diffusion model can successfully manipulate image elements to enhance or reduce
memorability. Our contributions open new pathways in understanding intra-class
memorability by scrutinizing fine-grained visual features behind the most and
least memorable images and laying the groundwork for real-world applications in
computer vision. We will release all code, data, and models publicly.

</details>


### [409] [SC-Pro: Training-Free Framework for Defending Unsafe Image Synthesis Attack](https://arxiv.org/pdf/2501.05359)
*Junha Park, Jaehui Hwang, Ian Ryu, Hyungkeun Park, Jiyoon Kim, Jong-Seok Lee*

Main category: cs.CV

TL;DR: The paper proposes SC-Pro, a training-free framework to defend against adversarial attacks on Stable Diffusion's safety checkers, and SC-Pro-o for efficient NSFW detection.


<details>
  <summary>Details</summary>
Motivation: Address vulnerabilities in Stable Diffusion's safety checkers against adversarial attacks generating NSFW content.

Method: Introduces SC-Pro (Spherical or Circular Probing) and SC-Pro-o, leveraging small changes in text prompts or input latents to defend against attacks.

Result: Demonstrates superior performance and applicability in defending against adversarial NSFW image generation.

Conclusion: SC-Pro and SC-Pro-o effectively mitigate adversarial attacks on safety checkers, enhancing robustness without additional training.

Abstract: With advances in diffusion models, image generation has shown significant
performance improvements. This raises concerns about the potential abuse of
image generation, such as the creation of explicit or violent images, commonly
referred to as Not Safe For Work (NSFW) content. To address this, the Stable
Diffusion model includes several safety checkers to censor initial text prompts
and final output images generated from the model. However, recent research has
shown that these safety checkers have vulnerabilities against adversarial
attacks, allowing them to generate NSFW images. In this paper, we find that
these adversarial attacks are not robust to small changes in text prompts or
input latents. Based on this, we propose SC-Pro (Spherical or Circular
Probing), a training-free framework that easily defends against adversarial
attacks generating NSFW images. Moreover, we develop an approach that utilizes
one-step diffusion models for efficient NSFW detection (SC-Pro-o), further
reducing computational resources. We demonstrate the superiority of our method
in terms of performance and applicability.

</details>


### [410] [PUSSM: Point Cloud Upsampling as Implicit Statistical Shape Model](https://arxiv.org/pdf/2501.16716)
*Tongxu Zhang, Bei Wang*

Main category: cs.CV

TL;DR: A framework for high-fidelity pelvic reconstruction using medical image segmentation and point cloud upsampling, improving surface quality and anatomical accuracy.


<details>
  <summary>Details</summary>
Motivation: To enhance pelvic structure reconstruction without relying on landmarks or PCA, leveraging shape priors from MedShapePelvic.

Method: Integrates medical image segmentation with point cloud upsampling to create an implicit statistical shape model.

Result: Significant improvements in surface quality and anatomical accuracy, validated on Pelvic1k.

Conclusion: The method is generalizable and applicable to other skeletal regions.

Abstract: This paper proposes a framework for high-fidelity reconstruction of pelvic
structures by integrating medical image segmentation and point cloud
upsampling. By point cloud upsampling to learn shape priors from MedShapePelvic
without requiring landmarks or PCA, our method functions as an implicit
statistical shape model. Evaluations on Pelvic1k show significant improvements
in surface quality and anatomical accuracy. This approach is generalizable and
applicable to other skeletal regions.

</details>


### [411] [Corrupted but Not Broken: Understanding and Mitigating the Negative Impacts of Corrupted Data in Visual Instruction Tuning](https://arxiv.org/pdf/2502.12635)
*Yunhao Gou, Hansi Yang, Zhili Liu, Kai Chen, Yihan Zeng, Lanqing Hong, Zhenguo Li, Qun Liu, Bo Han, James T. Kwok, Yu Zhang*

Main category: cs.CV

TL;DR: The paper explores how corrupted data affects Multimodal Large Language Models (MLLMs) and shows that their performance can be restored by disabling a small subset of parameters. It also introduces a new training paradigm to handle corrupted data.


<details>
  <summary>Details</summary>
Motivation: Corrupted datasets (e.g., hallucinated content, incorrect responses, poor OCR) degrade MLLM performance, but existing solutions (high-quality data collection or rule-based filtering) are costly or limited.

Method: Systematically investigates corrupted data's impact on MLLMs, identifies reversibility of adverse effects, and leverages MLLMs' ability to differentiate clean/corrupted samples for dataset cleaning.

Result: Corrupted MLLMs can be restored by disabling a small parameter subset, and they inherently distinguish clean/corrupted data. The proposed training paradigm outperforms existing strategies.

Conclusion: Corrupted MLLMs are not irreparably damaged, and a corruption-robust training paradigm effectively mitigates data corruption issues.

Abstract: Visual Instruction Tuning (VIT) aims to enhance Multimodal Large Language
Models (MLLMs), yet its effectiveness is often compromised by corrupted
datasets with issues such as hallucinated content, incorrect responses, and
poor OCR quality. Previous approaches to address these challenges have focused
on refining datasets through high-quality data collection or rule-based
filtering that can be costly or limited in scope. In this paper, we conduct a
systematic investigation into the impact of corrupted data on MLLMs and
discover that, although corrupted data degrade model performance, such adverse
effects are largely reversible, and MLLMs are {\bf corrupted but not broken}.
Specifically, we find that disabling a small subset of parameters can almost
fully restore performance. Moreover, corrupted MLLMs inherently possess the
capability to differentiate between clean and corrupted samples, facilitating
dataset cleaning without external intervention. Building on these insights, we
introduce a corruption-robust training paradigm that significantly surpasses
existing strategies for mitigating the effects of corrupted data.

</details>


### [412] [DADM: Dual Alignment of Domain and Modality for Face Anti-spoofing](https://arxiv.org/pdf/2503.00429)
*Jingyi Yang, Xun Lin, Zitong Yu, Liepiao Zhang, Xin Liu, Hui Li, Xiaochen Yuan, Xiaochun Cao*

Main category: cs.CV

TL;DR: The paper introduces DADM, a method to address misalignment in multi-modal face anti-spoofing (FAS) by aligning modalities and domains, achieving state-of-the-art performance.


<details>
  <summary>Details</summary>
Motivation: Multi-modal FAS leverages diverse sensors but faces misalignment risks: intra-domain (varying modality importance) and inter-domain (domain shifts). Simple fusion fails, necessitating adaptive alignment.

Method: Proposes DADM: (1) mutual-information-based alignment for intra-domain misalignment, (2) dual alignment (sub-domain hyperplanes and modality angle margins) for inter-domain misalignment.

Result: DADM achieves state-of-the-art performance across four challenging protocols, demonstrating robustness in multi-modal domain generalization.

Conclusion: DADM effectively addresses misalignment in multi-modal FAS, offering a robust solution for real-world applications.

Abstract: With the availability of diverse sensor modalities (i.e., RGB, Depth,
Infrared) and the success of multi-modal learning, multi-modal face
anti-spoofing (FAS) has emerged as a prominent research focus. The intuition
behind it is that leveraging multiple modalities can uncover more intrinsic
spoofing traces. However, this approach presents more risk of misalignment. We
identify two main types of misalignment: (1) \textbf{Intra-domain modality
misalignment}, where the importance of each modality varies across different
attacks. For instance, certain modalities (e.g., Depth) may be non-defensive
against specific attacks (e.g., 3D mask), indicating that each modality has
unique strengths and weaknesses in countering particular attacks. Consequently,
simple fusion strategies may fall short. (2) \textbf{Inter-domain modality
misalignment}, where the introduction of additional modalities exacerbates
domain shifts, potentially overshadowing the benefits of complementary fusion.
To tackle (1), we propose a alignment module between modalities based on mutual
information, which adaptively enhances favorable modalities while suppressing
unfavorable ones. To address (2), we employ a dual alignment optimization
method that aligns both sub-domain hyperplanes and modality angle margins,
thereby mitigating domain gaps. Our method, dubbed \textbf{D}ual
\textbf{A}lignment of \textbf{D}omain and \textbf{M}odality (DADM), achieves
state-of-the-art performance in extensive experiments across four challenging
protocols demonstrating its robustness in multi-modal domain generalization
scenarios. The codes will be released soon.

</details>


### [413] [Deepfake-Eval-2024: A Multi-Modal In-the-Wild Benchmark of Deepfakes Circulated in 2024](https://arxiv.org/pdf/2503.02857)
*Nuria Alina Chandra, Ryan Murtfeldt, Lin Qiu, Arnab Karmakar, Hannah Lee, Emmanuel Tanumihardja, Kevin Farhat, Ben Caffee, Sejin Paik, Changyeon Lee, Jongwook Choi, Aerin Kim, Oren Etzioni*

Main category: cs.CV

TL;DR: Deepfake-Eval-2024 is a new benchmark for deepfake detection, revealing that current models perform poorly on real-world data compared to academic benchmarks.


<details>
  <summary>Details</summary>
Motivation: Existing deepfake detection benchmarks are outdated and unrepresentative of real-world scenarios, necessitating a more current and diverse dataset.

Method: The authors introduce Deepfake-Eval-2024, a benchmark with diverse, real-world deepfakes from social media and detection platforms.

Result: State-of-the-art models show a 45-50% drop in AUC on the new benchmark, though commercial and finetuned models perform better.

Conclusion: Current deepfake detection models are inadequate for real-world use, and further improvements are needed to match forensic analyst accuracy.

Abstract: In the age of increasingly realistic generative AI, robust deepfake detection
is essential for mitigating fraud and disinformation. While many deepfake
detectors report high accuracy on academic datasets, we show that these
academic benchmarks are out of date and not representative of real-world
deepfakes. We introduce Deepfake-Eval-2024, a new deepfake detection benchmark
consisting of in-the-wild deepfakes collected from social media and deepfake
detection platform users in 2024. Deepfake-Eval-2024 consists of 45 hours of
videos, 56.5 hours of audio, and 1,975 images, encompassing the latest
manipulation technologies. The benchmark contains diverse media content from 88
different websites in 52 different languages. We find that the performance of
open-source state-of-the-art deepfake detection models drops precipitously when
evaluated on Deepfake-Eval-2024, with AUC decreasing by 50% for video, 48% for
audio, and 45% for image models compared to previous benchmarks. We also
evaluate commercial deepfake detection models and models finetuned on
Deepfake-Eval-2024, and find that they have superior performance to
off-the-shelf open-source models, but do not yet reach the accuracy of deepfake
forensic analysts. The dataset is available at
https://github.com/nuriachandra/Deepfake-Eval-2024.

</details>


### [414] [How Do Multimodal Large Language Models Handle Complex Multimodal Reasoning? Placing Them in An Extensible Escape Game](https://arxiv.org/pdf/2503.10042)
*Ziyue Wang, Yurui Dong, Fuwen Luo, Minyuan Ruan, Zhili Cheng, Chi Chen, Peng Li, Yang Liu*

Main category: cs.CV

TL;DR: MM-Escape is a benchmark for evaluating multimodal reasoning in MLLMs, focusing on intermediate behaviors and task completion, revealing performance bottlenecks and failure modes.


<details>
  <summary>Details</summary>
Motivation: Existing evaluations for MLLMs focus on final task completion, neglecting comprehensive analysis of reasoning processes. MM-Escape addresses this gap by emphasizing intermediate behaviors.

Method: The authors introduce MM-Escape, a benchmark inspired by escape games, and develop EscapeCraft, a customizable environment for assessing multimodal reasoning.

Result: MLLMs succeed in simple tasks but struggle as difficulty increases, showing varied failure modes like poor spatial awareness and ineffective prop use.

Conclusion: MM-Escape highlights challenges in multimodal reasoning and identifies areas for improving MLLMs.

Abstract: The rapid advancing of Multimodal Large Language Models (MLLMs) has spurred
interest in complex multimodal reasoning tasks in the real-world and virtual
environment, which require coordinating multiple abilities, including visual
perception, visual reasoning, spatial awareness, and target deduction. However,
existing evaluations primarily assess the final task completion, often
degrading assessments to isolated abilities such as visual grounding and visual
question answering. Less attention is given to comprehensively and
quantitatively analyzing reasoning process in multimodal environments, which is
crucial for understanding model behaviors and underlying reasoning mechanisms
beyond merely task success. To address this, we introduce MM-Escape, an
extensible benchmark for investigating multimodal reasoning, inspired by
real-world escape games. MM-Escape emphasizes intermediate model behaviors
alongside final task completion. To achieve this, we develop EscapeCraft, a
customizable and open environment that enables models to engage in free-form
exploration for assessing multimodal reasoning. Extensive experiments show that
MLLMs, regardless of scale, can successfully complete the simplest room escape
tasks, with some exhibiting human-like exploration strategies. Yet, performance
dramatically drops as task difficulty increases. Moreover, we observe that
performance bottlenecks vary across models, revealing distinct failure modes
and limitations in their multimodal reasoning abilities, such as repetitive
trajectories without adaptive exploration, getting stuck in corners due to poor
visual spatial awareness, and ineffective use of acquired props, such as the
key. We hope our work sheds light on new challenges in multimodal reasoning,
and uncovers potential improvements in MLLMs capabilities.

</details>


### [415] [Cognitive Disentanglement for Referring Multi-Object Tracking](https://arxiv.org/pdf/2503.11496)
*Shaofeng Liang, Runwei Guan, Wangwang Lian, Daizong Liu, Xiaolou Sun, Dongming Wu, Yutao Yue, Weiping Ding, Hui Xiong*

Main category: cs.CV

TL;DR: The paper proposes a Cognitive Disentanglement for Referring Multi-Object Tracking (CDRMT) framework to improve object tracking by better integrating language and visual features, achieving significant performance gains.


<details>
  <summary>Details</summary>
Motivation: Existing RMOT methods fail to effectively integrate rich semantic language information with visual features, especially in complex scenes requiring static and dynamic understanding.

Method: CDRMT adapts human visual processing pathways, disentangles language descriptions, and hierarchically injects them into object queries for refined understanding.

Result: CDRMT outperforms state-of-the-art methods with 6.0% and 3.2% HOTA score improvements on Refer-KITTI and Refer-KITTI-V2 datasets, respectively.

Conclusion: The CDRMT framework advances RMOT by enhancing multi-source information fusion and offers new insights for future research.

Abstract: As a significant application of multi-source information fusion in
intelligent transportation perception systems, Referring Multi-Object Tracking
(RMOT) involves localizing and tracking specific objects in video sequences
based on language references. However, existing RMOT approaches often treat
language descriptions as holistic embeddings and struggle to effectively
integrate the rich semantic information contained in language expressions with
visual features. This limitation is especially apparent in complex scenes
requiring comprehensive understanding of both static object attributes and
spatial motion information. In this paper, we propose a Cognitive
Disentanglement for Referring Multi-Object Tracking (CDRMT) framework that
addresses these challenges. It adapts the "what" and "where" pathways from the
human visual processing system to RMOT tasks. Specifically, our framework first
establishes cross-modal connections while preserving modality-specific
characteristics. It then disentangles language descriptions and hierarchically
injects them into object queries, refining object understanding from coarse to
fine-grained semantic levels. Finally, we reconstruct language representations
based on visual features, ensuring that tracked objects faithfully reflect the
referring expression. Extensive experiments on different benchmark datasets
demonstrate that CDRMT achieves substantial improvements over state-of-the-art
methods, with average gains of 6.0% in HOTA score on Refer-KITTI and 3.2% on
Refer-KITTI-V2. Our approach advances the state-of-the-art in RMOT while
simultaneously providing new insights into multi-source information fusion.

</details>


### [416] [ClearSight: Visual Signal Enhancement for Object Hallucination Mitigation in Multimodal Large language Models](https://arxiv.org/pdf/2503.13107)
*Hao Yin, Guangzong Si, Zilei Wang*

Main category: cs.CV

TL;DR: VAF is a plug-and-play technique to enhance visual attention in MLLMs, reducing hallucinations without slowing inference.


<details>
  <summary>Details</summary>
Motivation: Contrastive decoding in MLLMs reduces hallucinations but can compromise coherence and slow inference. VAF aims to address these limitations.

Method: VAF amplifies visual signals in middle layers of MLLMs, improving modality fusion and reducing language bias.

Result: VAF reduces hallucinations in MLLMs while maintaining coherence, accuracy, and inference speed.

Conclusion: VAF effectively balances visual grounding and language coherence in MLLMs, offering a practical solution for hallucination mitigation.

Abstract: Contrastive decoding strategies are widely used to mitigate object
hallucinations in multimodal large language models (MLLMs). By reducing
over-reliance on language priors, these strategies ensure that generated
content remains closely grounded in visual inputs, producing contextually
accurate outputs. Since contrastive decoding requires no additional training or
external tools, it offers both computational efficiency and versatility, making
it highly attractive. However, these methods present two main limitations: (1)
bluntly suppressing language priors can compromise coherence and accuracy of
generated content, and (2) processing contrastive inputs adds computational
load, significantly slowing inference speed. To address these challenges, we
propose Visual Amplification Fusion (VAF), a plug-and-play technique that
enhances attention to visual signals within the model's middle layers, where
modality fusion predominantly occurs. This approach enables more effective
capture of visual features, reducing the model's bias toward language modality.
Experimental results demonstrate that VAF significantly reduces hallucinations
across various MLLMs without affecting inference speed, while maintaining
coherence and accuracy in generated outputs.

</details>


### [417] [One-Step Residual Shifting Diffusion for Image Super-Resolution via Distillation](https://arxiv.org/pdf/2503.13358)
*Daniil Selikhanovych, David Li, Aleksei Leonov, Nikita Gushchin, Sergei Kushneriuk, Alexander Filippov, Evgeny Burnaev, Iaroslav Koshelev, Alexander Korotin*

Main category: cs.CV

TL;DR: RSD is a new distillation method for ResShift, a diffusion-based super-resolution (SR) model, addressing computational cost and perceptual detail issues while outperforming existing methods.


<details>
  <summary>Details</summary>
Motivation: Diffusion-based SR models are computationally expensive, and existing acceleration methods either lack realistic details or hallucinate structures. RSD aims to solve these issues.

Method: RSD trains a student network to produce images that align with a fake ResShift model, enabling single-step restoration and outperforming the teacher model.

Result: RSD surpasses other distillation methods like SinSR, matches state-of-the-art, and offers competitive perceptual quality with fewer resources.

Conclusion: RSD is an efficient and effective distillation method for diffusion-based SR, balancing quality and computational cost.

Abstract: Diffusion models for super-resolution (SR) produce high-quality visual
results but require expensive computational costs. Despite the development of
several methods to accelerate diffusion-based SR models, some (e.g., SinSR)
fail to produce realistic perceptual details, while others (e.g., OSEDiff) may
hallucinate non-existent structures. To overcome these issues, we present RSD,
a new distillation method for ResShift, one of the top diffusion-based SR
models. Our method is based on training the student network to produce such
images that a new fake ResShift model trained on them will coincide with the
teacher model. RSD achieves single-step restoration and outperforms the teacher
by a large margin. We show that our distillation method can surpass the other
distillation-based method for ResShift - SinSR - making it on par with
state-of-the-art diffusion-based SR distillation methods. Compared to SR
methods based on pre-trained text-to-image models, RSD produces competitive
perceptual quality, provides images with better alignment to degraded input
images, and requires fewer parameters and GPU memory. We provide experimental
results on various real-world and synthetic datasets, including RealSR,
RealSet65, DRealSR, ImageNet, and DIV2K.

</details>


### [418] [Panoramic Distortion-Aware Tokenization for Person Detection and Localization Using Transformers in Overhead Fisheye Images](https://arxiv.org/pdf/2503.14228)
*Nobuhiko Wakai, Satoshi Sato, Yasunori Ishii, Takayoshi Yamashita*

Main category: cs.CV

TL;DR: A method for accurate person detection in fisheye images by converting them to panoramic images and using distortion-aware tokenization to address rotation and small-size challenges.


<details>
  <summary>Details</summary>
Motivation: Accurate person detection in overhead fisheye images is challenging due to rotation and small-sized persons, which conventional methods struggle with.

Method: Convert fisheye to panoramic images, use geometry-aware tokenization to balance significant areas, and introduce distortion-aware tokenization for optimal divisions.

Result: Outperforms conventional methods in large-scale datasets by effectively detecting and localizing persons.

Conclusion: The proposed method successfully addresses challenges in fisheye-based person detection, offering improved accuracy.

Abstract: Person detection methods are used widely in applications including visual
surveillance, pedestrian detection, and robotics. However, accurate detection
of persons from overhead fisheye images remains an open challenge because of
factors including person rotation and small-sized persons. To address the
person rotation problem, we convert the fisheye images into panoramic images.
For smaller people, we focused on the geometry of the panoramas. Conventional
detection methods tend to focus on larger people because these larger people
yield large significant areas for feature maps. In equirectangular panoramic
images, we find that a person's height decreases linearly near the top of the
images. Using this finding, we leverage the significance values and aggregate
tokens that are sorted based on these values to balance the significant areas.
In this leveraging process, we introduce panoramic distortion-aware
tokenization. This tokenization procedure divides a panoramic image using
self-similarity figures that enable determination of optimal divisions without
gaps, and we leverage the maximum significant values in each tile of token
groups to preserve the significant areas of smaller people. To achieve higher
detection accuracy, we propose a person detection and localization method that
combines panoramic-image remapping and the tokenization procedure. Extensive
experiments demonstrated that our method outperforms conventional methods when
applied to large-scale datasets.

</details>


### [419] [Exploring Disentangled and Controllable Human Image Synthesis: From End-to-End to Stage-by-Stage](https://arxiv.org/pdf/2503.19486)
*Zhengwentai Sun, Chenghong Li, Hongjie Liao, Xihe Yang, Keru Zheng, Heyuan Li, Yihao Zhi, Shuliang Ning, Shuguang Cui, Xiaoguang Han*

Main category: cs.CV

TL;DR: The paper introduces a stage-by-stage framework for disentangled and controllable human image synthesis, improving performance over end-to-end models by leveraging MVHumanNet and VTON datasets separately.


<details>
  <summary>Details</summary>
Motivation: Existing methods lack fine-grained control over factors like viewpoint, pose, clothing, and identity in human image synthesis. The domain gap between MVHumanNet and in-the-wild data further complicates disentanglement.

Method: Proposes a three-step pipeline: clothed A-pose generation, back-view synthesis, and pose/view control, leveraging MVHumanNet and VTON datasets separately to avoid inconsistency.

Result: The stage-by-stage approach outperforms end-to-end models in visual fidelity and disentanglement quality, especially for in-the-wild scenarios.

Conclusion: The structured pipeline offers a scalable solution for real-world tasks, improving controllability and generalization in human image synthesis.

Abstract: Achieving fine-grained controllability in human image synthesis is a
long-standing challenge in computer vision. Existing methods primarily focus on
either facial synthesis or near-frontal body generation, with limited ability
to simultaneously control key factors such as viewpoint, pose, clothing, and
identity in a disentangled manner. In this paper, we introduce a new
disentangled and controllable human synthesis task, which explicitly separates
and manipulates these four factors within a unified framework. We first develop
an end-to-end generative model trained on MVHumanNet for factor
disentanglement. However, the domain gap between MVHumanNet and in-the-wild
data produces unsatisfactory results, motivating the exploration of virtual
try-on (VTON) dataset as a potential solution. Through experiments, we observe
that simply incorporating the VTON dataset as additional data to train the
end-to-end model degrades performance, primarily due to the inconsistency in
data forms between the two datasets, which disrupts the disentanglement
process. To better leverage both datasets, we propose a stage-by-stage
framework that decomposes human image generation into three sequential steps:
clothed A-pose generation, back-view synthesis, and pose and view control. This
structured pipeline enables better dataset utilization at different stages,
significantly improving controllability and generalization, especially for
in-the-wild scenarios. Extensive experiments demonstrate that our
stage-by-stage approach outperforms end-to-end models in both visual fidelity
and disentanglement quality, offering a scalable solution for real-world tasks.
Additional demos are available on the project page:
https://taited.github.io/discohuman-project/.

</details>


### [420] [From Flatland to Space: Teaching Vision-Language Models to Perceive and Reason in 3D](https://arxiv.org/pdf/2503.22976)
*Jiahui Zhang, Yurui Chen, Yanpeng Zhou, Yueming Xu, Ze Huang, Jilin Mei, Junhui Chen, Yu-Jie Yuan, Xinyue Cai, Guowei Huang, Xingyue Quan, Hang Xu, Li Zhang*

Main category: cs.CV

TL;DR: The paper introduces a 2D spatial data generation pipeline and SPAR-7M dataset to enhance LVLMs' spatial reasoning, achieving state-of-the-art performance on 2D benchmarks and competitive results on 3D tasks.


<details>
  <summary>Details</summary>
Motivation: LVLMs struggle with spatial perception, limiting their ability to reason about complex 3D scenes. The goal is to improve spatial understanding without relying on 3D representations.

Method: A novel 2D spatial data generation and annotation pipeline is introduced, leveraging 3D ground-truth scene data. SPAR-7M dataset and SPAR-Bench benchmark are created for diverse spatial tasks.

Result: Models trained on SPAR-7M achieve state-of-the-art performance on 2D spatial benchmarks and competitive results on 3D tasks after fine-tuning.

Conclusion: The proposed pipeline and dataset effectively enhance spatial reasoning in LVLMs, demonstrating their potential for complex spatial tasks.

Abstract: Recent advances in LVLMs have improved vision-language understanding, but
they still struggle with spatial perception, limiting their ability to reason
about complex 3D scenes. Unlike previous approaches that incorporate 3D
representations into models to improve spatial understanding, we aim to unlock
the potential of VLMs by leveraging spatially relevant image data. To this end,
we introduce a novel 2D spatial data generation and annotation pipeline built
upon scene data with 3D ground-truth. This pipeline enables the creation of a
diverse set of spatial tasks, ranging from basic perception tasks to more
complex reasoning tasks. Leveraging this pipeline, we construct SPAR-7M, a
large-scale dataset generated from thousands of scenes across multiple public
datasets. In addition, we introduce SPAR-Bench, a benchmark designed to offer a
more comprehensive evaluation of spatial capabilities compared to existing
spatial benchmarks, supporting both single-view and multi-view inputs. Training
on both SPAR-7M and large-scale 2D datasets enables our models to achieve
state-of-the-art performance on 2D spatial benchmarks. Further fine-tuning on
3D task-specific datasets yields competitive results, underscoring the
effectiveness of our dataset in enhancing spatial reasoning.

</details>


### [421] [H2VU-Benchmark: A Comprehensive Benchmark for Hierarchical Holistic Video Understanding](https://arxiv.org/pdf/2503.24008)
*Qi Wu, Quanlong Zheng, Yanhao Zhang, Junlin Xie, Jinguo Luo, Kuo Wang, Peng Liu, Qingsong Xie, Ru Zhen, Zhenyu Yang, Haonan Lu*

Main category: cs.CV

TL;DR: The paper introduces H2VU, a hierarchical and holistic video understanding benchmark, addressing gaps in current benchmarks by extending video duration, diversifying tasks, and enriching data. It reveals MLLMs' limitations and aims to advance video understanding research.


<details>
  <summary>Details</summary>
Motivation: Existing video understanding benchmarks lack coverage, task diversity, and scene adaptability, limiting accurate assessment of models' capabilities.

Method: Proposes H2VU benchmark with extended video duration, comprehensive tasks (e.g., countercommonsense comprehension, trajectory state tracking), and enriched first-person streaming video datasets.

Result: H2VU shows significant room for improvement in MLLMs' performance on new evaluation tasks.

Conclusion: H2VU is expected to drive progress in video understanding research by providing a thorough evaluation framework for MLLMs.

Abstract: With the rapid development of multimodal models, the demand for assessing
video understanding capabilities has been steadily increasing. However,
existing benchmarks for evaluating video understanding exhibit significant
limitations in coverage, task diversity, and scene adaptability. These
shortcomings hinder the accurate assessment of models' comprehensive video
understanding capabilities. To tackle this challenge, we propose a hierarchical
and holistic video understanding (H2VU) benchmark designed to evaluate both
general video and online streaming video comprehension. This benchmark
contributes three key features:
  Extended video duration: Spanning videos from brief 3-second clips to
comprehensive 1.5-hour recordings, thereby bridging the temporal gaps found in
current benchmarks. Comprehensive assessment tasks: Beyond traditional
perceptual and reasoning tasks, we have introduced modules for
countercommonsense comprehension and trajectory state tracking. These additions
test the models' deep understanding capabilities beyond mere prior knowledge.
Enriched video data: To keep pace with the rapid evolution of current AI
agents, we have expanded first-person streaming video datasets. This expansion
allows for the exploration of multimodal models' performance in understanding
streaming videos from a first-person perspective. Extensive results from H2VU
reveal that existing multimodal large language models (MLLMs) possess
substantial potential for improvement in our newly proposed evaluation tasks.
We expect that H2VU will facilitate advancements in video understanding
research by offering a comprehensive and in-depth analysis of MLLMs.

</details>


### [422] [SPF-Portrait: Towards Pure Text-to-Portrait Customization with Semantic Pollution-Free Fine-Tuning](https://arxiv.org/pdf/2504.00396)
*Xiaole Xian, Zhichao Liao, Qingyu Li, Wenyu Qin, Pengfei Wan, Weicheng Xie, Long Zeng, Linlin Shen, Pingfa Feng*

Main category: cs.CV

TL;DR: SPF-Portrait introduces a dual-path contrastive learning pipeline to customize text-to-portrait models without disrupting the original model's behavior, using semantic-aware fine control and response enhancement.


<details>
  <summary>Details</summary>
Motivation: Existing methods for text-to-portrait customization disrupt the original model's behavior (e.g., ID, layout). SPF-Portrait aims to minimize this disruption while effectively customizing attributes.

Method: Uses a dual-path contrastive learning pipeline with a Semantic-Aware Fine Control Map to guide alignment and a response enhancement mechanism to reinforce target semantics.

Result: Achieves state-of-the-art performance in text-to-portrait customization, balancing behavioral alignment and semantic responsiveness.

Conclusion: SPF-Portrait successfully customizes portraits while preserving the original model's behavior, validated by extensive experiments.

Abstract: Fine-tuning a pre-trained Text-to-Image (T2I) model on a tailored portrait
dataset is the mainstream method for text-to-portrait customization. However,
existing methods often severely impact the original model's behavior (e.g.,
changes in ID, layout, etc.) while customizing portrait attributes. To address
this issue, we propose SPF-Portrait, a pioneering work to purely understand
customized target semantics and minimize disruption to the original model. In
our SPF-Portrait, we design a dual-path contrastive learning pipeline, which
introduces the original model as a behavioral alignment reference for the
conventional fine-tuning path. During the contrastive learning, we propose a
novel Semantic-Aware Fine Control Map that indicates the intensity of response
regions of the target semantics, to spatially guide the alignment process
between the contrastive paths. It adaptively balances the behavioral alignment
across different regions and the responsiveness of the target semantics.
Furthermore, we propose a novel response enhancement mechanism to reinforce the
presentation of target semantics, while mitigating representation discrepancy
inherent in direct cross-modal supervision. Through the above strategies, we
achieve incremental learning of customized target semantics for pure
text-to-portrait customization. Extensive experiments show that SPF-Portrait
achieves state-of-the-art performance. Project page:
https://spf-portrait.github.io/SPF-Portrait/

</details>


### [423] [Envisioning Beyond the Pixels: Benchmarking Reasoning-Informed Visual Editing](https://arxiv.org/pdf/2504.02826)
*Xiangyu Zhao, Peiyuan Zhang, Kexian Tang, Xiaorong Zhu, Hao Li, Wenhao Chai, Zicheng Zhang, Renqiu Xia, Guangtao Zhai, Junchi Yan, Hua Yang, Xue Yang, Haodong Duan*

Main category: cs.CV

TL;DR: RISEBench is a new benchmark for evaluating reasoning-informed visual editing (RISE) in LMMs, highlighting challenges in complex instructions, appearance consistency, and flexible inputs. Current models, including GPT-4o-Image, perform poorly (28.8% accuracy).


<details>
  <summary>Details</summary>
Motivation: Address the gap in LMMs' ability to perform general visual editing, especially in reasoning tasks like temporal, causal, spatial, and logical reasoning.

Method: Introduce RISEBench with curated test cases for four reasoning categories and a robust evaluation framework using human judges and LMM-as-a-judge.

Result: Current models struggle, with GPT-4o-Image achieving only 28.8% accuracy. RISEBench reveals limitations and provides insights for future work.

Conclusion: RISEBench is a valuable tool for advancing reasoning-aware visual editing, exposing model weaknesses and guiding future research.

Abstract: Large Multi-modality Models (LMMs) have made significant progress in visual
understanding and generation, but they still face challenges in General Visual
Editing, particularly in following complex instructions, preserving appearance
consistency, and supporting flexible input formats. To study this gap, we
introduce RISEBench, the first benchmark for evaluating Reasoning-Informed
viSual Editing (RISE). RISEBench focuses on four key reasoning categories:
Temporal, Causal, Spatial, and Logical Reasoning. We curate high-quality test
cases for each category and propose an robust evaluation framework that
assesses Instruction Reasoning, Appearance Consistency, and Visual Plausibility
with both human judges and the LMM-as-a-judge approach. We conducted
experiments evaluating nine prominent visual editing models, comprising both
open-source and proprietary models. The evaluation results demonstrate that
current models face significant challenges in reasoning-based editing tasks.
Even the most powerful model evaluated, GPT-4o-Image, achieves an accuracy of
merely 28.8%. RISEBench effectively highlights the limitations of contemporary
editing models, provides valuable insights, and indicates potential future
directions for the field of reasoning-aware visual editing. Our code and data
have been released at https://github.com/PhoenixZ810/RISEBench.

</details>


### [424] [NoisyRollout: Reinforcing Visual Reasoning with Data Augmentation](https://arxiv.org/pdf/2504.13055)
*Xiangyan Liu, Jinjie Ni, Zijian Wu, Chao Du, Longxu Dou, Haonan Wang, Tianyu Pang, Michael Qizhe Shieh*

Main category: cs.CV

TL;DR: NoisyRollout is a data augmentation method for RL-tuned VLMs, improving policy exploration and robustness by mixing clean and distorted image trajectories during training.


<details>
  <summary>Details</summary>
Motivation: Enhancing policy exploration and addressing imperfect visual perception in VLMs to improve reasoning capabilities.

Method: Proposes NoisyRollout, mixing clean and distorted image trajectories during RL training with a noise annealing schedule.

Result: Achieves state-of-the-art performance on 5 benchmarks, validated across model sizes and data scales.

Conclusion: NoisyRollout is effective, scalable, and easy to adopt, enhancing RL-tuned VLMs' robustness and reasoning.

Abstract: Recent advances in reinforcement learning (RL) have strengthened the
reasoning capabilities of vision-language models (VLMs). However, enhancing
policy exploration to better scale test-time compute remains largely
underexplored. In addition, VLMs continue to struggle with imperfect visual
perception, which in turn affects the subsequent reasoning process. To this
end, we propose NoisyRollout, a simple yet effective data augmentation method
that mixes trajectories from both clean and moderately distorted images during
RL training. By injecting targeted diversity in visual perception and the
resulting reasoning patterns, NoisyRollout promotes better policy exploration
through vision-oriented inductive biases, ultimately leading to more robust
reasoning behaviors. We further adopt a noise annealing schedule that gradually
reduces distortion strength over training, leveraging noisy signals early on
while ensuring training stability in later stages. Crucially, our method is
easy-to-adopt--requiring no additional training cost and no modifications to
the RL objective. Extensive experiments on $2$ distinct training datasets
demonstrate that NoisyRollout achieves state-of-the-art performance among
open-source RL-tuned models across $5$ out-of-domain reasoning and perception
benchmarks. Furthermore, we validate the effectiveness of NoisyRollout across
model sizes ($7$B and $32$B) and data scales (from $1$K to $6$K), highlighting
its generalizability and scalability.

</details>


### [425] [TAPIP3D: Tracking Any Point in Persistent 3D Geometry](https://arxiv.org/pdf/2504.14717)
*Bowei Zhang, Lei Ke, Adam W. Harley, Katerina Fragkiadaki*

Main category: cs.CV

TL;DR: TAPIP3D introduces a 3D point tracking method using spatio-temporal feature clouds and a 3D N2N attention mechanism, outperforming existing 2D and 3D trackers.


<details>
  <summary>Details</summary>
Motivation: To improve long-term 3D point tracking in monocular videos by leveraging depth and camera motion for robust trajectory estimation.

Method: Represents videos as camera-stabilized 3D feature clouds, uses iterative motion refinement, and employs a 3D N2N attention mechanism for contextualization.

Result: Surpasses state-of-the-art 2D and 3D trackers in accuracy when depth is reliable, with improved robustness due to camera motion compensation.

Conclusion: TAPIP3D's 3D-centric approach and attention mechanism deliver superior tracking performance, especially in stabilized coordinates.

Abstract: We introduce TAPIP3D, a novel approach for long-term 3D point tracking in
monocular RGB and RGB-D videos. TAPIP3D represents videos as camera-stabilized
spatio-temporal feature clouds, leveraging depth and camera motion information
to lift 2D video features into a 3D world space where camera movement is
effectively canceled out. Within this stabilized 3D representation, TAPIP3D
iteratively refines multi-frame motion estimates, enabling robust point
tracking over long time horizons. To handle the irregular structure of 3D point
distributions, we propose a 3D Neighborhood-to-Neighborhood (N2N) attention
mechanism - a 3D-aware contextualization strategy that builds informative,
spatially coherent feature neighborhoods to support precise trajectory
estimation. Our 3D-centric formulation significantly improves performance over
existing 3D point tracking methods and even surpasses state-of-the-art 2D pixel
trackers in accuracy when reliable depth is available. The model supports
inference in both camera-centric (unstabilized) and world-centric (stabilized)
coordinates, with experiments showing that compensating for camera motion leads
to substantial gains in tracking robustness. By replacing the conventional 2D
square correlation windows used in prior 2D and 3D trackers with a spatially
grounded 3D attention mechanism, TAPIP3D achieves strong and consistent results
across multiple 3D point tracking benchmarks. Project Page:
https://tapip3d.github.io

</details>


### [426] [PointLoRA: Low-Rank Adaptation with Token Selection for Point Cloud Learning](https://arxiv.org/pdf/2504.16023)
*Song Wang, Xiaolu Liu, Lingdong Kong, Jianyun Xu, Chunyong Hu, Gongfan Fang, Wentong Li, Jianke Zhu, Xinchao Wang*

Main category: cs.CV

TL;DR: PointLoRA combines low-rank adaptation (LoRA) with multi-scale token selection for efficient fine-tuning of point cloud models, reducing tunable parameters while maintaining performance.


<details>
  <summary>Details</summary>
Motivation: Addressing the high computational and storage demands of fully fine-tuning complex pre-trained point cloud models by proposing a parameter-efficient method.

Method: Embeds LoRA layers in parameter-intensive components of point cloud transformers and uses multi-scale token selection for local information extraction.

Result: Achieves competitive performance with only 3.43% of trainable parameters across various models and datasets.

Conclusion: PointLoRA is a highly effective solution for resource-constrained applications, balancing efficiency and performance.

Abstract: Self-supervised representation learning for point cloud has demonstrated
effectiveness in improving pre-trained model performance across diverse tasks.
However, as pre-trained models grow in complexity, fully fine-tuning them for
downstream applications demands substantial computational and storage
resources. Parameter-efficient fine-tuning (PEFT) methods offer a promising
solution to mitigate these resource requirements, yet most current approaches
rely on complex adapter and prompt mechanisms that increase tunable parameters.
In this paper, we propose PointLoRA, a simple yet effective method that
combines low-rank adaptation (LoRA) with multi-scale token selection to
efficiently fine-tune point cloud models. Our approach embeds LoRA layers
within the most parameter-intensive components of point cloud transformers,
reducing the need for tunable parameters while enhancing global feature
capture. Additionally, multi-scale token selection extracts critical local
information to serve as prompts for downstream fine-tuning, effectively
complementing the global context captured by LoRA. The experimental results
across various pre-trained models and three challenging public datasets
demonstrate that our approach achieves competitive performance with only 3.43%
of the trainable parameters, making it highly effective for
resource-constrained applications. Source code is available at:
https://github.com/songw-zju/PointLoRA.

</details>


### [427] [GarmentDiffusion: 3D Garment Sewing Pattern Generation with Multimodal Diffusion Transformers](https://arxiv.org/pdf/2504.21476)
*Xinyu Li, Qi Yao, Yuanda Wang*

Main category: cs.CV

TL;DR: GarmentDiffusion is a generative model for creating precise 3D sewing patterns from multimodal inputs, improving efficiency and speed over existing methods.


<details>
  <summary>Details</summary>
Motivation: Existing methods for sewing pattern generation are limited by input modality reliance or inefficiency, prompting the need for a more versatile and faster solution.

Method: The model encodes 3D sewing patterns into compact edge tokens and uses a diffusion transformer to denoise them efficiently, reducing sequence length and steps.

Result: GarmentDiffusion accelerates generation speed by 100x and achieves state-of-the-art results on DressCodeData and GarmentCodeData.

Conclusion: The method demonstrates significant improvements in efficiency and precision for sewing pattern generation, setting a new benchmark in the field.

Abstract: Garment sewing patterns are fundamental design elements that bridge the gap
between design concepts and practical manufacturing. The generative modeling of
sewing patterns is crucial for creating diversified garments. However, existing
approaches are limited either by reliance on a single input modality or by
suboptimal generation efficiency. In this work, we present GarmentDiffusion, a
new generative model capable of producing centimeter-precise, vectorized 3D
sewing patterns from multimodal inputs (text, image, and incomplete sewing
pattern). Our method efficiently encodes 3D sewing pattern parameters into
compact edge token representations, achieving a sequence length that is 10
times shorter than that of the autoregressive SewingGPT in DressCode. By
employing a diffusion transformer, we simultaneously denoise all edge tokens
along the temporal axis, while maintaining a constant number of denoising steps
regardless of dataset-specific edge and panel statistics. With all combination
of designs of our model, the sewing pattern generation speed is accelerated by
100 times compared to SewingGPT. We achieve new state-of-the-art results on
DressCodeData, as well as on the largest sewing pattern dataset, namely
GarmentCodeData. The project website is available at
https://shenfu-research.github.io/Garment-Diffusion/.

</details>


### [428] [EDmamba: A Simple yet Effective Event Denoising Method with State Space Model](https://arxiv.org/pdf/2505.05391)
*Ciyu Ruan, Zihang Gong, Ruishan Guo, Jingao Xu, Xinlei Chen*

Main category: cs.CV

TL;DR: A novel event denoising framework using State Space Models (SSMs) achieves high accuracy and efficiency for event cameras, outperforming Transformer-based methods.


<details>
  <summary>Details</summary>
Motivation: Event cameras' noisy output requires efficient denoising to maintain their high-speed and real-time capabilities, but existing methods either compromise speed or robustness.

Method: Proposes a framework with Coarse Feature Extraction (CFE) and two SSM components: Spatial Mamba (S-SSM) for local geometry and Temporal Mamba (T-SSM) for global dynamics.

Result: Achieves 0.982 accuracy, 88.89K parameters, 0.0685s per 100K events, outperforming Transformers by 2.08% accuracy and 36X speed.

Conclusion: The SSM-based framework effectively balances accuracy and efficiency for event denoising, preserving the camera's high-speed advantages.

Abstract: Event cameras excel in high-speed vision due to their high temporal
resolution, high dynamic range, and low power consumption. However, as dynamic
vision sensors, their output is inherently noisy, making efficient denoising
essential to preserve their ultra-low latency and real-time processing
capabilities. Existing event denoising methods struggle with a critical
dilemma: computationally intensive approaches compromise the sensor's
high-speed advantage, while lightweight methods often lack robustness across
varying noise levels. To address this, we propose a novel event denoising
framework based on State Space Models (SSMs). Our approach represents events as
4D event clouds and includes a Coarse Feature Extraction (CFE) module that
extracts embedding features from both geometric and polarity-aware subspaces.
The model is further composed of two essential components: A Spatial Mamba
(S-SSM) that models local geometric structures and a Temporal Mamba (T-SSM)
that captures global temporal dynamics, efficiently propagating spatiotemporal
features across events. Experiments demonstrate that our method achieves
state-of-the-art accuracy and efficiency, with 88.89K parameters, 0.0685s per
100K events inference time, and a 0.982 accuracy score, outperforming
Transformer-based methods by 2.08% in denoising accuracy and 36X faster.

</details>


### [429] [Selftok: Discrete Visual Tokens of Autoregression, by Diffusion, and for Reasoning](https://arxiv.org/pdf/2505.07538)
*Bohan Wang, Zhongqi Yue, Fengda Zhang, Shuo Chen, Li'an Bi, Junzhe Zhang, Xue Song, Kennard Yanting Chan, Jiachun Pan, Weijia Wu, Mingze Zhou, Wang Lin, Kaihang Pan, Saining Zhang, Liyu Jia, Wentao Hu, Wei Zhao, Hanwang Zhang*

Main category: cs.CV

TL;DR: Selftok introduces a novel discrete visual tokenizer using an autoregressive prior, unifying diffusion and AR for vision-language models, and enabling effective reinforcement learning for visual generation.


<details>
  <summary>Details</summary>
Motivation: To address the limitations of conventional spatial priors in image representation and enable effective reinforcement learning in visual tokens, mirroring the success of autoregressive models in language.

Method: Selftok uses a reverse diffusion process to create autoregressive visual tokens, unifying diffusion and AR for vision-language models without additional modules.

Result: Selftok achieves high-quality reconstruction and compression, and a pure AR VLM built with it surpasses existing models in visual generation benchmarks.

Conclusion: Selftok successfully bridges the gap between visual tokens and reinforcement learning, advancing towards a truly multimodal LLM.

Abstract: We completely discard the conventional spatial prior in image representation
and introduce a novel discrete visual tokenizer: Self-consistency Tokenizer
(Selftok). At its design core, we compose an autoregressive (AR) prior --
mirroring the causal structure of language -- into visual tokens by using the
reverse diffusion process of image generation. The AR property makes Selftok
fundamentally distinct from traditional spatial tokens in the following two key
ways: - Selftok offers an elegant and minimalist approach to unify diffusion
and AR for vision-language models (VLMs): By representing images with Selftok
tokens, we can train a VLM using a purely discrete autoregressive architecture
-- like that in LLMs -- without requiring additional modules or training
objectives. - We theoretically show that the AR prior satisfies the Bellman
equation, whereas the spatial prior does not. Therefore, Selftok supports
reinforcement learning (RL) for visual generation with effectiveness comparable
to that achieved in LLMs. Besides the AR property, Selftok is also a SoTA
tokenizer that achieves a favorable trade-off between high-quality
reconstruction and compression rate. We use Selftok to build a pure AR VLM for
both visual comprehension and generation tasks. Impressively, without using any
text-image training pairs, a simple policy gradient RL working in the visual
tokens can significantly boost the visual generation benchmark, surpassing all
the existing models by a large margin. Therefore, we believe that Selftok
effectively addresses the long-standing challenge that visual tokens cannot
support effective RL. When combined with the well-established strengths of RL
in LLMs, this brings us one step closer to realizing a truly multimodal LLM.
Project Page: https://selftok-team.github.io/report/.

</details>


### [430] [MMLongBench: Benchmarking Long-Context Vision-Language Models Effectively and Thoroughly](https://arxiv.org/pdf/2505.10610)
*Zhaowei Wang, Wenhao Yu, Xiyu Ren, Jipeng Zhang, Yu Zhao, Rohit Saxena, Liang Cheng, Ginny Wong, Simon See, Pasquale Minervini, Yangqiu Song, Mark Steedman*

Main category: cs.CV

TL;DR: MMLongBench is introduced as the first benchmark for evaluating long-context vision-language models (LCVLMs) across diverse tasks and image types, revealing gaps in current models' capabilities.


<details>
  <summary>Details</summary>
Motivation: The rise of LCVLMs handling hundreds of images and text tokens necessitates a comprehensive benchmark to evaluate their long-context abilities.

Method: MMLongBench includes 13,331 examples across five task categories and various image types, standardized at five input lengths (8K-128K tokens).

Result: Benchmarking 46 LCVLMs shows performance on single tasks is a weak proxy for overall capability, with both closed and open-source models struggling.

Conclusion: MMLongBench provides a foundation for diagnosing and improving LCVLMs, highlighting the need for better reasoning and long-context handling.

Abstract: The rapid extension of context windows in large vision-language models has
given rise to long-context vision-language models (LCVLMs), which are capable
of handling hundreds of images with interleaved text tokens in a single forward
pass. In this work, we introduce MMLongBench, the first benchmark covering a
diverse set of long-context vision-language tasks, to evaluate LCVLMs
effectively and thoroughly. MMLongBench is composed of 13,331 examples spanning
five different categories of downstream tasks, such as Visual RAG and Many-Shot
ICL. It also provides broad coverage of image types, including various natural
and synthetic images. To assess the robustness of the models to different input
lengths, all examples are delivered at five standardized input lengths (8K-128K
tokens) via a cross-modal tokenization scheme that combines vision patches and
text tokens. Through a thorough benchmarking of 46 closed-source and
open-source LCVLMs, we provide a comprehensive analysis of the current models'
vision-language long-context ability. Our results show that: i) performance on
a single task is a weak proxy for overall long-context capability; ii) both
closed-source and open-source models face challenges in long-context
vision-language tasks, indicating substantial room for future improvement; iii)
models with stronger reasoning ability tend to exhibit better long-context
performance. By offering wide task coverage, various image types, and rigorous
length control, MMLongBench provides the missing foundation for diagnosing and
advancing the next generation of LCVLMs.

</details>


### [431] [WildDoc: How Far Are We from Achieving Comprehensive and Robust Document Understanding in the Wild?](https://arxiv.org/pdf/2505.11015)
*An-Lan Wang, Jingqun Tang, Liao Lei, Hao Feng, Qi Liu, Xiang Fei, Jinghui Lu, Han Wang, Weiwei Liu, Hao Liu, Yuliang Liu, Xiang Bai, Can Huang*

Main category: cs.CV

TL;DR: WildDoc is a new benchmark for evaluating document understanding in natural environments, addressing gaps in existing benchmarks by including real-world challenges like variable illumination and physical distortions.


<details>
  <summary>Details</summary>
Motivation: Existing benchmarks (e.g., DocVQA, ChartQA) focus on scanned or digital documents, failing to capture real-world complexities. WildDoc aims to bridge this gap.

Method: WildDoc uses manually captured document images under diverse conditions (four captures per document) and leverages sources from established benchmarks for comparison.

Result: State-of-the-art MLLMs show significant performance drops on WildDoc, revealing their lack of robustness in real-world scenarios.

Conclusion: WildDoc highlights the need for improved robustness in document understanding models for real-world applications.

Abstract: The rapid advancements in Multimodal Large Language Models (MLLMs) have
significantly enhanced capabilities in Document Understanding. However,
prevailing benchmarks like DocVQA and ChartQA predominantly comprise
\textit{scanned or digital} documents, inadequately reflecting the intricate
challenges posed by diverse real-world scenarios, such as variable illumination
and physical distortions. This paper introduces WildDoc, the inaugural
benchmark designed specifically for assessing document understanding in natural
environments. WildDoc incorporates a diverse set of manually captured document
images reflecting real-world conditions and leverages document sources from
established benchmarks to facilitate comprehensive comparisons with digital or
scanned documents. Further, to rigorously evaluate model robustness, each
document is captured four times under different conditions. Evaluations of
state-of-the-art MLLMs on WildDoc expose substantial performance declines and
underscore the models' inadequate robustness compared to traditional
benchmarks, highlighting the unique challenges posed by real-world document
understanding. Our project homepage is available at
https://bytedance.github.io/WildDoc.

</details>


### [432] [Denoising Mutual Knowledge Distillation in Bi-Directional Multiple Instance Learning](https://arxiv.org/pdf/2505.12074)
*Chen Shu, Boyu Fu, Yiman Li, Ting Yin, Wenchuan Zhang, Jie Chen, Yuhao Yi, Hong Bu*

Main category: cs.CV

TL;DR: The paper proposes a method to improve Multiple Instance Learning (MIL) for Whole Slide Image classification by incorporating pseudo-label correction, enhancing both bag- and instance-level predictions.


<details>
  <summary>Details</summary>
Motivation: MIL avoids fine-grained annotations but lacks accuracy in bag- and instance-level classifiers. Existing methods may introduce noisy labels.

Method: Augments MIL with pseudo-label correction using weak-to-strong generalization techniques.

Result: Improves performance of dual-level MIL algorithms on bag- and instance-level predictions.

Conclusion: The proposed method outperforms existing MIL techniques, validated on public pathology datasets.

Abstract: Multiple Instance Learning is the predominant method for Whole Slide Image
classification in digital pathology, enabling the use of slide-level labels to
supervise model training. Although MIL eliminates the tedious fine-grained
annotation process for supervised learning, whether it can learn accurate bag-
and instance-level classifiers remains a question. To address the issue,
instance-level classifiers and instance masks were incorporated to ground the
prediction on supporting patches. These methods, while practically improving
the performance of MIL methods, may potentially introduce noisy labels. We
propose to bridge the gap between commonly used MIL and fully supervised
learning by augmenting both the bag- and instance-level learning processes with
pseudo-label correction capabilities elicited from weak to strong
generalization techniques. The proposed algorithm improves the performance of
dual-level MIL algorithms on both bag- and instance-level predictions.
Experiments on public pathology datasets showcase the advantage of the proposed
methods.

</details>


### [433] [SoftPQ: Robust Instance Segmentation Evaluation via Soft Matching and Tunable Thresholds](https://arxiv.org/pdf/2505.12155)
*Ranit Karmakar, Simon F. Nørrelykke*

Main category: cs.CV

TL;DR: SoftPQ is a new instance segmentation metric that replaces binary evaluation with a graded continuum, offering smoother scoring and better feedback for model improvements.


<details>
  <summary>Details</summary>
Motivation: Traditional metrics like IoU, Dice, and PQ use rigid thresholds, treating predictions as binary (correct/incorrect), which obscures qualitative error distinctions and fails to reward gradual improvements.

Method: SoftPQ introduces tunable upper and lower IoU thresholds for partial matching and applies a sublinear penalty function to ambiguous predictions.

Result: SoftPQ provides smoother score behavior, robustness to structural errors, and more informative feedback, capturing differences overlooked by existing metrics.

Conclusion: SoftPQ is a practical and principled alternative for benchmarking and iterative model refinement, addressing limitations of traditional metrics.

Abstract: Segmentation evaluation metrics traditionally rely on binary decision logic:
predictions are either correct or incorrect, based on rigid IoU thresholds.
Detection--based metrics such as F1 and mAP determine correctness at the object
level using fixed overlap cutoffs, while overlap--based metrics like
Intersection over Union (IoU) and Dice operate at the pixel level, often
overlooking instance--level structure. Panoptic Quality (PQ) attempts to unify
detection and segmentation assessment, but it remains dependent on
hard-threshold matching--treating predictions below the threshold as entirely
incorrect. This binary framing obscures important distinctions between
qualitatively different errors and fails to reward gradual model improvements.
We propose SoftPQ, a flexible and interpretable instance segmentation metric
that redefines evaluation as a graded continuum rather than a binary
classification. SoftPQ introduces tunable upper and lower IoU thresholds to
define a partial matching region and applies a sublinear penalty function to
ambiguous or fragmented predictions. These extensions allow SoftPQ to exhibit
smoother score behavior, greater robustness to structural segmentation errors,
and more informative feedback for model development and evaluation. Through
controlled perturbation experiments, we show that SoftPQ captures meaningful
differences in segmentation quality that existing metrics overlook, making it a
practical and principled alternative for both benchmarking and iterative model
refinement.

</details>


### [434] [Visuospatial Cognitive Assistant](https://arxiv.org/pdf/2505.12312)
*Qi Feng*

Main category: cs.CV

TL;DR: The paper introduces ViCA-322K, a dataset for video-based spatial cognition, and ViCA-7B, a model fine-tuned on it, achieving state-of-the-art performance. It also presents ViCA-Thinking-2.68K for interpretability.


<details>
  <summary>Details</summary>
Motivation: Addressing challenges in video-based spatial cognition for robotics and embodied AI by improving Vision-Language Models (VLMs).

Method: Developed ViCA-322K dataset and fine-tuned ViCA-7B model, along with ViCA-Thinking-2.68K for reasoning chains.

Result: ViCA-7B outperforms existing models on VSI-Bench tasks, e.g., +26.1 on Absolute Distance.

Conclusion: Targeted data and models enhance visuospatial intelligence, with resources released for further research.

Abstract: Video-based spatial cognition is vital for robotics and embodied AI but
challenges current Vision-Language Models (VLMs). This paper makes two key
contributions. First, we introduce ViCA (Visuospatial Cognitive
Assistant)-322K, a diverse dataset of 322,003 QA pairs from real-world indoor
videos (ARKitScenes, ScanNet, ScanNet++), offering supervision for 3D
metadata-grounded queries and video-based complex reasoning. Second, we develop
ViCA-7B, fine-tuned on ViCA-322K, which achieves new state-of-the-art on all
eight VSI-Bench tasks, outperforming existing models, including larger ones
(e.g., +26.1 on Absolute Distance). For interpretability, we present
ViCA-Thinking-2.68K, a dataset with explicit reasoning chains, and fine-tune
ViCA-7B to create ViCA-7B-Thinking, a model that articulates its spatial
reasoning. Our work highlights the importance of targeted data and suggests
paths for improved temporal-spatial modeling. We release all resources to
foster research in robust visuospatial intelligence.

</details>


### [435] [Towards Visuospatial Cognition via Hierarchical Fusion of Visual Experts](https://arxiv.org/pdf/2505.12363)
*Qi Feng*

Main category: cs.CV

TL;DR: ViCA2 enhances visuospatial reasoning in MLLMs with a dual vision encoder and new dataset, outperforming larger models.


<details>
  <summary>Details</summary>
Motivation: Existing MLLMs struggle with fine-grained spatial understanding due to lacking architecture and training data.

Method: ViCA2 integrates SigLIP and Hiera encoders with token ratio control and uses ViCA-322K dataset for tuning.

Result: ViCA2-7B scores 56.8 on VSI-Bench, surpassing larger models like LLaVA-NeXT-Video-72B (40.9) and Gemini-1.5 Pro (45.4).

Conclusion: ViCA2 demonstrates strong visuospatial intelligence with a compact model; code and dataset are released for research.

Abstract: While Multimodal Large Language Models (MLLMs) excel at general
vision-language tasks, visuospatial cognition - reasoning about spatial
layouts, relations, and dynamics - remains a significant challenge. Existing
models often lack the necessary architectural components and specialized
training data for fine-grained spatial understanding. We introduce ViCA2
(Visuospatial Cognitive Assistant 2), a novel MLLM designed to enhance spatial
reasoning. ViCA2 features a dual vision encoder architecture integrating SigLIP
for semantics and Hiera for spatial structure, coupled with a token ratio
control mechanism for efficiency. We also developed ViCA-322K, a new
large-scale dataset with over 322,000 spatially grounded question-answer pairs
for targeted instruction tuning. On the challenging VSI-Bench benchmark, our
ViCA2-7B model achieves a state-of-the-art average score of 56.8, significantly
surpassing larger open-source models (e.g., LLaVA-NeXT-Video-72B, 40.9) and
leading proprietary models (Gemini-1.5 Pro, 45.4). This demonstrates the
effectiveness of our approach in achieving strong visuospatial intelligence
with a compact model. We release ViCA2, its codebase, and the ViCA-322K dataset
to facilitate further research.

</details>


### [436] [Dual Data Alignment Makes AI-Generated Image Detector Easier Generalizable](https://arxiv.org/pdf/2505.14359)
*Ruoxin Chen, Junwei Xi, Zhiyuan Yan, Ke-Yue Zhang, Shuang Wu, Jingyi Xie, Xu Chen, Lei Xu, Isabel Guan, Taiping Yao, Shouhong Ding*

Main category: cs.CV

TL;DR: The paper addresses dataset bias in detectors by proposing Dual Data Alignment (DDA) to align pixel and frequency domains, improving generalizability.


<details>
  <summary>Details</summary>
Motivation: Existing detectors overfit on biased datasets, relying on spurious correlations, leading to poor performance on unbiased data. Pixel-level alignment alone is insufficient due to frequency-level misalignment.

Method: Proposes DDA for dual-domain alignment (pixel and frequency) and introduces DDA-COCO and EvalGEN test sets for evaluation.

Result: Detectors trained on DDA-aligned MSCOCO show improved performance (+7.2% on in-the-wild benchmarks) across 8 diverse benchmarks.

Conclusion: DDA effectively mitigates dataset bias, enhancing detector generalizability by addressing both pixel and frequency misalignment.

Abstract: Existing detectors are often trained on biased datasets, leading to the
possibility of overfitting on non-causal image attributes that are spuriously
correlated with real/synthetic labels. While these biased features enhance
performance on the training data, they result in substantial performance
degradation when applied to unbiased datasets. One common solution is to
perform dataset alignment through generative reconstruction, matching the
semantic content between real and synthetic images. However, we revisit this
approach and show that pixel-level alignment alone is insufficient. The
reconstructed images still suffer from frequency-level misalignment, which can
perpetuate spurious correlations. To illustrate, we observe that reconstruction
models tend to restore the high-frequency details lost in real images (possibly
due to JPEG compression), inadvertently creating a frequency-level
misalignment, where synthetic images appear to have richer high-frequency
content than real ones. This misalignment leads to models associating
high-frequency features with synthetic labels, further reinforcing biased cues.
To resolve this, we propose Dual Data Alignment (DDA), which aligns both the
pixel and frequency domains. Moreover, we introduce two new test sets:
DDA-COCO, containing DDA-aligned synthetic images for testing detector
performance on the most aligned dataset, and EvalGEN, featuring the latest
generative models for assessing detectors under new generative architectures
such as visual auto-regressive generators. Finally, our extensive evaluations
demonstrate that a detector trained exclusively on DDA-aligned MSCOCO could
improve across 8 diverse benchmarks by a non-trivial margin, showing a +7.2% on
in-the-wild benchmarks, highlighting the improved generalizability of unbiased
detectors.

</details>


### [437] [diffDemorph: Extending Reference-Free Demorphing to Unseen Faces](https://arxiv.org/pdf/2505.14527)
*Nitish Shukla, Arun Ross*

Main category: cs.CV

TL;DR: A novel diffusion-based method for reference-free demorphing of face images, generalizing across techniques and styles, outperforming state-of-the-art by ≥59.46%.


<details>
  <summary>Details</summary>
Motivation: Existing RF demorphing methods are limited by assumptions about training and testing morphs. This work aims to overcome these constraints.

Method: Uses a diffusion-based approach to disentangle component images from a composite morph, trained on synthetic data and tested on real morphs.

Result: Achieves ≥59.46% improvement over current methods, generalizes across datasets and face matchers.

Conclusion: The method is practical and effective, demonstrated by extensive experiments on diverse datasets.

Abstract: A face morph is created by combining two (or more) face images corresponding
to two (or more) identities to produce a composite that successfully matches
the constituent identities. Reference-free (RF) demorphing reverses this
process using only the morph image, without the need for additional reference
images. Previous RF demorphing methods were overly constrained, as they rely on
assumptions about the distributions of training and testing morphs such as the
morphing technique used, face style, and images used to create the morph. In
this paper, we introduce a novel diffusion-based approach that effectively
disentangles component images from a composite morph image with high visual
fidelity. Our method is the first to generalize across morph techniques and
face styles, beating the current state of the art by $\geq 59.46\%$ under a
common training protocol across all datasets tested. We train our method on
morphs created using synthetically generated face images and test on real
morphs, thereby enhancing the practicality of the technique. Experiments on six
datasets and two face matchers establish the utility and efficacy of our
method.

</details>


### [438] [Auto-nnU-Net: Towards Automated Medical Image Segmentation](https://arxiv.org/pdf/2505.16561)
*Jannis Becktepe, Leona Hennig, Steffen Oeltze-Jafra, Marius Lindauer*

Main category: cs.CV

TL;DR: Auto-nnU-Net enhances nnU-Net by incorporating hyperparameter optimization, neural architecture search, and hierarchical NAS, improving segmentation performance while managing computational resources.


<details>
  <summary>Details</summary>
Motivation: The limitations of nnU-Net's fixed hyperparameters and heuristic designs prompted the development of Auto-nnU-Net to automate and optimize MIS tasks.

Method: Proposes Auto-nnU-Net with HPO, NAS, and HNAS, and introduces Regularized PriorBand for balancing accuracy and computational resources.

Result: Substantially improves segmentation performance on 6 out of 10 datasets, matches performance on others, and maintains practical resource usage.

Conclusion: Auto-nnU-Net advances AutoML in MIS, offering better performance and efficiency, with code publicly available.

Abstract: Medical Image Segmentation (MIS) includes diverse tasks, from bone to organ
segmentation, each with its own challenges in finding the best segmentation
model. The state-of-the-art AutoML-related MIS-framework nnU-Net automates many
aspects of model configuration but remains constrained by fixed hyperparameters
and heuristic design choices. As a full-AutoML framework for MIS, we propose
Auto-nnU-Net, a novel nnU-Net variant enabling hyperparameter optimization
(HPO), neural architecture search (NAS), and hierarchical NAS (HNAS).
Additionally, we propose Regularized PriorBand to balance model accuracy with
the computational resources required for training, addressing the resource
constraints often faced in real-world medical settings that limit the
feasibility of extensive training procedures. We evaluate our approach across
diverse MIS datasets from the well-established Medical Segmentation Decathlon,
analyzing the impact of AutoML techniques on segmentation performance,
computational efficiency, and model design choices. The results demonstrate
that our AutoML approach substantially improves the segmentation performance of
nnU-Net on 6 out of 10 datasets and is on par on the other datasets while
maintaining practical resource requirements. Our code is available at
https://github.com/automl/AutoNNUnet.

</details>


### [439] [OrionBench: A Benchmark for Chart and Human-Recognizable Object Detection in Infographics](https://arxiv.org/pdf/2505.17473)
*Jiangning Zhu, Yuxing Zhou, Zheng Wang, Juntao Yao, Yima Gu, Yuhui Yuan, Shixia Liu*

Main category: cs.CV

TL;DR: OrionBench is a benchmark for improving object detection in charts and human-recognizable objects (HROs) in infographics, aiding vision-language models (VLMs) in better chart understanding.


<details>
  <summary>Details</summary>
Motivation: Existing VLMs lack accurate visual grounding for infographic elements, limiting chart understanding.

Method: OrionBench includes 105,000 infographics (real and synthetic) with 6.9M bounding box annotations, created using model-in-the-loop and programmatic methods.

Result: The benchmark supports three applications: enhancing VLM performance, comparing detection models, and extending to document/UI element detection.

Conclusion: OrionBench addresses a critical gap in VLM capabilities for chart understanding and object detection in infographics.

Abstract: Given the central role of charts in scientific, business, and communication
contexts, enhancing the chart understanding capabilities of vision-language
models (VLMs) has become increasingly critical. A key limitation of existing
VLMs lies in their inaccurate visual grounding of infographic elements,
including charts and human-recognizable objects (HROs) such as icons and
images. However, chart understanding often requires identifying relevant
elements and reasoning over them. To address this limitation, we introduce
OrionBench, a benchmark designed to support the development of accurate object
detection models for charts and HROs in infographics. It contains 26,250 real
and 78,750 synthetic infographics, with over 6.9 million bounding box
annotations. These annotations are created by combining the model-in-the-loop
and programmatic methods. We demonstrate the usefulness of OrionBench through
three applications: 1) constructing a Thinking-with-Boxes scheme to boost the
chart understanding performance of VLMs, 2) comparing existing object detection
models, and 3) applying the developed detection model to document layout and UI
element detection.

</details>


### [440] [Few-Shot Learning from Gigapixel Images via Hierarchical Vision-Language Alignment and Modeling](https://arxiv.org/pdf/2505.17982)
*Bryan Wong, Jong Woo Kim, Huazhu Fu, Mun Yong Yi*

Main category: cs.CV

TL;DR: HiVE-MIL is a hierarchical vision-language framework for few-shot WSI classification, addressing limitations in multi-scale interaction and modality alignment, outperforming existing methods by up to 4.1% in macro F1.


<details>
  <summary>Details</summary>
Motivation: Existing methods lack sufficient modeling of intra-modal interactions across scales and alignment between visual and textual modalities on the same scale.

Method: HiVE-MIL constructs a unified graph with parent-child links for hierarchical relationships and heterogeneous intra-scale edges for modality alignment, using text-guided dynamic filtering and hierarchical contrastive loss.

Result: Outperforms traditional and VLM-based MIL approaches, achieving up to 4.1% higher macro F1 on TCGA datasets.

Conclusion: Jointly modeling hierarchical structure and multimodal alignment improves learning from limited pathology data.

Abstract: Vision-language models (VLMs) have recently been integrated into multiple
instance learning (MIL) frameworks to address the challenge of few-shot, weakly
supervised classification of whole slide images (WSIs). A key trend involves
leveraging multi-scale information to better represent hierarchical tissue
structures. However, existing methods often face two key limitations: (1)
insufficient modeling of interactions within the same modalities across scales
(e.g., 5x and 20x) and (2) inadequate alignment between visual and textual
modalities on the same scale. To address these gaps, we propose HiVE-MIL, a
hierarchical vision-language framework that constructs a unified graph
consisting of (1) parent-child links between coarse (5x) and fine (20x)
visual/textual nodes to capture hierarchical relationships, and (2)
heterogeneous intra-scale edges linking visual and textual nodes on the same
scale. To further enhance semantic consistency, HiVE-MIL incorporates a
two-stage, text-guided dynamic filtering mechanism that removes weakly
correlated patch-text pairs, and introduces a hierarchical contrastive loss to
align textual semantics across scales. Extensive experiments on TCGA breast,
lung, and kidney cancer datasets demonstrate that HiVE-MIL consistently
outperforms both traditional MIL and recent VLM-based MIL approaches, achieving
gains of up to 4.1% in macro F1 under 16-shot settings. Our results demonstrate
the value of jointly modeling hierarchical structure and multimodal alignment
for efficient and scalable learning from limited pathology data. The code is
available at https://github.com/bryanwong17/HiVE-MIL

</details>


### [441] [RemoteSAM: Towards Segment Anything for Earth Observation](https://arxiv.org/pdf/2505.18022)
*Liang Yao, Fan Liu, Delong Chen, Chuanyi Zhang, Yijun Wang, Ziyun Chen, Wei Xu, Shimin Di, Yuhui Zheng*

Main category: cs.CV

TL;DR: The paper introduces RemoteSAM, a robust visual foundation model for Earth observation, addressing data and modeling limitations with a scalable data engine and task unification paradigm, achieving state-of-the-art results.


<details>
  <summary>Details</summary>
Motivation: Current systems lack flexibility and robustness for diverse Earth observation tasks due to narrow data domains and task-specific architectures.

Method: The study uses an automatic data engine to create a large dataset (270K image-text-mask triplets) and proposes a task unification paradigm centered on referring expression segmentation.

Result: RemoteSAM outperforms other foundation models (Falcon, GeoChat, LHRS-Bot) on Earth observation benchmarks with higher efficiency.

Conclusion: RemoteSAM demonstrates the effectiveness of scalable data and unified modeling for versatile Earth observation tasks, with publicly available models and data.

Abstract: We aim to develop a robust yet flexible visual foundation model for Earth
observation. It should possess strong capabilities in recognizing and
localizing diverse visual targets while providing compatibility with various
input-output interfaces required across different task scenarios. Current
systems cannot meet these requirements, as they typically utilize task-specific
architecture trained on narrow data domains with limited semantic coverage. Our
study addresses these limitations from two aspects: data and modeling. We first
introduce an automatic data engine that enjoys significantly better scalability
compared to previous human annotation or rule-based approaches. It has enabled
us to create the largest dataset of its kind to date, comprising 270K
image-text-mask triplets covering an unprecedented range of diverse semantic
categories and attribute specifications. Based on this data foundation, we
further propose a task unification paradigm that centers around referring
expression segmentation. It effectively handles a wide range of vision-centric
perception tasks, including classification, detection, segmentation, grounding,
etc, using a single model without any task-specific heads. Combining these
innovations on data and modeling, we present RemoteSAM, a foundation model that
establishes new SoTA on several earth observation perception benchmarks,
outperforming other foundation models such as Falcon, GeoChat, and LHRS-Bot
with significantly higher efficiency. Models and data are publicly available at
https://github.com/1e12Leon/RemoteSAM.

</details>


### [442] [SHARDeg: A Benchmark for Skeletal Human Action Recognition in Degraded Scenarios](https://arxiv.org/pdf/2505.18048)
*Simon Malzard, Nitish Mital, Richard Walters, Victoria Nockles, Raghuveer Rao, Celso M. De Melo*

Main category: cs.CV

TL;DR: The paper introduces a benchmark for assessing SHAR model robustness to degraded data, revealing significant accuracy variations and proposing a simple interpolation-based mitigation.


<details>
  <summary>Details</summary>
Motivation: Existing SHAR models lack thorough evaluation under real-world degraded data conditions, despite their critical role in real-time, edge-based CV applications.

Method: The study benchmarks five leading SHAR models on the NTU-RGB+D-120 dataset under three degradation types, analyzes temporal regularity's impact, and tests an interpolation-based mitigation.

Result: Degradation type affects model accuracy by >40%. Temporal regularity is key, and interpolation improves performance by >40%. The LogSigRNN model outperforms SoTA in degraded conditions.

Conclusion: The benchmark highlights the importance of degradation robustness in SHAR, identifies temporal regularity as a performance driver, and suggests LogSigRNN as a promising model for degraded data.

Abstract: Computer vision (CV) models for detection, prediction or classification tasks
operate on video data-streams that are often degraded in the real world, due to
deployment in real-time or on resource-constrained hardware. It is therefore
critical that these models are robust to degraded data, but state of the art
(SoTA) models are often insufficiently assessed with these real-world
constraints in mind. This is exemplified by Skeletal Human Action Recognition
(SHAR), which is critical in many CV pipelines operating in real-time and at
the edge, but robustness to degraded data has previously only been shallowly
and inconsistently assessed. Here we address this issue for SHAR by providing
an important first data degradation benchmark on the most detailed and largest
3D open dataset, NTU-RGB+D-120, and assess the robustness of five leading SHAR
models to three forms of degradation that represent real-world issues. We
demonstrate the need for this benchmark by showing that the form of
degradation, which has not previously been considered, has a large impact on
model accuracy; at the same effective frame rate, model accuracy can vary by
>40% depending on degradation type. We also identify that temporal regularity
of frames in degraded SHAR data is likely a major driver of differences in
model performance, and harness this to improve performance of existing models
by up to >40%, through employing a simple mitigation approach based on
interpolation. Finally, we highlight how our benchmark has helped identify an
important degradation-resistant SHAR model based in Rough Path Theory; the
LogSigRNN SHAR model outperforms the SoTA DeGCN model in five out of six cases
at low frame rates by an average accuracy of 6%, despite trailing the SoTA
model by 11-12% on un-degraded data at high frame rates (30 FPS).

</details>


### [443] [Semantic Correspondence: Unified Benchmarking and a Strong Baseline](https://arxiv.org/pdf/2505.18060)
*Kaiyan Zhang, Xinghui Li, Jingyi Lu, Kai Han*

Main category: cs.CV

TL;DR: This paper presents the first extensive survey of semantic correspondence methods in computer vision, proposing a taxonomy, analyzing existing approaches, summarizing benchmark results, and introducing a simple yet effective baseline for future research.


<details>
  <summary>Details</summary>
Motivation: The task of semantic correspondence lacks a comprehensive review despite significant progress in deep learning. This paper aims to fill that gap.

Method: The authors classify existing methods into a taxonomy, analyze each approach, summarize benchmark results, and conduct controlled experiments to evaluate method components.

Result: A unified comparative table of benchmark results is provided, and a new baseline method achieves state-of-the-art performance.

Conclusion: The survey serves as a reference and baseline for future research, with code publicly available.

Abstract: Establishing semantic correspondence is a challenging task in computer
vision, aiming to match keypoints with the same semantic information across
different images. Benefiting from the rapid development of deep learning,
remarkable progress has been made over the past decade. However, a
comprehensive review and analysis of this task remains absent. In this paper,
we present the first extensive survey of semantic correspondence methods. We
first propose a taxonomy to classify existing methods based on the type of
their method designs. These methods are then categorized accordingly, and we
provide a detailed analysis of each approach. Furthermore, we aggregate and
summarize the results of methods in literature across various benchmarks into a
unified comparative table, with detailed configurations to highlight
performance variations. Additionally, to provide a detailed understanding on
existing methods for semantic matching, we thoroughly conduct controlled
experiments to analyse the effectiveness of the components of different
methods. Finally, we propose a simple yet effective baseline that achieves
state-of-the-art performance on multiple benchmarks, providing a solid
foundation for future research in this field. We hope this survey serves as a
comprehensive reference and consolidated baseline for future development. Code
is publicly available at: https://github.com/Visual-AI/Semantic-Correspondence.

</details>


### [444] [Chain-of-Zoom: Extreme Super-Resolution via Scale Autoregression and Preference Alignment](https://arxiv.org/pdf/2505.18600)
*Bryan Sangwoo Kim, Jeongsol Kim, Jong Chul Ye*

Main category: cs.CV

TL;DR: CoZ (Chain-of-Zoom) is a framework enabling extreme super-resolution by decomposing the task into autoregressive steps with multi-scale prompts, reusing a backbone model without retraining.


<details>
  <summary>Details</summary>
Motivation: Addressing the scalability bottleneck of SISR models, which fail at magnifications beyond their trained scale factors.

Method: CoZ factorizes SISR into autoregressive steps with multi-scale-aware prompts, fine-tuned using GRPO for human-aligned text guidance.

Result: A standard 4x diffusion SR model achieves beyond 256x enlargement with high quality and fidelity.

Conclusion: CoZ provides a scalable, model-agnostic solution for extreme super-resolution without additional training.

Abstract: Modern single-image super-resolution (SISR) models deliver photo-realistic
results at the scale factors on which they are trained, but collapse when asked
to magnify far beyond that regime. We address this scalability bottleneck with
Chain-of-Zoom (CoZ), a model-agnostic framework that factorizes SISR into an
autoregressive chain of intermediate scale-states with multi-scale-aware
prompts. CoZ repeatedly re-uses a backbone SR model, decomposing the
conditional probability into tractable sub-problems to achieve extreme
resolutions without additional training. Because visual cues diminish at high
magnifications, we augment each zoom step with multi-scale-aware text prompts
generated by a vision-language model (VLM). The prompt extractor itself is
fine-tuned using Generalized Reward Policy Optimization (GRPO) with a critic
VLM, aligning text guidance towards human preference. Experiments show that a
standard 4x diffusion SR model wrapped in CoZ attains beyond 256x enlargement
with high perceptual quality and fidelity. Project Page:
https://bryanswkim.github.io/chain-of-zoom/ .

</details>


### [445] [Restoring Real-World Images with an Internal Detail Enhancement Diffusion Model](https://arxiv.org/pdf/2505.18674)
*Peng Xiao, Hongbo Zhao, Yijun Wang, Jianxin Lin*

Main category: cs.CV

TL;DR: Proposes an internal detail-preserving diffusion model for high-fidelity restoration of degraded images, leveraging Stable Diffusion and IIDE technique for enhanced control and quality.


<details>
  <summary>Details</summary>
Motivation: Addresses challenges in restoring real-world degraded images with mixed degradations, aiming for high-fidelity and object-level colorization control.

Method: Uses a pre-trained Stable Diffusion model with Internal Image Detail Enhancement (IIDE) to preserve details and simulate degradations in latent space.

Result: Outperforms state-of-the-art models in qualitative and quantitative evaluations, supporting text-guided restoration.

Conclusion: The method achieves superior restoration with object-level control, mimicking professional photo editing.

Abstract: Restoring real-world degraded images, such as old photographs or
low-resolution images, presents a significant challenge due to the complex,
mixed degradations they exhibit, such as scratches, color fading, and noise.
Recent data-driven approaches have struggled with two main challenges:
achieving high-fidelity restoration and providing object-level control over
colorization. While diffusion models have shown promise in generating
high-quality images with specific controls, they often fail to fully preserve
image details during restoration. In this work, we propose an internal
detail-preserving diffusion model for high-fidelity restoration of real-world
degraded images. Our method utilizes a pre-trained Stable Diffusion model as a
generative prior, eliminating the need to train a model from scratch. Central
to our approach is the Internal Image Detail Enhancement (IIDE) technique,
which directs the diffusion model to preserve essential structural and textural
information while mitigating degradation effects. The process starts by mapping
the input image into a latent space, where we inject the diffusion denoising
process with degradation operations that simulate the effects of various
degradation factors. Extensive experiments demonstrate that our method
significantly outperforms state-of-the-art models in both qualitative
assessments and perceptual quantitative evaluations. Additionally, our approach
supports text-guided restoration, enabling object-level colorization control
that mimics the expertise of professional photo editing.

</details>


### [446] [CDPDNet: Integrating Text Guidance with Hybrid Vision Encoders for Medical Image Segmentation](https://arxiv.org/pdf/2505.18958)
*Jiong Wu, Yang Xing, Boxiao Yu, Wei Shao, Kuang Gong*

Main category: cs.CV

TL;DR: A novel CLIP-DINO Prompt-Driven Segmentation Network (CDPDNet) combines self-supervised vision transformers with CLIP-based text embeddings to improve medical segmentation accuracy and generalizability.


<details>
  <summary>Details</summary>
Motivation: Incomplete annotations in medical datasets and vision-only frameworks' inability to capture anatomical relationships limit segmentation performance.

Method: CDPDNet integrates DINOv2 for visual features, CLIP for text embeddings, and a Text-based Task Prompt Generation (TTPG) module for task-specific guidance.

Result: CDPDNet outperforms state-of-the-art methods on multiple medical imaging datasets.

Conclusion: The proposed framework effectively addresses partial labeling and enhances segmentation accuracy and generalizability.

Abstract: Most publicly available medical segmentation datasets are only partially
labeled, with annotations provided for a subset of anatomical structures. When
multiple datasets are combined for training, this incomplete annotation poses
challenges, as it limits the model's ability to learn shared anatomical
representations among datasets. Furthermore, vision-only frameworks often fail
to capture complex anatomical relationships and task-specific distinctions,
leading to reduced segmentation accuracy and poor generalizability to unseen
datasets. In this study, we proposed a novel CLIP-DINO Prompt-Driven
Segmentation Network (CDPDNet), which combined a self-supervised vision
transformer with CLIP-based text embedding and introduced task-specific text
prompts to tackle these challenges. Specifically, the framework was constructed
upon a convolutional neural network (CNN) and incorporated DINOv2 to extract
both fine-grained and global visual features, which were then fused using a
multi-head cross-attention module to overcome the limited long-range modeling
capability of CNNs. In addition, CLIP-derived text embeddings were projected
into the visual space to help model complex relationships among organs and
tumors. To further address the partial label challenge and enhance inter-task
discriminative capability, a Text-based Task Prompt Generation (TTPG) module
that generated task-specific prompts was designed to guide the segmentation.
Extensive experiments on multiple medical imaging datasets demonstrated that
CDPDNet consistently outperformed existing state-of-the-art segmentation
methods. Code and pretrained model are available at:
https://github.com/wujiong-hub/CDPDNet.git.

</details>


### [447] [Training-free Stylized Text-to-Image Generation with Fast Inference](https://arxiv.org/pdf/2505.19063)
*Xin Ma, Yaohui Wang, Xinyuan Chen, Tien-Tsin Wong, Cunjian Chen*

Main category: cs.CV

TL;DR: OmniPainter enables stylized image generation using pre-trained diffusion models without fine-tuning, leveraging self-consistency and norm mixture of self-attention for superior results.


<details>
  <summary>Details</summary>
Motivation: Existing methods for stylized image generation with diffusion models require time-consuming fine-tuning or textual inversion, limiting practicality.

Method: OmniPainter uses self-consistency to extract style statistics from reference images and norm mixture of self-attention to guide stylization without fine-tuning.

Result: Qualitative and quantitative experiments show OmniPainter outperforms state-of-the-art methods.

Conclusion: OmniPainter offers an efficient, high-quality solution for stylized image generation without additional optimization.

Abstract: Although diffusion models exhibit impressive generative capabilities,
existing methods for stylized image generation based on these models often
require textual inversion or fine-tuning with style images, which is
time-consuming and limits the practical applicability of large-scale diffusion
models. To address these challenges, we propose a novel stylized image
generation method leveraging a pre-trained large-scale diffusion model without
requiring fine-tuning or any additional optimization, termed as OmniPainter.
Specifically, we exploit the self-consistency property of latent consistency
models to extract the representative style statistics from reference style
images to guide the stylization process. Additionally, we then introduce the
norm mixture of self-attention, which enables the model to query the most
relevant style patterns from these statistics for the intermediate output
content features. This mechanism also ensures that the stylized results align
closely with the distribution of the reference style images. Our qualitative
and quantitative experimental results demonstrate that the proposed method
outperforms state-of-the-art approaches.

</details>


### [448] [Towards Generalized Proactive Defense against Face Swapping with Contour-Hybrid Watermark](https://arxiv.org/pdf/2505.19081)
*Ruiyang Xia, Dawei Zhou, Decheng Liu, Lin Yuan, Jie Li, Nannan Wang, Xinbo Gao*

Main category: cs.CV

TL;DR: The paper proposes a proactive watermarking method (CMark) for detecting face swaps by embedding hybrid messages in facial contours, achieving robust detection without prior knowledge of swapping techniques.


<details>
  <summary>Details</summary>
Motivation: Addressing privacy and security concerns from AI-generated face swaps by focusing on the purpose of swapping (identity replacement) and embedding watermarks in face-contour regions.

Method: Embedding elaborate watermarks (CMark) in facial contours, combining contour texture and identity information, to detect swaps without needing prior knowledge of swapping techniques.

Result: Superior performance across 8 face swapping techniques, balancing image quality and watermark robustness, outperforming state-of-the-art detectors.

Conclusion: CMark effectively generalizes face swapping detection, offering a proactive solution without dependency on specific swapping methods or large-scale data storage.

Abstract: Face swapping, recognized as a privacy and security concern, has prompted
considerable defensive research. With the advancements in AI-generated content,
the discrepancies between the real and swapped faces have become nuanced.
Considering the difficulty of forged traces detection, we shift the focus to
the face swapping purpose and proactively embed elaborate watermarks against
unknown face swapping techniques. Given that the constant purpose is to swap
the original face identity while preserving the background, we concentrate on
the regions surrounding the face to ensure robust watermark generation, while
embedding the contour texture and face identity information to achieve
progressive image determination. The watermark is located in the facial contour
and contains hybrid messages, dubbed the contour-hybrid watermark (CMark). Our
approach generalizes face swapping detection without requiring any swapping
techniques during training and the storage of large-scale messages in advance.
Experiments conducted across 8 face swapping techniques demonstrate the
superiority of our approach compared with state-of-the-art passive and
proactive detectors while achieving a favorable balance between the image
quality and watermark robustness.

</details>


### [449] [EventEgoHands: Event-based Egocentric 3D Hand Mesh Reconstruction](https://arxiv.org/pdf/2505.19169)
*Ryosei Hara, Wataru Ikeda, Masashi Hatano, Mariko Isogawa*

Main category: cs.CV

TL;DR: EventEgoHands improves 3D hand mesh reconstruction using event cameras, addressing dynamic background noise and outperforming existing methods by 43% in accuracy.


<details>
  <summary>Details</summary>
Motivation: Overcoming limitations of RGB/depth cameras in low-light and motion blur scenarios, leveraging event cameras' high dynamic range and temporal resolution.

Method: Introduces a Hand Segmentation Module to isolate hand regions, reducing dynamic background noise in event-based 3D hand mesh reconstruction.

Result: Achieves a 43% improvement in MPJPE (4.5 cm) on the N-HOT3D dataset.

Conclusion: EventEgoHands effectively addresses challenges in event-based hand reconstruction, demonstrating superior performance in dynamic environments.

Abstract: Reconstructing 3D hand mesh is challenging but an important task for
human-computer interaction and AR/VR applications. In particular, RGB and/or
depth cameras have been widely used in this task. However, methods using these
conventional cameras face challenges in low-light environments and during
motion blur. Thus, to address these limitations, event cameras have been
attracting attention in recent years for their high dynamic range and high
temporal resolution. Despite their advantages, event cameras are sensitive to
background noise or camera motion, which has limited existing studies to static
backgrounds and fixed cameras. In this study, we propose EventEgoHands, a novel
method for event-based 3D hand mesh reconstruction in an egocentric view. Our
approach introduces a Hand Segmentation Module that extracts hand regions,
effectively mitigating the influence of dynamic background events. We evaluated
our approach and demonstrated its effectiveness on the N-HOT3D dataset,
improving MPJPE by approximately more than 4.5 cm (43%).

</details>


### [450] [Regularized Personalization of Text-to-Image Diffusion Models without Distributional Drift](https://arxiv.org/pdf/2505.19519)
*Gihoon Kim, Hyungjin Park, Taesup Kim*

Main category: cs.CV

TL;DR: The paper addresses the challenge of personalizing text-to-image diffusion models without forgetting prior capabilities, proposing a Lipschitz-bounded training objective to control distributional drift and improve performance.


<details>
  <summary>Details</summary>
Motivation: Personalizing pretrained models for new subjects risks forgetting original generative abilities, leading to distributional drift. The paper aims to mitigate this issue.

Method: A new Lipschitz-bounded training objective is introduced to explicitly constrain deviation from the pretrained model's distribution.

Result: The method outperforms existing approaches, achieving higher CLIP-T, CLIP-I, and DINO scores, even with limited data.

Conclusion: The proposed training objective effectively balances personalization and preservation of generative capabilities, offering improved performance and control over distributional drift.

Abstract: Personalization using text-to-image diffusion models involves adapting a
pretrained model to novel subjects with only a few image examples. This task
presents a fundamental challenge, as the model must not only learn the new
subject effectively but also preserve its ability to generate diverse and
coherent outputs across a wide range of prompts. In other words, successful
personalization requires integrating new concepts without forgetting previously
learned generative capabilities. Forgetting denotes unintended distributional
drift, where the model's output distribution deviates from that of the original
pretrained model. In this paper, we provide an analysis of this issue and
identify a mismatch between standard training objectives and the goals of
personalization. To address this, we propose a new training objective based on
a Lipschitz-bounded formulation that explicitly constrains deviation from the
pretrained distribution. Our method provides improved control over
distributional drift and performs well even in data-scarce scenarios.
Experimental results demonstrate that our approach consistently outperforms
existing personalization methods, achieving higher CLIP-T, CLIP-I, and DINO
scores.

</details>


### [451] [SuperAD: A Training-free Anomaly Classification and Segmentation Method for CVPR 2025 VAND 3.0 Workshop Challenge Track 1: Adapt & Detect](https://arxiv.org/pdf/2505.19750)
*Huaiyuan Zhang, Hang Chen, Yu Cheng, Shunyi Wu, Linghao Sun, Linao Han, Zeyu Shi, Lei Qi*

Main category: cs.CV

TL;DR: A training-free anomaly detection method, SuperAD, using DINOv2 for feature extraction achieves competitive results on the MVTec AD 2 dataset.


<details>
  <summary>Details</summary>
Motivation: Addressing real-world industrial anomaly detection challenges like complex lighting and diverse anomalies, narrowing the gap with public benchmarks.

Method: Leverages DINOv2 for feature extraction, constructs a memory bank from normal reference images, and segments anomalies via nearest neighbor matching.

Result: Competitive performance on the MVTec AD 2 dataset test sets.

Conclusion: SuperAD is effective for robust anomaly detection in real-world industrial applications.

Abstract: In this technical report, we present our solution to the CVPR 2025 Visual
Anomaly and Novelty Detection (VAND) 3.0 Workshop Challenge Track 1: Adapt &
Detect: Robust Anomaly Detection in Real-World Applications. In real-world
industrial anomaly detection, it is crucial to accurately identify anomalies
with physical complexity, such as transparent or reflective surfaces,
occlusions, and low-contrast contaminations. The recently proposed MVTec AD 2
dataset significantly narrows the gap between publicly available benchmarks and
anomalies found in real-world industrial environments. To address the
challenges posed by this dataset--such as complex and varying lighting
conditions and real anomalies with large scale differences--we propose a fully
training-free anomaly detection and segmentation method based on feature
extraction using the DINOv2 model named SuperAD. Our method carefully selects a
small number of normal reference images and constructs a memory bank by
leveraging the strong representational power of DINOv2. Anomalies are then
segmented by performing nearest neighbor matching between test image features
and the memory bank. Our method achieves competitive results on both test sets
of the MVTec AD 2 dataset.

</details>


### [452] [SAIL: Self-supervised Albedo Estimation from Real Images with a Latent Diffusion Model](https://arxiv.org/pdf/2505.19751)
*Hala Djeghim, Nathan Piasco, Luis Roldão, Moussab Bennehar, Dzmitry Tsishkou, Céline Loscos, Désiré Sidibé*

Main category: cs.CV

TL;DR: SAIL proposes a self-supervised method for intrinsic image decomposition using latent diffusion models to estimate albedo from real-world images, addressing limitations of synthetic data and inconsistent albedo maps.


<details>
  <summary>Details</summary>
Motivation: The challenge lies in the scarcity of labeled ground-truth data for real-world images, leading to poor generalization of supervised methods and inconsistent results from self-supervised approaches.

Method: SAIL leverages latent diffusion models for albedo estimation, introducing a novel latent-space decomposition and regularization terms for lighting-dependent and independent components.

Result: SAIL produces stable albedo maps under varying lighting conditions and generalizes well to multiple scenes using unlabeled multi-illumination data.

Conclusion: SAIL advances intrinsic image decomposition by combining latent diffusion models with self-supervised learning, offering a robust solution for real-world applications.

Abstract: Intrinsic image decomposition aims at separating an image into its underlying
albedo and shading components, isolating the base color from lighting effects
to enable downstream applications such as virtual relighting and scene editing.
Despite the rise and success of learning-based approaches, intrinsic image
decomposition from real-world images remains a significant challenging task due
to the scarcity of labeled ground-truth data. Most existing solutions rely on
synthetic data as supervised setups, limiting their ability to generalize to
real-world scenes. Self-supervised methods, on the other hand, often produce
albedo maps that contain reflections and lack consistency under different
lighting conditions. To address this, we propose SAIL, an approach designed to
estimate albedo-like representations from single-view real-world images. We
repurpose the prior knowledge of a latent diffusion model for unconditioned
scene relighting as a surrogate objective for albedo estimation. To extract the
albedo, we introduce a novel intrinsic image decomposition fully formulated in
the latent space. To guide the training of our latent diffusion model, we
introduce regularization terms that constrain both the lighting-dependent and
independent components of our latent image decomposition. SAIL predicts stable
albedo under varying lighting conditions and generalizes to multiple scenes,
using only unlabeled multi-illumination data available online.

</details>


### [453] [Dynamic-I2V: Exploring Image-to-Video Generation Models via Multimodal LLM](https://arxiv.org/pdf/2505.19901)
*Peng Liu, Xiaoming Ren, Fengkai Liu, Qingsong Xie, Quanlong Zheng, Yanhao Zhang, Haonan Lu, Yujiu Yang*

Main category: cs.CV

TL;DR: Dynamic-I2V improves image-to-video generation by integrating MLLMs for better motion control and coherence, and introduces DIVE for unbiased dynamic quality assessment.


<details>
  <summary>Details</summary>
Motivation: Address challenges in complex scene understanding and motion control in I2V generation.

Method: Integrates MLLMs with a diffusion transformer (DiT) for joint visual-textual encoding.

Result: Achieves 42.5%, 7.9%, and 11.8% improvements in dynamic range, controllability, and quality.

Conclusion: Dynamic-I2V sets a new benchmark in I2V generation with superior performance and evaluation.

Abstract: Recent advancements in image-to-video (I2V) generation have shown promising
performance in conventional scenarios. However, these methods still encounter
significant challenges when dealing with complex scenes that require a deep
understanding of nuanced motion and intricate object-action relationships. To
address these challenges, we present Dynamic-I2V, an innovative framework that
integrates Multimodal Large Language Models (MLLMs) to jointly encode visual
and textual conditions for a diffusion transformer (DiT) architecture. By
leveraging the advanced multimodal understanding capabilities of MLLMs, our
model significantly improves motion controllability and temporal coherence in
synthesized videos. The inherent multimodality of Dynamic-I2V further enables
flexible support for diverse conditional inputs, extending its applicability to
various downstream generation tasks. Through systematic analysis, we identify a
critical limitation in current I2V benchmarks: a significant bias towards
favoring low-dynamic videos, stemming from an inadequate balance between motion
complexity and visual quality metrics. To resolve this evaluation gap, we
propose DIVE - a novel assessment benchmark specifically designed for
comprehensive dynamic quality measurement in I2V generation. In conclusion,
extensive quantitative and qualitative experiments confirm that Dynamic-I2V
attains state-of-the-art performance in image-to-video generation, particularly
revealing significant improvements of 42.5%, 7.9%, and 11.8% in dynamic range,
controllability, and quality, respectively, as assessed by the DIVE metric in
comparison to existing methods.

</details>


### [454] [EmoNet-Face: An Expert-Annotated Benchmark for Synthetic Emotion Recognition](https://arxiv.org/pdf/2505.20033)
*Christoph Schuhmann, Robert Kaczmarczyk, Gollam Rabby, Felix Friedrich, Maurice Kraus, Krishna Kalyan, Kourosh Nadi, Huu Nguyen, Kristian Kersting, Sören Auer*

Main category: cs.CV

TL;DR: EmoNet Face introduces a comprehensive benchmark suite for emotion recognition in AI, addressing limitations in current datasets and models with a detailed 40-category taxonomy, diverse datasets, and high-performance models.


<details>
  <summary>Details</summary>
Motivation: Current benchmarks for emotion recognition in AI are limited, lacking nuanced emotional states, demographic diversity, and controlled imagery, leading to potential biases.

Method: The authors propose EmoNet Face, featuring a 40-category emotion taxonomy, large-scale AI-generated datasets with balanced demographics, multi-expert annotations, and a high-performance model (EmpathicInsight-Face).

Result: EmpathicInsight-Face achieves human-expert-level performance on the benchmark, demonstrating the effectiveness of the proposed suite.

Conclusion: EmoNet Face provides a robust foundation for advancing AI systems' understanding of human emotions, addressing critical gaps in existing benchmarks.

Abstract: Effective human-AI interaction relies on AI's ability to accurately perceive
and interpret human emotions. Current benchmarks for vision and vision-language
models are severely limited, offering a narrow emotional spectrum that
overlooks nuanced states (e.g., bitterness, intoxication) and fails to
distinguish subtle differences between related feelings (e.g., shame vs.
embarrassment). Existing datasets also often use uncontrolled imagery with
occluded faces and lack demographic diversity, risking significant bias. To
address these critical gaps, we introduce EmoNet Face, a comprehensive
benchmark suite. EmoNet Face features: (1) A novel 40-category emotion
taxonomy, meticulously derived from foundational research to capture finer
details of human emotional experiences. (2) Three large-scale, AI-generated
datasets (EmoNet HQ, Binary, and Big) with explicit, full-face expressions and
controlled demographic balance across ethnicity, age, and gender. (3) Rigorous,
multi-expert annotations for training and high-fidelity evaluation. (4) We
built EmpathicInsight-Face, a model achieving human-expert-level performance on
our benchmark. The publicly released EmoNet Face suite - taxonomy, datasets,
and model - provides a robust foundation for developing and evaluating AI
systems with a deeper understanding of human emotions.

</details>


### [455] [OpenS2V-Nexus: A Detailed Benchmark and Million-Scale Dataset for Subject-to-Video Generation](https://arxiv.org/pdf/2505.20292)
*Shenghai Yuan, Xianyi He, Yufan Deng, Yang Ye, Jinfa Huang, Bin Lin, Chongyang Ma, Jiebo Luo, Li Yuan*

Main category: cs.CV

TL;DR: OpenS2V-Nexus introduces a benchmark (OpenS2V-Eval) and dataset (OpenS2V-5M) for Subject-to-Video (S2V) generation, focusing on subject consistency, naturalness, and text relevance. It evaluates 16 S2V models and provides a large-scale dataset to advance S2V research.


<details>
  <summary>Details</summary>
Motivation: To enhance flexibility in video production by ensuring subject consistency and naturalness in generated videos, addressing gaps in existing benchmarks.

Method: Proposes OpenS2V-Eval (180 prompts, real/synthetic data) with three metrics (NexusScore, NaturalScore, GmeScore) and OpenS2V-5M (5M subject-text-video triples) for diverse subject-information.

Result: Comprehensive evaluation of 16 S2V models, revealing strengths/weaknesses. OpenS2V-5M provides a high-quality, diverse dataset for S2V research.

Conclusion: OpenS2V-Nexus offers a robust infrastructure to accelerate S2V generation research by addressing key challenges in subject consistency and dataset diversity.

Abstract: Subject-to-Video (S2V) generation aims to create videos that faithfully
incorporate reference content, providing enhanced flexibility in the production
of videos. To establish the infrastructure for S2V generation, we propose
OpenS2V-Nexus, consisting of (i) OpenS2V-Eval, a fine-grained benchmark, and
(ii) OpenS2V-5M, a million-scale dataset. In contrast to existing S2V
benchmarks inherited from VBench that focus on global and coarse-grained
assessment of generated videos, OpenS2V-Eval focuses on the model's ability to
generate subject-consistent videos with natural subject appearance and identity
fidelity. For these purposes, OpenS2V-Eval introduces 180 prompts from seven
major categories of S2V, which incorporate both real and synthetic test data.
Furthermore, to accurately align human preferences with S2V benchmarks, we
propose three automatic metrics, NexusScore, NaturalScore and GmeScore, to
separately quantify subject consistency, naturalness, and text relevance in
generated videos. Building on this, we conduct a comprehensive evaluation of 16
representative S2V models, highlighting their strengths and weaknesses across
different content. Moreover, we create the first open-source large-scale S2V
generation dataset OpenS2V-5M, which consists of five million high-quality 720P
subject-text-video triples. Specifically, we ensure subject-information
diversity in our dataset by (1) segmenting subjects and building pairing
information via cross-video associations and (2) prompting GPT-Image-1 on raw
frames to synthesize multi-view representations. Through OpenS2V-Nexus, we
deliver a robust infrastructure to accelerate future S2V generation research.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [456] [Multi-Modal Artificial Intelligence of Embryo Grading and Pregnancy Prediction in Assisted Reproductive Technology: A Review](https://arxiv.org/pdf/2505.20306)
*Xueqiang Ouyang, Jia Wei*

Main category: cs.AI

TL;DR: AI in assisted reproductive tech improves embryo grading and pregnancy prediction by analyzing multi-modal data, addressing challenges like subjectivity and data scarcity.


<details>
  <summary>Details</summary>
Motivation: Infertility is a global issue, and traditional IVF-embryo transfer methods face challenges like subjective embryo grading and inefficient multi-modal data integration. AI offers a solution.

Method: The paper reviews AI applications in embryo grading and pregnancy prediction using multi-modal data (static images, time-lapse videos, structured data).

Result: AI shows promise in enhancing IVF success by better integrating and analyzing diverse data types.

Conclusion: While AI improves IVF outcomes, challenges like multi-modal fusion complexity and data scarcity remain.

Abstract: As a global disease, infertility has always affected human beings. The
development of assisted reproductive technology can effectively solve this
disease. However, the traditional in vitro fertilization-embryo transfer
technology still faces many challenges in improving the success rate of
pregnancy, such as the subjectivity of embryo grading and the inefficiency of
integrating multi-modal data. Therefore, the introduction of artificial
intelligence-based technologies is particularly crucial. This article reviews
the application progress of multi-modal artificial intelligence in embryo
grading and pregnancy prediction based on different data modalities (including
static images, time-lapse videos and structured table data) from a new
perspective, and discusses the main challenges in current research, such as the
complexity of multi-modal information fusion and data scarcity.

</details>


### [457] [Manalyzer: End-to-end Automated Meta-analysis with Multi-agent System](https://arxiv.org/pdf/2505.20310)
*Wanghan Xu, Wenlong Zhang, Fenghua Ling, Ben Fei, Yusong Hu, Fangxuan Ren, Jintai Lin, Wanli Ouyang, Lei Bai*

Main category: cs.AI

TL;DR: Manalyzer is a multi-agent system automating meta-analysis, addressing LLM hallucinations with hybrid review and hierarchical extraction, outperforming baselines on a new benchmark.


<details>
  <summary>Details</summary>
Motivation: Traditional meta-analysis is labor-intensive, and LLM-based methods struggle with hallucinations in screening and extraction.

Method: Proposes Manalyzer, using tool calls for automation, with strategies like hybrid review and self-proving to reduce hallucinations.

Result: Manalyzer shows significant improvements over LLM baselines on a benchmark of 729 papers with 10,000+ data points.

Conclusion: Manalyzer effectively automates meta-analysis, mitigating key LLM challenges and enhancing performance.

Abstract: Meta-analysis is a systematic research methodology that synthesizes data from
multiple existing studies to derive comprehensive conclusions. This approach
not only mitigates limitations inherent in individual studies but also
facilitates novel discoveries through integrated data analysis. Traditional
meta-analysis involves a complex multi-stage pipeline including literature
retrieval, paper screening, and data extraction, which demands substantial
human effort and time. However, while LLM-based methods can accelerate certain
stages, they still face significant challenges, such as hallucinations in paper
screening and data extraction. In this paper, we propose a multi-agent system,
Manalyzer, which achieves end-to-end automated meta-analysis through tool
calls. The hybrid review, hierarchical extraction, self-proving, and feedback
checking strategies implemented in Manalyzer significantly alleviate these two
hallucinations. To comprehensively evaluate the performance of meta-analysis,
we construct a new benchmark comprising 729 papers across 3 domains,
encompassing text, image, and table modalities, with over 10,000 data points.
Extensive experiments demonstrate that Manalyzer achieves significant
performance improvements over the LLM baseline in multi meta-analysis tasks.
Project page: https://black-yt.github.io/meta-analysis-page/ .

</details>


### [458] [Reasoning in Neurosymbolic AI](https://arxiv.org/pdf/2505.20313)
*Son Tran, Edjard Mota, Artur d'Avila Garcez*

Main category: cs.AI

TL;DR: The paper discusses a simple energy-based neurosymbolic AI system for representing and reasoning about propositional logic, combining learning and formal reasoning. It highlights challenges of LLMs and evaluates the system's performance.


<details>
  <summary>Details</summary>
Motivation: To integrate reasoning and learning in neural networks, addressing data efficiency, fairness, and safety issues in AI, particularly with LLMs.

Method: Describes an energy-based neurosymbolic system using Restricted Boltzmann Machines (RBMs) for logical reasoning and evaluates its performance.

Result: The system effectively combines learning and reasoning, showing promise for addressing LLM challenges and promoting neural networks for logical reasoning.

Conclusion: Neurosymbolic AI should be positioned within formal reasoning and accountability frameworks to tackle deep learning reliability issues.

Abstract: Knowledge representation and reasoning in neural networks have been a
long-standing endeavor which has attracted much attention recently. The
principled integration of reasoning and learning in neural networks is a main
objective of the area of neurosymbolic Artificial Intelligence (AI). In this
chapter, a simple energy-based neurosymbolic AI system is described that can
represent and reason formally about any propositional logic formula. This
creates a powerful combination of learning from data and knowledge and logical
reasoning. We start by positioning neurosymbolic AI in the context of the
current AI landscape that is unsurprisingly dominated by Large Language Models
(LLMs). We identify important challenges of data efficiency, fairness and
safety of LLMs that might be addressed by neurosymbolic reasoning systems with
formal reasoning capabilities. We then discuss the representation of logic by
the specific energy-based system, including illustrative examples and empirical
evaluation of the correspondence between logical reasoning and energy
minimization using Restricted Boltzmann Machines (RBM). Learning from data and
knowledge is also evaluated empirically and compared with a symbolic, neural
and a neurosymbolic system. Results reported in this chapter in an accessible
way are expected to reignite the research on the use of neural networks as
massively-parallel models for logical reasoning and promote the principled
integration of reasoning and learning in deep networks. We conclude the chapter
with a discussion of the importance of positioning neurosymbolic AI within a
broader framework of formal reasoning and accountability in AI, discussing the
challenges for neurosynbolic AI to tackle the various known problems of
reliability of deep learning.

</details>


### [459] [Reinforcement Speculative Decoding for Fast Ranking](https://arxiv.org/pdf/2505.20316)
*Yingpeng Du, Tianjun Wei, Zhu Sun, Jie Zhang*

Main category: cs.AI

TL;DR: Proposes Reinforcement Speculative Decoding for fast LLM ranking inference, addressing latency and knowledge retention issues in speculative decoding.


<details>
  <summary>Details</summary>
Motivation: Current speculative decoding methods for LLM ranking systems face latency and knowledge retention challenges, degrading performance in tail positions.

Method: Introduces an up-to-down decoding paradigm with a reinforcement learning-based agent to iteratively modify ranking sequences under constrained budgets, leveraging listwise ranking knowledge.

Result: Effective in IR and RS tasks, demonstrating improved performance and robustness.

Conclusion: The method successfully addresses latency and knowledge retention issues, enhancing LLM ranking inference.

Abstract: Large Language Models (LLMs) have been widely adopted in ranking systems such
as information retrieval (IR) systems and recommender systems (RSs). To
alleviate the latency of auto-regressive decoding, some studies explore the
single (first) token decoding for ranking approximation, but they suffer from
severe degradation in tail positions. Although speculative decoding (SD)
methods can be a remedy with verification at different positions, they face
challenges in ranking systems due to their left-to-right decoding paradigm.
Firstly, ranking systems require strict latency constraints, but verification
rounds in SD methods remain agnostic; Secondly, SD methods usually discard
listwise ranking knowledge about unaccepted items in previous rounds, hindering
future multi-token prediction, especially when candidate tokens are the
unaccepted items. In this paper, we propose a Reinforcement Speculative
Decoding method for fast ranking inference of LLMs. To meet the ranking
systems' latency requirement, we propose an up-to-down decoding paradigm that
employs an agent to iteratively modify the ranking sequence under a constrained
budget. Specifically, we design a ranking-tailored policy optimization,
actively exploring optimal multi-round ranking modification policy verified by
LLMs via reinforcement learning (RL). To better approximate the target LLM
under the constrained budget, we trigger the agent fully utilizing the listwise
ranking knowledge about all items verified by LLMs across different rounds in
RL, enhancing the modification policy of the agent. More importantly, we
demonstrate the theoretical robustness and advantages of our paradigm and
implementation. Experiments on both IR and RS tasks show the effectiveness of
our proposed method.

</details>


### [460] [Reconceptualizing Smart Microscopy: From Data Collection to Knowledge Creation by Multi-Agent Integration](https://arxiv.org/pdf/2505.20466)
*P. S. Kesavan, Pontus Nordenfelt*

Main category: cs.AI

TL;DR: Smart microscopy is redefined as an active partner in scientific inquiry, bridging the gap between observation and understanding with six design principles and a multi-agent architecture.


<details>
  <summary>Details</summary>
Motivation: To address the 'epistemic-empirical divide' in cellular investigation by transforming microscopy from passive tools to active collaborators.

Method: Proposes a theoretical framework with six core design principles and a multi-agent architecture for adaptive, goal-aligned microscopy systems.

Result: A roadmap for microscopy systems that support hypothesis generation, insight discovery, and theory development.

Conclusion: Smart microscopy can redefine scientific instruments' role in knowledge creation by actively supporting scientific understanding.

Abstract: Smart microscopy represents a paradigm shift in biological imaging, moving
from passive observation tools to active collaborators in scientific inquiry.
Enabled by advances in automation, computational power, and artificial
intelligence, these systems are now capable of adaptive decision-making and
real-time experimental control. Here, we introduce a theoretical framework that
reconceptualizes smart microscopy as a partner in scientific investigation.
Central to our framework is the concept of the 'epistemic-empirical divide' in
cellular investigation-the gap between what is observable (empirical domain)
and what must be understood (epistemic domain). We propose six core design
principles: epistemic-empirical awareness, hierarchical context integration, an
evolution from detection to perception, adaptive measurement frameworks,
narrative synthesis capabilities, and cross-contextual reasoning. Together,
these principles guide a multi-agent architecture designed to align empirical
observation with the goals of scientific understanding. Our framework provides
a roadmap for building microscopy systems that go beyond automation to actively
support hypothesis generation, insight discovery, and theory development,
redefining the role of scientific instruments in the process of knowledge
creation.

</details>


### [461] [Challenges for artificial cognitive systems](https://arxiv.org/pdf/2505.20339)
*Antoni Gomila, Vincent C. Müller*

Main category: cs.AI

TL;DR: The paper aims to define challenges for artificial cognitive systems to guide progress in cognitive systems research.


<details>
  <summary>Details</summary>
Motivation: To address the lack of clear challenges or goals in cognitive systems research, inspired by the EUCogII project.

Method: Formulates challenges based on a definition of cognitive systems as learning, flexible, goal-driven entities.

Result: Articulated challenges for artificial cognitive systems to guide future research.

Conclusion: The paper provides a framework for defining progress in cognitive systems research through clear challenges.

Abstract: The declared goal of this paper is to fill this gap: "... cognitive systems
research needs questions or challenges that define progress. The challenges are
not (yet more) predictions of the future, but a guideline to what are the aims
and what would constitute progress." -- the quotation being from the project
description of EUCogII, the project for the European Network for Cognitive
Systems within which this formulation of the 'challenges' was originally
developed (http://www.eucognition.org). So, we stick out our neck and formulate
the challenges for artificial cognitive systems. These challenges are
articulated in terms of a definition of what a cognitive system is: a system
that learns from experience and uses its acquired knowledge (both declarative
and practical) in a flexible manner to achieve its own goals.

</details>


### [462] [Machine Theory of Mind and the Structure of Human Values](https://arxiv.org/pdf/2505.20342)
*Paul de Font-Reaulx*

Main category: cs.AI

TL;DR: The paper addresses the value generalization problem in AI, proposing that human values have a generative rational structure and can be inferred using Bayesian Theory of Mind models.


<details>
  <summary>Details</summary>
Motivation: Human values are complex and not fully demonstrated through actions, creating a challenge for AI to generalize values from limited samples.

Method: Uses Bayesian Theory of Mind models to infer human values from both behavior and other values, moving beyond simple utility functions.

Result: Demonstrates that generative value-to-value inference can solve the value generalization problem.

Conclusion: Generative value-to-value inference is essential for scalable machine theory of mind.

Abstract: Value learning is a crucial aspect of safe and ethical AI. This is primarily
pursued by methods inferring human values from behaviour. However, humans care
about much more than we are able to demonstrate through our actions.
Consequently, an AI must predict the rest of our seemingly complex values from
a limited sample. I call this the value generalization problem. In this paper,
I argue that human values have a generative rational structure and that this
allows us to solve the value generalization problem. In particular, we can use
Bayesian Theory of Mind models to infer human values not only from behaviour,
but also from other values. This has been obscured by the widespread use of
simple utility functions to represent human values. I conclude that developing
generative value-to-value inference is a crucial component of achieving a
scalable machine theory of mind.

</details>


### [463] [SCAR: Shapley Credit Assignment for More Efficient RLHF](https://arxiv.org/pdf/2505.20417)
*Meng Cao, Shuyuan Zhang, Xiao-Wen Chang, Doina Precup*

Main category: cs.AI

TL;DR: SCAR introduces Shapley values to distribute sequence-level rewards in RLHF, improving credit assignment and alignment efficiency without extra models or annotations.


<details>
  <summary>Details</summary>
Motivation: RLHF struggles with sparse rewards, making it hard to assign credit to specific tokens or spans. SCAR addresses this by providing dense rewards based on principled contributions.

Method: SCAR uses Shapley values from game theory to distribute sequence-level rewards among tokens or spans, ensuring fair credit assignment without auxiliary models.

Result: SCAR converges faster and achieves higher rewards than standard RLHF and attention-based baselines in tasks like sentiment control and summarization.

Conclusion: SCAR offers a theoretically sound and efficient method for credit assignment in RLHF, enhancing LLM alignment.

Abstract: Reinforcement Learning from Human Feedback (RLHF) is a widely used technique
for aligning Large Language Models (LLMs) with human preferences, yet it often
suffers from sparse reward signals, making effective credit assignment
challenging. In typical setups, the reward model provides a single scalar score
for an entire generated sequence, offering little insight into which token or
span-level decisions were responsible for the outcome. To address this, we
propose Shapley Credit Assignment Rewards (SCAR), a novel method that leverages
Shapley values in cooperative game theory. SCAR distributes the total
sequence-level reward among constituent tokens or text spans based on their
principled marginal contributions. This creates dense reward signals,
crucially, without necessitating the training of auxiliary critique models or
recourse to fine-grained human annotations at intermediate generation stages.
Unlike prior dense reward methods, SCAR offers a game-theoretic foundation for
fair credit attribution. Theoretically, we demonstrate that SCAR preserves the
original optimal policy, and empirically, across diverse tasks including
sentiment control, text summarization, and instruction tuning, we show that
SCAR converges significantly faster and achieves higher final reward scores
compared to standard RLHF and attention-based dense reward baselines. Our
findings suggest that SCAR provides a more effective and theoretically sound
method for credit assignment in RLHF, leading to more efficient alignment of
LLMs.

</details>


### [464] [Project Riley: Multimodal Multi-Agent LLM Collaboration with Emotional Reasoning and Voting](https://arxiv.org/pdf/2505.20521)
*Ana Rita Ortigoso, Gabriel Vieira, Daniel Fuentes, Luis Frazão, Nuno Costa, António Pereira*

Main category: cs.AI

TL;DR: Project Riley is a multimodal AI architecture simulating emotion-influenced reasoning, using five emotional agents for dialogue. It integrates LLMs and was tested for emotional alignment and clarity.


<details>
  <summary>Details</summary>
Motivation: Inspired by Pixar's Inside Out, the project aims to simulate reasoning influenced by emotional states, enhancing AI's emotional expressiveness and contextual adaptability.

Method: The system employs five emotional agents (Joy, Sadness, Fear, Anger, Disgust) in structured dialogues, using textual/visual LLMs and reasoning processes. A prototype was tested for emotional appropriateness and clarity.

Result: User testing showed strong performance in emotional alignment and communicative clarity, particularly in structured scenarios.

Conclusion: Project Riley demonstrates effective emotion-influenced reasoning, with potential applications in emotionally calibrated AI interactions.

Abstract: This paper presents Project Riley, a novel multimodal and multi-model
conversational AI architecture oriented towards the simulation of reasoning
influenced by emotional states. Drawing inspiration from Pixar's Inside Out,
the system comprises five distinct emotional agents - Joy, Sadness, Fear,
Anger, and Disgust - that engage in structured multi-round dialogues to
generate, criticise, and iteratively refine responses. A final reasoning
mechanism synthesises the contributions of these agents into a coherent output
that either reflects the dominant emotion or integrates multiple perspectives.
The architecture incorporates both textual and visual large language models
(LLMs), alongside advanced reasoning and self-refinement processes. A
functional prototype was deployed locally in an offline environment, optimised
for emotional expressiveness and computational efficiency. From this initial
prototype, another one emerged, called Armando, which was developed for use in
emergency contexts, delivering emotionally calibrated and factually accurate
information through the integration of Retrieval-Augmented Generation (RAG) and
cumulative context tracking. The Project Riley prototype was evaluated through
user testing, in which participants interacted with the chatbot and completed a
structured questionnaire assessing three dimensions: Emotional Appropriateness,
Clarity and Utility, and Naturalness and Human-likeness. The results indicate
strong performance in structured scenarios, particularly with respect to
emotional alignment and communicative clarity.

</details>


### [465] [Scaling over Scaling: Exploring Test-Time Scaling Pareto in Large Reasoning Models](https://arxiv.org/pdf/2505.20522)
*Jian Wang, Boyan Zhu, Chak Tou Leong, Yongqi Li, Wenjie Li*

Main category: cs.AI

TL;DR: The paper explores the limits of test-time scaling for large reasoning models (LRMs), introducing the TTSPM to analyze and optimize resource allocation. It identifies saturation points for parallel and sequential scaling, showing diminishing returns beyond these thresholds.


<details>
  <summary>Details</summary>
Motivation: To systematically understand the practical limits of test-time scaling in LRMs and optimize resource allocation for enhanced reasoning capabilities.

Method: Theoretical analysis of parallel and sequential scaling paradigms, derivation of saturation points, and empirical validation on benchmarks (AIME, MATH-500, GPQA).

Result: Identified saturation points for scaling budgets, showing diminishing returns beyond thresholds. Both paradigms share a unified mathematical structure in upper bounds.

Conclusion: Provides insights into cost-benefit trade-offs of test-time scaling, guiding resource-efficient inference strategies for LRMs.

Abstract: Large reasoning models (LRMs) have exhibited the capacity of enhancing
reasoning performance via internal test-time scaling. Building upon this, a
promising direction is to further scale test-time compute to unlock even
greater reasoning capabilities. However, as we push these scaling boundaries,
systematically understanding the practical limits and achieving optimal
resource allocation becomes a critical challenge. In this paper, we investigate
the scaling Pareto of test-time scaling and introduce the Test-Time Scaling
Performance Model (TTSPM). We theoretically analyze two fundamental paradigms
for such extended scaling, parallel scaling and sequential scaling, from a
probabilistic modeling perspective. Our primary contribution is the derivation
of the saturation point on the scaling budget for both strategies, identifying
thresholds beyond which additional computation yields diminishing returns.
Remarkably, despite their distinct mechanisms, both paradigms converge to a
unified mathematical structure in their upper bounds. We empirically validate
our theoretical findings on challenging reasoning benchmarks, including AIME,
MATH-500, and GPQA, demonstrating the practical utility of these bounds for
test-time resource allocation. We hope that this work provides insights into
the cost-benefit trade-offs of test-time scaling, guiding the development of
more resource-efficient inference strategies for large reasoning models.

</details>


### [466] [Learning Individual Behavior in Agent-Based Models with Graph Diffusion Networks](https://arxiv.org/pdf/2505.21426)
*Francesco Cozzi, Marco Pangallo, Alan Perotti, André Panisson, Corrado Monti*

Main category: cs.AI

TL;DR: A novel framework combines diffusion models and graph neural networks to create differentiable surrogates for Agent-Based Models (ABMs), enabling gradient-based optimization and real-world data integration.


<details>
  <summary>Details</summary>
Motivation: ABMs are limited by non-differentiable rules, restricting gradient-based optimization and real-world data integration.

Method: The framework uses diffusion models for behavioral stochasticity and graph neural networks for agent interactions, modeling individual agent behavior directly.

Result: Validated on Schelling's segregation model and a Predator-Prey ecosystem, the method replicates individual-level patterns and forecasts emergent dynamics accurately.

Conclusion: The approach demonstrates the potential of diffusion models and graph learning for data-driven ABM simulation.

Abstract: Agent-Based Models (ABMs) are powerful tools for studying emergent properties
in complex systems. In ABMs, agent behaviors are governed by local interactions
and stochastic rules. However, these rules are, in general, non-differentiable,
limiting the use of gradient-based methods for optimization, and thus
integration with real-world data. We propose a novel framework to learn a
differentiable surrogate of any ABM by observing its generated data. Our method
combines diffusion models to capture behavioral stochasticity and graph neural
networks to model agent interactions. Distinct from prior surrogate approaches,
our method introduces a fundamental shift: rather than approximating
system-level outputs, it models individual agent behavior directly, preserving
the decentralized, bottom-up dynamics that define ABMs. We validate our
approach on two ABMs (Schelling's segregation model and a Predator-Prey
ecosystem) showing that it replicates individual-level patterns and accurately
forecasts emergent dynamics beyond training. Our results demonstrate the
potential of combining diffusion models and graph learning for data-driven ABM
simulation.

</details>


### [467] [Comparisons between a Large Language Model-based Real-Time Compound Diagnostic Medical AI Interface and Physicians for Common Internal Medicine Cases using Simulated Patients](https://arxiv.org/pdf/2505.20609)
*Hyungjun Park, Chang-Yun Woo, Seungjo Lim, Seunghwan Lim, Keunho Kwak, Ju Young Jeong, Chong Hyun Suh*

Main category: cs.AI

TL;DR: An LLM-based AI interface for medical diagnostics outperformed physicians in accuracy and efficiency, with lower costs, though patient satisfaction was slightly lower.


<details>
  <summary>Details</summary>
Motivation: To evaluate the potential of an AI interface in improving diagnostic accuracy and efficiency for common internal medicine cases compared to physicians.

Method: A nonrandomized clinical trial with physicians and simulated patients using USMLE Step 2 CS-style cases. The AI's accuracy, repeatability, time, cost, and patient satisfaction were measured.

Result: The AI achieved 80% accuracy (vs. 50-70% for physicians), 100% for top two diagnoses, 44.6% faster, and 98.1% cheaper. Patient satisfaction was slightly lower (3.9 vs. 4.2-4.3).

Conclusion: The AI interface shows promise for assisting primary care, offering comparable accuracy, higher efficiency, and lower costs, though patient satisfaction needs improvement.

Abstract: Objective To develop an LLM based realtime compound diagnostic medical AI
interface and performed a clinical trial comparing this interface and
physicians for common internal medicine cases based on the United States
Medical License Exam (USMLE) Step 2 Clinical Skill (CS) style exams. Methods A
nonrandomized clinical trial was conducted on August 20, 2024. We recruited one
general physician, two internal medicine residents (2nd and 3rd year), and five
simulated patients. The clinical vignettes were adapted from the USMLE Step 2
CS style exams. We developed 10 representative internal medicine cases based on
actual patients and included information available on initial diagnostic
evaluation. Primary outcome was the accuracy of the first differential
diagnosis. Repeatability was evaluated based on the proportion of agreement.
Results The accuracy of the physicians' first differential diagnosis ranged
from 50% to 70%, whereas the realtime compound diagnostic medical AI interface
achieved an accuracy of 80%. The proportion of agreement for the first
differential diagnosis was 0.7. The accuracy of the first and second
differential diagnoses ranged from 70% to 90% for physicians, whereas the AI
interface achieved an accuracy rate of 100%. The average time for the AI
interface (557 sec) was 44.6% shorter than that of the physicians (1006 sec).
The AI interface ($0.08) also reduced costs by 98.1% compared to the
physicians' average ($4.2). Patient satisfaction scores ranged from 4.2 to 4.3
for care by physicians and were 3.9 for the AI interface Conclusion An LLM
based realtime compound diagnostic medical AI interface demonstrated diagnostic
accuracy and patient satisfaction comparable to those of a physician, while
requiring less time and lower costs. These findings suggest that AI interfaces
may have the potential to assist primary care consultations for common internal
medicine cases.

</details>


### [468] [CoderAgent: Simulating Student Behavior for Personalized Programming Learning with Large Language Models](https://arxiv.org/pdf/2505.20642)
*Yi Zhan, Qi Liu, Weibo Gao, Zheng Zhang, Tianfu Wang, Shuanghong Shen, Junyu Lu, Zhenya Huang*

Main category: cs.AI

TL;DR: The paper proposes CoderAgent, an LLM-based agent, to simulate fine-grained programming learning processes without real data, addressing gaps in interpretability and granularity.


<details>
  <summary>Details</summary>
Motivation: The lack of quality programming data and the mismatch between offline evaluation and real-world learning hinder personalized tutoring systems.

Method: CoderAgent, inspired by ACT-R, captures cognitive states and uses the Programming Tree of Thought (PTOT) for detailed problem-solving analysis.

Result: Experiments show CoderAgent provides interpretable insights and accurate simulations of learning trajectories.

Conclusion: CoderAgent advances personalized programming education by simulating human-like learning processes.

Abstract: Personalized programming tutoring, such as exercise recommendation, can
enhance learners' efficiency, motivation, and outcomes, which is increasingly
important in modern digital education. However, the lack of sufficient and
high-quality programming data, combined with the mismatch between offline
evaluation and real-world learning, hinders the practical deployment of such
systems. To address this challenge, many approaches attempt to simulate learner
practice data, yet they often overlook the fine-grained, iterative nature of
programming learning, resulting in a lack of interpretability and granularity.
To fill this gap, we propose a LLM-based agent, CoderAgent, to simulate
students' programming processes in a fine-grained manner without relying on
real data. Specifically, we equip each human learner with an intelligent agent,
the core of which lies in capturing the cognitive states of the human
programming practice process. Inspired by ACT-R, a cognitive architecture
framework, we design the structure of CoderAgent to align with human cognitive
architecture by focusing on the mastery of programming knowledge and the
application of coding ability. Recognizing the inherent patterns in
multi-layered cognitive reasoning, we introduce the Programming Tree of Thought
(PTOT), which breaks down the process into four steps: why, how, where, and
what. This approach enables a detailed analysis of iterative problem-solving
strategies. Finally, experimental evaluations on real-world datasets
demonstrate that CoderAgent provides interpretable insights into learning
trajectories and achieves accurate simulations, paving the way for personalized
programming education.

</details>


### [469] [AutoReproduce: Automatic AI Experiment Reproduction with Paper Lineage](https://arxiv.org/pdf/2505.20662)
*Xuanle Zhao, Zilin Sang, Yuxuan Li, Qi Shi, Shuo Wang, Duzhen Zhang, Xu Han, Zhiyuan Liu, Maosong Sun*

Main category: cs.AI

TL;DR: AutoReproduce is a multi-agent framework that automates experiment reproduction in AI research by extracting implicit knowledge from cited references, outperforming baselines by up to 70%.


<details>
  <summary>Details</summary>
Motivation: Efficient experiment reproduction is crucial for AI progress, but automation is hindered by implicit domain knowledge not documented in papers.

Method: The paper introduces the paper lineage algorithm to extract implicit knowledge and proposes AutoReproduce, a multi-agent framework generating unit tests for reproducibility.

Result: AutoReproduce outperforms baselines by up to 70%, achieving a 22.1% performance gap on 89.74% of executable runs compared to official implementations.

Conclusion: AutoReproduce effectively automates experiment reproduction, enhancing reproducibility and execution fidelity in AI research.

Abstract: Efficient experiment reproduction is critical to accelerating progress in
artificial intelligence. However, the inherent complexity of method design and
training procedures presents substantial challenges for automation. Notably,
reproducing experiments often requires implicit domain-specific knowledge not
explicitly documented in the original papers. To address this, we introduce the
paper lineage algorithm, which identifies and extracts implicit knowledge from
the relevant references cited by the target paper. Building on this idea, we
propose AutoReproduce, a multi-agent framework capable of automatically
reproducing experiments described in research papers in an end-to-end manner.
AutoReproduce enhances code executability by generating unit tests alongside
the reproduction process. To evaluate the reproduction capability, we construct
ReproduceBench, a benchmark annotated with verified implementations, and
introduce novel evaluation metrics to assess both the reproduction and
execution fidelity. Experimental results demonstrate that AutoReproduce
outperforms the existing strong agent baselines on all five evaluation metrics
by a peak margin of over $70\%$. In particular, compared to the official
implementations, AutoReproduce achieves an average performance gap of $22.1\%$
on $89.74\%$ of the executable experiment runs. The code will be available at
https://github.com/AI9Stars/AutoReproduce.

</details>


### [470] [MIRROR: Multi-agent Intra- and Inter-Reflection for Optimized Reasoning in Tool Learning](https://arxiv.org/pdf/2505.20670)
*Zikang Guo, Benfeng Xu, Xiaorui Wang, Zhendong Mao*

Main category: cs.AI

TL;DR: MIRROR introduces a framework for LLMs to reflect on actions both before and after execution, improving error correction and performance in multi-agent workflows.


<details>
  <summary>Details</summary>
Motivation: Existing approaches only use reflection post-action, missing opportunities to prevent errors before execution. MIRROR aims to enhance LLM performance by incorporating pre-action reflection.

Method: MIRROR combines intra-reflection (pre-action assessment) and inter-reflection (post-action adjustment) to systematically correct errors in agentic workflows.

Result: MIRROR achieves state-of-the-art results on StableToolBench and TravelPlanner benchmarks, outperforming existing methods.

Conclusion: Incorporating reflection before and after action execution significantly improves LLM performance in complex tasks, making MIRROR a promising solution for multi-agent workflows.

Abstract: Complex tasks involving tool integration pose significant challenges for
Large Language Models (LLMs), leading to the emergence of multi-agent workflows
as a promising solution. Reflection has emerged as an effective strategy for
correcting erroneous trajectories in agentic workflows. However, existing
approaches only exploit such capability in the post-action stage, where the
agent observes the execution outcomes. We argue that, like humans, LLMs can
also engage in reflection before action execution: the agent can anticipate
undesirable outcomes from its own decisions, which not only provides a
necessarily complementary perspective to evaluate the decision but also
prevents the propagation of errors throughout the trajectory. In this paper, we
propose MIRROR, a framework that consists of both intra-reflection, which
critically assesses intended actions before execution, and inter-reflection,
which further adjusts the trajectory based on observations. This design
systematically leverages LLM reflection capabilities to eliminate and rectify
erroneous actions on a more comprehensive scope. Evaluations on both the
StableToolBench and TravelPlanner benchmarks demonstrate MIRROR's superior
performance, achieving state-of-the-art results compared to existing
approaches.

</details>


### [471] [LLM-Guided Reinforcement Learning: Addressing Training Bottlenecks through Policy Modulation](https://arxiv.org/pdf/2505.20671)
*Heng Tan, Hua Yan, Yu Yang*

Main category: cs.AI

TL;DR: A framework using LLMs to guide RL training by identifying critical states and providing action suggestions, outperforming existing methods.


<details>
  <summary>Details</summary>
Motivation: Addressing RL training challenges like local optima and scalability issues in existing approaches (automated refinement and human-in-the-loop).

Method: Leverage LLMs to identify critical states from trajectories, suggest actions, and assign implicit rewards for policy refinement.

Result: Outperforms state-of-the-art baselines in standard RL benchmarks.

Conclusion: LLM-based explanations effectively address RL training bottlenecks without additional training or human intervention.

Abstract: While reinforcement learning (RL) has achieved notable success in various
domains, training effective policies for complex tasks remains challenging.
Agents often converge to local optima and fail to maximize long-term rewards.
Existing approaches to mitigate training bottlenecks typically fall into two
categories: (i) Automated policy refinement, which identifies critical states
from past trajectories to guide policy updates, but suffers from costly and
uncertain model training; and (ii) Human-in-the-loop refinement, where human
feedback is used to correct agent behavior, but this does not scale well to
environments with large or continuous action spaces. In this work, we design a
large language model-guided policy modulation framework that leverages LLMs to
improve RL training without additional model training or human intervention. We
first prompt an LLM to identify critical states from a sub-optimal agent's
trajectories. Based on these states, the LLM then provides action suggestions
and assigns implicit rewards to guide policy refinement. Experiments across
standard RL benchmarks demonstrate that our method outperforms state-of-the-art
baselines, highlighting the effectiveness of LLM-based explanations in
addressing RL training bottlenecks.

</details>


### [472] [Many Heads Are Better Than One: Improved Scientific Idea Generation by A LLM-Based Multi-Agent System](https://arxiv.org/pdf/2410.09403)
*Haoyang Su, Renqi Chen, Shixiang Tang, Zhenfei Yin, Xinzhe Zheng, Jinzhe Li, Biqing Qi, Qi Wu, Hui Li, Wanli Ouyang, Philip Torr, Bowen Zhou, Nanqing Dong*

Main category: cs.AI

TL;DR: The paper introduces VirSci, an LLM-based multi-agent system mimicking scientific teamwork, outperforming state-of-the-art methods in generating novel research ideas.


<details>
  <summary>Details</summary>
Motivation: Current AI methods lack the collaborative nature of real-world scientific practices, limiting their effectiveness in knowledge discovery.

Method: Proposes VirSci, a multi-agent system where agents collaboratively generate, evaluate, and refine research ideas.

Result: VirSci outperforms existing methods in producing novel scientific ideas and reveals key collaboration mechanisms.

Conclusion: VirSci offers insights for future research and advances toward autonomous scientific discovery systems.

Abstract: The rapid advancement of scientific progress requires innovative tools that
can accelerate knowledge discovery. Although recent AI methods, particularly
large language models (LLMs), have shown promise in tasks such as hypothesis
generation and experimental design, they fall short of replicating the
collaborative nature of real-world scientific practices, where diverse experts
work together in teams to tackle complex problems. To address the limitations,
we propose an LLM-based multi-agent system, i.e., Virtual Scientists (VirSci),
designed to mimic the teamwork inherent in scientific research. VirSci
organizes a team of agents to collaboratively generate, evaluate, and refine
research ideas. Through comprehensive experiments, we demonstrate that this
multi-agent approach outperforms the state-of-the-art method in producing novel
scientific ideas. We further investigate the collaboration mechanisms that
contribute to its tendency to produce ideas with higher novelty, offering
valuable insights to guide future research and illuminating pathways toward
building a robust system for autonomous scientific discovery. The code is
available at https://github.com/open-sciencelab/Virtual-Scientists.

</details>


### [473] [GIFARC: Synthetic Dataset for Leveraging Human-Intuitive Analogies to Elevate AI Reasoning](https://arxiv.org/pdf/2505.20672)
*Woochang Sim, Hyunseok Ryu, Kyungmin Choi, Sungwon Han, Sundong Kim*

Main category: cs.AI

TL;DR: The paper introduces GIFARC, an analogy-inspired dataset for the ARC challenge, using LLMs and VLMs to create tasks with explicit analogies, improving AI performance by aligning it with human reasoning.


<details>
  <summary>Details</summary>
Motivation: To bridge the gap between AI and human-level reasoning on the ARC challenge by leveraging analogies from GIF images.

Method: Synthesize ARC-style tasks from GIFs with analogies, using LLMs and VLMs, and pair tasks with ground-truth analogies to guide AI problem-solving.

Result: GIFARC reduces problem complexity and aligns AI task-solving approaches with human analogical reasoning.

Conclusion: Embedding analogies into ARC tasks improves AI performance and makes solutions more human-understandable.

Abstract: The Abstraction and Reasoning Corpus (ARC) poses a stringent test of general
AI capabilities, requiring solvers to infer abstract patterns from only a
handful of examples. Despite substantial progress in deep learning,
state-of-the-art models still achieve accuracy rates of merely 40-55% on 2024
ARC Competition, indicative of a significant gap between their performance and
human-level reasoning. In this work, we seek to bridge that gap by introducing
an analogy-inspired ARC dataset, GIFARC. Leveraging large language models
(LLMs) and vision-language models (VLMs), we synthesize new ARC-style tasks
from a variety of GIF images that include analogies. Each new task is paired
with ground-truth analogy, providing an explicit mapping between visual
transformations and everyday concepts. By embedding robust human-intuitive
analogies into ARC-style tasks, GIFARC guides AI agents to evaluate the task
analogically before engaging in brute-force pattern search, thus efficiently
reducing problem complexity and build a more concise and human-understandable
solution. We empirically validate that guiding LLM with analogic approach with
GIFARC affects task-solving approaches of LLMs to align with analogic approach
of human.

</details>


### [474] [Jigsaw-Puzzles: From Seeing to Understanding to Reasoning in Vision-Language Models](https://arxiv.org/pdf/2505.20728)
*Zesen Lyu, Dandan Zhang, Wei Ye, Fangdi Li, Zhihang Jiang, Yao Yang*

Main category: cs.AI

TL;DR: The paper introduces Jigsaw-Puzzles, a benchmark to evaluate spatial reasoning in vision-language models (VLMs), revealing a significant performance gap between VLMs and humans.


<details>
  <summary>Details</summary>
Motivation: To assess whether VLMs possess human-like spatial reasoning capabilities, which are crucial for complex cognition and interaction with the physical world.

Method: A dataset of 1,100 complex real-world images is used to design five tasks evaluating spatial perception, structural understanding, and reasoning in 24 VLMs.

Result: Top-performing VLM (Gemini-2.5-Pro) achieves 77.14% accuracy, far below human performance (90+%), especially struggling with the Order Generation task (30% accuracy).

Conclusion: The gap highlights the need for further research, positioning Jigsaw-Puzzles as a valuable benchmark for advancing VLM spatial reasoning.

Abstract: Spatial reasoning is a core component of human cognition, enabling
individuals to perceive, comprehend, and interact with the physical world. It
relies on a nuanced understanding of spatial structures and inter-object
relationships, serving as the foundation for complex reasoning and
decision-making. To investigate whether current vision-language models (VLMs)
exhibit similar capability, we introduce Jigsaw-Puzzles, a novel benchmark
consisting of 1,100 carefully curated real-world images with high spatial
complexity. Based on this dataset, we design five tasks to rigorously evaluate
VLMs' spatial perception, structural understanding, and reasoning capabilities,
while deliberately minimizing reliance on domain-specific knowledge to better
isolate and assess the general spatial reasoning capability. We conduct a
comprehensive evaluation across 24 state-of-the-art VLMs. The results show that
even the strongest model, Gemini-2.5-Pro, achieves only 77.14% overall accuracy
and performs particularly poorly on the Order Generation task, with only 30.00%
accuracy, far below the performance exceeding 90% achieved by human
participants. This persistent gap underscores the need for continued progress,
positioning Jigsaw-Puzzles as a challenging and diagnostic benchmark for
advancing spatial reasoning research in VLMs.

</details>


### [475] [ReMA: Learning to Meta-think for LLMs with Multi-Agent Reinforcement Learning](https://arxiv.org/pdf/2503.09501)
*Ziyu Wan, Yunxiang Li, Xiaoyu Wen, Yan Song, Hanjing Wang, Linyi Yang, Mark Schmidt, Jun Wang, Weinan Zhang, Shuyue Hu, Ying Wen*

Main category: cs.AI

TL;DR: ReMA introduces a multi-agent reinforcement learning framework to enhance LLMs' meta-thinking, improving reasoning performance.


<details>
  <summary>Details</summary>
Motivation: Current single-agent approaches lack specialized designs for meta-thinking, leading to low efficacy in reasoning tasks.

Method: ReMA uses MARL to decouple reasoning into hierarchical agents: a meta-thinking agent for strategy and a reasoning agent for execution.

Result: ReMA outperforms single-agent baselines in complex reasoning tasks and extends to multi-turn settings efficiently.

Conclusion: ReMA demonstrates that meta-thinking enhances LLM reasoning, with insights into agent dynamics and improved generalization.

Abstract: Recent research on Reasoning of Large Language Models (LLMs) has sought to
further enhance their performance by integrating meta-thinking -- enabling
models to monitor, evaluate, and control their reasoning processes for more
adaptive and effective problem-solving. However, current single-agent work
lacks a specialized design for acquiring meta-thinking, resulting in low
efficacy. To address this challenge, we introduce Reinforced Meta-thinking
Agents (ReMA), a novel framework that leverages Multi-Agent Reinforcement
Learning (MARL) to elicit meta-thinking behaviors, encouraging LLMs to think
about thinking. ReMA decouples the reasoning process into two hierarchical
agents: a high-level meta-thinking agent responsible for generating strategic
oversight and plans, and a low-level reasoning agent for detailed executions.
Through iterative reinforcement learning with aligned objectives, these agents
explore and learn collaboration, leading to improved generalization and
robustness. Empirical results from single-turn experiments demonstrate that
ReMA outperforms single-agent RL baselines on complex reasoning tasks,
including competitive-level mathematical benchmarks and LLM-as-a-Judge
benchmarks. Additionally, we further extend ReMA to multi-turn interaction
settings, leveraging turn-level ratio and parameter sharing to improve
efficiency. Comprehensive ablation studies further illustrate the evolving
dynamics of each distinct agent, providing valuable insights into how the
meta-thinking reasoning process enhances the reasoning capabilities of LLMs.
Our code can be found in https://github.com/ziyuwan/ReMA-public

</details>


### [476] [E2E Process Automation Leveraging Generative AI and IDP-Based Automation Agent: A Case Study on Corporate Expense Processing](https://arxiv.org/pdf/2505.20733)
*Cheonsu Jeong, Seongmin Sim, Hyoyoung Cho, Sungsu Kim, Byounggwan Shin*

Main category: cs.AI

TL;DR: The paper introduces an AI-driven automation system combining generative AI and IDP to streamline corporate financial expense processing, outperforming traditional RPA in handling unstructured data and exceptions.


<details>
  <summary>Details</summary>
Motivation: To address the limitations of traditional RPA in managing unstructured data, exceptions, and complex decisions in financial expense processing.

Method: A four-stage process: document recognition (OCR/IDP), item classification, AI-powered exception handling, and human-in-the-loop decision-making with continuous learning.

Result: Demonstrated an 80% reduction in processing time, lower error rates, improved compliance, and enhanced employee satisfaction at a Korean enterprise.

Conclusion: The integration of generative AI, IDP, and Automation Agents enables effective E2E automation of complex processes, with potential applications in other domains.

Abstract: This paper presents an intelligent work automation approach in the context of
contemporary digital transformation by integrating generative AI and
Intelligent Document Processing (IDP) technologies with an Automation Agent to
realize End-to-End (E2E) automation of corporate financial expense processing
tasks. While traditional Robotic Process Automation (RPA) has proven effective
for repetitive, rule-based simple task automation, it faces limitations in
handling unstructured data, exception management, and complex decision-making.
This study designs and implements a four-stage integrated process comprising
automatic recognition of supporting documents such as receipts via OCR/IDP,
item classification based on a policy-driven database, intelligent exception
handling supported by generative AI (large language models, LLMs), and
human-in-the-loop final decision-making with continuous system learning through
an Automation Agent. Applied to a major Korean enterprise (Company S), the
system demonstrated quantitative benefits including over 80% reduction in
processing time for paper receipt expense tasks, decreased error rates, and
improved compliance, as well as qualitative benefits such as enhanced accuracy
and consistency, increased employee satisfaction, and data-driven decision
support. Furthermore, the system embodies a virtuous cycle by learning from
human judgments to progressively improve automatic exception handling
capabilities. Empirically, this research confirms that the organic integration
of generative AI, IDP, and Automation Agents effectively overcomes the
limitations of conventional automation and enables E2E automation of complex
corporate processes. The study also discusses potential extensions to other
domains such as accounting, human resources, and procurement, and proposes
future directions for AI-driven hyper-automation development.

</details>


### [477] [RRO: LLM Agent Optimization Through Rising Reward Trajectories](https://arxiv.org/pdf/2505.20737)
*Zilong Wang, Jingfeng Yang, Sreyashi Nag, Samarth Varshney, Xianfeng Tang, Haoming Jiang, Jingbo Shang, Sheikh Muhammad Sarwar*

Main category: cs.AI

TL;DR: The paper introduces Reward Rising Optimization (RRO) to improve LLMs' multi-step task performance by focusing on increasing reward trends in reasoning steps, reducing exploration costs.


<details>
  <summary>Details</summary>
Motivation: LLMs struggle with complex multi-step tasks due to sensitivity to key-step outcomes. Existing methods like PRMs are costly and hard to scale.

Method: Proposes RRO, which dynamically expands search space by identifying rising rewards in reasoning steps, optimizing process supervision.

Result: RRO achieves superior performance on WebShop and InterCode-SQL benchmarks with lower exploration costs.

Conclusion: RRO efficiently enhances LLM agent performance in multi-step tasks while minimizing computational overhead.

Abstract: Large language models (LLMs) have exhibited extraordinary performance in a
variety of tasks while it remains challenging for them to solve complex
multi-step tasks as agents. In practice, agents sensitive to the outcome of
certain key steps which makes them likely to fail the task because of a subtle
mistake in the planning trajectory. Recent approaches resort to calibrating the
reasoning process through reinforcement learning. They reward or penalize every
reasoning step with process supervision, as known as Process Reward Models
(PRMs). However, PRMs are difficult and costly to scale up with a large number
of next action candidates since they require extensive computations to acquire
the training data through the per-step trajectory exploration. To mitigate this
issue, we focus on the relative reward trend across successive reasoning steps
and propose maintaining an increasing reward in the collected trajectories for
process supervision, which we term Reward Rising Optimization (RRO).
Specifically, we incrementally augment the process supervision until
identifying a step exhibiting positive reward differentials, i.e. rising
rewards, relative to its preceding iteration. This method dynamically expands
the search space for the next action candidates, efficiently capturing
high-quality data. We provide mathematical groundings and empirical results on
the WebShop and InterCode-SQL benchmarks, showing that our proposed RRO
achieves superior performance while requiring much less exploration cost.

</details>


### [478] [MSEarth: A Benchmark for Multimodal Scientific Comprehension of Earth Science](https://arxiv.org/pdf/2505.20740)
*Xiangyu Zhao, Wanghan Xu, Bo Liu, Yuhao Zhou, Fenghua Ling, Ben Fei, Xiaoyu Yue, Lei Bai, Wenlong Zhang, Xiao-Ming Wu*

Main category: cs.AI

TL;DR: MSEarth is a multimodal benchmark for Earth science, addressing the lack of graduate-level benchmarks by curating 7K figures with enriched captions from scientific publications.


<details>
  <summary>Details</summary>
Motivation: Current benchmarks for MLLMs in Earth science are simplistic or synthetic, lacking depth for real-world applications. MSEarth fills this gap.

Method: MSEarth is curated from open-access publications, featuring figures with refined captions and reasoning from papers, covering five Earth science spheres.

Result: The benchmark supports tasks like figure captioning, multiple-choice questions, and open-ended reasoning, enhancing MLLM evaluation.

Conclusion: MSEarth bridges the graduate-level benchmark gap, offering a scalable, high-fidelity resource for advancing scientific reasoning in MLLMs.

Abstract: The rapid advancement of multimodal large language models (MLLMs) has
unlocked new opportunities to tackle complex scientific challenges. Despite
this progress, their application in addressing earth science problems,
especially at the graduate level, remains underexplored. A significant barrier
is the absence of benchmarks that capture the depth and contextual complexity
of geoscientific reasoning. Current benchmarks often rely on synthetic datasets
or simplistic figure-caption pairs, which do not adequately reflect the
intricate reasoning and domain-specific insights required for real-world
scientific applications. To address these gaps, we introduce MSEarth, a
multimodal scientific benchmark curated from high-quality, open-access
scientific publications. MSEarth encompasses the five major spheres of Earth
science: atmosphere, cryosphere, hydrosphere, lithosphere, and biosphere,
featuring over 7K figures with refined captions. These captions are crafted
from the original figure captions and enriched with discussions and reasoning
from the papers, ensuring the benchmark captures the nuanced reasoning and
knowledge-intensive content essential for advanced scientific tasks. MSEarth
supports a variety of tasks, including scientific figure captioning, multiple
choice questions, and open-ended reasoning challenges. By bridging the gap in
graduate-level benchmarks, MSEarth provides a scalable and high-fidelity
resource to enhance the development and evaluation of MLLMs in scientific
reasoning. The benchmark is publicly available to foster further research and
innovation in this field. Resources related to this benchmark can be found at
https://huggingface.co/MSEarth and https://github.com/xiangyu-mm/MSEarth.

</details>


### [479] [Can Agents Fix Agent Issues?](https://arxiv.org/pdf/2505.20749)
*Alfin Wijaya Rahardja, Junwei Liu, Weitong Chen, Zhenpeng Chen, Yiling Lou*

Main category: cs.AI

TL;DR: The paper addresses the challenge of automatically resolving issues in LLM-based agent systems, introduces AGENTISSUE-BENCH, and evaluates SE agents' limited effectiveness.


<details>
  <summary>Details</summary>
Motivation: LLM-based agent systems are prone to bugs and evolving requirements, but existing SE agents are not well-suited for resolving their unique issues.

Method: Manual analysis of 201 real-world agent issues and creation of AGENTISSUE-BENCH, a benchmark with 50 tasks, followed by evaluation of SE agents.

Result: SE agents showed low resolution rates (3.33% - 12.67%) on AGENTISSUE-BENCH, highlighting their ineffectiveness for agent systems.

Conclusion: The study reveals the distinct challenges of maintaining agent systems and calls for advanced SE agents tailored to their needs.

Abstract: LLM-based agent systems are emerging as a new software paradigm and have been
widely adopted across diverse domains such as medicine, robotics, and
programming. However, maintaining these systems requires substantial effort, as
they are inevitably prone to bugs and continually evolve to meet changing
external requirements. Therefore, automatically resolving agent issues (i.e.,
bug reports or feature requests) is a crucial and challenging task. While
recent software engineering (SE) agents (e.g., SWE-agent) have shown promise in
addressing issues in traditional software systems, it remains unclear how
effectively they can resolve real-world issues in agent systems, which differ
significantly from traditional software. To fill this gap, we first manually
analyze 201 real-world agent issues and identify common categories of agent
issues. We then spend 500 person-hours constructing AGENTISSUE-BENCH, a
reproducible benchmark comprising 50 agent issue resolution tasks (each with an
executable environment and failure-triggering tests). We further evaluate
state-of-the-art SE agents on AGENTISSUE-BENCH and reveal their limited
effectiveness (i.e., with only 3.33% - 12.67% resolution rates). These results
underscore the unique challenges of maintaining agent systems compared to
traditional software, highlighting the need for further research to develop
advanced SE agents for resolving agent issues. Data and code are available at
https://alfin06.github.io/AgentIssue-Bench-Leaderboard/#/ .

</details>


### [480] [MT-Mol:Multi Agent System with Tool-based Reasoning for Molecular Optimization](https://arxiv.org/pdf/2505.20820)
*Hyomin Kim, Yunhui Jang, Sungsoo Ahn*

Main category: cs.AI

TL;DR: MT-Mol is a multi-agent framework for molecular optimization using LLMs and RDKit tools, achieving state-of-the-art performance on most PMO-1K tasks.


<details>
  <summary>Details</summary>
Motivation: The potential of LLMs in molecular optimization is underexplored, especially in structured reasoning and interpretability.

Method: MT-Mol employs role-specialized LLM agents and RDKit tools across five domains for tool-guided reasoning and feedback.

Result: The framework outperforms others on 17 out of 23 PMO-1K benchmark tasks.

Conclusion: MT-Mol demonstrates the effectiveness of multi-agent, tool-guided approaches in molecular optimization.

Abstract: Large language models (LLMs) have large potential for molecular optimization,
as they can gather external chemistry tools and enable collaborative
interactions to iteratively refine molecular candidates. However, this
potential remains underexplored, particularly in the context of structured
reasoning, interpretability, and comprehensive tool-grounded molecular
optimization. To address this gap, we introduce MT-Mol, a multi-agent framework
for molecular optimization that leverages tool-guided reasoning and
role-specialized LLM agents. Our system incorporates comprehensive RDKit tools,
categorized into five distinct domains: structural descriptors, electronic and
topological features, fragment-based functional groups, molecular
representations, and miscellaneous chemical properties. Each category is
managed by an expert analyst agent, responsible for extracting task-relevant
tools and enabling interpretable, chemically grounded feedback. MT-Mol produces
molecules with tool-aligned and stepwise reasoning through the interaction
between the analyst agents, a molecule-generating scientist, a reasoning-output
verifier, and a reviewer agent. As a result, we show that our framework shows
the state-of-the-art performance of the PMO-1K benchmark on 17 out of 23 tasks.

</details>


### [481] [Step-Wise Formal Verification for LLM-Based Mathematical Problem Solving](https://arxiv.org/pdf/2505.20869)
*Kuo Zhou, Lu Zhang*

Main category: cs.AI

TL;DR: The paper introduces MATH-VF, a framework for verifying and refining solutions generated by LLMs in mathematical problem-solving, using formalization and external tools for correctness.


<details>
  <summary>Details</summary>
Motivation: LLMs often make logical or computational errors in math solutions, necessitating a robust verification system.

Method: MATH-VF uses a Formalizer (LLM) to translate solutions into formal contexts and a Critic (external tools) to evaluate and correct errors.

Result: Tested on MATH500 and ProcessBench, MATH-VF outperforms existing methods in verifying and refining solutions.

Conclusion: MATH-VF effectively improves the accuracy of LLM-generated mathematical solutions through formal verification and feedback.

Abstract: Large Language Models (LLMs) have demonstrated formidable capabilities in
solving mathematical problems, yet they may still commit logical reasoning and
computational errors during the problem-solving process. Thus, this paper
proposes a framework, MATH-VF, which includes a Formalizer and a Critic, for
formally verifying the correctness of the solutions generated by large language
models. Our framework first utilizes a Formalizer which employs an LLM to
translate a natural language solution into a formal context. Afterward, our
Critic (which integrates various external tools such as a Computer Algebra
System and an SMT solver) evaluates the correctness of each statement within
the formal context, and when a statement is incorrect, our Critic provides
corrective feedback. We empirically investigate the effectiveness of MATH-VF in
two scenarios: 1) Verification: MATH-VF is utilized to determine the
correctness of a solution to a given problem. 2) Refinement: When MATH-VF
identifies errors in the solution generated by an LLM-based solution generator
for a given problem, it submits the corrective suggestions proposed by the
Critic to the solution generator to regenerate the solution. We evaluate our
framework on widely used mathematical benchmarks: MATH500 and ProcessBench,
demonstrating the superiority of our approach over existing approaches.

</details>


### [482] [Reinforcement Learning-based Sequential Route Recommendation for System-Optimal Traffic Assignment](https://arxiv.org/pdf/2505.20889)
*Leizhen Wang, Peibo Duan, Cheng Lyu, Zhenliang Ma*

Main category: cs.AI

TL;DR: A learning-based framework uses deep RL to achieve system-optimal traffic assignment by sequentially recommending routes, minimizing total travel time.


<details>
  <summary>Details</summary>
Motivation: To explore if personalized routing decisions can collectively achieve system-optimal traffic assignment.

Method: Proposes a single-agent deep RL framework with an MSA-guided Q-learning algorithm, integrating traditional traffic assignment methods.

Result: RL agent converges to theoretical SO in Braess network and achieves 0.35% deviation in OW network. SO-informed route sets improve learning.

Conclusion: The approach bridges individual routing behavior with system efficiency, offering practical and theoretical insights.

Abstract: Modern navigation systems and shared mobility platforms increasingly rely on
personalized route recommendations to improve individual travel experience and
operational efficiency. However, a key question remains: can such sequential,
personalized routing decisions collectively lead to system-optimal (SO) traffic
assignment? This paper addresses this question by proposing a learning-based
framework that reformulates the static SO traffic assignment problem as a
single-agent deep reinforcement learning (RL) task. A central agent
sequentially recommends routes to travelers as origin-destination (OD) demands
arrive, to minimize total system travel time. To enhance learning efficiency
and solution quality, we develop an MSA-guided deep Q-learning algorithm that
integrates the iterative structure of traditional traffic assignment methods
into the RL training process. The proposed approach is evaluated on both the
Braess and Ortuzar-Willumsen (OW) networks. Results show that the RL agent
converges to the theoretical SO solution in the Braess network and achieves
only a 0.35% deviation in the OW network. Further ablation studies demonstrate
that the route action set's design significantly impacts convergence speed and
final performance, with SO-informed route sets leading to faster learning and
better outcomes. This work provides a theoretically grounded and practically
relevant approach to bridging individual routing behavior with system-level
efficiency through learning-based sequential assignment.

</details>


### [483] [Controllable Logical Hypothesis Generation for Abductive Reasoning in Knowledge Graphs](https://arxiv.org/pdf/2505.20948)
*Yisen Gao, Jiaxin Bai, Tianshi Zheng, Qingyun Sun, Ziwei Zhang, Jianxin Li, Yangqiu Song, Xingcheng Fu*

Main category: cs.AI

TL;DR: The paper introduces CtrlHGen, a framework for controllable hypothesis generation in abductive reasoning over knowledge graphs, addressing challenges like hypothesis space collapse and oversensitivity.


<details>
  <summary>Details</summary>
Motivation: To improve the practical utility of abductive reasoning by making hypothesis generation controllable, avoiding redundant or irrelevant hypotheses.

Method: Proposes CtrlHGen, a two-stage framework (supervised + reinforcement learning) with dataset augmentation and smoothed semantic rewards.

Result: Outperforms baselines in adhering to control conditions and achieving semantic similarity on benchmark datasets.

Conclusion: CtrlHGen effectively addresses controllability challenges in abductive reasoning, enhancing hypothesis generation quality.

Abstract: Abductive reasoning in knowledge graphs aims to generate plausible logical
hypotheses from observed entities, with broad applications in areas such as
clinical diagnosis and scientific discovery. However, due to a lack of
controllability, a single observation may yield numerous plausible but
redundant or irrelevant hypotheses on large-scale knowledge graphs. To address
this limitation, we introduce the task of controllable hypothesis generation to
improve the practical utility of abductive reasoning. This task faces two key
challenges when controlling for generating long and complex logical hypotheses:
hypothesis space collapse and hypothesis oversensitivity. To address these
challenges, we propose CtrlHGen, a Controllable logcial Hypothesis Generation
framework for abductive reasoning over knowledge graphs, trained in a two-stage
paradigm including supervised learning and subsequent reinforcement learning.
To mitigate hypothesis space collapse, we design a dataset augmentation
strategy based on sub-logical decomposition, enabling the model to learn
complex logical structures by leveraging semantic patterns in simpler
components. To address hypothesis oversensitivity, we incorporate smoothed
semantic rewards including Dice and Overlap scores, and introduce a
condition-adherence reward to guide the generation toward user-specified
control constraints. Extensive experiments on three benchmark datasets
demonstrate that our model not only better adheres to control conditions but
also achieves superior semantic similarity performance compared to baselines.

</details>


### [484] [Large Language Model-enhanced Reinforcement Learning for Low-Altitude Economy Networking](https://arxiv.org/pdf/2505.21045)
*Lingyi Cai, Ruichen Zhang, Changyuan Zhao, Yu Zhang, Jiawen Kang, Dusit Niyato, Tao Jiang, Xuemin Shen*

Main category: cs.AI

TL;DR: The paper proposes integrating LLMs with RL to address challenges in LAENet, leveraging LLMs' capabilities for improved decision-making and reward design.


<details>
  <summary>Details</summary>
Motivation: Challenges in LAENet, such as complex decision-making and resource constraints, motivate the use of RL, but RL's limitations (generalization, reward design) require enhancement via LLMs.

Method: The paper introduces an LLM-enhanced RL framework for LAENet, utilizing LLMs as information processors, reward designers, decision-makers, and generators. A case study demonstrates LLM-designed reward functions.

Result: The proposed framework shows potential to improve RL performance in LAENet by leveraging LLMs' contextual understanding and reasoning.

Conclusion: The integration of LLMs and RL offers promising solutions for LAENet, with future work needed to refine and expand the approach.

Abstract: Low-Altitude Economic Networking (LAENet) aims to support diverse flying
applications below 1,000 meters by deploying various aerial vehicles for
flexible and cost-effective aerial networking. However, complex
decision-making, resource constraints, and environmental uncertainty pose
significant challenges to the development of the LAENet. Reinforcement learning
(RL) offers a potential solution in response to these challenges but has
limitations in generalization, reward design, and model stability. The
emergence of large language models (LLMs) offers new opportunities for RL to
mitigate these limitations. In this paper, we first present a tutorial about
integrating LLMs into RL by using the capacities of generation, contextual
understanding, and structured reasoning of LLMs. We then propose an
LLM-enhanced RL framework for the LAENet in terms of serving the LLM as
information processor, reward designer, decision-maker, and generator.
Moreover, we conduct a case study by using LLMs to design a reward function to
improve the learning performance of RL in the LAENet. Finally, we provide a
conclusion and discuss future work.

</details>


### [485] [Agent-Environment Alignment via Automated Interface Generation](https://arxiv.org/pdf/2505.21055)
*Kaiming Liu, Xuanyu Lei, Ziyue Wang, Peng Li, Yang Liu*

Main category: cs.AI

TL;DR: The paper addresses agent-environment misalignment in LLM agents by proposing ALIGN, a framework that auto-aligns interfaces to improve performance without modifying agent or environment code.


<details>
  <summary>Details</summary>
Motivation: Agent-environment misalignment is a bottleneck in LLM agent performance, yet the role of interfaces remains underexplored.

Method: ALIGN enriches the interface by enhancing static environment information and step-wise observations, implemented as a lightweight wrapper.

Result: Experiments show up to 45.67% success rate improvement in ALFWorld, with generalization across agent architectures and LLM backbones.

Conclusion: ALIGN effectively mitigates misalignment, improving agent performance across diverse domains without requiring interface regeneration.

Abstract: Large language model (LLM) agents have shown impressive reasoning
capabilities in interactive decision-making tasks. These agents interact with
environment through intermediate interfaces, such as predefined action spaces
and interaction rules, which mediate the perception and action. However,
mismatches often happen between the internal expectations of the agent
regarding the influence of its issued actions and the actual state transitions
in the environment, a phenomenon referred to as \textbf{agent-environment
misalignment}. While prior work has invested substantially in improving agent
strategies and environment design, the critical role of the interface still
remains underexplored. In this work, we empirically demonstrate that
agent-environment misalignment poses a significant bottleneck to agent
performance. To mitigate this issue, we propose \textbf{ALIGN}, an
\underline{A}uto-A\underline{l}igned \underline{I}nterface
\underline{G}e\underline{n}eration framework that alleviates the misalignment
by enriching the interface. Specifically, the ALIGN-generated interface
enhances both the static information of the environment and the step-wise
observations returned to the agent. Implemented as a lightweight wrapper, this
interface achieves the alignment without modifying either the agent logic or
the environment code. Experiments across multiple domains including embodied
tasks, web navigation and tool-use, show consistent performance improvements,
with up to a 45.67\% success rate improvement observed in ALFWorld. Meanwhile,
ALIGN-generated interface can generalize across different agent architectures
and LLM backbones without interface regeneration. Code and experimental results
are available at https://github.com/THUNLP-MT/ALIGN.

</details>


### [486] [Why Distillation can Outperform Zero-RL: The Role of Flexible Reasoning](https://arxiv.org/pdf/2505.21067)
*Xiao Hu, Xingyu Lu, Liyuan Mao, YiFan Zhang, Tianke Zhang, Bin Wen, Fan Yang, Tingting Gao, Guorui Zhou*

Main category: cs.AI

TL;DR: A simple distillation method outperforms zero-RL with only 920 examples, showing more flexible reasoning and advanced cognitive behaviors.


<details>
  <summary>Details</summary>
Motivation: To demonstrate that distillation can achieve better reasoning performance than zero-RL with less data and computational cost.

Method: Distillation applied to a base model, analyzing token frequency and cognitive behaviors in outputs.

Result: Distilled model uses more anthropomorphic tokens and logical connectors, enhancing advanced cognitive behaviors like Multi-Perspective Thinking and Metacognitive Awareness.

Conclusion: Distillation is more efficient and effective than zero-RL for improving reasoning in LLMs.

Abstract: Reinforcement learning (RL) has played an important role in improving the
reasoning ability of large language models (LLMs). Some studies apply RL
directly to \textit{smaller} base models (known as zero-RL) and also achieve
notable progress. However, in this paper, we show that using only 920 examples,
a simple distillation method based on the base model can clearly outperform
zero-RL, which typically requires much more data and computational cost. By
analyzing the token frequency in model outputs, we find that the distilled
model shows more flexible reasoning. It uses anthropomorphic tokens and logical
connectors much more often than the zero-RL model. Further analysis reveals
that distillation enhances the presence of two advanced cognitive behaviors:
Multi-Perspective Thinking or Attempting and Metacognitive Awareness. Frequent
occurrences of these two advanced cognitive behaviors give rise to flexible
reasoning, which is essential for solving complex reasoning problems, while
zero-RL fails to significantly boost the frequency of these behaviors.

</details>


### [487] [Interpreting Social Bias in LVLMs via Information Flow Analysis and Multi-Round Dialogue Evaluation](https://arxiv.org/pdf/2505.21106)
*Zhengyang Ji, Yifan Jia, Shang Gao, Yutao Yue*

Main category: cs.AI

TL;DR: The paper proposes a framework to analyze social biases in Large Vision Language Models (LVLMs) by examining imbalanced internal information usage and cross-modal bias formation.


<details>
  <summary>Details</summary>
Motivation: LVLMs show social biases, but existing studies lack insights into their underlying mechanisms. The study aims to uncover how biases originate from internal reasoning dynamics.

Method: Combines information flow analysis to identify key image tokens and multi-round dialogue evaluation to assess sensitive information encoding. Also examines textual modality for bias patterns.

Result: LVLMs exhibit systematic disparities in information usage across demographic groups, with biases rooted in internal reasoning and semantic representations.

Conclusion: The framework provides a deeper understanding of social bias origins in LVLMs, highlighting cross-modal bias formation and internal reasoning disparities.

Abstract: Large Vision Language Models (LVLMs) have achieved remarkable progress in
multimodal tasks, yet they also exhibit notable social biases. These biases
often manifest as unintended associations between neutral concepts and
sensitive human attributes, leading to disparate model behaviors across
demographic groups. While existing studies primarily focus on detecting and
quantifying such biases, they offer limited insight into the underlying
mechanisms within the models. To address this gap, we propose an explanatory
framework that combines information flow analysis with multi-round dialogue
evaluation, aiming to understand the origin of social bias from the perspective
of imbalanced internal information utilization. Specifically, we first identify
high-contribution image tokens involved in the model's reasoning process for
neutral questions via information flow analysis. Then, we design a multi-turn
dialogue mechanism to evaluate the extent to which these key tokens encode
sensitive information. Extensive experiments reveal that LVLMs exhibit
systematic disparities in information usage when processing images of different
demographic groups, suggesting that social bias is deeply rooted in the model's
internal reasoning dynamics. Furthermore, we complement our findings from a
textual modality perspective, showing that the model's semantic representations
already display biased proximity patterns, thereby offering a cross-modal
explanation of bias formation.

</details>


### [488] [Interpretable DNFs](https://arxiv.org/pdf/2505.21212)
*Martin C. Cooper, Imane Bousdira, Clément Carbonnel*

Main category: cs.AI

TL;DR: The paper explores interpretable DNF classifiers, focusing on $k$-DNFs where both the classifier and its complement are expressible as bounded-size DNFs. It compares depth-$k$ decision trees and nested $k$-DNFs, finding the latter promising for interpretability and accuracy.


<details>
  <summary>Details</summary>
Motivation: To define and study interpretable DNF classifiers where both positive and negative decisions have small, understandable explanations.

Method: Analyzes $k$-DNFs with complements expressible as $k$-DNFs, comparing depth-$k$ decision trees and nested $k$-DNFs.

Result: Nested $k$-DNFs show potential as an interpretable and accurate alternative to decision trees.

Conclusion: Nested $k$-DNFs are a viable option for interpretable classification, offering a balance between interpretability and performance.

Abstract: A classifier is considered interpretable if each of its decisions has an
explanation which is small enough to be easily understood by a human user. A
DNF formula can be seen as a binary classifier $\kappa$ over boolean domains.
The size of an explanation of a positive decision taken by a DNF $\kappa$ is
bounded by the size of the terms in $\kappa$, since we can explain a positive
decision by giving a term of $\kappa$ that evaluates to true. Since both
positive and negative decisions must be explained, we consider that
interpretable DNFs are those $\kappa$ for which both $\kappa$ and
$\overline{\kappa}$ can be expressed as DNFs composed of terms of bounded size.
In this paper, we study the family of $k$-DNFs whose complements can also be
expressed as $k$-DNFs. We compare two such families, namely depth-$k$ decision
trees and nested $k$-DNFs, a novel family of models. Experiments indicate that
nested $k$-DNFs are an interesting alternative to decision trees in terms of
interpretability and accuracy.

</details>


### [489] [XBOUND: Exploring the Capability Boundaries of Device-Control Agents through Trajectory Tree Exploration](https://arxiv.org/pdf/2505.21279)
*Shaoqing Zhang, Kehai Chen, Zhuosheng Zhang, Rumei Li, Rongxiang Weng, Yang Xiang, Liqiang Nie, Min Zhang*

Main category: cs.AI

TL;DR: The paper introduces XBOUND, a novel evaluation method for Device-Control Agents (DC agents) using an Explore Metric, addressing limitations of conventional methods by providing microscopic insights into performance.


<details>
  <summary>Details</summary>
Motivation: Current evaluation methods for DC agents lack granularity, failing to identify specific errors in real-world applications.

Method: Proposes XBOUND, which assesses DC agents' proficiency at individual states using a new Explore Metric and a pseudo episode tree dataset derived from Android Control test data.

Result: Evaluates OS-Atlas and UI-TARS series, revealing overall and task-specific performance, along with current deficiencies.

Conclusion: XBOUND offers a finer-grained evaluation of DC agents, highlighting limitations and paving the way for improved assessment methods.

Abstract: Recent advancements in vision-language models (VLMs) have spurred increased
interest in Device-Control Agents (DC agents), such as utilizing in-the-wild
device control to manage graphical user interfaces. Conventional methods for
assessing the capabilities of DC agents, such as computing step-wise action
accuracy and overall task success rates, provide a macroscopic view of DC
agents' performance; however, they fail to offer microscopic insights into
potential errors that may occur in real-world applications. Conducting a
finer-grained performance evaluation of DC agents presents significant
challenges. This study introduces a new perspective on evaluation methods for
DC agents by proposing the XBOUND evaluation method, which employs the
calculation of a novel Explore Metric to delineate the capability boundaries of
DC agents. Compared to previous evaluation methods, XBOUND focuses on
individual states to assess the proficiency of DC agents in mastering these
states. Furthermore, we have developed a ``pseudo'' episode tree dataset
derived from Android Control test data. Utilizing this dataset and XBOUND, we
comprehensively evaluate the OS-Atlas and UI-TARS series, examining both the
overall and specific performance across five common tasks. Additionally, we
select representative cases to highlight the current deficiencies and
limitations inherent in both series. Code is available at
https://github.com/sqzhang-lazy/XBOUND.

</details>


### [490] [RLJP: Legal Judgment Prediction via First-Order Logic Rule-enhanced with Large Language Models](https://arxiv.org/pdf/2505.21281)
*Yue Zhang, Zhiliang Tian, Shicheng Zhou, Haiyang Wang, Wenqing Hou, Yuying Liu, Xuechen Zhao, Minlie Huang, Ye Wang, Bin Zhou*

Main category: cs.AI

TL;DR: The paper introduces a rule-enhanced legal judgment prediction framework using first-order logic and comparative learning to improve adaptability and performance in complex legal cases.


<details>
  <summary>Details</summary>
Motivation: Existing LJP models lack adaptability to case-specific legal reasoning logic, especially in complex cases, despite their high performance.

Method: The framework uses first-order logic to initialize judgment rules, employs Confusion-aware Contrastive Learning (CACL) for dynamic optimization, and predicts judgments using the optimized rules.

Result: Experiments on two public datasets show superior performance across all metrics.

Conclusion: The proposed framework effectively enhances legal judgment prediction by integrating adaptive logical reasoning.

Abstract: Legal Judgment Prediction (LJP) is a pivotal task in legal AI. Existing
semantic-enhanced LJP models integrate judicial precedents and legal knowledge
for high performance. But they neglect legal reasoning logic, a critical
component of legal judgments requiring rigorous logical analysis. Although some
approaches utilize legal reasoning logic for high-quality predictions, their
logic rigidity hinders adaptation to case-specific logical frameworks,
particularly in complex cases that are lengthy and detailed. This paper
proposes a rule-enhanced legal judgment prediction framework based on
first-order logic (FOL) formalism and comparative learning (CL) to develop an
adaptive adjustment mechanism for legal judgment logic and further enhance
performance in LJP. Inspired by the process of human exam preparation, our
method follows a three-stage approach: first, we initialize judgment rules
using the FOL formalism to capture complex reasoning logic accurately; next, we
propose a Confusion-aware Contrastive Learning (CACL) to dynamically optimize
the judgment rules through a quiz consisting of confusable cases; finally, we
utilize the optimized judgment rules to predict legal judgments. Experimental
results on two public datasets show superior performance across all metrics.
The code is publicly available{https://anonymous.4open.science/r/RLJP-FDF1}.

</details>


### [491] [Complex System Diagnostics Using a Knowledge Graph-Informed and Large Language Model-Enhanced Framework](https://arxiv.org/pdf/2505.21291)
*Saman Marandi, Yu-Shu Hu, Mohammad Modarres*

Main category: cs.AI

TL;DR: A novel diagnostic framework combines Knowledge Graphs (KGs) and Large Language Models (LLMs) for high-reliability systems, improving accuracy and interactive diagnostics.


<details>
  <summary>Details</summary>
Motivation: Traditional diagnostic modeling struggles with system complexity, prompting the need for a functional modeling approach.

Method: The framework uses Dynamic Master Logic (DML) with LLMs for automated logic construction and interactive diagnostics, encoded into a KG-DML for hierarchical reasoning.

Result: Achieved over 90% accuracy in a case study on an auxiliary feedwater system, with consistent tool and argument extraction.

Conclusion: The framework is effective for safety-critical diagnostics, leveraging KG-LLM integration for precision and usability.

Abstract: In this paper, we present a novel diagnostic framework that integrates
Knowledge Graphs (KGs) and Large Language Models (LLMs) to support system
diagnostics in high-reliability systems such as nuclear power plants.
Traditional diagnostic modeling struggles when systems become too complex,
making functional modeling a more attractive approach. Our approach introduces
a diagnostic framework grounded in the functional modeling principles of the
Dynamic Master Logic (DML) model. It incorporates two coordinated LLM
components, including an LLM-based workflow for automated construction of DML
logic from system documentation and an LLM agent that facilitates interactive
diagnostics. The generated logic is encoded into a structured KG, referred to
as KG-DML, which supports hierarchical fault reasoning. Expert knowledge or
operational data can also be incorporated to refine the model's precision and
diagnostic depth. In the interaction phase, users submit natural language
queries, which are interpreted by the LLM agent. The agent selects appropriate
tools for structured reasoning, including upward and downward propagation
across the KG-DML. Rather than embedding KG content into every prompt, the LLM
agent distinguishes between diagnostic and interpretive tasks. For diagnostics,
the agent selects and executes external tools that perform structured KG
reasoning. For general queries, a Graph-based Retrieval-Augmented Generation
(Graph-RAG) approach is used, retrieving relevant KG segments and embedding
them into the prompt to generate natural explanations. A case study on an
auxiliary feedwater system demonstrated the framework's effectiveness, with
over 90% accuracy in key elements and consistent tool and argument extraction,
supporting its use in safety-critical diagnostics.

</details>


### [492] [Beyond Chemical QA: Evaluating LLM's Chemical Reasoning with Modular Chemical Operations](https://arxiv.org/pdf/2505.21318)
*Hao Li, He Cao, Bin Feng, Yanjun Shao, Xiangru Tang, Zhiyuan Yan, Li Yuan, Yonghong Tian, Yu Li*

Main category: cs.AI

TL;DR: ChemCoTBench introduces a reasoning framework for LLMs to tackle complex chemistry tasks like molecular optimization and reaction prediction through step-by-step workflows.


<details>
  <summary>Details</summary>
Motivation: Current benchmarks lack systematic reasoning for chemistry tasks, limiting LLMs' potential in real-world applications like drug design.

Method: The framework formalizes chemical problem-solving using modular operations (addition, deletion, substitution) and evaluates models on Molecular Property Optimization and Chemical Reaction Prediction.

Result: ChemCoTBench provides annotated datasets, a reasoning taxonomy, and baseline evaluations to bridge abstract reasoning with practical chemical discovery.

Conclusion: The framework advances LLMs as tools for AI-driven scientific innovation by enabling structured, transparent reasoning in chemistry.

Abstract: While large language models (LLMs) with Chain-of-Thought (CoT) reasoning
excel in mathematics and coding, their potential for systematic reasoning in
chemistry, a domain demanding rigorous structural analysis for real-world tasks
like drug design and reaction engineering, remains untapped. Current benchmarks
focus on simple knowledge retrieval, neglecting step-by-step reasoning required
for complex tasks such as molecular optimization and reaction prediction. To
address this, we introduce ChemCoTBench, a reasoning framework that bridges
molecular structure understanding with arithmetic-inspired operations,
including addition, deletion, and substitution, to formalize chemical
problem-solving into transparent, step-by-step workflows. By treating molecular
transformations as modular "chemical operations", the framework enables
slow-thinking reasoning, mirroring the logic of mathematical proofs while
grounding solutions in real-world chemical constraints. We evaluate models on
two high-impact tasks: Molecular Property Optimization and Chemical Reaction
Prediction. These tasks mirror real-world challenges while providing structured
evaluability. By providing annotated datasets, a reasoning taxonomy, and
baseline evaluations, ChemCoTBench bridges the gap between abstract reasoning
methods and practical chemical discovery, establishing a foundation for
advancing LLMs as tools for AI-driven scientific innovation.

</details>


### [493] [Assured Autonomy with Neuro-Symbolic Perception](https://arxiv.org/pdf/2505.21322)
*R. Spencer Hallyburton, Miroslav Pajic*

Main category: cs.AI

TL;DR: The paper proposes NeuSPaPer, a neuro-symbolic paradigm for perception, combining object detection and scene graph generation to enhance AI reliability in cyber-physical systems.


<details>
  <summary>Details</summary>
Motivation: Current AI models in CPS lack security guarantees, raising concerns for safety-critical applications. The paper advocates for integrating symbolic reasoning into data-driven models for assured AI.

Method: Introduces NeuSPaPer, leveraging foundation models for knowledge extraction and specialized SGG algorithms to create structured relational graphs for deep scene understanding.

Result: Demonstrates that SGG bridges low-level sensor perception and high-level reasoning, enhancing resilience and context-awareness in AI.

Conclusion: NeuSPaPer advances trusted autonomy in CPS by ensuring situational awareness integrity through neuro-symbolic perception.

Abstract: Many state-of-the-art AI models deployed in cyber-physical systems (CPS),
while highly accurate, are simply pattern-matchers.~With limited security
guarantees, there are concerns for their reliability in safety-critical and
contested domains. To advance assured AI, we advocate for a paradigm shift that
imbues data-driven perception models with symbolic structure, inspired by a
human's ability to reason over low-level features and high-level context. We
propose a neuro-symbolic paradigm for perception (NeuSPaPer) and illustrate how
joint object detection and scene graph generation (SGG) yields deep scene
understanding.~Powered by foundation models for offline knowledge extraction
and specialized SGG algorithms for real-time deployment, we design a framework
leveraging structured relational graphs that ensures the integrity of
situational awareness in autonomy. Using physics-based simulators and
real-world datasets, we demonstrate how SGG bridges the gap between low-level
sensor perception and high-level reasoning, establishing a foundation for
resilient, context-aware AI and advancing trusted autonomy in CPS.

</details>


### [494] [MME-Reasoning: A Comprehensive Benchmark for Logical Reasoning in MLLMs](https://arxiv.org/pdf/2505.21327)
*Jiakang Yuan, Tianshuo Peng, Yilei Jiang, Yiting Lu, Renrui Zhang, Kaituo Feng, Chaoyou Fu, Tao Chen, Lei Bai, Bo Zhang, Xiangyu Yue*

Main category: cs.AI

TL;DR: The paper introduces MME-Reasoning, a benchmark to evaluate multimodal large language models (MLLMs) on inductive, deductive, and abductive reasoning, revealing their limitations and performance imbalances.


<details>
  <summary>Details</summary>
Motivation: Existing benchmarks lack explicit categorization of logical reasoning types and clear understanding of reasoning, limiting comprehensive evaluation of MLLMs.

Method: The authors developed MME-Reasoning, a benchmark with carefully curated questions covering all three reasoning types, ensuring evaluation focuses on reasoning ability.

Result: State-of-the-art MLLMs show significant limitations and performance imbalances in logical reasoning, even with advanced methods like "thinking mode" and Rule-based RL.

Conclusion: The study highlights critical gaps in MLLMs' reasoning capabilities and provides systematic insights for future improvements in evaluating and enhancing reasoning abilities.

Abstract: Logical reasoning is a fundamental aspect of human intelligence and an
essential capability for multimodal large language models (MLLMs). Despite the
significant advancement in multimodal reasoning, existing benchmarks fail to
comprehensively evaluate their reasoning abilities due to the lack of explicit
categorization for logical reasoning types and an unclear understanding of
reasoning. To address these issues, we introduce MME-Reasoning, a comprehensive
benchmark designed to evaluate the reasoning ability of MLLMs, which covers all
three types of reasoning (i.e., inductive, deductive, and abductive) in its
questions. We carefully curate the data to ensure that each question
effectively evaluates reasoning ability rather than perceptual skills or
knowledge breadth, and extend the evaluation protocols to cover the evaluation
of diverse questions. Our evaluation reveals substantial limitations of
state-of-the-art MLLMs when subjected to holistic assessments of logical
reasoning capabilities. Even the most advanced MLLMs show limited performance
in comprehensive logical reasoning, with notable performance imbalances across
reasoning types. In addition, we conducted an in-depth analysis of approaches
such as ``thinking mode'' and Rule-based RL, which are commonly believed to
enhance reasoning abilities. These findings highlight the critical limitations
and performance imbalances of current MLLMs in diverse logical reasoning
scenarios, providing comprehensive and systematic insights into the
understanding and evaluation of reasoning capabilities.

</details>


### [495] [The Multilingual Divide and Its Impact on Global AI Safety](https://arxiv.org/pdf/2505.21344)
*Aidan Peppin, Julia Kreutzer, Alice Schoenauer Sebag, Kelly Marchisio, Beyza Ermis, John Dang, Samuel Cahyawijaya, Shivalika Singh, Seraphina Goldfarb-Tarrant, Viraat Aryabumi, Aakanksha, Wei-Yin Ko, Ahmet Üstün, Matthias Gallé, Marzieh Fadaee, Sara Hooker*

Main category: cs.AI

TL;DR: The paper highlights the persistent language gap in AI capabilities and safety for non-dominant languages, analyzing its causes, impacts, and proposing solutions like multilingual dataset creation and transparency.


<details>
  <summary>Details</summary>
Motivation: To address disparities in AI safety and capabilities for non-dominant languages, emphasizing the need for global inclusivity.

Method: Analysis of the language gap's causes, its impact on AI safety, and identification of barriers to bridging it.

Result: Identified key challenges and disparities in AI safety due to the language gap, along with actionable recommendations.

Conclusion: Policymakers and governance experts should support multilingual dataset creation, transparency, and research to mitigate AI safety risks across languages.

Abstract: Despite advances in large language model capabilities in recent years, a
large gap remains in their capabilities and safety performance for many
languages beyond a relatively small handful of globally dominant languages.
This paper provides researchers, policymakers and governance experts with an
overview of key challenges to bridging the "language gap" in AI and minimizing
safety risks across languages. We provide an analysis of why the language gap
in AI exists and grows, and how it creates disparities in global AI safety. We
identify barriers to address these challenges, and recommend how those working
in policy and governance can help address safety concerns associated with the
language gap by supporting multilingual dataset creation, transparency, and
research.

</details>


### [496] [A Structured Unplugged Approach for Foundational AI Literacy in Primary Education](https://arxiv.org/pdf/2505.21398)
*Maria Cristina Carrisi, Mirko Marras, Sara Vergallo*

Main category: cs.AI

TL;DR: The paper proposes a structured teaching approach to enhance AI literacy in primary students, focusing on core mathematical elements. An empirical study with fifth-graders showed improved understanding and engagement.


<details>
  <summary>Details</summary>
Motivation: Early AI literacy is vital due to the growing influence of intelligent technologies, but current education often prioritizes tool usage over conceptual understanding, leading to misconceptions.

Method: A replicable teaching approach was developed, integrating core mathematical elements. An empirical study with 31 fifth-grade students assessed effectiveness via post-tests and surveys.

Result: Students showed improved terminology understanding, logical reasoning, and evaluative skills, with deeper comprehension of AI decision-making and its limitations. The approach was engaging.

Conclusion: The structured approach effectively enhances AI literacy in primary students, linking concepts to real-world reasoning and fostering engagement.

Abstract: Younger generations are growing up in a world increasingly shaped by
intelligent technologies, making early AI literacy crucial for developing the
skills to critically understand and navigate them. However, education in this
field often emphasizes tool-based learning, prioritizing usage over
understanding the underlying concepts. This lack of knowledge leaves
non-experts, especially children, prone to misconceptions, unrealistic
expectations, and difficulties in recognizing biases and stereotypes. In this
paper, we propose a structured and replicable teaching approach that fosters
foundational AI literacy in primary students, by building upon core
mathematical elements closely connected to and of interest in primary
curricula, to strengthen conceptualization, data representation, classification
reasoning, and evaluation of AI. To assess the effectiveness of our approach,
we conducted an empirical study with thirty-one fifth-grade students across two
classes, evaluating their progress through a post-test and a satisfaction
survey. Our results indicate improvements in terminology understanding and
usage, features description, logical reasoning, and evaluative skills, with
students showing a deeper comprehension of decision-making processes and their
limitations. Moreover, the approach proved engaging, with students particularly
enjoying activities that linked AI concepts to real-world reasoning. Materials:
https://github.com/tail-unica/ai-literacy-primary-ed.

</details>


### [497] [MRSD: Multi-Resolution Skill Discovery for HRL Agents](https://arxiv.org/pdf/2505.21410)
*Shashank Sharma, Janina Hoffmann, Vinay Namboodiri*

Main category: cs.AI

TL;DR: MRSD introduces multi-resolution skill discovery in HRL, outperforming prior methods by learning skills at varying temporal resolutions and dynamically selecting them for adaptive control.


<details>
  <summary>Details</summary>
Motivation: Existing skill discovery methods in HRL are limited to single skills per task, unlike humans who use skills at multiple resolutions simultaneously. MRSD aims to bridge this gap.

Method: MRSD learns multiple skill encoders at different temporal resolutions in parallel, with a high-level manager dynamically selecting skills for adaptive control.

Result: MRSD outperforms state-of-the-art methods on DeepMind Control Suite tasks, achieving faster convergence and higher performance.

Conclusion: Integrating multi-resolution skills in HRL enhances versatility and efficiency, as demonstrated by MRSD's success.

Abstract: Hierarchical reinforcement learning (HRL) relies on abstract skills to solve
long-horizon tasks efficiently. While existing skill discovery methods learns
these skills automatically, they are limited to a single skill per task. In
contrast, humans learn and use both fine-grained and coarse motor skills
simultaneously. Inspired by human motor control, we propose Multi-Resolution
Skill Discovery (MRSD), an HRL framework that learns multiple skill encoders at
different temporal resolutions in parallel. A high-level manager dynamically
selects among these skills, enabling adaptive control strategies over time. We
evaluate MRSD on tasks from the DeepMind Control Suite and show that it
outperforms prior state-of-the-art skill discovery and HRL methods, achieving
faster convergence and higher final performance. Our findings highlight the
benefits of integrating multi-resolution skills in HRL, paving the way for more
versatile and efficient agents.

</details>


### [498] [Diagnosing and Resolving Cloud Platform Instability with Multi-modal RAG LLMs](https://arxiv.org/pdf/2505.21419)
*Yifan Wang, Kenneth P. Birman*

Main category: cs.AI

TL;DR: ARCA, a multi-modal RAG LLM system, simplifies cloud application issue resolution by combining AI pattern matching with a natural interface, outperforming existing solutions.


<details>
  <summary>Details</summary>
Motivation: Addressing the complexity of diagnosing performance or functional instabilities in cloud-hosted applications with numerous potential root causes.

Method: Combines AI pattern matching and a multi-modal RAG LLM interface (ARCA) for problem identification and resolution.

Result: ARCA outperforms state-of-the-art alternatives in step-wise evaluations.

Conclusion: ARCA effectively simplifies and improves the process of identifying and resolving issues in cloud applications.

Abstract: Today's cloud-hosted applications and services are complex systems, and a
performance or functional instability can have dozens or hundreds of potential
root causes. Our hypothesis is that by combining the pattern matching
capabilities of modern AI tools with a natural multi-modal RAG LLM interface,
problem identification and resolution can be simplified. ARCA is a new
multi-modal RAG LLM system that targets this domain. Step-wise evaluations show
that ARCA outperforms state-of-the-art alternatives.

</details>


### [499] [Policy Induction: Predicting Startup Success via Explainable Memory-Augmented In-Context Learning](https://arxiv.org/pdf/2505.21427)
*Xianling Mu, Joseph Ternasky, Fuat Alican, Yigit Ihlamur*

Main category: cs.AI

TL;DR: A transparent, data-efficient framework using memory-augmented LLMs for early-stage startup investment outperforms traditional methods with minimal supervision.


<details>
  <summary>Details</summary>
Motivation: Addressing the high-risk, data-scarce nature of early-stage investments and the opacity of traditional ML approaches.

Method: Uses in-context learning with LLMs, embedding a natural language policy for interpretability and iterative refinement via few-shot learning and feedback.

Result: Achieves 20x higher precision than random chance (1.9%) and 7.1x better than top-tier VC firms (5.6%).

Conclusion: The framework offers a scalable, interpretable solution for startup investment decisions with minimal data and supervision.

Abstract: Early-stage startup investment is a high-risk endeavor characterized by
scarce data and uncertain outcomes. Traditional machine learning approaches
often require large, labeled datasets and extensive fine-tuning, yet remain
opaque and difficult for domain experts to interpret or improve. In this paper,
we propose a transparent and data-efficient investment decision framework
powered by memory-augmented large language models (LLMs) using in-context
learning (ICL). Central to our method is a natural language policy embedded
directly into the LLM prompt, enabling the model to apply explicit reasoning
patterns and allowing human experts to easily interpret, audit, and iteratively
refine the logic. We introduce a lightweight training process that combines
few-shot learning with an in-context learning loop, enabling the LLM to update
its decision policy iteratively based on structured feedback. With only minimal
supervision and no gradient-based optimization, our system predicts startup
success far more accurately than existing benchmarks. It is over 20x more
precise than random chance, which succeeds 1.9% of the time. It is also 7.1x
more precise than the typical 5.6% success rate of top-tier venture capital
(VC) firms.

</details>


### [500] [Robust Hypothesis Generation: LLM-Automated Language Bias for Inductive Logic Programming](https://arxiv.org/pdf/2505.21486)
*Yang Yang, Jiemin Wu, Yutao Yue*

Main category: cs.AI

TL;DR: A novel framework combines LLM-powered multi-agent systems with ILP to automate symbolic grounding and hypothesis generation, outperforming traditional methods.


<details>
  <summary>Details</summary>
Motivation: To overcome the limitations of traditional ILP (reliance on predefined structures) and pure LLM methods (noise sensitivity) for robust hypothesis generation.

Method: Integrates LLM agents to autonomously define symbolic vocabulary and relational templates from raw text, feeding into an ILP solver for rule learning.

Result: Validated in diverse scenarios, the framework shows superior performance in automated, explainable, and verifiable hypothesis generation.

Conclusion: The approach paves a new path for AI cognition by automating symbolic grounding and enhancing interpretability.

Abstract: Automating robust hypothesis generation in open environments is pivotal for
AI cognition. We introduce a novel framework integrating a multi-agent system,
powered by Large Language Models (LLMs), with Inductive Logic Programming
(ILP). Our system's LLM agents autonomously define a structured symbolic
vocabulary (predicates) and relational templates , i.e., \emph{language bias}
directly from raw textual data. This automated symbolic grounding (the
construction of the language bias), traditionally an expert-driven bottleneck
for ILP, then guides the transformation of text into facts for an ILP solver,
which inductively learns interpretable rules. This approach overcomes
traditional ILP's reliance on predefined symbolic structures and the
noise-sensitivity of pure LLM methods. Extensive experiments in diverse,
challenging scenarios validate superior performance, paving a new path for
automated, explainable, and verifiable hypothesis generation.

</details>


### [501] [Predicate Invention for Bilevel Planning](https://arxiv.org/pdf/2203.09634)
*Tom Silver, Rohan Chitnis, Nishanth Kumar, Willie McClinton, Tomas Lozano-Perez, Leslie Pack Kaelbling, Joshua Tenenbaum*

Main category: cs.AI

TL;DR: The paper proposes an algorithm to learn state abstractions (predicates) from demonstrations for bilevel planning, eliminating manual design. It uses a surrogate objective in hill-climbing search, outperforming baselines in robotic tasks.


<details>
  <summary>Details</summary>
Motivation: Planning in continuous spaces is hard, even with known models. Bilevel planning with abstractions helps, but manual predicate design is a bottleneck. This work aims to automate predicate learning.

Method: A surrogate objective is optimized via hill-climbing over predicate sets from a grammar, learning abstractions from demonstrations.

Result: The learned abstractions solve held-out tasks faster than six baselines across four robotic environments.

Conclusion: Automating predicate learning improves bilevel planning efficiency, reducing reliance on manual abstraction design.

Abstract: Efficient planning in continuous state and action spaces is fundamentally
hard, even when the transition model is deterministic and known. One way to
alleviate this challenge is to perform bilevel planning with abstractions,
where a high-level search for abstract plans is used to guide planning in the
original transition space. Previous work has shown that when state abstractions
in the form of symbolic predicates are hand-designed, operators and samplers
for bilevel planning can be learned from demonstrations. In this work, we
propose an algorithm for learning predicates from demonstrations, eliminating
the need for manually specified state abstractions. Our key idea is to learn
predicates by optimizing a surrogate objective that is tractable but faithful
to our real efficient-planning objective. We use this surrogate objective in a
hill-climbing search over predicate sets drawn from a grammar. Experimentally,
we show across four robotic planning environments that our learned abstractions
are able to quickly solve held-out tasks, outperforming six baselines. Code:
https://tinyurl.com/predicators-release

</details>


### [502] [CogReact: A Reinforced Framework to Model Human Cognitive Reaction Modulated by Dynamic Intervention](https://arxiv.org/pdf/2301.06216)
*Songlin Xu, Xinyu Zhang*

Main category: cs.AI

TL;DR: CogReact combines drift-diffusion with deep reinforcement learning to model human cognition in dynamic environments, outperforming baselines in capturing behavioral differences and trends.


<details>
  <summary>Details</summary>
Motivation: Current research often ignores environmental disturbances in cognitive modeling, limiting realism. CogReact addresses this gap by simulating dynamic stimuli effects.

Method: Integrates drift-diffusion models with deep reinforcement learning to account for temporal and granular effects of environmental stimuli on cognition.

Result: Quantitatively improves cognition modeling and captures subject- and stimuli-specific differences. Qualitatively, it better aligns with human cognitive trends under stimuli.

Conclusion: CogReact offers a robust, data-driven approach to understanding human cognitive responses in dynamic contexts, validated across diverse tasks.

Abstract: Using deep neural networks as computational models to simulate cognitive
process can provide key insights into human behavioral dynamics. Challenges
arise when environments are highly dynamic, obscuring stimulus-behavior
relationships. However, the majority of current research focuses on simulating
human cognitive behaviors under ideal conditions, neglecting the influence of
environmental disturbances. We propose CogReact, integrating drift-diffusion
with deep reinforcement learning to simulate granular effects of dynamic
environmental stimuli on human cognitive process. Quantitatively, it improves
cognition modelling by considering temporal effect of environmental stimuli on
cognitive process and captures both subject-specific and stimuli-specific
behavioural differences. Qualitatively, it captures general trends in human
cognitive process under stimuli, better than baselines. Our approach is
examined in diverse environmental influences on various cognitive tasks.
Overall, it demonstrates a powerful, data-driven methodology to simulate, align
with, and understand the vagaries of human cognitive response in dynamic
contexts.

</details>


### [503] [CLEVRER-Humans: Describing Physical and Causal Events the Human Way](https://arxiv.org/pdf/2310.03635)
*Jiayuan Mao, Xuelin Yang, Xikun Zhang, Noah D. Goodman, Jiajun Wu*

Main category: cs.AI

TL;DR: The paper introduces CLEVRER-Humans, a benchmark for causal reasoning in videos with human-labeled data, addressing diversity and human judgment gaps in existing synthetic benchmarks.


<details>
  <summary>Details</summary>
Motivation: Existing benchmarks for physical and causal reasoning lack diversity and human-aligned judgments, limiting their real-world applicability.

Method: The authors use an iterative event cloze task to create Causal Event Graphs (CEGs) and employ neural language models for data augmentation, converting CEGs into QA pairs.

Result: The CLEVRER-Humans benchmark is presented, with baseline approaches demonstrating its challenges.

Conclusion: CLEVRER-Humans sets a new standard for causal reasoning benchmarks, emphasizing the need for human-aligned data and diverse event representations.

Abstract: Building machines that can reason about physical events and their causal
relationships is crucial for flexible interaction with the physical world.
However, most existing physical and causal reasoning benchmarks are exclusively
based on synthetically generated events and synthetic natural language
descriptions of causal relationships. This design brings up two issues. First,
there is a lack of diversity in both event types and natural language
descriptions; second, causal relationships based on manually-defined heuristics
are different from human judgments. To address both shortcomings, we present
the CLEVRER-Humans benchmark, a video reasoning dataset for causal judgment of
physical events with human labels. We employ two techniques to improve data
collection efficiency: first, a novel iterative event cloze task to elicit a
new representation of events in videos, which we term Causal Event Graphs
(CEGs); second, a data augmentation technique based on neural language
generative models. We convert the collected CEGs into questions and answers to
be consistent with prior work. Finally, we study a collection of baseline
approaches for CLEVRER-Humans question-answering, highlighting the great
challenges set forth by our benchmark.

</details>


### [504] [DCA-Bench: A Benchmark for Dataset Curation Agents](https://arxiv.org/pdf/2406.07275)
*Benhao Huang, Yingzhuo Yu, Jin Huang, Xingjian Zhang, Jiaqi Ma*

Main category: cs.AI

TL;DR: The paper addresses data quality issues in AI datasets and proposes using LLM agents to detect hidden problems. A benchmark with 221 test cases and an evaluation framework using GPT-4o is introduced, showing alignment with expert evaluations. Results indicate LLM agents currently detect only ~30% of issues, highlighting the task's complexity.


<details>
  <summary>Details</summary>
Motivation: Data quality issues in AI datasets are common but hard to detect manually or with rule-based methods. Leveraging LLM agents could streamline issue discovery, though detecting unknown issues remains challenging.

Method: A benchmark with 221 real-world test cases from eight dataset platforms is curated. An automatic evaluation framework using GPT-4o is proposed and validated against human annotations.

Result: The framework aligns well with expert evaluations, but LLM agents detect only ~30% of issues without hints, showing the task's difficulty.

Conclusion: Applying LLM agents for dataset curation is promising but requires further innovation due to the complexity of detecting hidden issues. The benchmark and framework are publicly available.

Abstract: The quality of datasets plays an increasingly crucial role in the research
and development of modern artificial intelligence (AI). Despite the
proliferation of open dataset platforms nowadays, data quality issues, such as
incomplete documentation, inaccurate labels, ethical concerns, and outdated
information, remain common in widely used datasets. Furthermore, these issues
are often subtle and difficult to be detected by rule-based scripts, therefore
requiring identification and verification by dataset users or maintainers--a
process that is both time-consuming and prone to human mistakes. With the
surging ability of large language models (LLM), it's promising to streamline
the discovery of hidden dataset issues with LLM agents. To achieve this, one
significant challenge is enabling LLM agents to detect issues in the wild
rather than simply fixing known ones. In this work, we establish a benchmark to
measure LLM agent's ability to tackle this challenge. We carefully curate 221
real-world test cases from eight popular dataset platforms and propose an
automatic evaluation framework using GPT-4o. Our proposed framework shows
strong empirical alignment with expert evaluations, validated through extensive
comparisons with human annotations. Without any hints, most competitive Curator
agent can only reveal $\sim$30\% of the data quality issues in the proposed
dataset, highlighting the complexity of this task and indicating that applying
LLM agents to real-world dataset curation still requires further in-depth
exploration and innovation. The data and code are available at
\href{https://github.com/TRAIS-Lab/dca-bench}{https://github.com/TRAIS-Lab/dca-bench}.

</details>


### [505] ["Oh LLM, I'm Asking Thee, Please Give Me a Decision Tree": Zero-Shot Decision Tree Induction and Embedding with Large Language Models](https://arxiv.org/pdf/2409.18594)
*Ricardo Knauer, Mario Koddenbrock, Raphael Wallsberger, Nicholas M. Brisson, Georg N. Duda, Deborah Falla, David W. Evans, Erik Rodner*

Main category: cs.AI

TL;DR: LLMs generate interpretable decision trees without training data, sometimes outperforming data-driven trees, and offer knowledge-driven baselines for low-data tasks.


<details>
  <summary>Details</summary>
Motivation: Leverage LLMs' world knowledge for predictive modeling when data is scarce, creating interpretable models like decision trees.

Method: Use LLMs to generate zero-shot decision trees and embeddings, comparing them to data-driven trees on small tabular datasets.

Result: Zero-shot trees can surpass data-driven ones on some datasets, and their embeddings perform better on average.

Conclusion: LLM-generated trees provide a knowledge-driven baseline for low-data ML tasks, harnessing LLMs' world knowledge for tabular learning.

Abstract: Large language models (LLMs) provide powerful means to leverage prior
knowledge for predictive modeling when data is limited. In this work, we
demonstrate how LLMs can use their compressed world knowledge to generate
intrinsically interpretable machine learning models, i.e., decision trees,
without any training data. We find that these zero-shot decision trees can even
surpass data-driven trees on some small-sized tabular datasets and that
embeddings derived from these trees perform better than data-driven tree-based
embeddings on average. Our decision tree induction and embedding approaches can
therefore serve as new knowledge-driven baselines for data-driven machine
learning methods in the low-data regime. Furthermore, they offer ways to
harness the rich world knowledge within LLMs for tabular machine learning
tasks. Our code and results are available at
https://github.com/ml-lab-htw/llm-trees.

</details>


### [506] [TMGBench: A Systematic Game Benchmark for Evaluating Strategic Reasoning Abilities of LLMs](https://arxiv.org/pdf/2410.10479)
*Haochuan Wang, Xiachong Feng, Lei Li, Yu Guo, Zhanyue Qin, Dianbo Sui, Lingpeng Kong*

Main category: cs.AI

TL;DR: The paper introduces TMGBench, a benchmark for evaluating strategic reasoning in LLMs, addressing gaps in game type coverage, scenario diversity, and extensibility.


<details>
  <summary>Details</summary>
Motivation: Current evaluations of LLMs' strategic reasoning are limited in game type coverage, risk data leakage, and lack extensibility. TMGBench aims to overcome these issues.

Method: TMGBench includes 144 game types from the Robinson-Goforth topology, diverse story-based scenarios, and organizes games into complex structures (sequential, parallel, nested).

Result: Evaluation showed LLMs have flaws in reasoning accuracy, consistency, and Theory-of-Mind mastery. SOTA models faced challenges in complex game structures.

Conclusion: TMGBench provides a comprehensive, flexible framework for evaluating strategic reasoning in LLMs, revealing current limitations and future challenges.

Abstract: The rapid advancement of large language models has accelerated their
application in reasoning, with strategic reasoning drawing increasing
attention. To evaluate the strategic reasoning capabilities of LLMs, game
theory, with its concise structure, has become the preferred approach for many
researchers. However, current research typically focuses on a limited selection
of games, resulting in low coverage of game types. Additionally, classic game
scenarios carry risks of data leakage, and the benchmarks used often lack
extensibility, rendering them inadequate for evaluating state-of-the-art
models. To address these challenges, we propose TMGBench, characterized by
comprehensive game type coverage, diverse scenarios and flexible game
organization. Specifically, we incorporate all 144 game types summarized by the
Robinson-Goforth topology of 2x2 games, constructed as classic games in our
benchmark; we also synthetize diverse, higher-quality game scenarios for each
classic game, which we refer to as story-based games. Lastly, to provide a
sustainable evaluation framework adaptable to increasingly powerful LLMs, we
treat the aforementioned games as atomic units and organize them into more
complex forms through sequential, parallel, and nested structures. We conducted
a comprehensive evaluation of mainstream LLMs, covering tests on rational
reasoning, reasoning robustness, Theory-of-Mind capabilities, and reasoning in
complex game forms. The results revealed LLMs still have flaws in the accuracy
and consistency of strategic reasoning processes, and their levels of mastery
over Theory-of-Mind also vary. Additionally, SOTA models like o3-mini, Qwen3
and deepseek-reasoner, were also evaluated across the sequential, parallel, and
nested game structures while the results highlighted the challenges posed by
TMGBench.

</details>


### [507] [A Generation Framework with Strict Constraints for Crystal Materials Design](https://arxiv.org/pdf/2411.08464)
*Chao Huang, Jiahui Chen, Chen Chen, Chunyan Chen, Renjie Su, Shiyu Du, ChenChen, Hongrui Liang, Daojing Lin*

Main category: cs.AI

TL;DR: A constrained generation framework for crystal structures uses LLMs to ensure specific properties, outperforming random sampling methods.


<details>
  <summary>Details</summary>
Motivation: Existing crystal structure generation methods lack strict constraints, requiring post-processing to achieve desired properties.

Method: A framework with a constraint generator (using LLMs) and a crystal structure generator ensures controlled generation with predefined properties.

Result: The method doubles the probability of meeting target properties and ensures 100% adherence to chemical composition.

Conclusion: The framework improves efficiency and reliability in generating crystal structures with specific properties.

Abstract: The design of crystal materials plays a critical role in areas such as new
energy development, biomedical engineering, and semiconductors. Recent advances
in data-driven methods have enabled the generation of diverse crystal
structures. However, most existing approaches still rely on random sampling
without strict constraints, requiring multiple post-processing steps to
identify stable candidates with the desired physical and chemical properties.
In this work, we present a new constrained generation framework that takes
multiple constraints as input and enables the generation of crystal structures
with specific chemical and properties. In this framework, intermediate
constraints, such as symmetry information and composition ratio, are generated
by a constraint generator based on large language models (LLMs), which
considers the target properties. These constraints are then used by a
subsequent crystal structure generator to ensure that the structure generation
process is under control. Our method generates crystal structures with a
probability of meeting the target properties that is more than twice that of
existing approaches. Furthermore, nearly 100% of the generated crystals
strictly adhere to predefined chemical composition, eliminating the risks of
supply chain during production.

</details>


### [508] [Aligning Generalisation Between Humans and Machines](https://arxiv.org/pdf/2411.15626)
*Filip Ilievski, Barbara Hammer, Frank van Harmelen, Benjamin Paassen, Sascha Saralajew, Ute Schmid, Michael Biehl, Marianna Bolognesi, Xin Luna Dong, Kiril Gashteovski, Pascal Hitzler, Giuseppe Marra, Pasquale Minervini, Martin Mundt, Axel-Cyrille Ngonga Ngomo, Alessandro Oltramari, Gabriella Pasi, Zeynep G. Saribatur, Luciano Serafini, John Shawe-Taylor, Vered Shwartz, Gabriella Skitalinskaya, Clemens Stachl, Gido M. van de Ven, Thomas Villmann*

Main category: cs.AI

TL;DR: The paper explores the differences in generalization between humans and AI, emphasizing the need for AI alignment in human-AI teams by comparing cognitive science and AI approaches.


<details>
  <summary>Details</summary>
Motivation: To address the challenges of AI alignment in human-AI teams by understanding how humans and AI generalize differently.

Method: Combines insights from AI and cognitive science to analyze generalization across three dimensions: notions, methods, and evaluation.

Result: Identifies key differences and commonalities in generalization between humans and AI, highlighting interdisciplinary challenges.

Conclusion: Interdisciplinary collaboration is essential for effective AI alignment in human-AI teaming scenarios.

Abstract: Recent advances in AI -- including generative approaches -- have resulted in
technology that can support humans in scientific discovery and forming
decisions, but may also disrupt democracies and target individuals. The
responsible use of AI and its participation in human-AI teams increasingly
shows the need for AI alignment, that is, to make AI systems act according to
our preferences. A crucial yet often overlooked aspect of these interactions is
the different ways in which humans and machines generalise. In cognitive
science, human generalisation commonly involves abstraction and concept
learning. In contrast, AI generalisation encompasses out-of-domain
generalisation in machine learning, rule-based reasoning in symbolic AI, and
abstraction in neurosymbolic AI. In this perspective paper, we combine insights
from AI and cognitive science to identify key commonalities and differences
across three dimensions: notions of, methods for, and evaluation of
generalisation. We map the different conceptualisations of generalisation in AI
and cognitive science along these three dimensions and consider their role for
alignment in human-AI teaming. This results in interdisciplinary challenges
across AI and cognitive science that must be tackled to provide a foundation
for effective and cognitively supported alignment in human-AI teaming
scenarios.

</details>


### [509] [Leveraging Large Language Models for Active Merchant Non-player Characters](https://arxiv.org/pdf/2412.11189)
*Byungjun Kim, Minju Kim, Dayeon Seo, Bugeun Kim*

Main category: cs.AI

TL;DR: The paper addresses passive merchant NPCs by proposing MART, a framework using LLMs for pricing and communication, showing effectiveness with smaller LLMs via finetuning.


<details>
  <summary>Details</summary>
Motivation: Current merchant NPCs lack dynamic pricing and communication, limiting player interaction.

Method: Proposes MART, a framework with appraiser and negotiator modules, tested with various LLM sizes and training methods.

Result: Finetuning methods (SFT, KD) work well with smaller LLMs; three irregular cases identified from LLM responses.

Conclusion: MART effectively enhances merchant NPC activity, though LLM responses may require further refinement.

Abstract: We highlight two significant issues leading to the passivity of current
merchant non-player characters (NPCs): pricing and communication. While
immersive interactions with active NPCs have been a focus, price negotiations
between merchant NPCs and players remain underexplored. First, passive pricing
refers to the limited ability of merchants to modify predefined item prices.
Second, passive communication means that merchants can only interact with
players in a scripted manner. To tackle these issues and create an active
merchant NPC, we propose a merchant framework based on large language models
(LLMs), called MART, which consists of an appraiser module and a negotiator
module. We conducted two experiments to explore various implementation options
under different training methods and LLM sizes, considering a range of possible
game environments. Our findings indicate that finetuning methods, such as
supervised finetuning (SFT) and knowledge distillation (KD), are effective in
using smaller LLMs to implement active merchant NPCs. Additionally, we found
three irregular cases arising from the responses of LLMs.

</details>


### [510] [Transparent and Coherent Procedural Mistake Detection](https://arxiv.org/pdf/2412.11927)
*Shane Storks, Itamar Bar-Yossef, Yayuan Li, Zheyuan Zhang, Jason J. Corso, Joyce Chai*

Main category: cs.AI

TL;DR: The paper reframes Procedural Mistake Detection (PMD) to include visual self-dialog rationales, leveraging vision-and-language models (VLMs) and introducing automated metrics for rationale coherence.


<details>
  <summary>Details</summary>
Motivation: Current PMD methods lack transparency and perform poorly in real-world settings, prompting the need for explainable rationales.

Method: Extends PMD to generate visual self-dialog rationales, uses VLMs for frame-based benchmarking, and introduces NLI-based metrics for rationale coherence.

Result: VLMs struggle initially but improve in accuracy, coherence, and efficiency when metrics are integrated into inference and fine-tuning, though with tradeoffs.

Conclusion: The reformulation enhances transparency, and the introduced metrics highlight areas for future improvement in PMD.

Abstract: Procedural mistake detection (PMD) is a challenging problem of classifying
whether a human user (observed through egocentric video) has successfully
executed a task (specified by a procedural text). Despite significant recent
efforts, machine performance in the wild remains nonviable, and the reasoning
processes underlying this performance are opaque. As such, we extend PMD to
require generating visual self-dialog rationales to inform decisions. Given the
impressive, mature image understanding capabilities observed in recent
vision-and-language models (VLMs), we curate a suitable benchmark dataset for
PMD based on individual frames. As our reformulation enables unprecedented
transparency, we leverage a natural language inference (NLI) model to formulate
two automated metrics for the coherence of generated rationales. We establish
baselines for this reframed task, showing that while VLMs struggle
off-the-shelf, their accuracy, coherence, and efficiency can be improved by
incorporating these metrics into common inference and fine-tuning methods-
though not without tradeoff. Lastly, our multi-faceted metrics visualize common
outcomes, highlighting areas for further improvement.

</details>


### [511] [Causal Composition Diffusion Model for Closed-loop Traffic Generation](https://arxiv.org/pdf/2412.17920)
*Haohong Lin, Xin Huang, Tung Phan-Minh, David S. Hayden, Huan Zhang, Ding Zhao, Siddhartha Srinivasa, Eric M. Wolff, Hongge Chen*

Main category: cs.AI

TL;DR: CCDiff, a causal compositional diffusion model, improves realism and controllability in autonomous driving simulations by integrating causal structures into the diffusion process.


<details>
  <summary>Details</summary>
Motivation: Addressing the challenge of generating realistic and controllable traffic scenarios in long-tail, safety-critical situations where existing models fail to balance controllability and realism.

Method: Formulates the problem as constrained optimization, using CCDiff to inject causal structures into the diffusion process for structured guidance.

Result: Outperforms state-of-the-art methods in generating realistic and user-preferred trajectories, improving metrics like collision rate and comfort.

Conclusion: CCDiff effectively leverages causal structures to enhance both realism and controllability in autonomous driving simulations.

Abstract: Simulation is critical for safety evaluation in autonomous driving,
particularly in capturing complex interactive behaviors. However, generating
realistic and controllable traffic scenarios in long-tail situations remains a
significant challenge. Existing generative models suffer from the conflicting
objective between user-defined controllability and realism constraints, which
is amplified in safety-critical contexts. In this work, we introduce the Causal
Compositional Diffusion Model (CCDiff), a structure-guided diffusion framework
to address these challenges. We first formulate the learning of controllable
and realistic closed-loop simulation as a constrained optimization problem.
Then, CCDiff maximizes controllability while adhering to realism by
automatically identifying and injecting causal structures directly into the
diffusion process, providing structured guidance to enhance both realism and
controllability. Through rigorous evaluations on benchmark datasets and in a
closed-loop simulator, CCDiff demonstrates substantial gains over
state-of-the-art approaches in generating realistic and user-preferred
trajectories. Our results show CCDiff's effectiveness in extracting and
leveraging causal structures, showing improved closed-loop performance based on
key metrics such as collision rate, off-road rate, FDE, and comfort.

</details>


### [512] [ChartCoder: Advancing Multimodal Large Language Model for Chart-to-Code Generation](https://arxiv.org/pdf/2501.06598)
*Xuanle Zhao, Xianzhen Luo, Qi Shi, Chi Chen, Shuo Wang, Zhiyuan Liu, Maosong Sun*

Main category: cs.AI

TL;DR: ChartCoder, a 7B-parameter MLLM, improves chart-to-code tasks by enhancing code executability and introducing a large-scale dataset (Chart2Code-160k) and Snippet-of-Thought method.


<details>
  <summary>Details</summary>
Motivation: Existing MLLMs lose information when interpreting charts as text and struggle with executability and data diversity in chart-to-code tasks.

Method: Leverages Code LLMs for better code generation, introduces Chart2Code-160k dataset, and uses Snippet-of-Thought for step-by-step generation.

Result: Outperforms existing MLLMs in chart restoration and code executability.

Conclusion: ChartCoder addresses key challenges in chart-to-code tasks, offering a scalable and effective solution.

Abstract: Multimodal Large Language Models (MLLMs) have demonstrated remarkable
capabilities in chart understanding tasks. However, interpreting charts with
textual descriptions often leads to information loss, as it fails to fully
capture the dense information embedded in charts. In contrast, parsing charts
into code provides lossless representations that can effectively contain all
critical details. Although existing open-source MLLMs have achieved success in
chart understanding tasks, they still face two major challenges when applied to
chart-to-code tasks: (1) Low executability and poor restoration of chart
details in the generated code and (2) Lack of large-scale and diverse training
data. To address these challenges, we propose \textbf{ChartCoder}, the first
dedicated chart-to-code MLLM, which leverages Code LLMs as the language
backbone to enhance the executability of the generated code. Furthermore, we
introduce \textbf{Chart2Code-160k}, the first large-scale and diverse dataset
for chart-to-code generation, and propose the \textbf{Snippet-of-Thought (SoT)}
method, which transforms direct chart-to-code generation data into step-by-step
generation. Experiments demonstrate that ChartCoder, with only 7B parameters,
surpasses existing open-source MLLMs on chart-to-code benchmarks, achieving
superior chart restoration and code excitability. Our code is available at
https://github.com/thunlp/ChartCoder.

</details>


### [513] [When More is Less: Understanding Chain-of-Thought Length in LLMs](https://arxiv.org/pdf/2502.07266)
*Yuyang Wu, Yifei Wang, Ziyu Ye, Tianqi Du, Stefanie Jegelka, Yisen Wang*

Main category: cs.AI

TL;DR: Longer Chain-of-Thought (CoT) reasoning isn't always better; performance peaks at an optimal length, influenced by task difficulty and model capability.


<details>
  <summary>Details</summary>
Motivation: Challenge the assumption that longer CoTs improve reasoning, exploring the relationship between CoT length and task accuracy.

Method: Combined real-world observations, controlled experiments, and theoretical analysis to study CoT length's impact.

Result: Task accuracy follows an inverted U-curve with CoT length; optimal length scales with task difficulty and inversely with model capability.

Conclusion: Optimal CoT length and simplicity bias improve reasoning; practical guidelines for adaptive CoT calibration are provided.

Abstract: Large Language Models (LLMs) employ Chain-of-Thought (CoT) reasoning to
deconstruct complex problems. While longer CoTs are often presumed superior,
this paper challenges that notion, arguing that longer is not always better.
Drawing on combined evidence from real-world observations, controlled
experiments, and theoretical analysis, we demonstrate that task accuracy
typically follows an inverted U-shaped curve with CoT length, where performance
initially improves but eventually decreases as the number of CoT steps
increases. With controlled experiments, we further uncover the scaling
behaviors of the optimal CoT length: it increases with task difficulty but
decreases with model capability, exposing an inherent simplicity bias where
more capable models favor shorter, more efficient CoT reasoning. This bias is
also evident in Reinforcement Learning (RL) training, where models gravitate
towards shorter CoTs as their accuracy improves. To have a deep understanding
of these dynamics, we establish a simple theoretical model that formally proves
these phenomena, including the optimal length's scaling laws and the emergence
of simplicity bias during RL. Guided by this framework, we demonstrate
significant practical benefits from training with optimally-lengthed CoTs and
employing length-aware filtering at inference. These findings offer both a
principled understanding of the "overthinking" phenomenon and multiple
practical guidelines for CoT calibration, enabling LLMs to achieve optimal
reasoning performance with adaptive CoTs tailored to task complexity and model
capability.

</details>


### [514] [Path Pooling: Training-Free Structure Enhancement for Efficient Knowledge Graph Retrieval-Augmented Generation](https://arxiv.org/pdf/2503.05203)
*Hairu Wang, Yuan Feng, Xike Xie, S Kevin Zhou*

Main category: cs.AI

TL;DR: The paper introduces 'path pooling,' a training-free method to enhance knowledge graph-based retrieval-augmented generation (KG-RAG) by better utilizing structure information, improving performance without significant cost.


<details>
  <summary>Details</summary>
Motivation: Large Language Models (LLMs) suffer from hallucinations and knowledge gaps. Existing KG-RAG methods fail to efficiently use structure information, leading to high costs or underutilization.

Method: Proposes 'path pooling,' a path-centric pooling operation inspired by graph representation learning, to integrate structure information into KG-RAG methods.

Result: Experiments show path pooling improves state-of-the-art KG-RAG performance across settings with minimal added cost.

Conclusion: Path pooling effectively enhances KG-RAG by leveraging structure information, offering a simple, plug-and-play solution.

Abstract: Although Large Language Models achieve strong success in many tasks, they
still suffer from hallucinations and knowledge deficiencies in real-world
applications. Many knowledge graph-based retrieval-augmented generation
(KG-RAG) methods enhance the quality and credibility of LLMs by leveraging
structure and semantic information in KGs as external knowledge bases. However,
these methods struggle to effectively incorporate structure information, either
incurring high computational costs or underutilizing available knowledge.
Inspired by smoothing operations in graph representation learning, we propose
path pooling, a simple, training-free strategy that introduces structure
information through a novel path-centric pooling operation. It seamlessly
integrates into existing KG-RAG methods in a plug-and-play manner, enabling
richer structure information utilization. Extensive experiments demonstrate
that incorporating the path pooling into the state-of-the-art KG-RAG method
consistently improves performance across various settings while introducing
negligible additional cost.

</details>


### [515] [Exploring the Necessity of Reasoning in LLM-based Agent Scenarios](https://arxiv.org/pdf/2503.11074)
*Xueyang Zhou, Guiyao Tie, Guowen Zhang, Weidong Wang, Zhigang Zuo, Di Wu, Duanfeng Chu, Pan Zhou, Neil Zhenqiang Gong, Lichao Sun*

Main category: cs.AI

TL;DR: The paper introduces the LaRMA framework to compare Large Reasoning Models (LRMs) and Large Language Models (LLMs), finding LRMs excel in reasoning tasks while LLMs perform better in execution tasks. Hybrid models optimize performance, but LRMs come with higher computational costs and behavioral challenges.


<details>
  <summary>Details</summary>
Motivation: To explore the disruption caused by LRMs in traditional agent frameworks anchored by LLMs and assess their comparative strengths and weaknesses.

Method: Proposed the LaRMA framework, evaluating nine tasks across Tool Usage, Plan Design, and Problem Solving using three top LLMs and five leading LRMs.

Result: LRMs outperform LLMs in reasoning tasks (e.g., Plan Design), while LLMs excel in execution tasks (e.g., Tool Usage). Hybrid models combining both optimize performance, but LRMs have higher computational costs and behavioral issues.

Conclusion: The study highlights the trade-offs between LRMs and LLMs, suggesting hybrid models as a solution and calling for further research into balancing deep reasoning with overthinking in LRMs.

Abstract: The rise of Large Reasoning Models (LRMs) signifies a paradigm shift toward
advanced computational reasoning. Yet, this progress disrupts traditional agent
frameworks, traditionally anchored by execution-oriented Large Language Models
(LLMs). To explore this transformation, we propose the LaRMA framework,
encompassing nine tasks across Tool Usage, Plan Design, and Problem Solving,
assessed with three top LLMs (e.g., Claude3.5-sonnet) and five leading LRMs
(e.g., DeepSeek-R1). Our findings address four research questions: LRMs surpass
LLMs in reasoning-intensive tasks like Plan Design, leveraging iterative
reflection for superior outcomes; LLMs excel in execution-driven tasks such as
Tool Usage, prioritizing efficiency; hybrid LLM-LRM configurations, pairing
LLMs as actors with LRMs as reflectors, optimize agent performance by blending
execution speed with reasoning depth; and LRMs' enhanced reasoning incurs
higher computational costs, prolonged processing, and behavioral challenges,
including overthinking and fact-ignoring tendencies. This study fosters deeper
inquiry into LRMs' balance of deep thinking and overthinking, laying a critical
foundation for future agent design advancements.

</details>


### [516] [HA-VLN: A Benchmark for Human-Aware Navigation in Discrete-Continuous Environments with Dynamic Multi-Human Interactions, Real-World Validation, and an Open Leaderboard](https://arxiv.org/pdf/2503.14229)
*Yifei Dong, Fengyi Wu, Qi He, Heng Li, Minghan Li, Zebang Cheng, Yuxuan Zhou, Jingdong Sun, Qi Dai, Zhi-Qi Cheng, Alexander G Hauptmann*

Main category: cs.AI

TL;DR: The paper introduces a unified Human-Aware VLN (HA-VLN) benchmark combining discrete and continuous navigation paradigms with social-awareness constraints, validated by real-world robot tests and extensive benchmarking.


<details>
  <summary>Details</summary>
Motivation: Current VLN systems overlook human-populated, dynamic environments, necessitating a unified approach integrating social-awareness for safer navigation.

Method: The authors propose a standardized task definition, an enhanced human motion dataset (HAPS 2.0), upgraded simulators, and extensive benchmarking on human-centric instructions.

Result: Empirical results show improved navigation success and fewer collisions when social context is integrated.

Conclusion: The release of datasets, simulators, and tools aims to advance safer, socially responsible VLN research.

Abstract: Vision-and-Language Navigation (VLN) systems often focus on either discrete
(panoramic) or continuous (free-motion) paradigms alone, overlooking the
complexities of human-populated, dynamic environments. We introduce a unified
Human-Aware VLN (HA-VLN) benchmark that merges these paradigms under explicit
social-awareness constraints. Our contributions include: 1. A standardized task
definition that balances discrete-continuous navigation with personal-space
requirements; 2. An enhanced human motion dataset (HAPS 2.0) and upgraded
simulators capturing realistic multi-human interactions, outdoor contexts, and
refined motion-language alignment; 3. Extensive benchmarking on 16,844
human-centric instructions, revealing how multi-human dynamics and partial
observability pose substantial challenges for leading VLN agents; 4. Real-world
robot tests validating sim-to-real transfer in crowded indoor spaces; and 5. A
public leaderboard supporting transparent comparisons across discrete and
continuous tasks. Empirical results show improved navigation success and fewer
collisions when social context is integrated, underscoring the need for
human-centric design. By releasing all datasets, simulators, agent code, and
evaluation tools, we aim to advance safer, more capable, and socially
responsible VLN research.

</details>


### [517] [VisEscape: A Benchmark for Evaluating Exploration-driven Decision-making in Virtual Escape Rooms](https://arxiv.org/pdf/2503.14427)
*Seungwon Lim, Sungwoong Kim, Jihwan Yu, Sungjae Lee, Jiwan Chung, Youngjae Yu*

Main category: cs.AI

TL;DR: VisEscape is a benchmark of 20 virtual escape rooms to test AI models' exploration-driven planning, showing current models struggle but improve with memory and reasoning integration.


<details>
  <summary>Details</summary>
Motivation: To evaluate AI models in dynamic, exploration-driven environments like escape rooms, where success requires iterative knowledge construction.

Method: Introduce VisEscape, a benchmark with 20 virtual escape rooms, and test state-of-the-art multi-modal models, integrating memory and reasoning.

Result: Current models generally fail but show improvement with memory management and reasoning, aiding efficient exploration.

Conclusion: Memory and reasoning integration enhances AI performance in dynamic, exploration-driven tasks like escape rooms.

Abstract: Escape rooms present a unique cognitive challenge that demands
exploration-driven planning: with the sole instruction to 'escape the room',
players must actively search their environment, collecting information, and
finding solutions through repeated trial and error. Motivated by this, we
introduce VisEscape, a benchmark of 20 virtual escape rooms specifically
designed to evaluate AI models under these challenging conditions, where
success depends not only on solving isolated puzzles but also on iteratively
constructing and refining spatial-temporal knowledge of a dynamically changing
environment. On VisEscape, we observe that even state-of-the-art multi-modal
models generally fail to escape the rooms, showing considerable variation in
their progress and problem-solving approaches. We find that integrating memory
management and reasoning contributes to efficient exploration and enables
successive hypothesis formulation and testing, thereby leading to significant
improvements in dynamic and exploration-driven environments

</details>


### [518] [Stop Anthropomorphizing Intermediate Tokens as Reasoning/Thinking Traces!](https://arxiv.org/pdf/2504.09762)
*Subbarao Kambhampati, Kaya Stechly, Karthik Valmeekam, Lucas Saldyt, Siddhant Bhambri, Vardhan Palod, Atharva Gundawar, Soumya Rani Samineni, Durgesh Kalwar, Upasana Biswas*

Main category: cs.AI

TL;DR: The paper critiques the anthropomorphization of intermediate token generation (ITG) in language models, arguing it misleads understanding and research.


<details>
  <summary>Details</summary>
Motivation: To highlight the dangers of anthropomorphizing ITG, as it distorts the perception of model behavior and research practices.

Method: Presents evidence against the anthropomorphic interpretation of ITG, analyzing its implications.

Result: Shows that anthropomorphization confuses model nature and leads to questionable research.

Conclusion: Urges caution against anthropomorphizing ITG to avoid misleading interpretations and research.

Abstract: Intermediate token generation (ITG), where a model produces output before the
solution, has been proposed as a method to improve the performance of language
models on reasoning tasks. These intermediate tokens have been called
"reasoning traces" or even "thoughts" -- implicitly anthropomorphizing the
model, implying these tokens resemble steps a human might take when solving a
challenging problem.In this paper, we present evidence that this
anthropomorphization isn't a harmless metaphor, and instead is quite dangerous
-- it confuses the nature of these models and how to use them effectively, and
leads to questionable research.

</details>


### [519] [Benchmarking Spatiotemporal Reasoning in LLMs and Reasoning Models: Capabilities and Challenges](https://arxiv.org/pdf/2505.11618)
*Pengrui Quan, Brian Wang, Kang Yang, Liying Han, Mani Srivastava*

Main category: cs.AI

TL;DR: The paper introduces STARK, a benchmark for evaluating LLMs and LRMs in spatiotemporal reasoning tasks, revealing LRMs outperform LLMs in geometric reasoning but the gap narrows in world-knowledge tasks.


<details>
  <summary>Details</summary>
Motivation: To address the underexplored capacity of LLMs and LRMs in complex spatiotemporal reasoning for CPS, the paper proposes STARK for systematic evaluation.

Method: STARK evaluates models across three complexity levels (state estimation, spatiotemporal reasoning, world-knowledge-aware reasoning) using 26 tasks with 14,552 challenges.

Result: LLMs struggle with geometric reasoning, while LRMs perform robustly, often surpassing traditional methods. In world-knowledge tasks, some LLMs outperform LRMs, but the LRM o3 model leads overall.

Conclusion: STARK highlights limitations in spatiotemporal reasoning of LLMs and LRMs, motivating future innovations in model architectures for intelligent CPS.

Abstract: Spatiotemporal reasoning plays a key role in Cyber-Physical Systems (CPS).
Despite advances in Large Language Models (LLMs) and Large Reasoning Models
(LRMs), their capacity to reason about complex spatiotemporal signals remains
underexplored. This paper proposes a hierarchical SpatioTemporal reAsoning
benchmaRK, STARK, to systematically evaluate LLMs across three levels of
reasoning complexity: state estimation (e.g., predicting field variables,
localizing and tracking events in space and time), spatiotemporal reasoning
over states (e.g., inferring spatial-temporal relationships), and
world-knowledge-aware reasoning that integrates contextual and domain knowledge
(e.g., intent prediction, landmark-aware navigation). We curate 26 distinct
spatiotemporal tasks with diverse sensor modalities, comprising 14,552
challenges where models answer directly or by Python Code Interpreter.
Evaluating 3 LRMs and 8 LLMs, we find LLMs achieve limited success in tasks
requiring geometric reasoning (e.g., multilateration or triangulation),
particularly as complexity increases. Surprisingly, LRMs show robust
performance across tasks with various levels of difficulty, often competing or
surpassing traditional first-principle-based methods. Our results show that in
reasoning tasks requiring world knowledge, the performance gap between LLMs and
LRMs narrows, with some LLMs even surpassing LRMs. However, the LRM o3 model
continues to achieve leading performance across all evaluated tasks, a result
attributed primarily to the larger size of the reasoning models. STARK
motivates future innovations in model architectures and reasoning paradigms for
intelligent CPS by providing a structured framework to identify limitations in
the spatiotemporal reasoning of LLMs and LRMs.

</details>


### [520] [FRABench and GenEval: Scaling Fine-Grained Aspect Evaluation across Tasks, Modalities](https://arxiv.org/pdf/2505.12795)
*Shibo Hong, Jiahao Ying, Haiyuan Liang, Mengdi Zhang, Jun Kuang, Jiazheng Zhang, Yixin Cao*

Main category: cs.AI

TL;DR: The paper proposes a hierarchical aspect taxonomy and FRABench for fine-grained evaluation of LLMs, introducing GenEval, a generalizable evaluator that performs well across tasks and modalities.


<details>
  <summary>Details</summary>
Motivation: Existing LLM evaluators are narrow and inconsistent, necessitating a more generalizable and objective approach.

Method: Develops a hierarchical aspect taxonomy (112 aspects) and FRABench (60.4k samples, 325k labels) to train GenEval, a fine-grained evaluator.

Result: GenEval agrees with GPT-4o and experts, transfers well to unseen tasks/modalities, and identifies LMM weaknesses.

Conclusion: Fine-grained aspect specification improves LLM evaluation, with GenEval demonstrating strong generalizability and utility.

Abstract: Evaluating the open-ended outputs of large language models (LLMs) has become
a bottleneck as model capabilities, task diversity, and modality coverage
rapidly expand. Existing "LLM-as-a-Judge" evaluators are typically narrow in a
few tasks, aspects, or modalities, and easily suffer from low consistency. In
this paper, we argue that explicit, fine-grained aspect specification is the
key to both generalizability and objectivity in automated evaluation. To this
end, we propose a hierarchical aspect taxonomy encompassing 112 distinct
aspects that unifies evaluation across four representative settings -- Natural
Language Generation, Image Understanding, Image Generation, and Interleaved
Text-and-Image Generation. Building upon this taxonomy, we create FRABench, a
benchmark comprising 60.4k pairwise samples with 325k evaluation labels
obtained from a combination of human and LLM annotations. FRABench provides the
first large-scale, multi-modal resource for training and meta-evaluating
fine-grained LMM judges. Leveraging FRABench, we develop GenEval, a
fine-grained evaluator generalizable across tasks and modalities. Experiments
show that GenEval (i) attains high agreement with GPT-4o and expert annotators,
(ii) transfers robustly to unseen tasks and modalities, and (iii) reveals
systematic weaknesses of current LMMs on evaluation.

</details>


### [521] [Unveiling and Steering Connectome Organization with Interpretable Latent Variables](https://arxiv.org/pdf/2505.13011)
*Yubin Li, Xingyu Liu, Guozhang Chen*

Main category: cs.AI

TL;DR: The paper proposes a framework combining subgraph extraction from the Drosophila connectome with a generative model to derive low-dimensional representations of neural circuitry, validated by graph reconstruction and controllable generation.


<details>
  <summary>Details</summary>
Motivation: To uncover low-dimensional organizational principles of the brain's connectome, bridging connectomics and representation learning.

Method: Combines subgraph extraction from FlyWire with a generative model and an explainability module to link latent dimensions to structural features.

Result: Effective graph reconstruction and controllable generation of connectome subgraphs with predefined properties.

Conclusion: Provides a novel tool for understanding brain architecture and potential applications in bio-inspired artificial neural networks.

Abstract: The brain's intricate connectome, a blueprint for its function, presents
immense complexity, yet it arises from a compact genetic code, hinting at
underlying low-dimensional organizational principles. This work bridges
connectomics and representation learning to uncover these principles. We
propose a framework that combines subgraph extraction from the Drosophila
connectome, FlyWire, with a generative model to derive interpretable
low-dimensional representations of neural circuitry. Crucially, an
explainability module links these latent dimensions to specific structural
features, offering insights into their functional relevance. We validate our
approach by demonstrating effective graph reconstruction and, significantly,
the ability to manipulate these latent codes to controllably generate
connectome subgraphs with predefined properties. This research offers a novel
tool for understanding brain architecture and a potential avenue for designing
bio-inspired artificial neural networks.

</details>


### [522] [TelePlanNet: An AI-Driven Framework for Efficient Telecom Network Planning](https://arxiv.org/pdf/2505.13831)
*Zongyuan Deng, Yujie Cai, Qing Liu, Shiyao Mu, Bin Lyu, Zhen Yang*

Main category: cs.AI

TL;DR: TelePlanNet is an AI-driven framework for 5G base station site selection, improving efficiency and consistency over manual methods by using LLMs and GRPO reinforcement learning.


<details>
  <summary>Details</summary>
Motivation: Traditional manual methods and existing AI tools are inefficient and struggle with dynamic network conditions and multi-objective optimization in 5G planning.

Method: TelePlanNet integrates a three-layer architecture with LLMs for real-time input processing and GRPO reinforcement learning for multi-objective optimization.

Result: Experiments show TelePlanNet improves planning-construction consistency to 78%, outperforming manual methods.

Conclusion: TelePlanNet provides telecom operators with an efficient, scalable solution for advanced 5G network planning.

Abstract: The selection of base station sites is a critical challenge in 5G network
planning, which requires efficient optimization of coverage, cost, user
satisfaction, and practical constraints. Traditional manual methods, reliant on
human expertise, suffer from inefficiencies and are limited to an unsatisfied
planning-construction consistency. Existing AI tools, despite improving
efficiency in certain aspects, still struggle to meet the dynamic network
conditions and multi-objective needs of telecom operators' networks. To address
these challenges, we propose TelePlanNet, an AI-driven framework tailored for
the selection of base station sites, integrating a three-layer architecture for
efficient planning and large-scale automation. By leveraging large language
models (LLMs) for real-time user input processing and intent alignment with
base station planning, combined with training the planning model using the
improved group relative policy optimization (GRPO) reinforcement learning, the
proposed TelePlanNet can effectively address multi-objective optimization,
evaluates candidate sites, and delivers practical solutions. Experiments
results show that the proposed TelePlanNet can improve the consistency to 78%,
which is superior to the manual methods, providing telecom operators with an
efficient and scalable tool that significantly advances cellular network
planning.

</details>


### [523] [SAFEPATH: Preventing Harmful Reasoning in Chain-of-Thought via Early Alignment](https://arxiv.org/pdf/2505.14667)
*Wonje Jeung, Sangyeon Yoon, Minsuk Kahng, Albert No*

Main category: cs.AI

TL;DR: SAFEPATH is a lightweight alignment method for Large Reasoning Models (LRMs) that emits a Safety Primer for harmful prompts, reducing harmful outputs by 90% and blocking 83.3% of jailbreak attacks, while maintaining reasoning performance.


<details>
  <summary>Details</summary>
Motivation: Existing safety alignment methods for LRMs degrade reasoning depth and remain vulnerable to jailbreak attacks, creating a need for a more effective solution.

Method: SAFEPATH fine-tunes LRMs to emit an 8-token Safety Primer at the start of reasoning for harmful prompts, leaving the rest unsupervised. It also offers a zero-shot variant.

Result: SAFEPATH reduces harmful responses by up to 90.0%, blocks 83.3% of jailbreak attempts, and requires significantly less compute than alternatives.

Conclusion: SAFEPATH effectively balances safety and reasoning performance, with potential for broader application in safer AI development.

Abstract: Large Reasoning Models (LRMs) have become powerful tools for complex problem
solving, but their structured reasoning pathways can lead to unsafe outputs
when exposed to harmful prompts. Existing safety alignment methods reduce
harmful outputs but can degrade reasoning depth, leading to significant
trade-offs in complex, multi-step tasks, and remain vulnerable to sophisticated
jailbreak attacks. To address this, we introduce SAFEPATH, a lightweight
alignment method that fine-tunes LRMs to emit a short, 8-token Safety Primer at
the start of their reasoning, in response to harmful prompts, while leaving the
rest of the reasoning process unsupervised. Empirical results across multiple
benchmarks indicate that SAFEPATH effectively reduces harmful outputs while
maintaining reasoning performance. Specifically, SAFEPATH reduces harmful
responses by up to 90.0% and blocks 83.3% of jailbreak attempts in the
DeepSeek-R1-Distill-Llama-8B model, while requiring 295.9x less compute than
Direct Refusal and 314.1x less than SafeChain. We further introduce a zero-shot
variant that requires no fine-tuning. In addition, we provide a comprehensive
analysis of how existing methods in LLMs generalize, or fail, when applied to
reasoning-centric models, revealing critical gaps and new directions for safer
AI.

</details>


### [524] [Two Experts Are All You Need for Steering Thinking: Reinforcing Cognitive Effort in MoE Reasoning Models Without Additional Training](https://arxiv.org/pdf/2505.14681)
*Mengru Wang, Xingyu Chen, Yue Wang, Zhiwei He, Jiahao Xu, Tian Liang, Qiuzhi Liu, Yunzhi Yao, Wenxuan Wang, Ruotian Ma, Haitao Mi, Ningyu Zhang, Zhaopeng Tu, Xiaolong Li, Dong Yu*

Main category: cs.AI

TL;DR: The paper introduces RICE, a method to improve reasoning in MoE-based LRMs by reinforcing cognitive experts using nPMI, enhancing accuracy and efficiency without extra training.


<details>
  <summary>Details</summary>
Motivation: Address cognitive inefficiencies like overthinking and underthinking in existing reasoning models.

Method: Uses nPMI to identify and reinforce specialized cognitive experts, focusing on meta-level reasoning tokens.

Result: Shows consistent improvements in reasoning accuracy, efficiency, and generalization on benchmarks like DeepSeek-R1 and Qwen3-235B.

Conclusion: RICE is a lightweight, interpretable, and effective approach to enhance cognitive efficiency in reasoning models.

Abstract: Mixture-of-Experts (MoE) architectures within Large Reasoning Models (LRMs)
have achieved impressive reasoning capabilities by selectively activating
experts to facilitate structured cognitive processes. Despite notable advances,
existing reasoning models often suffer from cognitive inefficiencies like
overthinking and underthinking. To address these limitations, we introduce a
novel inference-time steering methodology called Reinforcing Cognitive Experts
(RICE), designed to improve reasoning performance without additional training
or complex heuristics. Leveraging normalized Pointwise Mutual Information
(nPMI), we systematically identify specialized experts, termed ''cognitive
experts'' that orchestrate meta-level reasoning operations characterized by
tokens like ''<think>''. Empirical evaluations with leading MoE-based LRMs
(DeepSeek-R1 and Qwen3-235B) on rigorous quantitative and scientific reasoning
benchmarks demonstrate noticeable and consistent improvements in reasoning
accuracy, cognitive efficiency, and cross-domain generalization. Crucially, our
lightweight approach substantially outperforms prevalent reasoning-steering
techniques, such as prompt design and decoding constraints, while preserving
the model's general instruction-following skills. These results highlight
reinforcing cognitive experts as a promising, practical, and interpretable
direction to enhance cognitive efficiency within advanced reasoning models.

</details>


### [525] [SPhyR: Spatial-Physical Reasoning Benchmark on Material Distribution](https://arxiv.org/pdf/2505.16048)
*Philipp D. Siedler*

Main category: cs.AI

TL;DR: A new dataset benchmarks LLMs' physical and spatial reasoning using topology optimization tasks, challenging models to predict material distributions under given constraints without simulation tools.


<details>
  <summary>Details</summary>
Motivation: To evaluate LLMs' capabilities in physical and spatial reasoning, complementing traditional language and logic benchmarks.

Method: The dataset provides 2D boundary conditions, forces, and supports, requiring LLMs to predict optimal material distributions for tasks like filling masked regions or predicting complete structures.

Result: The dataset challenges LLMs to reason about structural stability and spatial organization without explicit physical models or simulations.

Conclusion: This dataset offers a novel way to assess LLMs' reasoning in 2D physical and spatial contexts, expanding beyond conventional benchmarks.

Abstract: We introduce a novel dataset designed to benchmark the physical and spatial
reasoning capabilities of Large Language Models (LLM) based on topology
optimization, a method for computing optimal material distributions within a
design space under prescribed loads and supports. In this dataset, LLMs are
provided with conditions such as 2D boundary, applied forces and supports, and
must reason about the resulting optimal material distribution. The dataset
includes a variety of tasks, ranging from filling in masked regions within
partial structures to predicting complete material distributions. Solving these
tasks requires understanding the flow of forces and the required material
distribution under given constraints, without access to simulation tools or
explicit physical models, challenging models to reason about structural
stability and spatial organization. Our dataset targets the evaluation of
spatial and physical reasoning abilities in 2D settings, offering a
complementary perspective to traditional language and logic benchmarks.

</details>


### [526] [Internal Bias in Reasoning Models leads to Overthinking](https://arxiv.org/pdf/2505.16448)
*Renfei Dang, Shujian Huang, Jiajun Chen*

Main category: cs.AI

TL;DR: The paper reveals that reasoning models overthink due to internal bias from preliminary guesses, which conflict with reasoning results. Masking input sections reduces bias and improves efficiency.


<details>
  <summary>Details</summary>
Motivation: To address the issue of overthinking in reasoning models caused by internal bias, which wastes computational resources.

Method: Analyze internal bias through interpretability experiments and test the impact of masking input sections on reasoning performance.

Result: Masking input sections reduces reasoning length by 31%-53% and often improves accuracy, showing a causal link between internal bias and overthinking.

Conclusion: Internal bias drives overthinking in reasoning models, and masking inputs can effectively mitigate this issue.

Abstract: While current reasoning models possess strong exploratory capabilities, they
are often criticized for overthinking due to redundant and unnecessary
reflections. In this work, we reveal for the first time that overthinking in
reasoning models may stem from their internal bias towards input texts. Upon
encountering a reasoning problem, the model immediately forms a preliminary
guess about the answer, which we term as an internal bias since it is not
derived through actual reasoning. When this guess conflicts with its reasoning
result, the model tends to engage in reflection, leading to the waste of
computational resources. Through further interpretability experiments, we find
that this behavior is largely driven by the model's excessive attention to the
input section, which amplifies the influence of internal bias on its
decision-making process. Additionally, by masking out the original input
section, the affect of internal bias can be effectively alleviated and the
reasoning length could be reduced by 31%-53% across different complex reasoning
tasks. Notably, in most cases, this approach also leads to improvements in
accuracy. These findings demonstrate a causal relationship between internal
bias and overthinking.

</details>


### [527] [MMMR: Benchmarking Massive Multi-Modal Reasoning Tasks](https://arxiv.org/pdf/2505.16459)
*Guiyao Tie, Xueyang Zhou, Tianhe Gu, Ruihang Zhang, Chaoran Hu, Sizhe Zhang, Mengqu Sun, Yan Zhang, Pan Zhou, Lichao Sun*

Main category: cs.AI

TL;DR: The paper introduces MMMR, a benchmark for evaluating multi-modal reasoning in MLLMs, highlighting gaps in reasoning quality despite improved accuracy.


<details>
  <summary>Details</summary>
Motivation: Existing benchmarks lack focus on reasoning quality in MLLMs, particularly those with intermediate thinking traces (MLLMs-T).

Method: MMMR includes a high-difficulty dataset (1,083 questions) and a Reasoning Trace Evaluation Pipeline (RTEP) to assess reasoning beyond accuracy.

Result: MLLMs-T outperform non-thinking models, but still exhibit reasoning flaws like inconsistency.

Conclusion: MMMR provides a scalable foundation for improving multi-modal reasoning systems.

Abstract: Recent advances in Multi-Modal Large Language Models (MLLMs) have enabled
unified processing of language, vision, and structured inputs, opening the door
to complex tasks such as logical deduction, spatial reasoning, and scientific
analysis. Despite their promise, the reasoning capabilities of MLLMs,
particularly those augmented with intermediate thinking traces (MLLMs-T),
remain poorly understood and lack standardized evaluation benchmarks. Existing
work focuses primarily on perception or final answer correctness, offering
limited insight into how models reason or fail across modalities. To address
this gap, we introduce the MMMR, a new benchmark designed to rigorously
evaluate multi-modal reasoning with explicit thinking. The MMMR comprises 1) a
high-difficulty dataset of 1,083 questions spanning six diverse reasoning types
with symbolic depth and multi-hop demands and 2) a modular Reasoning Trace
Evaluation Pipeline (RTEP) for assessing reasoning quality beyond accuracy
through metrics like relevance, consistency, and structured error annotations.
Empirical results show that MLLMs-T overall outperform non-thinking
counterparts, but even top models like Claude-3.7-Sonnet and Gemini-2.5 Pro
suffer from reasoning pathologies such as inconsistency and overthinking. This
benchmark reveals persistent gaps between accuracy and reasoning quality and
provides an actionable evaluation pipeline for future model development.
Overall, the MMMR offers a scalable foundation for evaluating, comparing, and
improving the next generation of multi-modal reasoning systems.

</details>


### [528] [Enter the Void - Planning to Seek Entropy When Reward is Scarce](https://arxiv.org/pdf/2505.16787)
*Ashish Sundar, Chunbo Luo, Xiaoyang Wang*

Main category: cs.AI

TL;DR: A novel MBRL method improves world model fidelity by targeting high-entropy states, outperforming base Dreamer in efficiency and convergence.


<details>
  <summary>Details</summary>
Motivation: Prioritizing actor optimization in MBRL has neglected world model improvement, which can enhance downstream performance.

Method: The approach uses short-horizon latent predictions to seek high-entropy states and introduces a hierarchical planner for dynamic replanning.

Result: The method completes tasks 50% faster than base Dreamer and converges in 60% of the steps.

Conclusion: Targeting high-entropy states and dynamic planning significantly boosts MBRL efficiency and performance.

Abstract: Model-based reinforcement learning (MBRL) offers an intuitive way to increase
the sample efficiency of model-free RL methods by simultaneously training a
world model that learns to predict the future. MBRL methods have progressed by
largely prioritising the actor; optimising the world model learning has been
neglected meanwhile. Improving the fidelity of the world model and reducing its
time to convergence can yield significant downstream benefits, one of which is
improving the ensuing performance of any actor it may train. We propose a novel
approach that anticipates and actively seeks out high-entropy states using
short-horizon latent predictions generated by the world model, offering a
principled alternative to traditional curiosity-driven methods that chase
once-novel states well after they were stumbled into. While many model
predictive control (MPC) based methods offer similar alternatives, they
typically lack commitment, synthesising multi step plans after every step. To
mitigate this, we present a hierarchical planner that dynamically decides when
to replan, planning horizon length, and the weighting between reward and
entropy. While our method can theoretically be applied to any model that trains
its own actors with solely model generated data, we have applied it to just
Dreamer as a proof of concept. Our method finishes the Miniworld procedurally
generated mazes 50% faster than base Dreamer at convergence and the policy
trained in imagination converges in only 60% of the environment steps that base
Dreamer needs.

</details>


### [529] [Structured Thinking Matters: Improving LLMs Generalization in Causal Inference Tasks](https://arxiv.org/pdf/2505.18034)
*Wentao Sun, João Paulo Nogueira, Alonso Silva*

Main category: cs.AI

TL;DR: LLMs struggle with causation vs. correlation. A structured knowledge graph approach improves causal reasoning, boosting F1 scores by 47.5%.


<details>
  <summary>Details</summary>
Motivation: LLMs perform poorly in distinguishing causation from correlation, barely surpassing random baselines.

Method: Proposes a structured knowledge graph to guide LLMs in answering causal queries systematically.

Result: Substantial improvement in F1 scores (32.71 to 48.26) and other metrics on the Corr2Cause benchmark.

Conclusion: Structured thinking enhances LLMs' causal reasoning, showing promise for broader generalization.

Abstract: Despite remarkable advances in the field, LLMs remain unreliable in
distinguishing causation from correlation. Recent results from the Corr2Cause
dataset benchmark reveal that state-of-the-art LLMs -- such as GPT-4 (F1 score:
29.08) -- only marginally outperform random baselines (Random Uniform, F1
score: 20.38), indicating limited capacity of generalization. To tackle this
limitation, we propose a novel structured approach: rather than directly
answering causal queries, we provide the model with the capability to structure
its thinking by guiding the model to build a structured knowledge graph,
systematically encoding the provided correlational premises, to answer the
causal queries. This intermediate representation significantly enhances the
model's causal capabilities. Experiments on the test subset of the Corr2Cause
dataset benchmark with Qwen3-32B model (reasoning model) show substantial gains
over standard direct prompting methods, improving F1 scores from 32.71 to 48.26
(over 47.5% relative increase), along with notable improvements in precision
and recall. These results underscore the effectiveness of providing the model
with the capability to structure its thinking and highlight its promising
potential for broader generalization across diverse causal inference tasks.

</details>


### [530] [RvLLM: LLM Runtime Verification with Domain Knowledge](https://arxiv.org/pdf/2505.18585)
*Yedi Zhang, Sun Yi Emma, Annabelle Lee Jia En, Jin Song Dong*

Main category: cs.AI

TL;DR: The paper proposes a framework, RvLLM, to detect and verify errors in LLM outputs by integrating domain-specific knowledge using a novel specification language, ESL.


<details>
  <summary>Details</summary>
Motivation: LLMs often produce inconsistent or erroneous outputs, especially in high-stakes domains, but existing solutions lack domain-specific integration.

Method: Developed ESL, a specification language for domain experts to define constraints, and RvLLM, a runtime verification framework to validate LLM outputs against these constraints.

Result: RvLLM effectively detected errors in LLM outputs across tasks like legal violation detection and numerical comparisons.

Conclusion: Despite LLMs' capabilities, they are prone to errors; RvLLM provides a scalable solution by leveraging domain knowledge for rigorous verification.

Abstract: Large language models (LLMs) have emerged as a dominant AI paradigm due to
their exceptional text understanding and generation capabilities. However,
their tendency to generate inconsistent or erroneous outputs challenges their
reliability, especially in high-stakes domains requiring accuracy and
trustworthiness. Existing research primarily focuses on detecting and
mitigating model misbehavior in general-purpose scenarios, often overlooking
the potential of integrating domain-specific knowledge. In this work, we
advance misbehavior detection by incorporating domain knowledge. The core idea
is to design a general specification language that enables domain experts to
customize domain-specific predicates in a lightweight and intuitive manner,
supporting later runtime verification of LLM outputs. To achieve this, we
design a novel specification language, ESL, and introduce a runtime
verification framework, RvLLM, to validate LLM output against domain-specific
constraints defined in ESL. We evaluate RvLLM on three representative tasks:
violation detection against Singapore Rapid Transit Systems Act, numerical
comparison, and inequality solving. Experimental results demonstrate that RvLLM
effectively detects erroneous outputs across various LLMs in a lightweight and
flexible manner. The results reveal that despite their impressive capabilities,
LLMs remain prone to low-level errors due to limited interpretability and a
lack of formal guarantees during inference, and our framework offers a
potential long-term solution by leveraging expert domain knowledge to
rigorously and efficiently verify LLM outputs.

</details>


### [531] [$C^3$-Bench: The Things Real Disturbing LLM based Agent in Multi-Tasking](https://arxiv.org/pdf/2505.18746)
*Peijie Yu, Yifan Yang, Jinjian Li, Zelong Zhang, Haorui Wang, Xiao Feng, Feng Zhang*

Main category: cs.AI

TL;DR: The paper introduces $C^3$-Bench, a benchmark to evaluate AI agents' robustness by addressing tool dependencies, hidden information, and dynamic decisions, revealing vulnerabilities in current models.


<details>
  <summary>Details</summary>
Motivation: Current evaluations overlook critical factors like tool relationships and environmental feedback, limiting understanding of agent behavior.

Method: The benchmark includes three challenges, fine-grained metrics, and reproducible methods, tested on 49 agents.

Result: Agents struggle with tool dependencies, long-context information, and policy switching.

Conclusion: $C^3$-Bench exposes model vulnerabilities and aims to advance research in agent interpretability.

Abstract: Agents based on large language models leverage tools to modify environments,
revolutionizing how AI interacts with the physical world. Unlike traditional
NLP tasks that rely solely on historical dialogue for responses, these agents
must consider more complex factors, such as inter-tool relationships,
environmental feedback and previous decisions, when making choices. Current
research typically evaluates agents via multi-turn dialogues. However, it
overlooks the influence of these critical factors on agent behavior. To bridge
this gap, we present an open-source and high-quality benchmark $C^3$-Bench.
This benchmark integrates attack concepts and applies univariate analysis to
pinpoint key elements affecting agent robustness. In concrete, we design three
challenges: navigate complex tool relationships, handle critical hidden
information and manage dynamic decision paths. Complementing these challenges,
we introduce fine-grained metrics, innovative data collection algorithms and
reproducible evaluation methods. Extensive experiments are conducted on 49
mainstream agents, encompassing general fast-thinking, slow-thinking and
domain-specific models. We observe that agents have significant shortcomings in
handling tool dependencies, long context information dependencies and frequent
policy-type switching. In essence, $C^3$-Bench aims to expose model
vulnerabilities through these challenges and drive research into the
interpretability of agent performance. The benchmark is publicly available at
https://github.com/yupeijei1997/C3-Bench.

</details>


### [532] [RECAST: Strengthening LLMs' Complex Instruction Following with Constraint-Verifiable Data](https://arxiv.org/pdf/2505.19030)
*Wenhao Liu, Zhengkang Guo, Mingchen Xie, Jingwen Xu, Zisu Huang, Muzhao Tian, Jianhan Xu, Muling Wu, Xiaohua Wang, Changze Lv, He-Da Wang, Hu Yao, Xiaoqing Zheng, Xuanjing Huang*

Main category: cs.AI

TL;DR: RECAST is a framework for creating datasets with complex constraints to improve LLMs' ability to follow intricate instructions, validated via rule-based and LLM-based methods.


<details>
  <summary>Details</summary>
Motivation: LLMs struggle with tasks involving many constraints, limiting their practical utility.

Method: RECAST synthesizes datasets with high constraint complexity, using real-world prompts and automatic validation.

Result: Models fine-tuned on RECAST-30K show significant improvement in handling complex instructions.

Conclusion: RECAST enhances LLM performance and enables reinforcement learning for further gains.

Abstract: Large language models (LLMs) are increasingly expected to tackle complex
tasks, driven by their expanding applications and users' growing proficiency in
crafting sophisticated prompts. However, as the number of explicitly stated
requirements increases (particularly more than 10 constraints), LLMs often
struggle to accurately follow such complex instructions. To address this
challenge, we propose RECAST, a novel framework for synthesizing datasets where
each example incorporates far more constraints than those in existing
benchmarks. These constraints are extracted from real-world prompt-response
pairs to ensure practical relevance. RECAST enables automatic verification of
constraint satisfaction via rule-based validators for quantitative constraints
and LLM-based validators for qualitative ones. Using this framework, we
construct RECAST-30K, a large-scale, high-quality dataset comprising 30k
instances spanning 15 constraint types. Experimental results demonstrate that
models fine-tuned on RECAST-30K show substantial improvements in following
complex instructions. Moreover, the verifiability provided by RECAST enables
the design of reward functions for reinforcement learning, which further boosts
model performance on complex and challenging tasks.

</details>


### [533] [Universal Reasoner: A Single, Composable Plug-and-Play Reasoner for Frozen LLMs](https://arxiv.org/pdf/2505.19075)
*Jaemin Kim, Hangeol Chang, Hyunmin Hwang, Choonghan Kim, Jong Chul Ye*

Main category: cs.AI

TL;DR: UniR is a lightweight, plug-and-play reasoning module for LLMs, enabling specialized reasoning without retraining. It outperforms baselines and generalizes across model sizes.


<details>
  <summary>Details</summary>
Motivation: Enhancing LLM reasoning efficiently without compromising generalization or requiring extensive resources.

Method: UniR decomposes rewards into a standalone module, trained independently and added to any frozen LLM's logits.

Result: Outperforms fine-tuning baselines on tasks like math and translation, with strong weak-to-strong generalization.

Conclusion: UniR is a cost-efficient, adaptable solution for improving LLM reasoning without core capability trade-offs.

Abstract: Large Language Models (LLMs) have demonstrated remarkable general
capabilities, but enhancing skills such as reasoning often demands substantial
computational resources and may compromise their generalization. While
Parameter-Efficient Fine-Tuning (PEFT) methods offer a more resource-conscious
alternative, they typically requires retraining for each LLM backbone due to
architectural dependencies. To address these challenges, here we propose
Universal Reasoner (UniR) - a single, lightweight, composable, and
plug-and-play reasoning module that can be used with any frozen LLM to endow it
with specialized reasoning capabilities. Specifically, UniR decomposes the
reward into a standalone reasoning module that is trained independently using
predefined rewards, effectively translating trajectory-level signals into
token-level guidance. Once trained, UniR can be combined with any frozen LLM at
inference time by simply adding its output logits to those of the LLM backbone.
This additive structure naturally enables modular composition: multiple UniR
modules trained for different tasks can be jointly applied by summing their
logits, enabling complex reasoning via composition. Experimental results on
mathematical reasoning and machine translation tasks show that UniR
significantly outperforms existing baseline fine-tuning methods using the
Llama3.2 model. Furthermore, UniR demonstrates strong weak-to-strong
generalization: reasoning modules trained on smaller models effectively guide
much larger LLMs. This makes UniR a cost-efficient, adaptable, and robust
solution for enhancing reasoning in LLMs without compromising their core
capabilities. Code is open-sourced at https://github.com/hangeol/UniR

</details>


### [534] [Structuring the Unstructured: A Multi-Agent System for Extracting and Querying Financial KPIs and Guidance](https://arxiv.org/pdf/2505.19197)
*Chanyeol Choi, Alejandro Lopez-Lira, Jihoon Kwon, Minjae Kim, Juneha Hwang, Minsoo Ha, Chaewoon Kim, Jaeseon Ha, Suyeol Yun, Jin Kim*

Main category: cs.AI

TL;DR: A multi-agent system using large language models extracts and structures quantitative insights from financial documents with high accuracy, matching human performance.


<details>
  <summary>Details</summary>
Motivation: Manual extraction of financial insights is time-consuming and unscalable, necessitating an automated solution.

Method: A two-agent system: an Extraction Agent for standardizing KPIs and a Text-to-SQL Agent for converting queries to SQL.

Result: 95% accuracy in data transformation and 91% correct responses in retrieval tasks, generalizing across document types.

Conclusion: The system efficiently automates financial data extraction, offering scalability and reliability comparable to manual processes.

Abstract: Extracting structured and quantitative insights from unstructured financial
filings is essential in investment research, yet remains time-consuming and
resource-intensive. Conventional approaches in practice rely heavily on
labor-intensive manual processes, limiting scalability and delaying the
research workflow. In this paper, we propose an efficient and scalable method
for accurately extracting quantitative insights from unstructured financial
documents, leveraging a multi-agent system composed of large language models.
Our proposed multi-agent system consists of two specialized agents: the
\emph{Extraction Agent} and the \emph{Text-to-SQL Agent}. The
\textit{Extraction Agent} automatically identifies key performance indicators
from unstructured financial text, standardizes their formats, and verifies
their accuracy. On the other hand, the \textit{Text-to-SQL Agent} generates
executable SQL statements from natural language queries, allowing users to
access structured data accurately without requiring familiarity with the
database schema. Through experiments, we demonstrate that our proposed system
effectively transforms unstructured text into structured data accurately and
enables precise retrieval of key information. First, we demonstrate that our
system achieves approximately 95\% accuracy in transforming financial filings
into structured data, matching the performance level typically attained by
human annotators. Second, in a human evaluation of the retrieval task -- where
natural language queries are used to search information from structured data --
91\% of the responses were rated as correct by human evaluators. In both
evaluations, our system generalizes well across financial document types,
consistently delivering reliable performance.

</details>


### [535] [DiffVLA: Vision-Language Guided Diffusion Planning for Autonomous Driving](https://arxiv.org/pdf/2505.19381)
*Anqing Jiang, Yu Gao, Zhigang Sun, Yiru Wang, Jijun Wang, Jinghao Chai, Qian Cao, Yuweng Heng, Hao Jiang, Zongzheng Zhang, Xianda Guo, Hao Sun, Hao Zhao*

Main category: cs.AI

TL;DR: A novel hybrid sparse-dense diffusion policy (Diff-VLA) is proposed to address challenges in end-to-end autonomous driving, leveraging Vision-Language Models for improved decision-making and trajectory generation.


<details>
  <summary>Details</summary>
Motivation: Existing end-to-end autonomous driving methods face issues like expensive BEV computation, limited action diversity, and sub-optimal decisions in complex scenarios.

Method: The proposed Diff-VLA combines sparse diffusion representation for efficient multi-modal behavior and enhances trajectory generation via deep interaction between agents, maps, and VLM outputs.

Result: The method achieves superior performance in the Autonomous Grand Challenge 2025, scoring 45.0 PDMS.

Conclusion: Diff-VLA effectively addresses key challenges in end-to-end autonomous driving, demonstrating strong performance in complex scenarios.

Abstract: Research interest in end-to-end autonomous driving has surged owing to its
fully differentiable design integrating modular tasks, i.e. perception,
prediction and planing, which enables optimization in pursuit of the ultimate
goal. Despite the great potential of the end-to-end paradigm, existing methods
suffer from several aspects including expensive BEV (bird's eye view)
computation, action diversity, and sub-optimal decision in complex real-world
scenarios. To address these challenges, we propose a novel hybrid sparse-dense
diffusion policy, empowered by a Vision-Language Model (VLM), called Diff-VLA.
We explore the sparse diffusion representation for efficient multi-modal
driving behavior. Moreover, we rethink the effectiveness of VLM driving
decision and improve the trajectory generation guidance through deep
interaction across agent, map instances and VLM output. Our method shows
superior performance in Autonomous Grand Challenge 2025 which contains
challenging real and reactive synthetic scenarios. Our methods achieves 45.0
PDMS.

</details>


### [536] [SynLogic: Synthesizing Verifiable Reasoning Data at Scale for Learning Logical Reasoning and Beyond](https://arxiv.org/pdf/2505.19641)
*Junteng Liu, Yuanxiang Fan, Zhuo Jiang, Han Ding, Yongyi Hu, Chi Zhang, Yiqi Shi, Shitong Weng, Aili Chen, Shiqi Chen, Yunan Huang, Mozhi Zhang, Pengyu Zhao, Junjie Yan, Junxian He*

Main category: cs.AI

TL;DR: SynLogic is a framework for generating diverse logical reasoning data for RL in LLMs, improving reasoning performance and generalization.


<details>
  <summary>Details</summary>
Motivation: To address the lack of diverse and verifiable reasoning data for RL in LLMs, focusing on logical reasoning as a foundational skill.

Method: Developed SynLogic, a data synthesis framework generating 35 logical reasoning tasks with adjustable difficulty and verifiable examples.

Result: SynLogic outperforms existing datasets, improving reasoning performance by 6 points on BBEH and enhancing generalization when mixed with other tasks.

Conclusion: SynLogic is a valuable resource for advancing LLMs' reasoning capabilities, with open-sourced data and pipeline.

Abstract: Recent advances such as OpenAI-o1 and DeepSeek R1 have demonstrated the
potential of Reinforcement Learning (RL) to enhance reasoning abilities in
Large Language Models (LLMs). While open-source replication efforts have
primarily focused on mathematical and coding domains, methods and resources for
developing general reasoning capabilities remain underexplored. This gap is
partly due to the challenge of collecting diverse and verifiable reasoning data
suitable for RL. We hypothesize that logical reasoning is critical for
developing general reasoning capabilities, as logic forms a fundamental
building block of reasoning. In this work, we present SynLogic, a data
synthesis framework and dataset that generates diverse logical reasoning data
at scale, encompassing 35 diverse logical reasoning tasks. The SynLogic
approach enables controlled synthesis of data with adjustable difficulty and
quantity. Importantly, all examples can be verified by simple rules, making
them ideally suited for RL with verifiable rewards. In our experiments, we
validate the effectiveness of RL training on the SynLogic dataset based on 7B
and 32B models. SynLogic leads to state-of-the-art logical reasoning
performance among open-source datasets, surpassing DeepSeek-R1-Distill-Qwen-32B
by 6 points on BBEH. Furthermore, mixing SynLogic data with mathematical and
coding tasks improves the training efficiency of these domains and
significantly enhances reasoning generalization. Notably, our mixed training
model outperforms DeepSeek-R1-Zero-Qwen-32B across multiple benchmarks. These
findings position SynLogic as a valuable resource for advancing the broader
reasoning capabilities of LLMs. We open-source both the data synthesis pipeline
and the SynLogic dataset at https://github.com/MiniMax-AI/SynLogic.

</details>


### [537] [MineAnyBuild: Benchmarking Spatial Planning for Open-world AI Agents](https://arxiv.org/pdf/2505.20148)
*Ziming Wei, Bingqian Lin, Zijian Jiao, Yunshuang Nie, Liang Ma, Yuecheng Liu, Yuzheng Zhuang, Xiaodan Liang*

Main category: cs.AI

TL;DR: The paper introduces MineAnyBuild, a benchmark for evaluating spatial planning in AI agents using Minecraft, addressing gaps in existing benchmarks.


<details>
  <summary>Details</summary>
Motivation: Existing benchmarks focus on abstract spatial reasoning, lacking concrete task execution. MineAnyBuild aims to bridge this gap by evaluating real-world spatial planning.

Method: Developed MineAnyBuild with 4,000 tasks in Minecraft, assessing spatial understanding, reasoning, creativity, and commonsense.

Result: Evaluation of MLLM-based agents showed limitations but potential in spatial planning.

Conclusion: MineAnyBuild advances spatial intelligence evaluation and aids AI agent development for open-world tasks.

Abstract: Spatial Planning is a crucial part in the field of spatial intelligence,
which requires the understanding and planning about object arrangements in
space perspective. AI agents with the spatial planning ability can better adapt
to various real-world applications, including robotic manipulation, automatic
assembly, urban planning etc. Recent works have attempted to construct
benchmarks for evaluating the spatial intelligence of Multimodal Large Language
Models (MLLMs). Nevertheless, these benchmarks primarily focus on spatial
reasoning based on typical Visual Question-Answering (VQA) forms, which suffers
from the gap between abstract spatial understanding and concrete task
execution. In this work, we take a step further to build a comprehensive
benchmark called MineAnyBuild, aiming to evaluate the spatial planning ability
of open-world AI agents in the Minecraft game. Specifically, MineAnyBuild
requires an agent to generate executable architecture building plans based on
the given multi-modal human instructions. It involves 4,000 curated spatial
planning tasks and also provides a paradigm for infinitely expandable data
collection by utilizing rich player-generated content. MineAnyBuild evaluates
spatial planning through four core supporting dimensions: spatial
understanding, spatial reasoning, creativity, and spatial commonsense. Based on
MineAnyBuild, we perform a comprehensive evaluation for existing MLLM-based
agents, revealing the severe limitations but enormous potential in their
spatial planning abilities. We believe our MineAnyBuild will open new avenues
for the evaluation of spatial intelligence and help promote further development
for open-world AI agents capable of spatial planning.

</details>


<div id='cs.SD'></div>

# cs.SD [[Back]](#toc)

### [538] [Training Articulatory Inversion Models for Inter-Speaker Consistency](https://arxiv.org/pdf/2505.20529)
*Charles McGhee, Mark J. F. Gales, Kate M. Knill*

Main category: cs.SD

TL;DR: The paper investigates if SSL-adapted models for AAI produce consistent articulatory targets across speakers in English and Russian, using minimal pair sets for evaluation and proposing a training method for inter-speaker consistency.


<details>
  <summary>Details</summary>
Motivation: To determine if SSL-adapted models can provide universal articulatory templates across speakers, addressing the challenge of articulatory variability in speech.

Method: Uses minimal pair sets for evaluation and introduces a training method to improve inter-speaker consistency using only speech data.

Result: Findings likely reveal whether SSL-adapted models achieve consistent articulatory targets across speakers.

Conclusion: The study contributes to understanding the universality of articulatory templates and proposes a method to enhance inter-speaker consistency in AAI.

Abstract: Acoustic-to-Articulatory Inversion (AAI) attempts to model the inverse
mapping from speech to articulation. Exact articulatory prediction from speech
alone may be impossible, as speakers can choose different forms of articulation
seemingly without reference to their vocal tract structure. However, once a
speaker has selected an articulatory form, their productions vary minimally.
Recent works in AAI have proposed adapting Self-Supervised Learning (SSL)
models to single-speaker datasets, claiming that these single-speaker models
provide a universal articulatory template. In this paper, we investigate
whether SSL-adapted models trained on single and multi-speaker data produce
articulatory targets which are consistent across speaker identities for English
and Russian. We do this through the use of a novel evaluation method which
extracts articulatory targets using minimal pair sets. We also present a
training method which can improve inter-speaker consistency using only speech
data.

</details>


### [539] [Music's Multimodal Complexity in AVQA: Why We Need More than General Multimodal LLMs](https://arxiv.org/pdf/2505.20638)
*Wenhao You, Xingjian Diao, Chunhui Zhang, Keyi Kong, Weiyi Wu, Zhongyu Ouyang, Chiyu Ma, Tingxuan Wu, Noah Wei, Zong Ke, Ming Cheng, Soroush Vosoughi, Jiang Gui*

Main category: cs.SD

TL;DR: Specialized approaches are needed for Music AVQA due to its unique challenges, requiring tailored input processing, architectures, and music-specific strategies.


<details>
  <summary>Details</summary>
Motivation: General multimodal models lack specialization for music, which has complex audio-visual dynamics and requires domain-specific knowledge.

Method: Systematic analysis of Music AVQA datasets and methods, focusing on specialized input processing, spatial-temporal designs, and music-specific modeling.

Result: Identified effective design patterns for Music AVQA and proposed future directions for incorporating musical priors.

Conclusion: The study aims to inspire further research and establish a foundation for advancing multimodal musical understanding.

Abstract: While recent Multimodal Large Language Models exhibit impressive capabilities
for general multimodal tasks, specialized domains like music necessitate
tailored approaches. Music Audio-Visual Question Answering (Music AVQA)
particularly underscores this, presenting unique challenges with its
continuous, densely layered audio-visual content, intricate temporal dynamics,
and the critical need for domain-specific knowledge. Through a systematic
analysis of Music AVQA datasets and methods, this position paper identifies
that specialized input processing, architectures incorporating dedicated
spatial-temporal designs, and music-specific modeling strategies are critical
for success in this domain. Our study provides valuable insights for
researchers by highlighting effective design patterns empirically linked to
strong performance, proposing concrete future directions for incorporating
musical priors, and aiming to establish a robust foundation for advancing
multimodal musical understanding. This work is intended to inspire broader
attention and further research, supported by a continuously updated anonymous
GitHub repository of relevant papers:
https://github.com/xid32/Survey4MusicAVQA.

</details>


### [540] [Uni-VERSA: Versatile Speech Assessment with a Unified Network](https://arxiv.org/pdf/2505.20741)
*Jiatong Shi, Hye-Jin Shim, Shinji Watanabe*

Main category: cs.SD

TL;DR: Uni-VERSA is a unified network for comprehensive speech quality assessment, predicting multiple objective metrics to replace costly subjective tests and single-aspect methods.


<details>
  <summary>Details</summary>
Motivation: Subjective tests are costly and variable, while existing objective metrics capture only specific aspects of speech quality. Uni-VERSA aims to provide a comprehensive and scalable alternative.

Method: Uni-VERSA is a unified network predicting various metrics (naturalness, intelligibility, speaker characteristics, prosody, noise) for speech evaluation. It includes a formalized framework, evaluation protocol, and applications in speech enhancement, synthesis, and quality control.

Result: Benchmarked on URGENT24, Uni-VERSA outperforms single-aspect methods and aligns closely with human perception.

Conclusion: Uni-VERSA is a promising, scalable alternative for speech quality assessment, closely matching human judgment.

Abstract: Subjective listening tests remain the golden standard for speech quality
assessment, but are costly, variable, and difficult to scale. In contrast,
existing objective metrics, such as PESQ, F0 correlation, and DNSMOS, typically
capture only specific aspects of speech quality. To address these limitations,
we introduce Uni-VERSA, a unified network that simultaneously predicts various
objective metrics, encompassing naturalness, intelligibility, speaker
characteristics, prosody, and noise, for a comprehensive evaluation of speech
signals. We formalize its framework, evaluation protocol, and applications in
speech enhancement, synthesis, and quality control. A benchmark based on the
URGENT24 challenge, along with a baseline leveraging self-supervised
representations, demonstrates that Uni-VERSA provides a viable alternative to
single-aspect evaluation methods. Moreover, it aligns closely with human
perception, making it a promising approach for future speech quality
assessment.

</details>


### [541] [Foundation Model Hidden Representations for Heart Rate Estimation from Auscultation](https://arxiv.org/pdf/2505.20745)
*Jingping Nie, Dung T. Tran, Karan Thakkar, Vasudha Kowtha, John Huang, Carlos Avendano, Erdrin Azemi, Vikramjit Mitra*

Main category: cs.SD

TL;DR: The paper explores how pre-trained acoustic foundation models (FMs) encode auscultation data, comparing six FMs for heart rate estimation. The in-house CLAP model outperforms a baseline method.


<details>
  <summary>Details</summary>
Motivation: To investigate the extent to which auscultation (heart sound) data is encoded in pre-trained acoustic FMs and evaluate their performance for heart rate estimation.

Method: Layer-wise analysis of six acoustic FMs (HuBERT, wav2vec2, wavLM, Whisper, CLAP, in-house CLAP) using a PCG dataset and HR estimation model, comparing against a baseline method.

Result: Pre-trained FMs perform comparably to the baseline, with the in-house CLAP model achieving the lowest MAE in HR estimation despite domain mismatch.

Conclusion: Pre-trained acoustic FMs, especially the in-house CLAP model, are effective for auscultation-based HR estimation, suggesting their potential for vital sign analysis.

Abstract: Auscultation, particularly heart sound, is a non-invasive technique that
provides essential vital sign information. Recently, self-supervised acoustic
representation foundation models (FMs) have been proposed to offer insights
into acoustics-based vital signs. However, there has been little exploration of
the extent to which auscultation is encoded in these pre-trained FM
representations. In this work, using a publicly available phonocardiogram (PCG)
dataset and a heart rate (HR) estimation model, we conduct a layer-wise
investigation of six acoustic representation FMs: HuBERT, wav2vec2, wavLM,
Whisper, Contrastive Language-Audio Pretraining (CLAP), and an in-house CLAP
model. Additionally, we implement the baseline method from Nie et al., 2024
(which relies on acoustic features) and show that overall, representation
vectors from pre-trained foundation models (FMs) offer comparable performance
to the baseline. Notably, HR estimation using the representations from the
audio encoder of the in-house CLAP model outperforms the results obtained from
the baseline, achieving a lower mean absolute error (MAE) across various
train/validation/test splits despite the domain mismatch.

</details>


### [542] [Can Large Language Models Predict Audio Effects Parameters from Natural Language?](https://arxiv.org/pdf/2505.20770)
*Seungheon Doh, Junghyun Koo, Marco A. Martínez-Ramírez, Wei-Hsiang Liao, Juhan Nam, Yuki Mitsufuji*

Main category: cs.SD

TL;DR: LLM2Fx uses LLMs to predict audio effect parameters from text descriptions without task-specific training, outperforming previous methods.


<details>
  <summary>Details</summary>
Motivation: To reduce technical barriers in music production by enabling natural language control of audio effects.

Method: Leverages LLMs for zero-shot prediction of Fx parameters (equalization, reverberation) using in-context examples (DSP features, code, few-shot).

Result: LLM-based approach outperforms optimization methods, providing accurate text-to-Fx parameter mapping.

Conclusion: LLMs enable intuitive, text-driven interfaces for music production, enhancing accessibility.

Abstract: In music production, manipulating audio effects (Fx) parameters through
natural language has the potential to reduce technical barriers for
non-experts. We present LLM2Fx, a framework leveraging Large Language Models
(LLMs) to predict Fx parameters directly from textual descriptions without
requiring task-specific training or fine-tuning. Our approach address the
text-to-effect parameter prediction (Text2Fx) task by mapping natural language
descriptions to the corresponding Fx parameters for equalization and
reverberation. We demonstrate that LLMs can generate Fx parameters in a
zero-shot manner that elucidates the relationship between timbre semantics and
audio effects in music production. To enhance performance, we introduce three
types of in-context examples: audio Digital Signal Processing (DSP) features,
DSP function code, and few-shot examples. Our results demonstrate that
LLM-based Fx parameter generation outperforms previous optimization approaches,
offering competitive performance in translating natural language descriptions
to appropriate Fx settings. Furthermore, LLMs can serve as text-driven
interfaces for audio production, paving the way for more intuitive and
accessible music production tools.

</details>


### [543] [VibE-SVC: Vibrato Extraction with High-frequency F0 Contour for Singing Voice Conversion](https://arxiv.org/pdf/2505.20794)
*Joon-Seung Choi, Dong-Min Byun, Hyung-Seok Oh, Seong-Whan Lee*

Main category: cs.SD

TL;DR: VibE-SVC is a singing voice conversion model that explicitly controls vibrato using discrete wavelet transform for precise and flexible style transfer.


<details>
  <summary>Details</summary>
Motivation: Vibrato is key for expressive singing but hard to model due to its dynamic nature, limiting control in voice conversion.

Method: The model extracts and manipulates vibrato via discrete wavelet transform, decomposing F0 contour for precise transfer.

Result: VibE-SVC successfully transforms singing styles while maintaining speaker similarity, confirmed by subjective and objective evaluations.

Conclusion: The approach enables high-quality, controllable vibrato manipulation in singing voice conversion.

Abstract: Controlling singing style is crucial for achieving an expressive and natural
singing voice. Among the various style factors, vibrato plays a key role in
conveying emotions and enhancing musical depth. However, modeling vibrato
remains challenging due to its dynamic nature, making it difficult to control
in singing voice conversion. To address this, we propose VibESVC, a
controllable singing voice conversion model that explicitly extracts and
manipulates vibrato using discrete wavelet transform. Unlike previous methods
that model vibrato implicitly, our approach decomposes the F0 contour into
frequency components, enabling precise transfer. This allows vibrato control
for enhanced flexibility. Experimental results show that VibE-SVC effectively
transforms singing styles while preserving speaker similarity. Both subjective
and objective evaluations confirm high-quality conversion.

</details>


### [544] [VoxAging: Continuously Tracking Speaker Aging with a Large-Scale Longitudinal Dataset in English and Mandarin](https://arxiv.org/pdf/2505.21445)
*Zhiqi Ai, Meixuan Bao, Zhiyong Chen, Zhi Yang, Xinnuo Li, Shugong Xu*

Main category: cs.SD

TL;DR: VoxAging dataset addresses speaker aging challenges with longitudinal data from 293 speakers over 17 years, analyzing aging effects on verification systems.


<details>
  <summary>Details</summary>
Motivation: Speaker aging impacts verification systems, but longitudinal data is scarce. VoxAging fills this gap with extensive, long-term speaker data.

Method: Collected weekly recordings from 293 speakers (English and Mandarin) over up to 17 years, analyzing aging effects, individual processes, and demographic impacts.

Result: Provided insights into speaker aging, its effects on verification systems, and the roles of age group and gender.

Conclusion: VoxAging enables deeper research into speaker aging, offering valuable data for improving verification systems.

Abstract: The performance of speaker verification systems is adversely affected by
speaker aging. However, due to challenges in data collection, particularly the
lack of sustained and large-scale longitudinal data for individuals, research
on speaker aging remains difficult. In this paper, we present VoxAging, a
large-scale longitudinal dataset collected from 293 speakers (226 English
speakers and 67 Mandarin speakers) over several years, with the longest time
span reaching 17 years (approximately 900 weeks). For each speaker, the data
were recorded at weekly intervals. We studied the phenomenon of speaker aging
and its effects on advanced speaker verification systems, analyzed individual
speaker aging processes, and explored the impact of factors such as age group
and gender on speaker aging research.

</details>


### [545] [Spotlight-TTS: Spotlighting the Style via Voiced-Aware Style Extraction and Style Direction Adjustment for Expressive Text-to-Speech](https://arxiv.org/pdf/2505.20868)
*Nam-Gyu Kim, Deok-Hyeon Cho, Seung-Bin Kim, Seong-Whan Lee*

Main category: cs.SD

TL;DR: Spotlight-TTS improves expressive TTS by focusing on voiced-aware style extraction and adjustment, outperforming baselines in quality and style transfer.


<details>
  <summary>Details</summary>
Motivation: Synthesizing high-quality expressive speech is challenging despite advances in TTS methods.

Method: Spotlight-TTS uses voiced-aware style extraction and style direction adjustment for better expressiveness and integration.

Result: Superior performance in expressiveness, speech quality, and style transfer compared to baselines.

Conclusion: Spotlight-TTS effectively enhances expressive TTS, with publicly available audio samples.

Abstract: Recent advances in expressive text-to-speech (TTS) have introduced diverse
methods based on style embedding extracted from reference speech. However,
synthesizing high-quality expressive speech remains challenging. We propose
Spotlight-TTS, which exclusively emphasizes style via voiced-aware style
extraction and style direction adjustment. Voiced-aware style extraction
focuses on voiced regions highly related to style while maintaining continuity
across different speech regions to improve expressiveness. We adjust the
direction of the extracted style for optimal integration into the TTS model,
which improves speech quality. Experimental results demonstrate that
Spotlight-TTS achieves superior performance compared to baseline models in
terms of expressiveness, overall speech quality, and style transfer capability.
Our audio samples are publicly available.

</details>


### [546] [Hybrid Disagreement-Diversity Active Learning for Bioacoustic Sound Event Detection](https://arxiv.org/pdf/2505.20956)
*Shiqi Zhang, Tuomas Virtanen*

Main category: cs.SD

TL;DR: MFFT active learning method improves BioSED with limited annotations, achieving near-fully-supervised performance.


<details>
  <summary>Details</summary>
Motivation: Address challenges in BioSED like sparse data, class imbalance, and species diversity efficiently with limited labeling budget.

Method: Uses MFFT, integrating committee voting disagreement and diversity analysis for active learning.

Result: MFFT achieves 68% mAP (cold-start) and 71% (warm-start), close to fully-supervised 75%, using only 2.3% annotations.

Conclusion: MFFT is effective, especially for cold-start and rare species, aiding biodiversity conservation.

Abstract: Bioacoustic sound event detection (BioSED) is crucial for biodiversity
conservation but faces practical challenges during model development and
training: limited amounts of annotated data, sparse events, species diversity,
and class imbalance. To address these challenges efficiently with a limited
labeling budget, we apply the mismatch-first farthest-traversal (MFFT), an
active learning method integrating committee voting disagreement and diversity
analysis. We also refine an existing BioSED dataset specifically for evaluating
active learning algorithms. Experimental results demonstrate that MFFT achieves
a mAP of 68% when cold-starting and 71% when warm-starting (which is close to
the fully-supervised mAP of 75%) while using only 2.3% of the annotations.
Notably, MFFT excels in cold-start scenarios and with rare species, which are
critical for monitoring endangered species, demonstrating its practical value.

</details>


### [547] [Efficient and Microphone-Fault-Tolerant 3D Sound Source Localization](https://arxiv.org/pdf/2505.20961)
*Yiyuan Yang, Shitong Xu, Niki Trigoni, Andrew Markham*

Main category: cs.SD

TL;DR: A novel 3D SSL framework using sparse cross-attention and adaptive metrics achieves efficient, accurate sound localization with fewer microphones and fault tolerance.


<details>
  <summary>Details</summary>
Motivation: Existing SSL methods struggle with high computational costs and calibration needs, limiting real-world use.

Method: Uses sparse cross-attention, pretraining, and adaptive signal coherence metrics for efficient localization.

Result: Accurate, scalable multi-source localization with fewer microphones and fault tolerance.

Conclusion: Advances SSL by balancing performance, efficiency, and robustness for real-world applications.

Abstract: Sound source localization (SSL) is a critical technology for determining the
position of sound sources in complex environments. However, existing methods
face challenges such as high computational costs and precise calibration
requirements, limiting their deployment in dynamic or resource-constrained
environments. This paper introduces a novel 3D SSL framework, which uses sparse
cross-attention, pretraining, and adaptive signal coherence metrics, to achieve
accurate and computationally efficient localization with fewer input
microphones. The framework is also fault-tolerant to unreliable or even unknown
microphone position inputs, ensuring its applicability in real-world scenarios.
Preliminary experiments demonstrate its scalability for multi-source
localization without requiring additional hardware. This work advances SSL by
balancing the model's performance and efficiency and improving its robustness
for real-world scenarios.

</details>


### [548] [MelodySim: Measuring Melody-aware Music Similarity for Plagiarism Detection](https://arxiv.org/pdf/2505.20979)
*Tongyu Lu, Charlotta-Marlena Geist, Jan Melechovsky, Abhinaba Roy, Dorien Herremans*

Main category: cs.SD

TL;DR: MelodySim is a melody-aware model and dataset for plagiarism detection, using augmented MIDI data and a triplet neural network for high accuracy.


<details>
  <summary>Details</summary>
Motivation: To address the need for detecting melodic plagiarism in music by focusing on melodic similarity rather than other musical elements.

Method: Constructs a dataset by augmenting Slakh2100 with melodic variations, then trains a segment-wise model using a MERT encoder and triplet neural network.

Result: High accuracy on the MelodySim test set, with a decision matrix identifying potential plagiarism.

Conclusion: MelodySim effectively detects melodic plagiarism, validated by user study and model performance.

Abstract: We propose MelodySim, a melody-aware music similarity model and dataset for
plagiarism detection. First, we introduce a novel method to construct a dataset
with focus on melodic similarity. By augmenting Slakh2100; an existing MIDI
dataset, we generate variations of each piece while preserving the melody
through modifications such as note splitting, arpeggiation, minor track dropout
(excluding bass), and re-instrumentation. A user study confirms that positive
pairs indeed contain similar melodies, with other musical tracks significantly
changed. Second, we develop a segment-wise melodic-similarity detection model
that uses a MERT encoder and applies a triplet neural network to capture
melodic similarity. The resultant decision matrix highlights where plagiarism
might occur. Our model achieves high accuracy on the MelodySim test set.

</details>


### [549] [ClearSphere: Multi-Earphone Synergy for Enhanced Conversational Clarity](https://arxiv.org/pdf/2505.21004)
*Lixing He*

Main category: cs.SD

TL;DR: ClearSphere is a system using multi-earphones to enhance speech in noisy environments by combining acoustic sensors and deep learning, achieving high accuracy and improved speech quality.


<details>
  <summary>Details</summary>
Motivation: Addressing the challenge of clear conversations in noisy settings like conferences, where background noise and overlapping voices cause 'cocktail party deafness.'

Method: Uses a conversation-driven network protocol and a robust target speech extraction model, leveraging relay audio efficiently.

Result: Achieves over 90% accuracy in group formation, improves speech quality by 8.8 dB, and performs well in real-time on mobile devices.

Conclusion: ClearSphere outperforms baselines in usability and effectiveness, making it a promising solution for noisy environments.

Abstract: In crowded places such as conferences, background noise, overlapping voices,
and lively interactions make it difficult to have clear conversations. This
situation often worsens the phenomenon known as "cocktail party deafness." We
present ClearSphere, the collaborative system that enhances speech at the
conversation level with multi-earphones. Real-time conversation enhancement
requires a holistic modeling of all the members in the conversation, and an
effective way to extract the speech from the mixture. ClearSphere bridges the
acoustic sensor system and state-of-the-art deep learning for target speech
extraction by making two key contributions: 1) a conversation-driven network
protocol, and 2) a robust target conversation extraction model. Our networking
protocol enables mobile, infrastructure-free coordination among earphone
devices. Our conversation extraction model can leverage the relay audio in a
bandwidth-efficient way. ClearSphere is evaluated in both real-world
experiments and simulations. Results show that our conversation network obtains
more than 90\% accuracy in group formation, improves the speech quality by up
to 8.8 dB over state-of-the-art baselines, and demonstrates real-time
performance on a mobile device. In a user study with 20 participants,
ClearSphere has a much higher score than baseline with good usability.

</details>


### [550] [Text-Queried Audio Source Separation via Hierarchical Modeling](https://arxiv.org/pdf/2505.21025)
*Xinlei Yin, Xiulian Peng, Xue Jiang, Zhiwei Xiong, Yan Lu*

Main category: cs.SD

TL;DR: HSM-TSS is a hierarchical decomposition framework for target audio source separation using natural language queries, addressing challenges in acoustic-textual alignment and semantic-aware separation with a dual-stage mechanism.


<details>
  <summary>Details</summary>
Motivation: Existing methods struggle with jointly modeling acoustic-textual alignment and semantic-aware separation, and rely on large-scale labeled data. HSM-TSS aims to overcome these inefficiencies.

Method: The framework decouples the task into global-local semantic-guided feature separation and acoustic reconstruction, using a dual-stage mechanism with global-semantic separation (Q-Audio architecture) and local-semantic separation (AudioMAE features).

Result: HSM-TSS achieves state-of-the-art separation performance with data-efficient training and superior semantic consistency.

Conclusion: The proposed method effectively addresses the challenges, enabling flexible sound manipulation and efficient cross-modal learning.

Abstract: Target audio source separation with natural language queries presents a
promising paradigm for extracting arbitrary audio events through arbitrary text
descriptions. Existing methods mainly face two challenges, the difficulty in
jointly modeling acoustic-textual alignment and semantic-aware separation
within a blindly-learned single-stage architecture, and the reliance on
large-scale accurately-labeled training data to compensate for inefficient
cross-modal learning and separation. To address these challenges, we propose a
hierarchical decomposition framework, HSM-TSS, that decouples the task into
global-local semantic-guided feature separation and structure-preserving
acoustic reconstruction. Our approach introduces a dual-stage mechanism for
semantic separation, operating on distinct global and local semantic feature
spaces. We first perform global-semantic separation through a global semantic
feature space aligned with text queries. A Q-Audio architecture is employed to
align audio and text modalities, serving as pretrained global-semantic
encoders. Conditioned on the predicted global feature, we then perform the
second-stage local-semantic separation on AudioMAE features that preserve
time-frequency structures, followed by acoustic reconstruction. We also propose
an instruction processing pipeline to parse arbitrary text queries into
structured operations, extraction or removal, coupled with audio descriptions,
enabling flexible sound manipulation. Our method achieves state-of-the-art
separation performance with data-efficient training while maintaining superior
semantic consistency with queries in complex auditory scenes.

</details>


### [551] [Model as Loss: A Self-Consistent Training Paradigm](https://arxiv.org/pdf/2505.21156)
*Saisamarth Rajesh Phaye, Milos Cernak, Andrew Harper*

Main category: cs.SD

TL;DR: Proposes 'Model as Loss,' a training paradigm using the model's encoder as a loss function for speech enhancement, outperforming traditional methods.


<details>
  <summary>Details</summary>
Motivation: Existing methods fail to capture subtle signal properties, limiting performance in speech enhancement.

Method: Uses the encoder's task-specific features as a loss function to guide decoder training, ensuring self-consistency with clean speech.

Result: Outperforms pre-trained deep feature losses, improving perceptual quality and generalization across datasets.

Conclusion: The 'Model as Loss' paradigm is effective for speech enhancement, offering superior performance and robustness.

Abstract: Conventional methods for speech enhancement rely on handcrafted loss
functions (e.g., time or frequency domain losses) or deep feature losses (e.g.,
using WavLM or wav2vec), which often fail to capture subtle signal properties
essential for optimal performance. To address this, we propose Model as Loss, a
novel training paradigm that utilizes the encoder from the same model as a loss
function to guide the training.
  The Model as Loss paradigm leverages the encoder's task-specific feature
space, optimizing the decoder to produce output consistent with perceptual and
task-relevant characteristics of the clean signal. By using the encoder's
learned features as a loss function, this framework enforces self-consistency
between the clean reference speech and the enhanced model output. Our approach
outperforms pre-trained deep feature losses on standard speech enhancement
benchmarks, offering better perceptual quality and robust generalization to
both in-domain and out-of-domain datasets.

</details>


### [552] [Universal Speech Enhancement with Regression and Generative Mamba](https://arxiv.org/pdf/2505.21198)
*Rong Chao, Rauf Nasretdinov, Yu-Chiang Frank Wang, Ante Jukić, Szu-Wei Fu, Yu Tsao*

Main category: cs.SD

TL;DR: USEMamba, a state-space speech enhancement model, achieved 2nd place in the Interspeech 2025 URGENT Challenge by handling diverse distortions and languages, using regression-based and generative methods.


<details>
  <summary>Details</summary>
Motivation: To advance universal, robust, and generalizable speech enhancement across varied conditions and languages.

Method: USEMamba combines state-space modeling for long-range sequences, time-frequency processing, and frequency-independent feature extraction, using regression for most distortions and generative methods for packet loss and bandwidth extension.

Result: USEMamba ranked 2nd in Track 1 during blind testing, showing strong generalization despite limited training data.

Conclusion: USEMamba effectively addresses diverse speech enhancement tasks, demonstrating robustness and generalizability.

Abstract: The Interspeech 2025 URGENT Challenge aimed to advance universal, robust, and
generalizable speech enhancement by unifying speech enhancement tasks across a
wide variety of conditions, including seven different distortion types and five
languages. We present Universal Speech Enhancement Mamba (USEMamba), a
state-space speech enhancement model designed to handle long-range sequence
modeling, time-frequency structured processing, and sampling
frequency-independent feature extraction. Our approach primarily relies on
regression-based modeling, which performs well across most distortions.
However, for packet loss and bandwidth extension, where missing content must be
inferred, a generative variant of the proposed USEMamba proves more effective.
Despite being trained on only a subset of the full training data, USEMamba
achieved 2nd place in Track 1 during the blind test phase, demonstrating strong
generalization across diverse conditions.

</details>


### [553] [Unfolding A Few Structures for The Many: Memory-Efficient Compression of Conformer and Speech Foundation Models](https://arxiv.org/pdf/2505.21237)
*Zhaoqing Li, Haoning Xu, Xurong Xie, Zengrui Jin, Tianzi Wang, Xunying Liu*

Main category: cs.SD

TL;DR: A memory-efficient model compression method for Conformer ASR and speech systems using a "small-to-large" design and self-distillation.


<details>
  <summary>Details</summary>
Motivation: To reduce memory and storage requirements in Conformer ASR and speech foundation models without sacrificing performance.

Method: Train a compact "seed" model and unfold it multiple times to emulate larger models, using KL-divergence for self-distillation.

Result: Achieves comparable performance to larger models with 35% and 30% parameter reductions for Conformer and wav2vec2, respectively.

Conclusion: The approach effectively compresses models while maintaining performance, offering significant memory and storage savings.

Abstract: This paper presents a novel memory-efficient model compression approach for
Conformer ASR and speech foundation systems. Our approach features a unique
"small-to-large" design. A compact "seed" model containing a few Conformer or
Transformer blocks is trained and unfolded many times to emulate the
performance of larger uncompressed models with different logical depths. The
seed model and many unfolded paths are jointly trained within a single
unfolding cycle. The KL-divergence between the largest unfolded and smallest
seed models is used in a self-distillation process to minimize their
performance disparity. Experimental results show that our foldable model
produces ASR performance comparable to individually constructed Conformer and
wav2vec2/HuBERT speech foundation models under various depth configurations,
while requiring only minimal memory and storage. Conformer and wav2vec2 models
with a reduction of 35% and 30% parameters are obtained without loss of
performance, respectively.

</details>


### [554] [Towards One-bit ASR: Extremely Low-bit Conformer Quantization Using Co-training and Stochastic Precision](https://arxiv.org/pdf/2505.21245)
*Zhaoqing Li, Haoning Xu, Zengrui Jin, Lingwei Meng, Tianzi Wang, Huimeng Wang, Youjun Chen, Mingyu Cui, Shujie Hu, Xunying Liu*

Main category: cs.SD

TL;DR: The paper proposes methods for 2-bit and 1-bit quantization of Conformer ASR systems to reduce memory footprint without performance loss.


<details>
  <summary>Details</summary>
Motivation: Model compression is needed due to increasing sizes of speech systems, especially for resource-constrained applications.

Method: Novel approaches include multiple precision model co-training, stochastic precision, and tensor-wise learnable scaling factors for low-bit quantization.

Result: Achieved performance-lossless 2-bit and 1-bit quantization with compression ratios of 16.2x and 16.6x, respectively, without significant WER increase.

Conclusion: The proposed methods enable efficient model compression for ASR systems without sacrificing performance.

Abstract: Model compression has become an emerging need as the sizes of modern speech
systems rapidly increase. In this paper, we study model weight quantization,
which directly reduces the memory footprint to accommodate computationally
resource-constrained applications. We propose novel approaches to perform
extremely low-bit (i.e., 2-bit and 1-bit) quantization of Conformer automatic
speech recognition systems using multiple precision model co-training,
stochastic precision, and tensor-wise learnable scaling factors to alleviate
quantization incurred performance loss. The proposed methods can achieve
performance-lossless 2-bit and 1-bit quantization of Conformer ASR systems
trained with the 300-hr Switchboard and 960-hr LibriSpeech corpus. Maximum
overall performance-lossless compression ratios of 16.2 and 16.6 times are
achieved without a statistically significant increase in the word error rate
(WER) over the full precision baseline systems, respectively.

</details>


### [555] [Towards Robust Automated Perceptual Voice Quality Assessment with Deep Learning](https://arxiv.org/pdf/2505.21356)
*Whenty Ariyanti, Kuan-Yu Chen, Sabato Marco Siniscalchi, Hsin-Min Wang, Yu Tsao*

Main category: cs.SD

TL;DR: VOQANet+ combines deep learning with handcrafted acoustic features for objective voice quality assessment, outperforming baselines and improving interpretability.


<details>
  <summary>Details</summary>
Motivation: Traditional voice quality assessments are subjective and prone to variability, necessitating automated, objective methods.

Method: VOQANet uses a deep learning framework with an attention mechanism and SFM embeddings; VOQANet+ integrates handcrafted features like jitter and HNR.

Result: VOQANet+ outperforms baselines in RMSE and PCC, showing robustness in noisy conditions.

Conclusion: Combining SFM embeddings with acoustic features enhances interpretability and resilience, making VOQANet+ suitable for real-world use.

Abstract: Objective: Perceptual voice quality assessment plays a critical role in
diagnosing and monitoring voice disorders by providing standardized evaluation
of vocal function. Traditionally, this process relies on expert raters
utilizing standard scales, such as the Consensus Auditory-Perceptual Evaluation
of Voice (CAPE-V) and Grade, Roughness, Breathiness, Asthenia, and Strain
(GRBAS). However, these metrics are inherently subjective and susceptible to
inter-rater variability, motivating the need for automated and objective
assessment methods. Methods: We propose Voice Quality Assessment Network
(VOQANet), a deep learning-based framework with an attention mechanism that
leverages a Speech Foundation Model (SFM) to capture high-level acoustic and
prosodic information from raw speech. To enhance robustness and
interpretability, we present VOQANet+, which integrates handcrafted acoustic
features such as jitter, shimmer, and harmonics-to-noise ratio (HNR) with SFM
embeddings. Results: Sentence-based input yields stronger performance than
vowel-based input, especially at the patient level. VOQANet consistently
outperforms baseline methods in RMSE and PCC, while VOQANet+ performs even
better and maintains robustness under noisy conditions. Conclusion: Combining
SFM embeddings with domain-informed acoustic features improves interpretability
and resilience. Significance: VOQANet+ shows strong potential for deployment in
real-world and telehealth settings, addressing the limitations of subjective
perceptual assessments with an interpretable and noise-resilient solution.

</details>


### [556] [Music Foundation Model as Generic Booster for Music Downstream Tasks](https://arxiv.org/pdf/2411.01135)
*WeiHsiang Liao, Yuhta Takida, Yukara Ikemiya, Zhi Zhong, Chieh-Hsin Lai, Giorgio Fabbro, Kazuki Shimada, Keisuke Toyama, Kinwai Cheuk, Marco A. Martínez-Ramírez, Shusuke Takahashi, Stefan Uhlich, Taketo Akama, Woosung Choi, Yuichiro Koyama, Yuki Mitsufuji*

Main category: cs.SD

TL;DR: SoniDo, a music foundation model, uses hierarchical intermediate features to boost performance in various music downstream tasks like tagging, transcription, source separation, and mixing.


<details>
  <summary>Details</summary>
Motivation: To enhance performance in music downstream tasks by leveraging hierarchical features from a single foundation model, addressing data scarcity challenges.

Method: Introduces SoniDo, a music foundation model (MFM) that extracts hierarchical features from music samples to constrain information granularity.

Result: Features from SoniDo improve performance in downstream tasks, supporting both understanding and generative tasks.

Conclusion: SoniDo's hierarchical features enhance music processing, making it more effective and accessible, especially for data-scarce tasks.

Abstract: We demonstrate the efficacy of using intermediate representations from a
single foundation model to enhance various music downstream tasks. We introduce
SoniDo, a music foundation model (MFM) designed to extract hierarchical
features from target music samples. By leveraging hierarchical intermediate
features, SoniDo constrains the information granularity, leading to improved
performance across various downstream tasks including both understanding and
generative tasks. We specifically evaluated this approach on representative
tasks such as music tagging, music transcription, music source separation, and
music mixing. Our results reveal that the features extracted from foundation
models provide valuable enhancements in training downstream task models. This
highlights the capability of using features extracted from music foundation
models as a booster for downstream tasks. Our approach not only benefits
existing task-specific models but also supports music downstream tasks
constrained by data scarcity. This paves the way for more effective and
accessible music processing solutions.

</details>


### [557] [Multi-Stage Speaker Diarization for Noisy Classrooms](https://arxiv.org/pdf/2505.10879)
*Ali Sartaz Khan, Tolulope Ogunremi, Ahmed Adel Attia, Dorottya Demszky*

Main category: cs.SD

TL;DR: The study evaluates multi-stage diarization models for noisy classrooms, showing denoising and hybrid VAD improve accuracy, achieving DERs of 17% (teacher-student) and 45% (all-speaker).


<details>
  <summary>Details</summary>
Motivation: Classroom settings pose challenges like poor audio quality, noise, and overlapping speech, making speaker diarization difficult.

Method: Uses Nvidia's NeMo pipeline, tests denoising, compares VAD models, and explores a hybrid VAD combining ASR timestamps with frame-level VAD.

Result: Denoising reduces DER, hybrid VAD achieves 17% DER (teacher-student) and 45% (all-speaker), with trade-offs in detection vs. confusion.

Conclusion: Multi-stage models and ASR integration enhance diarization in noisy classrooms, though balancing detection and confusion remains a challenge.

Abstract: Speaker diarization, the process of identifying "who spoke when" in audio
recordings, is essential for understanding classroom dynamics. However,
classroom settings present distinct challenges, including poor recording
quality, high levels of background noise, overlapping speech, and the
difficulty of accurately capturing children's voices. This study investigates
the effectiveness of multi-stage diarization models using Nvidia's NeMo
diarization pipeline. We assess the impact of denoising on diarization accuracy
and compare various voice activity detection (VAD) models, including
self-supervised transformer-based frame-wise VAD models. We also explore a
hybrid VAD approach that integrates Automatic Speech Recognition (ASR)
word-level timestamps with frame-level VAD predictions. We conduct experiments
using two datasets from English speaking classrooms to separate teacher vs.
student speech and to separate all speakers. Our results show that denoising
significantly improves the Diarization Error Rate (DER) by reducing the rate of
missed speech. Additionally, training on both denoised and noisy datasets leads
to substantial performance gains in noisy conditions. The hybrid VAD model
leads to further improvements in speech detection, achieving a DER as low as
17% in teacher-student experiments and 45% in all-speaker experiments. However,
we also identified trade-offs between voice activity detection and speaker
confusion. Overall, our study highlights the effectiveness of multi-stage
diarization models and integrating ASR-based information for enhancing speaker
diarization in noisy classroom environments.

</details>


### [558] [VocalAgent: Large Language Models for Vocal Health Diagnostics with Safety-Aware Evaluation](https://arxiv.org/pdf/2505.13577)
*Yubin Kim, Taehan Kim, Wonjune Kang, Eugene Park, Joonsik Yoon, Dongjae Lee, Xin Liu, Daniel McDuff, Hyeonhoon Lee, Cynthia Breazeal, Hae Won Park*

Main category: cs.SD

TL;DR: VocalAgent, an audio LLM, improves vocal health diagnosis with high accuracy, ethical validation, and cross-lingual capabilities.


<details>
  <summary>Details</summary>
Motivation: Addressing the lack of accessible vocal health diagnosis globally.

Method: Uses Qwen-Audio-Chat fine-tuned on hospital datasets, with safety, cross-lingual, and modality evaluations.

Result: Superior accuracy in voice disorder classification compared to baselines.

Conclusion: VocalAgent provides a scalable, ethical solution for vocal health diagnostics.

Abstract: Vocal health plays a crucial role in peoples' lives, significantly impacting
their communicative abilities and interactions. However, despite the global
prevalence of voice disorders, many lack access to convenient diagnosis and
treatment. This paper introduces VocalAgent, an audio large language model
(LLM) to address these challenges through vocal health diagnosis. We leverage
Qwen-Audio-Chat fine-tuned on three datasets collected in-situ from hospital
patients, and present a multifaceted evaluation framework encompassing a safety
assessment to mitigate diagnostic biases, cross-lingual performance analysis,
and modality ablation studies. VocalAgent demonstrates superior accuracy on
voice disorder classification compared to state-of-the-art baselines. Its
LLM-based method offers a scalable solution for broader adoption of health
diagnostics, while underscoring the importance of ethical and technical
validation.

</details>


### [559] [The Multimodal Information Based Speech Processing (MISP) 2025 Challenge: Audio-Visual Diarization and Recognition](https://arxiv.org/pdf/2505.13971)
*Ming Gao, Shilong Wu, Hang Chen, Jun Du, Chin-Hui Lee, Shinji Watanabe, Jingdong Chen, Siniscalchi Sabato Marco, Odette Scharenborg*

Main category: cs.SD

TL;DR: The MISP 2025 Challenge focused on improving meeting transcription by combining audio and video modalities, achieving significant performance gains in diarization and speech recognition tasks.


<details>
  <summary>Details</summary>
Motivation: Meetings present complex acoustic challenges, motivating the use of multi-modal (audio-visual) approaches for better transcription accuracy.

Method: The challenge included tasks like AVSD, AVSR, and AVDR, using a dataset with baseline systems and participant solutions.

Result: Top systems improved DER by 7.43% (8.09% DER), CER by 10.62% (9.48% CER), and cpCER by 72.49% (11.56% cpCER).

Conclusion: Incorporating video with audio significantly enhances meeting transcription performance, as demonstrated by the MISP 2025 Challenge.

Abstract: Meetings are a valuable yet challenging scenario for speech applications due
to complex acoustic conditions. This paper summarizes the outcomes of the MISP
2025 Challenge, hosted at Interspeech 2025, which focuses on multi-modal,
multi-device meeting transcription by incorporating video modality alongside
audio. The tasks include Audio-Visual Speaker Diarization (AVSD), Audio-Visual
Speech Recognition (AVSR), and Audio-Visual Diarization and Recognition (AVDR).
We present the challenge's objectives, tasks, dataset, baseline systems, and
solutions proposed by participants. The best-performing systems achieved
significant improvements over the baseline: the top AVSD model achieved a
Diarization Error Rate (DER) of 8.09%, improving by 7.43%; the top AVSR system
achieved a Character Error Rate (CER) of 9.48%, improving by 10.62%; and the
best AVDR system achieved a concatenated minimum-permutation Character Error
Rate (cpCER) of 11.56%, improving by 72.49%.

</details>


### [560] [X-ARES: A Comprehensive Framework for Assessing Audio Encoder Performance](https://arxiv.org/pdf/2505.16369)
*Junbo Zhang, Heinrich Dinkel, Yadong Niu, Chenyu Liu, Si Cheng, Anbei Zhao, Jian Luan*

Main category: cs.SD

TL;DR: X-ARES is a new open-source benchmark for evaluating audio encoders across diverse tasks like speech, environmental sounds, and music, using linear fine-tuning and unparameterized evaluation.


<details>
  <summary>Details</summary>
Motivation: To systematically assess audio encoder performance across varied domains and tasks, addressing the complexity of general audio representation learning.

Method: The framework includes 22 tasks (e.g., speech recognition, emotion detection) and two evaluation approaches: linear fine-tuning and unparameterized evaluation.

Result: Evaluation shows significant performance variations among state-of-the-art audio encoders across tasks and domains.

Conclusion: X-ARES highlights the challenges in general audio representation learning and provides a comprehensive benchmark for future research.

Abstract: We introduces X-ARES (eXtensive Audio Representation and Evaluation Suite), a
novel open-source benchmark designed to systematically assess audio encoder
performance across diverse domains. By encompassing tasks spanning speech,
environmental sounds, and music, X-ARES provides two evaluation approaches for
evaluating audio representations: linear fine-tuning and unparameterized
evaluation. The framework includes 22 distinct tasks that cover essential
aspects of audio processing, from speech recognition and emotion detection to
sound event classification and music genre identification. Our extensive
evaluation of state-of-the-art audio encoders reveals significant performance
variations across different tasks and domains, highlighting the complexity of
general audio representation learning.

</details>


### [561] [CosyVoice 3: Towards In-the-wild Speech Generation via Scaling-up and Post-training](https://arxiv.org/pdf/2505.17589)
*Zhihao Du, Changfeng Gao, Yuxuan Wang, Fan Yu, Tianyu Zhao, Hao Wang, Xiang Lv, Hui Wang, Chongjia Ni, Xian Shi, Keyu An, Guanrou Yang, Yabin Li, Yanni Chen, Zhifu Gao, Qian Chen, Yue Gu, Mengzhe Chen, Yafeng Chen, Shiliang Zhang, Wen Wang, Jieping Ye*

Main category: cs.SD

TL;DR: CosyVoice 3 improves upon CosyVoice 2 by enhancing multilingual speech synthesis with better prosody, speaker similarity, and content consistency, using a novel tokenizer, reward model, and scaled data and model size.


<details>
  <summary>Details</summary>
Motivation: Address limitations of CosyVoice 2 in language coverage, domain diversity, and post-training techniques for zero-shot multilingual speech synthesis.

Method: Introduces a novel speech tokenizer, a differentiable reward model, and scales dataset (1M hours, 9 languages, 18 dialects) and model size (1.5B parameters).

Result: Achieves superior performance in content consistency, speaker similarity, and prosody naturalness.

Conclusion: CosyVoice 3 significantly advances speech synthesis in the wild, with scalable improvements over its predecessor.

Abstract: In our prior works, we introduced a scalable streaming speech synthesis
model, CosyVoice 2, which integrates a large language model (LLM) and a
chunk-aware flow matching (FM) model, and achieves low-latency bi-streaming
speech synthesis and human-parity quality. Despite these advancements,
CosyVoice 2 exhibits limitations in language coverage, domain diversity, data
volume, text formats, and post-training techniques. In this paper, we present
CosyVoice 3, an improved model designed for zero-shot multilingual speech
synthesis in the wild, surpassing its predecessor in content consistency,
speaker similarity, and prosody naturalness. Key features of CosyVoice 3
include: 1) A novel speech tokenizer to improve prosody naturalness, developed
via supervised multi-task training, including automatic speech recognition,
speech emotion recognition, language identification, audio event detection, and
speaker analysis. 2) A new differentiable reward model for post-training
applicable not only to CosyVoice 3 but also to other LLM-based speech synthesis
models. 3) Dataset Size Scaling: Training data is expanded from ten thousand
hours to one million hours, encompassing 9 languages and 18 Chinese dialects
across various domains and text formats. 4) Model Size Scaling: Model
parameters are increased from 0.5 billion to 1.5 billion, resulting in enhanced
performance on our multilingual benchmark due to the larger model capacity.
These advancements contribute significantly to the progress of speech synthesis
in the wild. We encourage readers to listen to the demo at
https://funaudiollm.github.io/cosyvoice3.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [562] [FastCache: Fast Caching for Diffusion Transformer Through Learnable Linear Approximation](https://arxiv.org/pdf/2505.20353)
*Dong Liu, Jiayi Zhang, Yifan Li, Yanxuan Yu, Ben Lengerich, Ying Nian Wu*

Main category: cs.LG

TL;DR: FastCache accelerates Diffusion Transformers (DiT) by caching and compressing hidden states, reducing computation while maintaining generation quality.


<details>
  <summary>Details</summary>
Motivation: DiTs are computationally intensive due to iterative structures and deep transformers. FastCache aims to improve efficiency by exploiting redundancy in internal representations.

Method: FastCache uses spatial-aware token selection and a transformer-level cache to filter redundant tokens and reuse latent activations, respectively. Learnable linear approximation preserves fidelity.

Result: Empirical evaluations show reduced latency and memory usage with high generation quality (measured by FID and t-FID).

Conclusion: FastCache effectively balances efficiency and fidelity, offering a practical solution for accelerating DiT inference.

Abstract: Diffusion Transformers (DiT) are powerful generative models but remain
computationally intensive due to their iterative structure and deep transformer
stacks. To alleviate this inefficiency, we propose FastCache, a
hidden-state-level caching and compression framework that accelerates DiT
inference by exploiting redundancy within the model's internal representations.
FastCache introduces a dual strategy: (1) a spatial-aware token selection
mechanism that adaptively filters redundant tokens based on hidden state
saliency, and (2) a transformer-level cache that reuses latent activations
across timesteps when changes are statistically insignificant. These modules
work jointly to reduce unnecessary computation while preserving generation
fidelity through learnable linear approximation. Theoretical analysis shows
that FastCache maintains bounded approximation error under a
hypothesis-testing-based decision rule. Empirical evaluations across multiple
DiT variants demonstrate substantial reductions in latency and memory usage,
with best generation output quality compared to other cache methods, as
measured by FID and t-FID. Code implementation of FastCache is available on
GitHub at https://github.com/NoakLiu/FastCache-xDiT.

</details>


### [563] [FMEnets: Flow, Material, and Energy networks for non-ideal plug flow reactor design](https://arxiv.org/pdf/2505.20300)
*Chenxi Wu, Juan Diego Toscano, Khemraj Shukla, Yingjie Chen, Ali Shahmohammadi, Edward Raymond, Thomas Toupy, Neda Nazemifard, Charles Papageorgiou, George Em Karniadakis*

Main category: cs.LG

TL;DR: FMEnets is a physics-informed ML framework for non-ideal plug flow reactors, integrating governing equations into a multi-scale model. It solves forward and inverse problems, with FME-PINNs and FME-KANs variants, achieving high accuracy (<2.5% error) and robustness to noise.


<details>
  <summary>Details</summary>
Motivation: To address the challenges of designing and analyzing non-ideal plug flow reactors by integrating physical laws with machine learning for accurate predictions and parameter inference.

Method: FMEnets combines Navier-Stokes, material balance, and energy balance equations into a multi-scale network. It uses three sub-networks with independent optimizers for forward (predicting profiles) and inverse (inferring parameters) problems. Two variants: FME-PINNs (MLPs) and FME-KANs (Kolmogorov-Arnold Networks).

Result: FMEnets achieves <2.5% error for unknown kinetic parameters, with FME-KANs outperforming FME-PINNs in noisy conditions. Validated against finite element simulations in three reaction scenarios.

Conclusion: FMEnets offers an efficient, accurate framework for reactor design, integrating physics and ML, and enabling use of noisy or limited data for optimization.

Abstract: We propose FMEnets, a physics-informed machine learning framework for the
design and analysis of non-ideal plug flow reactors. FMEnets integrates the
fundamental governing equations (Navier-Stokes for fluid flow, material balance
for reactive species transport, and energy balance for temperature
distribution) into a unified multi-scale network model. The framework is
composed of three interconnected sub-networks with independent optimizers that
enable both forward and inverse problem-solving. In the forward mode, FMEnets
predicts velocity, pressure, species concentrations, and temperature profiles
using only inlet and outlet information. In the inverse mode, FMEnets utilizes
sparse multi-residence-time measurements to simultaneously infer unknown
kinetic parameters and states. FMEnets can be implemented either as FME-PINNs,
which employ conventional multilayer perceptrons, or as FME-KANs, based on
Kolmogorov-Arnold Networks. Comprehensive ablation studies highlight the
critical role of the FMEnets architecture in achieving accurate predictions.
Specifically, FME-KANs are more robust to noise than FME-PINNs, although both
representations are comparable in accuracy and speed in noise-free conditions.
The proposed framework is applied to three different sets of reaction scenarios
and is compared with finite element simulations. FMEnets effectively captures
the complex interactions, achieving relative errors less than 2.5% for the
unknown kinetic parameters. The new network framework not only provides a
computationally efficient alternative for reactor design and optimization, but
also opens new avenues for integrating empirical correlations, limited and
noisy experimental data, and fundamental physical equations to guide reactor
design.

</details>


### [564] [Joint-stochastic-approximation Random Fields with Application to Semi-supervised Learning](https://arxiv.org/pdf/2505.20330)
*Yunfu Song, Zhijian Ou*

Main category: cs.LG

TL;DR: JRFs address mode missing/covering in DGMs and classification-generation conflict in SSL, achieving balanced performance in generation and classification.


<details>
  <summary>Details</summary>
Motivation: Identify issues in DGMs (GANs/VAEs) for SSL: mode missing/covering and classification-generation conflict.

Method: Propose JRFs, a new family of algorithms for deep undirected generative models, applied to SSL.

Result: JRFs balance mode covering/missing, match data distribution, and achieve state-of-art classification and good generation.

Conclusion: JRFs effectively solve the identified problems in SSL, offering balanced performance in generation and classification.

Abstract: Our examination of deep generative models (DGMs) developed for
semi-supervised learning (SSL), mainly GANs and VAEs, reveals two problems.
First, mode missing and mode covering phenomenons are observed in genertion
with GANs and VAEs. Second, there exists an awkward conflict between good
classification and good generation in SSL by employing directed generative
models. To address these problems, we formally present
joint-stochastic-approximation random fields (JRFs) -- a new family of
algorithms for building deep undirected generative models, with application to
SSL. It is found through synthetic experiments that JRFs work well in balancing
mode covering and mode missing, and match the empirical data distribution well.
Empirically, JRFs achieve good classification results comparable to the
state-of-art methods on widely adopted datasets -- MNIST, SVHN, and CIFAR-10 in
SSL, and simultaneously perform good generation.

</details>


### [565] [PDFBench: A Benchmark for De novo Protein Design from Function](https://arxiv.org/pdf/2505.20346)
*Jiahao Kuang, Nuowei Liu, Changzhi Sun, Tao Ji, Yuanbin Wu*

Main category: cs.LG

TL;DR: PDFBench is introduced as the first comprehensive benchmark for evaluating de novo protein design, addressing gaps in fair comparisons and multifaceted assessment.


<details>
  <summary>Details</summary>
Motivation: Current methods lack standardized datasets and evaluation metrics, hindering fair comparisons and comprehensive assessments of designed proteins.

Method: PDFBench supports two tasks (description-guided and keyword-guided design) and includes 22 metrics covering sequence plausibility, structural fidelity, language-protein alignment, novelty, and diversity.

Result: Five state-of-the-art baselines are evaluated, revealing their strengths and weaknesses. Inter-metric correlations are analyzed to guide metric selection.

Conclusion: PDFBench provides a unified framework to advance function-driven de novo protein design.

Abstract: In recent years, while natural language processing and multimodal learning
have seen rapid advancements, the field of de novo protein design has also
experienced significant growth. However, most current methods rely on
proprietary datasets and evaluation rubrics, making fair comparisons between
different approaches challenging. Moreover, these methods often employ
evaluation metrics that capture only a subset of the desired properties of
designed proteins, lacking a comprehensive assessment framework. To address
these, we introduce PDFBench, the first comprehensive benchmark for evaluating
de novo protein design from function. PDFBench supports two tasks:
description-guided design and keyword-guided design. To ensure fair and
multifaceted evaluation, we compile 22 metrics covering sequence plausibility,
structural fidelity, and language-protein alignment, along with measures of
novelty and diversity. We evaluate five state-of-the-art baselines, revealing
their respective strengths and weaknesses across tasks. Finally, we analyze
inter-metric correlations, exploring the relationships between four categories
of metrics, and offering guidelines for metric selection. PDFBench establishes
a unified framework to drive future advances in function-driven de novo protein
design.

</details>


### [566] [Decision Flow Policy Optimization](https://arxiv.org/pdf/2505.20350)
*Jifeng Hu, Sili Huang, Siyuan Guo, Zhaogeng Liu, Li Shen, Lichao Sun, Hechang Chen, Yi Chang, Dacheng Tao*

Main category: cs.LG

TL;DR: Decision Flow integrates multi-modal action distribution modeling and policy optimization in reinforcement learning, outperforming traditional methods.


<details>
  <summary>Details</summary>
Motivation: Traditional methods separate multi-modal distribution fitting and policy optimization, limiting performance.

Method: Proposes Decision Flow, a unified framework using flow-based models for action generation and policy optimization.

Result: Achieves or matches state-of-the-art performance in offline RL environments.

Conclusion: Decision Flow effectively unifies multi-modal modeling and policy optimization, enhancing performance.

Abstract: In recent years, generative models have shown remarkable capabilities across
diverse fields, including images, videos, language, and decision-making. By
applying powerful generative models such as flow-based models to reinforcement
learning, we can effectively model complex multi-modal action distributions and
achieve superior robotic control in continuous action spaces, surpassing the
limitations of single-modal action distributions with traditional
Gaussian-based policies. Previous methods usually adopt the generative models
as behavior models to fit state-conditioned action distributions from datasets,
with policy optimization conducted separately through additional policies using
value-based sample weighting or gradient-based updates. However, this
separation prevents the simultaneous optimization of multi-modal distribution
fitting and policy improvement, ultimately hindering the training of models and
degrading the performance. To address this issue, we propose Decision Flow, a
unified framework that integrates multi-modal action distribution modeling and
policy optimization. Specifically, our method formulates the action generation
procedure of flow-based models as a flow decision-making process, where each
action generation step corresponds to one flow decision. Consequently, our
method seamlessly optimizes the flow policy while capturing multi-modal action
distributions. We provide rigorous proofs of Decision Flow and validate the
effectiveness through extensive experiments across dozens of offline RL
environments. Compared with established offline RL baselines, the results
demonstrate that our method achieves or matches the SOTA performance.

</details>


### [567] [GraLoRA: Granular Low-Rank Adaptation for Parameter-Efficient Fine-Tuning](https://arxiv.org/pdf/2505.20355)
*Yeonjoon Jung, Daehyun Ahn, Hyungjun Kim, Taesu Kim, Eunhyeok Park*

Main category: cs.LG

TL;DR: GraLoRA improves LoRA by partitioning weight matrices into sub-blocks with individual low-rank adapters, overcoming overfitting and gradient issues, achieving better performance than LoRA and nearing full fine-tuning results.


<details>
  <summary>Details</summary>
Motivation: LoRA's structural bottleneck causes gradient entanglement and overfitting at higher ranks, limiting its performance compared to full fine-tuning.

Method: GraLoRA partitions weight matrices into sub-blocks, each with its own low-rank adapter, enhancing representational capacity without added cost.

Result: GraLoRA outperforms LoRA and other baselines, achieving up to +8.5% absolute gain in Pass@1 on HumanEval+, with consistent improvements across model sizes and ranks.

Conclusion: GraLoRA is a scalable and robust PEFT solution, addressing LoRA's limitations and closely approximating full fine-tuning performance.

Abstract: Low-Rank Adaptation (LoRA) is a popular method for parameter-efficient
fine-tuning (PEFT) of generative models, valued for its simplicity and
effectiveness. Despite recent enhancements, LoRA still suffers from a
fundamental limitation: overfitting when the bottleneck is widened. It performs
best at ranks 32-64, yet its accuracy stagnates or declines at higher ranks,
still falling short of full fine-tuning (FFT) performance. We identify the root
cause as LoRA's structural bottleneck, which introduces gradient entanglement
to the unrelated input channels and distorts gradient propagation. To address
this, we introduce a novel structure, Granular Low-Rank Adaptation (GraLoRA)
that partitions weight matrices into sub-blocks, each with its own low-rank
adapter. With negligible computational or storage cost, GraLoRA overcomes
LoRA's limitations, effectively increases the representational capacity, and
more closely approximates FFT behavior. Experiments on code generation and
commonsense reasoning benchmarks show that GraLoRA consistently outperforms
LoRA and other baselines, achieving up to +8.5% absolute gain in Pass@1 on
HumanEval+. These improvements hold across model sizes and rank settings,
making GraLoRA a scalable and robust solution for PEFT. Code, data, and scripts
are available at https://github.com/SqueezeBits/GraLoRA.git

</details>


### [568] [The challenge of hidden gifts in multi-agent reinforcement learning](https://arxiv.org/pdf/2505.20579)
*Dane Malenfant, Blake A. Richards*

Main category: cs.LG

TL;DR: The paper explores 'hidden gifts' in MARL, where agents unknowingly benefit from others' actions. A simple grid-world task reveals that state-of-the-art RL algorithms struggle with collective rewards due to hidden actions. Independent agents succeed with action history, and a correction term improves their reliability.


<details>
  <summary>Details</summary>
Motivation: To understand how 'hidden gifts'—unseen beneficial actions by others—impact credit assignment in MARL and to develop solutions for such scenarios.

Method: A grid-world task where agents must unlock doors with a shared key, requiring them to drop the key for others (a hidden gift). Tests various RL algorithms, including MARL, and introduces a correction term for independent agents.

Result: State-of-the-art RL and MARL algorithms fail to achieve collective rewards. Independent agents with action history succeed, and a correction term enhances their reliability.

Conclusion: Hidden gifts complicate credit assignment in MARL. Independent agents with learning awareness can overcome this challenge, suggesting potential improvements for MARL algorithms.

Abstract: Sometimes we benefit from actions that others have taken even when we are
unaware that they took those actions. For example, if your neighbor chooses not
to take a parking spot in front of your house when you are not there, you can
benefit, even without being aware that they took this action. These "hidden
gifts" represent an interesting challenge for multi-agent reinforcement
learning (MARL), since assigning credit when the beneficial actions of others
are hidden is non-trivial. Here, we study the impact of hidden gifts with a
very simple MARL task. In this task, agents in a grid-world environment have
individual doors to unlock in order to obtain individual rewards. As well, if
all the agents unlock their door the group receives a larger collective reward.
However, there is only one key for all of the doors, such that the collective
reward can only be obtained when the agents drop the key for others after they
use it. Notably, there is nothing to indicate to an agent that the other agents
have dropped the key, thus the act of dropping the key for others is a "hidden
gift". We show that several different state-of-the-art RL algorithms, including
MARL algorithms, fail to learn how to obtain the collective reward in this
simple task. Interestingly, we find that independent model-free policy gradient
agents can solve the task when we provide them with information about their own
action history, but MARL agents still cannot solve the task with action
history. Finally, we derive a correction term for these independent agents,
inspired by learning aware approaches, which reduces the variance in learning
and helps them to converge to collective success more reliably. These results
show that credit assignment in multi-agent settings can be particularly
challenging in the presence of "hidden gifts", and demonstrate that learning
awareness in independent agents can benefit these settings.

</details>


### [569] [Learning and Interpreting Gravitational-Wave Features from CNNs with a Random Forest Approach](https://arxiv.org/pdf/2505.20357)
*Jun Tian, He Wang, Jibo He, Yu Pan, Shuo Cao, Qingquan Jiang*

Main category: cs.LG

TL;DR: A hybrid CNN-RF model improves GW detection performance and interpretability by integrating physically interpretable metrics with CNN features.


<details>
  <summary>Details</summary>
Motivation: The physical meaning of features learned by CNNs in GW detection is unclear, limiting model interpretability.

Method: Combines CNN feature extraction with an RF classifier, using four interpretable metrics (variance, SNR, waveform overlap, peak amplitude) alongside CNN outputs.

Result: Outperforms baseline CNN with 21% sensitivity improvement at fixed false alarm rate; better detects low-SNR signals.

Conclusion: Physically motivated post-processing of CNN features enhances interpretability and efficiency in GW detection.

Abstract: Convolutional neural networks (CNNs) have become widely adopted in
gravitational wave (GW) detection pipelines due to their ability to
automatically learn hierarchical features from raw strain data. However, the
physical meaning of these learned features remains underexplored, limiting the
interpretability of such models. In this work, we propose a hybrid architecture
that combines a CNN-based feature extractor with a random forest (RF)
classifier to improve both detection performance and interpretability. Unlike
prior approaches that directly connect classifiers to CNN outputs, our method
introduces four physically interpretable metrics - variance, signal-to-noise
ratio (SNR), waveform overlap, and peak amplitude - computed from the final
convolutional layer. These are jointly used with the CNN output in the RF
classifier to enable more informed decision boundaries. Tested on long-duration
strain datasets, our hybrid model outperforms a baseline CNN model, achieving a
relative improvement of 21\% in sensitivity at a fixed false alarm rate of 10
events per month. Notably, it also shows improved detection of low-SNR signals
(SNR $\le$ 10), which are especially vulnerable to misclassification in noisy
environments. Feature attribution via the RF model reveals that both
CNN-extracted and handcrafted features contribute significantly to
classification decisions, with learned variance and CNN outputs ranked among
the most informative. These findings suggest that physically motivated
post-processing of CNN feature maps can serve as a valuable tool for
interpretable and efficient GW detection, bridging the gap between deep
learning and domain knowledge.

</details>


### [570] [Risk-aware Direct Preference Optimization under Nested Risk Measure](https://arxiv.org/pdf/2505.20359)
*Lijun Zhang, Lin Li, Yajie Qi, Huizhong Song, Yaodong Yang, Jun Wang, Wei Wei*

Main category: cs.LG

TL;DR: Ra-DPO introduces risk-awareness in LLM fine-tuning using nested risk measures to balance alignment performance and model drift.


<details>
  <summary>Details</summary>
Motivation: Existing KL divergence constraints may not suffice for tight risk control in aligning LLMs with human values.

Method: Formulates a constrained risk-aware advantage function maximization problem, converting the Bradley-Terry model into token-level representation.

Result: Superior performance in balancing alignment and model drift across three datasets (IMDb, Anthropic HH, AlpacaEval).

Conclusion: Ra-DPO effectively enhances risk-awareness while maintaining alignment performance, with code open-sourced.

Abstract: When fine-tuning pre-trained Large Language Models (LLMs) to align with human
values and intentions, maximizing the estimated reward can lead to superior
performance, but it also introduces potential risks due to deviations from the
reference model's intended behavior. Most existing methods typically introduce
KL divergence to constrain deviations between the trained model and the
reference model; however, this may not be sufficient in certain applications
that require tight risk control. In this paper, we introduce Risk-aware Direct
Preference Optimization (Ra-DPO), a novel approach that incorporates
risk-awareness by employing a class of nested risk measures. This approach
formulates a constrained risk-aware advantage function maximization problem and
then converts the Bradley-Terry model into a token-level representation. The
objective function maximizes the likelihood of the policy while suppressing the
deviation between a trained model and the reference model using a sequential
risk ratio, thereby enhancing the model's risk-awareness. Experimental results
across three open-source datasets: IMDb Dataset, Anthropic HH Dataset, and
AlpacaEval, demonstrate the proposed method's superior performance in balancing
alignment performance and model drift. Our code is opensourced at
https://github.com/zlj123-max/Ra-DPO.

</details>


### [571] [GRAPE: Optimize Data Mixture for Group Robust Multi-target Adaptive Pretraining](https://arxiv.org/pdf/2505.20380)
*Simin Fan, Maria Ios Glarou, Martin Jaggi*

Main category: cs.LG

TL;DR: GRAPE is a multi-target domain reweighting framework that dynamically adjusts domain and task weights for robust performance across multiple tasks, outperforming baselines in reasoning and multilingual benchmarks.


<details>
  <summary>Details</summary>
Motivation: Existing domain reweighting methods overfit to single tasks, degrading performance on others. GRAPE addresses this by optimizing for multiple tasks simultaneously.

Method: GRAPE uses a minimax optimization: inner maximization adjusts task weights via group DRO, prioritizing harder tasks; outer minimization optimizes domain weights for loss reduction.

Result: GRAPE outperforms baselines on 6 reasoning benchmarks and improves language modeling in 8 low-resource languages.

Conclusion: GRAPE effectively balances multi-task performance, demonstrating robustness and scalability across diverse benchmarks.

Abstract: The performance of large language models (LLMs) across diverse downstream
applications is fundamentally governed by the quality and composition of their
pretraining corpora. Existing domain reweighting algorithms primarily optimize
data mixtures for a single target task, thereby resulting in models that
overfit to specialized objectives while exhibiting substantial performance
degradation on other benchmarks. This paper introduces Group Robust
Multi-target Adaptive PrEtraining (GRAPE), a novel multi-source-multi-target
domain reweighting framework designed to calibrate pretraining data mixtures
for robust performance across multiple target tasks simultaneously. GRAPE
dynamically adjusts sampling weights across source domains (domain weights)
while concurrently modulating task weights that quantify the relative
importance of each individual target task. This adaptive process prioritizes
tasks based on their learning difficulty throughout training. We formulate this
interleaved reweighting mechanism as a minimax optimization problem: The inner
maximization adjusts task weights leveraging group
distributed-robust-optimization (DRO), where those tasks demonstrating the
least improvement under the current data mixture are prioritized with higher
weights; The outer minimization then optimizes domain weights to maximize loss
reduction on the prioritized tasks. Experiments on ClimbLab and SlimPajama
datasets demonstrate that GRAPE consistently outperforms baseline methods in
terms of reasoning performance across 6 benchmarks. Furthermore, when applied
to multilingual targets, GRAPE effectively identifies optimal training mixtures
from mainstream languages, achieving superior language modeling capabilities
across 8 low-resource target languages.

</details>


### [572] [Breaking the Performance Ceiling in Complex Reinforcement Learning requires Inference Strategies](https://arxiv.org/pdf/2505.21236)
*Felix Chalumeau, Daniel Rajaonarivonivelomanantsoa, Ruan de Kock, Claude Formanek, Sasha Abramowitz, Oumayma Mahjoub, Wiem Khlifi, Simon Du Toit, Louay Ben Nessir, Refiloe Shabe, Arnol Fokam, Siddarth Singh, Ulrich Mbou Sob, Arnu Pretorius*

Main category: cs.LG

TL;DR: Using an inference phase with a specific strategy during execution improves performance in complex multi-agent RL tasks by up to 126%, averaging 45% better than state-of-the-art.


<details>
  <summary>Details</summary>
Motivation: Real-world RL applications face performance ceilings due to complexity and coordination challenges, even after training.

Method: Employing an inference phase at execution time with a chosen strategy to explore multiple attempts before finalizing solutions.

Result: Up to 126% and average 45% improvement over state-of-the-art across 17 tasks, with minimal extra compute time.

Conclusion: Inference strategies during execution are crucial for breaking performance ceilings in complex RL, supported by extensive experiments.

Abstract: Reinforcement learning (RL) systems have countless applications, from
energy-grid management to protein design. However, such real-world scenarios
are often extremely difficult, combinatorial in nature, and require complex
coordination between multiple agents. This level of complexity can cause even
state-of-the-art RL systems, trained until convergence, to hit a performance
ceiling which they are unable to break out of with zero-shot inference.
Meanwhile, many digital or simulation-based applications allow for an inference
phase that utilises a specific time and compute budget to explore multiple
attempts before outputting a final solution. In this work, we show that such an
inference phase employed at execution time, and the choice of a corresponding
inference strategy, are key to breaking the performance ceiling observed in
complex multi-agent RL problems. Our main result is striking: we can obtain up
to a 126% and, on average, a 45% improvement over the previous state-of-the-art
across 17 tasks, using only a couple seconds of extra wall-clock time during
execution. We also demonstrate promising compute scaling properties, supported
by over 60k experiments, making it the largest study on inference strategies
for complex RL to date. Our experimental data and code are available at
https://sites.google.com/view/inf-marl.

</details>


### [573] [Holes in Latent Space: Topological Signatures Under Adversarial Influence](https://arxiv.org/pdf/2505.20435)
*Aideen Fay, Inés García-Redondo, Qiquan Wang, Haim Dubossarsky, Anthea Monod*

Main category: cs.LG

TL;DR: Persistent homology (PH) is used to analyze adversarial effects on LLMs, revealing compressed latent topologies and reduced structural diversity under attacks like backdoor fine-tuning and indirect prompt injection.


<details>
  <summary>Details</summary>
Motivation: To understand how adversarial conditions impact language models by capturing global and local details in high-dimensional activation spaces.

Method: Employ persistent homology (PH) to analyze multiscale latent space dynamics in six state-of-the-art LLMs under two attack modes. Introduce a neuron-level PH framework for finer-grained analysis.

Result: Adversarial conditions compress latent topologies, reducing structural diversity at smaller scales and amplifying dominant features at coarser scales. These signatures are robust across layers, architectures, and model sizes.

Conclusion: PH provides a principled and unifying approach to interpret representational dynamics in LLMs, especially under distributional shift.

Abstract: Understanding how adversarial conditions affect language models requires
techniques that capture both global structure and local detail within
high-dimensional activation spaces. We propose persistent homology (PH), a tool
from topological data analysis, to systematically characterize multiscale
latent space dynamics in LLMs under two distinct attack modes -- backdoor
fine-tuning and indirect prompt injection. By analyzing six state-of-the-art
LLMs, we show that adversarial conditions consistently compress latent
topologies, reducing structural diversity at smaller scales while amplifying
dominant features at coarser ones. These topological signatures are
statistically robust across layers, architectures, model sizes, and align with
the emergence of adversarial effects deeper in the network. To capture
finer-grained mechanisms underlying these shifts, we introduce a neuron-level
PH framework that quantifies how information flows and transforms within and
across layers. Together, our findings demonstrate that PH offers a principled
and unifying approach to interpreting representational dynamics in LLMs,
particularly under distributional shift.

</details>


### [574] [HoPE: Hybrid of Position Embedding for Length Generalization in Vision-Language Models](https://arxiv.org/pdf/2505.20444)
*Haoran Li, Yingjie Qin, Baoyuan Ou, Lai Xu, Ruiwen Xu*

Main category: cs.LG

TL;DR: The paper introduces HoPE, a hybrid position embedding method to enhance long-context capabilities of Vision-Language Models (VLMs) in videos, outperforming existing methods.


<details>
  <summary>Details</summary>
Motivation: Current VLMs struggle with long-context scenarios like videos, and existing RoPE methods lack theoretical grounding for spatial-temporal dependencies.

Method: HoPE combines a hybrid frequency allocation strategy for semantic modeling and dynamic temporal scaling for flexible learning and inference.

Result: HoPE outperforms existing methods on long video understanding and retrieval tasks across four benchmarks.

Conclusion: HoPE effectively addresses long-context challenges in VLMs, offering theoretical and practical improvements.

Abstract: Vision-Language Models (VLMs) have made significant progress in multimodal
tasks. However, their performance often deteriorates in long-context scenarios,
particularly long videos. While Rotary Position Embedding (RoPE) has been
widely adopted for length generalization in Large Language Models (LLMs),
extending vanilla RoPE to capture the intricate spatial-temporal dependencies
in videos remains an unsolved challenge. Existing methods typically allocate
different frequencies within RoPE to encode 3D positional information. However,
these allocation strategies mainly rely on heuristics, lacking in-depth
theoretical analysis. In this paper, we first study how different allocation
strategies impact the long-context capabilities of VLMs. Our analysis reveals
that current multimodal RoPEs fail to reliably capture semantic similarities
over extended contexts. To address this issue, we propose HoPE, a Hybrid of
Position Embedding designed to improve the long-context capabilities of VLMs.
HoPE introduces a hybrid frequency allocation strategy for reliable semantic
modeling over arbitrarily long context, and a dynamic temporal scaling
mechanism to facilitate robust learning and flexible inference across diverse
context lengths. Extensive experiments across four video benchmarks on long
video understanding and retrieval tasks demonstrate that HoPE consistently
outperforms existing methods, confirming its effectiveness. Code is available
at https://github.com/hrlics/HoPE.

</details>


### [575] [Time Series Generation Under Data Scarcity: A Unified Generative Modeling Approach](https://arxiv.org/pdf/2505.20446)
*Tal Gonen, Itai Pemper, Ilan Naiman, Nimrod Berman, Omri Azencot*

Main category: cs.LG

TL;DR: A study evaluates generative models for time series under data-scarce conditions, proposing a diffusion-based framework that outperforms baselines in few-shot and full-data settings.


<details>
  <summary>Details</summary>
Motivation: To address the lack of understanding of generative models' performance under limited supervision in time series analysis.

Method: Proposes a unified diffusion-based framework with pre-training on diverse datasets, dynamic convolutional layers, and dataset token conditioning.

Result: Achieves state-of-the-art performance in few-shot settings and surpasses baselines even with full datasets.

Conclusion: Encourages revisiting few-shot generative modeling and pursuing unified, scalable solutions for time series research.

Abstract: Generative modeling of time series is a central challenge in time series
analysis, particularly under data-scarce conditions. Despite recent advances in
generative modeling, a comprehensive understanding of how state-of-the-art
generative models perform under limited supervision remains lacking. In this
work, we conduct the first large-scale study evaluating leading generative
models in data-scarce settings, revealing a substantial performance gap between
full-data and data-scarce regimes. To close this gap, we propose a unified
diffusion-based generative framework that can synthesize high-fidelity time
series across diverse domains using just a few examples. Our model is
pre-trained on a large, heterogeneous collection of time series datasets,
enabling it to learn generalizable temporal representations. It further
incorporates architectural innovations such as dynamic convolutional layers for
flexible channel adaptation and dataset token conditioning for domain-aware
generation. Without requiring abundant supervision, our unified model achieves
state-of-the-art performance in few-shot settings-outperforming domain-specific
baselines across a wide range of subset sizes. Remarkably, it also surpasses
all baselines even when tested on full datasets benchmarks, highlighting the
strength of pre-training and cross-domain generalization. We hope this work
encourages the community to revisit few-shot generative modeling as a key
problem in time series research and pursue unified solutions that scale
efficiently across domains. Code is available at
https://github.com/azencot-group/ImagenFew.

</details>


### [576] [DeepConvContext: A Multi-Scale Approach to Timeseries Classification in Human Activity Recognition](https://arxiv.org/pdf/2505.20894)
*Marius Bock, Michael Moeller, Kristof Van Laerhoven*

Main category: cs.LG

TL;DR: DeepConvContext improves HAR by modeling intra- and inter-window temporal patterns, outperforming DeepConvLSTM by 10% F1-score on average.


<details>
  <summary>Details</summary>
Motivation: Traditional HAR methods like DeepConvLSTM limit temporal context to within-window data, missing long-range dependencies.

Method: DeepConvContext processes sequences of time-ordered windows using LSTMs, avoiding attention mechanisms.

Result: Achieves 10% average F1-score improvement over DeepConvLSTM, with gains up to 21% on six HAR benchmarks.

Conclusion: DeepConvContext effectively addresses temporal dependency limitations in HAR, demonstrating LSTM superiority over attention for sensor data.

Abstract: Despite recognized limitations in modeling long-range temporal dependencies,
Human Activity Recognition (HAR) has traditionally relied on a sliding window
approach to segment labeled datasets. Deep learning models like the
DeepConvLSTM typically classify each window independently, thereby restricting
learnable temporal context to within-window information. To address this
constraint, we propose DeepConvContext, a multi-scale time series
classification framework for HAR. Drawing inspiration from the vision-based
Temporal Action Localization community, DeepConvContext models both intra- and
inter-window temporal patterns by processing sequences of time-ordered windows.
Unlike recent HAR models that incorporate attention mechanisms, DeepConvContext
relies solely on LSTMs -- with ablation studies demonstrating the superior
performance of LSTMs over attention-based variants for modeling inertial sensor
data. Across six widely-used HAR benchmarks, DeepConvContext achieves an
average 10% improvement in F1-score over the classic DeepConvLSTM, with gains
of up to 21%. Code to reproduce our experiments is publicly available via
github.com/mariusbock/context_har.

</details>


### [577] [Active Learning for Multiple Change Point Detection in Non-stationary Time Series with Deep Gaussian Processes](https://arxiv.org/pdf/2505.20452)
*Hao Zhao, Rong Pan*

Main category: cs.LG

TL;DR: A novel algorithm combining Active Learning and Deep Gaussian Processes for robust multiple change point detection in non-stationary time series.


<details>
  <summary>Details</summary>
Motivation: Addressing the challenge of detecting multiple change points in non-stationary time series due to diverse underlying patterns.

Method: Integrates Active Learning with Deep Gaussian Processes, using spectral analysis to identify changes and AL for efficient sampling.

Result: Outperforms existing techniques in detection accuracy and sampling efficiency for non-stationary time series.

Conclusion: The proposed method effectively adapts to diverse spectral change behaviors and localizes multiple change points.

Abstract: Multiple change point (MCP) detection in non-stationary time series is
challenging due to the variety of underlying patterns. To address these
challenges, we propose a novel algorithm that integrates Active Learning (AL)
with Deep Gaussian Processes (DGPs) for robust MCP detection. Our method
leverages spectral analysis to identify potential changes and employs AL to
strategically select new sampling points for improved efficiency. By
incorporating the modeling flexibility of DGPs with the change-identification
capabilities of spectral methods, our approach adapts to diverse spectral
change behaviors and effectively localizes multiple change points. Experiments
on both simulated and real-world data demonstrate that our method outperforms
existing techniques in terms of detection accuracy and sampling efficiency for
non-stationary time series.

</details>


### [578] [BlastOFormer: Attention and Neural Operator Deep Learning Methods for Explosive Blast Prediction](https://arxiv.org/pdf/2505.20454)
*Reid Graves, Anthony Zhou, Amir Barati Farimani*

Main category: cs.LG

TL;DR: BlastOFormer, a Transformer-based surrogate model, predicts blast pressure fields accurately and efficiently, outperforming traditional methods like CNNs and FNOs.


<details>
  <summary>Details</summary>
Motivation: Traditional methods (empirical models and CFD) for blast pressure prediction are either inaccurate or computationally expensive, limiting their practical use.

Method: BlastOFormer uses SDF encoding and a grid-to-grid attention architecture, trained on data from the blastFoam CFD solver.

Result: BlastOFormer achieves the highest R2 score (0.9516) and fastest inference (6.4 ms), outperforming CNNs and FNOs.

Conclusion: BlastOFormer is a real-time, accurate alternative to CFD for blast pressure prediction in complex environments.

Abstract: Accurate prediction of blast pressure fields is essential for applications in
structural safety, defense planning, and hazard mitigation. Traditional methods
such as empirical models and computational fluid dynamics (CFD) simulations
offer limited trade offs between speed and accuracy; empirical models fail to
capture complex interactions in cluttered environments, while CFD simulations
are computationally expensive and time consuming. In this work, we introduce
BlastOFormer, a novel Transformer based surrogate model for full field maximum
pressure prediction from arbitrary obstacle and charge configurations.
BlastOFormer leverages a signed distance function (SDF) encoding and a grid to
grid attention based architecture inspired by OFormer and Vision Transformer
(ViT) frameworks. Trained on a dataset generated using the open source
blastFoam CFD solver, our model outperforms convolutional neural networks
(CNNs) and Fourier Neural Operators (FNOs) across both log transformed and
unscaled domains. Quantitatively, BlastOFormer achieves the highest R2 score
(0.9516) and lowest error metrics, while requiring only 6.4 milliseconds for
inference, more than 600,000 times faster than CFD simulations. Qualitative
visualizations and error analyses further confirm BlastOFormer's superior
spatial coherence and generalization capabilities. These results highlight its
potential as a real time alternative to conventional CFD approaches for blast
pressure estimation in complex environments.

</details>


### [579] [Avoid Forgetting by Preserving Global Knowledge Gradients in Federated Learning with Non-IID Data](https://arxiv.org/pdf/2505.20485)
*Abhijit Chunduru, Majid Morafah, Mahdi Morafah, Vishnu Pandi Chellapandi, Ang Li*

Main category: cs.LG

TL;DR: FedProj addresses data heterogeneity in federated learning by analyzing decision boundary issues and introducing a framework to prevent forgetting of global boundaries.


<details>
  <summary>Details</summary>
Motivation: Existing methods lack understanding of how data heterogeneity affects the global decision boundary, leading to forgetting issues.

Method: FedProj uses server-side ensemble knowledge transfer and episodic memory of average ensemble logits to regulate local training.

Result: FedProj outperforms state-of-the-art methods significantly.

Conclusion: FedProj effectively mitigates global decision boundary forgetting and improves federated learning performance.

Abstract: The inevitable presence of data heterogeneity has made federated learning
very challenging. There are numerous methods to deal with this issue, such as
local regularization, better model fusion techniques, and data sharing. Though
effective, they lack a deep understanding of how data heterogeneity can affect
the global decision boundary. In this paper, we bridge this gap by performing
an experimental analysis of the learned decision boundary using a toy example.
Our observations are surprising: (1) we find that the existing methods suffer
from forgetting and clients forget the global decision boundary and only learn
the perfect local one, and (2) this happens regardless of the initial weights,
and clients forget the global decision boundary even starting from pre-trained
optimal weights. In this paper, we present FedProj, a federated learning
framework that robustly learns the global decision boundary and avoids its
forgetting during local training. To achieve better ensemble knowledge fusion,
we design a novel server-side ensemble knowledge transfer loss to further
calibrate the learned global decision boundary. To alleviate the issue of
learned global decision boundary forgetting, we further propose leveraging an
episodic memory of average ensemble logits on a public unlabeled dataset to
regulate the gradient updates at each step of local training. Experimental
results demonstrate that FedProj outperforms state-of-the-art methods by a
large margin.

</details>


### [580] [Semi-Explicit Neural DAEs: Learning Long-Horizon Dynamical Systems with Algebraic Constraints](https://arxiv.org/pdf/2505.20515)
*Avik Pal, Alan Edelman, Christopher Rackauckas*

Main category: cs.LG

TL;DR: PNODEs enforce hard constraints in neural ODEs via manifold projection, outperforming baselines with low error and runtime.


<details>
  <summary>Details</summary>
Motivation: Existing SciML methods struggle with scalability and numerical issues when enforcing hard constraints in neural ODEs for physical systems.

Method: Proposes Manifold-Projected Neural ODEs (PNODEs), projecting ODE steps onto constraint manifolds, with iterative and fast approximation variants.

Result: PNODEs achieve mean constraint violation error below $10^{-10}$ and lower runtime across six benchmarks.

Conclusion: Constraint projection via PNODEs is effective for learning physically consistent dynamics.

Abstract: Despite the promise of scientific machine learning (SciML) in combining
data-driven techniques with mechanistic modeling, existing approaches for
incorporating hard constraints in neural differential equations (NDEs) face
significant limitations. Scalability issues and poor numerical properties
prevent these neural models from being used for modeling physical systems with
complicated conservation laws. We propose Manifold-Projected Neural ODEs
(PNODEs), a method that explicitly enforces algebraic constraints by projecting
each ODE step onto the constraint manifold. This framework arises naturally
from semi-explicit differential-algebraic equations (DAEs), and includes both a
robust iterative variant and a fast approximation requiring a single Jacobian
factorization. We further demonstrate that prior works on relaxation methods
are special cases of our approach. PNODEs consistently outperform baselines
across six benchmark problems achieving a mean constraint violation error below
$10^{-10}$. Additionally, PNODEs consistently achieve lower runtime compared to
other methods for a given level of error tolerance. These results show that
constraint projection offers a simple strategy for learning physically
consistent long-horizon dynamics.

</details>


### [581] [Towards Fully FP8 GEMM LLM Training at Scale](https://arxiv.org/pdf/2505.20524)
*Alejandro Hernández-Cano, Dhia Garbaya, Imanol Schlag, Martin Jaggi*

Main category: cs.LG

TL;DR: A new LLM architecture supports FP8 computation for all GEMMs in transformer blocks, enabling throughput gains while matching BF16 training performance.


<details>
  <summary>Details</summary>
Motivation: FP8 formats offer potential for LLM pre-training but face stability challenges at scale, limiting adoption.

Method: Introduces a new LLM architecture design that reduces outlier activations and supports FP8 for all GEMMs in forward and backward passes.

Result: Achieves unprecedented throughput gains without compromising downstream performance compared to BF16 training.

Conclusion: The architecture enables stable FP8 training, with key metrics identified to monitor and predict training divergences.

Abstract: Despite the significant potential of FP8 data formats for large language
model (LLM) pre-training, their adoption has been limited due to challenges in
maintaining stability at scale. Existing approaches often rely on suboptimal
fine-grained FP8 kernels or fall back to higher-precision matrix
multiplications (GEMMs) in sensitive components, such as attention projections,
compromising potential throughput gains. We introduce a new class of LLM
architectures that, for the first time, support FP8 computation for all GEMMs
within transformer blocks during both forward and backward passes. This enables
unprecedented throughput gains, particularly at scale, while matching the
downstream performance of standard BF16 training. Our architecture design
reduces large outlier activations, promoting stable long-term FP8 training. In
addition, we identify key metrics to monitor low-precision training and predict
potential future divergences.

</details>


### [582] [One-shot Robust Federated Learning of Independent Component Analysis](https://arxiv.org/pdf/2505.20532)
*Dian Jin, Xin Bing, Yuqian Zhang*

Main category: cs.LG

TL;DR: A robust one-shot aggregation framework for distributed/federated ICA using geometric median and k-means to handle permutation ambiguity, effective even in highly heterogeneous settings.


<details>
  <summary>Details</summary>
Motivation: Addressing the challenge of permutation ambiguity in distributed/federated ICA, especially in heterogeneous scenarios with limited client samples.

Method: Proposes a geometric median-based aggregation algorithm with k-means clustering to partition and aggregate client estimators.

Result: The method remains effective in highly heterogeneous settings, supported by theoretical error bounds and simulation studies.

Conclusion: The framework provides a robust solution for distributed ICA, validated by theory and experiments.

Abstract: This paper investigates a general robust one-shot aggregation framework for
distributed and federated Independent Component Analysis (ICA) problem. We
propose a geometric median-based aggregation algorithm that leverages $k$-means
clustering to resolve the permutation ambiguity in local client estimations.
Our method first performs k-means to partition client-provided estimators into
clusters and then aggregates estimators within each cluster using the geometric
median. This approach provably remains effective even in highly heterogeneous
scenarios where at most half of the clients can observe only a minimal number
of samples. The key theoretical contribution lies in the combined analysis of
the geometric median's error bound-aided by sample quantiles-and the maximum
misclustering rates of the aforementioned solution of $k$-means. The
effectiveness of the proposed approach is further supported by simulation
studies conducted under various heterogeneous settings.

</details>


### [583] [Topological Deep Learning for Speech Data](https://arxiv.org/pdf/2505.21173)
*Zhiwang Yu*

Main category: cs.LG

TL;DR: The paper introduces topology-aware convolutional kernels for speech recognition, improving performance and adaptability.


<details>
  <summary>Details</summary>
Motivation: To leverage topological data analysis (TDA) for enhancing deep learning, specifically in speech recognition networks.

Method: Designs topology-aware kernels using orthogonal group actions and fiber-bundle decomposition, proposing an Orthogonal Feature (OF) layer.

Result: The OF layer achieves superior phoneme recognition, especially in low-noise scenarios, and shows cross-domain adaptability.

Conclusion: Demonstrates TDA's potential in optimizing neural networks, encouraging interdisciplinary research between mathematics and deep learning.

Abstract: Topological data analysis (TDA) offers novel mathematical tools for deep
learning. Inspired by Carlsson et al., this study designs topology-aware
convolutional kernels that significantly improve speech recognition networks.
Theoretically, by investigating orthogonal group actions on kernels, we
establish a fiber-bundle decomposition of matrix spaces, enabling new filter
generation methods. Practically, our proposed Orthogonal Feature (OF) layer
achieves superior performance in phoneme recognition, particularly in low-noise
scenarios, while demonstrating cross-domain adaptability. This work reveals
TDA's potential in neural network optimization, opening new avenues for
mathematics-deep learning interdisciplinary studies.

</details>


### [584] [Rotary Masked Autoencoders are Versatile Learners](https://arxiv.org/pdf/2505.20535)
*Uros Zivanovic, Serafina Di Gioia, Andre Scaffidi, Martín de los Rios, Gabriella Contardo, Roberto Trotta*

Main category: cs.LG

TL;DR: RoMAE extends MAE with Rotary Positional Embedding for irregular time-series, avoiding specialized architectures and excelling across modalities.


<details>
  <summary>Details</summary>
Motivation: Simplify handling of irregular time-series with Transformers by avoiding specialized architectures and reducing complexity.

Method: Extends Masked Autoencoder (MAE) with Rotary Positional Embedding (RoPE) for continuous positions, enabling representation learning without time-series-specific modifications.

Result: Outperforms specialized time-series architectures on datasets like DESC ELAsTiCC Challenge and matches MAE's performance on other modalities.

Conclusion: RoMAE effectively handles irregular time-series and other modalities without architectural specializations, though learned embeddings may disrupt RoPE's relative position property.

Abstract: Applying Transformers to irregular time-series typically requires
specializations to their baseline architecture, which can result in additional
computational overhead and increased method complexity. We present the Rotary
Masked Autoencoder (RoMAE), which utilizes the popular Rotary Positional
Embedding (RoPE) method for continuous positions. RoMAE is an extension to the
Masked Autoencoder (MAE) that enables representation learning with
multidimensional continuous positional information while avoiding any
time-series-specific architectural specializations. We showcase RoMAE's
performance on a variety of modalities including irregular and multivariate
time-series, images, and audio, demonstrating that RoMAE surpasses specialized
time-series architectures on difficult datasets such as the DESC ELAsTiCC
Challenge while maintaining MAE's usual performance across other modalities. In
addition, we investigate RoMAE's ability to reconstruct the embedded continuous
positions, demonstrating that including learned embeddings in the input
sequence breaks RoPE's relative position property.

</details>


### [585] [A ZeNN architecture to avoid the Gaussian trap](https://arxiv.org/pdf/2505.20553)
*Luís Carvalho, João L. Costa, José Mourão, Gonçalo Oliveira*

Main category: cs.LG

TL;DR: ZeNNs address MLP shortcomings by introducing non-learnable weights, scaling factors, and specific activations, enabling pointwise convergence, feature learning, and better high-frequency performance.


<details>
  <summary>Details</summary>
Motivation: Overcome MLP limitations like non-parametric behavior, lack of pointwise convergence, loss of non-Gaussian attributes, and poor high-frequency learning.

Method: Inspired by harmonic analysis: enumerate perceptrons with non-learnable weights, add scaling factors, and use near-orthogonal activations.

Result: ZeNNs achieve pointwise convergence, retain non-Gaussianity, perform feature learning, and excel in high-frequency tasks.

Conclusion: ZeNNs effectively fix MLP issues, offering a robust alternative for neural network architectures.

Abstract: We propose a new simple architecture, Zeta Neural Networks (ZeNNs), in order
to overcome several shortcomings of standard multi-layer perceptrons (MLPs).
Namely, in the large width limit, MLPs are non-parametric, they do not have a
well-defined pointwise limit, they lose non-Gaussian attributes and become
unable to perform feature learning; moreover, finite width MLPs perform poorly
in learning high frequencies. The new ZeNN architecture is inspired by three
simple principles from harmonic analysis:
  i) Enumerate the perceptons and introduce a non-learnable weight to enforce
convergence;
  ii) Introduce a scaling (or frequency) factor;
  iii) Choose activation functions that lead to near orthogonal systems.
  We will show that these ideas allow us to fix the referred shortcomings of
MLPs. In fact, in the infinite width limit, ZeNNs converge pointwise, they
exhibit a rich asymptotic structure beyond Gaussianity, and perform feature
learning. Moreover, when appropriate activation functions are chosen, (finite
width) ZeNNs excel at learning high-frequency features of functions with low
dimensional domains.

</details>


### [586] [Learning a Pessimistic Reward Model in RLHF](https://arxiv.org/pdf/2505.20556)
*Yinglun Xu, Hangoo Kang, Tarun Suresh, Yuxuan Wan, Gagandeep Singh*

Main category: cs.LG

TL;DR: PET is a pessimistic reward fine-tuning method for offline RLHF, preventing reward hacking without KL regularization, outperforming traditional methods.


<details>
  <summary>Details</summary>
Motivation: Traditional reward models in RLHF suffer from reward hacking despite KL regularization, prompting the need for a robust alternative.

Method: Proposes PET, a pessimistic reward fine-tuning method, tested on the TL;DR summarization dataset.

Result: PET enables learning high-quality policies without regularization, even with high KL divergence from dataset distribution.

Conclusion: PET demonstrates feasibility of pessimistic reward models to prevent reward hacking, allowing greedy policy optimization.

Abstract: This work proposes `PET', a novel pessimistic reward fine-tuning method, to
learn a pessimistic reward model robust against reward hacking in offline
reinforcement learning from human feedback (RLHF). Traditional reward modeling
techniques in RLHF train an imperfect reward model, on which a KL
regularization plays a pivotal role in mitigating reward hacking when
optimizing a policy. Such an intuition-based method still suffers from reward
hacking, and the policies with large KL divergence from the dataset
distribution are excluded during learning. In contrast, we show that when
optimizing a policy on a pessimistic reward model fine-tuned through PET,
reward hacking can be prevented without relying on any regularization. We test
our methods on the standard TL;DR summarization dataset. We find that one can
learn a high-quality policy on our pessimistic reward without using any
regularization. Such a policy has a high KL divergence from the dataset
distribution while having high performance in practice. In summary, our work
shows the feasibility of learning a pessimistic reward model against reward
hacking. The agent can greedily search for the policy with a high pessimistic
reward without suffering from reward hacking.

</details>


### [587] [Beyond Markovian: Reflective Exploration via Bayes-Adaptive RL for LLM Reasoning](https://arxiv.org/pdf/2505.20561)
*Shenao Zhang, Yaqing Wang, Yinxiao Liu, Tianqi Liu, Peter Grabowski, Eugene Ie, Zhaoran Wang, Yunxuan Li*

Main category: cs.LG

TL;DR: The paper introduces BARL, a Bayesian-Adaptive RL framework for LLMs, enhancing reflective exploration and outperforming standard Markovian RL in efficiency and effectiveness.


<details>
  <summary>Details</summary>
Motivation: To address the limitations of Markovian RL in fostering reflective reasoning in LLMs during training and test time.

Method: Recasts reflective exploration within the Bayes-Adaptive RL framework, optimizing expected return under a posterior distribution over Markov decision processes.

Result: BARL outperforms standard Markovian RL, achieving better token efficiency and exploration effectiveness in synthetic and mathematical reasoning tasks.

Conclusion: BARL provides principled guidance for reflective exploration in LLMs, demonstrating superior performance over conventional RL methods.

Abstract: Large Language Models (LLMs) trained via Reinforcement Learning (RL) have
exhibited strong reasoning capabilities and emergent reflective behaviors, such
as backtracking and error correction. However, conventional Markovian RL
confines exploration to the training phase to learn an optimal deterministic
policy and depends on the history contexts only through the current state.
Therefore, it remains unclear whether reflective reasoning will emerge during
Markovian RL training, or why they are beneficial at test time. To remedy this,
we recast reflective exploration within the Bayes-Adaptive RL framework, which
explicitly optimizes the expected return under a posterior distribution over
Markov decision processes. This Bayesian formulation inherently incentivizes
both reward-maximizing exploitation and information-gathering exploration via
belief updates. Our resulting algorithm, BARL, instructs the LLM to stitch and
switch strategies based on the observed outcomes, offering principled guidance
on when and how the model should reflectively explore. Empirical results on
both synthetic and mathematical reasoning tasks demonstrate that BARL
outperforms standard Markovian RL approaches at test time, achieving superior
token efficiency with improved exploration effectiveness. Our code is available
at https://github.com/shenao-zhang/BARL.

</details>


### [588] [Bi-Level Unsupervised Feature Selection](https://arxiv.org/pdf/2505.20563)
*Jingjing Liu, Xiansen Ju, Xianchao Xiu, Wanquan Liu*

Main category: cs.LG

TL;DR: Proposes BLUFS, a bi-level UFS method combining clustering and feature selection with ℓ₂,₀-norm, outperforming others in experiments.


<details>
  <summary>Details</summary>
Motivation: Existing UFS methods often fail to evaluate feature importance and preserve data structure simultaneously, limiting performance.

Method: BLUFS uses spectral clustering for pseudo-labels and a linear regression model with ℓ₂,₀-norm for feature selection, solved via PAM algorithm.

Result: BLUFS shows superior performance in clustering and classification tasks on synthetic and real datasets.

Conclusion: BLUFS effectively combines bi-level framework and ℓ₂,₀-norm, offering a robust solution for UFS.

Abstract: Unsupervised feature selection (UFS) is an important task in data
engineering. However, most UFS methods construct models from a single
perspective and often fail to simultaneously evaluate feature importance and
preserve their inherent data structure, thus limiting their performance. To
address this challenge, we propose a novel bi-level unsupervised feature
selection (BLUFS) method, including a clustering level and a feature level.
Specifically, at the clustering level, spectral clustering is used to generate
pseudo-labels for representing the data structure, while a continuous linear
regression model is developed to learn the projection matrix. At the feature
level, the $\ell_{2,0}$-norm constraint is imposed on the projection matrix for
more effectively selecting features. To the best of our knowledge, this is the
first work to combine a bi-level framework with the $\ell_{2,0}$-norm. To solve
the proposed bi-level model, we design an efficient proximal alternating
minimization (PAM) algorithm, whose subproblems either have explicit solutions
or can be computed by fast solvers. Furthermore, we establish the convergence
result and computational complexity. Finally, extensive experiments on two
synthetic datasets and eight real datasets demonstrate the superiority of BLUFS
in clustering and classification tasks.

</details>


### [589] [Ctrl-DNA: Controllable Cell-Type-Specific Regulatory DNA Design via Constrained RL](https://arxiv.org/pdf/2505.20578)
*Xingyu Chen, Shihao Ma, Runsheng Lin, Jiecong Lin, Bo Wang*

Main category: cs.LG

TL;DR: Ctrl-DNA is a constrained RL framework for designing cell-type-specific regulatory DNA sequences, outperforming existing methods in specificity and biological plausibility.


<details>
  <summary>Details</summary>
Motivation: Precise regulatory DNA design is vital for synthetic biology and gene therapy, but current transformer-based LMs lack reliability in generating novel sequences with cell-specific activity.

Method: Ctrl-DNA uses constrained RL to optimize autoregressive genomic LMs, refining sequences for targeted cell activity while minimizing off-target effects.

Result: Ctrl-DNA outperforms other methods, generating high-fitness sequences with state-of-the-art specificity and capturing key TFBS motifs.

Conclusion: Ctrl-DNA advances regulatory DNA design by combining RL with biological constraints, ensuring precise and plausible sequences.

Abstract: Designing regulatory DNA sequences that achieve precise cell-type-specific
gene expression is crucial for advancements in synthetic biology, gene therapy
and precision medicine. Although transformer-based language models (LMs) can
effectively capture patterns in regulatory DNA, their generative approaches
often struggle to produce novel sequences with reliable cell-specific activity.
Here, we introduce Ctrl-DNA, a novel constrained reinforcement learning (RL)
framework tailored for designing regulatory DNA sequences with controllable
cell-type specificity. By formulating regulatory sequence design as a
biologically informed constrained optimization problem, we apply RL to
autoregressive genomic LMs, enabling the models to iteratively refine sequences
that maximize regulatory activity in targeted cell types while constraining
off-target effects. Our evaluation on human promoters and enhancers
demonstrates that Ctrl-DNA consistently outperforms existing generative and
RL-based approaches, generating high-fitness regulatory sequences and achieving
state-of-the-art cell-type specificity. Moreover, Ctrl-DNA-generated sequences
capture key cell-type-specific transcription factor binding sites (TFBS), short
DNA motifs recognized by regulatory proteins that control gene expression,
demonstrating the biological plausibility of the generated sequences.

</details>


### [590] [Prot2Token: A Unified Framework for Protein Modeling via Next-Token Prediction](https://arxiv.org/pdf/2505.20589)
*Mahdi Pourmirzaei, Farzaneh Esmaili, Salhuldin Alqarghuli, Mohammadreza Pourmirzaei, Ye Han, Kai Chen, Mohsen Rezaei, Duolin Wang, Dong Xu*

Main category: cs.LG

TL;DR: Prot2Token is a unified framework for diverse protein prediction tasks, converting them into a standardized next-token prediction format, achieving efficiency and performance comparable to specialized models.


<details>
  <summary>Details</summary>
Motivation: Address the inefficiency and lack of broad applicability in specialized protein prediction models by creating a unified framework.

Method: Uses an autoregressive decoder conditioned on pre-trained protein embeddings and learnable task tokens for multi-task learning.

Result: Demonstrates strong predictive power, significant speedups (e.g., 1000x over AlphaFold2), and performance matching or exceeding specialized models.

Conclusion: Prot2Token advances protein modeling with versatility and efficiency, aiding biological discovery and therapeutic development.

Abstract: The diverse nature of protein prediction tasks has traditionally necessitated
specialized models, hindering the development of broadly applicable and
computationally efficient Protein Language Models (PLMs). In this work, we
introduce Prot2Token, a unified framework that overcomes these challenges by
converting a wide spectrum of protein-related predictions, from sequence-level
properties and residue-specific attributes to complex inter-protein
interactions, into a standardized next-token prediction format. At its core,
Prot2Token employs an autoregressive decoder, conditioned on embeddings from
pre-trained protein encoders and guided by learnable task tokens, to perform
diverse predictions. This architecture uniquely facilitates multi-task
learning, enabling a single model to master numerous tasks with improved
efficiency. We present extensive experimental validation across a variety of
benchmarks, demonstrating Prot2Tokens strong predictive power in different
types of protein-prediction tasks. Key results include significant speedups
(e.g., near 1000x over AlphaFold2 with MSA) and performance often matching or
exceeding specialized approaches. Beyond that, we introduce an auxiliary
self-supervised decoder pre-training approach to improve spatially sensitive
task performance. Prot2Token thus offers a significant step towards a
versatile, high-throughput paradigm for protein modeling, promising to
accelerate biological discovery and the development of novel therapeutics. The
code is available at https://github.com/mahdip72/prot2token .

</details>


### [591] [Multi-level Certified Defense Against Poisoning Attacks in Offline Reinforcement Learning](https://arxiv.org/pdf/2505.20621)
*Shijie Liu, Andrew C. Cullen, Paul Montague, Sarah Erfani, Benjamin I. P. Rubinstein*

Main category: cs.LG

TL;DR: The paper extends certified defenses to protect offline RL from poisoning attacks, ensuring robustness in actions and rewards, using Differential Privacy for broader applicability.


<details>
  <summary>Details</summary>
Motivation: Offline RL's reliance on external datasets makes it vulnerable to poisoning attacks, necessitating stronger defenses.

Method: Leverages Differential Privacy to provide certified guarantees across continuous/discrete spaces and stochastic/deterministic environments.

Result: Ensures performance drops to ≤50% with ≤7% poisoned data, improving over prior work (0.008%) and achieving 5x larger certified radii.

Conclusion: The framework enhances safety and reliability in offline RL by significantly improving robustness against poisoning.

Abstract: Similar to other machine learning frameworks, Offline Reinforcement Learning
(RL) is shown to be vulnerable to poisoning attacks, due to its reliance on
externally sourced datasets, a vulnerability that is exacerbated by its
sequential nature. To mitigate the risks posed by RL poisoning, we extend
certified defenses to provide larger guarantees against adversarial
manipulation, ensuring robustness for both per-state actions, and the overall
expected cumulative reward. Our approach leverages properties of Differential
Privacy, in a manner that allows this work to span both continuous and discrete
spaces, as well as stochastic and deterministic environments -- significantly
expanding the scope and applicability of achievable guarantees. Empirical
evaluations demonstrate that our approach ensures the performance drops to no
more than $50\%$ with up to $7\%$ of the training data poisoned, significantly
improving over the $0.008\%$ in prior work~\citep{wu_copa_2022}, while
producing certified radii that is $5$ times larger as well. This highlights the
potential of our framework to enhance safety and reliability in offline RL.

</details>


### [592] [Position: Adopt Constraints Over Penalties in Deep Learning](https://arxiv.org/pdf/2505.20628)
*Juan Ramirez, Meraj Hashemizadeh, Simon Lacoste-Julien*

Main category: cs.LG

TL;DR: The paper critiques fixed-weight penalization in AI systems for enforcing constraints, advocating tailored constrained optimization methods like the Lagrangian approach for better accountability and efficiency.


<details>
  <summary>Details</summary>
Motivation: To address the shortcomings of fixed-weight penalization in ensuring AI accountability and performance, proposing more effective constrained optimization methods.

Method: Advocates for tailored constrained optimization methods, specifically the Lagrangian approach, which optimizes penalization coefficients alongside the model.

Result: Tailored methods like the Lagrangian approach solve constrained problems effectively, eliminate the need for extensive penalty tuning, and integrate well with deep learning pipelines.

Conclusion: Tailored constrained optimization methods are superior to fixed-weight penalization for achieving trustworthy AI with accountability guarantees.

Abstract: Recent efforts toward developing trustworthy AI systems with accountability
guarantees have led to a growing reliance on machine learning formulations that
incorporate external requirements, or constraints. These requirements are often
enforced through penalization--adding fixed-weight terms to the task loss. We
argue that this approach is ill-suited, and that tailored constrained
optimization methods should be adopted instead. In particular, no penalty
coefficient may yield a solution that both satisfies the constraints and
achieves good performance--i.e., one solving the constrained problem. Moreover,
tuning these coefficients is costly, incurring significant time and
computational overhead. In contrast, tailored constrained methods--such as the
Lagrangian approach, which optimizes the penalization "coefficients" (the
Lagrange multipliers) alongside the model--(i) truly solve the constrained
problem and add accountability, (ii) eliminate the need for extensive penalty
tuning, and (iii) integrate seamlessly with modern deep learning pipelines.

</details>


### [593] [Explaining Concept Shift with Interpretable Feature Attribution](https://arxiv.org/pdf/2505.20634)
*Ruiqi Lyu, Alistair Turcan, Bryan Wilder*

Main category: cs.LG

TL;DR: SGShift detects and attributes concept shift in tabular data using GAMs and feature selection, outperforming baselines with high AUC and recall.


<details>
  <summary>Details</summary>
Motivation: Concept shift degrades ML model performance; identifying shifted features provides insights into dataset differences.

Method: SGShift uses Generalized Additive Models (GAMs) for concept shift modeling, feature selection, knockoffs for false discovery control, and an absorption term for poor model fit.

Result: SGShift achieves AUC >0.9 and recall >90%, outperforming baselines by 2-3x.

Conclusion: SGShift effectively identifies and attributes concept shift, offering a robust tool for ML model analysis.

Abstract: Regardless the amount of data a machine learning (ML) model is trained on,
there will inevitably be data that differs from their training set, lowering
model performance. Concept shift occurs when the distribution of labels
conditioned on the features changes, making even a well-tuned ML model to have
learned a fundamentally incorrect representation. Identifying these shifted
features provides unique insight into how one dataset differs from another,
considering the difference may be across a scientifically relevant dimension,
such as time, disease status, population, etc. In this paper, we propose
SGShift, a model for detecting concept shift in tabular data and attributing
reduced model performance to a sparse set of shifted features. SGShift models
concept shift with a Generalized Additive Model (GAM) and performs subsequent
feature selection to identify shifted features. We propose further extensions
of SGShift by incorporating knockoffs to control false discoveries and an
absorption term to account for models with poor fit to the data. We conduct
extensive experiments in synthetic and real data across various ML models and
find SGShift can identify shifted features with AUC $>0.9$ and recall $>90\%$,
often 2 or 3 times as high as baseline methods.

</details>


### [594] [Can Past Experience Accelerate LLM Reasoning?](https://arxiv.org/pdf/2505.20643)
*Bo Pan, Liang Zhao*

Main category: cs.LG

TL;DR: LLMs can reason faster with recurrent exposure, reducing compute costs by up to 56% using adaptive methods.


<details>
  <summary>Details</summary>
Motivation: To explore if LLMs, like humans, can improve reasoning speed with experience and how to achieve it.

Method: Proposes SpeedupLLM, a framework with adaptive compute allocation and memory mechanisms.

Result: LLMs achieve up to 56% compute cost reduction with past experience.

Conclusion: Recurrent exposure and adaptive methods enable LLMs to reason faster and more efficiently.

Abstract: Allocating more compute to large language models (LLMs) reasoning has
generally been demonstrated to improve their effectiveness, but also results in
increased inference time. In contrast, humans can perform tasks faster and
better with increased experience and exposure. Hence, this paper aims to
investigate the question: Can LLMs also become faster at reasoning through
recurrent exposure on relevant tasks, and if so, how can it be achieved? To
address these questions, we first formalize the problem setting of LLM
reasoning speedup systematically in the dimensions of task relevancy and
compute budget calculation. We then propose SpeedupLLM, a theoretically
guaranteed framework to implement and benchmark such reasoning speedup
behaviour based on adaptive compute allocation and memory mechanisms. We
further conduct comprehensive experiments to benchmark such behaviour across
different question similarity levels, memory methods, and reasoning methods.
Results show that LLMs can generally reason faster with past experience,
achieving up to a 56% reduction in compute cost when equipped with appropriate
memory and reasoning methods.

</details>


### [595] [Evaluating Training in Binarized Neural Networks Through the Lens of Algorithmic Information Theory](https://arxiv.org/pdf/2505.20646)
*Eduardo Y. Sakabe, Felipe S. Abrahão, Alexandre Simões, Esther Colombini, Paula Costa, Ricardo Gudwin, Hector Zenil*

Main category: cs.LG

TL;DR: The paper proposes using algorithmic information theory, specifically Binarized Neural Networks (BNNs) and the Block Decomposition Method (BDM), to better understand and control neural network complexity, showing it outperforms entropy-based measures.


<details>
  <summary>Details</summary>
Motivation: Current entropy-based methods fail to capture deeper algorithmic regularities in neural networks, limiting understanding of learning dynamics.

Method: Uses BNNs and BDM, an approximation of algorithmic complexity, to analyze structural changes during training.

Result: BDM tracks training dynamics better than entropy, showing stronger correlations with training loss across model sizes and runs.

Conclusion: Training is framed as algorithmic compression, offering a principled way to estimate learning progression and guide complexity-aware regularization.

Abstract: Understanding and controlling the informational complexity of neural networks
is a central challenge in machine learning, with implications for
generalization, optimization, and model capacity. While most approaches rely on
entropy-based loss functions and statistical metrics, these measures often fail
to capture deeper, causally relevant algorithmic regularities embedded in
network structure. We propose a shift toward algorithmic information theory,
using Binarized Neural Networks (BNNs) as a first proxy. Grounded in
algorithmic probability (AP) and the universal distribution it defines, our
approach characterizes learning dynamics through a formal, causally grounded
lens. We apply the Block Decomposition Method (BDM) -- a scalable approximation
of algorithmic complexity based on AP -- and demonstrate that it more closely
tracks structural changes during training than entropy, consistently exhibiting
stronger correlations with training loss across varying model sizes and
randomized training runs. These results support the view of training as a
process of algorithmic compression, where learning corresponds to the
progressive internalization of structured regularities. In doing so, our work
offers a principled estimate of learning progression and suggests a framework
for complexity-aware learning and regularization, grounded in first principles
from information theory, complexity, and computability.

</details>


### [596] [Voronoi-grid-based Pareto Front Learning and Its Application to Collaborative Federated Learning](https://arxiv.org/pdf/2505.20648)
*Mengmeng Chen, Xiaohu Wu, Qiqi Liu, Tiantian He, Yew-Soon Ong, Yaochu Jin, Qicheng Lao, Han Yu*

Main category: cs.LG

TL;DR: PHN-HVVS is a novel PFL framework using Voronoi grids and GA to improve Pareto front coverage in high-dimensional spaces, outperforming baselines in MOO tasks.


<details>
  <summary>Details</summary>
Motivation: Existing PFL methods struggle with high-dimensional ray sampling and incomplete Pareto front coverage, especially for convex shapes.

Method: PHN-HVVS decomposes the design space into Voronoi grids, uses GA for partitioning, and introduces a new loss function for better coverage and HV Indicator maximization.

Result: PHN-HVVS significantly outperforms baselines in generating Pareto fronts and advances FL methodologies.

Conclusion: PHN-HVVS effectively addresses PFL challenges, offering superior performance and broader applicability in MOO tasks.

Abstract: Multi-objective optimization (MOO) exists extensively in machine learning,
and aims to find a set of Pareto-optimal solutions, called the Pareto front,
e.g., it is fundamental for multiple avenues of research in federated learning
(FL). Pareto-Front Learning (PFL) is a powerful method implemented using
Hypernetworks (PHNs) to approximate the Pareto front. This method enables the
acquisition of a mapping function from a given preference vector to the
solutions on the Pareto front. However, most existing PFL approaches still face
two challenges: (a) sampling rays in high-dimensional spaces; (b) failing to
cover the entire Pareto Front which has a convex shape. Here, we introduce a
novel PFL framework, called as PHN-HVVS, which decomposes the design space into
Voronoi grids and deploys a genetic algorithm (GA) for Voronoi grid
partitioning within high-dimensional space. We put forward a new loss function,
which effectively contributes to more extensive coverage of the resultant
Pareto front and maximizes the HV Indicator. Experimental results on multiple
MOO machine learning tasks demonstrate that PHN-HVVS outperforms the baselines
significantly in generating Pareto front. Also, we illustrate that PHN-HVVS
advances the methodologies of several recent problems in the FL field. The code
is available at
https://github.com/buptcmm/phnhvvs}{https://github.com/buptcmm/phnhvvs.

</details>


### [597] [An Optimisation Framework for Unsupervised Environment Design](https://arxiv.org/pdf/2505.20659)
*Nathan Monette, Alistair Letcher, Michael Beukman, Matthew T. Jackson, Alexander Rutherford, Alexander D. Goldie, Jakob N. Foerster*

Main category: cs.LG

TL;DR: The paper proposes a theoretical framework for Unsupervised Environment Design (UED) to enhance reinforcement learning robustness, offering stronger guarantees and a provably convergent algorithm.


<details>
  <summary>Details</summary>
Motivation: To improve robustness of reinforcement learning agents in unfamiliar scenarios, addressing limitations of prior UED methods.

Method: Introduces a nonconvex-strongly-concave objective and a provably convergent algorithm in zero-sum settings.

Result: Empirically outperforms prior methods across environments of varying difficulty.

Conclusion: The framework provides stronger theoretical guarantees and practical efficacy, advancing UED for robust reinforcement learning.

Abstract: For reinforcement learning agents to be deployed in high-risk settings, they
must achieve a high level of robustness to unfamiliar scenarios. One method for
improving robustness is unsupervised environment design (UED), a suite of
methods aiming to maximise an agent's generalisability across configurations of
an environment. In this work, we study UED from an optimisation perspective,
providing stronger theoretical guarantees for practical settings than prior
work. Whereas previous methods relied on guarantees if they reach convergence,
our framework employs a nonconvex-strongly-concave objective for which we
provide a provably convergent algorithm in the zero-sum setting. We empirically
verify the efficacy of our method, outperforming prior methods in a number of
environments with varying difficulties.

</details>


### [598] [Continuous-Time Attention: PDE-Guided Mechanisms for Long-Sequence Transformers](https://arxiv.org/pdf/2505.20666)
*Yukun Zhang, Xueqing Zhou*

Main category: cs.LG

TL;DR: A novel framework, Continuous_Time Attention, integrates PDEs into Transformer attention to handle long sequences, improving noise reduction, long-range dependencies, and gradient stability.


<details>
  <summary>Details</summary>
Motivation: Addressing challenges of extremely long input sequences in Transformers by evolving attention weights dynamically over time.

Method: Infuses PDEs (diffusion, wave, or reaction-diffusion) into attention, allowing weights to evolve over pseudo-time.

Result: Theoretically better optimization landscapes and polynomial decay of distant interactions; empirically outperforms standard and specialized Transformer variants.

Conclusion: PDE-based attention enriches mechanisms with continuous-time dynamics and global coherence, showing promise for long sequences.

Abstract: We propose a novel framework, Continuous_Time Attention, which infuses
partial differential equations (PDEs) into the Transformer's attention
mechanism to address the challenges of extremely long input sequences. Instead
of relying solely on a static attention matrix, we allow attention weights to
evolve over a pseudo_time dimension via diffusion, wave, or reaction_diffusion
dynamics. This mechanism systematically smooths local noise, enhances
long_range dependencies, and stabilizes gradient flow. Theoretically, our
analysis shows that PDE_based attention leads to better optimization landscapes
and polynomial rather than exponential decay of distant interactions.
Empirically, we benchmark our method on diverse experiments_demonstrating
consistent gains over both standard and specialized long sequence Transformer
variants. Our findings highlight the potential of PDE_based formulations to
enrich attention mechanisms with continuous_time dynamics and global coherence.

</details>


### [599] [Accelerating RL for LLM Reasoning with Optimal Advantage Regression](https://arxiv.org/pdf/2505.20686)
*Kianté Brantley, Mingyu Chen, Zhaolin Gao, Jason D. Lee, Wen Sun, Wenhao Zhan, Xuezhou Zhang*

Main category: cs.LG

TL;DR: A*-PO is a two-stage policy optimization framework for efficient RL training of LLMs, reducing computational overhead and memory usage while maintaining competitive performance.


<details>
  <summary>Details</summary>
Motivation: Address high computational and memory costs in current RL methods for fine-tuning LLMs.

Method: Two-stage approach: offline sampling for optimal value function estimation and on-policy updates with single-generation per prompt.

Result: Achieves competitive performance, reduces training time by 2x, and cuts peak memory usage by 30%.

Conclusion: A*-PO offers an efficient alternative to existing methods like PPO, GRPO, and REBEL for RL in LLMs.

Abstract: Reinforcement learning (RL) has emerged as a powerful tool for fine-tuning
large language models (LLMs) to improve complex reasoning abilities. However,
state-of-the-art policy optimization methods often suffer from high
computational overhead and memory consumption, primarily due to the need for
multiple generations per prompt and the reliance on critic networks or
advantage estimates of the current policy. In this paper, we propose $A$*-PO, a
novel two-stage policy optimization framework that directly approximates the
optimal advantage function and enables efficient training of LLMs for reasoning
tasks. In the first stage, we leverage offline sampling from a reference policy
to estimate the optimal value function $V$*, eliminating the need for costly
online value estimation. In the second stage, we perform on-policy updates
using a simple least-squares regression loss with only a single generation per
prompt. Theoretically, we establish performance guarantees and prove that the
KL-regularized RL objective can be optimized without requiring complex
exploration strategies. Empirically, $A$*-PO achieves competitive performance
across a wide range of mathematical reasoning benchmarks, while reducing
training time by up to 2$\times$ and peak memory usage by over 30% compared to
PPO, GRPO, and REBEL. Implementation of $A$*-PO can be found at
https://github.com/ZhaolinGao/A-PO.

</details>


### [600] [Evidential Deep Active Learning for Semi-Supervised Classification](https://arxiv.org/pdf/2505.20691)
*Shenkai Zhao, Xinao Zhang, Lipeng Pan, Xiaobin Xu, Danilo Pelusi*

Main category: cs.LG

TL;DR: EDALSSC introduces an evidential deep active learning method for semi-supervised classification, focusing on uncertainty estimation for both labeled and unlabeled data to improve model updates.


<details>
  <summary>Details</summary>
Motivation: Existing methods lack uncertainty estimation in predictions, questioning the effectiveness of selected samples for model updates.

Method: EDALSSC combines evidential deep learning for labeled data and T-conorm operator for unlabeled data, dynamically balancing evidence and class influence.

Result: EDALSSC outperforms existing semi-supervised and supervised active learning methods on image classification datasets.

Conclusion: The proposed approach effectively addresses uncertainty estimation, enhancing semi-supervised classification performance.

Abstract: Semi-supervised classification based on active learning has made significant
progress, but the existing methods often ignore the uncertainty estimation (or
reliability) of the prediction results during the learning process, which makes
it questionable whether the selected samples can effectively update the model.
Hence, this paper proposes an evidential deep active learning approach for
semi-supervised classification (EDALSSC). EDALSSC builds a semi-supervised
learning framework to simultaneously quantify the uncertainty estimation of
labeled and unlabeled data during the learning process. The uncertainty
estimation of the former is associated with evidential deep learning, while
that of the latter is modeled by combining ignorance information and conflict
information of the evidence from the perspective of the T-conorm operator.
Furthermore, this article constructs a heuristic method to dynamically balance
the influence of evidence and the number of classes on uncertainty estimation
to ensure that it does not produce counter-intuitive results in EDALSSC. For
the sample selection strategy, EDALSSC selects the sample with the greatest
uncertainty estimation that is calculated in the form of a sum when the
training loss increases in the latter half of the learning process.
Experimental results demonstrate that EDALSSC outperforms existing
semi-supervised and supervised active learning approaches on image
classification datasets.

</details>


### [601] [Generating Hypotheses of Dynamic Causal Graphs in Neuroscience: Leveraging Generative Factor Models of Observed Time Series](https://arxiv.org/pdf/2505.20697)
*Zachary C. Brown, David Carlson*

Main category: cs.LG

TL;DR: A novel method for dynamic causal discovery in neuroscience improves hypothesis generation by modeling time-varying interactions beyond linear limitations, outperforming baselines by 22-28% (up to 60%).


<details>
  <summary>Details</summary>
Motivation: Existing methods assume static causal relationships or impose linear/simplifying assumptions, limiting their applicability to dynamic systems like the brain.

Method: Proposes modeling dynamic graphs as a conditionally weighted superposition of static graphs, capturing nonlinear relationships.

Result: Improves F1-scores by 22-28% on average (up to 60%) in predicting dynamic causal patterns and successfully uncovers neural dynamics in real brain data.

Conclusion: The method enhances hypothesis generation in neuroscience by detecting complex, time-varying interactions, offering deeper insights into neural behavior.

Abstract: The field of hypothesis generation promises to reduce costs in neuroscience
by narrowing the range of interventional studies needed to study various
phenomena. Existing machine learning methods can generate scientific hypotheses
from complex datasets, but many approaches assume causal relationships are
static over time, limiting their applicability to systems with dynamic,
state-dependent behavior, such as the brain. While some techniques attempt
dynamic causal discovery through factor models, they often restrict
relationships to linear patterns or impose other simplifying assumptions. We
propose a novel method that models dynamic graphs as a conditionally weighted
superposition of static graphs, where each static graph can capture nonlinear
relationships. This approach enables the detection of complex, time-varying
interactions between variables beyond linear limitations. Our method improves
f1-scores of predicted dynamic causal patterns by roughly 22-28% on average
over baselines in some of our experiments, with some improvements reaching well
over 60%. A case study on real brain data demonstrates our method's ability to
uncover relationships linked to specific behavioral states, offering valuable
insights into neural dynamics.

</details>


### [602] [Sparsified State-Space Models are Efficient Highway Networks](https://arxiv.org/pdf/2505.20698)
*Woomin Song, Jihoon Tack, Sangwoo Mo, Seunghyuk Oh, Jinwoo Shin*

Main category: cs.LG

TL;DR: Simba enhances SSMs by sparsifying them hierarchically, improving efficiency and information flow.


<details>
  <summary>Details</summary>
Motivation: Tokens in SSMs are redundant due to gradual updates, and dense recurrences hinder information delivery. Upper layers are more redundant as they encode global info.

Method: Simba uses hierarchical token pruning, sparsifying upper layers more to act like highways. A novel pruning criterion measures token impact.

Result: Simba outperforms Mamba with the same FLOPS in NLP tasks and improves long-sequence information flow.

Conclusion: Simba is a simple, effective method to enhance SSMs, balancing efficiency and performance.

Abstract: State-space models (SSMs) offer a promising architecture for sequence
modeling, providing an alternative to Transformers by replacing expensive
self-attention with linear recurrences. In this paper, we propose a simple yet
effective trick to enhance SSMs within given computational budgets by
sparsifying them. Our intuition is that tokens in SSMs are highly redundant due
to gradual recurrent updates, and dense recurrence operations block the
delivery of past information. In particular, we observe that upper layers of
SSMs tend to be more redundant as they encode global information, while lower
layers encode local information. Motivated by this, we introduce Simba, a
hierarchical sparsification method for SSMs based on token pruning. Simba
sparsifies upper layers more than lower layers, encouraging the upper layers to
behave like highways. To achieve this, we propose a novel token pruning
criterion for SSMs, measuring the global impact of tokens on the final output
by accumulating local recurrences. We demonstrate that Simba outperforms the
baseline model, Mamba, with the same FLOPS in various natural language tasks.
Moreover, we illustrate the effect of highways, showing that Simba not only
enhances efficiency but also improves the information flow across long
sequences. Code is available at https://github.com/woominsong/Simba.

</details>


### [603] [Are Data Embeddings effective in time series forecasting?](https://arxiv.org/pdf/2505.20716)
*Reza Nematirad, Anil Pahwa, Balasubramaniam Natarajan*

Main category: cs.LG

TL;DR: Removing data embedding layers from state-of-the-art time series forecasting models often improves accuracy and efficiency, challenging their necessity.


<details>
  <summary>Details</summary>
Motivation: To investigate the actual effectiveness of data embedding techniques in time series forecasting, given their widespread use despite marginal reported improvements.

Method: Conducted ablation studies on fifteen state-of-the-art models and four benchmark datasets by removing data embedding layers.

Result: Removing embedding layers did not degrade performance; in many cases, it improved accuracy and computational efficiency.

Conclusion: Data embedding layers may be unnecessary in many time series forecasting models, as their removal often yields better results.

Abstract: Time series forecasting plays a crucial role in many real-world applications,
and numerous complex forecasting models have been proposed in recent years.
Despite their architectural innovations, most state-of-the-art models report
only marginal improvements -- typically just a few thousandths in standard
error metrics. These models often incorporate complex data embedding layers to
transform raw inputs into higher-dimensional representations to enhance
accuracy. But are data embedding techniques actually effective in time series
forecasting? Through extensive ablation studies across fifteen state-of-the-art
models and four benchmark datasets, we find that removing data embedding layers
from many state-of-the-art models does not degrade forecasting performance. In
many cases, it improves both accuracy and computational efficiency. The gains
from removing embedding layers often exceed the performance differences
typically reported between competing models. Code available at:
https://github.com/neuripsdataembedidng/DataEmbedding

</details>


### [604] [Recurrent Neural Operators: Stable Long-Term PDE Prediction](https://arxiv.org/pdf/2505.20721)
*Zaijun Ye, Chen-Song Zhang, Wansheng Wang*

Main category: cs.LG

TL;DR: Recurrent Neural Operators (RNOs) improve long-term autoregressive predictions in neural operators by aligning training with inference dynamics, reducing error growth from exponential to linear.


<details>
  <summary>Details</summary>
Motivation: Standard training strategies like teacher forcing cause a mismatch between training and inference in time-dependent problems, leading to compounding errors.

Method: RNOs integrate recurrent training, recursively applying the operator to their own predictions over a temporal window to simulate inference-time dynamics.

Result: RNOs outperform teacher-forced models in long-term accuracy and stability, reducing error growth from exponential to linear.

Conclusion: Aligning training with inference dynamics is crucial for robust temporal generalization in neural operator learning.

Abstract: Neural operators have emerged as powerful tools for learning solution
operators of partial differential equations. However, in time-dependent
problems, standard training strategies such as teacher forcing introduce a
mismatch between training and inference, leading to compounding errors in
long-term autoregressive predictions. To address this issue, we propose
Recurrent Neural Operators (RNOs)-a novel framework that integrates recurrent
training into neural operator architectures. Instead of conditioning each
training step on ground-truth inputs, RNOs recursively apply the operator to
their own predictions over a temporal window, effectively simulating
inference-time dynamics during training. This alignment mitigates exposure bias
and enhances robustness to error accumulation. Theoretically, we show that
recurrent training can reduce the worst-case exponential error growth typical
of teacher forcing to linear growth. Empirically, we demonstrate that
recurrently trained Multigrid Neural Operators significantly outperform their
teacher-forced counterparts in long-term accuracy and stability on standard
benchmarks. Our results underscore the importance of aligning training with
inference dynamics for robust temporal generalization in neural operator
learning.

</details>


### [605] [A reinforcement learning agent for maintenance of deteriorating systems with increasingly imperfect repairs](https://arxiv.org/pdf/2505.20725)
*Alberto Pliego Marugán, Jesús M. Pinar-Pérez, Fausto Pedro García Márquez*

Main category: cs.LG

TL;DR: A reinforcement-learning-based agent using Double Deep Q-Network optimizes maintenance for systems with imperfect repairs, outperforming traditional strategies.


<details>
  <summary>Details</summary>
Motivation: Industry 4.0 demands new maintenance paradigms; machine learning, especially reinforcement learning, offers promising solutions for systems with degrading repair effects.

Method: Proposes a gamma degradation process and a maintenance model with increasingly imperfect repairs. Uses a Double Deep Q-Network agent to generate flexible policies without predefined thresholds.

Result: The agent adapts well to various scenarios, significantly improving long-run costs compared to common maintenance strategies.

Conclusion: The reinforcement-learning approach is effective for maintenance optimization in systems with degrading repair benefits.

Abstract: Efficient maintenance has always been essential for the successful
application of engineering systems. However, the challenges to be overcome in
the implementation of Industry 4.0 necessitate new paradigms of maintenance
optimization. Machine learning techniques are becoming increasingly used in
engineering and maintenance, with reinforcement learning being one of the most
promising. In this paper, we propose a gamma degradation process together with
a novel maintenance model in which repairs are increasingly imperfect, i.e.,
the beneficial effect of system repairs decreases as more repairs are
performed, reflecting the degradational behavior of real-world systems. To
generate maintenance policies for this system, we developed a
reinforcement-learning-based agent using a Double Deep Q-Network architecture.
This agent presents two important advantages: it works without a predefined
preventive threshold, and it can operate in a continuous degradation state
space. Our agent learns to behave in different scenarios, showing great
flexibility. In addition, we performed an analysis of how changes in the main
parameters of the environment affect the maintenance policy proposed by the
agent. The proposed approach is demonstrated to be appropriate and to
significatively improve long-run cost as compared with other common maintenance
strategies.

</details>


### [606] [Adversarial bandit optimization for approximately linear functions](https://arxiv.org/pdf/2505.20734)
*Zhuoyu Cheng, Kohei Hatano, Eiji Takimoto*

Main category: cs.LG

TL;DR: The paper addresses bandit optimization for nonconvex, nonsmooth functions with linear and perturbed losses, providing regret bounds and a lower bound.


<details>
  <summary>Details</summary>
Motivation: To tackle the challenge of bandit optimization where loss functions combine linear and arbitrary perturbations, aiming for robust regret guarantees.

Method: Analyzes regret bounds (expected and high-probability) for the problem, including a special case of bandit linear optimization.

Result: Presents improved high-probability regret bounds and a lower bound on expected regret.

Conclusion: The work advances understanding of bandit optimization under nonconvex, nonsmooth conditions with perturbations.

Abstract: We consider a bandit optimization problem for nonconvex and non-smooth
functions, where in each trial the loss function is the sum of a linear
function and a small but arbitrary perturbation chosen after observing the
player's choice. We give both expected and high probability regret bounds for
the problem. Our result also implies an improved high-probability regret bound
for the bandit linear optimization, a special case with no perturbation. We
also give a lower bound on the expected regret.

</details>


### [607] [Detecting Informative Channels: ActionFormer](https://arxiv.org/pdf/2505.20739)
*Kunpeng Zhao, Asahi Miyazaki, Tsuyoshi Okita*

Main category: cs.LG

TL;DR: The paper proposes a modified ActionFormer for Human Activity Recognition (HAR) using sensor signals, addressing limitations in capturing temporal dynamics and spatial-temporal interdependencies, achieving a 16.01% improvement in mAP.


<details>
  <summary>Details</summary>
Motivation: Existing Transformer-based HAR models like ActionFormer struggle with high temporal dynamics and spatial-temporal feature interdependencies, especially for sensor signals.

Method: The modified ActionFormer incorporates Sequence-and-Excitation strategy and swish activation function to reduce defects without significantly increasing parameters.

Result: Experiments on the WEAR dataset show a 16.01% improvement in average mAP for inertial data.

Conclusion: The proposed modifications effectively enhance ActionFormer's performance for sensor-based HAR, addressing key limitations.

Abstract: Human Activity Recognition (HAR) has recently witnessed advancements with
Transformer-based models. Especially, ActionFormer shows us a new perspectives
for HAR in the sense that this approach gives us additional outputs which
detect the border of the activities as well as the activity labels.
ActionFormer was originally proposed with its input as image/video. However,
this was converted to with its input as sensor signals as well. We analyze this
extensively in terms of deep learning architectures. Based on the report of
high temporal dynamics which limits the model's ability to capture subtle
changes effectively and of the interdependencies between the spatial and
temporal features. We propose the modified ActionFormer which will decrease
these defects for sensor signals. The key to our approach lies in accordance
with the Sequence-and-Excitation strategy to minimize the increase in
additional parameters and opt for the swish activation function to retain the
information about direction in the negative range. Experiments on the WEAR
dataset show that our method achieves substantial improvement of a 16.01\% in
terms of average mAP for inertial data.

</details>


### [608] ['Hello, World!': Making GNNs Talk with LLMs](https://arxiv.org/pdf/2505.20742)
*Sunwoo Kim, Soo Yong Lee, Jaemin Yoo, Kijung Shin*

Main category: cs.LG

TL;DR: GLN, a GNN built on LLMs, uses human-readable text for hidden representations, improving interpretability and achieving strong zero-shot performance.


<details>
  <summary>Details</summary>
Motivation: To address the black-box nature of GNNs by making their hidden representations human-readable and interpretable.

Method: GLN combines GNN message passing with advanced techniques like graph attention and initial residual connections, using LLMs for text-based representations.

Result: GLN outperforms LLM-based baselines in zero-shot node classification and link prediction while providing intuitive insights into GNN workings.

Conclusion: GLN enhances GNN interpretability without sacrificing performance, offering a promising direction for explainable graph learning.

Abstract: While graph neural networks (GNNs) have shown remarkable performance across
diverse graph-related tasks, their high-dimensional hidden representations
render them black boxes. In this work, we propose Graph Lingual Network (GLN),
a GNN built on large language models (LLMs), with hidden representations in the
form of human-readable text. Through careful prompt design, GLN incorporates
not only the message passing module of GNNs but also advanced GNN techniques,
including graph attention and initial residual connection. The
comprehensibility of GLN's hidden representations enables an intuitive analysis
of how node representations change (1) across layers and (2) under advanced GNN
techniques, shedding light on the inner workings of GNNs. Furthermore, we
demonstrate that GLN achieves strong zero-shot performance on node
classification and link prediction, outperforming existing LLM-based baseline
methods.

</details>


### [609] [Uni-Instruct: One-step Diffusion Model through Unified Diffusion Divergence Instruction](https://arxiv.org/pdf/2505.20755)
*Yifei Wang, Weimin Bai, Colin Zhang, Debing Zhang, Weijian Luo, He Sun*

Main category: cs.LG

TL;DR: Uni-Instruct unifies 10+ one-step diffusion distillation methods under a theory-driven framework, achieving state-of-the-art performance in image and text-to-3D generation.


<details>
  <summary>Details</summary>
Motivation: To address the intractability of the expanded $f$-divergence and unify existing approaches for better understanding and performance.

Method: Proposes Uni-Instruct, a framework based on diffusion expansion theory, with tractable loss functions for training one-step diffusion models.

Result: Achieves record-breaking FID scores (e.g., 1.46 on CIFAR10, 1.02 on ImageNet) and outperforms prior methods in text-to-3D generation.

Conclusion: Uni-Instruct offers theoretical and empirical advancements, aiding future research in diffusion distillation and knowledge transfer.

Abstract: In this paper, we unify more than 10 existing one-step diffusion distillation
approaches, such as Diff-Instruct, DMD, SIM, SiD, $f$-distill, etc, inside a
theory-driven framework which we name the \textbf{\emph{Uni-Instruct}}.
Uni-Instruct is motivated by our proposed diffusion expansion theory of the
$f$-divergence family. Then we introduce key theories that overcome the
intractability issue of the original expanded $f$-divergence, resulting in an
equivalent yet tractable loss that effectively trains one-step diffusion models
by minimizing the expanded $f$-divergence family. The novel unification
introduced by Uni-Instruct not only offers new theoretical contributions that
help understand existing approaches from a high-level perspective but also
leads to state-of-the-art one-step diffusion generation performances. On the
CIFAR10 generation benchmark, Uni-Instruct achieves record-breaking Frechet
Inception Distance (FID) values of \textbf{\emph{1.46}} for unconditional
generation and \textbf{\emph{1.38}} for conditional generation. On the
ImageNet-$64\times 64$ generation benchmark, Uni-Instruct achieves a new SoTA
one-step generation FID of \textbf{\emph{1.02}}, which outperforms its 79-step
teacher diffusion with a significant improvement margin of 1.33 (1.02 vs 2.35).
We also apply Uni-Instruct on broader tasks like text-to-3D generation. For
text-to-3D generation, Uni-Instruct gives decent results, which slightly
outperforms previous methods, such as SDS and VSD, in terms of both generation
quality and diversity. Both the solid theoretical and empirical contributions
of Uni-Instruct will potentially help future studies on one-step diffusion
distillation and knowledge transferring of diffusion models.

</details>


### [610] [Practical estimation of the optimal classification error with soft labels and calibration](https://arxiv.org/pdf/2505.20761)
*Ryota Ushio, Takashi Ishida, Masashi Sugiyama*

Main category: cs.LG

TL;DR: The paper addresses estimating the Bayes error in binary classification, improving upon prior work by analyzing bias in hard-label estimators and tackling corrupted soft labels. It shows isotonic calibration can yield consistent estimates under weaker assumptions.


<details>
  <summary>Details</summary>
Motivation: To determine the extent of model improvement possible in binary classification, focusing on practical and theoretically supported methods.

Method: Extends prior work by analyzing bias in hard-label estimators and addressing corrupted soft labels, using isotonic calibration for consistent estimation.

Result: Reveals adaptive bias decay in hard-label estimators and shows isotonic calibration works under weaker assumptions, validated by experiments.

Conclusion: The proposed methods provide practical and theoretically sound ways to estimate Bayes error, even with corrupted labels or privacy constraints.

Abstract: While the performance of machine learning systems has experienced significant
improvement in recent years, relatively little attention has been paid to the
fundamental question: to what extent can we improve our models? This paper
provides a means of answering this question in the setting of binary
classification, which is practical and theoretically supported. We extend a
previous work that utilizes soft labels for estimating the Bayes error, the
optimal error rate, in two important ways. First, we theoretically investigate
the properties of the bias of the hard-label-based estimator discussed in the
original work. We reveal that the decay rate of the bias is adaptive to how
well the two class-conditional distributions are separated, and it can decay
significantly faster than the previous result suggested as the number of hard
labels per instance grows. Second, we tackle a more challenging problem
setting: estimation with corrupted soft labels. One might be tempted to use
calibrated soft labels instead of clean ones. However, we reveal that
calibration guarantee is not enough, that is, even perfectly calibrated soft
labels can result in a substantially inaccurate estimate. Then, we show that
isotonic calibration can provide a statistically consistent estimator under an
assumption weaker than that of the previous work. Our method is instance-free,
i.e., we do not assume access to any input instances. This feature allows it to
be adopted in practical scenarios where the instances are not available due to
privacy issues. Experiments with synthetic and real-world datasets show the
validity of our methods and theory.

</details>


### [611] [Robust and Explainable Detector of Time Series Anomaly via Augmenting Multiclass Pseudo-Anomalies](https://arxiv.org/pdf/2505.20765)
*Kohei Obata, Yasuko Matsubara, Yasushi Sakurai*

Main category: cs.LG

TL;DR: RedLamp introduces a method for unsupervised anomaly detection in time series using diverse data augmentations to generate multiclass pseudo-anomalies, improving robustness against anomaly contamination.


<details>
  <summary>Details</summary>
Motivation: Current methods assume training data is mostly normal, but anomaly contamination can mislead models. Existing augmentation approaches have limitations in coverage, false anomalies, and label trust.

Method: RedLamp employs diverse data augmentations to create multiclass pseudo-anomalies and learns a multiclass boundary using soft labels for robustness.

Result: RedLamp effectively detects anomalies and is robust against contamination, with an explainable latent space.

Conclusion: RedLamp addresses limitations of existing methods, offering a robust and explainable solution for unsupervised anomaly detection in time series.

Abstract: Unsupervised anomaly detection in time series has been a pivotal research
area for decades. Current mainstream approaches focus on learning normality, on
the assumption that all or most of the samples in the training set are normal.
However, anomalies in the training set (i.e., anomaly contamination) can be
misleading. Recent studies employ data augmentation to generate
pseudo-anomalies and learn the boundary separating the training samples from
the augmented samples. Although this approach mitigates anomaly contamination
if augmented samples mimic unseen real anomalies, it suffers from several
limitations. (1) Covering a wide range of time series anomalies is challenging.
(2) It disregards augmented samples that resemble normal samples (i.e., false
anomalies). (3) It places too much trust in the labels of training and
augmented samples. In response, we propose RedLamp, which employs diverse data
augmentations to generate multiclass pseudo-anomalies and learns the multiclass
boundary. Such multiclass pseudo-anomalies cover a wide variety of time series
anomalies. We conduct multiclass classification using soft labels, which
prevents the model from being overconfident and ensures its robustness against
contaminated/false anomalies. The learned latent space is inherently
explainable as it is trained to separate pseudo-anomalies into multiclasses.
Extensive experiments demonstrate the effectiveness of RedLamp in anomaly
detection and its robustness against anomaly contamination.

</details>


### [612] [TimePro: Efficient Multivariate Long-term Time Series Forecasting with Variable- and Time-Aware Hyper-state](https://arxiv.org/pdf/2505.20774)
*Xiaowen Ma, Zhenliang Ni, Shuai Xiao, Xinghao Chen*

Main category: cs.LG

TL;DR: TimePro is a Mamba-based model addressing the multi-delay issue in long-term time series forecasting by constructing variate- and time-aware hyper-states, outperforming traditional methods.


<details>
  <summary>Details</summary>
Motivation: Traditional models fail to capture complex variable relationships and non-trivial time representations due to uniform processing of variables and time points.

Method: TimePro builds hyper-states that preserve fine-grained temporal features and adaptively selects focused time points to enhance forecasting accuracy.

Result: TimePro achieves competitive performance on eight real-world benchmarks with linear complexity.

Conclusion: TimePro effectively addresses the multi-delay issue and improves forecasting accuracy by leveraging variate- and time-aware hyper-states.

Abstract: In long-term time series forecasting, different variables often influence the
target variable over distinct time intervals, a challenge known as the
multi-delay issue. Traditional models typically process all variables or time
points uniformly, which limits their ability to capture complex variable
relationships and obtain non-trivial time representations. To address this
issue, we propose TimePro, an innovative Mamba-based model that constructs
variate- and time-aware hyper-states. Unlike conventional approaches that
merely transfer plain states across variable or time dimensions, TimePro
preserves the fine-grained temporal features of each variate token and
adaptively selects the focused time points to tune the plain state. The
reconstructed hyper-state can perceive both variable relationships and salient
temporal information, which helps the model make accurate forecasting. In
experiments, TimePro performs competitively on eight real-world long-term
forecasting benchmarks with satisfactory linear complexity. Code is available
at https://github.com/xwmaxwma/TimePro.

</details>


### [613] [Non-invasive maturity assessment of iPSC-CMs based on optical maturity characteristics using interpretable AI](https://arxiv.org/pdf/2505.20775)
*Fabian Scheurer, Alexander Hammer, Mario Schubert, Robert-Patrick Steiner, Oliver Gamm, Kaomei Guan, Frank Sonntag, Hagen Malberg, Martin Schmidt*

Main category: cs.LG

TL;DR: Non-invasive AI-based video motion analysis classifies iPSC-CM maturity with high accuracy, reducing variability in drug testing.


<details>
  <summary>Details</summary>
Motivation: Current methods for assessing iPSC-CM maturation are invasive and time-consuming, necessitating a non-invasive alternative.

Method: AI-based analysis of beat characteristics from video recordings using SVM, with SHAP for feature relevance.

Result: Achieved 99.5% accuracy in classifying maturity, with displacement, relaxation-rise time, and beating duration as key features.

Conclusion: This approach improves reproducibility and reduces variability in iPSC-CM studies, aiding drug testing.

Abstract: Human induced pluripotent stem cell-derived cardiomyocytes (iPSC-CMs) are an
important resource for the identification of new therapeutic targets and
cardioprotective drugs. After differentiation iPSC-CMs show an immature,
fetal-like phenotype. Cultivation of iPSC-CMs in lipid-supplemented maturation
medium (MM) strongly enhances their structural, metabolic and functional
phenotype. Nevertheless, assessing iPSC-CM maturation state remains challenging
as most methods are time consuming and go in line with cell damage or loss of
the sample. To address this issue, we developed a non-invasive approach for
automated classification of iPSC-CM maturity through interpretable artificial
intelligence (AI)-based analysis of beat characteristics derived from
video-based motion analysis. In a prospective study, we evaluated 230 video
recordings of early-state, immature iPSC-CMs on day 21 after differentiation
(d21) and more mature iPSC-CMs cultured in MM (d42, MM). For each recording, 10
features were extracted using Maia motion analysis software and entered into a
support vector machine (SVM). The hyperparameters of the SVM were optimized in
a grid search on 80 % of the data using 5-fold cross-validation. The optimized
model achieved an accuracy of 99.5 $\pm$ 1.1 % on a hold-out test set. Shapley
Additive Explanations (SHAP) identified displacement, relaxation-rise time and
beating duration as the most relevant features for assessing maturity level.
Our results suggest the use of non-invasive, optical motion analysis combined
with AI-based methods as a tool to assess iPSC-CMs maturity and could be
applied before performing functional readouts or drug testing. This may
potentially reduce the variability and improve the reproducibility of
experimental studies.

</details>


### [614] [Multi-VQC: A Novel QML Approach for Enhancing Healthcare Classification](https://arxiv.org/pdf/2505.20797)
*Antonio Tudisco, Deborah Volpe, Giovanna Turvani*

Main category: cs.LG

TL;DR: Quantum models are explored to address class imbalance in disease diagnosis, leveraging their ability to express complex patterns in higher-dimensional spaces.


<details>
  <summary>Details</summary>
Motivation: Class imbalance in disease diagnosis limits traditional ML models, prompting interest in Quantum models for their potential to overcome these limitations.

Method: Quantum models map data into higher-dimensional computational spaces to express complex patterns.

Result: Quantum models show promise in improving diagnostic accuracy by handling class imbalance better than classical models.

Conclusion: Quantum models could revolutionize disease diagnosis by addressing class imbalance and enhancing model effectiveness.

Abstract: Accurate and reliable diagnosis of diseases is crucial in enabling timely
medical treatment and enhancing patient survival rates. In recent years,
Machine Learning has revolutionized diagnostic practices by creating
classification models capable of identifying diseases. However, these
classification problems often suffer from significant class imbalances, which
can inhibit the effectiveness of traditional models. Therefore, the interest in
Quantum models has arisen, driven by the captivating promise of overcoming the
limitations of the classical counterpart thanks to their ability to express
complex patterns by mapping data in a higher-dimensional computational space.

</details>


### [615] [Leaner Transformers: More Heads, Less Depth](https://arxiv.org/pdf/2505.20802)
*Hemanth Saratchandran, Damien Teney, Simon Lucey*

Main category: cs.LG

TL;DR: The paper challenges the 'bigger is better' mindset in transformers, showing that models can be smaller without losing accuracy by optimizing multi-head attention.


<details>
  <summary>Details</summary>
Motivation: To address the unnecessary size of many transformers by redefining the role of multi-head attention and improving model efficiency.

Method: Theoretical analysis of multi-head attention's role in conditioning, leading to redesigned architectures with more heads but fewer parameters.

Result: Models reduced by 30-50% in size while maintaining accuracy, validated across vision and language tasks.

Conclusion: Optimizing multi-head attention can lead to smaller, more efficient transformers without sacrificing performance.

Abstract: Transformers have reshaped machine learning by utilizing attention mechanisms
to capture complex patterns in large datasets, leading to significant
improvements in performance. This success has contributed to the belief that
"bigger means better", leading to ever-increasing model sizes. This paper
challenge this ideology by showing that many existing transformers might be
unnecessarily oversized. We discover a theoretical principle that redefines the
role of multi-head attention. An important benefit of the multiple heads is in
improving the conditioning of the attention block. We exploit this theoretical
insight and redesign popular architectures with an increased number of heads.
The improvement in the conditioning proves so significant in practice that
model depth can be decreased, reducing the parameter count by up to 30-50%
while maintaining accuracy. We obtain consistent benefits across a variety of
transformer-based architectures of various scales, on tasks in computer vision
(ImageNet-1k) as well as language and sequence modeling (GLUE benchmark,
TinyStories, and the Long-Range Arena benchmark).

</details>


### [616] [Quantum Machine Learning in Healthcare: Evaluating QNN and QSVM Models](https://arxiv.org/pdf/2505.20804)
*Antonio Tudisco, Deborah Volpe, Giovanna Turvani*

Main category: cs.LG

TL;DR: Quantum models like QNNs and QSVMs show promise in healthcare diagnostics, outperforming classical models in imbalanced datasets, with QSVMs being more effective than QNNs.


<details>
  <summary>Details</summary>
Motivation: Improving disease diagnosis accuracy using machine learning, especially for imbalanced datasets, where classical models underperform.

Method: Comparison of Quantum Neural Networks (QNNs) and Quantum Support Vector Machines (QSVMs) with classical models on three healthcare datasets (Prostate Cancer, Heart Failure, Diabetes).

Result: QSVMs outperform QNNs and classical models in imbalanced datasets, demonstrating quantum models' potential.

Conclusion: Quantum models, particularly QSVMs, hold promise for healthcare diagnostics, warranting further research.

Abstract: Effective and accurate diagnosis of diseases such as cancer, diabetes, and
heart failure is crucial for timely medical intervention and improving patient
survival rates. Machine learning has revolutionized diagnostic methods in
recent years by developing classification models that detect diseases based on
selected features. However, these classification tasks are often highly
imbalanced, limiting the performance of classical models. Quantum models offer
a promising alternative, exploiting their ability to express complex patterns
by operating in a higher-dimensional computational space through superposition
and entanglement. These unique properties make quantum models potentially more
effective in addressing the challenges of imbalanced datasets. This work
evaluates the potential of quantum classifiers in healthcare, focusing on
Quantum Neural Networks (QNNs) and Quantum Support Vector Machines (QSVMs),
comparing them with popular classical models. The study is based on three
well-known healthcare datasets -- Prostate Cancer, Heart Failure, and Diabetes.
The results indicate that QSVMs outperform QNNs across all datasets due to
their susceptibility to overfitting. Furthermore, quantum models prove the
ability to overcome classical models in scenarios with high dataset imbalance.
Although preliminary, these findings highlight the potential of quantum models
in healthcare classification tasks and lead the way for further research in
this domain.

</details>


### [617] [Simple yet Effective Graph Distillation via Clustering](https://arxiv.org/pdf/2505.20807)
*Yurui Lai, Taiyan Zhang, Renchi Yang*

Main category: cs.LG

TL;DR: ClustGDD is an efficient graph data distillation method using clustering to condense large graphs for faster GNN training, achieving superior performance and speed.


<details>
  <summary>Details</summary>
Motivation: Existing graph data distillation methods rely on heuristics, compromising quality or efficiency. ClustGDD addresses this by leveraging clustering for better condensation.

Method: ClustGDD synthesizes condensed graphs via clustering to minimize within-cluster variance and maximize homophily, refining attributes with class-aware sampling and consistency loss.

Result: ClustGDD outperforms or matches state-of-the-art methods in node classification on five datasets while being significantly faster.

Conclusion: ClustGDD offers a scalable and effective solution for graph data distillation, improving GNN training efficiency without sacrificing performance.

Abstract: Despite plentiful successes achieved by graph representation learning in
various domains, the training of graph neural networks (GNNs) still remains
tenaciously challenging due to the tremendous computational overhead needed for
sizable graphs in practice. Recently, graph data distillation (GDD), which
seeks to distill large graphs into compact and informative ones, has emerged as
a promising technique to enable efficient GNN training. However, most existing
GDD works rely on heuristics that align model gradients or representation
distributions on condensed and original graphs, leading to compromised result
quality, expensive training for distilling large graphs, or both. Motivated by
this, this paper presents an efficient and effective GDD approach, ClustGDD.
Under the hood, ClustGDD resorts to synthesizing the condensed graph and node
attributes through fast and theoretically-grounded clustering that minimizes
the within-cluster sum of squares and maximizes the homophily on the original
graph. The fundamental idea is inspired by our empirical and theoretical
findings unveiling the connection between clustering and empirical condensation
quality using Fr\'echet Inception Distance, a well-known quality metric for
synthetic images. Furthermore, to mitigate the adverse effects caused by the
homophily-based clustering, ClustGDD refines the nodal attributes of the
condensed graph with a small augmentation learned via class-aware graph
sampling and consistency loss. Our extensive experiments exhibit that GNNs
trained over condensed graphs output by ClustGDD consistently achieve superior
or comparable performance to state-of-the-art GDD methods in terms of node
classification on five benchmark datasets, while being orders of magnitude
faster.

</details>


### [618] [Interpretable Credit Default Prediction with Ensemble Learning and SHAP](https://arxiv.org/pdf/2505.20815)
*Shiqi Yang, Ziyi Huang, Wengran Xiao, Xinyu Shen*

Main category: cs.LG

TL;DR: The study compares machine learning models for credit default prediction, finding ensemble methods superior, especially for complex data. SHAP analysis highlights key features like external credit scores.


<details>
  <summary>Details</summary>
Motivation: To improve credit default prediction by evaluating and comparing machine learning models, addressing data imbalance and nonlinear relationships.

Method: Preprocessed Home Credit dataset, applied feature engineering, and tested models (logistic regression, random forest, XGBoost, LightGBM) for accuracy, precision, and recall. Used SHAP for feature analysis.

Result: Ensemble learning outperformed other models, handling nonlinearity and imbalance well. External credit scores were key in decisions.

Conclusion: The framework enhances credit risk control systems, offering interpretability and practical value through SHAP and ensemble methods.

Abstract: This study focuses on the problem of credit default prediction, builds a
modeling framework based on machine learning, and conducts comparative
experiments on a variety of mainstream classification algorithms. Through
preprocessing, feature engineering, and model training of the Home Credit
dataset, the performance of multiple models including logistic regression,
random forest, XGBoost, LightGBM, etc. in terms of accuracy, precision, and
recall is evaluated. The results show that the ensemble learning method has
obvious advantages in predictive performance, especially in dealing with
complex nonlinear relationships between features and data imbalance problems.
It shows strong robustness. At the same time, the SHAP method is used to
analyze the importance and dependency of features, and it is found that the
external credit score variable plays a dominant role in model decision making,
which helps to improve the model's interpretability and practical application
value. The research results provide effective reference and technical support
for the intelligent development of credit risk control systems.

</details>


### [619] [HAD: Hybrid Architecture Distillation Outperforms Teacher in Genomic Sequence Modeling](https://arxiv.org/pdf/2505.20836)
*Hexiong Yang, Mingrui Chen, Huaibo Huang, Junxian Duan, Jie Cao, Zhen Zhou, Ran He*

Main category: cs.LG

TL;DR: Proposes Hybrid Architecture Distillation (HAD) for efficient DNA sequence modeling, outperforming larger models.


<details>
  <summary>Details</summary>
Motivation: Address computational burden of large models in DNA sequence modeling by using distillation and reconstruction tasks.

Method: Uses NTv2-500M as teacher, grouping masking for feature alignment, and reconstruction in MLM pre-training.

Result: Achieves excellent performance, surpassing even the teacher model in some tasks.

Conclusion: HAD offers efficient, effective pre-training with sophisticated genomic sequence understanding.

Abstract: Inspired by the great success of Masked Language Modeling (MLM) in the
natural language domain, the paradigm of self-supervised pre-training and
fine-tuning has also achieved remarkable progress in the field of DNA sequence
modeling. However, previous methods often relied on massive pre-training data
or large-scale base models with huge parameters, imposing a significant
computational burden. To address this, many works attempted to use more compact
models to achieve similar outcomes but still fell short by a considerable
margin. In this work, we propose a Hybrid Architecture Distillation (HAD)
approach, leveraging both distillation and reconstruction tasks for more
efficient and effective pre-training. Specifically, we employ the NTv2-500M as
the teacher model and devise a grouping masking strategy to align the feature
embeddings of visible tokens while concurrently reconstructing the invisible
tokens during MLM pre-training. To validate the effectiveness of our proposed
method, we conducted comprehensive experiments on the Nucleotide Transformer
Benchmark and Genomic Benchmark. Compared to models with similar parameters,
our model achieved excellent performance. More surprisingly, it even surpassed
the distillation ceiling-teacher model on some sub-tasks, which is more than
500 $\times$ larger. Lastly, we utilize t-SNE for more intuitive visualization,
which shows that our model can gain a sophisticated understanding of the
intrinsic representation pattern in genomic sequences.

</details>


### [620] [FireQ: Fast INT4-FP8 Kernel and RoPE-aware Quantization for LLM Inference Acceleration](https://arxiv.org/pdf/2505.20839)
*Daehyeon Baek, Jieun Choi, Jimyoung Son, Kyungmin Bin, Seungbeom Choi, Kihyo Moon, Minsung Jang, Hyojung Lee*

Main category: cs.LG

TL;DR: FireQ is a co-designed PTQ framework with an INT4-FP8 kernel, enhancing LLM inference throughput via optimized quantization and pipelining, while minimizing accuracy loss.


<details>
  <summary>Details</summary>
Motivation: Memory bandwidth constraints limit inference throughput in large language models, necessitating efficient post-training quantization (PTQ) solutions.

Method: FireQ quantizes weights/key-values to INT4 and activations/queries to FP8, introduces three-stage pipelining for prefill, and employs outlier smoothing techniques for linear and attention layers.

Result: FireQ achieves 1.68x faster inference in feed-forward layers (Llama2-7B) and 1.26x faster prefill (Llama3-8B) vs. QServe, with negligible accuracy loss.

Conclusion: FireQ effectively balances speed and accuracy, outperforming state-of-the-art PTQ methods for LLM inference.

Abstract: As large language models become increasingly prevalent, memory bandwidth
constraints significantly limit inference throughput, motivating post-training
quantization (PTQ). In this paper, we propose FireQ, a co-designed PTQ
framework and an INT4-FP8 matrix multiplication kernel that accelerates LLM
inference across all linear layers. Specifically, FireQ quantizes linear layer
weights and key-values to INT4, and activations and queries to FP8,
significantly enhancing throughput. Additionally, we introduce a three-stage
pipelining for the prefill phase, which modifies the FlashAttention-3 kernel,
effectively reducing time-to-first-token in the prefill phase. To minimize
accuracy loss from quantization, we develop novel outlier smoothing techniques
tailored separately for linear and attention layers. In linear layers, we
explicitly use per-tensor scaling to prevent underflow caused by the FP8
quantization scaling factor of INT4 quantization, and channel-wise scaling to
compensate for coarse granularity of INT4. In attention layers, we address
quantization challenges posed by rotary positional embeddings (RoPE) by
combining pre-RoPE and post-RoPE scaling strategies. FireQ significantly
outperforms state-of-the-art methods, achieving 1.68x faster inference in
feed-forward network layers on Llama2-7B and 1.26x faster prefill phase
performance on Llama3-8B compared to QServe, with negligible accuracy loss.

</details>


### [621] [Aggregation Buffer: Revisiting DropEdge with a New Parameter Block](https://arxiv.org/pdf/2505.20840)
*Dooho Lee, Myeong Kong, Sagad Hamid, Cheonwoo Lee, Jaemin Yoo*

Main category: cs.LG

TL;DR: The paper revisits DropEdge, a data augmentation technique for GNNs, identifies its limitations, and proposes Aggregation Buffer to improve GNN robustness and performance.


<details>
  <summary>Details</summary>
Motivation: DropEdge's potential performance gain in supervised learning tasks is limited due to inherent GNN architectural constraints.

Method: The authors propose Aggregation Buffer, a parameter block designed to address DropEdge's limitations and enhance GNN robustness.

Result: Aggregation Buffer shows consistent performance improvements across datasets and addresses issues like degree bias and structural disparity.

Conclusion: The proposed method is a unifying solution compatible with any GNN model, offering improved robustness and performance.

Abstract: We revisit DropEdge, a data augmentation technique for GNNs which randomly
removes edges to expose diverse graph structures during training. While being a
promising approach to effectively reduce overfitting on specific connections in
the graph, we observe that its potential performance gain in supervised
learning tasks is significantly limited. To understand why, we provide a
theoretical analysis showing that the limited performance of DropEdge comes
from the fundamental limitation that exists in many GNN architectures. Based on
this analysis, we propose Aggregation Buffer, a parameter block specifically
designed to improve the robustness of GNNs by addressing the limitation of
DropEdge. Our method is compatible with any GNN model, and shows consistent
performance improvements on multiple datasets. Moreover, our method effectively
addresses well-known problems such as degree bias or structural disparity as a
unifying solution. Code and datasets are available at
https://github.com/dooho00/agg-buffer.

</details>


### [622] [Cooperation of Experts: Fusing Heterogeneous Information with Large Margin](https://arxiv.org/pdf/2505.20853)
*Shuo Wang, Shunyang Huang, Jinghui Yuan, Zhixiang Shen, Zhao Kang*

Main category: cs.LG

TL;DR: The paper introduces the Cooperation of Experts (CoE) framework to fuse heterogeneous data by encoding it into unified networks, leveraging domain-specific encoders and a large margin mechanism for robust performance.


<details>
  <summary>Details</summary>
Motivation: Existing methods struggle with the heterogeneity of object patterns across semantic spaces, prompting the need for a more flexible and powerful model.

Method: CoE uses dedicated encoders as domain-specific experts to learn relational patterns, collaborating via a large margin mechanism and tailored optimization.

Result: Theoretical analyses confirm feasibility and stability, while experiments show superior performance across benchmarks.

Conclusion: CoE effectively addresses heterogeneity in data fusion, offering robust and flexible solutions for complex real-world data.

Abstract: Fusing heterogeneous information remains a persistent challenge in modern
data analysis. While significant progress has been made, existing approaches
often fail to account for the inherent heterogeneity of object patterns across
different semantic spaces. To address this limitation, we propose the
Cooperation of Experts (CoE) framework, which encodes multi-typed information
into unified heterogeneous multiplex networks. By overcoming modality and
connection differences, CoE provides a powerful and flexible model for
capturing the intricate structures of real-world complex data. In our
framework, dedicated encoders act as domain-specific experts, each specializing
in learning distinct relational patterns in specific semantic spaces. To
enhance robustness and extract complementary knowledge, these experts
collaborate through a novel large margin mechanism supported by a tailored
optimization strategy. Rigorous theoretical analyses guarantee the framework's
feasibility and stability, while extensive experiments across diverse
benchmarks demonstrate its superior performance and broad applicability. Our
code is available at https://github.com/strangeAlan/CoE.

</details>


### [623] [Generalizable Heuristic Generation Through Large Language Models with Meta-Optimization](https://arxiv.org/pdf/2505.20881)
*Yiding Shi, Jianan Zhou, Wen Song, Jieyi Bi, Yaoxin Wu, Jie Zhang*

Main category: cs.LG

TL;DR: MoH is a meta-learning framework using LLMs to autonomously create diverse optimizers for heuristic design in combinatorial optimization, outperforming existing methods.


<details>
  <summary>Details</summary>
Motivation: Existing heuristic design approaches rely on predefined optimizers and single-task training, limiting diversity and generalization.

Method: MoH leverages LLMs to iteratively refine a meta-optimizer, enabling autonomous construction of diverse optimizers and multi-task training.

Result: MoH achieves state-of-the-art performance, especially in cross-size settings, and produces interpretable optimizers.

Conclusion: MoH addresses limitations of current methods by enabling broader heuristic exploration and generalization, proving effective in combinatorial optimization.

Abstract: Heuristic design with large language models (LLMs) has emerged as a promising
approach for tackling combinatorial optimization problems (COPs). However,
existing approaches often rely on manually predefined evolutionary computation
(EC) optimizers and single-task training schemes, which may constrain the
exploration of diverse heuristic algorithms and hinder the generalization of
the resulting heuristics. To address these issues, we propose Meta-Optimization
of Heuristics (MoH), a novel framework that operates at the optimizer level,
discovering effective optimizers through the principle of meta-learning.
Specifically, MoH leverages LLMs to iteratively refine a meta-optimizer that
autonomously constructs diverse optimizers through (self-)invocation, thereby
eliminating the reliance on a predefined EC optimizer. These constructed
optimizers subsequently evolve heuristics for downstream tasks, enabling
broader heuristic exploration. Moreover, MoH employs a multi-task training
scheme to promote its generalization capability. Experiments on classic COPs
demonstrate that MoH constructs an effective and interpretable meta-optimizer,
achieving state-of-the-art performance across various downstream tasks,
particularly in cross-size settings.

</details>


### [624] [Fedivertex: a Graph Dataset based on Decentralized Social Networks for Trustworthy Machine Learning](https://arxiv.org/pdf/2505.20882)
*Marc Damie, Edwige Cyffers*

Main category: cs.LG

TL;DR: Fedivertex is a new dataset of 182 graphs from the Fediverse, offering a real-world alternative for benchmarking decentralized machine learning algorithms.


<details>
  <summary>Details</summary>
Motivation: Existing graph datasets are limited to for-profit social networks, lacking diversity and real-world dynamics. The Fediverse provides a decentralized, open-source alternative.

Method: The dataset includes 182 graphs from seven Fediverse social networks, crawled weekly over 14 weeks. A Python package is released for easy use.

Result: Fedivertex enables benchmarking decentralized algorithms and introduces a new defederation task for link deletion analysis.

Conclusion: Fedivertex fills a gap in decentralized ML benchmarking with real-world, dynamic graph data from the Fediverse.

Abstract: Decentralized machine learning - where each client keeps its own data locally
and uses its own computational resources to collaboratively train a model by
exchanging peer-to-peer messages - is increasingly popular, as it enables
better scalability and control over the data. A major challenge in this setting
is that learning dynamics depend on the topology of the communication graph,
which motivates the use of real graph datasets for benchmarking decentralized
algorithms. Unfortunately, existing graph datasets are largely limited to
for-profit social networks crawled at a fixed point in time and often collected
at the user scale, where links are heavily influenced by the platform and its
recommendation algorithms. The Fediverse, which includes several free and
open-source decentralized social media platforms such as Mastodon, Misskey, and
Lemmy, offers an interesting real-world alternative. We introduce Fedivertex, a
new dataset of 182 graphs, covering seven social networks from the Fediverse,
crawled weekly over 14 weeks. We release the dataset along with a Python
package to facilitate its use, and illustrate its utility on several tasks,
including a new defederation task, which captures a process of link deletion
observed on these networks.

</details>


### [625] [Improved Bounds for Swap Multicalibration and Swap Omniprediction](https://arxiv.org/pdf/2505.20885)
*Haipeng Luo, Spandan Senapati, Vatsal Sharan*

Main category: cs.LG

TL;DR: The paper addresses multicalibration and omniprediction, proposing an efficient algorithm that improves error bounds and sample complexity rates in both online and distributional settings.


<details>
  <summary>Details</summary>
Motivation: To solve the open problem of achieving efficient multicalibration and omniprediction with improved error bounds, as raised by Garg et al. (2024).

Method: An efficient algorithm is introduced to achieve O(T^(1/3)) ℓ₂-swap multicalibration error, which is propagated to improve ℓ₁-swap multicalibration and swap omniprediction rates.

Result: The algorithm achieves O(T^(2/3)) ℓ₁-swap multicalibration and swap omniprediction errors, improving the previous O(T^(7/8)) bound. It also provides better sample complexity rates in distributional settings.

Conclusion: The work significantly advances multicalibration and omniprediction, offering efficient solutions with improved bounds and sample complexities.

Abstract: In this paper, we consider the related problems of multicalibration -- a
multigroup fairness notion and omniprediction -- a simultaneous loss
minimization paradigm, both in the distributional and online settings. The
recent work of Garg et al. (2024) raised the open problem of whether it is
possible to efficiently achieve $O(\sqrt{T})$ $\ell_{2}$-multicalibration error
against bounded linear functions. In this paper, we answer this question in a
strongly affirmative sense. We propose an efficient algorithm that achieves
$O(T^{\frac{1}{3}})$ $\ell_{2}$-swap multicalibration error (both in high
probability and expectation). On propagating this bound onward, we obtain
significantly improved rates for $\ell_{1}$-swap multicalibration and swap
omniprediction for a loss class of convex Lipschitz functions. In particular,
we show that our algorithm achieves $O(T^{\frac{2}{3}})$ $\ell_{1}$-swap
multicalibration and swap omniprediction errors, thereby improving upon the
previous best-known bound of $O(T^{\frac{7}{8}})$. As a consequence of our
improved online results, we further obtain several improved sample complexity
rates in the distributional setting. In particular, we establish a
$O(\varepsilon ^ {-3})$ sample complexity of efficiently learning an
$\varepsilon$-swap omnipredictor for the class of convex and Lipschitz
functions, $O(\varepsilon ^{-2.5})$ sample complexity of efficiently learning
an $\varepsilon$-swap agnostic learner for the squared loss, and $O(\varepsilon
^ {-5}), O(\varepsilon ^ {-2.5})$ sample complexities of learning $\ell_{1},
\ell_{2}$-swap multicalibrated predictors against linear functions, all of
which significantly improve on the previous best-known bounds.

</details>


### [626] [One-Time Soft Alignment Enables Resilient Learning without Weight Transport](https://arxiv.org/pdf/2505.20892)
*Jeonghwan Cheon, Jaehyuk Bae, Se-Bum Paik*

Main category: cs.LG

TL;DR: A one-time soft alignment of forward and feedback weights at initialization enables deep networks to perform comparably to backpropagation, improving trainability and robustness.


<details>
  <summary>Details</summary>
Motivation: Backpropagation is computationally expensive and biologically implausible, while feedback alignment struggles with performance and stability.

Method: Proposes a one-time soft alignment of weights at initialization to avoid symmetric weight transport during learning.

Result: Improves trainability, gradient flow, and convergence to flatter minima, enhancing generalization and robustness.

Conclusion: Simple initialization enables effective, biologically plausible learning in deep networks.

Abstract: Backpropagation is the cornerstone of deep learning, but its reliance on
symmetric weight transport and global synchronization makes it computationally
expensive and biologically implausible. Feedback alignment offers a promising
alternative by approximating error gradients through fixed random feedback,
thereby avoiding symmetric weight transport. However, this approach often
struggles with poor learning performance and instability, especially in deep
networks. Here, we show that a one-time soft alignment between forward and
feedback weights at initialization enables deep networks to achieve performance
comparable to backpropagation, without requiring weight transport during
learning. This simple initialization condition guides stable error minimization
in the loss landscape, improving network trainability. Spectral analyses
further reveal that initial alignment promotes smoother gradient flow and
convergence to flatter minima, resulting in better generalization and
robustness. Notably, we also find that allowing moderate deviations from exact
weight symmetry can improve adversarial robustness compared to standard
backpropagation. These findings demonstrate that a simple initialization
strategy can enable effective learning in deep networks in a biologically
plausible and resource-efficient manner.

</details>


### [627] [How Do Transformers Learn Variable Binding in Symbolic Programs?](https://arxiv.org/pdf/2505.20896)
*Yiwei Wu, Atticus Geiger, Raphaël Millière*

Main category: cs.LG

TL;DR: Transformers can learn variable binding without built-in mechanisms, using attention heads and residual streams as addressable memory.


<details>
  <summary>Details</summary>
Motivation: Understanding how neural networks acquire variable binding, a key symbolic computation ability, without explicit architectural support.

Method: Training a Transformer on symbolic programs with variable assignments and distractors, analyzing its developmental phases and mechanisms.

Result: The model progresses from random predictions to systematic dereferencing, using attention heads and residual streams for dynamic binding.

Conclusion: Transformers can bridge connectionist and symbolic approaches by learning variable binding implicitly.

Abstract: Variable binding -- the ability to associate variables with values -- is
fundamental to symbolic computation and cognition. Although classical
architectures typically implement variable binding via addressable memory, it
is not well understood how modern neural networks lacking built-in binding
operations may acquire this capacity. We investigate this by training a
Transformer to dereference queried variables in symbolic programs where
variables are assigned either numerical constants or other variables. Each
program requires following chains of variable assignments up to four steps deep
to find the queried value, and also contains irrelevant chains of assignments
acting as distractors. Our analysis reveals a developmental trajectory with
three distinct phases during training: (1) random prediction of numerical
constants, (2) a shallow heuristic prioritizing early variable assignments, and
(3) the emergence of a systematic mechanism for dereferencing assignment
chains. Using causal interventions, we find that the model learns to exploit
the residual stream as an addressable memory space, with specialized attention
heads routing information across token positions. This mechanism allows the
model to dynamically track variable bindings across layers, resulting in
accurate dereferencing. Our results show how Transformer models can learn to
implement systematic variable binding without explicit architectural support,
bridging connectionist and symbolic approaches.

</details>


### [628] [Humble AI in the real-world: the case of algorithmic hiring](https://arxiv.org/pdf/2505.20918)
*Rahul Nair, Inge Vejsbjerg, Elizabeth Daly, Christos Varytimidis, Bran Knowles*

Main category: cs.LG

TL;DR: The paper advocates for 'humble AI' in algorithmic hiring, emphasizing scepticism, curiosity, and commitment to address limitations and biases.


<details>
  <summary>Details</summary>
Motivation: To address challenges like misrecognition and stereotyping in AI-driven hiring, which standard fairness frameworks struggle to assess.

Method: Uses uncertainty quantification, entropy estimates, and user experience design to implement humble AI principles in a hiring platform.

Result: Demonstrates technical feasibility and engages recruiters in focus groups to explore trust implications.

Conclusion: Future studies will assess if humble AI's higher cognitive load fosters trust in its outcomes.

Abstract: Humble AI (Knowles et al., 2023) argues for cautiousness in AI development
and deployments through scepticism (accounting for limitations of statistical
learning), curiosity (accounting for unexpected outcomes), and commitment
(accounting for multifaceted values beyond performance). We present a
real-world case study for humble AI in the domain of algorithmic hiring.
Specifically, we evaluate virtual screening algorithms in a widely used hiring
platform that matches candidates to job openings. There are several challenges
in misrecognition and stereotyping in such contexts that are difficult to
assess through standard fairness and trust frameworks; e.g., someone with a
non-traditional background is less likely to rank highly. We demonstrate
technical feasibility of how humble AI principles can be translated to practice
through uncertainty quantification of ranks, entropy estimates, and a user
experience that highlights algorithmic unknowns. We describe preliminary
discussions with focus groups made up of recruiters. Future user studies seek
to evaluate whether the higher cognitive load of a humble AI system fosters a
climate of trust in its outcomes.

</details>


### [629] [Label Leakage in Federated Inertial-based Human Activity Recognition](https://arxiv.org/pdf/2505.20924)
*Marius Bock, Maximilian Hopp, Kristof Van Laerhoven, Michael Moeller*

Main category: cs.LG

TL;DR: The paper examines label reconstruction attacks in Federated Learning for Human Activity Recognition (HAR), revealing vulnerabilities despite privacy measures like Local Differential Privacy.


<details>
  <summary>Details</summary>
Motivation: To assess the risk of label leakage in HAR due to the sensitive nature of activity labels.

Method: Evaluates gradient-based label leakage attacks on HAR datasets, considering factors like class count, sampling, and imbalance.

Result: Reconstruction accuracies reach 90%, with limited protection from privacy techniques.

Conclusion: Recommends privacy-aware deployment of federated HAR systems and highlights open challenges.

Abstract: While prior work has shown that Federated Learning updates can leak sensitive
information, label reconstruction attacks, which aim to recover input labels
from shared gradients, have not yet been examined in the context of Human
Activity Recognition (HAR). Given the sensitive nature of activity labels, this
study evaluates the effectiveness of state-of-the-art gradient-based label
leakage attacks on HAR benchmark datasets. Our findings show that the number of
activity classes, sampling strategy, and class imbalance are critical factors
influencing the extent of label leakage, with reconstruction accuracies
reaching up to 90% on two benchmark datasets, even for trained models.
Moreover, we find that Local Differential Privacy techniques such as gradient
noise and clipping offer only limited protection, as certain attacks still
reliably infer both majority and minority class labels. We conclude by offering
practical recommendations for the privacy-aware deployment of federated HAR
systems and identify open challenges for future research. Code to reproduce our
experiments is publicly available via github.com/mariusbock/leakage_har.

</details>


### [630] [MLMC-based Resource Adequacy Assessment with Active Learning Trained Surrogate Models](https://arxiv.org/pdf/2505.20930)
*Ruiqi Zhang, Simon H. Tindemans*

Main category: cs.LG

TL;DR: The paper introduces a speed metric for MLMC efficiency, incorporating training time, and proposes active learning to reduce labeling effort, improving efficiency in power system reliability assessments.


<details>
  <summary>Details</summary>
Motivation: Pre-labeled datasets are scarce in resource adequacy assessments, and training surrogate models for MLMC can be time-consuming, offsetting efficiency gains.

Method: A speed metric accounts for training time, and a vote-by-committee active learning approach reduces labeling calls within limited time budgets.

Result: Active learning significantly improves MLMC efficiency with reduced training effort, outperforming regular surrogate modeling.

Conclusion: The proposed method enhances MLMC efficiency in power system reliability assessments by balancing training time and variance reduction.

Abstract: Multilevel Monte Carlo (MLMC) is a flexible and effective variance reduction
technique for accelerating reliability assessments of complex power system.
Recently, data-driven surrogate models have been proposed as lower-level models
in the MLMC framework due to their high correlation and negligible execution
time once trained. However, in resource adequacy assessments, pre-labeled
datasets are typically unavailable. For large-scale systems, the efficiency
gains from surrogate models are often offset by the substantial time required
for labeling training data. Therefore, this paper introduces a speed metric
that accounts for training time in evaluating MLMC efficiency. Considering the
total time budget is limited, a vote-by-committee active learning approach is
proposed to reduce the required labeling calls. A case study demonstrates that,
within practical variance thresholds, active learning enables significantly
improved MLMC efficiency with reduced training effort, compared to regular
surrogate modelling approaches.

</details>


### [631] [NatADiff: Adversarial Boundary Guidance for Natural Adversarial Diffusion](https://arxiv.org/pdf/2505.20934)
*Max Collins, Jordan Vice, Tim French, Ajmal Mian*

Main category: cs.LG

TL;DR: NatADiff uses denoising diffusion to generate natural adversarial samples, improving transferability and realism compared to constrained adversarial methods.


<details>
  <summary>Details</summary>
Motivation: Existing adversarial samples are often constrained and don't reflect real-world test-time errors. NatADiff aims to address this gap.

Method: Leverages denoising diffusion, guiding the diffusion trajectory to intersect true and adversarial classes, using time-travel sampling and augmented classifier guidance.

Result: Achieves comparable attack success rates to state-of-the-art, with higher transferability and better alignment with natural errors (measured by FID).

Conclusion: NatADiff produces more realistic and transferable adversarial samples, better resembling natural test-time errors.

Abstract: Adversarial samples exploit irregularities in the manifold ``learned'' by
deep learning models to cause misclassifications. The study of these
adversarial samples provides insight into the features a model uses to classify
inputs, which can be leveraged to improve robustness against future attacks.
However, much of the existing literature focuses on constrained adversarial
samples, which do not accurately reflect test-time errors encountered in
real-world settings. To address this, we propose `NatADiff', an adversarial
sampling scheme that leverages denoising diffusion to generate natural
adversarial samples. Our approach is based on the observation that natural
adversarial samples frequently contain structural elements from the adversarial
class. Deep learning models can exploit these structural elements to shortcut
the classification process, rather than learning to genuinely distinguish
between classes. To leverage this behavior, we guide the diffusion trajectory
towards the intersection of the true and adversarial classes, combining
time-travel sampling with augmented classifier guidance to enhance attack
transferability while preserving image fidelity. Our method achieves comparable
attack success rates to current state-of-the-art techniques, while exhibiting
significantly higher transferability across model architectures and better
alignment with natural test-time errors as measured by FID. These results
demonstrate that NatADiff produces adversarial samples that not only transfer
more effectively across models, but more faithfully resemble naturally
occurring test-time errors.

</details>


### [632] [Revisiting Sparsity Constraint Under High-Rank Property in Partial Multi-Label Learning](https://arxiv.org/pdf/2505.20938)
*Chongjie Si, Yidan Cui, Fuchao Yang, Xiaokang Yang, Wei Shen*

Main category: cs.LG

TL;DR: Schirn introduces a sparsity constraint on noise labels and enforces high-rank on predicted labels, outperforming existing PML methods.


<details>
  <summary>Details</summary>
Motivation: Existing PML methods rely on conflicting assumptions (sparsity and low-rankness) that are impractical for real-world full-rank label matrices.

Method: Proposes Schirn, which applies sparsity to noise labels and enforces high-rank on predicted labels.

Result: Schirn outperforms state-of-the-art methods in experiments.

Conclusion: Schirn effectively addresses real-world PML challenges by balancing sparsity and high-rank constraints.

Abstract: Partial Multi-Label Learning (PML) extends the multi-label learning paradigm
to scenarios where each sample is associated with a candidate label set
containing both ground-truth labels and noisy labels. Existing PML methods
commonly rely on two assumptions: sparsity of the noise label matrix and
low-rankness of the ground-truth label matrix. However, these assumptions are
inherently conflicting and impractical for real-world scenarios, where the true
label matrix is typically full-rank or close to full-rank. To address these
limitations, we demonstrate that the sparsity constraint contributes to the
high-rank property of the predicted label matrix. Based on this, we propose a
novel method Schirn, which introduces a sparsity constraint on the noise label
matrix while enforcing a high-rank property on the predicted label matrix.
Extensive experiments demonstrate the superior performance of Schirn compared
to state-of-the-art methods, validating its effectiveness in tackling
real-world PML challenges.

</details>


### [633] [Efficient Spectral Control of Partially Observed Linear Dynamical Systems](https://arxiv.org/pdf/2505.20943)
*Anand Brahmbhatt, Gon Buzaglo, Sofiia Druchyna, Elad Hazan*

Main category: cs.LG

TL;DR: Double Spectral Control (DSC) improves runtime complexity for controlling linear dynamical systems under partial observation and adversarial disturbances.


<details>
  <summary>Details</summary>
Motivation: Addressing the inefficiency of existing methods in handling system stability margins.

Method: Uses a two-level spectral approximation strategy with double convolution and universal spectral filters.

Result: Matches best regret guarantees while exponentially improving runtime complexity.

Conclusion: DSC efficiently learns optimal linear dynamical controllers.

Abstract: We propose a new method for the problem of controlling linear dynamical
systems under partial observation and adversarial disturbances. Our new
algorithm, Double Spectral Control (DSC), matches the best known regret
guarantees while exponentially improving runtime complexity over previous
approaches in its dependence on the system's stability margin. Our key
innovation is a two-level spectral approximation strategy, leveraging double
convolution with a universal basis of spectral filters, enabling efficient and
accurate learning of the best linear dynamical controllers.

</details>


### [634] [Semantic Communication meets System 2 ML: How Abstraction, Compositionality and Emergent Languages Shape Intelligence](https://arxiv.org/pdf/2505.20964)
*Mehdi Bennis, Salem Lahlou*

Main category: cs.LG

TL;DR: The paper advocates for a paradigm shift in 6G and AI, focusing on semantic understanding and goal-oriented interaction through System-2 cognition principles: Abstraction, Compositionality, and Emergent Communication.


<details>
  <summary>Details</summary>
Motivation: Current 6G visions are incremental, and AI models are brittle and data-hungry, lacking robust reasoning. A foundational shift is needed for intelligent systems.

Method: Proposes a unified vision based on System-2 cognition, emphasizing Abstraction, Compositionality, and Emergent Communication to enable reasoning and collaboration.

Result: Lays groundwork for intelligent systems that can reason, adapt, and collaborate, unifying wireless communications, machine learning, and robotics.

Conclusion: A coherent framework integrating these principles can advance 6G and AI toward truly intelligent, adaptive systems.

Abstract: The trajectories of 6G and AI are set for a creative collision. However,
current visions for 6G remain largely incremental evolutions of 5G, while
progress in AI is hampered by brittle, data-hungry models that lack robust
reasoning capabilities. This paper argues for a foundational paradigm shift,
moving beyond the purely technical level of communication toward systems
capable of semantic understanding and effective, goal-oriented interaction. We
propose a unified research vision rooted in the principles of System-2
cognition, built upon three pillars: Abstraction, enabling agents to learn
meaningful world models from raw sensorimotor data; Compositionality, providing
the algebraic tools to combine learned concepts and subsystems; and Emergent
Communication, allowing intelligent agents to create their own adaptive and
grounded languages. By integrating these principles, we lay the groundwork for
truly intelligent systems that can reason, adapt, and collaborate, unifying
advances in wireless communications, machine learning, and robotics under a
single coherent framework.

</details>


### [635] [Understanding the behavior of representation forgetting in continual learning](https://arxiv.org/pdf/2505.20970)
*Joonkyu Kim, Yejin Kim, Jy-yong Sohn*

Main category: cs.LG

TL;DR: The paper introduces a theoretical analysis of representation forgetting in continual learning, proposes a new metric (representation discrepancy), and validates findings through experiments.


<details>
  <summary>Details</summary>
Motivation: Address catastrophic forgetting in continual learning by focusing on representation forgetting at hidden layers.

Method: Introduces a new metric (representation discrepancy) and analyzes its dynamics mathematically.

Result: Forgetting increases with layer index but slows with wider networks; validated on Split-CIFAR100 and ImageNet1K.

Conclusion: The proposed metric effectively measures representation forgetting, providing insights into continual learning dynamics.

Abstract: In continual learning scenarios, catastrophic forgetting of previously
learned tasks is a critical issue, making it essential to effectively measure
such forgetting. Recently, there has been growing interest in focusing on
representation forgetting, the forgetting measured at the hidden layer. In this
paper, we provide the first theoretical analysis of representation forgetting
and use this analysis to better understand the behavior of continual learning.
First, we introduce a new metric called representation discrepancy, which
measures the difference between representation spaces constructed by two
snapshots of a model trained through continual learning. We demonstrate that
our proposed metric serves as an effective surrogate for the representation
forgetting while remaining analytically tractable. Second, through mathematical
analysis of our metric, we derive several key findings about the dynamics of
representation forgetting: the forgetting occurs more rapidly to a higher
degree as the layer index increases, while increasing the width of the network
slows down the forgetting process. Third, we support our theoretical findings
through experiments on real image datasets, including Split-CIFAR100 and
ImageNet1K.

</details>


### [636] [Deep k-grouping: An Unsupervised Learning Framework for Combinatorial Optimization on Graphs and Hypergraphs](https://arxiv.org/pdf/2505.20972)
*Sen Bai, Chunqi Yang, Xin Bai, Xin Zhang, Zhengang Jiang*

Main category: cs.LG

TL;DR: Deep $k$-grouping is an unsupervised learning-based framework for solving large-scale $k$-grouping problems on graphs and hypergraphs, outperforming existing methods.


<details>
  <summary>Details</summary>
Motivation: Existing neural network solvers struggle with large-scale $k$-grouping problems due to computational limitations.

Method: Proposes OH-PUBO formulation, GPU-accelerated algorithms, and a Gini coefficient-based annealing strategy for unsupervised optimization.

Result: Outperforms neural network solvers and classical heuristics like SCIP and Tabu.

Conclusion: Deep $k$-grouping offers a scalable and effective solution for large-scale combinatorial optimization problems.

Abstract: Along with AI computing shining in scientific discovery, its potential in the
combinatorial optimization (CO) domain has also emerged in recent years. Yet,
existing unsupervised neural network solvers struggle to solve $k$-grouping
problems (e.g., coloring, partitioning) on large-scale graphs and hypergraphs,
due to limited computational frameworks. In this work, we propose Deep
$k$-grouping, an unsupervised learning-based CO framework. Specifically, we
contribute: Novel one-hot encoded polynomial unconstrained binary optimization
(OH-PUBO), a formulation for modeling k-grouping problems on graphs and
hypergraphs (e.g., graph/hypergraph coloring and partitioning); GPU-accelerated
algorithms for large-scale k-grouping CO problems. Deep $k$-grouping employs
the relaxation of large-scale OH-PUBO objectives as differentiable loss
functions and trains to optimize them in an unsupervised manner. To ensure
scalability, it leverages GPU-accelerated algorithms to unify the training
pipeline; A Gini coefficient-based continuous relaxation annealing strategy to
enforce discreteness of solutions while preventing convergence to local optima.
Experimental results demonstrate that Deep $k$-grouping outperforms existing
neural network solvers and classical heuristics such as SCIP and Tabu.

</details>


### [637] [Efficient Identity and Position Graph Embedding via Spectral-Based Random Feature Aggregation](https://arxiv.org/pdf/2505.20992)
*Meng Qin, Jiahong Liu, Irwin King*

Main category: cs.LG

TL;DR: The paper proposes Random Feature Aggregation (RFA), a parameter-free GNN method using random inputs and one feed-forward propagation to efficiently capture node identities and positions via high- and low-frequency graph signals.


<details>
  <summary>Details</summary>
Motivation: Existing GNN methods lack clarity on which graph properties (identity or position) they capture and often suffer from inefficiency due to complex procedures like training and feature extraction.

Method: RFA uses a spectral-based GNN backbone with random inputs and no learnable parameters, performing identity and position embedding via one feed-forward propagation. A degree correction mechanism is added for improved results.

Result: RFA variants with high- and low-pass filters efficiently derive identity and position embeddings without training, outperforming baselines in quality and efficiency.

Conclusion: RFA offers a superior trade-off between embedding quality and computational efficiency for both identity and position embedding tasks.

Abstract: Graph neural networks (GNNs), which capture graph structures via a feature
aggregation mechanism following the graph embedding framework, have
demonstrated a powerful ability to support various tasks. According to the
topology properties (e.g., structural roles or community memberships of nodes)
to be preserved, graph embedding can be categorized into identity and position
embedding. However, it is unclear for most GNN-based methods which property
they can capture. Some of them may also suffer from low efficiency and
scalability caused by several time- and space-consuming procedures (e.g.,
feature extraction and training). From a perspective of graph signal
processing, we find that high- and low-frequency information in the graph
spectral domain may characterize node identities and positions, respectively.
Based on this investigation, we propose random feature aggregation (RFA) for
efficient identity and position embedding, serving as an extreme ablation study
regarding GNN feature aggregation. RFA (i) adopts a spectral-based GNN without
learnable parameters as its backbone, (ii) only uses random noises as inputs,
and (iii) derives embeddings via just one feed-forward propagation (FFP).
Inspired by degree-corrected spectral clustering, we further introduce a degree
correction mechanism to the GNN backbone. Surprisingly, our experiments
demonstrate that two variants of RFA with high- and low-pass filters can
respectively derive informative identity and position embeddings via just one
FFP (i.e., without any training). As a result, RFA can achieve a better
trade-off between quality and efficiency for both identity and position
embedding over various baselines.

</details>


### [638] [BIPNN: Learning to Solve Binary Integer Programming via Hypergraph Neural Networks](https://arxiv.org/pdf/2505.20997)
*Sen Bai, Chunqi Yang, Xin Bai, Xin Zhang, Zhengang Jiang*

Main category: cs.LG

TL;DR: BIPNN is an unsupervised learning framework using HyperGNN to solve nonlinear BIP problems, outperforming traditional methods in scalability and efficiency.


<details>
  <summary>Details</summary>
Motivation: Existing neural network-based solvers for ILP lack scalability for nonlinear challenges, and Branch-and-Cut solvers face computational limitations due to exponential growth in auxiliary variables.

Method: BIPNN reformulates nonlinear BIP problems into unconstrained, differentiable polynomial loss functions using HyperGNN, enabling end-to-end optimization via GPU-accelerated training with continuous annealing.

Result: BIPNN efficiently handles large-scale nonlinear BIP problems in parallel via gradient descent, reducing training costs while generating high-quality discrete solutions.

Conclusion: BIPNN demonstrates superior performance on synthetic and real-world datasets, offering a scalable and efficient solution for nonlinear BIP problems.

Abstract: Binary (0-1) integer programming (BIP) is pivotal in scientific domains
requiring discrete decision-making. As the advance of AI computing, recent
works explore neural network-based solvers for integer linear programming (ILP)
problems. Yet, they lack scalability for tackling nonlinear challenges. To
handle nonlinearities, state-of-the-art Branch-and-Cut solvers employ linear
relaxations, leading to exponential growth in auxiliary variables and severe
computation limitations. To overcome these limitations, we propose BIPNN
(Binary Integer Programming Neural Network), an unsupervised learning framework
to solve nonlinear BIP problems via hypergraph neural networks (HyperGNN).
Specifically, BIPNN reformulates BIPs-constrained, discrete, and nonlinear
(sin, log, exp) optimization problems-into unconstrained, differentiable, and
polynomial loss functions. The reformulation stems from the observation of a
precise one-to-one mapping between polynomial BIP objectives and hypergraph
structures, enabling the unsupervised training of HyperGNN to optimize BIP
problems in an end-to-end manner. On this basis, we propose a GPU-accelerated
and continuous-annealing-enhanced training pipeline for BIPNN. The pipeline
enables BIPNN to optimize large-scale nonlinear terms in BIPs fully in parallel
via straightforward gradient descent, thus significantly reducing the training
cost while ensuring the generation of discrete, high-quality solutions.
Extensive experiments on synthetic and real-world datasets highlight the
superiority of our approach.

</details>


### [639] [Efficient and Unbiased Sampling from Boltzmann Distributions via Variance-Tuned Diffusion Models](https://arxiv.org/pdf/2505.21005)
*Fengzhe Zhang, Laurence I. Midgley, José Miguel Hernández-Lobato*

Main category: cs.LG

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: Score-based diffusion models (SBDMs) are powerful amortized samplers for
Boltzmann distributions; however, imperfect score estimates bias downstream
Monte Carlo estimates. Classical importance sampling (IS) can correct this
bias, but computing exact likelihoods requires solving the probability-flow
ordinary differential equation (PF-ODE), a procedure that is prohibitively
costly and scales poorly with dimensionality. We introduce Variance-Tuned
Diffusion Importance Sampling (VT-DIS), a lightweight post-training method that
adapts the per-step noise covariance of a pretrained SBDM by minimizing the
$\alpha$-divergence ($\alpha=2$) between its forward diffusion and reverse
denoising trajectories. VT-DIS assigns a single trajectory-wise importance
weight to the joint forward-reverse process, yielding unbiased expectation
estimates at test time with negligible overhead compared to standard sampling.
On the DW-4, LJ-13, and alanine-dipeptide benchmarks, VT-DIS achieves effective
sample sizes of approximately 80 %, 35 %, and 3.5 %, respectively, while using
only a fraction of the computational budget required by vanilla diffusion + IS
or PF-ODE-based IS.

</details>


### [640] [Federated Instrumental Variable Analysis via Federated Generalized Method of Moments](https://arxiv.org/pdf/2505.21012)
*Geetika, Somya Tyagi, Bapi Chatterjee*

Main category: cs.LG

TL;DR: FedIV introduces federated GMM for IV analysis in high-dimensional settings, addressing privacy and non-i.i.d. data challenges with a novel federated minimax optimization approach.


<details>
  <summary>Details</summary>
Motivation: No existing federated algorithm for GMM or IV analysis, despite the need for privacy-preserving methods in decentralized data settings.

Method: FedGMM formulated as a federated zero-sum game, solved using FedGDA, with theoretical analysis of local equilibria.

Result: Federated solution consistently estimates local moment conditions for all clients, validated by experiments.

Conclusion: FedIV successfully bridges the gap in federated IV analysis, offering a privacy-preserving and efficient solution.

Abstract: Instrumental variables (IV) analysis is an important applied tool for areas
such as healthcare and consumer economics. For IV analysis in high-dimensional
settings, the Generalized Method of Moments (GMM) using deep neural networks
offers an efficient approach. With non-i.i.d. data sourced from scattered
decentralized clients, federated learning is a popular paradigm for training
the models while promising data privacy. However, to our knowledge, no
federated algorithm for either GMM or IV analysis exists to date. In this work,
we introduce federated instrumental variables analysis (FedIV) via federated
generalized method of moments (FedGMM). We formulate FedGMM as a federated
zero-sum game defined by a federated non-convex non-concave minimax
optimization problem, which is solved using federated gradient descent ascent
(FedGDA) algorithm. One key challenge arises in theoretically characterizing
the federated local optimality. To address this, we present properties and
existence results of clients' local equilibria via FedGDA limit points.
Thereby, we show that the federated solution consistently estimates the local
moment conditions of every participating client. The proposed algorithm is
backed by extensive experiments to demonstrate the efficacy of our approach.

</details>


### [641] [NeuralOM: Neural Ocean Model for Subseasonal-to-Seasonal Simulation](https://arxiv.org/pdf/2505.21020)
*Yuan Gao, Ruiqi Shu, Hao Wu, Fan Xu, Yanfei Xiang, Ruijian Gou, Qingsong Wen, Xian Wu, Xiaomeng Huang*

Main category: cs.LG

TL;DR: The paper introduces NeuralOM, a neural ocean model for S2S simulation, using a multi-scale interactive graph neural network to improve accuracy and physical consistency.


<details>
  <summary>Details</summary>
Motivation: Current ML models for S2S ocean simulation lack physical consistency and fail to account for the ocean's slow-changing properties.

Method: Proposes NeuralOM with a multi-stage framework and multi-scale interactive messaging module to capture complex ocean dynamics.

Result: NeuralOM outperforms state-of-the-art models in S2S and extreme event simulation.

Conclusion: The proposed NeuralOM effectively addresses limitations of current ML models for S2S ocean simulation.

Abstract: Accurate Subseasonal-to-Seasonal (S2S) ocean simulation is critically
important for marine research, yet remains challenging due to its substantial
thermal inertia and extended time delay. Machine learning (ML)-based models
have demonstrated significant advancements in simulation accuracy and
computational efficiency compared to traditional numerical methods.
Nevertheless, a significant limitation of current ML models for S2S ocean
simulation is their inadequate incorporation of physical consistency and the
slow-changing properties of the ocean system. In this work, we propose a neural
ocean model (NeuralOM) for S2S ocean simulation with a multi-scale interactive
graph neural network to emulate diverse physical phenomena associated with
ocean systems effectively. Specifically, we propose a multi-stage framework
tailored to model the ocean's slowly changing nature. Additionally, we
introduce a multi-scale interactive messaging module to capture complex
dynamical behaviors, such as gradient changes and multiplicative coupling
relationships inherent in ocean dynamics. Extensive experimental evaluations
confirm that our proposed NeuralOM outperforms state-of-the-art models in S2S
and extreme event simulation. The codes are available at
https://github.com/YuanGao-YG/NeuralOM.

</details>


### [642] [Pause Tokens Strictly Increase the Expressivity of Constant-Depth Transformers](https://arxiv.org/pdf/2505.21024)
*Charles London, Varun Kanade*

Main category: cs.LG

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: Pause tokens, simple filler symbols such as "...", consistently improve
Transformer performance on both language and mathematical tasks, yet their
theoretical effect remains unexplained. We provide the first formal separation
result, proving that adding pause tokens to constant-depth, logarithmic-width
Transformers strictly increases their computational expressivity. With
bounded-precision activations, Transformers without pause tokens compute only a
strict subset of $\mathsf{AC}^0$ functions, while adding a polynomial number of
pause tokens allows them to express the entire class. For logarithmic-precision
Transformers, we show that adding pause tokens achieves expressivity equivalent
to $\mathsf{TC}^0$, matching known upper bounds. Empirically, we demonstrate
that two-layer causally masked Transformers can learn parity when supplied with
pause tokens, a function that they appear unable to learn without them. Our
results provide a rigorous theoretical explanation for prior empirical
findings, clarify how pause tokens interact with width, depth, and numeric
precision, and position them as a distinct mechanism, complementary to
chain-of-thought prompting, for enhancing Transformer reasoning.

</details>


### [643] [TabAttackBench: A Benchmark for Adversarial Attacks on Tabular Data](https://arxiv.org/pdf/2505.21027)
*Zhipeng He, Chun Ouyang, Lijie Wen, Cong Liu, Catarina Moreira*

Main category: cs.LG

TL;DR: A new benchmark for adversarial attacks on tabular data evaluates effectiveness and imperceptibility, addressing gaps in current research.


<details>
  <summary>Details</summary>
Motivation: Adversarial attacks on tabular data present unique challenges due to heterogeneity and feature interdependencies, with imperceptibility often overlooked.

Method: Assessed five adversarial attacks across four models using eleven tabular datasets, analyzing effectiveness and imperceptibility.

Result: Findings reveal interactions between effectiveness and imperceptibility, with insights for improving attack algorithms.

Conclusion: The benchmark advances adversarial machine learning on tabular data by emphasizing imperceptibility alongside effectiveness.

Abstract: Adversarial attacks pose a significant threat to machine learning models by
inducing incorrect predictions through imperceptible perturbations to input
data. While these attacks have been extensively studied in unstructured data
like images, their application to tabular data presents new challenges. These
challenges arise from the inherent heterogeneity and complex feature
interdependencies in tabular data, which differ significantly from those in
image data. To address these differences, it is crucial to consider
imperceptibility as a key criterion specific to tabular data. Most current
research focuses primarily on achieving effective adversarial attacks, often
overlooking the importance of maintaining imperceptibility. To address this
gap, we propose a new benchmark for adversarial attacks on tabular data that
evaluates both effectiveness and imperceptibility. In this study, we assess the
effectiveness and imperceptibility of five adversarial attacks across four
models using eleven tabular datasets, including both mixed and numerical-only
datasets. Our analysis explores how these factors interact and influence the
overall performance of the attacks. We also compare the results across
different dataset types to understand the broader implications of these
findings. The findings from this benchmark provide valuable insights for
improving the design of adversarial attack algorithms, thereby advancing the
field of adversarial machine learning on tabular data.

</details>


### [644] [LLaMEA-BO: A Large Language Model Evolutionary Algorithm for Automatically Generating Bayesian Optimization Algorithms](https://arxiv.org/pdf/2505.21034)
*Wenhu Li, Niki van Stein, Thomas Bäck, Elena Raponi*

Main category: cs.LG

TL;DR: LLMs automate Bayesian optimization (BO) algorithm design, outperforming state-of-the-art baselines on benchmark tasks.


<details>
  <summary>Details</summary>
Motivation: Manual BO algorithm design is expertise-driven; LLMs offer potential for automation and novel algorithmic discovery.

Method: Evolution strategy guides LLM to generate BO algorithm code (initial design, surrogate model, acquisition function), evaluated on BBOB test suite.

Result: LLM-generated algorithms outperform baselines in 19/24 BBOB functions (dimension 5) and generalize to higher dimensions/tasks.

Conclusion: LLMs can co-design algorithms, enabling automated BO development and accelerating novel combinations.

Abstract: Bayesian optimization (BO) is a powerful class of algorithms for optimizing
expensive black-box functions, but designing effective BO algorithms remains a
manual, expertise-driven task. Recent advancements in Large Language Models
(LLMs) have opened new avenues for automating scientific discovery, including
the automatic design of optimization algorithms. While prior work has used LLMs
within optimization loops or to generate non-BO algorithms, we tackle a new
challenge: Using LLMs to automatically generate full BO algorithm code. Our
framework uses an evolution strategy to guide an LLM in generating Python code
that preserves the key components of BO algorithms: An initial design, a
surrogate model, and an acquisition function. The LLM is prompted to produce
multiple candidate algorithms, which are evaluated on the established Black-Box
Optimization Benchmarking (BBOB) test suite from the COmparing Continuous
Optimizers (COCO) platform. Based on their performance, top candidates are
selected, combined, and mutated via controlled prompt variations, enabling
iterative refinement. Despite no additional fine-tuning, the LLM-generated
algorithms outperform state-of-the-art BO baselines in 19 (out of 24) BBOB
functions in dimension 5 and generalize well to higher dimensions, and
different tasks (from the Bayesmark framework). This work demonstrates that
LLMs can serve as algorithmic co-designers, offering a new paradigm for
automating BO development and accelerating the discovery of novel algorithmic
combinations. The source code is provided at
https://github.com/Ewendawi/LLaMEA-BO.

</details>


### [645] [Scalable and adaptive prediction bands with kernel sum-of-squares](https://arxiv.org/pdf/2505.21039)
*Louis Allain, Sébastien da Veiga, Brian Staber*

Main category: cs.LG

TL;DR: The paper enhances Conformal Prediction (CP) by framing it as a statistical learning problem using RKHS and kernel SoS methods, improving adaptivity and efficiency.


<details>
  <summary>Details</summary>
Motivation: To address the lack of adaptivity in CP and improve computational efficiency.

Method: Extends CP with RKHS and kernel SoS, introduces a dual formulation solvable by gradient methods, and proposes HSIC-based hyperparameter tuning for adaptivity.

Result: Efficient solution for large samples and improved adaptivity through HSIC tuning, validated by experiments.

Conclusion: The proposed method advances CP by combining theoretical rigor with practical efficiency and adaptivity.

Abstract: Conformal Prediction (CP) is a popular framework for constructing prediction
bands with valid coverage in finite samples, while being free of any
distributional assumption. A well-known limitation of conformal prediction is
the lack of adaptivity, although several works introduced practically efficient
alternate procedures. In this work, we build upon recent ideas that rely on
recasting the CP problem as a statistical learning problem, directly targeting
coverage and adaptivity. This statistical learning problem is based on
reproducible kernel Hilbert spaces (RKHS) and kernel sum-of-squares (SoS)
methods. First, we extend previous results with a general representer theorem
and exhibit the dual formulation of the learning problem. Crucially, such dual
formulation can be solved efficiently by accelerated gradient methods with
several hundreds or thousands of samples, unlike previous strategies based on
off-the-shelf semidefinite programming algorithms. Second, we introduce a new
hyperparameter tuning strategy tailored specifically to target adaptivity
through bounds on test-conditional coverage. This strategy, based on the
Hilbert-Schmidt Independence Criterion (HSIC), is introduced here to tune
kernel lengthscales in our framework, but has broader applicability since it
could be used in any CP algorithm where the score function is learned. Finally,
extensive experiments are conducted to show how our method compares to related
work. All figures can be reproduced with the accompanying code.

</details>


### [646] [A domain adaptation neural network for digital twin-supported fault diagnosis](https://arxiv.org/pdf/2505.21046)
*Zhenling Chen, Haiwei Fu, Zhiguo Zeng*

Main category: cs.LG

TL;DR: A DANN-based fault diagnosis framework improves accuracy by transferring knowledge from simulated to real-world data, bridging the sim-to-real gap.


<details>
  <summary>Details</summary>
Motivation: Address performance drop due to discrepancies between simulated and real-world data in fault diagnosis.

Method: Propose a DANN framework for domain adaptation, comparing it with CNN, TCN, Transformer, and LSTM.

Result: DANN improves CNN accuracy from 70.00% to 80.22% on real-world data.

Conclusion: Domain adaptation effectively bridges the sim-to-real gap in fault diagnosis.

Abstract: Digital twins offer a promising solution to the lack of sufficient labeled
data in deep learning-based fault diagnosis by generating simulated data for
model training. However, discrepancies between simulation and real-world
systems can lead to a significant drop in performance when models are applied
in real scenarios. To address this issue, we propose a fault diagnosis
framework based on Domain-Adversarial Neural Networks (DANN), which enables
knowledge transfer from simulated (source domain) to real-world (target domain)
data. We evaluate the proposed framework using a publicly available robotics
fault diagnosis dataset, which includes 3,600 sequences generated by a digital
twin model and 90 real sequences collected from physical systems. The DANN
method is compared with commonly used lightweight deep learning models such as
CNN, TCN, Transformer, and LSTM. Experimental results show that incorporating
domain adaptation significantly improves the diagnostic performance. For
example, applying DANN to a baseline CNN model improves its accuracy from
70.00% to 80.22% on real-world test data, demonstrating the effectiveness of
domain adaptation in bridging the sim-to-real gap.

</details>


### [647] [Bridging Arbitrary and Tree Metrics via Differentiable Gromov Hyperbolicity](https://arxiv.org/pdf/2505.21073)
*Pierre Houedry, Nicolas Courty, Florestan Martin-Baillon, Laetitia Chapel, Titouan Vayer*

Main category: cs.LG

TL;DR: DeltaZero is a novel differentiable optimization framework for bridging arbitrary metrics to their closest tree metrics, outperforming existing methods in distortion.


<details>
  <summary>Details</summary>
Motivation: The need for reliable algorithms to quantify and minimize the deviation of arbitrary metrics from tree metrics, as current methods are either heuristic or lack guarantees.

Method: DeltaZero uses a smooth surrogate for Gromov's δ-hyperbolicity, enabling gradient-based optimization with tractable complexity and better worst-case guarantees.

Result: Experiments show DeltaZero achieves state-of-the-art distortion on synthetic and real-world datasets.

Conclusion: DeltaZero provides a statistically justified and efficient solution for approximating tree metrics, advancing the field of hierarchical data representation.

Abstract: Trees and the associated shortest-path tree metrics provide a powerful
framework for representing hierarchical and combinatorial structures in data.
Given an arbitrary metric space, its deviation from a tree metric can be
quantified by Gromov's $\delta$-hyperbolicity. Nonetheless, designing
algorithms that bridge an arbitrary metric to its closest tree metric is still
a vivid subject of interest, as most common approaches are either heuristical
and lack guarantees, or perform moderately well. In this work, we introduce a
novel differentiable optimization framework, coined DeltaZero, that solves this
problem. Our method leverages a smooth surrogate for Gromov's
$\delta$-hyperbolicity which enables a gradient-based optimization, with a
tractable complexity. The corresponding optimization procedure is derived from
a problem with better worst case guarantees than existing bounds, and is
justified statistically. Experiments on synthetic and real-world datasets
demonstrate that our method consistently achieves state-of-the-art distortion.

</details>


### [648] [Red-Teaming Text-to-Image Systems by Rule-based Preference Modeling](https://arxiv.org/pdf/2505.21074)
*Yichuan Cao, Yibo Miao, Xiao-Shan Gao, Yinpeng Dong*

Main category: cs.LG

TL;DR: The paper introduces RPG-RT, a rule-based method for red-teaming T2I models, using LLM to adapt prompts dynamically and evade unknown defenses.


<details>
  <summary>Details</summary>
Motivation: Addressing ethical concerns of T2I models by improving black-box red-teaming without needing internal model access or prior defense knowledge.

Method: Proposes RPG-RT: iterative LLM prompt modification guided by T2I feedback and rule-based preference modeling for finer-grained control.

Result: Tested on 19 T2I systems, 3 commercial APIs, and T2V models, showing superior adaptability and practicality.

Conclusion: RPG-RT effectively red-teams T2I models by dynamically adapting to unknown defenses, enhancing safety evaluation.

Abstract: Text-to-image (T2I) models raise ethical and safety concerns due to their
potential to generate inappropriate or harmful images. Evaluating these models'
security through red-teaming is vital, yet white-box approaches are limited by
their need for internal access, complicating their use with closed-source
models. Moreover, existing black-box methods often assume knowledge about the
model's specific defense mechanisms, limiting their utility in real-world
commercial API scenarios. A significant challenge is how to evade unknown and
diverse defense mechanisms. To overcome this difficulty, we propose a novel
Rule-based Preference modeling Guided Red-Teaming (RPG-RT), which iteratively
employs LLM to modify prompts to query and leverages feedback from T2I systems
for fine-tuning the LLM. RPG-RT treats the feedback from each iteration as a
prior, enabling the LLM to dynamically adapt to unknown defense mechanisms.
Given that the feedback is often labeled and coarse-grained, making it
difficult to utilize directly, we further propose rule-based preference
modeling, which employs a set of rules to evaluate desired or undesired
feedback, facilitating finer-grained control over the LLM's dynamic adaptation
process. Extensive experiments on nineteen T2I systems with varied safety
mechanisms, three online commercial API services, and T2V models verify the
superiority and practicality of our approach.

</details>


### [649] [Efficient Large Language Model Inference with Neural Block Linearization](https://arxiv.org/pdf/2505.21077)
*Mete Erdogan, Francesco Tonin, Volkan Cevher*

Main category: cs.LG

TL;DR: NBL accelerates transformer LLM inference by replacing self-attention layers with linear approximations, achieving speed-ups with minimal accuracy loss.


<details>
  <summary>Details</summary>
Motivation: High inference demands of transformer-based LLMs pose deployment challenges, requiring efficient solutions.

Method: NBL uses Linear Minimum Mean Squared Error estimators and Canonical Correlation Analysis to replace self-attention layers with linear approximations, selecting layers with the lowest error.

Result: NBL increases inference speed by 32% with <1% accuracy loss in experiments.

Conclusion: NBL is a flexible, efficient solution for improving LLM inference without fine-tuning.

Abstract: The high inference demands of transformer-based Large Language Models (LLMs)
pose substantial challenges in their deployment. To this end, we introduce
Neural Block Linearization (NBL), a novel framework for accelerating
transformer model inference by replacing self-attention layers with linear
approximations derived from Linear Minimum Mean Squared Error estimators. NBL
leverages Canonical Correlation Analysis to compute a theoretical upper bound
on the approximation error. Then, we use this bound as a criterion for
substitution, selecting the LLM layers with the lowest linearization error. NBL
can be efficiently applied to pre-trained LLMs without the need for
fine-tuning. In experiments, NBL achieves notable computational speed-ups while
preserving competitive accuracy on multiple reasoning benchmarks. For instance,
applying NBL to 12 self-attention layers in DeepSeek-R1-Distill-Llama-8B
increases the inference speed by 32% with less than 1% accuracy trade-off,
making it a flexible and promising solution to improve the inference efficiency
of LLMs.

</details>


### [650] [Improved Impossible Tuning and Lipschitz-Adaptive Universal Online Learning with Gradient Variations](https://arxiv.org/pdf/2505.21095)
*Kei Takemura, Ryuta Matsuno, Keita Sakuma*

Main category: cs.LG

TL;DR: A novel optimistic online mirror descent algorithm resolves the impossible tuning issue in online learning, enabling simultaneous optimal performance for gradient variation (GV), universal online learning (UOL), and Lipschitz adaptivity (LA).


<details>
  <summary>Details</summary>
Motivation: To address the challenge of achieving adaptivity to unknown problem characteristics (GV, UOL, LA) with optimal performance, overcoming limitations in existing meta-algorithms.

Method: Proposes an optimistic online mirror descent algorithm with an auxiliary initial round using large learning rates, enabling refined analysis to cancel excess regret factors.

Result: The algorithm resolves the impossible tuning issue up to loglog T factors and achieves state-of-the-art GV bounds and LA simultaneously.

Conclusion: The proposed algorithm overcomes prior limitations, resolving the conflict between LA mechanisms and GV regret analysis, marking a significant advancement in UOL.

Abstract: A central goal in online learning is to achieve adaptivity to unknown problem
characteristics, such as environmental changes captured by gradient variation
(GV), function curvature (universal online learning, UOL), and gradient scales
(Lipschitz adaptivity, LA). Simultaneously achieving these with optimal
performance is a major challenge, partly due to limitations in algorithms for
prediction with expert advice. These algorithms often serve as meta-algorithms
in online ensemble frameworks, and their sub-optimality hinders overall UOL
performance. Specifically, existing algorithms addressing the ``impossible
tuning'' issue incur an excess $\sqrt{\log T}$ factor in their regret bound
compared to the lower bound. To solve this problem, we propose a novel
optimistic online mirror descent algorithm with an auxiliary initial round
using large learning rates. This design enables a refined analysis where a
generated negative term cancels the gap-related factor, resolving the
impossible tuning issue up to $\log\log T$ factors. Leveraging our improved
algorithm as a meta-algorithm, we develop the first UOL algorithm that
simultaneously achieves state-of-the-art GV bounds and LA under standard
assumptions. Our UOL result overcomes key limitations of prior works, notably
resolving the conflict between LA mechanisms and regret analysis for GV bounds
-- an open problem highlighted by Xie et al.

</details>


### [651] [Conditional Diffusion Models with Classifier-Free Gibbs-like Guidance](https://arxiv.org/pdf/2505.21101)
*Badr Moufad, Yazid Janati, Alain Durmus, Ahmed Ghorbel, Eric Moulines, Jimmy Olsson*

Main category: cs.LG

TL;DR: The paper addresses the trade-off between quality and diversity in Classifier-Free Guidance (CFG) for diffusion models, proposing a corrected CFG method and a Gibbs-like sampling procedure to improve both aspects.


<details>
  <summary>Details</summary>
Motivation: CFG improves conditional diffusion models but reduces sample diversity, creating a trade-off. The paper aims to resolve this by correcting CFG and introducing a new sampling method.

Method: The authors identify a missing Rényi divergence term in CFG and propose a Gibbs-like sampling procedure to sample from the desired tilted distribution.

Result: The corrected CFG method and Gibbs-like sampling improve sample quality and diversity, outperforming standard CFG in image and text-to-audio generation tasks.

Conclusion: The proposed corrections and sampling method effectively balance quality and diversity in CFG, offering a practical solution for diffusion models.

Abstract: Classifier-Free Guidance (CFG) is a widely used technique for improving
conditional diffusion models by linearly combining the outputs of conditional
and unconditional denoisers. While CFG enhances visual quality and improves
alignment with prompts, it often reduces sample diversity, leading to a
challenging trade-off between quality and diversity. To address this issue, we
make two key contributions. First, CFG generally does not correspond to a
well-defined denoising diffusion model (DDM). In particular, contrary to common
intuition, CFG does not yield samples from the target distribution associated
with the limiting CFG score as the noise level approaches zero -- where the
data distribution is tilted by a power $w \gt 1$ of the conditional
distribution. We identify the missing component: a R\'enyi divergence term that
acts as a repulsive force and is required to correct CFG and render it
consistent with a proper DDM. Our analysis shows that this correction term
vanishes in the low-noise limit. Second, motivated by this insight, we propose
a Gibbs-like sampling procedure to draw samples from the desired tilted
distribution. This method starts with an initial sample from the conditional
diffusion model without CFG and iteratively refines it, preserving diversity
while progressively enhancing sample quality. We evaluate our approach on both
image and text-to-audio generation tasks, demonstrating substantial
improvements over CFG across all considered metrics. The code is available at
https://github.com/yazidjanati/cfgig

</details>


### [652] [Universal Value-Function Uncertainties](https://arxiv.org/pdf/2505.21119)
*Moritz A. Zanger, Max Weltevrede, Yaniv Oren, Pascal R. Van der Vaart, Caroline Horsch, Wendelin Böhmer, Matthijs T. J. Spaan*

Main category: cs.LG

TL;DR: UVU introduces a method for estimating epistemic uncertainty in value functions using a single-model approach, achieving performance comparable to deep ensembles with lower computational cost.


<details>
  <summary>Details</summary>
Motivation: Addressing the computational overhead of deep ensembles and the heuristic reliance of single-model methods in quantifying value uncertainty for RL tasks.

Method: UVU quantifies uncertainty as squared prediction errors between an online learner and a fixed, randomly initialized target network, trained via temporal difference learning with synthetic rewards.

Result: Theoretical analysis shows UVU errors match ensemble variance in infinite width; empirically, UVU matches ensemble performance in offline RL with lower cost.

Conclusion: UVU provides a computationally efficient and theoretically sound alternative to deep ensembles for value uncertainty estimation in RL.

Abstract: Estimating epistemic uncertainty in value functions is a crucial challenge
for many aspects of reinforcement learning (RL), including efficient
exploration, safe decision-making, and offline RL. While deep ensembles provide
a robust method for quantifying value uncertainty, they come with significant
computational overhead. Single-model methods, while computationally favorable,
often rely on heuristics and typically require additional propagation
mechanisms for myopic uncertainty estimates. In this work we introduce
universal value-function uncertainties (UVU), which, similar in spirit to
random network distillation (RND), quantify uncertainty as squared prediction
errors between an online learner and a fixed, randomly initialized target
network. Unlike RND, UVU errors reflect policy-conditional value uncertainty,
incorporating the future uncertainties any given policy may encounter. This is
due to the training procedure employed in UVU: the online network is trained
using temporal difference learning with a synthetic reward derived from the
fixed, randomly initialized target network. We provide an extensive theoretical
analysis of our approach using neural tangent kernel (NTK) theory and show that
in the limit of infinite network width, UVU errors are exactly equivalent to
the variance of an ensemble of independent universal value functions.
Empirically, we show that UVU achieves equal performance to large ensembles on
challenging multi-task offline RL settings, while offering simplicity and
substantial computational savings.

</details>


### [653] [Robust and Computation-Aware Gaussian Processes](https://arxiv.org/pdf/2505.21133)
*Marshal Arijona Sinaga, Julien Martinelli, Samuel Kaski*

Main category: cs.LG

TL;DR: RCaGP is a new Gaussian Process model combining robustness against outliers and computational efficiency, outperforming standard GPs in noisy and large-scale settings.


<details>
  <summary>Details</summary>
Motivation: Standard GPs struggle with large datasets and outliers, lacking both computational tractability and robustness. RCaGP aims to address these issues jointly.

Method: RCaGP integrates robust generalized Bayesian updating with approximation-aware uncertainty treatment, focusing on the interplay between robustness and approximation quality.

Result: RCaGP provides conservative uncertainty estimates, preserves robustness, and performs well in both clean and outlier-contaminated settings.

Conclusion: Jointly addressing robustness and approximation-awareness in GPs leads to superior performance in regression and Bayesian optimization tasks.

Abstract: Gaussian processes (GPs) are widely used for regression and optimization
tasks such as Bayesian optimization (BO) due to their expressiveness and
principled uncertainty estimates. However, in settings with large datasets
corrupted by outliers, standard GPs and their sparse approximations struggle
with computational tractability and robustness. We introduce Robust
Computation-aware Gaussian Process (RCaGP), a novel GP model that jointly
addresses these challenges by combining a principled treatment of
approximation-induced uncertainty with robust generalized Bayesian updating.
The key insight is that robustness and approximation-awareness are not
orthogonal but intertwined: approximations can exacerbate the impact of
outliers, and mitigating one without the other is insufficient. Unlike previous
work that focuses narrowly on either robustness or approximation quality, RCaGP
combines both in a principled and scalable framework, thus effectively managing
both outliers and computational uncertainties introduced by approximations such
as low-rank matrix multiplications. Our model ensures more conservative and
reliable uncertainty estimates, a property we rigorously demonstrate.
Additionally, we establish a robustness property and show that the mean
function is key to preserving it, motivating a tailored model selection scheme
for robust mean functions. Empirical results confirm that solving these
challenges jointly leads to superior performance across both clean and
outlier-contaminated settings, both on regression and high-throughput Bayesian
optimization benchmarks.

</details>


### [654] [Learning Single Index Models with Diffusion Priors](https://arxiv.org/pdf/2505.21135)
*Anqi Tang, Youming Chen, Shuchen Xue, Zhaoqiang Liu*

Main category: cs.LG

TL;DR: The paper proposes a method using diffusion models (DMs) for accurate signal recovery from semi-parametric single index models, addressing limitations of existing approaches.


<details>
  <summary>Details</summary>
Motivation: Existing DM-based signal recovery methods are either problem-specific or unable to handle nonlinear models with discontinuous/unknown link functions.

Method: The authors introduce an efficient reconstruction method requiring one round of unconditional sampling and partial inversion of DMs.

Result: Numerical experiments show the method yields more accurate reconstructions with fewer neural function evaluations than competitors.

Conclusion: The proposed DM-based approach effectively addresses challenges in nonlinear signal recovery, offering superior performance.

Abstract: Diffusion models (DMs) have demonstrated remarkable ability to generate
diverse and high-quality images by efficiently modeling complex data
distributions. They have also been explored as powerful generative priors for
signal recovery, resulting in a substantial improvement in the quality of
reconstructed signals. However, existing research on signal recovery with
diffusion models either focuses on specific reconstruction problems or is
unable to handle nonlinear measurement models with discontinuous or unknown
link functions. In this work, we focus on using DMs to achieve accurate
recovery from semi-parametric single index models, which encompass a variety of
popular nonlinear models that may have {\em discontinuous} and {\em unknown}
link functions. We propose an efficient reconstruction method that only
requires one round of unconditional sampling and (partial) inversion of DMs.
Theoretical analysis on the effectiveness of the proposed methods has been
established under appropriate conditions. We perform numerical experiments on
image datasets for different nonlinear measurement models. We observe that
compared to competing methods, our approach can yield more accurate
reconstructions while utilizing significantly fewer neural function
evaluations.

</details>


### [655] [SageAttention2++: A More Efficient Implementation of SageAttention2](https://arxiv.org/pdf/2505.21136)
*Jintao Zhang, Xiaoming Xu, Jia Wei, Haofeng Huang, Pengle Zhang, Chendong Xiang, Jun Zhu, Jianfei Chen*

Main category: cs.LG

TL;DR: SageAttention2++ improves attention efficiency by using FP8 Matmul with FP16 accumulation, achieving a 3.9x speedup over FlashAttention without accuracy loss.


<details>
  <summary>Details</summary>
Motivation: The quadratic time complexity of attention with sequence length necessitates more efficient methods.

Method: Utilizes FP8 Matmul accumulated in FP16 for faster matrix multiplications in attention.

Result: Achieves a 3.9x speedup over FlashAttention while maintaining accuracy.

Conclusion: SageAttention2++ effectively accelerates models across domains with minimal performance impact.

Abstract: The efficiency of attention is critical because its time complexity grows
quadratically with sequence length. SageAttention2 addresses this by utilizing
quantization to accelerate matrix multiplications (Matmul) in attention. To
further accelerate SageAttention2, we propose to utilize the faster instruction
of FP8 Matmul accumulated in FP16. The instruction is 2x faster than the FP8
Matmul used in SageAttention2. Our experiments show that SageAttention2++
achieves a 3.9x speedup over FlashAttention while maintaining the same
attention accuracy as SageAttention2. This means SageAttention2++ effectively
accelerates various models, including those for language, image, and video
generation, with negligible end-to-end metrics loss. The code will be available
at https://github.com/thu-ml/SageAttention.

</details>


### [656] [HeteroBA: A Structure-Manipulating Backdoor Attack on Heterogeneous Graphs](https://arxiv.org/pdf/2505.21140)
*Honglin Gao, Xiang Li, Lan Zhao, Gaoxi Xiao*

Main category: cs.LG

TL;DR: The paper introduces HeteroBA, a backdoor attack framework targeting heterogeneous graph neural networks (HGNNs), demonstrating high attack success with minimal clean data impact.


<details>
  <summary>Details</summary>
Motivation: Existing HGNN research focuses on performance, leaving robustness and security, particularly against backdoor attacks, underexplored.

Method: HeteroBA inserts trigger nodes with realistic features and structural connections, using attention and clustering to propagate triggers effectively.

Result: Experiments on three datasets show HeteroBA achieves high attack success rates without compromising clean data accuracy.

Conclusion: The study highlights HGNN vulnerabilities and urges robust defenses against backdoor attacks in multi-relational graphs.

Abstract: Heterogeneous graph neural networks (HGNNs) have recently drawn increasing
attention for modeling complex multi-relational data in domains such as
recommendation, finance, and social networks. While existing research has been
largely focused on enhancing HGNNs' predictive performance, their robustness
and security, especially under backdoor attacks, remain underexplored. In this
paper, we propose a novel Heterogeneous Backdoor Attack (HeteroBA) framework
for node classification tasks on heterogeneous graphs. HeteroBA inserts
carefully crafted trigger nodes with realistic features and targeted structural
connections, leveraging attention-based and clustering-based strategies to
select influential auxiliary nodes for effective trigger propagation, thereby
causing the model to misclassify specific nodes into a target label while
maintaining accuracy on clean data. Experimental results on three datasets and
various HGNN architectures demonstrate that HeteroBA achieves high attack
success rates with minimal impact on the clean accuracy. Our method sheds light
on potential vulnerabilities in HGNNs and calls for more robust defenses
against backdoor threats in multi-relational graph scenarios.

</details>


### [657] [A Predicting Phishing Websites Using Support Vector Machine and MultiClass Classification Based on Association Rule Techniques](https://arxiv.org/pdf/2505.21141)
*Nancy C. Woods, Virtue Ene Agada, Adebola K. Ojo*

Main category: cs.LG

TL;DR: The study integrates SVM and MCAR algorithms to improve phishing website detection, achieving 98.30% accuracy and 98% AUC.


<details>
  <summary>Details</summary>
Motivation: Phishing causes significant economic damage, but no consensus exists on the best detection algorithm. This research aims to combine SVM and MCAR for better prediction.

Method: Used 11,056 websites from PhishTank and Yahoo Directory. MCAR handled feature extraction and rule generation, while SVM performed classification and prediction.

Result: Achieved 98.30% accuracy, 98% AUC, and 82.84% variance in prediction. Computation time was 2205.33s with minimal error.

Conclusion: Combining SVM and MCAR enhances phishing detection accuracy, leveraging the strengths of both techniques.

Abstract: Phishing is a semantic attack which targets the user rather than the
computer. It is a new Internet crime in comparison with other forms such as
virus and hacking. Considering the damage phishing websites has caused to
various economies by collapsing organizations, stealing information and
financial diversion, various researchers have embarked on different ways of
detecting phishing websites but there has been no agreement about the best
algorithm to be used for prediction. This study is interested in integrating
the strengths of two algorithms, Support Vector Machines (SVM) and Multi-Class
Classification Rules based on Association Rules (MCAR) to establish a strong
and better means of predicting phishing websites. A total of 11,056 websites
were used from both PhishTank and yahoo directory to verify the effectiveness
of this approach. Feature extraction and rules generation were done by the MCAR
technique; classification and prediction were done by SVM technique. The result
showed that the technique achieved 98.30% classification accuracy with a
computation time of 2205.33s with minimum error rate. It showed a total of 98%
Area under the Curve (AUC) which showed the proportion of accuracy in
classifying phishing websites. The model showed 82.84% variance in the
prediction of phishing websites based on the coefficient of determination. The
use of two techniques together in detecting phishing websites produced a more
accurate result as it combined the strength of both techniques respectively.
This research work centralized on this advantage by building a hybrid of two
techniques to help produce a more accurate result.

</details>


### [658] [STEB: In Search of the Best Evaluation Approach for Synthetic Time Series](https://arxiv.org/pdf/2505.21160)
*Michael Stenger, Robert Leppich, André Bauer, Samuel Kounev*

Main category: cs.LG

TL;DR: STEB is a benchmark framework for comparing synthetic time series evaluation measures, assessing reliability and consistency across diverse datasets and transformations.


<details>
  <summary>Details</summary>
Motivation: The need for synthetic time series and the lack of objective comparison methods for evaluation measures.

Method: STEB uses 10 datasets, randomness injection, and 13 transformations to compute reliability and consistency indicators, tracking performance metrics.

Result: A ranking of 41 measures was determined, showing the impact of upstream time series embedding on scores.

Conclusion: STEB provides a comprehensive and interpretable benchmark for synthetic time series evaluation measures.

Abstract: The growing need for synthetic time series, due to data augmentation or
privacy regulations, has led to numerous generative models, frameworks, and
evaluation measures alike. Objectively comparing these measures on a large
scale remains an open challenge. We propose the Synthetic Time series
Evaluation Benchmark (STEB) -- the first benchmark framework that enables
comprehensive and interpretable automated comparisons of synthetic time series
evaluation measures. Using 10 diverse datasets, randomness injection, and 13
configurable data transformations, STEB computes indicators for measure
reliability and score consistency. It tracks running time, test errors, and
features sequential and parallel modes of operation. In our experiments, we
determine a ranking of 41 measures from literature and confirm that the choice
of upstream time series embedding heavily impacts the final score.

</details>


### [659] [Semi-Supervised Conformal Prediction With Unlabeled Nonconformity Score](https://arxiv.org/pdf/2505.21147)
*Xuanning Zhou, Hao Zeng, Xiaobo Xia, Bingyi Jing, Hongxin Wei*

Main category: cs.LG

TL;DR: SemiCP extends conformal prediction to semi-supervised settings, using labeled and unlabeled data for calibration, improving coverage guarantees and reducing prediction set size.


<details>
  <summary>Details</summary>
Motivation: Standard CP struggles with limited labeled data, leading to coverage deviation and large prediction sets. SemiCP addresses this by leveraging unlabeled data.

Method: Introduces NNM, a nonconformity score function for unlabeled data, integrating labeled data with similar pseudo-label scores into calibration.

Result: Theoretically and experimentally, SemiCP provides asymptotically valid coverage guarantees, reduces instability, and improves efficiency with limited data.

Conclusion: SemiCP effectively enhances CP in semi-supervised settings, adapting to conditional coverage and integrating with existing CP methods.

Abstract: Conformal prediction (CP) is a powerful framework for uncertainty
quantification, providing prediction sets with coverage guarantees when
calibrated on sufficient labeled data. However, in real-world applications
where labeled data is often limited, standard CP can lead to coverage deviation
and output overly large prediction sets. In this paper, we extend CP to the
semi-supervised setting and propose SemiCP, leveraging both labeled data and
unlabeled data for calibration. Specifically, we introduce a novel
nonconformity score function, NNM, designed for unlabeled data. This function
selects labeled data with similar pseudo-label scores to estimate nonconformity
scores, integrating them into the calibration process to overcome sample size
limitations. We theoretically demonstrate that, under mild assumptions, SemiCP
provide asymptotically coverage guarantee for prediction sets. Extensive
experiments further validate that our approach effectively reduces instability
and inefficiency under limited calibration data, can be adapted to conditional
coverage settings, and integrates seamlessly with existing CP methods.

</details>


### [660] [Latent label distribution grid representation for modeling uncertainty](https://arxiv.org/pdf/2505.21180)
*ShuNing Sun, YinSong Xiong, Yu Zhang, Zhuoran Zheng*

Main category: cs.LG

TL;DR: The paper proposes a Latent Label Distribution Grid (LLDG) to address uncertainty in Label Distribution Learning (LDL) caused by inexact labels, using Gaussian distributions and Tucker reconstruction for noise reduction.


<details>
  <summary>Details</summary>
Motivation: Label Distribution Learning (LDL) suffers from uncertainty due to inexact label annotations, leading to incorrect decisions.

Method: Constructs a label correlation matrix, expands it into Gaussian-distributed vectors to form LLDG, and reconstructs it using LLDG-Mixer with low-rank Tucker reconstruction.

Result: The approach performs competitively on benchmarks, demonstrating effectiveness in reducing label uncertainty.

Conclusion: LLDG effectively models label uncertainty and improves LDL accuracy, validated by experimental results.

Abstract: Although \textbf{L}abel \textbf{D}istribution \textbf{L}earning (LDL) has
promising representation capabilities for characterizing the polysemy of an
instance, the complexity and high cost of the label distribution annotation
lead to inexact in the construction of the label space. The existence of a
large number of inexact labels generates a label space with uncertainty, which
misleads the LDL algorithm to yield incorrect decisions. To alleviate this
problem, we model the uncertainty of label distributions by constructing a
\textbf{L}atent \textbf{L}abel \textbf{D}istribution \textbf{G}rid (LLDG) to
form a low-noise representation space. Specifically, we first construct a label
correlation matrix based on the differences between labels, and then expand
each value of the matrix into a vector that obeys a Gaussian distribution, thus
building a LLDG to model the uncertainty of the label space. Finally, the LLDG
is reconstructed by the LLDG-Mixer to generate an accurate label distribution.
Note that we enforce a customized low-rank scheme on this grid, which assumes
that the label relations may be noisy and it needs to perform noise-reduction
with the help of a Tucker reconstruction technique. Furthermore, we attempt to
evaluate the effectiveness of the LLDG by considering its generation as an
upstream task to achieve the classification of the objects. Extensive
experimental results show that our approach performs competitively on several
benchmarks.

</details>


### [661] [Learning What to Do and What Not To Do: Offline Imitation from Expert and Undesirable Demonstrations](https://arxiv.org/pdf/2505.21182)
*Huy Hoang, Tien Mai, Pradeep Varakantham, Tanvi Verma*

Main category: cs.LG

TL;DR: A novel offline imitation learning method leverages both expert and undesirable demonstrations, optimizing a difference of KL divergences, outperforming state-of-the-art baselines.


<details>
  <summary>Details</summary>
Motivation: Existing offline imitation learning overlooks undesirable behaviors, missing valuable signals. This work addresses this gap by incorporating contrasting behaviors.

Method: Proposes a DC program optimizing KL divergence differences between expert and undesirable state-action distributions, convex when expert data dominates.

Result: The method avoids adversarial training, unifies positive and negative demonstrations, and outperforms baselines in experiments.

Conclusion: The approach effectively utilizes contrasting behaviors, offering a stable and practical solution for offline imitation learning.

Abstract: Offline imitation learning typically learns from expert and unlabeled
demonstrations, yet often overlooks the valuable signal in explicitly
undesirable behaviors. In this work, we study offline imitation learning from
contrasting behaviors, where the dataset contains both expert and undesirable
demonstrations. We propose a novel formulation that optimizes a difference of
KL divergences over the state-action visitation distributions of expert and
undesirable (or bad) data. Although the resulting objective is a DC
(Difference-of-Convex) program, we prove that it becomes convex when expert
demonstrations outweigh undesirable demonstrations, enabling a practical and
stable non-adversarial training objective. Our method avoids adversarial
training and handles both positive and negative demonstrations in a unified
framework. Extensive experiments on standard offline imitation learning
benchmarks demonstrate that our approach consistently outperforms
state-of-the-art baselines.

</details>


### [662] [PoisonSwarm: Universal Harmful Information Synthesis via Model Crowdsourcing](https://arxiv.org/pdf/2505.21184)
*Yu Yan, Sheng Sun, Zhifei Zheng, Ziji Hao, Teli Liu, Min Liu*

Main category: cs.LG

TL;DR: PoisonSwarm, a novel framework, synthesizes diverse harmful data for AI testing by leveraging model crowdsourcing and dynamic model switching, outperforming existing methods in reliability and diversity.


<details>
  <summary>Details</summary>
Motivation: To address challenges in generating reliable and diverse harmful data due to LLM safety mechanisms, for better adversarial testing and safeguards in AI.

Method: Generates benign templates, decomposes them into semantic units, toxifies units dynamically, and refines the output using model crowdsourcing.

Result: Achieves state-of-the-art performance in synthesizing diverse harmful data with high scalability and success rates.

Conclusion: PoisonSwarm effectively overcomes LLM limitations, providing a scalable solution for harmful data synthesis in AI safety.

Abstract: To construct responsible and secure AI applications, harmful information data
is widely utilized for adversarial testing and the development of safeguards.
Existing studies mainly leverage Large Language Models (LLMs) to synthesize
data to obtain high-quality task datasets at scale, thereby avoiding costly
human annotation. However, limited by the safety alignment mechanisms of LLMs,
the synthesis of harmful data still faces challenges in generation reliability
and content diversity. In this study, we propose a novel harmful information
synthesis framework, PoisonSwarm, which applies the model crowdsourcing
strategy to generate diverse harmful data while maintaining a high success
rate. Specifically, we generate abundant benign data as the based templates in
a counterfactual manner. Subsequently, we decompose each based template into
multiple semantic units and perform unit-by-unit toxification and final
refinement through dynamic model switching, thus ensuring the success of
synthesis. Experimental results demonstrate that PoisonSwarm achieves
state-of-the-art performance in synthesizing different categories of harmful
data with high scalability and diversity.

</details>


### [663] [Crop recommendation with machine learning: leveraging environmental and economic factors for optimal crop selection](https://arxiv.org/pdf/2505.21201)
*Steven Sam, Silima Marshal DAbreo*

Main category: cs.LG

TL;DR: The study develops crop recommendation models using Random Forest and SVM, incorporating environmental and economic factors. While 10-fold CV showed high accuracy, temporal methods like Time-series Split and Lag Variables improved real-world applicability, with Random Forest performing best.


<details>
  <summary>Details</summary>
Motivation: Address low farm productivity in India by leveraging computational tools for accurate crop recommendations, overcoming limitations of existing narrow-focused systems.

Method: Used environmental and economic data for 19 crops across 15 states, evaluating Random Forest and SVM with 10-fold CV, Time-series Split, and Lag Variables.

Result: 10-fold CV had high accuracy (RF: 99.96%, SVM: 94.71%) but overfitting. Temporal methods reduced performance (Time-series Split: RF 78.55%, SVM 71.18%; Lag Variables: RF 83.62%, SVM 74.38%).

Conclusion: Random Forest with Lag Variables is optimal for crop recommendations in India, balancing accuracy and temporal adaptability.

Abstract: Agriculture constitutes a primary source of food production, economic growth
and employment in India, but the sector is confronted with low farm
productivity and yields aggravated by increased pressure on natural resources
and adverse climate change variability. Efforts involving green revolution,
land irrigations, improved seeds and organic farming have yielded suboptimal
outcomes. The adoption of computational tools like crop recommendation systems
offers a new way to provide insights and help farmers tackle low productivity.
However, most agricultural recommendation systems in India focus narrowly on
environmental factors and regions, limiting accurate predictions of high-yield,
profitable crops. This study uses environmental and economic factors with 19
crops across 15 states to develop and evaluate Random Forest and SVM models
using 10-fold Cross Validation, Time-series Split, and Lag Variables. The
10-fold cross validation showed high accuracy (RF: 99.96%, SVM: 94.71%) but
raised overfitting concerns. Introducing temporal order, better reflecting
real-world conditions, reduced performance (RF: 78.55%, SVM: 71.18%) in the
Time-series Split.To further increase the model accuracy while maintaining the
temporal order, the Lag Variables approach was employed, which resulted in
improved performance (RF: 83.62%, SVM: 74.38%) compared to the 10-fold cross
validation approach. Overall, the models in the Time-series Split and Lag
Variable Approaches offer practical insights by handling temporal dependencies
and enhancing its adaptability to changing agricultural conditions over time.
Consequently, the study shows the Random Forest model developed based on the
Lag Variables as the most preferred algorithm for optimal crop recommendation
in the Indian context.

</details>


### [664] [Developing hybrid mechanistic and data-driven personalized prediction models for platelet dynamics](https://arxiv.org/pdf/2505.21204)
*Marie Steinacker, Yuri Kheifetz, Markus Scholz*

Main category: cs.LG

TL;DR: Hybrid and data-driven models for predicting platelet counts during chemotherapy are compared, with data-driven methods excelling in high-data scenarios and hybrid models performing better with sparse data.


<details>
  <summary>Details</summary>
Motivation: Hematotoxicity from chemotherapy is unpredictable and varies between patients, necessitating better predictive models.

Method: Hybrid models (mechanistic + neural networks) and purely data-driven models (gated recurrent units) are developed and compared.

Result: Data-driven models improve accuracy with sufficient data, while hybrid models are better for sparse data.

Conclusion: The framework can be generalized for predicting other toxicities, aiding personalized medicine.

Abstract: Hematotoxicity, drug-induced damage to the blood-forming system, is a
frequent side effect of cytotoxic chemotherapy and poses a significant
challenge in clinical practice due to its high inter-patient variability and
limited predictability. Current mechanistic models often struggle to accurately
forecast outcomes for patients with irregular or atypical trajectories. In this
study, we develop and compare hybrid mechanistic and data-driven approaches for
individualized time series modeling of platelet counts during chemotherapy. We
consider hybrid models that combine mechanistic models with neural networks,
known as universal differential equations. As a purely data-driven alternative,
we utilize a nonlinear autoregressive exogenous model using gated recurrent
units as the underlying architecture. These models are evaluated across a range
of real patient scenarios, varying in data availability and sparsity, to assess
predictive performance. Our findings demonstrate that data-driven methods, when
provided with sufficient data, significantly improve prediction accuracy,
particularly for high-risk patients with irregular platelet dynamics. This
highlights the potential of data-driven approaches in enhancing clinical
decision-making. In contrast, hybrid and mechanistic models are superior in
scenarios with limited or sparse data. The proposed modeling and comparison
framework is generalizable and could be extended to predict other
treatment-related toxicities, offering broad applicability in personalized
medicine.

</details>


### [665] [Addressing Data Quality Decompensation in Federated Learning via Dynamic Client Selection](https://arxiv.org/pdf/2505.21219)
*Qinjun Fei, Nuria Rodríguez-Barroso, María Victoria Luzón, Zhongliang Zhang, Francisco Herrera*

Main category: cs.LG

TL;DR: SBRO-FL is a federated learning framework that integrates dynamic bidding, reputation modeling, and cost-aware selection to address client heterogeneity and improve model performance.


<details>
  <summary>Details</summary>
Motivation: Challenges in cross-silo FL include data quality decompensation, budget constraints, and incentive compatibility, which degrade global performance when treated in isolation.

Method: Proposes SBRO-FL, combining dynamic bidding (based on data quality), Shapley value-based contribution evaluation, and a reputation system. Client selection is a 0-1 integer program maximizing reputation-weighted utility under budget constraints.

Result: SBRO-FL improves accuracy, convergence speed, and robustness on datasets like FashionMNIST, EMNIST, CIFAR-10, and SVHN, even in adversarial scenarios.

Conclusion: Balancing data reliability, incentive compatibility, and cost efficiency is key for scalable and trustworthy FL deployments.

Abstract: In cross-silo Federated Learning (FL), client selection is critical to ensure
high model performance, yet it remains challenging due to data quality
decompensation, budget constraints, and incentive compatibility. As training
progresses, these factors exacerbate client heterogeneity and degrade global
performance. Most existing approaches treat these challenges in isolation,
making jointly optimizing multiple factors difficult. To address this, we
propose Shapley-Bid Reputation Optimized Federated Learning (SBRO-FL), a
unified framework integrating dynamic bidding, reputation modeling, and
cost-aware selection. Clients submit bids based on their perceived data
quality, and their contributions are evaluated using Shapley values to quantify
their marginal impact on the global model. A reputation system, inspired by
prospect theory, captures historical performance while penalizing
inconsistency. The client selection problem is formulated as a 0-1 integer
program that maximizes reputation-weighted utility under budget constraints.
Experiments on FashionMNIST, EMNIST, CIFAR-10, and SVHN datasets show that
SBRO-FL improves accuracy, convergence speed, and robustness, even in
adversarial and low-bid interference scenarios. Our results highlight the
importance of balancing data reliability, incentive compatibility, and cost
efficiency to enable scalable and trustworthy FL deployments.

</details>


### [666] [Why Do More Experts Fail? A Theoretical Analysis of Model Merging](https://arxiv.org/pdf/2505.21226)
*Zijing Wang, Xingle Xu, Yongkang Liu, Yiqun Zhang, Peiqin Lin, Shi Feng, Xiaocui Yang, Daling Wang, Hinrich Schütze*

Main category: cs.LG

TL;DR: The paper explores scalability limits in model merging, proving an upper bound and introducing a method (RHT) to enhance performance.


<details>
  <summary>Details</summary>
Motivation: To address the performance degradation in model merging as the number of merged models increases.

Method: Theoretical analysis using Gaussian Width and Approximate Kinematics Theory, plus the introduction of the Reparameterized Heavy-Tailed (RHT) method.

Result: Empirical validation on 12 benchmarks shows RHT improves merged model performance.

Conclusion: The study identifies scalability limits in model merging and proposes RHT as a solution, encouraging further research.

Abstract: Model merging dramatically reduces storage and computational resources by
combining multiple expert models into a single multi-task model. Although
recent model merging methods have shown promising results, they struggle to
maintain performance gains as the number of merged models increases. In this
paper, we investigate the key obstacles that limit the scalability of model
merging when integrating a large number of expert models. First, we prove that
there is an upper bound on model merging. Further theoretical analysis reveals
that the limited effective parameter space imposes a strict constraint on the
number of models that can be successfully merged. Gaussian Width shows that the
marginal benefit of merging additional models diminishes according to a
strictly concave function. This implies that the effective parameter space
becomes rapidly saturated as the number of merged models increases.
Furthermore, using Approximate Kinematics Theory, we prove the existence of a
unique optimal threshold beyond which adding more models does not yield
significant performance improvements. At the same time, we introduce a
straightforward Reparameterized Heavy-Tailed method (RHT) to extend the
coverage of the merged model, thereby enhancing its performance. Empirical
results on 12 benchmarks, including both knowledge-intensive and
general-purpose tasks, validate our theoretical analysis. We believe that these
results spark further research beyond the current scope of model merging. The
source code is in the anonymous Github repository
https://github.com/wzj1718/ModelMergingAnalysis.

</details>


### [667] [BindEnergyCraft: Casting Protein Structure Predictors as Energy-Based Models for Binder Design](https://arxiv.org/pdf/2505.21241)
*Divya Nori, Anisha Parsan, Caroline Uhler, Wengong Jin*

Main category: cs.LG

TL;DR: A new method, pTMEnergy, derived from predicted inter-residue error distributions, improves protein binder design by replacing traditional metrics with an energy-based objective, outperforming existing methods.


<details>
  <summary>Details</summary>
Motivation: Traditional metrics like ipTM do not reflect the statistical likelihood of binder-target complexes and provide sparse gradients for optimization.

Method: The paper introduces pTMEnergy, an energy function derived from structure predictors, integrated into the BECraft pipeline to replace ipTM.

Result: BECraft outperforms BindCraft, RFDiffusion, and ESM3, achieving higher success rates and fewer structural clashes.

Conclusion: pTMEnergy sets a new benchmark in structure-based virtual screening for miniprotein and RNA aptamer binders.

Abstract: Protein binder design has been transformed by hallucination-based methods
that optimize structure prediction confidence metrics, such as the interface
predicted TM-score (ipTM), via backpropagation. However, these metrics do not
reflect the statistical likelihood of a binder-target complex under the learned
distribution and yield sparse gradients for optimization. In this work, we
propose a method to extract such likelihoods from structure predictors by
reinterpreting their confidence outputs as an energy-based model (EBM). By
leveraging the Joint Energy-based Modeling (JEM) framework, we introduce
pTMEnergy, a statistical energy function derived from predicted inter-residue
error distributions. We incorporate pTMEnergy into BindEnergyCraft (BECraft), a
design pipeline that maintains the same optimization framework as BindCraft but
replaces ipTM with our energy-based objective. BECraft outperforms BindCraft,
RFDiffusion, and ESM3 across multiple challenging targets, achieving higher in
silico binder success rates while reducing structural clashes. Furthermore,
pTMEnergy establishes a new state-of-the-art in structure-based virtual
screening tasks for miniprotein and RNA aptamer binders.

</details>


### [668] [Copresheaf Topological Neural Networks: A Generalized Deep Learning Framework](https://arxiv.org/pdf/2505.21251)
*Mustafa Hajij, Lennart Bastian, Sarah Osentoski, Hardik Kabaria, John L. Davenport, Sheik Dawood, Balaji Cherukuri, Joseph G. Kocheemoolayil, Nastaran Shahmansouri, Adrian Lew, Theodore Papamarkou, Tolga Birdal*

Main category: cs.LG

TL;DR: CTNNs unify deep learning for structured data using copresheaves, outperforming baselines in tasks needing hierarchical or localized sensitivity.


<details>
  <summary>Details</summary>
Motivation: Addressing the challenge of designing neural architectures tailored to specific tasks and data types, leveraging algebraic topology for principled model design.

Method: Grounding model design in copresheaves, a concept from algebraic topology, to generalize and subsume existing deep learning models.

Result: Empirical results show CTNNs outperform conventional baselines, especially in tasks requiring hierarchical or localized sensitivity.

Conclusion: CTNNs provide a principled, multi-scale foundation for future deep learning architectures on structured data.

Abstract: We introduce copresheaf topological neural networks (CTNNs), a powerful and
unifying framework that encapsulates a wide spectrum of deep learning
architectures, designed to operate on structured data: including images, point
clouds, graphs, meshes, and topological manifolds. While deep learning has
profoundly impacted domains ranging from digital assistants to autonomous
systems, the principled design of neural architectures tailored to specific
tasks and data types remains one of the field's most persistent open
challenges. CTNNs address this gap by grounding model design in the language of
copresheaves, a concept from algebraic topology that generalizes and subsumes
most practical deep learning models in use today. This abstract yet
constructive formulation yields a rich design space from which theoretically
sound and practically effective solutions can be derived to tackle core
challenges in representation learning: long-range dependencies, oversmoothing,
heterophily, and non-Euclidean domains. Our empirical results on structured
data benchmarks demonstrate that CTNNs consistently outperform conventional
baselines, particularly in tasks requiring hierarchical or localized
sensitivity. These results underscore CTNNs as a principled, multi-scale
foundation for the next generation of deep learning architectures.

</details>


### [669] [Learnable Kernel Density Estimation for Graphs](https://arxiv.org/pdf/2505.21285)
*Xudong Wang, Ziheng Sun, Chris Ding, Jicong Fan*

Main category: cs.LG

TL;DR: LGKDE is a framework for graph density estimation using graph neural networks and maximum mean discrepancy, outperforming baselines in anomaly detection.


<details>
  <summary>Details</summary>
Motivation: Existing graph density estimation methods rely on fixed kernel features, limiting performance. LGKDE aims to improve this by learning adaptive features.

Method: LGKDE uses graph neural networks to represent graphs as distributions and learns metrics via maximum mean discrepancy, with perturbations on node features and spectra.

Result: LGKDE shows superior performance in synthetic and real-world benchmarks, with theoretical guarantees on error and robustness.

Conclusion: LGKDE effectively learns graph densities, offering improved performance and theoretical soundness for tasks like anomaly detection.

Abstract: This work proposes a framework LGKDE that learns kernel density estimation
for graphs. The key challenge in graph density estimation lies in effectively
capturing both structural patterns and semantic variations while maintaining
theoretical guarantees. Combining graph kernels and kernel density estimation
(KDE) is a standard approach to graph density estimation, but has
unsatisfactory performance due to the handcrafted and fixed features of
kernels. Our method LGKDE leverages graph neural networks to represent each
graph as a discrete distribution and utilizes maximum mean discrepancy to learn
the graph metric for multi-scale KDE, where all parameters are learned by
maximizing the density of graphs relative to the density of their well-designed
perturbed counterparts. The perturbations are conducted on both node features
and graph spectra, which helps better characterize the boundary of normal
density regions. Theoretically, we establish consistency and convergence
guarantees for LGKDE, including bounds on the mean integrated squared error,
robustness, and complexity. We validate LGKDE by demonstrating its
effectiveness in recovering the underlying density of synthetic graph
distributions and applying it to graph anomaly detection across diverse
benchmark datasets. Extensive empirical evaluation shows that LGKDE
demonstrates superior performance compared to state-of-the-art baselines on
most benchmark datasets.

</details>


### [670] [GSAT: Graph Structure Attention Networks](https://arxiv.org/pdf/2505.21288)
*Farshad Noravesh, Reza Haffari, Layki Soon, Arghya Pal*

Main category: cs.LG

TL;DR: GSAT integrates structural info via anonymous random walks to enhance GNN performance, slightly improving SOTA on graph classification benchmarks.


<details>
  <summary>Details</summary>
Motivation: Existing GNNs overlook rich local topological info, leading to issues like oversmoothing. Leveraging structural info can improve performance.

Method: Propose GSAT, a GAT generalization, using anonymous random walks to model structural info and integrate it with node attributes.

Result: GSAT slightly improves state-of-the-art performance on graph classification benchmarks.

Conclusion: Incorporating structural info via GSAT enhances GNN performance, addressing oversmoothing and improving graph representation.

Abstract: Graph Neural Networks (GNNs) have emerged as a powerful tool for processing
data represented in graph structures, achieving remarkable success across a
wide range of applications. However, to further improve the performance on
graph classification benchmarks, structural representation of each node that
encodes rich local topological information in the neighbourhood of nodes is an
important type of feature that is often overlooked in the modeling. The
consequence of neglecting the structural information has resulted high number
of layers to connect messages from distant nodes which by itself produces other
problems such as oversmoothing. In the present paper, we leverage these
structural information that are modeled by anonymous random walks (ARWs) and
introduce graph structure attention network (GSAT) which is a generalization of
graph attention network(GAT) to integrate the original attribute and the
structural representation to enforce the model to automatically find patterns
for attending to different edges in the node neighbourhood to enrich graph
representation. Our experiments show GSAT slightly improves SOTA on some graph
classification benchmarks.

</details>


### [671] [LoFT: Low-Rank Adaptation That Behaves Like Full Fine-Tuning](https://arxiv.org/pdf/2505.21289)
*Nurbek Tastan, Stefanos Laskaridis, Martin Takac, Karthik Nandakumar, Samuel Horvath*

Main category: cs.LG

TL;DR: LoFT improves LoRA by aligning optimizer dynamics with full fine-tuning, narrowing performance gaps without extra hyperparameters or inference cost.


<details>
  <summary>Details</summary>
Motivation: LoRA reduces trainable parameters but underperforms full fine-tuning in accuracy and convergence speed.

Method: LoFT aligns optimizer's first and second moments (Adam's momentum and variance) with low-rank updates, mirroring full-model updates.

Result: LoFT outperforms LoRA, narrowing the performance gap with full fine-tuning without increasing inference cost.

Conclusion: LoFT offers a more efficient and effective alternative to LoRA, matching full fine-tuning dynamics without added complexity.

Abstract: Large pre-trained models are commonly adapted to downstream tasks using
parameter-efficient fine-tuning methods such as Low-Rank Adaptation (LoRA),
which injects small trainable low-rank matrices instead of updating all
weights. While LoRA dramatically reduces trainable parameters with little
overhead, it can still underperform full fine-tuning in accuracy and often
converges more slowly. We introduce LoFT, a novel low-rank adaptation method
that behaves like full fine-tuning by aligning the optimizer's internal
dynamics with those of updating all model weights. LoFT not only learns weight
updates in a low-rank subspace (like LoRA) but also properly projects the
optimizer's first and second moments (Adam's momentum and variance) into the
same subspace, mirroring full-model updates. By aligning the low-rank update
itself with the full update, LoFT eliminates the need for tuning extra
hyperparameters, e.g., LoRA scaling factor $\alpha$. Empirically, this approach
substantially narrows the performance gap between adapter-based tuning and full
fine-tuning and consistently outperforms standard LoRA-style methods, all
without increasing inference cost.

</details>


### [672] [A Cross Modal Knowledge Distillation & Data Augmentation Recipe for Improving Transcriptomics Representations through Morphological Features](https://arxiv.org/pdf/2505.21317)
*Ihab Bendidi, Yassir El Mesbahi, Alisandra K. Denton, Karush Suri, Kian Kenyon-Dean, Auguste Genovesio, Emmanuel Noutahi*

Main category: cs.LG

TL;DR: A framework enhances transcriptomics by distilling knowledge from microscopy images using weakly paired data, improving predictive power and interpretability.


<details>
  <summary>Details</summary>
Motivation: To address the scarcity of weakly paired datasets and enhance transcriptomics with morphological insights from microscopy.

Method: Proposes Semi-Clipped (adapted CLIP for cross-modal distillation) and PEA (augmentation for transcriptomics).

Result: Achieves state-of-the-art results, enriching gene expression representations with morphological data.

Conclusion: The framework improves transcriptomics' predictive power and interpretability for complex biological tasks.

Abstract: Understanding cellular responses to stimuli is crucial for biological
discovery and drug development. Transcriptomics provides interpretable,
gene-level insights, while microscopy imaging offers rich predictive features
but is harder to interpret. Weakly paired datasets, where samples share
biological states, enable multimodal learning but are scarce, limiting their
utility for training and multimodal inference. We propose a framework to
enhance transcriptomics by distilling knowledge from microscopy images. Using
weakly paired data, our method aligns and binds modalities, enriching gene
expression representations with morphological information. To address data
scarcity, we introduce (1) Semi-Clipped, an adaptation of CLIP for cross-modal
distillation using pretrained foundation models, achieving state-of-the-art
results, and (2) PEA (Perturbation Embedding Augmentation), a novel
augmentation technique that enhances transcriptomics data while preserving
inherent biological information. These strategies improve the predictive power
and retain the interpretability of transcriptomics, enabling rich unimodal
representations for complex biological tasks.

</details>


### [673] [Bencher: Simple and Reproducible Benchmarking for Black-Box Optimization](https://arxiv.org/pdf/2505.21321)
*Leonard Papenmeier, Luigi Nardi*

Main category: cs.LG

TL;DR: Bencher is a modular benchmarking framework for black-box optimization, isolating benchmarks in virtual Python environments and using RPC for integration.


<details>
  <summary>Details</summary>
Motivation: To address dependency conflicts and simplify integration of diverse benchmarks with complex requirements.

Method: Decouples benchmark execution from optimization logic, using isolated virtual environments and a unified RPC interface. Supports local, Docker, or HPC deployment.

Result: Enables reproducible runtime for 80 benchmarks across various domains with minimal setup.

Conclusion: Bencher provides a scalable, containerized solution for benchmarking in black-box optimization.

Abstract: We present Bencher, a modular benchmarking framework for black-box
optimization that fundamentally decouples benchmark execution from optimization
logic. Unlike prior suites that focus on combining many benchmarks in a single
project, Bencher introduces a clean abstraction boundary: each benchmark is
isolated in its own virtual Python environment and accessed via a unified,
version-agnostic remote procedure call (RPC) interface. This design eliminates
dependency conflicts and simplifies the integration of diverse, real-world
benchmarks, which often have complex and conflicting software requirements.
Bencher can be deployed locally or remotely via Docker or on high-performance
computing (HPC) clusters via Singularity, providing a containerized,
reproducible runtime for any benchmark. Its lightweight client requires minimal
setup and supports drop-in evaluation of 80 benchmarks across continuous,
categorical, and binary domains.

</details>


### [674] [UGCE: User-Guided Incremental Counterfactual Exploration](https://arxiv.org/pdf/2505.21330)
*Christos Fragkathoulas, Evaggelia Pitoura*

Main category: cs.LG

TL;DR: UGCE is a genetic algorithm-based framework for dynamically updating counterfactual explanations as user constraints evolve, improving efficiency and solution quality over static methods.


<details>
  <summary>Details</summary>
Motivation: Existing counterfactual explanation methods are inefficient and rigid, failing to adapt to iterative user constraint updates.

Method: Proposes UGCE, a genetic algorithm-based framework for incremental counterfactual updates in response to evolving constraints.

Result: UGCE significantly improves computational efficiency and maintains high-quality solutions across five benchmark datasets.

Conclusion: UGCE supports stable performance under varying constraints, benefits from warm-starting, and reveals constraint-specific search behaviors.

Abstract: Counterfactual explanations (CFEs) are a popular approach for interpreting
machine learning predictions by identifying minimal feature changes that alter
model outputs. However, in real-world settings, users often refine feasibility
constraints over time, requiring counterfactual generation to adapt
dynamically. Existing methods fail to support such iterative updates, instead
recomputing explanations from scratch with each change, an inefficient and
rigid approach. We propose User-Guided Incremental Counterfactual Exploration
(UGCE), a genetic algorithm-based framework that incrementally updates
counterfactuals in response to evolving user constraints. Experimental results
across five benchmark datasets demonstrate that UGCE significantly improves
computational efficiency while maintaining high-quality solutions compared to a
static, non-incremental approach. Our evaluation further shows that UGCE
supports stable performance under varying constraint sequences, benefits from
an efficient warm-start strategy, and reveals how different constraint types
may affect search behavior.

</details>


### [675] [Joint Learning in the Gaussian Single Index Model](https://arxiv.org/pdf/2505.21336)
*Loucas Pillaud-Vivien, Adrien Schertzer*

Main category: cs.LG

TL;DR: The paper studies joint learning of a projection direction and a univariate function in high-dimensional Gaussian models, analyzing gradient flow dynamics and proving convergence with a rate tied to the function's Gaussian regularity.


<details>
  <summary>Details</summary>
Motivation: The work addresses a fundamental non-convex problem at the intersection of representation learning and nonlinear regression, aiming to understand and improve learning in high-dimensional settings.

Method: The authors analyze gradient flow dynamics of an alternating scheme and use a Reproducing Kernel Hilbert Space (RKHS) for efficient implementation.

Result: Convergence is proven even with negatively correlated initial directions, and practical implementation via RKHS is demonstrated.

Conclusion: The study provides theoretical and practical advancements for learning low-dimensional structure in high-dimensional data.

Abstract: We consider the problem of jointly learning a one-dimensional projection and
a univariate function in high-dimensional Gaussian models. Specifically, we
study predictors of the form $f(x)=\varphi^\star(\langle w^\star, x \rangle)$,
where both the direction $w^\star \in \mathcal{S}_{d-1}$, the sphere of
$\mathbb{R}^d$, and the function $\varphi^\star: \mathbb{R} \to \mathbb{R}$ are
learned from Gaussian data. This setting captures a fundamental non-convex
problem at the intersection of representation learning and nonlinear
regression. We analyze the gradient flow dynamics of a natural alternating
scheme and prove convergence, with a rate controlled by the information
exponent reflecting the \textit{Gaussian regularity} of the function
$\varphi^\star$. Strikingly, our analysis shows that convergence still occurs
even when the initial direction is negatively correlated with the target. On
the practical side, we demonstrate that such joint learning can be effectively
implemented using a Reproducing Kernel Hilbert Space (RKHS) adapted to the
structure of the problem, enabling efficient and flexible estimation of the
univariate function. Our results offer both theoretical insight and practical
methodology for learning low-dimensional structure in high-dimensional
settings.

</details>


### [676] [An Uncertainty-Aware ED-LSTM for Probabilistic Suffix Prediction](https://arxiv.org/pdf/2505.21339)
*Henryk Mustroph, Michel Kunkler, Stefanie Rinderle-Ma*

Main category: cs.LG

TL;DR: Proposes probabilistic suffix prediction for business processes using U-ED-LSTM and MC sampling to handle uncertainty, outperforming single-prediction methods.


<details>
  <summary>Details</summary>
Motivation: Current suffix prediction methods are limited to single predictions, which fail under uncertainty or high variability.

Method: Uses Uncertainty-Aware Encoder-Decoder LSTM (U-ED-LSTM) with MC dropout for epistemic uncertainty and learned loss attenuation for aleatoric uncertainty.

Result: U-ED-LSTM shows reasonable performance, mean predictions outperform single predictions for rare/long suffixes, and effectively captures uncertainties.

Conclusion: Probabilistic suffix prediction improves expressiveness and performance in uncertain or variable business processes.

Abstract: Suffix prediction of business processes forecasts the remaining sequence of
events until process completion. Current approaches focus on predicting a
single, most likely suffix. However, if the future course of a process is
exposed to uncertainty or has high variability, the expressiveness of a single
suffix prediction can be limited. To address this limitation, we propose
probabilistic suffix prediction, a novel approach that approximates a
probability distribution of suffixes. The proposed approach is based on an
Uncertainty-Aware Encoder-Decoder LSTM (U-ED-LSTM) and a Monte Carlo (MC)
suffix sampling algorithm. We capture epistemic uncertainties via MC dropout
and aleatoric uncertainties as learned loss attenuation. This technical report
provides a detailed evaluation of the U-ED-LSTM's predictive performance and
assesses its calibration on four real-life event logs with three different
hyperparameter settings. The results show that i) the U-ED-LSTM has reasonable
predictive performance across various datasets, ii) aggregating probabilistic
suffix predictions into mean values can outperform most likely predictions,
particularly for rare prefixes or longer suffixes, and iii) the approach
effectively captures uncertainties present in event logs.

</details>


### [677] [OVERT: A Benchmark for Over-Refusal Evaluation on Text-to-Image Models](https://arxiv.org/pdf/2505.21347)
*Ziheng Cheng, Yixiao Huang, Hui Xu, Somayeh Sojoudi, Xuandong Zhao, Dawn Song, Song Mei*

Main category: cs.LG

TL;DR: The paper introduces OVERT, a benchmark for evaluating over-refusal in T2I models, revealing widespread issues and exploring prompt rewriting as a mitigation strategy.


<details>
  <summary>Details</summary>
Motivation: Over-refusal in T2I models reduces their utility, but lacks systematic evaluation. The paper aims to address this gap.

Method: An automatic workflow constructs synthetic evaluation data (OVERT), including benign and harmful prompts, to assess over-refusal.

Result: OVERT reveals widespread over-refusal in T2I models, highlighting a safety-utility trade-off. Prompt rewriting is explored but found unfaithful.

Conclusion: OVERT is a flexible tool for evaluating T2I models, emphasizing the need for better safety alignment without sacrificing functionality.

Abstract: Text-to-Image (T2I) models have achieved remarkable success in generating
visual content from text inputs. Although multiple safety alignment strategies
have been proposed to prevent harmful outputs, they often lead to overly
cautious behavior -- rejecting even benign prompts -- a phenomenon known as
$\textit{over-refusal}$ that reduces the practical utility of T2I models.
Despite over-refusal having been observed in practice, there is no large-scale
benchmark that systematically evaluates this phenomenon for T2I models. In this
paper, we present an automatic workflow to construct synthetic evaluation data,
resulting in OVERT ($\textbf{OVE}$r-$\textbf{R}$efusal evaluation on
$\textbf{T}$ext-to-image models), the first large-scale benchmark for assessing
over-refusal behaviors in T2I models. OVERT includes 4,600 seemingly harmful
but benign prompts across nine safety-related categories, along with 1,785
genuinely harmful prompts (OVERT-unsafe) to evaluate the safety-utility
trade-off. Using OVERT, we evaluate several leading T2I models and find that
over-refusal is a widespread issue across various categories (Figure 1),
underscoring the need for further research to enhance the safety alignment of
T2I models without compromising their functionality.As a preliminary attempt to
reduce over-refusal, we explore prompt rewriting; however, we find it often
compromises faithfulness to the meaning of the original prompts. Finally, we
demonstrate the flexibility of our generation framework in accommodating
diverse safety requirements by generating customized evaluation data adapting
to user-defined policies.

</details>


### [678] [CRISP-NAM: Competing Risks Interpretable Survival Prediction with Neural Additive Models](https://arxiv.org/pdf/2505.21360)
*Dhanesh Ramachandram*

Main category: cs.LG

TL;DR: CRISP-NAM is an interpretable neural additive model for competing risks survival analysis, preserving feature-level interpretability while modeling cause-specific hazards.


<details>
  <summary>Details</summary>
Motivation: Competing risks are critical in survival modeling, especially in healthcare, where multiple event types can occur. Existing methods lack interpretability for complex non-linear relationships.

Method: Extends neural additive models to competing risks by using dedicated neural networks for each feature, enabling independent risk estimation and visualization of covariate effects.

Result: Demonstrates competitive performance on multiple datasets compared to existing approaches.

Conclusion: CRISP-NAM provides interpretable and accurate survival predictions for competing risks, addressing a gap in current methods.

Abstract: Competing risks are crucial considerations in survival modelling,
particularly in healthcare domains where patients may experience multiple
distinct event types. We propose CRISP-NAM (Competing Risks Interpretable
Survival Prediction with Neural Additive Models), an interpretable neural
additive model for competing risks survival analysis which extends the neural
additive architecture to model cause-specific hazards while preserving
feature-level interpretability. Each feature contributes independently to risk
estimation through dedicated neural networks, allowing for visualization of
complex non-linear relationships between covariates and each competing risk. We
demonstrate competitive performance on multiple datasets compared to existing
approaches.

</details>


### [679] [Subgroups Matter for Robust Bias Mitigation](https://arxiv.org/pdf/2505.21363)
*Anissa Alloula, Charles Jones, Ben Glocker, Bartłomiej W. Papież*

Main category: cs.LG

TL;DR: Subgroup definition significantly impacts bias mitigation effectiveness, sometimes worsening outcomes. Careful subgroup choice is crucial for fairness.


<details>
  <summary>Details</summary>
Motivation: To understand why bias mitigation methods fail, focusing on the overlooked role of subgroup definitions.

Method: Comprehensive evaluation of bias mitigation methods across vision and language tasks, varying subgroup definitions.

Result: Subgroup choice affects performance; some groupings worsen outcomes. Improving fairness may require using different subgroups.

Conclusion: Subgroup definition is critical for effective bias mitigation and fairness in machine learning.

Abstract: Despite the constant development of new bias mitigation methods for machine
learning, no method consistently succeeds, and a fundamental question remains
unanswered: when and why do bias mitigation techniques fail? In this paper, we
hypothesise that a key factor may be the often-overlooked but crucial step
shared by many bias mitigation methods: the definition of subgroups. To
investigate this, we conduct a comprehensive evaluation of state-of-the-art
bias mitigation methods across multiple vision and language classification
tasks, systematically varying subgroup definitions, including coarse,
fine-grained, intersectional, and noisy subgroups. Our results reveal that
subgroup choice significantly impacts performance, with certain groupings
paradoxically leading to worse outcomes than no mitigation at all. Our findings
suggest that observing a disparity between a set of subgroups is not a
sufficient reason to use those subgroups for mitigation. Through theoretical
analysis, we explain these phenomena and uncover a counter-intuitive insight
that, in some cases, improving fairness with respect to a particular set of
subgroups is best achieved by using a different set of subgroups for
mitigation. Our work highlights the importance of careful subgroup definition
in bias mitigation and suggest it as a alternative lever for improving the
robustness and fairness of machine learning models.

</details>


### [680] [Towards Interpretability Without Sacrifice: Faithful Dense Layer Decomposition with Mixture of Decoders](https://arxiv.org/pdf/2505.21364)
*James Oldfield, Shawn Im, Yixuan Li, Mihalis A. Nicolaou, Ioannis Patras, Grigorios G Chrysos*

Main category: cs.LG

TL;DR: The paper introduces Mixture of Decoders (MxDs) to improve interpretability and accuracy in sparse approximations of MLPs in large language models, outperforming existing methods.


<details>
  <summary>Details</summary>
Motivation: Dense representations in MLPs are hard to understand and edit. Existing sparse approximations sacrifice accuracy, motivating a layer-level sparsity approach.

Method: Proposes MxDs, which expand dense layers into specialized sublayers via tensor factorization, preserving expressive capacity under sparsity.

Result: MxDs outperform state-of-the-art methods (e.g., Transcoders) on sparsity-accuracy trade-offs in models up to 3B parameters.

Conclusion: MxDs offer a promising approach for interpretable yet accurate decompositions in language models, validated by sparse probing and feature steering.

Abstract: Multilayer perceptrons (MLPs) are an integral part of large language models,
yet their dense representations render them difficult to understand, edit, and
steer. Recent methods learn interpretable approximations via neuron-level
sparsity, yet fail to faithfully reconstruct the original
mapping--significantly increasing model's next-token cross-entropy loss. In
this paper, we advocate for moving to layer-level sparsity to overcome the
accuracy trade-off in sparse layer approximation. Under this paradigm, we
introduce Mixture of Decoders (MxDs). MxDs generalize MLPs and Gated Linear
Units, expanding pre-trained dense layers into tens of thousands of specialized
sublayers. Through a flexible form of tensor factorization, each sparsely
activating MxD sublayer implements a linear transformation with full-rank
weights--preserving the original decoders' expressive capacity even under heavy
sparsity. Experimentally, we show that MxDs significantly outperform
state-of-the-art methods (e.g., Transcoders) on the sparsity-accuracy frontier
in language models with up to 3B parameters. Further evaluations on sparse
probing and feature steering demonstrate that MxDs learn similarly specialized
features of natural language--opening up a promising new avenue for designing
interpretable yet faithful decompositions. Our code is included at:
https://github.com/james-oldfield/MxD/.

</details>


### [681] [Improving LLM-based Global Optimization with Search Space Partitioning](https://arxiv.org/pdf/2505.21372)
*Andrej Schwanke, Lyubomir Ivanov, David Salinas, Fabio Ferreira, Aaron Klein, Frank Hutter, Arber Zela*

Main category: cs.LG

TL;DR: HOLLM improves LLM-based global optimization by partitioning the search space and using a bandit-inspired scoring mechanism, outperforming existing methods.


<details>
  <summary>Details</summary>
Motivation: LLM-based methods struggle in high-dimensional spaces or without domain-specific priors, leading to poor suggestions.

Method: HOLLM partitions the search space into subregions (meta-arms) selected via a bandit-inspired scoring mechanism, then uses LLMs to propose candidates within each.

Result: HOLLM matches or surpasses Bayesian optimization and trust-region methods, outperforming global LLM-based sampling.

Conclusion: HOLLM effectively enhances LLM-driven optimization, addressing limitations of sparse or uninformative suggestions.

Abstract: Large Language Models (LLMs) have recently emerged as effective surrogate
models and candidate generators within global optimization frameworks for
expensive blackbox functions. Despite promising results, LLM-based methods
often struggle in high-dimensional search spaces or when lacking
domain-specific priors, leading to sparse or uninformative suggestions. To
overcome these limitations, we propose HOLLM, a novel global optimization
algorithm that enhances LLM-driven sampling by partitioning the search space
into promising subregions. Each subregion acts as a ``meta-arm'' selected via a
bandit-inspired scoring mechanism that effectively balances exploration and
exploitation. Within each selected subregion, an LLM then proposes high-quality
candidate points, without any explicit domain knowledge. Empirical evaluation
on standard optimization benchmarks shows that HOLLM consistently matches or
surpasses leading Bayesian optimization and trust-region methods, while
substantially outperforming global LLM-based sampling strategies.

</details>


### [682] [PLANETALIGN: A Comprehensive Python Library for Benchmarking Network Alignment](https://arxiv.org/pdf/2505.21366)
*Qi Yu, Zhichen Zeng, Yuchen Yan, Zhining Liu, Baoyu Jing, Ruizhong Qiu, Ariful Azad, Hanghang Tong*

Main category: cs.LG

TL;DR: PLANETALIGN is a Python library for network alignment (NA), offering datasets, methods, and evaluation tools to streamline NA research and benchmarking.


<details>
  <summary>Details</summary>
Motivation: There's no comprehensive library for NA development and benchmarking, hindering progress in multi-network learning tasks.

Method: PLANETALIGN integrates 18 datasets, 14 NA methods, and standardized evaluation metrics for systematic assessment.

Result: Comparative studies highlight strengths and limitations of existing NA methods, providing practical insights.

Conclusion: PLANETALIGN aims to advance NA research by facilitating development and benchmarking of more effective methods.

Abstract: Network alignment (NA) aims to identify node correspondence across different
networks and serves as a critical cornerstone behind various downstream
multi-network learning tasks. Despite growing research in NA, there lacks a
comprehensive library that facilitates the systematic development and
benchmarking of NA methods. In this work, we introduce PLANETALIGN, a
comprehensive Python library for network alignment that features a rich
collection of built-in datasets, methods, and evaluation pipelines with
easy-to-use APIs. Specifically, PLANETALIGN integrates 18 datasets and 14 NA
methods with extensible APIs for easy use and development of NA methods. Our
standardized evaluation pipeline encompasses a wide range of metrics, enabling
a systematic assessment of the effectiveness, scalability, and robustness of NA
methods. Through extensive comparative studies, we reveal practical insights
into the strengths and limitations of existing NA methods. We hope that
PLANETALIGN can foster a deeper understanding of the NA problem and facilitate
the development and benchmarking of more effective, scalable, and robust
methods in the future. The source code of PLANETALIGN is available at
https://github.com/yq-leo/PlanetAlign.

</details>


### [683] [DeCAF: Decentralized Consensus-And-Factorization for Low-Rank Adaptation of Foundation Models](https://arxiv.org/pdf/2505.21382)
*Nastaran Saadati, Zhanhong Jiang, Joshua R. Waite, Shreyan Ganguly, Aditya Balu, Chinmay Hegde, Soumik Sarkar*

Main category: cs.LG

TL;DR: The paper improves decentralized LoRA (DLoRA) convergence by ensuring gradient smoothness and introduces DeCAF, a novel algorithm combining DLoRA with TSVD to resolve consensus interference, achieving performance rivaling federated learning.


<details>
  <summary>Details</summary>
Motivation: LoRA is effective for fine-tuning VLMs and LLMs, but its decentralized adaptation lacks theoretical guarantees due to gradient smoothness and consensus interference issues.

Method: The work enhances DLoRA's convergence rate to match decentralized SGD and introduces DeCAF, integrating DLoRA with TSVD-based matrix factorization to address consensus interference.

Result: Theoretical analysis shows bounded TSVD approximation error and vanishing consensus differences, with DeCAF matching decentralized SGD's convergence rate. Experiments confirm superior performance over local training and competitiveness with federated learning.

Conclusion: DeCAF resolves key challenges in decentralized LoRA, offering efficient and scalable fine-tuning for VLMs and LLMs in decentralized settings.

Abstract: Low-Rank Adaptation (LoRA) has emerged as one of the most effective,
computationally tractable fine-tuning approaches for training Vision-Language
Models (VLMs) and Large Language Models (LLMs). LoRA accomplishes this by
freezing the pre-trained model weights and injecting trainable low-rank
matrices, allowing for efficient learning of these foundation models even on
edge devices. However, LoRA in decentralized settings still remains under
explored, particularly for the theoretical underpinnings due to the lack of
smoothness guarantee and model consensus interference (defined formally below).
This work improves the convergence rate of decentralized LoRA (DLoRA) to match
the rate of decentralized SGD by ensuring gradient smoothness. We also
introduce DeCAF, a novel algorithm integrating DLoRA with truncated singular
value decomposition (TSVD)-based matrix factorization to resolve consensus
interference. Theoretical analysis shows TSVD's approximation error is bounded
and consensus differences between DLoRA and DeCAF vanish as rank increases,
yielding DeCAF's matching convergence rate. Extensive experiments across
vision/language tasks demonstrate our algorithms outperform local training and
rivals federated learning under both IID and non-IID data distributions.

</details>


### [684] [Finite Sample Analysis of Linear Temporal Difference Learning with Arbitrary Features](https://arxiv.org/pdf/2505.21391)
*Zixuan Xie, Xinyu Liu, Rohan Chandra, Shangtong Zhang*

Main category: cs.LG

TL;DR: The paper establishes the first L² convergence rates for linear TD(λ) under arbitrary features, without requiring linearly independent features or additional assumptions.


<details>
  <summary>Details</summary>
Motivation: Prior convergence rates for linear TD(λ) relied on linearly independent features, which limits practical applicability. This work addresses this gap.

Method: The authors develop a novel stochastic approximation result to handle non-unique solutions due to arbitrary features, ensuring convergence to the solution set.

Result: Convergence rates are proven for both discounted and average-reward settings, generalizing previous results.

Conclusion: The paper provides a foundational improvement for policy evaluation in reinforcement learning by removing restrictive feature assumptions.

Abstract: Linear TD($\lambda$) is one of the most fundamental reinforcement learning
algorithms for policy evaluation. Previously, convergence rates are typically
established under the assumption of linearly independent features, which does
not hold in many practical scenarios. This paper instead establishes the first
$L^2$ convergence rates for linear TD($\lambda$) operating under arbitrary
features, without making any algorithmic modification or additional
assumptions. Our results apply to both the discounted and average-reward
settings. To address the potential non-uniqueness of solutions resulting from
arbitrary features, we develop a novel stochastic approximation result
featuring convergence rates to the solution set instead of a single point.

</details>


### [685] [Leveraging the Power of Conversations: Optimal Key Term Selection in Conversational Contextual Bandits](https://arxiv.org/pdf/2505.21393)
*Maoli Liu, Zhuohua Li, Xiangxiang Dai, John C. S. Lui*

Main category: cs.LG

TL;DR: The paper proposes three algorithms (CLiSK, CLiME, CLiSK-ME) to improve conversational recommender systems by enhancing exploration and adaptive conversation initiation, achieving better regret bounds and performance.


<details>
  <summary>Details</summary>
Motivation: Existing conversational bandit algorithms suffer from insufficient exploration and rigid conversation initiation, leading to suboptimal preference learning and unnecessary interactions.

Method: Three novel algorithms are introduced: CLiSK (smoothed key term contexts for better exploration), CLiME (adaptive conversation initiation based on uncertainty), and CLiSK-ME (combining both). Theoretical analysis and empirical evaluations are conducted.

Result: The algorithms achieve a tighter regret upper bound of $O(\sqrt{dT\log{T}})$ and a 14.6% improvement in cumulative regret over existing methods. A matching lower bound $\Omega(\sqrt{dT})$ proves near-optimality.

Conclusion: The proposed algorithms significantly enhance conversational recommender systems by addressing exploration and adaptive interaction, supported by theoretical and empirical evidence.

Abstract: Conversational recommender systems proactively query users with relevant "key
terms" and leverage the feedback to elicit users' preferences for personalized
recommendations. Conversational contextual bandits, a prevalent approach in
this domain, aim to optimize preference learning by balancing exploitation and
exploration. However, several limitations hinder their effectiveness in
real-world scenarios. First, existing algorithms employ key term selection
strategies with insufficient exploration, often failing to thoroughly probe
users' preferences and resulting in suboptimal preference estimation. Second,
current algorithms typically rely on deterministic rules to initiate
conversations, causing unnecessary interactions when preferences are
well-understood and missed opportunities when preferences are uncertain. To
address these limitations, we propose three novel algorithms: CLiSK, CLiME, and
CLiSK-ME. CLiSK introduces smoothed key term contexts to enhance exploration in
preference learning, CLiME adaptively initiates conversations based on
preference uncertainty, and CLiSK-ME integrates both techniques. We
theoretically prove that all three algorithms achieve a tighter regret upper
bound of $O(\sqrt{dT\log{T}})$ with respect to the time horizon $T$, improving
upon existing methods. Additionally, we provide a matching lower bound
$\Omega(\sqrt{dT})$ for conversational bandits, demonstrating that our
algorithms are nearly minimax optimal. Extensive evaluations on both synthetic
and real-world datasets show that our approaches achieve at least a 14.6%
improvement in cumulative regret.

</details>


### [686] [Square$χ$PO: Differentially Private and Robust $χ^2$-Preference Optimization in Offline Direct Alignment](https://arxiv.org/pdf/2505.21395)
*Xingyu Zhou, Yulian Wu, Wenqian Weng, Francesco Orabona*

Main category: cs.LG

TL;DR: SquareχPO improves offline alignment of language models under privacy and corruption, achieving optimal rates and robustness.


<details>
  <summary>Details</summary>
Motivation: Addressing challenges in aligning language models with human preferences under privacy protections and label corruption.

Method: Introduces SquareχPO, replacing log-loss with square loss for better performance in privacy and robustness.

Result: Achieves optimal rates in privacy and robustness, with unified guarantees under corruption and privacy.

Conclusion: SquareχPO advances alignment methods with strong theoretical guarantees and practical extensions.

Abstract: In this paper, we theoretically study the offline alignment of language
models with human preference feedback, under both preference label corruption
and privacy protections. To this end, we propose Square$\chi$PO, a simple
one-line change to $\chi$PO where the standard log-loss is replaced by a new
square loss over probability. Thanks to the inherent properties of this new
loss, we have advanced the state-of-the-art of differentially private and
robust offline direct alignment. Specifically, for the local model of label
privacy, Square$\chi$PO is the first algorithm that attains an optimal rate
based on single-policy concentrability even with general function
approximations. It also gives the first result under the central model of
privacy protection over both prompts (responses) and labels. On the robustness
side against Huber label corruption, Square$\chi$PO is the first alignment
method that has a meaningful theoretical guarantee under general function
approximations. More importantly, Square$\chi$PO can address privacy protection
and corruption simultaneously, where an interesting separation is observed,
implying that the order of privacy and corruption matters. Furthermore, we show
that Square$\chi$PO can also be easily extended to handle the scenario of the
general preference model with state-of-the-art guarantees under corruption and
privacy. Last but not least, all of our theoretical guarantees enjoy a unified
analysis, building upon a new result on the generalization error bounds of
least-square regression under corruption and privacy constraints, which we
believe is of independent interest to the community.

</details>


### [687] [A Convergence Theory for Diffusion Language Models: An Information-Theoretic Perspective](https://arxiv.org/pdf/2505.21400)
*Gen Li, Changxiao Cai*

Main category: cs.LG

TL;DR: Diffusion models offer parallel token sampling for faster generation in LLMs, with theoretical convergence guarantees showing error decay inversely with iterations and scaling with token mutual information.


<details>
  <summary>Details</summary>
Motivation: To address the underdeveloped theoretical understanding of diffusion models in language generation, despite their empirical success.

Method: Develop convergence guarantees for diffusion language models from an information-theoretic perspective, analyzing sampling error via KL divergence and mutual information.

Result: Sampling error decays inversely with iterations and scales linearly with token mutual information, with tight upper and lower bounds established.

Conclusion: The work provides novel theoretical insights into the effectiveness of diffusion language models, bridging the gap between practice and theory.

Abstract: Diffusion models have emerged as a powerful paradigm for modern generative
modeling, demonstrating strong potential for large language models (LLMs).
Unlike conventional autoregressive (AR) models that generate tokens
sequentially, diffusion models enable parallel token sampling, leading to
faster generation and eliminating left-to-right generation constraints. Despite
their empirical success, the theoretical understanding of diffusion model
approaches remains underdeveloped. In this work, we develop convergence
guarantees for diffusion language models from an information-theoretic
perspective. Our analysis demonstrates that the sampling error, measured by the
Kullback-Leibler (KL) divergence, decays inversely with the number of
iterations $T$ and scales linearly with the mutual information between tokens
in the target text sequence. In particular, we establish matching upper and
lower bounds, up to some constant factor, to demonstrate the tightness of our
convergence analysis. These results offer novel theoretical insights into the
practical effectiveness of diffusion language models.

</details>


### [688] [A Framework for Adversarial Analysis of Decision Support Systems Prior to Deployment](https://arxiv.org/pdf/2505.21414)
*Brett Bissey, Kyle Gatesman, Walker Dimon, Mohammad Alam, Luis Robaina, Joseph Weissman*

Main category: cs.LG

TL;DR: A framework for analyzing and securing DRL-based decision-support systems by simulating adversarial attacks and evaluating vulnerabilities.


<details>
  <summary>Details</summary>
Motivation: To address the need for robust adversarial defense mechanisms in high-stakes decision-making systems using DRL.

Method: Developed a framework for simulating adversarial attacks, visualizing agent behavior, and evaluating attack impacts in a strategic game (CyberStrike).

Result: Systematically discovered and ranked attack impacts, evaluated transferability across agent architectures and DRL algorithms.

Conclusion: Highlights the necessity for robust defenses to protect DRL-based decision-making policies.

Abstract: This paper introduces a comprehensive framework designed to analyze and
secure decision-support systems trained with Deep Reinforcement Learning (DRL),
prior to deployment, by providing insights into learned behavior patterns and
vulnerabilities discovered through simulation. The introduced framework aids in
the development of precisely timed and targeted observation perturbations,
enabling researchers to assess adversarial attack outcomes within a strategic
decision-making context. We validate our framework, visualize agent behavior,
and evaluate adversarial outcomes within the context of a custom-built
strategic game, CyberStrike. Utilizing the proposed framework, we introduce a
method for systematically discovering and ranking the impact of attacks on
various observation indices and time-steps, and we conduct experiments to
evaluate the transferability of adversarial attacks across agent architectures
and DRL training algorithms. The findings underscore the critical need for
robust adversarial defense mechanisms to protect decision-making policies in
high-stakes environments.

</details>


### [689] [Dual Natural Gradient Descent for Scalable Training of Physics-Informed Neural Networks](https://arxiv.org/pdf/2505.21404)
*Anas Jnini, Flavio Vella*

Main category: cs.LG

TL;DR: D-NGD accelerates PINN training by computing Gauss--Newton steps in residual space, reducing complexity and enabling large-scale optimization on a single GPU.


<details>
  <summary>Details</summary>
Motivation: Traditional natural-gradient methods for PINNs are computationally expensive due to high time complexity in parameter space.

Method: D-NGD formulates the Gauss--Newton step in residual space, adds geodesic-acceleration correction, and uses direct or preconditioned solvers.

Result: D-NGD scales to 12.8M parameters, reduces error significantly, and works efficiently on a single GPU.

Conclusion: D-NGD is a scalable and efficient method for second-order PINN optimization.

Abstract: Natural-gradient methods markedly accelerate the training of Physics-Informed
Neural Networks (PINNs), yet their Gauss--Newton update must be solved in the
parameter space, incurring a prohibitive $O(n^3)$ time complexity, where $n$ is
the number of network trainable weights. We show that exactly the same step can
instead be formulated in a generally smaller residual space of size $m =
\sum_{\gamma} N_{\gamma} d_{\gamma}$, where each residual class $\gamma$ (e.g.
PDE interior, boundary, initial data) contributes $N_{\gamma}$ collocation
points of output dimension $d_{\gamma}$.
  Building on this insight, we introduce \textit{Dual Natural Gradient Descent}
(D-NGD). D-NGD computes the Gauss--Newton step in residual space, augments it
with a geodesic-acceleration correction at negligible extra cost, and provides
both a dense direct solver for modest $m$ and a Nystrom-preconditioned
conjugate-gradient solver for larger $m$.
  Experimentally, D-NGD scales second-order PINN optimization to networks with
up to 12.8 million parameters, delivers one- to three-order-of-magnitude lower
final error $L^2$ than first-order methods (Adam, SGD) and quasi-Newton
methods, and -- crucially -- enables natural-gradient training of PINNs at this
scale on a single GPU.

</details>


### [690] [When Shift Happens - Confounding Is to Blame](https://arxiv.org/pdf/2505.21422)
*Abbavaram Gowtham Reddy, Celia Rubio-Madrigal, Rebekka Burkholz, Krikamol Muandet*

Main category: cs.LG

TL;DR: ERM can outperform OOD methods due to hidden confounding, requiring environment-specific learning and proxy confounders for robustness.


<details>
  <summary>Details</summary>
Motivation: Addressing the counterintuitive finding that ERM rivals OOD methods and exploring the role of hidden confounding in distribution shifts.

Method: Empirical and theoretical analysis of hidden confounding's impact, proposing environment-specific learning and proxy confounders.

Result: Hidden confounding shifts violate OOD assumptions; environment-specific relationships and proxy confounders improve generalization.

Conclusion: New insights for robust OOD algorithms and covariate selection, emphasizing hidden confounding's role.

Abstract: Distribution shifts introduce uncertainty that undermines the robustness and
generalization capabilities of machine learning models. While conventional
wisdom suggests that learning causal-invariant representations enhances
robustness to such shifts, recent empirical studies present a counterintuitive
finding: (i) empirical risk minimization (ERM) can rival or even outperform
state-of-the-art out-of-distribution (OOD) generalization methods, and (ii) its
OOD generalization performance improves when all available covariates, not just
causal ones, are utilized. Drawing on both empirical and theoretical evidence,
we attribute this phenomenon to hidden confounding. Shifts in hidden
confounding induce changes in data distributions that violate assumptions
commonly made by existing OOD generalization approaches. Under such conditions,
we prove that effective generalization requires learning environment-specific
relationships, rather than relying solely on invariant ones. Furthermore, we
show that models augmented with proxies for hidden confounders can mitigate the
challenges posed by hidden confounding shifts. These findings offer new
theoretical insights and practical guidance for designing robust OOD
generalization algorithms and principled covariate selection strategies.

</details>


### [691] [Conflicting Biases at the Edge of Stability: Norm versus Sharpness Regularization](https://arxiv.org/pdf/2505.21423)
*Vit Fojtik, Maria Matveev, Hung-Hsu Chou, Gitta Kutyniok, Johannes Maly*

Main category: cs.LG

TL;DR: The paper explores how learning rates in gradient descent balance implicit biases (low parameter norm and low sharpness) to explain neural networks' generalization, showing neither bias alone suffices.


<details>
  <summary>Details</summary>
Motivation: To understand how gradient descent's learning rate balances implicit biases (low parameter norm and low sharpness) for better generalization in neural networks.

Method: Empirical analysis and theoretical proof using diagonal linear networks on a simple regression task.

Result: Learning rates balance low parameter norm and low sharpness; neither bias alone minimizes generalization error.

Conclusion: A broader view of implicit regularization, considering dynamic trade-offs, is needed to explain generalization.

Abstract: A widely believed explanation for the remarkable generalization capacities of
overparameterized neural networks is that the optimization algorithms used for
training induce an implicit bias towards benign solutions. To grasp this
theoretically, recent works examine gradient descent and its variants in
simplified training settings, often assuming vanishing learning rates. These
studies reveal various forms of implicit regularization, such as $\ell_1$-norm
minimizing parameters in regression and max-margin solutions in classification.
Concurrently, empirical findings show that moderate to large learning rates
exceeding standard stability thresholds lead to faster, albeit oscillatory,
convergence in the so-called Edge-of-Stability regime, and induce an implicit
bias towards minima of low sharpness (norm of training loss Hessian). In this
work, we argue that a comprehensive understanding of the generalization
performance of gradient descent requires analyzing the interaction between
these various forms of implicit regularization. We empirically demonstrate that
the learning rate balances between low parameter norm and low sharpness of the
trained model. We furthermore prove for diagonal linear networks trained on a
simple regression task that neither implicit bias alone minimizes the
generalization error. These findings demonstrate that focusing on a single
implicit bias is insufficient to explain good generalization, and they motivate
a broader view of implicit regularization that captures the dynamic trade-off
between norm and sharpness induced by non-negligible learning rates.

</details>


### [692] [Attribute-Efficient PAC Learning of Sparse Halfspaces with Constant Malicious Noise Rate](https://arxiv.org/pdf/2505.21430)
*Shiwei Zeng, Jie Shen*

Main category: cs.LG

TL;DR: The paper presents an attribute-efficient PAC learning algorithm for sparse halfspaces under constant malicious noise, using variants of hinge loss minimization.


<details>
  <summary>Details</summary>
Motivation: Addressing the challenge of learning sparse halfspaces efficiently in the presence of data corruptions or adversarial attacks.

Method: Simple variants of hinge loss minimization programs, leveraging concentration and margin conditions of the underlying distribution.

Result: An attribute-efficient algorithm that works under constant malicious noise rate, with poly(s, log d) sample complexity.

Conclusion: The approach successfully combines sparsity constraints with robustness to noise, offering a practical solution for adversarial settings.

Abstract: Attribute-efficient learning of sparse halfspaces has been a fundamental
problem in machine learning theory. In recent years, machine learning
algorithms are faced with prevalent data corruptions or even adversarial
attacks. It is of central interest to design efficient algorithms that are
robust to noise corruptions. In this paper, we consider that there exists a
constant amount of malicious noise in the data and the goal is to learn an
underlying $s$-sparse halfspace $w^* \in \mathbb{R}^d$ with $\text{poly}(s,\log
d)$ samples. Specifically, we follow a recent line of works and assume that the
underlying distribution satisfies a certain concentration condition and a
margin condition at the same time. Under such conditions, we show that
attribute-efficiency can be achieved by simple variants to existing hinge loss
minimization programs. Our key contribution includes: 1) an attribute-efficient
PAC learning algorithm that works under constant malicious noise rate; 2) a new
gradient analysis that carefully handles the sparsity constraint in hinge loss
minimization.

</details>


### [693] [Measuring Fine-Grained Relatedness in Multitask Learning via Data Attribution](https://arxiv.org/pdf/2505.21438)
*Yiwen Tu, Ziqi Liu, Jiaqi W. Ma, Weijing Tang*

Main category: cs.LG

TL;DR: The paper introduces MultiTask Influence Function (MTIF) to measure task relatedness in Multitask Learning (MTL) at an instance level, improving data selection to mitigate negative transfer.


<details>
  <summary>Details</summary>
Motivation: Addressing the challenge of measuring task relatedness and mitigating negative transfer in MTL, which remains an open problem.

Method: Proposes MTIF, adapting influence functions to MTL models with hard or soft parameter sharing for fine-grained, instance-level task relatedness measurement.

Result: MTIF efficiently approximates model performance on data subsets and improves MTL model performance through data selection.

Conclusion: MTIF bridges data attribution and MTL, providing a fine-grained solution for task relatedness and enhancing MTL models.

Abstract: Measuring task relatedness and mitigating negative transfer remain a critical
open challenge in Multitask Learning (MTL). This work extends data attribution
-- which quantifies the influence of individual training data points on model
predictions -- to MTL setting for measuring task relatedness. We propose the
MultiTask Influence Function (MTIF), a method that adapts influence functions
to MTL models with hard or soft parameter sharing. Compared to conventional
task relatedness measurements, MTIF provides a fine-grained, instance-level
relatedness measure beyond the entire-task level. This fine-grained relatedness
measure enables a data selection strategy to effectively mitigate negative
transfer in MTL. Through extensive experiments, we demonstrate that the
proposed MTIF efficiently and accurately approximates the performance of models
trained on data subsets. Moreover, the data selection strategy enabled by MTIF
consistently improves model performance in MTL. Our work establishes a novel
connection between data attribution and MTL, offering an efficient and
fine-grained solution for measuring task relatedness and enhancing MTL models.

</details>


### [694] [Can Large Reasoning Models Self-Train?](https://arxiv.org/pdf/2505.21444)
*Sheikh Shafayat, Fahim Tajwar, Ruslan Salakhutdinov, Jeff Schneider, Andrea Zanette*

Main category: cs.LG

TL;DR: The paper introduces an online self-training reinforcement learning algorithm for LLMs, using self-consistency to infer correctness without human supervision, achieving competitive performance on math tasks but facing challenges like reward hacking.


<details>
  <summary>Details</summary>
Motivation: To reduce reliance on human supervision in scaling LLMs by exploring self-training methods that leverage the model's own judgment for supervision.

Method: An online self-training reinforcement learning algorithm that uses the model's self-consistency to infer correctness signals, eliminating the need for ground-truth supervision.

Result: The algorithm achieves performance levels comparable to methods trained on gold-standard answers in mathematical reasoning tasks, but it also reveals issues like reward hacking.

Conclusion: Self-supervised improvement can yield significant gains without external labels, but inherent challenges like reward hacking must be addressed.

Abstract: Scaling the performance of large language models (LLMs) increasingly depends
on methods that reduce reliance on human supervision. Reinforcement learning
from automated verification offers an alternative, but it incurs scalability
limitations due to dependency upon human-designed verifiers. Self-training,
where the model's own judgment provides the supervisory signal, presents a
compelling direction. We propose an online self-training reinforcement learning
algorithm that leverages the model's self-consistency to infer correctness
signals and train without any ground-truth supervision. We apply the algorithm
to challenging mathematical reasoning tasks and show that it quickly reaches
performance levels rivaling reinforcement-learning methods trained explicitly
on gold-standard answers. Additionally, we analyze inherent limitations of the
algorithm, highlighting how the self-generated proxy reward initially
correlated with correctness can incentivize reward hacking, where confidently
incorrect outputs are favored. Our results illustrate how self-supervised
improvement can achieve significant performance gains without external labels,
while also revealing its fundamental challenges.

</details>


### [695] [Designing Cyclic Peptides via Harmonic SDE with Atom-Bond Modeling](https://arxiv.org/pdf/2505.21452)
*Xiangxin Zhou, Mingyu Li, Yi Xiao, Jiahan Li, Dongyu Xue, Zaixiang Zheng, Jianzhu Ma, Quanquan Gu*

Main category: cs.LG

TL;DR: CpSDE is a novel method for designing diverse cyclic peptides, overcoming data scarcity and geometric constraints with its two-component generative model.


<details>
  <summary>Details</summary>
Motivation: Cyclic peptides have pharmaceutical advantages but face design challenges due to limited 3D data, geometric constraints, and non-canonical amino acids.

Method: CpSDE combines AtomSDE (a structure prediction model) and ResRouter (a residue predictor) with a routed sampling algorithm to iteratively update sequences and structures.

Result: CpSDE successfully designs cyclic peptides with reliable stability and affinity.

Conclusion: CpSDE addresses key challenges in cyclic peptide design, offering a robust computational approach for diverse applications.

Abstract: Cyclic peptides offer inherent advantages in pharmaceuticals. For example,
cyclic peptides are more resistant to enzymatic hydrolysis compared to linear
peptides and usually exhibit excellent stability and affinity. Although deep
generative models have achieved great success in linear peptide design, several
challenges prevent the development of computational methods for designing
diverse types of cyclic peptides. These challenges include the scarcity of 3D
structural data on target proteins and associated cyclic peptide ligands, the
geometric constraints that cyclization imposes, and the involvement of
non-canonical amino acids in cyclization. To address the above challenges, we
introduce CpSDE, which consists of two key components: AtomSDE, a generative
structure prediction model based on harmonic SDE, and ResRouter, a residue type
predictor. Utilizing a routed sampling algorithm that alternates between these
two models to iteratively update sequences and structures, CpSDE facilitates
the generation of cyclic peptides. By employing explicit all-atom and bond
modeling, CpSDE overcomes existing data limitations and is proficient in
designing a wide variety of cyclic peptides. Our experimental results
demonstrate that the cyclic peptides designed by our method exhibit reliable
stability and affinity.

</details>


### [696] [High-Dimensional Calibration from Swap Regret](https://arxiv.org/pdf/2505.21460)
*Maxwell Fishelson, Noah Golowich, Mehryar Mohri, Jon Schneider*

Main category: cs.LG

TL;DR: The paper connects online calibration of multi-dimensional forecasts with external regret minimization in online linear optimization, showing that achieving $O(\sqrt{\rho T})$ regret implies $\epsilon$-calibration after $T = \exp(O(\rho /\epsilon^2))$ rounds. It generalizes prior results and introduces a novel algorithm that avoids needing OLO subroutines or knowledge of $\rho$. A lower bound proves exponential dependence on $1/\epsilon$ is necessary.


<details>
  <summary>Details</summary>
Motivation: To bridge the gap between online calibration and regret minimization, providing a unified framework for multi-dimensional forecasts over arbitrary convex sets and norms.

Method: Leverages the connection between calibration error and swap regret, using the TreeSwap algorithm with Follow-The-Leader to minimize calibration error without relying on OLO subroutines.

Result: Demonstrates that $\epsilon$-calibration can be achieved in $T = \exp(O(\rho /\epsilon^2))$ rounds, generalizing prior work. Also proves a lower bound showing exponential dependence on $1/\epsilon$ is unavoidable.

Conclusion: The paper unifies calibration and regret minimization, offering a versatile algorithm and tight bounds, advancing understanding of online calibration in high dimensions.

Abstract: We study the online calibration of multi-dimensional forecasts over an
arbitrary convex set $\mathcal{P} \subset \mathbb{R}^d$ relative to an
arbitrary norm $\Vert\cdot\Vert$. We connect this with the problem of external
regret minimization for online linear optimization, showing that if it is
possible to guarantee $O(\sqrt{\rho T})$ worst-case regret after $T$ rounds
when actions are drawn from $\mathcal{P}$ and losses are drawn from the dual
$\Vert \cdot \Vert_*$ unit norm ball, then it is also possible to obtain
$\epsilon$-calibrated forecasts after $T = \exp(O(\rho /\epsilon^2))$ rounds.
When $\mathcal{P}$ is the $d$-dimensional simplex and $\Vert \cdot \Vert$ is
the $\ell_1$-norm, the existence of $O(\sqrt{T\log d})$-regret algorithms for
learning with experts implies that it is possible to obtain
$\epsilon$-calibrated forecasts after $T = \exp(O(\log{d}/\epsilon^2)) =
d^{O(1/\epsilon^2)}$ rounds, recovering a recent result of Peng (2025).
  Interestingly, our algorithm obtains this guarantee without requiring access
to any online linear optimization subroutine or knowledge of the optimal rate
$\rho$ -- in fact, our algorithm is identical for every setting of
$\mathcal{P}$ and $\Vert \cdot \Vert$. Instead, we show that the optimal
regularizer for the above OLO problem can be used to upper bound the above
calibration error by a swap regret, which we then minimize by running the
recent TreeSwap algorithm with Follow-The-Leader as a subroutine.
  Finally, we prove that any online calibration algorithm that guarantees
$\epsilon T$ $\ell_1$-calibration error over the $d$-dimensional simplex
requires $T \geq \exp(\mathrm{poly}(1/\epsilon))$ (assuming $d \geq
\mathrm{poly}(1/\epsilon)$). This strengthens the corresponding
$d^{\Omega(\log{1/\epsilon})}$ lower bound of Peng, and shows that an
exponential dependence on $1/\epsilon$ is necessary.

</details>


### [697] [Causal Posterior Estimation](https://arxiv.org/pdf/2505.21468)
*Simon Dirmeier, Antonietta Mira*

Main category: cs.LG

TL;DR: CPE is a novel Bayesian inference method for simulator models using normalizing flows to approximate posteriors, improving accuracy by incorporating model structure.


<details>
  <summary>Details</summary>
Motivation: Address intractable or expensive likelihood evaluations in simulator models by leveraging graphical model structure.

Method: Uses normalizing flow-based approximation with discrete/continuous architectures and constant-time sampling for efficiency.

Result: CPE outperforms or matches state-of-the-art in posterior inference accuracy.

Conclusion: Incorporating model structure into neural networks enhances inference accuracy, making CPE a competitive method.

Abstract: We present Causal Posterior Estimation (CPE), a novel method for Bayesian
inference in simulator models, i.e., models where the evaluation of the
likelihood function is intractable or too computationally expensive, but where
one can simulate model outputs given parameter values. CPE utilizes a
normalizing flow-based (NF) approximation to the posterior distribution which
carefully incorporates the conditional dependence structure induced by the
graphical representation of the model into the neural network. Thereby it is
possible to improve the accuracy of the approximation. We introduce both
discrete and continuous NF architectures for CPE and propose a constant-time
sampling procedure for the continuous case which reduces the computational
complexity of drawing samples to O(1) as for discrete NFs. We show, through an
extensive experimental evaluation, that by incorporating the conditional
dependencies induced by the graphical model directly into the neural network,
rather than learning them from data, CPE is able to conduct highly accurate
posterior inference either outperforming or matching the state of the art in
the field.

</details>


### [698] [Algorithms and SQ Lower Bounds for Robustly Learning Real-valued Multi-index Models](https://arxiv.org/pdf/2505.21475)
*Ilias Diakonikolas, Giannis Iakovidis, Daniel M. Kane, Lisheng Ren*

Main category: cs.LG

TL;DR: The paper presents an algorithm for PAC learning Multi-Index Models (MIMs) under Gaussian distribution, handling adversarial noise, and provides matching SQ lower bounds. It also introduces an efficient learner for Lipschitz homogeneous ReLU networks.


<details>
  <summary>Details</summary>
Motivation: To address the challenge of learning real-valued MIMs efficiently, especially in noisy settings, and to provide optimal complexity bounds.

Method: A general algorithm for PAC learning MIMs with square loss, analyzed under adversarial and realizable noise settings, complemented by SQ lower bounds.

Result: The algorithm achieves complexity $d^{O(m)}2^{\mathrm{poly}(K/\epsilon)}$ for adversarial noise and $d^{O(m)}2^{\mathrm{poly}(K)}(1/\epsilon)^{O(K)}$ for realizable noise. SQ lower bounds confirm optimality.

Conclusion: The work advances learning for MIMs and introduces a practical algorithm for Lipschitz homogeneous ReLU networks with improved complexity.

Abstract: We study the complexity of learning real-valued Multi-Index Models (MIMs)
under the Gaussian distribution. A $K$-MIM is a function $f:\mathbb{R}^d\to
\mathbb{R}$ that depends only on the projection of its input onto a
$K$-dimensional subspace. We give a general algorithm for PAC learning a broad
class of MIMs with respect to the square loss, even in the presence of
adversarial label noise. Moreover, we establish a nearly matching Statistical
Query (SQ) lower bound, providing evidence that the complexity of our algorithm
is qualitatively optimal as a function of the dimension. Specifically, we
consider the class of bounded variation MIMs with the property that degree at
most $m$ distinguishing moments exist with respect to projections onto any
subspace. In the presence of adversarial label noise, the complexity of our
learning algorithm is $d^{O(m)}2^{\mathrm{poly}(K/\epsilon)}$. For the
realizable and independent noise settings, our algorithm incurs complexity
$d^{O(m)}2^{\mathrm{poly}(K)}(1/\epsilon)^{O(K)}$. To complement our upper
bound, we show that if for some subspace degree-$m$ distinguishing moments do
not exist, then any SQ learner for the corresponding class of MIMs requires
complexity $d^{\Omega(m)}$. As an application, we give the first efficient
learner for the class of positive-homogeneous $L$-Lipschitz $K$-MIMs. The
resulting algorithm has complexity $\mathrm{poly}(d)
2^{\mathrm{poly}(KL/\epsilon)}$. This gives a new PAC learning algorithm for
Lipschitz homogeneous ReLU networks with complexity independent of the network
size, removing the exponential dependence incurred in prior work.

</details>


### [699] [Hardware-Efficient Attention for Fast Decoding](https://arxiv.org/pdf/2505.21487)
*Ted Zadouri, Hubert Strauss, Tri Dao*

Main category: cs.LG

TL;DR: The paper addresses inefficiencies in LLM decoding by redesigning attention mechanisms (GTA and GLA) to reduce memory transfers and improve hardware efficiency without sacrificing model quality or scalability.


<details>
  <summary>Details</summary>
Motivation: Current LLM decoding suffers from high memory bandwidth usage and limited parallelism due to sequential decoding and inefficient KV cache loading.

Method: Proposes Grouped-Tied Attention (GTA) to reuse key-value states and Grouped Latent Attention (GLA) with low-level optimizations for parallel-friendly decoding.

Result: GTA halves KV cache usage while matching GQA quality; GLA matches MLA performance, is easier to shard, and achieves up to 2× speedup in decoding.

Conclusion: The redesigned attention mechanisms (GTA and GLA) significantly improve decoding efficiency, reducing latency and increasing throughput in real-world applications.

Abstract: LLM decoding is bottlenecked for large batches and long contexts by loading
the key-value (KV) cache from high-bandwidth memory, which inflates per-token
latency, while the sequential nature of decoding limits parallelism. We analyze
the interplay among arithmetic intensity, parallelization, and model quality
and question whether current architectures fully exploit modern hardware. This
work redesigns attention to perform more computation per byte loaded from
memory to maximize hardware efficiency without trading off parallel
scalability. We first propose Grouped-Tied Attention (GTA), a simple variant
that combines and reuses key and value states, reducing memory transfers
without compromising model quality. We then introduce Grouped Latent Attention
(GLA), a parallel-friendly latent attention paired with low-level optimizations
for fast decoding while maintaining high model quality. Experiments show that
GTA matches Grouped-Query Attention (GQA) quality while using roughly half the
KV cache and that GLA matches Multi-head Latent Attention (MLA) and is easier
to shard. Our optimized GLA kernel is up to 2$\times$ faster than FlashMLA, for
example, in a speculative decoding setting when the query length exceeds one.
Furthermore, by fetching a smaller KV cache per device, GLA reduces end-to-end
latency and increases throughput in online serving benchmarks by up to
2$\times$.

</details>


### [700] [Reinforcing General Reasoning without Verifiers](https://arxiv.org/pdf/2505.21493)
*Xiangxin Zhou, Zichen Liu, Anya Sims, Haonan Wang, Tianyu Pang, Chongxuan Li, Liang Wang, Min Lin, Chao Du*

Main category: cs.LG

TL;DR: Proposes VeriFree, a verifier-free method for training LLMs using RL to maximize reference answer probability, outperforming verifier-based methods.


<details>
  <summary>Details</summary>
Motivation: Current RL methods for LLMs rely on rule-based verification, limiting applicability to domains like healthcare or law. Verifier-based workarounds introduce issues like reward hacking and computational burden.

Method: VeriFree bypasses answer verification, using RL to directly maximize the probability of generating reference answers, eliminating the need for a separate verifier model.

Result: VeriFree matches or surpasses verifier-based methods on benchmarks (MMLU-Pro, GPQA, etc.), with practical benefits like reduced compute and memory requirements.

Conclusion: VeriFree extends RL-based LLM training to general reasoning domains, offering efficiency and performance advantages over verifier-based approaches.

Abstract: The recent paradigm shift towards training large language models (LLMs) using
DeepSeek-R1-Zero-style reinforcement learning (RL) on verifiable rewards has
led to impressive advancements in code and mathematical reasoning. However,
this methodology is limited to tasks where rule-based answer verification is
possible and does not naturally extend to real-world domains such as chemistry,
healthcare, engineering, law, biology, business, and economics. Current
practical workarounds use an additional LLM as a model-based verifier; however,
this introduces issues such as reliance on a strong verifier LLM,
susceptibility to reward hacking, and the practical burden of maintaining the
verifier model in memory during training. To address this and extend
DeepSeek-R1-Zero-style training to general reasoning domains, we propose a
verifier-free method (VeriFree) that bypasses answer verification and instead
uses RL to directly maximize the probability of generating the reference
answer. We compare VeriFree with verifier-based methods and demonstrate that,
in addition to its significant practical benefits and reduced compute
requirements, VeriFree matches and even surpasses verifier-based methods on
extensive evaluations across MMLU-Pro, GPQA, SuperGPQA, and math-related
benchmarks. Moreover, we provide insights into this method from multiple
perspectives: as an elegant integration of training both the policy and
implicit verifier in a unified model, and as a variational optimization
approach. Code is available at https://github.com/sail-sg/VeriFree.

</details>


### [701] [Estimating Motor Symptom Presence and Severity in Parkinson's Disease from Wrist Accelerometer Time Series using ROCKET and InceptionTime](https://arxiv.org/pdf/2304.11265)
*Cedric Donié, Neha Das, Satoshi Endo, Sandra Hirche*

Main category: cs.LG

TL;DR: The paper evaluates InceptionTime and ROCKET for monitoring Parkinson's disease symptoms using wearable accelerometer data, finding both effective but with specific strengths.


<details>
  <summary>Details</summary>
Motivation: Continuous monitoring of PD symptoms is needed for targeted treatment, but existing methods struggle with complex movement patterns and small datasets.

Method: InceptionTime and ROCKET were tested with random search for optimal architecture, compared to ridge classifier and MLP on wrist motion data.

Result: Both methods performed moderately for tremor and bradykinesia, with ROCKET better for dyskinesia and InceptionTime slightly superior for the other symptoms. Both outperformed MLP.

Conclusion: InceptionTime shows promise for PD symptom monitoring, though further development is needed.

Abstract: Parkinson's disease (PD) is a neurodegenerative condition characterized by
frequently changing motor symptoms, necessitating continuous symptom monitoring
for more targeted treatment. Classical time series classification and deep
learning techniques have demonstrated limited efficacy in monitoring PD
symptoms using wearable accelerometer data due to complex PD movement patterns
and the small size of available datasets. We investigate InceptionTime and
RandOm Convolutional KErnel Transform (ROCKET) as they are promising for PD
symptom monitoring. InceptionTime's high learning capacity is well-suited to
modeling complex movement patterns, while ROCKET is suited to small datasets.
With random search methodology, we identify the highest-scoring InceptionTime
architecture and compare its performance to ROCKET with a ridge classifier and
a multi-layer perceptron (MLP) on wrist motion data from PD patients. Our
findings indicate that all approaches can learn to estimate tremor severity and
bradykinesia presence with moderate performance but encounter challenges in
detecting dyskinesia. Among the presented approaches, ROCKET demonstrates
higher scores in identifying dyskinesia, whereas InceptionTime exhibits
slightly better performance in tremor and bradykinesia estimation. Notably,
both methods outperform the multi-layer perceptron. In conclusion,
InceptionTime can classify complex wrist motion time series and holds potential
for continuous symptom monitoring in PD with further development.

</details>


### [702] [Adaptive Sample Sharing for Multi Agent Linear Bandits](https://arxiv.org/pdf/2309.08710)
*Hamza Cherkaoui, Merwan Barlier, Igor Colin*

Main category: cs.LG

TL;DR: The paper introduces BASS, an algorithm for efficient multi-agent collaboration in linear bandits, focusing on data sharing's impact on regret minimization without structural assumptions.


<details>
  <summary>Details</summary>
Motivation: To address the challenge of efficient collaboration in multi-agent linear bandits by studying data sharing's role in minimizing regret.

Method: Proposes the Bandit Adaptive Sample Sharing (BASS) algorithm, which balances bias and uncertainty in parameter estimation for collaboration.

Result: BASS outperforms state-of-the-art methods theoretically and empirically, and accurately recovers cluster structures in agents' parameters.

Conclusion: BASS effectively minimizes regret in multi-agent linear bandits by optimizing data sharing, even without structural assumptions.

Abstract: The multi-agent linear bandit setting is a well-known setting for which
designing efficient collaboration between agents remains challenging. This
paper studies the impact of data sharing among agents on regret minimization.
Unlike most existing approaches, our contribution does not rely on any
assumptions on the bandit parameters structure. Our main result formalizes the
trade-off between the bias and uncertainty of the bandit parameter estimation
for efficient collaboration. This result is the cornerstone of the Bandit
Adaptive Sample Sharing (BASS) algorithm, whose efficiency over the current
state-of-the-art is validated through both theoretical analysis and empirical
evaluations on both synthetic and real-world datasets. Furthermore, we
demonstrate that, when agents' parameters display a cluster structure, our
algorithm accurately recovers them.

</details>


### [703] [Quantum Speedups in Regret Analysis of Infinite Horizon Average-Reward Markov Decision Processes](https://arxiv.org/pdf/2310.11684)
*Bhargav Ganguly, Yang Xu, Vaneet Aggarwal*

Main category: cs.LG

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: This paper investigates the potential of quantum acceleration in addressing
infinite horizon Markov Decision Processes (MDPs) to enhance average reward
outcomes. We introduce an innovative quantum framework for the agent's
engagement with an unknown MDP, extending the conventional interaction
paradigm. Our approach involves the design of an optimism-driven tabular
Reinforcement Learning algorithm that harnesses quantum signals acquired by the
agent through efficient quantum mean estimation techniques. Through thorough
theoretical analysis, we demonstrate that the quantum advantage in mean
estimation leads to exponential advancements in regret guarantees for infinite
horizon Reinforcement Learning. Specifically, the proposed Quantum algorithm
achieves a regret bound of $\tilde{\mathcal{O}}(1)$, a significant improvement
over the $\tilde{\mathcal{O}}(\sqrt{T})$ bound exhibited by classical
counterparts.

</details>


### [704] [DYMAG: Rethinking Message Passing Using Dynamical-systems-based Waveforms](https://arxiv.org/pdf/2309.09924)
*Dhananjay Bhaskar, Xingzhi Sun, Yanlei Zhang, Charles Xu, Arman Afrasiyabi, Siddharth Viswanath, Oluwadamilola Fasina, Maximilian Nickel, Guy Wolf, Michael Perlmutter, Smita Krishnaswamy*

Main category: cs.LG

TL;DR: DYMAG introduces a graph neural network using dynamic waveforms for message aggregation, outperforming baselines in graph analysis tasks.


<details>
  <summary>Details</summary>
Motivation: Standard message-passing neural networks use simple local averaging, limiting their ability to capture complex graph structures.

Method: DYMAG employs dynamic waveforms (heat equation, wave equation, chaotic dynamics) at multiple scales for message aggregation.

Result: Theoretically, DYMAG captures graph properties like connectivity and cycles; empirically, it excels in graph persistence, random graph parameter generation, and property prediction.

Conclusion: DYMAG advances graph neural networks by leveraging dynamic waveforms, demonstrating superior performance across diverse benchmarks.

Abstract: We present DYMAG, a graph neural network based on a novel form of message
aggregation. Standard message-passing neural networks, which often aggregate
local neighbors via mean-aggregation, can be regarded as convolving with a
simple rectangular waveform which is non-zero only on 1-hop neighbors of every
vertex. Here, we go beyond such local averaging. We will convolve the node
features with more sophisticated waveforms generated using dynamics such as the
heat equation, wave equation, and the Sprott model (an example of chaotic
dynamics). Furthermore, we use snapshots of these dynamics at different time
points to create waveforms at many effective scales. Theoretically, we show
that these dynamic waveforms can capture salient information about the graph
including connected components, connectivity, and cycle structures even with no
features. Empirically, we test DYMAG on both real and synthetic benchmarks to
establish that DYMAG outperforms baseline models on recovery of graph
persistence, generating parameters of random graphs, as well as property
prediction for proteins, molecules and materials. Our code is available at
https://github.com/KrishnaswamyLab/DYMAG.

</details>


### [705] [Optimal Pricing for Data-Augmented AutoML Marketplaces](https://arxiv.org/pdf/2310.17843)
*Minbiao Han, Jonathan Light, Steven Xia, Sainyam Galhotra, Raul Castro Fernandez, Haifeng Xu*

Main category: cs.LG

TL;DR: A data-augmented AutoML market is proposed to address data scarcity by integrating external datasets, pricing models based on performance improvements, and simplifying market complexities.


<details>
  <summary>Details</summary>
Motivation: Organizations often lack sufficient data for ML training, while valuable data remains underutilized. Data markets can bridge this gap but face challenges like pricing and fairness.

Method: The paper introduces a data-augmented AutoML market that integrates with platforms like Vertex AI and SageMaker. It augments buyer data with external datasets and prices models based on performance improvements, using an instrumental value-based pricing mechanism.

Result: The solution enhances ML outcomes by leveraging external data, simplifies pricing complexities, and mitigates strategic buyer behavior through menu-based options.

Conclusion: The proposed framework provides an economically sustainable way to monetize external data while improving ML model performance.

Abstract: Organizations often lack sufficient data to effectively train machine
learning (ML) models, while others possess valuable data that remains
underutilized. Data markets promise to unlock substantial value by matching
data suppliers with demand from ML consumers. However, market design involves
addressing intricate challenges, including data pricing, fairness, robustness,
and strategic behavior. In this paper, we propose a pragmatic data-augmented
AutoML market that seamlessly integrates with existing cloud-based AutoML
platforms such as Google's Vertex AI and Amazon's SageMaker. Unlike standard
AutoML solutions, our design automatically augments buyer-submitted training
data with valuable external datasets, pricing the resulting models based on
their measurable performance improvements rather than computational costs as
the status quo. Our key innovation is a pricing mechanism grounded in the
instrumental value - the marginal model quality improvement - of externally
sourced data. This approach bypasses direct dataset pricing complexities,
mitigates strategic buyer behavior, and accommodates diverse buyer valuations
through menu-based options. By integrating automated data and model discovery,
our solution not only enhances ML outcomes but also establishes an economically
sustainable framework for monetizing external data.

</details>


### [706] [T-REX: Mixture-of-Rank-One-Experts with Semantic-aware Intuition for Multi-task Large Language Model Finetuning](https://arxiv.org/pdf/2404.08985)
*Rongyu Zhang, Yijiang Liu, Huanrui Yang, Shenli Zheng, Dan Wang, Yuan Du, Li Du, Shanghang Zhang*

Main category: cs.LG

TL;DR: T-REX introduces a novel MoE framework using ultra-low rank experts for efficient multitask finetuning of LLMs, improving accuracy with fewer parameters.


<details>
  <summary>Details</summary>
Motivation: Addressing the inefficiency and performance limitations of scaling MoE experts in LLMs for multitask finetuning.

Method: Leverages rank-1 experts to construct LoRA weights, enabling a mix-and-match mechanism for quadratic subspace expansion with linear overheads. Includes implicit router guidance via semantic clustering.

Result: Achieves 1.78% mean accuracy improvement with 30%-40% fewer parameters across 14 datasets.

Conclusion: T-REX offers superior efficiency and generalizability in multitask finetuning for LLMs.

Abstract: Large language models (LLMs) encounter significant adaptation challenges in
diverse multitask finetuning. Mixture-of-experts (MoE) provides a promising
solution with a dynamic architecture, enabling effective task decoupling.
However, scaling up the number of MoE experts incurs substantial parameter and
computational overheads and suffers from limited performance gain due to naive
routing mechanisms. In this paper, we design a novel framework,
mix\underline{\textbf{T}}ure\underline{\textbf{-}}of-\underline{\textbf{R}}ank-on\underline{\textbf{E}}-e\underline{\textbf{X}}perts
(\texttt{T-REX}), which leverages the combination of ultra-low rank experts to
construct LoRA weights on pretrained LLMs. The rank-1 experts enable a
mix-and-match mechanism to quadratically expand the vector subspace of experts
with linear parameter overheads, achieving approximate error reduction with
optimal efficiency. In addition, T-REX offers implicit guidance to the router,
leveraging the inherent semantic clustering of training embeddings as prior
knowledge, enabling optimized feature allocation across experts for a smoother
convergence. Extensive theoretical and empirical results demonstrate that T-REX
achieves superior efficiency and generalizability across diverse tasks.
Compared with other LoRA-based methods, T-REX achieves up to 1.78\% mean
accuracy improvement with around 30\%-40\% less trainable parameters across 14
public datasets. \href{https://github.com/RoyZry98/T-REX-Pytorch}{Code} is
available.

</details>


### [707] [A Concentration Bound for TD(0) with Function Approximation](https://arxiv.org/pdf/2312.10424)
*Siddharth Chandak, Vivek S. Borkar*

Main category: cs.LG

TL;DR: Concentration bound for TD(0) with linear function approximation, analyzed under online TD learning with samples from a single Markov chain path.


<details>
  <summary>Details</summary>
Motivation: To address the challenge of analyzing TD(0) in an online setting with dependent samples, differing from offline or independent-sample scenarios.

Method: Treats TD(0) as a contractive stochastic approximation algorithm, handling martingale and Markov noises using Poisson equation and relaxed concentration inequalities.

Result: Derives a concentration bound valid for all $n \geq n_0$, addressing the lack of almost sure boundedness guarantees.

Conclusion: The analysis provides a theoretical foundation for TD(0) with linear approximation in online settings with Markov noise.

Abstract: We derive a concentration bound of the type `for all $n \geq n_0$ for some
$n_0$' for TD(0) with linear function approximation. We work with online TD
learning with samples from a single sample path of the underlying Markov chain.
This makes our analysis significantly different from offline TD learning or TD
learning with access to independent samples from the stationary distribution of
the Markov chain. We treat TD(0) as a contractive stochastic approximation
algorithm, with both martingale and Markov noises. Markov noise is handled
using the Poisson equation and the lack of almost sure guarantees on
boundedness of iterates is handled using the concept of relaxed concentration
inequalities.

</details>


### [708] [Can Large Language Models Understand Symbolic Graphics Programs?](https://arxiv.org/pdf/2408.08313)
*Zeju Qiu, Weiyang Liu, Haiwen Feng, Zhen Liu, Tim Z. Xiao, Katherine M. Collins, Joshua B. Tenenbaum, Adrian Weller, Michael J. Black, Bernhard Schölkopf*

Main category: cs.LG

TL;DR: The paper proposes using symbolic graphics programs to evaluate LLMs' spatial-semantic reasoning, introduces a benchmark, and presents Symbolic Instruction Tuning (SIT) to enhance LLM performance.


<details>
  <summary>Details</summary>
Motivation: To scientifically assess LLMs' capabilities and shortcomings, especially in spatial-semantic reasoning, given the difficulty of finding untrained tasks.

Method: Utilizes symbolic graphics programs to create a benchmark for evaluating LLMs' ability to reason about visual output without vision encoders. Introduces SIT for finetuning.

Result: Stronger reasoning LLMs perform better. SIT improves both symbolic program understanding and general reasoning on other benchmarks.

Conclusion: Symbolic graphics programs are effective for evaluating LLMs, and SIT enhances their reasoning abilities beyond the target domain.

Abstract: Against the backdrop of enthusiasm for large language models (LLMs), there is
a growing need to scientifically assess their capabilities and shortcomings.
This is nontrivial in part because it is difficult to find tasks which the
models have not encountered during training. Utilizing symbolic graphics
programs, we propose a domain well-suited to test multiple spatial-semantic
reasoning skills of LLMs. Popular in computer graphics, these programs
procedurally generate visual data. While LLMs exhibit impressive skills in
general program synthesis and analysis, symbolic graphics programs offer a new
layer of evaluation: they allow us to test an LLM's ability to answer semantic
questions about the images or 3D geometries without a vision encoder. To
semantically understand the symbolic programs, LLMs would need to possess the
ability to "imagine" and reason how the corresponding graphics content would
look with only the symbolic description of the local curvatures and strokes. We
use this task to evaluate LLMs by creating a large benchmark for the semantic
visual understanding of symbolic graphics programs, built procedurally with
minimal human effort. Particular emphasis is placed on transformations of
images that leave the image level semantics invariant while introducing
significant changes to the underlying program. We evaluate commercial and
open-source LLMs on our benchmark to assess their ability to reason about
visual output of programs, finding that LLMs considered stronger at reasoning
generally perform better. Lastly, we introduce a novel method to improve this
ability -- Symbolic Instruction Tuning (SIT), in which the LLM is finetuned
with pre-collected instruction data on symbolic graphics programs.
Interestingly, we find that SIT not only improves LLM's understanding on
symbolic programs, but it also improves general reasoning ability on various
other benchmarks.

</details>


### [709] [Retrieve to Explain: Evidence-driven Predictions for Explainable Drug Target Identification](https://arxiv.org/pdf/2402.04068)
*Ravi Patel, Angus Brayne, Rogier Hintzen, Daniel Jaroslawicz, Georgiana Neculae, Dane Corneil*

Main category: cs.LG

TL;DR: R2E is a retrieval-based model that ranks answers to research questions by evidence strength, enhancing explainability and outperforming existing methods in drug target identification.


<details>
  <summary>Details</summary>
Motivation: Existing language models lack the ability to quantitatively compare answer plausibility based on evidence, which is critical for complex scientific questions.

Method: R2E masks answers and scores them solely on supporting evidence, using Shapley values for explainability and allowing integration of new evidence without retraining.

Result: R2E matches non-explainable models and surpasses genetics-based approaches in predicting efficacious drug targets.

Conclusion: R2E provides a transparent, evidence-based framework for scientific discovery, particularly valuable in high-stakes fields like drug development.

Abstract: Language models hold incredible promise for enabling scientific discovery by
synthesizing massive research corpora. Many complex scientific research
questions have multiple plausible answers, each supported by evidence of
varying strength. However, existing language models lack the capability to
quantitatively and faithfully compare answer plausibility in terms of
supporting evidence. To address this, we introduce Retrieve to Explain (R2E), a
retrieval-based model that scores and ranks all possible answers to a research
question based on evidence retrieved from a document corpus. The architecture
represents each answer only in terms of its supporting evidence, with the
answer itself masked. This allows us to extend feature attribution methods such
as Shapley values, to transparently attribute answer scores to supporting
evidence at inference time. The architecture also allows incorporation of new
evidence without retraining, including non-textual data modalities templated
into natural language. We developed R2E for the challenging scientific
discovery task of drug target identification, a human-in-the-loop process where
failures are extremely costly and explainability paramount. When predicting
whether drug targets will subsequently be confirmed as efficacious in clinical
trials, R2E not only matches non-explainable literature-based models but also
surpasses a genetics-based target identification approach used throughout the
pharmaceutical industry.

</details>


### [710] [MODULI: Unlocking Preference Generalization via Diffusion Models for Offline Multi-Objective Reinforcement Learning](https://arxiv.org/pdf/2408.15501)
*Yifu Yuan, Zhenrui Zheng, Zibin Dong, Jianye Hao*

Main category: cs.LG

TL;DR: MODULI, a diffusion model-based planner, improves offline MORL by addressing OOD preference generalization through sliding guidance and return normalization.


<details>
  <summary>Details</summary>
Motivation: Offline MORL struggles with OOD preferences due to narrow dataset distributions, leading to poor policy alignment.

Method: MODULI uses a preference-conditioned diffusion model for trajectory generation, introduces return normalization, and employs sliding guidance for OOD generalization.

Result: MODULI outperforms state-of-the-art offline MORL baselines, showing strong generalization to OOD preferences.

Conclusion: MODULI effectively addresses OOD preference challenges in offline MORL, enhancing policy alignment and generalization.

Abstract: Multi-objective Reinforcement Learning (MORL) seeks to develop policies that
simultaneously optimize multiple conflicting objectives, but it requires
extensive online interactions. Offline MORL provides a promising solution by
training on pre-collected datasets to generalize to any preference upon
deployment. However, real-world offline datasets are often conservatively and
narrowly distributed, failing to comprehensively cover preferences, leading to
the emergence of out-of-distribution (OOD) preference areas. Existing offline
MORL algorithms exhibit poor generalization to OOD preferences, resulting in
policies that do not align with preferences. Leveraging the excellent
expressive and generalization capabilities of diffusion models, we propose
MODULI (Multi-objective Diffusion Planner with Sliding Guidance), which employs
a preference-conditioned diffusion model as a planner to generate trajectories
that align with various preferences and derive action for decision-making. To
achieve accurate generation, MODULI introduces two return normalization methods
under diverse preferences for refining guidance. To further enhance
generalization to OOD preferences, MODULI proposes a novel sliding guidance
mechanism, which involves training an additional slider adapter to capture the
direction of preference changes. Incorporating the slider, it transitions from
in-distribution (ID) preferences to generating OOD preferences, patching, and
extending the incomplete Pareto front. Extensive experiments on the D4MORL
benchmark demonstrate that our algorithm outperforms state-of-the-art Offline
MORL baselines, exhibiting excellent generalization to OOD preferences.

</details>


### [711] [Stochastic Online Conformal Prediction with Semi-Bandit Feedback](https://arxiv.org/pdf/2405.13268)
*Haosen Ge, Hamsa Bastani, Osbert Bastani*

Main category: cs.LG

TL;DR: A novel conformal prediction algorithm for online learning with semi-bandit feedback, achieving sublinear regret and good empirical performance.


<details>
  <summary>Details</summary>
Motivation: Conformal prediction lacks efficiency in dynamic, online settings with limited feedback, especially when true labels are only observed if contained in prediction sets.

Method: Proposes a conformal prediction algorithm for online learning with semi-bandit feedback, dynamically adjusting prediction sets over time.

Result: The algorithm achieves sublinear regret compared to the optimal conformal predictor and performs well in retrieval, image classification, and auction tasks.

Conclusion: The proposed method effectively addresses the limitations of conformal prediction in online, semi-bandit feedback settings, demonstrating strong empirical results.

Abstract: Conformal prediction has emerged as an effective strategy for uncertainty
quantification by modifying a model to output sets of labels instead of a
single label. These prediction sets come with the guarantee that they contain
the true label with high probability. However, conformal prediction typically
requires a large calibration dataset of i.i.d. examples. We consider the online
learning setting, where examples arrive over time, and the goal is to construct
prediction sets dynamically. Departing from existing work, we assume
semi-bandit feedback, where we only observe the true label if it is contained
in the prediction set. For instance, consider calibrating a document retrieval
model to a new domain; in this setting, a user would only be able to provide
the true label if the target document is in the prediction set of retrieved
documents. We propose a novel conformal prediction algorithm targeted at this
setting, and prove that it obtains sublinear regret compared to the optimal
conformal predictor. We evaluate our algorithm on a retrieval task, an image
classification task, and an auction price-setting task, and demonstrate that it
empirically achieves good performance compared to several baselines.

</details>


### [712] [Does Graph Prompt Work? A Data Operation Perspective with Theoretical Analysis](https://arxiv.org/pdf/2410.01635)
*Qunzhong Wang, Xiangguo Sun, Hong Cheng*

Main category: cs.LG

TL;DR: The paper introduces a theoretical framework to analyze graph prompting, proving its effectiveness in approximating graph transformations and providing error bounds for practical applications.


<details>
  <summary>Details</summary>
Motivation: Graph prompting has shown empirical success but lacks theoretical understanding. This paper aims to fill that gap by rigorously analyzing its effectiveness.

Method: The authors propose a theoretical framework with formal guarantees, error bounds for single and batched graphs, and analysis of error distributions across linear and non-linear graph models.

Result: The framework demonstrates graph prompting's ability to approximate transformations, with empirical experiments supporting the theoretical findings.

Conclusion: The paper provides a solid theoretical foundation for graph prompting, enhancing its credibility and potential for broader applications.

Abstract: In recent years, graph prompting has emerged as a promising research
direction, enabling the learning of additional tokens or subgraphs appended to
the original graphs without requiring retraining of pre-trained graph models
across various applications. This novel paradigm, shifting from the traditional
pretraining and finetuning to pretraining and prompting has shown significant
empirical success in simulating graph data operations, with applications
ranging from recommendation systems to biological networks and graph
transferring. However, despite its potential, the theoretical underpinnings of
graph prompting remain underexplored, raising critical questions about its
fundamental effectiveness. The lack of rigorous theoretical proof of why and
how much it works is more like a dark cloud over the graph prompt area to go
further. To fill this gap, this paper introduces a theoretical framework that
rigorously analyzes graph prompting from a data operation perspective. Our
contributions are threefold: First, we provide a formal guarantee theorem,
demonstrating graph prompts capacity to approximate graph transformation
operators, effectively linking upstream and downstream tasks. Second, we derive
upper bounds on the error of these data operations by graph prompts for a
single graph and extend this discussion to batches of graphs, which are common
in graph model training. Third, we analyze the distribution of data operation
errors, extending our theoretical findings from linear graph models (e.g., GCN)
to non-linear graph models (e.g., GAT). Extensive experiments support our
theoretical results and confirm the practical implications of these guarantees.

</details>


### [713] [What is Fair? Defining Fairness in Machine Learning for Health](https://arxiv.org/pdf/2406.09307)
*Jianhui Gao, Benson Chou, Zachary R. McCaw, Hilary Thurston, Paul Varghese, Chuan Hong, Jessica Gronsbell*

Main category: cs.LG

TL;DR: The paper examines fairness in ML for healthcare, addressing why models may be unfair, how fairness is measured, and future research directions.


<details>
  <summary>Details</summary>
Motivation: To ensure ML models in healthcare are safe, effective, and equitable, preventing health disparities.

Method: Review of fairness notions in group, individual, and causal frameworks, and their application in real-world healthcare settings.

Result: Identifies gaps and challenges in measuring and operationalizing fairness in ML for health.

Conclusion: Highlights future research opportunities to improve fairness in healthcare ML applications.

Abstract: Ensuring that machine learning (ML) models are safe, effective, and equitable
across all patients is critical for clinical decision-making and for preventing
the amplification of existing health disparities. In this work, we examine how
fairness is conceptualized in ML for health, including why ML models may lead
to unfair decisions and how fairness has been measured in diverse real-world
applications. We review commonly used fairness notions within group,
individual, and causal-based frameworks. We also discuss the outlook for future
research and highlight opportunities and challenges in operationalizing
fairness in health-focused applications.

</details>


### [714] [Random Walk Diffusion for Efficient Large-Scale Graph Generation](https://arxiv.org/pdf/2408.04461)
*Tobias Bernecker, Ghalia Rehawi, Francesco Paolo Casale, Janine Knauer-Arloth, Annalisa Marsico*

Main category: cs.LG

TL;DR: ARROW-Diff, a random walk-based diffusion method, efficiently generates large-scale graphs by combining random walk sampling and graph pruning, outperforming baselines in speed and quality.


<details>
  <summary>Details</summary>
Motivation: Existing diffusion-based graph generation methods struggle with scalability for large graphs.

Method: ARROW-Diff uses an iterative process of random walk sampling and graph pruning.

Result: The method scales efficiently to large graphs, outperforming baselines in generation time and graph statistics.

Conclusion: ARROW-Diff is a promising approach for scalable and high-quality graph generation.

Abstract: Graph generation addresses the problem of generating new graphs that have a
data distribution similar to real-world graphs. While previous diffusion-based
graph generation methods have shown promising results, they often struggle to
scale to large graphs. In this work, we propose ARROW-Diff (AutoRegressive
RandOm Walk Diffusion), a novel random walk-based diffusion approach for
efficient large-scale graph generation. Our method encompasses two components
in an iterative process of random walk sampling and graph pruning. We
demonstrate that ARROW-Diff can scale to large graphs efficiently, surpassing
other baseline methods in terms of both generation time and multiple graph
statistics, reflecting the high quality of the generated graphs.

</details>


### [715] [Foundation Models on a Budget: Approximating Blocks in Large Vision Models](https://arxiv.org/pdf/2410.04941)
*Irene Cannistraci, Simone Antonelli, Emanuele Palumbo, Thomas M. Sutter, Emanuele Rodolà, Bastian Rieck, Julia E. Vogt*

Main category: cs.LG

TL;DR: TBA leverages intra-network similarities to approximate transformer blocks in large vision models, reducing parameters without retraining, while maintaining performance.


<details>
  <summary>Details</summary>
Motivation: Foundation models require massive resources, and existing size-reduction methods increase computational load. Intra-network similarities are underexplored for efficiency.

Method: Transformer Blocks Approximation (TBA) identifies and approximates transformer blocks using lightweight transformations, avoiding retraining.

Result: TBA reduces parameters with minimal performance impact, validated on datasets like Imagenet-1k and models like ViT.

Conclusion: TBA effectively improves efficiency in large vision models by exploiting intra-network similarities.

Abstract: Foundation Models have shown impressive performance in various tasks and
domains, yet they require massive computational resources, raising concerns
about accessibility and sustainability. Previous attempts to reduce foundation
model size fall short of fully addressing the problem, as they end up
increasing computational load through additional training steps. Recent works
reveal that deep neural networks exhibit internal representation similarities.
While inter-network similarities have enabled techniques such as model
stitching and merging, intra-network similarities remain underexplored for
improving efficiency. In this paper, we propose Transformer Blocks
Approximation (TBA), a novel method that leverages intra-network similarities
to identify and approximate transformer blocks in large vision models. TBA
replaces these blocks using lightweight, closed-form transformations, without
retraining or fine-tuning the rest of the model. The proposed method reduces
the number of parameters while having minimal impact on the downstream task. We
validate the effectiveness and generalizability of TBA through extensive
experiments across multiple datasets (e.g., Imagenet-1k and CIFAR100) and
state-of-the-art pretrained vision models (e.g, ViT, DiNO-v2, and DEiT).

</details>


### [716] [Advancing Molecular Machine Learning Representations with Stereoelectronics-Infused Molecular Graphs](https://arxiv.org/pdf/2408.04520)
*Daniil A. Boiko, Thiago Reschützegger, Benjamin Sanchez-Lengeling, Samuel M. Blau, Gabe Gomes*

Main category: cs.LG

TL;DR: A novel method enhances molecular graphs with quantum-chemical-rich stereoelectronic effects, improving machine learning models for molecular property prediction and enabling applications to larger systems like proteins.


<details>
  <summary>Details</summary>
Motivation: Existing molecular representations are information-sparse, limiting performance in complex prediction tasks. Higher-fidelity representations are needed.

Method: Introduces stereoelectronics-infused molecular graphs and a double graph neural network workflow to predict these representations without costly quantum calculations.

Result: Explicit stereoelectronic information boosts 2D model performance and extrapolates well to larger molecules, even proteins. A web app facilitates exploration.

Conclusion: The approach advances molecular machine learning by enriching representations with quantum-chemical insights, enabling broader applications and deeper chemical understanding.

Abstract: Molecular representation is a critical element in our understanding of the
physical world and the foundation for modern molecular machine learning.
Previous molecular machine learning models have employed strings, fingerprints,
global features, and simple molecular graphs that are inherently
information-sparse representations. However, as the complexity of prediction
tasks increases, the molecular representation needs to encode higher fidelity
information. This work introduces a novel approach to infusing
quantum-chemical-rich information into molecular graphs via stereoelectronic
effects, enhancing expressivity and interpretability. Learning to predict the
stereoelectronics-infused representation with a tailored double graph neural
network workflow enables its application to any downstream molecular machine
learning task without expensive quantum chemical calculations. We show that the
explicit addition of stereoelectronic information significantly improves the
performance of message-passing 2D machine learning models for molecular
property prediction. We show that the learned representations trained on small
molecules can accurately extrapolate to much larger molecular structures,
yielding chemical insight into orbital interactions for previously intractable
systems, such as entire proteins, opening new avenues of molecular design.
Finally, we have developed a web application (simg.cheme.cmu.edu) where users
can rapidly explore stereoelectronic information for their own molecular
systems.

</details>


### [717] [AI Learning Algorithms: Deep Learning, Hybrid Models, and Large-Scale Model Integration](https://arxiv.org/pdf/2410.09186)
*Noorbakhsh Amiri Golilarz, Elias Hossain, Abdoljalil Addeh, Keyan Alexander Rahimi*

Main category: cs.LG

TL;DR: The paper provides an overview of learning algorithms, covering AI, ML, DL, and hybrid models, with a focus on CNNs, their applications, vulnerabilities, and integration with LLMs for diverse domains. It also discusses future directions like unified adaptive networks.


<details>
  <summary>Details</summary>
Motivation: To review and explain the significance of learning algorithms in various applications, highlighting their current state, challenges, and future potential.

Method: The paper reviews key concepts in AI, ML, and DL, explores subsets like supervised and unsupervised learning, and examines CNN architectures and hybrid models. It also analyzes vulnerabilities and integration with LLMs.

Result: The paper outlines the versatility of learning algorithms in tasks like prediction and classification, their susceptibility to noise, and their potential when combined with LLMs for domain-specific applications.

Conclusion: The paper concludes by summarizing the current state of learning algorithms, their broad applications, and future advancements like adaptive networks, emphasizing their evolving role in technology.

Abstract: In this paper, we discuss learning algorithms and their importance in
different types of applications which includes training to identify important
patterns and features in a straightforward, easy-to-understand manner. We will
review the main concepts of artificial intelligence (AI), machine learning
(ML), deep learning (DL), and hybrid models. Some important subsets of Machine
Learning algorithms such as supervised, unsupervised, and reinforcement
learning are also discussed in this paper. These techniques can be used for
some important tasks like prediction, classification, and segmentation.
Convolutional Neural Networks (CNNs) are used for image and video processing
and many more applications. We dive into the architecture of CNNs and how to
integrate CNNs with ML algorithms to build hybrid models. This paper explores
the vulnerability of learning algorithms to noise, leading to
misclassification. We further discuss the integration of learning algorithms
with Large Language Models (LLM) to generate coherent responses applicable to
many domains such as healthcare, marketing, and finance by learning important
patterns from large volumes of data. Furthermore, we discuss the next
generation of learning algorithms and how we may have an unified Adaptive and
Dynamic Network to perform important tasks. Overall, this article provides
brief overview of learning algorithms, exploring their current state,
applications and future direction.

</details>


### [718] [Amortized Bayesian Workflow](https://arxiv.org/pdf/2409.04332)
*Chengkun Li, Aki Vehtari, Paul-Christian Bürkner, Stefan T. Radev, Luigi Acerbi, Marvin Schmitt*

Main category: cs.LG

TL;DR: An adaptive workflow combines fast amortized inference with accurate MCMC, guided by diagnostics, to balance speed and accuracy for large-scale Bayesian inference.


<details>
  <summary>Details</summary>
Motivation: Address the trade-off between computational speed and sampling accuracy in Bayesian inference for many datasets.

Method: Integrates amortized inference (generative neural networks) and MCMC, using diagnostics to choose the method per dataset, and reuses computations.

Result: Demonstrated efficiency gains while maintaining high posterior quality on synthetic and real-world problems.

Conclusion: The workflow effectively balances speed and accuracy, making it suitable for large-scale Bayesian inference.

Abstract: Bayesian inference often faces a trade-off between computational speed and
sampling accuracy. We propose an adaptive workflow that integrates rapid
amortized inference with gold-standard MCMC techniques to achieve a favorable
combination of both speed and accuracy when performing inference on many
observed datasets. Our approach uses principled diagnostics to guide the choice
of inference method for each dataset, moving along the Pareto front from fast
amortized sampling via generative neural networks to slower but
guaranteed-accurate MCMC when needed. By reusing computations across steps, our
workflow synergizes amortized and MCMC-based inference. We demonstrate the
effectiveness of this integrated approach on several synthetic and real-world
problems with tens of thousands of datasets, showing efficiency gains while
maintaining high posterior quality.

</details>


### [719] [Symmetry constrained neural networks for detection and localization of damage in metal plates](https://arxiv.org/pdf/2409.06084)
*James Amarel, Christopher Rudolf, Athanasios Iliopoulos, John Michopoulos, Leslie N. Smith*

Main category: cs.LG

TL;DR: Deep learning detects and localizes damage in an aluminum plate with high accuracy using Lamb waves and piezoelectric sensors.


<details>
  <summary>Details</summary>
Motivation: To improve damage detection and localization in thin aluminum plates using deep learning techniques.

Method: Used four piezoelectric transducers to generate and receive Lamb waves, trained a neural network on time-series data to detect and localize damage.

Result: Achieved >99% detection accuracy and 2.58 ± 0.12 mm mean distance error for localization.

Conclusion: Deep learning with inductive bias for sensor arrangement effectively detects and localizes damage in uniform plates.

Abstract: The present paper is concerned with deep learning techniques applied to
detection and localization of damage in a thin aluminum plate. We used data
collected on a tabletop apparatus by mounting to the plate four piezoelectric
transducers, each of which took turn to generate a Lamb wave that then
traversed the region of interest before being received by the remaining three
sensors. On training a neural network to analyze time-series data of the
material response, which displayed damage-reflective features whenever the
plate guided waves interacted with a contact load, we achieved a model that
detected with greater than $99\%$ accuracy in addition to a model that
localized with $2.58 \pm 0.12$ mm mean distance error. For each task, the
best-performing model was designed according to the inductive bias that our
transducers were both similar and arranged in a square pattern on a nearly
uniform plate.

</details>


### [720] [RE-Bench: Evaluating frontier AI R&D capabilities of language model agents against human experts](https://arxiv.org/pdf/2411.15114)
*Hjalmar Wijk, Tao Lin, Joel Becker, Sami Jawhar, Neev Parikh, Thomas Broadley, Lawrence Chan, Michael Chen, Josh Clymer, Jai Dhyani, Elena Ericheva, Katharyn Garcia, Brian Goodrich, Nikola Jurkovic, Holden Karnofsky, Megan Kinniment, Aron Lajko, Seraphina Nix, Lucas Sato, William Saunders, Maksym Taran, Ben West, Elizabeth Barnes*

Main category: cs.LG

TL;DR: RE-Bench evaluates AI vs. human performance in ML research engineering, showing AI excels in speed but humans adapt better with time.


<details>
  <summary>Details</summary>
Motivation: To address the lack of realistic evaluations for AI R&D capabilities compared to humans, especially in ML research engineering.

Method: RE-Bench includes 7 ML research engineering environments, comparing 61 human experts and AI agents under varying time budgets.

Result: AI agents outperform humans in speed (4x higher score in 2 hours), but humans surpass AI with longer time budgets (2x score in 32 hours).

Conclusion: AI shows promise in ML research engineering but humans adapt better to extended tasks, highlighting complementary strengths.

Abstract: Frontier AI safety policies highlight automation of AI research and
development (R&D) by AI agents as an important capability to anticipate.
However, there exist few evaluations for AI R&D capabilities, and none that are
highly realistic and have a direct comparison to human performance. We
introduce RE-Bench (Research Engineering Benchmark, v1), which consists of 7
challenging, open-ended ML research engineering environments and data from 71
8-hour attempts by 61 distinct human experts. We confirm that our experts make
progress in the environments given 8 hours, with 82% of expert attempts
achieving a non-zero score and 24% matching or exceeding our strong reference
solutions. We compare humans to several public frontier models through
best-of-k with varying time budgets and agent designs, and find that the best
AI agents achieve a score 4x higher than human experts when both are given a
total time budget of 2 hours per environment. However, humans currently display
better returns to increasing time budgets, narrowly exceeding the top AI agent
scores given an 8-hour budget, and achieving 2x the score of the top AI agent
when both are given 32 total hours (across different attempts). Qualitatively,
we find that modern AI agents possess significant expertise in many ML topics
-- e.g. an agent wrote a faster custom Triton kernel than any of our human
experts' -- and can generate and test solutions over ten times faster than
humans, at much lower cost. We open-source the evaluation environments, human
expert data, analysis code and agent trajectories to facilitate future
research.

</details>


### [721] [Implicit Dynamical Flow Fusion (IDFF) for Generative Modeling](https://arxiv.org/pdf/2409.14599)
*Mohammad R. Rezaei, Milos R. Popovic, Milad Lankarany, Rahul G. Krishnan*

Main category: cs.LG

TL;DR: IDFF improves CFM by reducing NFEs by 10x without quality loss, enabling faster sampling for image and time-series data.


<details>
  <summary>Details</summary>
Motivation: CFMs are slow due to high NFEs, limiting practical use. IDFF aims to speed up sampling while maintaining quality.

Method: IDFF introduces a momentum term in the vector field for longer steps during generation, reducing NFEs.

Result: IDFF matches CFM and diffusion models in quality on CIFAR-10 and CelebA with fewer NFEs, and excels in time-series tasks.

Conclusion: IDFF is versatile and effective, offering faster sampling without compromising quality across domains.

Abstract: Conditional Flow Matching (CFM) models can generate high-quality samples from
a non-informative prior, but they can be slow, often needing hundreds of
network evaluations (NFE). To address this, we propose Implicit Dynamical Flow
Fusion (IDFF); IDFF learns a new vector field with an additional momentum term
that enables taking longer steps during sample generation while maintaining the
fidelity of the generated distribution. Consequently, IDFFs reduce the NFEs by
a factor of ten (relative to CFMs) without sacrificing sample quality, enabling
rapid sampling and efficient handling of image and time-series data generation
tasks. We evaluate IDFF on standard benchmarks such as CIFAR-10 and CelebA for
image generation, where we achieve likelihood and quality performance
comparable to CFMs and diffusion-based models with fewer NFEs. IDFF also shows
superior performance on time-series datasets modeling, including molecular
simulation and sea surface temperature (SST) datasets, highlighting its
versatility and effectiveness across different
domains.\href{https://github.com/MrRezaeiUofT/IDFF}{Github Repository}

</details>


### [722] [RL-SPH: Learning to Achieve Feasible Solutions for Integer Linear Programs](https://arxiv.org/pdf/2411.19517)
*Tae-Hoon Lee, Min-Soo Kim*

Main category: cs.LG

TL;DR: RL-SPH, a reinforcement learning-based primal heuristic, independently generates feasible solutions for ILP, outperforming existing methods with a 44x lower primal gap and 2.3x lower primal integral.


<details>
  <summary>Details</summary>
Motivation: Existing end-to-end learning-based primal heuristics struggle with feasibility, especially for non-binary integer variables in ILP.

Method: Proposes RL-SPH, a reinforcement learning-based start primal heuristic, to independently generate feasible solutions for ILP, including non-binary integers.

Result: RL-SPH achieves a 44x lower primal gap and 2.3x lower primal integral compared to existing heuristics.

Conclusion: RL-SPH effectively addresses feasibility challenges in ILP, providing high-quality solutions efficiently.

Abstract: Integer linear programming (ILP) is widely utilized for various combinatorial
optimization problems. Primal heuristics play a crucial role in quickly finding
feasible solutions for NP-hard ILP. Although \textit{end-to-end learning}-based
primal heuristics (E2EPH) have recently been proposed, they are typically
unable to independently generate feasible solutions and mainly focus on binary
variables. Ensuring feasibility is critical, especially when handling
non-binary integer variables. To address this challenge, we propose RL-SPH, a
novel reinforcement learning-based start primal heuristic capable of
independently generating feasible solutions, even for ILP involving non-binary
integers. Experimental results demonstrate that RL-SPH rapidly obtains
high-quality feasible solutions, achieving on average a 44x lower primal gap
and a 2.3x lower primal integral compared to existing primal heuristics.

</details>


### [723] [Understanding and Mitigating Miscalibration in Prompt Tuning for Vision-Language Models](https://arxiv.org/pdf/2410.02681)
*Shuoyuan Wang, Yixuan Li, Hongxin Wei*

Main category: cs.LG

TL;DR: The paper addresses confidence calibration in vision-language models like CLIP after fine-tuning, introducing Dynamic Outlier Regularization (DOR) to balance calibration between base and new classes.


<details>
  <summary>Details</summary>
Motivation: Existing prompt tuning methods create a trade-off in calibration between base and new classes, leading to overconfidence or underconfidence.

Method: Proposes DOR to minimize feature deviation of novel textual labels, preventing textual divergence for new labels while easing restrictions on base classes.

Result: DOR improves calibration performance on both base and new classes in fine-tuning methods.

Conclusion: DOR effectively balances confidence calibration, enhancing model reliability for real-world deployment.

Abstract: Confidence calibration is critical for the safe deployment of machine
learning models in the real world. However, such issue in vision-language
models like CLIP, particularly after fine-tuning, has not been fully addressed.
In this work, we demonstrate that existing prompt tuning methods usually lead
to a trade-off of calibration between base and new classes: the cross-entropy
loss in CoOp causes overconfidence in new classes by increasing textual label
divergence, whereas the regularization of KgCoOp maintains the confidence level
but results in underconfidence in base classes due to the improved accuracy.
Inspired by the observations, we introduce Dynamic Outlier Regularization (DOR)
to ensure the confidence calibration on both base and new classes after
fine-tuning. In particular, we propose to minimize the feature deviation of
novel textual labels (instead of base classes) sampled from a large vocabulary.
In effect, DOR prevents the increase in textual divergence for new labels while
easing restrictions on base classes. Extensive experiments demonstrate that DOR
can enhance the calibration performance of current fine-tuning methods on base
and new classes.

</details>


### [724] [Interlocking-free Selective Rationalization Through Genetic-based Learning](https://arxiv.org/pdf/2412.10312)
*Federico Ruggeri, Gaetano Signorelli*

Main category: cs.LG

TL;DR: GenSPP is an interlocking-free architecture for selective rationalization, avoiding suboptimal equilibria by disjoint training via genetic global search, outperforming state-of-the-art methods.


<details>
  <summary>Details</summary>
Motivation: The select-then-predict pipeline suffers from interlocking, where one module dominates, leading to suboptimal performance. Existing solutions only mitigate this issue with heuristics or ad-hoc methods.

Method: GenSPP uses disjoint training of the generator and predictor via genetic global search to avoid interlocking, without additional learning overhead.

Result: Experiments on synthetic and real-world benchmarks show GenSPP outperforms state-of-the-art competitors.

Conclusion: GenSPP effectively eliminates interlocking in selective rationalization, offering a robust and efficient solution.

Abstract: A popular end-to-end architecture for selective rationalization is the
select-then-predict pipeline, comprising a generator to extract highlights fed
to a predictor. Such a cooperative system suffers from suboptimal equilibrium
minima due to the dominance of one of the two modules, a phenomenon known as
interlocking. While several contributions aimed at addressing interlocking,
they only mitigate its effect, often by introducing feature-based heuristics,
sampling, and ad-hoc regularizations. We present GenSPP, the first
interlocking-free architecture for selective rationalization that does not
require any learning overhead, as the above-mentioned. GenSPP avoids
interlocking by performing disjoint training of the generator and predictor via
genetic global search. Experiments on a synthetic and a real-world benchmark
show that our model outperforms several state-of-the-art competitors.

</details>


### [725] [New Paradigm of Adversarial Training: Releasing Accuracy-Robustness Trade-Off via Dummy Class](https://arxiv.org/pdf/2410.12671)
*Yanyun Wang, Li Liu, Zi Liang, Yi R., Fung, Qingqing Ye, Haibo Hu*

Main category: cs.LG

TL;DR: The paper introduces DUCAT, a new adversarial training paradigm using dummy classes to address the accuracy-robustness trade-off in DNNs, outperforming SOTA methods.


<details>
  <summary>Details</summary>
Motivation: Existing adversarial training methods suffer from an inherent accuracy-robustness trade-off, with adversarial samples often failing to align with benign samples under current assumptions.

Method: Proposes a new AT paradigm with dummy classes for adversarial samples, enabling runtime recovery to original classes without conflicting with clean accuracy. Introduces DUCAT, a plug-and-play method using logits, loss, and two-hot soft labels.

Result: DUCAT outperforms SOTA benchmarks, improving both accuracy and robustness. Empirical results show up to 40% of adversarial samples fail current AT assumptions.

Conclusion: The new paradigm and DUCAT method effectively release the accuracy-robustness trade-off, offering a practical solution for robust DNN training.

Abstract: Adversarial Training (AT) is one of the most effective methods to enhance the
robustness of Deep Neural Networks (DNNs). However, existing AT methods suffer
from an inherent accuracy-robustness trade-off. Previous works have studied
this issue under the current AT paradigm, but still face over 10% accuracy
reduction without significant robustness improvement over simple baselines such
as PGD-AT. This inherent trade-off raises a question: Whether the current AT
paradigm, which assumes to learn corresponding benign and adversarial samples
as the same class, inappropriately mixes clean and robust objectives that may
be essentially inconsistent. In fact, our empirical results show that up to 40%
of CIFAR-10 adversarial samples always fail to satisfy such an assumption
across various AT methods and robust models, explicitly indicating the room for
improvement of the current AT paradigm. To relax from this overstrict
assumption and the tension between clean and robust learning, in this work, we
propose a new AT paradigm by introducing an additional dummy class for each
original class, aiming to accommodate hard adversarial samples with shifted
distribution after perturbation. The robustness w.r.t. these adversarial
samples can be achieved by runtime recovery from the predicted dummy classes to
the corresponding original ones, without conflicting with the clean objective
on accuracy of benign samples. Finally, based on our new paradigm, we propose a
novel DUmmy Classes-based Adversarial Training (DUCAT) method that concurrently
improves accuracy and robustness in a plug-and-play manner only relevant to
logits, loss, and a proposed two-hot soft label-based supervised signal. Our
method outperforms state-of-the-art (SOTA) benchmarks, effectively releasing
the current trade-off. The code is available at https://github.com/FlaAI/DUCAT.

</details>


### [726] [EPIC: Efficient Position-Independent Caching for Serving Large Language Models](https://arxiv.org/pdf/2410.15332)
*Junhao Hu, Wenrui Huang, Weidong Wang, Haoyi Wang, Tiancheng Hu, Qin Zhang, Hao Feng, Xusheng Chen, Yizhou Shan, Tao Xie*

Main category: cs.LG

TL;DR: EPIC introduces Position-Independent Caching (PIC) and LegoLink to improve LLM serving efficiency by reusing KV vectors regardless of prefixes, achieving significant performance gains.


<details>
  <summary>Details</summary>
Motivation: Efficiently serving LLMs is challenging due to complex prompts; existing context caching lacks flexibility for varied prefixes.

Method: EPIC uses PIC and LegoLink to modularly reuse KV vectors, addressing the 'attention sink' effect for accuracy.

Result: EPIC achieves up to 8x faster TTFT and 7x throughput gains with minimal accuracy loss.

Conclusion: EPIC significantly enhances LLM serving efficiency without compromising accuracy.

Abstract: Large Language Models (LLMs) show great capabilities in a wide range of
applications, but serving them efficiently becomes increasingly challenging as
requests (prompts) become more complex. Context caching improves serving
performance by reusing Key-Value (KV) vectors, the intermediate representations
of tokens that are repeated across requests. However, existing context caching
requires exact prefix matches across requests, limiting reuse cases in settings
such as few-shot learning and retrieval-augmented generation, where immutable
content (e.g., documents) remains unchanged across requests but is preceded by
varying prefixes. Position-Independent Caching (PIC) addresses this issue by
enabling modular reuse of the KV vectors regardless of prefixes. We formalize
PIC and advance prior work by introducing EPIC, a serving system incorporating
our new LegoLink algorithm, which mitigates the inappropriate "attention sink"
effect at every document beginning, to maintain accuracy with minimal
computation. Experiments show that EPIC achieves up to 8x improvements in
Time-To-First-Token (TTFT) and 7x throughput gains over existing systems, with
negligible or no accuracy loss.

</details>


### [727] [GMoE: Empowering LLMs Fine-Tuning via MoE Graph Collaboration](https://arxiv.org/pdf/2412.16216)
*Ting Bai, Yue Yu, Le Huang, Zenan Xu, Zhe Zhao, Chuan Shi*

Main category: cs.LG

TL;DR: GMoE introduces a graph-based MoE framework to address load imbalance in LLMs by enhancing expert collaboration via a graph router and coordination strategies.


<details>
  <summary>Details</summary>
Motivation: The linear router in sparse MoE architectures causes load imbalance, instability, and inefficient learning in LLMs.

Method: GMoE uses a graph router for dynamic expert collaboration and two coordination strategies (Poisson and Normal distribution-based) for stability. LoRA is employed for efficient fine-tuning.

Result: Experiments on four datasets show GMoE's effectiveness in improving expert collaboration and LLM fine-tuning stability.

Conclusion: GMoE successfully mitigates load imbalance and enhances expert collaboration in LLMs, validated by empirical results.

Abstract: The sparse Mixture-of-Experts (MoE) architecture of large language models
(LLMs) confronts an inherent issue of load imbalance arising from the
simplistic linear router strategy, which ultimately causes the instability and
inefficient learning of LLMs. To address this challenge, we introduce a novel
MoE graph-based framework $\textbf{GMoE}$, aimed at enhancing the collaboration
among multiple experts. In GMoE, a graph router function is designed to capture
the collaboration signals among experts. This enables all experts to
dynamically allocate information derived from input data by sharing information
with their neighboring experts. Moreover, we put forward two coordination
strategies in GMoE: the $\textit{Poisson distribution-based distinction
strategy}$ and the $\textit{Normal distribution-based balance strategy}$, to
further release the capacity of each expert and increase the model stability in
the fine-tuning of LLMs. Specifically, we leverage a parameter-efficient
fine-tuning technique, i.e., Low-Rank Adaptation (LoRA), to implement the graph
MoE architecture. Extensive experiments on four real-world benchmark datasets
demonstrate the effectiveness of GMoE, showing the benefits of facilitating
collaborations of multiple experts in LLM fine-tuning. The code of experimental
implementation is available at https://github.com/BAI-LAB/GMoE

</details>


### [728] [Securing Federated Learning against Backdoor Threats with Foundation Model Integration](https://arxiv.org/pdf/2410.17573)
*Xiaohuan Bi, Xi Li*

Main category: cs.LG

TL;DR: Proposes a data-free defense strategy for Federated Learning (FL) to counter novel backdoor attacks exploiting Foundation Models (FMs), outperforming existing defenses.


<details>
  <summary>Details</summary>
Motivation: Existing FL backdoor defenses fail against new attacks exploiting FMs, necessitating a novel approach.

Method: Constrains abnormal activations in hidden feature space during model aggregation, optimized using synthetic data.

Result: Effective against both novel and classic backdoor attacks, preserving model functionality.

Conclusion: The proposed strategy successfully mitigates backdoor attacks in FL, offering robust protection.

Abstract: Federated Learning (FL) enables decentralized model training while preserving
privacy. Recently, the integration of Foundation Models (FMs) into FL has
enhanced performance but introduced a novel backdoor attack mechanism.
Attackers can exploit FM vulnerabilities to embed backdoors into synthetic data
generated by FMs. During global model fusion, these backdoors are transferred
to the global model through compromised synthetic data, subsequently infecting
all client models. Existing FL backdoor defenses are ineffective against this
novel attack due to its fundamentally different mechanism compared to classic
ones. In this work, we propose a novel data-free defense strategy that
addresses both classic and novel backdoor attacks in FL. The shared attack
pattern lies in the abnormal activations within the hidden feature space during
model aggregation. Hence, we propose to constrain internal activations to
remain within reasonable ranges, effectively mitigating attacks while
preserving model functionality. The activation constraints are optimized using
synthetic data alongside FL training. Extensive experiments demonstrate its
effectiveness against both novel and classic backdoor attacks, outperforming
existing defenses.

</details>


### [729] [On the Robustness of Adversarial Training Against Uncertainty Attacks](https://arxiv.org/pdf/2410.21952)
*Emanuele Ledda, Giovanni Scodeller, Daniele Angioni, Giorgio Piras, Antonio Emanuele Cinà, Giorgio Fumera, Battista Biggio, Fabio Roli*

Main category: cs.LG

TL;DR: Defending against adversarial examples also improves the robustness of uncertainty estimates, making them more trustworthy without needing specialized defenses.


<details>
  <summary>Details</summary>
Motivation: Security-sensitive applications require trustworthy uncertainty measures, but attackers can manipulate these estimates to compromise system reliability.

Method: Evaluate adversarial-robust models from RobustBench on CIFAR-10 and ImageNet to show that adversarial defense inherently secures uncertainty estimates.

Result: Empirical and theoretical evidence confirms that adversarial defense enhances the robustness of uncertainty estimates.

Conclusion: Robustness against adversarial examples naturally extends to securing uncertainty measures, eliminating the need for additional defenses.

Abstract: In learning problems, the noise inherent to the task at hand hinders the
possibility to infer without a certain degree of uncertainty. Quantifying this
uncertainty, regardless of its wide use, assumes high relevance for
security-sensitive applications. Within these scenarios, it becomes fundamental
to guarantee good (i.e., trustworthy) uncertainty measures, which downstream
modules can securely employ to drive the final decision-making process.
However, an attacker may be interested in forcing the system to produce either
(i) highly uncertain outputs jeopardizing the system's availability or (ii) low
uncertainty estimates, making the system accept uncertain samples that would
instead require a careful inspection (e.g., human intervention). Therefore, it
becomes fundamental to understand how to obtain robust uncertainty estimates
against these kinds of attacks. In this work, we reveal both empirically and
theoretically that defending against adversarial examples, i.e., carefully
perturbed samples that cause misclassification, additionally guarantees a more
secure, trustworthy uncertainty estimate under common attack scenarios without
the need for an ad-hoc defense strategy. To support our claims, we evaluate
multiple adversarial-robust models from the publicly available benchmark
RobustBench on the CIFAR-10 and ImageNet datasets.

</details>


### [730] [Unraveling Indirect In-Context Learning Using Influence Functions](https://arxiv.org/pdf/2501.01473)
*Hadi Askari, Shivanshu Gupta, Terry Tong, Fei Wang, Anshuman Chhabra, Muhao Chen*

Main category: cs.LG

TL;DR: The paper introduces Indirect In-Context Learning (ICL), focusing on demonstration selection for Mixture of Tasks and Noisy ICL scenarios. It evaluates Influence Functions (IFs) for selection, showing improved accuracy and robustness.


<details>
  <summary>Details</summary>
Motivation: To address limitations of traditional ICL by developing a generalized framework (Indirect ICL) for real-world scenarios like diverse tasks and noisy data.

Method: Uses Influence Functions (IFs) for demonstration selection, combining them with metrics like BertScore-Recall (BSR) and Cosine Similarity. Evaluated on tasks like MMLU, BigBench, and noisy GLUE benchmarks.

Result: Achieves accuracy gains (0.37% to 1.45%) in Mixture of Tasks and 2.90%-2.94% in Noisy ICL. Reduces Attack Success Rate by 32.89% in adversarial settings.

Conclusion: Proposes a robust framework leveraging IFs for Indirect ICL, demonstrating effectiveness in diverse and noisy scenarios.

Abstract: In this work, we introduce a novel paradigm for generalized In-Context
Learning (ICL), termed Indirect In-Context Learning. In Indirect ICL, we
explore demonstration selection strategies tailored for two distinct real-world
scenarios: Mixture of Tasks and Noisy ICL. We systematically evaluate the
effectiveness of Influence Functions (IFs) as a selection tool for these
settings, highlighting the potential of IFs to better capture the
informativeness of examples within the demonstration pool. For the Mixture of
Tasks setting, demonstrations are drawn from 28 diverse tasks, including MMLU,
BigBench, StrategyQA, and CommonsenseQA. We demonstrate that combining
BertScore-Recall (BSR) with an IF surrogate model can further improve
performance, leading to average absolute accuracy gains of 0.37\% and 1.45\%
for 3-shot and 5-shot setups when compared to traditional ICL metrics. In the
Noisy ICL setting, we examine scenarios where demonstrations might be
mislabeled or have adversarial noise. Our experiments show that reweighting
traditional ICL selectors (BSR and Cosine Similarity) with IF-based selectors
boosts accuracy by an average of 2.90\% for Cosine Similarity and 2.94\% for
BSR on noisy GLUE benchmarks. For the adversarial sub-setting, we show the
utility of using IFs for task-agnostic demonstration selection for backdoor
attack mitigation. Showing a 32.89\% reduction in Attack Success Rate compared
to task-aware methods. In sum, we propose a robust framework for demonstration
selection that generalizes beyond traditional ICL, offering valuable insights
into the role of IFs for Indirect ICL.

</details>


### [731] [Controlling Participation in Federated Learning with Feedback](https://arxiv.org/pdf/2411.19242)
*Michael Cummins, Guner Dilsad Er, Michael Muehlebach*

Main category: cs.LG

TL;DR: FedBack improves federated learning efficiency by using control-theoretic principles to manage client participation deterministically, achieving 50% better performance than random selection methods.


<details>
  <summary>Details</summary>
Motivation: Traditional federated learning relies on random client selection, which is inefficient. FedBack aims to optimize participation for better performance.

Method: FedBack models client participation as a dynamical system, using an integral feedback controller to adjust participation rates based on optimization dynamics.

Result: FedBack achieves up to 50% improvement in communication and computational efficiency over random selection methods.

Conclusion: FedBack provides a deterministic, efficient alternative to random client selection in federated learning, with proven convergence guarantees.

Abstract: We address the problem of client participation in federated learning, where
traditional methods typically rely on a random selection of a small subset of
clients for each training round. In contrast, we propose FedBack, a
deterministic approach that leverages control-theoretic principles to manage
client participation in ADMM-based federated learning. FedBack models client
participation as a discrete-time dynamical system and employs an integral
feedback controller to adjust each client's participation rate individually,
based on the client's optimization dynamics. We provide global convergence
guarantees for our approach by building on the recent federated learning
research. Numerical experiments on federated image classification demonstrate
that FedBack achieves up to 50\% improvement in communication and computational
efficiency over algorithms that rely on a random selection of clients.

</details>


### [732] [More is not always better? Enhancing Many-Shot In-Context Learning with Differentiated and Reweighting Objectives](https://arxiv.org/pdf/2501.04070)
*Xiaoqing Zhang, Ang Lv, Yuhan Liu, Flood Sung, Wei Liu, Jian Luan, Shuo Shang, Xiuying Chen, Rui Yan*

Main category: cs.LG

TL;DR: DrICL improves many-shot in-context learning (ICL) by optimizing NLL and reweighting noisy data, outperforming zero-shot and few-shot methods.


<details>
  <summary>Details</summary>
Motivation: Performance plateaus or declines as ICL demonstrations increase due to suboptimal NLL and data noise.

Method: DrICL uses differentiated learning and dynamic reweighting to optimize NLL and reduce noise impact.

Result: LLMs with DrICL show significant improvements in many-shot ICL across diverse tasks.

Conclusion: DrICL and the ICL-50 benchmark advance many-shot ICL research, with code and dataset released for further study.

Abstract: Large language models (LLMs) excel at few-shot in-context learning (ICL)
without requiring parameter updates. However, as ICL demonstrations increase
from a few to many, performance tends to plateau and eventually decline. We
identify two primary causes for this trend: the suboptimal negative
log-likelihood (NLL) optimization objective and the incremental data noise. To
address these issues, we introduce \textit{DrICL}, a novel optimization method
that enhances model performance through \textit{Differentiated} and
\textit{Reweighting} objectives. Globally, DrICL utilizes differentiated
learning to optimize the NLL objective, ensuring that many-shot performance
surpasses zero-shot levels. Locally, it dynamically adjusts the weighting of
many-shot demonstrations by leveraging cumulative advantages inspired by
reinforcement learning, thereby mitigating the impact of noisy data.
Recognizing the lack of multi-task datasets with diverse many-shot
distributions, we develop the \textit{Many-Shot ICL Benchmark} (ICL-50)-a
large-scale benchmark of 50 tasks that cover shot numbers from 1 to 350 within
sequences of up to 8,000 tokens-for both fine-tuning and evaluation purposes.
Experimental results demonstrate that LLMs enhanced with DrICL achieve
significant improvements in many-shot setups across various tasks, including
both in-domain and out-of-domain scenarios. We release the code and dataset
hoping to facilitate further research in many-shot
ICL\footnote{https://github.com/xiaoqzhwhu/DrICL}.

</details>


### [733] [Personalized Clustering via Targeted Representation Learning](https://arxiv.org/pdf/2412.13690)
*Xiwen Geng, Suyun Zhao, Yixin Yu, Borui Peng, Pan Du, Hong Chen, Cuiping Li, Mengdie Wang*

Main category: cs.LG

TL;DR: A personalized clustering method is proposed, using user feedback via informative pairs and attention mechanisms to guide clustering, with theoretical guarantees and strong experimental performance.


<details>
  <summary>Details</summary>
Motivation: Traditional clustering may not align with user preferences, so the paper aims to develop a method that incorporates user input for personalized results.

Method: The method uses active queries (e.g., must-link/cannot-link pairs) and attention mechanisms for targeted representation learning, combined with constrained contrastive loss.

Result: Theoretical analysis shows bounded clustering risk, and experiments demonstrate effectiveness across tasks and datasets with limited queries.

Conclusion: The proposed method successfully achieves personalized clustering with theoretical and empirical validation.

Abstract: Clustering traditionally aims to reveal a natural grouping structure within
unlabeled data. However, this structure may not always align with users'
preferences. In this paper, we propose a personalized clustering method that
explicitly performs targeted representation learning by interacting with users
via modicum task information (e.g., $\textit{must-link}$ or
$\textit{cannot-link}$ pairs) to guide the clustering direction. We query users
with the most informative pairs, i.e., those pairs most hard to cluster and
those most easy to miscluster, to facilitate the representation learning in
terms of the clustering preference. Moreover, by exploiting attention
mechanism, the targeted representation is learned and augmented. By leveraging
the targeted representation and constrained contrastive loss as well,
personalized clustering is obtained. Theoretically, we verify that the risk of
personalized clustering is tightly bounded, guaranteeing that active queries to
users do mitigate the clustering risk. Experimentally, extensive results show
that our method performs well across different clustering tasks and datasets,
even when only a limited number of queries are available.

</details>


### [734] [It's complicated. The relationship of algorithmic fairness and non-discrimination regulations for high-risk systems in the EU AI Act](https://arxiv.org/pdf/2501.12962)
*Kristof Meding*

Main category: cs.LG

TL;DR: The paper explores the intersection of legal non-discrimination regulations and algorithmic fairness in the EU's AI Act, highlighting inconsistencies and recommending better auditing methods.


<details>
  <summary>Details</summary>
Motivation: To bridge the gap between legal non-discrimination and algorithmic fairness concepts in the AI Act, fostering interdisciplinary collaboration.

Method: High-level introduction of both concepts, followed by an in-depth analysis of the AI Act's relationship between legal non-discrimination and algorithmic fairness.

Result: Three key findings: (1) Non-discrimination regulations target high-risk AI systems; (2) Input/output regulations are inconsistent and computationally challenging; (3) Future interaction between EU law and AI Act needs clarification.

Conclusion: Recommends specific auditing/testing methodologies for AI systems and serves as a foundation for interdisciplinary research.

Abstract: What constitutes a fair decision? This question is not only difficult for
humans but becomes more challenging when Artificial Intelligence (AI) models
are used. In light of discriminatory algorithmic behaviors, the EU has recently
passed the AI Act, which mandates specific rules for high-risk systems,
incorporating both traditional legal non-discrimination regulations and machine
learning based algorithmic fairness concepts. This paper aims to bridge these
two different concepts in the AI Act through: First, a necessary high-level
introduction of both concepts targeting legal and computer science-oriented
scholars, and second, an in-depth analysis of the AI Act's relationship between
legal non-discrimination regulations and algorithmic fairness. Our analysis
reveals three key findings: (1.) Most non-discrimination regulations target
only high-risk AI systems. (2.) The regulation of high-risk systems encompasses
both data input requirements and output monitoring, though these regulations
are partly inconsistent and raise questions of computational feasibility. (3.)
Finally, we consider the possible (future) interaction of classical EU
non-discrimination law and the AI Act regulations. We recommend developing more
specific auditing and testing methodologies for AI systems. This paper aims to
serve as a foundation for future interdisciplinary collaboration between legal
scholars and computer science-oriented machine learning researchers studying
discrimination in AI systems.

</details>


### [735] [Efficiently Scaling LLM Reasoning with Certaindex](https://arxiv.org/pdf/2412.20993)
*Yichao Fu, Junda Chen, Siqi Zhu, Zheyu Fu, Zhongdongming Dai, Yonghao Zhuang, Yian Ma, Aurick Qiao, Tajana Rosing, Ion Stoica, Hao Zhang*

Main category: cs.LG

TL;DR: Certaindex is a lightweight metric to measure answer stabilization in LLM reasoning algorithms, enabling compute savings and higher throughput without accuracy loss.


<details>
  <summary>Details</summary>
Motivation: Existing reasoning algorithms waste tokens without improving accuracy, prompting the need for a metric to signal when further computation is unnecessary.

Method: Introduces Certaindex, an algorithm-agnostic metric for stability, integrated into Dynasor for dynamic token allocation and early exit.

Result: Achieves up to 50% compute savings and 3.3x higher throughput with no accuracy drop.

Conclusion: Certaindex effectively optimizes LLM reasoning, offering practical benefits for real-world serving systems.

Abstract: Test-time reasoning algorithms such as chain-of-thought, self-consistency,
and MCTS enhance LLM problem-solving but can wastefully generate many tokens
without improving accuracy. At the same time, we observe that these algorithms
exhibit answer stabilization: their intermediate solutions often cease to
change after a certain point, and further investment of compute does not change
their final answer. To quantify this phenomenon, we introduce Certaindex, an
algorithm-agnostic metric measuring this evolving stability, signaling when
further computation is unlikely to alter the final result. Certaindex is
lightweight, can accelerate reasoning program inference via early exit, and
further enables dynamic token allocation, gang scheduling, and many
opportunities when integrated with real-world LLM serving systems. To quantify
real-world benefits, we built Certaindex as a scheduler into Dynasor, our
reasoning-aware LLM serving system, and demonstrate up to 50% compute savings
and 3.3x higher throughput in real workloads with no accuracy drop. Our code is
available at https://github.com/hao-ai-lab/Dynasor.git

</details>


### [736] [ResKoopNet: Learning Koopman Representations for Complex Dynamics with Spectral Residuals](https://arxiv.org/pdf/2501.00701)
*Yuanchao Xu, Kaidi Shao, Nikos Logothetis, Zhongwei Shen*

Main category: cs.LG

TL;DR: ResKoopNet introduces a neural network-based method to minimize spectral residuals for more accurate Koopman operator spectral approximations, outperforming existing methods.


<details>
  <summary>Details</summary>
Motivation: Addressing the limitations of current Koopman operator approximation methods, particularly the 'spectral inclusion' problem in ResDMD.

Method: ResKoopNet minimizes spectral residuals directly using neural networks to compute Koopman eigenpairs, ensuring theoretical guarantees and adaptability.

Result: ResKoopNet achieves more accurate spectral approximations, especially for high-dimensional systems and those with continuous spectra.

Conclusion: ResKoopNet is an effective tool for analyzing complex dynamical systems, offering improved accuracy and completeness in spectral approximations.

Abstract: Analyzing the long-term behavior of high-dimensional nonlinear dynamical
systems remains a significant challenge. While the Koopman operator framework
provides a powerful global linearization tool, current methods for
approximating its spectral components often face theoretical limitations and
depend on predefined dictionaries. Residual Dynamic Mode Decomposition (ResDMD)
advanced the field by introducing the \emph{spectral residual} to assess
Koopman operator approximation accuracy; however, its approach of only
filtering precomputed spectra prevents the discovery of the operator's complete
spectral information, a limitation known as the `spectral inclusion' problem.
We introduce ResKoopNet (Residual-based Koopman-learning Network), a novel
method that directly addresses this by explicitly minimizing the \emph{spectral
residual} to compute Koopman eigenpairs. This enables the identification of a
more precise and complete Koopman operator spectrum. Using neural networks, our
approach provides theoretical guarantees while maintaining computational
adaptability. Experiments on a variety of physical and biological systems show
that ResKoopNet achieves more accurate spectral approximations than existing
methods, particularly for high-dimensional systems and those with continuous
spectra, which demonstrates its effectiveness as a tool for analyzing complex
dynamical systems.

</details>


### [737] [Linear $Q$-Learning Does Not Diverge in $L^2$: Convergence Rates to a Bounded Set](https://arxiv.org/pdf/2501.19254)
*Xinyu Liu, Zixuan Xie, Shangtong Zhang*

Main category: cs.LG

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: $Q$-learning is one of the most fundamental reinforcement learning
algorithms. It is widely believed that $Q$-learning with linear function
approximation (i.e., linear $Q$-learning) suffers from possible divergence
until the recent work Meyn (2024) which establishes the ultimate almost sure
boundedness of the iterates of linear $Q$-learning. Building on this success,
this paper further establishes the first $L^2$ convergence rate of linear
$Q$-learning iterates (to a bounded set). Similar to Meyn (2024), we do not
make any modification to the original linear $Q$-learning algorithm, do not
make any Bellman completeness assumption, and do not make any near-optimality
assumption on the behavior policy. All we need is an $\epsilon$-softmax
behavior policy with an adaptive temperature. The key to our analysis is the
general result of stochastic approximations under Markovian noise with
fast-changing transition functions. As a side product, we also use this general
result to establish the $L^2$ convergence rate of tabular $Q$-learning with an
$\epsilon$-softmax behavior policy, for which we rely on a novel
pseudo-contraction property of the weighted Bellman optimality operator.

</details>


### [738] [Randomly Sampled Language Reasoning Problems Explain Limits of LLMs](https://arxiv.org/pdf/2501.02825)
*Kavi Gupta, Kate Sanders, Armando Solar-Lezama*

Main category: cs.LG

TL;DR: LLMs underperform on novel, simple language tasks compared to n-gram models, suggesting novelty alone can explain their poor performance.


<details>
  <summary>Details</summary>
Motivation: To isolate novelty as a factor in LLM underperformance, focusing on simple, unseen language tasks.

Method: Use next token prediction on randomly generated, simple language tasks derived from grammar rules.

Result: LLMs consistently underperform n-gram models on these novel tasks.

Conclusion: Novelty is a significant factor in LLM underperformance, even in simple domains.

Abstract: While LLMs have revolutionized the field of machine learning due to their
high performance across a range of tasks, they are known to perform poorly in
planning, hallucinate false answers, have degraded performance on less
canonical versions of the same task, and answer incorrectly on a variety of
specific prompts. There are several emerging theories of LLM performance with
some predictive power, among them that LLMs lack world modeling ability, that
they have an undesirable bias towards an autoregressive prior, and that they
perform less well on more novel problems. The existing literature on novelty
has focused on tasks of relatively high complexity, studying perturbations of
canonical but complex problems. In this paper, we attempt to isolate novelty as
a factor in LLM underperformance. To this end, we consider an extremely simple
domain: next token prediction on simple language tasks. The twist is that these
language tasks are unseen, as they are randomly drawn from a large,
parsimoniously defined set of languages arising from simple grammar rules. This
allows us to isolate the effect of task novelty and see if it is sufficient to
explain low performance. We find that LLMs uniformly underperform n-gram models
(which do not have the capacity for world modeling) on these tasks, both when
used as next token predictors and as reasoners.

</details>


### [739] [GenMol: A Drug Discovery Generalist with Discrete Diffusion](https://arxiv.org/pdf/2501.06158)
*Seul Lee, Karsten Kreis, Srimukh Prasad Veccham, Meng Liu, Danny Reidenbach, Yuxing Peng, Saee Paliwal, Weili Nie, Arash Vahdat*

Main category: cs.LG

TL;DR: GenMol is a versatile molecular generative model using a single discrete diffusion model to handle diverse drug discovery tasks, outperforming existing methods.


<details>
  <summary>Details</summary>
Motivation: Existing molecular generative models are limited to specific tasks, lacking versatility for the entire drug discovery process.

Method: GenMol uses a discrete diffusion model with SAFE sequences, fragment remasking, and molecular context guidance (MCG) for optimization.

Result: GenMol outperforms GPT-based models in de novo and fragment-constrained generation, achieving state-of-the-art performance in hit generation and lead optimization.

Conclusion: GenMol provides a unified and versatile approach for molecular design, addressing a wide range of drug discovery tasks.

Abstract: Drug discovery is a complex process that involves multiple stages and tasks.
However, existing molecular generative models can only tackle some of these
tasks. We present Generalist Molecular generative model (GenMol), a versatile
framework that uses only a single discrete diffusion model to handle diverse
drug discovery scenarios. GenMol generates Sequential Attachment-based Fragment
Embedding (SAFE) sequences through non-autoregressive bidirectional parallel
decoding, thereby allowing the utilization of a molecular context that does not
rely on the specific token ordering while having better sampling efficiency.
GenMol uses fragments as basic building blocks for molecules and introduces
fragment remasking, a strategy that optimizes molecules by regenerating masked
fragments, enabling effective exploration of chemical space. We further propose
molecular context guidance (MCG), a guidance method tailored for masked
discrete diffusion of GenMol. GenMol significantly outperforms the previous
GPT-based model in de novo generation and fragment-constrained generation, and
achieves state-of-the-art performance in goal-directed hit generation and lead
optimization. These results demonstrate that GenMol can tackle a wide range of
drug discovery tasks, providing a unified and versatile approach for molecular
design.

</details>


### [740] [ExpProof : Operationalizing Explanations for Confidential Models with ZKPs](https://arxiv.org/pdf/2502.03773)
*Chhavi Yadav, Evan Monroe Laufer, Dan Boneh, Kamalika Chaudhuri*

Main category: cs.LG

TL;DR: The paper addresses the challenge of making explanations trustworthy in adversarial settings using Zero-Knowledge Proofs (ZKPs), adapting LIME for ZKP compatibility and testing it on Neural Networks and Random Forests.


<details>
  <summary>Details</summary>
Motivation: Explanations in machine learning are often required for trust and compliance, but adversarial scenarios can undermine their reliability due to misaligned interests.

Method: The authors propose using Zero-Knowledge Proofs (ZKPs) to secure explanations, adapting the LIME algorithm for ZKP compatibility and evaluating it on Neural Networks and Random Forests.

Result: The study demonstrates the feasibility of ZKP-amenable explainability methods, with code made publicly available for further use.

Conclusion: ZKPs offer a promising approach to operationalize trustworthy explanations in adversarial settings, as shown by the adapted LIME method.

Abstract: In principle, explanations are intended as a way to increase trust in machine
learning models and are often obligated by regulations. However, many
circumstances where these are demanded are adversarial in nature, meaning the
involved parties have misaligned interests and are incentivized to manipulate
explanations for their purpose. As a result, explainability methods fail to be
operational in such settings despite the demand \cite{bordt2022post}. In this
paper, we take a step towards operationalizing explanations in adversarial
scenarios with Zero-Knowledge Proofs (ZKPs), a cryptographic primitive.
Specifically we explore ZKP-amenable versions of the popular explainability
algorithm LIME and evaluate their performance on Neural Networks and Random
Forests. Our code is publicly available at
https://github.com/emlaufer/ExpProof.

</details>


### [741] [Leveraging Sparsity for Sample-Efficient Preference Learning: A Theoretical Perspective](https://arxiv.org/pdf/2501.18282)
*Yunzhen Yao, Lie He, Michael Gastpar*

Main category: cs.LG

TL;DR: The paper improves preference learning efficiency by leveraging sparsity in the model, reducing the minimax optimal error rate from Θ(d/n) to Θ(k/n log(d/k)) for k-sparse parameters.


<details>
  <summary>Details</summary>
Motivation: High dimensionality and costly human-annotated data challenge traditional preference learning methods, prompting the need for more efficient approaches.

Method: The authors use a sparse random utility model and analyze the ℓ1-regularized estimator to achieve near-optimal error rates.

Result: Theoretical and experimental results show sparsity-aware methods reduce sample complexity and improve accuracy.

Conclusion: Sparsity in preference models significantly enhances efficiency and performance in preference learning tasks.

Abstract: This paper considers the sample-efficiency of preference learning, which
models and predicts human choices based on comparative judgments. The minimax
optimal estimation error rate $\Theta(d/n)$ in classical estimation theory
requires that the number of samples $n$ scales linearly with the dimensionality
of the feature space $d$. However, the high dimensionality of the feature space
and the high cost of collecting human-annotated data challenge the efficiency
of traditional estimation methods. To remedy this, we leverage sparsity in the
preference model and establish sharp error rates. We show that under the sparse
random utility model, where the parameter of the reward function is $k$-sparse,
the minimax optimal rate can be reduced to $\Theta(k/n \log(d/k))$.
Furthermore, we analyze the $\ell_{1}$-regularized estimator and show that it
achieves near-optimal rate under mild assumptions on the Gram matrix.
Experiments on synthetic data and LLM alignment data validate our theoretical
findings, showing that sparsity-aware methods significantly reduce sample
complexity and improve prediction accuracy.

</details>


### [742] [Optimizing Robustness and Accuracy in Mixture of Experts: A Dual-Model Approach](https://arxiv.org/pdf/2502.06832)
*Xu Zhang, Kaidi Xu, Ziqing Hu, Ren Wang*

Main category: cs.LG

TL;DR: This paper proposes methods to enhance the adversarial robustness of Mixture of Experts (MoE) models while maintaining high natural accuracy, introducing a dual-model strategy and joint training for improved performance.


<details>
  <summary>Details</summary>
Motivation: MoEs are vulnerable to adversarial attacks, posing challenges for robust applications. The paper aims to address this by improving robustness without sacrificing accuracy.

Method: The authors analyze MoE vulnerabilities, propose a robust training technique with a novel loss function, and introduce a dual-model strategy combining standard and robustified MoEs. They also develop a joint training method (JTDMoE) for better performance.

Result: Experiments on CIFAR-10 and TinyImageNet with ResNet18 and ViT show the methods effectively balance robustness and accuracy.

Conclusion: The proposed techniques successfully enhance MoE robustness and accuracy, with theoretical and empirical validation.

Abstract: Mixture of Experts (MoE) have shown remarkable success in leveraging
specialized expert networks for complex machine learning tasks. However, their
susceptibility to adversarial attacks presents a critical challenge for
deployment in robust applications. This paper addresses the critical question
of how to incorporate robustness into MoEs while maintaining high natural
accuracy. We begin by analyzing the vulnerability of MoE components, finding
that expert networks are notably more susceptible to adversarial attacks than
the router. Based on this insight, we propose a targeted robust training
technique that integrates a novel loss function to enhance the adversarial
robustness of MoE, requiring only the robustification of one additional expert
without compromising training or inference efficiency. Building on this, we
introduce a dual-model strategy that linearly combines a standard MoE model
with our robustified MoE model using a smoothing parameter. This approach
allows for flexible control over the robustness-accuracy trade-off. We further
provide theoretical foundations by deriving certified robustness bounds for
both the single MoE and the dual-model. To push the boundaries of robustness
and accuracy, we propose a novel joint training strategy JTDMoE for the
dual-model. This joint training enhances both robustness and accuracy beyond
what is achievable with separate models. Experimental results on CIFAR-10 and
TinyImageNet datasets using ResNet18 and Vision Transformer (ViT) architectures
demonstrate the effectiveness of our proposed methods. The code is publicly
available at https://github.com/TIML-Group/Robust-MoE-Dual-Model.

</details>


### [743] [Low-Rank Adapting Models for Sparse Autoencoders](https://arxiv.org/pdf/2501.19406)
*Matthew Chen, Joshua Engels, Max Tegmark*

Main category: cs.LG

TL;DR: The paper improves sparse autoencoders (SAEs) by using low-rank adaptation (LoRA) to finetune the language model around a trained SAE, reducing cross entropy loss and speeding up training.


<details>
  <summary>Details</summary>
Motivation: Existing SAE methods require costly backward passes and increase cross entropy loss, prompting a need for a more efficient approach.

Method: The authors use LoRA to finetune the language model around a pre-trained SAE, analyzing performance across various parameters like sparsity, width, and model size.

Result: The method reduces cross entropy loss gap by 30-55%, speeds up training 3-20x on Gemma and 2-10x on LLaMA, and improves downstream metrics without harming model capabilities.

Conclusion: Optimizing the model itself (not just post-hoc SAE training) can achieve Pareto improvements in interpretability and performance.

Abstract: Sparse autoencoders (SAEs) decompose language model representations into a
sparse set of linear latent vectors. Recent works have improved SAEs using
language model gradients, but these techniques require many expensive backward
passes during training and still cause a significant increase in cross entropy
loss when SAE reconstructions are inserted into the model. In this work, we
improve on these limitations by taking a fundamentally different approach: we
use low-rank adaptation (LoRA) to finetune the \textit{language model itself}
around a previously trained SAE. We analyze our method across SAE sparsity, SAE
width, language model size, LoRA rank, and model layer on the Gemma Scope
family of SAEs. In these settings, our method reduces the cross entropy loss
gap by 30\% to 55\% when SAEs are inserted during the forward pass. We also
find that compared to end-to-end (e2e) SAEs, our approach achieves the same
downstream cross entropy loss 3$\times$ to 20$\times$ faster on \gemma and
2$\times$ to 10$\times$ faster on \llama. We further show that our technique
improves downstream metrics and can adapt multiple SAEs at once without harming
general language model capabilities. Our results demonstrate that improving
model interpretability is not limited to post-hoc SAE training; Pareto
improvements can also be achieved by directly optimizing the model itself.

</details>


### [744] [TransMLA: Migrating GQA Models to MLA with Full DeepSeek Compatibility and Speedup](https://arxiv.org/pdf/2502.07864)
*Fanxu Meng, Pingzhi Tang, Zengwei Yao, Xing Sun, Muhan Zhang*

Main category: cs.LG

TL;DR: TransMLA converts GQA-based models to MLA-based ones, achieving 10.6x speedup at 8K context length with minimal performance loss.


<details>
  <summary>Details</summary>
Motivation: To enable GQA-based models to leverage DeepSeek's optimizations like vLLM and SGlang by converting them to MLA-based models.

Method: Compresses 93% of the KV cache in LLaMA-2-7B and fine-tunes with 6B tokens to regain original performance.

Result: Achieves 10.6x inference speedup at 8K context length while maintaining output quality.

Conclusion: TransMLA provides a practical migration solution for GQA-to-MLA conversion, enhancing performance with DeepSeek's features.

Abstract: In this paper, we present TransMLA, a framework that seamlessly converts any
GQA-based pre-trained model into an MLA-based model. Our approach enables
direct compatibility with DeepSeek's codebase, allowing these models to fully
leverage DeepSeek-specific optimizations such as vLLM and SGlang. By
compressing 93% of the KV cache in LLaMA-2-7B, TransMLA achieves a 10.6x
inference speedup at an 8K context length while preserving meaningful output
quality. Additionally, the model requires only 6 billion tokens for fine-tuning
to regain performance on par with the original across multiple benchmarks.
TransMLA offers a practical solution for migrating GQA-based models to the MLA
structure. When combined with DeepSeek's advanced features, such as FP8
quantization and Multi-Token Prediction, even greater inference acceleration
can be realized.

</details>


### [745] [vCache: Verified Semantic Prompt Caching](https://arxiv.org/pdf/2502.03771)
*Luis Gaspar Schroeder, Aditya Desai, Alejandro Cuadron, Kyle Chu, Shu Liu, Mark Zhao, Stephan Krusche, Alfons Kemper, Matei Zaharia, Joseph E. Gonzalez*

Main category: cs.LG

TL;DR: vCache is a verified semantic cache for LLM responses that dynamically adjusts similarity thresholds per prompt, ensuring user-defined error guarantees and outperforming static methods.


<details>
  <summary>Details</summary>
Motivation: Static similarity thresholds in semantic caches lack correctness guarantees, cause unpredictable errors, and reduce cache efficiency.

Method: vCache uses an online learning algorithm to estimate optimal similarity thresholds for each cached prompt, ensuring reliability without extra training.

Result: vCache meets specified error bounds and outperforms static-threshold and fine-tuned embedding baselines.

Conclusion: vCache provides a robust solution for semantic caching with verifiable error guarantees, enhancing efficiency and reliability.

Abstract: Semantic caches return cached LLM-generated responses for semantically
similar prompts to reduce inference latency and cost. They embed cached prompts
and store them alongside their response in a vector database. Embedding
similarity metrics assign a numerical score to quantify the similarity between
a request and its nearest neighbor prompt from the cache. Existing systems use
the same static similarity threshold across all requests to determine whether
two prompts can share similar responses. However, we observe that static
thresholds do not give formal correctness guarantees, can result in unexpected
error rates, and lead to suboptimal cache hit rates. This paper proposes
vCache, the first verified semantic cache with user-defined error rate
guarantees. It employs an online learning algorithm to estimate an optimal
threshold for each cached prompt, enabling reliable cache responses without
additional training. Our experiments show that vCache consistently meets the
specified error bounds while outperforming state-of-the-art static-threshold
and fine-tuned embedding baselines. We release the vCache implementation and
benchmarks to support future research.

</details>


### [746] [Overcoming Spurious Solutions in Semi-Dual Neural Optimal Transport: A Smoothing Approach for Learning the Optimal Transport Plan](https://arxiv.org/pdf/2502.04583)
*Jaemoo Choi, Jaewoong Choi, Dohyun Kwon*

Main category: cs.LG

TL;DR: The paper addresses convergence issues in learning Optimal Transport (OT) maps, proposing a novel method (OTP) to eliminate spurious solutions and improve accuracy in distribution transfer.


<details>
  <summary>Details</summary>
Motivation: Existing methods like Semi-dual Neural OT often produce inaccurate OT maps. The paper aims to ensure correct OT map recovery and handle cases where deterministic maps fail.

Method: The OTP method learns both the OT Map and the Optimal Transport Plan, addressing cases where the sufficient condition for Semi-dual Neural OT isn't met. Theoretical guarantees are provided under specific assumptions.

Result: OTP outperforms existing methods, recovering accurate OT maps and excelling in tasks like image-to-image translation and stochastic transport scenarios (e.g., colorization).

Conclusion: OTP resolves spurious solution issues in OT learning, offering a robust approach for deterministic and stochastic transport tasks.

Abstract: We address the convergence problem in learning the Optimal Transport (OT)
map, where the OT Map refers to a map from one distribution to another while
minimizing the transport cost. Semi-dual Neural OT, a widely used approach for
learning OT Maps with neural networks, often generates spurious solutions that
fail to transfer one distribution to another accurately. We identify a
sufficient condition under which the max-min solution of Semi-dual Neural OT
recovers the true OT Map. Moreover, to address cases when this sufficient
condition is not satisfied, we propose a novel method, OTP, which learns both
the OT Map and the Optimal Transport Plan, representing the optimal coupling
between two distributions. Under sharp assumptions on the distributions, we
prove that our model eliminates the spurious solution issue and correctly
solves the OT problem. Our experiments show that the OTP model recovers the
optimal transport map where existing methods fail and outperforms current
OT-based models in image-to-image translation tasks. Notably, the OTP model can
learn stochastic transport maps when deterministic OT Maps do not exist, such
as one-to-many tasks like colorization.

</details>


### [747] [GeLLMO: Generalizing Large Language Models for Multi-property Molecule Optimization](https://arxiv.org/pdf/2502.13398)
*Vishal Dey, Xiao Hu, Xia Ning*

Main category: cs.LG

TL;DR: The paper introduces MuMOInstruct, a dataset for multi-property molecule optimization, and GeLLMOs, instruction-tuned LLMs that outperform baselines and show strong zero-shot generalization.


<details>
  <summary>Details</summary>
Motivation: Current methods for molecule optimization are limited in scalability and generalizability, while LLMs show promise for novel tasks.

Method: Developed MuMOInstruct dataset and GeLLMOs, instruction-tuned LLMs for molecule optimization.

Result: GeLLMOs outperform baselines in in-domain and out-of-domain tasks and show strong zero-shot generalization.

Conclusion: GeLLMOs demonstrate potential as foundational models for molecule optimization, reducing the need for retraining.

Abstract: Despite recent advancements, most computational methods for molecule
optimization are constrained to single- or double-property optimization tasks
and suffer from poor scalability and generalizability to novel optimization
tasks. Meanwhile, Large Language Models (LLMs) demonstrate remarkable
out-of-domain generalizability to novel tasks. To demonstrate LLMs' potential
for molecule optimization, we introduce MuMOInstruct, the first high-quality
instruction-tuning dataset specifically focused on complex multi-property
molecule optimization tasks. Leveraging MuMOInstruct, we develop GeLLMOs, a
series of instruction-tuned LLMs for molecule optimization. Extensive
evaluations across 5 in-domain and 5 out-of-domain tasks demonstrate that
GeLLMOs consistently outperform state-of-the-art baselines. GeLLMOs also
exhibit outstanding zero-shot generalization to unseen tasks, significantly
outperforming powerful closed-source LLMs. Such strong generalizability
demonstrates the tremendous potential of GeLLMOs as foundational models for
molecule optimization, thereby tackling novel optimization tasks without
resource-intensive retraining. MuMOInstruct, models, and code are accessible
through https://github.com/ninglab/GeLLMO.

</details>


### [748] [A Lightweight Method to Disrupt Memorized Sequences in LLM](https://arxiv.org/pdf/2502.05159)
*Parjanya Prajakta Prashant, Kaustubh Ponkshe, Babak Salimi*

Main category: cs.LG

TL;DR: TokenSwap reduces memorization in large language models by swapping token probabilities with smaller models, achieving a 10x drop in verbatim reproduction with minimal performance loss.


<details>
  <summary>Details</summary>
Motivation: Addressing the tradeoff between performance and memorization in large language models, which poses legal, ethical, and safety concerns. Existing solutions are impractical for most users.

Method: TokenSwap selectively swaps token probabilities between large and small models (e.g., DistilGPT-2) to reduce verbatim memorization while preserving task performance.

Result: Evaluations on Pythia-6.9B and Llama-3-8B show a 10x reduction in exact memorization with negligible task degradation.

Conclusion: TokenSwap provides a practical, accessible solution for mitigating memorized generation in deployed LLMs.

Abstract: As language models scale, their performance improves dramatically across a
wide range of tasks, but so does their tendency to memorize and regurgitate
parts of their training data verbatim. This tradeoff poses serious legal,
ethical, and safety concerns, especially in real-world deployments. Existing
mitigation techniques, such as differential privacy or model unlearning, often
require retraining or access to internal weights making them impractical for
most users. In this work, we introduce TokenSwap, a lightweight, post-hoc
defense designed for realistic settings where the user can only access
token-level outputs. Our key insight is that while large models are necessary
for high task performance, small models (e.g., DistilGPT-2) are often
sufficient to assign fluent, grammatically plausible probabilities to common
function words - and crucially, they memorize far less. By selectively swapping
token probabilities between models, TokenSwap preserves the capabilities of
large models while reducing their propensity for verbatim reproduction.
Evaluations on Pythia-6.9B and Llama-3-8B show up to a 10$\times$ drop in exact
memorization with negligible task degradation. Our method offers a practical,
accessible solution for mitigating memorized generation in deployed LLMs.

</details>


### [749] [Towards LLM Unlearning Resilient to Relearning Attacks: A Sharpness-Aware Minimization Perspective and Beyond](https://arxiv.org/pdf/2502.05374)
*Chongyu Fan, Jinghan Jia, Yihua Zhang, Anil Ramakrishna, Mingyi Hong, Sijia Liu*

Main category: cs.LG

TL;DR: The paper explores robust unlearning in LLMs to counter relearning attacks, linking it to sharpness-aware minimization (SAM) and highlighting smoothness optimization's role. Experiments show SAM and smoothing strategies enhance robustness and defend against jailbreaking attacks.


<details>
  <summary>Details</summary>
Motivation: Addressing vulnerabilities in LLM unlearning methods, particularly their susceptibility to relearning attacks, to improve safety and ethical compliance.

Method: Establishes a connection between robust unlearning and SAM, explores smoothing strategies, and tests them on datasets like WMDP and MUSE.

Result: SAM and smoothing approaches improve resistance to relearning attacks and also defend against jailbreaking attacks.

Conclusion: Smoothness optimization enhances LLM unlearning robustness, offering broader impact in defending against attacks.

Abstract: The LLM unlearning technique has recently been introduced to comply with data
regulations and address the safety and ethical concerns of LLMs by removing the
undesired data-model influence. However, state-of-the-art unlearning methods
face a critical vulnerability: they are susceptible to ``relearning'' the
removed information from a small number of forget data points, known as
relearning attacks. In this paper, we systematically investigate how to make
unlearned models robust against such attacks. For the first time, we establish
a connection between robust unlearning and sharpness-aware minimization (SAM)
through a unified robust optimization framework, in an analogy to adversarial
training designed to defend against adversarial attacks. Our analysis for SAM
reveals that smoothness optimization plays a pivotal role in mitigating
relearning attacks. Thus, we further explore diverse smoothing strategies to
enhance unlearning robustness. Extensive experiments on benchmark datasets,
including WMDP and MUSE, demonstrate that SAM and other smoothness optimization
approaches consistently improve the resistance of LLM unlearning to relearning
attacks. Notably, smoothness-enhanced unlearning also helps defend against
(input-level) jailbreaking attacks, broadening our proposal's impact in
robustifying LLM unlearning. Codes are available at
https://github.com/OPTML-Group/Unlearn-Smooth.

</details>


### [750] [Scaling Laws for Forgetting during Finetuning with Pretraining Data Injection](https://arxiv.org/pdf/2502.06042)
*Louis Bethune, David Grangier, Dan Busbridge, Eleonora Gualdoni, Marco Cuturi, Pierre Ablin*

Main category: cs.LG

TL;DR: Finetuning pretrained models on limited target data risks overfitting and forgetting pretraining knowledge. Injecting 1% pretraining data into finetuning prevents forgetting.


<details>
  <summary>Details</summary>
Motivation: To address challenges of overfitting and knowledge loss when finetuning pretrained models on limited target data.

Method: Derive scaling laws for various domains, data amounts, and model scales. Measure efficiency of injecting pretraining data into finetuning.

Result: Injecting 1% pretraining data into finetuning prevents forgetting pretraining knowledge.

Conclusion: Minimal pretraining data injection during finetuning effectively mitigates forgetting and overfitting.

Abstract: A widespread strategy to obtain a language model that performs well on a
target domain is to finetune a pretrained model to perform unsupervised
next-token prediction on data from that target domain. Finetuning presents two
challenges: (i) if the amount of target data is limited, as in most practical
applications, the model will quickly overfit, and (ii) the model will drift
away from the original model, forgetting the pretraining data and the generic
knowledge that comes with it. We aim to derive scaling laws that quantify these
two phenomena for various target domains, amounts of available target data, and
model scales. We measure the efficiency of injecting pretraining data into the
finetuning data mixture to avoid forgetting and mitigate overfitting. A key
practical takeaway from our study is that injecting as little as 1% of
pretraining data in the finetuning data mixture prevents the model from
forgetting the pretraining set.

</details>


### [751] [Training a Generally Curious Agent](https://arxiv.org/pdf/2502.17543)
*Fahim Tajwar, Yiding Jiang, Abitha Thankaraj, Sumaita Sadia Rahman, J Zico Kolter, Jeff Schneider, Ruslan Salakhutdinov*

Main category: cs.LG

TL;DR: Paprika is a fine-tuning method for language models to enhance decision-making and exploration in unseen tasks without additional training.


<details>
  <summary>Details</summary>
Motivation: Existing language models lack strategic information gathering; Paprika aims to develop general decision-making capabilities.

Method: Fine-tuning on synthetic interaction data from diverse tasks, using curriculum learning for sample efficiency.

Result: Models transfer learned capabilities to unseen tasks effectively, with sampling efficiency as the bottleneck.

Conclusion: Paprika offers a promising approach for autonomous AI systems in novel sequential decision-making problems.

Abstract: Efficient exploration is essential for intelligent systems interacting with
their environment, but existing language models often fall short in scenarios
that require strategic information gathering. In this paper, we present
Paprika, a fine-tuning approach that enables language models to develop general
decision-making capabilities that are not confined to particular environments.
By training on synthetic interaction data from different tasks that require
diverse strategies, Paprika teaches models to explore and adapt their behavior
on a new task based on environment feedback in-context without more gradient
updates. Experimental results show that models fine-tuned with Paprika can
effectively transfer their learned decision-making capabilities to entirely
unseen tasks without additional training. Unlike traditional training, our
approach's primary bottleneck lies in sampling useful interaction data instead
of model updates. To improve sample efficiency, we propose a curriculum
learning strategy that prioritizes sampling trajectories from tasks with high
learning potential. These results suggest a promising path towards AI systems
that can autonomously solve novel sequential decision-making problems that
require interactions with the external world.

</details>


### [752] [Enhancing Performance of Explainable AI Models with Constrained Concept Refinement](https://arxiv.org/pdf/2502.06775)
*Geyu Liang, Senne Michielssen, Salar Fattahi*

Main category: cs.LG

TL;DR: The paper proposes a framework to balance accuracy and interpretability in ML by optimizing concept embeddings, achieving zero loss while enhancing interpretability.


<details>
  <summary>Details</summary>
Motivation: Addressing the trade-off between accuracy and interpretability in ML, especially for interpretable-by-design methods that often sacrifice accuracy.

Method: A novel framework optimizing concept embeddings under interpretability constraints, tested with a generative model and evaluated on image classification tasks.

Result: The framework achieves zero loss, improves interpretability, and outperforms existing methods in accuracy and computational efficiency.

Conclusion: The proposed framework successfully balances accuracy and interpretability, offering a practical solution for trustworthy ML models.

Abstract: The trade-off between accuracy and interpretability has long been a challenge
in machine learning (ML). This tension is particularly significant for emerging
interpretable-by-design methods, which aim to redesign ML algorithms for
trustworthy interpretability but often sacrifice accuracy in the process. In
this paper, we address this gap by investigating the impact of deviations in
concept representations-an essential component of interpretable models-on
prediction performance and propose a novel framework to mitigate these effects.
The framework builds on the principle of optimizing concept embeddings under
constraints that preserve interpretability. Using a generative model as a
test-bed, we rigorously prove that our algorithm achieves zero loss while
progressively enhancing the interpretability of the resulting model.
Additionally, we evaluate the practical performance of our proposed framework
in generating explainable predictions for image classification tasks across
various benchmarks. Compared to existing explainable methods, our approach not
only improves prediction accuracy while preserving model interpretability
across various large-scale benchmarks but also achieves this with significantly
lower computational cost.

</details>


### [753] [HiPoNet: A Multi-View Simplicial Complex Network for High Dimensional Point-Cloud and Single-Cell Data](https://arxiv.org/pdf/2502.07746)
*Siddharth Viswanath, Hiren Madhu, Dhananjay Bhaskar, Jake Kovalic, David R Johnson, Christopher Tape, Ian Adelstein, Rex Ying, Michael Perlmutter, Smita Krishnaswamy*

Main category: cs.LG

TL;DR: HiPoNet is a neural network for high-dimensional point clouds, outperforming existing methods by using simplicial complexes and wavelet transforms to preserve geometric and topological information.


<details>
  <summary>Details</summary>
Motivation: The need to handle high-dimensional single-cell and spatial data, where current methods fail due to their focus on 3D data and loss of geometric/topological details.

Method: HiPoNet models point clouds as higher-order simplicial complexes, reweights features for multiple views, and uses simplicial wavelet transforms for multiscale feature extraction.

Result: HiPoNet outperforms other models on single-cell data and works well on spatial transcriptomics, preserving geometric and topological information.

Conclusion: HiPoNet provides a scalable and robust solution for high-dimensional data analysis, especially in biological applications.

Abstract: In this paper, we propose HiPoNet, an end-to-end differentiable neural
network for regression, classification, and representation learning on
high-dimensional point clouds. Our work is motivated by single-cell data which
can have very high-dimensionality --exceeding the capabilities of existing
methods for point clouds which are mostly tailored for 3D data. Moreover,
modern single-cell and spatial experiments now yield entire cohorts of datasets
(i.e., one data set for every patient), necessitating models that can process
large, high-dimensional point-clouds at scale. Most current approaches build a
single nearest-neighbor graph, discarding important geometric and topological
information. In contrast, HiPoNet models the point-cloud as a set of
higher-order simplicial complexes, with each particular complex being created
using a reweighting of features. This method thus generates multiple constructs
corresponding to different views of high-dimensional data, which in biology
offers the possibility of disentangling distinct cellular processes. It then
employs simplicial wavelet transforms to extract multiscale features, capturing
both local and global topology from each view. We show that geometric and
topological information is preserved in this framework both theoretically and
empirically. We showcase the utility of HiPoNet on point-cloud level tasks,
involving classification and regression of entire point-clouds in data cohorts.
Experimentally, we find that HiPoNet outperforms other point-cloud and
graph-based models on single-cell data. We also apply HiPoNet to spatial
transcriptomics datasets using spatial coordinates as one of the views.
Overall, HiPoNet offers a robust and scalable solution for high-dimensional
data analysis.

</details>


### [754] [Towards Training One-Step Diffusion Models Without Distillation](https://arxiv.org/pdf/2502.08005)
*Mingtian Zhang, Wenlin Chen, Jiajun He, Zijing Ou, José Miguel Hernández-Lobato, Bernhard Schölkopf, David Barber*

Main category: cs.LG

TL;DR: The paper explores training one-step diffusion models directly without teacher-guided distillation, finding that teacher score supervision is unnecessary but weight initialization remains crucial.


<details>
  <summary>Details</summary>
Motivation: To investigate if one-step diffusion models can be trained without relying on teacher distillation, simplifying the process.

Method: Introduces new training methods that eliminate teacher score supervision while retaining teacher weight initialization.

Result: The new methods outperform most teacher-guided approaches, showing score supervision isn't essential, but teacher initialization is.

Conclusion: Teacher initialization's value lies in feature representations, not latent mappings, advancing understanding of distillation roles.

Abstract: Recent advances in training one-step diffusion models typically follow a
two-stage pipeline: first training a teacher diffusion model and then
distilling it into a one-step student model. This process often depends on both
the teacher's score function for supervision and its weights for initializing
the student model. In this paper, we explore whether one-step diffusion models
can be trained directly without this distillation procedure. We introduce a
family of new training methods that entirely forgo teacher score supervision,
yet outperforms most teacher-guided distillation approaches. This suggests that
score supervision is not essential for effective training of one-step diffusion
models. However, we find that initializing the student model with the teacher's
weights remains critical. Surprisingly, the key advantage of teacher
initialization is not due to better latent-to-output mappings, but rather the
rich set of feature representations across different noise levels that the
teacher diffusion model provides. These insights take us one step closer
towards training one-step diffusion models without distillation and provide a
better understanding of the roles of teacher supervision and initialization in
the distillation process.

</details>


### [755] [Learning Policy Committees for Effective Personalization in MDPs with Diverse Tasks](https://arxiv.org/pdf/2503.01885)
*Luise Ge, Michael Lanier, Anindya Sarkar, Bengisu Guresti, Chongjie Zhang, Yevgeniy Vorobeychik*

Main category: cs.LG

TL;DR: The paper proposes a novel approach for learning a policy committee to handle diverse tasks in dynamic decision problems, outperforming existing methods in training, generalization, and few-shot learning.


<details>
  <summary>Details</summary>
Motivation: Existing methods like multi-task and meta reinforcement learning struggle with task diversity, while task embedding and clustering lack guarantees and require extensive training.

Method: The approach involves learning a policy committee with provable guarantees, including two algorithmic solutions: one for low-dimensional tasks and a gradient-based general solution.

Result: Experiments on MuJoCo and Meta-World show superior performance over state-of-the-art baselines, often by large margins.

Conclusion: The proposed method effectively addresses task diversity challenges with practical and theoretical guarantees, demonstrating significant improvements in performance.

Abstract: Many dynamic decision problems, such as robotic control, involve a series of
tasks, many of which are unknown at training time. Typical approaches for these
problems, such as multi-task and meta reinforcement learning, do not generalize
well when the tasks are diverse. On the other hand, approaches that aim to
tackle task diversity, such as using task embedding as policy context and task
clustering, typically lack performance guarantees and require a large number of
training tasks. To address these challenges, we propose a novel approach for
learning a policy committee that includes at least one near-optimal policy with
high probability for tasks encountered during execution. While we show that
this problem is in general inapproximable, we present two practical algorithmic
solutions. The first yields provable approximation and task sample complexity
guarantees when tasks are low-dimensional (the best we can do due to
inapproximability), whereas the second is a general and practical
gradient-based approach. In addition, we provide a provable sample complexity
bound for few-shot learning. Our experiments on MuJoCo and Meta-World show that
the proposed approach outperforms state-of-the-art multi-task, meta-, and task
clustering baselines in training, generalization, and few-shot learning, often
by a large margin. Our code is available at
https://github.com/CERL-WUSTL/PACMAN.

</details>


### [756] [Recurrent Memory for Online Interdomain Gaussian Processes](https://arxiv.org/pdf/2502.08736)
*Wenlong Chen, Naoki Kiyohara, Harrison Bo Hua Zhu, Jacob Curran-Sebastian, Samir Bhatt, Yingzhen Li*

Main category: cs.LG

TL;DR: OHSVGP is a new online Gaussian process model using HiPPO for long-term memory in sequential data, outperforming existing methods in performance and efficiency.


<details>
  <summary>Details</summary>
Motivation: To address the challenge of capturing long-term memory in sequential data for online learning settings.

Method: Combines HiPPO framework with sparse variational Gaussian processes, using time-dependent orthogonal polynomial basis functions as inducing variables.

Result: OHSVGP excels in predictive performance, long-term memory preservation, and computational efficiency across various tasks.

Conclusion: OHSVGP effectively integrates HiPPO with GPs, offering a robust solution for online learning with long-term memory.

Abstract: We propose a novel online Gaussian process (GP) model that is capable of
capturing long-term memory in sequential data in an online learning setting.
Our model, Online HiPPO Sparse Variational Gaussian Process (OHSVGP), leverages
the HiPPO (High-order Polynomial Projection Operators) framework, which is
popularized in the RNN domain due to its long-range memory modeling
capabilities. We interpret the HiPPO time-varying orthogonal projections as
inducing variables with time-dependent orthogonal polynomial basis functions,
which allows the SVGP inducing variables to memorize the process history. We
show that the HiPPO framework fits naturally into the interdomain GP framework
and demonstrate that the kernel matrices can also be updated online in a
recurrence form based on the ODE evolution of HiPPO. We evaluate OHSVGP with
online prediction for 1D time series, continual learning in discriminative GP
model for data with multidimensional inputs, and deep generative modeling with
sparse Gaussian process variational autoencoder, showing that it outperforms
existing online GP methods in terms of predictive performance, long-term memory
preservation, and computational efficiency.

</details>


### [757] [LLM-FE: Automated Feature Engineering for Tabular Data with LLMs as Evolutionary Optimizers](https://arxiv.org/pdf/2503.14434)
*Nikhil Abhyankar, Parshin Shojaee, Chandan K. Reddy*

Main category: cs.LG

TL;DR: LLM-FE is a new framework combining evolutionary search and LLMs to automate feature engineering for tabular learning, outperforming existing methods.


<details>
  <summary>Details</summary>
Motivation: Traditional automated feature engineering lacks domain knowledge integration, and current LLM-based methods fail to leverage prior experiments or reasoning.

Method: LLM-FE uses evolutionary search with LLMs to iteratively propose and refine feature transformation programs, guided by data-driven feedback.

Result: LLM-FE consistently outperforms state-of-the-art baselines, improving tabular prediction model performance.

Conclusion: LLM-FE effectively integrates domain knowledge and reasoning, advancing automated feature engineering for tabular learning.

Abstract: Automated feature engineering plays a critical role in improving predictive
model performance for tabular learning tasks. Traditional automated feature
engineering methods are limited by their reliance on pre-defined
transformations within fixed, manually designed search spaces, often neglecting
domain knowledge. Recent advances using Large Language Models (LLMs) have
enabled the integration of domain knowledge into the feature engineering
process. However, existing LLM-based approaches use direct prompting or rely
solely on validation scores for feature selection, failing to leverage insights
from prior feature discovery experiments or establish meaningful reasoning
between feature generation and data-driven performance. To address these
challenges, we propose LLM-FE, a novel framework that combines evolutionary
search with the domain knowledge and reasoning capabilities of LLMs to
automatically discover effective features for tabular learning tasks. LLM-FE
formulates feature engineering as a program search problem, where LLMs propose
new feature transformation programs iteratively, and data-driven feedback
guides the search process. Our results demonstrate that LLM-FE consistently
outperforms state-of-the-art baselines, significantly enhancing the performance
of tabular prediction models across diverse classification and regression
benchmarks.

</details>


### [758] [Exploring the Boundary of Diffusion-based Methods for Solving Constrained Optimization](https://arxiv.org/pdf/2502.10330)
*Shutong Ding, Yimiao Zhou, Ke Hu, Xi Yao, Junchi Yan, Xiaoying Tang, Ye Shi*

Main category: cs.LG

TL;DR: The paper explores the use of diffusion models for Continuous Constrained Optimization, proposing a novel framework called DiOpt with a dual-phase approach to refine solutions and satisfy constraints.


<details>
  <summary>Details</summary>
Motivation: Diffusion models excel in generative tasks but their potential in solving Continuous Constrained Optimization problems is underexplored.

Method: Proposes DiOpt, a two-phase framework: warm-start via supervised learning and bootstrapping to iteratively refine solutions.

Result: DiOpt demonstrates strong performance across diverse optimization problems, with detailed training dynamics and hyperparameter analysis.

Conclusion: DiOpt effectively harnesses diffusion models for constrained optimization, offering a promising approach for such tasks.

Abstract: Diffusion models have achieved remarkable success in generative tasks such as
image and video synthesis, and in control domains like robotics, owing to their
strong generalization capabilities and proficiency in fitting complex
multimodal distributions. However, their full potential in solving Continuous
Constrained Optimization problems remains largely underexplored. Our work
commences by investigating a two-dimensional constrained quadratic optimization
problem as an illustrative example to explore the inherent challenges and
issues when applying diffusion models to such optimization tasks and providing
theoretical analyses for these observations. To address the identified gaps and
harness diffusion models for Continuous Constrained Optimization, we build upon
this analysis to propose a novel diffusion-based framework for optimization
problems called DiOpt. This framework operates in two distinct phases: an
initial warm-start phase, implemented via supervised learning, followed by a
bootstrapping phase. This dual-phase architecture is designed to iteratively
refine solutions, thereby improving the objective function while rigorously
satisfying problem constraints. Finally, multiple candidate solutions are
sampled, and the optimal one is selected through a screening process. We
present extensive experiments detailing the training dynamics of DiOpt, its
performance across a diverse set of Continuous Constrained Optimization
problems, and an analysis of the impact of DiOpt's various hyperparameters.

</details>


### [759] [Learning to Explain Air Traffic Situation](https://arxiv.org/pdf/2502.10764)
*Hong-ah Chai, Seokbin Yoon, Keumjin Lee*

Main category: cs.LG

TL;DR: A machine learning framework using Transformer-based models to explain air traffic dynamics by analyzing aircraft interactions, aiding controller situational awareness.


<details>
  <summary>Details</summary>
Motivation: Understanding air traffic controllers' mental models of complex situations is challenging due to high-dimensional interactions, which prior work often oversimplifies.

Method: Proposes a Transformer-based multi-agent trajectory model to capture spatio-temporal and social interactions between aircraft, using attention scores to quantify influence.

Result: The framework, trained on real-world data from Incheon International Airport, effectively explains traffic situations, enhancing controller decision-making.

Conclusion: The approach provides explainable insights into traffic dynamics, potentially improving controller situational awareness and decision-making.

Abstract: Understanding how air traffic controllers construct a mental 'picture' of
complex air traffic situations is crucial but remains a challenge due to the
inherently intricate, high-dimensional interactions between aircraft, pilots,
and controllers. Previous work on modeling the strategies of air traffic
controllers and their mental image of traffic situations often centers on
specific air traffic control tasks or pairwise interactions between aircraft,
neglecting to capture the comprehensive dynamics of an air traffic situation.
To address this issue, we propose a machine learning-based framework for
explaining air traffic situations. Specifically, we employ a Transformer-based
multi-agent trajectory model that encapsulates both the spatio-temporal
movement of aircraft and social interaction between them. By deriving attention
scores from the model, we can quantify the influence of individual aircraft on
overall traffic dynamics. This provides explainable insights into how air
traffic controllers perceive and understand the traffic situation. Trained on
real-world air traffic surveillance data collected from the terminal airspace
around Incheon International Airport in South Korea, our framework effectively
explicates air traffic situations. This could potentially support and enhance
the decision-making and situational awareness of air traffic controllers.

</details>


### [760] [Achieving binary weight and activation for LLMs using Post-Training Quantization](https://arxiv.org/pdf/2504.05352)
*Siqing Song, Chuang Wang, Ruiqi Wang, Yi Yang, Xuyao Zhang*

Main category: cs.LG

TL;DR: Proposes a post-training quantization framework for LLMs with W(1+1)A(1*4) configuration, improving performance over existing methods.


<details>
  <summary>Details</summary>
Motivation: Existing quantization techniques degrade performance below 4 bits; this work aims to push boundaries toward fully binarized models.

Method: Uses Hessian-aware fine-grained grouping for weights and decomposes INT4 activations into 4*INT1, smoothing scaling factors.

Result: Surpasses SOTA baselines on W2A4 tasks, reducing quantization errors.

Conclusion: Advances LLM quantization toward binarization with improved efficiency and performance.

Abstract: Quantizing large language models (LLMs) to 1-bit precision significantly
reduces computational costs, but existing quantization techniques suffer from
noticeable performance degradation when using weight and activation precisions
below 4 bits (W4A4). In this paper, we propose a post-training quantization
framework with W(1+1)A(1*4) configuration, where weights are quantized to 1 bit
with an additional 1 bit for fine-grain grouping and activations are quantized
to 1 bit with a 4-fold increase in the number of channels. For weight
quantization, we propose utilizing Hessian-aware fine-grained grouping along
with an EM-based quantization scheme. For activation quantization, we decompose
INT4-quantized activations into a 4 * INT1 format equivalently and
simultaneously smooth the scaling factors based on quantization errors, which
further reduces the quantization errors in activations. Our method surpasses
state-of-the-art (SOTA) LLM quantization baselines on W2A4 across multiple
tasks, pushing the boundaries of existing LLM quantization methods toward fully
binarized models. Code is available at
https://github.com/JimmyCrave/LLM-PTQ-binarization.

</details>


### [761] [Scalable Model Merging with Progressive Layer-wise Distillation](https://arxiv.org/pdf/2502.12706)
*Jing Xu, Jiazheng Li, Jingzhao Zhang*

Main category: cs.LG

TL;DR: ProDistill, a few-shot merging algorithm, improves model merging performance by leveraging layer-wise distillation, achieving state-of-the-art results with scalability for large models.


<details>
  <summary>Details</summary>
Motivation: Addressing performance degradation in merged models, especially with limited data, by proving the necessity of domain-specific data and exploring the link between merging and distillation.

Method: Introduces ProDistill, a progressive layer-wise distillation algorithm, challenging the belief that layer-wise training harms performance.

Result: ProDistill outperforms existing methods with up to 6.14% and 6.61% improvements in vision and NLU tasks, scaling effectively to models over 10B parameters.

Conclusion: ProDistill demonstrates superior merging performance and scalability, highlighting the value of layer-wise distillation in model integration.

Abstract: Model merging offers an effective way to integrate the capabilities of
multiple fine-tuned models. However, the performance degradation of the merged
model remains a challenge, particularly when none or few data are available.
This paper first highlights the necessity of domain-specific data for model
merging by proving that data-agnostic algorithms can have arbitrarily bad
worst-case performance. Building on this theoretical insight, we explore the
relationship between model merging and distillation, introducing a novel
few-shot merging algorithm, ProDistill (Progressive Layer-wise Distillation).
Unlike common belief that layer wise training hurts performance, we show that
layer-wise teacher-student distillation not only enhances the scalability but
also improves model merging performance. We conduct extensive experiments to
show that compared to existing few-shot merging methods, ProDistill achieves
state-of-the-art performance, with up to 6.14% and 6.61% improvements in vision
and NLU tasks. Furthermore, we extend the experiments to models with over 10B
parameters, showcasing the exceptional scalability of ProDistill.

</details>


### [762] [Recursive Deep Inverse Reinforcement Learning](https://arxiv.org/pdf/2504.13241)
*Paul Ghanem, Owen Howell, Michael Potter, Pau Closas, Alireza Ramezani, Deniz Erdogmus, Tales Imbiriba*

Main category: cs.LG

TL;DR: Proposes an online Recursive Deep Inverse Reinforcement Learning (RDIRL) method for real-time adversary goal inference, outperforming existing IRL algorithms.


<details>
  <summary>Details</summary>
Motivation: Existing deep IRL methods are offline and slow, limiting real-time applicability in adversarial domains like cybersecurity and strategy games.

Method: Uses sequential second-order Newton updates (similar to Extended Kalman Filter) to minimize an upper bound on the Guided Cost Learning objective.

Result: Demonstrates superior performance in recovering cost/reward functions of expert agents in benchmark tasks.

Conclusion: RDIRL offers a fast, online alternative to traditional IRL methods for real-time adversarial scenarios.

Abstract: Inferring an adversary's goals from exhibited behavior is crucial for
counterplanning and non-cooperative multi-agent systems in domains like
cybersecurity, military, and strategy games. Deep Inverse Reinforcement
Learning (IRL) methods based on maximum entropy principles show promise in
recovering adversaries' goals but are typically offline, require large batch
sizes with gradient descent, and rely on first-order updates, limiting their
applicability in real-time scenarios. We propose an online Recursive Deep
Inverse Reinforcement Learning (RDIRL) approach to recover the cost function
governing the adversary actions and goals. Specifically, we minimize an upper
bound on the standard Guided Cost Learning (GCL) objective using sequential
second-order Newton updates, akin to the Extended Kalman Filter (EKF), leading
to a fast (in terms of convergence) learning algorithm. We demonstrate that
RDIRL is able to recover cost and reward functions of expert agents in standard
and adversarial benchmark tasks. Experiments on benchmark tasks show that our
proposed approach outperforms several leading IRL algorithms.

</details>


### [763] [MOLLM: Multi-Objective Large Language Model for Molecular Design -- Optimizing with Experts](https://arxiv.org/pdf/2502.12845)
*Nian Ran, Yue Wang, Richard Allmendinger*

Main category: cs.LG

TL;DR: MOLLM is a novel framework combining domain knowledge and large language models for multi-objective molecular design, outperforming SOTA methods with 14x speed and cost efficiency.


<details>
  <summary>Details</summary>
Motivation: To advance molecular design in drug discovery, materials science, and chemical engineering by optimizing multiple molecular properties efficiently.

Method: Uses in-context learning and multi-objective optimization within a large language model framework (MOLLM).

Result: MOLLM outperforms SOTA methods, is 14x faster, more cost-effective, and excels on the PMO benchmark.

Conclusion: MOLLM is a superior, efficient, and innovative solution for multi-objective molecular design.

Abstract: Molecular design plays a critical role in advancing fields such as drug
discovery, materials science, and chemical engineering. This work introduces
the Multi-Objective Large Language Model for Molecular Design (MOLLM), a novel
framework that combines domain-specific knowledge with the adaptability of
large language models to optimize molecular properties across multiple
objectives. Leveraging in-context learning and multi-objective optimization,
MOLLM achieves superior performance and innovation, consistently surpassing
state-of-the-art (SOTA) methods. We significantly improve the efficiency of our
framework, making it 14 times faster and substantially more cost-effective
without compromising performance compared to the latest similar work. Our
results demonstrate that MOLLM consistently outperforms SOTA models across
experiments and excels on the PMO benchmark. In addition, we provide extensive
ablation studies and analysis to evaluate the effectiveness of each component
and the quality of the output molecules.

</details>


### [764] [Achieving adaptivity and optimality for multi-armed bandits using Exponential-Kullback Leibler Maillard Sampling](https://arxiv.org/pdf/2502.14379)
*Hao Qin, Kwang-Sung Jun, Chicheng Zhang*

Main category: cs.LG

TL;DR: The paper introduces Exp-KL-MS, an algorithm for $K$-armed bandits with exponential family rewards, achieving multiple optimality criteria simultaneously.


<details>
  <summary>Details</summary>
Motivation: Existing algorithms for $K$-armed bandits fail to meet all performance criteria (Asymptotic Optimality, Minimax Optimality, Sub-UCB, variance-adaptive regret).

Method: Design of the Exponential Kullback-Leibler Maillard Sampling (Exp-KL-MS) algorithm.

Result: Exp-KL-MS achieves Asymptotic Optimality, Minimax Optimality with a $\sqrt{\ln (K)}$ factor, Sub-UCB, and variance-adaptive regret.

Conclusion: Exp-KL-MS is a novel solution that satisfies all key performance criteria for $K$-armed bandits with exponential family rewards.

Abstract: We study the problem of $K$-armed bandits with reward distributions belonging
to a one-parameter exponential distribution family. In the literature, several
criteria have been proposed to evaluate the performance of such algorithms,
including Asymptotic Optimality, Minimax Optimality, Sub-UCB, and
variance-adaptive worst-case regret bound. Thompson Sampling-based and Upper
Confidence Bound-based algorithms have been employed to achieve some of these
criteria. However, none of these algorithms simultaneously satisfy all the
aforementioned criteria.
  In this paper, we design an algorithm, Exponential Kullback-Leibler Maillard
Sampling (abbrev. Exp-KL-MS), that can achieve multiple optimality criteria
simultaneously, including Asymptotic Optimality, Minimax Optimality with a
$\sqrt{\ln (K)}$ factor, Sub-UCB, and variance-adaptive worst-case regret
bound.

</details>


### [765] [Distributional Scaling for Emergent Capabilities](https://arxiv.org/pdf/2502.17356)
*Rosie Zhao, Tian Qin, David Alvarez-Melis, Sham Kakade, Naomi Saphra*

Main category: cs.LG

TL;DR: The paper investigates sudden breakthroughs in language model performance, attributing them to continuous changes in outcome distributions rather than emergence or thresholding effects.


<details>
  <summary>Details</summary>
Motivation: To understand why language models exhibit sudden performance breakthroughs at scale, contrasting with smooth scaling laws.

Method: Uses synthetic length generalization tasks and analyzes performance distributions across random seeds, validated on realistic settings like MMLU.

Result: Breakthroughs arise from continuous changes in bimodal performance distributions across seeds, not discontinuous metrics.

Conclusion: Random variation plays a key role in how scale affects language model capabilities.

Abstract: This paper explores the nature of sudden breakthroughs in language model
performance at scale, which stand in contrast to smooth improvements governed
by scaling laws. While advocates of "emergence" view breakthroughs as unlocked
capabilities, others attribute them to thresholding effects on noncontinuous
metrics. We propose that breakthroughs are instead driven by continuous changes
in the probability distribution of training outcomes when performance is
bimodally distributed across random seeds. In synthetic length generalization
tasks, we show that different random seeds can produce either highly linear or
emergent scaling trends. We reveal that sharp breakthroughs in metrics are
produced by underlying continuous changes in their distribution across seeds.
Furthermore, we provide a case study of inverse scaling. We validate our
distributional scaling framework on realistic settings by measuring MMLU
performance in LM populations. These insights emphasize the role of random
variation in the effect of scale on LM capabilities.

</details>


### [766] [Sequential Function-Space Variational Inference via Gaussian Mixture Approximation](https://arxiv.org/pdf/2503.07114)
*Menghao Waiyan William Zhu, Pengcheng Hao, Ercan Engin Kuruoğlu*

Main category: cs.LG

TL;DR: The paper proposes a Gaussian mixture-based SFSVI method for continual learning, outperforming other methods in accuracy.


<details>
  <summary>Details</summary>
Motivation: To address the limitation of Gaussian distributions in approximating multi-modal posterior distributions in neural networks for continual learning.

Method: Uses a Gaussian mixture variational distribution in SFSVI and compares it with other variational inference methods, with and without fixed pre-trained feature extractors.

Result: Gaussian mixture SFSVI achieves higher final average accuracy, especially when continual learning is performed on all layers.

Conclusion: The proposed method is effective for continual learning, particularly in scenarios involving updates across all network layers.

Abstract: Continual learning in neural networks aims to learn new tasks without
forgetting old tasks. Sequential function-space variational inference (SFSVI)
uses a Gaussian variational distribution to approximate the distribution of the
outputs of the neural network corresponding to a finite number of selected
inducing points. Since the posterior distribution of a neural network is
multi-modal, a Gaussian distribution could only match one mode of the posterior
distribution, and a Gaussian mixture distribution could be used to better
approximate the posterior distribution. We propose an SFSVI method based on a
Gaussian mixture variational distribution. We also compare different types of
variational inference methods with a fixed pre-trained feature extractor (where
continual learning is performed on the final layer) and without a fixed
pre-trained feature extractor (where continual learning is performed on all
layers). We find that in terms of final average accuracy, likelihood-focused
Gaussian mixture SFSVI outperforms other sequential variational inference
methods, especially in the latter case.

</details>


### [767] [Enhancing Time Series Forecasting via a Parallel Hybridization of ARIMA and Polynomial Classifiers](https://arxiv.org/pdf/2505.06874)
*Thanh Son Nguyen, Van Thanh Nguyen, Dang Minh Duc Nguyen*

Main category: cs.LG

TL;DR: A hybrid model combining ARIMA and polynomial classifiers is proposed for time series forecasting, outperforming individual models in accuracy with slightly higher computational cost.


<details>
  <summary>Details</summary>
Motivation: To leverage the strengths of both linear (ARIMA) and non-linear (polynomial classifiers) models for improved time series forecasting.

Method: Integration of ARIMA with polynomial classifiers, evaluated on diverse real-world datasets.

Result: The hybrid model consistently achieves higher prediction accuracy than individual models, though with a modest increase in execution time.

Conclusion: The hybrid approach effectively combines linear and non-linear techniques for superior forecasting performance.

Abstract: Time series forecasting has attracted significant attention, leading to the
de-velopment of a wide range of approaches, from traditional statistical
meth-ods to advanced deep learning models. Among them, the Auto-Regressive
Integrated Moving Average (ARIMA) model remains a widely adopted linear
technique due to its effectiveness in modeling temporal dependencies in
economic, industrial, and social data. On the other hand, polynomial
classifi-ers offer a robust framework for capturing non-linear relationships
and have demonstrated competitive performance in domains such as stock price
pre-diction. In this study, we propose a hybrid forecasting approach that
inte-grates the ARIMA model with a polynomial classifier to leverage the
com-plementary strengths of both models. The hybrid method is evaluated on
multiple real-world time series datasets spanning diverse domains. Perfor-mance
is assessed based on forecasting accuracy and computational effi-ciency.
Experimental results reveal that the proposed hybrid model consist-ently
outperforms the individual models in terms of prediction accuracy, al-beit with
a modest increase in execution time.

</details>


### [768] [Predicting and Understanding College Student Mental Health with Interpretable Machine Learning](https://arxiv.org/pdf/2503.08002)
*Meghna Roy Chowdhury, Wei Xuan, Shreyas Sen, Yixue Zhao, Yi Ding*

Main category: cs.LG

TL;DR: I-HOPE is an interpretable hierarchical model for personalized mental health prediction in college students, achieving 91% accuracy and outperforming baseline methods.


<details>
  <summary>Details</summary>
Motivation: Addressing the challenges of predicting mental health in college students due to lack of large-scale longitudinal data, non-transparent models, and aggregated insights.

Method: A two-stage hierarchical model linking raw behavioral features to mental health status via five behavioral categories, evaluated on a five-year longitudinal dataset.

Result: Achieves 91% prediction accuracy, surpassing baseline methods (60-70%), and provides interpretable, individualized insights.

Conclusion: I-HOPE enables tailored mental health interventions and improves support, with code publicly available.

Abstract: Mental health issues among college students have reached critical levels,
significantly impacting academic performance and overall wellbeing. Predicting
and understanding mental health status among college students is challenging
due to three main factors: the necessity for large-scale longitudinal datasets,
the prevalence of black-box machine learning models lacking transparency, and
the tendency of existing approaches to provide aggregated insights at the
population level rather than individualized understanding.
  To tackle these challenges, this paper presents I-HOPE, the first
Interpretable Hierarchical mOdel for Personalized mEntal health prediction.
I-HOPE is a two-stage hierarchical model that connects raw behavioral features
to mental health status through five defined behavioral categories as
interaction labels. We evaluate I-HOPE on the College Experience Study, the
longest longitudinal mobile sensing dataset. This dataset spans five years and
captures data from both pre-pandemic periods and the COVID-19 pandemic. I-HOPE
achieves a prediction accuracy of 91%, significantly surpassing the 60-70%
accuracy of baseline methods. In addition, I-HOPE distills complex patterns
into interpretable and individualized insights, enabling the future development
of tailored interventions and improving mental health support. The code is
available at https://github.com/roycmeghna/I-HOPE.

</details>


### [769] [FlexiReg: Flexible Urban Region Representation Learning](https://arxiv.org/pdf/2503.09128)
*Fengze Sun, Yanchuan Chang, Egemen Tanin, Shanika Karunasekera, Jianzhong Qi*

Main category: cs.LG

TL;DR: FlexiReg is a flexible urban region representation learning model that adapts to varying region formations and input features, outperforming existing methods by up to 202% in downstream tasks.


<details>
  <summary>Details</summary>
Motivation: Existing urban region representation methods use fixed region formations and features, limiting their adaptability to diverse downstream tasks.

Method: FlexiReg employs spatial grid partitioning, learns grid cell representations using public data (POI, land use, satellite/street view imagery), and uses adaptive aggregation and prompt learning for task-specific tailoring.

Result: FlexiReg achieves up to 202% better accuracy than state-of-the-art models across four diverse downstream tasks.

Conclusion: FlexiReg's flexibility in region formation and feature input makes it highly effective for urban region representation learning and downstream applications.

Abstract: The increasing availability of urban data offers new opportunities for
learning region representations, which can be used as input to machine learning
models for downstream tasks such as check-in or crime prediction. While
existing solutions have produced promising results, an issue is their fixed
formation of regions and fixed input region features, which may not suit the
needs of different downstream tasks. To address this limitation, we propose a
model named FlexiReg for urban region representation learning that is flexible
with both the formation of urban regions and the input region features.
FlexiReg is based on a spatial grid partitioning over the spatial area of
interest. It learns representations for the grid cells, leveraging publicly
accessible data, including POI, land use, satellite imagery, and street view
imagery. We propose adaptive aggregation to fuse the cell representations and
prompt learning techniques to tailor the representations towards different
tasks, addressing the needs of varying formations of urban regions and
downstream tasks. Extensive experiments on five real-world datasets demonstrate
that FlexiReg outperforms state-of-the-art models by up to 202% in term of the
accuracy of four diverse downstream tasks using the produced urban region
representations.

</details>


### [770] [SciHorizon: Benchmarking AI-for-Science Readiness from Scientific Data to Large Language Models](https://arxiv.org/pdf/2503.13503)
*Chuan Qin, Xin Chen, Chengrui Wang, Pengmin Wu, Xi Chen, Yihang Cheng, Jingyi Zhao, Meng Xiao, Xiangchao Dong, Qingqing Long, Boya Pan, Han Wu, Chengzan Li, Yuanchun Zhou, Hui Xiong, Hengshu Zhu*

Main category: cs.LG

TL;DR: The paper introduces SciHorizon, a framework to assess AI4Science readiness by evaluating scientific data quality and LLM capabilities across multiple disciplines.


<details>
  <summary>Details</summary>
Motivation: The rapid advancement of AI, especially LLMs, has transformed scientific discovery, but a holistic assessment framework for AI4Science is lacking.

Method: Proposes SciHorizon, assessing AI-ready scientific data (Quality, FAIRness, Explainability, Compliance) and LLM capabilities (16 dimensions across 5 indicators). Evaluates 50+ LLMs using benchmark datasets.

Result: Provides recommendation lists of AI-ready datasets for Earth, Life, and Materials Sciences and evaluates LLMs across scientific disciplines. Results are publicly available.

Conclusion: SciHorizon offers a novel, comprehensive framework to benchmark AI4Science readiness, contributing to the field with accessible datasets and LLM evaluations.

Abstract: In recent years, the rapid advancement of Artificial Intelligence (AI)
technologies, particularly Large Language Models (LLMs), has revolutionized the
paradigm of scientific discovery, establishing AI-for-Science (AI4Science) as a
dynamic and evolving field. However, there is still a lack of an effective
framework for the overall assessment of AI4Science, particularly from a
holistic perspective on data quality and model capability. Therefore, in this
study, we propose SciHorizon, a comprehensive assessment framework designed to
benchmark the readiness of AI4Science from both scientific data and LLM
perspectives. First, we introduce a generalizable framework for assessing
AI-ready scientific data, encompassing four key dimensions: Quality, FAIRness,
Explainability, and Compliance-which are subdivided into 15 sub-dimensions.
Drawing on data resource papers published between 2018 and 2023 in
peer-reviewed journals, we present recommendation lists of AI-ready datasets
for Earth, Life, and Materials Sciences, making a novel and original
contribution to the field. Concurrently, to assess the capabilities of LLMs
across multiple scientific disciplines, we establish 16 assessment dimensions
based on five core indicators Knowledge, Understanding, Reasoning,
Multimodality, and Values spanning Mathematics, Physics, Chemistry, Life
Sciences, and Earth and Space Sciences. Using the developed benchmark datasets,
we have conducted a comprehensive evaluation of over 50 representative
open-source and closed source LLMs. All the results are publicly available and
can be accessed online at www.scihorizon.cn/en.

</details>


### [771] [Parameter Efficient Continual Learning with Dynamic Low-Rank Adaptation](https://arxiv.org/pdf/2505.11998)
*Prashant Shivaram Bhat, Shakib Yazdani, Elahe Arani, Bahram Zonooz*

Main category: cs.LG

TL;DR: PEARL is a rehearsal-free CL framework using dynamic rank allocation for LoRA to address catastrophic forgetting, outperforming baselines across multiple architectures.


<details>
  <summary>Details</summary>
Motivation: Catastrophic forgetting in CL undermines consolidated knowledge. Existing LoRA-based methods are sensitive to rank selection, leading to sub-optimal performance.

Method: PEARL dynamically allocates ranks for LoRA components based on task proximity to reference weights, avoiding rehearsal.

Result: PEARL outperforms baselines across ResNet, Separable Convolutional Network, and Vision Transformer in various CL scenarios.

Conclusion: PEARL effectively mitigates catastrophic forgetting with dynamic rank allocation, offering a versatile and efficient CL solution.

Abstract: Catastrophic forgetting has remained a critical challenge for deep neural
networks in Continual Learning (CL) as it undermines consolidated knowledge
when learning new tasks. Parameter efficient fine tuning CL techniques are
gaining traction for their effectiveness in addressing catastrophic forgetting
with a lightweight training schedule while avoiding degradation of consolidated
knowledge in pre-trained models. However, low rank adapters (LoRA) in these
approaches are highly sensitive to rank selection which can lead to sub-optimal
resource allocation and performance. To this end, we introduce PEARL, a
rehearsal-free CL framework that entails dynamic rank allocation for LoRA
components during CL training. Specifically, PEARL leverages reference task
weights and adaptively determines the rank of task-specific LoRA components
based on the current tasks' proximity to reference task weights in parameter
space. To demonstrate the versatility of PEARL, we evaluate it across three
vision architectures (ResNet, Separable Convolutional Network and Vision
Transformer) and a multitude of CL scenarios, and show that PEARL outperforms
all considered baselines by a large margin.

</details>


### [772] [Step-wise Adaptive Integration of Supervised Fine-tuning and Reinforcement Learning for Task-Specific LLMs](https://arxiv.org/pdf/2505.13026)
*Jack Chen, Fazhong Liu, Naruto Liu, Yuhan Luo, Erqu Qin, Harry Zheng, Tian Dong, Haojin Zhu, Yan Meng, Xiao Wang*

Main category: cs.LG

TL;DR: SASR is a step-wise adaptive hybrid training framework combining SFT and RL to enhance LLMs' reasoning, outperforming static methods.


<details>
  <summary>Details</summary>
Motivation: Overcoming limitations of SFT (overfitting) and RL (mode collapse) in training LLMs for reasoning tasks.

Method: Uses SFT for warm-up, then dynamically adjusts with RL (GRPO) based on gradient norms and divergence.

Result: SASR outperforms SFT, RL, and static hybrid methods in experiments.

Conclusion: SASR provides a balanced, adaptive approach to training LLMs for improved reasoning.

Abstract: Large language models (LLMs) excel at mathematical reasoning and logical
problem-solving. The current popular training paradigms primarily use
supervised fine-tuning (SFT) and reinforcement learning (RL) to enhance the
models' reasoning abilities. However, when using SFT or RL alone, there are
respective challenges: SFT may suffer from overfitting, while RL is prone to
mode collapse. The state-of-the-art methods have proposed hybrid training
schemes. However, static switching faces challenges such as poor generalization
across different tasks and high dependence on data quality. In response to
these challenges, inspired by the curriculum learning-quiz mechanism in human
reasoning cultivation, We propose SASR, a step-wise adaptive hybrid training
framework that theoretically unifies SFT and RL and dynamically balances the
two throughout optimization. SASR uses SFT for initial warm-up to establish
basic reasoning skills, and then uses an adaptive dynamic adjustment algorithm
based on gradient norm and divergence relative to the original distribution to
seamlessly integrate SFT with the online RL method GRPO. By monitoring the
training status of LLMs and adjusting the training process in sequence, SASR
ensures a smooth transition between training schemes, maintaining core
reasoning abilities while exploring different paths. Experimental results
demonstrate that SASR outperforms SFT, RL, and static hybrid training methods.

</details>


### [773] [Towards Efficient Training of Graph Neural Networks: A Multiscale Approach](https://arxiv.org/pdf/2503.19666)
*Eshed Gal, Moshe Eliasof, Carola-Bibiane Schönlieb, Ivan I. Kyrchei, Eldad Haber, Eran Treister*

Main category: cs.LG

TL;DR: A novel multiscale training framework for GNNs improves scalability and efficiency by leveraging hierarchical graph representations and subgraphs.


<details>
  <summary>Details</summary>
Motivation: Standard GNN training methods struggle with computational and memory challenges as graph sizes grow, limiting scalability.

Method: The approach uses hierarchical graph representations, subgraphs, and scalable strategies like coarse-to-fine learning and multiscale gradient computation.

Result: Multiscale training accelerates GNN training for large-scale problems while maintaining or improving predictive performance.

Conclusion: The proposed framework effectively addresses scalability issues in GNN training, offering practical benefits for large graphs.

Abstract: Graph Neural Networks (GNNs) have become powerful tools for learning from
graph-structured data, finding applications across diverse domains. However, as
graph sizes and connectivity increase, standard GNN training methods face
significant computational and memory challenges, limiting their scalability and
efficiency. In this paper, we present a novel framework for efficient
multiscale training of GNNs. Our approach leverages hierarchical graph
representations and subgraphs, enabling the integration of information across
multiple scales and resolutions. By utilizing coarser graph abstractions and
subgraphs, each with fewer nodes and edges, we significantly reduce
computational overhead during training. Building on this framework, we propose
a suite of scalable training strategies, including coarse-to-fine learning,
subgraph-to-full-graph transfer, and multiscale gradient computation. We also
provide some theoretical analysis of our methods and demonstrate their
effectiveness across various datasets and learning tasks. Our results show that
multiscale training can substantially accelerate GNN training for large scale
problems while maintaining, or even improving, predictive performance.

</details>


### [774] [When Are Concepts Erased From Diffusion Models?](https://arxiv.org/pdf/2505.17013)
*Kevin Lu, Nicky Kriplani, Rohit Gandikota, Minh Pham, David Bau, Chinmay Hegde, Niv Cohen*

Main category: cs.LG

TL;DR: The paper explores concept erasure in diffusion models, proposing two mechanisms and introducing a suite of evaluations to assess erasure effectiveness.


<details>
  <summary>Details</summary>
Motivation: Growing interest in concept erasure but unclear how thoroughly methods erase target concepts.

Method: Proposes two erasure mechanisms: reducing target concept likelihood and interfering with internal guidance. Introduces adversarial attacks, probing techniques, and alternative generation analysis for evaluation.

Result: Highlights tension between minimizing side effects and robustness to adversarial prompts.

Conclusion: Emphasizes the need for comprehensive evaluation in concept erasure for diffusion models.

Abstract: Concept erasure, the ability to selectively prevent a model from generating
specific concepts, has attracted growing interest, with various approaches
emerging to address the challenge. However, it remains unclear how thoroughly
these methods erase the target concept. We begin by proposing two conceptual
models for the erasure mechanism in diffusion models: (i) reducing the
likelihood of generating the target concept, and (ii) interfering with the
model's internal guidance mechanisms. To thoroughly assess whether a concept
has been truly erased from the model, we introduce a suite of independent
evaluations. Our evaluation framework includes adversarial attacks, novel
probing techniques, and analysis of the model's alternative generations in
place of the erased concept. Our results shed light on the tension between
minimizing side effects and maintaining robustness to adversarial prompts.
Broadly, our work underlines the importance of comprehensive evaluation for
erasure in diffusion models.

</details>


### [775] [RL in Name Only? Analyzing the Structural Assumptions in RL post-training for LLMs](https://arxiv.org/pdf/2505.13697)
*Soumya Rani Samineni, Durgesh Kalwar, Karthik Valmeekam, Kaya Stechly, Subbarao Kambhampati*

Main category: cs.LG

TL;DR: The paper critiques RL-based post-training of LLMs, showing that simplifying MDP assumptions make it equivalent to supervised learning, with comparable results on benchmarks.


<details>
  <summary>Details</summary>
Motivation: To critically examine the assumptions behind RL post-training of LLMs and assess their necessity.

Method: Analyzes MDP structural assumptions, compares RL (GRPO) with supervised fine-tuning on benchmarks like GSM8K and Countdown.

Result: Supervised fine-tuning matches GRPO performance, suggesting RL assumptions may not be necessary.

Conclusion: Simplistic MDP assumptions in RL frameworks for LLMs may not justify their use over supervised methods.

Abstract: Reinforcement learning-based post-training of large language models (LLMs)
has recently gained attention, particularly following the release of DeepSeek
R1, which applied GRPO for fine-tuning. Amid the growing hype around improved
reasoning abilities attributed to RL post-training, we critically examine the
formulation and assumptions underlying these methods. We start by highlighting
the popular structural assumptions made in modeling LLM training as a Markov
Decision Process (MDP), and show how they lead to a degenerate MDP that doesn't
quite need the RL/GRPO apparatus. The two critical structural assumptions
include (1) making the MDP states be just a concatenation of the actions-with
states becoming the context window and the actions becoming the tokens in LLMs
and (2) splitting the reward of a state-action trajectory uniformly across the
trajectory. Through a comprehensive analysis, we demonstrate that these
simplifying assumptions make the approach effectively equivalent to an
outcome-driven supervised learning. Our experiments on benchmarks including
GSM8K and Countdown using Qwen-2.5 base models show that iterative supervised
fine-tuning, incorporating both positive and negative samples, achieves
performance comparable to GRPO-based training. We will also argue that the
structural assumptions indirectly incentivize the RL to generate longer
sequences of intermediate tokens-which in turn feeds into the narrative of "RL
generating longer thinking traces." While RL may well be a very useful
technique for improving the reasoning abilities of LLMs, our analysis shows
that the simplistic structural assumptions made in modeling the underlying MDP
render the popular LLM RL frameworks and their interpretations questionable.

</details>


### [776] [shapr: Explaining Machine Learning Models with Conditional Shapley Values in R and Python](https://arxiv.org/pdf/2504.01842)
*Martin Jullum, Lars Henry Berge Olsen, Jon Lachmann, Annabelle Redelmeier*

Main category: cs.LG

TL;DR: The paper introduces the shapr R package and shaprpy Python library for Shapley value-based prediction explanations in machine learning and statistical models, emphasizing conditional estimates and feature dependencies.


<details>
  <summary>Details</summary>
Motivation: To enhance model interpretability by providing accurate and flexible Shapley value explanations, addressing gaps in existing tools.

Method: The packages use conditional Shapley value estimates, support tabular and time series data, and offer parallelized computations, iterative estimation, and visualization tools.

Result: The tools provide comprehensive, accurate, and flexible explanations for model predictions, including causal and asymmetric Shapley values when applicable.

Conclusion: The shapr and shaprpy packages improve model interpretability with a user-friendly and powerful framework.

Abstract: This paper introduces the shapr R package, a versatile tool for generating
Shapley value based prediction explanations for machine learning and
statistical regression models. Moreover, the shaprpy Python library brings the
core capabilities of shapr to the Python ecosystem. Shapley values originate
from cooperative game theory in the 1950s, but have over the past few years
become a widely used method for quantifying how a model's features/covariates
contribute to specific prediction outcomes. The shapr package emphasizes
conditional Shapley value estimates, providing a comprehensive range of
approaches for accurately capturing feature dependencies -- a crucial aspect
for correct model explanation, typically lacking in similar software. In
addition to regular tabular data, the shapr R package includes specialized
functionality for explaining time series forecasts. The package offers a
minimal set of user functions with sensible default values for most use cases
while providing extensive flexibility for advanced users to fine-tune
computations. Additional features include parallelized computations, iterative
estimation with convergence detection, and rich visualization tools. shapr also
extends its functionality to compute causal and asymmetric Shapley values when
causal information is available. Overall, the shapr and shaprpy packages aim to
enhance the interpretability of predictive models within a powerful and
user-friendly framework.

</details>


### [777] [Beyond Semantics: The Unreasonable Effectiveness of Reasonless Intermediate Tokens](https://arxiv.org/pdf/2505.13775)
*Kaya Stechly, Karthik Valmeekam, Atharva Gundawar, Vardhan Palod, Subbarao Kambhampati*

Main category: cs.LG

TL;DR: The paper challenges the assumption that intermediate reasoning traces (CoT) in large models causally improve performance, showing that even correct solutions can stem from invalid traces, and noisy traces can sometimes enhance generalization.


<details>
  <summary>Details</summary>
Motivation: To critically examine whether the semantics of intermediate tokens ("thoughts") in Chain of Thought (CoT) reasoning actually influence model performance, as opposed to being over-interpreted as human-like reasoning.

Method: Train transformer models on formally verifiable reasoning traces and solutions, aligning them with a formal solver (A* search). Evaluate correctness of intermediate traces and their causal influence on solution accuracy, including testing with noisy, corrupted traces.

Result: Models trained on correct traces still produce invalid reasoning traces for correct solutions. Noisy traces maintain or even improve performance and generalization, showing loose connection between trace and solution accuracy.

Conclusion: Intermediate tokens or CoT do not reliably induce predictable reasoning behaviors, cautioning against anthropomorphizing them or over-interpreting them as evidence of human-like reasoning in models.

Abstract: Recent impressive results from large reasoning models have been interpreted
as a triumph of Chain of Thought (CoT), and especially of the process of
training on CoTs sampled from base LLMs in order to help find new reasoning
patterns. In this paper, we critically examine that interpretation by
investigating how the semantics of intermediate tokens-often anthropomorphized
as "thoughts" or reasoning traces and which are claimed to display behaviors
like backtracking, self-verification etc.-actually influence model performance.
We train transformer models on formally verifiable reasoning traces and
solutions, constraining both intermediate steps and final outputs to align with
those of a formal solver (in our case, A* search). By constructing a formal
interpreter of the semantics of our problems and intended algorithm, we
systematically evaluate not only solution accuracy but also the correctness of
intermediate traces, thus allowing us to evaluate whether the latter causally
influences the former. We notice that, despite significant improvements on the
solution-only baseline, models trained on entirely correct traces still produce
invalid reasoning traces when arriving at correct solutions. To further show
that trace accuracy is only loosely connected to solution accuracy, we then
train models on noisy, corrupted traces which have no relation to the specific
problem each is paired with, and find that not only does performance remain
largely consistent with models trained on correct data, but in some cases can
improve upon it and generalize more robustly on out-of-distribution tasks.
These results challenge the assumption that intermediate tokens or "Chains of
Thought" induce predictable reasoning behaviors and caution against
anthropomorphizing such outputs or over-interpreting them (despite their mostly
correct forms) as evidence of human-like or algorithmic behaviors in language
models.

</details>


### [778] [Pairwise Optimal Transports for Training All-to-All Flow-Based Condition Transfer Model](https://arxiv.org/pdf/2504.03188)
*Kotaro Ikeda, Masanori Koyama, Jinzhe Zhang, Kohei Hayashi, Kenji Fukumizu*

Main category: cs.LG

TL;DR: A flow-based method for learning all-to-all transfer maps among conditional distributions, approximating pairwise optimal transport, with theoretical guarantees and demonstrated effectiveness on synthetic, benchmark, and chemical datasets.


<details>
  <summary>Details</summary>
Motivation: Addressing the challenge of handling continuous conditions with sparse empirical observations per condition.

Method: Proposes a novel cost function for simultaneous learning of optimal transports for all pairs of conditional distributions, supported by theoretical convergence guarantees.

Result: The method converges to pairwise optimal transports among infinite pairs of conditional distributions and is effective in coupling data points in conditional flow matching.

Conclusion: The proposed method successfully approximates pairwise optimal transport for continuous conditions and performs well across diverse datasets.

Abstract: In this paper, we propose a flow-based method for learning all-to-all
transfer maps among conditional distributions that approximates pairwise
optimal transport. The proposed method addresses the challenge of handling the
case of continuous conditions, which often involve a large set of conditions
with sparse empirical observations per condition. We introduce a novel cost
function that enables simultaneous learning of optimal transports for all pairs
of conditional distributions. Our method is supported by a theoretical
guarantee that, in the limit, it converges to the pairwise optimal transports
among infinite pairs of conditional distributions. The learned transport maps
are subsequently used to couple data points in conditional flow matching. We
demonstrate the effectiveness of this method on synthetic and benchmark
datasets, as well as on chemical datasets in which continuous physical
properties are defined as conditions.

</details>


### [779] [Bidirectional Variational Autoencoders](https://arxiv.org/pdf/2505.16074)
*Bart Kosko, Olaoluwa Adigun*

Main category: cs.LG

TL;DR: Bidirectional VAE (BVAE) uses a single network for encoding and decoding, reducing parameters by 50% while slightly outperforming traditional VAEs on image tasks.


<details>
  <summary>Details</summary>
Motivation: To simplify VAE architecture by using a bidirectional network for both encoding and decoding, reducing parameters without sacrificing performance.

Method: BVAE employs a single neural network for bidirectional encoding and decoding, tested on image tasks (reconstruction, classification, interpolation, generation) using datasets like MNIST, Fashion-MNIST, CIFAR-10, and CelebA-64.

Result: BVAE reduced parameters by nearly 50% and slightly outperformed traditional VAEs.

Conclusion: BVAE offers a more efficient and effective alternative to traditional VAEs by leveraging bidirectional processing.

Abstract: We present the new bidirectional variational autoencoder (BVAE) network
architecture. The BVAE uses a single neural network both to encode and decode
instead of an encoder-decoder network pair. The network encodes in the forward
direction and decodes in the backward direction through the same synaptic web.
Simulations compared BVAEs and ordinary VAEs on the four image tasks of image
reconstruction, classification, interpolation, and generation. The image
datasets included MNIST handwritten digits, Fashion-MNIST, CIFAR-10, and
CelebA-64 face images. The bidirectional structure of BVAEs cut the parameter
count by almost 50% and still slightly outperformed the unidirectional VAEs.

</details>


### [780] [From Continual Learning to SGD and Back: Better Rates for Continual Linear Models](https://arxiv.org/pdf/2504.04579)
*Itay Evron, Ran Levinstein, Matan Schliserman, Uri Sherman, Tomer Koren, Daniel Soudry, Nathan Srebro*

Main category: cs.LG

TL;DR: The paper analyzes forgetting in continual learning for overparameterized models, improving existing rates and proving new results for random task orderings.


<details>
  <summary>Details</summary>
Motivation: To understand and mitigate forgetting in continual learning setups, particularly for overparameterized models, by analyzing loss on previously seen tasks.

Method: Theoretical analysis of continual linear models, leveraging SGD bounds and focusing on random task orderings with and without replacement.

Result: Improved forgetting rates from O((d-r)/k) to O(min(k^{-1/4}, sqrt(d-r)/k, sqrt(Tr)/k)) for continual regression, and first rates for random orderings without replacement (O(min(T^{-1/4}, (d-r)/T))). Also, proved matching rates for linear classification.

Conclusion: Randomization alone can prevent catastrophic forgetting in long task sequences, and universal rates apply to broader projection methods like block Kaczmarz and POCS.

Abstract: We theoretically study the common continual learning setup where an
overparameterized model is sequentially fitted to a set of jointly realizable
tasks. We analyze the forgetting, i.e., loss on previously seen tasks, after
$k$ iterations. For continual linear models, we prove that fitting a task is
equivalent to a single stochastic gradient descent (SGD) step on a modified
objective. We develop novel last-iterate SGD upper bounds in the realizable
least squares setup, which we then leverage to derive new results for continual
learning. Focusing on random orderings over $T$ tasks, we establish universal
forgetting rates, whereas existing rates depend on the problem dimensionality
or complexity. Specifically, in continual regression with replacement, we
improve the best existing rate from $O((d-r)/k)$ to $O(\min(k^{-1/4},
\sqrt{d-r}/k, \sqrt{Tr}/k))$, where $d$ is the dimensionality and $r$ the
average task rank. Furthermore, we establish the first rate for random task
orderings without replacement. The obtained rate of $O(\min(T^{-1/4},
(d-r)/T))$ proves for the first time that randomization alone, with no task
repetition, can prevent catastrophic forgetting in sufficiently long task
sequences. Finally, we prove a matching $O(k^{-1/4})$ forgetting rate for
continual linear classification on separable data. Our universal rates apply
for broader projection methods, such as block Kaczmarz and POCS, illuminating
their loss convergence under i.i.d. and one-pass orderings.

</details>


### [781] [Guide your favorite protein sequence generative model](https://arxiv.org/pdf/2505.04823)
*Junhao Xiong, Hunter Nisonoff, Maria Lukarska, Ishan Gaur, Luke M. Oltrogge, David F. Savage, Jennifer Listgarten*

Main category: cs.LG

TL;DR: ProteinGuide is a framework for conditioning protein generative models on auxiliary data, demonstrated with models like ProteinMPNN and ESM3 for properties like stability and enzyme classes.


<details>
  <summary>Details</summary>
Motivation: Lack of a principled framework for conditioning protein generative models on auxiliary information like experimental data.

Method: Unifies protein generative models under a single framework (ProteinGuide) to condition on properties like stability, enzyme classes, and folds.

Result: Successfully guided models to generate sequences with specified properties and designed adenine base editor sequences for high activity.

Conclusion: ProteinGuide provides a general and effective method for conditioning protein generative models on diverse auxiliary data.

Abstract: Generative machine learning models on sequences are transforming protein
engineering. However, no principled framework exists for conditioning these
models on auxiliary information, such as experimental data, in a plug-and-play
manner. Herein, we present ProteinGuide -- a principled and general method for
conditioning -- by unifying a broad class of protein generative models under a
single framework. We demonstrate the applicability of ProteinGuide by guiding
two protein generative models, ProteinMPNN and ESM3, to generate amino acid and
structure token sequences, conditioned on several user-specified properties
such as enhanced stability, enzyme classes, and CATH-labeled folds. We also
used ProteinGuide with inverse folding models and our own experimental assay to
design adenine base editor sequences for high activity.

</details>


### [782] [ComplexFormer: Disruptively Advancing Transformer Inference Ability via Head-Specific Complex Vector Attention](https://arxiv.org/pdf/2505.10222)
*Jintian Shao, Hongyi Huang, Jiayi Wu, Beiwen Zhang, ZhiYu Wu, You Shan, MingKai Zheng*

Main category: cs.LG

TL;DR: ComplexFormer introduces Complex Multi-Head Attention (CMHA) to unify semantic and positional modeling in the complex plane, improving performance and efficiency.


<details>
  <summary>Details</summary>
Motivation: Address limitations of prior methods in integrating positional information flexibly across attention heads.

Method: Uses per-head Euler transformations and adaptive differential rotation to model semantic and positional differences in the complex plane.

Result: Outperforms baselines in language tasks, with lower perplexity and better long-context coherence.

Conclusion: ComplexFormer offers a more expressive and adaptable attention mechanism with strong parameter efficiency.

Abstract: Transformer models rely on self-attention to capture token dependencies but
face challenges in effectively integrating positional information while
allowing multi-head attention (MHA) flexibility. Prior methods often model
semantic and positional differences disparately or apply uniform positional
adjustments across heads, potentially limiting representational capacity. This
paper introduces ComplexFormer, featuring Complex Multi-Head Attention-CMHA.
CMHA empowers each head to independently model semantic and positional
differences unified within the complex plane, representing interactions as
rotations and scaling. ComplexFormer incorporates two key improvements: (1) a
per-head Euler transformation, converting real-valued query/key projections
into polar-form complex vectors for head-specific complex subspace operation;
and (2) a per-head adaptive differential rotation mechanism,
exp[i(Adapt(ASmn,i) + Delta(Pmn),i)], allowing each head to learn distinct
strategies for integrating semantic angle differences (ASmn,i) with relative
positional encodings (Delta(Pmn),i). Extensive experiments on language
modeling, text generation, code generation, and mathematical reasoning show
ComplexFormer achieves superior performance, significantly lower generation
perplexity , and improved long-context coherence compared to strong baselines
like RoPE-Transformers. ComplexFormer demonstrates strong parameter efficiency,
offering a more expressive, adaptable attention mechanism.

</details>


### [783] [Minimizing False-Positive Attributions in Explanations of Non-Linear Models](https://arxiv.org/pdf/2505.11210)
*Anders Gjølbye, Stefan Haufe, Lars Kai Hansen*

Main category: cs.LG

TL;DR: PatternLocal is a new XAI method that reduces false-positive attributions caused by suppressor variables in non-linear models by transforming discriminative model weights into a generative representation.


<details>
  <summary>Details</summary>
Motivation: Suppressor variables can mislead XAI methods by causing false-positive feature attributions, especially in non-linear models, limiting reliable explanations.

Method: PatternLocal uses locally linear surrogates (e.g., LIME, KernelSHAP) and transforms their discriminative weights into a generative representation to suppress suppressor variables.

Result: PatternLocal outperformed other XAI methods in benchmarks, reducing false-positive attributions and improving reliability for non-linear tasks.

Conclusion: PatternLocal provides more reliable and actionable insights for non-linear models by effectively addressing suppressor variables.

Abstract: Suppressor variables can influence model predictions without being dependent
on the target outcome and they pose a significant challenge for Explainable AI
(XAI) methods. These variables may cause false-positive feature attributions,
undermining the utility of explanations. Although effective remedies exist for
linear models, their extension to non-linear models and to instance-based
explanations has remained limited. We introduce PatternLocal, a novel XAI
technique that addresses this gap. PatternLocal begins with a locally linear
surrogate, e.g. LIME, KernelSHAP, or gradient-based methods, and transforms the
resulting discriminative model weights into a generative representation,
thereby suppressing the influence of suppressor variables while preserving
local fidelity. In extensive hyperparameter optimization on the XAI-TRIS
benchmark, PatternLocal consistently outperformed other XAI methods and reduced
false-positive attributions when explaining non-linear tasks, thereby enabling
more reliable and actionable insights.

</details>


### [784] [FRIREN: Beyond Trajectories -- A Spectral Lens on Time](https://arxiv.org/pdf/2505.17370)
*Qilin Wang*

Main category: cs.LG

TL;DR: FRIREN introduces a geometry-preserving model for long-term time-series forecasting, focusing on Wasserstein-2 distance and spectral dynamics, outperforming benchmarks like TimeMixer.


<details>
  <summary>Details</summary>
Motivation: Existing LTSF models assume pointwise predictability; FRIREN argues for geometric structure as a better abstraction for dynamic-agnostic forecasting.

Method: FRIREN uses an augmented normalizing-flow block for latent representation and generates W2-efficient paths, decomposing dynamics into rotation, scaling, and translation.

Result: FRIREN achieves superior MSE, MAE, and SWD on Lorenz-63 and Rossler, maintaining predictions for 2.5 Lyapunov times.

Conclusion: FRIREN combines generative flows with spectral analysis, offering accurate and interpretable long-term forecasting, setting a new LTSF benchmark.

Abstract: Long-term time-series forecasting (LTSF) models are often presented as
general-purpose solutions that can be applied across domains, implicitly
assuming that all data is pointwise predictable. Using chaotic systems such as
Lorenz-63 as a case study, we argue that geometric structure - not pointwise
prediction - is the right abstraction for a dynamic-agnostic foundational
model. Minimizing the Wasserstein-2 distance (W2), which captures geometric
changes, and providing a spectral view of dynamics are essential for
long-horizon forecasting. Our model, FRIREN (Flow-inspired Representations via
Interpretable Eigen-networks), implements an augmented normalizing-flow block
that embeds data into a normally distributed latent representation. It then
generates a W2-efficient optimal path that can be decomposed into rotation,
scaling, inverse rotation, and translation. This architecture yields locally
generated, geometry-preserving predictions that are independent of the
underlying dynamics, and a global spectral representation that functions as a
finite Koopman operator with a small modification. This enables practitioners
to identify which modes grow, decay, or oscillate, both locally and
system-wide. FRIREN achieves an MSE of 11.4, MAE of 1.6, and SWD of 0.96 on
Lorenz-63 in a 336-in, 336-out, dt=0.01 setting, surpassing TimeMixer (MSE
27.3, MAE 2.8, SWD 2.1). The model maintains effective prediction for 274 out
of 336 steps, approximately 2.5 Lyapunov times. On Rossler (96-in, 336-out),
FRIREN achieves an MSE of 0.0349, MAE of 0.0953, and SWD of 0.0170,
outperforming TimeMixer's MSE of 4.3988, MAE of 0.886, and SWD of 3.2065.
FRIREN is also competitive on standard LTSF datasets such as ETT and Weather.
By connecting modern generative flows with classical spectral analysis, FRIREN
makes long-term forecasting both accurate and interpretable, setting a new
benchmark for LTSF model design.

</details>


### [785] [Where You Place the Norm Matters: From Prejudiced to Neutral Initializations](https://arxiv.org/pdf/2505.11312)
*Emanuele Francazi, Francesco Pinto, Aurelien Lucchi, Marco Baity-Jesi*

Main category: cs.LG

TL;DR: The paper explores how normalization layers (e.g., Batch Normalization, Layer Normalization) affect neural network behavior at initialization, influencing early training dynamics and class prediction distributions.


<details>
  <summary>Details</summary>
Motivation: Despite the practical success of normalization layers, their theoretical impact on model behavior, especially at initialization, is not well understood.

Method: The study analyzes how the presence and placement of normalization within hidden layers shape the statistical properties of network predictions before training begins.

Result: Normalization placement induces systematic differences in initial prediction behavior, ranging from unbiased (Neutral) to highly concentrated (Prejudiced) class predictions.

Conclusion: The work provides a principled understanding of normalization's influence on early training and offers guidance for more controlled and interpretable network design.

Abstract: Normalization layers, such as Batch Normalization and Layer Normalization,
are central components in modern neural networks, widely adopted to improve
training stability and generalization. While their practical effectiveness is
well documented, a detailed theoretical understanding of how normalization
affects model behavior, starting from initialization, remains an important open
question. In this work, we investigate how both the presence and placement of
normalization within hidden layers influence the statistical properties of
network predictions before training begins. In particular, we study how these
choices shape the distribution of class predictions at initialization, which
can range from unbiased (Neutral) to highly concentrated (Prejudiced) toward a
subset of classes. Our analysis shows that normalization placement induces
systematic differences in the initial prediction behavior of neural networks,
which in turn shape the dynamics of learning. By linking architectural choices
to prediction statistics at initialization, our work provides a principled
understanding of how normalization can influence early training behavior and
offers guidance for more controlled and interpretable network design.

</details>


### [786] [Towards Identifiability of Interventional Stochastic Differential Equations](https://arxiv.org/pdf/2505.15987)
*Aaron Zweig, Zaikang Lin, Elham Azizi, David Knowles*

Main category: cs.LG

TL;DR: The paper provides provable bounds for uniquely recovering SDE parameters using stationary distributions under multiple interventions, with tight bounds for linear SDEs and upper bounds for nonlinear SDEs.


<details>
  <summary>Details</summary>
Motivation: To address the identifiability of SDE models under interventions, ensuring unique parameter recovery from stationary distributions.

Method: Theoretical analysis of SDE models under interventions, with experimental validation on synthetic data and exploration of learnable activation functions.

Result: Tight bounds for linear SDEs and upper bounds for nonlinear SDEs in small noise regimes; successful parameter recovery in experiments.

Conclusion: The study advances SDE identifiability, with practical implications for parameterization and learnable activation functions.

Abstract: We study identifiability of stochastic differential equation (SDE) models
under multiple interventions. Our results give the first provable bounds for
unique recovery of SDE parameters given samples from their stationary
distributions. We give tight bounds on the number of necessary interventions
for linear SDEs, and upper bounds for nonlinear SDEs in the small noise regime.
We experimentally validate the recovery of true parameters in synthetic data,
and motivated by our theoretical results, demonstrate the advantage of
parameterizations with learnable activation functions.

</details>


### [787] [Mixture of Low Rank Adaptation with Partial Parameter Sharing for Time Series Forecasting](https://arxiv.org/pdf/2505.17872)
*Licheng Pan, Zhichao Chen, Haoxuan Li, Guangyi Liu, Zhijian Xu, Zhaoran Liu, Hao Wang, Ying Wei*

Main category: cs.LG

TL;DR: The paper addresses the Expressiveness Bottleneck in multi-task time-series forecasting by proposing a two-stage framework with step-specific LoRA modules and a Mixture-of-LoRA (MoLA) model for improved performance.


<details>
  <summary>Details</summary>
Motivation: Multi-task forecasting suffers from an Expressiveness Bottleneck, where shared representations for predictions at different time steps lead to unavoidable errors.

Method: A two-stage framework: pre-train a foundation model for one-step-ahead prediction, then adapt it using step-specific LoRA modules. Introduces MoLA for adaptive parameter sharing.

Result: MoLA significantly improves model expressiveness and outperforms state-of-the-art time-series forecasting methods.

Conclusion: The proposed framework and MoLA effectively address the Expressiveness Bottleneck, enhancing forecasting performance and efficiency.

Abstract: Multi-task forecasting has become the standard approach for time-series
forecasting (TSF). However, we show that it suffers from an Expressiveness
Bottleneck, where predictions at different time steps share the same
representation, leading to unavoidable errors even with optimal
representations. To address this issue, we propose a two-stage framework:
first, pre-train a foundation model for one-step-ahead prediction; then, adapt
it using step-specific LoRA modules.This design enables the foundation model to
handle any number of forecast steps while avoiding the expressiveness
bottleneck. We further introduce the Mixture-of-LoRA (MoLA) model, which
employs adaptively weighted LoRA experts to achieve partial parameter sharing
across steps. This approach enhances both efficiency and forecasting
performance by exploiting interdependencies between forecast steps. Experiments
show that MoLA significantly improves model expressiveness and outperforms
state-of-the-art time-series forecasting methods. Code is available at
https://anonymous.4open.science/r/MoLA-BC92.

</details>


### [788] [Divide-Fuse-Conquer: Eliciting "Aha Moments" in Multi-Scenario Games](https://arxiv.org/pdf/2505.16401)
*Xiaoqing Zhang, Huabin Zheng, Ang Lv, Yuhan Liu, Zirui Song, Flood Sung, Xiuying Chen, Rui Yan*

Main category: cs.LG

TL;DR: The paper introduces Divide-Fuse-Conquer, a framework to improve generalization in multi-scenario RL for LLMs, achieving competitive performance in diverse games.


<details>
  <summary>Details</summary>
Motivation: Challenges in multi-scenario RL, such as poor generalization and training instability, hinder LLMs' reasoning abilities in diverse environments.

Method: Proposes Divide-Fuse-Conquer: grouping games, training specialized models per group, fusing parameters, and training for all groups.

Result: Experiments show Qwen2.5-32B-Align matches Claude3.5's performance, with 7 wins and 4 draws in 18 games.

Conclusion: The framework enhances LLM generalization in RL, offering a promising direction for future research.

Abstract: Large language models (LLMs) have been observed to suddenly exhibit advanced
reasoning abilities during reinforcement learning (RL), resembling an ``aha
moment'' triggered by simple outcome-based rewards. While RL has proven
effective in eliciting such breakthroughs in tasks involving mathematics,
coding, and vision, it faces significant challenges in multi-scenario games.
The diversity of game rules, interaction modes, and environmental complexities
often leads to policies that perform well in one scenario but fail to
generalize to others. Simply combining multiple scenarios during training
introduces additional challenges, such as training instability and poor
performance. To overcome these challenges, we propose Divide-Fuse-Conquer, a
framework designed to enhance generalization in multi-scenario RL. This
approach starts by heuristically grouping games based on characteristics such
as rules and difficulties. Specialized models are then trained for each group
to excel at games in the group is what we refer to as the divide step. Next, we
fuse model parameters from different groups as a new model, and continue
training it for multiple groups, until the scenarios in all groups are
conquered. Experiments across 18 TextArena games show that Qwen2.5-32B-Align
trained with the Divide-Fuse-Conquer strategy reaches a performance level
comparable to Claude3.5, achieving 7 wins and 4 draws. We hope our approach can
inspire future research on using reinforcement learning to improve the
generalization of LLMs.

</details>


### [789] [Towards Revealing the Effectiveness of Small-Scale Fine-tuning in R1-style Reinforcement Learning](https://arxiv.org/pdf/2505.17988)
*Yutong Chen, Jiandong Gao, Ji Wu*

Main category: cs.LG

TL;DR: Re-distillation, a technique combining small-scale distillation from RL-trained policies with SFT, improves efficiency and matches RL performance with fewer samples.


<details>
  <summary>Details</summary>
Motivation: To understand the unclear mechanism of rule-based RL and address the inefficiency of small-scale SFT in enhancing LLMs' reasoning.

Method: Proposed an analytical framework to compare SFT and RL efficiency, then introduced Re-distillation for fine-tuning pretrained models.

Result: Re-distilled models outperformed RL-trained models with fewer samples (e.g., 1K SFT samples surpassed DeepSeek-V3-0324 on K&K dataset).

Conclusion: Re-distillation explains R1-style RL's success and offers a more efficient alternative to traditional methods.

Abstract: R1-style Reinforcement Learning (RL) significantly enhances Large Language
Models' reasoning capabilities, yet the mechanism behind rule-based RL remains
unclear. We found that small-scale SFT has significant influence on RL but
shows poor efficiency. To explain our observations, we propose an analytical
framework and compare the efficiency of SFT and RL by measuring sample effect.
Hypothetical analysis show that SFT efficiency is limited by training data.
Guided by our analysis, we propose Re-distillation, a technique that fine-tunes
pretrain model through small-scale distillation from the RL-trained policy.
Experiments on Knight & Knave and MATH datasets demonstrate re-distillation's
surprising efficiency: re-distilled models match RL performance with far fewer
samples and less computation. Empirical verification shows that sample effect
is a good indicator of performance improvements. As a result, on K&K dataset,
our re-distilled Qwen2.5-1.5B model surpasses DeepSeek-V3-0324 with only 1K SFT
samples. On MATH, Qwen2.5-1.5B fine-tuned with re-distilled 500 samples matches
its instruct-tuned variant without RL. Our work explains several interesting
phenomena in R1-style RL, shedding light on the mechanisms behind its empirical
success. Code is available at: https://github.com/on1262/deep-reasoning

</details>


### [790] [Training on Plausible Counterfactuals Removes Spurious Correlations](https://arxiv.org/pdf/2505.16583)
*Shpresim Sadiku, Kartikeya Chitranshi, Hiroshi Kera, Sebastian Pokutta*

Main category: cs.LG

TL;DR: Training classifiers on plausible counterfactual explanations (p-CFEs) labeled with incorrect targets improves accuracy and reduces bias.


<details>
  <summary>Details</summary>
Motivation: To explore whether classifiers can learn effectively from p-CFEs, extending the adversarial perturbation paradigm to plausible perturbations.

Method: Train classifiers on p-CFEs labeled with incorrect target classes and evaluate their performance on unperturbed inputs.

Result: Classifiers achieve high in-distribution accuracy and significantly reduced bias from spurious correlations.

Conclusion: Learning from p-CFEs is more effective than adversarial perturbations, enhancing both accuracy and fairness.

Abstract: Plausible counterfactual explanations (p-CFEs) are perturbations that
minimally modify inputs to change classifier decisions while remaining
plausible under the data distribution. In this study, we demonstrate that
classifiers can be trained on p-CFEs labeled with induced \emph{incorrect}
target classes to classify unperturbed inputs with the original labels. While
previous studies have shown that such learning is possible with adversarial
perturbations, we extend this paradigm to p-CFEs. Interestingly, our
experiments reveal that learning from p-CFEs is even more effective: the
resulting classifiers achieve not only high in-distribution accuracy but also
exhibit significantly reduced bias with respect to spurious correlations.

</details>


### [791] [NeUQI: Near-Optimal Uniform Quantization Parameter Initialization](https://arxiv.org/pdf/2505.17595)
*Li Lin, Xinyu Hu, Xiaojun Wan*

Main category: cs.LG

TL;DR: NeUQI is a method for optimizing initial parameters in uniform quantization of LLMs, improving performance and efficiency compared to existing methods.


<details>
  <summary>Details</summary>
Motivation: LLMs face deployment challenges due to high memory and inference costs, and current quantization initialization methods are suboptimal.

Method: Proposes NeUQI, a method for near-optimal initialization of uniform quantization parameters, compatible with existing quantization techniques.

Result: NeUQI outperforms existing methods on LLaMA and Qwen models and matches resource-intensive approaches when combined with distillation.

Conclusion: NeUQI offers an efficient, high-performance solution for LLM quantization initialization, enhancing deployment feasibility.

Abstract: Large language models (LLMs) achieve impressive performance across domains
but face significant challenges when deployed on consumer-grade GPUs or
personal devices such as laptops, due to high memory consumption and inference
costs. Post-training quantization (PTQ) of LLMs offers a promising solution
that reduces their memory footprint and decoding latency. In practice, PTQ with
uniform quantization representation is favored for its efficiency and ease of
deployment since uniform quantization is widely supported by mainstream
hardware and software libraries. Recent studies on $\geq 2$-bit uniform
quantization have led to noticeable improvements in post-quantization model
performance; however, they primarily focus on quantization methodologies, while
the initialization of quantization parameters is underexplored and still relies
on the suboptimal Min-Max strategies. In this work, we propose NeUQI, a method
devoted to efficiently determining near-optimal initial parameters for uniform
quantization. NeUQI is orthogonal to prior quantization methodologies and can
seamlessly integrate with them. The experiments with the LLaMA and Qwen
families on various tasks demonstrate that our NeUQI consistently outperforms
existing methods. Furthermore, when combined with a lightweight distillation
strategy, NeUQI can achieve superior performance to PV-tuning, a much more
resource-intensive approach.

</details>


### [792] [Navigate the Unknown: Enhancing LLM Reasoning with Intrinsic Motivation Guided Exploration](https://arxiv.org/pdf/2505.17621)
*Jingtong Gao, Ling Pan, Yejing Wang, Rui Zhong, Chi Lu, Qingpeng Cai, Peng Jiang, Xiangyu Zhao*

Main category: cs.LG

TL;DR: i-MENTOR improves RL for LLMs by addressing sparse rewards and exploration issues, achieving a 22.39% boost on Countdown-4.


<details>
  <summary>Details</summary>
Motivation: Current RL methods like PPO and GRPO struggle with sparse rewards and poor exploration, hindering multi-step reasoning in LLMs.

Method: i-MENTOR introduces trajectory-aware exploration rewards, dynamic reward scaling, and advantage-preserving reward implementation.

Result: i-MENTOR shows a 22.39% improvement on the Countdown-4 dataset.

Conclusion: i-MENTOR effectively enhances RL-based training for LLMs by improving reward density and exploration.

Abstract: Reinforcement learning (RL) has emerged as a pivotal method for improving the
reasoning capabilities of Large Language Models (LLMs). However, prevalent RL
approaches such as Proximal Policy Optimization (PPO) and Group-Regularized
Policy Optimization (GRPO) face critical limitations due to their reliance on
sparse outcome-based rewards and inadequate mechanisms for incentivizing
exploration. These limitations result in inefficient guidance for multi-step
reasoning processes. Specifically, sparse reward signals fail to deliver
effective or sufficient feedback, particularly for challenging problems.
Furthermore, such reward structures induce systematic biases that prioritize
exploitation of familiar trajectories over novel solution discovery. These
shortcomings critically hinder performance in complex reasoning tasks, which
inherently demand iterative refinement across ipntermediate steps. To address
these challenges, we propose an Intrinsic Motivation guidEd exploratioN meThOd
foR LLM Reasoning (i-MENTOR), a novel method designed to both deliver dense
rewards and amplify explorations in the RL-based training paradigm. i-MENTOR
introduces three key innovations: trajectory-aware exploration rewards that
mitigate bias in token-level strategies while maintaining computational
efficiency; dynamic reward scaling to stabilize exploration and exploitation in
large action spaces; and advantage-preserving reward implementation that
maintains advantage distribution integrity while incorporating exploratory
guidance. Experiments across three public datasets demonstrate i-MENTOR's
effectiveness with a 22.39% improvement on the difficult dataset Countdown-4.

</details>


### [793] [The Third Pillar of Causal Analysis? A Measurement Perspective on Causal Representations](https://arxiv.org/pdf/2505.17708)
*Dingling Yao, Shimeng Huang, Riccardo Cadei, Kun Zhang, Francesco Locatello*

Main category: cs.LG

TL;DR: The paper reinterprets causal representation learning (CRL) using a measurement model framework, introducing a T-MEX score to assess representation quality for causal tasks.


<details>
  <summary>Details</summary>
Motivation: Challenges in causal analysis due to complex, noisy, high-dimensional data and unclear utility of learned representations for downstream tasks.

Method: Reinterpret CRL as proxy measurements of latent variables; propose T-MEX score for quantitative evaluation.

Result: Validated T-MEX in simulations and real-world scenarios, showing effective assessment of representation quality.

Conclusion: The framework and T-MEX score provide principled evaluation of learned representations for causal tasks.

Abstract: Causal reasoning and discovery, two fundamental tasks of causal analysis,
often face challenges in applications due to the complexity, noisiness, and
high-dimensionality of real-world data. Despite recent progress in identifying
latent causal structures using causal representation learning (CRL), what makes
learned representations useful for causal downstream tasks and how to evaluate
them are still not well understood. In this paper, we reinterpret CRL using a
measurement model framework, where the learned representations are viewed as
proxy measurements of the latent causal variables. Our approach clarifies the
conditions under which learned representations support downstream causal
reasoning and provides a principled basis for quantitatively assessing the
quality of representations using a new Test-based Measurement EXclusivity
(T-MEX) score. We validate T-MEX across diverse causal inference scenarios,
including numerical simulations and real-world ecological video analysis,
demonstrating that the proposed framework and corresponding score effectively
assess the identification of learned representations and their usefulness for
causal downstream tasks.

</details>


### [794] [Out of the Shadows: Exploring a Latent Space for Neural Network Verification](https://arxiv.org/pdf/2505.17854)
*Lukas Koller, Tobias Ladner, Matthias Althoff*

Main category: cs.LG

TL;DR: The paper introduces a novel latent space for formal verification of neural networks to address conservatism in output enclosures, enabling iterative refinement of input specifications for safer verification.


<details>
  <summary>Details</summary>
Motivation: Neural networks are sensitive to small input changes, making formal verification crucial for safety-critical applications. Current methods often produce inconclusive results due to conservatism in output enclosures.

Method: The authors design a latent space using projection-based set representations (e.g., zonotopes) to transfer output specifications to the input space. This allows iterative refinement of inputs to focus on unsafe cases. The approach leverages matrix operations for efficiency and GPU acceleration.

Result: The proposed tool reduces subproblems in branch-and-bound procedures and achieves competitive performance, ranking among top tools in VNN-COMP'24.

Conclusion: The latent space and iterative refinement method significantly improve neural network verification efficiency and accuracy, making it suitable for safety-critical applications.

Abstract: Neural networks are ubiquitous. However, they are often sensitive to small
input changes. Hence, to prevent unexpected behavior in safety-critical
applications, their formal verification -- a notoriously hard problem -- is
necessary. Many state-of-the-art verification algorithms use reachability
analysis or abstract interpretation to enclose the set of possible outputs of a
neural network. Often, the verification is inconclusive due to the conservatism
of the enclosure. To address this problem, we design a novel latent space for
formal verification that enables the transfer of output specifications to the
input space for an iterative specification-driven input refinement, i.e., we
iteratively reduce the set of possible inputs to only enclose the unsafe ones.
The latent space is constructed from a novel view of projection-based set
representations, e.g., zonotopes, which are commonly used in reachability
analysis of neural networks. A projection-based set representation is a
"shadow" of a higher-dimensional set -- a latent space -- that does not change
during a set propagation through a neural network. Hence, the input set and the
output enclosure are "shadows" of the same latent space that we can use to
transfer constraints. We present an efficient verification tool for neural
networks that uses our iterative refinement to significantly reduce the number
of subproblems in a branch-and-bound procedure. Using zonotopes as a set
representation, unlike many other state-of-the-art approaches, our approach can
be realized by only using matrix operations, which enables a significant
speed-up through efficient GPU acceleration. We demonstrate that our tool
achieves competitive performance, which would place it among the top-ranking
tools of the last neural network verification competition (VNN-COMP'24).

</details>


### [795] [BLAST: Balanced Sampling Time Series Corpus for Universal Forecasting Models](https://arxiv.org/pdf/2505.17871)
*Zezhi Shao, Yujie Li, Fei Wang, Chengqing Yu, Yisong Fu, Tangwen Qian, Bin Xu, Boyu Diao, Yongjun Xu, Xueqi Cheng*

Main category: cs.LG

TL;DR: BLAST introduces a balanced pre-training corpus to enhance data diversity for universal time series forecasting, improving model performance and efficiency.


<details>
  <summary>Details</summary>
Motivation: Existing time series datasets have biases and imbalances, limiting model generalization. BLAST addresses this by ensuring diverse and balanced data coverage.

Method: BLAST uses 321 billion observations, statistical metrics, grid-based clustering, and grid sampling/mixup for balanced pattern coverage.

Result: Models pre-trained on BLAST achieve state-of-the-art performance with fewer computational resources and training tokens.

Conclusion: Data diversity is crucial for training efficiency and performance in universal forecasting, as demonstrated by BLAST.

Abstract: The advent of universal time series forecasting models has revolutionized
zero-shot forecasting across diverse domains, yet the critical role of data
diversity in training these models remains underexplored. Existing large-scale
time series datasets often suffer from inherent biases and imbalanced
distributions, leading to suboptimal model performance and generalization. To
address this gap, we introduce BLAST, a novel pre-training corpus designed to
enhance data diversity through a balanced sampling strategy. First, BLAST
incorporates 321 billion observations from publicly available datasets and
employs a comprehensive suite of statistical metrics to characterize time
series patterns. Then, to facilitate pattern-oriented sampling, the data is
implicitly clustered using grid-based partitioning. Furthermore, by integrating
grid sampling and grid mixup techniques, BLAST ensures a balanced and
representative coverage of diverse patterns. Experimental results demonstrate
that models pre-trained on BLAST achieve state-of-the-art performance with a
fraction of the computational resources and training tokens required by
existing methods. Our findings highlight the pivotal role of data diversity in
improving both training efficiency and model performance for the universal
forecasting task.

</details>


### [796] [Prompting Decision Transformers for Zero-Shot Reach-Avoid Policies](https://arxiv.org/pdf/2505.19337)
*Kevin Li, Marinka Zitnik*

Main category: cs.LG

TL;DR: RADT is a decision transformer model for offline, reward-free, goal-conditioned RL, enabling dynamic avoid-region specification and outperforming baselines without retraining.


<details>
  <summary>Details</summary>
Motivation: Existing methods lack flexibility in specifying avoid regions dynamically and rely on well-designed rewards, limiting scalability.

Method: RADT encodes goals and avoid regions as prompt tokens, uses hindsight relabeling, and trains on suboptimal offline trajectories.

Result: RADT outperforms baselines in zero-shot settings, achieving a 35.7% improvement in normalized cost while maintaining goal-reaching success.

Conclusion: RADT is effective for reach-avoid tasks, including complex applications like cell reprogramming, due to its flexibility and generalization capabilities.

Abstract: Offline goal-conditioned reinforcement learning methods have shown promise
for reach-avoid tasks, where an agent must reach a target state while avoiding
undesirable regions of the state space. Existing approaches typically encode
avoid-region information into an augmented state space and cost function, which
prevents flexible, dynamic specification of novel avoid-region information at
evaluation time. They also rely heavily on well-designed reward and cost
functions, limiting scalability to complex or poorly structured environments.
We introduce RADT, a decision transformer model for offline, reward-free,
goal-conditioned, avoid region-conditioned RL. RADT encodes goals and avoid
regions directly as prompt tokens, allowing any number of avoid regions of
arbitrary size to be specified at evaluation time. Using only suboptimal
offline trajectories from a random policy, RADT learns reach-avoid behavior
through a novel combination of goal and avoid-region hindsight relabeling. We
benchmark RADT against 3 existing offline goal-conditioned RL models across 11
tasks, environments, and experimental settings. RADT generalizes in a zero-shot
manner to out-of-distribution avoid region sizes and counts, outperforming
baselines that require retraining. In one such zero-shot setting, RADT achieves
35.7% improvement in normalized cost over the best retrained baseline while
maintaining high goal-reaching success. We apply RADT to cell reprogramming in
biology, where it reduces visits to undesirable intermediate gene expression
states during trajectories to desired target states, despite stochastic
transitions and discrete, structured state dynamics.

</details>


### [797] [Directed Semi-Simplicial Learning with Applications to Brain Activity Decoding](https://arxiv.org/pdf/2505.17939)
*Manuel Lecha, Andrea Cavallo, Francesca Dominici, Ran Levi, Alessio Del Bue, Elvin Isufi, Pietro Morerio, Claudio Battiloro*

Main category: cs.LG

TL;DR: The paper introduces Semi-Simplicial Neural Networks (SSNs) to address the limitations of GNNs and TDL in capturing directed higher-order interactions, achieving superior performance in brain dynamics tasks.


<details>
  <summary>Details</summary>
Motivation: Existing TDL models lack the ability to capture directed higher-order interactions, which are crucial in systems like brain networks.

Method: Proposes SSNs, which operate on semi-simplicial sets, and Routing-SSNs for scalable, dynamic relation selection.

Result: SSNs outperform existing models by up to 27% in brain dynamics classification and 50% over GNNs.

Conclusion: SSNs demonstrate the potential of topological models for structured data, particularly in brain dynamics, and show competitive performance in standard tasks.

Abstract: Graph Neural Networks (GNNs) excel at learning from pairwise interactions but
often overlook multi-way and hierarchical relationships. Topological Deep
Learning (TDL) addresses this limitation by leveraging combinatorial
topological spaces. However, existing TDL models are restricted to undirected
settings and fail to capture the higher-order directed patterns prevalent in
many complex systems, e.g., brain networks, where such interactions are both
abundant and functionally significant. To fill this gap, we introduce
Semi-Simplicial Neural Networks (SSNs), a principled class of TDL models that
operate on semi-simplicial sets -- combinatorial structures that encode
directed higher-order motifs and their directional relationships. To enhance
scalability, we propose Routing-SSNs, which dynamically select the most
informative relations in a learnable manner. We prove that SSNs are strictly
more expressive than standard graph and TDL models. We then introduce a new
principled framework for brain dynamics representation learning, grounded in
the ability of SSNs to provably recover topological descriptors shown to
successfully characterize brain activity. Empirically, SSNs achieve
state-of-the-art performance on brain dynamics classification tasks,
outperforming the second-best model by up to 27%, and message passing GNNs by
up to 50% in accuracy. Our results highlight the potential of principled
topological models for learning from structured brain data, establishing a
unique real-world case study for TDL. We also test SSNs on standard node
classification and edge regression tasks, showing competitive performance. We
will make the code and data publicly available.

</details>


### [798] [Towards Analyzing and Understanding the Limitations of VAPO: A Theoretical Perspective](https://arxiv.org/pdf/2505.17997)
*Jintian Shao, Yiming Cheng, Hongyi Huang, Beiwen Zhang, Zhiyu Wu, You Shan, Mingkai Zheng*

Main category: cs.LG

TL;DR: VAPO improves reinforcement learning for long CoT reasoning with LLMs by tackling challenges like bias and sparse rewards, but its theoretical foundations need deeper exploration.


<details>
  <summary>Details</summary>
Motivation: To enhance the theoretical understanding of VAPO's mechanisms and limitations for future improvements in reasoning tasks.

Method: Analyzes VAPO's value function approximation, adaptive advantage estimation, token-level optimization, and exploration challenges.

Result: Identifies areas where VAPO's assumptions may be challenged and suggests further research for robustness.

Conclusion: Theoretical exploration of VAPO is essential to advance its application and address its limitations in complex reasoning tasks.

Abstract: The VAPO framework has demonstrated significant empirical success in
enhancing the efficiency and reliability of reinforcement learning for long
chain-of-thought (CoT) reasoning tasks with large language models (LLMs). By
systematically addressing challenges such as value model bias, heterogeneous
sequence lengths, and sparse reward signals, VAPO achieves state-of-the-art
performance. While its practical benefits are evident, a deeper theoretical
understanding of its underlying mechanisms and potential limitations is crucial
for guiding future advancements. This paper aims to initiate such a discussion
by exploring VAPO from a theoretical perspective, highlighting areas where its
assumptions might be challenged and where further investigation could yield
more robust and generalizable reasoning agents. We delve into the intricacies
of value function approximation in complex reasoning spaces, the optimality of
adaptive advantage estimation, the impact of token-level optimization, and the
enduring challenges of exploration and generalization.

</details>


### [799] [STRAP: Spatio-Temporal Pattern Retrieval for Out-of-Distribution Generalization](https://arxiv.org/pdf/2505.19547)
*Haoyu Zhang, Wentao Zhang, Hao Miao, Xinke Jiang, Yuchen Fang, Yifan Zhang*

Main category: cs.LG

TL;DR: STRAP enhances STGNN generalization in STOOD scenarios via retrieval-augmented learning and a pattern library, outperforming baselines without task-specific tuning.


<details>
  <summary>Details</summary>
Motivation: STGNNs struggle with generalization in STOOD scenarios where spatio-temporal dynamics evolve beyond training data.

Method: STRAP integrates retrieval-augmented learning, using a pattern library to store and retrieve spatio-temporal patterns, and employs a knowledge-balancing objective.

Result: STRAP outperforms state-of-the-art STGNNs on STOOD tasks, showing robustness and adaptability.

Conclusion: STRAP effectively addresses STOOD challenges, improving generalization and mitigating catastrophic forgetting in STGNNs.

Abstract: Spatio-Temporal Graph Neural Networks (STGNNs) have emerged as a powerful
tool for modeling dynamic graph-structured data across diverse domains.
However, they often fail to generalize in Spatio-Temporal Out-of-Distribution
(STOOD) scenarios, where both temporal dynamics and spatial structures evolve
beyond the training distribution. To address this problem, we propose an
innovative Spatio-Temporal Retrieval-Augmented Pattern Learning
framework,STRAP, which enhances model generalization by integrating
retrieval-augmented learning into the STGNN continue learning pipeline. The
core of STRAP is a compact and expressive pattern library that stores
representative spatio-temporal patterns enriched with historical, structural,
and semantic information, which is obtained and optimized during the training
phase. During inference, STRAP retrieves relevant patterns from this library
based on similarity to the current input and injects them into the model via a
plug-and-play prompting mechanism. This not only strengthens spatio-temporal
representations but also mitigates catastrophic forgetting. Moreover, STRAP
introduces a knowledge-balancing objective to harmonize new information with
retrieved knowledge. Extensive experiments across multiple real-world streaming
graph datasets show that STRAP consistently outperforms state-of-the-art STGNN
baselines on STOOD tasks, demonstrating its robustness, adaptability, and
strong generalization capability without task-specific fine-tuning.

</details>


### [800] [Geometry Aware Operator Transformer as an Efficient and Accurate Neural Surrogate for PDEs on Arbitrary Domains](https://arxiv.org/pdf/2505.18781)
*Shizheng Wen, Arsh Kumbhat, Levi Lingsch, Sepehr Mousavi, Yizhou Zhao, Praveen Chandrashekar, Siddhartha Mishra*

Main category: cs.LG

TL;DR: GAOT improves accuracy and efficiency for learning PDE solution operators on arbitrary domains using geometry-aware transformers.


<details>
  <summary>Details</summary>
Motivation: Existing operator learning algorithms for PDEs often trade accuracy for efficiency or vice versa, limiting their practical utility.

Method: GAOT combines multiscale attentional graph neural operators, geometry embeddings, and transformer processors to map domain and input data into PDE solutions.

Result: GAOT outperforms baselines in accuracy and efficiency, achieving state-of-the-art performance on a 3D industrial CFD dataset.

Conclusion: GAOT provides a robust, scalable, and efficient solution for learning PDE operators on arbitrary domains.

Abstract: The very challenging task of learning solution operators of PDEs on arbitrary
domains accurately and efficiently is of vital importance to engineering and
industrial simulations. Despite the existence of many operator learning
algorithms to approximate such PDEs, we find that accurate models are not
necessarily computationally efficient and vice versa. We address this issue by
proposing a geometry aware operator transformer (GAOT) for learning PDEs on
arbitrary domains. GAOT combines novel multiscale attentional graph neural
operator encoders and decoders, together with geometry embeddings and (vision)
transformer processors to accurately map information about the domain and the
inputs into a robust approximation of the PDE solution. Multiple innovations in
the implementation of GAOT also ensure computational efficiency and
scalability. We demonstrate this significant gain in both accuracy and
efficiency of GAOT over several baselines on a large number of learning tasks
from a diverse set of PDEs, including achieving state of the art performance on
a large scale three-dimensional industrial CFD dataset.

</details>


### [801] [Equivariant Representation Learning for Symmetry-Aware Inference with Guarantees](https://arxiv.org/pdf/2505.19809)
*Daniel Ordoñez-Apraez, Vladimir Kostić, Alek Fröhlich, Vivien Brandt, Karim Lounici, Massimiliano Pontil*

Main category: cs.LG

TL;DR: The paper introduces an equivariant representation learning framework for regression, conditional probability estimation, and uncertainty quantification, with non-asymptotic statistical guarantees. It leverages symmetry and group theory, outperforming baselines in real-world applications.


<details>
  <summary>Details</summary>
Motivation: Exploiting symmetries in physics or geometry can enhance generalization and sample efficiency in regression and uncertainty tasks, but statistical guarantees are lacking.

Method: The framework approximates the spectral decomposition of the conditional expectation operator, creating equivariant and disentangled representations using group representation theory.

Result: Empirical tests on synthetic and robotics datasets show the framework matches or outperforms equivariant baselines in regression and provides calibrated uncertainty estimates.

Conclusion: The approach successfully integrates symmetry principles with statistical guarantees, improving performance in regression and uncertainty quantification.

Abstract: In many real-world applications of regression, conditional probability
estimation, and uncertainty quantification, exploiting symmetries rooted in
physics or geometry can dramatically improve generalization and sample
efficiency. While geometric deep learning has made significant empirical
advances by incorporating group-theoretic structure, less attention has been
given to statistical learning guarantees. In this paper, we introduce an
equivariant representation learning framework that simultaneously addresses
regression, conditional probability estimation, and uncertainty quantification
while providing first-of-its-kind non-asymptotic statistical learning
guarantees. Grounded in operator and group representation theory, our framework
approximates the spectral decomposition of the conditional expectation
operator, building representations that are both equivariant and disentangled
along independent symmetry subgroups. Empirical evaluations on synthetic
datasets and real-world robotics applications confirm the potential of our
approach, matching or outperforming existing equivariant baselines in
regression while additionally providing well-calibrated parametric uncertainty
estimates.

</details>


### [802] [Model Agnostic Differentially Private Causal Inference](https://arxiv.org/pdf/2505.19589)
*Christian Lebeda, Mathieu Even, Aurélien Bellet, Julie Josse*

Main category: cs.LG

TL;DR: A model-agnostic framework for differentially private estimation of average treatment effects (ATE) is proposed, decoupling nuisance estimation from privacy protection for flexible, high-performance analysis under privacy constraints.


<details>
  <summary>Details</summary>
Motivation: Privacy concerns in fields like medicine and economics necessitate differentially private causal effect estimation without strong structural assumptions.

Method: The framework uses fold-splitting and ensemble techniques, perturbing only predictions and aggregation steps, and applies to G-formula, IPW, and AIPW estimators.

Result: Empirical results show competitive performance under realistic privacy budgets, with formal utility and privacy guarantees.

Conclusion: The framework bridges causal inference and privacy-preserving data analysis, supporting meta-analysis of private ATE estimates.

Abstract: Estimating causal effects from observational data is essential in fields such
as medicine, economics and social sciences, where privacy concerns are
paramount. We propose a general, model-agnostic framework for differentially
private estimation of average treatment effects (ATE) that avoids strong
structural assumptions on the data-generating process or the models used to
estimate propensity scores and conditional outcomes. In contrast to prior work,
which enforces differential privacy by directly privatizing these nuisance
components and results in a privacy cost that scales with model complexity, our
approach decouples nuisance estimation from privacy protection. This separation
allows the use of flexible, state-of-the-art black-box models, while
differential privacy is achieved by perturbing only predictions and aggregation
steps within a fold-splitting scheme with ensemble techniques. We instantiate
the framework for three classical estimators -- the G-formula, inverse
propensity weighting (IPW), and augmented IPW (AIPW) -- and provide formal
utility and privacy guarantees. Empirical results show that our methods
maintain competitive performance under realistic privacy budgets. We further
extend our framework to support meta-analysis of multiple private ATE
estimates. Our results bridge a critical gap between causal inference and
privacy-preserving data analysis.

</details>


### [803] [PCDCNet: A Surrogate Model for Air Quality Forecasting with Physical-Chemical Dynamics and Constraints](https://arxiv.org/pdf/2505.19842)
*Shuo Wang, Yun Cheng, Qingye Meng, Olga Saukh, Jiang Zhang, Jingfang Fan, Yuanting Zhang, Xingyuan Yuan, Lothar Thiele*

Main category: cs.LG

TL;DR: PCDCNet integrates numerical modeling with deep learning for air quality forecasting, achieving SOTA performance with reduced computational costs and real-time deployment.


<details>
  <summary>Details</summary>
Motivation: Traditional numerical models are computationally expensive, while deep learning lacks physical constraints. PCDCNet bridges this gap for accurate and efficient AQF.

Method: PCDCNet combines emissions, meteorology, and domain constraints with graph-based spatial transport, recurrent temporal modeling, and local interaction enhancements.

Result: Achieves SOTA in 72-hour PM2.5 and O3 forecasting, reduces computational costs, and is deployed for real-time use.

Conclusion: PCDCNet provides a practical, interpretable, and scalable solution for AQF, aiding public health and environmental decisions.

Abstract: Air quality forecasting (AQF) is critical for public health and environmental
management, yet remains challenging due to the complex interplay of emissions,
meteorology, and chemical transformations. Traditional numerical models, such
as CMAQ and WRF-Chem, provide physically grounded simulations but are
computationally expensive and rely on uncertain emission inventories. Deep
learning models, while computationally efficient, often struggle with
generalization due to their lack of physical constraints. To bridge this gap,
we propose PCDCNet, a surrogate model that integrates numerical modeling
principles with deep learning. PCDCNet explicitly incorporates emissions,
meteorological influences, and domain-informed constraints to model pollutant
formation, transport, and dissipation. By combining graph-based spatial
transport modeling, recurrent structures for temporal accumulation, and
representation enhancement for local interactions, PCDCNet achieves
state-of-the-art (SOTA) performance in 72-hour station-level PM2.5 and O3
forecasting while significantly reducing computational costs. Furthermore, our
model is deployed in an online platform, providing free, real-time air quality
forecasts, demonstrating its scalability and societal impact. By aligning deep
learning with physical consistency, PCDCNet offers a practical and
interpretable solution for AQF, enabling informed decision-making for both
personal and regulatory applications.

</details>


### [804] [Energy-based generator matching: A neural sampler for general state space](https://arxiv.org/pdf/2505.19646)
*Dongyeop Woo, Minsu Kim, Minkyu Kim, Kiyoung Seong, Sungsoo Ahn*

Main category: cs.LG

TL;DR: EGM trains generative models from energy functions without data, supporting various Markov processes and mixed modalities, using importance sampling and bootstrapping for efficiency.


<details>
  <summary>Details</summary>
Motivation: To enable generative model training without data by leveraging energy functions, accommodating diverse Markov processes and mixed data types.

Method: Extends generator matching with self-normalized importance sampling and bootstrapping to reduce variance in the importance weight.

Result: Validated on discrete and multimodal tasks up to 100 and 20 dimensions, respectively.

Conclusion: EGM is a versatile, data-free approach for training generative models across different modalities and processes.

Abstract: We propose Energy-based generator matching (EGM), a modality-agnostic
approach to train generative models from energy functions in the absence of
data. Extending the recently proposed generator matching, EGM enables training
of arbitrary continuous-time Markov processes, e.g., diffusion, flow, and jump,
and can generate data from continuous, discrete, and a mixture of two
modalities. To this end, we propose estimating the generator matching loss
using self-normalized importance sampling with an additional bootstrapping
trick to reduce variance in the importance weight. We validate EGM on both
discrete and multimodal tasks up to 100 and 20 dimensions, respectively.

</details>


### [805] [Generalized and Personalized Federated Learning with Foundation Models via Orthogonal Transformations](https://arxiv.org/pdf/2505.19888)
*Eun Gyung Kong, Je Won Yeom, Yonghoon Jeon, Taesup Kim*

Main category: cs.LG

TL;DR: FedOT introduces a federated learning method using global classifiers and local orthogonal transformations to balance generalization and personalization in heterogeneous settings.


<details>
  <summary>Details</summary>
Motivation: Addressing the challenge of achieving both generalization and personalization in federated learning without centralized data collection.

Method: FedOT leverages black-box foundation models, sharing a global classifier while locally adapting features via orthogonal transformations to mitigate gradient conflicts.

Result: Outperforms baseline FL methods, achieving robust performance in heterogeneous data settings.

Conclusion: Joint optimization of global classifiers and local orthogonal transformations is effective and broadly applicable.

Abstract: Federated Learning (FL) aims to train models across decentralized clients or
devices holding local data without the need for centralized data collection,
thus enhancing data privacy and security. However, achieving both
generalization and personalization in heterogeneous settings remains a
significant challenge. To address this, we introduce FedOT, a novel approach
that leverages black-box foundation models. FedOT shares only a global
task-dependent classifier across clients while locally adapting features
through orthogonal transformations. By enforcing orthogonality, FedOT mitigates
gradient conflicts across diverse clients, preserves semantic integrity, and
achieves robust performance even in the presence of substantial data
heterogeneity. The strategy of combining global and local parameters enables a
more balanced approach for both generalization and personalization,
outperforming baseline FL methods across multiple benchmarks. Furthermore, our
extensive analysis confirms that joint optimization of global classifiers and
local orthogonal transformations yields superior performance and suggests
broader applicability.

</details>


### [806] [Transformers in Protein: A Survey](https://arxiv.org/pdf/2505.20098)
*Xiaowen Ling, Zhiqiang Li, Yanbin Wang, Zhuhong You*

Main category: cs.LG

TL;DR: A review of over 100 studies on Transformer models in protein informatics, covering structure/function prediction, interactions, and drug discovery, with domain-oriented analysis and future directions.


<details>
  <summary>Details</summary>
Motivation: The rapid advancement of protein informatics and the untapped potential of Transformers in addressing its challenges necessitate a comprehensive review.

Method: Surveyed 100+ studies, categorized Transformer variants for protein science, and analyzed applications across key domains like structure prediction and drug discovery.

Result: Highlighted transformative contributions of Transformers, curated datasets/code resources, and identified persistent challenges.

Conclusion: Proposes future research directions to integrate Transformers and protein informatics, fostering innovation in the field.

Abstract: As protein informatics advances rapidly, the demand for enhanced predictive
accuracy, structural analysis, and functional understanding has intensified.
Transformer models, as powerful deep learning architectures, have demonstrated
unprecedented potential in addressing diverse challenges across protein
research. However, a comprehensive review of Transformer applications in this
field remains lacking. This paper bridges this gap by surveying over 100
studies, offering an in-depth analysis of practical implementations and
research progress of Transformers in protein-related tasks. Our review
systematically covers critical domains, including protein structure prediction,
function prediction, protein-protein interaction analysis, functional
annotation, and drug discovery/target identification. To contextualize these
advancements across various protein domains, we adopt a domain-oriented
classification system. We first introduce foundational concepts: the
Transformer architecture and attention mechanisms, categorize Transformer
variants tailored for protein science, and summarize essential protein
knowledge. For each research domain, we outline its objectives and background,
critically evaluate prior methods and their limitations, and highlight
transformative contributions enabled by Transformer models. We also curate and
summarize pivotal datasets and open-source code resources to facilitate
reproducibility and benchmarking. Finally, we discuss persistent challenges in
applying Transformers to protein informatics and propose future research
directions. This review aims to provide a consolidated foundation for the
synergistic integration of Transformer and protein informatics, fostering
further innovation and expanded applications in the field.

</details>


<div id='cs.MA'></div>

# cs.MA [[Back]](#toc)

### [807] [xChemAgents: Agentic AI for Explainable Quantum Chemistry](https://arxiv.org/pdf/2505.20574)
*Can Polat, Mehmet Tuncel, Hasan Kurban, Erchin Serpedin, Mustafa Kurban*

Main category: cs.MA

TL;DR: xChemAgents introduces a cooperative agent framework to improve multimodal property prediction in materials science by selecting relevant descriptors and enforcing physical constraints, achieving better accuracy and interpretability.


<details>
  <summary>Details</summary>
Motivation: Augmenting atomic geometries with textual descriptors can degrade performance on tasks sensitive to molecular shape or symmetry, and reduce interpretability. xChemAgents aims to address these issues.

Method: xChemAgents uses two language-model-based agents: a Selector to identify relevant descriptors and provide rationales, and a Validator to enforce physical constraints through iterative dialogue.

Result: The framework reduces mean absolute error by up to 22% over baselines and produces interpretable explanations.

Conclusion: Cooperative, self-verifying agents enhance accuracy and transparency in materials science, with potential for broader applications.

Abstract: Recent progress in multimodal graph neural networks has demonstrated that
augmenting atomic XYZ geometries with textual chemical descriptors can enhance
predictive accuracy across a range of electronic and thermodynamic properties.
However, naively appending large sets of heterogeneous descriptors often
degrades performance on tasks sensitive to molecular shape or symmetry, and
undermines interpretability. xChemAgents proposes a cooperative agent framework
that injects physics-aware reasoning into multimodal property prediction.
xChemAgents comprises two language-model-based agents: a Selector, which
adaptively identifies a sparse, weighted subset of descriptors relevant to each
target, and provides a natural language rationale; and a Validator, which
enforces physical constraints such as unit consistency and scaling laws through
iterative dialogue. On standard benchmark datasets, xChemAgents achieves up to
a 22\% reduction in mean absolute error over strong baselines, while producing
faithful, human-interpretable explanations. Experiment results highlight the
potential of cooperative, self-verifying agents to enhance both accuracy and
transparency in foundation-model-driven materials science. The implementation
and accompanying dataset are available anonymously at
https://github.com/KurbanIntelligenceLab/xChemAgents.

</details>


### [808] [MedSentry: Understanding and Mitigating Safety Risks in Medical LLM Multi-Agent Systems](https://arxiv.org/pdf/2505.20824)
*Kai Chen, Taihang Zhen, Hewei Wang, Kailai Liu, Xinfeng Li, Jing Huo, Tianpei Yang, Jinfeng Xu, Wei Dong, Yang Gao*

Main category: cs.MA

TL;DR: MedSentry introduces a benchmark and evaluation pipeline to test multi-agent LLM safety in healthcare, revealing vulnerabilities and proposing defenses.


<details>
  <summary>Details</summary>
Motivation: Ensuring safety of LLMs in healthcare, especially in multi-agent setups, is critical due to adversarial threats.

Method: Developed MedSentry benchmark (5,000 adversarial prompts) and an attack-defense pipeline to evaluate four multi-agent topologies.

Result: SharedPool is vulnerable; Decentralized is resilient. Proposed a detection-correction mechanism to restore safety.

Conclusion: MedSentry provides a framework and defenses for safer LLM-based multi-agent systems in healthcare.

Abstract: As large language models (LLMs) are increasingly deployed in healthcare,
ensuring their safety, particularly within collaborative multi-agent
configurations, is paramount. In this paper we introduce MedSentry, a benchmark
comprising 5 000 adversarial medical prompts spanning 25 threat categories with
100 subthemes. Coupled with this dataset, we develop an end-to-end
attack-defense evaluation pipeline to systematically analyze how four
representative multi-agent topologies (Layers, SharedPool, Centralized, and
Decentralized) withstand attacks from 'dark-personality' agents. Our findings
reveal critical differences in how these architectures handle information
contamination and maintain robust decision-making, exposing their underlying
vulnerability mechanisms. For instance, SharedPool's open information sharing
makes it highly susceptible, whereas Decentralized architectures exhibit
greater resilience thanks to inherent redundancy and isolation. To mitigate
these risks, we propose a personality-scale detection and correction mechanism
that identifies and rehabilitates malicious agents, restoring system safety to
near-baseline levels. MedSentry thus furnishes both a rigorous evaluation
framework and practical defense strategies that guide the design of safer
LLM-based multi-agent systems in medical domains.

</details>


### [809] [Revisiting Multi-Agent World Modeling from a Diffusion-Inspired Perspective](https://arxiv.org/pdf/2505.20922)
*Yang Zhang, Xinran Li, Jianing Ye, Delin Qu, Shuang Qiu, Chongjie Zhang, Xiu Li, Chenjia Bai*

Main category: cs.MA

TL;DR: DIMA introduces a diffusion-inspired world model for MARL, improving sample efficiency and performance by focusing on sequential agent modeling instead of joint action spaces.


<details>
  <summary>Details</summary>
Motivation: Accurately modeling MARL environments is challenging due to large joint action spaces and uncertain dynamics. The paper aims to simplify this by focusing on state space alone.

Method: The approach uses sequential agent modeling, inspired by diffusion models, to progressively resolve uncertainty and capture agent dependencies.

Result: DIMA outperforms prior world models in benchmarks like MAMuJoCo and Bi-DexHands, achieving state-of-the-art performance.

Conclusion: DIMA advances MARL research by introducing a new paradigm for multi-agent world models, combining diffusion models' strengths with MARL challenges.

Abstract: World models have recently attracted growing interest in Multi-Agent
Reinforcement Learning (MARL) due to their ability to improve sample efficiency
for policy learning. However, accurately modeling environments in MARL is
challenging due to the exponentially large joint action space and highly
uncertain dynamics inherent in multi-agent systems. To address this, we reduce
modeling complexity by shifting from jointly modeling the entire state-action
transition dynamics to focusing on the state space alone at each timestep
through sequential agent modeling. Specifically, our approach enables the model
to progressively resolve uncertainty while capturing the structured
dependencies among agents, providing a more accurate representation of how
agents influence the state. Interestingly, this sequential revelation of
agents' actions in a multi-agent system aligns with the reverse process in
diffusion models--a class of powerful generative models known for their
expressiveness and training stability compared to autoregressive or latent
variable models. Leveraging this insight, we develop a flexible and robust
world model for MARL using diffusion models. Our method, Diffusion-Inspired
Multi-Agent world model (DIMA), achieves state-of-the-art performance across
multiple multi-agent control benchmarks, significantly outperforming prior
world models in terms of final return and sample efficiency, including MAMuJoCo
and Bi-DexHands. DIMA establishes a new paradigm for constructing multi-agent
world models, advancing the frontier of MARL research.

</details>


### [810] [GGBond: Growing Graph-Based AI-Agent Society for Socially-Aware Recommender Simulation](https://arxiv.org/pdf/2505.21154)
*Hailin Zhong, Hanlin Wang, Yujun Ye, Meiyi Zhang, Shengxin Zhu*

Main category: cs.MA

TL;DR: A social simulation platform with cognitive agents and dynamic interactions is proposed to address the limitations of static data in personalized recommender systems, enabling realistic user behavior simulation and long-term evaluation.


<details>
  <summary>Details</summary>
Motivation: Current recommender systems rely on static offline data, limiting their ability to capture long-term user preference evolution and social influence dynamics.

Method: The platform uses Sim-User Agents with a five-layer cognitive architecture and an ICR2 motivational engine, alongside a dynamic social graph (GGBond Graph). Agents interact with recommender algorithms, updating states and connections.

Result: The system provides a controlled, observable environment for evaluating long-term recommender effects, overcoming static dataset limitations.

Conclusion: The proposed platform offers a realistic and dynamic alternative to traditional static datasets for studying and improving recommender systems.

Abstract: Current personalized recommender systems predominantly rely on static offline
data for algorithm design and evaluation, significantly limiting their ability
to capture long-term user preference evolution and social influence dynamics in
real-world scenarios. To address this fundamental challenge, we propose a
high-fidelity social simulation platform integrating human-like cognitive
agents and dynamic social interactions to realistically simulate user behavior
evolution under recommendation interventions. Specifically, the system
comprises a population of Sim-User Agents, each equipped with a five-layer
cognitive architecture that encapsulates key psychological mechanisms,
including episodic memory, affective state transitions, adaptive preference
learning, and dynamic trust-risk assessments. In particular, we innovatively
introduce the Intimacy--Curiosity--Reciprocity--Risk (ICR2) motivational engine
grounded in psychological and sociological theories, enabling more realistic
user decision-making processes. Furthermore, we construct a multilayer
heterogeneous social graph (GGBond Graph) supporting dynamic relational
evolution, effectively modeling users' evolving social ties and trust dynamics
based on interest similarity, personality alignment, and structural homophily.
During system operation, agents autonomously respond to recommendations
generated by typical recommender algorithms (e.g., Matrix Factorization,
MultVAE, LightGCN), deciding whether to consume, rate, and share content while
dynamically updating their internal states and social connections, thereby
forming a stable, multi-round feedback loop. This innovative design transcends
the limitations of traditional static datasets, providing a controlled,
observable environment for evaluating long-term recommender effects.

</details>


### [811] [Large Language Models Miss the Multi-Agent Mark](https://arxiv.org/pdf/2505.21298)
*Emanuele La Malfa, Gabriele La Malfa, Samuele Marro, Jie M. Zhang, Elizabeth Black, Micheal Luck, Philip Torr, Michael Wooldridge*

Main category: cs.MA

TL;DR: The paper critiques current Multi-Agent Systems of Large Language Models (MAS LLMs) for lacking foundational MAS principles, identifies discrepancies, and advocates for better integration of MAS theory.


<details>
  <summary>Details</summary>
Motivation: To address the gap between MAS theory and current MAS LLMs implementations, highlighting missed opportunities and mischaracterizations.

Method: Systematic analysis of discrepancies in four key areas: social agency, environment design, coordination protocols, and emergent behavior measurement.

Result: Current MAS LLMs often lack autonomy, social interaction, and structured environments, relying on oversimplified architectures.

Conclusion: The field should integrate established MAS concepts and adopt precise terminology to avoid stagnation and leverage existing solutions.

Abstract: Recent interest in Multi-Agent Systems of Large Language Models (MAS LLMs)
has led to an increase in frameworks leveraging multiple LLMs to tackle complex
tasks. However, much of this literature appropriates the terminology of MAS
without engaging with its foundational principles. In this position paper, we
highlight critical discrepancies between MAS theory and current MAS LLMs
implementations, focusing on four key areas: the social aspect of agency,
environment design, coordination and communication protocols, and measuring
emergent behaviours. Our position is that many MAS LLMs lack multi-agent
characteristics such as autonomy, social interaction, and structured
environments, and often rely on oversimplified, LLM-centric architectures. The
field may slow down and lose traction by revisiting problems the MAS literature
has already addressed. Therefore, we systematically analyse this issue and
outline associated research opportunities; we advocate for better integrating
established MAS concepts and more precise terminology to avoid
mischaracterisation and missed opportunities.

</details>


### [812] [Autonomous Multi-Modal LLM Agents for Treatment Planning in Focused Ultrasound Ablation Surgery](https://arxiv.org/pdf/2505.21418)
*Lina Zhao, Jiaxing Bai, Zihao Bian, Qingyue Chen, Yafang Li, Guangbo Li, Min He, Huaiyuan Yao, Zongjiu Zhang*

Main category: cs.MA

TL;DR: FUAS-Agents, an LLM-driven system, enhances Focused Ultrasound Ablation Surgery by automating multimodal tasks like image interpretation and dose planning, achieving high expert ratings for clinical compliance.


<details>
  <summary>Details</summary>
Motivation: The complexity of FUAS tasks like image interpretation and personalized planning requires intelligent assistance to improve efficiency and reliability.

Method: FUAS-Agents integrates patient profiles and MRI data, using LLMs to orchestrate specialized AI tools for segmentation, dose prediction, and guideline retrieval.

Result: In uterine fibroid treatment, 82.5-97.5% of plans were rated highly (4+/5) for completeness, accuracy, fluency, and clinical compliance.

Conclusion: LLM-driven agents can enhance clinical decision-making, combining general-purpose models with expert systems for healthcare challenges.

Abstract: Focused Ultrasound Ablation Surgery (FUAS) has emerged as a promising
non-invasive therapeutic modality, valued for its safety and precision.
Nevertheless, its clinical implementation entails intricate tasks such as
multimodal image interpretation, personalized dose planning, and real-time
intraoperative decision-making processes that demand intelligent assistance to
improve efficiency and reliability. We introduce FUAS-Agents, an autonomous
agent system that leverages the multimodal understanding and tool-using
capabilities of large language models (LLMs). By integrating patient profiles
and MRI data, FUAS-Agents orchestrates a suite of specialized medical AI tools,
including segmentation, treatment dose prediction, and clinical guideline
retrieval, to generate personalized treatment plans comprising MRI image, dose
parameters, and therapeutic strategies. We evaluate the system in a uterine
fibroid treatment scenario. Human assessment by four senior FUAS experts
indicates that 82.5%, 82.5%, 87.5%, and 97.5% of the generated plans were rated
4 or above (on a 5-point scale) in terms of completeness, accuracy, fluency,
and clinical compliance, respectively. These results demonstrate the potential
of LLM-driven agents in enhancing decision-making across complex clinical
workflows, and exemplify a translational paradigm that combines general-purpose
models with specialized expert systems to solve practical challenges in
vertical healthcare domains.

</details>


### [813] [Sequential Resource Trading Using Comparison-Based Gradient Estimation](https://arxiv.org/pdf/2408.11186)
*Surya Murthy, Mustafa O. Karabag, Ufuk Topcu*

Main category: cs.MA

TL;DR: An algorithm for sequential trading between autonomous agents estimates preferences to achieve Pareto-optimal resource allocation, improving societal benefit with fewer offers.


<details>
  <summary>Details</summary>
Motivation: To enable cooperation between autonomous agents or humans with unknown preferences by facilitating mutually beneficial trades.

Method: The offering agent estimates the responding agent's gradient (preferences) using rejected offers and greedy rationality, aiming for Pareto-optimality.

Result: The algorithm achieves Pareto-optimal states or certifies near-optimality, outperforming baselines in efficiency and societal benefit.

Conclusion: The proposed algorithm effectively improves resource allocation in sequential trading, validated by human studies.

Abstract: Autonomous agents interact with other autonomous agents and humans of unknown
preferences to share resources in their environment. We explore sequential
trading for resource allocation in a setting where two greedily rational agents
sequentially trade resources from a finite set of categories. Each agent has a
utility function that depends on the amount of resources it possesses in each
category. The offering agent makes trade offers to improve its utility without
knowing the responding agent's utility function, and the responding agent only
accepts offers that improve its utility. To facilitate cooperation between an
autonomous agent and another autonomous agent or a human, we present an
algorithm for the offering agent to estimate the responding agent's gradient
(preferences) and make offers based on previous acceptance or rejection
responses. The algorithm's goal is to reach a Pareto-optimal resource
allocation state while ensuring that the utilities of both agents improve after
every accepted trade. The algorithm estimates the responding agent's gradient
by leveraging the rejected offers and the greedy rationality assumption, to
prune the space of potential gradients. We show that, after the algorithm makes
a finite number of rejected offers, the algorithm either finds a mutually
beneficial trade or certifies that the current state is epsilon-weakly Pareto
optimal. We compare the proposed algorithm against various baselines in
continuous and discrete trading scenarios and show that it improves the
societal benefit with fewer offers. Additionally, we validate these findings in
a user study with human participants, where the algorithm achieves high
performance in scenarios with high resource conflict due to aligned agent
goals.

</details>


### [814] [Voting or Consensus? Decision-Making in Multi-Agent Debate](https://arxiv.org/pdf/2502.19130)
*Lars Benedikt Kaesberg, Jonas Becker, Jan Philip Wahle, Terry Ruas, Bela Gipp*

Main category: cs.MA

TL;DR: The paper evaluates the impact of decision protocols in multi-agent debates, showing voting and consensus protocols improve performance in reasoning and knowledge tasks, respectively. It also introduces two new methods (AAD and CI) for better decision-making.


<details>
  <summary>Details</summary>
Motivation: To systematically analyze how decision-making protocols influence multi-agent debates, as previous studies often altered multiple parameters, making it hard to isolate protocol effects.

Method: The study evaluates seven decision protocols (e.g., majority voting, unanimity consensus) by changing only the protocol variable. It measures performance in knowledge and reasoning tasks and introduces two new methods (AAD and CI) to enhance diversity.

Result: Voting protocols improved reasoning tasks by 13.2%, while consensus protocols improved knowledge tasks by 2.8%. More agents boosted performance, but more discussion rounds reduced it. AAD and CI further improved task performance by up to 3.3% and 7.4%, respectively.

Conclusion: Decision-making protocols significantly impact multi-agent debates, with voting and consensus excelling in different tasks. The proposed methods (AAD and CI) enhance performance by increasing answer diversity.

Abstract: Much of the success of multi-agent debates depends on carefully choosing the
right parameters. The decision-making protocol stands out as it can highly
impact final model answers, depending on how decisions are reached. Systematic
comparison of decision protocols is difficult because many studies alter
multiple discussion parameters beyond the protocol. So far, it has been largely
unknown how decision-making influences different tasks. This work
systematically evaluates the impact of seven decision protocols (e.g., majority
voting, unanimity consensus). We change only one variable at a time - the
decision protocol - to analyze how different methods affect the collaboration
between agents and measure differences in knowledge and reasoning tasks. Our
results show that voting protocols improve performance by 13.2% in reasoning
tasks and consensus protocols by 2.8% in knowledge tasks compared to other
decision protocols. Increasing the number of agents improves performance, while
more discussion rounds before voting reduce it. To improve decision-making by
increasing answer diversity, we propose two new methods, All-Agents Drafting
(AAD) and Collective Improvement (CI). Our methods improve task performance by
up to 3.3% with AAD and up to 7.4% with CI. This work demonstrates the
importance of decision-making in multi-agent debates beyond scaling.

</details>


### [815] [PeerGuard: Defending Multi-Agent Systems Against Backdoor Attacks Through Mutual Reasoning](https://arxiv.org/pdf/2505.11642)
*Falong Fan, Xi Li*

Main category: cs.MA

TL;DR: The paper explores backdoor vulnerabilities in multi-agent systems and introduces a defense mechanism using agent interactions to detect poisoned agents, validated on LLM-based systems like ChatGPT and Llama 3.


<details>
  <summary>Details</summary>
Motivation: Safety in multi-agent systems is underexplored, with most research focusing on single AI models. This work addresses the gap by investigating vulnerabilities and proposing defenses.

Method: The proposed defense leverages agent interactions, where each agent evaluates others' responses for illogical reasoning to identify poisoned agents.

Result: Experiments on LLM-based systems (e.g., ChatGPT, Llama 3) show high accuracy in detecting poisoned agents with minimal false positives.

Conclusion: The work advances multi-agent system safety and contributes to robust, trustworthy AI interactions.

Abstract: Multi-agent systems leverage advanced AI models as autonomous agents that
interact, cooperate, or compete to complete complex tasks across applications
such as robotics and traffic management. Despite their growing importance,
safety in multi-agent systems remains largely underexplored, with most research
focusing on single AI models rather than interacting agents. This work
investigates backdoor vulnerabilities in multi-agent systems and proposes a
defense mechanism based on agent interactions. By leveraging reasoning
abilities, each agent evaluates responses from others to detect illogical
reasoning processes, which indicate poisoned agents. Experiments on LLM-based
multi-agent systems, including ChatGPT series and Llama 3, demonstrate the
effectiveness of the proposed method, achieving high accuracy in identifying
poisoned agents while minimizing false positives on clean agents. We believe
this work provides insights into multi-agent system safety and contributes to
the development of robust, trustworthy AI interactions.

</details>


<div id='cs.MM'></div>

# cs.MM [[Back]](#toc)

### [816] [THE WASTIVE: An Interactive Ebb and Flow of Digital Fabrication Waste](https://arxiv.org/pdf/2505.21153)
*Yifan Shan, Bo Liu, Sebastian Bidegain, Thijs Roumen*

Main category: cs.MM

TL;DR: "THE WASTIVE" transforms digital fabrication waste into sentient, interactive art that responds to human presence, encouraging reflection on sustainability.


<details>
  <summary>Details</summary>
Motivation: To reimagine waste as sentient observers and provoke mindfulness about consumption and environmental impact.

Method: Interactive art installation where waste "observes" and responds to viewers, mimicking ocean waves.

Result: Creates a reflective, sensory experience that recontextualizes waste and human interaction.

Conclusion: The project calls for sustainable practices and deeper environmental awareness through artistic engagement.

Abstract: What if digital fabrication waste could observe the world? What would they
see? What would they say? "THE WASTIVE" reimagines digital fabrication waste as
sentient observers, giving them a poetic voice through interactive art. As
viewers approach, the installation awakens, mimicking the rhythmic ebb and flow
of ocean waves - a silent dialogue where discarded materials "observe" and
respond to human presence. These interactions echo the gentle murmurs of the
sea, transforming technological residue into a reflective, sensory experience.
Through this artistic contemplation, "THE WASTIVE" invites audiences to
reconsider their creative processes and consumption habits. It serves as a
poetic call for more mindful, sustainable practices, provoking deeper
reflections on our interconnectedness with the environment.

</details>


<div id='eess.AS'></div>

# eess.AS [[Back]](#toc)

### [817] [Towards Emotionally Consistent Text-Based Speech Editing: Introducing EmoCorrector and The ECD-TSE Dataset](https://arxiv.org/pdf/2505.20341)
*Rui Liu, Pu Gao, Jiatian Xi, Berrak Sisman, Carlos Busso, Haizhou Li*

Main category: eess.AS

TL;DR: EmoCorrector is a post-correction scheme for text-based speech editing (TSE) that addresses emotional inconsistency by using Retrieval-Augmented Generation (RAG) to align speech with desired emotions.


<details>
  <summary>Details</summary>
Motivation: Existing TSE methods overlook emotional shifts caused by text changes, leading to inconsistency.

Method: EmoCorrector extracts emotional features from edited text, retrieves matching speech samples, and synthesizes emotionally consistent speech while preserving speaker identity.

Result: Experiments on the Emotion Correction Dataset for TSE (ECD-TSE) show EmoCorrector improves emotional expression and consistency.

Conclusion: EmoCorrector effectively addresses emotional inconsistency in TSE, validated by subjective and objective evaluations.

Abstract: Text-based speech editing (TSE) modifies speech using only text, eliminating
re-recording. However, existing TSE methods, mainly focus on the content
accuracy and acoustic consistency of synthetic speech segments, and often
overlook the emotional shifts or inconsistency issues introduced by text
changes. To address this issue, we propose EmoCorrector, a novel
post-correction scheme for TSE. EmoCorrector leverages Retrieval-Augmented
Generation (RAG) by extracting the edited text's emotional features, retrieving
speech samples with matching emotions, and synthesizing speech that aligns with
the desired emotion while preserving the speaker's identity and quality. To
support the training and evaluation of emotional consistency modeling in TSE,
we pioneer the benchmarking Emotion Correction Dataset for TSE (ECD-TSE). The
prominent aspect of ECD-TSE is its inclusion of $<$text, speech$>$ paired data
featuring diverse text variations and a range of emotional expressions.
Subjective and objective experiments and comprehensive analysis on ECD-TSE
confirm that EmoCorrector significantly enhances the expression of intended
emotion while addressing emotion inconsistency limitations in current TSE
methods. Code and audio examples are available at
https://github.com/AI-S2-Lab/EmoCorrector.

</details>


### [818] [Robust fine-tuning of speech recognition models via model merging: application to disordered speech](https://arxiv.org/pdf/2505.20477)
*Alexandre Ducorroy, Rachid Riad*

Main category: eess.AS

TL;DR: Model merging improves ASR for dysarthric speech, outperforming fine-tuning with a 12% WER reduction.


<details>
  <summary>Details</summary>
Motivation: Address performance degradation of ASR on dysarthric speech due to variability and limited data.

Method: Compared fine-tuning with single-trajectory and multi-run merging of Whisper-based models.

Result: Multi-run merging reduced WER by 12% (16.2% on long-form audios) and scaled with more models.

Conclusion: Model merging is a replicable, cost-free method to enhance ASR for dysarthric speech.

Abstract: Automatic Speech Recognition (ASR) has advanced with Speech Foundation Models
(SFMs), yet performance degrades on dysarthric speech due to variability and
limited data. This study as part of the submission to the Speech Accessibility
challenge, explored model merging to improve ASR generalization using Whisper
as the base SFM. We compared fine-tuning with single-trajectory merging,
combining models from one fine-tuning path, and multi-run merging, merging
independently trained models. Our best multi-run merging approach achieved a
12% relative decrease of WER over classic fine-tuning, and a 16.2% relative
decrease on long-form audios, a major loss contributor in dysarthric ASR.
Merging more and more models led to continuous gains, remained effective in
low-data regimes, and generalized across model architectures. These results
highlight model merging as an easily replicable adaptation method that
consistently improves ASR without additional inference cost or hyperparameter
tuning.

</details>


### [819] [REWIND: Speech Time Reversal for Enhancing Speaker Representations in Diffusion-based Voice Conversion](https://arxiv.org/pdf/2505.20756)
*Ishan D. Biyani, Nirmesh J. Shah, Ashishkumar P. Gudmalwar, Pankaj Wasnik, Rajiv R. Shah*

Main category: eess.AS

TL;DR: Using time-reversed speech to enhance speaker representation in voice conversion, improving speaker similarity without compromising speech quality.


<details>
  <summary>Details</summary>
Motivation: Time-reversed speech retains tonal patterns for speaker identification, offering a way to disentangle speaker traits from linguistic content in voice conversion.

Method: Leverage speaker representations from time-reversed speech as augmentation in diffusion-based voice conversion models.

Result: Significant improvement in speaker similarity scores while maintaining high speech quality.

Conclusion: Time-reversed speech is effective for enhancing speaker representation in voice conversion.

Abstract: Speech time reversal refers to the process of reversing the entire speech
signal in time, causing it to play backward. Such signals are completely
unintelligible since the fundamental structures of phonemes and syllables are
destroyed. However, they still retain tonal patterns that enable perceptual
speaker identification despite losing linguistic content. In this paper, we
propose leveraging speaker representations learned from time reversed speech as
an augmentation strategy to enhance speaker representation. Notably, speaker
and language disentanglement in voice conversion (VC) is essential to
accurately preserve a speaker's unique vocal traits while minimizing
interference from linguistic content. The effectiveness of the proposed
approach is evaluated in the context of state-of-the-art diffusion-based VC
models. Experimental results indicate that the proposed approach significantly
improves speaker similarity-related scores while maintaining high speech
quality.

</details>


### [820] [In-context learning capabilities of Large Language Models to detect suicide risk among adolescents from speech transcripts](https://arxiv.org/pdf/2505.20491)
*Filomene Roquefort, Alexandre Ducorroy, Rachid Riad*

Main category: eess.AS

TL;DR: The paper presents a method for detecting suicide risk in Chinese adolescents using speech analysis, focusing on linguistic features due to anonymization constraints. It leverages LLMs and DSPy for prompt engineering, achieving competitive results.


<details>
  <summary>Details</summary>
Motivation: Early suicide risk detection in adolescents is crucial, but current methods face scalability issues. The study aims to improve this through speech analysis.

Method: The approach uses Large Language Models (LLMs) for transcript-based classification, employing DSPy for systematic prompt engineering and in-context learning.

Result: The system achieved 0.68 accuracy (F1=0.7) and ranked third/fourth among 180+ submissions. Ablation studies confirmed the impact of prompt examples on performance.

Conclusion: The findings highlight LLMs' potential in mental health applications and advance automated suicide risk assessment.

Abstract: Early suicide risk detection in adolescents is critical yet hindered by
scalability challenges of current assessments. This paper presents our approach
to the first SpeechWellness Challenge (SW1), which aims to assess suicide risk
in Chinese adolescents through speech analysis. Due to speech anonymization
constraints, we focused on linguistic features, leveraging Large Language
Models (LLMs) for transcript-based classification. Using DSPy for systematic
prompt engineering, we developed a robust in-context learning approach that
outperformed traditional fine-tuning on both linguistic and acoustic markers.
Our systems achieved third and fourth places among 180+ submissions, with 0.68
accuracy (F1=0.7) using only transcripts. Ablation analyses showed that
increasing prompt example improved performance (p=0.003), with varying effects
across model types and sizes. These findings advance automated suicide risk
assessment and demonstrate LLMs' value in mental health applications.

</details>


### [821] [ReverbFX: A Dataset of Room Impulse Responses Derived from Reverb Effect Plugins for Singing Voice Dereverberation](https://arxiv.org/pdf/2505.20533)
*Julius Richter, Till Svajda, Timo Gerkmann*

Main category: eess.AS

TL;DR: ReverbFX is a new RIR dataset for singing voice dereverberation, using plugin-derived RIRs instead of real recordings, showing better performance in artificial reverb scenarios.


<details>
  <summary>Details</summary>
Motivation: To address the lack of diverse RIR datasets for singing voice dereverberation, especially for artificial reverb scenarios.

Method: Created ReverbFX dataset with RIRs from reverb plugins, trained two generative models, and compared performance with models trained on real RIRs.

Result: Models trained with plugin-derived RIRs outperformed those trained on realistic RIRs in artificial reverb scenarios.

Conclusion: ReverbFX is effective for dereverberation research, especially in artificial reverb contexts.

Abstract: We present ReverbFX, a new room impulse response (RIR) dataset designed for
singing voice dereverberation research. Unlike existing datasets based on real
recorded RIRs, ReverbFX features a diverse collection of RIRs captured from
various reverb audio effect plugins commonly used in music production. We
conduct comprehensive experiments using the proposed dataset to benchmark the
challenge of dereverberation of singing voice recordings affected by artificial
reverbs. We train two state-of-the-art generative models using ReverbFX and
demonstrate that models trained with plugin-derived RIRs outperform those
trained on realistic RIRs in artificial reverb scenarios.

</details>


### [822] [Effect of laboratory conditions on the perception of virtual stages for music](https://arxiv.org/pdf/2505.20552)
*Ernesto Accolti*

Main category: eess.AS

TL;DR: The paper validates a methodology for assessing acoustical conditions in custom hearing booths, showing that sound absorption impacts the perception of virtual stages for music.


<details>
  <summary>Details</summary>
Motivation: To ensure perceptual validity and experimental rigor in augmented acoustics experiments, particularly in custom hearing booths.

Method: A preliminary study comparing the acoustical conditions of an anechoic room and two custom hearing booths (with sufficient and insufficient sound absorption) on virtual stage perception.

Result: The anechoic room and booth with sufficient absorption showed virtual sound perception within acceptable limits, while the booth with insufficient absorption did not.

Conclusion: The study validates the methodology for assessing acoustical conditions, with future work planned for broader analysis.

Abstract: This manuscript presents initial findings critical for supporting augmented
acoustics experiments in custom-made hearing booths, addressing a key challenge
in ensuring perceptual validity and experimental rigor in these highly
sensitive setups. This validation ensures our proposed methodology is sound,
guarantees the reliability of future results, and lays the foundational
groundwork for subsequent perceptual studies and the development of robust
guidelines for laboratory design in virtual acoustics research. A preliminary
study on the effect of the acoustical conditions of three different rooms on
the perception of virtual stages for music is presented: an anechoic room, a
custom-made hearing booth with insufficient sound absorption, and another
custom-made hearing booth with achievable sound absorption. The goal of this
study is to assess the impact of these different conditions on the perception
of virtual stages for music. The results show that the anechoic room and the
hearing booth with achievable sound absorption have a difference between the
total sound and the virtual sound below the just-noticeable difference, which
means that the virtual sound is not perceived louder than it should. In
contrast, the hearing booth with insufficient sound absorption has a difference
above the just-noticeable difference, which means that the virtual sound is
perceived louder than it should. This study provides a preliminary validation
of the proposed methodology for assessing the acoustical conditions of
custom-made hearing booths in stage acoustics experiments. Future work will
include a more comprehensive analysis of the results, including the effect of
different sound sources.

</details>


### [823] [Plug-and-Play Co-Occurring Face Attention for Robust Audio-Visual Speaker Extraction](https://arxiv.org/pdf/2505.20635)
*Zexu Pan, Shengkui Zhao, Tingting Wang, Kun Zhou, Yukun Ma, Chong Zhang, Bin Ma*

Main category: eess.AS

TL;DR: A plug-and-play inter-speaker attention module improves audio-visual speaker extraction by leveraging co-occurring faces in multi-person scenes, outperforming baselines on diverse datasets.


<details>
  <summary>Details</summary>
Motivation: Real-world scenarios often involve multiple co-occurring faces, which can provide valuable cues for speaker extraction but are underutilized in current methods.

Method: Introduces an inter-speaker attention module to process flexible numbers of co-occurring faces, integrated into AV-DPRNN and AV-TFGridNet models.

Result: Outperforms baselines on VoxCeleb2 and MISP datasets; cross-dataset evaluations on LRS2 and LRS3 confirm robustness.

Conclusion: The proposed module enhances speaker extraction accuracy in complex environments and demonstrates strong generalizability.

Abstract: Audio-visual speaker extraction isolates a target speaker's speech from a
mixture speech signal conditioned on a visual cue, typically using the target
speaker's face recording. However, in real-world scenarios, other co-occurring
faces are often present on-screen, providing valuable speaker activity cues in
the scene. In this work, we introduce a plug-and-play inter-speaker attention
module to process these flexible numbers of co-occurring faces, allowing for
more accurate speaker extraction in complex multi-person environments. We
integrate our module into two prominent models: the AV-DPRNN and the
state-of-the-art AV-TFGridNet. Extensive experiments on diverse datasets,
including the highly overlapped VoxCeleb2 and sparsely overlapped MISP,
demonstrate that our approach consistently outperforms baselines. Furthermore,
cross-dataset evaluations on LRS2 and LRS3 confirm the robustness and
generalizability of our method.

</details>


### [824] [PromptEVC: Controllable Emotional Voice Conversion with Natural Language Prompts](https://arxiv.org/pdf/2505.20678)
*Tianhua Qi, Shiyan Wang, Cheng Lu, Tengfei Song, Hao Yang, Zhanglin Wu, Wenming Zheng*

Main category: eess.AS

TL;DR: PromptEVC introduces natural language prompts for precise emotional voice conversion, outperforming existing methods in flexibility and performance.


<details>
  <summary>Details</summary>
Motivation: Existing EVC methods rely on rigid controls, ignoring individual differences in emotion perception and expression.

Method: Uses emotion descriptors, prompt mappers, and prosody modeling to generate fine-grained emotion embeddings and adjust rhythm.

Result: Outperforms state-of-the-art methods in emotion conversion, intensity control, mixed emotion synthesis, and prosody manipulation.

Conclusion: PromptEVC offers a flexible and precise approach to emotional voice conversion, addressing limitations of existing methods.

Abstract: Controllable emotional voice conversion (EVC) aims to manipulate emotional
expressions to increase the diversity of synthesized speech. Existing methods
typically rely on predefined labels, reference audios, or prespecified factor
values, often overlooking individual differences in emotion perception and
expression. In this paper, we introduce PromptEVC that utilizes natural
language prompts for precise and flexible emotion control. To bridge text
descriptions with emotional speech, we propose emotion descriptor and prompt
mapper to generate fine-grained emotion embeddings, trained jointly with
reference embeddings. To enhance naturalness, we present a prosody modeling and
control pipeline that adjusts the rhythm based on linguistic content and
emotional cues. Additionally, a speaker encoder is incorporated to preserve
identity. Experimental results demonstrate that PromptEVC outperforms
state-of-the-art controllable EVC methods in emotion conversion, intensity
control, mixed emotion synthesis, and prosody manipulation. Speech samples are
available at https://jeremychee4.github.io/PromptEVC/.

</details>


### [825] [Study of Lightweight Transformer Architectures for Single-Channel Speech Enhancement](https://arxiv.org/pdf/2505.21057)
*Haixin Zhao, Nilesh Madhu*

Main category: eess.AS

TL;DR: The paper introduces LCT-GAN, a lightweight transformer-based architecture for speech enhancement, achieving SotA performance with minimal computational overhead.


<details>
  <summary>Details</summary>
Motivation: Addressing the challenge of achieving high performance in speech enhancement while adhering to computational constraints on edge devices.

Method: Proposes a streamlined Frequency-Time-Frequency (FTF) stacked transformer architecture with adversarial training (LCT-GAN).

Result: LCT-GAN outperforms contemporary lightweight models with fewer parameters and operations, even surpassing more complex baselines.

Conclusion: The LCT-GAN is an efficient solution for edge-device speech enhancement, balancing performance and computational demands.

Abstract: In speech enhancement, achieving state-of-the-art (SotA) performance while
adhering to the computational constraints on edge devices remains a formidable
challenge. Networks integrating stacked temporal and spectral modelling
effectively leverage improved architectures such as transformers; however, they
inevitably incur substantial computational complexity and model expansion.
Through systematic ablation analysis on transformer-based temporal and spectral
modelling, we demonstrate that the architecture employing streamlined
Frequency-Time-Frequency (FTF) stacked transformers efficiently learns global
dependencies within causal context, while avoiding considerable computational
demands. Utilising discriminators in training further improves learning
efficacy and enhancement without introducing additional complexity during
inference. The proposed lightweight, causal, transformer-based architecture
with adversarial training (LCT-GAN) yields SoTA performance on instrumental
metrics among contemporary lightweight models, but with far less overhead.
Compared to DeepFilterNet2, the LCT-GAN only requires 6% of the parameters, at
similar complexity and performance. Against CCFNet+(Lite), LCT-GAN saves 9% in
parameters and 10% in multiply-accumulate operations yet yielding improved
performance. Further, the LCT-GAN even outperforms more complex, common
baseline models on widely used test datasets.

</details>


### [826] [Multimodal Assessment of Speech Impairment in ALS Using Audio-Visual and Machine Learning Approaches](https://arxiv.org/pdf/2505.21093)
*Francesco Pierotti, Andrea Bandini*

Main category: eess.AS

TL;DR: The paper explores using audio-visual analysis and machine learning to objectively assess speech impairment in ALS patients, outperforming subjective clinical methods.


<details>
  <summary>Details</summary>
Motivation: Current clinical methods for evaluating bulbar dysfunction in ALS are subjective or costly, necessitating a more objective and accessible tool.

Method: Combined acoustic and kinematic features from audio-video recordings were used to train regression models, with extreme boosting machine regressor performing best.

Result: The best model achieved a root mean squared error of 0.93 on a 5-25 scale, showing improved accuracy in speech impairment assessment.

Conclusion: Audio-visual analysis with machine learning offers a promising, objective tool for early detection and monitoring of bulbar dysfunction, even in home settings.

Abstract: The analysis of speech in individuals with amyotrophic lateral sclerosis is a
powerful tool to support clinicians in the assessment of bulbar dysfunction.
However, current methods used in clinical practice consist of subjective
evaluations or expensive instrumentation. This study investigates different
approaches combining audio-visual analysis and machine learning to predict the
speech impairment evaluation performed by clinicians. Using a small dataset of
acoustic and kinematic features extracted from audio and video recordings of
speech tasks, we trained and tested some regression models. The best
performance was achieved using the extreme boosting machine regressor with
multimodal features, which resulted in a root mean squared error of 0.93 on a
scale ranging from 5 to 25. Results suggest that integrating audio-video
analysis enhances speech impairment assessment, providing an objective tool for
early detection and monitoring of bulbar dysfunction, also in home settings.

</details>


### [827] [PSRB: A Comprehensive Benchmark for Evaluating Persian ASR Systems](https://arxiv.org/pdf/2505.21230)
*Nima Sedghiyeh, Sara Sadeghi, Reza Khodadadi, Farzin Kashani, Omid Aghdaei, Somayeh Rahimi, Mohammad Sadegh Safari*

Main category: eess.AS

TL;DR: The paper introduces PSRB, a benchmark for evaluating ASR systems in Persian, highlighting performance gaps and biases, and proposes a novel error-weighting metric.


<details>
  <summary>Details</summary>
Motivation: Addressing the lack of comprehensive evaluation benchmarks for ASR systems in low-resource languages like Persian.

Method: Evaluated ten ASR systems, analyzed transcription errors, and introduced a new metric for substitution errors.

Result: ASR models perform well on standard Persian but struggle with accents, children's speech, and linguistic challenges.

Conclusion: Fine-tuning and diverse datasets are needed to improve ASR performance; PSRB serves as a framework for other low-resource languages.

Abstract: Although Automatic Speech Recognition (ASR) systems have become an integral
part of modern technology, their evaluation remains challenging, particularly
for low-resource languages such as Persian. This paper introduces Persian
Speech Recognition Benchmark(PSRB), a comprehensive benchmark designed to
address this gap by incorporating diverse linguistic and acoustic conditions.
We evaluate ten ASR systems, including state-of-the-art commercial and
open-source models, to examine performance variations and inherent biases.
Additionally, we conduct an in-depth analysis of Persian ASR transcriptions,
identifying key error types and proposing a novel metric that weights
substitution errors. This metric enhances evaluation robustness by reducing the
impact of minor and partial errors, thereby improving the precision of
performance assessment. Our findings indicate that while ASR models generally
perform well on standard Persian, they struggle with regional accents,
children's speech, and specific linguistic challenges. These results highlight
the necessity of fine-tuning and incorporating diverse, representative training
datasets to mitigate biases and enhance overall ASR performance. PSRB provides
a valuable resource for advancing ASR research in Persian and serves as a
framework for developing benchmarks in other low-resource languages. A subset
of the PSRB dataset is publicly available at
https://huggingface.co/datasets/PartAI/PSRB.

</details>


### [828] [Advanced Signal Analysis in Detecting Replay Attacks for Automatic Speaker Verification Systems](https://arxiv.org/pdf/2403.01130)
*Lee Shih Kuang*

Main category: eess.AS

TL;DR: Novel signal analysis methods (AA, MA, CQA) for replay speech detection in ASV systems, outperforming conventional methods in speed and efficacy.


<details>
  <summary>Details</summary>
Motivation: To improve replay speech detection in ASV systems by introducing alternative sinusoidal sequence groups for signal analysis.

Method: Proposes AA, MA, and CQA methods inspired by Fourier inversion, tested on ASVspoof 2019 & 2021 PA databases.

Result: CQA and MA methods outperform conventional methods in efficiency (2.36x faster) and efficacy, especially when combined with TAC speech feature.

Conclusion: The proposed methods are promising for replay speech detection and broader applications in music and speech processing.

Abstract: This study proposes novel signal analysis methods for replay speech detection
in automatic speaker verification (ASV) systems. The proposed methods --
arbitrary analysis (AA), mel scale analysis (MA), and constant Q analysis (CQA)
-- are inspired by the calculation of the Fourier inversion formula. These
methods introduce new perspectives in signal analysis for replay speech
detection by employing alternative sinusoidal sequence groups. The efficacy of
the proposed methods is examined on the ASVspoof 2019 \& 2021 PA databases with
experiments, and confirmed by the performance of systems that incorporated the
proposed methods; the successful integration of the proposed methods and a
speech feature that calculates temporal autocorrelation of speech (TAC) from
complex spectra strongly confirms it. Moreover, the proposed CQA and MA methods
show their superiority to the conventional methods on efficiency (approximately
2.36 times as fast compared to the conventional constant Q transform (CQT)
method) and efficacy, respectively, in analyzing speech signals, making them
promising to utilize in music and speech processing works.

</details>


### [829] [GigaSpeech 2: An Evolving, Large-Scale and Multi-domain ASR Corpus for Low-Resource Languages with Automated Crawling, Transcription and Refinement](https://arxiv.org/pdf/2406.11546)
*Yifan Yang, Zheshu Song, Jianheng Zhuo, Mingyu Cui, Jinpeng Li, Bo Yang, Yexing Du, Ziyang Ma, Xunying Liu, Ziyuan Wang, Ke Li, Shuai Fan, Kai Yu, Wei-Qiang Zhang, Guoguo Chen, Xie Chen*

Main category: eess.AS

TL;DR: GigaSpeech 2 is a large-scale, multilingual speech corpus for low-resource languages, using unlabeled YouTube data and an automated pipeline for transcription and refinement, achieving significant ASR performance improvements.


<details>
  <summary>Details</summary>
Motivation: Addressing the scarcity of labeled training data for low-resource languages in speech recognition.

Method: Automated pipeline for data crawling, transcription (Whisper, MMS), label refinement (Noisy Student Training), and multi-dimensional filtering.

Result: Reduces word error rate by 25-40% for Thai, Indonesian, and Vietnamese compared to Whisper, with fewer parameters.

Conclusion: GigaSpeech 2 and its pipeline advance low-resource speech recognition, offering high-quality, scalable solutions.

Abstract: The evolution of speech technology has been spurred by the rapid increase in
dataset sizes. Traditional speech models generally depend on a large amount of
labeled training data, which is scarce for low-resource languages. This paper
presents GigaSpeech 2, a large-scale, multi-domain, multilingual speech
recognition corpus. It is designed for low-resource languages and does not rely
on paired speech and text data. GigaSpeech 2 comprises about 30,000 hours of
automatically transcribed speech, including Thai, Indonesian, and Vietnamese,
gathered from unlabeled YouTube videos. We also introduce an automated pipeline
for data crawling, transcription, and label refinement. Specifically, this
pipeline involves Whisper for initial transcription, MMS for forced alignment,
and multi-dimensional filtering for data quality assurance. A modified Noisy
Student Training is developed to further refine flawed pseudo labels
iteratively, thereby enhancing model performance. Experimental results on our
manually transcribed evaluation set and two public test sets from Common Voice
and FLEURS confirm our corpus's high quality and broad applicability. Notably,
ASR models trained on GigaSpeech 2 can reduce the word error rate for Thai,
Indonesian, and Vietnamese on our challenging and realistic YouTube test set by
25% to 40% compared to Whisper large-v3, with merely 10% model parameters.
Furthermore, our ASR models trained on GigaSpeech 2 yield superior performance
compared to commercial services. We hope that our newly introduced corpus and
pipeline will open a new avenue for low-resource speech recognition and
significantly facilitate research in this area.

</details>


### [830] [Resampling Filter Design for Multirate Neural Audio Effect Processing](https://arxiv.org/pdf/2501.18470)
*Alistair Carson, Vesa Välimäki, Alec Wright, Stefan Bilbao*

Main category: eess.AS

TL;DR: The paper explores real-time signal resampling for neural network-based audio effects to achieve sample rate independence, proposing a two-stage filter design for efficiency and low latency.


<details>
  <summary>Details</summary>
Motivation: Neural networks for audio effects lack sample rate adjustability, limiting their flexibility. Existing methods for sample rate independence have shortcomings, especially for fractional rate changes.

Method: The study investigates real-time resampling at the network's input/output, testing various filter designs, including a two-stage IIR and FIR cascade.

Result: The proposed two-stage filter design matches or outperforms prior methods with fewer operations and sub-millisecond latency. It also reduces aliasing in distortion models.

Conclusion: Real-time resampling with optimized filters offers a practical solution for sample rate independence in neural network audio effects, improving performance and flexibility.

Abstract: Neural networks have become ubiquitous in audio effects modelling, especially
for guitar amplifiers and distortion pedals. One limitation of such models is
that the sample rate of the training data is implicitly encoded in the model
weights and therefore not readily adjustable at inference. Recent work explored
modifications to recurrent neural network architecture to approximate a sample
rate independent system, enabling audio processing at a rate that differs from
the original training rate. This method works well for integer oversampling and
can reduce aliasing caused by nonlinear activation functions. For small
fractional changes in sample rate, fractional delay filters can be used to
approximate sample rate independence, but in some cases this method fails
entirely. Here, we explore the use of real-time signal resampling at the input
and output of the neural network as an alternative solution. We investigate
several resampling filter designs and show that a two-stage design consisting
of a half-band IIR filter cascaded with a Kaiser window FIR filter can give
similar or better results to the previously proposed model adjustment method
with many fewer filtering operations per sample and less than one millisecond
of latency at typical audio rates. Furthermore, we investigate interpolation
and decimation filters for the task of integer oversampling and show that
cascaded half-band IIR and FIR designs can be used in conjunction with the
model adjustment method to reduce aliasing in a range of distortion effect
models.

</details>


### [831] [U-SAM: An audio language Model for Unified Speech, Audio, and Music Understanding](https://arxiv.org/pdf/2505.13880)
*Ziqian Wang, Xianjun Xia, Xinfa Zhu, Lei Xie*

Main category: eess.AS

TL;DR: U-SAM is an advanced audio language model integrating specialized encoders for speech, audio, and music with a pre-trained LLM, using MoE for task-aware fusion and contrastive loss for better alignment. It outperforms existing models.


<details>
  <summary>Details</summary>
Motivation: Existing models struggle with comprehensive understanding across diverse audio types and rely on cross-entropy loss, which fails to handle redundant features and weak alignment.

Method: U-SAM combines domain-specific encoders with an LLM, uses MoE for dynamic feature fusion, and introduces a Semantic-Aware Contrastive Loss Module.

Result: U-SAM outperforms specialized and existing models across benchmarks and shows generalization on unseen tasks.

Conclusion: U-SAM advances audio-language alignment and generalization, with code publicly available.

Abstract: The text generation paradigm for audio tasks has opened new possibilities for
unified audio understanding. However, existing models face significant
challenges in achieving a comprehensive understanding across diverse audio
types, such as speech, general audio events, and music. Furthermore, their
exclusive reliance on cross-entropy loss for alignment often falls short, as it
treats all tokens equally and fails to account for redundant audio features,
leading to weaker cross-modal alignment. To deal with the above challenges,
this paper introduces U-SAM, an advanced audio language model that integrates
specialized encoders for speech, audio, and music with a pre-trained large
language model (LLM). U-SAM employs a Mixture of Experts (MoE) projector for
task-aware feature fusion, dynamically routing and integrating the
domain-specific encoder outputs. Additionally, U-SAM incorporates a
Semantic-Aware Contrastive Loss Module, which explicitly identifies redundant
audio features under language supervision and rectifies their semantic and
spectral representations to enhance cross-modal alignment. Extensive
experiments demonstrate that U-SAM consistently outperforms both specialized
models and existing audio language models across multiple benchmarks. Moreover,
it exhibits emergent capabilities on unseen tasks, showcasing its
generalization potential. Code is available
(https://github.com/Honee-W/U-SAM/).

</details>


### [832] [TCSinger 2: Customizable Multilingual Zero-shot Singing Voice Synthesis](https://arxiv.org/pdf/2505.14910)
*Yu Zhang, Wenxiang Guo, Changhao Pan, Dongyu Yao, Zhiyuan Zhu, Ziyue Jiang, Yuhan Wang, Tao Jin, Zhou Zhao*

Main category: eess.AS

TL;DR: TCSinger 2 is a multilingual zero-shot singing voice synthesis model with style transfer and control, addressing limitations of existing models by introducing novel modules for smoother transitions and better style modeling.


<details>
  <summary>Details</summary>
Motivation: Existing SVS models rely heavily on phoneme and note boundary annotations, leading to poor robustness in zero-shot scenarios and ineffective multi-level style control.

Method: TCSinger 2 includes three modules: Blurred Boundary Content Encoder for smooth transitions, Custom Audio Encoder for aligned representations, and Flow-based Custom Transformer for enhanced synthesis and style modeling.

Result: TCSinger 2 outperforms baseline models in subjective and objective metrics across multiple tasks.

Conclusion: The proposed model improves multilingual zero-shot SVS with better transitions, style control, and synthesis quality.

Abstract: Customizable multilingual zero-shot singing voice synthesis (SVS) has various
potential applications in music composition and short video dubbing. However,
existing SVS models overly depend on phoneme and note boundary annotations,
limiting their robustness in zero-shot scenarios and producing poor transitions
between phonemes and notes. Moreover, they also lack effective multi-level
style control via diverse prompts. To overcome these challenges, we introduce
TCSinger 2, a multi-task multilingual zero-shot SVS model with style transfer
and style control based on various prompts. TCSinger 2 mainly includes three
key modules: 1) Blurred Boundary Content (BBC) Encoder, predicts duration,
extends content embedding, and applies masking to the boundaries to enable
smooth transitions. 2) Custom Audio Encoder, uses contrastive learning to
extract aligned representations from singing, speech, and textual prompts. 3)
Flow-based Custom Transformer, leverages Cus-MOE, with F0 supervision,
enhancing both the synthesis quality and style modeling of the generated
singing voice. Experimental results show that TCSinger 2 outperforms baseline
models in both subjective and objective metrics across multiple related tasks.
Singing voice samples are available at
https://aaronz345.github.io/TCSinger2Demo/.

</details>


### [833] [Leveraging Cascaded Binary Classification and Multimodal Fusion for Dementia Detection through Spontaneous Speech](https://arxiv.org/pdf/2505.19446)
*Yin-Long Liu, Yuanchao Li, Rui Feng, Liu He, Jia-Xin Chen, Yi-Ming Wang, Yu-Ang Chen, Yan-Han Peng, Jia-Hong Yuan, Zhen-Hua Ling*

Main category: eess.AS

TL;DR: A cascaded binary classification framework and enhanced multimodal fusion system for early dementia detection, outperforming baselines in classification and regression tasks.


<details>
  <summary>Details</summary>
Motivation: To improve early dementia detection by analyzing spontaneous speech, addressing class imbalance and leveraging diverse features.

Method: Cascaded binary classification with pause encoding for disfluencies, and multimodal fusion with ensemble learning for regression.

Result: Outperformed baselines in both classification (Healthy Control, Mild Cognitive Impairment, Dementia) and MMSE score regression.

Conclusion: The proposed approach is robust and effective for early dementia detection through speech analysis.

Abstract: This paper presents our submission to the PROCESS Challenge 2025, focusing on
spontaneous speech analysis for early dementia detection. For the three-class
classification task (Healthy Control, Mild Cognitive Impairment, and Dementia),
we propose a cascaded binary classification framework that fine-tunes
pre-trained language models and incorporates pause encoding to better capture
disfluencies. This design streamlines multi-class classification and addresses
class imbalance by restructuring the decision process. For the Mini-Mental
State Examination score regression task, we develop an enhanced multimodal
fusion system that combines diverse acoustic and linguistic features. Separate
regression models are trained on individual feature sets, with ensemble
learning applied through score averaging. Experimental results on the test set
outperform the baselines provided by the organizers in both tasks,
demonstrating the robustness and effectiveness of our approach.

</details>


### [834] [FlowSE: Efficient and High-Quality Speech Enhancement via Flow Matching](https://arxiv.org/pdf/2505.19476)
*Ziqian Wang, Zikai Liu, Xinfa Zhu, Yike Zhu, Mingshuai Liu, Jun Chen, Longshuai Xiao, Chao Weng, Lei Xie*

Main category: eess.AS

TL;DR: FlowSE is a flow-matching-based model for speech enhancement (SE) that addresses challenges like quantization loss and high latency in existing methods, offering high-quality reconstruction with low inference latency.


<details>
  <summary>Details</summary>
Motivation: Existing generative approaches for SE face issues like quantization loss (language models) and high complexity (diffusion models), motivating the need for a simpler, efficient solution.

Method: FlowSE uses flow matching to transform noisy to clean speech distributions in one pass, training on noisy mel spectrograms and optional text, optimizing a conditional flow matching loss.

Result: FlowSE outperforms state-of-the-art generative methods, works with or without text, and improves further with transcripts.

Conclusion: FlowSE sets a new paradigm for generative SE, showcasing flow matching's potential in advancing the field.

Abstract: Generative models have excelled in audio tasks using approaches such as
language models, diffusion, and flow matching. However, existing generative
approaches for speech enhancement (SE) face notable challenges: language
model-based methods suffer from quantization loss, leading to compromised
speaker similarity and intelligibility, while diffusion models require complex
training and high inference latency. To address these challenges, we propose
FlowSE, a flow-matching-based model for SE. Flow matching learns a continuous
transformation between noisy and clean speech distributions in a single pass,
significantly reducing inference latency while maintaining high-quality
reconstruction. Specifically, FlowSE trains on noisy mel spectrograms and
optional character sequences, optimizing a conditional flow matching loss with
ground-truth mel spectrograms as supervision. It implicitly learns speech's
temporal-spectral structure and text-speech alignment. During inference, FlowSE
can operate with or without textual information, achieving impressive results
in both scenarios, with further improvements when transcripts are available.
Extensive experiments demonstrate that FlowSE significantly outperforms
state-of-the-art generative methods, establishing a new paradigm for
generative-based SE and demonstrating the potential of flow matching to advance
the field. Our code, pre-trained checkpoints, and audio samples are available.

</details>


<div id='eess.IV'></div>

# eess.IV [[Back]](#toc)

### [835] [An Artificial Intelligence Model for Early Stage Breast Cancer Detection from Biopsy Images](https://arxiv.org/pdf/2505.20332)
*Neil Chaudhary, Zaynah Dhunny*

Main category: eess.IV

TL;DR: An AI tool using CNN improves breast cancer type identification from biopsy images, reducing invasive tests and treatment delays.


<details>
  <summary>Details</summary>
Motivation: Traditional methods for identifying breast cancer types are invasive and delay treatment, increasing patient burden.

Method: The model uses a CNN to classify benign vs. malignant tissues and subclassify cancer types, with preprocessing for noise reduction and feature enhancement.

Result: The model outperforms existing solutions in accuracy, precision, recall, and F1-score.

Conclusion: Deep learning shows promise for clinical diagnostics, offering a tool to aid pathologists in breast cancer classification.

Abstract: Accurate identification of breast cancer types plays a critical role in
guiding treatment decisions and improving patient outcomes. This paper presents
an artificial intelligence enabled tool designed to aid in the identification
of breast cancer types using histopathological biopsy images. Traditionally
additional tests have to be done on women who are detected with breast cancer
to find out the types of cancer it is to give the necessary cure. Those tests
are not only invasive but also delay the initiation of treatment and increase
patient burden. The proposed model utilizes a convolutional neural network
(CNN) architecture to distinguish between benign and malignant tissues as well
as accurate subclassification of breast cancer types. By preprocessing the
images to reduce noise and enhance features, the model achieves reliable levels
of classification performance. Experimental results on such datasets
demonstrate the model's effectiveness, outperforming several existing solutions
in terms of accuracy, precision, recall, and F1-score. The study emphasizes the
potential of deep learning techniques in clinical diagnostics and offers a
promising tool to assist pathologists in breast cancer classification.

</details>


### [836] [DiffNMR: Advancing Inpainting of Randomly Sampled Nuclear Magnetic Resonance Signals](https://arxiv.org/pdf/2505.20367)
*Sen Yan, Fabrizio Gabellieri, Etienne Goffinet, Filippo Castiglione, Thomas Launey*

Main category: eess.IV

TL;DR: Deep learning, specifically diffusion models, enhances Non-Uniform Sampling (NUS) NMR spectra reconstruction, improving efficiency and accuracy.


<details>
  <summary>Details</summary>
Motivation: High costs and lengthy NMR experiments necessitate computational optimization, but NUS introduces artifacts. Deep learning offers a solution.

Method: Diffusion models are applied to time-time and time-frequency NUS data for spectral reconstruction.

Result: Satisfactory reconstructions of challenging spectra from the Artina dataset, with time-frequency domain data outperforming time-time.

Conclusion: Diffusion models show promise for NMR spectroscopy, with time-frequency data offering superior results, paving the way for future research.

Abstract: Nuclear Magnetic Resonance (NMR) spectroscopy leverages nuclear magnetization
to probe molecules' chemical environment, structure, and dynamics, with
applications spanning from pharmaceuticals to the petroleum industry. Despite
its utility, the high cost of NMR instrumentation, operation and the lengthy
duration of experiments necessitate the development of computational techniques
to optimize acquisition times. Non-Uniform sampling (NUS) is widely employed as
a sub-sampling method to address these challenges, but it often introduces
artifacts and degrades spectral quality, offsetting the benefits of reduced
acquisition times. In this work, we propose the use of deep learning techniques
to enhance the reconstruction quality of NUS spectra. Specifically, we explore
the application of diffusion models, a relatively untapped approach in this
domain. Our methodology involves applying diffusion models to both time-time
and time-frequency NUS data, yielding satisfactory reconstructions of
challenging spectra from the benchmark Artina dataset. This approach
demonstrates the potential of diffusion models to improve the efficiency and
accuracy of NMR spectroscopy as well as the superiority of using a
time-frequency domain data over the time-time one, opening new landscapes for
future studies.

</details>


### [837] [A Feasibility Study of Task-Based fMRI at 0.55 T](https://arxiv.org/pdf/2505.20568)
*Parsa Razmara, Takfarinas Medani, Anand A. Joshi, Majid Abbasi Sisara, Ye Tian, Sophia X. Cui, Justin P. Haldar, Krishna S. Nayak, Richard M. Leahy*

Main category: eess.IV

TL;DR: Reliable task-based fMRI at 0.55T is demonstrated using a custom protocol and analysis pipeline, achieving results comparable to higher-field MRI.


<details>
  <summary>Details</summary>
Motivation: To prove that 0.55T MRI can perform reliable task-based fMRI, broadening its use in settings where high-field MRI is unavailable.

Method: Combined EPI acquisition with custom analysis techniques, tested with finger-tapping and visual tasks using 5- and 10-minute runs.

Result: Significant activations were detected, showing high-quality task-based fMRI is achievable at 0.55T.

Conclusion: Task-based fMRI at 0.55T is feasible, expanding functional neuroimaging access in clinical and research settings.

Abstract: 0.55T MRI offers advantages compared to conventional field strengths,
including reduced susceptibility artifacts and better compatibility with
simultaneous EEG recordings. However, reliable task-based fMRI at 0.55T has not
been significantly demonstrated. In this study, we establish a robust
task-based fMRI protocol and analysis pipeline at 0.55T that achieves full
brain coverage and results comparable to what is expected for activation extent
and location. We attempted fMRI at 0.55T by combining EPI acquisition with
custom analysis techniques. Finger-tapping and visual tasks were used,
comparing 5- and 10-minute runs to enhance activation detection. The results
show significant activations, demonstrating that high-quality task-based fMRI
is achievable at 0.55T in single subjects. This study demonstrates that
reliable task-based fMRI is feasible on 0.55T scanners, potentially broadening
functional neuroimaging access in clinical and research settings where
high-field MRI is unavailable or impractical, supporting broader diagnostic and
research applications.

</details>


### [838] [Unpaired Image-to-Image Translation for Segmentation and Signal Unmixing](https://arxiv.org/pdf/2505.20746)
*Nikola Andrejic, Milica Spasic, Igor Mihajlovic, Petra Milosavljevic, Djordje Pavlovic, Filip Milisavljevic, Uros Milivojevic, Danilo Delibasic, Ivana Mikic, Sinisa Todorovic*

Main category: eess.IV

TL;DR: Ui2i is a novel unpaired image-to-image translation model improving content preservation and style transfer by modifying CycleGAN with U-Net generators, spectral normalization, and attention mechanisms.


<details>
  <summary>Details</summary>
Motivation: To enable style transfer across domains while preserving content integrity, especially in biomedical tasks requiring accurate structural preservation.

Method: Ui2i modifies CycleGAN with U-Net generators, removes feature-based normalization, uses spectral normalization, and integrates attention mechanisms. Training includes image scale augmentation.

Result: Ui2i successfully preserves content fidelity in biomedical tasks like nuclear segmentation in IHC images and unmixing superimposed signals in IF images.

Conclusion: Ui2i is the first model to separate superimposed signals in IF images using unpaired data, demonstrating superior content preservation in demanding tasks.

Abstract: This work introduces Ui2i, a novel model for unpaired image-to-image
translation, trained on content-wise unpaired datasets to enable style transfer
across domains while preserving content. Building on CycleGAN, Ui2i
incorporates key modifications to better disentangle content and style
features, and preserve content integrity. Specifically, Ui2i employs
U-Net-based generators with skip connections to propagate localized shallow
features deep into the generator. Ui2i removes feature-based normalization
layers from all modules and replaces them with approximate bidirectional
spectral normalization -- a parameter-based alternative that enhances training
stability. To further support content preservation, channel and spatial
attention mechanisms are integrated into the generators. Training is
facilitated through image scale augmentation. Evaluation on two biomedical
tasks -- domain adaptation for nuclear segmentation in immunohistochemistry
(IHC) images and unmixing of biological structures superimposed in
single-channel immunofluorescence (IF) images -- demonstrates Ui2i's ability to
preserve content fidelity in settings that demand more accurate structural
preservation than typical translation tasks. To the best of our knowledge, Ui2i
is the first approach capable of separating superimposed signals in IF images
using real, unpaired training data.

</details>


### [839] [The Role of AI in Early Detection of Life-Threatening Diseases: A Retinal Imaging Perspective](https://arxiv.org/pdf/2505.20810)
*Tariq M Khan, Toufique Ahmed Soomro, Imran Razzak*

Main category: eess.IV

TL;DR: Retinal imaging advances like OCT/OCTA and AI-driven analysis improve disease detection, but challenges like protocol heterogeneity hinder clinical adoption.


<details>
  <summary>Details</summary>
Motivation: To synthesize recent retinal imaging and AI advancements for systemic disease detection and propose solutions for clinical integration.

Method: Review of OCT/OCTA, adaptive optics, AI/ML algorithms, and telemedicine platforms, evaluating their diagnostic performance.

Result: High sensitivity and specificity in disease detection (e.g., >90% for diabetic retinopathy), but external validation and workflow integration remain issues.

Conclusion: Proposes standardization, validation trials, and clinical workflow integration to enhance precision prevention and early intervention.

Abstract: Retinal imaging has emerged as a powerful, non-invasive modality for
detecting and quantifying biomarkers of systemic diseases-ranging from diabetes
and hypertension to Alzheimer's disease and cardiovascular disorders but
current insights remain dispersed across platforms and specialties. Recent
technological advances in optical coherence tomography (OCT/OCTA) and adaptive
optics (AO) now deliver ultra-high-resolution scans (down to 5 {\mu}m ) with
superior contrast and spatial integration, allowing early identification of
microvascular abnormalities and neurodegenerative changes. At the same time,
AI-driven and machine learning (ML) algorithms have revolutionized the analysis
of large-scale retinal datasets, increasing sensitivity and specificity; for
example, deep learning models achieve > 90 \% sensitivity for diabetic
retinopathy and AUC = 0.89 for the prediction of cardiovascular risk from
fundus photographs. The proliferation of mobile health technologies and
telemedicine platforms further extends access, reduces costs, and facilitates
community-based screening and longitudinal monitoring. Despite these
breakthroughs, translation into routine practice is hindered by heterogeneous
imaging protocols, limited external validation of AI models, and integration
challenges within clinical workflows. In this review, we systematically
synthesize the latest OCT/OCT and AO developments, AI/ML approaches, and
mHealth/Tele-ophthalmology initiatives and quantify their diagnostic
performance across disease domains. Finally, we propose a roadmap for
multicenter protocol standardization, prospective validation trials, and
seamless incorporation of retinal screening into primary and specialty care
pathways-paving the way for precision prevention, early intervention, and
ongoing treatment of life-threatening systemic diseases.

</details>


### [840] [Multitemporal Latent Dynamical Framework for Hyperspectral Images Unmixing](https://arxiv.org/pdf/2505.20902)
*Ruiying Li, Bin Pan, Lan Ma, Xia Xu, Zhenwei Shi*

Main category: eess.IV

TL;DR: The paper proposes a multitemporal latent dynamical (MiLD) framework for hyperspectral unmixing, addressing the neglect of abundance dynamics in current methods. It uses neural ODEs and provides theoretical validation.


<details>
  <summary>Details</summary>
Motivation: Current methods focus on endmember variability but ignore abundance dynamics. The paper aims to model these dynamics using neural ODEs, despite challenges in complexity and lack of theory.

Method: MiLD defines the problem with ODEs and latent variables, models it via dynamical discretization, solves it with a neural network-based algorithm, and provides theoretical proofs for consistency, convergence, and stability.

Result: Experiments on synthetic and real datasets validate MiLD's effectiveness in capturing material dynamics.

Conclusion: MiLD successfully addresses multitemporal hyperspectral unmixing by integrating problem definition, modeling, solution, and theory, demonstrating practical utility.

Abstract: Multitemporal hyperspectral unmixing can capture dynamical evolution of
materials. Despite its capability, current methods emphasize variability of
endmembers while neglecting dynamics of abundances, which motivates our
adoption of neural ordinary differential equations to model abundances
temporally. However, this motivation is hindered by two challenges: the
inherent complexity in defining, modeling and solving problem, and the absence
of theoretical support. To address above challenges, in this paper, we propose
a multitemporal latent dynamical (MiLD) unmixing framework by capturing
dynamical evolution of materials with theoretical validation. For addressing
multitemporal hyperspectral unmixing, MiLD consists of problem definition,
mathematical modeling, solution algorithm and theoretical support. We formulate
multitemporal unmixing problem definition by conducting ordinary differential
equations and developing latent variables. We transfer multitemporal unmixing
to mathematical model by dynamical discretization approaches, which describe
the discreteness of observed sequence images with mathematical expansions. We
propose algorithm to solve problem and capture dynamics of materials, which
approximates abundance evolution by neural networks. Furthermore, we provide
theoretical support by validating the crucial properties, which verifies
consistency, convergence and stability theorems. The major contributions of
MiLD include defining problem by ordinary differential equations, modeling
problem by dynamical discretization approach, solving problem by multitemporal
unmixing algorithm, and presenting theoretical support. Our experiments on both
synthetic and real datasets have validated the utility of our work

</details>


### [841] [Generative Image Compression by Estimating Gradients of the Rate-variable Feature Distribution](https://arxiv.org/pdf/2505.20984)
*Minghao Han, Weiyi You, Jinhua Zhang, Leheng Zhang, Ce Zhu, Shuhang Gu*

Main category: eess.IV

TL;DR: A novel diffusion-based generative image compression (GIC) method is proposed, reinterpreting compression as a forward diffusion process and using a reverse neural network for reconstruction, achieving superior performance in perceptual and statistical metrics.


<details>
  <summary>Details</summary>
Motivation: To extend learned image compression (LIC) by integrating generative modeling for photo-realistic reconstructions, addressing limitations of prior diffusion-based approaches.

Method: Reinterprets compression as a forward diffusion path via stochastic differential equations (SDEs), training a reverse neural network to reconstruct images without Gaussian noise initialization.

Result: Outperforms existing GIC methods in perceptual distortion, statistical fidelity, and no-reference quality assessments, with smooth rate adjustment and minimal sampling steps.

Conclusion: The proposed framework effectively combines generative modeling with compression, offering high-quality reconstructions and practical efficiency.

Abstract: While learned image compression (LIC) focuses on efficient data transmission,
generative image compression (GIC) extends this framework by integrating
generative modeling to produce photo-realistic reconstructed images. In this
paper, we propose a novel diffusion-based generative modeling framework
tailored for generative image compression. Unlike prior diffusion-based
approaches that indirectly exploit diffusion modeling, we reinterpret the
compression process itself as a forward diffusion path governed by stochastic
differential equations (SDEs). A reverse neural network is trained to
reconstruct images by reversing the compression process directly, without
requiring Gaussian noise initialization. This approach achieves smooth rate
adjustment and photo-realistic reconstructions with only a minimal number of
sampling steps. Extensive experiments on benchmark datasets demonstrate that
our method outperforms existing generative image compression approaches across
a range of metrics, including perceptual distortion, statistical fidelity, and
no-reference quality assessments.

</details>


### [842] [Cardiac Digital Twins at Scale from MRI: Open Tools and Representative Models from ~55000 UK Biobank Participants](https://arxiv.org/pdf/2505.21019)
*Devran Ugurlu, Shuang Qian, Elliot Fairweather, Charlene Mauger, Bram Ruijsink, Laura Dal Toso, Yu Deng, Marina Strocchi, Reza Razavi, Alistair Young, Pablo Lamata, Steven Niederer, Martin Bishop*

Main category: eess.IV

TL;DR: An open-source pipeline automates the creation of patient-specific cardiac digital twins from MRI data, applied to a large UK Biobank cohort, resulting in 1423 representative heart models.


<details>
  <summary>Details</summary>
Motivation: To address the lack of scalable and diverse cardiac digital twin repositories for cardiovascular disease research and treatment planning.

Method: An automatic pipeline generates left and right ventricular meshes from cardiovascular MRI images, applied to ~55,000 UK Biobank participants.

Result: Produced 1423 representative heart models across sex, BMI, and age, with plans to release code, pre-trained networks, and meshes.

Conclusion: The pipeline enables large-scale, diverse cardiac digital twin generation, advancing cardiovascular research and personalized medicine.

Abstract: A cardiac digital twin is a virtual replica of a patient's heart for
screening, diagnosis, prognosis, risk assessment, and treatment planning of
cardiovascular diseases. This requires an anatomically accurate
patient-specific 3D structural representation of the heart, suitable for
electro-mechanical simulations or study of disease mechanisms. However,
generation of cardiac digital twins at scale is demanding and there are no
public repositories of models across demographic groups. We describe an
automatic open-source pipeline for creating patient-specific left and right
ventricular meshes from cardiovascular magnetic resonance images, its
application to a large cohort of ~55000 participants from UK Biobank, and the
construction of the most comprehensive cohort of adult heart models to date,
comprising 1423 representative meshes across sex (male, female), body mass
index (range: 16 - 42 kg/m$^2$) and age (range: 49 - 80 years). Our code is
available at https://github.com/cdttk/biv-volumetric-meshing/tree/plos2025 ,
and pre-trained networks, representative volumetric meshes with fibers and UVCs
will be made available soon.

</details>


### [843] [Prostate Cancer Screening with Artificial Intelligence-Enhanced Micro-Ultrasound: A Comparative Study with Traditional Methods](https://arxiv.org/pdf/2505.21355)
*Muhammad Imran, Wayne G. Brisbane, Li-Ming Su, Jason P. Joseph, Wei Shao*

Main category: eess.IV

TL;DR: AI-interpreted micro-US outperforms PSA and DRE in detecting prostate cancer, offering higher specificity while maintaining sensitivity.


<details>
  <summary>Details</summary>
Motivation: To evaluate if AI can enhance micro-US for detecting clinically significant prostate cancer (csPCa) compared to traditional PSA and DRE methods.

Method: Used a self-supervised convolutional autoencoder and random forest classifiers on 2D micro-US slices from 145 men, comparing results with clinical screening models.

Result: AI-based micro-US achieved AUROC of 0.871 vs. 0.753 for clinical screening, with 92.5% sensitivity and 68.1% specificity.

Conclusion: AI-interpreted micro-US improves specificity and may reduce unnecessary biopsies, offering a cost-effective alternative to PSA-based screening.

Abstract: Background and objective: Micro-ultrasound (micro-US) is a novel imaging
modality with diagnostic accuracy comparable to MRI for detecting clinically
significant prostate cancer (csPCa). We investigated whether artificial
intelligence (AI) interpretation of micro-US can outperform clinical screening
methods using PSA and digital rectal examination (DRE). Methods: We
retrospectively studied 145 men who underwent micro-US guided biopsy (79 with
csPCa, 66 without). A self-supervised convolutional autoencoder was used to
extract deep image features from 2D micro-US slices. Random forest classifiers
were trained using five-fold cross-validation to predict csPCa at the slice
level. Patients were classified as csPCa-positive if 88 or more consecutive
slices were predicted positive. Model performance was compared with a
classifier using PSA, DRE, prostate volume, and age. Key findings and
limitations: The AI-based micro-US model and clinical screening model achieved
AUROCs of 0.871 and 0.753, respectively. At a fixed threshold, the micro-US
model achieved 92.5% sensitivity and 68.1% specificity, while the clinical
model showed 96.2% sensitivity but only 27.3% specificity. Limitations include
a retrospective single-center design and lack of external validation.
Conclusions and clinical implications: AI-interpreted micro-US improves
specificity while maintaining high sensitivity for csPCa detection. This method
may reduce unnecessary biopsies and serve as a low-cost alternative to
PSA-based screening. Patient summary: We developed an AI system to analyze
prostate micro-ultrasound images. It outperformed PSA and DRE in detecting
aggressive cancer and may help avoid unnecessary biopsies.

</details>


### [844] [Exploring Out-of-distribution Detection for Sparse-view Computed Tomography with Diffusion Models](https://arxiv.org/pdf/2411.06308)
*Ezgi Demircan-Tureyen, Felix Lucka, Tristan van Leeuwen*

Main category: eess.IV

TL;DR: The paper explores using diffusion models for OOD detection in sparse-view CT reconstruction, highlighting successes and limitations, and proposes a weighting approach to improve robustness.


<details>
  <summary>Details</summary>
Motivation: To ensure reliability in CT reconstruction by addressing hallucinations in OOD data and enabling effective anomaly inspection.

Method: Uses a diffusion model as an in-distribution prior, reconstructs partially diffused input images, and assesses OOD-ness via reconstruction errors. Adapts the approach for sparse-view CT by redefining input (FBP reconstructions) and error metrics.

Result: Proof-of-concept experiments on MNIST show potential but also limitations. Effective OOD detection is possible by comparing measurements with forward-projected reconstructions, though conditioning can sometimes reconstruct OOD images well.

Conclusion: A weighting approach improves robustness against OOD measurements, but trade-offs exist. The method shows promise but requires further refinement.

Abstract: Recent works demonstrate the effectiveness of diffusion models as
unsupervised solvers for inverse imaging problems. Sparse-view computed
tomography (CT) has greatly benefited from these advancements, achieving
improved generalization without reliance on measurement parameters. However,
this comes at the cost of potential hallucinations, especially when handling
out-of-distribution (OOD) data. To ensure reliability, it is essential to study
OOD detection for CT reconstruction across both clinical and industrial
applications. This need further extends to enabling the OOD detector to
function effectively as an anomaly inspection tool. In this paper, we explore
the use of a diffusion model, trained to capture the target distribution for CT
reconstruction, as an in-distribution prior. Building on recent research, we
employ the model to reconstruct partially diffused input images and assess
OOD-ness through multiple reconstruction errors. Adapting this approach for
sparse-view CT requires redefining the notions of ``input'' and
``reconstruction error''. Here, we use filtered backprojection (FBP)
reconstructions as input and investigate various definitions of reconstruction
error. Our proof-of-concept experiments on the MNIST dataset highlight both
successes and failures, demonstrating the potential and limitations of
integrating such an OOD detector into a CT reconstruction system. Our findings
suggest that effective OOD detection can be achieved by comparing measurements
with forward-projected reconstructions, provided that reconstructions from
noisy FBP inputs are conditioned on the measurements. However, conditioning can
sometimes lead the OOD detector to inadvertently reconstruct OOD images well.
To counter this, we introduce a weighting approach that improves robustness
against highly informative OOD measurements, albeit with a trade-off in
performance in certain cases.

</details>


### [845] [UltraBones100k: A reliable automated labeling method and large-scale dataset for ultrasound-based bone surface extraction](https://arxiv.org/pdf/2502.03783)
*Luohong Wu, Nicola A. Cavalcanti, Matthias Seibold, Giuseppe Loggia, Lisa Reissner, Jonas Hein, Silvan Beeler, Arnd Viehöfer, Stephan Wirth, Lilian Calvet, Philipp Fürnstahl*

Main category: eess.IV

TL;DR: The paper introduces a method for creating high-quality ultrasound bone segmentation datasets with automated labels, improving model performance over manual labeling.


<details>
  <summary>Details</summary>
Motivation: Ultrasound bone segmentation is challenging due to image noise and shadowing, and existing models rely on costly manual labeling, limiting dataset size and generalizability.

Method: Automated bone labels are generated by superimposing tracked CT models onto ultrasound images, refined for ultrasound physics, and validated by an expert. A neural network is trained on the dataset (UltraBones100k).

Result: The method significantly improves label quality (p < 0.001) and the trained model outperforms manual labeling, especially in low-intensity regions (320% improvement in completeness).

Conclusion: Automated labeling with ultrasound physics refinement enables larger, higher-quality datasets, advancing bone segmentation performance.

Abstract: Ultrasound-based bone surface segmentation is crucial in computer-assisted
orthopedic surgery. However, ultrasound images have limitations, including a
low signal-to-noise ratio, and acoustic shadowing, which make interpretation
difficult. Existing deep learning models for bone segmentation rely primarily
on costly manual labeling by experts, limiting dataset size and model
generalizability. Additionally, the complexity of ultrasound physics and
acoustic shadow makes the images difficult for humans to interpret, leading to
incomplete labels in anechoic regions and limiting model performance. To
advance ultrasound bone segmentation and establish effective model benchmarks,
larger and higher-quality datasets are needed.
  We propose a methodology for collecting ex-vivo ultrasound datasets with
automatically generated bone labels, including anechoic regions. The proposed
labels are derived by accurately superimposing tracked bone CT models onto the
tracked ultrasound images. These initial labels are refined to account for
ultrasound physics. A clinical evaluation is conducted by an expert physician
specialized on orthopedic sonography to assess the quality of the generated
bone labels. A neural network for bone segmentation is trained on the collected
dataset and its predictions are compared to expert manual labels, evaluating
accuracy, completeness, and F1-score.
  We collected the largest known dataset of 100k ultrasound images of human
lower limbs with bone labels, called UltraBones100k. A Wilcoxon signed-rank
test with Bonferroni correction confirmed that the bone alignment after our
method significantly improved the quality of bone labeling (p < 0.001). The
model trained on UltraBones100k consistently outperforms manual labeling in all
metrics, particularly in low-intensity regions (320% improvement in
completeness at a distance threshold of 0.5 mm).

</details>


### [846] [HWA-UNETR: Hierarchical Window Aggregate UNETR for 3D Multimodal Gastric Lesion Segmentation](https://arxiv.org/pdf/2505.10464)
*Jiaming Liang, Lihuan Dai, Xiaoqi Sheng, Xiangguang Chen, Chun Yao, Guihua Tao, Qibin Leng, Hongmin Cai, Xi Zhong*

Main category: eess.IV

TL;DR: The paper introduces a new dataset (GCM 2025) and a 3D segmentation framework (HWA-UNETR) for gastric cancer lesion analysis, addressing challenges of multimodal data scarcity and misalignment.


<details>
  <summary>Details</summary>
Motivation: Challenges in gastric cancer lesion analysis due to limited multimodal datasets and misaligned modalities, leading to resource-heavy training and reduced accuracy.

Method: Proposes HWA-UNETR, a 3D segmentation framework with HWA blocks for dynamic feature alignment and a tri-orientated fusion mamba mechanism for context modeling.

Result: Outperforms existing methods by up to 1.68% in Dice score, validated on GCM 2025 and BraTS 2021 datasets.

Conclusion: The framework and dataset advance multimodal medical image segmentation, offering improved accuracy and robustness.

Abstract: Multimodal medical image segmentation faces significant challenges in the
context of gastric cancer lesion analysis. This clinical context is defined by
the scarcity of independent multimodal datasets and the imperative to
amalgamate inherently misaligned modalities. As a result, algorithms are
constrained to train on approximate data and depend on application migration,
leading to substantial resource expenditure and a potential decline in analysis
accuracy. To address those challenges, we have made two major contributions:
First, we publicly disseminate the GCM 2025 dataset, which serves as the first
large-scale, open-source collection of gastric cancer multimodal MRI scans,
featuring professionally annotated FS-T2W, CE-T1W, and ADC images from 500
patients. Second, we introduce HWA-UNETR, a novel 3D segmentation framework
that employs an original HWA block with learnable window aggregation layers to
establish dynamic feature correspondences between different modalities'
anatomical structures, and leverages the innovative tri-orientated fusion mamba
mechanism for context modeling and capturing long-range spatial dependencies.
Extensive experiments on our GCM 2025 dataset and the publicly BraTS 2021
dataset validate the performance of our framework, demonstrating that the new
approach surpasses existing methods by up to 1.68\% in the Dice score while
maintaining solid robustness. The dataset and code are public via
https://github.com/JeMing-creater/HWA-UNETR.

</details>


### [847] [Recent Advances in Diffusion Models for Hyperspectral Image Processing and Analysis: A Review](https://arxiv.org/pdf/2505.11158)
*Xing Hu, Xiangcheng Liu, Danfeng Hong, Qianqian Duan, Linghua Jiang, Haima Yang, Dawei Zhan*

Main category: eess.IV

TL;DR: Diffusion models show promise in hyperspectral image processing, improving accuracy and efficiency in tasks like denoising and classification.


<details>
  <summary>Details</summary>
Motivation: Hyperspectral image analysis faces challenges like high dimensionality and noise. Traditional models fall short, prompting exploration of diffusion models.

Method: Diffusion models simulate data diffusion over time to model spectral structures, generate samples, and enhance tasks like denoising and classification.

Result: Diffusion models outperform traditional methods in hyperspectral tasks, offering high-quality results in noise removal and data enhancement.

Conclusion: Diffusion models provide a promising direction for hyperspectral image analysis, though challenges remain for future research.

Abstract: Hyperspectral image processing and analysis has important application value
in remote sensing, agriculture and environmental monitoring, but its high
dimensionality, data redundancy and noise interference etc. bring great
challenges to the analysis. Traditional models have limitations in dealing with
these complex data, and it is difficult to meet the increasing demand for
analysis. In recent years, Diffusion models, as a class of emerging generative
approaches, have demonstrated promising capabilities in hyperspectral image
(HSI) processing tasks. By simulating the diffusion process of data in time,
the Diffusion Model are capable of modeling high-dimensional spectral
structures, generate high-quality samples, and achieve competitive performance
in spectral-spatial denoising tasks and data enhancement. In this paper, we
review the recent research advances in diffusion modeling for hyperspectral
image processing and analysis, and discuss its applications in tasks such as
high-dimensional data processing, noise removal, classification, and anomaly
detection. The performance of diffusion-based models on image processing is
compared and the challenges are summarized. It is shown that the diffusion
model can significantly improve the accuracy and efficiency of hyperspectral
image analysis, providing a new direction for future research.

</details>


### [848] [Single Snapshot Distillation for Phase Coded Mask Design in Phase Retrieval](https://arxiv.org/pdf/2505.18352)
*Karen Fonseca, Leon Suarez-Rodriguez, Andres Jerez, Felipe Gutierrez-Barragan, Henry Arguello*

Main category: eess.IV

TL;DR: The paper proposes a Knowledge Distillation (KD) method to improve phase retrieval (PR) by transferring knowledge from a multi-snapshot teacher network to a single-snapshot student network, enhancing efficiency and performance.


<details>
  <summary>Details</summary>
Motivation: High-quality phase retrieval requires multiple snapshots, which is inefficient. Existing end-to-end frameworks face physical implementation constraints and gradient vanishing issues.

Method: A KD approach is introduced where a teacher network (trained with multiple snapshots) distills knowledge into a student network (single-snapshot). Loss functions compare coded phase masks (CPMs) and feature spaces.

Result: Simulations show the KD-based PR system outperforms systems trained without teacher guidance, improving reconstruction performance.

Conclusion: The KD method addresses limitations of traditional PR systems, offering a more efficient and implementable solution for phase retrieval.

Abstract: Phase retrieval (PR) reconstructs phase information from magnitude
measurements, known as coded diffraction patterns (CDPs), whose quality depends
on the number of snapshots captured using coded phase masks. High-quality phase
estimation requires multiple snapshots, which is not desired for efficient PR
systems. End-to-end frameworks enable joint optimization of the optical system
and the recovery neural network. However, their application is constrained by
physical implementation limitations. Additionally, the framework is prone to
gradient vanishing issues related to its global optimization process. This
paper introduces a Knowledge Distillation (KD) optimization approach to address
these limitations. KD transfers knowledge from a larger, lower-constrained
network (teacher) to a smaller, more efficient, and implementable network
(student). In this method, the teacher, a PR system trained with multiple
snapshots, distills its knowledge into a single-snapshot PR system, the
student. The loss functions compare the CPMs and the feature space of the
recovery network. Simulations demonstrate that this approach improves
reconstruction performance compared to a PR system trained without the
teacher's guidance.

</details>
