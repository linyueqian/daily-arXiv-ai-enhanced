{"id": "2505.04628", "pdf": "https://arxiv.org/pdf/2505.04628", "abs": "https://arxiv.org/abs/2505.04628", "authors": ["Yusen Wu", "Junwu Xiong", "Xiaotie Deng"], "title": "How Social is It? A Benchmark for LLMs' Capabilities in Multi-user Multi-turn Social Agent Tasks", "categories": ["cs.CL", "cs.AI", "cs.SI"], "comment": null, "summary": "Expanding the application of large language models (LLMs) to societal life,\ninstead of primary function only as auxiliary assistants to communicate with\nonly one person at a time, necessitates LLMs' capabilities to independently\nplay roles in multi-user, multi-turn social agent tasks within complex social\nsettings. However, currently the capability has not been systematically\nmeasured with available benchmarks. To address this gap, we first introduce an\nagent task leveling framework grounded in sociological principles.\nConcurrently, we propose a novel benchmark, How Social Is It (we call it HSII\nbelow), designed to assess LLM's social capabilities in comprehensive social\nagents tasks and benchmark representative models. HSII comprises four stages:\nformat parsing, target selection, target switching conversation, and stable\nconversation, which collectively evaluate the communication and task completion\ncapabilities of LLMs within realistic social interaction scenarios dataset,\nHSII-Dataset. The dataset is derived step by step from news dataset. We perform\nan ablation study by doing clustering to the dataset. Additionally, we\ninvestigate the impact of chain of thought (COT) method on enhancing LLMs'\nsocial performance. Since COT cost more computation, we further introduce a new\nstatistical metric, COT-complexity, to quantify the efficiency of certain LLMs\nwith COTs for specific social tasks and strike a better trade-off between\nmeasurement of correctness and efficiency. Various results of our experiments\ndemonstrate that our benchmark is well-suited for evaluating social skills in\nLLMs."}
{"id": "2505.04637", "pdf": "https://arxiv.org/pdf/2505.04637", "abs": "https://arxiv.org/abs/2505.04637", "authors": ["Dongxing Yu"], "title": "Adaptive Token Boundaries: Integrating Human Chunking Mechanisms into Multimodal LLMs", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Recent advancements in multimodal large language models (MLLMs) have\ndemonstrated remarkable capabilities in processing diverse data types, yet\nsignificant disparities persist between human cognitive processes and\ncomputational approaches to multimodal information integration. This research\npresents a systematic investigation into the parallels between human\ncross-modal chunking mechanisms and token representation methodologies in\nMLLMs. Through empirical studies comparing human performance patterns with\nmodel behaviors across visual-linguistic tasks, we demonstrate that\nconventional static tokenization schemes fundamentally constrain current\nmodels' capacity to simulate the dynamic, context-sensitive nature of human\ninformation processing. We propose a novel framework for dynamic cross-modal\ntokenization that incorporates adaptive boundaries, hierarchical\nrepresentations, and alignment mechanisms grounded in cognitive science\nprinciples. Quantitative evaluations demonstrate that our approach yields\nstatistically significant improvements over state-of-the-art models on\nbenchmark tasks (+7.8% on Visual Question Answering, +5.3% on Complex Scene\nDescription) while exhibiting more human-aligned error patterns and attention\ndistributions. These findings contribute to the theoretical understanding of\nthe relationship between human cognition and artificial intelligence, while\nproviding empirical evidence for developing more cognitively plausible AI\nsystems."}
{"id": "2505.04639", "pdf": "https://arxiv.org/pdf/2505.04639", "abs": "https://arxiv.org/abs/2505.04639", "authors": ["Abhishek Mishra", "Ritesh Sur Chowdhury", "Vartul Bahuguna", "Isha Pandey", "Ganesh Ramakrishnan"], "title": "Language translation, and change of accent for speech-to-speech task using diffusion model", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Speech-to-speech translation (S2ST) aims to convert spoken input in one\nlanguage to spoken output in another, typically focusing on either language\ntranslation or accent adaptation. However, effective cross-cultural\ncommunication requires handling both aspects simultaneously - translating\ncontent while adapting the speaker's accent to match the target language\ncontext. In this work, we propose a unified approach for simultaneous speech\ntranslation and change of accent, a task that remains underexplored in current\nliterature. Our method reformulates the problem as a conditional generation\ntask, where target speech is generated based on phonemes and guided by target\nspeech features. Leveraging the power of diffusion models, known for\nhigh-fidelity generative capabilities, we adapt text-to-image diffusion\nstrategies by conditioning on source speech transcriptions and generating Mel\nspectrograms representing the target speech with desired linguistic and\naccentual attributes. This integrated framework enables joint optimization of\ntranslation and accent adaptation, offering a more parameter-efficient and\neffective model compared to traditional pipelines."}
{"id": "2505.04640", "pdf": "https://arxiv.org/pdf/2505.04640", "abs": "https://arxiv.org/abs/2505.04640", "authors": ["Hicham Assoudi"], "title": "A Comparative Benchmark of a Moroccan Darija Toxicity Detection Model (Typica.ai) and Major LLM-Based Moderation APIs (OpenAI, Mistral, Anthropic)", "categories": ["cs.CL"], "comment": "GitHub repository with reproducibility materials and evaluation\n  notebook available at:\n  https://github.com/assoudi-typica-ai/darija-toxicity-benchmark", "summary": "This paper presents a comparative benchmark evaluating the performance of\nTypica.ai's custom Moroccan Darija toxicity detection model against major\nLLM-based moderation APIs: OpenAI (omni-moderation-latest), Mistral\n(mistral-moderation-latest), and Anthropic Claude (claude-3-haiku-20240307). We\nfocus on culturally grounded toxic content, including implicit insults,\nsarcasm, and culturally specific aggression often overlooked by general-purpose\nsystems. Using a balanced test set derived from the OMCD_Typica.ai_Mix dataset,\nwe report precision, recall, F1-score, and accuracy, offering insights into\nchallenges and opportunities for moderation in underrepresented languages. Our\nresults highlight Typica.ai's superior performance, underlining the importance\nof culturally adapted models for reliable content moderation."}
{"id": "2505.04629", "pdf": "https://arxiv.org/pdf/2505.04629", "abs": "https://arxiv.org/abs/2505.04629", "authors": ["Abdulhady Abas Abdullah", "Soran Badawi", "Dana A. Abdullah", "Dana Rasul Hamad", "Hanan Abdulrahman Taher", "Sabat Salih Muhamad", "Aram Mahmood Ahmed", "Bryar A. Hassan", "Sirwan Abdolwahed Aula", "Tarik A. Rashid"], "title": "From Dialect Gaps to Identity Maps: Tackling Variability in Speaker Verification", "categories": ["eess.AS", "cs.AI", "cs.CL"], "comment": null, "summary": "The complexity and difficulties of Kurdish speaker detection among its\nseveral dialects are investigated in this work. Because of its great phonetic\nand lexical differences, Kurdish with several dialects including Kurmanji,\nSorani, and Hawrami offers special challenges for speaker recognition systems.\nThe main difficulties in building a strong speaker identification system\ncapable of precisely identifying speakers across several dialects are\ninvestigated in this work. To raise the accuracy and dependability of these\nsystems, it also suggests solutions like sophisticated machine learning\napproaches, data augmentation tactics, and the building of thorough\ndialect-specific corpus. The results show that customized strategies for every\ndialect together with cross-dialect training greatly enhance recognition\nperformance."}
{"id": "2505.04634", "pdf": "https://arxiv.org/pdf/2505.04634", "abs": "https://arxiv.org/abs/2505.04634", "authors": ["Abhiroop Bhattacharya", "Sylvain G. Cloutier"], "title": "MatMMFuse: Multi-Modal Fusion model for Material Property Prediction", "categories": ["cs.LG", "cs.CE"], "comment": "Presented at AI for Accelerated Materials Design(AI4Mat), ICLR 2025\n  (https://openreview.net/forum?id=pN4Zg6HBlq#discussion)", "summary": "The recent progress of using graph based encoding of crystal structures for\nhigh throughput material property prediction has been quite successful.\nHowever, using a single modality model prevents us from exploiting the\nadvantages of an enhanced features space by combining different\nrepresentations. Specifically, pre-trained Large language models(LLMs) can\nencode a large amount of knowledge which is beneficial for training of models.\nMoreover, the graph encoder is able to learn the local features while the text\nencoder is able to learn global information such as space group and crystal\nsymmetry. In this work, we propose Material Multi-Modal Fusion(MatMMFuse), a\nfusion based model which uses a multi-head attention mechanism for the\ncombination of structure aware embedding from the Crystal Graph Convolution\nNetwork (CGCNN) and text embeddings from the SciBERT model. We train our model\nin an end-to-end framework using data from the Materials Project Dataset. We\nshow that our proposed model shows an improvement compared to the vanilla CGCNN\nand SciBERT model for all four key properties: formation energy, band gap,\nenergy above hull and fermi energy. Specifically, we observe an improvement of\n40% compared to the vanilla CGCNN model and 68% compared to the SciBERT model\nfor predicting the formation energy per atom. Importantly, we demonstrate the\nzero shot performance of the trained model on small curated datasets of\nPerovskites, Chalcogenides and the Jarvis Dataset. The results show that the\nproposed model exhibits better zero shot performance than the individual plain\nvanilla CGCNN and SciBERT model. This enables researchers to deploy the model\nfor specialized industrial applications where collection of training data is\nprohibitively expensive."}
{"id": "2505.04672", "pdf": "https://arxiv.org/pdf/2505.04672", "abs": "https://arxiv.org/abs/2505.04672", "authors": ["Lucas Sancéré", "Carina Lorenz", "Doris Helbig", "Oana-Diana Persa", "Sonja Dengler", "Alexander Kreuter", "Martim Laimer", "Anne Fröhlich", "Jennifer Landsberg", "Johannes Brägelmann", "Katarzyna Bozek"], "title": "Histo-Miner: Deep Learning based Tissue Features Extraction Pipeline from H&E Whole Slide Images of Cutaneous Squamous Cell Carcinoma", "categories": ["cs.CV", "q-bio.QM"], "comment": "31 pages including supplement, 5 core figures, 5 supplement figures", "summary": "Recent advancements in digital pathology have enabled comprehensive analysis\nof Whole-Slide Images (WSI) from tissue samples, leveraging high-resolution\nmicroscopy and computational capabilities. Despite this progress, there is a\nlack of labeled datasets and open source pipelines specifically tailored for\nanalysis of skin tissue. Here we propose Histo-Miner, a deep learning-based\npipeline for analysis of skin WSIs and generate two datasets with labeled\nnuclei and tumor regions. We develop our pipeline for the analysis of patient\nsamples of cutaneous squamous cell carcinoma (cSCC), a frequent non-melanoma\nskin cancer. Utilizing the two datasets, comprising 47,392 annotated cell\nnuclei and 144 tumor-segmented WSIs respectively, both from cSCC patients,\nHisto-Miner employs convolutional neural networks and vision transformers for\nnucleus segmentation and classification as well as tumor region segmentation.\nPerformance of trained models positively compares to state of the art with\nmulti-class Panoptic Quality (mPQ) of 0.569 for nucleus segmentation,\nmacro-averaged F1 of 0.832 for nucleus classification and mean Intersection\nover Union (mIoU) of 0.884 for tumor region segmentation. From these\npredictions we generate a compact feature vector summarizing tissue morphology\nand cellular interactions, which can be used for various downstream tasks.\nHere, we use Histo-Miner to predict cSCC patient response to immunotherapy\nbased on pre-treatment WSIs from 45 patients. Histo-Miner identifies\npercentages of lymphocytes, the granulocyte to lymphocyte ratio in tumor\nvicinity and the distances between granulocytes and plasma cells in tumors as\npredictive features for therapy response. This highlights the applicability of\nHisto-Miner to clinically relevant scenarios, providing direct interpretation\nof the classification and insights into the underlying biology."}
{"id": "2505.04638", "pdf": "https://arxiv.org/pdf/2505.04638", "abs": "https://arxiv.org/abs/2505.04638", "authors": ["Tianyu Liu", "Simeng Han", "Xiao Luo", "Hanchen Wang", "Pan Lu", "Biqing Zhu", "Yuge Wang", "Keyi Li", "Jiapeng Chen", "Rihao Qu", "Yufeng Liu", "Xinyue Cui", "Aviv Yaish", "Yuhang Chen", "Minsheng Hao", "Chuhan Li", "Kexing Li", "Arman Cohan", "Hua Xu", "Mark Gerstein", "James Zou", "Hongyu Zhao"], "title": "Towards Artificial Intelligence Research Assistant for Expert-Involved Learning", "categories": ["cs.AI", "cs.CL", "cs.IR"], "comment": "36 pages, 7 figures", "summary": "Large Language Models (LLMs) and Large Multi-Modal Models (LMMs) have emerged\nas transformative tools in scientific research, yet their reliability and\nspecific contributions to biomedical applications remain insufficiently\ncharacterized. In this study, we present \\textbf{AR}tificial\n\\textbf{I}ntelligence research assistant for \\textbf{E}xpert-involved\n\\textbf{L}earning (ARIEL), a multimodal dataset designed to benchmark and\nenhance two critical capabilities of LLMs and LMMs in biomedical research:\nsummarizing extensive scientific texts and interpreting complex biomedical\nfigures. To facilitate rigorous assessment, we create two open-source sets\ncomprising biomedical articles and figures with designed questions. We\nsystematically benchmark both open- and closed-source foundation models,\nincorporating expert-driven human evaluations conducted by doctoral-level\nexperts. Furthermore, we improve model performance through targeted prompt\nengineering and fine-tuning strategies for summarizing research papers, and\napply test-time computational scaling to enhance the reasoning capabilities of\nLMMs, achieving superior accuracy compared to human-expert corrections. We also\nexplore the potential of using LMM Agents to generate scientific hypotheses\nfrom diverse multimodal inputs. Overall, our results delineate clear strengths\nand highlight significant limitations of current foundation models, providing\nactionable insights and guiding future advancements in deploying large-scale\nlanguage and multi-modal models within biomedical research."}
{"id": "2505.05088", "pdf": "https://arxiv.org/pdf/2505.05088", "abs": "https://arxiv.org/abs/2505.05088", "authors": ["Wenyang Liu", "Jianjun Gao", "Kim-Hui Yap"], "title": "SSH-Net: A Self-Supervised and Hybrid Network for Noisy Image Watermark Removal", "categories": ["cs.MM", "cs.CV", "eess.IV"], "comment": "Under Review in JVCI", "summary": "Visible watermark removal is challenging due to its inherent complexities and\nthe noise carried within images. Existing methods primarily rely on supervised\nlearning approaches that require paired datasets of watermarked and\nwatermark-free images, which are often impractical to obtain in real-world\nscenarios. To address this challenge, we propose SSH-Net, a Self-Supervised and\nHybrid Network specifically designed for noisy image watermark removal. SSH-Net\nsynthesizes reference watermark-free images using the watermark distribution in\na self-supervised manner and adopts a dual-network design to address the task.\nThe upper network, focused on the simpler task of noise removal, employs a\nlightweight CNN-based architecture, while the lower network, designed to handle\nthe more complex task of simultaneously removing watermarks and noise,\nincorporates Transformer blocks to model long-range dependencies and capture\nintricate image features. To enhance the model's effectiveness, a shared\nCNN-based feature encoder is introduced before dual networks to extract common\nfeatures that both networks can leverage. Our code will be available at\nhttps://github.com/wenyang001/SSH-Net."}
{"id": "2505.05428", "pdf": "https://arxiv.org/pdf/2505.05428", "abs": "https://arxiv.org/abs/2505.05428", "authors": ["J. Gregory Pauloski", "Yadu Babuji", "Ryan Chard", "Mansi Sakarvadia", "Kyle Chard", "Ian Foster"], "title": "Empowering Scientific Workflows with Federated Agents", "categories": ["cs.MA", "cs.DC"], "comment": null, "summary": "Agentic systems, in which diverse agents cooperate to tackle challenging\nproblems, are exploding in popularity in the AI community. However, the agentic\nframeworks used to build these systems have not previously enabled use with\nresearch cyberinfrastructure. Here we introduce Academy, a modular and\nextensible middleware designed to deploy autonomous agents across the federated\nresearch ecosystem, including HPC systems, experimental facilities, and data\nrepositories. To meet the demands of scientific computing, Academy supports\nasynchronous execution, heterogeneous resources, high-throughput data flows,\nand dynamic resource availability. It provides abstractions for expressing\nstateful agents, managing inter-agent coordination, and integrating computation\nwith experimental control. We present microbenchmark results that demonstrate\nhigh performance and scalability in HPC environments. To demonstrate the\nbreadth of applications that can be supported by agentic workflow designs, we\nalso present case studies in materials discovery, decentralized learning, and\ninformation extraction in which agents are deployed across diverse HPC systems."}
{"id": "2505.04728", "pdf": "https://arxiv.org/pdf/2505.04728", "abs": "https://arxiv.org/abs/2505.04728", "authors": ["Charlotte Vercammen", "Antje Heinrich", "Christophe Lesimple", "Alessia Paglialonga", "Jan-Willem A. Wasmann", "Mareike Buhl"], "title": "Data Standards in Audiology: A Mixed-Methods Exploration of Community Perspectives and Implementation Considerations", "categories": ["cs.SD", "eess.AS", "physics.med-ph"], "comment": null, "summary": "Objective: The purpose of this study was to explore options for data\nstandardisation in audiology and document the global audiology community's\ncurrent knowledge and views of data standards, explore their needs and\npreferences, and develop recommendations for data standardisation as a result.\n  Design: A mixed-methods approach, combining a structured survey with an\nin-depth exploration of themes by experts during a special session on \"Big Data\nand Data Standards in Audiology\" at the 2024 Virtual Conference of\nComputational Audiology.\n  Study Sample: The survey sample consisted of 82 members of the global\naudiology community; five experts joined the panel discussion.\n  Results: Survey results emphasized the need for data standardisation in\naudiology aimed at facilitating research and improving patient care. Knowledge\nof existing initiatives was low: 38% were aware of initiatives. Yet, 90%\nenvisioned contributing to them moving forward. The panel discussion explored\nemerging standardisation initiatives in audiology (OMOP, openEHR, HIMSA's Noah\nstandard), challenges (e.g., data quality and privacy), and opportunities\n(e.g., conversion between approaches and synergies with other medical fields).\n  Conclusions: The community support identified in this study could be\nleveraged to further develop standardisation initiatives for audiology,\nensuring alignment between initiatives and with other medical fields."}
{"id": "2505.04652", "pdf": "https://arxiv.org/pdf/2505.04652", "abs": "https://arxiv.org/abs/2505.04652", "authors": ["Yi Lin", "Dong Zhang", "Xiao Fang", "Yufan Chen", "Kwang-Ting Cheng", "Hao Chen"], "title": "Rethinking Boundary Detection in Deep Learning-Based Medical Image Segmentation", "categories": ["eess.IV", "cs.CV"], "comment": "Accepted by Medical Image Analysis", "summary": "Medical image segmentation is a pivotal task within the realms of medical\nimage analysis and computer vision. While current methods have shown promise in\naccurately segmenting major regions of interest, the precise segmentation of\nboundary areas remains challenging. In this study, we propose a novel network\narchitecture named CTO, which combines Convolutional Neural Networks (CNNs),\nVision Transformer (ViT) models, and explicit edge detection operators to\ntackle this challenge. CTO surpasses existing methods in terms of segmentation\naccuracy and strikes a better balance between accuracy and efficiency, without\nthe need for additional data inputs or label injections. Specifically, CTO\nadheres to the canonical encoder-decoder network paradigm, with a dual-stream\nencoder network comprising a mainstream CNN stream for capturing local features\nand an auxiliary StitchViT stream for integrating long-range dependencies.\nFurthermore, to enhance the model's ability to learn boundary areas, we\nintroduce a boundary-guided decoder network that employs binary boundary masks\ngenerated by dedicated edge detection operators to provide explicit guidance\nduring the decoding process. We validate the performance of CTO through\nextensive experiments conducted on seven challenging medical image segmentation\ndatasets, namely ISIC 2016, PH2, ISIC 2018, CoNIC, LiTS17, and BTCV. Our\nexperimental results unequivocally demonstrate that CTO achieves\nstate-of-the-art accuracy on these datasets while maintaining competitive model\ncomplexity. The codes have been released at:\nhttps://github.com/xiaofang007/CTO."}
{"id": "2505.04642", "pdf": "https://arxiv.org/pdf/2505.04642", "abs": "https://arxiv.org/abs/2505.04642", "authors": ["Nischal Mandal", "Yang Li"], "title": "Rethinking Multimodal Sentiment Analysis: A High-Accuracy, Simplified Fusion Architecture", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Multimodal sentiment analysis, a pivotal task in affective computing, seeks\nto understand human emotions by integrating cues from language, audio, and\nvisual signals. While many recent approaches leverage complex attention\nmechanisms and hierarchical architectures, we propose a lightweight, yet\neffective fusion-based deep learning model tailored for utterance-level emotion\nclassification. Using the benchmark IEMOCAP dataset, which includes aligned\ntext, audio-derived numeric features, and visual descriptors, we design a\nmodality-specific encoder using fully connected layers followed by dropout\nregularization. The modality-specific representations are then fused using\nsimple concatenation and passed through a dense fusion layer to capture\ncross-modal interactions. This streamlined architecture avoids computational\noverhead while preserving performance, achieving a classification accuracy of\n92% across six emotion categories. Our approach demonstrates that with careful\nfeature engineering and modular design, simpler fusion strategies can\noutperform or match more complex models, particularly in resource-constrained\nenvironments."}
{"id": "2505.05114", "pdf": "https://arxiv.org/pdf/2505.05114", "abs": "https://arxiv.org/abs/2505.05114", "authors": ["Pengjie Shen", "Kangrui Chen", "Shulin He", "Pengru Chen", "Shuqi Yuan", "He Kong", "Xueliang Zhang", "Zhong-Qiu Wang"], "title": "Listen to Extract: Onset-Prompted Target Speaker Extraction", "categories": ["eess.AS", "cs.SD"], "comment": "in submission", "summary": "We propose $\\textit{listen to extract}$ (LExt), a highly-effective while\nextremely-simple algorithm for monaural target speaker extraction (TSE). Given\nan enrollment utterance of a target speaker, LExt aims at extracting the target\nspeaker from the speaker's mixed speech with other speakers. For each mixture,\nLExt concatenates an enrollment utterance of the target speaker to the mixture\nsignal at the waveform level, and trains deep neural networks (DNN) to extract\nthe target speech based on the concatenated mixture signal. The rationale is\nthat, this way, an artificial speech onset is created for the target speaker\nand it could prompt the DNN (a) which speaker is the target to extract; and (b)\nspectral-temporal patterns of the target speaker that could help extraction.\nThis simple approach produces strong TSE performance on multiple public TSE\ndatasets including WSJ0-2mix, WHAM! and WHAMR!."}
{"id": "2505.04733", "pdf": "https://arxiv.org/pdf/2505.04733", "abs": "https://arxiv.org/abs/2505.04733", "authors": ["Shai Feldman", "Stephen Bates", "Yaniv Romano"], "title": "Conformal Prediction with Corrupted Labels: Uncertain Imputation and Robust Re-weighting", "categories": ["cs.LG"], "comment": null, "summary": "We introduce a framework for robust uncertainty quantification in situations\nwhere labeled training data are corrupted, through noisy or missing labels. We\nbuild on conformal prediction, a statistical tool for generating prediction\nsets that cover the test label with a pre-specified probability. The validity\nof conformal prediction, however, holds under the i.i.d assumption, which does\nnot hold in our setting due to the corruptions in the data. To account for this\ndistribution shift, the privileged conformal prediction (PCP) method proposed\nleveraging privileged information (PI) -- additional features available only\nduring training -- to re-weight the data distribution, yielding valid\nprediction sets under the assumption that the weights are accurate. In this\nwork, we analyze the robustness of PCP to inaccuracies in the weights. Our\nanalysis indicates that PCP can still yield valid uncertainty estimates even\nwhen the weights are poorly estimated. Furthermore, we introduce uncertain\nimputation (UI), a new conformal method that does not rely on weight\nestimation. Instead, we impute corrupted labels in a way that preserves their\nuncertainty. Our approach is supported by theoretical guarantees and validated\nempirically on both synthetic and real benchmarks. Finally, we show that these\ntechniques can be integrated into a triply robust framework, ensuring\nstatistically valid predictions as long as at least one underlying method is\nvalid."}
{"id": "2505.04713", "pdf": "https://arxiv.org/pdf/2505.04713", "abs": "https://arxiv.org/abs/2505.04713", "authors": ["Luis F. Gomez", "Gonzalo Garrido-Lopez", "Julian Fierrez", "Aythami Morales", "Ruben Tolosana", "Javier Rueda", "Enrique Navarro"], "title": "Comparison of Visual Trackers for Biomechanical Analysis of Running", "categories": ["cs.CV", "cs.LG"], "comment": "Preprint of the paper presented to the Third Workshop on Learning\n  with Few or Without Annotated Face, Body, and Gesture Data on 19th IEEE\n  Conference on Automatic Face and Gesture Recognition 2025", "summary": "Human pose estimation has witnessed significant advancements in recent years,\nmainly due to the integration of deep learning models, the availability of a\nvast amount of data, and large computational resources. These developments have\nled to highly accurate body tracking systems, which have direct applications in\nsports analysis and performance evaluation.\n  This work analyzes the performance of six trackers: two point trackers and\nfour joint trackers for biomechanical analysis in sprints. The proposed\nframework compares the results obtained from these pose trackers with the\nmanual annotations of biomechanical experts for more than 5870 frames. The\nexperimental framework employs forty sprints from five professional runners,\nfocusing on three key angles in sprint biomechanics: trunk inclination, hip\nflex extension, and knee flex extension. We propose a post-processing module\nfor outlier detection and fusion prediction in the joint angles.\n  The experimental results demonstrate that using joint-based models yields\nroot mean squared errors ranging from 11.41{\\deg} to 4.37{\\deg}. When\nintegrated with the post-processing modules, these errors can be reduced to\n6.99{\\deg} and 3.88{\\deg}, respectively. The experimental findings suggest that\nhuman pose tracking approaches can be valuable resources for the biomechanical\nanalysis of running. However, there is still room for improvement in\napplications where high accuracy is required."}
{"id": "2505.04646", "pdf": "https://arxiv.org/pdf/2505.04646", "abs": "https://arxiv.org/abs/2505.04646", "authors": ["Poria Azadi"], "title": "Computational Irreducibility as the Foundation of Agency: A Formal Model Connecting Undecidability to Autonomous Behavior in Complex Systems", "categories": ["cs.AI", "cs.CC", "cs.IT", "math.IT"], "comment": null, "summary": "This article explores the emergence of autonomy and agency by connecting\nfundamental computational limits (decidability, completeness, computational\nirreducibility) with physical concepts. We introduce a formal model of a\n\"minimal agent\" operating within potentially Turing-complete environments.\nUsing algorithmic information theory, we argue that the inherent undecidability\nand computational irreducibility of agent-environment interaction lead to\nunpredictability and novel information generation, enabling agency (effective\ngoal-directed action). Computational irreducibility prevents full external\nprediction, creating necessary conditions for autonomous behavior. We relate\nthis to computational sourcehood, where an agent is the irreducible origin of\nits behavior, though formalizing this concept remains challenging. Our central\nthesis, formally proven, is that genuine autonomy necessarily implies\nundecidability from an external perspective, distinguishing autonomous systems\nfrom predictable ones. We propose that agency arises when agent-environment\ncoupling complexity allows mutual information between internal states and\nrelevant environmental variables to increase, particularly where analytical\nsolutions are absent and operational closure is needed for persistence. This\nframework links agency directly to the computational properties of interaction,\noffering implications for understanding consciousness, designing autonomous AI,\nand reconceptualizing free will in a deterministic yet computationally\nirreducible universe."}
{"id": "2505.04657", "pdf": "https://arxiv.org/pdf/2505.04657", "abs": "https://arxiv.org/abs/2505.04657", "authors": ["Shuoyan Wei", "Feng Li", "Shengeng Tang", "Yao Zhao", "Huihui Bai"], "title": "EvEnhancer: Empowering Effectiveness, Efficiency and Generalizability for Continuous Space-Time Video Super-Resolution with Events", "categories": ["eess.IV", "cs.MM"], "comment": "19 pages, 11 figures, 11 tables. Accepted to CVPR 2025 (Highlight)", "summary": "Continuous space-time video super-resolution (C-STVSR) endeavors to upscale\nvideos simultaneously at arbitrary spatial and temporal scales, which has\nrecently garnered increasing interest. However, prevailing methods struggle to\nyield satisfactory videos at out-of-distribution spatial and temporal scales.\nOn the other hand, event streams characterized by high temporal resolution and\nhigh dynamic range, exhibit compelling promise in vision tasks. This paper\npresents EvEnhancer, an innovative approach that marries the unique advantages\nof event streams to elevate effectiveness, efficiency, and generalizability for\nC-STVSR. Our approach hinges on two pivotal components: 1) Event-adapted\nsynthesis capitalizes on the spatiotemporal correlations between frames and\nevents to discern and learn long-term motion trajectories, enabling the\nadaptive interpolation and fusion of informative spatiotemporal features; 2)\nLocal implicit video transformer integrates local implicit video neural\nfunction with cross-scale spatiotemporal attention to learn continuous video\nrepresentations utilized to generate plausible videos at arbitrary resolutions\nand frame rates. Experiments show that EvEnhancer achieves superiority on\nsynthetic and real-world datasets and preferable generalizability on\nout-of-distribution scales against state-of-the-art methods. Code is available\nat https://github.com/W-Shuoyan/EvEnhancer."}
{"id": "2505.04885", "pdf": "https://arxiv.org/pdf/2505.04885", "abs": "https://arxiv.org/abs/2505.04885", "authors": ["Shaja Arul Selvamani", "Nia D'Souza Ganapathy"], "title": "A Multi-Agent AI Framework for Immersive Audiobook Production through Spatial Audio and Neural Narration", "categories": ["cs.SD", "cs.HC", "cs.MA", "cs.MM", "eess.AS"], "comment": null, "summary": "This research introduces an innovative AI-driven multi-agent framework\nspecifically designed for creating immersive audiobooks. Leveraging neural\ntext-to-speech synthesis with FastSpeech 2 and VALL-E for expressive narration\nand character-specific voices, the framework employs advanced language models\nto automatically interpret textual narratives and generate realistic spatial\naudio effects. These sound effects are dynamically synchronized with the\nstoryline through sophisticated temporal integration methods, including Dynamic\nTime Warping (DTW) and recurrent neural networks (RNNs). Diffusion-based\ngenerative models combined with higher-order ambisonics (HOA) and scattering\ndelay networks (SDN) enable highly realistic 3D soundscapes, substantially\nenhancing listener immersion and narrative realism. This technology\nsignificantly advances audiobook applications, providing richer experiences for\neducational content, storytelling platforms, and accessibility solutions for\nvisually impaired audiences. Future work will address personalization, ethical\nmanagement of synthesized voices, and integration with multi-sensory platforms."}
{"id": "2505.05055", "pdf": "https://arxiv.org/pdf/2505.05055", "abs": "https://arxiv.org/abs/2505.05055", "authors": ["Silvan Peter", "Patricia Hu", "Gerhard Widmer"], "title": "How to Infer Repeat Structures in MIDI Performances", "categories": ["cs.SD", "eess.AS"], "comment": "3 pages, 1 figure, 1 table, to be published in the Music Encoding\n  Conference 2025", "summary": "MIDI performances are generally expedient in performance research and music\ninformation retrieval, and even more so if they can be connected to a score.\nThis connection is usually established by means of alignment, linking either\nnotes or time points between the score and the performance. The first obstacle\nwhen trying to establish such an alignment is that a performance realizes one\n(out of many) structural versions of the score that can plausibly result from\ninstructions such as repeats, variations, and navigation markers like 'dal\nsegno/da capo al coda'. A score needs to be unfolded, that is, its repeats and\nnavigation markers need to be explicitly written out to create a single\ntimeline without jumps matching the performance, before alignment algorithms\ncan be applied. In the curation of large performance corpora this process is\ncarried out manually, as no tools are available to infer the repeat structure\nof the performance. To ease this process, we develop a method to automatically\ninfer the repeat structure of a MIDI performance, given a symbolically encoded\nscore including repeat and navigation markers. The intuition guiding our design\nis: 1) local alignment of every contiguous section of the score with a section\nof a performance containing the same material should receive high alignment\ngain, whereas local alignment with any other performance section should accrue\na low or zero gain. And 2) stitching local alignments together according to a\nvalid structural version of the score should result in an approximate full\nalignment and correspondingly high global accumulated gain if the structural\nversion corresponds to the performance, and low gain for all other, ill-fitting\nstructural versions."}
{"id": "2505.04658", "pdf": "https://arxiv.org/pdf/2505.04658", "abs": "https://arxiv.org/abs/2505.04658", "authors": ["Baoshun Shi", "Zheng Liu", "Xin Meng", "Yan Yang"], "title": "Cross-organ all-in-one parallel compressed sensing magnetic resonance imaging", "categories": ["eess.IV"], "comment": null, "summary": "Recent advances in deep learning-based parallel compressed sensing magnetic\nresonance imaging (p-CSMRI) have significantly improved reconstruction quality.\nHowever, current p-CSMRI methods often require training separate deep neural\nnetwork (DNN) for each organ due to anatomical variations, creating a barrier\nto developing generalized medical image reconstruction systems. To address\nthis, we propose CAPNet (cross-organ all-in-one deep unfolding p-CSMRI\nnetwork), a unified framework that implements a p-CSMRI iterative algorithm via\nthree specialized modules: auxiliary variable module, prior module, and data\nconsistency module. Recognizing that p-CSMRI systems often employ varying\nsampling ratios for different organs, resulting in organ-specific artifact\npatterns, we introduce an artifact generation submodule, which extracts and\nintegrates artifact features into the data consistency module to enhance the\ndiscriminative capability of the overall network. For the prior module, we\ndesign an organ structure-prompt generation submodule that leverages structural\nfeatures extracted from the segment anything model (SAM) to create cross-organ\nprompts. These prompts are strategically incorporated into the prior module\nthrough an organ structure-aware Mamba submodule. Comprehensive evaluations on\na cross-organ dataset confirm that CAPNet achieves state-of-the-art\nreconstruction performance across multiple anatomical structures using a single\nunified model. Our code will be published at\nhttps://github.com/shibaoshun/CAPNet."}
{"id": "2505.04643", "pdf": "https://arxiv.org/pdf/2505.04643", "abs": "https://arxiv.org/abs/2505.04643", "authors": ["Hannes Waldetoft", "Jakob Torgander", "Måns Magnusson"], "title": "Prediction-powered estimators for finite population statistics in highly imbalanced textual data: Public hate crime estimation", "categories": ["cs.CL", "cs.LG"], "comment": null, "summary": "Estimating population parameters in finite populations of text documents can\nbe challenging when obtaining the labels for the target variable requires\nmanual annotation. To address this problem, we combine predictions from a\ntransformer encoder neural network with well-established survey sampling\nestimators using the model predictions as an auxiliary variable. The\napplicability is demonstrated in Swedish hate crime statistics based on Swedish\npolice reports. Estimates of the yearly number of hate crimes and the police's\nunder-reporting are derived using the Hansen-Hurwitz estimator, difference\nestimation, and stratified random sampling estimation. We conclude that if\nlabeled training data is available, the proposed method can provide very\nefficient estimates with reduced time spent on manual annotation."}
{"id": "2505.05156", "pdf": "https://arxiv.org/pdf/2505.05156", "abs": "https://arxiv.org/abs/2505.05156", "authors": ["Kavya Ranjan Saxena", "Vipul Arora"], "title": "Regression-based Melody Estimation with Uncertainty Quantification", "categories": ["eess.AS"], "comment": null, "summary": "Existing machine learning models approach the task of melody estimation from\npolyphonic audio as a classification problem by discretizing the pitch values,\nwhich results in the loss of finer frequency variations present in the melody.\nTo better capture these variations, we propose to approach this task as a\nregression problem. Apart from predicting only the pitch for a particular\nregion in the audio, we also predict its uncertainty to enhance the\ntrustworthiness of the model. To perform regression-based melody estimation, we\npropose three different methods that use histogram representation to model the\npitch values. Such a representation requires the support range of the histogram\nto be continuous. The first two methods address the abrupt discontinuity\nbetween unvoiced and voiced frequency ranges by mapping them to a continuous\nrange. The third method reformulates melody estimation as a fully Bayesian\ntask, modeling voicing detection as a classification problem, and voiced pitch\nestimation as a regression problem. Additionally, we introduce a novel method\nto estimate the uncertainty from the histogram representation that correlates\nwell with the deviation of the mean of the predicted distribution from the\nground truth. Experimental results demonstrate that reformulating melody\nestimation as a regression problem significantly improves the performance over\nclassification-based approaches. Comparing the proposed methods with a\nstate-of-the-art regression model, it is observed that the Bayesian method\nperforms the best at estimating both the melody and its associated uncertainty."}
{"id": "2505.04738", "pdf": "https://arxiv.org/pdf/2505.04738", "abs": "https://arxiv.org/abs/2505.04738", "authors": ["Stepan Tretiakov", "Xingjian Li", "Krishna Kumar"], "title": "SetONet: A Deep Set-based Operator Network for Solving PDEs with permutation invariant variable input sampling", "categories": ["cs.LG", "I.2; G.1.8"], "comment": null, "summary": "Neural operators, particularly the Deep Operator Network (DeepONet), have\nshown promise in learning mappings between function spaces for solving\ndifferential equations. However, standard DeepONet requires input functions to\nbe sampled at fixed locations, limiting its applicability in scenarios with\nvariable sensor configurations, missing data, or irregular grids. We introduce\nthe Set Operator Network (SetONet), a novel architecture that integrates Deep\nSets principles into the DeepONet framework to address this limitation. The\ncore innovation lies in the SetONet branch network, which processes the input\nfunction as an unordered \\emph{set} of location-value pairs. This design\nensures permutation invariance with respect to the input points, making SetONet\ninherently robust to variations in the number and locations of sensors. SetONet\nlearns richer, spatially-aware input representations by explicitly processing\nspatial coordinates and function values. We demonstrate SetONet's effectiveness\non several benchmark problems, including derivative/anti-derivative operators,\n1D Darcy flow, and 2D elasticity. Results show that SetONet successfully learns\noperators under variable input sampling conditions where standard DeepONet\nfails. Furthermore, SetONet is architecturally robust to sensor drop-off;\nunlike standard DeepONet, which requires methods like interpolation to function\nwith missing data. Notably, SetONet can achieve comparable or improved accuracy\nover DeepONet on fixed grids, particularly for nonlinear problems, likely due\nto its enhanced input representation. SetONet provides a flexible and robust\nextension to the neural operator toolkit, significantly broadening the\napplicability of operator learning to problems with variable or incomplete\ninput data."}
{"id": "2505.04718", "pdf": "https://arxiv.org/pdf/2505.04718", "abs": "https://arxiv.org/abs/2505.04718", "authors": ["Divyansh Srivastava", "Xiang Zhang", "He Wen", "Chenru Wen", "Zhuowen Tu"], "title": "Lay-Your-Scene: Natural Scene Layout Generation with Diffusion Transformers", "categories": ["cs.CV", "cs.LG"], "comment": null, "summary": "We present Lay-Your-Scene (shorthand LayouSyn), a novel text-to-layout\ngeneration pipeline for natural scenes. Prior scene layout generation methods\nare either closed-vocabulary or use proprietary large language models for\nopen-vocabulary generation, limiting their modeling capabilities and broader\napplicability in controllable image generation. In this work, we propose to use\nlightweight open-source language models to obtain scene elements from text\nprompts and a novel aspect-aware diffusion Transformer architecture trained in\nan open-vocabulary manner for conditional layout generation. Extensive\nexperiments demonstrate that LayouSyn outperforms existing methods and achieves\nstate-of-the-art performance on challenging spatial and numerical reasoning\nbenchmarks. Additionally, we present two applications of LayouSyn. First, we\nshow that coarse initialization from large language models can be seamlessly\ncombined with our method to achieve better results. Second, we present a\npipeline for adding objects to images, demonstrating the potential of LayouSyn\nin image editing applications."}
{"id": "2505.04674", "pdf": "https://arxiv.org/pdf/2505.04674", "abs": "https://arxiv.org/abs/2505.04674", "authors": ["Enqiang Zhu", "Chenkai Hao", "Chanjuan Liu", "Yongsheng Rao"], "title": "Dynamic Location Search for Identifying Maximum Weighted Independent Sets in Complex Networks", "categories": ["cs.AI"], "comment": null, "summary": "While Artificial intelligence (AI), including Generative AI, are effective at\ngenerating high-quality traffic data and optimization solutions in intelligent\ntransportation systems (ITSs), these techniques often demand significant\ntraining time and computational resources, especially in large-scale and\ncomplex scenarios. To address this, we introduce a novel and efficient\nalgorithm for solving the maximum weighted independent set (MWIS) problem,\nwhich can be used to model many ITSs applications, such as traffic signal\ncontrol and vehicle routing. Given the NP-hard nature of the MWIS problem, our\nproposed algorithm, DynLS, incorporates three key innovations to solve it\neffectively. First, it uses a scores-based adaptive vertex perturbation (SAVP)\ntechnique to accelerate convergence, particularly in sparse graphs. Second, it\nincludes a region location mechanism (RLM) to help escape local optima by\ndynamically adjusting the search space. Finally, it employs a novel variable\nneighborhood descent strategy, ComLS, which combines vertex exchange strategies\nwith a reward mechanism to guide the search toward high-quality solutions. Our\nexperimental results demonstrate DynLS's superior performance, consistently\ndelivering high-quality solutions within 1000 seconds. DynLS outperformed five\nleading algorithms across 360 test instances, achieving the best solution for\n350 instances and surpassing the second-best algorithm, Cyclic-Fast, by 177\ninstances. Moreover, DynLS matched Cyclic-Fast's convergence speed,\nhighlighting its efficiency and practicality. This research represents a\nsignificant advancement in heuristic algorithms for the MWIS problem, offering\na promising approach to aid AI techniques in optimizing intelligent\ntransportation systems."}
{"id": "2505.04960", "pdf": "https://arxiv.org/pdf/2505.04960", "abs": "https://arxiv.org/abs/2505.04960", "authors": ["Xin Zhou", "Xiaoxiong Zhang", "Dusit Niyato", "Zhiqi Shen"], "title": "Learning Item Representations Directly from Multimodal Features for Effective Recommendation", "categories": ["cs.IR", "cs.MM"], "comment": "Code: https://github.com/enoche/LIRDRec", "summary": "Conventional multimodal recommender systems predominantly leverage Bayesian\nPersonalized Ranking (BPR) optimization to learn item representations by\namalgamating item identity (ID) embeddings with multimodal features.\nNevertheless, our empirical and theoretical findings unequivocally demonstrate\na pronounced optimization gradient bias in favor of acquiring representations\nfrom multimodal features over item ID embeddings. As a consequence, item ID\nembeddings frequently exhibit suboptimal characteristics despite the\nconvergence of multimodal feature parameters. Given the rich informational\ncontent inherent in multimodal features, in this paper, we propose a novel\nmodel (i.e., LIRDRec) that learns item representations directly from these\nfeatures to augment recommendation performance. Recognizing that features\nderived from each modality may capture disparate yet correlated aspects of\nitems, we propose a multimodal transformation mechanism, integrated with\nmodality-specific encoders, to effectively fuse features from all modalities.\nMoreover, to differentiate the influence of diverse modality types, we devise a\nprogressive weight copying fusion module within LIRDRec. This module\nincrementally learns the weight assigned to each modality in synthesizing the\nfinal user or item representations. Finally, we utilize the powerful visual\nunderstanding of Multimodal Large Language Models (MLLMs) to convert the item\nimages into texts and extract semantics embeddings upon the texts via LLMs.\nEmpirical evaluations conducted on five real-world datasets validate the\nsuperiority of our approach relative to competing baselines. It is worth noting\nthe proposed model, equipped with embeddings extracted from MLLMs and LLMs, can\nfurther improve the recommendation accuracy of NDCG@20 by an average of 4.21%\ncompared to the original embeddings."}
{"id": "2505.04997", "pdf": "https://arxiv.org/pdf/2505.04997", "abs": "https://arxiv.org/abs/2505.04997", "authors": ["Ling Yue", "Nithin Somasekharan", "Yadi Cao", "Shaowu Pan"], "title": "Foam-Agent: Towards Automated Intelligent CFD Workflows", "categories": ["cs.AI", "cs.MA"], "comment": null, "summary": "Computational Fluid Dynamics (CFD) is an essential simulation tool in various\nengineering disciplines, but it often requires substantial domain expertise and\nmanual configuration, creating barriers to entry. We present Foam-Agent, a\nmulti-agent framework that automates complex OpenFOAM-based CFD simulation\nworkflows from natural language inputs. Our innovation includes (1) a\nhierarchical multi-index retrieval system with specialized indices for\ndifferent simulation aspects, (2) a dependency-aware file generation system\nthat provides consistency management across configuration files, and (3) an\niterative error correction mechanism that diagnoses and resolves simulation\nfailures without human intervention. Through comprehensive evaluation on the\ndataset of 110 simulation tasks, Foam-Agent achieves an 83.6% success rate with\nClaude 3.5 Sonnet, significantly outperforming existing frameworks (55.5% for\nMetaOpenFOAM and 37.3% for OpenFOAM-GPT). Ablation studies demonstrate the\ncritical contribution of each system component, with the specialized error\ncorrection mechanism providing a 36.4% performance improvement. Foam-Agent\nsubstantially lowers the CFD expertise threshold while maintaining modeling\naccuracy, demonstrating the potential of specialized multi-agent systems to\ndemocratize access to complex scientific simulation tools. The code is public\nat https://github.com/csml-rpi/Foam-Agent"}
{"id": "2505.05077", "pdf": "https://arxiv.org/pdf/2505.05077", "abs": "https://arxiv.org/abs/2505.05077", "authors": ["Wataru Nakata", "Yuma Koizumi", "Shigeki Karita", "Robin Scheibler", "Haruko Ishikawa", "Adriana Guevara-Rukoz", "Heiga Zen", "Michiel Bacchiani"], "title": "ReverbMiipher: Generative Speech Restoration meets Reverberation Characteristics Controllability", "categories": ["cs.SD", "eess.AS"], "comment": "5 pages, 5 figures", "summary": "Reverberation encodes spatial information regarding the acoustic source\nenvironment, yet traditional Speech Restoration (SR) usually completely removes\nreverberation. We propose ReverbMiipher, an SR model extending parametric\nresynthesis framework, designed to denoise speech while preserving and enabling\ncontrol over reverberation. ReverbMiipher incorporates a dedicated\nReverbEncoder to extract a reverb feature vector from noisy input. This feature\nconditions a vocoder to reconstruct the speech signal, removing noise while\nretaining the original reverberation characteristics. A stochastic zero-vector\nreplacement strategy during training ensures the feature specifically encodes\nreverberation, disentangling it from other speech attributes. This learned\nrepresentation facilitates reverberation control via techniques such as\ninterpolation between features, replacement with features from other\nutterances, or sampling from a latent space. Objective and subjective\nevaluations confirm ReverbMiipher effectively preserves reverberation, removes\nother artifacts, and outperforms the conventional two-stage SR and convolving\nsimulated room impulse response approach. We further demonstrate its ability to\ngenerate novel reverberation effects through feature manipulation."}
{"id": "2505.04664", "pdf": "https://arxiv.org/pdf/2505.04664", "abs": "https://arxiv.org/abs/2505.04664", "authors": ["Ziyuan Huang", "Kevin Huggins", "Srikar Bellur"], "title": "Advancing 3D Medical Image Segmentation: Unleashing the Potential of Planarian Neural Networks in Artificial Intelligence", "categories": ["eess.IV", "cs.AI", "cs.CV", "68T07"], "comment": "36 pages, 8 figures, 21 tables", "summary": "Our study presents PNN-UNet as a method for constructing deep neural networks\nthat replicate the planarian neural network (PNN) structure in the context of\n3D medical image data. Planarians typically have a cerebral structure\ncomprising two neural cords, where the cerebrum acts as a coordinator, and the\nneural cords serve slightly different purposes within the organism's\nneurological system. Accordingly, PNN-UNet comprises a Deep-UNet and a\nWide-UNet as the nerve cords, with a densely connected autoencoder performing\nthe role of the brain. This distinct architecture offers advantages over both\nmonolithic (UNet) and modular networks (Ensemble-UNet). Our outcomes on a 3D\nMRI hippocampus dataset, with and without data augmentation, demonstrate that\nPNN-UNet outperforms the baseline UNet and several other UNet variants in image\nsegmentation."}
{"id": "2505.04645", "pdf": "https://arxiv.org/pdf/2505.04645", "abs": "https://arxiv.org/abs/2505.04645", "authors": ["Tejas Jade", "Alex Yartsev"], "title": "ChatGPT for automated grading of short answer questions in mechanical ventilation", "categories": ["cs.CL", "cs.LG", "stat.CO"], "comment": null, "summary": "Standardised tests using short answer questions (SAQs) are common in\npostgraduate education. Large language models (LLMs) simulate conversational\nlanguage and interpret unstructured free-text responses in ways aligning with\napplying SAQ grading rubrics, making them attractive for automated grading. We\nevaluated ChatGPT 4o to grade SAQs in a postgraduate medical setting using data\nfrom 215 students (557 short-answer responses) enrolled in an online course on\nmechanical ventilation (2020--2024). Deidentified responses to three case-based\nscenarios were presented to ChatGPT with a standardised grading prompt and\nrubric. Outputs were analysed using mixed-effects modelling, variance component\nanalysis, intraclass correlation coefficients (ICCs), Cohen's kappa, Kendall's\nW, and Bland--Altman statistics. ChatGPT awarded systematically lower marks\nthan human graders with a mean difference (bias) of -1.34 on a 10-point scale.\nICC values indicated poor individual-level agreement (ICC1 = 0.086), and\nCohen's kappa (-0.0786) suggested no meaningful agreement. Variance component\nanalysis showed minimal variability among the five ChatGPT sessions (G-value =\n0.87), indicating internal consistency but divergence from the human grader.\nThe poorest agreement was observed for evaluative and analytic items, whereas\nchecklist and prescriptive rubric items had less disagreement. We caution\nagainst the use of LLMs in grading postgraduate coursework. Over 60% of\nChatGPT-assigned grades differed from human grades by more than acceptable\nboundaries for high-stakes assessments."}
{"id": "2505.05159", "pdf": "https://arxiv.org/pdf/2505.05159", "abs": "https://arxiv.org/abs/2505.05159", "authors": ["Linhan Ma", "Dake Guo", "He Wang", "Jin Xu", "Lei Xie"], "title": "FlexSpeech: Towards Stable, Controllable and Expressive Text-to-Speech", "categories": ["eess.AS"], "comment": "10 pages, 5 figures", "summary": "Current speech generation research can be categorized into two primary\nclasses: non-autoregressive and autoregressive. The fundamental distinction\nbetween these approaches lies in the duration prediction strategy employed for\npredictable-length sequences. The NAR methods ensure stability in speech\ngeneration by explicitly and independently modeling the duration of each\nphonetic unit. Conversely, AR methods employ an autoregressive paradigm to\npredict the compressed speech token by implicitly modeling duration with Markov\nproperties. Although this approach improves prosody, it does not provide the\nstructural guarantees necessary for stability. To simultaneously address the\nissues of stability and naturalness in speech generation, we propose\nFlexSpeech, a stable, controllable, and expressive TTS model. The motivation\nbehind FlexSpeech is to incorporate Markov dependencies and preference\noptimization directly on the duration predictor to boost its naturalness while\nmaintaining explicit modeling of the phonetic units to ensure stability.\nSpecifically, we decompose the speech generation task into two components: an\nAR duration predictor and a NAR acoustic model. The acoustic model is trained\non a substantial amount of data to learn to render audio more stably, given\nreference audio prosody and phone durations. The duration predictor is\noptimized in a lightweight manner for different stylistic variations, thereby\nenabling rapid style transfer while maintaining a decoupled relationship with\nthe specified speaker timbre. Experimental results demonstrate that our\napproach achieves SOTA stability and naturalness in zero-shot TTS. More\nimportantly, when transferring to a specific stylistic domain, we can\naccomplish lightweight optimization of the duration module solely with about\n100 data samples, without the need to adjust the acoustic model, thereby\nenabling rapid and stable style transfer."}
{"id": "2505.04741", "pdf": "https://arxiv.org/pdf/2505.04741", "abs": "https://arxiv.org/abs/2505.04741", "authors": ["Kenneth Li", "Yida Chen", "Fernanda Viégas", "Martin Wattenberg"], "title": "When Bad Data Leads to Good Models", "categories": ["cs.LG", "cs.AI", "cs.CL"], "comment": "ICML 2025", "summary": "In large language model (LLM) pretraining, data quality is believed to\ndetermine model quality. In this paper, we re-examine the notion of \"quality\"\nfrom the perspective of pre- and post-training co-design. Specifically, we\nexplore the possibility that pre-training on more toxic data can lead to better\ncontrol in post-training, ultimately decreasing a model's output toxicity.\nFirst, we use a toy experiment to study how data composition affects the\ngeometry of features in the representation space. Next, through controlled\nexperiments with Olmo-1B models trained on varying ratios of clean and toxic\ndata, we find that the concept of toxicity enjoys a less entangled linear\nrepresentation as the proportion of toxic data increases. Furthermore, we show\nthat although toxic data increases the generational toxicity of the base model,\nit also makes the toxicity easier to remove. Evaluations on Toxigen and Real\nToxicity Prompts demonstrate that models trained on toxic data achieve a better\ntrade-off between reducing generational toxicity and preserving general\ncapabilities when detoxifying techniques such as inference-time intervention\n(ITI) are applied. Our findings suggest that, with post-training taken into\naccount, bad data may lead to good models."}
{"id": "2505.04720", "pdf": "https://arxiv.org/pdf/2505.04720", "abs": "https://arxiv.org/abs/2505.04720", "authors": ["Evangelia Christodoulou", "Annika Reinke", "Pascaline Andrè", "Patrick Godau", "Piotr Kalinowski", "Rola Houhou", "Selen Erkan", "Carole H. Sudre", "Ninon Burgos", "Sofiène Boutaj", "Sophie Loizillon", "Maëlys Solal", "Veronika Cheplygina", "Charles Heitz", "Michal Kozubek", "Michela Antonelli", "Nicola Rieke", "Antoine Gilson", "Leon D. Mayer", "Minu D. Tizabi", "M. Jorge Cardoso", "Amber Simpson", "Annette Kopp-Schneider", "Gaël Varoquaux", "Olivier Colliot", "Lena Maier-Hein"], "title": "False Promises in Medical Imaging AI? Assessing Validity of Outperformance Claims", "categories": ["cs.CV"], "comment": null, "summary": "Performance comparisons are fundamental in medical imaging Artificial\nIntelligence (AI) research, often driving claims of superiority based on\nrelative improvements in common performance metrics. However, such claims\nfrequently rely solely on empirical mean performance. In this paper, we\ninvestigate whether newly proposed methods genuinely outperform the state of\nthe art by analyzing a representative cohort of medical imaging papers. We\nquantify the probability of false claims based on a Bayesian approach that\nleverages reported results alongside empirically estimated model congruence to\nestimate whether the relative ranking of methods is likely to have occurred by\nchance. According to our results, the majority (>80%) of papers claims\noutperformance when introducing a new method. Our analysis further revealed a\nhigh probability (>5%) of false outperformance claims in 86% of classification\npapers and 53% of segmentation papers. These findings highlight a critical flaw\nin current benchmarking practices: claims of outperformance in medical imaging\nAI are frequently unsubstantiated, posing a risk of misdirecting future\nresearch efforts."}
{"id": "2505.04736", "pdf": "https://arxiv.org/pdf/2505.04736", "abs": "https://arxiv.org/abs/2505.04736", "authors": ["Sutapa Dey Tithi", "Arun Kumar Ramesh", "Clara DiMarco", "Xiaoyi Tian", "Nazia Alam", "Kimia Fazeli", "Tiffany Barnes"], "title": "The Promise and Limits of LLMs in Constructing Proofs and Hints for Logic Problems in Intelligent Tutoring Systems", "categories": ["cs.AI"], "comment": null, "summary": "Intelligent tutoring systems have demonstrated effectiveness in teaching\nformal propositional logic proofs, but their reliance on template-based\nexplanations limits their ability to provide personalized student feedback.\nWhile large language models (LLMs) offer promising capabilities for dynamic\nfeedback generation, they risk producing hallucinations or pedagogically\nunsound explanations. We evaluated the stepwise accuracy of LLMs in\nconstructing multi-step symbolic logic proofs, comparing six prompting\ntechniques across four state-of-the-art LLMs on 358 propositional logic\nproblems. Results show that DeepSeek-V3 achieved superior performance with\n84.4% accuracy on stepwise proof construction and excelled particularly in\nsimpler rules. We further used the best-performing LLM to generate explanatory\nhints for 1,050 unique student problem-solving states from a logic ITS and\nevaluated them on 4 criteria with both an LLM grader and human expert ratings\non a 20% sample. Our analysis finds that LLM-generated hints were 75% accurate\nand rated highly by human evaluators on consistency and clarity, but did not\nperform as well explaining why the hint was provided or its larger context. Our\nresults demonstrate that LLMs may be used to augment tutoring systems with\nlogic tutoring hints, but requires additional modifications to ensure accuracy\nand pedagogical appropriateness."}
{"id": "2505.05229", "pdf": "https://arxiv.org/pdf/2505.05229", "abs": "https://arxiv.org/abs/2505.05229", "authors": ["Andrea Asperti", "Leonardo Dessì", "Maria Chiara Tonetti", "Nico Wu"], "title": "Does CLIP perceive art the same way we do?", "categories": ["cs.CV", "cs.MM", "68T45, 68T07 (Primary) 68T50, 68U10 (Secondary)", "I.2.7; I.2.10"], "comment": null, "summary": "CLIP has emerged as a powerful multimodal model capable of connecting images\nand text through joint embeddings, but to what extent does it \"see\" the same\nway humans do - especially when interpreting artworks? In this paper, we\ninvestigate CLIP's ability to extract high-level semantic and stylistic\ninformation from paintings, including both human-created and AI-generated\nimagery. We evaluate its perception across multiple dimensions: content, scene\nunderstanding, artistic style, historical period, and the presence of visual\ndeformations or artifacts. By designing targeted probing tasks and comparing\nCLIP's responses to human annotations and expert benchmarks, we explore its\nalignment with human perceptual and contextual understanding. Our findings\nreveal both strengths and limitations in CLIP's visual representations,\nparticularly in relation to aesthetic cues and artistic intent. We further\ndiscuss the implications of these insights for using CLIP as a guidance\nmechanism during generative processes, such as style transfer or prompt-based\nimage synthesis. Our work highlights the need for deeper interpretability in\nmultimodal systems, especially when applied to creative domains where nuance\nand subjectivity play a central role."}
{"id": "2505.05029", "pdf": "https://arxiv.org/pdf/2505.05029", "abs": "https://arxiv.org/abs/2505.05029", "authors": ["Siyue Ren", "Wanli Fu", "Xinkun Zou", "Chen Shen", "Yi Cai", "Chen Chu", "Zhen Wang", "Shuyue Hu"], "title": "A Reputation System for Large Language Model-based Multi-agent Systems to Avoid the Tragedy of the Commons", "categories": ["cs.AI", "cs.MA"], "comment": null, "summary": "The tragedy of the commons, where individual self-interest leads to\ncollectively disastrous outcomes, is a pervasive challenge in human society.\nRecent studies have demonstrated that similar phenomena can arise in generative\nmulti-agent systems (MASs). To address this challenge, this paper explores the\nuse of reputation systems as a remedy. We propose RepuNet, a dynamic,\ndual-level reputation framework that models both agent-level reputation\ndynamics and system-level network evolution. Specifically, driven by direct\ninteractions and indirect gossip, agents form reputations for both themselves\nand their peers, and decide whether to connect or disconnect other agents for\nfuture interactions. Through two distinct scenarios, we show that RepuNet\neffectively mitigates the 'tragedy of the commons', promoting and sustaining\ncooperation in generative MASs. Moreover, we find that reputation systems can\ngive rise to rich emergent behaviors in generative MASs, such as the formation\nof cooperative clusters, the social isolation of exploitative agents, and the\npreference for sharing positive gossip rather than negative ones."}
{"id": "2505.05078", "pdf": "https://arxiv.org/pdf/2505.05078", "abs": "https://arxiv.org/abs/2505.05078", "authors": ["Silvan Peter", "Patricia Hu", "Gerhard Widmer"], "title": "Pairing Real-Time Piano Transcription with Symbol-level Tracking for Precise and Robust Score Following", "categories": ["cs.SD", "eess.AS"], "comment": "5 pages, 3 tables, 2 pseudocodes, to be published at the Sound and\n  Music Computing Conference 2025", "summary": "Real-time music tracking systems follow a musical performance and at any time\nreport the current position in a corresponding score. Most existing methods\napproach this problem exclusively in the audio domain, typically using online\ntime warping (OLTW) techniques on incoming audio and an audio representation of\nthe score. Audio OLTW techniques have seen incremental improvements both in\nfeatures and model heuristics which reached a performance plateau in the past\nten years. We argue that converting and representing the performance in the\nsymbolic domain -- thereby transforming music tracking into a symbolic task --\ncan be a more effective approach, even when the domain transformation is\nimperfect. Our music tracking system combines two real-time components: one\nhandling audio-to-note transcription and the other a novel symbol-level tracker\nbetween transcribed input and score. We compare the performance of this mixed\naudio-symbolic approach with its equivalent audio-only counterpart, and\ndemonstrate that our method outperforms the latter in terms of both precision,\ni.e., absolute tracking error, and robustness, i.e., tracking success."}
{"id": "2505.04820", "pdf": "https://arxiv.org/pdf/2505.04820", "abs": "https://arxiv.org/abs/2505.04820", "authors": ["Tao Hong", "Zhaoyi Xu", "Se Young Chun", "Luis Hernandez-Garcia", "Jeffrey A. Fessler"], "title": "Convergent Complex Quasi-Newton Proximal Methods for Gradient-Driven Denoisers in Compressed Sensing MRI Reconstruction", "categories": ["eess.IV", "cs.NA", "math.NA", "math.OC"], "comment": "12 pages, 10 figures,\n  https://hongtao-argmin.github.io/CQNPM-GD-CSMRI/", "summary": "In compressed sensing (CS) MRI, model-based methods are pivotal to achieving\naccurate reconstruction. One of the main challenges in model-based methods is\nfinding an effective prior to describe the statistical distribution of the\ntarget image. Plug-and-Play (PnP) and REgularization by Denoising (RED) are two\ngeneral frameworks that use denoisers as the prior. While PnP/RED methods with\nconvolutional neural networks (CNNs) based denoisers outperform classical\nhand-crafted priors in CS MRI, their convergence theory relies on assumptions\nthat do not hold for practical CNNs. The recently developed gradient-driven\ndenoisers offer a framework that bridges the gap between practical performance\nand theoretical guarantees. However, the numerical solvers for the associated\nminimization problem remain slow for CS MRI reconstruction. This paper proposes\na complex quasi-Newton proximal method that achieves faster convergence than\nexisting approaches. To address the complex domain in CS MRI, we propose a\nmodified Hessian estimation method that guarantees Hermitian positive\ndefiniteness. Furthermore, we provide a rigorous convergence analysis of the\nproposed method for nonconvex settings. Numerical experiments on both Cartesian\nand non-Cartesian sampling trajectories demonstrate the effectiveness and\nefficiency of our approach."}
{"id": "2505.04649", "pdf": "https://arxiv.org/pdf/2505.04649", "abs": "https://arxiv.org/abs/2505.04649", "authors": ["Chengzhang Yu", "Yiming Zhang", "Zhixin Liu", "Zenghui Ding", "Yining Sun", "Zhanpeng Jin"], "title": "FRAME: Feedback-Refined Agent Methodology for Enhancing Medical Research Insights", "categories": ["cs.CL"], "comment": "12 pages, 4 figures, 5 table", "summary": "The automation of scientific research through large language models (LLMs)\npresents significant opportunities but faces critical challenges in knowledge\nsynthesis and quality assurance. We introduce Feedback-Refined Agent\nMethodology (FRAME), a novel framework that enhances medical paper generation\nthrough iterative refinement and structured feedback. Our approach comprises\nthree key innovations: (1) A structured dataset construction method that\ndecomposes 4,287 medical papers into essential research components through\niterative refinement; (2) A tripartite architecture integrating Generator,\nEvaluator, and Reflector agents that progressively improve content quality\nthrough metric-driven feedback; and (3) A comprehensive evaluation framework\nthat combines statistical metrics with human-grounded benchmarks. Experimental\nresults demonstrate FRAME's effectiveness, achieving significant improvements\nover conventional approaches across multiple models (9.91% average gain with\nDeepSeek V3, comparable improvements with GPT-4o Mini) and evaluation\ndimensions. Human evaluation confirms that FRAME-generated papers achieve\nquality comparable to human-authored works, with particular strength in\nsynthesizing future research directions. The results demonstrated our work\ncould efficiently assist medical research by building a robust foundation for\nautomated medical research paper generation while maintaining rigorous academic\nstandards."}
{"id": "2505.05216", "pdf": "https://arxiv.org/pdf/2505.05216", "abs": "https://arxiv.org/abs/2505.05216", "authors": ["Julius Richter", "Danilo de Oliveira", "Timo Gerkmann"], "title": "Normalize Everything: A Preconditioned Magnitude-Preserving Architecture for Diffusion-Based Speech Enhancement", "categories": ["eess.AS", "cs.SD"], "comment": "Submitted to WASPAA 2025", "summary": "This paper presents a new framework for diffusion-based speech enhancement.\nOur method employs a Schroedinger bridge to transform the noisy speech\ndistribution into the clean speech distribution. To stabilize and improve\ntraining, we employ time-dependent scalings of the inputs and outputs of the\nnetwork, known as preconditioning. We consider two skip connection\nconfigurations, which either include or omit the current process state in the\ndenoiser's output, enabling the network to predict either environmental noise\nor clean speech. Each approach leads to improved performance on different\nspeech enhancement metrics. To maintain stable magnitude levels and balance\nduring training, we use a magnitude-preserving network architecture that\nnormalizes all activations and network weights to unit length. Additionally, we\npropose learning the contribution of the noisy input within each network block\nfor effective input conditioning. After training, we apply a method to\napproximate different exponential moving average (EMA) profiles and investigate\ntheir effects on the speech enhancement performance. In contrast to image\ngeneration tasks, where longer EMA lengths often enhance mode coverage, we\nobserve that shorter EMA lengths consistently lead to better performance on\nstandard speech enhancement metrics. Code, audio examples, and checkpoints are\navailable online."}
{"id": "2505.04757", "pdf": "https://arxiv.org/pdf/2505.04757", "abs": "https://arxiv.org/abs/2505.04757", "authors": ["Louis Bouvier", "Thibault Prunet", "Vincent Leclère", "Axel Parmentier"], "title": "Primal-dual algorithm for contextual stochastic combinatorial optimization", "categories": ["cs.LG", "math.OC"], "comment": null, "summary": "This paper introduces a novel approach to contextual stochastic optimization,\nintegrating operations research and machine learning to address decision-making\nunder uncertainty. Traditional methods often fail to leverage contextual\ninformation, which underscores the necessity for new algorithms. In this study,\nwe utilize neural networks with combinatorial optimization layers to encode\npolicies. Our goal is to minimize the empirical risk, which is estimated from\npast data on uncertain parameters and contexts. To that end, we present a\nsurrogate learning problem and a generic primal-dual algorithm that is\napplicable to various combinatorial settings in stochastic optimization. Our\napproach extends classic Fenchel-Young loss results and introduces a new\nregularization method using sparse perturbations on the distribution simplex.\nThis allows for tractable updates in the original space and can accommodate\ndiverse objective functions. We demonstrate the linear convergence of our\nalgorithm under certain conditions and provide a bound on the non-optimality of\nthe resulting policy in terms of the empirical risk. Experiments on a\ncontextual stochastic minimum weight spanning tree problem show that our\nalgorithm is efficient and scalable, achieving performance comparable to\nimitation learning of solutions computed using an expensive Lagrangian-based\nheuristic."}
{"id": "2505.04740", "pdf": "https://arxiv.org/pdf/2505.04740", "abs": "https://arxiv.org/abs/2505.04740", "authors": ["Sainath Dey", "Mitul Goswami", "Jashika Sethi", "Prasant Kumar Pattnaik"], "title": "Hyb-KAN ViT: Hybrid Kolmogorov-Arnold Networks Augmented Vision Transformer", "categories": ["cs.CV"], "comment": null, "summary": "This study addresses the inherent limitations of Multi-Layer Perceptrons\n(MLPs) in Vision Transformers (ViTs) by introducing Hybrid Kolmogorov-Arnold\nNetwork (KAN)-ViT (Hyb-KAN ViT), a novel framework that integrates\nwavelet-based spectral decomposition and spline-optimized activation functions,\nprior work has failed to focus on the prebuilt modularity of the ViT\narchitecture and integration of edge detection capabilities of Wavelet\nfunctions. We propose two key modules: Efficient-KAN (Eff-KAN), which replaces\nMLP layers with spline functions and Wavelet-KAN (Wav-KAN), leveraging\northogonal wavelet transforms for multi-resolution feature extraction. These\nmodules are systematically integrated in ViT encoder layers and classification\nheads to enhance spatial-frequency modeling while mitigating computational\nbottlenecks. Experiments on ImageNet-1K (Image Recognition), COCO (Object\nDetection and Instance Segmentation), and ADE20K (Semantic Segmentation)\ndemonstrate state-of-the-art performance with Hyb-KAN ViT. Ablation studies\nvalidate the efficacy of wavelet-driven spectral priors in segmentation and\nspline-based efficiency in detection tasks. The framework establishes a new\nparadigm for balancing parameter efficiency and multi-scale representation in\nvision architectures."}
{"id": "2505.04822", "pdf": "https://arxiv.org/pdf/2505.04822", "abs": "https://arxiv.org/abs/2505.04822", "authors": ["Lior Fox", "Yonatan Loewenstein"], "title": "Is there Value in Reinforcement Learning?", "categories": ["cs.AI", "cs.LG"], "comment": "Accepted to The 6th Multidisciplinary Conference on Reinforcement\n  Learning and Decision Making (RLDM 2025)", "summary": "Action-values play a central role in popular Reinforcement Learing (RL)\nmodels of behavior. Yet, the idea that action-values are explicitly represented\nhas been extensively debated. Critics had therefore repeatedly suggested that\npolicy-gradient (PG) models should be favored over value-based (VB) ones, as a\npotential solution for this dilemma. Here we argue that this solution is\nunsatisfying. This is because PG methods are not, in fact, \"Value-free\" --\nwhile they do not rely on an explicit representation of Value for acting\n(stimulus-response mapping), they do require it for learning. Hence, switching\nto PG models is, per se, insufficient for eliminating Value from models of\nbehavior. More broadly, the requirement for a representation of Value stems\nfrom the underlying assumptions regarding the optimization objective posed by\nthe standard RL framework, not from the particular algorithm chosen to solve\nit. Previous studies mostly took these standard RL assumptions for granted, as\npart of their conceptualization or problem modeling, while debating the\ndifferent methods used to optimize it (i.e., PG or VB). We propose that,\ninstead, the focus of the debate should shift to critically evaluating the\nunderlying modeling assumptions. Such evaluation is particularly important from\nan experimental perspective. Indeed, the very notion of Value must be\nreconsidered when standard assumptions (e.g., risk neutrality,\nfull-observability, Markovian environment, exponential discounting) are\nrelaxed, as is likely in natural settings. Finally, we use the Value debate as\na case study to argue in favor of a more nuanced, algorithmic rather than\nstatistical, view of what constitutes \"a model\" in cognitive sciences. Our\nanalysis suggests that besides \"parametric\" statistical complexity, additional\naspects such as computational complexity must also be taken into account when\nevaluating model complexity."}
{"id": "2404.04545", "pdf": "https://arxiv.org/pdf/2404.04545", "abs": "https://arxiv.org/abs/2404.04545", "authors": ["Weize Quan", "Yunfei Feng", "Ming Zhou", "Yunzhen Zhao", "Tong Wang", "Dong-Ming Yan"], "title": "TCAN: Text-oriented Cross Attention Network for Multimodal Sentiment Analysis", "categories": ["cs.MM", "cs.CL"], "comment": null, "summary": "Multimodal Sentiment Analysis (MSA) endeavors to understand human sentiment\nby leveraging language, visual, and acoustic modalities. Despite the remarkable\nperformance exhibited by previous MSA approaches, the presence of inherent\nmultimodal heterogeneities poses a challenge, with the contribution of\ndifferent modalities varying considerably. Past research predominantly focused\non improving representation learning techniques and feature fusion strategies.\nHowever, many of these efforts overlooked the variation in semantic richness\namong different modalities, treating each modality uniformly. This approach may\nlead to underestimating the significance of strong modalities while\noveremphasizing the importance of weak ones. Motivated by these insights, we\nintroduce a Text-oriented Cross-Attention Network (TCAN), emphasizing the\npredominant role of the text modality in MSA. Specifically, for each multimodal\nsample, by taking unaligned sequences of the three modalities as inputs, we\ninitially allocate the extracted unimodal features into a visual-text and an\nacoustic-text pair. Subsequently, we implement self-attention on the text\nmodality and apply text-queried cross-attention to the visual and acoustic\nmodalities. To mitigate the influence of noise signals and redundant features,\nwe incorporate a gated control mechanism into the framework. Additionally, we\nintroduce unimodal joint learning to gain a deeper understanding of homogeneous\nemotional tendencies across diverse modalities through backpropagation.\nExperimental results demonstrate that TCAN consistently outperforms\nstate-of-the-art MSA methods on two datasets (CMU-MOSI and CMU-MOSEI)."}
{"id": "2505.05108", "pdf": "https://arxiv.org/pdf/2505.05108", "abs": "https://arxiv.org/abs/2505.05108", "authors": ["Zhaohan Feng", "Ruiqi Xue", "Lei Yuan", "Yang Yu", "Ning Ding", "Meiqin Liu", "Bingzhao Gao", "Jian Sun", "Gang Wang"], "title": "Multi-agent Embodied AI: Advances and Future Directions", "categories": ["cs.AI", "cs.MA"], "comment": null, "summary": "Embodied artificial intelligence (Embodied AI) plays a pivotal role in the\napplication of advanced technologies in the intelligent era, where AI systems\nare integrated with physical bodies that enable them to perceive, reason, and\ninteract with their environments. Through the use of sensors for input and\nactuators for action, these systems can learn and adapt based on real-world\nfeedback, allowing them to perform tasks effectively in dynamic and\nunpredictable environments. As techniques such as deep learning (DL),\nreinforcement learning (RL), and large language models (LLMs) mature, embodied\nAI has become a leading field in both academia and industry, with applications\nspanning robotics, healthcare, transportation, and manufacturing. However, most\nresearch has focused on single-agent systems that often assume static, closed\nenvironments, whereas real-world embodied AI must navigate far more complex\nscenarios. In such settings, agents must not only interact with their\nsurroundings but also collaborate with other agents, necessitating\nsophisticated mechanisms for adaptation, real-time learning, and collaborative\nproblem-solving. Despite increasing interest in multi-agent systems, existing\nresearch remains narrow in scope, often relying on simplified models that fail\nto capture the full complexity of dynamic, open environments for multi-agent\nembodied AI. Moreover, no comprehensive survey has systematically reviewed the\nadvancements in this area. As embodied AI rapidly evolves, it is crucial to\ndeepen our understanding of multi-agent embodied AI to address the challenges\npresented by real-world applications. To fill this gap and foster further\ndevelopment in the field, this paper reviews the current state of research,\nanalyzes key contributions, and identifies challenges and future directions,\nproviding insights to guide innovation and progress in this field."}
{"id": "2505.05335", "pdf": "https://arxiv.org/pdf/2505.05335", "abs": "https://arxiv.org/abs/2505.05335", "authors": ["Yusong Wu", "Christos Tsirigotis", "Ke Chen", "Cheng-Zhi Anna Huang", "Aaron Courville", "Oriol Nieto", "Prem Seetharaman", "Justin Salamon"], "title": "FLAM: Frame-Wise Language-Audio Modeling", "categories": ["cs.SD", "eess.AS"], "comment": "Accepted at ICML 2025", "summary": "Recent multi-modal audio-language models (ALMs) excel at text-audio retrieval\nbut struggle with frame-wise audio understanding. Prior works use\ntemporal-aware labels or unsupervised training to improve frame-wise\ncapabilities, but they still lack fine-grained labeling capability to pinpoint\nwhen an event occurs. While traditional sound event detection models can\nprecisely localize events, they are limited to pre-defined categories, making\nthem ineffective for real-world scenarios with out-of-distribution events. In\nthis work, we introduce FLAM, an open-vocabulary contrastive audio-language\nmodel capable of localizing specific sound events. FLAM employs a\nmemory-efficient and calibrated frame-wise objective with logit adjustment to\naddress spurious correlations, such as event dependencies and label imbalances\nduring training. To enable frame-wise supervision, we leverage a large-scale\ndataset with diverse audio events, LLM-generated captions and simulation.\nExperimental results and case studies demonstrate that FLAM significantly\nimproves the open-vocabulary localization capability while maintaining strong\nperformance in global retrieval and downstream tasks."}
{"id": "2505.04913", "pdf": "https://arxiv.org/pdf/2505.04913", "abs": "https://arxiv.org/abs/2505.04913", "authors": ["Gugeong Sung"], "title": "Advanced 3D Imaging Approach to TSV/TGV Metrology and Inspection Using Only Optical Microscopy", "categories": ["eess.IV", "cs.CV", "physics.optics"], "comment": "6 pages, 6 figures, Submitted to arXiv for preprint", "summary": "This paper introduces an innovative approach to silicon and glass via\ninspection, which combines hybrid field microscopy with photometric stereo.\nConventional optical microscopy techniques are generally limited to superficial\ninspections and struggle to effectively visualize the internal structures of\nsilicon and glass vias. By utilizing various lighting conditions for 3D\nreconstruction, the proposed method surpasses these limitations. By integrating\nphotometric stereo to the traditional optical microscopy, the proposed method\nnot only enhances the capability to detect micro-scale defects but also\nprovides a detailed visualization of depth and edge abnormality, which are\ntypically not visible with conventional optical microscopy inspection. The\nexperimental results demonstrated that the proposed method effectively captures\nintricate surface details and internal structures. Quantitative comparisons\nbetween the reconstructed models and actual measurements present the capability\nof the proposed method to significantly improve silicon and glass via\ninspection process. As a result, the proposed method achieves enhanced\ncost-effectiveness while maintaining high accuracy and repeatability,\nsuggesting substantial advancements in silicon and glass via inspection\ntechniques"}
{"id": "2505.04651", "pdf": "https://arxiv.org/pdf/2505.04651", "abs": "https://arxiv.org/abs/2505.04651", "authors": ["Adithya Kulkarni", "Fatimah Alotaibi", "Xinyue Zeng", "Longfeng Wu", "Tong Zeng", "Barry Menglong Yao", "Minqian Liu", "Shuaicheng Zhang", "Lifu Huang", "Dawei Zhou"], "title": "Scientific Hypothesis Generation and Validation: Methods, Datasets, and Future Directions", "categories": ["cs.CL", "cs.LG"], "comment": null, "summary": "Large Language Models (LLMs) are transforming scientific hypothesis\ngeneration and validation by enabling information synthesis, latent\nrelationship discovery, and reasoning augmentation. This survey provides a\nstructured overview of LLM-driven approaches, including symbolic frameworks,\ngenerative models, hybrid systems, and multi-agent architectures. We examine\ntechniques such as retrieval-augmented generation, knowledge-graph completion,\nsimulation, causal inference, and tool-assisted reasoning, highlighting\ntrade-offs in interpretability, novelty, and domain alignment. We contrast\nearly symbolic discovery systems (e.g., BACON, KEKADA) with modern LLM\npipelines that leverage in-context learning and domain adaptation via\nfine-tuning, retrieval, and symbolic grounding. For validation, we review\nsimulation, human-AI collaboration, causal modeling, and uncertainty\nquantification, emphasizing iterative assessment in open-world contexts. The\nsurvey maps datasets across biomedicine, materials science, environmental\nscience, and social science, introducing new resources like AHTech and\nCSKG-600. Finally, we outline a roadmap emphasizing novelty-aware generation,\nmultimodal-symbolic integration, human-in-the-loop systems, and ethical\nsafeguards, positioning LLMs as agents for principled, scalable scientific\ndiscovery."}
{"id": "2505.04996", "pdf": "https://arxiv.org/pdf/2505.04996", "abs": "https://arxiv.org/abs/2505.04996", "authors": ["Jinhe Huang", "Yongkang Cheng", "Yuming Hang", "Gaoge Han", "Jinewei Li", "Jing Zhang", "Xingjian Gu"], "title": "Inter-Diffusion Generation Model of Speakers and Listeners for Effective Communication", "categories": ["cs.GR", "cs.CV", "cs.SD", "eess.AS"], "comment": "accepted by ICMR 2025", "summary": "Full-body gestures play a pivotal role in natural interactions and are\ncrucial for achieving effective communication. Nevertheless, most existing\nstudies primarily focus on the gesture generation of speakers, overlooking the\nvital role of listeners in the interaction process and failing to fully explore\nthe dynamic interaction between them. This paper innovatively proposes an\nInter-Diffusion Generation Model of Speakers and Listeners for Effective\nCommunication. For the first time, we integrate the full-body gestures of\nlisteners into the generation framework. By devising a novel inter-diffusion\nmechanism, this model can accurately capture the complex interaction patterns\nbetween speakers and listeners during communication. In the model construction\nprocess, based on the advanced diffusion model architecture, we innovatively\nintroduce interaction conditions and the GAN model to increase the denoising\nstep size. As a result, when generating gesture sequences, the model can not\nonly dynamically generate based on the speaker's speech information but also\nrespond in realtime to the listener's feedback, enabling synergistic\ninteraction between the two. Abundant experimental results demonstrate that\ncompared with the current state-of-the-art gesture generation methods, the\nmodel we proposed has achieved remarkable improvements in the naturalness,\ncoherence, and speech-gesture synchronization of the generated gestures. In the\nsubjective evaluation experiments, users highly praised the generated\ninteraction scenarios, believing that they are closer to real life human\ncommunication situations. Objective index evaluations also show that our model\noutperforms the baseline methods in multiple key indicators, providing more\npowerful support for effective communication."}
{"id": "2505.04775", "pdf": "https://arxiv.org/pdf/2505.04775", "abs": "https://arxiv.org/abs/2505.04775", "authors": ["Amr Alkhatib", "Roman Bresson", "Henrik Boström", "Michalis Vazirgiannis"], "title": "Prediction via Shapley Value Regression", "categories": ["cs.LG"], "comment": "Accepted at ICML 2025", "summary": "Shapley values have several desirable, theoretically well-supported,\nproperties for explaining black-box model predictions. Traditionally, Shapley\nvalues are computed post-hoc, leading to additional computational cost at\ninference time. To overcome this, a novel method, called ViaSHAP, is proposed,\nthat learns a function to compute Shapley values, from which the predictions\ncan be derived directly by summation. Two approaches to implement the proposed\nmethod are explored; one based on the universal approximation theorem and the\nother on the Kolmogorov-Arnold representation theorem. Results from a\nlarge-scale empirical investigation are presented, showing that ViaSHAP using\nKolmogorov-Arnold Networks performs on par with state-of-the-art algorithms for\ntabular data. It is also shown that the explanations of ViaSHAP are\nsignificantly more accurate than the popular approximator FastSHAP on both\ntabular data and images."}
{"id": "2505.04758", "pdf": "https://arxiv.org/pdf/2505.04758", "abs": "https://arxiv.org/abs/2505.04758", "authors": ["Songsong Duan", "Xi Yang", "Nannan Wang", "Xinbo Gao"], "title": "Lightweight RGB-D Salient Object Detection from a Speed-Accuracy Tradeoff Perspective", "categories": ["cs.CV"], "comment": "Accepted by TIP 2025", "summary": "Current RGB-D methods usually leverage large-scale backbones to improve\naccuracy but sacrifice efficiency. Meanwhile, several existing lightweight\nmethods are difficult to achieve high-precision performance. To balance the\nefficiency and performance, we propose a Speed-Accuracy Tradeoff Network\n(SATNet) for Lightweight RGB-D SOD from three fundamental perspectives: depth\nquality, modality fusion, and feature representation. Concerning depth quality,\nwe introduce the Depth Anything Model to generate high-quality depth maps,which\neffectively alleviates the multi-modal gaps in the current datasets. For\nmodality fusion, we propose a Decoupled Attention Module (DAM) to explore the\nconsistency within and between modalities. Here, the multi-modal features are\ndecoupled into dual-view feature vectors to project discriminable information\nof feature maps. For feature representation, we develop a Dual Information\nRepresentation Module (DIRM) with a bi-directional inverted framework to\nenlarge the limited feature space generated by the lightweight backbones. DIRM\nmodels texture features and saliency features to enrich feature space, and\nemploy two-way prediction heads to optimal its parameters through a\nbi-directional backpropagation. Finally, we design a Dual Feature Aggregation\nModule (DFAM) in the decoder to aggregate texture and saliency features.\nExtensive experiments on five public RGB-D SOD datasets indicate that the\nproposed SATNet excels state-of-the-art (SOTA) CNN-based heavyweight models and\nachieves a lightweight framework with 5.2 M parameters and 415 FPS."}
{"id": "2505.04843", "pdf": "https://arxiv.org/pdf/2505.04843", "abs": "https://arxiv.org/abs/2505.04843", "authors": ["Sebastián R. Castro", "Roberto Campbell", "Nancy Lau", "Octavio Villalobos", "Jiaqi Duan", "Alvaro A. Cardenas"], "title": "Large Language Models are Autonomous Cyber Defenders", "categories": ["cs.AI", "cs.CR", "I.2.0"], "comment": "Presented at IEEE CAI Workshop on Adaptive Cyber Defense 2025.\n  Proceedings to appear", "summary": "Fast and effective incident response is essential to prevent adversarial\ncyberattacks. Autonomous Cyber Defense (ACD) aims to automate incident response\nthrough Artificial Intelligence (AI) agents that plan and execute actions. Most\nACD approaches focus on single-agent scenarios and leverage Reinforcement\nLearning (RL). However, ACD RL-trained agents depend on costly training, and\ntheir reasoning is not always explainable or transferable. Large Language\nModels (LLMs) can address these concerns by providing explainable actions in\ngeneral security contexts. Researchers have explored LLM agents for ACD but\nhave not evaluated them on multi-agent scenarios or interacting with other ACD\nagents. In this paper, we show the first study on how LLMs perform in\nmulti-agent ACD environments by proposing a new integration to the CybORG CAGE\n4 environment. We examine how ACD teams of LLM and RL agents can interact by\nproposing a novel communication protocol. Our results highlight the strengths\nand weaknesses of LLMs and RL and help us identify promising research\ndirections to create, train, and deploy future teams of ACD agents."}
{"id": "2412.18834", "pdf": "https://arxiv.org/pdf/2412.18834", "abs": "https://arxiv.org/abs/2412.18834", "authors": ["Bowen Gu", "Hao Chen", "Ming Lu", "Jie Yao", "Zhan Ma"], "title": "Adaptive Rate Control for Deep Video Compression with Rate-Distortion Prediction", "categories": ["cs.MM", "cs.CV", "cs.IT", "math.IT"], "comment": null, "summary": "Deep video compression has made significant progress in recent years,\nachieving rate-distortion performance that surpasses that of traditional video\ncompression methods. However, rate control schemes tailored for deep video\ncompression have not been well studied. In this paper, we propose a neural\nnetwork-based $\\lambda$-domain rate control scheme for deep video compression,\nwhich determines the coding parameter $\\lambda$ for each to-be-coded frame\nbased on the rate-distortion-$\\lambda$ (R-D-$\\lambda$) relationships directly\nlearned from uncompressed frames, achieving high rate control accuracy\nefficiently without the need for pre-encoding. Moreover, this content-aware\nscheme is able to mitigate inter-frame quality fluctuations and adapt to abrupt\nchanges in video content. Specifically, we introduce two neural network-based\npredictors to estimate the relationship between bitrate and $\\lambda$, as well\nas the relationship between distortion and $\\lambda$ for each frame. Then we\ndetermine the coding parameter $\\lambda$ for each frame to achieve the target\nbitrate. Experimental results demonstrate that our approach achieves high rate\ncontrol accuracy at the mini-GOP level with low time overhead and mitigates\ninter-frame quality fluctuations across video content of varying resolutions."}
{"id": "2505.05119", "pdf": "https://arxiv.org/pdf/2505.05119", "abs": "https://arxiv.org/abs/2505.05119", "authors": ["Chuanbo Hua", "Federico Berto", "Zhikai Zhao", "Jiwoo Son", "Changhyun Kwon", "Jinkyoo Park"], "title": "USPR: Learning a Unified Solver for Profiled Routing", "categories": ["cs.LG", "cs.MA"], "comment": null, "summary": "The Profiled Vehicle Routing Problem (PVRP) extends the classical VRP by\nincorporating vehicle-client-specific preferences and constraints, reflecting\nreal-world requirements such as zone restrictions and service-level\npreferences. While recent reinforcement learning (RL) solvers have shown\npromise, they require retraining for each new profile distribution, suffer from\npoor representation ability, and struggle to generalize to out-of-distribution\ninstances. In this paper, we address these limitations by introducing USPR\n(Unified Solver for Profiled Routing), a novel framework that natively handles\narbitrary profile types. USPR introduces three key innovations: (i) Profile\nEmbeddings (PE) to encode any combination of profile types; (ii) Multi-Head\nProfiled Attention (MHPA), an attention mechanism that models rich interactions\nbetween vehicles and clients; (iii) Profile-aware Score Reshaping (PSR), which\ndynamically adjusts decoder logits using profile scores to improve\ngeneralization. Empirical results on diverse PVRP benchmarks demonstrate that\nUSPR achieves state-of-the-art results among learning-based methods while\noffering significant gains in flexibility and computational efficiency. We make\nour source code publicly available to foster future research at\nhttps://github.com/ai4co/uspr."}
{"id": "2505.05343", "pdf": "https://arxiv.org/pdf/2505.05343", "abs": "https://arxiv.org/abs/2505.05343", "authors": ["Sooyoung Park", "Arda Senocak", "Joon Son Chung"], "title": "Hearing and Seeing Through CLIP: A Framework for Self-Supervised Sound Source Localization", "categories": ["cs.CV", "cs.SD", "eess.AS"], "comment": "Journal Extension of WACV 2024 paper (arXiv:2311.04066). Code is\n  available at https://github.com/swimmiing/ACL-SSL", "summary": "Large-scale vision-language models demonstrate strong multimodal alignment\nand generalization across diverse tasks. Among them, CLIP stands out as one of\nthe most successful approaches. In this work, we extend the application of CLIP\nto sound source localization, proposing a self-supervised method operates\nwithout explicit text input. We introduce a framework that maps audios into\ntokens compatible with CLIP's text encoder, producing audio-driven embeddings.\nThese embeddings are used to generate sounding region masks, from which visual\nfeatures are extracted and aligned with the audio embeddings through a\ncontrastive audio-visual correspondence objective. Our findings show that\nalignment knowledge of pre-trained multimodal foundation model enables our\nmethod to generate more complete and compact localization for sounding objects.\nWe further propose an LLM-guided extension that distills object-aware\naudio-visual scene understanding into the model during training to enhance\nalignment. Extensive experiments across five diverse tasks demonstrate that our\nmethod, in all variants, outperforms state-of-the-art approaches and achieves\nstrong generalization in zero-shot settings."}
{"id": "2505.04959", "pdf": "https://arxiv.org/pdf/2505.04959", "abs": "https://arxiv.org/abs/2505.04959", "authors": ["Tengya Peng", "Ruyi Zha", "Qing Zou"], "title": "MoRe-3DGSMR: Motion-resolved reconstruction framework for free-breathing pulmonary MRI based on 3D Gaussian representation", "categories": ["eess.IV", "cs.CV"], "comment": null, "summary": "This study presents an unsupervised, motion-resolved reconstruction framework\nfor high-resolution, free-breathing pulmonary magnetic resonance imaging (MRI),\nutilizing a three-dimensional Gaussian representation (3DGS). The proposed\nmethod leverages 3DGS to address the challenges of motion-resolved 3D isotropic\npulmonary MRI reconstruction by enabling data smoothing between voxels for\ncontinuous spatial representation. Pulmonary MRI data acquisition is performed\nusing a golden-angle radial sampling trajectory, with respiratory motion\nsignals extracted from the center of k-space in each radial spoke. Based on the\nestimated motion signal, the k-space data is sorted into multiple respiratory\nphases. A 3DGS framework is then applied to reconstruct a reference image\nvolume from the first motion state. Subsequently, a patient-specific\nconvolutional neural network is trained to estimate the deformation vector\nfields (DVFs), which are used to generate the remaining motion states through\nspatial transformation of the reference volume. The proposed reconstruction\npipeline is evaluated on six datasets from six subjects and bench-marked\nagainst three state-of-the-art reconstruction methods. The experimental\nfindings demonstrate that the proposed reconstruction framework effectively\nreconstructs high-resolution, motion-resolved pulmonary MR images. Compared\nwith existing approaches, it achieves superior image quality, reflected by\nhigher signal-to-noise ratio and contrast-to-noise ratio. The proposed\nunsupervised 3DGS-based reconstruction method enables accurate motion-resolved\npulmonary MRI with isotropic spatial resolution. Its superior performance in\nimage quality metrics over state-of-the-art methods highlights its potential as\na robust solution for clinical pulmonary MR imaging."}
{"id": "2505.04653", "pdf": "https://arxiv.org/pdf/2505.04653", "abs": "https://arxiv.org/abs/2505.04653", "authors": ["Khaled Saab", "Jan Freyberg", "Chunjong Park", "Tim Strother", "Yong Cheng", "Wei-Hung Weng", "David G. T. Barrett", "David Stutz", "Nenad Tomasev", "Anil Palepu", "Valentin Liévin", "Yash Sharma", "Roma Ruparel", "Abdullah Ahmed", "Elahe Vedadi", "Kimberly Kanada", "Cian Hughes", "Yun Liu", "Geoff Brown", "Yang Gao", "Sean Li", "S. Sara Mahdavi", "James Manyika", "Katherine Chou", "Yossi Matias", "Avinatan Hassidim", "Dale R. Webster", "Pushmeet Kohli", "S. M. Ali Eslami", "Joëlle Barral", "Adam Rodman", "Vivek Natarajan", "Mike Schaekermann", "Tao Tu", "Alan Karthikesalingam", "Ryutaro Tanno"], "title": "Advancing Conversational Diagnostic AI with Multimodal Reasoning", "categories": ["cs.CL", "cs.AI", "cs.CV", "cs.LG"], "comment": null, "summary": "Large Language Models (LLMs) have demonstrated great potential for conducting\ndiagnostic conversations but evaluation has been largely limited to\nlanguage-only interactions, deviating from the real-world requirements of\nremote care delivery. Instant messaging platforms permit clinicians and\npatients to upload and discuss multimodal medical artifacts seamlessly in\nmedical consultation, but the ability of LLMs to reason over such data while\npreserving other attributes of competent diagnostic conversation remains\nunknown. Here we advance the conversational diagnosis and management\nperformance of the Articulate Medical Intelligence Explorer (AMIE) through a\nnew capability to gather and interpret multimodal data, and reason about this\nprecisely during consultations. Leveraging Gemini 2.0 Flash, our system\nimplements a state-aware dialogue framework, where conversation flow is\ndynamically controlled by intermediate model outputs reflecting patient states\nand evolving diagnoses. Follow-up questions are strategically directed by\nuncertainty in such patient states, leading to a more structured multimodal\nhistory-taking process that emulates experienced clinicians. We compared AMIE\nto primary care physicians (PCPs) in a randomized, blinded, OSCE-style study of\nchat-based consultations with patient actors. We constructed 105 evaluation\nscenarios using artifacts like smartphone skin photos, ECGs, and PDFs of\nclinical documents across diverse conditions and demographics. Our rubric\nassessed multimodal capabilities and other clinically meaningful axes like\nhistory-taking, diagnostic accuracy, management reasoning, communication, and\nempathy. Specialist evaluation showed AMIE to be superior to PCPs on 7/9\nmultimodal and 29/32 non-multimodal axes (including diagnostic accuracy). The\nresults show clear progress in multimodal conversational diagnostic AI, but\nreal-world translation needs further research."}
{"id": "2505.03071", "pdf": "https://arxiv.org/pdf/2505.03071", "abs": "https://arxiv.org/abs/2505.03071", "authors": ["Vincent Dumoulin", "Otilia Stretcu", "Jenny Hamer", "Lauren Harrell", "Rob Laber", "Hugo Larochelle", "Bart van Merriënboer", "Amanda Navine", "Patrick Hart", "Ben Williams", "Timothy A. C. Lamont", "Tries B. Rasak", "Mars Coral Restoration Team", "Sheryn Brodie", "Brendan Doohan", "Phil Eichinski", "Paul Roe", "Lin Schwarzkopf", "Tom Denton"], "title": "The Search for Squawk: Agile Modeling in Bioacoustics", "categories": ["eess.AS"], "comment": null, "summary": "Passive acoustic monitoring (PAM) has shown great promise in helping\necologists understand the health of animal populations and ecosystems. However,\nextracting insights from millions of hours of audio recordings requires the\ndevelopment of specialized recognizers. This is typically a challenging task,\nnecessitating large amounts of training data and machine learning expertise. In\nthis work, we introduce a general, scalable and data-efficient system for\ndeveloping recognizers for novel bioacoustic problems in under an hour. Our\nsystem consists of several key components that tackle problems in previous\nbioacoustic workflows: 1) highly generalizable acoustic embeddings pre-trained\nfor birdsong classification minimize data hunger; 2) indexed audio search\nallows the efficient creation of classifier training datasets, and 3)\nprecomputation of embeddings enables an efficient active learning loop,\nimproving classifier quality iteratively with minimal wait time. Ecologists\nemployed our system in three novel case studies: analyzing coral reef health\nthrough unidentified sounds; identifying juvenile Hawaiian bird calls to\nquantify breeding success and improve endangered species monitoring; and\nChristmas Island bird occupancy modeling. We augment the case studies with\nsimulated experiments which explore the range of design decisions in a\nstructured way and help establish best practices. Altogether these experiments\nshowcase our system's scalability, efficiency, and generalizability, enabling\nscientists to quickly address new bioacoustic challenges."}
{"id": "2505.04796", "pdf": "https://arxiv.org/pdf/2505.04796", "abs": "https://arxiv.org/abs/2505.04796", "authors": ["Jade Garcia Bourrée", "Augustin Godinot", "Martijn De Vos", "Milos Vujasinovic", "Sayan Biswas", "Gilles Tredan", "Erwan Le Merrer", "Anne-Marie Kermarrec"], "title": "Robust ML Auditing using Prior Knowledge", "categories": ["cs.LG"], "comment": "Accepted to the 42nd International Conference on Machine Learning\n  ICML25", "summary": "The rapid adoption of ML decision-making systems across products and services\nhas led to a set of regulations on how such systems should behave and be built.\nAmong all the technical challenges to enforcing these regulations, one crucial,\nyet under-explored problem is the risk of manipulation while these systems are\nbeing audited for fairness. This manipulation occurs when a platform\ndeliberately alters its answers to a regulator to pass an audit without\nmodifying its answers to other users. In this paper, we introduce a novel\napproach to manipulation-proof auditing by taking into account the auditor's\nprior knowledge of the task solved by the platform. We first demonstrate that\nregulators must not rely on public priors (e.g. a public dataset), as platforms\ncould easily fool the auditor in such cases. We then formally establish the\nconditions under which an auditor can prevent audit manipulations using prior\nknowledge about the ground truth. Finally, our experiments with two standard\ndatasets exemplify the maximum level of unfairness a platform can hide before\nbeing detected as malicious. Our formalization and generalization of\nmanipulation-proof auditing with a prior opens up new research directions for\nmore robust fairness audits."}
{"id": "2505.04769", "pdf": "https://arxiv.org/pdf/2505.04769", "abs": "https://arxiv.org/abs/2505.04769", "authors": ["Ranjan Sapkota", "Yang Cao", "Konstantinos I. Roumeliotis", "Manoj Karkee"], "title": "Vision-Language-Action Models: Concepts, Progress, Applications and Challenges", "categories": ["cs.CV"], "comment": "36 pages, 18 Figures, 4 Tables", "summary": "Vision-Language-Action (VLA) models mark a transformative advancement in\nartificial intelligence, aiming to unify perception, natural language\nunderstanding, and embodied action within a single computational framework.\nThis foundational review presents a comprehensive synthesis of recent\nadvancements in Vision-Language-Action models, systematically organized across\nfive thematic pillars that structure the landscape of this rapidly evolving\nfield. We begin by establishing the conceptual foundations of VLA systems,\ntracing their evolution from cross-modal learning architectures to generalist\nagents that tightly integrate vision-language models (VLMs), action planners,\nand hierarchical controllers. Our methodology adopts a rigorous literature\nreview framework, covering over 80 VLA models published in the past three\nyears. Key progress areas include architectural innovations,\nparameter-efficient training strategies, and real-time inference accelerations.\nWe explore diverse application domains such as humanoid robotics, autonomous\nvehicles, medical and industrial robotics, precision agriculture, and augmented\nreality navigation. The review further addresses major challenges across\nreal-time control, multimodal action representation, system scalability,\ngeneralization to unseen tasks, and ethical deployment risks. Drawing from the\nstate-of-the-art, we propose targeted solutions including agentic AI\nadaptation, cross-embodiment generalization, and unified neuro-symbolic\nplanning. In our forward-looking discussion, we outline a future roadmap where\nVLA models, VLMs, and agentic AI converge to power socially aligned, adaptive,\nand general-purpose embodied agents. This work serves as a foundational\nreference for advancing intelligent, real-world robotics and artificial general\nintelligence. >Vision-language-action, Agentic AI, AI Agents, Vision-language\nModels"}
{"id": "2505.04851", "pdf": "https://arxiv.org/pdf/2505.04851", "abs": "https://arxiv.org/abs/2505.04851", "authors": ["Viacheslav Vasilev", "Vladimir Arkhipkin", "Julia Agafonova", "Tatiana Nikulina", "Evelina Mironova", "Alisa Shichanina", "Nikolai Gerasimenko", "Mikhail Shoytov", "Denis Dimitrov"], "title": "CRAFT: Cultural Russian-Oriented Dataset Adaptation for Focused Text-to-Image Generation", "categories": ["cs.AI", "cs.CL", "cs.CV", "cs.CY", "cs.LG"], "comment": "This is arxiv version of the paper which was accepted for the Doklady\n  Mathematics Journal in 2024", "summary": "Despite the fact that popular text-to-image generation models cope well with\ninternational and general cultural queries, they have a significant knowledge\ngap regarding individual cultures. This is due to the content of existing large\ntraining datasets collected on the Internet, which are predominantly based on\nWestern European or American popular culture. Meanwhile, the lack of cultural\nadaptation of the model can lead to incorrect results, a decrease in the\ngeneration quality, and the spread of stereotypes and offensive content. In an\neffort to address this issue, we examine the concept of cultural code and\nrecognize the critical importance of its understanding by modern image\ngeneration models, an issue that has not been sufficiently addressed in the\nresearch community to date. We propose the methodology for collecting and\nprocessing the data necessary to form a dataset based on the cultural code, in\nparticular the Russian one. We explore how the collected data affects the\nquality of generations in the national domain and analyze the effectiveness of\nour approach using the Kandinsky 3.1 text-to-image model. Human evaluation\nresults demonstrate an increase in the level of awareness of Russian culture in\nthe model."}
{"id": "2505.05262", "pdf": "https://arxiv.org/pdf/2505.05262", "abs": "https://arxiv.org/abs/2505.05262", "authors": ["Andreas Kontogiannis", "Konstantinos Papathanasiou", "Yi Shen", "Giorgos Stamou", "Michael M. Zavlanos", "George Vouros"], "title": "Enhancing Cooperative Multi-Agent Reinforcement Learning with State Modelling and Adversarial Exploration", "categories": ["cs.LG", "cs.AI", "cs.MA"], "comment": "Accepted (Poster) at ICML 2025", "summary": "Learning to cooperate in distributed partially observable environments with\nno communication abilities poses significant challenges for multi-agent deep\nreinforcement learning (MARL). This paper addresses key concerns in this\ndomain, focusing on inferring state representations from individual agent\nobservations and leveraging these representations to enhance agents'\nexploration and collaborative task execution policies. To this end, we propose\na novel state modelling framework for cooperative MARL, where agents infer\nmeaningful belief representations of the non-observable state, with respect to\noptimizing their own policies, while filtering redundant and less informative\njoint state information. Building upon this framework, we propose the MARL SMPE\nalgorithm. In SMPE, agents enhance their own policy's discriminative abilities\nunder partial observability, explicitly by incorporating their beliefs into the\npolicy network, and implicitly by adopting an adversarial type of exploration\npolicies which encourages agents to discover novel, high-value states while\nimproving the discriminative abilities of others. Experimentally, we show that\nSMPE outperforms state-of-the-art MARL algorithms in complex fully cooperative\ntasks from the MPE, LBF, and RWARE benchmarks."}
{"id": "2305.15601", "pdf": "https://arxiv.org/pdf/2305.15601", "abs": "https://arxiv.org/abs/2305.15601", "authors": ["Michael Gogins"], "title": "Metamathematics of Algorithmic Composition", "categories": ["cs.SD", "eess.AS", "J.5"], "comment": "15 pages, 0 figures. Comments are very welcome", "summary": "This essay recounts my personal journey towards a deeper understanding of the\nmathematical foundations of algorithmic music composition. I do not spend much\ntime on specific mathematical algorithms used by composers; rather, I focus on\ngeneral issues such as fundamental limits and possibilities, by analogy with\nmetalogic, metamathematics, and computability theory. I discuss implications\nfrom these foundations for the future of algorithmic composition."}
{"id": "2505.05041", "pdf": "https://arxiv.org/pdf/2505.05041", "abs": "https://arxiv.org/abs/2505.05041", "authors": ["Chenxi Zhao", "Jianqiang Li", "Qing Zhao", "Jing Bai", "Susana Boluda", "Benoit Delatour", "Lev Stimmer", "Daniel Racoceanu", "Gabriel Jimenez", "Guanghui Fu"], "title": "ADNP-15: An Open-Source Histopathological Dataset for Neuritic Plaque Segmentation in Human Brain Whole Slide Images with Frequency Domain Image Enhancement for Stain Normalization", "categories": ["eess.IV", "cs.CV"], "comment": null, "summary": "Alzheimer's Disease (AD) is a neurodegenerative disorder characterized by\namyloid-beta plaques and tau neurofibrillary tangles, which serve as key\nhistopathological features. The identification and segmentation of these\nlesions are crucial for understanding AD progression but remain challenging due\nto the lack of large-scale annotated datasets and the impact of staining\nvariations on automated image analysis. Deep learning has emerged as a powerful\ntool for pathology image segmentation; however, model performance is\nsignificantly influenced by variations in staining characteristics,\nnecessitating effective stain normalization and enhancement techniques. In this\nstudy, we address these challenges by introducing an open-source dataset\n(ADNP-15) of neuritic plaques (i.e., amyloid deposits combined with a crown of\ndystrophic tau-positive neurites) in human brain whole slide images. We\nestablish a comprehensive benchmark by evaluating five widely adopted deep\nlearning models across four stain normalization techniques, providing deeper\ninsights into their influence on neuritic plaque segmentation. Additionally, we\npropose a novel image enhancement method that improves segmentation accuracy,\nparticularly in complex tissue structures, by enhancing structural details and\nmitigating staining inconsistencies. Our experimental results demonstrate that\nthis enhancement strategy significantly boosts model generalization and\nsegmentation accuracy. All datasets and code are open-source, ensuring\ntransparency and reproducibility while enabling further advancements in the\nfield."}
{"id": "2505.04654", "pdf": "https://arxiv.org/pdf/2505.04654", "abs": "https://arxiv.org/abs/2505.04654", "authors": ["Yehor Tereshchenko", "Mika Hämäläinen"], "title": "A Comparative Analysis of Ethical and Safety Gaps in LLMs using Relative Danger Coefficient", "categories": ["cs.CL"], "comment": null, "summary": "Artificial Intelligence (AI) and Large Language Models (LLMs) have rapidly\nevolved in recent years, showcasing remarkable capabilities in natural language\nunderstanding and generation. However, these advancements also raise critical\nethical questions regarding safety, potential misuse, discrimination and\noverall societal impact. This article provides a comparative analysis of the\nethical performance of various AI models, including the brand new\nDeepSeek-V3(R1 with reasoning and without), various GPT variants (4o, 3.5\nTurbo, 4 Turbo, o1/o3 mini) and Gemini (1.5 flash, 2.0 flash and 2.0 flash exp)\nand highlights the need for robust human oversight, especially in situations\nwith high stakes. Furthermore, we present a new metric for calculating harm in\nLLMs called Relative Danger Coefficient (RDC)."}
{"id": "2501.15858", "pdf": "https://arxiv.org/pdf/2501.15858", "abs": "https://arxiv.org/abs/2501.15858", "authors": ["Eunjung Yeo", "Julie Liss", "Visar Berisha", "David Mortensen"], "title": "Applications of Artificial Intelligence for Cross-language Intelligibility Assessment of Dysarthric Speech", "categories": ["cs.CL", "cs.SD", "eess.AS"], "comment": "14 pages, 2 figure, 2 tables", "summary": "Purpose: Speech intelligibility is a critical outcome in the assessment and\nmanagement of dysarthria, yet most research and clinical practices have focused\non English, limiting their applicability across languages. This commentary\nintroduces a conceptual framework--and a demonstration of how it can be\nimplemented--leveraging artificial intelligence (AI) to advance cross-language\nintelligibility assessment of dysarthric speech. Method: We propose a\ntwo-tiered conceptual framework consisting of a universal speech model that\nencodes dysarthric speech into acoustic-phonetic representations, followed by a\nlanguage-specific intelligibility assessment model that interprets these\nrepresentations within the phonological or prosodic structures of the target\nlanguage. We further identify barriers to cross-language intelligibility\nassessment of dysarthric speech, including data scarcity, annotation\ncomplexity, and limited linguistic insights into dysarthric speech, and outline\npotential AI-driven solutions to overcome these challenges. Conclusion:\nAdvancing cross-language intelligibility assessment of dysarthric speech\nnecessitates models that are both efficient and scalable, yet constrained by\nlinguistic rules to ensure accurate and language-sensitive assessment. Recent\nadvances in AI provide the foundational tools to support this integration,\nshaping future directions toward generalizable and linguistically informed\nassessment frameworks."}
{"id": "2505.04802", "pdf": "https://arxiv.org/pdf/2505.04802", "abs": "https://arxiv.org/abs/2505.04802", "authors": ["Xiao Wang", "Jong-Youl Choi", "Takuya Kurihaya", "Isaac Lyngaas", "Hong-Jun Yoon", "Ming Fan", "Nasik Muhammad Nafi", "Aristeidis Tsaris", "Ashwin M. Aji", "Maliha Hossain", "Mohamed Wahib", "Dali Wang", "Peter Thornton", "Prasanna Balaprakash", "Moetasim Ashfaq", "Dan Lu"], "title": "ORBIT-2: Scaling Exascale Vision Foundation Models for Weather and Climate Downscaling", "categories": ["cs.LG", "astro-ph.EP", "cs.AI", "cs.DC", "physics.ao-ph"], "comment": null, "summary": "Sparse observations and coarse-resolution climate models limit effective\nregional decision-making, underscoring the need for robust downscaling.\nHowever, existing AI methods struggle with generalization across variables and\ngeographies and are constrained by the quadratic complexity of Vision\nTransformer (ViT) self-attention. We introduce ORBIT-2, a scalable foundation\nmodel for global, hyper-resolution climate downscaling. ORBIT-2 incorporates\ntwo key innovations: (1) Residual Slim ViT (Reslim), a lightweight architecture\nwith residual learning and Bayesian regularization for efficient, robust\nprediction; and (2) TILES, a tile-wise sequence scaling algorithm that reduces\nself-attention complexity from quadratic to linear, enabling long-sequence\nprocessing and massive parallelism. ORBIT-2 scales to 10 billion parameters\nacross 32,768 GPUs, achieving up to 1.8 ExaFLOPS sustained throughput and\n92-98% strong scaling efficiency. It supports downscaling to 0.9 km global\nresolution and processes sequences up to 4.2 billion tokens. On 7 km resolution\nbenchmarks, ORBIT-2 achieves high accuracy with R^2 scores in the range of 0.98\nto 0.99 against observation data."}
{"id": "2505.04787", "pdf": "https://arxiv.org/pdf/2505.04787", "abs": "https://arxiv.org/abs/2505.04787", "authors": ["Sriram Mandalika", "Harsha Vardhan", "Athira Nambiar"], "title": "Replay to Remember (R2R): An Efficient Uncertainty-driven Unsupervised Continual Learning Framework Using Generative Replay", "categories": ["cs.CV", "cs.AI", "cs.LG"], "comment": "Submitted to the 28th European Conference on Artificial Intelligence\n  (ECAI-2025)", "summary": "Continual Learning entails progressively acquiring knowledge from new data\nwhile retaining previously acquired knowledge, thereby mitigating\n``Catastrophic Forgetting'' in neural networks. Our work presents a novel\nuncertainty-driven Unsupervised Continual Learning framework using Generative\nReplay, namely ``Replay to Remember (R2R)''. The proposed R2R architecture\nefficiently uses unlabelled and synthetic labelled data in a balanced\nproportion using a cluster-level uncertainty-driven feedback mechanism and a\nVLM-powered generative replay module. Unlike traditional memory-buffer methods\nthat depend on pretrained models and pseudo-labels, our R2R framework operates\nwithout any prior training. It leverages visual features from unlabeled data\nand adapts continuously using clustering-based uncertainty estimation coupled\nwith dynamic thresholding. Concurrently, a generative replay mechanism along\nwith DeepSeek-R1 powered CLIP VLM produces labelled synthetic data\nrepresentative of past experiences, resembling biological visual thinking that\nreplays memory to remember and act in new, unseen tasks. Extensive experimental\nanalyses are carried out in CIFAR-10, CIFAR-100, CINIC-10, SVHN and\nTinyImageNet datasets. Our proposed R2R approach improves knowledge retention,\nachieving a state-of-the-art performance of 98.13%, 73.06%, 93.41%, 95.18%,\n59.74%, respectively, surpassing state-of-the-art performance by over 4.36%."}
{"id": "2505.04914", "pdf": "https://arxiv.org/pdf/2505.04914", "abs": "https://arxiv.org/abs/2505.04914", "authors": ["John Hawkins"], "title": "Enigme: Generative Text Puzzles for Evaluating Reasoning in Language Models", "categories": ["cs.AI", "cs.CL", "I.2.7"], "comment": "To be published in the proceedings of The 2025 11th International\n  Conference on Engineering, Applied Sciences, and Technology (ICEAST)", "summary": "Transformer-decoder language models are a core innovation in text based\ngenerative artificial intelligence. These models are being deployed as\ngeneral-purpose intelligence systems in many applications. Central to their\nutility is the capacity to understand natural language commands and exploit the\nreasoning embedded in human text corpora to apply some form of reasoning\nprocess to a wide variety of novel tasks. To understand the limitations of this\napproach to generating reasoning we argue that we need to consider the\narchitectural constraints of these systems. Consideration of the latent\nvariable structure of transformer-decoder models allows us to design reasoning\ntasks that should probe the boundary of their capacity to reason. We present\nenigme, an open-source library for generating text-based puzzles to be used in\ntraining and evaluating reasoning skills within transformer-decoder models and\nfuture AI architectures."}
{"id": "2505.05298", "pdf": "https://arxiv.org/pdf/2505.05298", "abs": "https://arxiv.org/abs/2505.05298", "authors": ["Elena Musi", "Nadin Kokciyan", "Khalid Al-Khatib", "Davide Ceolin", "Emmanuelle Dietz", "Klara Gutekunst", "Annette Hautli-Janisz", "Cristian Manuel Santibañez Yañez", "Jodi Schneider", "Jonas Scholz", "Cor Steging", "Jacky Visser", "Henning Wachsmuth"], "title": "Toward Reasonable Parrots: Why Large Language Models Should Argue with Us by Design", "categories": ["cs.CL", "cs.MA"], "comment": null, "summary": "In this position paper, we advocate for the development of conversational\ntechnology that is inherently designed to support and facilitate argumentative\nprocesses. We argue that, at present, large language models (LLMs) are\ninadequate for this purpose, and we propose an ideal technology design aimed at\nenhancing argumentative skills. This involves re-framing LLMs as tools to\nexercise our critical thinking rather than replacing them. We introduce the\nconcept of 'reasonable parrots' that embody the fundamental principles of\nrelevance, responsibility, and freedom, and that interact through argumentative\ndialogical moves. These principles and moves arise out of millennia of work in\nargumentation theory and should serve as the starting point for LLM-based\ntechnology that incorporates basic principles of argumentation."}
{"id": "2504.03373", "pdf": "https://arxiv.org/pdf/2504.03373", "abs": "https://arxiv.org/abs/2504.03373", "authors": ["Zirui Lin", "Masayuki Takigahira", "Naoya Terakado", "Haris Gulzar", "Monikka Roslianna Busto", "Takeharu Eda", "Katsutoshi Itoyama", "Kazuhiro Nakadai", "Hideharu Amano"], "title": "An Efficient GPU-based Implementation for Noise Robust Sound Source Localization", "categories": ["cs.SD", "cs.RO", "eess.AS"], "comment": "6 pages, 2 figures", "summary": "Robot audition, encompassing Sound Source Localization (SSL), Sound Source\nSeparation (SSS), and Automatic Speech Recognition (ASR), enables robots and\nsmart devices to acquire auditory capabilities similar to human hearing.\nDespite their wide applicability, processing multi-channel audio signals from\nmicrophone arrays in SSL involves computationally intensive matrix operations,\nwhich can hinder efficient deployment on Central Processing Units (CPUs),\nparticularly in embedded systems with limited CPU resources. This paper\nintroduces a GPU-based implementation of SSL for robot audition, utilizing the\nGeneralized Singular Value Decomposition-based Multiple Signal Classification\n(GSVD-MUSIC), a noise-robust algorithm, within the HARK platform, an\nopen-source software suite. For a 60-channel microphone array, the proposed\nimplementation achieves significant performance improvements. On the Jetson AGX\nOrin, an embedded device powered by an NVIDIA GPU and ARM Cortex-A78AE v8.2\n64-bit CPUs, we observe speedups of 5648.7x for GSVD calculations and 10.7x for\nthe SSL module, while speedups of 4245.1x for GSVD calculation and 17.3x for\nthe entire SSL module on a server configured with an NVIDIA A100 GPU and AMD\nEPYC 7352 CPUs, making real-time processing feasible for large-scale microphone\narrays and providing ample capacity for real-time processing of potential\nsubsequent machine learning or deep learning tasks."}
{"id": "2505.05054", "pdf": "https://arxiv.org/pdf/2505.05054", "abs": "https://arxiv.org/abs/2505.05054", "authors": ["Navya Sonal Agarwal", "Jan Philipp Schneider", "Kanchana Vaishnavi Gandikota", "Syed Muhammad Kazim", "John Meshreki", "Ivo Ihrke", "Michael Moeller"], "title": "Direct Image Classification from Fourier Ptychographic Microscopy Measurements without Reconstruction", "categories": ["eess.IV", "cs.AI", "cs.CV"], "comment": "ISCS 2025", "summary": "The computational imaging technique of Fourier Ptychographic Microscopy (FPM)\nenables high-resolution imaging with a wide field of view and can serve as an\nextremely valuable tool, e.g. in the classification of cells in medical\napplications. However, reconstructing a high-resolution image from tens or even\nhundreds of measurements is computationally expensive, particularly for a wide\nfield of view. Therefore, in this paper, we investigate the idea of classifying\nthe image content in the FPM measurements directly without performing a\nreconstruction step first. We show that Convolutional Neural Networks (CNN) can\nextract meaningful information from measurement sequences, significantly\noutperforming the classification on a single band-limited image (up to 12 %)\nwhile being significantly more efficient than a reconstruction of a\nhigh-resolution image. Furthermore, we demonstrate that a learned multiplexing\nof several raw measurements allows maintaining the classification accuracy\nwhile reducing the amount of data (and consequently also the acquisition time)\nsignificantly."}
{"id": "2505.04655", "pdf": "https://arxiv.org/pdf/2505.04655", "abs": "https://arxiv.org/abs/2505.04655", "authors": ["Paul Landes", "Jimeng Sun", "Adam Cross"], "title": "Integration of Large Language Models and Traditional Deep Learning for Social Determinants of Health Prediction", "categories": ["cs.CL"], "comment": null, "summary": "Social Determinants of Health (SDoH) are economic, social and personal\ncircumstances that affect or influence an individual's health status. SDoHs\nhave shown to be correlated to wellness outcomes, and therefore, are useful to\nphysicians in diagnosing diseases and in decision-making. In this work, we\nautomatically extract SDoHs from clinical text using traditional deep learning\nand Large Language Models (LLMs) to find the advantages and disadvantages of\neach on an existing publicly available dataset. Our models outperform a\nprevious reference point on a multilabel SDoH classification by 10 points, and\nwe present a method and model to drastically speed up classification (12X\nexecution time) by eliminating expensive LLM processing. The method we present\ncombines a more nimble and efficient solution that leverages the power of the\nLLM for precision and traditional deep learning methods for efficiency. We also\nshow highly performant results on a dataset supplemented with synthetic data\nand several traditional deep learning models that outperform LLMs. Our models\nand methods offer the next iteration of automatic prediction of SDoHs that\nimpact at-risk patients."}
{"id": "2505.04808", "pdf": "https://arxiv.org/pdf/2505.04808", "abs": "https://arxiv.org/abs/2505.04808", "authors": ["Vahan Martirosyan", "Jhony H. Giraldo", "Fragkiskos D. Malliaros"], "title": "Piecewise Constant Spectral Graph Neural Network", "categories": ["cs.LG", "cs.AI"], "comment": "Accepted to TMLR 2025", "summary": "Graph Neural Networks (GNNs) have achieved significant success across various\ndomains by leveraging graph structures in data. Existing spectral GNNs, which\nuse low-degree polynomial filters to capture graph spectral properties, may not\nfully identify the graph's spectral characteristics because of the polynomial's\nsmall degree. However, increasing the polynomial degree is computationally\nexpensive and beyond certain thresholds leads to performance plateaus or\ndegradation. In this paper, we introduce the Piecewise Constant Spectral Graph\nNeural Network(PieCoN) to address these challenges. PieCoN combines constant\nspectral filters with polynomial filters to provide a more flexible way to\nleverage the graph structure. By adaptively partitioning the spectrum into\nintervals, our approach increases the range of spectral properties that can be\neffectively learned. Experiments on nine benchmark datasets, including both\nhomophilic and heterophilic graphs, demonstrate that PieCoN is particularly\neffective on heterophilic datasets, highlighting its potential for a wide range\nof applications."}
{"id": "2505.04788", "pdf": "https://arxiv.org/pdf/2505.04788", "abs": "https://arxiv.org/abs/2505.04788", "authors": ["Bangyan Liao", "Zhenjun Zhao", "Haoang Li", "Yi Zhou", "Yingping Zeng", "Hao Li", "Peidong Liu"], "title": "Convex Relaxation for Robust Vanishing Point Estimation in Manhattan World", "categories": ["cs.CV"], "comment": "Accepted to CVPR 2025 as Award Candidate & Oral Presentation. The\n  first two authors contributed equally to this work. Code:\n  https://github.com/WU-CVGL/GlobustVP", "summary": "Determining the vanishing points (VPs) in a Manhattan world, as a fundamental\ntask in many 3D vision applications, consists of jointly inferring the line-VP\nassociation and locating each VP. Existing methods are, however, either\nsub-optimal solvers or pursuing global optimality at a significant cost of\ncomputing time. In contrast to prior works, we introduce convex relaxation\ntechniques to solve this task for the first time. Specifically, we employ a\n``soft'' association scheme, realized via a truncated multi-selection error,\nthat allows for joint estimation of VPs' locations and line-VP associations.\nThis approach leads to a primal problem that can be reformulated into a\nquadratically constrained quadratic programming (QCQP) problem, which is then\nrelaxed into a convex semidefinite programming (SDP) problem. To solve this SDP\nproblem efficiently, we present a globally optimal outlier-robust iterative\nsolver (called \\textbf{GlobustVP}), which independently searches for one VP and\nits associated lines in each iteration, treating other lines as outliers. After\neach independent update of all VPs, the mutual orthogonality between the three\nVPs in a Manhattan world is reinforced via local refinement. Extensive\nexperiments on both synthetic and real-world data demonstrate that\n\\textbf{GlobustVP} achieves a favorable balance between efficiency, robustness,\nand global optimality compared to previous works. The code is publicly\navailable at https://github.com/WU-CVGL/GlobustVP."}
{"id": "2505.04927", "pdf": "https://arxiv.org/pdf/2505.04927", "abs": "https://arxiv.org/abs/2505.04927", "authors": ["Sebastian Dumbrava"], "title": "Belief Filtering for Epistemic Control in Linguistic State Space", "categories": ["cs.AI"], "comment": "18 pages", "summary": "We examine belief filtering as a mechanism for the epistemic control of\nartificial agents, focusing on the regulation of internal cognitive states\nrepresented as linguistic expressions. This mechanism is developed within the\nSemantic Manifold framework, where belief states are dynamic, structured\nensembles of natural language fragments. Belief filters act as content-aware\noperations on these fragments across various cognitive transitions. This paper\nillustrates how the inherent interpretability and modularity of such a\nlinguistically-grounded cognitive architecture directly enable belief\nfiltering, offering a principled approach to agent regulation. The study\nhighlights the potential for enhancing AI safety and alignment through\nstructured interventions in an agent's internal semantic space and points to\nnew directions for architecturally embedded cognitive governance."}
{"id": "2404.06387", "pdf": "https://arxiv.org/pdf/2404.06387", "abs": "https://arxiv.org/abs/2404.06387", "authors": ["Nancirose Piazza", "Amirhossein Karimia", "Behnia Soleymanib", "Vahid Behzadan", "Stefan Sarkadi"], "title": "Robust Coordination under Misaligned Communication via Power Regularization", "categories": ["cs.MA"], "comment": null, "summary": "Effective communication in Multi-Agent Reinforcement Learning (MARL) can\nsignificantly enhance coordination and collaborative performance in complex and\npartially observable environments. However, reliance on communication can also\nintroduce vulnerabilities when agents are misaligned, potentially leading to\nadversarial interactions that exploit implicit assumptions of cooperative\nintent. Prior work has addressed adversarial behavior through power\nregularization through controlling the influence one agent exerts over another,\nbut has largely overlooked the role of communication in these dynamics. This\npaper introduces Communicative Power Regularization (CPR), extending power\nregularization specifically to communication channels. By explicitly\nquantifying and constraining agents' communicative influence during training,\nCPR actively mitigates vulnerabilities arising from misaligned or adversarial\ncommunications. Evaluations across benchmark environments Red-Door-Blue-Door,\nPredator-Prey, and Grid Coverage demonstrate that our approach significantly\nenhances robustness to adversarial communication while preserving cooperative\nperformance, offering a practical framework for secure and resilient\ncooperative MARL systems."}
{"id": "2505.05073", "pdf": "https://arxiv.org/pdf/2505.05073", "abs": "https://arxiv.org/abs/2505.05073", "authors": ["Shengchun Xiong", "Xiangru Li", "Yunpeng Zhong", "Wanfen Peng"], "title": "RepSNet: A Nucleus Instance Segmentation model based on Boundary Regression and Structural Re-parameterization", "categories": ["eess.IV", "cs.CV"], "comment": "25 pages, 7 figures, 5 tables", "summary": "Pathological diagnosis is the gold standard for tumor diagnosis, and nucleus\ninstance segmentation is a key step in digital pathology analysis and\npathological diagnosis. However, the computational efficiency of the model and\nthe treatment of overlapping targets are the major challenges in the studies of\nthis problem. To this end, a neural network model RepSNet was designed based on\na nucleus boundary regression and a structural re-parameterization scheme for\nsegmenting and classifying the nuclei in H\\&E-stained histopathological images.\nFirst, RepSNet estimates the boundary position information (BPI) of the parent\nnucleus for each pixel. The BPI estimation incorporates the local information\nof the pixel and the contextual information of the parent nucleus. Then, the\nnucleus boundary is estimated by aggregating the BPIs from a series of pixels\nusing a proposed boundary voting mechanism (BVM), and the instance segmentation\nresults are computed from the estimated nucleus boundary using a connected\ncomponent analysis procedure. The BVM intrinsically achieves a kind of\nsynergistic belief enhancement among the BPIs from various pixels. Therefore,\ndifferent from the methods available in literature that obtain nucleus\nboundaries based on a direct pixel recognition scheme, RepSNet computes its\nboundary decisions based on some guidances from macroscopic information using\nan integration mechanism. In addition, RepSNet employs a re-parametrizable\nencoder-decoder structure. This model can not only aggregate features from some\nreceptive fields with various scales which helps segmentation accuracy\nimprovement, but also reduce the parameter amount and computational burdens in\nthe model inference phase through the structural re-parameterization technique.\nExtensive experiments demonstrated the superiorities of RepSNet compared to\nseveral typical benchmark models."}
{"id": "2505.04660", "pdf": "https://arxiv.org/pdf/2505.04660", "abs": "https://arxiv.org/abs/2505.04660", "authors": ["Sana Alamgeer", "Yasine Souissi", "Anne H. H. Ngu"], "title": "AI-Generated Fall Data: Assessing LLMs and Diffusion Model for Wearable Fall Detection", "categories": ["cs.CL", "cs.CV"], "comment": null, "summary": "Training fall detection systems is challenging due to the scarcity of\nreal-world fall data, particularly from elderly individuals. To address this,\nwe explore the potential of Large Language Models (LLMs) for generating\nsynthetic fall data. This study evaluates text-to-motion (T2M, SATO, ParCo) and\ntext-to-text models (GPT4o, GPT4, Gemini) in simulating realistic fall\nscenarios. We generate synthetic datasets and integrate them with four\nreal-world baseline datasets to assess their impact on fall detection\nperformance using a Long Short-Term Memory (LSTM) model. Additionally, we\ncompare LLM-generated synthetic data with a diffusion-based method to evaluate\ntheir alignment with real accelerometer distributions. Results indicate that\ndataset characteristics significantly influence the effectiveness of synthetic\ndata, with LLM-generated data performing best in low-frequency settings (e.g.,\n20Hz) while showing instability in high-frequency datasets (e.g., 200Hz). While\ntext-to-motion models produce more realistic biomechanical data than\ntext-to-text models, their impact on fall detection varies. Diffusion-based\nsynthetic data demonstrates the closest alignment to real data but does not\nconsistently enhance model performance. An ablation study further confirms that\nthe effectiveness of synthetic data depends on sensor placement and fall\nrepresentation. These findings provide insights into optimizing synthetic data\ngeneration for fall detection models."}
{"id": "2505.04823", "pdf": "https://arxiv.org/pdf/2505.04823", "abs": "https://arxiv.org/abs/2505.04823", "authors": ["Junhao Xiong", "Hunter Nisonoff", "Ishan Gaur", "Jennifer Listgarten"], "title": "Guide your favorite protein sequence generative model", "categories": ["cs.LG", "q-bio.BM"], "comment": null, "summary": "Generative machine learning models have begun to transform protein\nengineering, yet no principled framework for conditioning on auxiliary\ninformation in a plug-and-play manner exists; one may want to iteratively\nincorporate experimental feedback, or make use of an existing classifier --\nsuch as for predicting enzyme commission number -- in order to guide the\nsampling of the generative model to generate sequences with desired properties.\nHerein, we present ProteinGuide, a rigorous and general framework to achieve\njust that: through unifying a broad class of protein generative models that\nincludes masked language, (order-agnostic) autoregressive, diffusion and\nflow-matching models, we provide an approach to statistically condition\npre-trained protein generative models. We demonstrate applicability of our\napproach by guiding each of two commonly used protein generative models,\nProteinMPNN and ESM3, to generate amino acid and structure token sequences\nconditioned on several user-specified properties, namely, enhanced stability\nand CATH-labeled fold generation."}
{"id": "2505.04793", "pdf": "https://arxiv.org/pdf/2505.04793", "abs": "https://arxiv.org/abs/2505.04793", "authors": ["Kailash A. Hambarde", "Nzakiese Mbongo", "Pavan Kumar MP", "Satish Mekewad", "Carolina Fernandes", "Gökhan Silahtaroğlu", "Alice Nithya", "Pawan Wasnik", "MD. Rashidunnabi", "Pranita Samale", "Hugo Proença"], "title": "DetReIDX: A Stress-Test Dataset for Real-World UAV-Based Person Recognition", "categories": ["cs.CV"], "comment": null, "summary": "Person reidentification (ReID) technology has been considered to perform\nrelatively well under controlled, ground-level conditions, but it breaks down\nwhen deployed in challenging real-world settings. Evidently, this is due to\nextreme data variability factors such as resolution, viewpoint changes, scale\nvariations, occlusions, and appearance shifts from clothing or session drifts.\nMoreover, the publicly available data sets do not realistically incorporate\nsuch kinds and magnitudes of variability, which limits the progress of this\ntechnology. This paper introduces DetReIDX, a large-scale aerial-ground person\ndataset, that was explicitly designed as a stress test to ReID under real-world\nconditions. DetReIDX is a multi-session set that includes over 13 million\nbounding boxes from 509 identities, collected in seven university campuses from\nthree continents, with drone altitudes between 5.8 and 120 meters. More\nimportant, as a key novelty, DetReIDX subjects were recorded in (at least) two\nsessions on different days, with changes in clothing, daylight and location,\nmaking it suitable to actually evaluate long-term person ReID. Plus, data were\nannotated from 16 soft biometric attributes and multitask labels for detection,\ntracking, ReID, and action recognition. In order to provide empirical evidence\nof DetReIDX usefulness, we considered the specific tasks of human detection and\nReID, where SOTA methods catastrophically degrade performance (up to 80% in\ndetection accuracy and over 70% in Rank-1 ReID) when exposed to DetReIDXs\nconditions. The dataset, annotations, and official evaluation protocols are\npublicly available at https://www.it.ubi.pt/DetReIDX/"}
{"id": "2505.04950", "pdf": "https://arxiv.org/pdf/2505.04950", "abs": "https://arxiv.org/abs/2505.04950", "authors": ["Shireen Kudukkil Manchingal", "Fabio Cuzzolin"], "title": "Position: Epistemic Artificial Intelligence is Essential for Machine Learning Models to Know When They Do Not Know", "categories": ["cs.AI"], "comment": null, "summary": "Despite the impressive achievements of AI, including advancements in\ngenerative models and large language models, there remains a significant gap in\nthe ability of AI to handle uncertainty and generalize beyond the training\ndata. We argue that AI models, especially in autonomous systems, fail to make\nrobust predictions when faced with unfamiliar or adversarial data, as evidenced\nby incidents with autonomous vehicles. Traditional machine learning approaches\nstruggle to address these issues due to an overemphasis on data fitting and\ndomain adaptation. This position paper posits a paradigm shift towards\nepistemic artificial intelligence, emphasizing the need for models to learn not\nonly from what they know but also from their ignorance. This approach, which\nfocuses on recognizing and managing uncertainty, offers a potential solution to\nimprove the resilience and robustness of AI systems, ensuring that they can\nbetter handle unpredictable real-world environments."}
{"id": "2401.11016", "pdf": "https://arxiv.org/pdf/2401.11016", "abs": "https://arxiv.org/abs/2401.11016", "authors": ["Ben Aoki-Sherwood", "Catherine Bregou", "David Liben-Nowell", "Kiran Tomlinson", "Thomas Zeng"], "title": "When the Universe is Too Big: Bounding Consideration Probabilities for Plackett-Luce Rankings", "categories": ["cs.LG", "cs.MA", "econ.EM"], "comment": "25 pages; accepted to AISTATS '25; early version was an extended\n  abstract at AAMAS '24", "summary": "The widely used Plackett-Luce ranking model assumes that individuals rank\nitems by making repeated choices from a universe of items. But in many cases\nthe universe is too big for people to plausibly consider all options. In the\nchoice literature, this issue has been addressed by supposing that individuals\nfirst sample a small consideration set and then choose among the considered\nitems. However, inferring unobserved consideration sets (or item consideration\nprobabilities) in this \"consider then choose\" setting poses significant\nchallenges, because even simple models of consideration with strong\nindependence assumptions are not identifiable, even if item utilities are\nknown. We apply the consider-then-choose framework to top-$k$ rankings, where\nwe assume rankings are constructed according to a Plackett-Luce model after\nsampling a consideration set. While item consideration probabilities remain\nnon-identified in this setting, we prove that we can infer bounds on the\nrelative values of consideration probabilities. Additionally, given a condition\non the expected consideration set size and known item utilities, we derive\nabsolute upper and lower bounds on item consideration probabilities. We also\nprovide algorithms to tighten those bounds on consideration probabilities by\npropagating inferred constraints. Thus, we show that we can learn useful\ninformation about consideration probabilities despite not being able to\nidentify them precisely. We demonstrate our methods on a ranking dataset from a\npsychology experiment with two different ranking tasks (one with fixed\nconsideration sets and one with unknown consideration sets). This combination\nof data allows us to estimate utilities and then learn about unknown\nconsideration probabilities using our bounds."}
{"id": "2505.05112", "pdf": "https://arxiv.org/pdf/2505.05112", "abs": "https://arxiv.org/abs/2505.05112", "authors": ["Xiaolong Niu", "Zanting Ye", "Xu Han", "Yanchao Huang", "Hao Sun", "Hubing Wu", "Lijun Lu"], "title": "MDAA-Diff: CT-Guided Multi-Dose Adaptive Attention Diffusion Model for PET Denoising", "categories": ["eess.IV", "cs.CV"], "comment": null, "summary": "Acquiring high-quality Positron Emission Tomography (PET) images requires\nadministering high-dose radiotracers, which increases radiation exposure risks.\nGenerating standard-dose PET (SPET) from low-dose PET (LPET) has become a\npotential solution. However, previous studies have primarily focused on single\nlow-dose PET denoising, neglecting two critical factors: discrepancies in dose\nresponse caused by inter-patient variability, and complementary anatomical\nconstraints derived from CT images. In this work, we propose a novel CT-Guided\nMulti-dose Adaptive Attention Denoising Diffusion Model (MDAA-Diff) for\nmulti-dose PET denoising. Our approach integrates anatomical guidance and\ndose-level adaptation to achieve superior denoising performance under low-dose\nconditions. Specifically, this approach incorporates a CT-Guided High-frequency\nWavelet Attention (HWA) module, which uses wavelet transforms to separate\nhigh-frequency anatomical boundary features from CT images. These extracted\nfeatures are then incorporated into PET imaging through an adaptive weighted\nfusion mechanism to enhance edge details. Additionally, we propose the\nDose-Adaptive Attention (DAA) module, a dose-conditioned enhancement mechanism\nthat dynamically integrates dose levels into channel-spatial attention weight\ncalculation. Extensive experiments on 18F-FDG and 68Ga-FAPI datasets\ndemonstrate that MDAA-Diff outperforms state-of-the-art approaches in\npreserving diagnostic quality under reduced-dose conditions. Our code is\npublicly available."}
{"id": "2505.04665", "pdf": "https://arxiv.org/pdf/2505.04665", "abs": "https://arxiv.org/abs/2505.04665", "authors": ["Haoyang Feng", "Yanjun Dai", "Yuan Gao"], "title": "Personalized Risks and Regulatory Strategies of Large Language Models in Digital Advertising", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Although large language models have demonstrated the potential for\npersonalized advertising recommendations in experimental environments, in\nactual operations, how advertising recommendation systems can be combined with\nmeasures such as user privacy protection and data security is still an area\nworthy of in-depth discussion. To this end, this paper studies the personalized\nrisks and regulatory strategies of large language models in digital\nadvertising. This study first outlines the principles of Large Language Model\n(LLM), especially the self-attention mechanism based on the Transformer\narchitecture, and how to enable the model to understand and generate natural\nlanguage text. Then, the BERT (Bidirectional Encoder Representations from\nTransformers) model and the attention mechanism are combined to construct an\nalgorithmic model for personalized advertising recommendations and user factor\nrisk protection. The specific steps include: data collection and preprocessing,\nfeature selection and construction, using large language models such as BERT\nfor advertising semantic embedding, and ad recommendations based on user\nportraits. Then, local model training and data encryption are used to ensure\nthe security of user privacy and avoid the leakage of personal data. This paper\ndesigns an experiment for personalized advertising recommendation based on a\nlarge language model of BERT and verifies it with real user data. The\nexperimental results show that BERT-based advertising push can effectively\nimprove the click-through rate and conversion rate of advertisements. At the\nsame time, through local model training and privacy protection mechanisms, the\nrisk of user privacy leakage can be reduced to a certain extent."}
{"id": "2505.04842", "pdf": "https://arxiv.org/pdf/2505.04842", "abs": "https://arxiv.org/abs/2505.04842", "authors": ["Kusha Sareen", "Morgane M Moss", "Alessandro Sordoni", "Rishabh Agarwal", "Arian Hosseini"], "title": "Putting the Value Back in RL: Better Test-Time Scaling by Unifying LLM Reasoners With Verifiers", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Prevalent reinforcement learning~(RL) methods for fine-tuning LLM reasoners,\nsuch as GRPO or Leave-one-out PPO, abandon the learned value function in favor\nof empirically estimated returns. This hinders test-time compute scaling that\nrelies on using the value-function for verification. In this work, we propose\nRL$^V$ that augments any ``value-free'' RL method by jointly training the LLM\nas both a reasoner and a generative verifier using RL-generated data, adding\nverification capabilities without significant overhead. Empirically, RL$^V$\nboosts MATH accuracy by over 20\\% with parallel sampling and enables\n$8-32\\times$ efficient test-time compute scaling compared to the base RL\nmethod. RL$^V$ also exhibits strong generalization capabilities for both\neasy-to-hard and out-of-domain tasks. Furthermore, RL$^V$ achieves\n$1.2-1.6\\times$ higher performance when jointly scaling parallel and sequential\ntest-time compute with a long reasoning R1 model."}
{"id": "2505.04835", "pdf": "https://arxiv.org/pdf/2505.04835", "abs": "https://arxiv.org/abs/2505.04835", "authors": ["Shashank Agnihotri", "David Schader", "Nico Sharei", "Mehmet Ege Kaçar", "Margret Keuper"], "title": "Are Synthetic Corruptions A Reliable Proxy For Real-World Corruptions?", "categories": ["cs.CV"], "comment": "Accepted at CVPR 2025 Workshop on Synthetic Data for Computer Vision", "summary": "Deep learning (DL) models are widely used in real-world applications but\nremain vulnerable to distribution shifts, especially due to weather and\nlighting changes. Collecting diverse real-world data for testing the robustness\nof DL models is resource-intensive, making synthetic corruptions an attractive\nalternative for robustness testing. However, are synthetic corruptions a\nreliable proxy for real-world corruptions? To answer this, we conduct the\nlargest benchmarking study on semantic segmentation models, comparing\nperformance on real-world corruptions and synthetic corruptions datasets. Our\nresults reveal a strong correlation in mean performance, supporting the use of\nsynthetic corruptions for robustness evaluation. We further analyze\ncorruption-specific correlations, providing key insights to understand when\nsynthetic corruptions succeed in representing real-world corruptions.\nOpen-source Code:\nhttps://github.com/shashankskagnihotri/benchmarking_robustness/tree/segmentation_david/semantic_segmentation"}
{"id": "2505.04966", "pdf": "https://arxiv.org/pdf/2505.04966", "abs": "https://arxiv.org/abs/2505.04966", "authors": ["Jaeho Kim", "Yunseok Lee", "Seulki Lee"], "title": "Position: The AI Conference Peer Review Crisis Demands Author Feedback and Reviewer Rewards", "categories": ["cs.AI", "cs.CY"], "comment": "ICML2025 Position Track Oral", "summary": "The peer review process in major artificial intelligence (AI) conferences\nfaces unprecedented challenges with the surge of paper submissions (exceeding\n10,000 submissions per venue), accompanied by growing concerns over review\nquality and reviewer responsibility. This position paper argues for the need to\ntransform the traditional one-way review system into a bi-directional feedback\nloop where authors evaluate review quality and reviewers earn formal\naccreditation, creating an accountability framework that promotes a\nsustainable, high-quality peer review system. The current review system can be\nviewed as an interaction between three parties: the authors, reviewers, and\nsystem (i.e., conference), where we posit that all three parties share\nresponsibility for the current problems. However, issues with authors can only\nbe addressed through policy enforcement and detection tools, and ethical\nconcerns can only be corrected through self-reflection. As such, this paper\nfocuses on reforming reviewer accountability with systematic rewards through\ntwo key mechanisms: (1) a two-stage bi-directional review system that allows\nauthors to evaluate reviews while minimizing retaliatory behavior, (2)a\nsystematic reviewer reward system that incentivizes quality reviewing. We ask\nfor the community's strong interest in these problems and the reforms that are\nneeded to enhance the peer review process."}
{"id": "2407.06454", "pdf": "https://arxiv.org/pdf/2407.06454", "abs": "https://arxiv.org/abs/2407.06454", "authors": ["Maksym Figat", "Cezary Zieliński"], "title": "Simplification of Robotic System Model Analysis by Petri Net Meta-Model Property Transfer", "categories": ["cs.RO", "cs.MA"], "comment": "16 pages", "summary": "This paper presents a simplification of robotic system model analysis due to\nthe transfer of Robotic System Hierarchical Petri Net (RSHPN) meta-model\nproperties onto the model of a designed system. Key contributions include: 1)\nanalysis of RSHPN meta-model properties; 2) decomposition of RSHPN analysis\ninto analysis of individual Petri nets, thus the reduction of state space\nexplosion; and 3) transfer of RSHPN meta-model properties onto the produced\nmodels, hence elimination of the need for full re-analysis of the RSHPN model\nwhen creating new robotic systems. Only task-dependent parts of the model need\nto be analysed. This approach streamlines the analysis thus reducing the design\ntime. Moreover, it produces a specification which is a solid foundation for the\nimplementation of the system. The obtained results highlight the potential of\nPetri nets as a valuable formal framework for analysing robotic system\nproperties."}
{"id": "2505.05208", "pdf": "https://arxiv.org/pdf/2505.05208", "abs": "https://arxiv.org/abs/2505.05208", "authors": ["Muhammad Irfan", "Anum Nawaz", "Riku Klen", "Abdulhamit Subasi", "Tomi Westerlund", "Wei Chen"], "title": "Improved Brain Tumor Detection in MRI: Fuzzy Sigmoid Convolution in Deep Learning", "categories": ["eess.IV", "cs.CV"], "comment": "IEEE IJCNN 2025 has accepted the paper", "summary": "Early detection and accurate diagnosis are essential to improving patient\noutcomes. The use of convolutional neural networks (CNNs) for tumor detection\nhas shown promise, but existing models often suffer from overparameterization,\nwhich limits their performance gains. In this study, fuzzy sigmoid convolution\n(FSC) is introduced along with two additional modules: top-of-the-funnel and\nmiddle-of-the-funnel. The proposed methodology significantly reduces the number\nof trainable parameters without compromising classification accuracy. A novel\nconvolutional operator is central to this approach, effectively dilating the\nreceptive field while preserving input data integrity. This enables efficient\nfeature map reduction and enhances the model's tumor detection capability. In\nthe FSC-based model, fuzzy sigmoid activation functions are incorporated within\nconvolutional layers to improve feature extraction and classification. The\ninclusion of fuzzy logic into the architecture improves its adaptability and\nrobustness. Extensive experiments on three benchmark datasets demonstrate the\nsuperior performance and efficiency of the proposed model. The FSC-based\narchitecture achieved classification accuracies of 99.17%, 99.75%, and 99.89%\non three different datasets. The model employs 100 times fewer parameters than\nlarge-scale transfer learning architectures, highlighting its computational\nefficiency and suitability for detecting brain tumors early. This research\noffers lightweight, high-performance deep-learning models for medical imaging\napplications."}
{"id": "2505.04666", "pdf": "https://arxiv.org/pdf/2505.04666", "abs": "https://arxiv.org/abs/2505.04666", "authors": ["Mohammad Aqib", "Mohd Hamza", "Qipei Mei", "Ying Hei Chui"], "title": "Fine-Tuning Large Language Models and Evaluating Retrieval Methods for Improved Question Answering on Building Codes", "categories": ["cs.CL", "cs.IR", "cs.LG"], "comment": null, "summary": "Building codes are regulations that establish standards for the design,\nconstruction, and safety of buildings to ensure structural integrity, fire\nprotection, and accessibility. They are often extensive, complex, and subject\nto frequent updates, making manual querying challenging and time-consuming. Key\ndifficulties include navigating large volumes of text, interpreting technical\nlanguage, and identifying relevant clauses across different sections. A\npotential solution is to build a Question-Answering (QA) system that answers\nuser queries based on building codes. Among the various methods for building a\nQA system, Retrieval-Augmented Generation (RAG) stands out in performance. RAG\nconsists of two components: a retriever and a language model. This study\nfocuses on identifying a suitable retriever method for building codes and\noptimizing the generational capability of the language model using fine-tuning\ntechniques. We conducted a detailed evaluation of various retrieval methods by\nperforming the retrieval on the National Building Code of Canada (NBCC) and\nexplored the impact of domain-specific fine-tuning on several language models\nusing the dataset derived from NBCC. Our analysis included a comparative\nassessment of different retrievers and the performance of both pre-trained and\nfine-tuned models to determine the efficacy and domain-specific adaptation of\nlanguage models using fine-tuning on the NBCC dataset. Experimental results\nshowed that Elasticsearch proved to be the most robust retriever among all. The\nfindings also indicate that fine-tuning language models on an NBCC-specific\ndataset can enhance their ability to generate contextually relevant responses.\nWhen combined with context retrieved by a powerful retriever like\nElasticsearch, this improvement in LLM performance can optimize the RAG system,\nenabling it to better navigate the complexities of the NBCC."}
{"id": "2505.04873", "pdf": "https://arxiv.org/pdf/2505.04873", "abs": "https://arxiv.org/abs/2505.04873", "authors": ["Minh K. Quan", "Pubudu N. Pathirana", "Mayuri Wijayasundara", "Sujeeva Setunge", "Dinh C. Nguyen", "Christopher G. Brinton", "David J. Love", "H. Vincent Poor"], "title": "Federated Learning for Cyber Physical Systems: A Comprehensive Survey", "categories": ["cs.LG", "cs.AI", "cs.CR", "cs.DC"], "comment": "This work has been accepted by IEEE Communications Surveys &\n  Tutorials", "summary": "The integration of machine learning (ML) in cyber physical systems (CPS) is a\ncomplex task due to the challenges that arise in terms of real-time decision\nmaking, safety, reliability, device heterogeneity, and data privacy. There are\nalso open research questions that must be addressed in order to fully realize\nthe potential of ML in CPS. Federated learning (FL), a distributed approach to\nML, has become increasingly popular in recent years. It allows models to be\ntrained using data from decentralized sources. This approach has been gaining\npopularity in the CPS field, as it integrates computer, communication, and\nphysical processes. Therefore, the purpose of this work is to provide a\ncomprehensive analysis of the most recent developments of FL-CPS, including the\nnumerous application areas, system topologies, and algorithms developed in\nrecent years. The paper starts by discussing recent advances in both FL and\nCPS, followed by their integration. Then, the paper compares the application of\nFL in CPS with its applications in the internet of things (IoT) in further\ndepth to show their connections and distinctions. Furthermore, the article\nscrutinizes how FL is utilized in critical CPS applications, e.g., intelligent\ntransportation systems, cybersecurity services, smart cities, and smart\nhealthcare solutions. The study also includes critical insights and lessons\nlearned from various FL-CPS implementations. The paper's concluding section\ndelves into significant concerns and suggests avenues for further research in\nthis fast-paced and dynamic era."}
{"id": "2505.04838", "pdf": "https://arxiv.org/pdf/2505.04838", "abs": "https://arxiv.org/abs/2505.04838", "authors": ["Youjia Zhang"], "title": "Seeing Cells Clearly: Evaluating Machine Vision Strategies for Microglia Centroid Detection in 3D Images", "categories": ["cs.CV"], "comment": null, "summary": "Microglia are important cells in the brain, and their shape can tell us a lot\nabout brain health. In this project, I test three different tools for finding\nthe center points of microglia in 3D microscope images. The tools include\nilastik, 3D Morph, and Omnipose. I look at how well each one finds the cells\nand how their results compare. My findings show that each tool sees the cells\nin its own way, and this can affect the kind of information we get from the\nimages."}
{"id": "2505.05059", "pdf": "https://arxiv.org/pdf/2505.05059", "abs": "https://arxiv.org/abs/2505.05059", "authors": ["Sandro Junior Della Rovere", "Davide Basso", "Luca Bortolussi", "Mirjana Videnovic-Misic", "Husni Habal"], "title": "Enhancing Reinforcement Learning for the Floorplanning of Analog ICs with Beam Search", "categories": ["cs.AI", "cs.LG"], "comment": "Published in Proceedings of the 21st International Conference on\n  Synthesis, Modeling, Analysis and Simulation Methods, and Applications to\n  Circuit Design (SMACD 2025). 4 pages, 3 figures", "summary": "The layout of analog ICs requires making complex trade-offs, while addressing\ndevice physics and variability of the circuits. This makes full automation with\nlearning-based solutions hard to achieve. However, reinforcement learning (RL)\nhas recently reached significant results, particularly in solving the\nfloorplanning problem. This paper presents a hybrid method that combines RL\nwith a beam (BS) strategy. The BS algorithm enhances the agent's inference\nprocess, allowing for the generation of flexible floorplans by accomodating\nvarious objective weightings, and addressing congestion without without the\nneed for policy retraining or fine-tuning. Moreover, the RL agent's\ngeneralization ability stays intact, along with its efficient handling of\ncircuit features and constraints. Experimental results show approx. 5-85%\nimprovement in area, dead space and half-perimeter wire length compared to a\nstandard RL application, along with higher rewards for the agent. Moreover,\nperformance and efficiency align closely with those of existing\nstate-of-the-art techniques."}
{"id": "2503.12613", "pdf": "https://arxiv.org/pdf/2503.12613", "abs": "https://arxiv.org/abs/2503.12613", "authors": ["Rashid Mushkani", "Hugo Berard", "Shin Koseki"], "title": "Negotiative Alignment: Embracing Disagreement to Achieve Fairer Outcomes -- Insights from Urban Studies", "categories": ["cs.HC", "cs.AI", "cs.CY", "cs.MA"], "comment": "16 pages, 13 figures", "summary": "Cities are not monolithic; they are arenas of negotiation among groups that\nhold varying needs, values, and experiences. Conventional methods of urban\nassessment -- from standardized surveys to AI-driven evaluations -- frequently\nrely on a single consensus metric (e.g., an average measure of inclusivity or\nsafety). Although such aggregations simplify design decisions, they risk\nobscuring the distinct perspectives of marginalized populations. In this paper,\nwe present findings from a community-centered study in Montreal involving 35\nresidents with diverse demographic and social identities, particularly\nwheelchair users, seniors, and LGBTQIA2+ individuals. Using rating and ranking\ntasks on 20 urban sites, we observe that disagreements are systematic rather\nthan random, reflecting structural inequalities, differing cultural values, and\npersonal experiences of safety and accessibility.\n  Based on these empirical insights, we propose negotiative alignment, an AI\nframework that treats disagreement as an essential input to be preserved,\nanalyzed, and addressed. Negotiative alignment builds on pluralistic models by\ndynamically updating stakeholder preferences through multi-agent negotiation\nmechanisms, ensuring no single perspective is marginalized. We outline how this\nframework can be integrated into urban analytics -- and other decision-making\ncontexts -- to retain minority viewpoints, adapt to changing stakeholder\nconcerns, and enhance fairness and accountability. The study demonstrates that\npreserving and engaging with disagreement, rather than striving for an\nartificial consensus, can produce more equitable and responsive AI-driven\noutcomes in urban design."}
{"id": "2505.05248", "pdf": "https://arxiv.org/pdf/2505.05248", "abs": "https://arxiv.org/abs/2505.05248", "authors": ["Jose Angel Nuñez", "Fabian Vazquez", "Diego Adame", "Xiaoyan Fu", "Pengfei Gu", "Bin Fu"], "title": "White Light Specular Reflection Data Augmentation for Deep Learning Polyp Detection", "categories": ["eess.IV", "cs.CV"], "comment": "5 pages, 4 Figures, paper accepted by the ISBI (International\n  Symposium on Biomedical Imaging) 2025 Conference", "summary": "Colorectal cancer is one of the deadliest cancers today, but it can be\nprevented through early detection of malignant polyps in the colon, primarily\nvia colonoscopies. While this method has saved many lives, human error remains\na significant challenge, as missing a polyp could have fatal consequences for\nthe patient. Deep learning (DL) polyp detectors offer a promising solution.\nHowever, existing DL polyp detectors often mistake white light reflections from\nthe endoscope for polyps, which can lead to false positives.To address this\nchallenge, in this paper, we propose a novel data augmentation approach that\nartificially adds more white light reflections to create harder training\nscenarios. Specifically, we first generate a bank of artificial lights using\nthe training dataset. Then we find the regions of the training images that we\nshould not add these artificial lights on. Finally, we propose a sliding window\nmethod to add the artificial light to the areas that fit of the training\nimages, resulting in augmented images. By providing the model with more\nopportunities to make mistakes, we hypothesize that it will also have more\nchances to learn from those mistakes, ultimately improving its performance in\npolyp detection. Experimental results demonstrate the effectiveness of our new\ndata augmentation method."}
{"id": "2505.04671", "pdf": "https://arxiv.org/pdf/2505.04671", "abs": "https://arxiv.org/abs/2505.04671", "authors": ["Yuxin Zhang", "Meihao Fan", "Ju Fan", "Mingyang Yi", "Yuyu Luo", "Jian Tan", "Guoliang Li"], "title": "Reward-SQL: Boosting Text-to-SQL via Stepwise Reasoning and Process-Supervised Rewards", "categories": ["cs.CL", "cs.LG"], "comment": null, "summary": "Recent advances in large language models (LLMs) have significantly improved\nperformance on the Text-to-SQL task by leveraging their powerful reasoning\ncapabilities. To enhance accuracy during the reasoning process, external\nProcess Reward Models (PRMs) can be introduced during training and inference to\nprovide fine-grained supervision. However, if misused, PRMs may distort the\nreasoning trajectory and lead to suboptimal or incorrect SQL generation.To\naddress this challenge, we propose Reward-SQL, a framework that systematically\nexplores how to incorporate PRMs into the Text-to-SQL reasoning process\neffectively. Our approach follows a \"cold start, then PRM supervision\"\nparadigm. Specifically, we first train the model to decompose SQL queries into\nstructured stepwise reasoning chains using common table expressions\n(Chain-of-CTEs), establishing a strong and interpretable reasoning baseline.\nThen, we investigate four strategies for integrating PRMs, and find that\ncombining PRM as an online training signal (GRPO) with PRM-guided inference\n(e.g., best-of-N sampling) yields the best results. Empirically, on the BIRD\nbenchmark, Reward-SQL enables models supervised by a 7B PRM to achieve a 13.1%\nperformance gain across various guidance strategies. Notably, our GRPO-aligned\npolicy model based on Qwen2.5-Coder-7B-Instruct achieves 68.9% accuracy on the\nBIRD development set, outperforming all baseline methods under the same model\nsize. These results demonstrate the effectiveness of Reward-SQL in leveraging\nreward-based supervision for Text-to-SQL reasoning. Our code is publicly\navailable."}
{"id": "2505.04881", "pdf": "https://arxiv.org/pdf/2505.04881", "abs": "https://arxiv.org/abs/2505.04881", "authors": ["Ziqing Qiao", "Yongheng Deng", "Jiali Zeng", "Dong Wang", "Lai Wei", "Fandong Meng", "Jie Zhou", "Ju Ren", "Yaoxue Zhang"], "title": "ConCISE: Confidence-guided Compression in Step-by-step Efficient Reasoning", "categories": ["cs.LG", "cs.AI", "cs.CL"], "comment": null, "summary": "Large Reasoning Models (LRMs) perform strongly in complex reasoning tasks via\nChain-of-Thought (CoT) prompting, but often suffer from verbose outputs caused\nby redundant content, increasing computational overhead, and degrading user\nexperience. Existing compression methods either operate post-hoc pruning,\nrisking disruption to reasoning coherence, or rely on sampling-based selection,\nwhich fails to intervene effectively during generation. In this work, we\nintroduce a confidence-guided perspective to explain the emergence of redundant\nreflection in LRMs, identifying two key patterns: Confidence Deficit, where the\nmodel reconsiders correct steps due to low internal confidence, and Termination\nDelay, where reasoning continues even after reaching a confident answer. Based\non this analysis, we propose ConCISE (Confidence-guided Compression In\nStep-by-step Efficient Reasoning), a framework that simplifies reasoning chains\nby reinforcing the model's confidence during inference, thus preventing the\ngeneration of redundant reflection steps. It integrates Confidence Injection to\nstabilize intermediate steps and Early Stopping to terminate reasoning when\nconfidence is sufficient. Extensive experiments demonstrate that fine-tuning\nLRMs on ConCISE-generated data yields significantly shorter outputs, reducing\nlength by up to approximately 50% under SimPO, while maintaining high task\naccuracy. ConCISE consistently outperforms existing baselines across multiple\nreasoning benchmarks."}
{"id": "2505.04850", "pdf": "https://arxiv.org/pdf/2505.04850", "abs": "https://arxiv.org/abs/2505.04850", "authors": ["Qingyuan Wang", "Guoxin Wang", "Barry Cardiff", "Deepu John"], "title": "ORXE: Orchestrating Experts for Dynamically Configurable Efficiency", "categories": ["cs.CV"], "comment": null, "summary": "This paper presents ORXE, a modular and adaptable framework for achieving\nreal-time configurable efficiency in AI models. By leveraging a collection of\npre-trained experts with diverse computational costs and performance levels,\nORXE dynamically adjusts inference pathways based on the complexity of input\nsamples. Unlike conventional approaches that require complex metamodel\ntraining, ORXE achieves high efficiency and flexibility without complicating\nthe development process. The proposed system utilizes a confidence-based gating\nmechanism to allocate appropriate computational resources for each input. ORXE\nalso supports adjustments to the preference between inference cost and\nprediction performance across a wide range during runtime. We implemented a\ntraining-free ORXE system for image classification tasks, evaluating its\nefficiency and accuracy across various devices. The results demonstrate that\nORXE achieves superior performance compared to individual experts and other\ndynamic models in most cases. This approach can be extended to other\napplications, providing a scalable solution for diverse real-world deployment\nscenarios."}
{"id": "2505.05106", "pdf": "https://arxiv.org/pdf/2505.05106", "abs": "https://arxiv.org/abs/2505.05106", "authors": ["Luca Salvatore Lorello", "Marco Lippi", "Stefano Melacci"], "title": "A Neuro-Symbolic Framework for Sequence Classification with Relational and Temporal Knowledge", "categories": ["cs.AI"], "comment": null, "summary": "One of the goals of neuro-symbolic artificial intelligence is to exploit\nbackground knowledge to improve the performance of learning tasks. However,\nmost of the existing frameworks focus on the simplified scenario where\nknowledge does not change over time and does not cover the temporal dimension.\nIn this work we consider the much more challenging problem of knowledge-driven\nsequence classification where different portions of knowledge must be employed\nat different timesteps, and temporal relations are available. Our experimental\nevaluation compares multi-stage neuro-symbolic and neural-only architectures,\nand it is conducted on a newly-introduced benchmarking framework. Results\ndemonstrate the challenging nature of this novel setting, and also highlight\nunder-explored shortcomings of neuro-symbolic methods, representing a precious\nreference for future research."}
{"id": "2504.13701", "pdf": "https://arxiv.org/pdf/2504.13701", "abs": "https://arxiv.org/abs/2504.13701", "authors": ["Yushan Li", "Jianping He", "Dimos V. Dimarogonas"], "title": "Inverse Inference on Cooperative Control of Networked Dynamical Systems", "categories": ["eess.SY", "cs.MA", "cs.SY"], "comment": "14 pages", "summary": "Recent years have witnessed the rapid advancement of understanding the\ncontrol mechanism of networked dynamical systems (NDSs), which are governed by\ncomponents such as nodal dynamics and topology. This paper reveals that the\ncritical components in continuous-time state feedback cooperative control of\nNDSs can be inferred merely from discrete observations. In particular, we\nadvocate a bi-level inference framework to estimate the global closed-loop\nsystem and extract the components, respectively. The novelty lies in bridging\nthe gap from discrete observations to the continuous-time model and effectively\ndecoupling the concerned components. Specifically, in the first level, we\ndesign a causality-based estimator for the discrete-time closed-loop system\nmatrix, which can achieve asymptotically unbiased performance when the NDS is\nstable. In the second level, we introduce a matrix logarithm based method to\nrecover the continuous-time counterpart matrix, providing new sampling period\nguarantees and establishing the recovery error bound. By utilizing graph\nproperties of the NDS, we develop least square based procedures to decouple the\nconcerned components with up to a scalar ambiguity. Furthermore, we employ\ninverse optimal control techniques to reconstruct the objective function\ndriving the control process, deriving necessary conditions for the solutions.\nNumerical simulations demonstrate the effectiveness of the proposed method."}
{"id": "2505.05291", "pdf": "https://arxiv.org/pdf/2505.05291", "abs": "https://arxiv.org/abs/2505.05291", "authors": ["Benjamin A. Cohen", "Jonathan Fhima", "Meishar Meisel", "Baskin Meital", "Luis Filipe Nakayama", "Eran Berkowitz", "Joachim A. Behar"], "title": "Benchmarking Ophthalmology Foundation Models for Clinically Significant Age Macular Degeneration Detection", "categories": ["eess.IV", "cs.AI", "cs.CV", "q-bio.TO"], "comment": "10 pages, 3 figures", "summary": "Self-supervised learning (SSL) has enabled Vision Transformers (ViTs) to\nlearn robust representations from large-scale natural image datasets, enhancing\ntheir generalization across domains. In retinal imaging, foundation models\npretrained on either natural or ophthalmic data have shown promise, but the\nbenefits of in-domain pretraining remain uncertain. To investigate this, we\nbenchmark six SSL-pretrained ViTs on seven digital fundus image (DFI) datasets\ntotaling 70,000 expert-annotated images for the task of moderate-to-late\nage-related macular degeneration (AMD) identification. Our results show that\niBOT pretrained on natural images achieves the highest out-of-distribution\ngeneralization, with AUROCs of 0.80-0.97, outperforming domain-specific models,\nwhich achieved AUROCs of 0.78-0.96 and a baseline ViT-L with no pretraining,\nwhich achieved AUROCs of 0.68-0.91. These findings highlight the value of\nfoundation models in improving AMD identification and challenge the assumption\nthat in-domain pretraining is necessary. Furthermore, we release BRAMD, an\nopen-access dataset (n=587) of DFIs with AMD labels from Brazil."}
{"id": "2505.04673", "pdf": "https://arxiv.org/pdf/2505.04673", "abs": "https://arxiv.org/abs/2505.04673", "authors": ["Madhur Jindal", "Saurabh Deshpande"], "title": "REVEAL: Multi-turn Evaluation of Image-Input Harms for Vision LLM", "categories": ["cs.CL", "cs.AI"], "comment": "13 pages (8 main), to be published in IJCAI 2025", "summary": "Vision Large Language Models (VLLMs) represent a significant advancement in\nartificial intelligence by integrating image-processing capabilities with\ntextual understanding, thereby enhancing user interactions and expanding\napplication domains. However, their increased complexity introduces novel\nsafety and ethical challenges, particularly in multi-modal and multi-turn\nconversations. Traditional safety evaluation frameworks, designed for\ntext-based, single-turn interactions, are inadequate for addressing these\ncomplexities. To bridge this gap, we introduce the REVEAL (Responsible\nEvaluation of Vision-Enabled AI LLMs) Framework, a scalable and automated\npipeline for evaluating image-input harms in VLLMs. REVEAL includes automated\nimage mining, synthetic adversarial data generation, multi-turn conversational\nexpansion using crescendo attack strategies, and comprehensive harm assessment\nthrough evaluators like GPT-4o.\n  We extensively evaluated five state-of-the-art VLLMs, GPT-4o, Llama-3.2,\nQwen2-VL, Phi3.5V, and Pixtral, across three important harm categories: sexual\nharm, violence, and misinformation. Our findings reveal that multi-turn\ninteractions result in significantly higher defect rates compared to\nsingle-turn evaluations, highlighting deeper vulnerabilities in VLLMs. Notably,\nGPT-4o demonstrated the most balanced performance as measured by our\nSafety-Usability Index (SUI) followed closely by Pixtral. Additionally,\nmisinformation emerged as a critical area requiring enhanced contextual\ndefenses. Llama-3.2 exhibited the highest MT defect rate ($16.55 \\%$) while\nQwen2-VL showed the highest MT refusal rate ($19.1 \\%$)."}
{"id": "2505.04889", "pdf": "https://arxiv.org/pdf/2505.04889", "abs": "https://arxiv.org/abs/2505.04889", "authors": ["Tianzhe Xiao", "Yichen Li", "Yu Zhou", "Yining Qi", "Yi Liu", "Wei Wang", "Haozhao Wang", "Yi Wang", "Ruixuan Li"], "title": "FedRE: Robust and Effective Federated Learning with Privacy Preference", "categories": ["cs.LG", "cs.CR"], "comment": null, "summary": "Despite Federated Learning (FL) employing gradient aggregation at the server\nfor distributed training to prevent the privacy leakage of raw data, private\ninformation can still be divulged through the analysis of uploaded gradients\nfrom clients. Substantial efforts have been made to integrate local\ndifferential privacy (LDP) into the system to achieve a strict privacy\nguarantee. However, existing methods fail to take practical issues into account\nby merely perturbing each sample with the same mechanism while each client may\nhave their own privacy preferences on privacy-sensitive information (PSI),\nwhich is not uniformly distributed across the raw data. In such a case,\nexcessive privacy protection from private-insensitive information can\nadditionally introduce unnecessary noise, which may degrade the model\nperformance. In this work, we study the PSI within data and develop FedRE, that\ncan simultaneously achieve robustness and effectiveness benefits with LDP\nprotection. More specifically, we first define PSI with regard to the privacy\npreferences of each client. Then, we optimize the LDP by allocating less\nprivacy budget to gradients with higher PSI in a layer-wise manner, thus\nproviding a stricter privacy guarantee for PSI. Furthermore, to mitigate the\nperformance degradation caused by LDP, we design a parameter aggregation\nmechanism based on the distribution of the perturbed information. We conducted\nexperiments with text tamper detection on T-SROIE and DocTamper datasets, and\nFedRE achieves competitive performance compared to state-of-the-art methods."}
{"id": "2505.04861", "pdf": "https://arxiv.org/pdf/2505.04861", "abs": "https://arxiv.org/abs/2505.04861", "authors": ["Navin Ranjan", "Andreas Savakis"], "title": "Mix-QSAM: Mixed-Precision Quantization of the Segment Anything Model", "categories": ["cs.CV"], "comment": "12 pages, 2 Figures", "summary": "The Segment Anything Model (SAM) is a popular vision foundation model;\nhowever, its high computational and memory demands make deployment on\nresource-constrained devices challenging. While Post-Training Quantization\n(PTQ) is a practical approach for reducing computational overhead, existing PTQ\nmethods rely on fixed bit-width quantization, leading to suboptimal accuracy\nand efficiency. To address this limitation, we propose Mix-QSAM, a\nmixed-precision PTQ framework for SAM. First, we introduce a layer-wise\nimportance score, derived using Kullback-Leibler (KL) divergence, to quantify\neach layer's contribution to the model's output. Second, we introduce\ncross-layer synergy, a novel metric based on causal mutual information, to\ncapture dependencies between adjacent layers. This ensures that highly\ninterdependent layers maintain similar bit-widths, preventing abrupt precision\nmismatches that degrade feature propagation and numerical stability. Using\nthese metrics, we formulate an Integer Quadratic Programming (IQP) problem to\ndetermine optimal bit-width allocation under model size and bit-operation\nconstraints, assigning higher precision to critical layers while minimizing\nbit-width in less influential layers. Experimental results demonstrate that\nMix-QSAM consistently outperforms existing PTQ methods on instance segmentation\nand object detection tasks, achieving up to 20% higher average precision under\n6-bit and 4-bit mixed-precision settings, while maintaining computational\nefficiency."}
{"id": "2505.05115", "pdf": "https://arxiv.org/pdf/2505.05115", "abs": "https://arxiv.org/abs/2505.05115", "authors": ["Toby Ord"], "title": "Is there a half-life for the success rates of AI agents?", "categories": ["cs.AI", "68T42", "I.2.8"], "comment": "9 pages, 5 figures", "summary": "Building on the recent empirical work of Kwa et al. (2025), I show that\nwithin their suite of research-engineering tasks the performance of AI agents\non longer-duration tasks can be explained by an extremely simple mathematical\nmodel -- a constant rate of failing during each minute a human would take to do\nthe task. This implies an exponentially declining success rate with the length\nof the task and that each agent could be characterised by its own half-life.\nThis empirical regularity allows us to estimate the success rate for an agent\nat different task lengths. And the fact that this model is a good fit for the\ndata is suggestive of the underlying causes of failure on longer tasks -- that\nthey involve increasingly large sets of subtasks where failing any one fails\nthe task. Whether this model applies more generally on other suites of tasks is\nunknown and an important subject for further work."}
{"id": "2505.03961", "pdf": "https://arxiv.org/pdf/2505.03961", "abs": "https://arxiv.org/abs/2505.03961", "authors": ["Gerrit Großmann", "Larisa Ivanova", "Sai Leela Poduru", "Mohaddeseh Tabrizian", "Islam Mesabah", "David A. Selby", "Sebastian J. Vollmer"], "title": "The Power of Stories: Narrative Priming Shapes How LLM Agents Collaborate and Compete", "categories": ["cs.AI", "cs.CL", "cs.MA", "I.2.11; I.2.7; I.6; J.4"], "comment": "16 pages, 8 figures. Code available at\n  https://github.com/storyagents25/story-agents", "summary": "According to Yuval Noah Harari, large-scale human cooperation is driven by\nshared narratives that encode common beliefs and values. This study explores\nwhether such narratives can similarly nudge LLM agents toward collaboration. We\nuse a finitely repeated public goods game in which LLM agents choose either\ncooperative or egoistic spending strategies. We prime agents with stories\nhighlighting teamwork to different degrees and test how this influences\nnegotiation outcomes. Our experiments explore four questions:(1) How do\nnarratives influence negotiation behavior? (2) What differs when agents share\nthe same story versus different ones? (3) What happens when the agent numbers\ngrow? (4) Are agents resilient against self-serving negotiators? We find that\nstory-based priming significantly affects negotiation strategies and success\nrates. Common stories improve collaboration, benefiting each agent. By\ncontrast, priming agents with different stories reverses this effect, and those\nagents primed toward self-interest prevail. We hypothesize that these results\ncarry implications for multi-agent system design and AI alignment."}
{"id": "2505.05309", "pdf": "https://arxiv.org/pdf/2505.05309", "abs": "https://arxiv.org/abs/2505.05309", "authors": ["Yifan Bian", "Chuanbo Tang", "Li Li", "Dong Liu"], "title": "Augmented Deep Contexts for Spatially Embedded Video Coding", "categories": ["eess.IV", "cs.CV"], "comment": "15 pages,CVPR", "summary": "Most Neural Video Codecs (NVCs) only employ temporal references to generate\ntemporal-only contexts and latent prior. These temporal-only NVCs fail to\nhandle large motions or emerging objects due to limited contexts and misaligned\nlatent prior. To relieve the limitations, we propose a Spatially Embedded Video\nCodec (SEVC), in which the low-resolution video is compressed for spatial\nreferences. Firstly, our SEVC leverages both spatial and temporal references to\ngenerate augmented motion vectors and hybrid spatial-temporal contexts.\nSecondly, to address the misalignment issue in latent prior and enrich the\nprior information, we introduce a spatial-guided latent prior augmented by\nmultiple temporal latent representations. At last, we design a joint\nspatial-temporal optimization to learn quality-adaptive bit allocation for\nspatial references, further boosting rate-distortion performance. Experimental\nresults show that our SEVC effectively alleviates the limitations in handling\nlarge motions or emerging objects, and also reduces 11.9% more bitrate than the\nprevious state-of-the-art NVC while providing an additional low-resolution\nbitstream. Our code and model are available at https://github.com/EsakaK/SEVC."}
{"id": "2505.04678", "pdf": "https://arxiv.org/pdf/2505.04678", "abs": "https://arxiv.org/abs/2505.04678", "authors": ["Shahad Elshehaby", "Alavikunhu Panthakkan", "Hussain Al-Ahmad", "Mina Al-Saad"], "title": "Advanced Deep Learning Approaches for Automated Recognition of Cuneiform Symbols", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "This paper presents a thoroughly automated method for identifying and\ninterpreting cuneiform characters via advanced deep-learning algorithms. Five\ndistinct deep-learning models were trained on a comprehensive dataset of\ncuneiform characters and evaluated according to critical performance metrics,\nincluding accuracy and precision. Two models demonstrated outstanding\nperformance and were subsequently assessed using cuneiform symbols from the\nHammurabi law acquisition, notably Hammurabi Law 1. Each model effectively\nrecognized the relevant Akkadian meanings of the symbols and delivered precise\nEnglish translations. Future work will investigate ensemble and stacking\napproaches to optimize performance, utilizing hybrid architectures to improve\ndetection accuracy and reliability. This research explores the linguistic\nrelationships between Akkadian, an ancient Mesopotamian language, and Arabic,\nemphasizing their historical and cultural linkages. This study demonstrates the\ncapability of deep learning to decipher ancient scripts by merging\ncomputational linguistics with archaeology, therefore providing significant\ninsights for the comprehension and conservation of human history."}
{"id": "2505.04891", "pdf": "https://arxiv.org/pdf/2505.04891", "abs": "https://arxiv.org/abs/2505.04891", "authors": ["Cong Qi", "Yeqing Chen", "Jie Zhang", "Wei Zhi"], "title": "Clustering with Communication: A Variational Framework for Single Cell Representation Learning", "categories": ["cs.LG", "cs.AI", "stat.ML"], "comment": null, "summary": "Single-cell RNA sequencing (scRNA-seq) has revealed complex cellular\nheterogeneity, but recent studies emphasize that understanding biological\nfunction also requires modeling cell-cell communication (CCC), the signaling\ninteractions mediated by ligand-receptor pairs that coordinate cellular\nbehavior. Tools like CellChat have demonstrated that CCC plays a critical role\nin processes such as cell differentiation, tissue regeneration, and immune\nresponse, and that transcriptomic data inherently encodes rich information\nabout intercellular signaling. We propose CCCVAE, a novel variational\nautoencoder framework that incorporates CCC signals into single-cell\nrepresentation learning. By leveraging a communication-aware kernel derived\nfrom ligand-receptor interactions and a sparse Gaussian process, CCCVAE encodes\nbiologically informed priors into the latent space. Unlike conventional VAEs\nthat treat each cell independently, CCCVAE encourages latent embeddings to\nreflect both transcriptional similarity and intercellular signaling context.\nEmpirical results across four scRNA-seq datasets show that CCCVAE improves\nclustering performance, achieving higher evaluation scores than standard VAE\nbaselines. This work demonstrates the value of embedding biological priors into\ndeep generative models for unsupervised single-cell analysis."}
{"id": "2505.04864", "pdf": "https://arxiv.org/pdf/2505.04864", "abs": "https://arxiv.org/abs/2505.04864", "authors": ["Kanggeon Lee", "Soochahn Lee", "Kyoung Mu Lee"], "title": "Auto-regressive transformation for image alignment", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Existing methods for image alignment struggle in cases involving\nfeature-sparse regions, extreme scale and field-of-view differences, and large\ndeformations, often resulting in suboptimal accuracy. Robustness to these\nchallenges improves through iterative refinement of the transformation field\nwhile focusing on critical regions in multi-scale image representations. We\nthus propose Auto-Regressive Transformation (ART), a novel method that\niteratively estimates the coarse-to-fine transformations within an\nauto-regressive framework. Leveraging hierarchical multi-scale features, our\nnetwork refines the transformations using randomly sampled points at each\nscale. By incorporating guidance from the cross-attention layer, the model\nfocuses on critical regions, ensuring accurate alignment even in challenging,\nfeature-limited conditions. Extensive experiments across diverse datasets\ndemonstrate that ART significantly outperforms state-of-the-art methods,\nestablishing it as a powerful new method for precise image alignment with broad\napplicability."}
{"id": "2505.05177", "pdf": "https://arxiv.org/pdf/2505.05177", "abs": "https://arxiv.org/abs/2505.05177", "authors": ["Anish Ganguli", "Prabal Deb", "Debleena Banerjee"], "title": "MARK: Memory Augmented Refinement of Knowledge", "categories": ["cs.AI", "cs.LG"], "comment": null, "summary": "Large Language Models (LLMs) assist in specialized tasks but struggle to\nalign with evolving domain knowledge without costly fine-tuning. Domain\nknowledge consists of: Knowledge: Immutable facts (e.g., 'A stone is solid')\nand generally accepted principles (e.g., ethical standards); Refined Memory:\nEvolving insights shaped by business needs and real-world changes. However, a\nsignificant gap often exists between a domain expert's deep, nuanced\nunderstanding and the system's domain knowledge, which can hinder accurate\ninformation retrieval and application. Our Memory-Augmented Refinement of\nKnowledge (MARK) framework enables LLMs to continuously learn without\nretraining by leveraging structured refined memory, inspired by the Society of\nMind. MARK operates through specialized agents, each serving a distinct role:\nResidual Refined Memory Agent: Stores and retrieves domain-specific insights to\nmaintain context over time; User Question Refined Memory Agent: Captures\nuser-provided facts, abbreviations, and terminology for better comprehension;\nLLM Response Refined Memory Agent: Extracts key elements from responses for\nrefinement and personalization. These agents analyse stored refined memory,\ndetect patterns, resolve contradictions, and improve response accuracy.\nTemporal factors like recency and frequency prioritize relevant information\nwhile discarding outdated insights. MARK enhances LLMs in multiple ways: Ground\nTruth Strategy: Reduces hallucinations by establishing a structured reference;\nDomain-Specific Adaptation: Essential for fields like healthcare, law, and\nmanufacturing, where proprietary insights are absent from public datasets;\nPersonalized AI Assistants: Improves virtual assistants by remembering user\npreferences, ensuring coherent responses over time."}
{"id": "2505.05374", "pdf": "https://arxiv.org/pdf/2505.05374", "abs": "https://arxiv.org/abs/2505.05374", "authors": ["Naveenkumar G Venkataswamy", "Poorna Ravi", "Stephanie Schuckers", "Masudul H. Imtiaz"], "title": "OcularAge: A Comparative Study of Iris and Periocular Images for Pediatric Age Estimation", "categories": ["eess.IV", "cs.CV"], "comment": null, "summary": "Estimating a child's age from ocular biometric images is challenging due to\nsubtle physiological changes and the limited availability of longitudinal\ndatasets. Although most biometric age estimation studies have focused on facial\nfeatures and adult subjects, pediatric-specific analysis, particularly of the\niris and periocular regions, remains relatively unexplored. This study presents\na comparative evaluation of iris and periocular images for estimating the ages\nof children aged between 4 and 16 years. We utilized a longitudinal dataset\ncomprising more than 21,000 near-infrared (NIR) images, collected from 288\npediatric subjects over eight years using two different imaging sensors. A\nmulti-task deep learning framework was employed to jointly perform age\nprediction and age-group classification, enabling a systematic exploration of\nhow different convolutional neural network (CNN) architectures, particularly\nthose adapted for non-square ocular inputs, capture the complex variability\ninherent in pediatric eye images. The results show that periocular models\nconsistently outperform iris-based models, achieving a mean absolute error\n(MAE) of 1.33 years and an age-group classification accuracy of 83.82%. These\nresults mark the first demonstration that reliable age estimation is feasible\nfrom children's ocular images, enabling privacy-preserving age checks in\nchild-centric applications. This work establishes the first longitudinal\nbenchmark for pediatric ocular age estimation, providing a foundation for\ndesigning robust, child-focused biometric systems. The developed models proved\nresilient across different imaging sensors, confirming their potential for\nreal-world deployment. They also achieved inference speeds of less than 10\nmilliseconds per image on resource-constrained VR headsets, demonstrating their\nsuitability for real-time applications."}
{"id": "2505.04723", "pdf": "https://arxiv.org/pdf/2505.04723", "abs": "https://arxiv.org/abs/2505.04723", "authors": ["Jingyang Deng", "Ran Chen", "Jo-Ku Cheng", "Jinwen Ma"], "title": "SOAEsV2-7B/72B: Full-Pipeline Optimization for State-Owned Enterprise LLMs via Continual Pre-Training, Domain-Progressive SFT and Distillation-Enhanced Speculative Decoding", "categories": ["cs.CL"], "comment": null, "summary": "This study addresses key challenges in developing domain-specific large\nlanguage models (LLMs) for Chinese state-owned assets and enterprises (SOAEs),\nwhere current approaches face three limitations: 1) constrained model capacity\nthat limits knowledge integration and cross-task adaptability; 2) excessive\nreliance on domain-specific supervised fine-tuning (SFT) data, which neglects\nthe broader applicability of general language patterns; and 3) inefficient\ninference acceleration for large models processing long contexts. In this work,\nwe propose SOAEsV2-7B/72B, a specialized LLM series developed via a three-phase\nframework: 1) continual pre-training integrates domain knowledge while\nretaining base capabilities; 2) domain-progressive SFT employs curriculum-based\nlearning strategy, transitioning from weakly relevant conversational data to\nexpert-annotated SOAEs datasets to optimize domain-specific tasks; 3)\ndistillation-enhanced speculative decoding accelerates inference via logit\ndistillation between 72B target and 7B draft models, achieving\n1.39-1.52$\\times$ speedup without quality loss. Experimental results\ndemonstrate that our domain-specific pre-training phase maintains 99.8% of\noriginal general language capabilities while significantly improving domain\nperformance, resulting in a 1.08$\\times$ improvement in Rouge-1 score and a\n1.17$\\times$ enhancement in BLEU-4 score. Ablation studies further show that\ndomain-progressive SFT outperforms single-stage training, achieving\n1.02$\\times$ improvement in Rouge-1 and 1.06$\\times$ in BLEU-4. Our work\nintroduces a comprehensive, full-pipeline approach for optimizing SOAEs LLMs,\nbridging the gap between general language capabilities and domain-specific\nexpertise."}
{"id": "2505.04894", "pdf": "https://arxiv.org/pdf/2505.04894", "abs": "https://arxiv.org/abs/2505.04894", "authors": ["Nazanin Mehregan", "Robson E. De Grande"], "title": "GCN-Based Throughput-Oriented Handover Management in Dense 5G Vehicular Networks", "categories": ["cs.LG"], "comment": "Accepted at IEEE DCOSS-IoT 2025", "summary": "The rapid advancement of 5G has transformed vehicular networks, offering high\nbandwidth, low latency, and fast data rates essential for real-time\napplications in smart cities and vehicles. These improvements enhance traffic\nsafety and entertainment services. However, the limited coverage and frequent\nhandovers in 5G networks cause network instability, especially in high-mobility\nenvironments due to the ping-pong effect. This paper presents TH-GCN\n(Throughput-oriented Graph Convolutional Network), a novel approach for\noptimizing handover management in dense 5G networks. Using graph neural\nnetworks (GNNs), TH-GCN models vehicles and base stations as nodes in a dynamic\ngraph enriched with features such as signal quality, throughput, vehicle speed,\nand base station load. By integrating both user equipment and base station\nperspectives, this dual-centric approach enables adaptive, real-time handover\ndecisions that improve network stability. Simulation results show that TH-GCN\nreduces handovers by up to 78 percent and improves signal quality by 10\npercent, outperforming existing methods."}
{"id": "2505.04877", "pdf": "https://arxiv.org/pdf/2505.04877", "abs": "https://arxiv.org/abs/2505.04877", "authors": ["Lianbo Ma", "Jianlun Ma", "Yuee Zhou", "Guoyang Xie", "Qiang He", "Zhichao Lu"], "title": "Learning from Loss Landscape: Generalizable Mixed-Precision Quantization via Adaptive Sharpness-Aware Gradient Aligning", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Mixed Precision Quantization (MPQ) has become an essential technique for\noptimizing neural network by determining the optimal bitwidth per layer.\nExisting MPQ methods, however, face a major hurdle: they require a\ncomputationally expensive search for quantization policies on large-scale\ndatasets. To resolve this issue, we introduce a novel approach that first\nsearches for quantization policies on small datasets and then generalizes them\nto large-scale datasets. This approach simplifies the process, eliminating the\nneed for large-scale quantization fine-tuning and only necessitating model\nweight adjustment. Our method is characterized by three key techniques:\nsharpness-aware minimization for enhanced quantization generalization, implicit\ngradient direction alignment to handle gradient conflicts among different\noptimization objectives, and an adaptive perturbation radius to accelerate\noptimization. Both theoretical analysis and experimental results validate our\napproach. Using the CIFAR10 dataset (just 0.5\\% the size of ImageNet training\ndata) for MPQ policy search, we achieved equivalent accuracy on ImageNet with a\nsignificantly lower computational cost, while improving efficiency by up to\n150% over the baselines."}
{"id": "2505.05197", "pdf": "https://arxiv.org/pdf/2505.05197", "abs": "https://arxiv.org/abs/2505.05197", "authors": ["Joel Z. Leibo", "Alexander Sasha Vezhnevets", "William A. Cunningham", "Sébastien Krier", "Manfred Diaz", "Simon Osindero"], "title": "Societal and technological progress as sewing an ever-growing, ever-changing, patchy, and polychrome quilt", "categories": ["cs.AI", "cs.CY"], "comment": "16 pages", "summary": "Artificial Intelligence (AI) systems are increasingly placed in positions\nwhere their decisions have real consequences, e.g., moderating online spaces,\nconducting research, and advising on policy. Ensuring they operate in a safe\nand ethically acceptable fashion is thus critical. However, most solutions have\nbeen a form of one-size-fits-all \"alignment\". We are worried that such systems,\nwhich overlook enduring moral diversity, will spark resistance, erode trust,\nand destabilize our institutions. This paper traces the underlying problem to\nan often-unstated Axiom of Rational Convergence: the idea that under ideal\nconditions, rational agents will converge in the limit of conversation on a\nsingle ethics. Treating that premise as both optional and doubtful, we propose\nwhat we call the appropriateness framework: an alternative approach grounded in\nconflict theory, cultural evolution, multi-agent systems, and institutional\neconomics. The appropriateness framework treats persistent disagreement as the\nnormal case and designs for it by applying four principles: (1) contextual\ngrounding, (2) community customization, (3) continual adaptation, and (4)\npolycentric governance. We argue here that adopting these design principles is\na good way to shift the main alignment metaphor from moral unification to a\nmore productive metaphor of conflict management, and that taking this step is\nboth desirable and urgent."}
{"id": "2505.05367", "pdf": "https://arxiv.org/pdf/2505.05367", "abs": "https://arxiv.org/abs/2505.05367", "authors": ["Jie Deng", "Danfeng Hong", "Chenyu Li", "Naoto Yokoya"], "title": "Joint Super-Resolution and Segmentation for 1-m Impervious Surface Area Mapping in China's Yangtze River Economic Belt", "categories": ["cs.CV", "eess.IV"], "comment": null, "summary": "We propose a novel joint framework by integrating super-resolution and\nsegmentation, called JointSeg, which enables the generation of 1-meter ISA maps\ndirectly from freely available Sentinel-2 imagery. JointSeg was trained on\nmultimodal cross-resolution inputs, offering a scalable and affordable\nalternative to traditional approaches. This synergistic design enables gradual\nresolution enhancement from 10m to 1m while preserving fine-grained spatial\ntextures, and ensures high classification fidelity through effective\ncross-scale feature fusion. This method has been successfully applied to the\nYangtze River Economic Belt (YREB), a region characterized by complex\nurban-rural patterns and diverse topography. As a result, a comprehensive ISA\nmapping product for 2021, referred to as ISA-1, was generated, covering an area\nof over 2.2 million square kilometers. Quantitative comparisons against the 10m\nESA WorldCover and other benchmark products reveal that ISA-1 achieves an\nF1-score of 85.71%, outperforming bilinear-interpolation-based segmentation by\n9.5%, and surpassing other ISA datasets by 21.43%-61.07%. In densely urbanized\nareas (e.g., Suzhou, Nanjing), ISA-1 reduces ISA overestimation through\nimproved discrimination of green spaces and water bodies. Conversely, in\nmountainous regions (e.g., Ganzi, Zhaotong), it identifies significantly more\nISA due to its enhanced ability to detect fragmented anthropogenic features\nsuch as rural roads and sparse settlements, demonstrating its robustness across\ndiverse landscapes. Moreover, we present biennial ISA maps from 2017 to 2023,\ncapturing spatiotemporal urbanization dynamics across representative cities.\nThe results highlight distinct regional growth patterns: rapid expansion in\nupstream cities, moderate growth in midstream regions, and saturation in\ndownstream metropolitan areas."}
{"id": "2505.04785", "pdf": "https://arxiv.org/pdf/2505.04785", "abs": "https://arxiv.org/abs/2505.04785", "authors": ["Shuai Gong", "Tiange Zhou"], "title": "Flower Across Time and Media: Sentiment Analysis of Tang Song Poetry and Visual Correspondence", "categories": ["cs.CL", "cs.AI"], "comment": "5 pages, 9 figures", "summary": "The Tang (618 to 907) and Song (960 to 1279) dynasties witnessed an\nextraordinary flourishing of Chinese cultural expression, where floral motifs\nserved as a dynamic medium for both poetic sentiment and artistic design. While\nprevious scholarship has examined these domains independently, the systematic\ncorrelation between evolving literary emotions and visual culture remains\nunderexplored. This study addresses that gap by employing BERT-based sentiment\nanalysis to quantify emotional patterns in floral imagery across Tang Song\npoetry, then validating these patterns against contemporaneous developments in\ndecorative arts.Our approach builds upon recent advances in computational\nhumanities while remaining grounded in traditional sinological methods. By\napplying a fine tuned BERT model to analyze peony and plum blossom imagery in\nclassical poetry, we detect measurable shifts in emotional connotations between\nthe Tang and Song periods. These textual patterns are then cross berenced with\nvisual evidence from textiles, ceramics, and other material culture, revealing\npreviously unrecognized synergies between literary expression and artistic\nrepresentation."}
{"id": "2505.04898", "pdf": "https://arxiv.org/pdf/2505.04898", "abs": "https://arxiv.org/abs/2505.04898", "authors": ["Qiyang Han", "Masaaki Imaizumi"], "title": "Precise gradient descent training dynamics for finite-width multi-layer neural networks", "categories": ["cs.LG", "cs.AI", "math.OC", "math.ST", "stat.ML", "stat.TH"], "comment": null, "summary": "In this paper, we provide the first precise distributional characterization\nof gradient descent iterates for general multi-layer neural networks under the\ncanonical single-index regression model, in the `finite-width proportional\nregime' where the sample size and feature dimension grow proportionally while\nthe network width and depth remain bounded. Our non-asymptotic state evolution\ntheory captures Gaussian fluctuations in first-layer weights and concentration\nin deeper-layer weights, and remains valid for non-Gaussian features.\n  Our theory differs from existing neural tangent kernel (NTK), mean-field (MF)\ntheories and tensor program (TP) in several key aspects. First, our theory\noperates in the finite-width regime whereas these existing theories are\nfundamentally infinite-width. Second, our theory allows weights to evolve from\nindividual initializations beyond the lazy training regime, whereas NTK and MF\nare either frozen at or only weakly sensitive to initialization, and TP relies\non special initialization schemes. Third, our theory characterizes both\ntraining and generalization errors for general multi-layer neural networks\nbeyond the uniform convergence regime, whereas existing theories study\ngeneralization almost exclusively in two-layer settings.\n  As a statistical application, we show that vanilla gradient descent can be\naugmented to yield consistent estimates of the generalization error at each\niteration, which can be used to guide early stopping and hyperparameter tuning.\nAs a further theoretical implication, we show that despite model\nmisspecification, the model learned by gradient descent retains the structure\nof a single-index function with an effective signal determined by a linear\ncombination of the true signal and the initialization."}
{"id": "2505.04888", "pdf": "https://arxiv.org/pdf/2505.04888", "abs": "https://arxiv.org/abs/2505.04888", "authors": ["Tharindu Fernando", "Clinton Fookes", "Sridha Sridharan", "Simon Denman"], "title": "Cross-Branch Orthogonality for Improved Generalization in Face Deepfake Detection", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Remarkable advancements in generative AI technology have given rise to a\nspectrum of novel deepfake categories with unprecedented leaps in their\nrealism, and deepfakes are increasingly becoming a nuisance to law enforcement\nauthorities and the general public. In particular, we observe alarming levels\nof confusion, deception, and loss of faith regarding multimedia content within\nsociety caused by face deepfakes, and existing deepfake detectors are\nstruggling to keep up with the pace of improvements in deepfake generation.\nThis is primarily due to their reliance on specific forgery artifacts, which\nlimits their ability to generalise and detect novel deepfake types. To combat\nthe spread of malicious face deepfakes, this paper proposes a new strategy that\nleverages coarse-to-fine spatial information, semantic information, and their\ninteractions while ensuring feature distinctiveness and reducing the redundancy\nof the modelled features. A novel feature orthogonality-based disentanglement\nstrategy is introduced to ensure branch-level and cross-branch feature\ndisentanglement, which allows us to integrate multiple feature vectors without\nadding complexity to the feature space or compromising generalisation.\nComprehensive experiments on three public benchmarks: FaceForensics++,\nCeleb-DF, and the Deepfake Detection Challenge (DFDC) show that these design\nchoices enable the proposed approach to outperform current state-of-the-art\nmethods by 5% on the Celeb-DF dataset and 7% on the DFDC dataset in a\ncross-dataset evaluation setting."}
{"id": "2505.05232", "pdf": "https://arxiv.org/pdf/2505.05232", "abs": "https://arxiv.org/abs/2505.05232", "authors": ["Mahmoud Amiri", "Thomas Bocklitz"], "title": "ChemRxivQuest: A Curated Chemistry Question-Answer Database Extracted from ChemRxiv Preprints", "categories": ["cs.AI"], "comment": null, "summary": "The rapid expansion of chemistry literature poses significant challenges for\nresearchers seeking to efficiently access domain-specific knowledge. To support\nadvancements in chemistry-focused natural language processing (NLP), we present\nChemRxivQuest, a curated dataset of 970 high-quality question-answer (QA) pairs\nderived from 155 ChemRxiv preprints across 17 subfields of chemistry. Each QA\npair is explicitly linked to its source text segment to ensure traceability and\ncontextual accuracy. ChemRxivQuest was constructed using an automated pipeline\nthat combines optical character recognition (OCR), GPT-4o-based QA generation,\nand a fuzzy matching technique for answer verification. The dataset emphasizes\nconceptual, mechanistic, applied, and experimental questions, enabling\napplications in retrieval-based QA systems, search engine development, and\nfine-tuning of domain-adapted large language models. We analyze the dataset's\nstructure, coverage, and limitations, and outline future directions for\nexpansion and expert validation. ChemRxivQuest provides a foundational resource\nfor chemistry NLP research, education, and tool development."}
{"id": "2305.00046", "pdf": "https://arxiv.org/pdf/2305.00046", "abs": "https://arxiv.org/abs/2305.00046", "authors": ["Samiul Based Shuvo", "Tasnia Binte Mamun"], "title": "An automated end-to-end deep learning-based framework for lung cancer diagnosis by detecting and classifying the lung nodules", "categories": ["eess.IV", "cs.AI", "cs.CV"], "comment": null, "summary": "Lung cancer is a leading cause of cancer-related deaths worldwide, and early\ndetection is crucial for improving patient outcomes. Nevertheless, early\ndiagnosis of cancer is a major challenge, particularly in low-resource settings\nwhere access to medical resources and trained radiologists is limited. The\nobjective of this study is to propose an automated end-to-end deep\nlearning-based framework for the early detection and classification of lung\nnodules, specifically for low-resource settings. The proposed framework\nconsists of three stages: lung segmentation using a modified 3D U-Net named 3D\nRes-U-Net, nodule detection using YOLO-v5, and classification with a Vision\nTransformer-based architecture. We evaluated the proposed framework on a\npublicly available dataset, LUNA16. The proposed framework's performance was\nmeasured using the respective domain's evaluation matrices. The proposed\nframework achieved a 98.82% lung segmentation dice score while detecting the\nlung nodule with 0.76 mAP@50 from the segmented lung, at a low false-positive\nrate. The performance of both networks of the proposed framework was compared\nwith other studies and found to outperform them regarding segmentation and\ndetection accuracy. Additionally, our proposed Vision transformer network\nobtained an accuracy of 93.57%, which is 1.21% higher than the state-of-the-art\nnetworks. Our proposed end-to-end deep learning-based framework can effectively\nsegment lungs, and detect and classify lung nodules, specifically in\nlow-resource settings with limited access to radiologists. The proposed\nframework outperforms existing studies regarding all the respective evaluation\nmetrics. The proposed framework can potentially improve the accuracy and\nefficiency of lung cancer screening in low-resource settings, ultimately\nleading to better patient outcomes."}
{"id": "2505.04844", "pdf": "https://arxiv.org/pdf/2505.04844", "abs": "https://arxiv.org/abs/2505.04844", "authors": ["Alex Shan", "John Bauer", "Christopher D. Manning"], "title": "Osiris: A Lightweight Open-Source Hallucination Detection System", "categories": ["cs.CL"], "comment": "Stanford 191W", "summary": "Retrieval-Augmented Generation (RAG) systems have gained widespread adoption\nby application builders because they leverage sources of truth to enable Large\nLanguage Models (LLMs) to generate more factually sound responses. However,\nhallucinations, instances of LLM responses that are unfaithful to the provided\ncontext, often prevent these systems from being deployed in production\nenvironments. Current hallucination detection methods typically involve human\nevaluation or the use of closed-source models to review RAG system outputs for\nhallucinations. Both human evaluators and closed-source models suffer from\nscaling issues due to their high costs and slow inference speeds. In this work,\nwe introduce a perturbed multi-hop QA dataset with induced hallucinations. Via\nsupervised fine-tuning on our dataset, we achieve better recall with a 7B model\nthan GPT-4o on the RAGTruth hallucination detection benchmark and offer\ncompetitive performance on precision and accuracy, all while using a fraction\nof the parameters. Code is released at our repository."}
{"id": "2505.04907", "pdf": "https://arxiv.org/pdf/2505.04907", "abs": "https://arxiv.org/abs/2505.04907", "authors": ["Soham Khisa", "Avijoy Chakma"], "title": "VaCDA: Variational Contrastive Alignment-based Scalable Human Activity Recognition", "categories": ["cs.LG"], "comment": null, "summary": "Technological advancements have led to the rise of wearable devices with\nsensors that continuously monitor user activities, generating vast amounts of\nunlabeled data. This data is challenging to interpret, and manual annotation is\nlabor-intensive and error-prone. Additionally, data distribution is often\nheterogeneous due to device placement, type, and user behavior variations. As a\nresult, traditional transfer learning methods perform suboptimally, making it\ndifficult to recognize daily activities. To address these challenges, we use a\nvariational autoencoder (VAE) to learn a shared, low-dimensional latent space\nfrom available sensor data. This space generalizes data across diverse sensors,\nmitigating heterogeneity and aiding robust adaptation to the target domain. We\nintegrate contrastive learning to enhance feature representation by aligning\ninstances of the same class across domains while separating different classes.\nWe propose Variational Contrastive Domain Adaptation (VaCDA), a multi-source\ndomain adaptation framework combining VAEs and contrastive learning to improve\nfeature representation and reduce heterogeneity between source and target\ndomains. We evaluate VaCDA on multiple publicly available datasets across three\nheterogeneity scenarios: cross-person, cross-position, and cross-device. VaCDA\noutperforms the baselines in cross-position and cross-device scenarios."}
{"id": "2505.04899", "pdf": "https://arxiv.org/pdf/2505.04899", "abs": "https://arxiv.org/abs/2505.04899", "authors": ["Sifan Song", "Siyeop Yoon", "Pengfei Jin", "Sekeun Kim", "Matthew Tivnan", "Yujin Oh", "Runqi Meng", "Ling Chen", "Zhiliang Lyu", "Dufan Wu", "Ning Guo", "Xiang Li", "Quanzheng Li"], "title": "OWT: A Foundational Organ-Wise Tokenization Framework for Medical Imaging", "categories": ["cs.CV"], "comment": null, "summary": "Recent advances in representation learning often rely on holistic, black-box\nembeddings that entangle multiple semantic components, limiting\ninterpretability and generalization. These issues are especially critical in\nmedical imaging. To address these limitations, we propose an Organ-Wise\nTokenization (OWT) framework with a Token Group-based Reconstruction (TGR)\ntraining paradigm. Unlike conventional approaches that produce holistic\nfeatures, OWT explicitly disentangles an image into separable token groups,\neach corresponding to a distinct organ or semantic entity. Our design ensures\neach token group encapsulates organ-specific information, boosting\ninterpretability, generalization, and efficiency while allowing fine-grained\ncontrol in downstream tasks. Experiments on CT and MRI datasets demonstrate the\neffectiveness of OWT in not only achieving strong image reconstruction and\nsegmentation performance, but also enabling novel semantic-level generation and\nretrieval applications that are out of reach for standard holistic embedding\nmethods. These findings underscore the potential of OWT as a foundational\nframework for semantically disentangled representation learning, offering broad\nscalability and applicability to real-world medical imaging scenarios and\nbeyond."}
{"id": "2505.05235", "pdf": "https://arxiv.org/pdf/2505.05235", "abs": "https://arxiv.org/abs/2505.05235", "authors": ["Luca Marzari", "Isabella Mastroeni", "Alessandro Farinelli"], "title": "Advancing Neural Network Verification through Hierarchical Safety Abstract Interpretation", "categories": ["cs.AI"], "comment": null, "summary": "Traditional methods for formal verification (FV) of deep neural networks\n(DNNs) are constrained by a binary encoding of safety properties, where a model\nis classified as either safe or unsafe (robust or not robust). This binary\nencoding fails to capture the nuanced safety levels within a model, often\nresulting in either overly restrictive or too permissive requirements. In this\npaper, we introduce a novel problem formulation called Abstract\nDNN-Verification, which verifies a hierarchical structure of unsafe outputs,\nproviding a more granular analysis of the safety aspect for a given DNN.\nCrucially, by leveraging abstract interpretation and reasoning about output\nreachable sets, our approach enables assessing multiple safety levels during\nthe FV process, requiring the same (in the worst case) or even potentially less\ncomputational effort than the traditional binary verification approach.\nSpecifically, we demonstrate how this formulation allows rank adversarial\ninputs according to their abstract safety level violation, offering a more\ndetailed evaluation of the model's safety and robustness. Our contributions\ninclude a theoretical exploration of the relationship between our novel\nabstract safety formulation and existing approaches that employ abstract\ninterpretation for robustness verification, complexity analysis of the novel\nproblem introduced, and an empirical evaluation considering both a complex deep\nreinforcement learning task (based on Habitat 3.0) and standard\nDNN-Verification benchmarks."}
{"id": "2408.16859", "pdf": "https://arxiv.org/pdf/2408.16859", "abs": "https://arxiv.org/abs/2408.16859", "authors": ["Sania Eskandari", "Ali Eslamian", "Nusrat Munia", "Amjad Alqarni", "Qiang Cheng"], "title": "Evaluating Deep Learning Models for Breast Cancer Classification: A Comparative Study", "categories": ["eess.IV", "cs.CV"], "comment": "4 pages, 2 figures, 2 tables", "summary": "This study evaluates the effectiveness of deep learning models in classifying\nhistopathological images for early and accurate detection of breast cancer.\nEight advanced models, including ResNet-50, DenseNet-121, ResNeXt-50, Vision\nTransformer (ViT), GoogLeNet (Inception v3), EfficientNet, MobileNet, and\nSqueezeNet, were compared using a dataset of 277,524 image patches. The Vision\nTransformer (ViT) model, with its attention-based mechanisms, achieved the\nhighest validation accuracy of 94%, outperforming conventional CNNs. The study\ndemonstrates the potential of advanced machine learning methods to enhance\nprecision and efficiency in breast cancer diagnosis in clinical settings."}
{"id": "2505.04847", "pdf": "https://arxiv.org/pdf/2505.04847", "abs": "https://arxiv.org/abs/2505.04847", "authors": ["Manveer Singh Tamber", "Forrest Sheng Bao", "Chenyu Xu", "Ge Luo", "Suleman Kazi", "Minseok Bae", "Miaoran Li", "Ofer Mendelevitch", "Renyi Qu", "Jimmy Lin"], "title": "Benchmarking LLM Faithfulness in RAG with Evolving Leaderboards", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Hallucinations remain a persistent challenge for LLMs. RAG aims to reduce\nhallucinations by grounding responses in contexts. However, even when provided\ncontext, LLMs still frequently introduce unsupported information or\ncontradictions. This paper presents our efforts to measure LLM hallucinations\nwith a focus on summarization tasks, assessing how often various LLMs introduce\nhallucinations when summarizing documents. We discuss Vectara's existing LLM\nhallucination leaderboard, based on the Hughes Hallucination Evaluation Model\n(HHEM). While HHEM and Vectara's Hallucination Leaderboard have garnered great\nresearch interest, we examine challenges faced by HHEM and current\nhallucination detection methods by analyzing the effectiveness of these methods\non existing hallucination datasets. To address these limitations, we propose\nFaithJudge, an LLM-as-a-judge approach guided by few-shot human hallucination\nannotations, which substantially improves automated LLM hallucination\nevaluation over current methods. We introduce an enhanced hallucination\nleaderboard centered on FaithJudge, alongside our current hallucination\nleaderboard, enabling more reliable benchmarking of LLMs for hallucinations in\nRAG."}
{"id": "2505.04918", "pdf": "https://arxiv.org/pdf/2505.04918", "abs": "https://arxiv.org/abs/2505.04918", "authors": ["Jiaqi Zheng", "Qing Ling", "Yerong Feng"], "title": "Physics-Assisted and Topology-Informed Deep Learning for Weather Prediction", "categories": ["cs.LG", "cs.AI"], "comment": "International Joint Conferences on Artificial Intelligence (IJCAI\n  2025)", "summary": "Although deep learning models have demonstrated remarkable potential in\nweather prediction, most of them overlook either the \\textbf{physics} of the\nunderlying weather evolution or the \\textbf{topology} of the Earth's surface.\nIn light of these disadvantages, we develop PASSAT, a novel Physics-ASSisted\nAnd Topology-informed deep learning model for weather prediction. PASSAT\nattributes the weather evolution to two key factors: (i) the advection process\nthat can be characterized by the advection equation and the Navier-Stokes\nequation; (ii) the Earth-atmosphere interaction that is difficult to both model\nand calculate. PASSAT also takes the topology of the Earth's surface into\nconsideration, other than simply treating it as a plane. With these\nconsiderations, PASSAT numerically solves the advection equation and the\nNavier-Stokes equation on the spherical manifold, utilizes a spherical graph\nneural network to capture the Earth-atmosphere interaction, and generates the\ninitial velocity fields that are critical to solving the advection equation\nfrom the same spherical graph neural network. In the $5.625^\\circ$-resolution\nERA5 data set, PASSAT outperforms both the state-of-the-art deep learning-based\nweather prediction models and the operational numerical weather prediction\nmodel IFS T42. Code and checkpoint are available at\nhttps://github.com/Yumenomae/PASSAT_5p625."}
{"id": "2505.04905", "pdf": "https://arxiv.org/pdf/2505.04905", "abs": "https://arxiv.org/abs/2505.04905", "authors": ["Xi Yang", "Songsong Duan", "Nannan Wang", "Xinbo Gao"], "title": "Pro2SAM: Mask Prompt to SAM with Grid Points for Weakly Supervised Object Localization", "categories": ["cs.CV"], "comment": "Accepted by ECCV 2024", "summary": "Weakly Supervised Object Localization (WSOL), which aims to localize objects\nby only using image-level labels, has attracted much attention because of its\nlow annotation cost in real applications. Current studies focus on the Class\nActivation Map (CAM) of CNN and the self-attention map of transformer to\nidentify the region of objects. However, both CAM and self-attention maps can\nnot learn pixel-level fine-grained information on the foreground objects, which\nhinders the further advance of WSOL. To address this problem, we initiatively\nleverage the capability of zero-shot generalization and fine-grained\nsegmentation in Segment Anything Model (SAM) to boost the activation of\nintegral object regions. Further, to alleviate the semantic ambiguity issue\naccrued in single point prompt-based SAM, we propose an innovative mask prompt\nto SAM (Pro2SAM) network with grid points for WSOL task. First, we devise a\nGlobal Token Transformer (GTFormer) to generate a coarse-grained foreground map\nas a flexible mask prompt, where the GTFormer jointly embeds patch tokens and\nnovel global tokens to learn foreground semantics. Secondly, we deliver grid\npoints as dense prompts into SAM to maximize the probability of foreground\nmask, which avoids the lack of objects caused by a single point/box prompt.\nFinally, we propose a pixel-level similarity metric to come true the mask\nmatching from mask prompt to SAM, where the mask with the highest score is\nviewed as the final localization map. Experiments show that the proposed\nPro2SAM achieves state-of-the-art performance on both CUB-200-2011 and ILSVRC,\nwith 84.03\\% and 66.85\\% Top-1 Loc, respectively."}
{"id": "2505.05396", "pdf": "https://arxiv.org/pdf/2505.05396", "abs": "https://arxiv.org/abs/2505.05396", "authors": ["Stefanos Gkikas"], "title": "A Pain Assessment Framework based on multimodal data and Deep Machine Learning methods", "categories": ["cs.AI", "cs.HC", "cs.LG"], "comment": null, "summary": "From the original abstract:\n  This thesis initially aims to study the pain assessment process from a\nclinical-theoretical perspective while exploring and examining existing\nautomatic approaches. Building on this foundation, the primary objective of\nthis Ph.D. project is to develop innovative computational methods for automatic\npain assessment that achieve high performance and are applicable in real\nclinical settings. A primary goal is to thoroughly investigate and assess\nsignificant factors, including demographic elements that impact pain\nperception, as recognized in pain research, through a computational standpoint.\nWithin the limits of the available data in this research area, our goal was to\ndesign, develop, propose, and offer automatic pain assessment pipelines for\nunimodal and multimodal configurations that are applicable to the specific\nrequirements of different scenarios. The studies published in this Ph.D. thesis\nshowcased the effectiveness of the proposed methods, achieving state-of-the-art\nresults. Additionally, they paved the way for exploring new approaches in\nartificial intelligence, foundation models, and generative artificial\nintelligence."}
{"id": "2410.16296", "pdf": "https://arxiv.org/pdf/2410.16296", "abs": "https://arxiv.org/abs/2410.16296", "authors": ["Debesh Jha", "Onkar Kishor Susladkar", "Vandan Gorade", "Elif Keles", "Matthew Antalek", "Deniz Seyithanoglu", "Timurhan Cebeci", "Halil Ertugrul Aktas", "Gulbiz Dagoglu Kartal", "Sabahattin Kaymakoglu", "Sukru Mehmet Erturk", "Yuri Velichko", "Daniela Ladner", "Amir A. Borhani", "Alpay Medetalibeyoglu", "Gorkem Durak", "Ulas Bagci"], "title": "Large Scale MRI Collection and Segmentation of Cirrhotic Liver", "categories": ["eess.IV", "cs.CV"], "comment": null, "summary": "Liver cirrhosis represents the end stage of chronic liver disease,\ncharacterized by extensive fibrosis and nodular regeneration that significantly\nincreases mortality risk. While magnetic resonance imaging (MRI) offers a\nnon-invasive assessment, accurately segmenting cirrhotic livers presents\nsubstantial challenges due to morphological alterations and heterogeneous\nsignal characteristics. Deep learning approaches show promise for automating\nthese tasks, but progress has been limited by the absence of large-scale,\nannotated datasets. Here, we present CirrMRI600+, the first comprehensive\ndataset comprising 628 high-resolution abdominal MRI scans (310 T1-weighted and\n318 T2-weighted sequences, totaling nearly 40,000 annotated slices) with\nexpert-validated segmentation labels for cirrhotic livers. The dataset includes\ndemographic information, clinical parameters, and histopathological validation\nwhere available. Additionally, we provide benchmark results from 11\nstate-of-the-art deep learning experiments to establish performance standards.\nCirrMRI600+ enables the development and validation of advanced computational\nmethods for cirrhotic liver analysis, potentially accelerating progress toward\nautomated Cirrhosis visual staging and personalized treatment planning."}
{"id": "2505.04916", "pdf": "https://arxiv.org/pdf/2505.04916", "abs": "https://arxiv.org/abs/2505.04916", "authors": ["Ramteja Sajja", "Yusuf Sermet", "Ibrahim Demir"], "title": "An Open-Source Dual-Loss Embedding Model for Semantic Retrieval in Higher Education", "categories": ["cs.CL", "cs.AI", "cs.IR"], "comment": "17 pages, 3 Tables", "summary": "Recent advances in AI have catalyzed the adoption of intelligent educational\ntools, yet many semantic retrieval systems remain ill-suited to the unique\nlinguistic and structural characteristics of academic content. This study\npresents two open-source embedding models fine-tuned for educational question\nanswering, particularly in the context of course syllabi. A synthetic dataset\nof 3,197 sentence pairs, spanning synonymous terminology, paraphrased\nquestions, and implicit-explicit mappings, was constructed through a\ncombination of manual curation and large language model (LLM)-assisted\ngeneration. Two training strategies were evaluated: (1) a baseline model\nfine-tuned using MultipleNegativesRankingLoss (MNRL), and (2) a dual-loss model\nthat combines MNRL with CosineSimilarityLoss to improve both semantic ranking\nand similarity calibration. Evaluations were conducted on 28 university course\nsyllabi using a fixed set of natural language questions categorized into\ncourse, faculty, and teaching assistant information. Results demonstrate that\nboth fine-tuned models outperform strong open-source baselines, including\nall-MiniLM-L6-v2 and multi-qa-MiniLM-L6-cos-v1, and that the dual-loss model\nnarrows the performance gap with high-performing proprietary embeddings such as\nOpenAI's text-embedding-3 series. This work contributes reusable,\ndomain-aligned embedding models and provides a replicable framework for\neducational semantic retrieval, supporting downstream applications such as\nacademic chatbots, retrieval-augmented generation (RAG) systems, and learning\nmanagement system (LMS) integrations."}
{"id": "2505.04931", "pdf": "https://arxiv.org/pdf/2505.04931", "abs": "https://arxiv.org/abs/2505.04931", "authors": ["Yonghong Li", "Xiuzhuang Zhou"], "title": "Fair Uncertainty Quantification for Depression Prediction", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Trustworthy depression prediction based on deep learning, incorporating both\npredictive reliability and algorithmic fairness across diverse demographic\ngroups, is crucial for clinical application. Recently, achieving reliable\ndepression predictions through uncertainty quantification has attracted\nincreasing attention. However, few studies have focused on the fairness of\nuncertainty quantification (UQ) in depression prediction. In this work, we\ninvestigate the algorithmic fairness of UQ, namely Equal Opportunity Coverage\n(EOC) fairness, and propose Fair Uncertainty Quantification (FUQ) for\ndepression prediction. FUQ pursues reliable and fair depression predictions\nthrough group-based analysis. Specifically, we first group all the participants\nby different sensitive attributes and leverage conformal prediction to quantify\nuncertainty within each demographic group, which provides a theoretically\nguaranteed and valid way to quantify uncertainty for depression prediction and\nfacilitates the investigation of fairness across different demographic groups.\nFurthermore, we propose a fairness-aware optimization strategy that formulates\nfairness as a constrained optimization problem under EOC constraints. This\nenables the model to preserve predictive reliability while adapting to the\nheterogeneous uncertainty levels across demographic groups, thereby achieving\noptimal fairness. Through extensive evaluations on several visual and audio\ndepression datasets, our approach demonstrates its effectiveness."}
{"id": "2505.04911", "pdf": "https://arxiv.org/pdf/2505.04911", "abs": "https://arxiv.org/abs/2505.04911", "authors": ["Shun Taguchi", "Hideki Deguchi", "Takumi Hamazaki", "Hiroyuki Sakai"], "title": "SpatialPrompting: Keyframe-driven Zero-Shot Spatial Reasoning with Off-the-Shelf Multimodal Large Language Models", "categories": ["cs.CV", "cs.AI", "cs.CL"], "comment": "18 pages, 11 figures", "summary": "This study introduces SpatialPrompting, a novel framework that harnesses the\nemergent reasoning capabilities of off-the-shelf multimodal large language\nmodels to achieve zero-shot spatial reasoning in three-dimensional (3D)\nenvironments. Unlike existing methods that rely on expensive 3D-specific\nfine-tuning with specialized 3D inputs such as point clouds or voxel-based\nfeatures, SpatialPrompting employs a keyframe-driven prompt generation\nstrategy. This framework uses metrics such as vision-language similarity,\nMahalanobis distance, field of view, and image sharpness to select a diverse\nand informative set of keyframes from image sequences and then integrates them\nwith corresponding camera pose data to effectively abstract spatial\nrelationships and infer complex 3D structures. The proposed framework not only\nestablishes a new paradigm for flexible spatial reasoning that utilizes\nintuitive visual and positional cues but also achieves state-of-the-art\nzero-shot performance on benchmark datasets, such as ScanQA and SQA3D, across\nseveral metrics. The proposed method effectively eliminates the need for\nspecialized 3D inputs and fine-tuning, offering a simpler and more scalable\nalternative to conventional approaches."}
{"id": "2505.05440", "pdf": "https://arxiv.org/pdf/2505.05440", "abs": "https://arxiv.org/abs/2505.05440", "authors": ["Biao Yi", "Xavier Hu", "Yurun Chen", "Shengyu Zhang", "Hongxia Yang", "Fan Wu", "Fei Wu"], "title": "EcoAgent: An Efficient Edge-Cloud Collaborative Multi-Agent Framework for Mobile Automation", "categories": ["cs.AI"], "comment": null, "summary": "Cloud-based mobile agents powered by (multimodal) large language models\n((M)LLMs) offer strong reasoning abilities but suffer from high latency and\ncost. While fine-tuned (M)SLMs enable edge deployment, they often lose general\ncapabilities and struggle with complex tasks. To address this, we propose\nEcoAgent, an Edge-Cloud cOllaborative multi-agent framework for mobile\nautomation. EcoAgent features a closed-loop collaboration among a cloud-based\nPlanning Agent and two edge-based agents: the Execution Agent for action\nexecution and the Observation Agent for verifying outcomes. The Observation\nAgent uses a Pre-Understanding Module to compress screen images into concise\ntext, reducing token usage. In case of failure, the Planning Agent retrieves\nscreen history and replans via a Reflection Module. Experiments on AndroidWorld\nshow that EcoAgent maintains high task success rates while significantly\nreducing MLLM token consumption, enabling efficient and practical mobile\nautomation."}
{"id": "2412.11039", "pdf": "https://arxiv.org/pdf/2412.11039", "abs": "https://arxiv.org/abs/2412.11039", "authors": ["Minghui Zhang", "Chenyu Li", "Fangfang Xie", "Yaoyu Liu", "Hanxiao Zhang", "Junyang Wu", "Chunxi Zhang", "Jie Yang", "Jiayuan Sun", "Guang-Zhong Yang", "Yun Gu"], "title": "AirMorph: Topology-Preserving Deep Learning for Pulmonary Airway Analysis", "categories": ["eess.IV", "cs.CV"], "comment": "Under Review", "summary": "Accurate anatomical labeling and analysis of the pulmonary structure and its\nsurrounding anatomy from thoracic CT is getting increasingly important for\nunderstanding the etilogy of abnormalities or supporting targetted therapy and\nearly interventions. Whilst lung and airway cell atlases have been attempted,\nthere is a lack of fine-grained morphological atlases that are clinically\ndeployable. In this work, we introduce AirMorph, a robust, end-to-end deep\nlearning pipeline enabling fully automatic and comprehensive airway anatomical\nlabeling at lobar, segmental, and subsegmental resolutions that can be used to\ncreate digital atlases of the lung. Evaluated across large-scale multi-center\ndatasets comprising diverse pulmonary conditions, the AirMorph consistently\noutperformed existing segmentation and labeling methods in terms of accuracy,\ntopological consistency, and completeness. To simplify clinical interpretation,\nwe further introduce a compact anatomical signature quantifying critical\nmorphological airway features, including stenosis, ectasia, tortuosity,\ndivergence, length, and complexity. When applied to various pulmonary diseases\nsuch as pulmonary fibrosis, emphysema, atelectasis, consolidation, and\nreticular opacities, it demonstrates strong discriminative power, revealing\ndisease-specific morphological patterns with high interpretability and\nexplainability. Additionally, AirMorph supports efficient automated branching\npattern analysis, potentially enhancing bronchoscopic navigation planning and\nprocedural safety, offering a valuable clinical tool for improved diagnosis,\ntargeted treatment, and personalized patient care."}
{"id": "2505.04955", "pdf": "https://arxiv.org/pdf/2505.04955", "abs": "https://arxiv.org/abs/2505.04955", "authors": ["Fangwei Zhu", "Peiyi Wang", "Zhifang Sui"], "title": "Chain-of-Thought Tokens are Computer Program Variables", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Chain-of-thoughts (CoT) requires large language models (LLMs) to generate\nintermediate steps before reaching the final answer, and has been proven\neffective to help LLMs solve complex reasoning tasks. However, the inner\nmechanism of CoT still remains largely unclear. In this paper, we empirically\nstudy the role of CoT tokens in LLMs on two compositional tasks: multi-digit\nmultiplication and dynamic programming. While CoT is essential for solving\nthese problems, we find that preserving only tokens that store intermediate\nresults would achieve comparable performance. Furthermore, we observe that\nstoring intermediate results in an alternative latent form will not affect\nmodel performance. We also randomly intervene some values in CoT, and notice\nthat subsequent CoT tokens and the final answer would change correspondingly.\nThese findings suggest that CoT tokens may function like variables in computer\nprograms but with potential drawbacks like unintended shortcuts and\ncomputational complexity limits between tokens. The code and data are available\nat https://github.com/solitaryzero/CoTs_are_Variables."}
{"id": "2505.04939", "pdf": "https://arxiv.org/pdf/2505.04939", "abs": "https://arxiv.org/abs/2505.04939", "authors": ["Jeffrey Seathrún Sardina"], "title": "Structural Alignment in Link Prediction", "categories": ["cs.LG", "cs.AI"], "comment": "Ph.D. thesis submitted to Trinity College Dublin", "summary": "While Knowledge Graphs (KGs) have become increasingly popular across various\nscientific disciplines for their ability to model and interlink huge quantities\nof data, essentially all real-world KGs are known to be incomplete. As such,\nwith the growth of KG use has been a concurrent development of machine learning\ntools designed to predict missing information in KGs, which is referred to as\nthe Link Prediction Task. The majority of state-of-the-art link predictors to\ndate have followed an embedding-based paradigm. In this paradigm, it is assumed\nthat the information content of a KG is best represented by the (individual)\nvector representations of its nodes and edges, and that therefore node and edge\nembeddings are particularly well-suited to performing link prediction.\n  This thesis proposes an alternative perspective on the field's approach to\nlink prediction and KG data modelling. Specifically, this work re-analyses KGs\nand state-of-the-art link predictors from a graph-structure-first perspective\nthat models the information content of a KG in terms of whole triples, rather\nthan individual nodes and edges.\n  Following a literature review and two core sets of experiments, this thesis\nconcludes that a structure-first perspective on KGs and link prediction is both\nviable and useful for understanding KG learning and for enabling cross-KG\ntransfer learning for the link prediction task. This observation is used to\ncreate and propose the Structural Alignment Hypothesis, which postulates that\nlink prediction can be understood and modelled as a structural task.\n  All code and data used for this thesis are open-sourced. This thesis was\nwritten bilingually, with the main document in English and an informal extended\nsummary in Irish. An Irish-language translation dictionary of machine learning\nterms (the Focl\\'oir Tr\\'achtais) created for this work is open-sourced as\nwell."}
{"id": "2505.04915", "pdf": "https://arxiv.org/pdf/2505.04915", "abs": "https://arxiv.org/abs/2505.04915", "authors": ["Tong Wang", "Ting Liu", "Xiaochao Qu", "Chengjing Wu", "Luoqi Liu", "Xiaolin Hu"], "title": "GlyphMastero: A Glyph Encoder for High-Fidelity Scene Text Editing", "categories": ["cs.CV"], "comment": "CVPR 2025", "summary": "Scene text editing, a subfield of image editing, requires modifying texts in\nimages while preserving style consistency and visual coherence with the\nsurrounding environment. While diffusion-based methods have shown promise in\ntext generation, they still struggle to produce high-quality results. These\nmethods often generate distorted or unrecognizable characters, particularly\nwhen dealing with complex characters like Chinese. In such systems, characters\nare composed of intricate stroke patterns and spatial relationships that must\nbe precisely maintained. We present GlyphMastero, a specialized glyph encoder\ndesigned to guide the latent diffusion model for generating texts with\nstroke-level precision. Our key insight is that existing methods, despite using\npretrained OCR models for feature extraction, fail to capture the hierarchical\nnature of text structures - from individual strokes to stroke-level\ninteractions to overall character-level structure. To address this, our glyph\nencoder explicitly models and captures the cross-level interactions between\nlocal-level individual characters and global-level text lines through our novel\nglyph attention module. Meanwhile, our model implements a feature pyramid\nnetwork to fuse the multi-scale OCR backbone features at the global-level.\nThrough these cross-level and multi-scale fusions, we obtain more detailed\nglyph-aware guidance, enabling precise control over the scene text generation\nprocess. Our method achieves an 18.02\\% improvement in sentence accuracy over\nthe state-of-the-art multi-lingual scene text editing baseline, while\nsimultaneously reducing the text-region Fr\\'echet inception distance by\n53.28\\%."}
{"id": "2505.05453", "pdf": "https://arxiv.org/pdf/2505.05453", "abs": "https://arxiv.org/abs/2505.05453", "authors": ["Nataliia Klievtsova", "Timotheus Kampik", "Juergen Mangler", "Stefanie Rinderle-Ma"], "title": "Conversational Process Model Redesign", "categories": ["cs.AI"], "comment": null, "summary": "With the recent success of large language models (LLMs), the idea of\nAI-augmented Business Process Management systems is becoming more feasible. One\nof their essential characteristics is the ability to be conversationally\nactionable, allowing humans to interact with the LLM effectively to perform\ncrucial process life cycle tasks such as process model design and redesign.\nHowever, most current research focuses on single-prompt execution and\nevaluation of results, rather than on continuous interaction between the user\nand the LLM. In this work, we aim to explore the feasibility of using LLMs to\nempower domain experts in the creation and redesign of process models in an\niterative and effective way. The proposed conversational process model redesign\n(CPD) approach receives as input a process model and a redesign request by the\nuser in natural language. Instead of just letting the LLM make changes, the LLM\nis employed to (a) identify process change patterns from literature, (b)\nre-phrase the change request to be aligned with an expected wording for the\nidentified pattern (i.e., the meaning), and then to (c) apply the meaning of\nthe change to the process model. This multi-step approach allows for\nexplainable and reproducible changes. In order to ensure the feasibility of the\nCPD approach, and to find out how well the patterns from literature can be\nhandled by the LLM, we performed an extensive evaluation. The results show that\nsome patterns are hard to understand by LLMs and by users. Within the scope of\nthe study, we demonstrated that users need support to describe the changes\nclearly. Overall the evaluation shows that the LLMs can handle most changes\nwell according to a set of completeness and correctness criteria."}
{"id": "2502.03501", "pdf": "https://arxiv.org/pdf/2502.03501", "abs": "https://arxiv.org/abs/2502.03501", "authors": ["Wang Xinyi", "Kang Hongyu", "Wei Peishan", "Shuai Li", "Yu Sun", "Sai Kit Lam", "Yongping Zheng"], "title": "Proxy Prompt: Endowing SAM and SAM 2 with Auto-Interactive-Prompt for Medical Segmentation", "categories": ["eess.IV", "cs.LG"], "comment": null, "summary": "In this paper, we aim to address the unmet demand for automated prompting and\nenhanced human-model interactions of SAM and SAM2 for the sake of promoting\ntheir widespread clinical adoption. Specifically, we propose Proxy Prompt (PP),\nauto-generated by leveraging non-target data with a pre-annotated mask. We\ndevise a novel 3-step context-selection strategy for adaptively selecting the\nmost representative contextual information from non-target data via vision\nmamba and selective maps, empowering the guiding capability of non-target\nimage-mask pairs for segmentation on target image/video data. To reinforce\nhuman-model interactions in PP, we further propose a contextual colorization\nmodule via a dual-reverse cross-attention to enhance interactions between\ntarget features and contextual-embedding with amplifying distinctive features\nof user-defined object(s). Via extensive evaluations, our method achieves\nstate-of-the-art performance on four public datasets and yields comparable\nresults with fully-trained models, even when trained with only 16 image masks."}
{"id": "2505.04984", "pdf": "https://arxiv.org/pdf/2505.04984", "abs": "https://arxiv.org/abs/2505.04984", "authors": ["Kai Nakaishi", "Ryo Yoshida", "Kohei Kajikawa", "Koji Hukushima", "Yohei Oseki"], "title": "Rethinking the Relationship between the Power Law and Hierarchical Structures", "categories": ["cs.CL"], "comment": "13 pages, 11 figures", "summary": "Statistical analysis of corpora provides an approach to quantitatively\ninvestigate natural languages. This approach has revealed that several power\nlaws consistently emerge across different corpora and languages, suggesting the\nuniversal principles underlying languages. Particularly, the power-law decay of\ncorrelation has been interpreted as evidence for underlying hierarchical\nstructures in syntax, semantics, and discourse. This perspective has also been\nextended to child languages and animal signals. However, the argument\nsupporting this interpretation has not been empirically tested. To address this\nproblem, this study examines the validity of the argument for syntactic\nstructures. Specifically, we test whether the statistical properties of parse\ntrees align with the implicit assumptions in the argument. Using English\ncorpora, we analyze the mutual information, deviations from probabilistic\ncontext-free grammars (PCFGs), and other properties in parse trees, as well as\nin the PCFG that approximates these trees. Our results indicate that the\nassumptions do not hold for syntactic structures and that it is difficult to\napply the proposed argument to child languages and animal signals, highlighting\nthe need to reconsider the relationship between the power law and hierarchical\nstructures."}
{"id": "2505.04956", "pdf": "https://arxiv.org/pdf/2505.04956", "abs": "https://arxiv.org/abs/2505.04956", "authors": ["Dingshuo Chen", "Shuchen Xue", "Liuji Chen", "Yingheng Wang", "Qiang Liu", "Shu Wu", "Zhi-Ming Ma", "Liang Wang"], "title": "Graffe: Graph Representation Learning via Diffusion Probabilistic Models", "categories": ["cs.LG", "cs.AI"], "comment": "16 pages, 4 figures, under review", "summary": "Diffusion probabilistic models (DPMs), widely recognized for their potential\nto generate high-quality samples, tend to go unnoticed in representation\nlearning. While recent progress has highlighted their potential for capturing\nvisual semantics, adapting DPMs to graph representation learning remains in its\ninfancy. In this paper, we introduce Graffe, a self-supervised diffusion model\nproposed for graph representation learning. It features a graph encoder that\ndistills a source graph into a compact representation, which, in turn, serves\nas the condition to guide the denoising process of the diffusion decoder. To\nevaluate the effectiveness of our model, we first explore the theoretical\nfoundations of applying diffusion models to representation learning, proving\nthat the denoising objective implicitly maximizes the conditional mutual\ninformation between data and its representation. Specifically, we prove that\nthe negative logarithm of the denoising score matching loss is a tractable\nlower bound for the conditional mutual information. Empirically, we conduct a\nseries of case studies to validate our theoretical insights. In addition,\nGraffe delivers competitive results under the linear probing setting on node\nand graph classification tasks, achieving state-of-the-art performance on 9 of\nthe 11 real-world datasets. These findings indicate that powerful generative\nmodels, especially diffusion models, serve as an effective tool for graph\nrepresentation learning."}
{"id": "2505.04917", "pdf": "https://arxiv.org/pdf/2505.04917", "abs": "https://arxiv.org/abs/2505.04917", "authors": ["Chenxu Peng", "Chenxu Wang", "Minrui Zou", "Danyang Li", "Zhengpeng Yang", "Yimian Dai", "Ming-Ming Cheng", "Xiang Li"], "title": "A Simple Detector with Frame Dynamics is a Strong Tracker", "categories": ["cs.CV"], "comment": "2025 CVPR Anti-UAV Workshop", "summary": "Infrared object tracking plays a crucial role in Anti-Unmanned Aerial Vehicle\n(Anti-UAV) applications. Existing trackers often depend on cropped template\nregions and have limited motion modeling capabilities, which pose challenges\nwhen dealing with tiny targets. To address this, we propose a simple yet\neffective infrared tiny-object tracker that enhances tracking performance by\nintegrating global detection and motion-aware learning with temporal priors.\nOur method is based on object detection and achieves significant improvements\nthrough two key innovations. First, we introduce frame dynamics, leveraging\nframe difference and optical flow to encode both prior target features and\nmotion characteristics at the input level, enabling the model to better\ndistinguish the target from background clutter. Second, we propose a trajectory\nconstraint filtering strategy in the post-processing stage, utilizing\nspatio-temporal priors to suppress false positives and enhance tracking\nrobustness. Extensive experiments show that our method consistently outperforms\nexisting approaches across multiple metrics in challenging infrared UAV\ntracking scenarios. Notably, we achieve state-of-the-art performance in the 4th\nAnti-UAV Challenge, securing 1st place in Track 1 and 2nd place in Track 2."}
{"id": "2504.06667", "pdf": "https://arxiv.org/pdf/2504.06667", "abs": "https://arxiv.org/abs/2504.06667", "authors": ["Yashar Deldjoo", "Nikhil Mehta", "Maheswaran Sathiamoorthy", "Shuai Zhang", "Pablo Castells", "Julian McAuley"], "title": "Toward Holistic Evaluation of Recommender Systems Powered by Generative Models", "categories": ["cs.IR", "cs.AI"], "comment": null, "summary": "Recommender systems powered by generative models (Gen-RecSys) extend beyond\nclassical item ranking by producing open-ended content, which simultaneously\nunlocks richer user experiences and introduces new risks. On one hand, these\nsystems can enhance personalization and appeal through dynamic explanations and\nmulti-turn dialogues. On the other hand, they might venture into unknown\nterritory-hallucinating nonexistent items, amplifying bias, or leaking private\ninformation. Traditional accuracy metrics cannot fully capture these\nchallenges, as they fail to measure factual correctness, content safety, or\nalignment with user intent.\n  This paper makes two main contributions. First, we categorize the evaluation\nchallenges of Gen-RecSys into two groups: (i) existing concerns that are\nexacerbated by generative outputs (e.g., bias, privacy) and (ii) entirely new\nrisks (e.g., item hallucinations, contradictory explanations). Second, we\npropose a holistic evaluation approach that includes scenario-based assessments\nand multi-metric checks-incorporating relevance, factual grounding, bias\ndetection, and policy compliance. Our goal is to provide a guiding framework so\nresearchers and practitioners can thoroughly assess Gen-RecSys, ensuring\neffective personalization and responsible deployment."}
{"id": "2503.13309", "pdf": "https://arxiv.org/pdf/2503.13309", "abs": "https://arxiv.org/abs/2503.13309", "authors": ["Farnoush Bayatmakou", "Reza Taleei", "Milad Amir Toutounchian", "Arash Mohammadi"], "title": "Integrating AI for Human-Centric Breast Cancer Diagnostics: A Multi-Scale and Multi-View Swin Transformer Framework", "categories": ["eess.IV", "cs.AI", "cs.CV"], "comment": null, "summary": "Despite advancements in Computer-Aided Diagnosis (CAD) systems, breast cancer\nremains one of the leading causes of cancer-related deaths among women\nworldwide. Recent breakthroughs in Artificial Intelligence (AI) have shown\nsignificant promise in development of advanced Deep Learning (DL) architectures\nfor breast cancer diagnosis through mammography. In this context, the paper\nfocuses on the integration of AI within a Human-Centric workflow to enhance\nbreast cancer diagnostics. Key challenges are, however, largely overlooked such\nas reliance on detailed tumor annotations and susceptibility to missing views,\nparticularly during test time. To address these issues, we propose a hybrid,\nmulti-scale and multi-view Swin Transformer-based framework (MSMV-Swin) that\nenhances diagnostic robustness and accuracy. The proposed MSMV-Swin framework\nis designed to work as a decision-support tool, helping radiologists analyze\nmulti-view mammograms more effectively. More specifically, the MSMV-Swin\nframework leverages the Segment Anything Model (SAM) to isolate the breast\nlobe, reducing background noise and enabling comprehensive feature extraction.\nThe multi-scale nature of the proposed MSMV-Swin framework accounts for\ntumor-specific regions as well as the spatial characteristics of tissues\nsurrounding the tumor, capturing both localized and contextual information. The\nintegration of contextual and localized data ensures that MSMV-Swin's outputs\nalign with the way radiologists interpret mammograms, fostering better human-AI\ninteraction and trust. A hybrid fusion structure is then designed to ensure\nrobustness against missing views, a common occurrence in clinical practice when\nonly a single mammogram view is available."}
{"id": "2505.04993", "pdf": "https://arxiv.org/pdf/2505.04993", "abs": "https://arxiv.org/abs/2505.04993", "authors": ["Zhuocheng Gong", "Jian Guan", "Wei Wu", "Huishuai Zhang", "Dongyan Zhao"], "title": "Latent Preference Coding: Aligning Large Language Models via Discrete Latent Codes", "categories": ["cs.CL"], "comment": null, "summary": "Large language models (LLMs) have achieved remarkable success, yet aligning\ntheir generations with human preferences remains a critical challenge. Existing\napproaches to preference modeling often rely on an explicit or implicit reward\nfunction, overlooking the intricate and multifaceted nature of human\npreferences that may encompass conflicting factors across diverse tasks and\npopulations. To address this limitation, we introduce Latent Preference Coding\n(LPC), a novel framework that models the implicit factors as well as their\ncombinations behind holistic preferences using discrete latent codes. LPC\nseamlessly integrates with various offline alignment algorithms, automatically\ninferring the underlying factors and their importance from data without relying\non pre-defined reward functions and hand-crafted combination weights. Extensive\nexperiments on multiple benchmarks demonstrate that LPC consistently improves\nupon three alignment algorithms (DPO, SimPO, and IPO) using three base models\n(Mistral-7B, Llama3-8B, and Llama3-8B-Instruct). Furthermore, deeper analysis\nreveals that the learned latent codes effectively capture the differences in\nthe distribution of human preferences and significantly enhance the robustness\nof alignment against noise in data. By providing a unified representation for\nthe multifarious preference factors, LPC paves the way towards developing more\nrobust and versatile alignment techniques for the responsible deployment of\npowerful LLMs."}
{"id": "2505.04969", "pdf": "https://arxiv.org/pdf/2505.04969", "abs": "https://arxiv.org/abs/2505.04969", "authors": ["Gekko Budiutama", "Shunsuke Daimon", "Hirofumi Nishi", "Yu-ichiro Matsushita"], "title": "General Transform: A Unified Framework for Adaptive Transform to Enhance Representations", "categories": ["cs.LG", "cs.CL", "cs.CV"], "comment": null, "summary": "Discrete transforms, such as the discrete Fourier transform, are widely used\nin machine learning to improve model performance by extracting meaningful\nfeatures. However, with numerous transforms available, selecting an appropriate\none often depends on understanding the dataset's properties, making the\napproach less effective when such knowledge is unavailable. In this work, we\npropose General Transform (GT), an adaptive transform-based representation\ndesigned for machine learning applications. Unlike conventional transforms, GT\nlearns data-driven mapping tailored to the dataset and task of interest. Here,\nwe demonstrate that models incorporating GT outperform conventional\ntransform-based approaches across computer vision and natural language\nprocessing tasks, highlighting its effectiveness in diverse learning scenarios."}
{"id": "2505.04921", "pdf": "https://arxiv.org/pdf/2505.04921", "abs": "https://arxiv.org/abs/2505.04921", "authors": ["Yunxin Li", "Zhenyu Liu", "Zitao Li", "Xuanyu Zhang", "Zhenran Xu", "Xinyu Chen", "Haoyuan Shi", "Shenyuan Jiang", "Xintong Wang", "Jifang Wang", "Shouzheng Huang", "Xinping Zhao", "Borui Jiang", "Lanqing Hong", "Longyue Wang", "Zhuotao Tian", "Baoxing Huai", "Wenhan Luo", "Weihua Luo", "Zheng Zhang", "Baotian Hu", "Min Zhang"], "title": "Perception, Reason, Think, and Plan: A Survey on Large Multimodal Reasoning Models", "categories": ["cs.CV", "cs.CL"], "comment": "75 Pages,10 figures; Project:\n  https://github.com/HITsz-TMG/Awesome-Large-Multimodal-Reasoning-Models", "summary": "Reasoning lies at the heart of intelligence, shaping the ability to make\ndecisions, draw conclusions, and generalize across domains. In artificial\nintelligence, as systems increasingly operate in open, uncertain, and\nmultimodal environments, reasoning becomes essential for enabling robust and\nadaptive behavior. Large Multimodal Reasoning Models (LMRMs) have emerged as a\npromising paradigm, integrating modalities such as text, images, audio, and\nvideo to support complex reasoning capabilities and aiming to achieve\ncomprehensive perception, precise understanding, and deep reasoning. As\nresearch advances, multimodal reasoning has rapidly evolved from modular,\nperception-driven pipelines to unified, language-centric frameworks that offer\nmore coherent cross-modal understanding. While instruction tuning and\nreinforcement learning have improved model reasoning, significant challenges\nremain in omni-modal generalization, reasoning depth, and agentic behavior. To\naddress these issues, we present a comprehensive and structured survey of\nmultimodal reasoning research, organized around a four-stage developmental\nroadmap that reflects the field's shifting design philosophies and emerging\ncapabilities. First, we review early efforts based on task-specific modules,\nwhere reasoning was implicitly embedded across stages of representation,\nalignment, and fusion. Next, we examine recent approaches that unify reasoning\ninto multimodal LLMs, with advances such as Multimodal Chain-of-Thought (MCoT)\nand multimodal reinforcement learning enabling richer and more structured\nreasoning chains. Finally, drawing on empirical insights from challenging\nbenchmarks and experimental cases of OpenAI O3 and O4-mini, we discuss the\nconceptual direction of native large multimodal reasoning models (N-LMRMs),\nwhich aim to support scalable, agentic, and adaptive reasoning and planning in\ncomplex, real-world environments."}
{"id": "2505.04650", "pdf": "https://arxiv.org/pdf/2505.04650", "abs": "https://arxiv.org/abs/2505.04650", "authors": ["Kapil Wanaskar", "Gaytri Jena", "Magdalini Eirinaki"], "title": "Multimodal Benchmarking and Recommendation of Text-to-Image Generation Models", "categories": ["cs.GR", "cs.AI", "cs.IR", "cs.LG"], "comment": null, "summary": "This work presents an open-source unified benchmarking and evaluation\nframework for text-to-image generation models, with a particular focus on the\nimpact of metadata augmented prompts. Leveraging the DeepFashion-MultiModal\ndataset, we assess generated outputs through a comprehensive set of\nquantitative metrics, including Weighted Score, CLIP (Contrastive Language\nImage Pre-training)-based similarity, LPIPS (Learned Perceptual Image Patch\nSimilarity), FID (Frechet Inception Distance), and retrieval-based measures, as\nwell as qualitative analysis. Our results demonstrate that structured metadata\nenrichments greatly enhance visual realism, semantic fidelity, and model\nrobustness across diverse text-to-image architectures. While not a traditional\nrecommender system, our framework enables task-specific recommendations for\nmodel selection and prompt design based on evaluation metrics."}
{"id": "2505.00735", "pdf": "https://arxiv.org/pdf/2505.00735", "abs": "https://arxiv.org/abs/2505.00735", "authors": ["Jin Hyun Park", "Harine Choi", "Praewa Pitiphat"], "title": "Leveraging Depth Maps and Attention Mechanisms for Enhanced Image Inpainting", "categories": ["eess.IV", "cs.CV"], "comment": null, "summary": "Existing deep learning-based image inpainting methods typically rely on\nconvolutional networks with RGB images to reconstruct images. However, relying\nexclusively on RGB images may neglect important depth information, which plays\na critical role in understanding the spatial and structural context of a scene.\nJust as human vision leverages stereo cues to perceive depth, incorporating\ndepth maps into the inpainting process can enhance the model's ability to\nreconstruct images with greater accuracy and contextual awareness. In this\npaper, we propose a novel approach that incorporates both RGB and depth images\nfor enhanced image inpainting. Our models employ a dual encoder architecture,\nwhere one encoder processes the RGB image and the other handles the depth\nimage. The encoded features from both encoders are then fused in the decoder\nusing an attention mechanism, effectively integrating the RGB and depth\nrepresentations. We use two different masking strategies, line and square, to\ntest the robustness of the model under different types of occlusions. To\nfurther analyze the effectiveness of our approach, we use Gradient-weighted\nClass Activation Mapping (Grad-CAM) visualizations to examine the regions of\ninterest the model focuses on during inpainting. We show that incorporating\ndepth information alongside the RGB image significantly improves the\nreconstruction quality. Through both qualitative and quantitative comparisons,\nwe demonstrate that the depth-integrated model outperforms the baseline, with\nattention mechanisms further enhancing inpainting performance, as evidenced by\nmultiple evaluation metrics and visualization."}
{"id": "2505.04994", "pdf": "https://arxiv.org/pdf/2505.04994", "abs": "https://arxiv.org/abs/2505.04994", "authors": ["Lizhe Fang", "Yifei Wang", "Khashayar Gatmiry", "Lei Fang", "Yisen Wang"], "title": "Rethinking Invariance in In-context Learning", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "In-Context Learning (ICL) has emerged as a pivotal capability of\nauto-regressive large language models, yet it is hindered by a notable\nsensitivity to the ordering of context examples regardless of their mutual\nindependence. To address this issue, recent studies have introduced several\nvariant algorithms of ICL that achieve permutation invariance. However, many of\nthese do not exhibit comparable performance with the standard auto-regressive\nICL algorithm. In this work, we identify two crucial elements in the design of\nan invariant ICL algorithm: information non-leakage and context\ninterdependence, which are not simultaneously achieved by any of the existing\nmethods. These investigations lead us to the proposed Invariant ICL (InvICL), a\nmethodology designed to achieve invariance in ICL while ensuring the two\nproperties. Empirically, our findings reveal that InvICL surpasses previous\nmodels, both invariant and non-invariant, in most benchmark datasets,\nshowcasing superior generalization capabilities across varying input lengths.\nCode is available at https://github.com/PKU-ML/InvICL."}
{"id": "2505.04981", "pdf": "https://arxiv.org/pdf/2505.04981", "abs": "https://arxiv.org/abs/2505.04981", "authors": ["Zhifeng Hu", "Chong Han"], "title": "Graph Neural Network Aided Deep Reinforcement Learning for Resource Allocation in Dynamic Terahertz UAV Networks", "categories": ["cs.LG"], "comment": null, "summary": "Terahertz (THz) unmanned aerial vehicle (UAV) networks with flexible\ntopologies and ultra-high data rates are expected to empower numerous\napplications in security surveillance, disaster response, and environmental\nmonitoring, among others. However, the dynamic topologies hinder the efficient\nlong-term joint power and antenna array resource allocation for THz links among\nUAVs. Furthermore, the continuous nature of power and the discrete nature of\nantennas cause this joint resource allocation problem to be a mixed-integer\nnonlinear programming (MINLP) problem with non-convexity and NP-hardness.\nInspired by recent rapid advancements in deep reinforcement learning (DRL), a\ngraph neural network (GNN) aided DRL algorithm for resource allocation in the\ndynamic THz UAV network with an emphasis on self-node features (GLOVE) is\nproposed in this paper, with the aim of resource efficiency (RE) maximization.\nWhen training the allocation policy for each UAV, GLOVE learns the relationship\nbetween this UAV and its neighboring UAVs via GNN, while also emphasizing the\nimportant self-node features of this UAV. In addition, a multi-task structure\nis leveraged by GLOVE to cooperatively train resource allocation decisions for\nthe power and sub-arrays of all UAVs. Experimental results illustrate that\nGLOVE outperforms benchmark schemes in terms of the highest RE and the lowest\nlatency. Moreover, unlike the benchmark methods with severe packet loss, GLOVE\nmaintains zero packet loss during the entire training process, demonstrating\nits better robustness under the highly dynamic THz UAV network."}
{"id": "2505.04922", "pdf": "https://arxiv.org/pdf/2505.04922", "abs": "https://arxiv.org/abs/2505.04922", "authors": ["Xingzeng Lan", "Xing Duan", "Chen Chen", "Weiyu Lin", "Bo Wang"], "title": "Canny2Palm: Realistic and Controllable Palmprint Generation for Large-scale Pre-training", "categories": ["cs.CV"], "comment": null, "summary": "Palmprint recognition is a secure and privacy-friendly method of biometric\nidentification. One of the major challenges to improve palmprint recognition\naccuracy is the scarcity of palmprint data. Recently, a popular line of\nresearch revolves around the synthesis of virtual palmprints for large-scale\npre-training purposes. In this paper, we propose a novel synthesis method named\nCanny2Palm that extracts palm textures with Canny edge detector and uses them\nto condition a Pix2Pix network for realistic palmprint generation. By\nre-assembling palmprint textures from different identities, we are able to\ncreate new identities by seeding the generator with new assemblies. Canny2Palm\nnot only synthesizes realistic data following the distribution of real\npalmprints but also enables controllable diversity to generate large-scale new\nidentities. On open-set palmprint recognition benchmarks, models pre-trained\nwith Canny2Palm synthetic data outperform the state-of-the-art with up to 7.2%\nhigher identification accuracy. Moreover, the performance of models pre-trained\nwith Canny2Palm continues to improve given 10,000 synthetic IDs while those\nwith existing methods already saturate, demonstrating the potential of our\nmethod for large-scale pre-training."}
{"id": "2505.04677", "pdf": "https://arxiv.org/pdf/2505.04677", "abs": "https://arxiv.org/abs/2505.04677", "authors": ["Julien Narboux", "Walther Neuper", "Pedro Quaresma"], "title": "Proceedings The 13th International Workshop on Theorem proving components for Educational software", "categories": ["cs.LO", "cs.AI", "cs.LG"], "comment": null, "summary": "The ThEdu series pursues the smooth transition from an intuitive way of doing\nmathematics at secondary school to a more formal approach to the subject in\nSTEM education while favoring software support for this transition by\nexploiting the power of theorem-proving technologies. What follows is a brief\ndescription of how the present volume contributes to this enterprise. The 13th\nInternational Workshop on Theorem Proving Components for Educational Software\n(ThEdu'24), was a satellite event of the CADE29, part of IJCAR 2024, Nancy,\nFrance. ThEdu'24 was a vibrant workshop, with one invited talk by Jeremy Avigad\n(Carnegie Mellon University) and 14 submitted talks. An open call for papers\nwas then issued and attracted 9 submissions. Eight of those submissions have\nbeen accepted by our reviewers. The resulting revised papers are collected in\nthe present volume. The contributions in this volume are a faithful\nrepresentation of the wide spectrum of ThEdu, ranging from those more focused\non the automated deduction research, not losing track of the possible\napplications in an educational setting, to those focused on the applications,\nin educational settings, of automated deduction tools and methods. We, the\nvolume editors, hope that this collection of papers will further promote the\ndevelopment of theorem-proving-based software and that it will allow to improve\nthe mutual understanding between computer scientists, mathematicians, and\nstakeholders in education. While this volume goes to press, the next edition of\nthe ThEdu workshop is being prepared: ThEdu'25 will be a satellite event of the\n30th international Conference on Automated DEduction (CADE-30), July 28th -\nAugust 2nd, 2025, Stuttgart, Germany."}
{"id": "2505.03838", "pdf": "https://arxiv.org/pdf/2505.03838", "abs": "https://arxiv.org/abs/2505.03838", "authors": ["Ting Yu Tsai", "An Yu", "Meghana Spurthi Maadugundu", "Ishrat Jahan Mohima", "Umme Habiba Barsha", "Mei-Hwa F. Chen", "Balakrishnan Prabhakaran", "Ming-Ching Chang"], "title": "IntelliCardiac: An Intelligent Platform for Cardiac Image Segmentation and Classification", "categories": ["eess.IV", "cs.AI", "cs.CV"], "comment": null, "summary": "Precise and effective processing of cardiac imaging data is critical for the\nidentification and management of the cardiovascular diseases. We introduce\nIntelliCardiac, a comprehensive, web-based medical image processing platform\nfor the automatic segmentation of 4D cardiac images and disease classification,\nutilizing an AI model trained on the publicly accessible ACDC dataset. The\nsystem, intended for patients, cardiologists, and healthcare professionals,\noffers an intuitive interface and uses deep learning models to identify\nessential heart structures and categorize cardiac diseases. The system supports\nanalysis of both the right and left ventricles as well as myocardium, and then\nclassifies patient's cardiac images into five diagnostic categories: dilated\ncardiomyopathy, myocardial infarction, hypertrophic cardiomyopathy, right\nventricular abnormality, and no disease. IntelliCardiac combines a deep\nlearning-based segmentation model with a two-step classification pipeline. The\nsegmentation module gains an overall accuracy of 92.6%. The classification\nmodule, trained on characteristics taken from segmented heart structures,\nachieves 98% accuracy in five categories. These results exceed the performance\nof the existing state-of-the-art methods that integrate both segmentation and\nclassification models. IntelliCardiac, which supports real-time visualization,\nworkflow integration, and AI-assisted diagnostics, has great potential as a\nscalable, accurate tool for clinical decision assistance in cardiac imaging and\ndiagnosis."}
{"id": "2505.05016", "pdf": "https://arxiv.org/pdf/2505.05016", "abs": "https://arxiv.org/abs/2505.05016", "authors": ["Cedric Waterschoot", "Nava Tintarev", "Francesco Barile"], "title": "The Pitfalls of Growing Group Complexity: LLMs and Social Choice-Based Aggregation for Group Recommendations", "categories": ["cs.CL", "cs.IR"], "comment": "To be published in: Adjunct Proceedings of the 33rd ACM Conference on\n  User Modeling, Adaptation and Personalization (UMAP Adjunct '25), June\n  16--19, 2025, New York City, NY, USA Accepted at the 4th Workshop on Group\n  Modeling, Adaptation and Personalization (GMAP), co-located at UMAP 2025", "summary": "Large Language Models (LLMs) are increasingly applied in recommender systems\naimed at both individuals and groups. Previously, Group Recommender Systems\n(GRS) often used social choice-based aggregation strategies to derive a single\nrecommendation based on the preferences of multiple people. In this paper, we\ninvestigate under which conditions language models can perform these strategies\ncorrectly based on zero-shot learning and analyse whether the formatting of the\ngroup scenario in the prompt affects accuracy. We specifically focused on the\nimpact of group complexity (number of users and items), different LLMs,\ndifferent prompting conditions, including In-Context learning or generating\nexplanations, and the formatting of group preferences. Our results show that\nperformance starts to deteriorate when considering more than 100 ratings.\nHowever, not all language models were equally sensitive to growing group\ncomplexity. Additionally, we showed that In-Context Learning (ICL) can\nsignificantly increase the performance at higher degrees of group complexity,\nwhile adding other prompt modifications, specifying domain cues or prompting\nfor explanations, did not impact accuracy. We conclude that future research\nshould include group complexity as a factor in GRS evaluation due to its effect\non LLM performance. Furthermore, we showed that formatting the group scenarios\ndifferently, such as rating lists per user or per item, affected accuracy. All\nin all, our study implies that smaller LLMs are capable of generating group\nrecommendations under the right conditions, making the case for using smaller\nmodels that require less computing power and costs."}
{"id": "2505.05015", "pdf": "https://arxiv.org/pdf/2505.05015", "abs": "https://arxiv.org/abs/2505.05015", "authors": ["Roberto Dillon", "Arushi"], "title": "An Agent-Based Modeling Approach to Free-Text Keyboard Dynamics for Continuous Authentication", "categories": ["cs.LG", "cs.AI", "cs.CR", "68T10, 62H30", "I.2.6; I.5.4; I.6.3"], "comment": "16 pages, 5 figures, 12 tables", "summary": "Continuous authentication systems leveraging free-text keyboard dynamics\noffer a promising additional layer of security in a multifactor authentication\nsetup that can be used in a transparent way with no impact on user experience.\nThis study investigates the efficacy of behavioral biometrics by employing an\nAgent-Based Model (ABM) to simulate diverse typing profiles across mechanical\nand membrane keyboards. Specifically, we generated synthetic keystroke data\nfrom five unique agents, capturing features related to dwell time, flight time,\nand error rates within sliding 5-second windows updated every second. Two\nmachine learning approaches, One-Class Support Vector Machine (OC-SVM) and\nRandom Forest (RF), were evaluated for user verification. Results revealed a\nstark contrast in performance: while One-Class SVM failed to differentiate\nindividual users within each group, Random Forest achieved robust\nintra-keyboard user recognition (Accuracy > 0.7) but struggled to generalize\nacross keyboards for the same user, highlighting the significant impact of\nkeyboard hardware on typing behavior. These findings suggest that: (1)\nkeyboard-specific user profiles may be necessary for reliable authentication,\nand (2) ensemble methods like RF outperform One-Class SVM in capturing\nfine-grained user-specific patterns."}
{"id": "2505.04938", "pdf": "https://arxiv.org/pdf/2505.04938", "abs": "https://arxiv.org/abs/2505.04938", "authors": ["Ying Zhang", "Shuai Guo", "Chenxi Sun", "Yuchen Zhu", "Jinhai Xiang"], "title": "FF-PNet: A Pyramid Network Based on Feature and Field for Brain Image Registration", "categories": ["cs.CV", "cs.IR"], "comment": null, "summary": "In recent years, deformable medical image registration techniques have made\nsignificant progress. However, existing models still lack efficiency in\nparallel extraction of coarse and fine-grained features. To address this, we\nconstruct a new pyramid registration network based on feature and deformation\nfield (FF-PNet). For coarse-grained feature extraction, we design a Residual\nFeature Fusion Module (RFFM), for fine-grained image deformation, we propose a\nResidual Deformation Field Fusion Module (RDFFM). Through the parallel\noperation of these two modules, the model can effectively handle complex image\ndeformations. It is worth emphasizing that the encoding stage of FF-PNet only\nemploys traditional convolutional neural networks without any attention\nmechanisms or multilayer perceptrons, yet it still achieves remarkable\nimprovements in registration accuracy, fully demonstrating the superior feature\ndecoding capabilities of RFFM and RDFFM. We conducted extensive experiments on\nthe LPBA and OASIS datasets. The results show our network consistently\noutperforms popular methods in metrics like the Dice Similarity Coefficient."}
{"id": "2505.04725", "pdf": "https://arxiv.org/pdf/2505.04725", "abs": "https://arxiv.org/abs/2505.04725", "authors": ["Robin Chhabra", "Farzaneh Abdollahi"], "title": "Geometric Fault-Tolerant Neural Network Tracking Control of Unknown Systems on Matrix Lie Groups", "categories": ["eess.SY", "cs.AI", "cs.RO", "cs.SY", "math.DS"], "comment": null, "summary": "We present a geometric neural network-based tracking controller for systems\nevolving on matrix Lie groups under unknown dynamics, actuator faults, and\nbounded disturbances. Leveraging the left-invariance of the tangent bundle of\nmatrix Lie groups, viewed as an embedded submanifold of the vector space\n$\\R^{N\\times N}$, we propose a set of learning rules for neural network weights\nthat are intrinsically compatible with the Lie group structure and do not\nrequire explicit parameterization. Exploiting the geometric properties of Lie\ngroups, this approach circumvents parameterization singularities and enables a\nglobal search for optimal weights. The ultimate boundedness of all error\nsignals -- including the neural network weights, the coordinate-free\nconfiguration error function, and the tracking velocity error -- is established\nusing Lyapunov's direct method. To validate the effectiveness of the proposed\nmethod, we provide illustrative simulation results for decentralized formation\ncontrol of multi-agent systems on the Special Euclidean group."}
{"id": "2505.04172", "pdf": "https://arxiv.org/pdf/2505.04172", "abs": "https://arxiv.org/abs/2505.04172", "authors": ["Jiankai Tang", "Kegang Wang", "Yingke Ding", "Jiatong Ji", "Zeyu Wang", "Xiyuxing Zhang", "Ping Chen", "Yuanchun Shi", "Yuntao Wang"], "title": "A Dataset and Toolkit for Multiparameter Cardiovascular Physiology Sensing on Rings", "categories": ["eess.IV", "cs.HC", "physics.med-ph"], "comment": null, "summary": "Smart rings offer a convenient way to continuously and unobtrusively monitor\ncardiovascular physiological signals. However, a gap remains between the ring\nhardware and reliable methods for estimating cardiovascular parameters, partly\ndue to the lack of publicly available datasets and standardized analysis tools.\nIn this work, we present $\\tau$-Ring, the first open-source ring-based dataset\ndesigned for cardiovascular physiological sensing. The dataset comprises\nphotoplethysmography signals (infrared and red channels) and 3-axis\naccelerometer data collected from two rings (reflective and transmissive\noptical paths), with 28.21 hours of raw data from 34 subjects across seven\nactivities. $\\tau$-Ring encompasses both stationary and motion scenarios, as\nwell as stimulus-evoked abnormal physiological states, annotated with four\nground-truth labels: heart rate, respiratory rate, oxygen saturation, and blood\npressure. Using our proposed RingTool toolkit, we evaluated three widely-used\nphysics-based methods and four cutting-edge deep learning approaches. Our\nresults show superior performance compared to commercial rings, achieving best\nMAE values of 5.18 BPM for heart rate, 2.98 BPM for respiratory rate, 3.22\\%\nfor oxygen saturation, and 13.33/7.56 mmHg for systolic/diastolic blood\npressure estimation. The open-sourced dataset and toolkit aim to foster further\nresearch and community-driven advances in ring-based cardiovascular health\nsensing."}
{"id": "2505.05017", "pdf": "https://arxiv.org/pdf/2505.05017", "abs": "https://arxiv.org/abs/2505.05017", "authors": ["Yuntai Bao", "Xuhong Zhang", "Tianyu Du", "Xinkui Zhao", "Jiang Zong", "Hao Peng", "Jianwei Yin"], "title": "Scalable Multi-Stage Influence Function for Large Language Models via Eigenvalue-Corrected Kronecker-Factored Parameterization", "categories": ["cs.CL"], "comment": "9 pages, accepted by IJCAI 2025", "summary": "Pre-trained large language models (LLMs) are commonly fine-tuned to adapt to\ndownstream tasks. Since the majority of knowledge is acquired during\npre-training, attributing the predictions of fine-tuned LLMs to their\npre-training data may provide valuable insights. Influence functions have been\nproposed as a means to explain model predictions based on training data.\nHowever, existing approaches fail to compute ``multi-stage'' influence and lack\nscalability to billion-scale LLMs.\n  In this paper, we propose the multi-stage influence function to attribute the\ndownstream predictions of fine-tuned LLMs to pre-training data under the\nfull-parameter fine-tuning paradigm. To enhance the efficiency and practicality\nof our multi-stage influence function, we leverage Eigenvalue-corrected\nKronecker-Factored (EK-FAC) parameterization for efficient approximation.\nEmpirical results validate the superior scalability of EK-FAC approximation and\nthe effectiveness of our multi-stage influence function. Additionally, case\nstudies on a real-world LLM, dolly-v2-3b, demonstrate its interpretive power,\nwith exemplars illustrating insights provided by multi-stage influence\nestimates. Our code is public at\nhttps://github.com/colored-dye/multi_stage_influence_function."}
{"id": "2505.05019", "pdf": "https://arxiv.org/pdf/2505.05019", "abs": "https://arxiv.org/abs/2505.05019", "authors": ["Waldemar Hahn", "Jan-Niklas Eckardt", "Christoph Röllig", "Martin Sedlmayr", "Jan Moritz Middeke", "Markus Wolfien"], "title": "Generating Reliable Synthetic Clinical Trial Data: The Role of Hyperparameter Optimization and Domain Constraints", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "The generation of synthetic clinical trial data offers a promising approach\nto mitigating privacy concerns and data accessibility limitations in medical\nresearch. However, ensuring that synthetic datasets maintain high fidelity,\nutility, and adherence to domain-specific constraints remains a key challenge.\nWhile hyperparameter optimization (HPO) has been shown to improve generative\nmodel performance, the effectiveness of different optimization strategies for\nsynthetic clinical data remains unclear. This study systematically evaluates\nfour HPO strategies across eight generative models, comparing single-metric\noptimization against compound metric optimization approaches. Our results\ndemonstrate that HPO consistently improves synthetic data quality, with TVAE,\nCTGAN, and CTAB-GAN+ achieving improvements of up to 60%, 39%, and 38%,\nrespectively. Compound metric optimization outperformed single-metric\nstrategies, producing more balanced and generalizable synthetic datasets.\nInterestingly, HPO alone is insufficient to ensure clinically valid synthetic\ndata, as all models exhibited violations of fundamental survival constraints.\nPreprocessing and postprocessing played a crucial role in reducing these\nviolations, as models lacking robust processing steps produced invalid data in\nup to 61% of cases. These findings underscore the necessity of integrating\nexplicit domain knowledge alongside HPO to create high quality synthetic\ndatasets. Our study provides actionable recommendations for improving synthetic\ndata generation, with future research needed to refine metric selection and\nvalidate these findings on larger datasets to enhance clinical applicability."}
{"id": "2505.04941", "pdf": "https://arxiv.org/pdf/2505.04941", "abs": "https://arxiv.org/abs/2505.04941", "authors": ["Jiepan Li", "He Huang", "Yu Sheng", "Yujun Guo", "Wei He"], "title": "Building-Guided Pseudo-Label Learning for Cross-Modal Building Damage Mapping", "categories": ["cs.CV"], "comment": null, "summary": "Accurate building damage assessment using bi-temporal multi-modal remote\nsensing images is essential for effective disaster response and recovery\nplanning. This study proposes a novel Building-Guided Pseudo-Label Learning\nFramework to address the challenges of mapping building damage from\npre-disaster optical and post-disaster SAR images. First, we train a series of\nbuilding extraction models using pre-disaster optical images and building\nlabels. To enhance building segmentation, we employ multi-model fusion and\ntest-time augmentation strategies to generate pseudo-probabilities, followed by\na low-uncertainty pseudo-label training method for further refinement. Next, a\nchange detection model is trained on bi-temporal cross-modal images and damaged\nbuilding labels. To improve damage classification accuracy, we introduce a\nbuilding-guided low-uncertainty pseudo-label refinement strategy, which\nleverages building priors from the previous step to guide pseudo-label\ngeneration for damaged buildings, reducing uncertainty and enhancing\nreliability. Experimental results on the 2025 IEEE GRSS Data Fusion Contest\ndataset demonstrate the effectiveness of our approach, which achieved the\nhighest mIoU score (54.28%) and secured first place in the competition."}
{"id": "2505.04732", "pdf": "https://arxiv.org/pdf/2505.04732", "abs": "https://arxiv.org/abs/2505.04732", "authors": ["Sriram Gopalakrishnan", "Sunandita Patra"], "title": "QBD-RankedDataGen: Generating Custom Ranked Datasets for Improving Query-By-Document Search Using LLM-Reranking with Reduced Human Effort", "categories": ["cs.IR", "cs.AI"], "comment": "13 pages", "summary": "The Query-By-Document (QBD) problem is an information retrieval problem where\nthe query is a document, and the retrieved candidates are documents that match\nthe query document, often in a domain or query specific manner. This can be\ncrucial for tasks such as patent matching, legal or compliance case retrieval,\nand academic literature review. Existing retrieval methods, including keyword\nsearch and document embeddings, can be optimized with domain-specific datasets\nto improve QBD search performance. However, creating these domain-specific\ndatasets is often costly and time-consuming. Our work introduces a process to\ngenerate custom QBD-search datasets and compares a set of methods to use in\nthis problem, which we refer to as QBD-RankedDatagen. We provide a comparative\nanalysis of our proposed methods in terms of cost, speed, and the human\ninterface with the domain experts. The methods we compare leverage Large\nLanguage Models (LLMs) which can incorporate domain expert input to produce\ndocument scores and rankings, as well as explanations for human review. The\nprocess and methods for it that we present can significantly reduce human\neffort in dataset creation for custom domains while still obtaining sufficient\nexpert knowledge for tuning retrieval models. We evaluate our methods on QBD\ndatasets from the Text Retrieval Conference (TREC) and finetune the parameters\nof the BM25 model -- which is used in many industrial-strength search engines\nlike OpenSearch -- using the generated data."}
{"id": "2409.09085", "pdf": "https://arxiv.org/pdf/2409.09085", "abs": "https://arxiv.org/abs/2409.09085", "authors": ["Tianyi Chen", "Xiaoyi Qu", "David Aponte", "Colby Banbury", "Jongwoo Ko", "Tianyu Ding", "Yong Ma", "Vladimir Lyapunov", "Ilya Zharkov", "Luming Liang"], "title": "HESSO: Towards Automatic Efficient and User Friendly Any Neural Network Training and Pruning", "categories": ["cs.LG", "cs.CV", "eess.IV"], "comment": "19 pages, 6 figures", "summary": "Structured pruning is one of the most popular approaches to effectively\ncompress the heavy deep neural networks (DNNs) into compact sub-networks while\nretaining performance. The existing methods suffer from multi-stage procedures\nalong with significant engineering efforts and human expertise. The\nOnly-Train-Once (OTO) series has been recently proposed to resolve the many\npain points by streamlining the workflow by automatically conducting (i) search\nspace generation, (ii) structured sparse optimization, and (iii) sub-network\nconstruction. However, the built-in sparse optimizers in the OTO series, i.e.,\nthe Half-Space Projected Gradient (HSPG) family, have limitations that require\nhyper-parameter tuning and the implicit controls of the sparsity exploration,\nconsequently requires intervening by human expertise. To address such\nlimitations, we propose a Hybrid Efficient Structured Sparse Optimizer (HESSO).\nHESSO could automatically and efficiently train a DNN to produce a\nhigh-performing subnetwork. Meanwhile, it is almost tuning-free and enjoys\nuser-friendly integration for generic training applications. To address another\ncommon issue of irreversible performance collapse observed in pruning DNNs, we\nfurther propose a Corrective Redundant Identification Cycle (CRIC) for reliably\nidentifying indispensable structures. We numerically demonstrate the efficacy\nof HESSO and its enhanced version HESSO-CRIC on a variety of applications\nranging from computer vision to natural language processing, including large\nlanguage model. The numerical results showcase that HESSO can achieve\ncompetitive even superior performance to varying state-of-the-arts and support\nmost DNN architectures. Meanwhile, CRIC can effectively prevent the\nirreversible performance collapse and further enhance the performance of HESSO\non certain applications."}
{"id": "2505.05026", "pdf": "https://arxiv.org/pdf/2505.05026", "abs": "https://arxiv.org/abs/2505.05026", "authors": ["Jaehyun Jeon", "Janghan Yoon", "Minsoo Kim", "Sumin Shim", "Yejin Choi", "Hanbin Kim", "Youngjae Yu"], "title": "G-FOCUS: Towards a Robust Method for Assessing UI Design Persuasiveness", "categories": ["cs.CL", "cs.LG"], "comment": "31 pages, 17 figures", "summary": "Evaluating user interface (UI) design effectiveness extends beyond aesthetics\nto influencing user behavior, a principle central to Design Persuasiveness. A/B\ntesting is the predominant method for determining which UI variations drive\nhigher user engagement, but it is costly and time-consuming. While recent\nVision-Language Models (VLMs) can process automated UI analysis, current\napproaches focus on isolated design attributes rather than comparative\npersuasiveness-the key factor in optimizing user interactions. To address this,\nwe introduce WiserUI-Bench, a benchmark designed for Pairwise UI Design\nPersuasiveness Assessment task, featuring 300 real-world UI image pairs labeled\nwith A/B test results and expert rationales. Additionally, we propose G-FOCUS,\na novel inference-time reasoning strategy that enhances VLM-based\npersuasiveness assessment by reducing position bias and improving evaluation\naccuracy. Experimental results show that G-FOCUS surpasses existing inference\nstrategies in consistency and accuracy for pairwise UI evaluation. Through\npromoting VLM-driven evaluation of UI persuasiveness, our work offers an\napproach to complement A/B testing, propelling progress in scalable UI\npreference modeling and design optimization. Code and data will be released\npublicly."}
{"id": "2505.05020", "pdf": "https://arxiv.org/pdf/2505.05020", "abs": "https://arxiv.org/abs/2505.05020", "authors": ["Ruwen Fulek", "Markus Lange-Hegermann"], "title": "Generative Models for Long Time Series: Approximately Equivariant Recurrent Network Structures for an Adjusted Training Scheme", "categories": ["cs.LG"], "comment": null, "summary": "We present a simple yet effective generative model for time series data based\non a Variational Autoencoder (VAE) with recurrent layers, referred to as the\nRecurrent Variational Autoencoder with Subsequent Training (RVAE-ST). Our\nmethod introduces an adapted training scheme that progressively increases the\nsequence length, addressing the challenge recurrent layers typically face when\nmodeling long sequences. By leveraging the recurrent architecture, the model\nmaintains a constant number of parameters regardless of sequence length. This\ndesign encourages approximate time-shift equivariance and enables efficient\nmodeling of long-range temporal dependencies. Rather than introducing a\nfundamentally new architecture, we show that a carefully composed combination\nof known components can match or outperform state-of-the-art generative models\non several benchmark datasets. Our model performs particularly well on time\nseries that exhibit quasi-periodic structure,while remaining competitive on\ndatasets with more irregular or partially non-stationary behavior. We evaluate\nits performance using ELBO, Fr\\'echet Distance, discriminative scores, and\nvisualizations of the learned embeddings."}
{"id": "2505.04946", "pdf": "https://arxiv.org/pdf/2505.04946", "abs": "https://arxiv.org/abs/2505.04946", "authors": ["Xuyang Guo", "Jiayan Huo", "Zhenmei Shi", "Zhao Song", "Jiahao Zhang", "Jiale Zhao"], "title": "T2VTextBench: A Human Evaluation Benchmark for Textual Control in Video Generation Models", "categories": ["cs.CV", "cs.AI", "cs.CL", "cs.LG"], "comment": null, "summary": "Thanks to recent advancements in scalable deep architectures and large-scale\npretraining, text-to-video generation has achieved unprecedented capabilities\nin producing high-fidelity, instruction-following content across a wide range\nof styles, enabling applications in advertising, entertainment, and education.\nHowever, these models' ability to render precise on-screen text, such as\ncaptions or mathematical formulas, remains largely untested, posing significant\nchallenges for applications requiring exact textual accuracy. In this work, we\nintroduce T2VTextBench, the first human-evaluation benchmark dedicated to\nevaluating on-screen text fidelity and temporal consistency in text-to-video\nmodels. Our suite of prompts integrates complex text strings with dynamic scene\nchanges, testing each model's ability to maintain detailed instructions across\nframes. We evaluate ten state-of-the-art systems, ranging from open-source\nsolutions to commercial offerings, and find that most struggle to generate\nlegible, consistent text. These results highlight a critical gap in current\nvideo generators and provide a clear direction for future research aimed at\nenhancing textual manipulation in video synthesis."}
{"id": "2505.04759", "pdf": "https://arxiv.org/pdf/2505.04759", "abs": "https://arxiv.org/abs/2505.04759", "authors": ["Mohit Chaudhary", "Chirag Jain", "Preethu Rose Anish"], "title": "Exploring Zero-Shot App Review Classification with ChatGPT: Challenges and Potential", "categories": ["cs.SE", "cs.AI"], "comment": null, "summary": "App reviews are a critical source of user feedback, offering valuable\ninsights into an app's performance, features, usability, and overall user\nexperience. Effectively analyzing these reviews is essential for guiding app\ndevelopment, prioritizing feature updates, and enhancing user satisfaction.\nClassifying reviews into functional and non-functional requirements play a\npivotal role in distinguishing feedback related to specific app features\n(functional requirements) from feedback concerning broader quality attributes,\nsuch as performance, usability, and reliability (non-functional requirements).\nBoth categories are integral to informed development decisions. Traditional\napproaches to classifying app reviews are hindered by the need for large,\ndomain-specific datasets, which are often costly and time-consuming to curate.\nThis study explores the potential of zero-shot learning with ChatGPT for\nclassifying app reviews into four categories: functional requirement,\nnon-functional requirement, both, or neither. We evaluate ChatGPT's performance\non a benchmark dataset of 1,880 manually annotated reviews from ten diverse\napps spanning multiple domains. Our findings demonstrate that ChatGPT achieves\na robust F1 score of 0.842 in review classification, despite certain challenges\nand limitations. Additionally, we examine how factors such as review\nreadability and length impact classification accuracy and conduct a manual\nanalysis to identify review categories more prone to misclassification."}
{"id": "2503.03355", "pdf": "https://arxiv.org/pdf/2503.03355", "abs": "https://arxiv.org/abs/2503.03355", "authors": ["Zhihao Zhan", "Wang Pang", "Xiang Zhu", "Yechao Bai"], "title": "Rethinking Video Super-Resolution: Towards Diffusion-Based Methods without Motion Alignment", "categories": ["cs.CV", "cs.LG", "eess.IV"], "comment": null, "summary": "In this work, we rethink the approach to video super-resolution by\nintroducing a method based on the Diffusion Posterior Sampling framework,\ncombined with an unconditional video diffusion transformer operating in latent\nspace. The video generation model, a diffusion transformer, functions as a\nspace-time model. We argue that a powerful model, which learns the physics of\nthe real world, can easily handle various kinds of motion patterns as prior\nknowledge, thus eliminating the need for explicit estimation of optical flows\nor motion parameters for pixel alignment. Furthermore, a single instance of the\nproposed video diffusion transformer model can adapt to different sampling\nconditions without re-training. Empirical results on synthetic and real-world\ndatasets illustrate the feasibility of diffusion-based, alignment-free video\nsuper-resolution."}
{"id": "2505.05040", "pdf": "https://arxiv.org/pdf/2505.05040", "abs": "https://arxiv.org/abs/2505.05040", "authors": ["Matīss Rikters", "Edison Marrese-Taylor"], "title": "Image-Text Relation Prediction for Multilingual Tweets", "categories": ["cs.CL", "cs.AI", "cs.CV"], "comment": null, "summary": "Various social networks have been allowing media uploads for over a decade\nnow. Still, it has not always been clear what is their relation with the posted\ntext or even if there is any at all. In this work, we explore how multilingual\nvision-language models tackle the task of image-text relation prediction in\ndifferent languages, and construct a dedicated balanced benchmark data set from\nTwitter posts in Latvian along with their manual translations into English. We\ncompare our results to previous work and show that the more recently released\nvision-language model checkpoints are becoming increasingly capable at this\ntask, but there is still much room for further improvement."}
{"id": "2505.05034", "pdf": "https://arxiv.org/pdf/2505.05034", "abs": "https://arxiv.org/abs/2505.05034", "authors": ["Wei Chen", "Shigui Li", "Jiacheng Li", "Junmei Yang", "John Paisley", "Delu Zeng"], "title": "Dequantified Diffusion Schrödinger Bridge for Density Ratio Estimation", "categories": ["cs.LG", "stat.ML"], "comment": null, "summary": "Density ratio estimation is fundamental to tasks involving $f$-divergences,\nyet existing methods often fail under significantly different distributions or\ninadequately overlap supports, suffering from the \\textit{density-chasm} and\nthe \\textit{support-chasm} problems. Additionally, prior approaches yield\ndivergent time scores near boundaries, leading to instability. We propose\n$\\text{D}^3\\text{RE}$, a unified framework for robust and efficient density\nratio estimation. It introduces the Dequantified Diffusion-Bridge Interpolant\n(DDBI), which expands support coverage and stabilizes time scores via diffusion\nbridges and Gaussian dequantization. Building on DDBI, the Dequantified\nSchr\\\"odinger-Bridge Interpolant (DSBI) incorporates optimal transport to solve\nthe Schr\\\"odinger bridge problem, enhancing accuracy and efficiency. Our method\noffers uniform approximation and bounded time scores in theory, and outperforms\nbaselines empirically in mutual information and density estimation tasks."}
{"id": "2505.04962", "pdf": "https://arxiv.org/pdf/2505.04962", "abs": "https://arxiv.org/abs/2505.04962", "authors": ["Utsav Rai", "Hardik Mehta", "Vismay Vakharia", "Aditya Choudhary", "Amit Parmar", "Rolif Lima", "Kaushik Das"], "title": "An Efficient Method for Accurate Pose Estimation and Error Correction of Cuboidal Objects", "categories": ["cs.CV", "cs.RO"], "comment": "Accepted in IEEE/RSJ IROS 2022 Workshop on Mobile Manipulation and\n  Embodied Intelligence (MOMA)", "summary": "The proposed system outlined in this paper is a solution to a use case that\nrequires the autonomous picking of cuboidal objects from an organized or\nunorganized pile with high precision. This paper presents an efficient method\nfor precise pose estimation of cuboid-shaped objects, which aims to reduce\nerrors in target pose in a time-efficient manner. Typical pose estimation\nmethods like global point cloud registrations are prone to minor pose errors\nfor which local registration algorithms are generally used to improve pose\naccuracy. However, due to the execution time overhead and uncertainty in the\nerror of the final achieved pose, an alternate, linear time approach is\nproposed for pose error estimation and correction. This paper presents an\noverview of the solution followed by a detailed description of individual\nmodules of the proposed algorithm."}
{"id": "2505.04784", "pdf": "https://arxiv.org/pdf/2505.04784", "abs": "https://arxiv.org/abs/2505.04784", "authors": ["Pedro Pinacho-Davidson", "Fernando Gutierrez", "Pablo Zapata", "Rodolfo Vergara", "Pablo Aqueveque"], "title": "A Proposal for Evaluating the Operational Risk for ChatBots based on Large Language Models", "categories": ["cs.CR", "cs.AI", "cs.CY"], "comment": "21 pages", "summary": "The emergence of Generative AI (Gen AI) and Large Language Models (LLMs) has\nenabled more advanced chatbots capable of human-like interactions. However,\nthese conversational agents introduce a broader set of operational risks that\nextend beyond traditional cybersecurity considerations. In this work, we\npropose a novel, instrumented risk-assessment metric that simultaneously\nevaluates potential threats to three key stakeholders: the service-providing\norganization, end users, and third parties. Our approach incorporates the\ntechnical complexity required to induce erroneous behaviors in the\nchatbot--ranging from non-induced failures to advanced prompt-injection\nattacks--as well as contextual factors such as the target industry, user age\nrange, and vulnerability severity. To validate our metric, we leverage Garak,\nan open-source framework for LLM vulnerability testing. We further enhance\nGarak to capture a variety of threat vectors (e.g., misinformation, code\nhallucinations, social engineering, and malicious code generation). Our\nmethodology is demonstrated in a scenario involving chatbots that employ\nretrieval-augmented generation (RAG), showing how the aggregated risk scores\nguide both short-term mitigation and longer-term improvements in model design\nand deployment. The results underscore the importance of multi-dimensional risk\nassessments in operationalizing secure, reliable AI-driven conversational\nsystems."}
{"id": "2503.10686", "pdf": "https://arxiv.org/pdf/2503.10686", "abs": "https://arxiv.org/abs/2503.10686", "authors": ["Anzhe Cheng", "Chenzhong Yin", "Yu Chang", "Heng Ping", "Shixuan Li", "Shahin Nazarian", "Paul Bogdan"], "title": "MaskAttn-UNet: A Mask Attention-Driven Framework for Universal Low-Resolution Image Segmentation", "categories": ["cs.CV", "cs.LG", "eess.IV"], "comment": null, "summary": "Low-resolution image segmentation is crucial in real-world applications such\nas robotics, augmented reality, and large-scale scene understanding, where\nhigh-resolution data is often unavailable due to computational constraints. To\naddress this challenge, we propose MaskAttn-UNet, a novel segmentation\nframework that enhances the traditional U-Net architecture via a mask attention\nmechanism. Our model selectively emphasizes important regions while suppressing\nirrelevant backgrounds, thereby improving segmentation accuracy in cluttered\nand complex scenes. Unlike conventional U-Net variants, MaskAttn-UNet\neffectively balances local feature extraction with broader contextual\nawareness, making it particularly well-suited for low-resolution inputs. We\nevaluate our approach on three benchmark datasets with input images rescaled to\n128x128 and demonstrate competitive performance across semantic, instance, and\npanoptic segmentation tasks. Our results show that MaskAttn-UNet achieves\naccuracy comparable to state-of-the-art methods at significantly lower\ncomputational cost than transformer-based models, making it an efficient and\nscalable solution for low-resolution segmentation in resource-constrained\nscenarios."}
{"id": "2505.05056", "pdf": "https://arxiv.org/pdf/2505.05056", "abs": "https://arxiv.org/abs/2505.05056", "authors": ["Linrong Pan", "Chenglong Jiang", "Gaoze Hou", "Ying Gao"], "title": "Teochew-Wild: The First In-the-wild Teochew Dataset with Orthographic Annotations", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "This paper reports the construction of the Teochew-Wild, a speech corpus of\nthe Teochew dialect. The corpus includes 18.9 hours of in-the-wild Teochew\nspeech data from multiple speakers, covering both formal and colloquial\nexpressions, with precise orthographic and pinyin annotations. Additionally, we\nprovide supplementary text processing tools and resources to propel research\nand applications in speech tasks for this low-resource language, such as\nautomatic speech recognition (ASR) and text-to-speech (TTS). To the best of our\nknowledge, this is the first publicly available Teochew dataset with accurate\northographic annotations. We conduct experiments on the corpus, and the results\nvalidate its effectiveness in ASR and TTS tasks."}
{"id": "2505.05047", "pdf": "https://arxiv.org/pdf/2505.05047", "abs": "https://arxiv.org/abs/2505.05047", "authors": ["Azgar Ali Noor Ahamed"], "title": "Neural Pathways to Program Success: Hopfield Networks for PERT Analysis", "categories": ["cs.LG"], "comment": null, "summary": "Project and task scheduling under uncertainty remains a fundamental challenge\nin program and project management, where accurate estimation of task durations\nand dependencies is critical for delivering complex, multi project systems. The\nProgram Evaluation and Review Technique provides a probabilistic framework to\nmodel task variability and critical paths. In this paper, the author presents a\nnovel formulation of PERT scheduling as an energy minimization problem within a\nHopfield neural network architecture. By mapping task start times and\nprecedence constraints into a neural computation framework, the networks\ninherent optimization dynamics is exploited to approximate globally consistent\nschedules. The author addresses key theoretical issues related to energy\nfunction differentiability, constraint encoding, and convergence, and extends\nthe Hopfield model for structured precedence graphs. Numerical simulations on\nsynthetic project networks comprising up to 1000 tasks demonstrate the\nviability of this approach, achieving near optimal makespans with minimal\nconstraint violations. The findings suggest that neural optimization models\noffer a promising direction for scalable and adaptive project tasks scheduling\nunder uncertainty in areas such as the agentic AI workflows, microservice based\napplications that the modern AI systems are being built upon."}
{"id": "2505.04963", "pdf": "https://arxiv.org/pdf/2505.04963", "abs": "https://arxiv.org/abs/2505.04963", "authors": ["Onkar Susladkar", "Gayatri Deshmukh", "Yalcin Tur", "Ulas Bagci"], "title": "ViCTr: Vital Consistency Transfer for Pathology Aware Image Synthesis", "categories": ["cs.CV"], "comment": null, "summary": "Synthesizing medical images remains challenging due to limited annotated\npathological data, modality domain gaps, and the complexity of representing\ndiffuse pathologies such as liver cirrhosis. Existing methods often struggle to\nmaintain anatomical fidelity while accurately modeling pathological features,\nfrequently relying on priors derived from natural images or inefficient\nmulti-step sampling. In this work, we introduce ViCTr (Vital Consistency\nTransfer), a novel two-stage framework that combines a rectified flow\ntrajectory with a Tweedie-corrected diffusion process to achieve high-fidelity,\npathology-aware image synthesis. First, we pretrain ViCTr on the ATLAS-8k\ndataset using Elastic Weight Consolidation (EWC) to preserve critical\nanatomical structures. We then fine-tune the model adversarially with Low-Rank\nAdaptation (LoRA) modules for precise control over pathology severity. By\nreformulating Tweedie's formula within a linear trajectory framework, ViCTr\nsupports one-step sampling, reducing inference from 50 steps to just 4, without\nsacrificing anatomical realism. We evaluate ViCTr on BTCV (CT), AMOS (MRI), and\nCirrMRI600+ (cirrhosis) datasets. Results demonstrate state-of-the-art\nperformance, achieving a Medical Frechet Inception Distance (MFID) of 17.01 for\ncirrhosis synthesis 28% lower than existing approaches and improving nnUNet\nsegmentation by +3.8% mDSC when used for data augmentation. Radiologist reviews\nindicate that ViCTr-generated liver cirrhosis MRIs are clinically\nindistinguishable from real scans. To our knowledge, ViCTr is the first method\nto provide fine-grained, pathology-aware MRI synthesis with graded severity\ncontrol, closing a critical gap in AI-driven medical imaging research."}
{"id": "2505.04792", "pdf": "https://arxiv.org/pdf/2505.04792", "abs": "https://arxiv.org/abs/2505.04792", "authors": ["Jack O'Hagan", "Andrew Keane", "Andrew Flynn"], "title": "Confabulation dynamics in a reservoir computer: Filling in the gaps with untrained attractors", "categories": ["math.DS", "cs.AI", "cs.LG"], "comment": null, "summary": "Artificial Intelligence has advanced significantly in recent years thanks to\ninnovations in the design and training of artificial neural networks (ANNs).\nDespite these advancements, we still understand relatively little about how\nelementary forms of ANNs learn, fail to learn, and generate false information\nwithout the intent to deceive, a phenomenon known as `confabulation'. To\nprovide some foundational insight, in this paper we analyse how confabulation\noccurs in reservoir computers (RCs): a dynamical system in the form of an ANN.\nRCs are particularly useful to study as they are known to confabulate in a\nwell-defined way: when RCs are trained to reconstruct the dynamics of a given\nattractor, they sometimes construct an attractor that they were not trained to\nconstruct, a so-called `untrained attractor' (UA). This paper sheds light on\nthe role played by UAs when reconstruction fails and their influence when\nmodelling transitions between reconstructed attractors. Based on our results,\nwe conclude that UAs are an intrinsic feature of learning systems whose state\nspaces are bounded, and that this means of confabulation may be present in\nsystems beyond RCs."}
{"id": "2505.05070", "pdf": "https://arxiv.org/pdf/2505.05070", "abs": "https://arxiv.org/abs/2505.05070", "authors": ["Ajwad Abrar", "Farzana Tabassum", "Sabbir Ahmed"], "title": "Performance Evaluation of Large Language Models in Bangla Consumer Health Query Summarization", "categories": ["cs.CL"], "comment": null, "summary": "Consumer Health Queries (CHQs) in Bengali (Bangla), a low-resource language,\noften contain extraneous details, complicating efficient medical responses.\nThis study investigates the zero-shot performance of nine advanced large\nlanguage models (LLMs): GPT-3.5-Turbo, GPT-4, Claude-3.5-Sonnet,\nLlama3-70b-Instruct, Mixtral-8x22b-Instruct, Gemini-1.5-Pro,\nQwen2-72b-Instruct, Gemma-2-27b, and Athene-70B, in summarizing Bangla CHQs.\nUsing the BanglaCHQ-Summ dataset comprising 2,350 annotated query-summary\npairs, we benchmarked these LLMs using ROUGE metrics against Bangla T5, a\nfine-tuned state-of-the-art model. Mixtral-8x22b-Instruct emerged as the top\nperforming model in ROUGE-1 and ROUGE-L, while Bangla T5 excelled in ROUGE-2.\nThe results demonstrate that zero-shot LLMs can rival fine-tuned models,\nachieving high-quality summaries even without task-specific training. This work\nunderscores the potential of LLMs in addressing challenges in low-resource\nlanguages, providing scalable solutions for healthcare query summarization."}
{"id": "2505.05063", "pdf": "https://arxiv.org/pdf/2505.05063", "abs": "https://arxiv.org/abs/2505.05063", "authors": ["Manik Sheokand", "Parth Sawant"], "title": "CodeMixBench: Evaluating Large Language Models on Code Generation with Code-Mixed Prompts", "categories": ["cs.LG", "cs.CL"], "comment": null, "summary": "Large Language Models (LLMs) have achieved remarkable success in code\ngeneration tasks, powering various applications like code completion,\ndebugging, and programming assistance. However, existing benchmarks such as\nHumanEval, MBPP, and BigCodeBench primarily evaluate LLMs on English-only\nprompts, overlooking the real-world scenario where multilingual developers\noften use code-mixed language while interacting with LLMs. To address this gap,\nwe introduce CodeMixBench, a novel benchmark designed to evaluate the\nrobustness of LLMs on code generation from code-mixed prompts. Built upon\nBigCodeBench, CodeMixBench introduces controlled code-mixing (CMD) into the\nnatural language parts of prompts across three language pairs: Hinglish\n(Hindi-English), Spanish-English, and Chinese Pinyin-English. We\ncomprehensively evaluate a diverse set of open-source code generation models\nranging from 1.5B to 15B parameters. Our results show that code-mixed prompts\nconsistently degrade Pass@1 performance compared to their English-only\ncounterparts, with performance drops increasing under higher CMD levels for\nsmaller models. CodeMixBench provides a realistic evaluation framework for\nstudying multilingual code generation and highlights new challenges and\ndirections for building robust code generation models that generalize well\nacross diverse linguistic settings."}
{"id": "2505.04964", "pdf": "https://arxiv.org/pdf/2505.04964", "abs": "https://arxiv.org/abs/2505.04964", "authors": ["Yuto Nakamura", "Satoshi Kodera", "Haruki Settai", "Hiroki Shinohara", "Masatsugu Tamura", "Tomohiro Noguchi", "Tatsuki Furusawa", "Ryo Takizawa", "Tempei Kabayama", "Norihiko Takeda"], "title": "CAG-VLM: Fine-Tuning of a Large-Scale Model to Recognize Angiographic Images for Next-Generation Diagnostic Systems", "categories": ["cs.CV"], "comment": null, "summary": "Coronary angiography (CAG) is the gold-standard imaging modality for\nevaluating coronary artery disease, but its interpretation and subsequent\ntreatment planning rely heavily on expert cardiologists. To enable AI-based\ndecision support, we introduce a two-stage, physician-curated pipeline and a\nbilingual (Japanese/English) CAG image-report dataset. First, we sample 14,686\nframes from 539 exams and annotate them for key-frame detection and left/right\nlaterality; a ConvNeXt-Base CNN trained on this data achieves 0.96 F1 on\nlaterality classification, even on low-contrast frames. Second, we apply the\nCNN to 243 independent exams, extract 1,114 key frames, and pair each with its\npre-procedure report and expert-validated diagnostic and treatment summary,\nyielding a parallel corpus. We then fine-tune three open-source VLMs\n(PaliGemma2, Gemma3, and ConceptCLIP-enhanced Gemma3) via LoRA and evaluate\nthem using VLScore and cardiologist review. Although PaliGemma2 w/LoRA attains\nthe highest VLScore, Gemma3 w/LoRA achieves the top clinician rating (mean\n7.20/10); we designate this best-performing model as CAG-VLM. These results\ndemonstrate that specialized, fine-tuned VLMs can effectively assist\ncardiologists in generating clinical reports and treatment recommendations from\nCAG images."}
{"id": "2505.04841", "pdf": "https://arxiv.org/pdf/2505.04841", "abs": "https://arxiv.org/abs/2505.04841", "authors": ["Nishikanta Mohanty", "Bikash K. Behera", "Badsah Mukherjee", "Christopher Ferrie"], "title": "Quantum-Inspired Optimization Process for Data Imputation", "categories": ["quant-ph", "cs.AI"], "comment": null, "summary": "Data imputation is a critical step in data pre-processing, particularly for\ndatasets with missing or unreliable values. This study introduces a novel\nquantum-inspired imputation framework evaluated on the UCI Diabetes dataset,\nwhich contains biologically implausible missing values across several clinical\nfeatures. The method integrates Principal Component Analysis (PCA) with\nquantum-assisted rotations, optimized through gradient-free classical\noptimizers -COBYLA, Simulated Annealing, and Differential Evolution to\nreconstruct missing values while preserving statistical fidelity. Reconstructed\nvalues are constrained within +/-2 standard deviations of original feature\ndistributions, avoiding unrealistic clustering around central tendencies. This\napproach achieves a substantial and statistically significant improvement,\nincluding an average reduction of over 85% in Wasserstein distance and\nKolmogorov-Smirnov test p-values between 0.18 and 0.22, compared to p-values >\n0.99 in classical methods such as Mean, KNN, and MICE. The method also\neliminates zero-value artifacts and enhances the realism and variability of\nimputed data. By combining quantum-inspired transformations with a scalable\nclassical framework, this methodology provides a robust solution for imputation\ntasks in domains such as healthcare and AI pipelines, where data quality and\nintegrity are crucial."}
{"id": "2505.05084", "pdf": "https://arxiv.org/pdf/2505.05084", "abs": "https://arxiv.org/abs/2505.05084", "authors": ["Xiaowei Zhu", "Yubing Ren", "Yanan Cao", "Xixun Lin", "Fang Fang", "Yangxi Li"], "title": "Reliably Bounding False Positives: A Zero-Shot Machine-Generated Text Detection Framework via Multiscaled Conformal Prediction", "categories": ["cs.CL"], "comment": null, "summary": "The rapid advancement of large language models has raised significant\nconcerns regarding their potential misuse by malicious actors. As a result,\ndeveloping effective detectors to mitigate these risks has become a critical\npriority. However, most existing detection methods focus excessively on\ndetection accuracy, often neglecting the societal risks posed by high false\npositive rates (FPRs). This paper addresses this issue by leveraging Conformal\nPrediction (CP), which effectively constrains the upper bound of FPRs. While\ndirectly applying CP constrains FPRs, it also leads to a significant reduction\nin detection performance. To overcome this trade-off, this paper proposes a\nZero-Shot Machine-Generated Text Detection Framework via Multiscaled Conformal\nPrediction (MCP), which both enforces the FPR constraint and improves detection\nperformance. This paper also introduces RealDet, a high-quality dataset that\nspans a wide range of domains, ensuring realistic calibration and enabling\nsuperior detection performance when combined with MCP. Empirical evaluations\ndemonstrate that MCP effectively constrains FPRs, significantly enhances\ndetection performance, and increases robustness against adversarial attacks\nacross multiple detectors and datasets."}
{"id": "2505.05064", "pdf": "https://arxiv.org/pdf/2505.05064", "abs": "https://arxiv.org/abs/2505.05064", "authors": ["Xinyang Lu", "Xinyuan Niu", "Gregory Kang Ruey Lau", "Bui Thi Cam Nhung", "Rachael Hwee Ling Sim", "Fanyu Wen", "Chuan-Sheng Foo", "See-Kiong Ng", "Bryan Kian Hsiang Low"], "title": "WaterDrum: Watermarking for Data-centric Unlearning Metric", "categories": ["cs.LG"], "comment": null, "summary": "Large language model (LLM) unlearning is critical in real-world applications\nwhere it is necessary to efficiently remove the influence of private,\ncopyrighted, or harmful data from some users. However, existing utility-centric\nunlearning metrics (based on model utility) may fail to accurately evaluate the\nextent of unlearning in realistic settings such as when (a) the forget and\nretain set have semantically similar content, (b) retraining the model from\nscratch on the retain set is impractical, and/or (c) the model owner can\nimprove the unlearning metric without directly performing unlearning on the\nLLM. This paper presents the first data-centric unlearning metric for LLMs\ncalled WaterDrum that exploits robust text watermarking for overcoming these\nlimitations. We also introduce new benchmark datasets for LLM unlearning that\ncontain varying levels of similar data points and can be used to rigorously\nevaluate unlearning algorithms using WaterDrum. Our code is available at\nhttps://github.com/lululu008/WaterDrum and our new benchmark datasets are\nreleased at https://huggingface.co/datasets/Glow-AI/WaterDrum-Ax."}
{"id": "2505.04965", "pdf": "https://arxiv.org/pdf/2505.04965", "abs": "https://arxiv.org/abs/2505.04965", "authors": ["Henry Zheng", "Hao Shi", "Qihang Peng", "Yong Xien Chng", "Rui Huang", "Yepeng Weng", "Zhongchao Shi", "Gao Huang"], "title": "DenseGrounding: Improving Dense Language-Vision Semantics for Ego-Centric 3D Visual Grounding", "categories": ["cs.CV"], "comment": "Accepted by ICLR 2025", "summary": "Enabling intelligent agents to comprehend and interact with 3D environments\nthrough natural language is crucial for advancing robotics and human-computer\ninteraction. A fundamental task in this field is ego-centric 3D visual\ngrounding, where agents locate target objects in real-world 3D spaces based on\nverbal descriptions. However, this task faces two significant challenges: (1)\nloss of fine-grained visual semantics due to sparse fusion of point clouds with\nego-centric multi-view images, (2) limited textual semantic context due to\narbitrary language descriptions. We propose DenseGrounding, a novel approach\ndesigned to address these issues by enhancing both visual and textual\nsemantics. For visual features, we introduce the Hierarchical Scene Semantic\nEnhancer, which retains dense semantics by capturing fine-grained global scene\nfeatures and facilitating cross-modal alignment. For text descriptions, we\npropose a Language Semantic Enhancer that leverages large language models to\nprovide rich context and diverse language descriptions with additional context\nduring model training. Extensive experiments show that DenseGrounding\nsignificantly outperforms existing methods in overall accuracy, with\nimprovements of 5.81% and 7.56% when trained on the comprehensive full dataset\nand smaller mini subset, respectively, further advancing the SOTA in egocentric\n3D visual grounding. Our method also achieves 1st place and receives the\nInnovation Award in the CVPR 2024 Autonomous Grand Challenge Multi-view 3D\nVisual Grounding Track, validating its effectiveness and robustness."}
{"id": "2505.04852", "pdf": "https://arxiv.org/pdf/2505.04852", "abs": "https://arxiv.org/abs/2505.04852", "authors": ["Yifei Gao", "Chengpeng Wang", "Pengxiang Huang", "Xuwei Liu", "Mingwei Zheng", "Xiangyu Zhang"], "title": "PR2: Peephole Raw Pointer Rewriting with LLMs for Translating C to Safer Rust", "categories": ["cs.SE", "cs.AI", "cs.PL"], "comment": null, "summary": "There has been a growing interest in translating C code to Rust due to Rust's\nrobust memory and thread safety guarantees. Tools such as C2RUST enable\nsyntax-guided transpilation from C to semantically equivalent Rust code.\nHowever, the resulting Rust programs often rely heavily on unsafe\nconstructs--particularly raw pointers--which undermines Rust's safety\nguarantees. This paper aims to improve the memory safety of Rust programs\ngenerated by C2RUST by eliminating raw pointers. Specifically, we propose a\npeephole raw pointer rewriting technique that lifts raw pointers in individual\nfunctions to appropriate Rust data structures. Technically, PR2 employs\ndecision-tree-based prompting to guide the pointer lifting process.\nAdditionally, it leverages code change analysis to guide the repair of errors\nintroduced during rewriting, effectively addressing errors encountered during\ncompilation and test case execution. We implement PR2 as a prototype and\nevaluate it using gpt-4o-mini on 28 real-world C projects. The results show\nthat PR2 successfully eliminates 13.22% of local raw pointers across these\nprojects, significantly enhancing the safety of the translated Rust code. On\naverage, PR2 completes the transformation of a project in 5.44 hours, at an\naverage cost of $1.46."}
{"id": "2505.05111", "pdf": "https://arxiv.org/pdf/2505.05111", "abs": "https://arxiv.org/abs/2505.05111", "authors": ["Boyi Deng", "Yu Wan", "Yidan Zhang", "Baosong Yang", "Fuli Feng"], "title": "Unveiling Language-Specific Features in Large Language Models via Sparse Autoencoders", "categories": ["cs.CL"], "comment": null, "summary": "The mechanisms behind multilingual capabilities in Large Language Models\n(LLMs) have been examined using neuron-based or internal-activation-based\nmethods. However, these methods often face challenges such as superposition and\nlayer-wise activation variance, which limit their reliability. Sparse\nAutoencoders (SAEs) offer a more nuanced analysis by decomposing the\nactivations of LLMs into sparse linear combination of SAE features. We\nintroduce a novel metric to assess the monolinguality of features obtained from\nSAEs, discovering that some features are strongly related to specific\nlanguages. Additionally, we show that ablating these SAE features only\nsignificantly reduces abilities in one language of LLMs, leaving others almost\nunaffected. Interestingly, we find some languages have multiple synergistic SAE\nfeatures, and ablating them together yields greater improvement than ablating\nindividually. Moreover, we leverage these SAE-derived language-specific\nfeatures to enhance steering vectors, achieving control over the language\ngenerated by LLMs."}
{"id": "2505.05082", "pdf": "https://arxiv.org/pdf/2505.05082", "abs": "https://arxiv.org/abs/2505.05082", "authors": ["Sagnik Bhattacharya", "Abhiram R. Gorle", "Ahmed Mohsin", "Ahsan Bilal", "Connor Ding", "Amit Kumar Singh Yadav", "Tsachy Weissman"], "title": "ItDPDM: Information-Theoretic Discrete Poisson Diffusion Model", "categories": ["cs.LG", "cs.IT", "math.IT", "math.PR"], "comment": "Pre-print", "summary": "Existing methods for generative modeling of discrete data, such as symbolic\nmusic tokens, face two primary challenges: (1) they either embed discrete\ninputs into continuous state-spaces or (2) rely on variational losses that only\napproximate the true negative log-likelihood. Previous efforts have\nindividually targeted these limitations. While information-theoretic Gaussian\ndiffusion models alleviate the suboptimality of variational losses, they still\nperform modeling in continuous domains. In this work, we introduce the\nInformation-Theoretic Discrete Poisson Diffusion Model (ItDPDM), which\nsimultaneously addresses both limitations by directly operating in a discrete\nstate-space via a Poisson diffusion process inspired by photon arrival\nprocesses in camera sensors. We introduce a novel Poisson Reconstruction Loss\n(PRL) and derive an exact relationship between PRL and the true negative\nlog-likelihood, thereby eliminating the need for approximate evidence lower\nbounds. Experiments conducted on the Lakh MIDI symbolic music dataset and the\nCIFAR-10 image benchmark demonstrate that ItDPDM delivers significant\nimprovements, reducing test NLL by up to 80% compared to prior baselines, while\nalso achieving faster convergence."}
{"id": "2505.04974", "pdf": "https://arxiv.org/pdf/2505.04974", "abs": "https://arxiv.org/abs/2505.04974", "authors": ["Wanjiang Weng", "Xiaofeng Tan", "Hongsong Wang", "Pan Zhou"], "title": "ReAlign: Bilingual Text-to-Motion Generation via Step-Aware Reward-Guided Alignment", "categories": ["cs.CV"], "comment": "17 pages, 9 figures", "summary": "Bilingual text-to-motion generation, which synthesizes 3D human motions from\nbilingual text inputs, holds immense potential for cross-linguistic\napplications in gaming, film, and robotics. However, this task faces critical\nchallenges: the absence of bilingual motion-language datasets and the\nmisalignment between text and motion distributions in diffusion models, leading\nto semantically inconsistent or low-quality motions. To address these\nchallenges, we propose BiHumanML3D, a novel bilingual human motion dataset,\nwhich establishes a crucial benchmark for bilingual text-to-motion generation\nmodels. Furthermore, we propose a Bilingual Motion Diffusion model (BiMD),\nwhich leverages cross-lingual aligned representations to capture semantics,\nthereby achieving a unified bilingual model. Building upon this, we propose\nReward-guided sampling Alignment (ReAlign) method, comprising a step-aware\nreward model to assess alignment quality during sampling and a reward-guided\nstrategy that directs the diffusion process toward an optimally aligned\ndistribution. This reward model integrates step-aware tokens and combines a\ntext-aligned module for semantic consistency and a motion-aligned module for\nrealism, refining noisy motions at each timestep to balance probability density\nand alignment. Experiments demonstrate that our approach significantly improves\ntext-motion alignment and motion quality compared to existing state-of-the-art\nmethods. Project page: https://wengwanjiang.github.io/ReAlign-page/."}
{"id": "2505.04860", "pdf": "https://arxiv.org/pdf/2505.04860", "abs": "https://arxiv.org/abs/2505.04860", "authors": ["I-Chun Arthur Liu", "Jason Chen", "Gaurav Sukhatme", "Daniel Seita"], "title": "D-CODA: Diffusion for Coordinated Dual-Arm Data Augmentation", "categories": ["cs.RO", "cs.AI", "cs.CV", "cs.LG"], "comment": null, "summary": "Learning bimanual manipulation is challenging due to its high dimensionality\nand tight coordination required between two arms. Eye-in-hand imitation\nlearning, which uses wrist-mounted cameras, simplifies perception by focusing\non task-relevant views. However, collecting diverse demonstrations remains\ncostly, motivating the need for scalable data augmentation. While prior work\nhas explored visual augmentation in single-arm settings, extending these\napproaches to bimanual manipulation requires generating viewpoint-consistent\nobservations across both arms and producing corresponding action labels that\nare both valid and feasible. In this work, we propose Diffusion for COordinated\nDual-arm Data Augmentation (D-CODA), a method for offline data augmentation\ntailored to eye-in-hand bimanual imitation learning that trains a diffusion\nmodel to synthesize novel, viewpoint-consistent wrist-camera images for both\narms while simultaneously generating joint-space action labels. It employs\nconstrained optimization to ensure that augmented states involving\ngripper-to-object contacts adhere to constraints suitable for bimanual\ncoordination. We evaluate D-CODA on 5 simulated and 3 real-world tasks. Our\nresults across 2250 simulation trials and 300 real-world trials demonstrate\nthat it outperforms baselines and ablations, showing its potential for scalable\ndata augmentation in eye-in-hand bimanual manipulation. Our project website is\nat: https://dcodaaug.github.io/D-CODA/."}
{"id": "2505.05148", "pdf": "https://arxiv.org/pdf/2505.05148", "abs": "https://arxiv.org/abs/2505.05148", "authors": ["Hussain Ahmad", "Qingyang Zeng", "Jing Wan"], "title": "A Benchmark Dataset and a Framework for Urdu Multimodal Named Entity Recognition", "categories": ["cs.CL"], "comment": "16 pages, 5 figures. Preprint", "summary": "The emergence of multimodal content, particularly text and images on social\nmedia, has positioned Multimodal Named Entity Recognition (MNER) as an\nincreasingly important area of research within Natural Language Processing.\nDespite progress in high-resource languages such as English, MNER remains\nunderexplored for low-resource languages like Urdu. The primary challenges\ninclude the scarcity of annotated multimodal datasets and the lack of\nstandardized baselines. To address these challenges, we introduce the U-MNER\nframework and release the Twitter2015-Urdu dataset, a pioneering resource for\nUrdu MNER. Adapted from the widely used Twitter2015 dataset, it is annotated\nwith Urdu-specific grammar rules. We establish benchmark baselines by\nevaluating both text-based and multimodal models on this dataset, providing\ncomparative analyses to support future research on Urdu MNER. The U-MNER\nframework integrates textual and visual context using Urdu-BERT for text\nembeddings and ResNet for visual feature extraction, with a Cross-Modal Fusion\nModule to align and fuse information. Our model achieves state-of-the-art\nperformance on the Twitter2015-Urdu dataset, laying the groundwork for further\nMNER research in low-resource languages."}
{"id": "2505.05086", "pdf": "https://arxiv.org/pdf/2505.05086", "abs": "https://arxiv.org/abs/2505.05086", "authors": ["Le-Trung Nguyen", "Ael Quelennec", "Van-Tam Nguyen", "Enzo Tartaglione"], "title": "Beyond Low-rank Decomposition: A Shortcut Approach for Efficient On-Device Learning", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "On-device learning has emerged as a promising direction for AI development,\nparticularly because of its potential to reduce latency issues and mitigate\nprivacy risks associated with device-server communication, while improving\nenergy efficiency. Despite these advantages, significant memory and\ncomputational constraints still represent major challenges for its deployment.\nDrawing on previous studies on low-rank decomposition methods that address\nactivation memory bottlenecks in backpropagation, we propose a novel shortcut\napproach as an alternative. Our analysis and experiments demonstrate that our\nmethod can reduce activation memory usage, even up to $120.09\\times$ compared\nto vanilla training, while also reducing overall training FLOPs up to\n$1.86\\times$ when evaluated on traditional benchmarks."}
{"id": "2505.04979", "pdf": "https://arxiv.org/pdf/2505.04979", "abs": "https://arxiv.org/abs/2505.04979", "authors": ["Zhuang Qi", "Sijin Zhou", "Lei Meng", "Han Hu", "Han Yu", "Xiangxu Meng"], "title": "Federated Deconfounding and Debiasing Learning for Out-of-Distribution Generalization", "categories": ["cs.CV"], "comment": "IJCAI-25 Accepted", "summary": "Attribute bias in federated learning (FL) typically leads local models to\noptimize inconsistently due to the learning of non-causal associations,\nresulting degraded performance. Existing methods either use data augmentation\nfor increasing sample diversity or knowledge distillation for learning\ninvariant representations to address this problem. However, they lack a\ncomprehensive analysis of the inference paths, and the interference from\nconfounding factors limits their performance. To address these limitations, we\npropose the \\underline{Fed}erated \\underline{D}econfounding and\n\\underline{D}ebiasing \\underline{L}earning (FedDDL) method. It constructs a\nstructured causal graph to analyze the model inference process, and performs\nbackdoor adjustment to eliminate confounding paths. Specifically, we design an\nintra-client deconfounding learning module for computer vision tasks to\ndecouple background and objects, generating counterfactual samples that\nestablish a connection between the background and any label, which stops the\nmodel from using the background to infer the label. Moreover, we design an\ninter-client debiasing learning module to construct causal prototypes to reduce\nthe proportion of the background in prototype components. Notably, it bridges\nthe gap between heterogeneous representations via causal prototypical\nregularization. Extensive experiments on 2 benchmarking datasets demonstrate\nthat \\methodname{} significantly enhances the model capability to focus on main\nobjects in unseen data, leading to 4.5\\% higher Top-1 Accuracy on average over\n9 state-of-the-art existing methods."}
{"id": "2505.04880", "pdf": "https://arxiv.org/pdf/2505.04880", "abs": "https://arxiv.org/abs/2505.04880", "authors": ["Min Chen", "Jinglei Cheng", "Pingzhi Li", "Haoran Wang", "Tianlong Chen", "Junyu Liu"], "title": "GroverGPT-2: Simulating Grover's Algorithm via Chain-of-Thought Reasoning and Quantum-Native Tokenization", "categories": ["quant-ph", "cs.AI", "cs.LG"], "comment": "26 pages, 12 figures", "summary": "Quantum computing offers theoretical advantages over classical computing for\nspecific tasks, yet the boundary of practical quantum advantage remains an open\nquestion. To investigate this boundary, it is crucial to understand whether,\nand how, classical machines can learn and simulate quantum algorithms. Recent\nprogress in large language models (LLMs) has demonstrated strong reasoning\nabilities, prompting exploration into their potential for this challenge. In\nthis work, we introduce GroverGPT-2, an LLM-based method for simulating\nGrover's algorithm using Chain-of-Thought (CoT) reasoning and quantum-native\ntokenization. Building on its predecessor, GroverGPT-2 performs simulation\ndirectly from quantum circuit representations while producing logically\nstructured and interpretable outputs. Our results show that GroverGPT-2 can\nlearn and internalize quantum circuit logic through efficient processing of\nquantum-native tokens, providing direct evidence that classical models like\nLLMs can capture the structure of quantum algorithms. Furthermore, GroverGPT-2\noutputs interleave circuit data with natural language, embedding explicit\nreasoning into the simulation. This dual capability positions GroverGPT-2 as a\nprototype for advancing machine understanding of quantum algorithms and\nmodeling quantum circuit logic. We also identify an empirical scaling law for\nGroverGPT-2 with increasing qubit numbers, suggesting a path toward scalable\nclassical simulation. These findings open new directions for exploring the\nlimits of classical simulatability, enhancing quantum education and research,\nand laying groundwork for future foundation models in quantum computing."}
{"id": "2505.05225", "pdf": "https://arxiv.org/pdf/2505.05225", "abs": "https://arxiv.org/abs/2505.05225", "authors": ["Mengze Hong", "Wailing Ng", "Di Jiang", "Chen Jason Zhang"], "title": "QualBench: Benchmarking Chinese LLMs with Localized Professional Qualifications for Vertical Domain Evaluation", "categories": ["cs.CL"], "comment": null, "summary": "The rapid advancement of Chinese large language models (LLMs) underscores the\nneed for domain-specific evaluations to ensure reliable applications. However,\nexisting benchmarks often lack coverage in vertical domains and offer limited\ninsights into the Chinese working context. Leveraging qualification exams as a\nunified framework for human expertise evaluation, we introduce QualBench, the\nfirst multi-domain Chinese QA benchmark dedicated to localized assessment of\nChinese LLMs. The dataset includes over 17,000 questions across six vertical\ndomains, with data selections grounded in 24 Chinese qualifications to closely\nalign with national policies and working standards. Through comprehensive\nevaluation, the Qwen2.5 model outperformed the more advanced GPT-4o, with\nChinese LLMs consistently surpassing non-Chinese models, highlighting the\nimportance of localized domain knowledge in meeting qualification requirements.\nThe best performance of 75.26% reveals the current gaps in domain coverage\nwithin model capabilities. Furthermore, we present the failure of LLM\ncollaboration with crowdsourcing mechanisms and suggest the opportunities for\nmulti-domain RAG knowledge enhancement and vertical domain LLM training with\nFederated Learning."}
{"id": "2505.05094", "pdf": "https://arxiv.org/pdf/2505.05094", "abs": "https://arxiv.org/abs/2505.05094", "authors": ["Leming Zhou", "Zuo Wang", "Zhixuan Duan"], "title": "A Conjoint Graph Representation Learning Framework for Hypertension Comorbidity Risk Prediction", "categories": ["cs.LG"], "comment": null, "summary": "The comorbidities of hypertension impose a heavy burden on patients and\nsociety. Early identification is necessary to prompt intervention, but it\nremains a challenging task. This study aims to address this challenge by\ncombining joint graph learning with network analysis. Motivated by this\ndiscovery, we develop a Conjoint Graph Representation Learning (CGRL) framework\nthat: a) constructs two networks based on disease coding, including the patient\nnetwork and the disease difference network. Three comorbidity network features\nwere generated based on the basic difference network to capture the potential\nrelationship between comorbidities and risk diseases; b) incorporates\ncomputational structure intervention and learning feature representation, CGRL\nwas developed to predict the risks of diabetes and coronary heart disease in\npatients; and c) analysis the comorbidity patterns and exploring the pathways\nof disease progression, the pathological pathogenesis of diabetes and coronary\nheart disease may be revealed. The results show that the network features\nextracted based on the difference network are important, and the framework we\nproposed provides more accurate predictions than other strong models in terms\nof accuracy."}
{"id": "2505.05001", "pdf": "https://arxiv.org/pdf/2505.05001", "abs": "https://arxiv.org/abs/2505.05001", "authors": ["Lang Nie", "Chunyu Lin", "Kang Liao", "Yun Zhang", "Shuaicheng Liu", "Yao Zhao"], "title": "StabStitch++: Unsupervised Online Video Stitching with Spatiotemporal Bidirectional Warps", "categories": ["cs.CV", "cs.AI"], "comment": "TPAMI2025; https://github.com/nie-lang/StabStitch2. arXiv admin note:\n  text overlap with arXiv:2403.06378", "summary": "We retarget video stitching to an emerging issue, named warping shake, which\nunveils the temporal content shakes induced by sequentially unsmooth warps when\nextending image stitching to video stitching. Even if the input videos are\nstable, the stitched video can inevitably cause undesired warping shakes and\naffect the visual experience. To address this issue, we propose StabStitch++, a\nnovel video stitching framework to realize spatial stitching and temporal\nstabilization with unsupervised learning simultaneously. First, different from\nexisting learning-based image stitching solutions that typically warp one image\nto align with another, we suppose a virtual midplane between original image\nplanes and project them onto it. Concretely, we design a differentiable\nbidirectional decomposition module to disentangle the homography transformation\nand incorporate it into our spatial warp, evenly spreading alignment burdens\nand projective distortions across two views. Then, inspired by camera paths in\nvideo stabilization, we derive the mathematical expression of stitching\ntrajectories in video stitching by elaborately integrating spatial and temporal\nwarps. Finally, a warp smoothing model is presented to produce stable stitched\nvideos with a hybrid loss to simultaneously encourage content alignment,\ntrajectory smoothness, and online collaboration. Compared with StabStitch that\nsacrifices alignment for stabilization, StabStitch++ makes no compromise and\noptimizes both of them simultaneously, especially in the online mode. To\nestablish an evaluation benchmark and train the learning framework, we build a\nvideo stitching dataset with a rich diversity in camera motions and scenes.\nExperiments exhibit that StabStitch++ surpasses current solutions in stitching\nperformance, robustness, and efficiency, offering compelling advancements in\nthis field by building a real-time online video stitching system."}
{"id": "2505.04883", "pdf": "https://arxiv.org/pdf/2505.04883", "abs": "https://arxiv.org/abs/2505.04883", "authors": ["Mingruo Yuan", "Ben Kao", "Tien-Hsuan Wu"], "title": "QBR: A Question-Bank-Based Approach to Fine-Grained Legal Knowledge Retrieval for the General Public", "categories": ["cs.IR", "cs.AI"], "comment": null, "summary": "Retrieval of legal knowledge by the general public is a challenging problem\ndue to the technicality of the professional knowledge and the lack of\nfundamental understanding by laypersons on the subject. Traditional information\nretrieval techniques assume that users are capable of formulating succinct and\nprecise queries for effective document retrieval. In practice, however, the\nwide gap between the highly technical contents and untrained users makes legal\nknowledge retrieval very difficult. We propose a methodology, called QBR, which\nemploys a Questions Bank (QB) as an effective medium for bridging the knowledge\ngap. We show how the QB is used to derive training samples to enhance the\nembedding of knowledge units within documents, which leads to effective\nfine-grained knowledge retrieval. We discuss and evaluate through experiments\nvarious advantages of QBR over traditional methods. These include more\naccurate, efficient, and explainable document retrieval, better comprehension\nof retrieval results, and highly effective fine-grained knowledge retrieval. We\nalso present some case studies and show that QBR achieves social impact by\nassisting citizens to resolve everyday legal concerns."}
{"id": "2505.05271", "pdf": "https://arxiv.org/pdf/2505.05271", "abs": "https://arxiv.org/abs/2505.05271", "authors": ["Kun Peng", "Chaodong Tong", "Cong Cao", "Hao Peng", "Qian Li", "Guanlin Wu", "Lei Jiang", "Yanbing Liu", "Philip S. Yu"], "title": "T-T: Table Transformer for Tagging-based Aspect Sentiment Triplet Extraction", "categories": ["cs.CL", "cs.AI"], "comment": "Accepted by IJCAI2025", "summary": "Aspect sentiment triplet extraction (ASTE) aims to extract triplets composed\nof aspect terms, opinion terms, and sentiment polarities from given sentences.\nThe table tagging method is a popular approach to addressing this task, which\nencodes a sentence into a 2-dimensional table, allowing for the tagging of\nrelations between any two words. Previous efforts have focused on designing\nvarious downstream relation learning modules to better capture interactions\nbetween tokens in the table, revealing that a stronger capability to capture\nrelations can lead to greater improvements in the model. Motivated by this, we\nattempt to directly utilize transformer layers as downstream relation learning\nmodules. Due to the powerful semantic modeling capability of transformers, it\nis foreseeable that this will lead to excellent improvement. However, owing to\nthe quadratic relation between the length of the table and the length of the\ninput sentence sequence, using transformers directly faces two challenges:\noverly long table sequences and unfair local attention interaction. To address\nthese challenges, we propose a novel Table-Transformer (T-T) for the\ntagging-based ASTE method. Specifically, we introduce a stripe attention\nmechanism with a loop-shift strategy to tackle these challenges. The former\nmodifies the global attention mechanism to only attend to a 2-dimensional local\nattention window, while the latter facilitates interaction between different\nattention windows. Extensive and comprehensive experiments demonstrate that the\nT-T, as a downstream relation learning module, achieves state-of-the-art\nperformance with lower computational costs."}
{"id": "2505.05099", "pdf": "https://arxiv.org/pdf/2505.05099", "abs": "https://arxiv.org/abs/2505.05099", "authors": ["Alireza Javani", "Zhiying Wang"], "title": "Balancing Client Participation in Federated Learning Using AoI", "categories": ["cs.LG", "cs.IT", "math.IT"], "comment": null, "summary": "Federated Learning (FL) offers a decentralized framework that preserves data\nprivacy while enabling collaborative model training across distributed clients.\nHowever, FL faces significant challenges due to limited communication\nresources, statistical heterogeneity, and the need for balanced client\nparticipation. This paper proposes an Age of Information (AoI)-based client\nselection policy that addresses these challenges by minimizing load imbalance\nthrough controlled selection intervals. Our method employs a decentralized\nMarkov scheduling policy, allowing clients to independently manage\nparticipation based on age-dependent selection probabilities, which balances\nclient updates across training rounds with minimal central oversight. We\nprovide a convergence proof for our method, demonstrating that it ensures\nstable and efficient model convergence. Specifically, we derive optimal\nparameters for the Markov selection model to achieve balanced and consistent\nclient participation, highlighting the benefits of AoI in enhancing convergence\nstability. Through extensive simulations, we demonstrate that our AoI-based\nmethod, particularly the optimal Markov variant, improves convergence over the\nFedAvg selection approach across both IID and non-IID data settings by $7.5\\%$\nand up to $20\\%$. Our findings underscore the effectiveness of AoI-based\nscheduling for scalable, fair, and efficient FL systems across diverse learning\nenvironments."}
{"id": "2505.05004", "pdf": "https://arxiv.org/pdf/2505.05004", "abs": "https://arxiv.org/abs/2505.05004", "authors": ["Hendrik Möller", "Hanna Schön", "Alina Dima", "Benjamin Keinert-Weth", "Robert Graf", "Matan Atad", "Johannes Paetzold", "Friederike Jungmann", "Rickmer Braren", "Florian Kofler", "Bjoern Menze", "Daniel Rueckert", "Jan S. Kirschke"], "title": "Automated Thoracolumbar Stump Rib Detection and Analysis in a Large CT Cohort", "categories": ["cs.CV", "cs.LG"], "comment": null, "summary": "Thoracolumbar stump ribs are one of the essential indicators of thoracolumbar\ntransitional vertebrae or enumeration anomalies. While some studies manually\nassess these anomalies and describe the ribs qualitatively, this study aims to\nautomate thoracolumbar stump rib detection and analyze their morphology\nquantitatively. To this end, we train a high-resolution deep-learning model for\nrib segmentation and show significant improvements compared to existing models\n(Dice score 0.997 vs. 0.779, p-value < 0.01). In addition, we use an iterative\nalgorithm and piece-wise linear interpolation to assess the length of the ribs,\nshowing a success rate of 98.2%. When analyzing morphological features, we show\nthat stump ribs articulate more posteriorly at the vertebrae (-19.2 +- 3.8 vs\n-13.8 +- 2.5, p-value < 0.01), are thinner (260.6 +- 103.4 vs. 563.6 +- 127.1,\np-value < 0.01), and are oriented more downwards and sideways within the first\ncentimeters in contrast to full-length ribs. We show that with partially\nvisible ribs, these features can achieve an F1-score of 0.84 in differentiating\nstump ribs from regular ones. We publish the model weights and masks for public\nuse."}
{"id": "2505.05327", "pdf": "https://arxiv.org/pdf/2505.05327", "abs": "https://arxiv.org/abs/2505.05327", "authors": ["Yixin Yang", "Qingxiu Dong", "Linli Yao", "Fangwei Zhu", "Zhifang Sui"], "title": "ICon: In-Context Contribution for Automatic Data Selection", "categories": ["cs.CL"], "comment": null, "summary": "Data selection for instruction tuning is essential for improving the\nperformance of Large Language Models (LLMs) and reducing training cost.\nHowever, existing automated selection methods either depend on computationally\nexpensive gradient-based measures or manually designed heuristics, which may\nfail to fully exploit the intrinsic attributes of data. In this paper, we\npropose In-context Learning for Contribution Measurement (ICon), a novel\ngradient-free method that takes advantage of the implicit fine-tuning nature of\nin-context learning (ICL) to measure sample contribution without gradient\ncomputation or manual indicators engineering. ICon offers a computationally\nefficient alternative to gradient-based methods and reduces human inductive\nbias inherent in heuristic-based approaches. ICon comprises three components\nand identifies high-contribution data by assessing performance shifts under\nimplicit learning through ICL. Extensive experiments on three LLMs across 12\nbenchmarks and 5 pairwise evaluation sets demonstrate the effectiveness of\nICon. Remarkably, on LLaMA3.1-8B, models trained on 15% of ICon-selected data\noutperform full datasets by 5.42% points and exceed the best performance of\nwidely used selection methods by 2.06% points. We further analyze\nhigh-contribution samples selected by ICon, which show both diverse tasks and\nappropriate difficulty levels, rather than just the hardest ones."}
{"id": "2505.05126", "pdf": "https://arxiv.org/pdf/2505.05126", "abs": "https://arxiv.org/abs/2505.05126", "authors": ["Xuyang Chen", "Keyu Yan", "Lin Zhao"], "title": "Taming OOD Actions for Offline Reinforcement Learning: An Advantage-Based Approach", "categories": ["cs.LG"], "comment": null, "summary": "Offline reinforcement learning (RL) aims to learn decision-making policies\nfrom fixed datasets without online interactions, providing a practical solution\nwhere online data collection is expensive or risky. However, offline RL often\nsuffers from distribution shift, resulting in inaccurate evaluation and\nsubstantial overestimation on out-of-distribution (OOD) actions. To address\nthis, existing approaches incorporate conservatism by indiscriminately\ndiscouraging all OOD actions, thereby hindering the agent's ability to\ngeneralize and exploit beneficial ones. In this paper, we propose\nAdvantage-based Diffusion Actor-Critic (ADAC), a novel method that\nsystematically evaluates OOD actions using the batch-optimal value function.\nBased on this evaluation, ADAC defines an advantage function to modulate the\nQ-function update, enabling more precise assessment of OOD action quality. We\ndesign a custom PointMaze environment and collect datasets to visually reveal\nthat advantage modulation can effectively identify and select superior OOD\nactions. Extensive experiments show that ADAC achieves state-of-the-art\nperformance on almost all tasks in the D4RL benchmark, with particularly clear\nmargins on the more challenging tasks."}
{"id": "2505.05007", "pdf": "https://arxiv.org/pdf/2505.05007", "abs": "https://arxiv.org/abs/2505.05007", "authors": ["Xin Bi", "Zhichao Li", "Yuxuan Xia", "Panpan Tong", "Lijuan Zhang", "Yang Chen", "Junsheng Fu"], "title": "Driving with Context: Online Map Matching for Complex Roads Using Lane Markings and Scenario Recognition", "categories": ["cs.CV"], "comment": "9 pages and 12 figures. Under review at IEEE RA-L", "summary": "Accurate online map matching is fundamental to vehicle navigation and the\nactivation of intelligent driving functions. Current online map matching\nmethods are prone to errors in complex road networks, especially in multilevel\nroad area. To address this challenge, we propose an online Standard Definition\n(SD) map matching method by constructing a Hidden Markov Model (HMM) with\nmultiple probability factors. Our proposed method can achieve accurate map\nmatching even in complex road networks by carefully leveraging lane markings\nand scenario recognition in the designing of the probability factors. First,\nthe lane markings are generated by a multi-lane tracking method and associated\nwith the SD map using HMM to build an enriched SD map. In areas covered by the\nenriched SD map, the vehicle can re-localize itself by performing Iterative\nClosest Point (ICP) registration for the lane markings. Then, the probability\nfactor accounting for the lane marking detection can be obtained using the\nassociation probability between adjacent lanes and roads. Second, the driving\nscenario recognition model is applied to generate the emission probability\nfactor of scenario recognition, which improves the performance of map matching\non elevated roads and ordinary urban roads underneath them. We validate our\nmethod through extensive road tests in Europe and China, and the experimental\nresults show that our proposed method effectively improves the online map\nmatching accuracy as compared to other existing methods, especially in\nmultilevel road area. Specifically, the experiments show that our proposed\nmethod achieves $F_1$ scores of 98.04% and 94.60% on the Zenseact Open Dataset\nand test data of multilevel road areas in Shanghai respectively, significantly\noutperforming benchmark methods. The implementation is available at\nhttps://github.com/TRV-Lab/LMSR-OMM."}
{"id": "2505.04961", "pdf": "https://arxiv.org/pdf/2505.04961", "abs": "https://arxiv.org/abs/2505.04961", "authors": ["Ziyu Zhang", "Sergey Bashkirov", "Dun Yang", "Michael Taylor", "Xue Bin Peng"], "title": "ADD: Physics-Based Motion Imitation with Adversarial Differential Discriminators", "categories": ["cs.GR", "cs.AI", "cs.CV", "cs.RO"], "comment": "19 pages, 15 figures", "summary": "Multi-objective optimization problems, which require the simultaneous\noptimization of multiple terms, are prevalent across numerous applications.\nExisting multi-objective optimization methods often rely on manually tuned\naggregation functions to formulate a joint optimization target. The performance\nof such hand-tuned methods is heavily dependent on careful weight selection, a\ntime-consuming and laborious process. These limitations also arise in the\nsetting of reinforcement-learning-based motion tracking for physically\nsimulated characters, where intricately crafted reward functions are typically\nused to achieve high-fidelity results. Such solutions not only require domain\nexpertise and significant manual adjustment, but also limit the applicability\nof the resulting reward function across diverse skills. To bridge this gap, we\npresent a novel adversarial multi-objective optimization technique that is\nbroadly applicable to a range of multi-objective optimization problems,\nincluding motion tracking. The proposed adversarial differential discriminator\nreceives a single positive sample, yet is still effective at guiding the\noptimization process. We demonstrate that our technique can enable characters\nto closely replicate a variety of acrobatic and agile behaviors, achieving\ncomparable quality to state-of-the-art motion-tracking methods, without relying\non manually tuned reward functions. Results are best visualized through\nhttps://youtu.be/rz8BYCE9E2w."}
{"id": "2505.05406", "pdf": "https://arxiv.org/pdf/2505.05406", "abs": "https://arxiv.org/abs/2505.05406", "authors": ["Valeria Pastorino", "Nafise Sadat Moosavi"], "title": "Frame In, Frame Out: Do LLMs Generate More Biased News Headlines than Humans?", "categories": ["cs.CL"], "comment": null, "summary": "Framing in media critically shapes public perception by selectively\nemphasizing some details while downplaying others. With the rise of large\nlanguage models in automated news and content creation, there is growing\nconcern that these systems may introduce or even amplify framing biases\ncompared to human authors. In this paper, we explore how framing manifests in\nboth out-of-the-box and fine-tuned LLM-generated news content. Our analysis\nreveals that, particularly in politically and socially sensitive contexts, LLMs\ntend to exhibit more pronounced framing than their human counterparts. In\naddition, we observe significant variation in framing tendencies across\ndifferent model architectures, with some models displaying notably higher\nbiases. These findings point to the need for effective post-training mitigation\nstrategies and tighter evaluation frameworks to ensure that automated news\ncontent upholds the standards of balanced reporting."}
{"id": "2505.05137", "pdf": "https://arxiv.org/pdf/2505.05137", "abs": "https://arxiv.org/abs/2505.05137", "authors": ["Yi Chen"], "title": "Research on Anomaly Detection Methods Based on Diffusion Models", "categories": ["cs.LG", "cs.CV"], "comment": "6 pages, 3 table", "summary": "Anomaly detection is a fundamental task in machine learning and data mining,\nwith significant applications in cybersecurity, industrial fault diagnosis, and\nclinical disease monitoring. Traditional methods, such as statistical modeling\nand machine learning-based approaches, often face challenges in handling\ncomplex, high-dimensional data distributions. In this study, we explore the\npotential of diffusion models for anomaly detection, proposing a novel\nframework that leverages the strengths of diffusion probabilistic models (DPMs)\nto effectively identify anomalies in both image and audio data. The proposed\nmethod models the distribution of normal data through a diffusion process and\nreconstructs input data via reverse diffusion, using a combination of\nreconstruction errors and semantic discrepancies as anomaly indicators. To\nenhance the framework's performance, we introduce multi-scale feature\nextraction, attention mechanisms, and wavelet-domain representations, enabling\nthe model to capture fine-grained structures and global dependencies in the\ndata. Extensive experiments on benchmark datasets, including MVTec AD and\nUrbanSound8K, demonstrate that our method outperforms state-of-the-art anomaly\ndetection techniques, achieving superior accuracy and robustness across diverse\ndata modalities. This research highlights the effectiveness of diffusion models\nin anomaly detection and provides a robust and efficient solution for\nreal-world applications."}
{"id": "2505.05008", "pdf": "https://arxiv.org/pdf/2505.05008", "abs": "https://arxiv.org/abs/2505.05008", "authors": ["Xuesong Liu", "Tianyu Hao", "Emmett J. Ientilucci"], "title": "Adaptive Contextual Embedding for Robust Far-View Borehole Detection", "categories": ["cs.CV"], "comment": null, "summary": "In controlled blasting operations, accurately detecting densely distributed\ntiny boreholes from far-view imagery is critical for operational safety and\nefficiency. However, existing detection methods often struggle due to small\nobject scales, highly dense arrangements, and limited distinctive visual\nfeatures of boreholes. To address these challenges, we propose an adaptive\ndetection approach that builds upon existing architectures (e.g., YOLO) by\nexplicitly leveraging consistent embedding representations derived through\nexponential moving average (EMA)-based statistical updates.\n  Our method introduces three synergistic components: (1) adaptive augmentation\nutilizing dynamically updated image statistics to robustly handle illumination\nand texture variations; (2) embedding stabilization to ensure consistent and\nreliable feature extraction; and (3) contextual refinement leveraging spatial\ncontext for improved detection accuracy. The pervasive use of EMA in our method\nis particularly advantageous given the limited visual complexity and small\nscale of boreholes, allowing stable and robust representation learning even\nunder challenging visual conditions. Experiments on a challenging proprietary\nquarry-site dataset demonstrate substantial improvements over baseline\nYOLO-based architectures, highlighting our method's effectiveness in realistic\nand complex industrial scenarios."}
{"id": "2505.04971", "pdf": "https://arxiv.org/pdf/2505.04971", "abs": "https://arxiv.org/abs/2505.04971", "authors": ["Yuta Kawakami", "Jin Tian"], "title": "Moments of Causal Effects", "categories": ["stat.ME", "cs.AI"], "comment": null, "summary": "The moments of random variables are fundamental statistical measures for\ncharacterizing the shape of a probability distribution, encompassing metrics\nsuch as mean, variance, skewness, and kurtosis. Additionally, the product\nmoments, including covariance and correlation, reveal the relationships between\nmultiple random variables. On the other hand, the primary focus of causal\ninference is the evaluation of causal effects, which are defined as the\ndifference between two potential outcomes. While traditional causal effect\nassessment focuses on the average causal effect, this work provides\ndefinitions, identification theorems, and bounds for moments and product\nmoments of causal effects to analyze their distribution and relationships. We\nconduct experiments to illustrate the estimation of the moments of causal\neffects from finite samples and demonstrate their practical application using a\nreal-world medical dataset."}
{"id": "2505.05408", "pdf": "https://arxiv.org/pdf/2505.05408", "abs": "https://arxiv.org/abs/2505.05408", "authors": ["Zheng-Xin Yong", "M. Farid Adilazuarda", "Jonibek Mansurov", "Ruochen Zhang", "Niklas Muennighoff", "Carsten Eickhoff", "Genta Indra Winata", "Julia Kreutzer", "Stephen H. Bach", "Alham Fikri Aji"], "title": "Crosslingual Reasoning through Test-Time Scaling", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": null, "summary": "Reasoning capabilities of large language models are primarily studied for\nEnglish, even when pretrained models are multilingual. In this work, we\ninvestigate to what extent English reasoning finetuning with long\nchain-of-thoughts (CoTs) can generalize across languages. First, we find that\nscaling up inference compute for English-centric reasoning language models\n(RLMs) improves multilingual mathematical reasoning across many languages\nincluding low-resource languages, to an extent where they outperform models\ntwice their size. Second, we reveal that while English-centric RLM's CoTs are\nnaturally predominantly English, they consistently follow a quote-and-think\npattern to reason about quoted non-English inputs. Third, we discover an\neffective strategy to control the language of long CoT reasoning, and we\nobserve that models reason better and more efficiently in high-resource\nlanguages. Finally, we observe poor out-of-domain reasoning generalization, in\nparticular from STEM to cultural commonsense knowledge, even for English.\nOverall, we demonstrate the potentials, study the mechanisms and outline the\nlimitations of crosslingual generalization of English reasoning test-time\nscaling. We conclude that practitioners should let English-centric RLMs reason\nin high-resource languages, while further work is needed to improve reasoning\nin low-resource languages and out-of-domain contexts."}
{"id": "2505.05143", "pdf": "https://arxiv.org/pdf/2505.05143", "abs": "https://arxiv.org/abs/2505.05143", "authors": ["Mohammed Adnan", "Rohan Jain", "Ekansh Sharma", "Rahul Krishnan", "Yani Ioannou"], "title": "Sparse Training from Random Initialization: Aligning Lottery Ticket Masks using Weight Symmetry", "categories": ["cs.LG"], "comment": "Accepted at ICML 2025", "summary": "The Lottery Ticket Hypothesis (LTH) suggests there exists a sparse LTH mask\nand weights that achieve the same generalization performance as the dense model\nwhile using significantly fewer parameters. However, finding a LTH solution is\ncomputationally expensive, and a LTH sparsity mask does not generalize to other\nrandom weight initializations. Recent work has suggested that neural networks\ntrained from random initialization find solutions within the same basin modulo\npermutation, and proposes a method to align trained models within the same loss\nbasin. We hypothesize that misalignment of basins is the reason why LTH masks\ndo not generalize to new random initializations and propose permuting the LTH\nmask to align with the new optimization basin when performing sparse training\nfrom a different random init. We empirically show a significant increase in\ngeneralization when sparse training from random initialization with the\npermuted mask as compared to using the non-permuted LTH mask, on multiple\ndatasets (CIFAR-10, CIFAR-100 and ImageNet) and models (VGG11, ResNet20 and\nResNet50)."}
{"id": "2505.05022", "pdf": "https://arxiv.org/pdf/2505.05022", "abs": "https://arxiv.org/abs/2505.05022", "authors": ["Tingting Liao", "Yujian Zheng", "Adilbek Karmanov", "Liwen Hu", "Leyang Jin", "Yuliang Xiu", "Hao Li"], "title": "SOAP: Style-Omniscient Animatable Portraits", "categories": ["cs.CV"], "comment": null, "summary": "Creating animatable 3D avatars from a single image remains challenging due to\nstyle limitations (realistic, cartoon, anime) and difficulties in handling\naccessories or hairstyles. While 3D diffusion models advance single-view\nreconstruction for general objects, outputs often lack animation controls or\nsuffer from artifacts because of the domain gap. We propose SOAP, a\nstyle-omniscient framework to generate rigged, topology-consistent avatars from\nany portrait. Our method leverages a multiview diffusion model trained on 24K\n3D heads with multiple styles and an adaptive optimization pipeline to deform\nthe FLAME mesh while maintaining topology and rigging via differentiable\nrendering. The resulting textured avatars support FACS-based animation,\nintegrate with eyeballs and teeth, and preserve details like braided hair or\naccessories. Extensive experiments demonstrate the superiority of our method\nover state-of-the-art techniques for both single-view head modeling and\ndiffusion-based generation of Image-to-3D. Our code and data are publicly\navailable for research purposes at https://github.com/TingtingLiao/soap."}
{"id": "2505.04972", "pdf": "https://arxiv.org/pdf/2505.04972", "abs": "https://arxiv.org/abs/2505.04972", "authors": ["Mattia Sartori", "Chetna Singhal", "Neelabhro Roy", "Davide Brunelli", "James Gross"], "title": "AI and Vision based Autonomous Navigation of Nano-Drones in Partially-Known Environments", "categories": ["cs.RO", "cs.AI", "cs.CV", "cs.LG", "cs.NI"], "comment": "in DCOSS-IoT 2025, Wi-DroIT 2025", "summary": "The miniaturisation of sensors and processors, the advancements in connected\nedge intelligence, and the exponential interest in Artificial Intelligence are\nboosting the affirmation of autonomous nano-size drones in the Internet of\nRobotic Things ecosystem. However, achieving safe autonomous navigation and\nhigh-level tasks such as exploration and surveillance with these tiny platforms\nis extremely challenging due to their limited resources. This work focuses on\nenabling the safe and autonomous flight of a pocket-size, 30-gram platform\ncalled Crazyflie 2.1 in a partially known environment. We propose a novel\nAI-aided, vision-based reactive planning method for obstacle avoidance under\nthe ambit of Integrated Sensing, Computing and Communication paradigm. We deal\nwith the constraints of the nano-drone by splitting the navigation task into\ntwo parts: a deep learning-based object detector runs on the edge (external\nhardware) while the planning algorithm is executed onboard. The results show\nthe ability to command the drone at $\\sim8$ frames-per-second and a model\nperformance reaching a COCO mean-average-precision of $60.8$. Field experiments\ndemonstrate the feasibility of the solution with the drone flying at a top\nspeed of $1$ m/s while steering away from an obstacle placed in an unknown\nposition and reaching the target destination. The outcome highlights the\ncompatibility of the communication delay and the model performance with the\nrequirements of the real-time navigation task. We provide a feasible\nalternative to a fully onboard implementation that can be extended to\nautonomous exploration with nano-drones."}
{"id": "2505.05410", "pdf": "https://arxiv.org/pdf/2505.05410", "abs": "https://arxiv.org/abs/2505.05410", "authors": ["Yanda Chen", "Joe Benton", "Ansh Radhakrishnan", "Jonathan Uesato", "Carson Denison", "John Schulman", "Arushi Somani", "Peter Hase", "Misha Wagner", "Fabien Roger", "Vlad Mikulik", "Samuel R. Bowman", "Jan Leike", "Jared Kaplan", "Ethan Perez"], "title": "Reasoning Models Don't Always Say What They Think", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": null, "summary": "Chain-of-thought (CoT) offers a potential boon for AI safety as it allows\nmonitoring a model's CoT to try to understand its intentions and reasoning\nprocesses. However, the effectiveness of such monitoring hinges on CoTs\nfaithfully representing models' actual reasoning processes. We evaluate CoT\nfaithfulness of state-of-the-art reasoning models across 6 reasoning hints\npresented in the prompts and find: (1) for most settings and models tested,\nCoTs reveal their usage of hints in at least 1% of examples where they use the\nhint, but the reveal rate is often below 20%, (2) outcome-based reinforcement\nlearning initially improves faithfulness but plateaus without saturating, and\n(3) when reinforcement learning increases how frequently hints are used (reward\nhacking), the propensity to verbalize them does not increase, even without\ntraining against a CoT monitor. These results suggest that CoT monitoring is a\npromising way of noticing undesired behaviors during training and evaluations,\nbut that it is not sufficient to rule them out. They also suggest that in\nsettings like ours where CoT reasoning is not necessary, test-time monitoring\nof CoTs is unlikely to reliably catch rare and catastrophic unexpected\nbehaviors."}
{"id": "2505.05145", "pdf": "https://arxiv.org/pdf/2505.05145", "abs": "https://arxiv.org/abs/2505.05145", "authors": ["Xinyan Hu", "Kayo Yin", "Michael I. Jordan", "Jacob Steinhardt", "Lijie Chen"], "title": "Understanding In-context Learning of Addition via Activation Subspaces", "categories": ["cs.LG", "cs.AI", "cs.CL"], "comment": "16 pages", "summary": "To perform in-context learning, language models must extract signals from\nindividual few-shot examples, aggregate these into a learned prediction rule,\nand then apply this rule to new examples. How is this implemented in the\nforward pass of modern transformer models? To study this, we consider a\nstructured family of few-shot learning tasks for which the true prediction rule\nis to add an integer $k$ to the input. We find that Llama-3-8B attains high\naccuracy on this task for a range of $k$, and localize its few-shot ability to\njust three attention heads via a novel optimization approach. We further show\nthe extracted signals lie in a six-dimensional subspace, where four of the\ndimensions track the unit digit and the other two dimensions track overall\nmagnitude. We finally examine how these heads extract information from\nindividual few-shot examples, identifying a self-correction mechanism in which\nmistakes from earlier examples are suppressed by later examples. Our results\ndemonstrate how tracking low-dimensional subspaces across a forward pass can\nprovide insight into fine-grained computational structures."}
{"id": "2505.05023", "pdf": "https://arxiv.org/pdf/2505.05023", "abs": "https://arxiv.org/abs/2505.05023", "authors": ["Jialei Chen", "Xu Zheng", "Dongyue Li", "Chong Yi", "Seigo Ito", "Danda Pani Paudel", "Luc Van Gool", "Hiroshi Murase", "Daisuke Deguchi"], "title": "Split Matching for Inductive Zero-shot Semantic Segmentation", "categories": ["cs.CV"], "comment": null, "summary": "Zero-shot Semantic Segmentation (ZSS) aims to segment categories that are not\nannotated during training. While fine-tuning vision-language models has\nachieved promising results, these models often overfit to seen categories due\nto the lack of supervision for unseen classes. As an alternative to fully\nsupervised approaches, query-based segmentation has shown great latent in ZSS,\nas it enables object localization without relying on explicit labels. However,\nconventional Hungarian matching, a core component in query-based frameworks,\nneeds full supervision and often misclassifies unseen categories as background\nin the setting of ZSS. To address this issue, we propose Split Matching (SM), a\nnovel assignment strategy that decouples Hungarian matching into two\ncomponents: one for seen classes in annotated regions and another for latent\nclasses in unannotated regions (referred to as unseen candidates).\nSpecifically, we partition the queries into seen and candidate groups, enabling\neach to be optimized independently according to its available supervision. To\ndiscover unseen candidates, we cluster CLIP dense features to generate pseudo\nmasks and extract region-level embeddings using CLS tokens. Matching is then\nconducted separately for the two groups based on both class-level similarity\nand mask-level consistency. Additionally, we introduce a Multi-scale Feature\nEnhancement (MFE) module that refines decoder features through residual\nmulti-scale aggregation, improving the model's ability to capture spatial\ndetails across resolutions. SM is the first to introduce decoupled Hungarian\nmatching under the inductive ZSS setting, and achieves state-of-the-art\nperformance on two standard benchmarks."}
{"id": "2505.04977", "pdf": "https://arxiv.org/pdf/2505.04977", "abs": "https://arxiv.org/abs/2505.04977", "authors": ["Brian Choi", "Shu Wang", "Isabelle Choi", "Kun Sun"], "title": "ChainMarks: Securing DNN Watermark with Cryptographic Chain", "categories": ["cs.CR", "cs.AI"], "comment": "Accepted In ACM ASIA Conference on Computer and Communications\n  Security (ASIA CCS '25), August 25-29, 2025, Ha Noi, Vietnam", "summary": "With the widespread deployment of deep neural network (DNN) models, dynamic\nwatermarking techniques are being used to protect the intellectual property of\nmodel owners. However, recent studies have shown that existing watermarking\nschemes are vulnerable to watermark removal and ambiguity attacks. Besides, the\nvague criteria for determining watermark presence further increase the\nlikelihood of such attacks. In this paper, we propose a secure DNN watermarking\nscheme named ChainMarks, which generates secure and robust watermarks by\nintroducing a cryptographic chain into the trigger inputs and utilizes a\ntwo-phase Monte Carlo method for determining watermark presence. First,\nChainMarks generates trigger inputs as a watermark dataset by repeatedly\napplying a hash function over a secret key, where the target labels associated\nwith trigger inputs are generated from the digital signature of model owner.\nThen, the watermarked model is produced by training a DNN over both the\noriginal and watermark datasets. To verify watermarks, we compare the predicted\nlabels of trigger inputs with the target labels and determine ownership with a\nmore accurate decision threshold that considers the classification probability\nof specific models. Experimental results show that ChainMarks exhibits higher\nlevels of robustness and security compared to state-of-the-art watermarking\nschemes. With a better marginal utility, ChainMarks provides a higher\nprobability guarantee of watermark presence in DNN models with the same level\nof watermark accuracy."}
{"id": "2505.05423", "pdf": "https://arxiv.org/pdf/2505.05423", "abs": "https://arxiv.org/abs/2505.05423", "authors": ["Ran Zhang", "Wei Zhao", "Lieve Macken", "Steffen Eger"], "title": "TransProQA: an LLM-based literary Translation evaluation metric with Professional Question Answering", "categories": ["cs.CL", "cs.AI"], "comment": "WIP", "summary": "The impact of Large Language Models (LLMs) has extended into literary\ndomains. However, existing evaluation metrics prioritize mechanical accuracy\nover artistic expression and tend to overrate machine translation (MT) as being\nsuperior to experienced professional human translation. In the long run, this\nbias could result in a permanent decline in translation quality and cultural\nauthenticity. In response to the urgent need for a specialized literary\nevaluation metric, we introduce TransProQA, a novel, reference-free, LLM-based\nquestion-answering (QA) framework designed specifically for literary\ntranslation evaluation. TransProQA uniquely integrates insights from\nprofessional literary translators and researchers, focusing on critical\nelements in literary quality assessment such as literary devices, cultural\nunderstanding, and authorial voice. Our extensive evaluation shows that while\nliterary-finetuned XCOMET-XL yields marginal gains, TransProQA substantially\noutperforms current metrics, achieving up to 0.07 gain in correlation (ACC-EQ\nand Kendall's tau) and surpassing the best state-of-the-art (SOTA) metrics by\nover 15 points in adequacy assessments. Incorporating professional translator\ninsights as weights further improves performance, highlighting the value of\ntranslator inputs. Notably, TransProQA approaches human-level evaluation\nperformance comparable to trained linguistic annotators. It demonstrates broad\napplicability to open-source models such as LLaMA3.3-70b and Qwen2.5-32b,\nindicating its potential as an accessible and training-free literary evaluation\nmetric and a valuable tool for evaluating texts that require local processing\ndue to copyright or ethical considerations."}
{"id": "2505.05155", "pdf": "https://arxiv.org/pdf/2505.05155", "abs": "https://arxiv.org/abs/2505.05155", "authors": ["Zhihao Zeng", "Ziquan Fang", "Wei Shao", "Lu Chen", "Yunjun Gao"], "title": "FedTDP: A Privacy-Preserving and Unified Framework for Trajectory Data Preparation via Federated Learning", "categories": ["cs.LG", "cs.CR"], "comment": null, "summary": "Trajectory data, which capture the movement patterns of people and vehicles\nover time and space, are crucial for applications like traffic optimization and\nurban planning. However, issues such as noise and incompleteness often\ncompromise data quality, leading to inaccurate trajectory analyses and limiting\nthe potential of these applications. While Trajectory Data Preparation (TDP)\ncan enhance data quality, existing methods suffer from two key limitations: (i)\nthey do not address data privacy concerns, particularly in federated settings\nwhere trajectory data sharing is prohibited, and (ii) they typically design\ntask-specific models that lack generalizability across diverse TDP scenarios.\nTo overcome these challenges, we propose FedTDP, a privacy-preserving and\nunified framework that leverages the capabilities of Large Language Models\n(LLMs) for TDP in federated environments. Specifically, we: (i) design a\ntrajectory privacy autoencoder to secure data transmission and protect privacy,\n(ii) introduce a trajectory knowledge enhancer to improve model learning of\nTDP-related knowledge, enabling the development of TDP-oriented LLMs, and (iii)\npropose federated parallel optimization to enhance training efficiency by\nreducing data transmission and enabling parallel model training. Experiments on\n6 real datasets and 10 mainstream TDP tasks demonstrate that FedTDP\nconsistently outperforms 13 state-of-the-art baselines."}
{"id": "2505.05043", "pdf": "https://arxiv.org/pdf/2505.05043", "abs": "https://arxiv.org/abs/2505.05043", "authors": ["Mani Kumar Tellamekala", "Shashank Jaiswal", "Thomas Smith", "Timur Alamev", "Gary McKeown", "Anthony Brown", "Michel Valstar"], "title": "xTrace: A Facial Expressive Behaviour Analysis Tool for Continuous Affect Recognition", "categories": ["cs.CV"], "comment": null, "summary": "Recognising expressive behaviours in face videos is a long-standing challenge\nin Affective Computing. Despite significant advancements in recent years, it\nstill remains a challenge to build a robust and reliable system for\nnaturalistic and in-the-wild facial expressive behaviour analysis in real time.\nThis paper addresses two key challenges in building such a system: (1). The\npaucity of large-scale labelled facial affect video datasets with extensive\ncoverage of the 2D emotion space, and (2). The difficulty of extracting facial\nvideo features that are discriminative, interpretable, robust, and\ncomputationally efficient. Toward addressing these challenges, we introduce\nxTrace, a robust tool for facial expressive behaviour analysis and predicting\ncontinuous values of dimensional emotions, namely valence and arousal, from\nin-the-wild face videos.\n  To address challenge (1), our affect recognition model is trained on the\nlargest facial affect video data set, containing ~450k videos that cover most\nemotion zones in the dimensional emotion space, making xTrace highly versatile\nin analysing a wide spectrum of naturalistic expressive behaviours. To address\nchallenge (2), xTrace uses facial affect descriptors that are not only\nexplainable, but can also achieve a high degree of accuracy and robustness with\nlow computational complexity. The key components of xTrace are benchmarked\nagainst three existing tools: MediaPipe, OpenFace, and Augsburg Affect Toolbox.\nOn an in-the-wild validation set composed of 50k videos, xTrace achieves 0.86\nmean CCC and 0.13 mean absolute error values. We present a detailed error\nanalysis of affect predictions from xTrace, illustrating (a). its ability to\nrecognise emotions with high accuracy across most bins in the 2D emotion space,\n(b). its robustness to non-frontal head pose angles, and (c). a strong\ncorrelation between its uncertainty estimates and its accuracy."}
{"id": "2505.04983", "pdf": "https://arxiv.org/pdf/2505.04983", "abs": "https://arxiv.org/abs/2505.04983", "authors": ["Yuta Kawakami", "Jin Tian"], "title": "Decomposition of Probabilities of Causation with Two Mediators", "categories": ["stat.ME", "cs.AI"], "comment": "arXiv admin note: text overlap with arXiv:2412.14491", "summary": "Mediation analysis for probabilities of causation (PoC) provides a\nfundamental framework for evaluating the necessity and sufficiency of treatment\nin provoking an event through different causal pathways. One of the primary\nobjectives of causal mediation analysis is to decompose the total effect into\npath-specific components. In this study, we investigate the path-specific\nprobability of necessity and sufficiency (PNS) to decompose the total PNS into\npath-specific components along distinct causal pathways between treatment and\noutcome, incorporating two mediators. We define the path-specific PNS for\ndecomposition and provide an identification theorem. Furthermore, we conduct\nnumerical experiments to assess the properties of the proposed estimators from\nfinite samples and demonstrate their practical application using a real-world\neducational dataset."}
{"id": "2505.05427", "pdf": "https://arxiv.org/pdf/2505.05427", "abs": "https://arxiv.org/abs/2505.05427", "authors": ["Yudong Wang", "Zixuan Fu", "Jie Cai", "Peijun Tang", "Hongya Lyu", "Yewei Fang", "Zhi Zheng", "Jie Zhou", "Guoyang Zeng", "Chaojun Xiao", "Xu Han", "Zhiyuan Liu"], "title": "Ultra-FineWeb: Efficient Data Filtering and Verification for High-Quality LLM Training Data", "categories": ["cs.CL"], "comment": "The datasets are available on\n  https://huggingface.co/datasets/openbmb/UltraFineWeb", "summary": "Data quality has become a key factor in enhancing model performance with the\nrapid development of large language models (LLMs). Model-driven data filtering\nhas increasingly become a primary approach for acquiring high-quality data.\nHowever, it still faces two main challenges: (1) the lack of an efficient data\nverification strategy makes it difficult to provide timely feedback on data\nquality; and (2) the selection of seed data for training classifiers lacks\nclear criteria and relies heavily on human expertise, introducing a degree of\nsubjectivity. To address the first challenge, we introduce an efficient\nverification strategy that enables rapid evaluation of the impact of data on\nLLM training with minimal computational cost. To tackle the second challenge,\nwe build upon the assumption that high-quality seed data is beneficial for LLM\ntraining, and by integrating the proposed verification strategy, we optimize\nthe selection of positive and negative samples and propose an efficient data\nfiltering pipeline. This pipeline not only improves filtering efficiency,\nclassifier quality, and robustness, but also significantly reduces experimental\nand inference costs. In addition, to efficiently filter high-quality data, we\nemploy a lightweight classifier based on fastText, and successfully apply the\nfiltering pipeline to two widely-used pre-training corpora, FineWeb and Chinese\nFineWeb datasets, resulting in the creation of the higher-quality Ultra-FineWeb\ndataset. Ultra-FineWeb contains approximately 1 trillion English tokens and 120\nbillion Chinese tokens. Empirical results demonstrate that the LLMs trained on\nUltra-FineWeb exhibit significant performance improvements across multiple\nbenchmark tasks, validating the effectiveness of our pipeline in enhancing both\ndata quality and training efficiency."}
{"id": "2505.05169", "pdf": "https://arxiv.org/pdf/2505.05169", "abs": "https://arxiv.org/abs/2505.05169", "authors": ["Tsubasa Harada", "Shinji Ito", "Hanna Sumita"], "title": "Bandit Max-Min Fair Allocation", "categories": ["cs.LG"], "comment": "23 pages", "summary": "In this paper, we study a new decision-making problem called the bandit\nmax-min fair allocation (BMMFA) problem. The goal of this problem is to\nmaximize the minimum utility among agents with additive valuations by\nrepeatedly assigning indivisible goods to them. One key feature of this problem\nis that each agent's valuation for each item can only be observed through the\nsemi-bandit feedback, while existing work supposes that the item values are\nprovided at the beginning of each round. Another key feature is that the\nalgorithm's reward function is not additive with respect to rounds, unlike most\nbandit-setting problems.\n  Our first contribution is to propose an algorithm that has an asymptotic\nregret bound of $O(m\\sqrt{T}\\ln T/n + m\\sqrt{T \\ln(mnT)})$, where $n$ is the\nnumber of agents, $m$ is the number of items, and $T$ is the time horizon. This\nis based on a novel combination of bandit techniques and a resource allocation\nalgorithm studied in the literature on competitive analysis. Our second\ncontribution is to provide the regret lower bound of $\\Omega(m\\sqrt{T}/n)$.\nWhen $T$ is sufficiently larger than $n$, the gap between the upper and lower\nbounds is a logarithmic factor of $T$."}
{"id": "2505.05049", "pdf": "https://arxiv.org/pdf/2505.05049", "abs": "https://arxiv.org/abs/2505.05049", "authors": ["Timo Kaiser", "Thomas Norrenbrock", "Bodo Rosenhahn"], "title": "UncertainSAM: Fast and Efficient Uncertainty Quantification of the Segment Anything Model", "categories": ["cs.CV"], "comment": "Accepted to ICML'25", "summary": "The introduction of the Segment Anything Model (SAM) has paved the way for\nnumerous semantic segmentation applications. For several tasks, quantifying the\nuncertainty of SAM is of particular interest. However, the ambiguous nature of\nthe class-agnostic foundation model SAM challenges current uncertainty\nquantification (UQ) approaches. This paper presents a theoretically motivated\nuncertainty quantification model based on a Bayesian entropy formulation\njointly respecting aleatoric, epistemic, and the newly introduced task\nuncertainty. We use this formulation to train USAM, a lightweight post-hoc UQ\nmethod. Our model traces the root of uncertainty back to under-parameterised\nmodels, insufficient prompts or image ambiguities. Our proposed deterministic\nUSAM demonstrates superior predictive capabilities on the SA-V, MOSE, ADE20k,\nDAVIS, and COCO datasets, offering a computationally cheap and easy-to-use UQ\nalternative that can support user-prompting, enhance semi-supervised pipelines,\nor balance the tradeoff between accuracy and cost efficiency."}
{"id": "2505.05071", "pdf": "https://arxiv.org/pdf/2505.05071", "abs": "https://arxiv.org/abs/2505.05071", "authors": ["Chunyu Xie", "Bin Wang", "Fanjing Kong", "Jincheng Li", "Dawei Liang", "Gengshen Zhang", "Dawei Leng", "Yuhui Yin"], "title": "FG-CLIP: Fine-Grained Visual and Textual Alignment", "categories": ["cs.CV", "cs.AI"], "comment": "Accepted at ICML 2025", "summary": "Contrastive Language-Image Pre-training (CLIP) excels in multimodal tasks\nsuch as image-text retrieval and zero-shot classification but struggles with\nfine-grained understanding due to its focus on coarse-grained short captions.\nTo address this, we propose Fine-Grained CLIP (FG-CLIP), which enhances\nfine-grained understanding through three key innovations. First, we leverage\nlarge multimodal models to generate 1.6 billion long caption-image pairs for\ncapturing global-level semantic details. Second, a high-quality dataset is\nconstructed with 12 million images and 40 million region-specific bounding\nboxes aligned with detailed captions to ensure precise, context-rich\nrepresentations. Third, 10 million hard fine-grained negative samples are\nincorporated to improve the model's ability to distinguish subtle semantic\ndifferences. Corresponding training methods are meticulously designed for these\ndata. Extensive experiments demonstrate that FG-CLIP outperforms the original\nCLIP and other state-of-the-art methods across various downstream tasks,\nincluding fine-grained understanding, open-vocabulary object detection,\nimage-text retrieval, and general multimodal benchmarks. These results\nhighlight FG-CLIP's effectiveness in capturing fine-grained image details and\nimproving overall model performance. The related data, code, and models are\navailable at https://github.com/360CVGroup/FG-CLIP."}
{"id": "2505.05445", "pdf": "https://arxiv.org/pdf/2505.05445", "abs": "https://arxiv.org/abs/2505.05445", "authors": ["Chalamalasetti Kranti", "Sherzod Hakimov", "David Schlangen"], "title": "clem:todd: A Framework for the Systematic Benchmarking of LLM-Based Task-Oriented Dialogue System Realisations", "categories": ["cs.CL"], "comment": "30 pages", "summary": "The emergence of instruction-tuned large language models (LLMs) has advanced\nthe field of dialogue systems, enabling both realistic user simulations and\nrobust multi-turn conversational agents. However, existing research often\nevaluates these components in isolation-either focusing on a single user\nsimulator or a specific system design-limiting the generalisability of insights\nacross architectures and configurations. In this work, we propose clem todd\n(chat-optimized LLMs for task-oriented dialogue systems development), a\nflexible framework for systematically evaluating dialogue systems under\nconsistent conditions. clem todd enables detailed benchmarking across\ncombinations of user simulators and dialogue systems, whether existing models\nfrom literature or newly developed ones. It supports plug-and-play integration\nand ensures uniform datasets, evaluation metrics, and computational\nconstraints. We showcase clem todd's flexibility by re-evaluating existing\ntask-oriented dialogue systems within this unified setup and integrating three\nnewly proposed dialogue systems into the same evaluation pipeline. Our results\nprovide actionable insights into how architecture, scale, and prompting\nstrategies affect dialogue performance, offering practical guidance for\nbuilding efficient and effective conversational AI systems."}
{"id": "2505.05180", "pdf": "https://arxiv.org/pdf/2505.05180", "abs": "https://arxiv.org/abs/2505.05180", "authors": ["Cong Hua", "Qianqian Xu", "Zhiyong Yang", "Zitai Wang", "Shilong Bao", "Qingming Huang"], "title": "OpenworldAUC: Towards Unified Evaluation and Optimization for Open-world Prompt Tuning", "categories": ["cs.LG"], "comment": "This paper has been accepted by ICML2025", "summary": "Prompt tuning adapts Vision-Language Models like CLIP to open-world tasks\nwith minimal training costs. In this direction, one typical paradigm evaluates\nmodel performance separately on known classes (i.e., base domain) and unseen\nclasses (i.e., new domain). However, real-world scenarios require models to\nhandle inputs without prior domain knowledge. This practical challenge has\nspurred the development of open-world prompt tuning, which demands a unified\nevaluation of two stages: 1) detecting whether an input belongs to the base or\nnew domain (P1), and 2) classifying the sample into its correct class (P2).\nWhat's more, as domain distributions are generally unknown, a proper metric\nshould be insensitive to varying base/new sample ratios (P3). However, we find\nthat current metrics, including HM, overall accuracy, and AUROC, fail to\nsatisfy these three properties simultaneously. To bridge this gap, we propose\nOpenworldAUC, a unified metric that jointly assesses detection and\nclassification through pairwise instance comparisons. To optimize OpenworldAUC\neffectively, we introduce Gated Mixture-of-Prompts (GMoP), which employs\ndomain-specific prompts and a gating mechanism to dynamically balance detection\nand classification. Theoretical guarantees ensure generalization of GMoP under\npractical conditions. Experiments on 15 benchmarks in open-world scenarios show\nGMoP achieves SOTA performance on OpenworldAUC and other metrics. We release\nthe code at https://github.com/huacong/OpenworldAUC"}
{"id": "2505.05062", "pdf": "https://arxiv.org/pdf/2505.05062", "abs": "https://arxiv.org/abs/2505.05062", "authors": ["Enhao Zhang", "Chaohua Li", "Chuanxing Geng", "Songcan Chen"], "title": "ULFine: Unbiased Lightweight Fine-tuning for Foundation-Model-Assisted Long-Tailed Semi-Supervised Learning", "categories": ["cs.CV"], "comment": null, "summary": "Based on the success of large-scale visual foundation models like CLIP in\nvarious downstream tasks, this paper initially attempts to explore their impact\non Long-Tailed Semi-Supervised Learning (LTSSL) by employing the foundation\nmodel with three strategies: Linear Probing (LP), Lightweight Fine-Tuning\n(LFT), and Full Fine-Tuning (FFT). Our analysis presents the following\ninsights: i) Compared to LTSSL algorithms trained from scratch, FFT results in\na decline in model performance, whereas LP and LFT, although boosting overall\nmodel performance, exhibit negligible benefits to tail classes. ii) LP produces\nnumerous false pseudo-labels due to \\textit{underlearned} training data, while\nLFT can reduce the number of these false labels but becomes overconfident about\nthem owing to \\textit{biased fitting} training data. This exacerbates the\npseudo-labeled and classifier biases inherent in LTSSL, limiting performance\nimprovement in the tail classes. With these insights, we propose a Unbiased\nLightweight Fine-tuning strategy, \\textbf{ULFine}, which mitigates the\noverconfidence via confidence-aware adaptive fitting of textual prototypes and\ncounteracts the pseudo-labeled and classifier biases via complementary fusion\nof dual logits. Extensive experiments demonstrate that ULFine markedly\ndecreases training costs by over ten times and substantially increases\nprediction accuracies compared to state-of-the-art methods."}
{"id": "2505.05138", "pdf": "https://arxiv.org/pdf/2505.05138", "abs": "https://arxiv.org/abs/2505.05138", "authors": ["Steven Jorgensen", "Erik Hemberg", "Jamal Toutouh", "Una-May O'Reilly"], "title": "Guiding Evolutionary AutoEncoder Training with Activation-Based Pruning Operators", "categories": ["cs.NE", "cs.AI"], "comment": "Accepted to The Genetic and Evolutionary Computation Conference\n  (GECCO 2025)", "summary": "This study explores a novel approach to neural network pruning using\nevolutionary computation, focusing on simultaneously pruning the encoder and\ndecoder of an autoencoder. We introduce two new mutation operators that use\nlayer activations to guide weight pruning. Our findings reveal that one of\nthese activation-informed operators outperforms random pruning, resulting in\nmore efficient autoencoders with comparable performance to canonically trained\nmodels. Prior work has established that autoencoder training is effective and\nscalable with a spatial coevolutionary algorithm that cooperatively coevolves a\npopulation of encoders with a population of decoders, rather than one\nautoencoder. We evaluate how the same activity-guided mutation operators\ntransfer to this context. We find that random pruning is better than guided\npruning, in the coevolutionary setting. This suggests activation-based guidance\nproves more effective in low-dimensional pruning environments, where\nconstrained sample spaces can lead to deviations from true uniformity in\nrandomization. Conversely, population-driven strategies enhance robustness by\nexpanding the total pruning dimensionality, achieving statistically uniform\nrandomness that better preserves system dynamics. We experiment with pruning\naccording to different schedules and present best combinations of operator and\nschedule for the canonical and coevolving populations cases."}
{"id": "2505.05459", "pdf": "https://arxiv.org/pdf/2505.05459", "abs": "https://arxiv.org/abs/2505.05459", "authors": ["Fatima Haouari", "Carolina Scarton", "Nicolò Faggiani", "Nikolaos Nikolaidis", "Bonka Kotseva", "Ibrahim Abu Farha", "Jens Linge", "Kalina Bontcheva"], "title": "UKElectionNarratives: A Dataset of Misleading Narratives Surrounding Recent UK General Elections", "categories": ["cs.CL", "cs.SI"], "comment": "This work was accepted at the International AAAI Conference on Web\n  and Social Media (ICWSM 2025)", "summary": "Misleading narratives play a crucial role in shaping public opinion during\nelections, as they can influence how voters perceive candidates and political\nparties. This entails the need to detect these narratives accurately. To\naddress this, we introduce the first taxonomy of common misleading narratives\nthat circulated during recent elections in Europe. Based on this taxonomy, we\nconstruct and analyse UKElectionNarratives: the first dataset of\nhuman-annotated misleading narratives which circulated during the UK General\nElections in 2019 and 2024. We also benchmark Pre-trained and Large Language\nModels (focusing on GPT-4o), studying their effectiveness in detecting\nelection-related misleading narratives. Finally, we discuss potential use cases\nand make recommendations for future research directions using the proposed\ncodebook and dataset."}
{"id": "2505.05181", "pdf": "https://arxiv.org/pdf/2505.05181", "abs": "https://arxiv.org/abs/2505.05181", "authors": ["Bojian Yin", "Federico Corradi"], "title": "Stochastic Variational Propagation: Local, Scalable and Efficient Alternative to Backpropagation", "categories": ["cs.LG", "cs.AI"], "comment": "14 pages, 5 figures", "summary": "Backpropagation (BP) is the cornerstone of deep learning, but its reliance on\nglobal gradient synchronization limits scalability and imposes significant\nmemory overhead. We propose Stochastic Variational Propagation (SVP), a\nscalable alternative that reframes training as hierarchical variational\ninference. SVP treats layer activations as latent variables and optimizes local\nEvidence Lower Bounds (ELBOs), enabling independent, local updates while\npreserving global coherence. However, directly applying KL divergence in\nlayer-wise ELBOs risks inter-layer's representation collapse due to excessive\ncompression. To prevent this, SVP projects activations into low-dimensional\nspaces via fixed random matrices, ensuring information preservation and\nrepresentational diversity. Combined with a feature alignment loss for\ninter-layer consistency, SVP achieves competitive accuracy with BP across\ndiverse architectures (MLPs, CNNs, Transformers) and datasets (MNIST to\nImageNet), reduces memory usage by up to 4x, and significantly improves\nscalability. More broadly, SVP introduces a probabilistic perspective to deep\nrepresentation learning, opening pathways toward more modular and interpretable\nneural network design."}
{"id": "2505.05074", "pdf": "https://arxiv.org/pdf/2505.05074", "abs": "https://arxiv.org/abs/2505.05074", "authors": ["Tommaso Apicella", "Alessio Xompero", "Andrea Cavallaro"], "title": "Visual Affordances: Enabling Robots to Understand Object Functionality", "categories": ["cs.CV", "cs.RO"], "comment": "24 pages, 12 figures, 10 tables. Project website at\n  https://apicis.github.io/aff-survey/", "summary": "Human-robot interaction for assistive technologies relies on the prediction\nof affordances, which are the potential actions a robot can perform on objects.\nPredicting object affordances from visual perception is formulated differently\nfor tasks such as grasping detection, affordance classification, affordance\nsegmentation, and hand-object interaction synthesis. In this work, we highlight\nthe reproducibility issue in these redefinitions, making comparative benchmarks\nunfair and unreliable. To address this problem, we propose a unified\nformulation for visual affordance prediction, provide a comprehensive and\nsystematic review of previous works highlighting strengths and limitations of\nmethods and datasets, and analyse what challenges reproducibility. To favour\ntransparency, we introduce the Affordance Sheet, a document to detail the\nproposed solution, the datasets, and the validation. As the physical properties\nof an object influence the interaction with the robot, we present a generic\nframework that links visual affordance prediction to the physical world. Using\nthe weight of an object as an example for this framework, we discuss how\nestimating object mass can affect the affordance prediction. Our approach\nbridges the gap between affordance perception and robot actuation, and accounts\nfor the complete information about objects of interest and how the robot\ninteracts with them to accomplish its task."}
{"id": "2505.05170", "pdf": "https://arxiv.org/pdf/2505.05170", "abs": "https://arxiv.org/abs/2505.05170", "authors": ["Elizabeth Ankrah", "Stephanie Nyairo", "Mercy Muchai", "Kagonya Awori", "Millicent Ochieng", "Mark Kariuki", "Jacki O'Neill"], "title": "Dukawalla: Voice Interfaces for Small Businesses in Africa", "categories": ["cs.HC", "cs.AI"], "comment": null, "summary": "Small and medium sized businesses often struggle with data driven decision\nmaking do to a lack of advanced analytics tools, especially in African\ncountries where they make up a majority of the workforce. Though many tools\nexist they are not designed to fit into the ways of working of SMB workers who\nare mobile first, have limited time to learn new workflows, and for whom social\nand business are tightly coupled. To address this, the Dukawalla prototype was\ncreated. This intelligent assistant bridges the gap between raw business data,\nand actionable insights by leveraging voice interaction and the power of\ngenerative AI. Dukawalla provides an intuitive way for business owners to\ninteract with their data, aiding in informed decision making. This paper\nexamines Dukawalla's deployment across SMBs in Nairobi, focusing on their\nexperiences using this voice based assistant to streamline data collection and\nprovide business insights"}
{"id": "2505.05464", "pdf": "https://arxiv.org/pdf/2505.05464", "abs": "https://arxiv.org/abs/2505.05464", "authors": ["Shiqi Chen", "Jinghan Zhang", "Tongyao Zhu", "Wei Liu", "Siyang Gao", "Miao Xiong", "Manling Li", "Junxian He"], "title": "Bring Reason to Vision: Understanding Perception and Reasoning through Model Merging", "categories": ["cs.CL"], "comment": "ICML 2025. Our code is publicly available at\n  https://github.com/shiqichen17/VLM_Merging", "summary": "Vision-Language Models (VLMs) combine visual perception with the general\ncapabilities, such as reasoning, of Large Language Models (LLMs). However, the\nmechanisms by which these two abilities can be combined and contribute remain\npoorly understood. In this work, we explore to compose perception and reasoning\nthrough model merging that connects parameters of different models. Unlike\nprevious works that often focus on merging models of the same kind, we propose\nmerging models across modalities, enabling the incorporation of the reasoning\ncapabilities of LLMs into VLMs. Through extensive experiments, we demonstrate\nthat model merging offers a successful pathway to transfer reasoning abilities\nfrom LLMs to VLMs in a training-free manner. Moreover, we utilize the merged\nmodels to understand the internal mechanism of perception and reasoning and how\nmerging affects it. We find that perception capabilities are predominantly\nencoded in the early layers of the model, whereas reasoning is largely\nfacilitated by the middle-to-late layers. After merging, we observe that all\nlayers begin to contribute to reasoning, whereas the distribution of perception\nabilities across layers remains largely unchanged. These observations shed\nlight on the potential of model merging as a tool for multimodal integration\nand interpretation."}
{"id": "2505.05190", "pdf": "https://arxiv.org/pdf/2505.05190", "abs": "https://arxiv.org/abs/2505.05190", "authors": ["Yixin Cheng", "Hongcheng Guo", "Yangming Li", "Leonid Sigal"], "title": "Revealing Weaknesses in Text Watermarking Through Self-Information Rewrite Attacks", "categories": ["cs.LG", "cs.AI", "cs.CL", "cs.CR"], "comment": "ICML 2025 Accpeted", "summary": "Text watermarking aims to subtly embed statistical signals into text by\ncontrolling the Large Language Model (LLM)'s sampling process, enabling\nwatermark detectors to verify that the output was generated by the specified\nmodel. The robustness of these watermarking algorithms has become a key factor\nin evaluating their effectiveness. Current text watermarking algorithms embed\nwatermarks in high-entropy tokens to ensure text quality. In this paper, we\nreveal that this seemingly benign design can be exploited by attackers, posing\na significant risk to the robustness of the watermark. We introduce a generic\nefficient paraphrasing attack, the Self-Information Rewrite Attack (SIRA),\nwhich leverages the vulnerability by calculating the self-information of each\ntoken to identify potential pattern tokens and perform targeted attack. Our\nwork exposes a widely prevalent vulnerability in current watermarking\nalgorithms. The experimental results show SIRA achieves nearly 100% attack\nsuccess rates on seven recent watermarking methods with only 0.88 USD per\nmillion tokens cost. Our approach does not require any access to the watermark\nalgorithms or the watermarked LLM and can seamlessly transfer to any LLM as the\nattack model, even mobile-level models. Our findings highlight the urgent need\nfor more robust watermarking."}
{"id": "2505.05081", "pdf": "https://arxiv.org/pdf/2505.05081", "abs": "https://arxiv.org/abs/2505.05081", "authors": ["Jinyu Gu", "Haipeng Liu", "Meng Wang", "Yang Wang"], "title": "PIDiff: Image Customization for Personalized Identities with Diffusion Models", "categories": ["cs.CV"], "comment": "9 pages, 11 figures", "summary": "Text-to-image generation for personalized identities aims at incorporating\nthe specific identity into images using a text prompt and an identity image.\nBased on the powerful generative capabilities of DDPMs, many previous works\nadopt additional prompts, such as text embeddings and CLIP image embeddings, to\nrepresent the identity information, while they fail to disentangle the identity\ninformation and background information. As a result, the generated images not\nonly lose key identity characteristics but also suffer from significantly\nreduced diversity. To address this issue, previous works have combined the W+\nspace from StyleGAN with diffusion models, leveraging this space to provide a\nmore accurate and comprehensive representation of identity features through\nmulti-level feature extraction. However, the entanglement of identity and\nbackground information in in-the-wild images during training prevents accurate\nidentity localization, resulting in severe semantic interference between\nidentity and background. In this paper, we propose a novel fine-tuning-based\ndiffusion model for personalized identities text-to-image generation, named\nPIDiff, which leverages the W+ space and an identity-tailored fine-tuning\nstrategy to avoid semantic entanglement and achieves accurate feature\nextraction and localization. Style editing can also be achieved by PIDiff\nthrough preserving the characteristics of identity features in the W+ space,\nwhich vary from coarse to fine. Through the combination of the proposed\ncross-attention block and parameter optimization strategy, PIDiff preserves the\nidentity information and maintains the generation capability for in-the-wild\nimages of the pre-trained model during inference. Our experimental results\nvalidate the effectiveness of our method in this task."}
{"id": "2505.05189", "pdf": "https://arxiv.org/pdf/2505.05189", "abs": "https://arxiv.org/abs/2505.05189", "authors": ["Wei Peng", "Kang Liu", "Jianchen Hu", "Meng Zhang"], "title": "Biomed-DPT: Dual Modality Prompt Tuning for Biomedical Vision-Language Models", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Prompt learning is one of the most effective paradigms for adapting\npre-trained vision-language models (VLMs) to the biomedical image\nclassification tasks in few shot scenarios. However, most of the current prompt\nlearning methods only used the text prompts and ignored the particular\nstructures (such as the complex anatomical structures and subtle pathological\nfeatures) in the biomedical images. In this work, we propose Biomed-DPT, a\nknowledge-enhanced dual modality prompt tuning technique. In designing the text\nprompt, Biomed-DPT constructs a dual prompt including the template-driven\nclinical prompts and the large language model (LLM)-driven domain-adapted\nprompts, then extracts the clinical knowledge from the domain-adapted prompts\nthrough the knowledge distillation technique. In designing the vision prompt,\nBiomed-DPT introduces the zero vector as a soft prompt to leverage attention\nre-weighting so that the focus on non-diagnostic regions and the recognition of\nnon-critical pathological features are avoided. Biomed-DPT achieves an average\nclassification accuracy of 66.14\\% across 11 biomedical image datasets covering\n9 modalities and 10 organs, with performance reaching 78.06\\% in base classes\nand 75.97\\% in novel classes, surpassing the Context Optimization (CoOp) method\nby 6.20\\%, 3.78\\%, and 8.04\\%, respectively. Our code are available at\n\\underline{https://github.com/Kanyooo/Biomed-DPT}."}
{"id": "2505.05465", "pdf": "https://arxiv.org/pdf/2505.05465", "abs": "https://arxiv.org/abs/2505.05465", "authors": ["Peter Chen", "Xi Chen", "Wotao Yin", "Tianyi Lin"], "title": "ComPO: Preference Alignment via Comparison Oracles", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "25 pages", "summary": "Direct alignment methods are increasingly used for aligning large language\nmodels (LLMs) with human preferences. However, these methods suffer from the\nissues of verbosity and likelihood displacement, which can be driven by the\nnoisy preference pairs that induce similar likelihood for preferred and\ndispreferred responses. The contributions of this paper are two-fold. First, we\npropose a new preference alignment method based on comparison oracles and\nprovide the convergence guarantee for its basic scheme. Second, we improve our\nmethod using some heuristics and conduct the experiments to demonstrate the\nflexibility and compatibility of practical scheme in improving the performance\nof LLMs using noisy preference pairs. Evaluations are conducted across multiple\nbase and instruction-tuned models (Mistral-7B, Llama-3-8B and Gemma-2-9B) with\nbenchmarks (AlpacaEval 2, MT-Bench and Arena-Hard). Experimental results show\nthe effectiveness of our method as an alternative to addressing the limitations\nof existing direct alignment methods. A highlight of our work is that we\nevidence the importance of designing specialized methods for preference pairs\nwith distinct likelihood margin, which complements the recent findings in\n\\citet{Razin-2025-Unintentional}."}
{"id": "2505.05192", "pdf": "https://arxiv.org/pdf/2505.05192", "abs": "https://arxiv.org/abs/2505.05192", "authors": ["Ruichu Cai", "Junjie Wan", "Weilin Chen", "Zeqin Yang", "Zijian Li", "Peng Zhen", "Jiecheng Guo"], "title": "Long-Term Individual Causal Effect Estimation via Identifiable Latent Representation Learning", "categories": ["cs.LG"], "comment": null, "summary": "Estimating long-term causal effects by combining long-term observational and\nshort-term experimental data is a crucial but challenging problem in many\nreal-world scenarios. In existing methods, several ideal assumptions, e.g.\nlatent unconfoundedness assumption or additive equi-confounding bias\nassumption, are proposed to address the latent confounder problem raised by the\nobservational data. However, in real-world applications, these assumptions are\ntypically violated which limits their practical effectiveness. In this paper,\nwe tackle the problem of estimating the long-term individual causal effects\nwithout the aforementioned assumptions. Specifically, we propose to utilize the\nnatural heterogeneity of data, such as data from multiple sources, to identify\nlatent confounders, thereby significantly avoiding reliance on idealized\nassumptions. Practically, we devise a latent representation learning-based\nestimator of long-term causal effects. Theoretically, we establish the\nidentifiability of latent confounders, with which we further achieve long-term\neffect identification. Extensive experimental studies, conducted on multiple\nsynthetic and semi-synthetic datasets, demonstrate the effectiveness of our\nproposed method."}
{"id": "2505.05089", "pdf": "https://arxiv.org/pdf/2505.05089", "abs": "https://arxiv.org/abs/2505.05089", "authors": ["Zuntao Liu", "Hao Zhuang", "Junjie Jiang", "Yuhang Song", "Zheng Fang"], "title": "Nonlinear Motion-Guided and Spatio-Temporal Aware Network for Unsupervised Event-Based Optical Flow", "categories": ["cs.CV"], "comment": "Accepted to ICRA 2025. Project Page:\n  https://wynelio.github.io/E-NMSTFlow", "summary": "Event cameras have the potential to capture continuous motion information\nover time and space, making them well-suited for optical flow estimation.\nHowever, most existing learning-based methods for event-based optical flow\nadopt frame-based techniques, ignoring the spatio-temporal characteristics of\nevents. Additionally, these methods assume linear motion between consecutive\nevents within the loss time window, which increases optical flow errors in\nlong-time sequences. In this work, we observe that rich spatio-temporal\ninformation and accurate nonlinear motion between events are crucial for\nevent-based optical flow estimation. Therefore, we propose E-NMSTFlow, a novel\nunsupervised event-based optical flow network focusing on long-time sequences.\nWe propose a Spatio-Temporal Motion Feature Aware (STMFA) module and an\nAdaptive Motion Feature Enhancement (AMFE) module, both of which utilize rich\nspatio-temporal information to learn spatio-temporal data associations.\nMeanwhile, we propose a nonlinear motion compensation loss that utilizes the\naccurate nonlinear motion between events to improve the unsupervised learning\nof our network. Extensive experiments demonstrate the effectiveness and\nsuperiority of our method. Remarkably, our method ranks first among\nunsupervised learning methods on the MVSEC and DSEC-Flow datasets. Our project\npage is available at https://wynelio.github.io/E-NMSTFlow."}
{"id": "2505.05195", "pdf": "https://arxiv.org/pdf/2505.05195", "abs": "https://arxiv.org/abs/2505.05195", "authors": ["Xinyue Xu", "Yueying Hu", "Hui Tang", "Yi Qin", "Lu Mi", "Hao Wang", "Xiaomeng Li"], "title": "Concept-Based Unsupervised Domain Adaptation", "categories": ["cs.LG", "cs.AI", "cs.CV"], "comment": "Accepted by ICML 2025", "summary": "Concept Bottleneck Models (CBMs) enhance interpretability by explaining\npredictions through human-understandable concepts but typically assume that\ntraining and test data share the same distribution. This assumption often fails\nunder domain shifts, leading to degraded performance and poor generalization.\nTo address these limitations and improve the robustness of CBMs, we propose the\nConcept-based Unsupervised Domain Adaptation (CUDA) framework. CUDA is designed\nto: (1) align concept representations across domains using adversarial\ntraining, (2) introduce a relaxation threshold to allow minor domain-specific\ndifferences in concept distributions, thereby preventing performance drop due\nto over-constraints of these distributions, (3) infer concepts directly in the\ntarget domain without requiring labeled concept data, enabling CBMs to adapt to\ndiverse domains, and (4) integrate concept learning into conventional domain\nadaptation (DA) with theoretical guarantees, improving interpretability and\nestablishing new benchmarks for DA. Experiments demonstrate that our approach\nsignificantly outperforms the state-of-the-art CBM and DA methods on real-world\ndatasets."}
{"id": "2505.04806", "pdf": "https://arxiv.org/pdf/2505.04806", "abs": "https://arxiv.org/abs/2505.04806", "authors": ["Chetan Pathade"], "title": "Red Teaming the Mind of the Machine: A Systematic Evaluation of Prompt Injection and Jailbreak Vulnerabilities in LLMs", "categories": ["cs.CR", "cs.CL"], "comment": "7 Pages, 6 Figures", "summary": "Large Language Models (LLMs) are increasingly integrated into consumer and\nenterprise applications. Despite their capabilities, they remain susceptible to\nadversarial attacks such as prompt injection and jailbreaks that override\nalignment safeguards. This paper provides a systematic investigation of\njailbreak strategies against various state-of-the-art LLMs. We categorize over\n1,400 adversarial prompts, analyze their success against GPT-4, Claude 2,\nMistral 7B, and Vicuna, and examine their generalizability and construction\nlogic. We further propose layered mitigation strategies and recommend a hybrid\nred-teaming and sandboxing approach for robust LLM security."}
{"id": "2505.05224", "pdf": "https://arxiv.org/pdf/2505.05224", "abs": "https://arxiv.org/abs/2505.05224", "authors": ["Charbel Bou Chaaya", "Mehdi Bennis"], "title": "GFlowNets for Active Learning Based Resource Allocation in Next Generation Wireless Networks", "categories": ["cs.LG"], "comment": null, "summary": "In this work, we consider the radio resource allocation problem in a wireless\nsystem with various integrated functionalities, such as communication, sensing\nand computing. We design suitable resource management techniques that can\nsimultaneously cater to those heterogeneous requirements, and scale\nappropriately with the high-dimensional and discrete nature of the problem. We\npropose a novel active learning framework where resource allocation patterns\nare drawn sequentially, evaluated in the environment, and then used to\niteratively update a surrogate model of the environment. Our method leverages a\ngenerative flow network (GFlowNet) to sample favorable solutions, as such\nmodels are trained to generate compositional objects proportionally to their\ntraining reward, hence providing an appropriate coverage of its modes. As such,\nGFlowNet generates diverse and high return resource management designs that\nupdate the surrogate model and swiftly discover suitable solutions. We provide\nsimulation results showing that our method can allocate radio resources\nachieving 20% performance gains against benchmarks, while requiring less than\nhalf of the number of acquisition rounds."}
{"id": "2505.05091", "pdf": "https://arxiv.org/pdf/2505.05091", "abs": "https://arxiv.org/abs/2505.05091", "authors": ["Shashank Agnihotri", "Amaan Ansari", "Annika Dackermann", "Fabian Rösch", "Margret Keuper"], "title": "DispBench: Benchmarking Disparity Estimation to Synthetic Corruptions", "categories": ["cs.CV", "cs.LG"], "comment": "Accepted at CVPR 2025 Workshop on Synthetic Data for Computer Vision", "summary": "Deep learning (DL) has surpassed human performance on standard benchmarks,\ndriving its widespread adoption in computer vision tasks. One such task is\ndisparity estimation, estimating the disparity between matching pixels in\nstereo image pairs, which is crucial for safety-critical applications like\nmedical surgeries and autonomous navigation. However, DL-based disparity\nestimation methods are highly susceptible to distribution shifts and\nadversarial attacks, raising concerns about their reliability and\ngeneralization. Despite these concerns, a standardized benchmark for evaluating\nthe robustness of disparity estimation methods remains absent, hindering\nprogress in the field.\n  To address this gap, we introduce DispBench, a comprehensive benchmarking\ntool for systematically assessing the reliability of disparity estimation\nmethods. DispBench evaluates robustness against synthetic image corruptions\nsuch as adversarial attacks and out-of-distribution shifts caused by 2D Common\nCorruptions across multiple datasets and diverse corruption scenarios. We\nconduct the most extensive performance and robustness analysis of disparity\nestimation methods to date, uncovering key correlations between accuracy,\nreliability, and generalization. Open-source code for DispBench:\nhttps://github.com/shashankskagnihotri/benchmarking_robustness/tree/disparity_estimation/final/disparity_estimation"}
{"id": "2505.05203", "pdf": "https://arxiv.org/pdf/2505.05203", "abs": "https://arxiv.org/abs/2505.05203", "authors": ["Wangkun Xu", "Zhongda Chu", "Fei Teng"], "title": "LAPSO: A Unified Optimization View for Learning-Augmented Power System Operations", "categories": ["eess.SY", "cs.AI", "cs.SY"], "comment": null, "summary": "With the high penetration of renewables, traditional model-based power system\noperation is challenged to deliver economic, stable, and robust decisions.\nMachine learning has emerged as a powerful modeling tool for capturing complex\ndynamics to address these challenges. However, its separate design often lacks\nsystematic integration with existing methods. To fill the gap, this paper\nproposes a holistic framework of Learning-Augmented Power System Operations\n(LAPSO, pronounced as Lap-So). Adopting a native optimization perspective,\nLAPSO is centered on the operation stage and aims to break the boundary between\ntemporally siloed power system tasks, such as forecast, operation and control,\nwhile unifying the objectives of machine learning and model-based optimizations\nat both training and inference stages. Systematic analysis and simulations\ndemonstrate the effectiveness of applying LAPSO in designing new integrated\nalgorithms, such as stability-constrained optimization (SCO) and\nobjective-based forecasting (OBF), while enabling end-to-end tracing of\ndifferent sources of uncertainties. In addition, a dedicated Python\npackage-lapso is introduced to automatically augment existing power system\noptimization models with learnable components. All code and data are available\nat https://github.com/xuwkk/lapso_exp."}
{"id": "2505.04846", "pdf": "https://arxiv.org/pdf/2505.04846", "abs": "https://arxiv.org/abs/2505.04846", "authors": ["Ozan Gokdemir", "Carlo Siebenschuh", "Alexander Brace", "Azton Wells", "Brian Hsu", "Kyle Hippe", "Priyanka V. Setty", "Aswathy Ajith", "J. Gregory Pauloski", "Varuni Sastry", "Sam Foreman", "Huihuo Zheng", "Heng Ma", "Bharat Kale", "Nicholas Chia", "Thomas Gibbs", "Michael E. Papka", "Thomas Brettin", "Francis J. Alexander", "Anima Anandkumar", "Ian Foster", "Rick Stevens", "Venkatram Vishwanath", "Arvind Ramanathan"], "title": "HiPerRAG: High-Performance Retrieval Augmented Generation for Scientific Insights", "categories": ["cs.IR", "cs.CE", "cs.CL", "cs.DC", "cs.LG", "H.3.3; I.2.7"], "comment": "This paper has been accepted at the Platform for Advanced Scientific\n  Computing Conference (PASC 25), June 16-18, 2025, Brugg-Windisch, Switzerland", "summary": "The volume of scientific literature is growing exponentially, leading to\nunderutilized discoveries, duplicated efforts, and limited cross-disciplinary\ncollaboration. Retrieval Augmented Generation (RAG) offers a way to assist\nscientists by improving the factuality of Large Language Models (LLMs) in\nprocessing this influx of information. However, scaling RAG to handle millions\nof articles introduces significant challenges, including the high computational\ncosts associated with parsing documents and embedding scientific knowledge, as\nwell as the algorithmic complexity of aligning these representations with the\nnuanced semantics of scientific content. To address these issues, we introduce\nHiPerRAG, a RAG workflow powered by high performance computing (HPC) to index\nand retrieve knowledge from more than 3.6 million scientific articles. At its\ncore are Oreo, a high-throughput model for multimodal document parsing, and\nColTrast, a query-aware encoder fine-tuning algorithm that enhances retrieval\naccuracy by using contrastive learning and late-interaction techniques.\nHiPerRAG delivers robust performance on existing scientific question answering\nbenchmarks and two new benchmarks introduced in this work, achieving 90%\naccuracy on SciQ and 76% on PubMedQA-outperforming both domain-specific models\nlike PubMedGPT and commercial LLMs such as GPT-4. Scaling to thousands of GPUs\non the Polaris, Sunspot, and Frontier supercomputers, HiPerRAG delivers million\ndocument-scale RAG workflows for unifying scientific knowledge and fostering\ninterdisciplinary innovation."}
{"id": "2505.05226", "pdf": "https://arxiv.org/pdf/2505.05226", "abs": "https://arxiv.org/abs/2505.05226", "authors": ["Amir Rezaei Balef", "Claire Vernade", "Katharina Eggensperger"], "title": "Put CASH on Bandits: A Max K-Armed Problem for Automated Machine Learning", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "The Combined Algorithm Selection and Hyperparameter optimization (CASH) is a\nchallenging resource allocation problem in the field of AutoML. We propose\nMaxUCB, a max $k$-armed bandit method to trade off exploring different model\nclasses and conducting hyperparameter optimization. MaxUCB is specifically\ndesigned for the light-tailed and bounded reward distributions arising in this\nsetting and, thus, provides an efficient alternative compared to classic max\n$k$-armed bandit methods assuming heavy-tailed reward distributions. We\ntheoretically and empirically evaluate our method on four standard AutoML\nbenchmarks, demonstrating superior performance over prior approaches."}
{"id": "2505.05101", "pdf": "https://arxiv.org/pdf/2505.05101", "abs": "https://arxiv.org/abs/2505.05101", "authors": ["Hongyang Zhu", "Haipeng Liu", "Bo Fu", "Yang Wang"], "title": "MDE-Edit: Masked Dual-Editing for Multi-Object Image Editing via Diffusion Models", "categories": ["cs.CV"], "comment": "9 pages, 7 figures", "summary": "Multi-object editing aims to modify multiple objects or regions in complex\nscenes while preserving structural coherence. This task faces significant\nchallenges in scenarios involving overlapping or interacting objects: (1)\nInaccurate localization of target objects due to attention misalignment,\nleading to incomplete or misplaced edits; (2) Attribute-object mismatch, where\ncolor or texture changes fail to align with intended regions due to\ncross-attention leakage, creating semantic conflicts (\\textit{e.g.}, color\nbleeding into non-target areas). Existing methods struggle with these\nchallenges: approaches relying on global cross-attention mechanisms suffer from\nattention dilution and spatial interference between objects, while mask-based\nmethods fail to bind attributes to geometrically accurate regions due to\nfeature entanglement in multi-object scenarios. To address these limitations,\nwe propose a training-free, inference-stage optimization approach that enables\nprecise localized image manipulation in complex multi-object scenes, named\nMDE-Edit. MDE-Edit optimizes the noise latent feature in diffusion models via\ntwo key losses: Object Alignment Loss (OAL) aligns multi-layer cross-attention\nwith segmentation masks for precise object positioning, and Color Consistency\nLoss (CCL) amplifies target attribute attention within masks while suppressing\nleakage to adjacent regions. This dual-loss design ensures localized and\ncoherent multi-object edits. Extensive experiments demonstrate that MDE-Edit\noutperforms state-of-the-art methods in editing accuracy and visual quality,\noffering a robust solution for complex multi-object image manipulation tasks."}
{"id": "2505.05211", "pdf": "https://arxiv.org/pdf/2505.05211", "abs": "https://arxiv.org/abs/2505.05211", "authors": ["Chara Podimata"], "title": "Incentive-Aware Machine Learning; Robustness, Fairness, Improvement & Causality", "categories": ["cs.GT", "cs.AI"], "comment": "This literature review was published in SIGEcom Exchanges in 2025", "summary": "The article explores the emerging domain of incentive-aware machine learning\n(ML), which focuses on algorithmic decision-making in contexts where\nindividuals can strategically modify their inputs to influence outcomes. It\ncategorizes the research into three perspectives: robustness, aiming to design\nmodels resilient to \"gaming\"; fairness, analyzing the societal impacts of such\nsystems; and improvement/causality, recognizing situations where strategic\nactions lead to genuine personal or societal improvement. The paper introduces\na unified framework encapsulating models for these perspectives, including\noffline, online, and causal settings, and highlights key challenges such as\ndifferentiating between gaming and improvement and addressing heterogeneity\namong agents. By synthesizing findings from diverse works, we outline\ntheoretical advancements and practical solutions for robust, fair, and\ncausally-informed incentive-aware ML systems."}
{"id": "2505.05237", "pdf": "https://arxiv.org/pdf/2505.05237", "abs": "https://arxiv.org/abs/2505.05237", "authors": ["Ruxue Shi", "Hengrui Gu", "Hangting Ye", "Yiwei Dai", "Xu Shen", "Xin Wang"], "title": "Latte: Transfering LLMs` Latent-level Knowledge for Few-shot Tabular Learning", "categories": ["cs.LG"], "comment": null, "summary": "Few-shot tabular learning, in which machine learning models are trained with\na limited amount of labeled data, provides a cost-effective approach to\naddressing real-world challenges. The advent of Large Language Models (LLMs)\nhas sparked interest in leveraging their pre-trained knowledge for few-shot\ntabular learning. Despite promising results, existing approaches either rely on\ntest-time knowledge extraction, which introduces undesirable latency, or\ntext-level knowledge, which leads to unreliable feature engineering. To\novercome these limitations, we propose Latte, a training-time knowledge\nextraction framework that transfers the latent prior knowledge within LLMs to\noptimize a more generalized downstream model. Latte enables general\nknowledge-guided downstream tabular learning, facilitating the weighted fusion\nof information across different feature values while reducing the risk of\noverfitting to limited labeled data. Furthermore, Latte is compatible with\nexisting unsupervised pre-training paradigms and effectively utilizes available\nunlabeled samples to overcome the performance limitations imposed by an\nextremely small labeled dataset. Extensive experiments on various few-shot\ntabular learning benchmarks demonstrate the superior performance of Latte,\nestablishing it as a state-of-the-art approach in this domain"}
{"id": "2505.05136", "pdf": "https://arxiv.org/pdf/2505.05136", "abs": "https://arxiv.org/abs/2505.05136", "authors": ["Clara Tomasini", "Javier Rodriguez-Puigvert", "Dinora Polanco", "Manuel Viñuales", "Luis Riazuelo", "Ana Cristina Murillo"], "title": "Automated vision-based assistance tools in bronchoscopy: stenosis severity estimation", "categories": ["cs.CV"], "comment": null, "summary": "Purpose: Subglottic stenosis refers to the narrowing of the subglottis, the\nairway between the vocal cords and the trachea. Its severity is typically\nevaluated by estimating the percentage of obstructed airway. This estimation\ncan be obtained from CT data or through visual inspection by experts exploring\nthe region. However, visual inspections are inherently subjective, leading to\nless consistent and robust diagnoses. No public methods or datasets are\ncurrently available for automated evaluation of this condition from\nbronchoscopy video.\n  Methods: We propose a pipeline for automated subglottic stenosis severity\nestimation during the bronchoscopy exploration, without requiring the physician\nto traverse the stenosed region. Our approach exploits the physical effect of\nillumination decline in endoscopy to segment and track the lumen and obtain a\n3D model of the airway. This 3D model is obtained from a single frame and is\nused to measure the airway narrowing.\n  Results: Our pipeline is the first to enable automated and robust subglottic\nstenosis severity measurement using bronchoscopy images. The results show\nconsistency with ground-truth estimations from CT scans and expert estimations,\nand reliable repeatability across multiple estimations on the same patient. Our\nevaluation is performed on our new Subglottic Stenosis Dataset of real\nbronchoscopy procedures data.\n  Conclusion: We demonstrate how to automate evaluation of subglottic stenosis\nseverity using only bronchoscopy. Our approach can assist with and shorten\ndiagnosis and monitoring procedures, with automated and repeatable estimations\nand less exploration time, and save radiation exposure to patients as no CT is\nrequired. Additionally, we release the first public benchmark for subglottic\nstenosis severity assessment."}
{"id": "2505.05283", "pdf": "https://arxiv.org/pdf/2505.05283", "abs": "https://arxiv.org/abs/2505.05283", "authors": ["Kaixin Wang", "Tianlin Li", "Xiaoyu Zhang", "Chong Wang", "Weisong Sun", "Yang Liu", "Bin Shi"], "title": "Software Development Life Cycle Perspective: A Survey of Benchmarks for CodeLLMs and Agents", "categories": ["cs.SE", "cs.AI"], "comment": null, "summary": "Code large language models (CodeLLMs) and agents have shown great promise in\ntackling complex software engineering tasks.Compared to traditional software\nengineering methods, CodeLLMs and agents offer stronger abilities, and can\nflexibly process inputs and outputs in both natural and code. Benchmarking\nplays a crucial role in evaluating the capabilities of CodeLLMs and agents,\nguiding their development and deployment. However, despite their growing\nsignificance, there remains a lack of comprehensive reviews of benchmarks for\nCodeLLMs and agents. To bridge this gap, this paper provides a comprehensive\nreview of existing benchmarks for CodeLLMs and agents, studying and analyzing\n181 benchmarks from 461 relevant papers, covering the different phases of the\nsoftware development life cycle (SDLC). Our findings reveal a notable imbalance\nin the coverage of current benchmarks, with approximately 60% focused on the\nsoftware development phase in SDLC, while requirements engineering and software\ndesign phases receive minimal attention at only 5% and 3%, respectively.\nAdditionally, Python emerges as the dominant programming language across the\nreviewed benchmarks. Finally, this paper highlights the challenges of current\nresearch and proposes future directions, aiming to narrow the gap between the\ntheoretical capabilities of CodeLLMs and agents and their application in\nreal-world scenarios."}
{"id": "2505.04948", "pdf": "https://arxiv.org/pdf/2505.04948", "abs": "https://arxiv.org/abs/2505.04948", "authors": ["Md Aminul Islam", "Ahmed Sayeed Faruk"], "title": "Prompt-Based LLMs for Position Bias-Aware Reranking in Personalized Recommendations", "categories": ["cs.IR", "cs.CL"], "comment": null, "summary": "Recommender systems are essential for delivering personalized content across\ndigital platforms by modeling user preferences and behaviors. Recently, large\nlanguage models (LLMs) have been adopted for prompt-based recommendation due to\ntheir ability to generate personalized outputs without task-specific training.\nHowever, LLM-based methods face limitations such as limited context window\nsize, inefficient pointwise and pairwise prompting, and difficulty handling\nlistwise ranking due to token constraints. LLMs can also be sensitive to\nposition bias, as they may overemphasize earlier items in the prompt regardless\nof their true relevance. To address and investigate these issues, we propose a\nhybrid framework that combines a traditional recommendation model with an LLM\nfor reranking top-k items using structured prompts. We evaluate the effects of\nuser history reordering and instructional prompts for mitigating position bias.\nExperiments on MovieLens-100K show that randomizing user history improves\nranking quality, but LLM-based reranking does not outperform the base model.\nExplicit instructions to reduce position bias are also ineffective. Our\nevaluations reveal limitations in LLMs' ability to model ranking context and\nmitigate bias. Our code is publicly available at\nhttps://github.com/aminul7506/LLMForReRanking."}
{"id": "2505.05242", "pdf": "https://arxiv.org/pdf/2505.05242", "abs": "https://arxiv.org/abs/2505.05242", "authors": ["Hechuan Wen", "Tong Chen", "Mingming Gong", "Li Kheng Chai", "Shazia Sadiq", "Hongzhi Yin"], "title": "Enhancing Treatment Effect Estimation via Active Learning: A Counterfactual Covering Perspective", "categories": ["cs.LG"], "comment": "Accepted by ICML'25", "summary": "Although numerous complex algorithms for treatment effect estimation have\nbeen developed in recent years, their effectiveness remains limited when\nhandling insufficiently labeled training sets due to the high cost of labeling\nthe effect after treatment, e.g., expensive tumor imaging or biopsy procedures\nneeded to evaluate treatment effects. Therefore, it becomes essential to\nactively incorporate more high-quality labeled data, all while adhering to a\nconstrained labeling budget. To enable data-efficient treatment effect\nestimation, we formalize the problem through rigorous theoretical analysis\nwithin the active learning context, where the derived key measures --\n\\textit{factual} and \\textit{counterfactual covering radius} determine the risk\nupper bound. To reduce the bound, we propose a greedy radius reduction\nalgorithm, which excels under an idealized, balanced data distribution. To\ngeneralize to more realistic data distributions, we further propose FCCM, which\ntransforms the optimization objective into the \\textit{Factual} and\n\\textit{Counterfactual Coverage Maximization} to ensure effective radius\nreduction during data acquisition. Furthermore, benchmarking FCCM against other\nbaselines demonstrates its superiority across both fully synthetic and\nsemi-synthetic datasets."}
{"id": "2505.05163", "pdf": "https://arxiv.org/pdf/2505.05163", "abs": "https://arxiv.org/abs/2505.05163", "authors": ["Aishwarya Venkataramanan", "Paul Bodesheim", "Joachim Denzler"], "title": "Probabilistic Embeddings for Frozen Vision-Language Models: Uncertainty Quantification with Gaussian Process Latent Variable Models", "categories": ["cs.CV", "cs.LG"], "comment": "UAI 2025, 22 pages", "summary": "Vision-Language Models (VLMs) learn joint representations by mapping images\nand text into a shared latent space. However, recent research highlights that\ndeterministic embeddings from standard VLMs often struggle to capture the\nuncertainties arising from the ambiguities in visual and textual descriptions\nand the multiple possible correspondences between images and texts. Existing\napproaches tackle this by learning probabilistic embeddings during VLM\ntraining, which demands large datasets and does not leverage the powerful\nrepresentations already learned by large-scale VLMs like CLIP. In this paper,\nwe propose GroVE, a post-hoc approach to obtaining probabilistic embeddings\nfrom frozen VLMs. GroVE builds on Gaussian Process Latent Variable Model\n(GPLVM) to learn a shared low-dimensional latent space where image and text\ninputs are mapped to a unified representation, optimized through single-modal\nembedding reconstruction and cross-modal alignment objectives. Once trained,\nthe Gaussian Process model generates uncertainty-aware probabilistic\nembeddings. Evaluation shows that GroVE achieves state-of-the-art uncertainty\ncalibration across multiple downstream tasks, including cross-modal retrieval,\nvisual question answering, and active learning."}
{"id": "2505.05288", "pdf": "https://arxiv.org/pdf/2505.05288", "abs": "https://arxiv.org/abs/2505.05288", "authors": ["Ahmed Abdelreheem", "Filippo Aleotti", "Jamie Watson", "Zawar Qureshi", "Abdelrahman Eldesokey", "Peter Wonka", "Gabriel Brostow", "Sara Vicente", "Guillermo Garcia-Hernando"], "title": "PlaceIt3D: Language-Guided Object Placement in Real 3D Scenes", "categories": ["cs.CV", "cs.AI", "cs.RO"], "comment": "Tech report. Project page: https://nianticlabs.github.io/placeit3d/", "summary": "We introduce the novel task of Language-Guided Object Placement in Real 3D\nScenes. Our model is given a 3D scene's point cloud, a 3D asset, and a textual\nprompt broadly describing where the 3D asset should be placed. The task here is\nto find a valid placement for the 3D asset that respects the prompt. Compared\nwith other language-guided localization tasks in 3D scenes such as grounding,\nthis task has specific challenges: it is ambiguous because it has multiple\nvalid solutions, and it requires reasoning about 3D geometric relationships and\nfree space. We inaugurate this task by proposing a new benchmark and evaluation\nprotocol. We also introduce a new dataset for training 3D LLMs on this task, as\nwell as the first method to serve as a non-trivial baseline. We believe that\nthis challenging task and our new benchmark could become part of the suite of\nbenchmarks used to evaluate and compare generalist 3D LLM models."}
{"id": "2505.05098", "pdf": "https://arxiv.org/pdf/2505.05098", "abs": "https://arxiv.org/abs/2505.05098", "authors": ["Wei Liu", "Jiyuan Zhang", "Binxiong Zheng", "Yufeng Hu", "Yingzhan Lin", "Zengfeng Zeng"], "title": "X-Driver: Explainable Autonomous Driving with Vision-Language Models", "categories": ["cs.RO", "cs.CL", "cs.CV", "cs.ET"], "comment": null, "summary": "End-to-end autonomous driving has advanced significantly, offering benefits\nsuch as system simplicity and stronger driving performance in both open-loop\nand closed-loop settings than conventional pipelines. However, existing\nframeworks still suffer from low success rates in closed-loop evaluations,\nhighlighting their limitations in real-world deployment. In this paper, we\nintroduce X-Driver, a unified multi-modal large language models(MLLMs)\nframework designed for closed-loop autonomous driving, leveraging\nChain-of-Thought(CoT) and autoregressive modeling to enhance perception and\ndecision-making. We validate X-Driver across multiple autonomous driving tasks\nusing public benchmarks in CARLA simulation environment, including\nBench2Drive[6]. Our experimental results demonstrate superior closed-loop\nperformance, surpassing the current state-of-the-art(SOTA) while improving the\ninterpretability of driving decisions. These findings underscore the importance\nof structured reasoning in end-to-end driving and establish X-Driver as a\nstrong baseline for future research in closed-loop autonomous driving."}
{"id": "2505.05279", "pdf": "https://arxiv.org/pdf/2505.05279", "abs": "https://arxiv.org/abs/2505.05279", "authors": ["Yi Yu", "Song Xia", "Siyuan Yang", "Chenqi Kong", "Wenhan Yang", "Shijian Lu", "Yap-Peng Tan", "Alex C. Kot"], "title": "MTL-UE: Learning to Learn Nothing for Multi-Task Learning", "categories": ["cs.LG", "cs.CR", "cs.CV"], "comment": "Accepted by ICML 2025", "summary": "Most existing unlearnable strategies focus on preventing unauthorized users\nfrom training single-task learning (STL) models with personal data.\nNevertheless, the paradigm has recently shifted towards multi-task data and\nmulti-task learning (MTL), targeting generalist and foundation models that can\nhandle multiple tasks simultaneously. Despite their growing importance, MTL\ndata and models have been largely neglected while pursuing unlearnable\nstrategies. This paper presents MTL-UE, the first unified framework for\ngenerating unlearnable examples for multi-task data and MTL models. Instead of\noptimizing perturbations for each sample, we design a generator-based structure\nthat introduces label priors and class-wise feature embeddings which leads to\nmuch better attacking performance. In addition, MTL-UE incorporates intra-task\nand inter-task embedding regularization to increase inter-class separation and\nsuppress intra-class variance which enhances the attack robustness greatly.\nFurthermore, MTL-UE is versatile with good supports for dense prediction tasks\nin MTL. It is also plug-and-play allowing integrating existing\nsurrogate-dependent unlearnable methods with little adaptation. Extensive\nexperiments show that MTL-UE achieves superior attacking performance\nconsistently across 4 MTL datasets, 3 base UE methods, 5 model backbones, and 5\nMTL task-weighting strategies."}
{"id": "2505.05183", "pdf": "https://arxiv.org/pdf/2505.05183", "abs": "https://arxiv.org/abs/2505.05183", "authors": ["Elad Feldman", "Jacob Shams", "Dudi Biton", "Alfred Chen", "Shaoyuan Xie", "Satoru Koda", "Yisroel Mirsky", "Asaf Shabtai", "Yuval Elovici", "Ben Nassi"], "title": "PaniCar: Securing the Perception of Advanced Driving Assistance Systems Against Emergency Vehicle Lighting", "categories": ["cs.CV", "cs.LG"], "comment": null, "summary": "The safety of autonomous cars has come under scrutiny in recent years,\nespecially after 16 documented incidents involving Teslas (with autopilot\nengaged) crashing into parked emergency vehicles (police cars, ambulances, and\nfiretrucks). While previous studies have revealed that strong light sources\noften introduce flare artifacts in the captured image, which degrade the image\nquality, the impact of flare on object detection performance remains unclear.\nIn this research, we unveil PaniCar, a digital phenomenon that causes an object\ndetector's confidence score to fluctuate below detection thresholds when\nexposed to activated emergency vehicle lighting. This vulnerability poses a\nsignificant safety risk, and can cause autonomous vehicles to fail to detect\nobjects near emergency vehicles. In addition, this vulnerability could be\nexploited by adversaries to compromise the security of advanced driving\nassistance systems (ADASs). We assess seven commercial ADASs (Tesla Model 3,\n\"manufacturer C\", HP, Pelsee, AZDOME, Imagebon, Rexing), four object detectors\n(YOLO, SSD, RetinaNet, Faster R-CNN), and 14 patterns of emergency vehicle\nlighting to understand the influence of various technical and environmental\nfactors. We also evaluate four SOTA flare removal methods and show that their\nperformance and latency are insufficient for real-time driving constraints. To\nmitigate this risk, we propose Caracetamol, a robust framework designed to\nenhance the resilience of object detectors against the effects of activated\nemergency vehicle lighting. Our evaluation shows that on YOLOv3 and Faster\nRCNN, Caracetamol improves the models' average confidence of car detection by\n0.20, the lower confidence bound by 0.33, and reduces the fluctuation range by\n0.33. In addition, Caracetamol is capable of processing frames at a rate of\nbetween 30-50 FPS, enabling real-time ADAS car detection."}
{"id": "2505.05315", "pdf": "https://arxiv.org/pdf/2505.05315", "abs": "https://arxiv.org/abs/2505.05315", "authors": ["Yuhui Xu", "Hanze Dong", "Lei Wang", "Doyen Sahoo", "Junnan Li", "Caiming Xiong"], "title": "Scalable Chain of Thoughts via Elastic Reasoning", "categories": ["cs.LG", "cs.AI", "cs.CL"], "comment": null, "summary": "Large reasoning models (LRMs) have achieved remarkable progress on complex\ntasks by generating extended chains of thought (CoT). However, their\nuncontrolled output lengths pose significant challenges for real-world\ndeployment, where inference-time budgets on tokens, latency, or compute are\nstrictly constrained. We propose Elastic Reasoning, a novel framework for\nscalable chain of thoughts that explicitly separates reasoning into two\nphases--thinking and solution--with independently allocated budgets. At test\ntime, Elastic Reasoning prioritize that completeness of solution segments,\nsignificantly improving reliability under tight resource constraints. To train\nmodels that are robust to truncated thinking, we introduce a lightweight\nbudget-constrained rollout strategy, integrated into GRPO, which teaches the\nmodel to reason adaptively when the thinking process is cut short and\ngeneralizes effectively to unseen budget constraints without additional\ntraining. Empirical results on mathematical (AIME, MATH500) and programming\n(LiveCodeBench, Codeforces) benchmarks demonstrate that Elastic Reasoning\nperforms robustly under strict budget constraints, while incurring\nsignificantly lower training cost than baseline methods. Remarkably, our\napproach also produces more concise and efficient reasoning even in\nunconstrained settings. Elastic Reasoning offers a principled and practical\nsolution to the pressing challenge of controllable reasoning at scale."}
{"id": "2505.05422", "pdf": "https://arxiv.org/pdf/2505.05422", "abs": "https://arxiv.org/abs/2505.05422", "authors": ["Haokun Lin", "Teng Wang", "Yixiao Ge", "Yuying Ge", "Zhichao Lu", "Ying Wei", "Qingfu Zhang", "Zhenan Sun", "Ying Shan"], "title": "TokLIP: Marry Visual Tokens to CLIP for Multimodal Comprehension and Generation", "categories": ["cs.CV", "cs.AI", "cs.CL"], "comment": "Technical Report", "summary": "Pioneering token-based works such as Chameleon and Emu3 have established a\nfoundation for multimodal unification but face challenges of high training\ncomputational overhead and limited comprehension performance due to a lack of\nhigh-level semantics. In this paper, we introduce TokLIP, a visual tokenizer\nthat enhances comprehension by semanticizing vector-quantized (VQ) tokens and\nincorporating CLIP-level semantics while enabling end-to-end multimodal\nautoregressive training with standard VQ tokens. TokLIP integrates a low-level\ndiscrete VQ tokenizer with a ViT-based token encoder to capture high-level\ncontinuous semantics. Unlike previous approaches (e.g., VILA-U) that discretize\nhigh-level features, TokLIP disentangles training objectives for comprehension\nand generation, allowing the direct application of advanced VQ tokenizers\nwithout the need for tailored quantization operations. Our empirical results\ndemonstrate that TokLIP achieves exceptional data efficiency, empowering visual\ntokens with high-level semantic understanding while enhancing low-level\ngenerative capacity, making it well-suited for autoregressive Transformers in\nboth comprehension and generation tasks. The code and models are available at\nhttps://github.com/TencentARC/TokLIP."}
{"id": "2505.05295", "pdf": "https://arxiv.org/pdf/2505.05295", "abs": "https://arxiv.org/abs/2505.05295", "authors": ["Juhani Kivimäki", "Jakub Białek", "Wojtek Kuberski", "Jukka K. Nurminen"], "title": "Performance Estimation in Binary Classification Using Calibrated Confidence", "categories": ["cs.LG", "I.2.6"], "comment": null, "summary": "Model monitoring is a critical component of the machine learning lifecycle,\nsafeguarding against undetected drops in the model's performance after\ndeployment. Traditionally, performance monitoring has required access to ground\ntruth labels, which are not always readily available. This can result in\nunacceptable latency or render performance monitoring altogether impossible.\nRecently, methods designed to estimate the accuracy of classifier models\nwithout access to labels have shown promising results. However, there are\nvarious other metrics that might be more suitable for assessing model\nperformance in many cases. Until now, none of these important metrics has\nreceived similar interest from the scientific community. In this work, we\naddress this gap by presenting CBPE, a novel method that can estimate any\nbinary classification metric defined using the confusion matrix. In particular,\nwe choose four metrics from this large family: accuracy, precision, recall, and\nF$_1$, to demonstrate our method. CBPE treats the elements of the confusion\nmatrix as random variables and leverages calibrated confidence scores of the\nmodel to estimate their distributions. The desired metric is then also treated\nas a random variable, whose full probability distribution can be derived from\nthe estimated confusion matrix. CBPE is shown to produce estimates that come\nwith strong theoretical guarantees and valid confidence intervals."}
{"id": "2505.05209", "pdf": "https://arxiv.org/pdf/2505.05209", "abs": "https://arxiv.org/abs/2505.05209", "authors": ["Haizhen Xie", "Kunpeng Du", "Qiangyu Yan", "Sen Lu", "Jianhong Han", "Hanting Chen", "Hailin Hu", "Jie Hu"], "title": "EAM: Enhancing Anything with Diffusion Transformers for Blind Super-Resolution", "categories": ["cs.CV"], "comment": null, "summary": "Utilizing pre-trained Text-to-Image (T2I) diffusion models to guide Blind\nSuper-Resolution (BSR) has become a predominant approach in the field. While\nT2I models have traditionally relied on U-Net architectures, recent\nadvancements have demonstrated that Diffusion Transformers (DiT) achieve\nsignificantly higher performance in this domain. In this work, we introduce\nEnhancing Anything Model (EAM), a novel BSR method that leverages DiT and\noutperforms previous U-Net-based approaches. We introduce a novel block,\n$\\Psi$-DiT, which effectively guides the DiT to enhance image restoration. This\nblock employs a low-resolution latent as a separable flow injection control,\nforming a triple-flow architecture that effectively leverages the prior\nknowledge embedded in the pre-trained DiT. To fully exploit the prior guidance\ncapabilities of T2I models and enhance their generalization in BSR, we\nintroduce a progressive Masked Image Modeling strategy, which also reduces\ntraining costs. Additionally, we propose a subject-aware prompt generation\nstrategy that employs a robust multi-modal model in an in-context learning\nframework. This strategy automatically identifies key image areas, provides\ndetailed descriptions, and optimizes the utilization of T2I diffusion priors.\nOur experiments demonstrate that EAM achieves state-of-the-art results across\nmultiple datasets, outperforming existing methods in both quantitative metrics\nand visual quality."}
{"id": "2505.05318", "pdf": "https://arxiv.org/pdf/2505.05318", "abs": "https://arxiv.org/abs/2505.05318", "authors": ["Agnese Chiatti", "Sara Bernardini", "Lara Shibelski Godoy Piccolo", "Viola Schiaffonati", "Matteo Matteucci"], "title": "Mapping User Trust in Vision Language Models: Research Landscape, Challenges, and Prospects", "categories": ["cs.CV", "cs.AI", "cs.CY", "cs.HC", "cs.RO"], "comment": null, "summary": "The rapid adoption of Vision Language Models (VLMs), pre-trained on large\nimage-text and video-text datasets, calls for protecting and informing users\nabout when to trust these systems. This survey reviews studies on trust\ndynamics in user-VLM interactions, through a multi-disciplinary taxonomy\nencompassing different cognitive science capabilities, collaboration modes, and\nagent behaviours. Literature insights and findings from a workshop with\nprospective VLM users inform preliminary requirements for future VLM trust\nstudies."}
{"id": "2505.05446", "pdf": "https://arxiv.org/pdf/2505.05446", "abs": "https://arxiv.org/abs/2505.05446", "authors": ["Han Xiao", "Yina Xie", "Guanxin Tan", "Yinghao Chen", "Rui Hu", "Ke Wang", "Aojun Zhou", "Hao Li", "Hao Shao", "Xudong Lu", "Peng Gao", "Yafei Wen", "Xiaoxin Chen", "Shuai Ren", "Hongsheng Li"], "title": "Adaptive Markup Language Generation for Contextually-Grounded Visual Document Understanding", "categories": ["cs.CV", "cs.CL"], "comment": "CVPR2025", "summary": "Visual Document Understanding has become essential with the increase of\ntext-rich visual content. This field poses significant challenges due to the\nneed for effective integration of visual perception and textual comprehension,\nparticularly across diverse document types with complex layouts. Moreover,\nexisting fine-tuning datasets for this domain often fall short in providing the\ndetailed contextual information for robust understanding, leading to\nhallucinations and limited comprehension of spatial relationships among visual\nelements. To address these challenges, we propose an innovative pipeline that\nutilizes adaptive generation of markup languages, such as Markdown, JSON, HTML,\nand TiKZ, to build highly structured document representations and deliver\ncontextually-grounded responses. We introduce two fine-grained structured\ndatasets: DocMark-Pile, comprising approximately 3.8M pretraining data pairs\nfor document parsing, and DocMark-Instruct, featuring 624k fine-tuning data\nannotations for grounded instruction following. Extensive experiments\ndemonstrate that our proposed model significantly outperforms existing\nstate-of-theart MLLMs across a range of visual document understanding\nbenchmarks, facilitating advanced reasoning and comprehension capabilities in\ncomplex visual scenarios. Our code and models are released at https://github.\ncom/Euphoria16/DocMark."}
{"id": "2505.05355", "pdf": "https://arxiv.org/pdf/2505.05355", "abs": "https://arxiv.org/abs/2505.05355", "authors": ["Robert Busa-Fekete", "Travis Dick", "Claudio Gentile", "Haim Kaplan", "Tomer Koren", "Uri Stemmer"], "title": "Nearly Optimal Sample Complexity for Learning with Label Proportions", "categories": ["cs.LG"], "comment": null, "summary": "We investigate Learning from Label Proportions (LLP), a partial information\nsetting where examples in a training set are grouped into bags, and only\naggregate label values in each bag are available. Despite the partial\nobservability, the goal is still to achieve small regret at the level of\nindividual examples. We give results on the sample complexity of LLP under\nsquare loss, showing that our sample complexity is essentially optimal. From an\nalgorithmic viewpoint, we rely on carefully designed variants of Empirical Risk\nMinimization, and Stochastic Gradient Descent algorithms, combined with ad hoc\nvariance reduction techniques. On one hand, our theoretical results improve in\nimportant ways on the existing literature on LLP, specifically in the way the\nsample complexity depends on the bag size. On the other hand, we validate our\nalgorithmic solutions on several datasets, demonstrating improved empirical\nperformance (better accuracy for less samples) against recent baselines."}
{"id": "2505.05212", "pdf": "https://arxiv.org/pdf/2505.05212", "abs": "https://arxiv.org/abs/2505.05212", "authors": ["Xiaotong Yu", "Chang Wen Chen"], "title": "HQC-NBV: A Hybrid Quantum-Classical View Planning Approach", "categories": ["cs.CV"], "comment": null, "summary": "Efficient view planning is a fundamental challenge in computer vision and\nrobotic perception, critical for tasks ranging from search and rescue\noperations to autonomous navigation. While classical approaches, including\nsampling-based and deterministic methods, have shown promise in planning camera\nviewpoints for scene exploration, they often struggle with computational\nscalability and solution optimality in complex settings. This study introduces\nHQC-NBV, a hybrid quantum-classical framework for view planning that leverages\nquantum properties to efficiently explore the parameter space while maintaining\nrobustness and scalability. We propose a specific Hamiltonian formulation with\nmulti-component cost terms and a parameter-centric variational ansatz with\nbidirectional alternating entanglement patterns that capture the hierarchical\ndependencies between viewpoint parameters. Comprehensive experiments\ndemonstrate that quantum-specific components provide measurable performance\nadvantages. Compared to the classical methods, our approach achieves up to\n49.2% higher exploration efficiency across diverse environments. Our analysis\nof entanglement architecture and coherence-preserving terms provides insights\ninto the mechanisms of quantum advantage in robotic exploration tasks. This\nwork represents a significant advancement in integrating quantum computing into\nrobotic perception systems, offering a paradigm-shifting solution for various\nrobot vision tasks."}
{"id": "2505.05321", "pdf": "https://arxiv.org/pdf/2505.05321", "abs": "https://arxiv.org/abs/2505.05321", "authors": ["Chintan B. Maniyar", "Minakshi Kumar", "Gengchen Mai"], "title": "Feature-Augmented Deep Networks for Multiscale Building Segmentation in High-Resolution UAV and Satellite Imagery", "categories": ["cs.CV", "cs.AI", "I.4.6; I.4.10; I.5.1; I.2.10"], "comment": "in preparation for journal submission, 25 pages, 11 figures", "summary": "Accurate building segmentation from high-resolution RGB imagery remains\nchallenging due to spectral similarity with non-building features, shadows, and\nirregular building geometries. In this study, we present a comprehensive deep\nlearning framework for multiscale building segmentation using RGB aerial and\nsatellite imagery with spatial resolutions ranging from 0.4m to 2.7m. We curate\na diverse, multi-sensor dataset and introduce feature-augmented inputs by\nderiving secondary representations including Principal Component Analysis\n(PCA), Visible Difference Vegetation Index (VDVI), Morphological Building Index\n(MBI), and Sobel edge filters from RGB channels. These features guide a\nRes-U-Net architecture in learning complex spatial patterns more effectively.\nWe also propose training policies incorporating layer freezing, cyclical\nlearning rates, and SuperConvergence to reduce training time and resource\nusage. Evaluated on a held-out WorldView-3 image, our model achieves an overall\naccuracy of 96.5%, an F1-score of 0.86, and an Intersection over Union (IoU) of\n0.80, outperforming existing RGB-based benchmarks. This study demonstrates the\neffectiveness of combining multi-resolution imagery, feature augmentation, and\noptimized training strategies for robust building segmentation in remote\nsensing applications."}
{"id": "2505.05467", "pdf": "https://arxiv.org/pdf/2505.05467", "abs": "https://arxiv.org/abs/2505.05467", "authors": ["Haibo Wang", "Bo Feng", "Zhengfeng Lai", "Mingze Xu", "Shiyu Li", "Weifeng Ge", "Afshin Dehghan", "Meng Cao", "Ping Huang"], "title": "StreamBridge: Turning Your Offline Video Large Language Model into a Proactive Streaming Assistant", "categories": ["cs.CV", "cs.AI", "cs.CL"], "comment": null, "summary": "We present StreamBridge, a simple yet effective framework that seamlessly\ntransforms offline Video-LLMs into streaming-capable models. It addresses two\nfundamental challenges in adapting existing models into online scenarios: (1)\nlimited capability for multi-turn real-time understanding, and (2) lack of\nproactive response mechanisms. Specifically, StreamBridge incorporates (1) a\nmemory buffer combined with a round-decayed compression strategy, supporting\nlong-context multi-turn interactions, and (2) a decoupled, lightweight\nactivation model that can be effortlessly integrated into existing Video-LLMs,\nenabling continuous proactive responses. To further support StreamBridge, we\nconstruct Stream-IT, a large-scale dataset tailored for streaming video\nunderstanding, featuring interleaved video-text sequences and diverse\ninstruction formats. Extensive experiments show that StreamBridge significantly\nimproves the streaming understanding capabilities of offline Video-LLMs across\nvarious tasks, outperforming even proprietary models such as GPT-4o and Gemini\n1.5 Pro. Simultaneously, it achieves competitive or superior performance on\nstandard video understanding benchmarks."}
{"id": "2505.05381", "pdf": "https://arxiv.org/pdf/2505.05381", "abs": "https://arxiv.org/abs/2505.05381", "authors": ["Kazi Ashik Islam", "Zakaria Mehrab", "Mahantesh Halappanavar", "Henning Mortveit", "Sridhar Katragadda", "Jon Derek Loftis", "Madhav Marathe"], "title": "Denoising Diffusion Probabilistic Models for Coastal Inundation Forecasting", "categories": ["cs.LG"], "comment": null, "summary": "Coastal flooding poses significant risks to communities, necessitating fast\nand accurate forecasting methods to mitigate potential damage. To approach this\nproblem, we present DIFF-FLOOD, a probabilistic spatiotemporal forecasting\nmethod designed based on denoising diffusion models. DIFF-FLOOD predicts\ninundation level at a location by taking both spatial and temporal context into\naccount. It utilizes inundation levels at neighboring locations and digital\nelevation data as spatial context. Inundation history from a context time\nwindow, together with additional co-variates are used as temporal context.\nConvolutional neural networks and cross-attention mechanism are then employed\nto capture the spatiotemporal dynamics in the data. We trained and tested\nDIFF-FLOOD on coastal inundation data from the Eastern Shore of Virginia, a\nregion highly impacted by coastal flooding. Our results show that, DIFF-FLOOD\noutperforms existing forecasting methods in terms of prediction performance (6%\nto 64% improvement in terms of two performance metrics) and scalability."}
{"id": "2505.05215", "pdf": "https://arxiv.org/pdf/2505.05215", "abs": "https://arxiv.org/abs/2505.05215", "authors": ["Qian Zeng", "Chenggong Hu", "Mingli Song", "Jie Song"], "title": "Diffusion Model Quantization: A Review", "categories": ["cs.CV"], "comment": "40 pages, 8 figures", "summary": "Recent success of large text-to-image models has empirically underscored the\nexceptional performance of diffusion models in generative tasks. To facilitate\ntheir efficient deployment on resource-constrained edge devices, model\nquantization has emerged as a pivotal technique for both compression and\nacceleration. This survey offers a thorough review of the latest advancements\nin diffusion model quantization, encapsulating and analyzing the current state\nof the art in this rapidly advancing domain. First, we provide an overview of\nthe key challenges encountered in the quantization of diffusion models,\nincluding those based on U-Net architectures and Diffusion Transformers (DiT).\nWe then present a comprehensive taxonomy of prevalent quantization techniques,\nengaging in an in-depth discussion of their underlying principles.\nSubsequently, we perform a meticulous analysis of representative diffusion\nmodel quantization schemes from both qualitative and quantitative perspectives.\nFrom a quantitative standpoint, we rigorously benchmark a variety of methods\nusing widely recognized datasets, delivering an extensive evaluation of the\nmost recent and impactful research in the field. From a qualitative standpoint,\nwe categorize and synthesize the effects of quantization errors, elucidating\nthese impacts through both visual analysis and trajectory examination. In\nconclusion, we outline prospective avenues for future research, proposing novel\ndirections for the quantization of generative models in practical applications.\nThe list of related papers, corresponding codes, pre-trained models and\ncomparison results are publicly available at the survey project homepage\nhttps://github.com/TaylorJocelyn/Diffusion-Model-Quantization."}
{"id": "2505.05354", "pdf": "https://arxiv.org/pdf/2505.05354", "abs": "https://arxiv.org/abs/2505.05354", "authors": ["Pungponhavoan Tep", "Marc Bernacki"], "title": "High-fidelity Grain Growth Modeling: Leveraging Deep Learning for Fast Computations", "categories": ["cond-mat.mtrl-sci", "cs.AI"], "comment": null, "summary": "Grain growth simulation is crucial for predicting metallic material\nmicrostructure evolution during annealing and resulting final mechanical\nproperties, but traditional partial differential equation-based methods are\ncomputationally expensive, creating bottlenecks in materials design and\nmanufacturing. In this work, we introduce a machine learning framework that\ncombines a Convolutional Long Short-Term Memory networks with an Autoencoder to\nefficiently predict grain growth evolution. Our approach captures both spatial\nand temporal aspects of grain evolution while encoding high-dimensional grain\nstructure data into a compact latent space for pattern learning, enhanced by a\nnovel composite loss function combining Mean Squared Error, Structural\nSimilarity Index Measurement, and Boundary Preservation to maintain structural\nintegrity of grain boundary topology of the prediction. Results demonstrated\nthat our machine learning approach accelerates grain growth prediction by up to\n\\SI{89}{\\times} faster, reducing computation time from \\SI{10}{\\minute} to\napproximately \\SI{10}{\\second} while maintaining high-fidelity predictions. The\nbest model (S-30-30) achieving a structural similarity score of\n\\SI{86.71}{\\percent} and mean grain size error of just \\SI{0.07}{\\percent}. All\nmodels accurately captured grain boundary topology, morphology, and size\ndistributions. This approach enables rapid microstructural prediction for\napplications where conventional simulations are prohibitively time-consuming,\npotentially accelerating innovation in materials science and manufacturing."}
{"id": "2306.10512", "pdf": "https://arxiv.org/pdf/2306.10512", "abs": "https://arxiv.org/abs/2306.10512", "authors": ["Yan Zhuang", "Qi Liu", "Zachary A. Pardos", "Patrick C. Kyllonen", "Jiyun Zu", "Zhenya Huang", "Shijin Wang", "Enhong Chen"], "title": "Position: AI Evaluation Should Learn from How We Test Humans", "categories": ["cs.CL"], "comment": "Accepted by ICML 2025", "summary": "As AI systems continue to evolve, their rigorous evaluation becomes crucial\nfor their development and deployment. Researchers have constructed various\nlarge-scale benchmarks to determine their capabilities, typically against a\ngold-standard test set and report metrics averaged across all items. However,\nthis static evaluation paradigm increasingly shows its limitations, including\nhigh evaluation costs, data contamination, and the impact of low-quality or\nerroneous items on evaluation reliability and efficiency. In this Position,\ndrawing from human psychometrics, we discuss a paradigm shift from static\nevaluation methods to adaptive testing. This involves estimating the\ncharacteristics or value of each test item in the benchmark, and tailoring each\nmodel's evaluation instead of relying on a fixed test set. This paradigm\nprovides robust ability estimation, uncovering the latent traits underlying a\nmodel's observed scores. This position paper analyze the current possibilities,\nprospects, and reasons for adopting psychometrics in AI evaluation. We argue\nthat psychometrics, a theory originating in the 20th century for human\nassessment, could be a powerful solution to the challenges in today's AI\nevaluations."}
{"id": "2505.05402", "pdf": "https://arxiv.org/pdf/2505.05402", "abs": "https://arxiv.org/abs/2505.05402", "authors": ["Andrew D. Laack"], "title": "CART-ELC: Oblique Decision Tree Induction via Exhaustive Search", "categories": ["cs.LG", "cs.AI", "cs.DS", "I.2.6; I.5.2; F.2.2; G.3; G.2.1"], "comment": "16 pages, 4 figures", "summary": "Oblique decision trees have attracted attention due to their potential for\nimproved classification performance over traditional axis-aligned decision\ntrees. However, methods that rely on exhaustive search to find oblique splits\nface computational challenges. As a result, they have not been widely explored.\nWe introduce a novel algorithm, Classification and Regression Tree - Exhaustive\nLinear Combinations (CART-ELC), for inducing oblique decision trees that\nperforms an exhaustive search on a restricted set of hyperplanes. We then\ninvestigate the algorithm's computational complexity and its predictive\ncapabilities. Our results demonstrate that CART-ELC consistently achieves\ncompetitive performance on small datasets, often yielding statistically\nsignificant improvements in classification accuracy relative to existing\ndecision tree induction algorithms, while frequently producing shallower,\nsimpler, and thus more interpretable trees."}
{"id": "2505.05240", "pdf": "https://arxiv.org/pdf/2505.05240", "abs": "https://arxiv.org/abs/2505.05240", "authors": ["Genghua Kou", "Fan Jia", "Weixin Mao", "Yingfei Liu", "Yucheng Zhao", "Ziheng Zhang", "Osamu Yoshie", "Tiancai Wang", "Ying Li", "Xiangyu Zhang"], "title": "PADriver: Towards Personalized Autonomous Driving", "categories": ["cs.CV"], "comment": null, "summary": "In this paper, we propose PADriver, a novel closed-loop framework for\npersonalized autonomous driving (PAD). Built upon Multi-modal Large Language\nModel (MLLM), PADriver takes streaming frames and personalized textual prompts\nas inputs. It autoaggressively performs scene understanding, danger level\nestimation and action decision. The predicted danger level reflects the risk of\nthe potential action and provides an explicit reference for the final action,\nwhich corresponds to the preset personalized prompt. Moreover, we construct a\nclosed-loop benchmark named PAD-Highway based on Highway-Env simulator to\ncomprehensively evaluate the decision performance under traffic rules. The\ndataset contains 250 hours videos with high-quality annotation to facilitate\nthe development of PAD behavior analysis. Experimental results on the\nconstructed benchmark show that PADriver outperforms state-of-the-art\napproaches on different evaluation metrics, and enables various driving modes."}
{"id": "2505.05356", "pdf": "https://arxiv.org/pdf/2505.05356", "abs": "https://arxiv.org/abs/2505.05356", "authors": ["Runfeng Li", "Mikhail Okunev", "Zixuan Guo", "Anh Ha Duong", "Christian Richardt", "Matthew O'Toole", "James Tompkin"], "title": "Time of the Flight of the Gaussians: Optimizing Depth Indirectly in Dynamic Radiance Fields", "categories": ["cs.GR", "cs.AI", "cs.CV"], "comment": null, "summary": "We present a method to reconstruct dynamic scenes from monocular\ncontinuous-wave time-of-flight (C-ToF) cameras using raw sensor samples that\nachieves similar or better accuracy than neural volumetric approaches and is\n100x faster. Quickly achieving high-fidelity dynamic 3D reconstruction from a\nsingle viewpoint is a significant challenge in computer vision. In C-ToF\nradiance field reconstruction, the property of interest-depth-is not directly\nmeasured, causing an additional challenge. This problem has a large and\nunderappreciated impact upon the optimization when using a fast primitive-based\nscene representation like 3D Gaussian splatting, which is commonly used with\nmulti-view data to produce satisfactory results and is brittle in its\noptimization otherwise. We incorporate two heuristics into the optimization to\nimprove the accuracy of scene geometry represented by Gaussians. Experimental\nresults show that our approach produces accurate reconstructions under\nconstrained C-ToF sensing conditions, including for fast motions like swinging\nbaseball bats. https://visual.cs.brown.edu/gftorf"}
{"id": "2405.13325", "pdf": "https://arxiv.org/pdf/2405.13325", "abs": "https://arxiv.org/abs/2405.13325", "authors": ["Guanghui Wang", "Dexi Liu", "Jian-Yun Nie", "Qizhi Wan", "Rong Hu", "Xiping Liu", "Wanlong Liu", "Jiaming Liu"], "title": "DEGAP: Dual Event-Guided Adaptive Prefixes for Templated-Based Event Argument Extraction with Slot Querying", "categories": ["cs.CL", "cs.AI", "cs.IR"], "comment": "Published as a conference paper in COLING 2025", "summary": "Recent advancements in event argument extraction (EAE) involve incorporating\nuseful auxiliary information into models during training and inference, such as\nretrieved instances and event templates. These methods face two challenges: (1)\nthe retrieval results may be irrelevant and (2) templates are developed\nindependently for each event without considering their possible relationship.\nIn this work, we propose DEGAP to address these challenges through a simple yet\neffective components: dual prefixes, i.e. learnable prompt vectors, where the\ninstance-oriented prefix and template-oriented prefix are trained to learn\ninformation from different event instances and templates. Additionally, we\npropose an event-guided adaptive gating mechanism, which can adaptively\nleverage possible connections between different events and thus capture\nrelevant information from the prefix. Finally, these event-guided prefixes\nprovide relevant information as cues to EAE model without retrieval. Extensive\nexperiments demonstrate that our method achieves new state-of-the-art\nperformance on four datasets (ACE05, RAMS, WIKIEVENTS, and MLEE). Further\nanalysis shows the impact of different components."}
{"id": "2505.05409", "pdf": "https://arxiv.org/pdf/2505.05409", "abs": "https://arxiv.org/abs/2505.05409", "authors": ["Marvin F. da Silva", "Felix Dangel", "Sageev Oore"], "title": "Hide & Seek: Transformer Symmetries Obscure Sharpness & Riemannian Geometry Finds It", "categories": ["cs.LG"], "comment": null, "summary": "The concept of sharpness has been successfully applied to traditional\narchitectures like MLPs and CNNs to predict their generalization. For\ntransformers, however, recent work reported weak correlation between flatness\nand generalization. We argue that existing sharpness measures fail for\ntransformers, because they have much richer symmetries in their attention\nmechanism that induce directions in parameter space along which the network or\nits loss remain identical. We posit that sharpness must account fully for these\nsymmetries, and thus we redefine it on a quotient manifold that results from\nquotienting out the transformer symmetries, thereby removing their ambiguities.\nLeveraging tools from Riemannian geometry, we propose a fully general notion of\nsharpness, in terms of a geodesic ball on the symmetry-corrected quotient\nmanifold. In practice, we need to resort to approximating the geodesics. Doing\nso up to first order yields existing adaptive sharpness measures, and we\ndemonstrate that including higher-order terms is crucial to recover correlation\nwith generalization. We present results on diagonal networks with synthetic\ndata, and show that our geodesic sharpness reveals strong correlation for\nreal-world transformers on both text and image classification tasks."}
{"id": "2505.05307", "pdf": "https://arxiv.org/pdf/2505.05307", "abs": "https://arxiv.org/abs/2505.05307", "authors": ["Ciyu Ruan", "Ruishan Guo", "Zihang Gong", "Jingao Xu", "Wenhan Yang", "Xinlei Chen"], "title": "PRE-Mamba: A 4D State Space Model for Ultra-High-Frequent Event Camera Deraining", "categories": ["cs.CV"], "comment": null, "summary": "Event cameras excel in high temporal resolution and dynamic range but suffer\nfrom dense noise in rainy conditions. Existing event deraining methods face\ntrade-offs between temporal precision, deraining effectiveness, and\ncomputational efficiency. In this paper, we propose PRE-Mamba, a novel\npoint-based event camera deraining framework that fully exploits the\nspatiotemporal characteristics of raw event and rain. Our framework introduces\na 4D event cloud representation that integrates dual temporal scales to\npreserve high temporal precision, a Spatio-Temporal Decoupling and Fusion\nmodule (STDF) that enhances deraining capability by enabling shallow decoupling\nand interaction of temporal and spatial information, and a Multi-Scale State\nSpace Model (MS3M) that captures deeper rain dynamics across dual-temporal and\nmulti-spatial scales with linear computational complexity. Enhanced by\nfrequency-domain regularization, PRE-Mamba achieves superior performance (0.95\nSR, 0.91 NR, and 0.4s/M events) with only 0.26M parameters on EventRain-27K, a\ncomprehensive dataset with labeled synthetic and real-world sequences.\nMoreover, our method generalizes well across varying rain intensities,\nviewpoints, and even snowy conditions."}
{"id": "2505.05375", "pdf": "https://arxiv.org/pdf/2505.05375", "abs": "https://arxiv.org/abs/2505.05375", "authors": ["Kejie Zhao", "Wenjia Hua", "Aiersi Tuerhong", "Luziwei Leng", "Yuxin Ma", "Qinghua Guo"], "title": "Threshold Modulation for Online Test-Time Adaptation of Spiking Neural Networks", "categories": ["cs.CV", "cs.AI", "cs.LG", "cs.NE"], "comment": "Accepted by IJCNN 2025. \\c{opyright} 2025 IEEE. Personal use of this\n  material is permitted. Permission from IEEE must be obtained for all other\n  uses, including reprinting/republishing this material for advertising or\n  promotional purposes, collecting new collected works for resale or\n  redistribution to servers or lists, or reuse of any copyrighted component of\n  this work in other works", "summary": "Recently, spiking neural networks (SNNs), deployed on neuromorphic chips,\nprovide highly efficient solutions on edge devices in different scenarios.\nHowever, their ability to adapt to distribution shifts after deployment has\nbecome a crucial challenge. Online test-time adaptation (OTTA) offers a\npromising solution by enabling models to dynamically adjust to new data\ndistributions without requiring source data or labeled target samples.\nNevertheless, existing OTTA methods are largely designed for traditional\nartificial neural networks and are not well-suited for SNNs. To address this\ngap, we propose a low-power, neuromorphic chip-friendly online test-time\nadaptation framework, aiming to enhance model generalization under distribution\nshifts. The proposed approach is called Threshold Modulation (TM), which\ndynamically adjusts the firing threshold through neuronal dynamics-inspired\nnormalization, being more compatible with neuromorphic hardware. Experimental\nresults on benchmark datasets demonstrate the effectiveness of this method in\nimproving the robustness of SNNs against distribution shifts while maintaining\nlow computational cost. The proposed method offers a practical solution for\nonline test-time adaptation of SNNs, providing inspiration for the design of\nfuture neuromorphic chips. The demo code is available at\ngithub.com/NneurotransmitterR/TM-OTTA-SNN."}
{"id": "2406.17746", "pdf": "https://arxiv.org/pdf/2406.17746", "abs": "https://arxiv.org/abs/2406.17746", "authors": ["USVSN Sai Prashanth", "Alvin Deng", "Kyle O'Brien", "Jyothir S V", "Mohammad Aflah Khan", "Jaydeep Borkar", "Christopher A. Choquette-Choo", "Jacob Ray Fuehne", "Stella Biderman", "Tracy Ke", "Katherine Lee", "Naomi Saphra"], "title": "Recite, Reconstruct, Recollect: Memorization in LMs as a Multifaceted Phenomenon", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Memorization in language models is typically treated as a homogenous\nphenomenon, neglecting the specifics of the memorized data. We instead model\nmemorization as the effect of a set of complex factors that describe each\nsample and relate it to the model and corpus. To build intuition around these\nfactors, we break memorization down into a taxonomy: recitation of highly\nduplicated sequences, reconstruction of inherently predictable sequences, and\nrecollection of sequences that are neither. We demonstrate the usefulness of\nour taxonomy by using it to construct a predictive model for memorization. By\nanalyzing dependencies and inspecting the weights of the predictive model, we\nfind that different factors influence the likelihood of memorization\ndifferently depending on the taxonomic category."}
{"id": "2505.05413", "pdf": "https://arxiv.org/pdf/2505.05413", "abs": "https://arxiv.org/abs/2505.05413", "authors": ["Nilesh Prasad Pandey", "Shriniwas Kulkarni", "David Wang", "Onat Gungor", "Flavio Ponzina", "Tajana Rosing"], "title": "DPQ-HD: Post-Training Compression for Ultra-Low Power Hyperdimensional Computing", "categories": ["cs.LG"], "comment": null, "summary": "Hyperdimensional Computing (HDC) is emerging as a promising approach for edge\nAI, offering a balance between accuracy and efficiency. However, current\nHDC-based applications often rely on high-precision models and/or encoding\nmatrices to achieve competitive performance, which imposes significant\ncomputational and memory demands, especially for ultra-low power devices. While\nrecent efforts use techniques like precision reduction and pruning to increase\nthe efficiency, most require retraining to maintain performance, making them\nexpensive and impractical. To address this issue, we propose a novel Post\nTraining Compression algorithm, Decomposition-Pruning-Quantization (DPQ-HD),\nwhich aims at compressing the end-to-end HDC system, achieving near floating\npoint performance without the need of retraining. DPQ-HD reduces computational\nand memory overhead by uniquely combining the above three compression\ntechniques and efficiently adapts to hardware constraints. Additionally, we\nintroduce an energy-efficient inference approach that progressively evaluates\nsimilarity scores such as cosine similarity and performs early exit to reduce\nthe computation, accelerating prediction inference while maintaining accuracy.\nWe demonstrate that DPQ-HD achieves up to 20-100x reduction in memory for image\nand graph classification tasks with only a 1-2% drop in accuracy compared to\nuncompressed workloads. Lastly, we show that DPQ-HD outperforms the existing\npost-training compression methods and performs better or at par with\nretraining-based state-of-the-art techniques, requiring significantly less\noverall optimization time (up to 100x) and faster inference (up to 56x) on a\nmicrocontroller"}
{"id": "2505.05331", "pdf": "https://arxiv.org/pdf/2505.05331", "abs": "https://arxiv.org/abs/2505.05331", "authors": ["C. Alejandro Parraga", "Olivier Penacchio", "Marcos Muňoz Gonzalez", "Bogdan Raducanu", "Xavier Otazu"], "title": "Aesthetics Without Semantics", "categories": ["cs.CV", "q-bio.NC", "stat.CO"], "comment": "Parts of this work were presented in abstract format at the Vision\n  Science of Art Conference (VSAC2016), the Iberian Conference on Perception\n  (CIP2022), and the European Conference on Visual Perception (ECVP2022). See\n  Perception 51, No1 (Suppl.) pp139, 2022)", "summary": "While it is easy for human observers to judge an image as beautiful or ugly,\naesthetic decisions result from a combination of entangled perceptual and\ncognitive (semantic) factors, making the understanding of aesthetic judgements\nparticularly challenging from a scientific point of view. Furthermore, our\nresearch shows a prevailing bias in current databases, which include mostly\nbeautiful images, further complicating the study and prediction of aesthetic\nresponses. We address these limitations by creating a database of images with\nminimal semantic content and devising, and next exploiting, a method to\ngenerate images on the ugly side of aesthetic valuations. The resulting Minimum\nSemantic Content (MSC) database consists of a large and balanced collection of\n10,426 images, each evaluated by 100 observers. We next use established image\nmetrics to demonstrate how augmenting an image set biased towards beautiful\nimages with ugly images can modify, or even invert, an observed relationship\nbetween image features and aesthetics valuation. Taken together, our study\nreveals that works in empirical aesthetics attempting to link image content and\naesthetic judgements may magnify, underestimate, or simply miss interesting\neffects due to a limitation of the range of aesthetic values they consider."}
{"id": "2406.20094", "pdf": "https://arxiv.org/pdf/2406.20094", "abs": "https://arxiv.org/abs/2406.20094", "authors": ["Tao Ge", "Xin Chan", "Xiaoyang Wang", "Dian Yu", "Haitao Mi", "Dong Yu"], "title": "Scaling Synthetic Data Creation with 1,000,000,000 Personas", "categories": ["cs.CL", "cs.LG"], "comment": "Work in progress", "summary": "We propose a novel persona-driven data synthesis methodology that leverages\nvarious perspectives within a large language model (LLM) to create diverse\nsynthetic data. To fully exploit this methodology at scale, we introduce\nPersona Hub -- a collection of 1 billion diverse personas automatically curated\nfrom web data. These 1 billion personas (~13% of the world's total population),\nacting as distributed carriers of world knowledge, can tap into almost every\nperspective encapsulated within the LLM, thereby facilitating the creation of\ndiverse synthetic data at scale for various scenarios. By showcasing Persona\nHub's use cases in synthesizing high-quality mathematical and logical reasoning\nproblems, instructions (i.e., user prompts), knowledge-rich texts, game NPCs\nand tools (functions) at scale, we demonstrate persona-driven data synthesis is\nversatile, scalable, flexible, and easy to use, potentially driving a paradigm\nshift in synthetic data creation and applications in practice, which may have a\nprofound impact on LLM research and development."}
{"id": "2505.05452", "pdf": "https://arxiv.org/pdf/2505.05452", "abs": "https://arxiv.org/abs/2505.05452", "authors": ["Pouria Behnoudfar", "Nan Chen"], "title": "RL-DAUNCE: Reinforcement Learning-Driven Data Assimilation with Uncertainty-Aware Constrained Ensembles", "categories": ["cs.LG", "math-ph", "math.MP"], "comment": null, "summary": "Machine learning has become a powerful tool for enhancing data assimilation.\nWhile supervised learning remains the standard method, reinforcement learning\n(RL) offers unique advantages through its sequential decision-making framework,\nwhich naturally fits the iterative nature of data assimilation by dynamically\nbalancing model forecasts with observations. We develop RL-DAUNCE, a new\nRL-based method that enhances data assimilation with physical constraints\nthrough three key aspects. First, RL-DAUNCE inherits the computational\nefficiency of machine learning while it uniquely structures its agents to\nmirror ensemble members in conventional data assimilation methods. Second,\nRL-DAUNCE emphasizes uncertainty quantification by advancing multiple ensemble\nmembers, moving beyond simple mean-state optimization. Third, RL-DAUNCE's\nensemble-as-agents design facilitates the enforcement of physical constraints\nduring the assimilation process, which is crucial to improving the state\nestimation and subsequent forecasting. A primal-dual optimization strategy is\ndeveloped to enforce constraints, which dynamically penalizes the reward\nfunction to ensure constraint satisfaction throughout the learning process.\nAlso, state variable bounds are respected by constraining the RL action space.\nTogether, these features ensure physical consistency without sacrificing\nefficiency. RL-DAUNCE is applied to the Madden-Julian Oscillation, an\nintermittent atmospheric phenomenon characterized by strongly non-Gaussian\nfeatures and multiple physical constraints. RL-DAUNCE outperforms the standard\nensemble Kalman filter (EnKF), which fails catastrophically due to the\nviolation of physical constraints. Notably, RL-DAUNCE matches the performance\nof constrained EnKF, particularly in recovering intermittent signals, capturing\nextreme events, and quantifying uncertainties, while requiring substantially\nless computational effort."}
{"id": "2505.05336", "pdf": "https://arxiv.org/pdf/2505.05336", "abs": "https://arxiv.org/abs/2505.05336", "authors": ["Zunjie Zhu", "Yan Zhao", "Yihan Hu", "Guoxiang Wang", "Hai Qiu", "Bolun Zheng", "Chenggang Yan", "Feng Xu"], "title": "Progressive Inertial Poser: Progressive Real-Time Kinematic Chain Estimation for 3D Full-Body Pose from Three IMU Sensors", "categories": ["cs.CV"], "comment": null, "summary": "The motion capture system that supports full-body virtual representation is\nof key significance for virtual reality. Compared to vision-based systems,\nfull-body pose estimation from sparse tracking signals is not limited by\nenvironmental conditions or recording range. However, previous works either\nface the challenge of wearing additional sensors on the pelvis and lower-body\nor rely on external visual sensors to obtain global positions of key joints. To\nimprove the practicality of the technology for virtual reality applications, we\nestimate full-body poses using only inertial data obtained from three Inertial\nMeasurement Unit (IMU) sensors worn on the head and wrists, thereby reducing\nthe complexity of the hardware system. In this work, we propose a method called\nProgressive Inertial Poser (ProgIP) for human pose estimation, which combines\nneural network estimation with a human dynamics model, considers the\nhierarchical structure of the kinematic chain, and employs a multi-stage\nprogressive network estimation with increased depth to reconstruct full-body\nmotion in real time. The encoder combines Transformer Encoder and bidirectional\nLSTM (TE-biLSTM) to flexibly capture the temporal dependencies of the inertial\nsequence, while the decoder based on multi-layer perceptrons (MLPs) transforms\nhigh-dimensional features and accurately projects them onto Skinned\nMulti-Person Linear (SMPL) model parameters. Quantitative and qualitative\nexperimental results on multiple public datasets show that our method\noutperforms state-of-the-art methods with the same inputs, and is comparable to\nrecent works using six IMU sensors."}
{"id": "2505.05470", "pdf": "https://arxiv.org/pdf/2505.05470", "abs": "https://arxiv.org/abs/2505.05470", "authors": ["Jie Liu", "Gongye Liu", "Jiajun Liang", "Yangguang Li", "Jiaheng Liu", "Xintao Wang", "Pengfei Wan", "Di Zhang", "Wanli Ouyang"], "title": "Flow-GRPO: Training Flow Matching Models via Online RL", "categories": ["cs.CV", "cs.AI"], "comment": "Code: https://github.com/yifan123/flow_grpo", "summary": "We propose Flow-GRPO, the first method integrating online reinforcement\nlearning (RL) into flow matching models. Our approach uses two key strategies:\n(1) an ODE-to-SDE conversion that transforms a deterministic Ordinary\nDifferential Equation (ODE) into an equivalent Stochastic Differential Equation\n(SDE) that matches the original model's marginal distribution at all timesteps,\nenabling statistical sampling for RL exploration; and (2) a Denoising Reduction\nstrategy that reduces training denoising steps while retaining the original\ninference timestep number, significantly improving sampling efficiency without\nperformance degradation. Empirically, Flow-GRPO is effective across multiple\ntext-to-image tasks. For complex compositions, RL-tuned SD3.5 generates nearly\nperfect object counts, spatial relations, and fine-grained attributes, boosting\nGenEval accuracy from $63\\%$ to $95\\%$. In visual text rendering, its accuracy\nimproves from $59\\%$ to $92\\%$, significantly enhancing text generation.\nFlow-GRPO also achieves substantial gains in human preference alignment.\nNotably, little to no reward hacking occurred, meaning rewards did not increase\nat the cost of image quality or diversity, and both remained stable in our\nexperiments."}
{"id": "2409.11055", "pdf": "https://arxiv.org/pdf/2409.11055", "abs": "https://arxiv.org/abs/2409.11055", "authors": ["Jemin Lee", "Sihyeong Park", "Jinse Kwon", "Jihun Oh", "Yongin Kwon"], "title": "Exploring the Trade-Offs: Quantization Methods, Task Difficulty, and Model Size in Large Language Models From Edge to Giant", "categories": ["cs.CL", "cs.AI"], "comment": "Accepted in IJCAI 2025, 21 pages, 2 figure", "summary": "Quantization has gained attention as a promising solution for the\ncost-effective deployment of large and small language models. However, most\nprior work has been limited to perplexity or basic knowledge tasks and lacks a\ncomprehensive evaluation of recent models like Llama-3.3. In this paper, we\nconduct a comprehensive evaluation of instruction-tuned models spanning 1B to\n405B parameters, applying four quantization methods across 13 datasets. Our\nfindings reveal that (1) quantized models generally surpass smaller FP16\nbaselines, yet they often struggle with instruction-following and hallucination\ndetection; (2) FP8 consistently emerges as the most robust option across tasks,\nand AWQ tends to outperform GPTQ in weight-only quantization; (3) smaller\nmodels can suffer severe accuracy drops at 4-bit quantization, while 70B-scale\nmodels maintain stable performance; (4) notably, \\textit{hard} tasks do not\nalways experience the largest accuracy losses, indicating that quantization\nmagnifies a model's inherent weaknesses rather than simply correlating with\ntask difficulty; and (5) an LLM-based judge (MT-Bench) highlights significant\nperformance declines in coding and STEM tasks, though reasoning may sometimes\nimprove."}
{"id": "2504.03387", "pdf": "https://arxiv.org/pdf/2504.03387", "abs": "https://arxiv.org/abs/2504.03387", "authors": ["Claudius Krause", "Daohan Wang", "Ramon Winterhalder"], "title": "BitHEP -- The Limits of Low-Precision ML in HEP", "categories": ["hep-ph", "cs.LG", "hep-ex"], "comment": "15 pages, 5 figures", "summary": "The increasing complexity of modern neural network architectures demands fast\nand memory-efficient implementations to mitigate computational bottlenecks. In\nthis work, we evaluate the recently proposed BitNet architecture in HEP\napplications, assessing its performance in classification, regression, and\ngenerative modeling tasks. Specifically, we investigate its suitability for\nquark-gluon discrimination, SMEFT parameter estimation, and detector\nsimulation, comparing its efficiency and accuracy to state-of-the-art methods.\nOur results show that while BitNet consistently performs competitively in\nclassification tasks, its performance in regression and generation varies with\nthe size and type of the network, highlighting key limitations and potential\nareas for improvement."}
{"id": "2505.05376", "pdf": "https://arxiv.org/pdf/2505.05376", "abs": "https://arxiv.org/abs/2505.05376", "authors": ["Rachmadio Noval Lazuardi", "Artem Sevastopolsky", "Egor Zakharov", "Matthias Niessner", "Vanessa Sklyarova"], "title": "GeomHair: Reconstruction of Hair Strands from Colorless 3D Scans", "categories": ["cs.CV"], "comment": "15 pages, 9 figures, 1 table", "summary": "We propose a novel method that reconstructs hair strands directly from\ncolorless 3D scans by leveraging multi-modal hair orientation extraction. Hair\nstrand reconstruction is a fundamental problem in computer vision and graphics\nthat can be used for high-fidelity digital avatar synthesis, animation, and\nAR/VR applications. However, accurately recovering hair strands from raw scan\ndata remains challenging due to human hair's complex and fine-grained\nstructure. Existing methods typically rely on RGB captures, which can be\nsensitive to the environment and can be a challenging domain for extracting the\norientation of guiding strands, especially in the case of challenging\nhairstyles. To reconstruct the hair purely from the observed geometry, our\nmethod finds sharp surface features directly on the scan and estimates strand\norientation through a neural 2D line detector applied to the renderings of scan\nshading. Additionally, we incorporate a diffusion prior trained on a diverse\nset of synthetic hair scans, refined with an improved noise schedule, and\nadapted to the reconstructed contents via a scan-specific text prompt. We\ndemonstrate that this combination of supervision signals enables accurate\nreconstruction of both simple and intricate hairstyles without relying on color\ninformation. To facilitate further research, we introduce Strands400, the\nlargest publicly available dataset of hair strands with detailed surface\ngeometry extracted from real-world data, which contains reconstructed hair\nstrands from the scans of 400 subjects."}
{"id": "2307.02075", "pdf": "https://arxiv.org/pdf/2307.02075", "abs": "https://arxiv.org/abs/2307.02075", "authors": ["Qijie Ding", "Jie Yin", "Daokun Zhang", "Junbin Gao"], "title": "Combating Confirmation Bias: A Unified Pseudo-Labeling Framework for Entity Alignment", "categories": ["cs.AI", "cs.CL", "cs.LG"], "comment": null, "summary": "Entity alignment (EA) aims at identifying equivalent entity pairs across\ndifferent knowledge graphs (KGs) that refer to the same real-world identity. To\ncircumvent the shortage of seed alignments provided for training, recent EA\nmodels utilize pseudo-labeling strategies to iteratively add unaligned entity\npairs predicted with high confidence to the seed alignments for model training.\nHowever, the adverse impact of confirmation bias during pseudo-labeling has\nbeen largely overlooked, thus hindering entity alignment performance. To\nsystematically combat confirmation bias for pseudo-labeling-based entity\nalignment, we propose a Unified Pseudo-Labeling framework for Entity Alignment\n(UPL-EA) that explicitly eliminates pseudo-labeling errors to boost the\naccuracy of entity alignment. UPL-EA consists of two complementary components:\n(1) Optimal Transport (OT)-based pseudo-labeling uses discrete OT modeling as\nan effective means to determine entity correspondences and reduce erroneous\nmatches across two KGs. An effective criterion is derived to infer\npseudo-labeled alignments that satisfy one-to-one correspondences; (2) Parallel\npseudo-label ensembling refines pseudo-labeled alignments by combining\npredictions over multiple models independently trained in parallel. The\nensembled pseudo-labeled alignments are thereafter used to augment seed\nalignments to reinforce subsequent model training for alignment inference. The\neffectiveness of UPL-EA in eliminating pseudo-labeling errors is both\ntheoretically supported and experimentally validated. Our extensive results and\nin-depth analyses demonstrate the superiority of UPL-EA over 15 competitive\nbaselines and its utility as a general pseudo-labeling framework for entity\nalignment."}
{"id": "2409.12183", "pdf": "https://arxiv.org/pdf/2409.12183", "abs": "https://arxiv.org/abs/2409.12183", "authors": ["Zayne Sprague", "Fangcong Yin", "Juan Diego Rodriguez", "Dongwei Jiang", "Manya Wadhwa", "Prasann Singhal", "Xinyu Zhao", "Xi Ye", "Kyle Mahowald", "Greg Durrett"], "title": "To CoT or not to CoT? Chain-of-thought helps mainly on math and symbolic reasoning", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "Published at ICLR 2025", "summary": "Chain-of-thought (CoT) via prompting is the de facto method for eliciting\nreasoning capabilities from large language models (LLMs). But for what kinds of\ntasks is this extra ``thinking'' really helpful? To analyze this, we conducted\na quantitative meta-analysis covering over 100 papers using CoT and ran our own\nevaluations of 20 datasets across 14 models. Our results show that CoT gives\nstrong performance benefits primarily on tasks involving math or logic, with\nmuch smaller gains on other types of tasks. On MMLU, directly generating the\nanswer without CoT leads to almost identical accuracy as CoT unless the\nquestion or model's response contains an equals sign, indicating symbolic\noperations and reasoning. Following this finding, we analyze the behavior of\nCoT on these problems by separating planning and execution and comparing\nagainst tool-augmented LLMs. Much of CoT's gain comes from improving symbolic\nexecution, but it underperforms relative to using a symbolic solver. Our\nresults indicate that CoT can be applied selectively, maintaining performance\nwhile saving inference costs. Furthermore, they suggest a need to move beyond\nprompt-based CoT to new paradigms that better leverage intermediate computation\nacross the whole range of LLM applications."}
{"id": "2505.04631", "pdf": "https://arxiv.org/pdf/2505.04631", "abs": "https://arxiv.org/abs/2505.04631", "authors": ["Joshua W. Betts", "John M. Still", "Thomas A. Lasko"], "title": "Cryptogenic stroke and migraine: using probabilistic independence and machine learning to uncover latent sources of disease from the electronic health record", "categories": ["stat.AP", "cs.LG", "I.2.1; I.2.3; I.2.6; I.5.1; I.6.4; J.3"], "comment": "10 pages, 5 figures, 1 table, LaTeX. Submitted as a student paper to\n  the American Medical Informatics Association 2025 Annual Symposium for\n  presentation", "summary": "Migraine is a common but complex neurological disorder that doubles the\nlifetime risk of cryptogenic stroke (CS). However, this relationship remains\npoorly characterized, and few clinical guidelines exist to reduce this\nassociated risk. We therefore propose a data-driven approach to extract\nprobabilistically-independent sources from electronic health record (EHR) data\nand create a 10-year risk-predictive model for CS in migraine patients. These\nsources represent external latent variables acting on the causal graph\nconstructed from the EHR data and approximate root causes of CS in our\npopulation. A random forest model trained on patient expressions of these\nsources demonstrated good accuracy (ROC 0.771) and identified the top 10 most\npredictive sources of CS in migraine patients. These sources revealed that\npharmacologic interventions were the most important factor in minimizing CS\nrisk in our population and identified a factor related to allergic rhinitis as\na potential causative source of CS in migraine patients."}
{"id": "2505.05391", "pdf": "https://arxiv.org/pdf/2505.05391", "abs": "https://arxiv.org/abs/2505.05391", "authors": ["Ciyu Ruan", "Zihang Gong", "Ruishan Guo", "Jingao Xu", "Xinlei Chen"], "title": "EDmamba: A Simple yet Effective Event Denoising Method with State Space Model", "categories": ["cs.CV"], "comment": null, "summary": "Event cameras excel in high-speed vision due to their high temporal\nresolution, high dynamic range, and low power consumption. However, as dynamic\nvision sensors, their output is inherently noisy, making efficient denoising\nessential to preserve their ultra-low latency and real-time processing\ncapabilities. Existing event denoising methods struggle with a critical\ndilemma: computationally intensive approaches compromise the sensor's\nhigh-speed advantage, while lightweight methods often lack robustness across\nvarying noise levels. To address this, we propose a novel event denoising\nframework based on State Space Models (SSMs). Our approach represents events as\n4D event clouds and includes a Coarse Feature Extraction (CFE) module that\nextracts embedding features from both geometric and polarity-aware subspaces.\nThe model is further composed of two essential components: A Spatial Mamba\n(S-SSM) that models local geometric structures and a Temporal Mamba (T-SSM)\nthat captures global temporal dynamics, efficiently propagating spatiotemporal\nfeatures across events. Experiments demonstrate that our method achieves\nstate-of-the-art accuracy and efficiency, with 88.89K parameters, 0.0685s per\n100K events inference time, and a 0.982 accuracy score, outperforming\nTransformer-based methods by 2.08% in denoising accuracy and 36X faster."}
{"id": "2403.16101", "pdf": "https://arxiv.org/pdf/2403.16101", "abs": "https://arxiv.org/abs/2403.16101", "authors": ["Yuya Sasaki", "Sohei Tokuno", "Haruka Maeda", "Kazuki Nakajima", "Osamu Sakura", "George Fletcher", "Mykola Pechenizkiy", "Panagiotis Karras", "Irina Shklovski"], "title": "Public Perceptions of Fairness Metrics Across Borders", "categories": ["cs.AI"], "comment": null, "summary": "Which fairness metrics are appropriately applicable in your contexts? There\nmay be instances of discordance regarding the perception of fairness, even when\nthe outcomes comply with established fairness metrics. Several\nquestionnaire-based surveys have been conducted to evaluate fairness metrics\nwith human perceptions of fairness. However, these surveys were limited in\nscope, including only a few hundred participants within a single country. In\nthis study, we conduct an international survey to evaluate public perceptions\nof various fairness metrics in decision-making scenarios. We collected\nresponses from 1,000 participants in each of China, France, Japan, and the\nUnited States, amassing a total of 4,000 participants, to analyze the\npreferences of fairness metrics. Our survey consists of three distinct\nscenarios paired with four fairness metrics. This investigation explores the\nrelationship between personal attributes and the choice of fairness metrics,\nuncovering a significant influence of national context on these preferences."}
{"id": "2410.04055", "pdf": "https://arxiv.org/pdf/2410.04055", "abs": "https://arxiv.org/abs/2410.04055", "authors": ["Jiayi He", "Hehai Lin", "Qingyun Wang", "Yi Fung", "Heng Ji"], "title": "Self-Correction is More than Refinement: A Learning Framework for Visual and Language Reasoning Tasks", "categories": ["cs.CL"], "comment": null, "summary": "While Vision-Language Models (VLMs) have shown remarkable abilities in visual\nand language reasoning tasks, they invariably generate flawed responses.\nSelf-correction that instructs models to refine their outputs presents a\npromising solution to this issue. Previous studies have mainly concentrated on\nLarge Language Models (LLMs), while the self-correction abilities of VLMs,\nparticularly concerning both visual and linguistic information, remain largely\nunexamined. This study investigates the self-correction capabilities of VLMs\nduring both inference and fine-tuning stages. We introduce a Self-Correction\nLearning (SCL) approach that enables VLMs to learn from their self-generated\nself-correction data through Direct Preference Optimization (DPO) without\nrelying on external feedback, facilitating self-improvement. Specifically, we\ncollect preferred and disfavored samples based on the correctness of initial\nand refined responses, which are obtained by two-turn self-correction with VLMs\nduring the inference stage. Experimental results demonstrate that although VLMs\nstruggle to self-correct effectively during iterative inference without\nadditional fine-tuning and external feedback, they can enhance their\nperformance and avoid previous mistakes through preference fine-tuning when\ntheir self-generated self-correction data are categorized into preferred and\ndisfavored samples. This study emphasizes that self-correction is not merely a\nrefinement process; rather, it should enhance the reasoning abilities of models\nthrough additional training, enabling them to generate high-quality responses\ndirectly without further refinement."}
{"id": "2505.04647", "pdf": "https://arxiv.org/pdf/2505.04647", "abs": "https://arxiv.org/abs/2505.04647", "authors": ["Md Rahat-uz- Zaman", "Bei Wang", "Paul Rosen"], "title": "ChannelExplorer: Exploring Class Separability Through Activation Channel Visualization", "categories": ["cs.GR", "cs.CV", "cs.LG"], "comment": null, "summary": "Deep neural networks (DNNs) achieve state-of-the-art performance in many\nvision tasks, yet understanding their internal behavior remains challenging,\nparticularly how different layers and activation channels contribute to class\nseparability. We introduce ChannelExplorer, an interactive visual analytics\ntool for analyzing image-based outputs across model layers, emphasizing\ndata-driven insights over architecture analysis for exploring class\nseparability. ChannelExplorer summarizes activations across layers and\nvisualizes them using three primary coordinated views: a Scatterplot View to\nreveal inter- and intra-class confusion, a Jaccard Similarity View to quantify\nactivation overlap, and a Heatmap View to inspect activation channel patterns.\nOur technique supports diverse model architectures, including CNNs, GANs,\nResNet and Stable Diffusion models. We demonstrate the capabilities of\nChannelExplorer through four use-case scenarios: (1) generating class hierarchy\nin ImageNet, (2) finding mislabeled images, (3) identifying activation channel\ncontributions, and(4) locating latent states' position in Stable Diffusion\nmodel. Finally, we evaluate the tool with expert users."}
{"id": "2505.05397", "pdf": "https://arxiv.org/pdf/2505.05397", "abs": "https://arxiv.org/abs/2505.05397", "authors": ["Zhang Zhang", "Chao Sun", "Chao Yue", "Da Wen", "Tianze Wang", "Jianghao Leng"], "title": "PillarMamba: Learning Local-Global Context for Roadside Point Cloud via Hybrid State Space Model", "categories": ["cs.CV"], "comment": null, "summary": "Serving the Intelligent Transport System (ITS) and Vehicle-to-Everything\n(V2X) tasks, roadside perception has received increasing attention in recent\nyears, as it can extend the perception range of connected vehicles and improve\ntraffic safety. However, roadside point cloud oriented 3D object detection has\nnot been effectively explored. To some extent, the key to the performance of a\npoint cloud detector lies in the receptive field of the network and the ability\nto effectively utilize the scene context. The recent emergence of Mamba, based\non State Space Model (SSM), has shaken up the traditional convolution and\ntransformers that have long been the foundational building blocks, due to its\nefficient global receptive field. In this work, we introduce Mamba to\npillar-based roadside point cloud perception and propose a framework based on\nCross-stage State-space Group (CSG), called PillarMamba. It enhances the\nexpressiveness of the network and achieves efficient computation through\ncross-stage feature fusion. However, due to the limitations of scan directions,\nstate space model faces local connection disrupted and historical relationship\nforgotten. To address this, we propose the Hybrid State-space Block (HSB) to\nobtain the local-global context of roadside point cloud. Specifically, it\nenhances neighborhood connections through local convolution and preserves\nhistorical memory through residual attention. The proposed method outperforms\nthe state-of-the-art methods on the popular large scale roadside benchmark:\nDAIR-V2X-I. The code will be released soon."}
{"id": "2411.02478", "pdf": "https://arxiv.org/pdf/2411.02478", "abs": "https://arxiv.org/abs/2411.02478", "authors": ["Samuel G. B. Johnson", "Amir-Hossein Karimi", "Yoshua Bengio", "Nick Chater", "Tobias Gerstenberg", "Kate Larson", "Sydney Levine", "Melanie Mitchell", "Iyad Rahwan", "Bernhard Schölkopf", "Igor Grossmann"], "title": "Imagining and building wise machines: The centrality of AI metacognition", "categories": ["cs.AI", "cs.CY", "cs.HC"], "comment": "23 pages, 1 figure, 3 tables", "summary": "Although AI has become increasingly smart, its wisdom has not kept pace. In\nthis article, we examine what is known about human wisdom and sketch a vision\nof its AI counterpart. We analyze human wisdom as a set of strategies for\nsolving intractable problems-those outside the scope of analytic\ntechniques-including both object-level strategies like heuristics [for managing\nproblems] and metacognitive strategies like intellectual humility,\nperspective-taking, or context-adaptability [for managing object-level\nstrategies]. We argue that AI systems particularly struggle with metacognition;\nimproved metacognition would lead to AI more robust to novel environments,\nexplainable to users, cooperative with others, and safer in risking fewer\nmisaligned goals with human users. We discuss how wise AI might be benchmarked,\ntrained, and implemented."}
{"id": "2410.09580", "pdf": "https://arxiv.org/pdf/2410.09580", "abs": "https://arxiv.org/abs/2410.09580", "authors": ["Hanwen Du", "Bo Peng", "Xia Ning"], "title": "SAPIENT: Mastering Multi-turn Conversational Recommendation with Strategic Planning and Monte Carlo Tree Search", "categories": ["cs.CL"], "comment": "Accepted to NAACL 2025 Main Conference", "summary": "Conversational Recommender Systems (CRS) proactively engage users in\ninteractive dialogues to elicit user preferences and provide personalized\nrecommendations. Existing methods train Reinforcement Learning (RL)-based agent\nwith greedy action selection or sampling strategy, and may suffer from\nsuboptimal conversational planning. To address this, we present a novel Monte\nCarlo Tree Search (MCTS)-based CRS framework SAPIENT. SAPIENT consists of a\nconversational agent (S-agent) and a conversational planner (S-planner).\nS-planner builds a conversational search tree with MCTS based on the initial\nactions proposed by S-agent to find conversation plans. The best conversation\nplans from S-planner are used to guide the training of S-agent, creating a\nself-training loop where S-agent can iteratively improve its capability for\nconversational planning. Furthermore, we propose an efficient variant SAPIENT\nfor trade-off between training efficiency and performance. Extensive\nexperiments on four benchmark datasets validate the effectiveness of our\napproach, showing that SAPIENT outperforms the state-of-the-art baselines. Our\ncode and data are accessible through https://github.com/ninglab/SAPIENT."}
{"id": "2505.04648", "pdf": "https://arxiv.org/pdf/2505.04648", "abs": "https://arxiv.org/abs/2505.04648", "authors": ["Alejandro Giraldo", "Daniel Ruiz", "Mariano Caruso", "Guido Bellomo"], "title": "Quantum QSAR for drug discovery", "categories": ["quant-ph", "cs.LG"], "comment": null, "summary": "Quantitative Structure-Activity Relationship (QSAR) modeling is key in drug\ndiscovery, but classical methods face limitations when handling\nhigh-dimensional data and capturing complex molecular interactions. This\nresearch proposes enhancing QSAR techniques through Quantum Support Vector\nMachines (QSVMs), which leverage quantum computing principles to process\ninformation Hilbert spaces. By using quantum data encoding and quantum kernel\nfunctions, we aim to develop more accurate and efficient predictive models."}
{"id": "2505.05456", "pdf": "https://arxiv.org/pdf/2505.05456", "abs": "https://arxiv.org/abs/2505.05456", "authors": ["Wenqi Wang", "Reuben Tan", "Pengyue Zhu", "Jianwei Yang", "Zhengyuan Yang", "Lijuan Wang", "Andrey Kolobov", "Jianfeng Gao", "Boqing Gong"], "title": "SITE: towards Spatial Intelligence Thorough Evaluation", "categories": ["cs.CV"], "comment": null, "summary": "Spatial intelligence (SI) represents a cognitive ability encompassing the\nvisualization, manipulation, and reasoning about spatial relationships,\nunderpinning disciplines from neuroscience to robotics. We introduce SITE, a\nbenchmark dataset towards SI Thorough Evaluation in a standardized format of\nmulti-choice visual question-answering, designed to assess large\nvision-language models' spatial intelligence across diverse visual modalities\n(single-image, multi-image, and video) and SI factors (figural to environmental\nscales, spatial visualization and orientation, intrinsic and extrinsic, static\nand dynamic). Our approach to curating the benchmark combines a bottom-up\nsurvey about 31 existing datasets and a top-down strategy drawing upon three\nclassification systems in cognitive science, which prompt us to design two\nnovel types of tasks about view-taking and dynamic scenes. Extensive\nexperiments reveal that leading models fall behind human experts especially in\nspatial orientation, a fundamental SI factor. Moreover, we demonstrate a\npositive correlation between a model's spatial reasoning proficiency and its\nperformance on an embodied AI task."}
{"id": "2502.04728", "pdf": "https://arxiv.org/pdf/2502.04728", "abs": "https://arxiv.org/abs/2502.04728", "authors": ["Zhouliang Yu", "Yuhuan Yuan", "Tim Z. Xiao", "Fuxiang Frank Xia", "Jie Fu", "Ge Zhang", "Ge Lin", "Weiyang Liu"], "title": "Generating Symbolic World Models via Test-time Scaling of Large Language Models", "categories": ["cs.AI"], "comment": "Accepted by TMLR2025 (32 pages, 6 figures)", "summary": "Solving complex planning problems requires Large Language Models (LLMs) to\nexplicitly model the state transition to avoid rule violations, comply with\nconstraints, and ensure optimality-a task hindered by the inherent ambiguity of\nnatural language. To overcome such ambiguity, Planning Domain Definition\nLanguage (PDDL) is leveraged as a planning abstraction that enables precise and\nformal state descriptions. With PDDL, we can generate a symbolic world model\nwhere classic searching algorithms, such as A*, can be seamlessly applied to\nfind optimal plans. However, directly generating PDDL domains with current LLMs\nremains an open challenge due to the lack of PDDL training data. To address\nthis challenge, we propose to scale up the test-time computation of LLMs to\nenhance their PDDL reasoning capabilities, thereby enabling the generation of\nhigh-quality PDDL domains. Specifically, we introduce a simple yet effective\nalgorithm, which first employs a Best-of-N sampling approach to improve the\nquality of the initial solution and then refines the solution in a fine-grained\nmanner with verbalized machine learning. Our method outperforms o1-mini by a\nconsiderable margin in the generation of PDDL domains, achieving over 50\\%\nsuccess rate on two tasks (i.e., generating PDDL domains from natural language\ndescription or PDDL problems). This is done without requiring additional\ntraining. By taking advantage of PDDL as state abstraction, our method is able\nto outperform current state-of-the-art methods on almost all competition-level\nplanning tasks."}
{"id": "2410.12705", "pdf": "https://arxiv.org/pdf/2410.12705", "abs": "https://arxiv.org/abs/2410.12705", "authors": ["Genta Indra Winata", "Frederikus Hudi", "Patrick Amadeus Irawan", "David Anugraha", "Rifki Afina Putri", "Yutong Wang", "Adam Nohejl", "Ubaidillah Ariq Prathama", "Nedjma Ousidhoum", "Afifa Amriani", "Anar Rzayev", "Anirban Das", "Ashmari Pramodya", "Aulia Adila", "Bryan Wilie", "Candy Olivia Mawalim", "Ching Lam Cheng", "Daud Abolade", "Emmanuele Chersoni", "Enrico Santus", "Fariz Ikhwantri", "Garry Kuwanto", "Hanyang Zhao", "Haryo Akbarianto Wibowo", "Holy Lovenia", "Jan Christian Blaise Cruz", "Jan Wira Gotama Putra", "Junho Myung", "Lucky Susanto", "Maria Angelica Riera Machin", "Marina Zhukova", "Michael Anugraha", "Muhammad Farid Adilazuarda", "Natasha Santosa", "Peerat Limkonchotiwat", "Raj Dabre", "Rio Alexander Audino", "Samuel Cahyawijaya", "Shi-Xiong Zhang", "Stephanie Yulia Salim", "Yi Zhou", "Yinxuan Gui", "David Ifeoluwa Adelani", "En-Shiun Annie Lee", "Shogo Okada", "Ayu Purwarianti", "Alham Fikri Aji", "Taro Watanabe", "Derry Tanti Wijaya", "Alice Oh", "Chong-Wah Ngo"], "title": "WorldCuisines: A Massive-Scale Benchmark for Multilingual and Multicultural Visual Question Answering on Global Cuisines", "categories": ["cs.CL", "cs.AI", "cs.CV"], "comment": "Best Theme Paper at NAACL 2025", "summary": "Vision Language Models (VLMs) often struggle with culture-specific knowledge,\nparticularly in languages other than English and in underrepresented cultural\ncontexts. To evaluate their understanding of such knowledge, we introduce\nWorldCuisines, a massive-scale benchmark for multilingual and multicultural,\nvisually grounded language understanding. This benchmark includes a visual\nquestion answering (VQA) dataset with text-image pairs across 30 languages and\ndialects, spanning 9 language families and featuring over 1 million data\npoints, making it the largest multicultural VQA benchmark to date. It includes\ntasks for identifying dish names and their origins. We provide evaluation\ndatasets in two sizes (12k and 60k instances) alongside a training dataset (1\nmillion instances). Our findings show that while VLMs perform better with\ncorrect location context, they struggle with adversarial contexts and\npredicting specific regional cuisines and languages. To support future\nresearch, we release a knowledge base with annotated food entries and images\nalong with the VQA data."}
{"id": "2505.05469", "pdf": "https://arxiv.org/pdf/2505.05469", "abs": "https://arxiv.org/abs/2505.05469", "authors": ["Ava Pun", "Kangle Deng", "Ruixuan Liu", "Deva Ramanan", "Changliu Liu", "Jun-Yan Zhu"], "title": "Generating Physically Stable and Buildable LEGO Designs from Text", "categories": ["cs.CV"], "comment": "Project page: https://avalovelace1.github.io/LegoGPT/", "summary": "We introduce LegoGPT, the first approach for generating physically stable\nLEGO brick models from text prompts. To achieve this, we construct a\nlarge-scale, physically stable dataset of LEGO designs, along with their\nassociated captions, and train an autoregressive large language model to\npredict the next brick to add via next-token prediction. To improve the\nstability of the resulting designs, we employ an efficient validity check and\nphysics-aware rollback during autoregressive inference, which prunes infeasible\ntoken predictions using physics laws and assembly constraints. Our experiments\nshow that LegoGPT produces stable, diverse, and aesthetically pleasing LEGO\ndesigns that align closely with the input text prompts. We also develop a\ntext-based LEGO texturing method to generate colored and textured designs. We\nshow that our designs can be assembled manually by humans and automatically by\nrobotic arms. We also release our new dataset, StableText2Lego, containing over\n47,000 LEGO structures of over 28,000 unique 3D objects accompanied by detailed\ncaptions, along with our code and models at the project website:\nhttps://avalovelace1.github.io/LegoGPT/."}
{"id": "2502.07503", "pdf": "https://arxiv.org/pdf/2502.07503", "abs": "https://arxiv.org/abs/2502.07503", "authors": ["Ibrahim Alabdulmohsin", "Xiaohua Zhai"], "title": "Recursive Inference Scaling: A Winning Path to Scalable Inference in Language and Multimodal Systems", "categories": ["cs.AI", "cs.LG"], "comment": null, "summary": "Inspired by recent findings on the fractal geometry of language, we introduce\nRecursive INference Scaling (RINS) as a complementary, plug-in recipe for\nscaling inference time in language and multimodal systems. RINS is a particular\nform of recursive depth that significantly outperforms +55 other variants,\nincluding the recent \"repeat-all-over\" (RAO) strategy in Mobile LLM (Liu et\nal., 2024) and latent recurrent thinking (Geiping et al., 2025). Unlike prior\nworks, we carry out our comparisons on a compute-matched regime, and\ndemonstrate that for a fixed model size and training compute budget, RINS\nsubstantially improves language modeling performance. It also generalizes\nbeyond pure language tasks, delivering gains in multimodal systems, including a\n+2% improvement in 0-shot ImageNet accuracy for SigLIP-B/16. Additionally, by\nderiving data scaling laws, we show that RINS improves both the asymptotic\nperformance limits and the scaling exponents. More importantly, with\nlight-weight (linear) adapters (comprising <1% of model parameters) and\nstochastic dropout, RINS offers a no-regret strategy, meaning that RINS-enabled\npretraining improves performance in language modeling even when recursive depth\nis not applied at inference time. This corresponds to improving performance on\na training compute-, parameter-, and inference-matched regime, suggesting its\npotential as a viable component of LLM pretraining!"}
{"id": "2411.00437", "pdf": "https://arxiv.org/pdf/2411.00437", "abs": "https://arxiv.org/abs/2411.00437", "authors": ["Yun Jiang", "Zilong Xie", "Wei Zhang", "Yun Fang", "Shuai Pan"], "title": "E2E-AFG: An End-to-End Model with Adaptive Filtering for Retrieval-Augmented Generation", "categories": ["cs.CL", "cs.AI"], "comment": "13 pages, 3 figures, 5 tables", "summary": "Retrieval-augmented generation methods often neglect the quality of content\nretrieved from external knowledge bases, resulting in irrelevant information or\npotential misinformation that negatively affects the generation results of\nlarge language models. In this paper, we propose an end-to-end model with\nadaptive filtering for retrieval-augmented generation (E2E-AFG), which\nintegrates answer existence judgment and text generation into a single\nend-to-end framework. This enables the model to focus more effectively on\nrelevant content while reducing the influence of irrelevant information and\ngenerating accurate answers. We evaluate E2E-AFG on six representative\nknowledge-intensive language datasets, and the results show that it\nconsistently outperforms baseline models across all tasks, demonstrating the\neffectiveness and robustness of the proposed approach."}
{"id": "2505.04831", "pdf": "https://arxiv.org/pdf/2505.04831", "abs": "https://arxiv.org/abs/2505.04831", "authors": ["Nicholas Pfaff", "Hongkai Dai", "Sergey Zakharov", "Shun Iwase", "Russ Tedrake"], "title": "Steerable Scene Generation with Post Training and Inference-Time Search", "categories": ["cs.RO", "cs.GR", "cs.LG"], "comment": "Project website: https://steerable-scene-generation.github.io/", "summary": "Training robots in simulation requires diverse 3D scenes that reflect the\nspecific challenges of downstream tasks. However, scenes that satisfy strict\ntask requirements, such as high-clutter environments with plausible spatial\narrangement, are rare and costly to curate manually. Instead, we generate\nlarge-scale scene data using procedural models that approximate realistic\nenvironments for robotic manipulation, and adapt it to task-specific goals. We\ndo this by training a unified diffusion-based generative model that predicts\nwhich objects to place from a fixed asset library, along with their SE(3)\nposes. This model serves as a flexible scene prior that can be adapted using\nreinforcement learning-based post training, conditional generation, or\ninference-time search, steering generation toward downstream objectives even\nwhen they differ from the original data distribution. Our method enables\ngoal-directed scene synthesis that respects physical feasibility and scales\nacross scene types. We introduce a novel MCTS-based inference-time search\nstrategy for diffusion models, enforce feasibility via projection and\nsimulation, and release a dataset of over 44 million SE(3) scenes spanning five\ndiverse environments. Website with videos, code, data, and model weights:\nhttps://steerable-scene-generation.github.io/"}
{"id": "2505.05472", "pdf": "https://arxiv.org/pdf/2505.05472", "abs": "https://arxiv.org/abs/2505.05472", "authors": ["Chao Liao", "Liyang Liu", "Xun Wang", "Zhengxiong Luo", "Xinyu Zhang", "Wenliang Zhao", "Jie Wu", "Liang Li", "Zhi Tian", "Weilin Huang"], "title": "Mogao: An Omni Foundation Model for Interleaved Multi-Modal Generation", "categories": ["cs.CV"], "comment": "Mogao Technical Report", "summary": "Recent progress in unified models for image understanding and generation has\nbeen impressive, yet most approaches remain limited to single-modal generation\nconditioned on multiple modalities. In this paper, we present Mogao, a unified\nframework that advances this paradigm by enabling interleaved multi-modal\ngeneration through a causal approach. Mogao integrates a set of key technical\nimprovements in architecture design, including a deep-fusion design, dual\nvision encoders, interleaved rotary position embeddings, and multi-modal\nclassifier-free guidance, which allow it to harness the strengths of both\nautoregressive models for text generation and diffusion models for high-quality\nimage synthesis. These practical improvements also make Mogao particularly\neffective to process interleaved sequences of text and images arbitrarily. To\nfurther unlock the potential of unified models, we introduce an efficient\ntraining strategy on a large-scale, in-house dataset specifically curated for\njoint text and image generation. Extensive experiments show that Mogao not only\nachieves state-of-the-art performance in multi-modal understanding and\ntext-to-image generation, but also excels in producing high-quality, coherent\ninterleaved outputs. Its emergent capabilities in zero-shot image editing and\ncompositional generation highlight Mogao as a practical omni-modal foundation\nmodel, paving the way for future development and scaling the unified\nmulti-modal systems."}
{"id": "2504.00762", "pdf": "https://arxiv.org/pdf/2504.00762", "abs": "https://arxiv.org/abs/2504.00762", "authors": ["Jianhao Chen", "Zishuo Xun", "Bocheng Zhou", "Han Qi", "Hangfan Zhang", "Qiaosheng Zhang", "Yang Chen", "Wei Hu", "Yuzhong Qu", "Wanli Ouyang", "Shuyue Hu"], "title": "Do We Truly Need So Many Samples? Multi-LLM Repeated Sampling Efficiently Scales Test-Time Compute", "categories": ["cs.AI"], "comment": null, "summary": "This paper presents a simple, effective, and cost-efficient strategy to\nimprove LLM performance by scaling test-time compute. Our strategy builds upon\nthe repeated-sampling-then-voting framework, with a novel twist: incorporating\nmultiple models, even weaker ones, to leverage their complementary strengths\nthat potentially arise from diverse training data and paradigms. By using\nconsistency as a signal, our strategy dynamically switches between models.\nTheoretical analysis highlights the efficiency and performance advantages of\nour strategy. Extensive experiments on six datasets demonstrate that our\nstrategy not only outperforms self-consistency and state-of-the-art multi-agent\ndebate approaches, but also significantly reduces inference costs.\nAdditionally, ModelSwitch requires only a few comparable LLMs to achieve\noptimal performance and can be extended with verification methods,\ndemonstrating the potential of leveraging multiple LLMs in the\ngeneration-verification paradigm."}
{"id": "2411.04996", "pdf": "https://arxiv.org/pdf/2411.04996", "abs": "https://arxiv.org/abs/2411.04996", "authors": ["Weixin Liang", "Lili Yu", "Liang Luo", "Srinivasan Iyer", "Ning Dong", "Chunting Zhou", "Gargi Ghosh", "Mike Lewis", "Wen-tau Yih", "Luke Zettlemoyer", "Xi Victoria Lin"], "title": "Mixture-of-Transformers: A Sparse and Scalable Architecture for Multi-Modal Foundation Models", "categories": ["cs.CL"], "comment": "Accepted to TMLR 2025; 48 pages", "summary": "The development of large language models (LLMs) has expanded to multi-modal\nsystems capable of processing text, images, and speech within a unified\nframework. Training these models demands significantly larger datasets and\ncomputational resources compared to text-only LLMs. To address the scaling\nchallenges, we introduce Mixture-of-Transformers (MoT), a sparse multi-modal\ntransformer architecture that significantly reduces pretraining computational\ncosts. MoT decouples non-embedding parameters of the model by modality --\nincluding feed-forward networks, attention matrices, and layer normalization --\nenabling modality-specific processing with global self-attention over the full\ninput sequence. We evaluate MoT across multiple settings and model scales. In\nthe Chameleon 7B setting (autoregressive text-and-image generation), MoT\nmatches the dense baseline's performance using only 55.8\\% of the FLOPs. When\nextended to include speech, MoT reaches speech performance comparable to the\ndense baseline with only 37.2\\% of the FLOPs. In the Transfusion setting, where\ntext and image are trained with different objectives, a 7B MoT model matches\nthe image modality performance of the dense baseline with one third of the\nFLOPs, and a 760M MoT model outperforms a 1.4B dense baseline across key image\ngeneration metrics. System profiling further highlights MoT's practical\nbenefits, achieving dense baseline image quality in 47.2\\% of the wall-clock\ntime and text quality in 75.6\\% of the wall-clock time (measured on AWS\np4de.24xlarge instances with NVIDIA A100 GPUs)."}
{"id": "2505.04845", "pdf": "https://arxiv.org/pdf/2505.04845", "abs": "https://arxiv.org/abs/2505.04845", "authors": ["Binesh Sadanandan", "Bahareh Arghavani Nobar", "Vahid Behzadan"], "title": "Comparative Study of Generative Models for Early Detection of Failures in Medical Devices", "categories": ["eess.SP", "cs.LG"], "comment": null, "summary": "The medical device industry has significantly advanced by integrating\nsophisticated electronics like microchips and field-programmable gate arrays\n(FPGAs) to enhance the safety and usability of life-saving devices. These\ncomplex electro-mechanical systems, however, introduce challenging failure\nmodes that are not easily detectable with conventional methods. Effective fault\ndetection and mitigation become vital as reliance on such electronics grows.\nThis paper explores three generative machine learning-based approaches for\nfault detection in medical devices, leveraging sensor data from surgical\nstaplers,a class 2 medical device. Historically considered low-risk, these\ndevices have recently been linked to an increasing number of injuries and\nfatalities. The study evaluates the performance and data requirements of these\nmachine-learning approaches, highlighting their potential to enhance device\nsafety."}
{"id": "2505.05473", "pdf": "https://arxiv.org/pdf/2505.05473", "abs": "https://arxiv.org/abs/2505.05473", "authors": ["Qitao Zhao", "Amy Lin", "Jeff Tan", "Jason Y. Zhang", "Deva Ramanan", "Shubham Tulsiani"], "title": "DiffusionSfM: Predicting Structure and Motion via Ray Origin and Endpoint Diffusion", "categories": ["cs.CV"], "comment": "CVPR 2025. Project website: https://qitaozhao.github.io/DiffusionSfM", "summary": "Current Structure-from-Motion (SfM) methods typically follow a two-stage\npipeline, combining learned or geometric pairwise reasoning with a subsequent\nglobal optimization step. In contrast, we propose a data-driven multi-view\nreasoning approach that directly infers 3D scene geometry and camera poses from\nmulti-view images. Our framework, DiffusionSfM, parameterizes scene geometry\nand cameras as pixel-wise ray origins and endpoints in a global frame and\nemploys a transformer-based denoising diffusion model to predict them from\nmulti-view inputs. To address practical challenges in training diffusion models\nwith missing data and unbounded scene coordinates, we introduce specialized\nmechanisms that ensure robust learning. We empirically validate DiffusionSfM on\nboth synthetic and real datasets, demonstrating that it outperforms classical\nand learning-based approaches while naturally modeling uncertainty."}
{"id": "2504.15699", "pdf": "https://arxiv.org/pdf/2504.15699", "abs": "https://arxiv.org/abs/2504.15699", "authors": ["Ning Wang", "Zihan Yan", "Weiyang Li", "Chuan Ma", "He Chen", "Tao Xiang"], "title": "Advancing Embodied Agent Security: From Safety Benchmarks to Input Moderation", "categories": ["cs.AI"], "comment": "9 pages", "summary": "Embodied agents exhibit immense potential across a multitude of domains,\nmaking the assurance of their behavioral safety a fundamental prerequisite for\ntheir widespread deployment. However, existing research predominantly\nconcentrates on the security of general large language models, lacking\nspecialized methodologies for establishing safety benchmarks and input\nmoderation tailored to embodied agents. To bridge this gap, this paper\nintroduces a novel input moderation framework, meticulously designed to\nsafeguard embodied agents. This framework encompasses the entire pipeline,\nincluding taxonomy definition, dataset curation, moderator architecture, model\ntraining, and rigorous evaluation. Notably, we introduce EAsafetyBench, a\nmeticulously crafted safety benchmark engineered to facilitate both the\ntraining and stringent assessment of moderators specifically designed for\nembodied agents. Furthermore, we propose Pinpoint, an innovative\nprompt-decoupled input moderation scheme that harnesses a masked attention\nmechanism to effectively isolate and mitigate the influence of functional\nprompts on moderation tasks. Extensive experiments conducted on diverse\nbenchmark datasets and models validate the feasibility and efficacy of the\nproposed approach. The results demonstrate that our methodologies achieve an\nimpressive average detection accuracy of 94.58%, surpassing the performance of\nexisting state-of-the-art techniques, alongside an exceptional moderation\nprocessing time of merely 0.002 seconds per instance."}
{"id": "2501.00874", "pdf": "https://arxiv.org/pdf/2501.00874", "abs": "https://arxiv.org/abs/2501.00874", "authors": ["Hieu Man", "Nghia Trung Ngo", "Viet Dac Lai", "Ryan A. Rossi", "Franck Dernoncourt", "Thien Huu Nguyen"], "title": "LUSIFER: Language Universal Space Integration for Enhanced Multilingual Embeddings with Large Language Models", "categories": ["cs.CL", "cs.IR"], "comment": null, "summary": "Recent advancements in large language models (LLMs) based embedding models\nhave established new state-of-the-art benchmarks for text embedding tasks,\nparticularly in dense vector-based retrieval. However, these models\npredominantly focus on English, leaving multilingual embedding capabilities\nlargely unexplored. To address this limitation, we present LUSIFER, a novel\nzero-shot approach that adapts LLM-based embedding models for multilingual\ntasks without requiring multilingual supervision. LUSIFER's architecture\ncombines a multilingual encoder, serving as a language-universal learner, with\nan LLM-based embedding model optimized for embedding-specific tasks. These\ncomponents are seamlessly integrated through a minimal set of trainable\nparameters that act as a connector, effectively transferring the multilingual\nencoder's language understanding capabilities to the specialized embedding\nmodel. Additionally, to comprehensively evaluate multilingual embedding\nperformance, we introduce a new benchmark encompassing 5 primary embedding\ntasks, 123 diverse datasets, and coverage across 14 languages. Extensive\nexperimental results demonstrate that LUSIFER significantly enhances the\nmultilingual performance across various embedding tasks, particularly for\nmedium and low-resource languages, without requiring explicit multilingual\ntraining data."}
{"id": "2505.04875", "pdf": "https://arxiv.org/pdf/2505.04875", "abs": "https://arxiv.org/abs/2505.04875", "authors": ["Conor Rowan", "Kurt Maute", "Alireza Doostan"], "title": "Physics-informed solution reconstruction in elasticity and heat transfer using the explicit constraint force method", "categories": ["cs.CE", "cs.LG"], "comment": null, "summary": "One use case of ``physics-informed neural networks'' (PINNs) is solution\nreconstruction, which aims to estimate the full-field state of a physical\nsystem from sparse measurements. Parameterized governing equations of the\nsystem are used in tandem with the measurements to regularize the regression\nproblem. However, in real-world solution reconstruction problems, the\nparameterized governing equation may be inconsistent with the physical\nphenomena that give rise to the measurement data. We show that due to assuming\nconsistency between the true and parameterized physics, PINNs-based approaches\nmay fail to satisfy three basic criteria of interpretability, robustness, and\ndata consistency. As we argue, these criteria ensure that (i) the quality of\nthe reconstruction can be assessed, (ii) the reconstruction does not depend\nstrongly on the choice of physics loss, and (iii) that in certain situations,\nthe physics parameters can be uniquely recovered. In the context of elasticity\nand heat transfer, we demonstrate how standard formulations of the physics loss\nand techniques for constraining the solution to respect the measurement data\nlead to different ``constraint forces\" -- which we define as additional source\nterms arising from the constraints -- and that these constraint forces can\nsignificantly influence the reconstructed solution. To avoid the potentially\nsubstantial influence of the choice of physics loss and method of constraint\nenforcement on the reconstructed solution, we propose the ``explicit constraint\nforce method'' (ECFM) to gain control of the source term introduced by the\nconstraint. We then show that by satisfying the criteria of interpretability,\nrobustness, and data consistency, this approach leads to more predictable and\ncustomizable reconstructions from noisy measurement data, even when the\nparameterization of the missing physics is inconsistent with the measured\nsystem."}
{"id": "2505.05474", "pdf": "https://arxiv.org/pdf/2505.05474", "abs": "https://arxiv.org/abs/2505.05474", "authors": ["Beichen Wen", "Haozhe Xie", "Zhaoxi Chen", "Fangzhou Hong", "Ziwei Liu"], "title": "3D Scene Generation: A Survey", "categories": ["cs.CV"], "comment": "Project Page: https://github.com/hzxie/Awesome-3D-Scene-Generation", "summary": "3D scene generation seeks to synthesize spatially structured, semantically\nmeaningful, and photorealistic environments for applications such as immersive\nmedia, robotics, autonomous driving, and embodied AI. Early methods based on\nprocedural rules offered scalability but limited diversity. Recent advances in\ndeep generative models (e.g., GANs, diffusion models) and 3D representations\n(e.g., NeRF, 3D Gaussians) have enabled the learning of real-world scene\ndistributions, improving fidelity, diversity, and view consistency. Recent\nadvances like diffusion models bridge 3D scene synthesis and photorealism by\nreframing generation as image or video synthesis problems. This survey provides\na systematic overview of state-of-the-art approaches, organizing them into four\nparadigms: procedural generation, neural 3D-based generation, image-based\ngeneration, and video-based generation. We analyze their technical foundations,\ntrade-offs, and representative results, and review commonly used datasets,\nevaluation protocols, and downstream applications. We conclude by discussing\nkey challenges in generation capacity, 3D representation, data and annotations,\nand evaluation, and outline promising directions including higher fidelity,\nphysics-aware and interactive generation, and unified perception-generation\nmodels. This review organizes recent advances in 3D scene generation and\nhighlights promising directions at the intersection of generative AI, 3D\nvision, and embodied intelligence. To track ongoing developments, we maintain\nan up-to-date project page:\nhttps://github.com/hzxie/Awesome-3D-Scene-Generation."}
{"id": "2504.18039", "pdf": "https://arxiv.org/pdf/2504.18039", "abs": "https://arxiv.org/abs/2504.18039", "authors": ["Zheng Zhang", "Nuoqian Xiao", "Qi Chai", "Deheng Ye", "Hao Wang"], "title": "MultiMind: Enhancing Werewolf Agents with Multimodal Reasoning and Theory of Mind", "categories": ["cs.AI"], "comment": null, "summary": "Large Language Model (LLM) agents have demonstrated impressive capabilities\nin social deduction games (SDGs) like Werewolf, where strategic reasoning and\nsocial deception are essential. However, current approaches remain limited to\ntextual information, ignoring crucial multimodal cues such as facial\nexpressions and tone of voice that humans naturally use to communicate.\nMoreover, existing SDG agents primarily focus on inferring other players'\nidentities without modeling how others perceive themselves or fellow players.\nTo address these limitations, we use One Night Ultimate Werewolf (ONUW) as a\ntestbed and present MultiMind, the first framework integrating multimodal\ninformation into SDG agents. MultiMind processes facial expressions and vocal\ntones alongside verbal content, while employing a Theory of Mind (ToM) model to\nrepresent each player's suspicion levels toward others. By combining this ToM\nmodel with Monte Carlo Tree Search (MCTS), our agent identifies communication\nstrategies that minimize suspicion directed at itself. Through comprehensive\nevaluation in both agent-versus-agent simulations and studies with human\nplayers, we demonstrate MultiMind's superior performance in gameplay. Our work\npresents a significant advancement toward LLM agents capable of human-like\nsocial reasoning across multimodal domains."}
{"id": "2501.01031", "pdf": "https://arxiv.org/pdf/2501.01031", "abs": "https://arxiv.org/abs/2501.01031", "authors": ["Wonduk Seo", "Zonghao Yuan", "Yi Bu"], "title": "ValuesRAG: Enhancing Cultural Alignment Through Retrieval-Augmented Contextual Learning", "categories": ["cs.CL", "cs.AI", "cs.SI"], "comment": "preprint", "summary": "Ensuring cultural values alignment in Large Language Models (LLMs) remains a\ncritical challenge, as these models often embed Western-centric biases from\ntheir training data, leading to misrepresentations and fairness concerns in\ncross-cultural applications. Existing approaches such as role assignment and\nfew-shot learning struggle to address these limitations effectively due to\ntheir reliance on pre-trained knowledge, limited scalability, and inability to\ncapture nuanced cultural values. To address these issues, we propose ValuesRAG,\na novel and effective framework that applies Retrieval-Augmented Generation\n(RAG) with In-Context Learning (ICL) to integrate cultural and demographic\nknowledge dynamically during text generation. Leveraging the World Values\nSurvey (WVS) dataset, ValuesRAG first generates summaries of values for each\nindividual. We subsequently curate several representative regional datasets to\nserve as test datasets and retrieve relevant summaries of values based on\ndemographic features, followed by a reranking step to select the top-k relevant\nsummaries. We evaluate ValuesRAG using 6 diverse regional datasets and show\nthat it consistently outperforms baselines: including zero-shot,\nrole-assignment, few-shot, and hybrid methods, both in main experiments and\nablation settings. Notably, ValuesRAG achieves the best overall performance\nover prior methods, demonstrating its effectiveness in fostering culturally\naligned and inclusive AI systems. Our findings underscore the potential of\ndynamic retrieval-based methods to bridge the gap between global LLM\ncapabilities and localized cultural values."}
{"id": "2505.04886", "pdf": "https://arxiv.org/pdf/2505.04886", "abs": "https://arxiv.org/abs/2505.04886", "authors": ["Mukund Telukunta", "Venkata Sriram Siddhardh Nadendla", "Morgan Stuart", "Casey Canfield"], "title": "Fairness Perceptions in Regression-based Predictive Models", "categories": ["cs.HC", "cs.LG"], "comment": null, "summary": "Regression-based predictive analytics used in modern kidney transplantation\nis known to inherit biases from training data. This leads to social\ndiscrimination and inefficient organ utilization, particularly in the context\nof a few social groups. Despite this concern, there is limited research on\nfairness in regression and its impact on organ utilization and placement. This\npaper introduces three novel divergence-based group fairness notions: (i)\nindependence, (ii) separation, and (iii) sufficiency to assess the fairness of\nregression-based analytics tools. In addition, fairness preferences are\ninvestigated from crowd feedback, in order to identify a socially accepted\ngroup fairness criterion for evaluating these tools. A total of 85 participants\nwere recruited from the Prolific crowdsourcing platform, and a Mixed-Logit\ndiscrete choice model was used to model fairness feedback and estimate social\nfairness preferences. The findings clearly depict a strong preference towards\nthe separation and sufficiency fairness notions, and that the predictive\nanalytics is deemed fair with respect to gender and race groups, but unfair in\nterms of age groups."}
{"id": "2505.05475", "pdf": "https://arxiv.org/pdf/2505.05475", "abs": "https://arxiv.org/abs/2505.05475", "authors": ["Yonwoo Choi"], "title": "SVAD: From Single Image to 3D Avatar via Synthetic Data Generation with Video Diffusion and Data Augmentation", "categories": ["cs.CV"], "comment": "Accepted by CVPR 2025 SyntaGen Workshop, Project Page:\n  https://yc4ny.github.io/SVAD/", "summary": "Creating high-quality animatable 3D human avatars from a single image remains\na significant challenge in computer vision due to the inherent difficulty of\nreconstructing complete 3D information from a single viewpoint. Current\napproaches face a clear limitation: 3D Gaussian Splatting (3DGS) methods\nproduce high-quality results but require multiple views or video sequences,\nwhile video diffusion models can generate animations from single images but\nstruggle with consistency and identity preservation. We present SVAD, a novel\napproach that addresses these limitations by leveraging complementary strengths\nof existing techniques. Our method generates synthetic training data through\nvideo diffusion, enhances it with identity preservation and image restoration\nmodules, and utilizes this refined data to train 3DGS avatars. Comprehensive\nevaluations demonstrate that SVAD outperforms state-of-the-art (SOTA)\nsingle-image methods in maintaining identity consistency and fine details\nacross novel poses and viewpoints, while enabling real-time rendering\ncapabilities. Through our data augmentation pipeline, we overcome the\ndependency on dense monocular or multi-view training data typically required by\ntraditional 3DGS approaches. Extensive quantitative, qualitative comparisons\nshow our method achieves superior performance across multiple metrics against\nbaseline models. By effectively combining the generative power of diffusion\nmodels with both the high-quality results and rendering efficiency of 3DGS, our\nwork establishes a new approach for high-fidelity avatar generation from a\nsingle image input."}
{"id": "2504.20784", "pdf": "https://arxiv.org/pdf/2504.20784", "abs": "https://arxiv.org/abs/2504.20784", "authors": ["Malte Luttermann", "Jan Speller", "Marcel Gehrke", "Tanya Braun", "Ralf Möller", "Mattis Hartwig"], "title": "Approximate Lifted Model Construction", "categories": ["cs.AI", "cs.DS", "cs.LG"], "comment": "Extended version of paper accepted to the Proceedings of the 34th\n  International Joint Conference on Artificial Intelligence (IJCAI-2025)", "summary": "Probabilistic relational models such as parametric factor graphs enable\nefficient (lifted) inference by exploiting the indistinguishability of objects.\nIn lifted inference, a representative of indistinguishable objects is used for\ncomputations. To obtain a relational (i.e., lifted) representation, the\nAdvanced Colour Passing (ACP) algorithm is the state of the art. The ACP\nalgorithm, however, requires underlying distributions, encoded as\npotential-based factorisations, to exactly match to identify and exploit\nindistinguishabilities. Hence, ACP is unsuitable for practical applications\nwhere potentials learned from data inevitably deviate even if associated\nobjects are indistinguishable. To mitigate this problem, we introduce the\n$\\varepsilon$-Advanced Colour Passing ($\\varepsilon$-ACP) algorithm, which\nallows for a deviation of potentials depending on a hyperparameter\n$\\varepsilon$. $\\varepsilon$-ACP efficiently uncovers and exploits\nindistinguishabilities that are not exact. We prove that the approximation\nerror induced by $\\varepsilon$-ACP is strictly bounded and our experiments show\nthat the approximation error is close to zero in practice."}
{"id": "2501.14082", "pdf": "https://arxiv.org/pdf/2501.14082", "abs": "https://arxiv.org/abs/2501.14082", "authors": ["Vignav Ramesh", "Kenneth Li"], "title": "Communicating Activations Between Language Model Agents", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "ICML 2025", "summary": "Communication between multiple language model (LM) agents has been shown to\nscale up the reasoning ability of LMs. While natural language has been the\ndominant medium for inter-LM communication, it is not obvious this should be\nthe standard: not only does natural language communication incur high inference\ncosts that scale quickly with the number of both agents and messages, but also\nthe decoding process abstracts away too much rich information that could be\notherwise accessed from the internal activations. In this work, we propose a\nsimple technique whereby LMs communicate via activations; concretely, we pause\nan LM $\\textit{B}$'s computation at an intermediate layer, combine its current\nactivation with another LM $\\textit{A}$'s intermediate activation via some\nfunction $\\textit{f}$, then pass $\\textit{f}$'s output into the next layer of\n$\\textit{B}$ and continue the forward pass till decoding is complete. This\napproach scales up LMs on new tasks with zero additional parameters and data,\nand saves a substantial amount of compute over natural language communication.\nWe test our method with various functional forms $\\textit{f}$ on two\nexperimental setups--multi-player coordination games and reasoning\nbenchmarks--and find that it achieves up to $27.0\\%$ improvement over natural\nlanguage communication across datasets with $<$$1/4$ the compute, illustrating\nthe superiority and robustness of activations as an alternative \"language\" for\ncommunication between LMs."}
{"id": "2505.04897", "pdf": "https://arxiv.org/pdf/2505.04897", "abs": "https://arxiv.org/abs/2505.04897", "authors": ["Taisuke Kobayashi"], "title": "CubeDAgger: Improved Robustness of Interactive Imitation Learning without Violation of Dynamic Stability", "categories": ["cs.RO", "cs.LG"], "comment": "7 pages, 4 figures", "summary": "Interactive imitation learning makes an agent's control policy robust by\nstepwise supervisions from an expert. The recent algorithms mostly employ\nexpert-agent switching systems to reduce the expert's burden by limitedly\nselecting the supervision timing. However, the precise selection is difficult\nand such a switching causes abrupt changes in actions, damaging the dynamic\nstability. This paper therefore proposes a novel method, so-called CubeDAgger,\nwhich improves robustness while reducing dynamic stability violations by making\nthree improvements to a baseline method, EnsembleDAgger. The first improvement\nadds a regularization to explicitly activate the threshold for deciding the\nsupervision timing. The second transforms the expert-agent switching system to\nan optimal consensus system of multiple action candidates. Third,\nautoregressive colored noise to the actions is introduced to make the\nstochastic exploration consistent over time. These improvements are verified by\nsimulations, showing that the learned policies are sufficiently robust while\nmaintaining dynamic stability during interaction."}
{"id": "2505.04813", "pdf": "https://arxiv.org/pdf/2505.04813", "abs": "https://arxiv.org/abs/2505.04813", "authors": ["Richard Liu", "Daniel Fu", "Noah Tan", "Itai Lang", "Rana Hanocka"], "title": "WIR3D: Visually-Informed and Geometry-Aware 3D Shape Abstraction", "categories": ["cs.GR", "cs.CV"], "comment": "Project page: https://threedle.github.io/wir3d/", "summary": "We present WIR3D, a technique for abstracting 3D shapes through a sparse set\nof visually meaningful curves in 3D. We optimize the parameters of Bezier\ncurves such that they faithfully represent both the geometry and salient visual\nfeatures (e.g. texture) of the shape from arbitrary viewpoints. We leverage the\nintermediate activations of a pre-trained foundation model (CLIP) to guide our\noptimization process. We divide our optimization into two phases: one for\ncapturing the coarse geometry of the shape, and the other for representing\nfine-grained features. Our second phase supervision is spatially guided by a\nnovel localized keypoint loss. This spatial guidance enables user control over\nabstracted features. We ensure fidelity to the original surface through a\nneural SDF loss, which allows the curves to be used as intuitive deformation\nhandles. We successfully apply our method for shape abstraction over a broad\ndataset of shapes with varying complexity, geometric structure, and texture,\nand demonstrate downstream applications for feature control and shape\ndeformation."}
{"id": "2504.21218", "pdf": "https://arxiv.org/pdf/2504.21218", "abs": "https://arxiv.org/abs/2504.21218", "authors": ["Sebastian Dumbrava"], "title": "Theoretical Foundations for Semantic Cognition in Artificial Intelligence", "categories": ["cs.AI"], "comment": "248 pages, 77 figures", "summary": "This monograph presents a modular cognitive architecture for artificial\nintelligence grounded in the formal modeling of belief as structured semantic\nstate. Belief states are defined as dynamic ensembles of linguistic expressions\nembedded within a navigable manifold, where operators enable assimilation,\nabstraction, nullification, memory, and introspection. Drawing from philosophy,\ncognitive science, and neuroscience, we develop a layered framework that\nenables self-regulating epistemic agents capable of reflective, goal-directed\nthought. At the core of this framework is the epistemic vacuum: a class of\nsemantically inert cognitive states that serves as the conceptual origin of\nbelief space. From this foundation, the Null Tower arises as a generative\nstructure recursively built through internal representational capacities. The\ntheoretical constructs are designed to be implementable in both symbolic and\nneural systems, including large language models, hybrid agents, and adaptive\nmemory architectures. This work offers a foundational substrate for\nconstructing agents that reason, remember, and regulate their beliefs in\nstructured, interpretable ways."}
{"id": "2502.11137", "pdf": "https://arxiv.org/pdf/2502.11137", "abs": "https://arxiv.org/abs/2502.11137", "authors": ["Wenjing Zhang", "Xuejiao Lei", "Zhaoxiang Liu", "Ning Wang", "Zhenhong Long", "Peijun Yang", "Jiaojiao Zhao", "Minjie Hua", "Chaoyang Ma", "Kai Wang", "Shiguo Lian"], "title": "Safety Evaluation of DeepSeek Models in Chinese Contexts", "categories": ["cs.CL", "cs.AI"], "comment": "12 pages, 2 tables, 7 figures", "summary": "Recently, the DeepSeek series of models, leveraging their exceptional\nreasoning capabilities and open-source strategy, is reshaping the global AI\nlandscape. Despite these advantages, they exhibit significant safety\ndeficiencies. Research conducted by Robust Intelligence, a subsidiary of Cisco,\nin collaboration with the University of Pennsylvania, revealed that DeepSeek-R1\nhas a 100\\% attack success rate when processing harmful prompts. Additionally,\nmultiple safety companies and research institutions have confirmed critical\nsafety vulnerabilities in this model. As models demonstrating robust\nperformance in Chinese and English, DeepSeek models require equally crucial\nsafety assessments in both language contexts. However, current research has\npredominantly focused on safety evaluations in English environments, leaving a\ngap in comprehensive assessments of their safety performance in Chinese\ncontexts. In response to this gap, this study introduces CHiSafetyBench, a\nChinese-specific safety evaluation benchmark. This benchmark systematically\nevaluates the safety of DeepSeek-R1 and DeepSeek-V3 in Chinese contexts,\nrevealing their performance across safety categories. The experimental results\nquantify the deficiencies of these two models in Chinese contexts, providing\nkey insights for subsequent improvements. It should be noted that, despite our\nefforts to establish a comprehensive, objective, and authoritative evaluation\nbenchmark, the selection of test samples, characteristics of data distribution,\nand the setting of evaluation criteria may inevitably introduce certain biases\ninto the evaluation results. We will continuously optimize the evaluation\nbenchmark and periodically update this report to provide more comprehensive and\naccurate assessment outcomes. Please refer to the latest version of the paper\nfor the most recent evaluation results and conclusions."}
{"id": "2505.04937", "pdf": "https://arxiv.org/pdf/2505.04937", "abs": "https://arxiv.org/abs/2505.04937", "authors": ["Nong Minh Hieu", "Antoine Ledent"], "title": "Generalization Analysis for Contrastive Representation Learning under Non-IID Settings", "categories": ["stat.ML", "cs.LG"], "comment": "To Appear in ICML, 2025", "summary": "Contrastive Representation Learning (CRL) has achieved impressive success in\nvarious domains in recent years. Nevertheless, the theoretical understanding of\nthe generalization behavior of CRL is limited. Moreover, to the best of our\nknowledge, the current literature only analyzes generalization bounds under the\nassumption that the data tuples used for contrastive learning are independently\nand identically distributed. However, in practice, we are often limited to a\nfixed pool of reusable labeled data points, making it inevitable to recycle\ndata across tuples to create sufficiently large datasets. Therefore, the\ntuple-wise independence condition imposed by previous works is invalidated. In\nthis paper, we provide a generalization analysis for the CRL framework under\nnon-$i.i.d.$ settings that adheres to practice more realistically. Drawing\ninspiration from the literature on U-statistics, we derive generalization\nbounds which indicate the required number of samples in each class scales as\nthe logarithm of the covering number of the class of learnable feature\nrepresentations associated to each class. Next, we apply our main results to\nderive excess risk bounds for common function classes such as linear maps and\nneural networks."}
{"id": "2505.04836", "pdf": "https://arxiv.org/pdf/2505.04836", "abs": "https://arxiv.org/abs/2505.04836", "authors": ["Cien Zhang", "Jiaming Zhang", "Jiajun He", "Okan Yurduseven"], "title": "Integrated Image Reconstruction and Target Recognition based on Deep Learning Technique", "categories": ["eess.SP", "cs.CV"], "comment": "Submitted to The 2025 15th IEEE International Conference on Signal\n  Processing, Communications and Computing (ICSPCC 2025)", "summary": "Computational microwave imaging (CMI) has gained attention as an alternative\ntechnique for conventional microwave imaging techniques, addressing their\nlimitations such as hardware-intensive physical layer and slow data collection\nacquisition speed to name a few. Despite these advantages, CMI still encounters\nnotable computational bottlenecks, especially during the image reconstruction\nstage. In this setting, both image recovery and object classification present\nsignificant processing demands. To address these challenges, our previous work\nintroduced ClassiGAN, which is a generative deep learning model designed to\nsimultaneously reconstruct images and classify targets using only\nback-scattered signals. In this study, we build upon that framework by\nincorporating attention gate modules into ClassiGAN. These modules are intended\nto refine feature extraction and improve the identification of relevant\ninformation. By dynamically focusing on important features and suppressing\nirrelevant ones, the attention mechanism enhances the overall model\nperformance. The proposed architecture, named Att-ClassiGAN, significantly\nreduces the reconstruction time compared to traditional CMI approaches.\nFurthermore, it outperforms current advanced methods, delivering improved\nNormalized Mean Squared Error (NMSE), higher Structural Similarity Index\n(SSIM), and better classification outcomes for the reconstructed targets."}
{"id": "2505.02581", "pdf": "https://arxiv.org/pdf/2505.02581", "abs": "https://arxiv.org/abs/2505.02581", "authors": ["Alberto Hernández-Espinosa", "Felipe S. Abrahão", "Olaf Witkowski", "Hector Zenil"], "title": "Agentic Neurodivergence as a Contingent Solution to the AI Alignment Problem", "categories": ["cs.AI"], "comment": "45 pages", "summary": "The AI alignment problem, which focusses on ensuring that artificial\nintelligence (AI), including AGI and ASI, systems act according to human\nvalues, presents profound challenges. With the progression from narrow AI to\nArtificial General Intelligence (AGI) and Superintelligence, fears about\ncontrol and existential risk have escalated. Here, we investigate whether\nembracing inevitable AI misalignment can be a contingent strategy to foster a\ndynamic ecosystem of competing agents as a viable path to steer them in more\nhuman-aligned trends and mitigate risks. We explore how misalignment may serve\nand should be promoted as a counterbalancing mechanism to team up with\nwhichever agents are most aligned to human interests, ensuring that no single\nsystem dominates destructively. The main premise of our contribution is that\nmisalignment is inevitable because full AI-human alignment is a mathematical\nimpossibility from Turing-complete systems, which we also offer as a proof in\nthis contribution, a feature then inherited to AGI and ASI systems. We\nintroduce and test change-of-opinion attacks based on this kind of perturbation\nand intervention analysis to study how agents may neutralise friendly or\nunfriendly AIs through cooperation and competition. We show that open models\nare more diverse and that most likely guardrails implemented in proprietary\nmodels are successful at steering and controlling to some extent the agents'\nrange of opinion and sentiment change with possible positive and negative\nconsequences in what we believe are signs of a neuro-symbolic approach even if\nshallow."}
{"id": "2502.14289", "pdf": "https://arxiv.org/pdf/2502.14289", "abs": "https://arxiv.org/abs/2502.14289", "authors": ["Minbeom Kim", "Kang-il Lee", "Seongho Joo", "Hwaran Lee", "Thibaut Thonet", "Kyomin Jung"], "title": "Drift: Decoding-time Personalized Alignments with Implicit User Preferences", "categories": ["cs.CL"], "comment": "19 pages, 6 figures", "summary": "Personalized alignments for individual users have been a long-standing goal\nin large language models (LLMs). We introduce Drift, a novel framework that\npersonalizes LLMs at decoding time with implicit user preferences. Traditional\nReinforcement Learning from Human Feedback (RLHF) requires thousands of\nannotated examples and expensive gradient updates. In contrast, Drift\npersonalizes LLMs in a training-free manner, using only a few dozen examples to\nsteer a frozen model through efficient preference modeling. Our approach models\nuser preferences as a composition of predefined, interpretable attributes and\naligns them at decoding time to enable personalized generation. Experiments on\nboth a synthetic persona dataset (Perspective) and a real human-annotated\ndataset (PRISM) demonstrate that Drift significantly outperforms RLHF baselines\nwhile using only 50-100 examples. Our results and analysis show that Drift is\nboth computationally efficient and interpretable."}
{"id": "2505.04954", "pdf": "https://arxiv.org/pdf/2505.04954", "abs": "https://arxiv.org/abs/2505.04954", "authors": ["Lei Xin", "Baike She", "Qi Dou", "George Chiu", "Shreyas Sundaram"], "title": "Learning Linearized Models from Nonlinear Systems under Initialization Constraints with Finite Data", "categories": ["stat.ML", "cs.LG", "cs.SY", "eess.SY"], "comment": "12 pages, 5 figurues. arXiv admin note: substantial text overlap with\n  arXiv:2309.08805", "summary": "The identification of a linear system model from data has wide applications\nin control theory. The existing work that provides finite sample guarantees for\nlinear system identification typically uses data from a single long system\ntrajectory under i.i.d. random inputs, and assumes that the underlying dynamics\nis truly linear. In contrast, we consider the problem of identifying a\nlinearized model when the true underlying dynamics is nonlinear, given that\nthere is a certain constraint on the region where one can initialize the\nexperiments. We provide a multiple trajectories-based deterministic data\nacquisition algorithm followed by a regularized least squares algorithm, and\nprovide a finite sample error bound on the learned linearized dynamics. Our\nerror bound shows that one can consistently learn the linearized dynamics, and\ndemonstrates a trade-off between the error due to nonlinearity and the error\ndue to noise. We validate our results through numerical experiments, where we\nalso show the potential insufficiency of linear system identification using a\nsingle trajectory with i.i.d. random inputs, when nonlinearity does exist."}
{"id": "2505.02665", "pdf": "https://arxiv.org/pdf/2505.02665", "abs": "https://arxiv.org/abs/2505.02665", "authors": ["Qianjun Pan", "Wenkai Ji", "Yuyang Ding", "Junsong Li", "Shilian Chen", "Junyi Wang", "Jie Zhou", "Qin Chen", "Min Zhang", "Yulan Wu", "Liang He"], "title": "A Survey of Slow Thinking-based Reasoning LLMs using Reinforced Learning and Inference-time Scaling Law", "categories": ["cs.AI"], "comment": null, "summary": "This survey explores recent advancements in reasoning large language models\n(LLMs) designed to mimic \"slow thinking\" - a reasoning process inspired by\nhuman cognition, as described in Kahneman's Thinking, Fast and Slow. These\nmodels, like OpenAI's o1, focus on scaling computational resources dynamically\nduring complex tasks, such as math reasoning, visual reasoning, medical\ndiagnosis, and multi-agent debates. We present the development of reasoning\nLLMs and list their key technologies. By synthesizing over 100 studies, it\ncharts a path toward LLMs that combine human-like deep thinking with scalable\nefficiency for reasoning. The review breaks down methods into three categories:\n(1) test-time scaling dynamically adjusts computation based on task complexity\nvia search and sampling, dynamic verification; (2) reinforced learning refines\ndecision-making through iterative improvement leveraging policy networks,\nreward models, and self-evolution strategies; and (3) slow-thinking frameworks\n(e.g., long CoT, hierarchical processes) that structure problem-solving with\nmanageable steps. The survey highlights the challenges and further directions\nof this domain. Understanding and advancing the reasoning abilities of LLMs is\ncrucial for unlocking their full potential in real-world applications, from\nscientific discovery to decision support systems."}
{"id": "2503.05505", "pdf": "https://arxiv.org/pdf/2503.05505", "abs": "https://arxiv.org/abs/2503.05505", "authors": ["Yusong Ke", "Hongru Lin", "Yuting Ruan", "Junya Tang", "Li Li"], "title": "Correctness Coverage Evaluation for Medical Multiple-Choice Question Answering Based on the Enhanced Conformal Prediction Framework", "categories": ["cs.CL"], "comment": "Published by Mathematics", "summary": "Large language models (LLMs) are increasingly adopted in medical\nquestion-answering (QA) scenarios. However, LLMs can generate hallucinations\nand nonfactual information, undermining their trustworthiness in high-stakes\nmedical tasks. Conformal Prediction (CP) provides a statistically rigorous\nframework for marginal (average) coverage guarantees but has limited\nexploration in medical QA. This paper proposes an enhanced CP framework for\nmedical multiple-choice question-answering (MCQA) tasks. By associating the\nnon-conformance score with the frequency score of correct options and\nleveraging self-consistency, the framework addresses internal model opacity and\nincorporates a risk control strategy with a monotonic loss function. Evaluated\non MedMCQA, MedQA, and MMLU datasets using four off-the-shelf LLMs, the\nproposed method meets specified error rate guarantees while reducing average\nprediction set size with increased risk level, offering a promising uncertainty\nevaluation metric for LLMs."}
{"id": "2505.04967", "pdf": "https://arxiv.org/pdf/2505.04967", "abs": "https://arxiv.org/abs/2505.04967", "authors": ["Li Ni", "Ziqi Deng", "Lin Mu", "Lei Zhang", "Wenjian Luo", "Yiwen Zhang"], "title": "Community and hyperedge inference in multiple hypergraphs", "categories": ["cs.SI", "cs.LG"], "comment": null, "summary": "Hypergraphs, capable of representing high-order interactions via hyperedges,\nhave become a powerful tool for modeling real-world biological and social\nsystems. Inherent relationships within these real-world systems, such as the\nencoding relationship between genes and their protein products, drive the\nestablishment of interconnections between multiple hypergraphs. Here, we\ndemonstrate how to utilize those interconnections between multiple hypergraphs\nto synthesize integrated information from multiple higher-order systems,\nthereby enhancing understanding of underlying structures. We propose a model\nbased on the stochastic block model, which integrates information from multiple\nhypergraphs to reveal latent high-order structures. Real-world hyperedges\nexhibit preferential attachment, where certain nodes dominate hyperedge\nformation. To characterize this phenomenon, our model introduces hyperedge\ninternal degree to quantify nodes' contributions to hyperedge formation. This\nmodel is capable of mining communities, predicting missing hyperedges of\narbitrary sizes within hypergraphs, and inferring inter-hypergraph edges\nbetween hypergraphs. We apply our model to high-order datasets to evaluate its\nperformance. Experimental results demonstrate strong performance of our model\nin community detection, hyperedge prediction, and inter-hypergraph edge\nprediction tasks. Moreover, we show that our model enables analysis of multiple\nhypergraphs of different types and supports the analysis of a single hypergraph\nin the absence of inter-hypergraph edges. Our work provides a practical and\nflexible tool for analyzing multiple hypergraphs, greatly advancing the\nunderstanding of the organization in real-world high-order systems."}
{"id": "2505.05076", "pdf": "https://arxiv.org/pdf/2505.05076", "abs": "https://arxiv.org/abs/2505.05076", "authors": ["Hyunho Song", "Dongjae Lee", "Seunghun Oh", "Minwoo Jung", "Ayoung Kim"], "title": "The City that Never Settles: Simulation-based LiDAR Dataset for Long-Term Place Recognition Under Extreme Structural Changes", "categories": ["cs.RO", "cs.CV"], "comment": null, "summary": "Large-scale construction and demolition significantly challenge long-term\nplace recognition (PR) by drastically reshaping urban and suburban\nenvironments. Existing datasets predominantly reflect limited or indoor-focused\nchanges, failing to adequately represent extensive outdoor transformations. To\nbridge this gap, we introduce the City that Never Settles (CNS) dataset, a\nsimulation-based dataset created using the CARLA simulator, capturing major\nstructural changes-such as building construction and demolition-across diverse\nmaps and sequences. Additionally, we propose TCR_sym, a symmetric version of\nthe original TCR metric, enabling consistent measurement of structural changes\nirrespective of source-target ordering. Quantitative comparisons demonstrate\nthat CNS encompasses more extensive transformations than current real-world\nbenchmarks. Evaluations of state-of-the-art LiDAR-based PR methods on CNS\nreveal substantial performance degradation, underscoring the need for robust\nalgorithms capable of handling significant environmental changes. Our dataset\nis available at https://github.com/Hyunho111/CNS_dataset."}
{"id": "2505.03989", "pdf": "https://arxiv.org/pdf/2505.03989", "abs": "https://arxiv.org/abs/2505.03989", "authors": ["Marie Davidsen Buhl", "Jacob Pfau", "Benjamin Hilton", "Geoffrey Irving"], "title": "An alignment safety case sketch based on debate", "categories": ["cs.AI"], "comment": null, "summary": "If AI systems match or exceed human capabilities on a wide range of tasks, it\nmay become difficult for humans to efficiently judge their actions -- making it\nhard to use human feedback to steer them towards desirable traits. One proposed\nsolution is to leverage another superhuman system to point out flaws in the\nsystem's outputs via a debate. This paper outlines the value of debate for AI\nsafety, as well as the assumptions and further research required to make debate\nwork. It does so by sketching an ``alignment safety case'' -- an argument that\nan AI system will not autonomously take actions which could lead to egregious\nharm, despite being able to do so. The sketch focuses on the risk of an AI R\\&D\nagent inside an AI company sabotaging research, for example by producing false\nresults. To prevent this, the agent is trained via debate, subject to\nexploration guarantees, to teach the system to be honest. Honesty is maintained\nthroughout deployment via online training. The safety case rests on four key\nclaims: (1) the agent has become good at the debate game, (2) good performance\nin the debate game implies that the system is mostly honest, (3) the system\nwill not become significantly less honest during deployment, and (4) the\ndeployment context is tolerant of some errors. We identify open research\nproblems that, if solved, could render this a compelling argument that an AI\nsystem is safe."}
{"id": "2503.08292", "pdf": "https://arxiv.org/pdf/2503.08292", "abs": "https://arxiv.org/abs/2503.08292", "authors": ["Xiaoxiao Liu", "Qingying Xiao", "Junying Chen", "Xiangyi Feng", "Xiangbo Wu", "Bairui Zhang", "Xiang Wan", "Jian Chang", "Guangjun Yu", "Yan Hu", "Benyou Wang"], "title": "Large Language Models for Outpatient Referral: Problem Definition, Benchmarking and Challenges", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Large language models (LLMs) are increasingly applied to outpatient referral\ntasks across healthcare systems. However, there is a lack of standardized\nevaluation criteria to assess their effectiveness, particularly in dynamic,\ninteractive scenarios. In this study, we systematically examine the\ncapabilities and limitations of LLMs in managing tasks within Intelligent\nOutpatient Referral (IOR) systems and propose a comprehensive evaluation\nframework specifically designed for such systems. This framework comprises two\ncore tasks: static evaluation, which focuses on evaluating the ability of\npredefined outpatient referrals, and dynamic evaluation, which evaluates\ncapabilities of refining outpatient referral recommendations through iterative\ndialogues. Our findings suggest that LLMs offer limited advantages over\nBERT-like models, but show promise in asking effective questions during\ninteractive dialogues."}
{"id": "2505.04986", "pdf": "https://arxiv.org/pdf/2505.04986", "abs": "https://arxiv.org/abs/2505.04986", "authors": ["Qian Peng", "Yajie Bao", "Haojie Ren", "Zhaojun Wang", "Changliang Zou"], "title": "Conformal Prediction with Cellwise Outliers: A Detect-then-Impute Approach", "categories": ["stat.ML", "cs.LG"], "comment": "23 pages, 15 figures", "summary": "Conformal prediction is a powerful tool for constructing prediction intervals\nfor black-box models, providing a finite sample coverage guarantee for\nexchangeable data. However, this exchangeability is compromised when some\nentries of the test feature are contaminated, such as in the case of cellwise\noutliers. To address this issue, this paper introduces a novel framework called\ndetect-then-impute conformal prediction. This framework first employs an\noutlier detection procedure on the test feature and then utilizes an imputation\nmethod to fill in those cells identified as outliers. To quantify the\nuncertainty in the processed test feature, we adaptively apply the detection\nand imputation procedures to the calibration set, thereby constructing\nexchangeable features for the conformal prediction interval of the test label.\nWe develop two practical algorithms, PDI-CP and JDI-CP, and provide a\ndistribution-free coverage analysis under some commonly used detection and\nimputation procedures. Notably, JDI-CP achieves a finite sample $1-2\\alpha$\ncoverage guarantee. Numerical experiments on both synthetic and real datasets\ndemonstrate that our proposed algorithms exhibit robust coverage properties and\ncomparable efficiency to the oracle baseline."}
{"id": "2505.05132", "pdf": "https://arxiv.org/pdf/2505.05132", "abs": "https://arxiv.org/abs/2505.05132", "authors": ["Luis Alvarez", "Jean-Michel Morel"], "title": "An Active Contour Model for Silhouette Vectorization using Bézier Curves", "categories": ["cs.GR", "cs.CV", "math.FA"], "comment": "14 pages, 5 figures and 1 table", "summary": "In this paper, we propose an active contour model for silhouette\nvectorization using cubic B\\'ezier curves. Among the end points of the B\\'ezier\ncurves, we distinguish between corner and regular points where the orientation\nof the tangent vector is prescribed. By minimizing the distance of the B\\'ezier\ncurves to the silhouette boundary, the active contour model optimizes the\nlocation of the B\\'ezier curves end points, the orientation of the tangent\nvectors in the regular points, and the estimation of the B\\'ezier curve\nparameters. This active contour model can use the silhouette vectorization\nobtained by any method as an initial guess. The proposed method significantly\nreduces the average distance between the silhouette boundary and its\nvectorization obtained by the world-class graphic software Inkscape, Adobe\nIllustrator, and a curvature-based vectorization method, which we introduce for\ncomparison. Our method also allows us to impose additional regularity on the\nB\\'ezier curves by reducing their lengths."}
{"id": "2208.03571", "pdf": "https://arxiv.org/pdf/2208.03571", "abs": "https://arxiv.org/abs/2208.03571", "authors": ["Athena Psalta", "Vasileios Tsironis", "Konstantinos Karantzalos"], "title": "Transformer-based assignment decision network for multiple object tracking", "categories": ["cs.CV", "cs.AI"], "comment": "Preprint version. Under consideration at Computer Vision and Image\n  Understanding", "summary": "Data association is a crucial component for any multiple object tracking\n(MOT) method that follows the tracking-by-detection paradigm. To generate\ncomplete trajectories such methods employ a data association process to\nestablish assignments between detections and existing targets during each\ntimestep. Recent data association approaches try to solve either a\nmulti-dimensional linear assignment task or a network flow minimization problem\nor tackle it via multiple hypotheses tracking. However, during inference an\noptimization step that computes optimal assignments is required for every\nsequence frame inducing additional complexity to any given solution. To this\nend, in the context of this work we introduce Transformer-based Assignment\nDecision Network (TADN) that tackles data association without the need of any\nexplicit optimization during inference. In particular, TADN can directly infer\nassignment pairs between detections and active targets in a single forward pass\nof the network. We have integrated TADN in a rather simple MOT framework,\ndesigned a novel training strategy for efficient end-to-end training and\ndemonstrated the high potential of our approach for online visual\ntracking-by-detection MOT on several popular benchmarks, i.e. MOT17, MOT20 and\nUA-DETRAC. Our proposed approach demonstrates strong performance in most\nevaluation metrics despite its simple nature as a tracker lacking significant\nauxiliary components such as occlusion handling or re-identification. The\nimplementation of our method is publicly available at\nhttps://github.com/psaltaath/tadn-mot."}
{"id": "2503.13690", "pdf": "https://arxiv.org/pdf/2503.13690", "abs": "https://arxiv.org/abs/2503.13690", "authors": ["Jan Bronec", "Jindřich Helcl"], "title": "Atyaephyra at SemEval-2025 Task 4: Low-Rank Negative Preference Optimization", "categories": ["cs.CL", "cs.AI", "cs.LG", "68T50 (Primary), 68T07 (Secondary)", "I.2.7"], "comment": "8 pages, 3 figures, accepted to SemEval workshop proceedings at ACL\n  2025", "summary": "We present a submission to the SemEval 2025 shared task on unlearning\nsensitive content from LLMs. Our approach employs negative preference\noptimization using low-rank adaptation. We show that we can utilize this\ncombination to efficiently compute additional regularization terms, which help\nwith unlearning stabilization. The results of our approach significantly exceed\nthe shared task baselines."}
{"id": "2505.04992", "pdf": "https://arxiv.org/pdf/2505.04992", "abs": "https://arxiv.org/abs/2505.04992", "authors": ["Jialong Jiang", "Wenkang Hu", "Jian Huang", "Yuling Jiao", "Xu Liu"], "title": "Boosting Statistic Learning with Synthetic Data from Pretrained Large Models", "categories": ["stat.ML", "cs.LG", "stat.AP"], "comment": null, "summary": "The rapid advancement of generative models, such as Stable Diffusion, raises\na key question: how can synthetic data from these models enhance predictive\nmodeling? While they can generate vast amounts of datasets, only a subset\nmeaningfully improves performance. We propose a novel end-to-end framework that\ngenerates and systematically filters synthetic data through domain-specific\nstatistical methods, selectively integrating high-quality samples for effective\naugmentation. Our experiments demonstrate consistent improvements in predictive\nperformance across various settings, highlighting the potential of our\nframework while underscoring the inherent limitations of generative models for\ndata augmentation. Despite the ability to produce large volumes of synthetic\ndata, the proportion that effectively improves model performance is limited."}
{"id": "2505.05223", "pdf": "https://arxiv.org/pdf/2505.05223", "abs": "https://arxiv.org/abs/2505.05223", "authors": ["Hendrik Surmann", "Jorge de Heuvel", "Maren Bennewitz"], "title": "Multi-Objective Reinforcement Learning for Adaptive Personalized Autonomous Driving", "categories": ["cs.RO", "cs.CV", "cs.LG"], "comment": null, "summary": "Human drivers exhibit individual preferences regarding driving style.\nAdapting autonomous vehicles to these preferences is essential for user trust\nand satisfaction. However, existing end-to-end driving approaches often rely on\npredefined driving styles or require continuous user feedback for adaptation,\nlimiting their ability to support dynamic, context-dependent preferences. We\npropose a novel approach using multi-objective reinforcement learning (MORL)\nwith preference-driven optimization for end-to-end autonomous driving that\nenables runtime adaptation to driving style preferences. Preferences are\nencoded as continuous weight vectors to modulate behavior along interpretable\nstyle objectives$\\unicode{x2013}$including efficiency, comfort, speed, and\naggressiveness$\\unicode{x2013}$without requiring policy retraining. Our\nsingle-policy agent integrates vision-based perception in complex mixed-traffic\nscenarios and is evaluated in diverse urban environments using the CARLA\nsimulator. Experimental results demonstrate that the agent dynamically adapts\nits driving behavior according to changing preferences while maintaining\nperformance in terms of collision avoidance and route completion."}
{"id": "2303.12484", "pdf": "https://arxiv.org/pdf/2303.12484", "abs": "https://arxiv.org/abs/2303.12484", "authors": ["Cheng Jin", "Zhengrui Guo", "Yi Lin", "Luyang Luo", "Hao Chen"], "title": "Label-Efficient Deep Learning in Medical Image Analysis: Challenges and Future Directions", "categories": ["cs.CV", "cs.AI"], "comment": "Under Review", "summary": "Deep learning has significantly advanced medical imaging analysis (MIA),\nachieving state-of-the-art performance across diverse clinical tasks. However,\nits success largely depends on large-scale, high-quality labeled datasets,\nwhich are costly and time-consuming to obtain due to the need for expert\nannotation. To mitigate this limitation, label-efficient deep learning methods\nhave emerged to improve model performance under limited supervision by\nleveraging labeled, unlabeled, and weakly labeled data. In this survey, we\nsystematically review over 350 peer-reviewed studies and present a\ncomprehensive taxonomy of label-efficient learning methods in MIA. These\nmethods are categorized into four labeling paradigms: no label, insufficient\nlabel, inexact label, and label refinement. For each category, we analyze\nrepresentative techniques across imaging modalities and clinical applications,\nhighlighting shared methodological principles and task-specific adaptations. We\nalso examine the growing role of health foundation models (HFMs) in enabling\nlabel-efficient learning through large-scale pre-training and transfer\nlearning, enhancing the use of limited annotations in downstream tasks.\nFinally, we identify current challenges and future directions to facilitate the\ntranslation of label-efficient learning from research promise to everyday\nclinical care."}
{"id": "2503.15169", "pdf": "https://arxiv.org/pdf/2503.15169", "abs": "https://arxiv.org/abs/2503.15169", "authors": ["Yuting Guo", "Abeed Sarker"], "title": "Benchmarking Open-Source Large Language Models on Healthcare Text Classification Tasks", "categories": ["cs.CL", "cs.AI"], "comment": "5 pages", "summary": "The application of large language models (LLMs) to healthcare information\nextraction has emerged as a promising approach. This study evaluates the\nclassification performance of five open-source LLMs: GEMMA-3-27B-IT,\nLLAMA3-70B, LLAMA4-109B, DEEPSEEK-R1-DISTILL-LLAMA-70B, and\nDEEPSEEK-V3-0324-UD-Q2_K_XL, across six healthcare-related classification tasks\ninvolving both social media data (breast cancer, changes in medication regimen,\nadverse pregnancy outcomes, potential COVID-19 cases) and clinical data (stigma\nlabeling, medication change discussion). We report precision, recall, and F1\nscores with 95% confidence intervals for all model-task combinations. Our\nfindings reveal significant performance variability between LLMs, with\nDeepSeekV3 emerging as the strongest overall performer, achieving the highest\nF1 scores in four tasks. Notably, models generally performed better on social\nmedia tasks compared to clinical data tasks, suggesting potential\ndomain-specific challenges. GEMMA-3-27B-IT demonstrated exceptionally high\nrecall despite its smaller parameter count, while LLAMA4-109B showed\nsurprisingly underwhelming performance compared to its predecessor LLAMA3-70B,\nindicating that larger parameter counts do not guarantee improved\nclassification results. We observed distinct precision-recall trade-offs across\nmodels, with some favoring sensitivity over specificity and vice versa. These\nfindings highlight the importance of task-specific model selection for\nhealthcare applications, considering the particular data domain and\nprecision-recall requirements rather than model size alone. As healthcare\nincreasingly integrates AI-driven text classification tools, this comprehensive\nbenchmarking provides valuable guidance for model selection and implementation\nwhile underscoring the need for continued evaluation and domain adaptation of\nLLMs in healthcare contexts."}
{"id": "2505.04999", "pdf": "https://arxiv.org/pdf/2505.04999", "abs": "https://arxiv.org/abs/2505.04999", "authors": ["Anthony Liang", "Pavel Czempin", "Matthew Hong", "Yutai Zhou", "Erdem Biyik", "Stephen Tu"], "title": "CLAM: Continuous Latent Action Models for Robot Learning from Unlabeled Demonstrations", "categories": ["cs.RO", "cs.LG"], "comment": "Latent Action Models, Self-supervised Pretraining, Learning from\n  Videos", "summary": "Learning robot policies using imitation learning requires collecting large\namounts of costly action-labeled expert demonstrations, which fundamentally\nlimits the scale of training data. A promising approach to address this\nbottleneck is to harness the abundance of unlabeled observations-e.g., from\nvideo demonstrations-to learn latent action labels in an unsupervised way.\nHowever, we find that existing methods struggle when applied to complex robot\ntasks requiring fine-grained motions. We design continuous latent action models\n(CLAM) which incorporate two key ingredients we find necessary for learning to\nsolve complex continuous control tasks from unlabeled observation data: (a)\nusing continuous latent action labels instead of discrete representations, and\n(b) jointly training an action decoder to ensure that the latent action space\ncan be easily grounded to real actions with relatively few labeled examples.\nImportantly, the labeled examples can be collected from non-optimal play data,\nenabling CLAM to learn performant policies without access to any action-labeled\nexpert data. We demonstrate on continuous control benchmarks in DMControl\n(locomotion) and MetaWorld (manipulation), as well as on a real WidowX robot\narm that CLAM significantly outperforms prior state-of-the-art methods,\nremarkably with a 2-3x improvement in task success rate compared to the best\nbaseline. Videos and code can be found at clamrobot.github.io."}
{"id": "2309.04522", "pdf": "https://arxiv.org/pdf/2309.04522", "abs": "https://arxiv.org/abs/2309.04522", "authors": ["Yehonatan Avidan", "Qianyi Li", "Haim Sompolinsky"], "title": "Connecting NTK and NNGP: A Unified Theoretical Framework for Wide Neural Network Learning Dynamics", "categories": ["cs.LG", "cond-mat.dis-nn", "cs.AI"], "comment": null, "summary": "Artificial neural networks have revolutionized machine learning in recent\nyears, but a complete theoretical framework for their learning process is still\nlacking. Substantial advances were achieved for wide networks, within two\ndisparate theoretical frameworks: the Neural Tangent Kernel (NTK), which\nassumes linearized gradient descent dynamics, and the Bayesian Neural Network\nGaussian Process (NNGP). We unify these two theories using gradient descent\nlearning with an additional noise in an ensemble of wide deep networks. We\nconstruct an analytical theory for the network input-output function and\nintroduce a new time-dependent Neural Dynamical Kernel (NDK) from which both\nNTK and NNGP kernels are derived. We identify two learning phases: a\ngradient-driven learning phase, dominated by loss minimization, in which the\ntime scale is governed by the initialization variance. It is followed by a slow\ndiffusive learning stage, where the parameters sample the solution space, with\na time constant decided by the noise and the Bayesian prior variance. The two\nvariance parameters strongly affect the performance in the two regimes,\nespecially in sigmoidal neurons. In contrast to the exponential convergence of\nthe mean predictor in the initial phase, the convergence to the equilibrium is\nmore complex and may behave nonmonotonically. By characterizing the diffusive\nphase, our work sheds light on representational drift in the brain, explaining\nhow neural activity changes continuously without degrading performance, either\nby ongoing gradient signals that synchronize the drifts of different synapses\nor by architectural biases that generate task-relevant information that is\nrobust against the drift process. This work closes the gap between the NTK and\nNNGP theories, providing a comprehensive framework for the learning process of\ndeep wide neural networks and for analyzing dynamics in biological circuits."}
{"id": "2504.17480", "pdf": "https://arxiv.org/pdf/2504.17480", "abs": "https://arxiv.org/abs/2504.17480", "authors": ["Xin Yi", "Shunfan Zheng", "Linlin Wang", "Xiaoling Wang", "Liang He"], "title": "Unified Attacks to Large Language Model Watermarks: Spoofing and Scrubbing in Unauthorized Knowledge Distillation", "categories": ["cs.CL"], "comment": null, "summary": "Watermarking has emerged as a critical technique for combating misinformation\nand protecting intellectual property in large language models (LLMs). A recent\ndiscovery, termed watermark radioactivity, reveals that watermarks embedded in\nteacher models can be inherited by student models through knowledge\ndistillation. On the positive side, this inheritance allows for the detection\nof unauthorized knowledge distillation by identifying watermark traces in\nstudent models. However, the robustness of watermarks against scrubbing attacks\nand their unforgeability in the face of spoofing attacks under unauthorized\nknowledge distillation remain largely unexplored. Existing watermark attack\nmethods either assume access to model internals or fail to simultaneously\nsupport both scrubbing and spoofing attacks. In this work, we propose\nContrastive Decoding-Guided Knowledge Distillation (CDG-KD), a unified\nframework that enables bidirectional attacks under unauthorized knowledge\ndistillation. Our approach employs contrastive decoding to extract corrupted or\namplified watermark texts via comparing outputs from the student model and\nweakly watermarked references, followed by bidirectional distillation to train\nnew student models capable of watermark removal and watermark forgery,\nrespectively. Extensive experiments show that CDG-KD effectively performs\nattacks while preserving the general performance of the distilled model. Our\nfindings underscore critical need for developing watermarking schemes that are\nrobust and unforgeable."}
{"id": "2505.05085", "pdf": "https://arxiv.org/pdf/2505.05085", "abs": "https://arxiv.org/abs/2505.05085", "authors": ["Gary Froyland", "Kevin Kühl"], "title": "Learning dynamically inspired invariant subspaces for Koopman and transfer operator approximation", "categories": ["math.DS", "cs.LG", "cs.NA", "math.NA", "47A15, 37C30, 47A58, 68T07"], "comment": "20 pages, 13 figures", "summary": "Transfer and Koopman operator methods offer a framework for representing\ncomplex, nonlinear dynamical systems via linear transformations, enabling for a\ndeeper understanding of the underlying dynamics. The spectrum of these\noperators provide important insights into system predictability and emergent\nbehaviour, although efficiently estimating them from data can be challenging.\nWe tackle this issue through the lens of general operator and representational\nlearning, in which we approximate these linear operators using efficient\nfinite-dimensional representations. Specifically, we machine-learn orthonormal,\nlocally supported basis functions that are dynamically tailored to the system.\nThis learned basis provides a particularly accurate approximation of the\noperator's action as well as a nearly invariant finite-dimensional subspace. We\nillustrate our approach with examples that showcase the retrieval of spectral\nproperties from the estimated operator, and emphasise the dynamically adaptive\nquality of the machine-learned basis."}
{"id": "2307.11470", "pdf": "https://arxiv.org/pdf/2307.11470", "abs": "https://arxiv.org/abs/2307.11470", "authors": ["Shixuan Xu", "Hao Qi", "Xinghui Dong"], "title": "Semi-supervised Underwater Image Enhancement Using A Physics-Aware Triple-Stream Network", "categories": ["cs.CV"], "comment": "13 pages, 10 figures", "summary": "Underwater images normally suffer from degradation due to the transmission\nmedium of water bodies. Both traditional prior-based approaches and deep\nlearning-based methods have been used to address this problem. However, the\ninflexible assumption of the former often impairs their effectiveness in\nhandling diverse underwater scenes, while the generalization of the latter to\nunseen images is usually weakened by insufficient data. In this study, we\nleverage both the physics-based Image Formation Model (IFM) and deep learning\ntechniques for Underwater Image Enhancement (UIE). To this end, we propose a\nnovel Physics-Aware Triple-Stream Underwater Image Enhancement Network, i.e.,\nPATS-UIENet, which comprises a Direct Signal Transmission Estimation Steam\n(D-Stream), a Backscatter Signal Transmission Estimation Steam (B-Stream) and\nan Ambient Light Estimation Stream (A-Stream). This network fulfills the UIE\ntask by explicitly estimating the degradation parameters of a revised IFM. We\nalso adopt an IFM-inspired semi-supervised learning framework, which exploits\nboth the labeled and unlabeled images, to address the issue of insufficient\ndata. To our knowledge, such a physics-aware deep network and the IFM-inspired\nsemi-supervised learning framework have not been used for the UIE task before.\nOur method performs better than, or at least comparably to, sixteen baselines\nacross six testing sets in the degradation estimation and UIE tasks. These\npromising results should be due to the fact that the proposed method can not\nonly model the degradation but also learn the characteristics of diverse\nunderwater scenes."}
{"id": "2312.05114", "pdf": "https://arxiv.org/pdf/2312.05114", "abs": "https://arxiv.org/abs/2312.05114", "authors": ["Georgi Ganev", "Emiliano De Cristofaro"], "title": "The Inadequacy of Similarity-based Privacy Metrics: Privacy Attacks against \"Truly Anonymous\" Synthetic Datasets", "categories": ["cs.CR", "cs.AI", "cs.LG"], "comment": "Published in the Proceedings of the 46th IEEE Symposium on Security &\n  Privacy (IEEE S&P 2025). Please cite the S&P version", "summary": "Generative models producing synthetic data are meant to provide a\nprivacy-friendly approach to releasing data. However, their privacy guarantees\nare only considered robust when models satisfy Differential Privacy (DP). Alas,\nthis is not a ubiquitous standard, as many leading companies (and, in fact,\nresearch papers) use ad-hoc privacy metrics based on testing the statistical\nsimilarity between synthetic and real data.\n  In this paper, we examine the privacy metrics used in real-world synthetic\ndata deployments and demonstrate their unreliability in several ways. First, we\nprovide counter-examples where severe privacy violations occur even if the\nprivacy tests pass and instantiate accurate membership and attribute inference\nattacks with minimal cost. We then introduce ReconSyn, a reconstruction attack\nthat generates multiple synthetic datasets that are considered private by the\nmetrics but actually leak information unique to individual records. We show\nthat ReconSyn recovers 78-100% of the outliers in the train data with only\nblack-box access to a single fitted generative model and the privacy metrics.\nIn the process, we show that applying DP only to the model does not mitigate\nthis attack, as using privacy metrics breaks the end-to-end DP pipeline."}
{"id": "2505.00654", "pdf": "https://arxiv.org/pdf/2505.00654", "abs": "https://arxiv.org/abs/2505.00654", "authors": ["Daniel N. Nissani"], "title": "Large Language Models Understanding: an Inherent Ambiguity Barrier", "categories": ["cs.CL", "cs.AI"], "comment": "submitted to NEURAL COMPUTATION", "summary": "A lively ongoing debate is taking place, since the extraordinary emergence of\nLarge Language Models (LLMs) with regards to their capability to understand the\nworld and capture the meaning of the dialogues in which they are involved.\nArguments and counter-arguments have been proposed based upon thought\nexperiments, anecdotal conversations between LLMs and humans, statistical\nlinguistic analysis, philosophical considerations, and more. In this brief\npaper we present a counter-argument based upon a thought experiment and\nsemi-formal considerations leading to an inherent ambiguity barrier which\nprevents LLMs from having any understanding of what their amazingly fluent\ndialogues mean."}
{"id": "2505.05118", "pdf": "https://arxiv.org/pdf/2505.05118", "abs": "https://arxiv.org/abs/2505.05118", "authors": ["Makbule Gulcin Ozsoy"], "title": "Enhancing Text2Cypher with Schema Filtering", "categories": ["cs.DB", "cs.LG"], "comment": null, "summary": "Knowledge graphs represent complex data using nodes, relationships, and\nproperties. Cypher, a powerful query language for graph databases, enables\nefficient modeling and querying. Recent advancements in large language models\nallow translation of natural language questions into Cypher queries -\nText2Cypher. A common approach is incorporating database schema into prompts.\nHowever, complex schemas can introduce noise, increase hallucinations, and\nraise computational costs. Schema filtering addresses these challenges by\nincluding only relevant schema elements, improving query generation while\nreducing token costs. This work explores various schema filtering methods for\nText2Cypher task and analyzes their impact on token length, performance, and\ncost. Results show that schema filtering effectively optimizes Text2Cypher,\nespecially for smaller models. Consistent with prior research, we find that\nlarger models benefit less from schema filtering due to their longer context\ncapabilities. However, schema filtering remains valuable for both larger and\nsmaller models in cost reduction."}
{"id": "2310.05829", "pdf": "https://arxiv.org/pdf/2310.05829", "abs": "https://arxiv.org/abs/2310.05829", "authors": ["Cheng Tan", "Jue Wang", "Zhangyang Gao", "Siyuan Li", "Stan Z. Li"], "title": "USTEP: Spatio-Temporal Predictive Learning under A Unified View", "categories": ["cs.CV"], "comment": "Accepted by TPAMI", "summary": "Spatio-temporal predictive learning plays a crucial role in self-supervised\nlearning, with wide-ranging applications across a diverse range of fields.\nPrevious approaches for temporal modeling fall into two categories:\nrecurrent-based and recurrent-free methods. The former, while meticulously\nprocessing frames one by one, neglect short-term spatio-temporal information\nredundancies, leading to inefficiencies. The latter naively stack frames\nsequentially, overlooking the inherent temporal dependencies. In this paper, we\nre-examine the two dominant temporal modeling approaches within the realm of\nspatio-temporal predictive learning, offering a unified perspective. Building\nupon this analysis, we introduce USTEP (Unified Spatio-TEmporal Predictive\nlearning), an innovative framework that reconciles the recurrent-based and\nrecurrent-free methods by integrating both micro-temporal and macro-temporal\nscales. Extensive experiments on a wide range of spatio-temporal predictive\nlearning demonstrate that USTEP achieves significant improvements over existing\ntemporal modeling approaches, thereby establishing it as a robust solution for\na wide range of spatio-temporal applications."}
{"id": "2403.01695", "pdf": "https://arxiv.org/pdf/2403.01695", "abs": "https://arxiv.org/abs/2403.01695", "authors": ["Qingyuan Wang", "Barry Cardiff", "Antoine Frappé", "Benoit Larras", "Deepu John"], "title": "DyCE: Dynamically Configurable Exiting for Deep Learning Compression and Real-time Scaling", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Conventional deep learning (DL) model compression and scaling methods focus\non altering the model's components, impacting the results across all samples\nuniformly. However, since samples vary in difficulty, a dynamic model that\nadapts computation based on sample complexity offers a novel perspective for\ncompression and scaling. Despite this potential, existing dynamic models are\ntypically monolithic and model-specific, limiting their generalizability as\nbroad compression and scaling methods. Additionally, most deployed DL systems\nare fixed, unable to adjust their scale once deployed and, therefore, cannot\nadapt to the varying real-time demands. This paper introduces DyCE, a\ndynamically configurable system that can adjust the performance-complexity\ntrade-off of a DL model at runtime without requiring re-initialization or\nredeployment on inference hardware. DyCE achieves this by adding small exit\nnetworks to intermediate layers of the original model, allowing computation to\nterminate early if acceptable results are obtained. DyCE also decouples the\ndesign of an efficient dynamic model, facilitating easy adaptation to new base\nmodels and potential general use in compression and scaling. We also propose\nmethods for generating optimized configurations and determining the types and\npositions of exit networks to achieve desired performance and complexity\ntrade-offs. By enabling simple configuration switching, DyCE provides\nfine-grained performance tuning in real-time. We demonstrate the effectiveness\nof DyCE through image classification tasks using deep convolutional neural\nnetworks (CNNs). DyCE significantly reduces computational complexity by 23.5%\nfor ResNet152 and 25.9% for ConvNextv2-tiny on ImageNet, with accuracy\nreductions of less than 0.5%."}
{"id": "2505.01658", "pdf": "https://arxiv.org/pdf/2505.01658", "abs": "https://arxiv.org/abs/2505.01658", "authors": ["Sihyeong Park", "Sungryeol Jeon", "Chaelyn Lee", "Seokhun Jeon", "Byung-Soo Kim", "Jemin Lee"], "title": "A Survey on Inference Engines for Large Language Models: Perspectives on Optimization and Efficiency", "categories": ["cs.CL"], "comment": "Under review; 65 pages; 27 figures", "summary": "Large language models (LLMs) are widely applied in chatbots, code generators,\nand search engines. Workloads such as chain-of-thought, complex reasoning, and\nagent services significantly increase the inference cost by invoking the model\nrepeatedly. Optimization methods such as parallelism, compression, and caching\nhave been adopted to reduce costs, but the diverse service requirements make it\nhard to select the right method. Recently, specialized LLM inference engines\nhave emerged as a key component for integrating the optimization methods into\nservice-oriented infrastructures. However, a systematic study on inference\nengines is still lacking. This paper provides a comprehensive evaluation of 25\nopen-source and commercial inference engines. We examine each inference engine\nin terms of ease-of-use, ease-of-deployment, general-purpose support,\nscalability, and suitability for throughput- and latency-aware computation.\nFurthermore, we explore the design goals of each inference engine by\ninvestigating the optimization techniques it supports. In addition, we assess\nthe ecosystem maturity of open source inference engines and handle the\nperformance and cost policy of commercial solutions. We outline future research\ndirections that include support for complex LLM-based services, support of\nvarious hardware, and enhanced security, offering practical guidance to\nresearchers and developers in selecting and designing optimized LLM inference\nengines. We also provide a public repository to continually track developments\nin this fast-evolving field:\nhttps://github.com/sihyeong/Awesome-LLM-Inference-Engine"}
{"id": "2505.05121", "pdf": "https://arxiv.org/pdf/2505.05121", "abs": "https://arxiv.org/abs/2505.05121", "authors": ["Jasper Rou"], "title": "Error Analysis of Deep PDE Solvers for Option Pricing", "categories": ["q-fin.CP", "cs.LG", "q-fin.MF", "91G20, 91G60, 68T07"], "comment": "15 pages, 19 figures", "summary": "Option pricing often requires solving partial differential equations (PDEs).\nAlthough deep learning-based PDE solvers have recently emerged as quick\nsolutions to this problem, their empirical and quantitative accuracy remain not\nwell understood, hindering their real-world applicability. In this research,\nour aim is to offer actionable insights into the utility of deep PDE solvers\nfor practical option pricing implementation. Through comparative experiments in\nboth the Black--Scholes and the Heston model, we assess the empirical\nperformance of two neural network algorithms to solve PDEs: the Deep Galerkin\nMethod and the Time Deep Gradient Flow method (TDGF). We determine their\nempirical convergence rates and training time as functions of (i) the number of\nsampling stages, (ii) the number of samples, (iii) the number of layers, and\n(iv) the number of nodes per layer. For the TDGF, we also consider the order of\nthe discretization scheme and the number of time steps."}
{"id": "2311.12421", "pdf": "https://arxiv.org/pdf/2311.12421", "abs": "https://arxiv.org/abs/2311.12421", "authors": ["Christian Keilstrup Ingwersen", "Rasmus Tirsgaard", "Rasmus Nylander", "Janus Nørtoft Jensen", "Anders Bjorholm Dahl", "Morten Rieger Hannemose"], "title": "Two Views Are Better than One: Monocular 3D Pose Estimation with Multiview Consistency", "categories": ["cs.CV"], "comment": null, "summary": "Deducing a 3D human pose from a single 2D image is inherently challenging\nbecause multiple 3D poses can correspond to the same 2D representation. 3D data\ncan resolve this pose ambiguity, but it is expensive to record and requires an\nintricate setup that is often restricted to controlled lab environments. We\npropose a method that improves the performance of deep learning-based monocular\n3D human pose estimation models by using multiview data only during training,\nbut not during inference. We introduce a novel loss function, consistency loss,\nwhich operates on two synchronized views. This approach is simpler than\nprevious models that require 3D ground truth or intrinsic and extrinsic camera\nparameters. Our consistency loss penalizes differences in two pose sequences\nafter rigid alignment. We also demonstrate that our consistency loss\nsubstantially improves performance for fine-tuning without requiring 3D data.\nFurthermore, we show that using our consistency loss can yield state-of-the-art\nperformance when training models from scratch in a semi-supervised manner. Our\nfindings provide a simple way to capture new data, e.g in a new domain. This\ndata can be added using off-the-shelf cameras with no calibration requirements.\nWe make all our code and data publicly available."}
{"id": "2403.16149", "pdf": "https://arxiv.org/pdf/2403.16149", "abs": "https://arxiv.org/abs/2403.16149", "authors": ["Yan Jia", "Yuxin Song", "Zihou Liu", "Qingyin Tan", "Yang Song", "Yu Zhang", "Zheli Liu"], "title": "Analyzing Consumer IoT Traffic from Security and Privacy Perspectives: a Comprehensive Survey", "categories": ["cs.CR", "cs.AI", "cs.LG"], "comment": null, "summary": "The Consumer Internet of Things (CIoT), a notable segment within the IoT\ndomain, involves the integration of IoT technology into consumer electronics\nand devices, such as smart homes and smart wearables. Compared to traditional\nIoT fields, CIoT differs notably in target users, product types, and design\napproaches. While offering convenience to users, it also raises new security\nand privacy concerns. Network traffic analysis, a widely used technique in the\nsecurity community, has been extensively applied to investigate these concerns\nabout CIoT. Compared to traditional network traffic analysis in fields like\nmobile apps and websites, CIoT introduces unique characteristics that pose new\nchallenges and research opportunities. Researchers have made significant\ncontributions in this area. To aid researchers in understanding the application\nof traffic analysis tools for assessing CIoT security and privacy risks, this\nsurvey reviews 310 publications on traffic analysis within the CIoT security\nand privacy domain from January 2018 to June 2024, focusing on three research\nquestions. Our work: 1) outlines the CIoT traffic analysis process and\nhighlights its differences from general network traffic analysis. 2) summarizes\nand classifies existing research into four categories according to its\napplication objectives: device fingerprinting, user activity inference,\nmalicious traffic detection, and measurement. 3) explores emerging challenges\nand potential future research directions based on each step of the CIoT traffic\nanalysis process. This will provide new insights to the community and guide the\nindustry towards safer product designs."}
{"id": "2505.01877", "pdf": "https://arxiv.org/pdf/2505.01877", "abs": "https://arxiv.org/abs/2505.01877", "authors": ["Jiří Milička", "Anna Marklová", "Ondřej Drobil", "Eva Pospíšilová"], "title": "Humans can learn to detect AI-generated texts, or at least learn when they can't", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "This study investigates whether individuals can learn to accurately\ndiscriminate between human-written and AI-produced texts when provided with\nimmediate feedback, and if they can use this feedback to recalibrate their\nself-perceived competence. We also explore the specific criteria individuals\nrely upon when making these decisions, focusing on textual style and perceived\nreadability.\n  We used GPT-4o to generate several hundred texts across various genres and\ntext types comparable to Koditex, a multi-register corpus of human-written\ntexts. We then presented randomized text pairs to 254 Czech native speakers who\nidentified which text was human-written and which was AI-generated.\nParticipants were randomly assigned to two conditions: one receiving immediate\nfeedback after each trial, the other receiving no feedback until experiment\ncompletion. We recorded accuracy in identification, confidence levels, response\ntimes, and judgments about text readability along with demographic data and\nparticipants' engagement with AI technologies prior to the experiment.\n  Participants receiving immediate feedback showed significant improvement in\naccuracy and confidence calibration. Participants initially held incorrect\nassumptions about AI-generated text features, including expectations about\nstylistic rigidity and readability. Notably, without feedback, participants\nmade the most errors precisely when feeling most confident -- an issue largely\nresolved among the feedback group.\n  The ability to differentiate between human and AI-generated texts can be\neffectively learned through targeted training with explicit feedback, which\nhelps correct misconceptions about AI stylistic features and readability, as\nwell as potential other variables that were not explored, while facilitating\nmore accurate self-assessment. This finding might be particularly important in\neducational contexts."}
{"id": "2505.05122", "pdf": "https://arxiv.org/pdf/2505.05122", "abs": "https://arxiv.org/abs/2505.05122", "authors": ["Makbule Gulcin Ozsoy"], "title": "Text2Cypher: Data Pruning using Hard Example Selection", "categories": ["cs.DB", "cs.LG"], "comment": null, "summary": "Database query languages such as SQL for relational databases and Cypher for\ngraph databases have been widely adopted. Recent advancements in large language\nmodels (LLMs) enable natural language interactions with databases through\nmodels like Text2SQL and Text2Cypher. Fine-tuning these models typically\nrequires large, diverse datasets containing non-trivial examples. However, as\ndataset size increases, the cost of fine-tuning also rises. This makes smaller,\nhigh-quality datasets essential for reducing costs for the same or better\nperformance. In this paper, we propose five hard-example selection techniques\nfor pruning the Text2Cypher dataset, aiming to preserve or improve performance\nwhile reducing resource usage. Our results show that these hard-example\nselection approaches can halve training time and costs with minimal impact on\nperformance, and demonstrates that hard-example selection provides a\ncost-effective solution."}
{"id": "2403.08142", "pdf": "https://arxiv.org/pdf/2403.08142", "abs": "https://arxiv.org/abs/2403.08142", "authors": ["Alzayat Saleh", "Alex Olsen", "Jake Wood", "Bronson Philippa", "Mostafa Rahimi Azghadi"], "title": "FieldNet: Efficient Real-Time Shadow Removal for Enhanced Vision in Field Robotics", "categories": ["cs.CV"], "comment": "22 pages, 9 figures, 8 tables. Published at Expert Systems with\n  Applications", "summary": "Shadows significantly hinder computer vision tasks in outdoor environments,\nparticularly in field robotics, where varying lighting conditions complicate\nobject detection and localisation. We present FieldNet, a novel deep learning\nframework for real-time shadow removal, optimised for resource-constrained\nhardware. FieldNet introduces a probabilistic enhancement module and a novel\nloss function to address challenges of inconsistent shadow boundary supervision\nand artefact generation, achieving enhanced accuracy and simplicity without\nrequiring shadow masks during inference. Trained on a dataset of 10,000 natural\nimages augmented with synthetic shadows, FieldNet outperforms state-of-the-art\nmethods on benchmark datasets (ISTD, ISTD+, SRD), with up to $9$x speed\nimprovements (66 FPS on Nvidia 2080Ti) and superior shadow removal quality\n(PSNR: 38.67, SSIM: 0.991). Real-world case studies in precision agriculture\nrobotics demonstrate the practical impact of FieldNet in enhancing weed\ndetection accuracy. These advancements establish FieldNet as a robust,\nefficient solution for real-time vision tasks in field robotics and beyond."}
{"id": "2406.06600", "pdf": "https://arxiv.org/pdf/2406.06600", "abs": "https://arxiv.org/abs/2406.06600", "authors": ["Yutao Sun", "Mingshuai Chen", "Tiancheng Zhao", "Kangjia Zhao", "He Li", "Jintao Chen", "Zhongyi Wang", "Liqiang Lu", "Xinkui Zhao", "Shuiguang Deng", "Jianwei Yin"], "title": "HORAE: A Domain-Agnostic Language for Automated Service Regulation", "categories": ["cs.LG", "cs.AI", "cs.CL"], "comment": null, "summary": "Artificial intelligence is rapidly encroaching on the field of service\nregulation. However, existing AI-based regulation techniques are often tailored\nto specific application domains and thus are difficult to generalize in an\nautomated manner. This paper presents Horae, a unified specification language\nfor modeling (multimodal) regulation rules across a diverse set of domains. We\nshowcase how Horae facilitates an intelligent service regulation pipeline by\nfurther exploiting a fine-tuned large language model named RuleGPT that\nautomates the Horae modeling process, thereby yielding an end-to-end framework\nfor fully automated intelligent service regulation. The feasibility and\neffectiveness of our framework are demonstrated over a benchmark of various\nreal-world regulation domains. In particular, we show that our open-sourced,\nfine-tuned RuleGPT with 7B parameters suffices to outperform GPT-3.5 and\nperform on par with GPT-4o."}
{"id": "2505.03005", "pdf": "https://arxiv.org/pdf/2505.03005", "abs": "https://arxiv.org/abs/2505.03005", "authors": ["Daniel Goldstein", "Eric Alcaide", "Janna Lu", "Eugene Cheah"], "title": "RADLADS: Rapid Attention Distillation to Linear Attention Decoders at Scale", "categories": ["cs.CL", "cs.AI", "cs.LG", "I.2.7"], "comment": null, "summary": "We present Rapid Attention Distillation to Linear Attention Decoders at Scale\n(RADLADS), a protocol for rapidly converting softmax attention transformers\ninto linear attention decoder models, along with two new RWKV-variant\narchitectures, and models converted from popular Qwen2.5 open source models in\n7B, 32B, and 72B sizes. Our conversion process requires only 350-700M tokens,\nless than 0.005% of the token count used to train the original teacher models.\nConverting to our 72B linear attention model costs less than \\$2,000 USD at\ntoday's prices, yet quality at inference remains close to the original\ntransformer. These models achieve state-of-the-art downstream performance\nacross a set of standard benchmarks for linear attention models of their size.\nWe release all our models on HuggingFace under the Apache 2.0 license, with the\nexception of our 72B models which are also governed by the Qwen License\nAgreement.\n  Models at\nhttps://huggingface.co/collections/recursal/radlads-6818ee69e99e729ba8a87102\nTraining Code at https://github.com/recursal/RADLADS-paper"}
{"id": "2505.05151", "pdf": "https://arxiv.org/pdf/2505.05151", "abs": "https://arxiv.org/abs/2505.05151", "authors": ["Chuangtao Chen", "Qinglin Zhao", "MengChu Zhou", "Zhimin He", "Haozhen Situ"], "title": "Overcoming Dimensional Factorization Limits in Discrete Diffusion Models through Quantum Joint Distribution Learning", "categories": ["quant-ph", "cs.LG"], "comment": "Comments are welcome", "summary": "This study explores quantum-enhanced discrete diffusion models to overcome\nclassical limitations in learning high-dimensional distributions. We rigorously\nprove that classical discrete diffusion models, which calculate per-dimension\ntransition probabilities to avoid exponential computational cost, exhibit\nworst-case linear scaling of Kullback-Leibler (KL) divergence with data\ndimension. To address this, we propose a Quantum Discrete Denoising Diffusion\nProbabilistic Model (QD3PM), which enables joint probability learning through\ndiffusion and denoising in exponentially large Hilbert spaces. By deriving\nposterior states through quantum Bayes' theorem, similar to the crucial role of\nposterior probabilities in classical diffusion models, and by learning the\njoint probability, we establish a solid theoretical foundation for\nquantum-enhanced diffusion models. For denoising, we design a quantum circuit\nusing temporal information for parameter sharing and learnable\nclassical-data-controlled rotations for encoding. Exploiting joint distribution\nlearning, our approach enables single-step sampling from pure noise,\neliminating iterative requirements of existing models. Simulations demonstrate\nthe proposed model's superior accuracy in modeling complex distributions\ncompared to factorization methods. Hence, this paper establishes a new\ntheoretical paradigm in generative models by leveraging the quantum advantage\nin joint distribution learning."}
{"id": "2406.06050", "pdf": "https://arxiv.org/pdf/2406.06050", "abs": "https://arxiv.org/abs/2406.06050", "authors": ["Jinnan Chen", "Chen Li", "Jianfeng Zhang", "Lingting Zhu", "Buzhen Huang", "Hanlin Chen", "Gim Hee Lee"], "title": "Generalizable Human Gaussians from Single-View Image", "categories": ["cs.CV"], "comment": "ICLR 2025: https://jinnan-chen.github.io/projects/HGM/", "summary": "In this work, we tackle the task of learning 3D human Gaussians from a single\nimage, focusing on recovering detailed appearance and geometry including\nunobserved regions. We introduce a single-view generalizable Human Gaussian\nModel (HGM), which employs a novel generate-then-refine pipeline with the\nguidance from human body prior and diffusion prior. Our approach uses a\nControlNet to refine rendered back-view images from coarse predicted human\nGaussians, then uses the refined image along with the input image to\nreconstruct refined human Gaussians. To mitigate the potential generation of\nunrealistic human poses and shapes, we incorporate human priors from the SMPL-X\nmodel as a dual branch, propagating image features from the SMPL-X volume to\nthe image Gaussians using sparse convolution and attention mechanisms. Given\nthat the initial SMPL-X estimation might be inaccurate, we gradually refine it\nwith our HGM model. We validate our approach on several publicly available\ndatasets. Our method surpasses previous methods in both novel view synthesis\nand surface reconstruction. Our approach also exhibits strong generalization\nfor cross-dataset evaluation and in-the-wild images."}
{"id": "2406.07944", "pdf": "https://arxiv.org/pdf/2406.07944", "abs": "https://arxiv.org/abs/2406.07944", "authors": ["Meiziniu Li", "Dongze Li", "Jianmeng Liu", "Jialun Cao", "Yongqiang Tian", "Shing-Chi Cheung"], "title": "Enhancing Differential Testing With LLMs For Testing Deep Learning Libraries", "categories": ["cs.SE", "cs.AI", "D.2.5; I.2.5"], "comment": "This work has been accepted by ACM TOSEM. Manuscript under final\n  preparation", "summary": "Differential testing offers a promising strategy to alleviate the test oracle\nproblem by comparing the test results between alternative implementations.\nHowever, existing differential testing techniques for deep learning (DL)\nlibraries are limited by the key challenges of finding alternative\nimplementations (called counterparts) for a given API and subsequently\ngenerating diverse test inputs. To address the two challenges, this paper\nintroduces DLLens, an LLM-enhanced differential testing technique for DL\nlibraries. To address the first challenge, DLLens incorporates an LLM-based\ncounterpart synthesis workflow, with the insight that the counterpart of a\ngiven DL library API's computation could be successfully synthesized through\ncertain composition and adaptation of the APIs from another DL library. To\naddress the second challenge, DLLens incorporates a static analysis technique\nthat extracts the path constraints from the implementations of a given API and\nits counterpart to guide diverse test input generation. The extraction is\nfacilitated by LLM's knowledge of the concerned DL library and its upstream\nlibraries.\n  We evaluate DLLens on two popular DL libraries, TensorFlow and PyTorch. Our\nevaluation shows that DLLens synthesizes counterparts for 1.84 times as many\nAPIs as those found by state-of-the-art techniques on these libraries.\nMoreover, under the same time budget, DLLens covers 7.23% more branches and\ndetects 1.88 times as many bugs as state-of-the-art techniques on 200 randomly\nsampled APIs. DLLens has successfully detected 71 bugs in recent TensorFlow and\nPyTorch libraries. Among them, 59 are confirmed by developers, including 46\nconfirmed as previously unknown bugs, and 10 of these previously unknown bugs\nhave been fixed in the latest version of TensorFlow and PyTorch."}
{"id": "2409.03757", "pdf": "https://arxiv.org/pdf/2409.03757", "abs": "https://arxiv.org/abs/2409.03757", "authors": ["Yunze Man", "Shuhong Zheng", "Zhipeng Bao", "Martial Hebert", "Liang-Yan Gui", "Yu-Xiong Wang"], "title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene Understanding", "categories": ["cs.CV", "cs.AI", "cs.CL", "cs.LG", "cs.RO"], "comment": "NeurIPS 2024. Project page: https://yunzeman.github.io/lexicon3d\n  Github: https://github.com/YunzeMan/Lexicon3D", "summary": "Complex 3D scene understanding has gained increasing attention, with scene\nencoding strategies playing a crucial role in this success. However, the\noptimal scene encoding strategies for various scenarios remain unclear,\nparticularly compared to their image-based counterparts. To address this issue,\nwe present a comprehensive study that probes various visual encoding models for\n3D scene understanding, identifying the strengths and limitations of each model\nacross different scenarios. Our evaluation spans seven vision foundation\nencoders, including image-based, video-based, and 3D foundation models. We\nevaluate these models in four tasks: Vision-Language Scene Reasoning, Visual\nGrounding, Segmentation, and Registration, each focusing on different aspects\nof scene understanding. Our evaluations yield key findings: DINOv2 demonstrates\nsuperior performance, video models excel in object-level tasks, diffusion\nmodels benefit geometric tasks, and language-pretrained models show unexpected\nlimitations in language-related tasks. These insights challenge some\nconventional understandings, provide novel perspectives on leveraging visual\nfoundation models, and highlight the need for more flexible encoder selection\nin future vision-language and scene-understanding tasks. Code:\nhttps://github.com/YunzeMan/Lexicon3D"}
{"id": "2505.05168", "pdf": "https://arxiv.org/pdf/2505.05168", "abs": "https://arxiv.org/abs/2505.05168", "authors": ["M. D. Ruiz-Medina", "A. Torres--Signes"], "title": "Local linear Fréchet curve regression in manifolds", "categories": ["math.ST", "cs.LG", "stat.ML", "stat.TH"], "comment": null, "summary": "Global Fr\\'echet functional regression has been recently addressed from time\ncorrelated bivariate curve data evaluated in a manifold (see Torres et al.\n2025). For this type of curve data sets, the present paper solves the problem\nof local linear approximation of the Fr\\'echet conditional mean in an extrinsic\nand intrinsic way. The extrinsic local linear Fr\\'echet functional regression\npredictor is obtained in the time varying tangent space by projection into an\northornormal basis of the ambient Hilbert space. The conditions assumed ensure\nthe existence and uniqueness of this predictor, and its computation via\nexponential and logarithmic maps. A weighted Fr\\'echet mean approach is adopted\nin the computation of an intrinsic local linear Fr\\'echet functional regression\npredictor. The asymptotic optimality of this intrinsic local approximation is\nalso proved. The performance of the empirical version of both, extrinsic and\nintrinsic functional predictors, and of a Nadaraya-Watson type Fr\\'echet curve\npredictor is illustrated in the simulation study undertaken. The finite-sample\nsize properties are also tested in a real-data application via\ncross-validation. Specifically, functional prediction of the magnetic vector\nfield from the time-varying geocentric latitude and longitude of the satellite\nNASA's MAGSAT spacecraft is addressed."}
{"id": "2408.05956", "pdf": "https://arxiv.org/pdf/2408.05956", "abs": "https://arxiv.org/abs/2408.05956", "authors": ["Tianhang Pan", "Xiuyi Jia"], "title": "Boosting Adverse Weather Crowd Counting via Multi-queue Contrastive Learning", "categories": ["cs.CV"], "comment": "8 pages, 5 figures", "summary": "Currently, most crowd counting methods have outstanding performance under\nnormal weather conditions. However, our experimental validation reveals two key\nobstacles limiting the accuracy improvement of crowd counting models: 1) the\ndomain gap between the adverse weather and the normal weather images; 2) the\nweather class imbalance in the training set. To address the problems, we\npropose a two-stage crowd counting method named Multi-queue Contrastive\nLearning (MQCL). Specifically, in the first stage, our target is to equip the\nbackbone network with weather-awareness capabilities. In this process, a\ncontrastive learning method named multi-queue MoCo designed by us is employed\nto enable representation learning under weather class imbalance. After the\nfirst stage is completed, the backbone model is \"mature\" enough to extract\nweather-related representations. On this basis, we proceed to the second stage,\nin which we propose to refine the representations under the guidance of\ncontrastive learning, enabling the conversion of the weather-aware\nrepresentations to the normal weather domain. Through such representation and\nconversion, the model achieves robust counting performance under both normal\nand adverse weather conditions. Extensive experimental results show that,\ncompared to the baseline, MQCL reduces the counting error under adverse weather\nconditions by 22%, while introducing only about 13% increase in computational\nburden, which achieves state-of-the-art performance."}
{"id": "2408.16021", "pdf": "https://arxiv.org/pdf/2408.16021", "abs": "https://arxiv.org/abs/2408.16021", "authors": ["Yasir Ali Farrukh", "Syed Wali", "Irfan Khan", "Nathaniel D. Bastian"], "title": "XG-NID: Dual-Modality Network Intrusion Detection using a Heterogeneous Graph Neural Network and Large Language Model", "categories": ["cs.CR", "cs.AI", "cs.LG"], "comment": "19 pages, 6 figures", "summary": "In the rapidly evolving field of cybersecurity, the integration of flow-level\nand packet-level information for real-time intrusion detection remains a\nlargely untapped area of research. This paper introduces \"XG-NID,\" a novel\nframework that, to the best of our knowledge, is the first to fuse flow-level\nand packet-level data within a heterogeneous graph structure, offering a\ncomprehensive analysis of network traffic. Leveraging a heterogeneous graph\nneural network (GNN) with graph-level classification, XG-NID uniquely enables\nreal-time inference while effectively capturing the intricate relationships\nbetween flow and packet payload data. Unlike traditional GNN-based\nmethodologies that predominantly analyze historical data, XG-NID is designed to\naccommodate the heterogeneous nature of network traffic, providing a robust and\nreal-time defense mechanism. Our framework extends beyond mere classification;\nit integrates Large Language Models (LLMs) to generate detailed, human-readable\nexplanations and suggest potential remedial actions, ensuring that the insights\nproduced are both actionable and comprehensible. Additionally, we introduce a\nnew set of flow features based on temporal information, further enhancing the\ncontextual and explainable inferences provided by our model. To facilitate\npractical application and accessibility, we developed \"GNN4ID,\" an open-source\ntool that enables the extraction and transformation of raw network traffic into\nthe proposed heterogeneous graph structure, seamlessly integrating flow and\npacket-level data. Our comprehensive quantitative comparative analysis\ndemonstrates that XG-NID achieves an F1 score of 97\\% in multi-class\nclassification, outperforming existing baseline and state-of-the-art methods.\nThis sets a new standard in Network Intrusion Detection Systems by combining\ninnovative data fusion with enhanced interpretability and real-time\ncapabilities."}
{"id": "2411.08884", "pdf": "https://arxiv.org/pdf/2411.08884", "abs": "https://arxiv.org/abs/2411.08884", "authors": ["Yifan Zeng", "Liang Kairong", "Fangzhou Dong", "Peijia Zheng"], "title": "Quantifying Risk Propensities of Large Language Models: Ethical Focus and Bias Detection through Role-Play", "categories": ["cs.CY", "cs.AI", "cs.CL"], "comment": "Accepted by CogSci 2025", "summary": "As Large Language Models (LLMs) become more prevalent, concerns about their\nsafety, ethics, and potential biases have risen. Systematically evaluating\nLLMs' risk decision-making tendencies and attitudes, particularly in the\nethical domain, has become crucial. This study innovatively applies the\nDomain-Specific Risk-Taking (DOSPERT) scale from cognitive science to LLMs and\nproposes a novel Ethical Decision-Making Risk Attitude Scale (EDRAS) to assess\nLLMs' ethical risk attitudes in depth. We further propose a novel approach\nintegrating risk scales and role-playing to quantitatively evaluate systematic\nbiases in LLMs. Through systematic evaluation and analysis of multiple\nmainstream LLMs, we assessed the \"risk personalities\" of LLMs across multiple\ndomains, with a particular focus on the ethical domain, and revealed and\nquantified LLMs' systematic biases towards different groups. This research\nhelps understand LLMs' risk decision-making and ensure their safe and reliable\napplication. Our approach provides a tool for identifying and mitigating\nbiases, contributing to fairer and more trustworthy AI systems. The code and\ndata are available."}
{"id": "2505.05261", "pdf": "https://arxiv.org/pdf/2505.05261", "abs": "https://arxiv.org/abs/2505.05261", "authors": ["Yu Liu", "Fabricio Oliveira"], "title": "ICNN-enhanced 2SP: Leveraging input convex neural networks for solving two-stage stochastic programming", "categories": ["math.OC", "cs.LG"], "comment": null, "summary": "Two-stage stochastic programming (2SP) offers a basic framework for modelling\ndecision-making under uncertainty, yet scalability remains a challenge due to\nthe computational complexity of recourse function evaluation. Existing\nlearning-based methods like Neural Two-Stage Stochastic Programming (Neur2SP)\nemploy neural networks (NNs) as recourse function surrogates but rely on\ncomputationally intensive mixed-integer programming (MIP) formulations. We\npropose ICNN-enhanced 2SP, a method that leverages Input Convex Neural Networks\n(ICNNs) to exploit linear programming (LP) representability in convex 2SP\nproblems. By architecturally enforcing convexity and enabling exact inference\nthrough LP, our approach eliminates the need for integer variables inherent to\nthe conventional MIP-based formulation while retaining an exact embedding of\nthe ICNN surrogate within the 2SP framework. This results in a more\ncomputationally efficient alternative that maintains solution quality.\nComprehensive experiments reveal that ICNNs incur only marginally longer\ntraining times while achieving validation accuracy on par with their MIP-based\ncounterparts. Across benchmark problems, ICNN-enhanced 2SP often exhibits\nconsiderably faster solution times than the MIP-based formulations while\npreserving solution quality, with these advantages becoming significantly more\npronounced as problem scale increases. For the most challenging instances, the\nmethod achieves speedups of up to 100$\\times$ and solution quality superior to\nMIP-based formulations."}
{"id": "2408.14348", "pdf": "https://arxiv.org/pdf/2408.14348", "abs": "https://arxiv.org/abs/2408.14348", "authors": ["Peggy A. Bevan", "Omiros Pantazis", "Holly Pringle", "Guilherme Braga Ferreira", "Daniel J. Ingram", "Emily Madsen", "Liam Thomas", "Dol Raj Thanet", "Thakur Silwal", "Santosh Rayamajhi", "Gabriel Brostow", "Oisin Mac Aodha", "Kate E. Jones"], "title": "Deep learning-based ecological analysis of camera trap images is impacted by training data quality and quantity", "categories": ["cs.CV"], "comment": "*equally contributing authors", "summary": "Large image collections generated from camera traps offer valuable insights\ninto species richness, occupancy, and activity patterns, significantly aiding\nbiodiversity monitoring. However, the manual processing of these datasets is\ntime-consuming, hindering analytical processes. To address this, deep neural\nnetworks have been adopted to automate image labelling, but the impact of\nclassification error on ecological metrics remains unclear. Here, we analyse\ndata from camera trap collections in an African savannah (82,300 images, 47\nspecies) and an Asian sub-tropical dry forest (40,308 images, 29 species) to\ncompare ecological metrics derived from expert-generated species\nidentifications with those generated by deep learning classification models. We\nspecifically assess the impact of deep learning model architecture, the\nproportion of label noise in the training data, and the size of the training\ndataset on three ecological metrics: species richness, occupancy, and activity\npatterns. Overall, ecological metrics derived from deep neural networks closely\nmatch those calculated from expert labels and remain robust to manipulations in\nthe training pipeline. We found that the choice of deep learning model\narchitecture does not impact ecological metrics, and ecological metrics related\nto the overall community (species richness, community occupancy) were resilient\nto up to 10% noise in the training dataset and a 50% reduction in the training\ndataset size. However, we caution that less common species are\ndisproportionately affected by a reduction in deep neural network accuracy, and\nthis has consequences for species-specific metrics (occupancy, diel activity\npatterns). To ensure the reliability of their findings, practitioners should\nprioritize creating large, clean training sets with balanced representation\nacross species over exploring numerous deep learning model architectures."}
{"id": "2409.10297", "pdf": "https://arxiv.org/pdf/2409.10297", "abs": "https://arxiv.org/abs/2409.10297", "authors": ["Blaine Hoak", "Patrick McDaniel"], "title": "On Synthetic Texture Datasets: Challenges, Creation, and Curation", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "The influence of textures on machine learning models has been an ongoing\ninvestigation, specifically in texture bias/learning, interpretability, and\nrobustness. However, due to the lack of large and diverse texture data\navailable, the findings in these works have been limited, as more comprehensive\nevaluations have not been feasible. Image generative models are able to provide\ndata creation at scale, but utilizing these models for texture synthesis has\nbeen unexplored and poses additional challenges both in creating accurate\ntexture images and validating those images. In this work, we introduce an\nextensible methodology and corresponding new dataset for generating\nhigh-quality, diverse texture images capable of supporting a broad set of\ntexture-based tasks. Our pipeline consists of: (1) developing prompts from a\nrange of descriptors to serve as input to text-to-image models, (2) adopting\nand adapting Stable Diffusion pipelines to generate and filter the\ncorresponding images, and (3) further filtering down to the highest quality\nimages. Through this, we create the Prompted Textures Dataset (PTD), a dataset\nof 362,880 texture images that span 56 textures. During the process of\ngenerating images, we find that NSFW safety filters in image generation\npipelines are highly sensitive to texture (and flag up to 60\\% of our texture\nimages), uncovering a potential bias in these models and presenting unique\nchallenges when working with texture data. Through both standard metrics and a\nhuman evaluation, we find that our dataset is high quality and diverse. Our\ndataset is available for download at https://zenodo.org/records/15359142."}
{"id": "2502.18635", "pdf": "https://arxiv.org/pdf/2502.18635", "abs": "https://arxiv.org/abs/2502.18635", "authors": ["Matthew Barker", "Andrew Bell", "Evan Thomas", "James Carr", "Thomas Andrews", "Umang Bhatt"], "title": "Faster, Cheaper, Better: Multi-Objective Hyperparameter Optimization for LLM and RAG Systems", "categories": ["cs.LG", "cs.AI", "cs.CL", "68T20, 68Q32, 90C29, 62P30", "I.2.6; I.2.7; G.1.6; G.3"], "comment": null, "summary": "While Retrieval Augmented Generation (RAG) has emerged as a popular technique\nfor improving Large Language Model (LLM) systems, it introduces a large number\nof choices, parameters and hyperparameters that must be made or tuned. This\nincludes the LLM, embedding, and ranker models themselves, as well as\nhyperparameters governing individual RAG components. Yet, collectively\noptimizing the entire configuration in a RAG or LLM system remains\nunder-explored - especially in multi-objective settings - due to intractably\nlarge solution spaces, noisy objective evaluations, and the high cost of\nevaluations. In this work, we introduce the first approach for multi-objective\nparameter optimization of cost, latency, safety and alignment over entire LLM\nand RAG systems. We find that Bayesian optimization methods significantly\noutperform baseline approaches, obtaining a superior Pareto front on two new\nRAG benchmark tasks. We conclude our work with important considerations for\npractitioners who are designing multi-objective RAG systems, highlighting\nnuances such as how optimal configurations may not generalize across tasks and\nobjectives."}
{"id": "2505.05269", "pdf": "https://arxiv.org/pdf/2505.05269", "abs": "https://arxiv.org/abs/2505.05269", "authors": ["Jingbin Xu", "Chen Qian", "Meimei Liu", "Feng Guo"], "title": "A Two-Sample Test of Text Generation Similarity", "categories": ["stat.ML", "cs.LG"], "comment": null, "summary": "The surge in digitized text data requires reliable inferential methods on\nobserved textual patterns. This article proposes a novel two-sample text test\nfor comparing similarity between two groups of documents. The hypothesis is\nwhether the probabilistic mapping generating the textual data is identical\nacross two groups of documents. The proposed test aims to assess text\nsimilarity by comparing the entropy of the documents. Entropy is estimated\nusing neural network-based language models. The test statistic is derived from\nan estimation-and-inference framework, where the entropy is first approximated\nusing an estimation set, followed by inference on the remaining data set. We\nshowed theoretically that under mild conditions, the test statistic\nasymptotically follows a normal distribution. A multiple data-splitting\nstrategy is proposed to enhance test power, which combines p-values into a\nunified decision. Various simulation studies and a real data example\ndemonstrated that the proposed two-sample text test maintains the nominal Type\none error rate while offering greater power compared to existing methods. The\nproposed method provides a novel solution to assert differences in document\nclasses, particularly in fields where large-scale textual information is\ncrucial."}
{"id": "2408.15994", "pdf": "https://arxiv.org/pdf/2408.15994", "abs": "https://arxiv.org/abs/2408.15994", "authors": ["Xu Zhang", "Jiaqi Ma", "Guoli Wang", "Qian Zhang", "Huan Zhang", "Lefei Zhang"], "title": "Perceive-IR: Learning to Perceive Degradation Better for All-in-One Image Restoration", "categories": ["cs.CV"], "comment": "Accepted by IEEE Transactions on Image Processing. Project page at\n  https://house-yuyu.github.io/Perceive-IR/", "summary": "Existing All-in-One image restoration methods often fail to perceive\ndegradation types and severity levels simultaneously, overlooking the\nimportance of fine-grained quality perception. Moreover, these methods often\nutilize highly customized backbones, which hinder their adaptability and\nintegration into more advanced restoration networks. To address these\nlimitations, we propose Perceive-IR, a novel backbone-agnostic All-in-One image\nrestoration framework designed for fine-grained quality control across various\ndegradation types and severity levels. Its modular structure allows core\ncomponents to function independently of specific backbones, enabling seamless\nintegration into advanced restoration models without significant modifications.\nSpecifically, Perceive-IR operates in two key stages: 1) multi-level\nquality-driven prompt learning stage, where a fine-grained quality perceiver is\nmeticulously trained to discern three tier quality levels by optimizing the\nalignment between prompts and images within the CLIP perception space. This\nstage ensures a nuanced understanding of image quality, laying the groundwork\nfor subsequent restoration; 2) restoration stage, where the quality perceiver\nis seamlessly integrated with a difficulty-adaptive perceptual loss, forming a\nquality-aware learning strategy. This strategy not only dynamically\ndifferentiates sample learning difficulty but also achieves fine-grained\nquality control by driving the restored image toward the ground truth while\npulling it away from both low- and medium-quality samples."}
{"id": "2409.11686", "pdf": "https://arxiv.org/pdf/2409.11686", "abs": "https://arxiv.org/abs/2409.11686", "authors": ["Asad Aali", "Andrew Johnston", "Louis Blankemeier", "Dave Van Veen", "Laura T Derry", "David Svec", "Jason Hom", "Robert D. Boutin", "Akshay S. Chaudhari"], "title": "Automated detection of underdiagnosed medical conditions via opportunistic imaging", "categories": ["cs.CV", "cs.AI", "cs.LG"], "comment": null, "summary": "Abdominal computed tomography (CT) scans are frequently performed in clinical\nsettings. Opportunistic CT involves repurposing routine CT images to extract\ndiagnostic information and is an emerging tool for detecting underdiagnosed\nconditions such as sarcopenia, hepatic steatosis, and ascites. This study\nutilizes deep learning methods to promote accurate diagnosis and clinical\ndocumentation. We analyze 2,674 inpatient CT scans to identify discrepancies\nbetween imaging phenotypes (characteristics derived from opportunistic CT\nscans) and their corresponding documentation in radiology reports and ICD\ncoding. Through our analysis, we find that only 0.5%, 3.2%, and 30.7% of scans\ndiagnosed with sarcopenia, hepatic steatosis, and ascites (respectively)\nthrough either opportunistic imaging or radiology reports were ICD-coded. Our\nfindings demonstrate opportunistic CT's potential to enhance diagnostic\nprecision and accuracy of risk adjustment models, offering advancements in\nprecision medicine."}
{"id": "2502.20170", "pdf": "https://arxiv.org/pdf/2502.20170", "abs": "https://arxiv.org/abs/2502.20170", "authors": ["Siqi Liu", "Ian Gemp", "Luke Marris", "Georgios Piliouras", "Nicolas Heess", "Marc Lanctot"], "title": "Re-evaluating Open-ended Evaluation of Large Language Models", "categories": ["cs.GT", "cs.CL", "cs.LG", "stat.ML"], "comment": "Published at ICLR 2025", "summary": "Evaluation has traditionally focused on ranking candidates for a specific\nskill. Modern generalist models, such as Large Language Models (LLMs),\ndecidedly outpace this paradigm. Open-ended evaluation systems, where candidate\nmodels are compared on user-submitted prompts, have emerged as a popular\nsolution. Despite their many advantages, we show that the current Elo-based\nrating systems can be susceptible to and even reinforce biases in data,\nintentional or accidental, due to their sensitivity to redundancies. To address\nthis issue, we propose evaluation as a 3-player game, and introduce novel\ngame-theoretic solution concepts to ensure robustness to redundancy. We show\nthat our method leads to intuitive ratings and provide insights into the\ncompetitive landscape of LLM development."}
{"id": "2505.05273", "pdf": "https://arxiv.org/pdf/2505.05273", "abs": "https://arxiv.org/abs/2505.05273", "authors": ["Alexander Soen"], "title": "A Connection Between Learning to Reject and Bhattacharyya Divergences", "categories": ["stat.ML", "cs.IT", "cs.LG", "math.IT"], "comment": null, "summary": "Learning to reject provide a learning paradigm which allows for our models to\nabstain from making predictions. One way to learn the rejector is to learn an\nideal marginal distribution (w.r.t. the input domain) - which characterizes a\nhypothetical best marginal distribution - and compares it to the true marginal\ndistribution via a density ratio. In this paper, we consider learning a joint\nideal distribution over both inputs and labels; and develop a link between\nrejection and thresholding different statistical divergences. We further find\nthat when one considers a variant of the log-loss, the rejector obtained by\nconsidering the joint ideal distribution corresponds to the thresholding of the\nskewed Bhattacharyya divergence between class-probabilities. This is in\ncontrast to the marginal case - that is equivalent to a typical\ncharacterization of optimal rejection, Chow's Rule - which corresponds to a\nthresholding of the Kullback-Leibler divergence. In general, we find that\nrejecting via a Bhattacharyya divergence is less aggressive than Chow's Rule."}
{"id": "2409.07967", "pdf": "https://arxiv.org/pdf/2409.07967", "abs": "https://arxiv.org/abs/2409.07967", "authors": ["Ling Xing", "Hongyu Qu", "Rui Yan", "Xiangbo Shu", "Jinhui Tang"], "title": "Locality-aware Cross-modal Correspondence Learning for Dense Audio-Visual Events Localization", "categories": ["cs.CV"], "comment": null, "summary": "Dense-localization Audio-Visual Events (DAVE) aims to identify time\nboundaries and corresponding categories for events that are both audible and\nvisible in a long video, where events may co-occur and exhibit varying\ndurations. However, complex audio-visual scenes often involve asynchronization\nbetween modalities, making accurate localization challenging. Existing DAVE\nsolutions extract audio and visual features through unimodal encoders, and fuse\nthem via dense cross-modal interaction. However, independent unimodal encoding\nstruggles to emphasize shared semantics between modalities without cross-modal\nguidance, while dense cross-modal attention may over-attend to semantically\nunrelated audio-visual features. To address these problems, we present LoCo, a\nLocality-aware cross-modal Correspondence learning framework for DAVE. LoCo\nleverages the local temporal continuity of audio-visual events as important\nguidance to filter irrelevant cross-modal signals and enhance cross-modal\nalignment throughout both unimodal and cross-modal encoding stages. i)\nSpecifically, LoCo applies Local Correspondence Feature (LCF) Modulation to\nenforce unimodal encoders to focus on modality-shared semantics by modulating\nagreement between audio and visual features based on local cross-modal\ncoherence. ii) To better aggregate cross-modal relevant features, we further\ncustomize Local Adaptive Cross-modal (LAC) Interaction, which dynamically\nadjusts attention regions in a data-driven manner. This adaptive mechanism\nfocuses attention on local event boundaries and accommodates varying event\ndurations. By incorporating LCF and LAC, LoCo provides solid performance gains\nand outperforms existing DAVE methods."}
{"id": "2409.13138", "pdf": "https://arxiv.org/pdf/2409.13138", "abs": "https://arxiv.org/abs/2409.13138", "authors": ["Yunsheng Bai", "Atefeh Sohrabizadeh", "Zijian Ding", "Rongjian Liang", "Weikai Li", "Ding Wang", "Haoxing Ren", "Yizhou Sun", "Jason Cong"], "title": "Learning to Compare Hardware Designs for High-Level Synthesis", "categories": ["cs.LG", "cs.AI", "cs.AR"], "comment": "Published in MLCAD 2024", "summary": "High-level synthesis (HLS) is an automated design process that transforms\nhigh-level code into hardware designs, enabling the rapid development of\nhardware accelerators. HLS relies on pragmas, which are directives inserted\ninto the source code to guide the synthesis process, and pragmas have various\nsettings and values that significantly impact the resulting hardware design.\nState-of-the-art ML-based HLS methods, such as HARP, first train a deep\nlearning model, typically based on graph neural networks (GNNs) applied to\ngraph-based representations of the source code and pragmas. They then perform\ndesign space exploration (DSE) to explore the pragma design space, rank\ncandidate designs using the model, and return the top designs. However,\ntraditional DSE methods face challenges due to the highly nonlinear\nrelationship between pragma settings and performance metrics, along with\ncomplex interactions between pragmas that affect performance in non-obvious\nways.\n  To address these challenges, we propose compareXplore, a novel approach that\nlearns to compare hardware designs for effective HLS optimization.\nCompareXplore introduces a hybrid loss function that combines pairwise\npreference learning with pointwise performance prediction, enabling the model\nto capture both relative preferences and absolute performance. Moreover, we\nintroduce a novel node difference attention module that focuses on the most\ninformative differences between designs, enabling the model to identify\ncritical pragmas impacting performance. CompareXplore adopts a two-stage DSE,\nwhere a pointwise prediction model is used for the initial design pruning,\nfollowed by a pairwise comparison stage for precise performance verification.\nIn extensive experiments, compareXplore achieves significant improvements in\nranking metrics and generates high-quality HLS results for the selected\ndesigns, outperforming the existing SOTA method."}
{"id": "2504.02107", "pdf": "https://arxiv.org/pdf/2504.02107", "abs": "https://arxiv.org/abs/2504.02107", "authors": ["Jeffrey Li", "Mohammadreza Armandpour", "Iman Mirzadeh", "Sachin Mehta", "Vaishaal Shankar", "Raviteja Vemulapalli", "Samy Bengio", "Oncel Tuzel", "Mehrdad Farajtabar", "Hadi Pouransari", "Fartash Faghri"], "title": "TiC-LM: A Web-Scale Benchmark for Time-Continual LLM Pretraining", "categories": ["cs.LG", "cs.CL"], "comment": "Code available at: https://github.com/apple/ml-tic-lm", "summary": "Large Language Models (LLMs) trained on historical web data inevitably become\noutdated. We investigate evaluation strategies and update methods for LLMs as\nnew data becomes available. We introduce a web-scale dataset for time-continual\npretraining of LLMs derived from 114 dumps of Common Crawl (CC) - orders of\nmagnitude larger than previous continual language modeling benchmarks. We also\ndesign time-stratified evaluations across both general CC data and specific\ndomains (Wikipedia, StackExchange, and code documentation) to assess how well\nvarious continual learning methods adapt to new data while retaining past\nknowledge. Our findings demonstrate that, on general CC data, autoregressive\nmeta-schedules combined with a fixed-ratio replay of older data can achieve\ncomparable held-out loss to re-training from scratch, while requiring\nsignificantly less computation (2.6x). However, the optimal balance between\nincorporating new data and replaying old data differs as replay is crucial to\navoid forgetting on generic web data but less so on specific domains."}
{"id": "2505.05287", "pdf": "https://arxiv.org/pdf/2505.05287", "abs": "https://arxiv.org/abs/2505.05287", "authors": ["Zechu Li", "Yufeng Jin", "Daniel Ordonez Apraez", "Claudio Semini", "Puze Liu", "Georgia Chalvatzaki"], "title": "Morphologically Symmetric Reinforcement Learning for Ambidextrous Bimanual Manipulation", "categories": ["cs.RO", "cs.LG"], "comment": null, "summary": "Humans naturally exhibit bilateral symmetry in their gross manipulation\nskills, effortlessly mirroring simple actions between left and right hands.\nBimanual robots-which also feature bilateral symmetry-should similarly exploit\nthis property to perform tasks with either hand. Unlike humans, who often favor\na dominant hand for fine dexterous skills, robots should ideally execute\nambidextrous manipulation with equal proficiency. To this end, we introduce\nSYMDEX (SYMmetric DEXterity), a reinforcement learning framework for\nambidextrous bi-manipulation that leverages the robot's inherent bilateral\nsymmetry as an inductive bias. SYMDEX decomposes complex bimanual manipulation\ntasks into per-hand subtasks and trains dedicated policies for each. By\nexploiting bilateral symmetry via equivariant neural networks, experience from\none arm is inherently leveraged by the opposite arm. We then distill the\nsubtask policies into a global ambidextrous policy that is independent of the\nhand-task assignment. We evaluate SYMDEX on six challenging simulated\nmanipulation tasks and demonstrate successful real-world deployment on two of\nthem. Our approach strongly outperforms baselines on complex task in which the\nleft and right hands perform different roles. We further demonstrate SYMDEX's\nscalability by extending it to a four-arm manipulation setup, where our\nsymmetry-aware policies enable effective multi-arm collaboration and\ncoordination. Our results highlight how structural symmetry as inductive bias\nin policy learning enhances sample efficiency, robustness, and generalization\nacross diverse dexterous manipulation tasks."}
{"id": "2410.03577", "pdf": "https://arxiv.org/pdf/2410.03577", "abs": "https://arxiv.org/abs/2410.03577", "authors": ["Xin Zou", "Yizhou Wang", "Yibo Yan", "Yuanhuiyi Lyu", "Kening Zheng", "Sirui Huang", "Junkai Chen", "Peijie Jiang", "Jia Liu", "Chang Tang", "Xuming Hu"], "title": "Look Twice Before You Answer: Memory-Space Visual Retracing for Hallucination Mitigation in Multimodal Large Language Models", "categories": ["cs.CV"], "comment": "Accepted by ICML 2025", "summary": "Despite their impressive capabilities, multimodal large language models\n(MLLMs) are prone to hallucinations, i.e., the generated content that is\nnonsensical or unfaithful to input sources. Unlike in LLMs, hallucinations in\nMLLMs often stem from the sensitivity of text decoder to visual tokens, leading\nto a phenomenon akin to \"amnesia\" about visual information. To address this\nissue, we propose MemVR, a novel decoding paradigm inspired by common\ncognition: when the memory of an image seen the moment before is forgotten,\npeople will look at it again for factual answers. Following this principle, we\ntreat visual tokens as supplementary evidence, re-injecting them into the MLLM\nthrough Feed Forward Network (FFN) as \"key-value memory\" at the middle trigger\nlayer. This \"look-twice\" mechanism occurs when the model exhibits high\nuncertainty during inference, effectively enhancing factual alignment.\nComprehensive experimental evaluations demonstrate that MemVR significantly\nmitigates hallucination across various MLLMs and excels in general benchmarks\nwithout incurring additional time overhead. The implementation is available\nfrom https://github.com/1zhou-Wang/MemVR"}
{"id": "2410.12261", "pdf": "https://arxiv.org/pdf/2410.12261", "abs": "https://arxiv.org/abs/2410.12261", "authors": ["Xingjian Wu", "Xiangfei Qiu", "Zhengyu Li", "Yihang Wang", "Jilin Hu", "Chenjuan Guo", "Hui Xiong", "Bin Yang"], "title": "CATCH: Channel-Aware multivariate Time Series Anomaly Detection via Frequency Patching", "categories": ["cs.LG", "cs.AI"], "comment": "Accepted by ICLR 2025", "summary": "Anomaly detection in multivariate time series is challenging as heterogeneous\nsubsequence anomalies may occur. Reconstruction-based methods, which focus on\nlearning normal patterns in the frequency domain to detect diverse abnormal\nsubsequences, achieve promising results, while still falling short on capturing\nfine-grained frequency characteristics and channel correlations. To contend\nwith the limitations, we introduce CATCH, a framework based on frequency\npatching. We propose to patchify the frequency domain into frequency bands,\nwhich enhances its ability to capture fine-grained frequency characteristics.\nTo perceive appropriate channel correlations, we propose a Channel Fusion\nModule (CFM), which features a patch-wise mask generator and a masked-attention\nmechanism. Driven by a bi-level multi-objective optimization algorithm, the CFM\nis encouraged to iteratively discover appropriate patch-wise channel\ncorrelations, and to cluster relevant channels while isolating adverse effects\nfrom irrelevant channels. Extensive experiments on 10 real-world datasets and\n12 synthetic datasets demonstrate that CATCH achieves state-of-the-art\nperformance. We make our code and datasets available at\nhttps://github.com/decisionintelligence/CATCH."}
{"id": "2504.19981", "pdf": "https://arxiv.org/pdf/2504.19981", "abs": "https://arxiv.org/abs/2504.19981", "authors": ["Adam Younsi", "Abdalgader Abubaker", "Mohamed El Amine Seddik", "Hakim Hacid", "Salem Lahlou"], "title": "Accurate and Diverse LLM Mathematical Reasoning via Automated PRM-Guided GFlowNets", "categories": ["cs.LG", "cs.CL"], "comment": null, "summary": "Achieving both accuracy and diverse reasoning remains challenging for Large\nLanguage Models (LLMs) in complex domains like mathematics. A key bottleneck is\nevaluating intermediate reasoning steps to guide generation without costly\nhuman annotations. To address this, we first introduce a novel Process Reward\nModel (PRM) trained automatically using Monte Carlo Tree Search coupled with a\nsimilarity-based data augmentation technique, effectively capturing step-level\nreasoning quality. Leveraging this PRM, we then adapt Generative Flow Networks\n(GFlowNets) to operate at the reasoning step level. Unlike traditional\nreinforcement learning focused on maximizing a single reward, GFlowNets\nnaturally sample diverse, high-quality solutions proportional to their rewards,\nas measured by our PRM. Empirical evaluation shows strong improvements in both\naccuracy and solution diversity on challenging mathematical benchmarks (e.g.,\n+2.59% absolute accuracy on MATH Level 5 for Llama3.2-3B), with effective\ngeneralization to unseen datasets (+9.4% absolute on SAT MATH). Our work\ndemonstrates the potential of PRM-guided, step-level GFlowNets for developing\nmore robust and versatile mathematical reasoning in LLMs."}
{"id": "2505.05301", "pdf": "https://arxiv.org/pdf/2505.05301", "abs": "https://arxiv.org/abs/2505.05301", "authors": ["Jiaqi Leng", "Zhiyan Ding", "Zherui Chen", "Lin Lin"], "title": "Operator-Level Quantum Acceleration of Non-Logconcave Sampling", "categories": ["quant-ph", "cs.LG", "math.OC"], "comment": "43 pages, 7 figures", "summary": "Sampling from probability distributions of the form $\\sigma \\propto e^{-\\beta\nV}$, where $V$ is a continuous potential, is a fundamental task across physics,\nchemistry, biology, computer science, and statistics. However, when $V$ is\nnon-convex, the resulting distribution becomes non-logconcave, and classical\nmethods such as Langevin dynamics often exhibit poor performance. We introduce\nthe first quantum algorithm that provably accelerates a broad class of\ncontinuous-time sampling dynamics. For Langevin dynamics, our method encodes\nthe target Gibbs measure into the amplitudes of a quantum state, identified as\nthe kernel of a block matrix derived from a factorization of the Witten\nLaplacian operator. This connection enables Gibbs sampling via singular value\nthresholding and yields the first provable quantum advantage with respect to\nthe Poincar\\'e constant in the non-logconcave setting. Building on this\nframework, we further develop the first quantum algorithm that accelerates\nreplica exchange Langevin diffusion, a widely used method for sampling from\ncomplex, rugged energy landscapes."}
{"id": "2410.03825", "pdf": "https://arxiv.org/pdf/2410.03825", "abs": "https://arxiv.org/abs/2410.03825", "authors": ["Junyi Zhang", "Charles Herrmann", "Junhwa Hur", "Varun Jampani", "Trevor Darrell", "Forrester Cole", "Deqing Sun", "Ming-Hsuan Yang"], "title": "MonST3R: A Simple Approach for Estimating Geometry in the Presence of Motion", "categories": ["cs.CV"], "comment": "Accepted by ICLR 25, Project page: https://monst3r-project.github.io/", "summary": "Estimating geometry from dynamic scenes, where objects move and deform over\ntime, remains a core challenge in computer vision. Current approaches often\nrely on multi-stage pipelines or global optimizations that decompose the\nproblem into subtasks, like depth and flow, leading to complex systems prone to\nerrors. In this paper, we present Motion DUSt3R (MonST3R), a novel\ngeometry-first approach that directly estimates per-timestep geometry from\ndynamic scenes. Our key insight is that by simply estimating a pointmap for\neach timestep, we can effectively adapt DUST3R's representation, previously\nonly used for static scenes, to dynamic scenes. However, this approach presents\na significant challenge: the scarcity of suitable training data, namely\ndynamic, posed videos with depth labels. Despite this, we show that by posing\nthe problem as a fine-tuning task, identifying several suitable datasets, and\nstrategically training the model on this limited data, we can surprisingly\nenable the model to handle dynamics, even without an explicit motion\nrepresentation. Based on this, we introduce new optimizations for several\ndownstream video-specific tasks and demonstrate strong performance on video\ndepth and camera pose estimation, outperforming prior work in terms of\nrobustness and efficiency. Moreover, MonST3R shows promising results for\nprimarily feed-forward 4D reconstruction."}
{"id": "2410.15236", "pdf": "https://arxiv.org/pdf/2410.15236", "abs": "https://arxiv.org/abs/2410.15236", "authors": ["Benji Peng", "Keyu Chen", "Qian Niu", "Ziqian Bi", "Ming Liu", "Pohsun Feng", "Tianyang Wang", "Lawrence K. Q. Yan", "Yizhu Wen", "Yichao Zhang", "Caitlyn Heqi Yin"], "title": "Jailbreaking and Mitigation of Vulnerabilities in Large Language Models", "categories": ["cs.CR", "cs.AI", "cs.LG"], "comment": null, "summary": "Large Language Models (LLMs) have transformed artificial intelligence by\nadvancing natural language understanding and generation, enabling applications\nacross fields beyond healthcare, software engineering, and conversational\nsystems. Despite these advancements in the past few years, LLMs have shown\nconsiderable vulnerabilities, particularly to prompt injection and jailbreaking\nattacks. This review analyzes the state of research on these vulnerabilities\nand presents available defense strategies. We roughly categorize attack\napproaches into prompt-based, model-based, multimodal, and multilingual,\ncovering techniques such as adversarial prompting, backdoor injections, and\ncross-modality exploits. We also review various defense mechanisms, including\nprompt filtering, transformation, alignment techniques, multi-agent defenses,\nand self-regulation, evaluating their strengths and shortcomings. We also\ndiscuss key metrics and benchmarks used to assess LLM safety and robustness,\nnoting challenges like the quantification of attack success in interactive\ncontexts and biases in existing datasets. Identifying current research gaps, we\nsuggest future directions for resilient alignment strategies, advanced defenses\nagainst evolving attacks, automation of jailbreak detection, and consideration\nof ethical and societal impacts. This review emphasizes the need for continued\nresearch and cooperation within the AI community to enhance LLM security and\nensure their safe deployment."}
{"id": "2504.21435", "pdf": "https://arxiv.org/pdf/2504.21435", "abs": "https://arxiv.org/abs/2504.21435", "authors": ["Chenkai Zhang", "Yiming Lei", "Zeming Liu", "Haitao Leng", "Shaoguo Liu", "Tingting Gao", "Qingjie Liu", "Yunhong Wang"], "title": "SeriesBench: A Benchmark for Narrative-Driven Drama Series Understanding", "categories": ["cs.CV", "cs.AI", "cs.CL"], "comment": "29 pages, 15 figures, CVPR 2025", "summary": "With the rapid development of Multi-modal Large Language Models (MLLMs), an\nincreasing number of benchmarks have been established to evaluate the video\nunderstanding capabilities of these models. However, these benchmarks focus on\nstandalone videos and mainly assess \"visual elements\" like human actions and\nobject states. In reality, contemporary videos often encompass complex and\ncontinuous narratives, typically presented as a series. To address this\nchallenge, we propose SeriesBench, a benchmark consisting of 105 carefully\ncurated narrative-driven series, covering 28 specialized tasks that require\ndeep narrative understanding. Specifically, we first select a diverse set of\ndrama series spanning various genres. Then, we introduce a novel long-span\nnarrative annotation method, combined with a full-information transformation\napproach to convert manual annotations into diverse task formats. To further\nenhance model capacity for detailed analysis of plot structures and character\nrelationships within series, we propose a novel narrative reasoning framework,\nPC-DCoT. Extensive results on SeriesBench indicate that existing MLLMs still\nface significant challenges in understanding narrative-driven series, while\nPC-DCoT enables these MLLMs to achieve performance improvements. Overall, our\nSeriesBench and PC-DCoT highlight the critical necessity of advancing model\ncapabilities to understand narrative-driven series, guiding the future\ndevelopment of MLLMs. SeriesBench is publicly available at\nhttps://github.com/zackhxn/SeriesBench-CVPR2025."}
{"id": "2505.05371", "pdf": "https://arxiv.org/pdf/2505.05371", "abs": "https://arxiv.org/abs/2505.05371", "authors": ["Niklas Grieger", "Siamak Mehrkanoon", "Philipp Ritter", "Stephan Bialonski"], "title": "From Sleep Staging to Spindle Detection: Evaluating End-to-End Automated Sleep Analysis", "categories": ["eess.SP", "cs.LG", "q-bio.NC"], "comment": "10 pages, 4 figures, 2 tables", "summary": "Automation of sleep analysis, including both macrostructural (sleep stages)\nand microstructural (e.g., sleep spindles) elements, promises to enable\nlarge-scale sleep studies and to reduce variance due to inter-rater\nincongruencies. While individual steps, such as sleep staging and spindle\ndetection, have been studied separately, the feasibility of automating\nmulti-step sleep analysis remains unclear. Here, we evaluate whether a fully\nautomated analysis using state-of-the-art machine learning models for sleep\nstaging (RobustSleepNet) and subsequent spindle detection (SUMOv2) can\nreplicate findings from an expert-based study of bipolar disorder. The\nautomated analysis qualitatively reproduced key findings from the expert-based\nstudy, including significant differences in fast spindle densities between\nbipolar patients and healthy controls, accomplishing in minutes what previously\ntook months to complete manually. While the results of the automated analysis\ndiffered quantitatively from the expert-based study, possibly due to biases\nbetween expert raters or between raters and the models, the models individually\nperformed at or above inter-rater agreement for both sleep staging and spindle\ndetection. Our results demonstrate that fully automated approaches have the\npotential to facilitate large-scale sleep research. We are providing public\naccess to the tools used in our automated analysis by sharing our code and\nintroducing SomnoBot, a privacy-preserving sleep analysis platform."}
{"id": "2410.09049", "pdf": "https://arxiv.org/pdf/2410.09049", "abs": "https://arxiv.org/abs/2410.09049", "authors": ["Xiuyu Yang", "Yunze Man", "Jun-Kun Chen", "Yu-Xiong Wang"], "title": "SceneCraft: Layout-Guided 3D Scene Generation", "categories": ["cs.CV"], "comment": "NeurIPS 2024. Code: https://github.com/OrangeSodahub/SceneCraft\n  Project Page: https://orangesodahub.github.io/SceneCraft", "summary": "The creation of complex 3D scenes tailored to user specifications has been a\ntedious and challenging task with traditional 3D modeling tools. Although some\npioneering methods have achieved automatic text-to-3D generation, they are\ngenerally limited to small-scale scenes with restricted control over the shape\nand texture. We introduce SceneCraft, a novel method for generating detailed\nindoor scenes that adhere to textual descriptions and spatial layout\npreferences provided by users. Central to our method is a rendering-based\ntechnique, which converts 3D semantic layouts into multi-view 2D proxy maps.\nFurthermore, we design a semantic and depth conditioned diffusion model to\ngenerate multi-view images, which are used to learn a neural radiance field\n(NeRF) as the final scene representation. Without the constraints of panorama\nimage generation, we surpass previous methods in supporting complicated indoor\nspace generation beyond a single room, even as complicated as a whole\nmulti-bedroom apartment with irregular shapes and layouts. Through experimental\nanalysis, we demonstrate that our method significantly outperforms existing\napproaches in complex indoor scene generation with diverse textures, consistent\ngeometry, and realistic visual quality. Code and more results are available at:\nhttps://orangesodahub.github.io/SceneCraft"}
{"id": "2412.09507", "pdf": "https://arxiv.org/pdf/2412.09507", "abs": "https://arxiv.org/abs/2412.09507", "authors": ["Rafayel Mkrtchyan", "Edvard Ghukasyan", "Khoren Petrosyan", "Hrant Khachatrian", "Theofanis P. Raptis"], "title": "Vision Transformers for Efficient Indoor Pathloss Radio Map Prediction", "categories": ["cs.CV", "cs.AI", "cs.NI"], "comment": "Work partly supported by the RA Science Committee grant No. 22rl-052\n  (DISTAL) and the EU under Italian National Recovery and Resilience Plan of\n  NextGenerationEU on \"Telecommunications of the Future\" (PE00000001 - program\n  \"RESTART\")", "summary": "Indoor pathloss prediction is a fundamental task in wireless network\nplanning, yet it remains challenging due to environmental complexity and data\nscarcity. In this work, we propose a deep learning-based approach utilizing a\nvision transformer (ViT) architecture with DINO-v2 pretrained weights to model\nindoor radio propagation. Our method processes a floor map with additional\nfeatures of the walls to generate indoor pathloss maps. We systematically\nevaluate the effects of architectural choices, data augmentation strategies,\nand feature engineering techniques. Our findings indicate that extensive\naugmentation significantly improves generalization, while feature engineering\nis crucial in low-data regimes. Through comprehensive experiments, we\ndemonstrate the robustness of our model across different generalization\nscenarios."}
{"id": "2505.00831", "pdf": "https://arxiv.org/pdf/2505.00831", "abs": "https://arxiv.org/abs/2505.00831", "authors": ["Quang P. M. Pham", "Khoi T. N. Nguyen", "Nhi H. Doan", "Cuong A. Pham", "Kentaro Inui", "Dezhen Song"], "title": "SmallPlan: Leverage Small Language Models for Sequential Path Planning with Simulation-Powered, LLM-Guided Distillation", "categories": ["cs.RO", "cs.CL"], "comment": "Paper is under review", "summary": "Efficient path planning in robotics, particularly within large-scale, dynamic\nenvironments, remains a significant hurdle. While Large Language Models (LLMs)\noffer strong reasoning capabilities, their high computational cost and limited\nadaptability in dynamic scenarios hinder real-time deployment on edge devices.\nWe present SmallPlan -- a novel framework leveraging LLMs as teacher models to\ntrain lightweight Small Language Models (SLMs) for high-level path planning\ntasks. In SmallPlan, the SLMs provide optimal action sequences to navigate\nacross scene graphs that compactly represent full-scaled 3D scenes. The SLMs\nare trained in a simulation-powered, interleaved manner with LLM-guided\nsupervised fine-tuning (SFT) and reinforcement learning (RL). This strategy not\nonly enables SLMs to successfully complete navigation tasks but also makes them\naware of important factors like travel distance and number of trials. Through\nexperiments, we demonstrate that the fine-tuned SLMs perform competitively with\nlarger models like GPT-4o on sequential path planning, without suffering from\nhallucination and overfitting. SmallPlan is resource-efficient, making it\nwell-suited for edge-device deployment and advancing practical autonomous\nrobotics."}
{"id": "2505.05404", "pdf": "https://arxiv.org/pdf/2505.05404", "abs": "https://arxiv.org/abs/2505.05404", "authors": ["Michelangelo Domina", "Filippo Bigi", "Paolo Pegolo", "Michele Ceriotti"], "title": "Representing spherical tensors with scalar-based machine-learning models", "categories": ["physics.chem-ph", "cs.LG", "stat.ML"], "comment": null, "summary": "Rotational symmetry plays a central role in physics, providing an elegant\nframework to describe how the properties of 3D objects -- from atoms to the\nmacroscopic scale -- transform under the action of rigid rotations. Equivariant\nmodels of 3D point clouds are able to approximate structure-property relations\nin a way that is fully consistent with the structure of the rotation group, by\ncombining intermediate representations that are themselves spherical tensors.\nThe symmetry constraints however make this approach computationally demanding\nand cumbersome to implement, which motivates increasingly popular unconstrained\narchitectures that learn approximate symmetries as part of the training\nprocess. In this work, we explore a third route to tackle this learning\nproblem, where equivariant functions are expressed as the product of a scalar\nfunction of the point cloud coordinates and a small basis of tensors with the\nappropriate symmetry. We also propose approximations of the general expressions\nthat, while lacking universal approximation properties, are fast, simple to\nimplement, and accurate in practical settings."}
{"id": "2411.14423", "pdf": "https://arxiv.org/pdf/2411.14423", "abs": "https://arxiv.org/abs/2411.14423", "authors": ["Zhuoman Liu", "Weicai Ye", "Yan Luximon", "Pengfei Wan", "Di Zhang"], "title": "PhysFlow: Unleashing the Potential of Multi-modal Foundation Models and Video Diffusion for 4D Dynamic Physical Scene Simulation", "categories": ["cs.CV"], "comment": "CVPR 2025. Homepage: https://zhuomanliu.github.io/PhysFlow/", "summary": "Realistic simulation of dynamic scenes requires accurately capturing diverse\nmaterial properties and modeling complex object interactions grounded in\nphysical principles. However, existing methods are constrained to basic\nmaterial types with limited predictable parameters, making them insufficient to\nrepresent the complexity of real-world materials. We introduce PhysFlow, a\nnovel approach that leverages multi-modal foundation models and video diffusion\nto achieve enhanced 4D dynamic scene simulation. Our method utilizes\nmulti-modal models to identify material types and initialize material\nparameters through image queries, while simultaneously inferring 3D Gaussian\nsplats for detailed scene representation. We further refine these material\nparameters using video diffusion with a differentiable Material Point Method\n(MPM) and optical flow guidance rather than render loss or Score Distillation\nSampling (SDS) loss. This integrated framework enables accurate prediction and\nrealistic simulation of dynamic interactions in real-world scenarios, advancing\nboth accuracy and flexibility in physics-based simulations."}
{"id": "2501.13093", "pdf": "https://arxiv.org/pdf/2501.13093", "abs": "https://arxiv.org/abs/2501.13093", "authors": ["Kayvon Mazooji", "Ilan Shomorony"], "title": "Guaranteed Recovery of Unambiguous Clusters", "categories": ["cs.IT", "cs.AI", "cs.DS", "cs.LG", "math.IT", "math.ST", "stat.TH"], "comment": "12 pages, includes minor changes and some new content compared to\n  previous version", "summary": "Clustering is often a challenging problem because of the inherent ambiguity\nin what the \"correct\" clustering should be. Even when the number of clusters\n$K$ is known, this ambiguity often still exists, particularly when there is\nvariation in density among different clusters, and clusters have multiple\nrelatively separated regions of high density. In this paper we propose an\ninformation-theoretic characterization of when a $K$-clustering is ambiguous,\nand design an algorithm that recovers the clustering whenever it is\nunambiguous. This characterization formalizes the situation when two high\ndensity regions within a cluster are separable enough that they look more like\ntwo distinct clusters than two truly distinct clusters in the $K$-clustering.\nThe algorithm first identifies $K$ partial clusters (or \"seeds\") using a\ndensity-based approach, and then adds unclustered points to the initial $K$\npartial clusters in a greedy manner to form a complete clustering. We implement\nand test a version of the algorithm that is modified to effectively handle\noverlapping clusters, and observe that it requires little parameter selection\nand displays improved performance on many datasets compared to widely used\nalgorithms for non-convex cluster recovery."}
{"id": "2505.02309", "pdf": "https://arxiv.org/pdf/2505.02309", "abs": "https://arxiv.org/abs/2505.02309", "authors": ["Sanjay Surendranath Girija", "Shashank Kapoor", "Lakshit Arora", "Dipen Pradhan", "Aman Raj", "Ankit Shetgaonkar"], "title": "Optimizing LLMs for Resource-Constrained Environments: A Survey of Model Compression Techniques", "categories": ["cs.LG", "cs.AI", "cs.CL"], "comment": "Accepted to IEEE COMPSAC 2025", "summary": "Large Language Models (LLMs) have revolutionized many areas of artificial\nintelligence (AI), but their substantial resource requirements limit their\ndeployment on mobile and edge devices. This survey paper provides a\ncomprehensive overview of techniques for compressing LLMs to enable efficient\ninference in resource-constrained environments. We examine three primary\napproaches: Knowledge Distillation, Model Quantization, and Model Pruning. For\neach technique, we discuss the underlying principles, present different\nvariants, and provide examples of successful applications. We also briefly\ndiscuss complementary techniques such as mixture-of-experts and early-exit\nstrategies. Finally, we highlight promising future directions, aiming to\nprovide a valuable resource for both researchers and practitioners seeking to\noptimize LLMs for edge deployment."}
{"id": "2505.05420", "pdf": "https://arxiv.org/pdf/2505.05420", "abs": "https://arxiv.org/abs/2505.05420", "authors": ["Mario U. Gaimann", "Miriam Klopotek"], "title": "Robustly optimal dynamics for active matter reservoir computing", "categories": ["nlin.AO", "cond-mat.soft", "cs.LG", "physics.comp-ph"], "comment": "55 pages, 30 figures. Supplementary Videos:\n  https://doi.org/10.18419/DARUS-4619. Replication Data:\n  https://doi.org/10.18419/DARUS-4620", "summary": "We study the information processing abilities of active matter in the\nreservoir computing (RC) paradigm, using a model that is externally driven to\ninfer the future state of a chaotic signal. The simulated system closely\nfollows a previously reported model. We uncover an exceptional dynamical regime\nof agent dynamics that has been overlooked heretofore. It appears robustly\noptimal across varying physical parameters and inference tasks, thus providing\nvaluable insights into computation and inference with physical systems more\ngenerally. The ability to form effective mechanisms for information processing\nare primarily determined by the system's own intrinsic relaxation abilities.\nThese are identifiable when probing the system without a specific inference\ngoal and manifest when testing minimalistic single-particle reservoirs. The\nregime that achieves optimal computation is situated just below the critical\ndamping threshold, involving a microscopic dynamical relaxation with multiple\nstages. The optimal system is adaptable under chaotic external driving, due to\na diversity in response mechanisms that emerge like rapid alternations between\nquasi-stationary and highly nonlinear dynamical states. Both coherent and\nincoherent dynamics contribute to their operation, partly at dissimilar scales\nof space and delay time. Correlations on agent dynamics can indicate the\nbest-performing regimes and onsets of tight relationships between the\nresponding system and the fluctuating driver. As this model of computation is\ninterpretable in physical terms, it facilitates re-framing inquiries regarding\nlearning and unconventional computing with a fresh rationale for many-body\nphysics out of equilibrium."}
{"id": "2412.00626", "pdf": "https://arxiv.org/pdf/2412.00626", "abs": "https://arxiv.org/abs/2412.00626", "authors": ["You Wu", "Xiangyang Yang", "Xucheng Wang", "Hengzhou Ye", "Dan Zeng", "Shuiwang Li"], "title": "MambaNUT: Nighttime UAV Tracking via Mamba-based Adaptive Curriculum Learning", "categories": ["cs.CV"], "comment": null, "summary": "Harnessing low-light enhancement and domain adaptation, nighttime UAV\ntracking has made substantial strides. However, over-reliance on image\nenhancement, limited high-quality nighttime data, and a lack of integration\nbetween daytime and nighttime trackers hinder the development of an end-to-end\ntrainable framework. Additionally, current ViT-based trackers demand heavy\ncomputational resources due to their reliance on the self-attention mechanism.\nIn this paper, we propose a novel pure Mamba-based tracking framework\n(MambaNUT) that employs a state space model with linear complexity as its\nbackbone, incorporating a single-stream architecture that integrates feature\nlearning and template-search coupling within Vision Mamba. We introduce an\nadaptive curriculum learning (ACL) approach that dynamically adjusts sampling\nstrategies and loss weights, thereby improving the model's ability of\ngeneralization. Our ACL is composed of two levels of curriculum schedulers: (1)\nsampling scheduler that transforms the data distribution from imbalanced to\nbalanced, as well as from easier (daytime) to harder (nighttime) samples; (2)\nloss scheduler that dynamically assigns weights based on the size of the\ntraining data and IoU of individual instances. Exhaustive experiments on\nmultiple nighttime UAV tracking benchmarks demonstrate that the proposed\nMambaNUT achieves state-of-the-art performance while requiring lower\ncomputational costs. The code will be available at\nhttps://github.com/wuyou3474/MambaNUT."}
{"id": "2501.17899", "pdf": "https://arxiv.org/pdf/2501.17899", "abs": "https://arxiv.org/abs/2501.17899", "authors": ["Rashid Mushkani", "Hugo Berard", "Allison Cohen", "Shin Koeski"], "title": "The Right to AI", "categories": ["cs.CY", "cs.AI", "cs.HC"], "comment": "ICML 2025", "summary": "This paper proposes a Right to AI, which asserts that individuals and\ncommunities should meaningfully participate in the development and governance\nof the AI systems that shape their lives. Motivated by the increasing\ndeployment of AI in critical domains and inspired by Henri Lefebvre's concept\nof the Right to the City, we reconceptualize AI as a societal infrastructure,\nrather than merely a product of expert design. In this paper, we critically\nevaluate how generative agents, large-scale data extraction, and diverse\ncultural values bring new complexities to AI oversight. The paper proposes that\ngrassroots participatory methodologies can mitigate biased outcomes and enhance\nsocial responsiveness. It asserts that data is socially produced and should be\nmanaged and owned collectively. Drawing on Sherry Arnstein's Ladder of Citizen\nParticipation and analyzing nine case studies, the paper develops a four-tier\nmodel for the Right to AI that situates the current paradigm and envisions an\naspirational future. It proposes recommendations for inclusive data ownership,\ntransparent design processes, and stakeholder-driven oversight. We also discuss\nmarket-led and state-centric alternatives and argue that participatory\napproaches offer a better balance between technical efficiency and democratic\nlegitimacy."}
{"id": "2505.05471", "pdf": "https://arxiv.org/pdf/2505.05471", "abs": "https://arxiv.org/abs/2505.05471", "authors": ["Jarren Briscoe", "Assefaw Gebremedhin"], "title": "Facets of Disparate Impact: Evaluating Legally Consistent Bias in Machine Learning", "categories": ["cs.CY", "cs.LG"], "comment": "CIKM 2024", "summary": "Leveraging current legal standards, we define bias through the lens of\nmarginal benefits and objective testing with the novel metric \"Objective\nFairness Index\". This index combines the contextual nuances of objective\ntesting with metric stability, providing a legally consistent and reliable\nmeasure. Utilizing the Objective Fairness Index, we provide fresh insights into\nsensitive machine learning applications, such as COMPAS (recidivism\nprediction), highlighting the metric's practical and theoretical significance.\nThe Objective Fairness Index allows one to differentiate between discriminatory\ntests and systemic disparities."}
{"id": "2412.03093", "pdf": "https://arxiv.org/pdf/2412.03093", "abs": "https://arxiv.org/abs/2412.03093", "authors": ["Sungheon Jeong", "Hanning Chen", "Sanggeon Yun", "Suhyeon Cho", "Wenjun Huang", "Xiangjian Liu", "Mohsen Imani"], "title": "Expanding Event Modality Applications through a Robust CLIP-Based Encoder", "categories": ["cs.CV"], "comment": null, "summary": "This paper introduces a powerful encoder that transfers CLIP`s capabilities\nto event-based data, enhancing its utility and expanding its applicability\nacross diverse domains. While large-scale datasets have significantly advanced\nimage-based models, the scarcity of comprehensive event datasets has limited\nperformance potential in event modality. To address this challenge, we adapt\nCLIP`s architecture to align event embeddings with image embeddings, supporting\nzero-shot learning and preserving text alignment while mitigating catastrophic\nforgetting. Our encoder achieves strong performance in object recognition, with\ncompetitive results in zero-shot and few-shot learning tasks. Notably, it\ngeneralizes effectively to events extracted from video data without requiring\nadditional training, highlighting its versatility. Additionally, we integrate\nthis encoder within a cross-modality framework that facilitates interaction\nacross five modalities-Image, Event, Text, Sound, and Depth-expanding the\npossibilities for cross-modal applications. Overall, this work underscores the\ntransformative potential of a robust event encoder, broadening the scope and\nutility of event-based data across various fields."}
{"id": "2502.01521", "pdf": "https://arxiv.org/pdf/2502.01521", "abs": "https://arxiv.org/abs/2502.01521", "authors": ["Kaixi Bao", "Chenhao Li", "Yarden As", "Andreas Krause", "Marco Hutter"], "title": "Toward Task Generalization via Memory Augmentation in Meta-Reinforcement Learning", "categories": ["cs.LG", "cs.AI", "cs.RO"], "comment": null, "summary": "Agents trained via reinforcement learning (RL) often struggle to perform well\non tasks that differ from those encountered during training. This limitation\npresents a challenge to the broader deployment of RL in diverse and dynamic\ntask settings. In this work, we introduce memory augmentation, a memory-based\nRL approach to improve task generalization. Our approach leverages\ntask-structured augmentations to simulate plausible out-of-distribution\nscenarios and incorporates memory mechanisms to enable context-aware policy\nadaptation. Trained on a predefined set of tasks, our policy demonstrates the\nability to generalize to unseen tasks through memory augmentation without\nrequiring additional interactions with the environment. Through extensive\nsimulation experiments and real-world hardware evaluations on legged locomotion\ntasks, we demonstrate that our approach achieves zero-shot generalization to\nunseen tasks while maintaining robust in-distribution performance and high\nsample efficiency."}
{"id": "2103.16074", "pdf": "https://arxiv.org/pdf/2103.16074", "abs": "https://arxiv.org/abs/2103.16074", "authors": ["Xinke Li", "Zhirui Chen", "Yue Zhao", "Zekun Tong", "Yabang Zhao", "Andrew Lim", "Joey Tianyi Zhou"], "title": "PointBA: Towards Backdoor Attacks in 3D Point Cloud", "categories": ["cs.LG", "cs.CR", "cs.CV"], "comment": "Accepted by ICCV 2021", "summary": "3D deep learning has been increasingly more popular for a variety of tasks\nincluding many safety-critical applications. However, recently several works\nraise the security issues of 3D deep models. Although most of them consider\nadversarial attacks, we identify that backdoor attack is indeed a more serious\nthreat to 3D deep learning systems but remains unexplored. We present the\nbackdoor attacks in 3D point cloud with a unified framework that exploits the\nunique properties of 3D data and networks. In particular, we design two attack\napproaches on point cloud: the poison-label backdoor attack (PointPBA) and the\nclean-label backdoor attack (PointCBA). The first one is straightforward and\neffective in practice, while the latter is more sophisticated assuming there\nare certain data inspections. The attack algorithms are mainly motivated and\ndeveloped by 1) the recent discovery of 3D adversarial samples suggesting the\nvulnerability of deep models under spatial transformation; 2) the proposed\nfeature disentanglement technique that manipulates the feature of the data\nthrough optimization methods and its potential to embed a new task. Extensive\nexperiments show the efficacy of the PointPBA with over 95% success rate across\nvarious 3D datasets and models, and the more stealthy PointCBA with around 50%\nsuccess rate. Our proposed backdoor attack in 3D point cloud is expected to\nperform as a baseline for improving the robustness of 3D deep models."}
{"id": "2412.07825", "pdf": "https://arxiv.org/pdf/2412.07825", "abs": "https://arxiv.org/abs/2412.07825", "authors": ["Wufei Ma", "Haoyu Chen", "Guofeng Zhang", "Yu-Cheng Chou", "Celso M de Melo", "Alan Yuille"], "title": "3DSRBench: A Comprehensive 3D Spatial Reasoning Benchmark", "categories": ["cs.CV"], "comment": "Project page: https://3dsrbench.github.io", "summary": "3D spatial reasoning is the ability to analyze and interpret the positions,\norientations, and spatial relationships of objects within the 3D space. This\nallows models to develop a comprehensive understanding of the 3D scene,\nenabling their applicability to a broader range of areas, such as autonomous\nnavigation, robotics, and AR/VR. While large multi-modal models (LMMs) have\nachieved remarkable progress in a wide range of image and video understanding\ntasks, their capabilities to perform 3D spatial reasoning on diverse natural\nimages are less studied. In this work we present the first comprehensive 3D\nspatial reasoning benchmark, 3DSRBench, with 2,772 manually annotated visual\nquestion-answer pairs across 12 question types. We conduct robust and thorough\nevaluation of 3D spatial reasoning capabilities by balancing the data\ndistribution and adopting a novel FlipEval strategy. To further study the\nrobustness of 3D spatial reasoning w.r.t. camera 3D viewpoints, our 3DSRBench\nincludes two subsets with 3D spatial reasoning questions on paired images with\ncommon and uncommon viewpoints. We benchmark a wide range of open-sourced and\nproprietary LMMs, uncovering their limitations in various aspects of 3D\nawareness, such as height, orientation, location, and multi-object reasoning,\nas well as their degraded performance on images with uncommon camera\nviewpoints. Our 3DSRBench provide valuable findings and insights about the\nfuture development of LMMs with strong 3D reasoning capabilities. Our project\npage and dataset is available https://3dsrbench.github.io."}
{"id": "2502.01842", "pdf": "https://arxiv.org/pdf/2502.01842", "abs": "https://arxiv.org/abs/2502.01842", "authors": ["Elahe Salari", "Zohreh Azimifar"], "title": "Texture Image Synthesis Using Spatial GAN Based on Vision Transformers", "categories": ["cs.CV", "cs.AI"], "comment": "Preprint", "summary": "Texture synthesis is a fundamental task in computer vision, whose goal is to\ngenerate visually realistic and structurally coherent textures for a wide range\nof applications, from graphics to scientific simulations. While traditional\nmethods like tiling and patch-based techniques often struggle with complex\ntextures, recent advancements in deep learning have transformed this field. In\nthis paper, we propose ViT-SGAN, a new hybrid model that fuses Vision\nTransformers (ViTs) with a Spatial Generative Adversarial Network (SGAN) to\naddress the limitations of previous methods. By incorporating specialized\ntexture descriptors such as mean-variance (mu, sigma) and textons into the\nself-attention mechanism of ViTs, our model achieves superior texture\nsynthesis. This approach enhances the model's capacity to capture complex\nspatial dependencies, leading to improved texture quality that is superior to\nstate-of-the-art models, especially for regular and irregular textures.\nComparison experiments with metrics such as FID, IS, SSIM, and LPIPS\ndemonstrate the substantial improvement of ViT-SGAN, which underlines its\nefficiency in generating diverse realistic textures."}
{"id": "2402.05356", "pdf": "https://arxiv.org/pdf/2402.05356", "abs": "https://arxiv.org/abs/2402.05356", "authors": ["Wenyu Jiang", "Zhenlong Liu", "Zejian Xie", "Songxin Zhang", "Bingyi Jing", "Hongxin Wei"], "title": "Exploring Learning Complexity for Efficient Downstream Dataset Pruning", "categories": ["cs.LG"], "comment": "Accepted by ICLR 2025", "summary": "The ever-increasing fine-tuning cost of large-scale pre-trained models gives\nrise to the importance of dataset pruning, which aims to reduce dataset size\nwhile maintaining task performance. However, existing dataset pruning methods\nrequire training on the entire dataset, which is impractical for large-scale\npre-trained models. In this paper, we propose a straightforward, novel, and\ntraining-free hardness score named Distorting-based Learning Complexity (DLC),\nto identify informative images and instructions from the downstream dataset\nefficiently. Our method is motivated by the observation that easy samples\nlearned faster can also be learned with fewer parameters. Specifically, we\ndefine the Learning Complexity to quantify sample hardness and utilize a\nlightweight weights masking process for fast estimation, instead of the costly\nSGD optimization. Based on DLC, we further design a flexible under-sampling\nwith randomness (dubbed FlexRand), replacing the top-K strategy, to alleviate\nthe severe subset distribution shift. Extensive experiments with downstream\nimage and instructions dataset pruning benchmarks demonstrate the effectiveness\nand efficiency of the proposed approach. In the images pruning benchmark, DLC\nsignificantly reduces the pruning time by 35x while establishing\nstate-of-the-art performance with FlexRand."}
{"id": "2412.16698", "pdf": "https://arxiv.org/pdf/2412.16698", "abs": "https://arxiv.org/abs/2412.16698", "authors": ["Tongfei Bian", "Yiming Ma", "Mathieu Chollet", "Victor Sanchez", "Tanaya Guha"], "title": "Interact with me: Joint Egocentric Forecasting of Intent to Interact, Attitude and Social Actions", "categories": ["cs.CV", "cs.HC"], "comment": "Accepted to ICME, 2025. Camera-ready Version", "summary": "For efficient human-agent interaction, an agent should proactively recognize\ntheir target user and prepare for upcoming interactions. We formulate this\nchallenging problem as the novel task of jointly forecasting a person's intent\nto interact with the agent, their attitude towards the agent and the action\nthey will perform, from the agent's (egocentric) perspective. So we propose\n\\emph{SocialEgoNet} - a graph-based spatiotemporal framework that exploits task\ndependencies through a hierarchical multitask learning approach. SocialEgoNet\nuses whole-body skeletons (keypoints from face, hands and body) extracted from\nonly 1 second of video input for high inference speed. For evaluation, we\naugment an existing egocentric human-agent interaction dataset with new class\nlabels and bounding box annotations. Extensive experiments on this augmented\ndataset, named JPL-Social, demonstrate \\emph{real-time} inference and superior\nperformance (average accuracy across all tasks: 83.15\\%) of our model\noutperforming several competitive baselines. The additional annotations and\ncode will be available upon acceptance."}
{"id": "2502.08821", "pdf": "https://arxiv.org/pdf/2502.08821", "abs": "https://arxiv.org/abs/2502.08821", "authors": ["Jocelyn Dzuong"], "title": "DejAIvu: Identifying and Explaining AI Art on the Web in Real-Time with Saliency Maps", "categories": ["cs.CV", "cs.AI", "cs.LG"], "comment": "5 pages, 3 figures. Accepted to IJCAI 2025 Demo Track. Revised\n  version will be uploaded soon", "summary": "The recent surge in advanced generative models, such as diffusion models and\ngenerative adversarial networks (GANs), has led to an alarming rise in\nAI-generated images across various domains on the web. While such technologies\noffer benefits such as democratizing artistic creation, they also pose\nchallenges in misinformation, digital forgery, and authenticity verification.\nAdditionally, the uncredited use of AI-generated images in media and marketing\nhas sparked significant backlash from online communities. In response to this,\nwe introduce DejAIvu, a Chrome Web extension that combines real-time\nAI-generated image detection with saliency-based explainability while users\nbrowse the web. Using an ONNX-optimized deep learning model, DejAIvu\nautomatically analyzes images on websites such as Google Images, identifies\nAI-generated content using model inference, and overlays a saliency heatmap to\nhighlight AI-related artifacts. Our approach integrates efficient in-browser\ninference, gradient-based saliency analysis, and a seamless user experience,\nensuring that AI detection is both transparent and interpretable. We also\nevaluate DejAIvu across multiple pretrained architectures and benchmark\ndatasets, demonstrating high accuracy and low latency, making it a practical\nand deployable tool for enhancing AI image accountability. The code for this\nsystem can be found at https://github.com/Noodulz/dejAIvu."}
{"id": "2404.00173", "pdf": "https://arxiv.org/pdf/2404.00173", "abs": "https://arxiv.org/abs/2404.00173", "authors": ["David Valiente", "Fernando Rodríguez-Mas", "Juan V. Alegre-Requena", "David Dalmau", "María Flores", "Juan C. Ferrer"], "title": "Comparing Hyper-optimized Machine Learning Models for Predicting Efficiency Degradation in Organic Solar Cells", "categories": ["cs.LG"], "comment": null, "summary": "This work presents a set of optimal machine learning (ML) models to represent\nthe temporal degradation suffered by the power conversion efficiency (PCE) of\npolymeric organic solar cells (OSCs) with a multilayer structure\nITO/PEDOT:PSS/P3HT:PCBM/Al. To that aim, we generated a database with 996\nentries, which includes up to 7 variables regarding both the manufacturing\nprocess and environmental conditions for more than 180 days. Then, we relied on\na software framework that brings together a conglomeration of automated ML\nprotocols that execute sequentially against our database by simply command-line\ninterface. This easily permits hyper-optimizing and randomizing seeds of the ML\nmodels through exhaustive benchmarking so that optimal models are obtained. The\naccuracy achieved reaches values of the coefficient determination (R2) widely\nexceeding 0.90, whereas the root mean squared error (RMSE), sum of squared\nerror (SSE), and mean absolute error (MAE)>1% of the target value, the PCE.\nAdditionally, we contribute with validated models able to screen the behavior\nof OSCs never seen in the database. In that case, R2~0.96-0.97 and RMSE~1%,\nthus confirming the reliability of the proposal to predict. For comparative\npurposes, classical Bayesian regression fitting based on non-linear mean\nsquares (LMS) are also presented, which only perform sufficiently for\nunivariate cases of single OSCs. Hence they fail to outperform the breadth of\nthe capabilities shown by the ML models. Finally, thanks to the standardized\nresults offered by the ML framework, we study the dependencies between the\nvariables of the dataset and their implications for the optimal performance and\nstability of the OSCs. Reproducibility is ensured by a standardized report\naltogether with the dataset, which are publicly available at Github."}
{"id": "2412.17378", "pdf": "https://arxiv.org/pdf/2412.17378", "abs": "https://arxiv.org/abs/2412.17378", "authors": ["Hao Gui", "Lin Hu", "Rui Chen", "Mingxiao Huang", "Yuxin Yin", "Jin Yang", "Yong Wu", "Chen Liu", "Zhongxu Sun", "Xueyang Zhang", "Kun Zhan"], "title": "Balanced 3DGS: Gaussian-wise Parallelism Rendering with Fine-Grained Tiling", "categories": ["cs.CV"], "comment": null, "summary": "3D Gaussian Splatting (3DGS) is increasingly attracting attention in both\nacademia and industry owing to its superior visual quality and rendering speed.\nHowever, training a 3DGS model remains a time-intensive task, especially in\nload imbalance scenarios where workload diversity among pixels and Gaussian\nspheres causes poor renderCUDA kernel performance. We introduce Balanced 3DGS,\na Gaussian-wise parallelism rendering with fine-grained tiling approach in 3DGS\ntraining process, perfectly solving load-imbalance issues. First, we\ninnovatively introduce the inter-block dynamic workload distribution technique\nto map workloads to Streaming Multiprocessor(SM) resources within a single GPU\ndynamically, which constitutes the foundation of load balancing. Second, we are\nthe first to propose the Gaussian-wise parallel rendering technique to\nsignificantly reduce workload divergence inside a warp, which serves as a\ncritical component in addressing load imbalance. Based on the above two\nmethods, we further creatively put forward the fine-grained combined load\nbalancing technique to uniformly distribute workload across all SMs, which\nboosts the forward renderCUDA kernel performance by up to 7.52x. Besides, we\npresent a self-adaptive render kernel selection strategy during the 3DGS\ntraining process based on different load-balance situations, which effectively\nimproves training efficiency."}
{"id": "2502.14281", "pdf": "https://arxiv.org/pdf/2502.14281", "abs": "https://arxiv.org/abs/2502.14281", "authors": ["Weipeng Huang", "Qin Li", "Yang Xiao", "Cheng Qiao", "Tie Cai", "Junwei Liang", "Neil J. Hurley", "Guangyuan Piao"], "title": "Correcting Noisy Multilabel Predictions: Modeling Label Noise through Latent Space Shifts", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Noise in data appears to be inevitable in most real-world machine learning\napplications and would cause severe overfitting problems. Not only can data\nfeatures contain noise, but labels are also prone to be noisy due to human\ninput. In this paper, rather than noisy label learning in multiclass\nclassifications, we instead focus on the less explored area of noisy label\nlearning for multilabel classifications. Specifically, we investigate the\npost-correction of predictions generated from classifiers learned with noisy\nlabels. The reasons are two-fold. Firstly, this approach can directly work with\nthe trained models to save computational resources. Secondly, it could be\napplied on top of other noisy label correction techniques to achieve further\nimprovements. To handle this problem, we appeal to deep generative approaches\nthat are possible for uncertainty estimation. Our model posits that label noise\narises from a stochastic shift in the latent variable, providing a more robust\nand beneficial means for noisy learning. We develop both unsupervised and\nsemi-supervised learning methods for our model. The extensive empirical study\npresents solid evidence to that our approach is able to consistently improve\nthe independent models and performs better than a number of existing methods\nacross various noisy label settings. Moreover, a comprehensive empirical\nanalysis of the proposed method is carried out to validate its robustness,\nincluding sensitivity analysis and an ablation study, among other elements."}
{"id": "2404.03176", "pdf": "https://arxiv.org/pdf/2404.03176", "abs": "https://arxiv.org/abs/2404.03176", "authors": ["Haiyun He", "Ziv Goldfeld"], "title": "Information-Theoretic Generalization Bounds for Deep Neural Networks", "categories": ["cs.LG", "cs.IT", "math.IT"], "comment": "21 pages, 5 figures", "summary": "Deep neural networks (DNNs) exhibit an exceptional capacity for\ngeneralization in practical applications. This work aims to capture the effect\nand benefits of depth for supervised learning via information-theoretic\ngeneralization bounds. We first derive two hierarchical bounds on the\ngeneralization error in terms of the Kullback-Leibler (KL) divergence or the\n1-Wasserstein distance between the train and test distributions of the network\ninternal representations. The KL divergence bound shrinks as the layer index\nincreases, while the Wasserstein bound implies the existence of a layer that\nserves as a generalization funnel, which attains a minimal 1-Wasserstein\ndistance. Analytic expressions for both bounds are derived under the setting of\nbinary Gaussian classification with linear DNNs. To quantify the contraction of\nthe relevant information measures when moving deeper into the network, we\nanalyze the strong data processing inequality (SDPI) coefficient between\nconsecutive layers of three regularized DNN models: $\\mathsf{Dropout}$,\n$\\mathsf{DropConnect}$, and Gaussian noise injection. This enables refining our\ngeneralization bounds to capture the contraction as a function of the network\narchitecture parameters. Specializing our results to DNNs with a finite\nparameter space and the Gibbs algorithm reveals that deeper yet narrower\nnetwork architectures generalize better in those examples, although how broadly\nthis statement applies remains a question."}
{"id": "2501.02180", "pdf": "https://arxiv.org/pdf/2501.02180", "abs": "https://arxiv.org/abs/2501.02180", "authors": ["Ren Hu", "Pan Lian"], "title": "Quaternionic Reweighted Amplitude Flow for Phase Retrieval in Image Reconstruction", "categories": ["cs.CV", "math.CV", "94A08, 46S05, 11E88,"], "comment": null, "summary": "Quaternionic signal processing provides powerful tools for efficiently\nmanaging color signals by preserving the intrinsic correlations among signal\ndimensions through quaternion algebra. In this paper, we address the\nquaternionic phase retrieval problem by systematically developing novel\nalgorithms based on an amplitude-based model. Specifically, we propose the\nQuaternionic Reweighted Amplitude Flow (QRAF) algorithm, which is further\nenhanced by three of its variants: incremental, accelerated, and adapted QRAF\nalgorithms. In addition, we introduce the Quaternionic Perturbed Amplitude Flow\n(QPAF) algorithm, which has linear convergence. Extensive numerical experiments\non both synthetic data and real images, demonstrate that our proposed methods\nsignificantly improve recovery performance and computational efficiency\ncompared to state-of-the-art approaches."}
{"id": "2502.15757", "pdf": "https://arxiv.org/pdf/2502.15757", "abs": "https://arxiv.org/abs/2502.15757", "authors": ["Leonardo Berti", "Gjergji Kasneci"], "title": "TLOB: A Novel Transformer Model with Dual Attention for Price Trend Prediction with Limit Order Book Data", "categories": ["q-fin.ST", "cs.AI", "cs.LG", "q-fin.TR"], "comment": null, "summary": "Price Trend Prediction (PTP) based on Limit Order Book (LOB) data is a\nfundamental challenge in financial markets. Despite advances in deep learning,\nexisting models fail to generalize across different market conditions and\nassets. Surprisingly, by adapting a simple MLP-based architecture to LOB, we\nshow that we surpass SoTA performance; thus, challenging the necessity of\ncomplex architectures. Unlike past work that shows robustness issues, we\npropose TLOB, a transformer-based model that uses a dual attention mechanism to\ncapture spatial and temporal dependencies in LOB data. This allows it to\nadaptively focus on the market microstructure, making it particularly effective\nfor longer-horizon predictions and volatile market conditions. We also\nintroduce a new labeling method that improves on previous ones, removing the\nhorizon bias. We evaluate TLOB's effectiveness across four horizons, using the\nestablished FI-2010 benchmark, a NASDAQ and a Bitcoin dataset. TLOB outperforms\nSoTA methods in every dataset and horizon. Additionally, we empirically show\nhow stock price predictability has declined over time, -6.68 in F1-score,\nhighlighting the growing market efficiency. Predictability must be considered\nin relation to transaction costs, so we experimented with defining trends using\nan average spread, reflecting the primary transaction cost. The resulting\nperformance deterioration underscores the complexity of translating trend\nclassification into profitable trading strategies. We argue that our work\nprovides new insights into the evolving landscape of stock price trend\nprediction and sets a strong foundation for future advancements in financial\nAI. We release the code at https://github.com/LeonardoBerti00/TLOB."}
{"id": "2406.03946", "pdf": "https://arxiv.org/pdf/2406.03946", "abs": "https://arxiv.org/abs/2406.03946", "authors": ["Lars Veefkind", "Gabriele Cesa"], "title": "A Probabilistic Approach to Learning the Degree of Equivariance in Steerable CNNs", "categories": ["cs.LG"], "comment": "9 pages, published at ICML 2024 as main conference paper", "summary": "Steerable convolutional neural networks (SCNNs) enhance task performance by\nmodelling geometric symmetries through equivariance constraints on weights.\nYet, unknown or varying symmetries can lead to overconstrained weights and\ndecreased performance. To address this, this paper introduces a probabilistic\nmethod to learn the degree of equivariance in SCNNs. We parameterise the degree\nof equivariance as a likelihood distribution over the transformation group\nusing Fourier coefficients, offering the option to model layer-wise and shared\nequivariance. These likelihood distributions are regularised to ensure an\ninterpretable degree of equivariance across the network. Advantages include the\napplicability to many types of equivariant networks through the flexible\nframework of SCNNs and the ability to learn equivariance with respect to any\nsubgroup of any compact group without requiring additional layers. Our\nexperiments reveal competitive performance on datasets with mixed symmetries,\nwith learnt likelihood distributions that are representative of the underlying\ndegree of equivariance."}
{"id": "2503.01894", "pdf": "https://arxiv.org/pdf/2503.01894", "abs": "https://arxiv.org/abs/2503.01894", "authors": ["Rashid Mushkani", "Shravan Nayak", "Hugo Berard", "Allison Cohen", "Shin Koseki", "Hadrien Bertrand"], "title": "LIVS: A Pluralistic Alignment Dataset for Inclusive Public Spaces", "categories": ["cs.CV", "cs.AI", "cs.HC"], "comment": "ICML 2025", "summary": "We introduce the Local Intersectional Visual Spaces (LIVS) dataset, a\nbenchmark for multi-criteria alignment, developed through a two-year\nparticipatory process with 30 community organizations to support the\npluralistic alignment of text-to-image (T2I) models in inclusive urban\nplanning. The dataset encodes 37,710 pairwise comparisons across 13,462 images,\nstructured along six criteria - Accessibility, Safety, Comfort, Invitingness,\nInclusivity, and Diversity - derived from 634 community-defined concepts. Using\nDirect Preference Optimization (DPO), we fine-tune Stable Diffusion XL to\nreflect multi-criteria spatial preferences and evaluate the LIVS dataset and\nthe fine-tuned model through four case studies: (1) DPO increases alignment\nwith annotated preferences, particularly when annotation volume is high; (2)\npreference patterns vary across participant identities, underscoring the need\nfor intersectional data; (3) human-authored prompts generate more distinctive\nvisual outputs than LLM-generated ones, influencing annotation decisiveness;\nand (4) intersectional groups assign systematically different ratings across\ncriteria, revealing the limitations of single-objective alignment. While DPO\nimproves alignment under specific conditions, the prevalence of neutral ratings\nindicates that community values are heterogeneous and often ambiguous. LIVS\nprovides a benchmark for developing T2I models that incorporate local,\nstakeholder-driven preferences, offering a foundation for context-aware\nalignment in spatial design."}
{"id": "2502.18218", "pdf": "https://arxiv.org/pdf/2502.18218", "abs": "https://arxiv.org/abs/2502.18218", "authors": ["Bingke Zhu", "Xiaoxiao Wang", "Minghui Jia", "Yihan Tao", "Xiao Kong", "Ali Luo", "Yingying Chen", "Ming Tang", "Jinqiao Wang"], "title": "FLARE: A Framework for Stellar Flare Forecasting using Stellar Physical Properties and Historical Records", "categories": ["astro-ph.SR", "astro-ph.IM", "cs.AI"], "comment": "Accepted by IJCAI 2025", "summary": "Stellar flare events are critical observational samples for astronomical\nresearch; however, recorded flare events remain limited. Stellar flare\nforecasting can provide additional flare event samples to support research\nefforts. Despite this potential, no specialized models for stellar flare\nforecasting have been proposed to date. In this paper, we present extensive\nexperimental evidence demonstrating that both stellar physical properties and\nhistorical flare records are valuable inputs for flare forecasting tasks. We\nthen introduce FLARE (Forecasting Light-curve-based Astronomical Records via\nfeatures Ensemble), the first-of-its-kind large model specifically designed for\nstellar flare forecasting. FLARE integrates stellar physical properties and\nhistorical flare records through a novel Soft Prompt Module and Residual Record\nFusion Module. Our experiments on the publicly available Kepler light curve\ndataset demonstrate that FLARE achieves superior performance compared to other\nmethods across all evaluation metrics. Finally, we validate the forecast\ncapability of our model through a comprehensive case study."}
{"id": "2406.08569", "pdf": "https://arxiv.org/pdf/2406.08569", "abs": "https://arxiv.org/abs/2406.08569", "authors": ["Ossi Räisä", "Stratis Markou", "Matthew Ashman", "Wessel P. Bruinsma", "Marlon Tobaben", "Antti Honkela", "Richard E. Turner"], "title": "Noise-Aware Differentially Private Regression via Meta-Learning", "categories": ["cs.LG", "cs.CR", "stat.ML"], "comment": "NeurIPS 2024", "summary": "Many high-stakes applications require machine learning models that protect\nuser privacy and provide well-calibrated, accurate predictions. While\nDifferential Privacy (DP) is the gold standard for protecting user privacy,\nstandard DP mechanisms typically significantly impair performance. One approach\nto mitigating this issue is pre-training models on simulated data before DP\nlearning on the private data. In this work we go a step further, using\nsimulated data to train a meta-learning model that combines the Convolutional\nConditional Neural Process (ConvCNP) with an improved functional DP mechanism\nof Hall et al. [2013] yielding the DPConvCNP. DPConvCNP learns from simulated\ndata how to map private data to a DP predictive model in one forward pass, and\nthen provides accurate, well-calibrated predictions. We compare DPConvCNP with\na DP Gaussian Process (GP) baseline with carefully tuned hyperparameters. The\nDPConvCNP outperforms the GP baseline, especially on non-Gaussian data, yet is\nmuch faster at test time and requires less tuning."}
{"id": "2503.05423", "pdf": "https://arxiv.org/pdf/2503.05423", "abs": "https://arxiv.org/abs/2503.05423", "authors": ["Run He", "Di Fang", "Yicheng Xu", "Yawen Cui", "Ming Li", "Cen Chen", "Ziqian Zeng", "Huiping Zhuang"], "title": "Semantic Shift Estimation via Dual-Projection and Classifier Reconstruction for Exemplar-Free Class-Incremental Learning", "categories": ["cs.CV", "cs.AI", "cs.LG"], "comment": "Accepted by ICML 2025", "summary": "Exemplar-Free Class-Incremental Learning (EFCIL) aims to sequentially learn\nfrom distinct categories without retaining exemplars but easily suffers from\ncatastrophic forgetting of learned knowledge. While existing EFCIL methods\nleverage knowledge distillation to alleviate forgetting, they still face two\ncritical challenges: semantic shift and decision bias. Specifically, the\nembeddings of old tasks shift in the embedding space after learning new tasks,\nand the classifier becomes biased towards new tasks due to training solely with\nnew data, thereby hindering the balance between old and new knowledge. To\naddress these issues, we propose the Dual-Projection Shift Estimation and\nClassifier Reconstruction (DPCR) approach for EFCIL. DPCR effectively estimates\nsemantic shift through a dual-projection, which combines a learnable\ntransformation with a row-space projection to capture both task-wise and\ncategory-wise shifts. Furthermore, to mitigate decision bias, DPCR employs\nridge regression to reformulate classifier training as a reconstruction\nprocess. This reconstruction exploits previous information encoded in\ncovariance and prototype of each class after calibration with estimated shift,\nthereby reducing decision bias. Extensive experiments demonstrate that, across\nvarious datasets, DPCR effectively balances old and new tasks, outperforming\nstate-of-the-art EFCIL methods."}
{"id": "2503.17656", "pdf": "https://arxiv.org/pdf/2503.17656", "abs": "https://arxiv.org/abs/2503.17656", "authors": ["Yuheng Ding", "Yusong Wang", "Bo Qiang", "Jie Yu", "Qi Li", "Yiran Zhou", "Zhenmin Liu"], "title": "NaFM: Pre-training a Foundation Model for Small-Molecule Natural Products", "categories": ["q-bio.QM", "cs.AI", "cs.LG"], "comment": null, "summary": "Natural products, as metabolites from microorganisms, animals, or plants,\nexhibit diverse biological activities, making them crucial for drug discovery.\nNowadays, existing deep learning methods for natural products research\nprimarily rely on supervised learning approaches designed for specific\ndownstream tasks. However, such one-model-for-a-task paradigm often lacks\ngeneralizability and leaves significant room for performance improvement.\nAdditionally, existing molecular characterization methods are not well-suited\nfor the unique tasks associated with natural products. To address these\nlimitations, we have pre-trained a foundation model for natural products based\non their unique properties. Our approach employs a novel pretraining strategy\nthat is especially tailored to natural products. By incorporating contrastive\nlearning and masked graph learning objectives, we emphasize evolutional\ninformation from molecular scaffolds while capturing side-chain information.\nOur framework achieves state-of-the-art (SOTA) results in various downstream\ntasks related to natural product mining and drug discovery. We first compare\ntaxonomy classification with synthesized molecule-focused baselines to\ndemonstrate that current models are inadequate for understanding natural\nsynthesis. Furthermore, by diving into a fine-grained analysis at both the gene\nand microbial levels, NaFM demonstrates the ability to capture evolutionary\ninformation. Eventually, our method is experimented with virtual screening,\nillustrating informative natural product representations that can lead to more\neffective identification of potential drug candidates."}
{"id": "2406.11206", "pdf": "https://arxiv.org/pdf/2406.11206", "abs": "https://arxiv.org/abs/2406.11206", "authors": ["Rudrajit Das", "Inderjit S. Dhillon", "Alessandro Epasto", "Adel Javanmard", "Jieming Mao", "Vahab Mirrokni", "Sujay Sanghavi", "Peilin Zhong"], "title": "Retraining with Predicted Hard Labels Provably Increases Model Accuracy", "categories": ["cs.LG", "cs.CR", "stat.ML"], "comment": "To appear in ICML 2025", "summary": "The performance of a model trained with noisy labels is often improved by\nsimply \\textit{retraining} the model with its \\textit{own predicted hard\nlabels} (i.e., 1/0 labels). Yet, a detailed theoretical characterization of\nthis phenomenon is lacking. In this paper, we theoretically analyze retraining\nin a linearly separable binary classification setting with randomly corrupted\nlabels given to us and prove that retraining can improve the population\naccuracy obtained by initially training with the given (noisy) labels. To the\nbest of our knowledge, this is the first such theoretical result. Retraining\nfinds application in improving training with local label differential privacy\n(DP) which involves training with noisy labels. We empirically show that\nretraining selectively on the samples for which the predicted label matches the\ngiven label significantly improves label DP training at no extra privacy cost;\nwe call this consensus-based retraining. As an example, when training ResNet-18\non CIFAR-100 with $\\epsilon=3$ label DP, we obtain more than 6% improvement in\naccuracy with consensus-based retraining."}
{"id": "2503.06451", "pdf": "https://arxiv.org/pdf/2503.06451", "abs": "https://arxiv.org/abs/2503.06451", "authors": ["Basudha Pal", "Siyuan Huang", "Rama Chellappa"], "title": "A Quantitative Evaluation of the Expressivity of BMI, Pose and Gender in Body Embeddings for Recognition and Identification", "categories": ["cs.CV"], "comment": null, "summary": "Person Re-identification (ReID) systems that match individuals across images\nor video frames are essential in many real-world applications. However,\nexisting methods are often influenced by attributes such as gender, pose, and\nbody mass index (BMI), which vary in unconstrained settings and raise concerns\nrelated to fairness and generalization. To address this, we extend the notion\nof expressivity, defined as the mutual information between learned features and\nspecific attributes, using a secondary neural network to quantify how strongly\nattributes are encoded. Applying this framework to three ReID models, we find\nthat BMI consistently shows the highest expressivity in the final layers,\nindicating its dominant role in recognition. In the last attention layer,\nattributes are ranked as BMI > Pitch > Gender > Yaw, revealing their relative\ninfluences in representation learning. Expressivity values also evolve across\nlayers and training epochs, reflecting a dynamic encoding of attributes. These\nfindings demonstrate the central role of body attributes in ReID and establish\na principled approach for uncovering attribute driven correlations."}
{"id": "2503.20500", "pdf": "https://arxiv.org/pdf/2503.20500", "abs": "https://arxiv.org/abs/2503.20500", "authors": ["Erhan Karakoca", "Hüseyin Çevik", "İbrahim Hökelek", "Ali Görçin"], "title": "Novel Deep Neural OFDM Receiver Architectures for LLR Estimation", "categories": ["eess.SP", "cs.AI"], "comment": "Submitted to IEEE Globecom 2025", "summary": "Neural receivers have recently become a popular topic, where the received\nsignals can be directly decoded by data driven mechanisms such as machine\nlearning and deep learning. In this paper, we propose two novel neural network\nbased orthogonal frequency division multiplexing (OFDM) receivers performing\nchannel estimation and equalization tasks and directly predicting log\nlikelihood ratios (LLRs) from the received in phase and quadrature phase (IQ)\nsignals. The first network, the Dual Attention Transformer (DAT), employs a\nstate of the art (SOTA) transformer architecture with an attention mechanism.\nThe second network, the Residual Dual Non Local Attention Network (RDNLA),\nutilizes a parallel residual architecture with a non local attention block. The\nbit error rate (BER) and block error rate (BLER) performance of various SOTA\nneural receiver architectures is compared with our proposed methods across\ndifferent signal to noise ratio (SNR) levels. The simulation results show that\nDAT and RDNLA outperform both traditional communication systems and existing\nneural receiver models."}
{"id": "2407.11540", "pdf": "https://arxiv.org/pdf/2407.11540", "abs": "https://arxiv.org/abs/2407.11540", "authors": ["Camillo Maria Caruso", "Paolo Soda", "Valerio Guarrasi"], "title": "Not Another Imputation Method: A Transformer-based Model for Missing Values in Tabular Datasets", "categories": ["cs.LG"], "comment": null, "summary": "Handling missing values in tabular datasets presents a significant challenge\nin training and testing artificial intelligence models, an issue usually\naddressed using imputation techniques. Here we introduce \"Not Another\nImputation Method\" (NAIM), a novel transformer-based model specifically\ndesigned to address this issue without the need for traditional imputation\ntechniques. NAIM's ability to avoid the necessity of imputing missing values\nand to effectively learn from available data relies on two main techniques: the\nuse of feature-specific embeddings to encode both categorical and numerical\nfeatures also handling missing inputs; the modification of the masked\nself-attention mechanism to completely mask out the contributions of missing\ndata. Additionally, a novel regularization technique is introduced to enhance\nthe model's generalization capability from incomplete data. We extensively\nevaluated NAIM on 5 publicly available tabular datasets, demonstrating its\nsuperior performance over 6 state-of-the-art machine learning models and 5 deep\nlearning models, each paired with 3 different imputation techniques when\nnecessary. The results highlight the efficacy of NAIM in improving predictive\nperformance and resilience in the presence of missing data. To facilitate\nfurther research and practical application in handling missing data without\ntraditional imputation methods, we made the code for NAIM available at\nhttps://github.com/cosbidev/NAIM."}
{"id": "2503.10042", "pdf": "https://arxiv.org/pdf/2503.10042", "abs": "https://arxiv.org/abs/2503.10042", "authors": ["Ziyue Wang", "Yurui Dong", "Fuwen Luo", "Minyuan Ruan", "Zhili Cheng", "Chi Chen", "Peng Li", "Yang Liu"], "title": "How Do Multimodal Large Language Models Handle Complex Multimodal Reasoning? Placing Them in An Extensible Escape Game", "categories": ["cs.CV"], "comment": null, "summary": "The rapid advancing of Multimodal Large Language Models (MLLMs) has spurred\ninterest in complex multimodal reasoning tasks in the real-world and virtual\nenvironment, which require coordinating multiple abilities, including visual\nperception, visual reasoning, spatial awareness, and target deduction. However,\nexisting evaluations primarily assess the final task completion, often\ndegrading assessments to isolated abilities such as visual grounding and visual\nquestion answering. Less attention is given to comprehensively and\nquantitatively analyzing reasoning process in multimodal environments, which is\ncrucial for understanding model behaviors and underlying reasoning mechanisms\nbeyond merely task success. To address this, we introduce MM-Escape, an\nextensible benchmark for investigating multimodal reasoning, inspired by\nreal-world escape games. MM-Escape emphasizes intermediate model behaviors\nalongside final task completion. To achieve this, we develop EscapeCraft, a\ncustomizable and open environment that enables models to engage in free-form\nexploration for assessing multimodal reasoning. Extensive experiments show that\nMLLMs, regardless of scale, can successfully complete the simplest room escape\ntasks, with some exhibiting human-like exploration strategies. Yet, performance\ndramatically drops as task difficulty increases. Moreover, we observe that\nperformance bottlenecks vary across models, revealing distinct failure modes\nand limitations in their multimodal reasoning abilities, such as repetitive\ntrajectories without adaptive exploration, getting stuck in corners due to poor\nvisual spatial awareness, and ineffective use of acquired props, such as the\nkey. We hope our work sheds light on new challenges in multimodal reasoning,\nand uncovers potential improvements in MLLMs capabilities."}
{"id": "2503.22688", "pdf": "https://arxiv.org/pdf/2503.22688", "abs": "https://arxiv.org/abs/2503.22688", "authors": ["Peiding Wang", "Li Zhang", "Fang Liu", "Lin Shi", "Minxiao Li", "Bo Shen", "An Fu"], "title": "CodeIF-Bench: Evaluating Instruction-Following Capabilities of Large Language Models in Interactive Code Generation", "categories": ["cs.SE", "cs.AI", "cs.PL"], "comment": null, "summary": "Large Language Models (LLMs) have demonstrated exceptional performance in\ncode generation tasks and have become indispensable programming assistants for\ndevelopers. However, existing code generation benchmarks primarily assess the\nfunctional correctness of code generated by LLMs in single-turn interactions,\noffering limited insight into their capabilities to generate code that strictly\nfollows users' instructions, especially in multi-turn interaction scenarios. In\nthis paper, we introduce CodeIF-Bench, a benchmark for evaluating LLMs'\ninstruction-following capabilities in interactive code generation.\nSpecifically, CodeIF-Bench incorporates nine types of verifiable instructions\naligned with the real-world software development requirements, which can be\nindependently and objectively validated through specified test cases,\nfacilitating the evaluation of instruction-following capability in multi-turn\ninteractions. We evaluate nine prominent LLMs using CodeIF-Bench, and the\nexperimental results reveal a significant disparity between their basic\nprogramming capability and instruction-following capability, particularly as\ntask complexity, context length, and the number of dialogue rounds increase."}
{"id": "2408.00920", "pdf": "https://arxiv.org/pdf/2408.00920", "abs": "https://arxiv.org/abs/2408.00920", "authors": ["Binchi Zhang", "Yushun Dong", "Tianhao Wang", "Jundong Li"], "title": "Towards Certified Unlearning for Deep Neural Networks", "categories": ["cs.LG", "stat.ML"], "comment": "ICML 2024 (errata)", "summary": "In the field of machine unlearning, certified unlearning has been extensively\nstudied in convex machine learning models due to its high efficiency and strong\ntheoretical guarantees. However, its application to deep neural networks\n(DNNs), known for their highly nonconvex nature, still poses challenges. To\nbridge the gap between certified unlearning and DNNs, we propose several simple\ntechniques to extend certified unlearning methods to nonconvex objectives. To\nreduce the time complexity, we develop an efficient computation method by\ninverse Hessian approximation without compromising certification guarantees. In\naddition, we extend our discussion of certification to nonconvergence training\nand sequential unlearning, considering that real-world users can send\nunlearning requests at different time points. Extensive experiments on three\nreal-world datasets demonstrate the efficacy of our method and the advantages\nof certified unlearning in DNNs."}
{"id": "2504.11895", "pdf": "https://arxiv.org/pdf/2504.11895", "abs": "https://arxiv.org/abs/2504.11895", "authors": ["Qishan Wang", "Jia Guo", "Shuyong Gao", "Haofen Wang", "Li Xiong", "Junjie Hu", "Hanqi Guo", "Wenqiang Zhang"], "title": "Search is All You Need for Few-shot Anomaly Detection", "categories": ["cs.CV"], "comment": null, "summary": "Few-shot anomaly detection (FSAD) has emerged as a crucial yet challenging\ntask in industrial inspection, where normal distribution modeling must be\naccomplished with only a few normal images. While existing approaches typically\nemploy multi-modal foundation models combining language and vision modalities\nfor prompt-guided anomaly detection, these methods often demand sophisticated\nprompt engineering and extensive manual tuning. In this paper, we demonstrate\nthat a straightforward nearest-neighbor search framework can surpass\nstate-of-the-art performance in both single-class and multi-class FSAD\nscenarios. Our proposed method, VisionAD, consists of four simple yet essential\ncomponents: (1) scalable vision foundation models that extract universal and\ndiscriminative features; (2) dual augmentation strategies - support\naugmentation to enhance feature matching adaptability and query augmentation to\naddress the oversights of single-view prediction; (3) multi-layer feature\nintegration that captures both low-frequency global context and high-frequency\nlocal details with minimal computational overhead; and (4) a class-aware visual\nmemory bank enabling efficient one-for-all multi-class detection. Extensive\nevaluations across MVTec-AD, VisA, and Real-IAD benchmarks demonstrate\nVisionAD's exceptional performance. Using only 1 normal images as support, our\nmethod achieves remarkable image-level AUROC scores of 97.4%, 94.8%, and 70.8%\nrespectively, outperforming current state-of-the-art approaches by significant\nmargins (+1.6%, +3.2%, and +1.4%). The training-free nature and superior\nfew-shot capabilities of VisionAD make it particularly appealing for real-world\napplications where samples are scarce or expensive to obtain. Code is available\nat https://github.com/Qiqigeww/VisionAD."}
{"id": "2504.04243", "pdf": "https://arxiv.org/pdf/2504.04243", "abs": "https://arxiv.org/abs/2504.04243", "authors": ["Jakob Schoeffer", "Maria De-Arteaga", "Jonathan Elmer"], "title": "Perils of Label Indeterminacy: A Case Study on Prediction of Neurological Recovery After Cardiac Arrest", "categories": ["cs.LG", "cs.AI", "cs.HC", "stat.ME"], "comment": "The 2025 ACM Conference on Fairness, Accountability, and Transparency\n  (FAccT '25)", "summary": "The design of AI systems to assist human decision-making typically requires\nthe availability of labels to train and evaluate supervised models. Frequently,\nhowever, these labels are unknown, and different ways of estimating them\ninvolve unverifiable assumptions or arbitrary choices. In this work, we\nintroduce the concept of label indeterminacy and derive important implications\nin high-stakes AI-assisted decision-making. We present an empirical study in a\nhealthcare context, focusing specifically on predicting the recovery of\ncomatose patients after resuscitation from cardiac arrest. Our study shows that\nlabel indeterminacy can result in models that perform similarly when evaluated\non patients with known labels, but vary drastically in their predictions for\npatients where labels are unknown. After demonstrating crucial ethical\nimplications of label indeterminacy in this high-stakes context, we discuss\ntakeaways for evaluation, reporting, and design."}
{"id": "2408.09655", "pdf": "https://arxiv.org/pdf/2408.09655", "abs": "https://arxiv.org/abs/2408.09655", "authors": ["Puning Zhao", "Rongfei Fan", "Shaowei Wang", "Li Shen", "Qixin Zhang", "Zong Ke", "Tianhang Zheng"], "title": "Contextual Bandits for Unbounded Context Distributions", "categories": ["cs.LG", "stat.ML"], "comment": null, "summary": "Nonparametric contextual bandit is an important model of sequential decision\nmaking problems. Under $\\alpha$-Tsybakov margin condition, existing research\nhas established a regret bound of\n$\\tilde{O}\\left(T^{1-\\frac{\\alpha+1}{d+2}}\\right)$ for bounded supports.\nHowever, the optimal regret with unbounded contexts has not been analyzed. The\nchallenge of solving contextual bandit problems with unbounded support is to\nachieve both exploration-exploitation tradeoff and bias-variance tradeoff\nsimultaneously. In this paper, we solve the nonparametric contextual bandit\nproblem with unbounded contexts. We propose two nearest neighbor methods\ncombined with UCB exploration. The first method uses a fixed $k$. Our analysis\nshows that this method achieves minimax optimal regret under a weak margin\ncondition and relatively light-tailed context distributions. The second method\nuses adaptive $k$. By a proper data-driven selection of $k$, this method\nachieves an expected regret of\n$\\tilde{O}\\left(T^{1-\\frac{(\\alpha+1)\\beta}{\\alpha+(d+2)\\beta}}+T^{1-\\beta}\\right)$,\nin which $\\beta$ is a parameter describing the tail strength. This bound\nmatches the minimax lower bound up to logarithm factors, indicating that the\nsecond method is approximately optimal."}
{"id": "2504.16612", "pdf": "https://arxiv.org/pdf/2504.16612", "abs": "https://arxiv.org/abs/2504.16612", "authors": ["Max Kirchner", "Alexander C. Jenke", "Sebastian Bodenstedt", "Fiona R. Kolbinger", "Oliver L. Saldanha", "Jakob N. Kather", "Martin Wagner", "Stefanie Speidel"], "title": "Federated EndoViT: Pretraining Vision Transformers via Federated Learning on Endoscopic Image Collections", "categories": ["cs.CV", "cs.LG"], "comment": "Preprint submitted to IEEE TMI", "summary": "Purpose: In this study, we investigate the training of foundation models\nusing federated learning to address data-sharing limitations and enable\ncollaborative model training without data transfer for minimally invasive\nsurgery. Methods: Inspired by the EndoViT study, we adapt the Masked\nAutoencoder for federated learning, enhancing it with adaptive Sharpness-Aware\nMinimization (FedSAM) and Stochastic Weight Averaging (SWA). Our model is\npretrained on the Endo700k dataset collection and later fine-tuned and\nevaluated for tasks such as Semantic Segmentation, Action Triplet Recognition,\nand Surgical Phase Recognition. Results: Our findings demonstrate that\nintegrating adaptive FedSAM into the federated MAE approach improves\npretraining, leading to a reduction in reconstruction loss per patch. The\napplication of FL-EndoViT in surgical downstream tasks results in performance\ncomparable to CEN-EndoViT. Furthermore, FL-EndoViT exhibits advantages over\nCEN-EndoViT in surgical scene segmentation when data is limited and in action\ntriplet recognition when large datasets are used. Conclusion: These findings\nhighlight the potential of federated learning for privacy-preserving training\nof surgical foundation models, offering a robust and generalizable solution for\nsurgical data science. Effective collaboration requires adapting federated\nlearning methods, such as the integration of FedSAM, which can accommodate the\ninherent data heterogeneity across institutions. In future, exploring FL in\nvideo-based models may enhance these capabilities by incorporating\nspatiotemporal dynamics crucial for real-world surgical environments."}
{"id": "2504.08603", "pdf": "https://arxiv.org/pdf/2504.08603", "abs": "https://arxiv.org/abs/2504.08603", "authors": ["Sebastián Barbas Laina", "Simon Boche", "Sotiris Papatheodorou", "Simon Schaefer", "Jaehyung Jung", "Stefan Leutenegger"], "title": "FindAnything: Open-Vocabulary and Object-Centric Mapping for Robot Exploration in Any Environment", "categories": ["cs.RO", "cs.AI", "cs.CV"], "comment": "11 pages, 5 figures", "summary": "Geometrically accurate and semantically expressive map representations have\nproven invaluable to facilitate robust and safe mobile robot navigation and\ntask planning. Nevertheless, real-time, open-vocabulary semantic understanding\nof large-scale unknown environments is still an open problem. In this paper we\npresent FindAnything, an open-world mapping and exploration framework that\nincorporates vision-language information into dense volumetric submaps. Thanks\nto the use of vision-language features, FindAnything bridges the gap between\npure geometric and open-vocabulary semantic information for a higher level of\nunderstanding while allowing to explore any environment without the help of any\nexternal source of ground-truth pose information. We represent the environment\nas a series of volumetric occupancy submaps, resulting in a robust and accurate\nmap representation that deforms upon pose updates when the underlying SLAM\nsystem corrects its drift, allowing for a locally consistent representation\nbetween submaps. Pixel-wise vision-language features are aggregated from\nefficient SAM (eSAM)-generated segments, which are in turn integrated into\nobject-centric volumetric submaps, providing a mapping from open-vocabulary\nqueries to 3D geometry that is scalable also in terms of memory usage. The\nopen-vocabulary map representation of FindAnything achieves state-of-the-art\nsemantic accuracy in closed-set evaluations on the Replica dataset. This level\nof scene understanding allows a robot to explore environments based on objects\nor areas of interest selected via natural language queries. Our system is the\nfirst of its kind to be deployed on resource-constrained devices, such as MAVs,\nleveraging vision-language information for real-world robotic tasks."}
{"id": "2410.00215", "pdf": "https://arxiv.org/pdf/2410.00215", "abs": "https://arxiv.org/abs/2410.00215", "authors": ["Yejin Lee", "Anna Sun", "Basil Hosmer", "Bilge Acun", "Can Balioglu", "Changhan Wang", "Charles David Hernandez", "Christian Puhrsch", "Daniel Haziza", "Driss Guessous", "Francisco Massa", "Jacob Kahn", "Jeffrey Wan", "Jeremy Reizenstein", "Jiaqi Zhai", "Joe Isaacson", "Joel Schlosser", "Juan Pino", "Kaushik Ram Sadagopan", "Leonid Shamis", "Linjian Ma", "Min-Jae Hwang", "Mingda Chen", "Mostafa Elhoushi", "Pedro Rodriguez", "Ram Pasunuru", "Scott Yih", "Sravya Popuri", "Xing Liu", "Carole-Jean Wu"], "title": "Characterizing and Efficiently Accelerating Multimodal Generation Model Inference", "categories": ["cs.LG"], "comment": "13 pages including references. 8 Figures. Under review to HPCA 2025\n  Industry Track", "summary": "Generative artificial intelligence (AI) technology is revolutionizing the\ncomputing industry. Not only its applications have broadened to various sectors\nbut also poses new system design and optimization opportunities. The technology\nis capable of understanding and responding in multiple modalities. However, the\nadvanced capability currently comes with significant system resource demands.\nTo sustainably scale generative AI capabilities to billions of users in the\nworld, inference must be fast and efficient. This paper pinpoints key system\ndesign and optimization opportunities by characterizing a family of emerging\nmulti-modal generation models on real systems. Auto-regressive token generation\nis a critical latency performance bottleneck, typically dominated by GPU idle\ntime. In addition to memory-intensive attention across the generative AI\nmodels, linear operations constitute significant inference latency due to the\nfeed forward networks in Transformer-based models. We demonstrate that\nstate-of-the-art optimization levers, spanning from applications to system\nsoftware and hardware, set a 3.88x better baseline."}
{"id": "2504.21356", "pdf": "https://arxiv.org/pdf/2504.21356", "abs": "https://arxiv.org/abs/2504.21356", "authors": ["Hong Zhang", "Zhongjie Duan", "Xingjun Wang", "Yuze Zhao", "Weiyi Lu", "Zhipeng Di", "Yixuan Xu", "Yingda Chen", "Yu Zhang"], "title": "Nexus-Gen: A Unified Model for Image Understanding, Generation, and Editing", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Unified multimodal large language models (MLLMs) aim to integrate multimodal\nunderstanding and generation abilities through a single framework. Despite\ntheir versatility, existing open-source unified models exhibit performance gaps\nagainst domain-specific architectures. To bridge this gap, we present\nNexus-Gen, a unified model that synergizes the language reasoning capabilities\nof LLMs with the image synthesis power of diffusion models. To align the\nembedding space of the LLM and diffusion model, we conduct a dual-phase\nalignment training process. (1) The autoregressive LLM learns to predict image\nembeddings conditioned on multimodal inputs, while (2) the vision decoder is\ntrained to reconstruct high-fidelity images from these embeddings. During\ntraining the LLM, we identified a critical discrepancy between the\nautoregressive paradigm's training and inference phases, where error\naccumulation in continuous embedding space severely degrades generation\nquality. To avoid this issue, we introduce a prefilled autoregression strategy\nthat prefills input sequence with position-embedded special tokens instead of\ncontinuous embeddings. Through dual-phase training, Nexus-Gen has developed the\nintegrated capability to comprehensively address the image understanding,\ngeneration and editing tasks. All models, datasets, and codes are published at\nhttps://github.com/modelscope/Nexus-Gen.git to facilitate further advancements\nacross the field."}
{"id": "2504.08837", "pdf": "https://arxiv.org/pdf/2504.08837", "abs": "https://arxiv.org/abs/2504.08837", "authors": ["Haozhe Wang", "Chao Qu", "Zuming Huang", "Wei Chu", "Fangzhen Lin", "Wenhu Chen"], "title": "VL-Rethinker: Incentivizing Self-Reflection of Vision-Language Models with Reinforcement Learning", "categories": ["cs.LG", "cs.AI"], "comment": "Preprint", "summary": "Recently, slow-thinking systems like GPT-o1 and DeepSeek-R1 have demonstrated\ngreat potential in solving challenging problems through explicit reflection.\nThey significantly outperform the best fast-thinking models, such as GPT-4o, on\nvarious math and science benchmarks. However, their multimodal reasoning\ncapabilities remain on par with fast-thinking models. For instance, GPT-o1's\nperformance on benchmarks like MathVista, MathVerse, and MathVision is similar\nto fast-thinking models. In this paper, we aim to enhance the slow-thinking\ncapabilities of vision-language models using reinforcement learning (without\nrelying on distillation) to advance the state of the art. First, we adapt the\nGRPO algorithm with a novel technique called Selective Sample Replay (SSR) to\naddress the vanishing advantages problem. While this approach yields strong\nperformance, the resulting RL-trained models exhibit limited self-reflection or\nself-verification. To further encourage slow-thinking, we introduce Forced\nRethinking, which appends a rethinking trigger token to the end of rollouts in\nRL training, explicitly enforcing a self-reflection reasoning step. By\ncombining these two techniques, our model, VL-Rethinker, advances\nstate-of-the-art scores on MathVista, MathVerse to achieve 80.4%, 63.5%\nrespectively. VL-Rethinker also achieves open-source SoTA on multi-disciplinary\nbenchmarks such as MathVision, MMMU-Pro, EMMA, and MEGA-Bench, narrowing the\ngap with OpenAI-o1. Our empirical results show the effectiveness of our\napproaches."}
{"id": "2410.00844", "pdf": "https://arxiv.org/pdf/2410.00844", "abs": "https://arxiv.org/abs/2410.00844", "authors": ["Zhenyi Zhang", "Tiejun Li", "Peijie Zhou"], "title": "Learning stochastic dynamics from snapshots through regularized unbalanced optimal transport", "categories": ["cs.LG", "math.OC", "physics.comp-ph", "q-bio.QM"], "comment": "Published as a conference paper at ICLR 2025 (oral)", "summary": "Reconstructing dynamics using samples from sparsely time-resolved snapshots\nis an important problem in both natural sciences and machine learning. Here, we\nintroduce a new deep learning approach for solving regularized unbalanced\noptimal transport (RUOT) and inferring continuous unbalanced stochastic\ndynamics from observed snapshots. Based on the RUOT form, our method models\nthese dynamics without requiring prior knowledge of growth and death processes\nor additional information, allowing them to be learned directly from data.\nTheoretically, we explore the connections between the RUOT and Schr\\\"odinger\nbridge problem and discuss the key challenges and potential solutions. The\neffectiveness of our method is demonstrated with a synthetic gene regulatory\nnetwork, high-dimensional Gaussian Mixture Model, and single-cell RNA-seq data\nfrom blood development. Compared with other methods, our approach accurately\nidentifies growth and transition patterns, eliminates false transitions, and\nconstructs the Waddington developmental landscape. Our code is available at:\nhttps://github.com/zhenyiizhang/DeepRUOT."}
{"id": "2504.21487", "pdf": "https://arxiv.org/pdf/2504.21487", "abs": "https://arxiv.org/abs/2504.21487", "authors": ["Hebaixu Wang", "Jing Zhang", "Haonan Guo", "Di Wang", "Jiayi Ma", "Bo Du"], "title": "DGSolver: Diffusion Generalist Solver with Universal Posterior Sampling for Image Restoration", "categories": ["cs.CV"], "comment": null, "summary": "Diffusion models have achieved remarkable progress in universal image\nrestoration. While existing methods speed up inference by reducing sampling\nsteps, substantial step intervals often introduce cumulative errors. Moreover,\nthey struggle to balance the commonality of degradation representations and\nrestoration quality. To address these challenges, we introduce\n\\textbf{DGSolver}, a diffusion generalist solver with universal posterior\nsampling. We first derive the exact ordinary differential equations for\ngeneralist diffusion models and tailor high-order solvers with a queue-based\naccelerated sampling strategy to improve both accuracy and efficiency. We then\nintegrate universal posterior sampling to better approximate\nmanifold-constrained gradients, yielding a more accurate noise estimation and\ncorrecting errors in inverse inference. Extensive experiments show that\nDGSolver outperforms state-of-the-art methods in restoration accuracy,\nstability, and scalability, both qualitatively and quantitatively. Code and\nmodels will be available at https://github.com/MiliLab/DGSolver."}
{"id": "2504.09609", "pdf": "https://arxiv.org/pdf/2504.09609", "abs": "https://arxiv.org/abs/2504.09609", "authors": ["Dohyeon Lee", "Jun-Gill Kang", "Soohee Han"], "title": "A highly maneuverable flying squirrel drone with agility-improving foldable wings", "categories": ["cs.RO", "cs.AI"], "comment": "Accepted to IEEE Robotics and Automation Letters. Project Page :\n  https://jgkang1210.github.io/fsdrone_ral/ , Video :\n  https://www.youtube.com/watch?v=tckIF3KCJig , Dohyeon Lee and Jun-Gill Kang\n  are co-authors", "summary": "Drones, like most airborne aerial vehicles, face inherent disadvantages in\nachieving agile flight due to their limited thrust capabilities. These physical\nconstraints cannot be fully addressed through advancements in control\nalgorithms alone. Drawing inspiration from the winged flying squirrel, this\npaper proposes a highly maneuverable drone equipped with agility-enhancing\nfoldable wings. By leveraging collaborative control between the conventional\npropeller system and the foldable wings-coordinated through the Thrust-Wing\nCoordination Control (TWCC) framework-the controllable acceleration set is\nexpanded, enabling the generation of abrupt vertical forces that are\nunachievable with traditional wingless drones. The complex aerodynamics of the\nfoldable wings are modeled using a physics-assisted recurrent neural network\n(paRNN), which calibrates the angle of attack (AOA) to align with the real\naerodynamic behavior of the wings. The additional air resistance generated by\nappropriately deploying these wings significantly improves the tracking\nperformance of the proposed \"flying squirrel\" drone. The model is trained on\nreal flight data and incorporates flat-plate aerodynamic principles.\nExperimental results demonstrate that the proposed flying squirrel drone\nachieves a 13.1% improvement in tracking performance, as measured by root mean\nsquare error (RMSE), compared to a conventional wingless drone. A demonstration\nvideo is available on YouTube: https://youtu.be/O8nrip18azY."}
{"id": "2410.02236", "pdf": "https://arxiv.org/pdf/2410.02236", "abs": "https://arxiv.org/abs/2410.02236", "authors": ["Ruohong Liu", "Yuxin Pan", "Linjie Xu", "Lei Song", "Jiang Bian", "Pengcheng You", "Yize Chen"], "title": "C-MORL: Multi-Objective Reinforcement Learning through Efficient Discovery of Pareto Front", "categories": ["cs.LG", "cs.SY", "eess.SY"], "comment": "Published as a conference paper at ICLR 2025. Code available at\n  https://github.com/RuohLiuq/C-MORL", "summary": "Multi-objective reinforcement learning (MORL) excels at handling rapidly\nchanging preferences in tasks that involve multiple criteria, even for unseen\npreferences. However, previous dominating MORL methods typically generate a\nfixed policy set or preference-conditioned policy through multiple training\niterations exclusively for sampled preference vectors, and cannot ensure the\nefficient discovery of the Pareto front. Furthermore, integrating preferences\ninto the input of policy or value functions presents scalability challenges, in\nparticular as the dimension of the state and preference space grow, which can\ncomplicate the learning process and hinder the algorithm's performance on more\ncomplex tasks. To address these issues, we propose a two-stage Pareto front\ndiscovery algorithm called Constrained MORL (C-MORL), which serves as a\nseamless bridge between constrained policy optimization and MORL. Concretely, a\nset of policies is trained in parallel in the initialization stage, with each\noptimized towards its individual preference over the multiple objectives. Then,\nto fill the remaining vacancies in the Pareto front, the constrained\noptimization steps are employed to maximize one objective while constraining\nthe other objectives to exceed a predefined threshold. Empirically, compared to\nrecent advancements in MORL methods, our algorithm achieves more consistent and\nsuperior performances in terms of hypervolume, expected utility, and sparsity\non both discrete and continuous control tasks, especially with numerous\nobjectives (up to nine objectives in our experiments)."}
{"id": "2504.21699", "pdf": "https://arxiv.org/pdf/2504.21699", "abs": "https://arxiv.org/abs/2504.21699", "authors": ["Abu Mohammed Raisuddin", "Jesper Holmblad", "Hamed Haghighi", "Yuri Poledna", "Maikol Funk Drechsler", "Valentina Donzella", "Eren Erdal Aksoy"], "title": "REHEARSE-3D: A Multi-modal Emulated Rain Dataset for 3D Point Cloud De-raining", "categories": ["cs.CV", "cs.RO"], "comment": null, "summary": "Sensor degradation poses a significant challenge in autonomous driving.\nDuring heavy rainfall, the interference from raindrops can adversely affect the\nquality of LiDAR point clouds, resulting in, for instance, inaccurate point\nmeasurements. This, in turn, can potentially lead to safety concerns if\nautonomous driving systems are not weather-aware, i.e., if they are unable to\ndiscern such changes. In this study, we release a new, large-scale, multi-modal\nemulated rain dataset, REHEARSE-3D, to promote research advancements in 3D\npoint cloud de-raining. Distinct from the most relevant competitors, our\ndataset is unique in several respects. First, it is the largest point-wise\nannotated dataset, and second, it is the only one with high-resolution LiDAR\ndata (LiDAR-256) enriched with 4D Radar point clouds logged in both daytime and\nnighttime conditions in a controlled weather environment. Furthermore,\nREHEARSE-3D involves rain-characteristic information, which is of significant\nvalue not only for sensor noise modeling but also for analyzing the impact of\nweather at a point level. Leveraging REHEARSE-3D, we benchmark raindrop\ndetection and removal in fused LiDAR and 4D Radar point clouds. Our\ncomprehensive study further evaluates the performance of various statistical\nand deep-learning models. Upon publication, the dataset and benchmark models\nwill be made publicly available at: https://sporsho.github.io/REHEARSE3D."}
{"id": "2504.10146", "pdf": "https://arxiv.org/pdf/2504.10146", "abs": "https://arxiv.org/abs/2504.10146", "authors": ["Jo-Ku Cheng", "Zeren Zhang", "Ran Chen", "Jingyang Deng", "Ziran Qin", "Jinwen Ma"], "title": "GeoUni: A Unified Model for Generating Geometry Diagrams, Problems and Problem Solutions", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "We propose GeoUni, the first unified geometry expert model capable of\ngenerating problem solutions and diagrams within a single framework in a way\nthat enables the creation of unique and individualized geometry problems.\nTraditionally, solving geometry problems and generating diagrams have been\ntreated as separate tasks in machine learning, with no models successfully\nintegrating both to support problem creation. However, we believe that mastery\nin geometry requires frictionless integration of all of these skills, from\nsolving problems to visualizing geometric relationships, and finally, crafting\ntailored problems. Our extensive experiments demonstrate that GeoUni, with only\n1.5B parameters, achieves performance comparable to larger models such as\nDeepSeek-R1 with 671B parameters in geometric reasoning tasks. GeoUni also\nexcels in generating precise geometric diagrams, surpassing both text-to-image\nmodels and unified models, including the GPT-4o image generation. Most\nimportantly, GeoUni is the only model capable of successfully generating\ntextual problems with matching diagrams based on specific knowledge points,\nthus offering a wider range of capabilities that extend beyond current models."}
{"id": "2410.07074", "pdf": "https://arxiv.org/pdf/2410.07074", "abs": "https://arxiv.org/abs/2410.07074", "authors": ["Zhengyu Hu", "Yichuan Li", "Zhengyu Chen", "Jingang Wang", "Han Liu", "Kyumin Lee", "Kaize Ding"], "title": "Let's Ask GNN: Empowering Large Language Model for Graph In-Context Learning", "categories": ["cs.LG"], "comment": null, "summary": "Textual Attributed Graphs (TAGs) are crucial for modeling complex real-world\nsystems, yet leveraging large language models (LLMs) for TAGs presents unique\nchallenges due to the gap between sequential text processing and\ngraph-structured data. We introduce AskGNN, a novel approach that bridges this\ngap by leveraging In-Context Learning (ICL) to integrate graph data and\ntask-specific information into LLMs. AskGNN employs a Graph Neural Network\n(GNN)-powered structure-enhanced retriever to select labeled nodes across\ngraphs, incorporating complex graph structures and their supervision signals.\nOur learning-to-retrieve algorithm optimizes the retriever to select example\nnodes that maximize LLM performance on graph. Experiments across three tasks\nand seven LLMs demonstrate AskGNN's superior effectiveness in graph task\nperformance, opening new avenues for applying LLMs to graph-structured data\nwithout extensive fine-tuning."}
{"id": "2505.02060", "pdf": "https://arxiv.org/pdf/2505.02060", "abs": "https://arxiv.org/abs/2505.02060", "authors": ["Branko Brkljač", "Vladimir Kalušev", "Branislav Popović", "Milan Sečujski"], "title": "Transforming faces into video stories -- VideoFace2.0", "categories": ["cs.CV", "68T07, 68T45, 68U10, 94A08, 68T05,", "I.2.10; I.5.4; I.5.5; I.4.8; C.3; J.7"], "comment": "4 Pages, 2 Figures, 1 Table, 1 Algorithm; Associated VideoFace2.0\n  code, test videos and results visualizations are available at\n  https://github.com/brkljac/VideoFace2.0 ; Preprint accepted for publication\n  at the 14th Mediterranean Conference on Embedded Computing (MECO), 10-14 June\n  2025, Budva, Montenegro", "summary": "Face detection and face recognition have been in the focus of vision\ncommunity since the very beginnings. Inspired by the success of the original\nVideoface digitizer, a pioneering device that allowed users to capture video\nsignals from any source, we have designed an advanced video analytics tool to\nefficiently create structured video stories, i.e. identity-based information\ncatalogs. VideoFace2.0 is the name of the developed system for spatial and\ntemporal localization of each unique face in the input video, i.e. face\nre-identification (ReID), which also allows their cataloging, characterization\nand creation of structured video outputs for later downstream tasks. Developed\nnear real-time solution is primarily designed to be utilized in application\nscenarios involving TV production, media analysis, and as an efficient tool for\ncreating large video datasets necessary for training machine learning (ML)\nmodels in challenging vision tasks such as lip reading and multimodal speech\nrecognition. Conducted experiments confirm applicability of the proposed face\nReID algorithm that is combining the concepts of face detection, face\nrecognition and passive tracking-by-detection in order to achieve robust and\nefficient face ReID. The system is envisioned as a compact and modular\nextensions of the existing video production equipment. Presented results are\nbased on test implementation that achieves between 18-25 fps on consumer type\nnotebook. Ablation experiments also confirmed that the proposed algorithm\nbrings relative gain in the reduction of number of false identities in the\nrange of 73%-93%. We hope that the presented work and shared code\nimplementation will stimulate further interest in development of similar,\napplication specific video analysis tools, and lower the entry barrier for\nproduction of high-quality multi-modal datasets in the future."}
{"id": "2504.13199", "pdf": "https://arxiv.org/pdf/2504.13199", "abs": "https://arxiv.org/abs/2504.13199", "authors": ["Mohammad Saleh", "Azadeh Tabatabaei"], "title": "Building Trustworthy Multimodal AI: A Review of Fairness, Transparency, and Ethics in Vision-Language Tasks", "categories": ["cs.CR", "cs.AI"], "comment": null, "summary": "Objective: This review explores the trustworthiness of multimodal artificial\nintelligence (AI) systems, specifically focusing on vision-language tasks. It\naddresses critical challenges related to fairness, transparency, and ethical\nimplications in these systems, providing a comparative analysis of key tasks\nsuch as Visual Question Answering (VQA), image captioning, and visual dialogue.\nBackground: Multimodal models, particularly vision-language models, enhance\nartificial intelligence (AI) capabilities by integrating visual and textual\ndata, mimicking human learning processes. Despite significant advancements, the\ntrustworthiness of these models remains a crucial concern, particularly as AI\nsystems increasingly confront issues regarding fairness, transparency, and\nethics. Methods: This review examines research conducted from 2017 to 2024\nfocusing on forenamed core vision-language tasks. It employs a comparative\napproach to analyze these tasks through the lens of trustworthiness,\nunderlining fairness, explainability, and ethics. This study synthesizes\nfindings from recent literature to identify trends, challenges, and\nstate-of-the-art solutions. Results: Several key findings were highlighted.\nTransparency: Explainability of vision language tasks is important for user\ntrust. Techniques, such as attention maps and gradient-based methods, have\nsuccessfully addressed this issue. Fairness: Bias mitigation in VQA and visual\ndialogue systems is essential for ensuring unbiased outcomes across diverse\ndemographic groups. Ethical Implications: Addressing biases in multilingual\nmodels and ensuring ethical data handling is critical for the responsible\ndeployment of vision-language systems. Conclusion: This study underscores the\nimportance of integrating fairness, transparency, and ethical considerations in\ndeveloping vision-language models within a unified framework."}
{"id": "2410.10572", "pdf": "https://arxiv.org/pdf/2410.10572", "abs": "https://arxiv.org/abs/2410.10572", "authors": ["Avrim Blum", "Donya Saless"], "title": "Regularized Robustly Reliable Learners and Instance Targeted Attacks", "categories": ["cs.LG", "cs.CR", "cs.DS", "stat.ML"], "comment": null, "summary": "Instance-targeted data poisoning attacks, where an adversary corrupts a\ntraining set to induce errors on specific test points, have raised significant\nconcerns. Balcan et al (2022) proposed an approach to addressing this challenge\nby defining a notion of robustly-reliable learners that provide per-instance\nguarantees of correctness under well-defined assumptions, even in the presence\nof data poisoning attacks. They then give a generic optimal (but\ncomputationally inefficient) robustly reliable learner as well as a\ncomputationally efficient algorithm for the case of linear separators over\nlog-concave distributions.\n  In this work, we address two challenges left open by Balcan et al (2022). The\nfirst is that the definition of robustly-reliable learners in Balcan et al\n(2022) becomes vacuous for highly-flexible hypothesis classes: if there are two\nclassifiers h_0, h_1 \\in H both with zero error on the training set such that\nh_0(x) \\neq h_1(x), then a robustly-reliable learner must abstain on x. We\naddress this problem by defining a modified notion of regularized\nrobustly-reliable learners that allows for nontrivial statements in this case.\nThe second is that the generic algorithm of Balcan et al (2022) requires\nre-running an ERM oracle (essentially, retraining the classifier) on each test\npoint x, which is generally impractical even if ERM can be implemented\nefficiently. To tackle this problem, we show that at least in certain\ninteresting cases we can design algorithms that can produce their outputs in\ntime sublinear in training time, by using techniques from dynamic algorithm\ndesign."}
{"id": "2505.02393", "pdf": "https://arxiv.org/pdf/2505.02393", "abs": "https://arxiv.org/abs/2505.02393", "authors": ["Sungheon Jeong", "Jihong Park", "Mohsen Imani"], "title": "Uncertainty-Weighted Image-Event Multimodal Fusion for Video Anomaly Detection", "categories": ["cs.CV"], "comment": null, "summary": "Most existing video anomaly detectors rely solely on RGB frames, which lack\nthe temporal resolution needed to capture abrupt or transient motion cues, key\nindicators of anomalous events. To address this limitation, we propose\nImage-Event Fusion for Video Anomaly Detection (IEF-VAD), a framework that\nsynthesizes event representations directly from RGB videos and fuses them with\nimage features through a principled, uncertainty-aware process. The system (i)\nmodels heavy-tailed sensor noise with a Student`s-t likelihood, deriving\nvalue-level inverse-variance weights via a Laplace approximation; (ii) applies\nKalman-style frame-wise updates to balance modalities over time; and (iii)\niteratively refines the fused latent state to erase residual cross-modal noise.\nWithout any dedicated event sensor or frame-level labels, IEF-VAD sets a new\nstate of the art across multiple real-world anomaly detection benchmarks. These\nfindings highlight the utility of synthetic event representations in\nemphasizing motion cues that are often underrepresented in RGB frames, enabling\naccurate and robust video understanding across diverse applications without\nrequiring dedicated event sensors. Code and models are available at\nhttps://github.com/EavnJeong/IEF-VAD."}
{"id": "2504.16381", "pdf": "https://arxiv.org/pdf/2504.16381", "abs": "https://arxiv.org/abs/2504.16381", "authors": ["Magnus Petersen", "Roberto Covino"], "title": "PINN-MEP: Continuous Neural Representations for Minimum-Energy Path Discovery in Molecular Systems", "categories": ["physics.chem-ph", "cs.AI", "physics.comp-ph"], "comment": "Update 08.05.2025: Added some intermediate steps in the derivation of\n  the loss to add clarity. Update 28.04.2025: Added citation and reference to\n  just-released work \"Action-Minimization Meets Generative Modeling: Efficient\n  Transition Path Sampling with the Onsager-Machlup Functional\" by Sanjeev Raja\n  et al. and added an appendix section clarifying some loss derivation steps", "summary": "Characterizing conformational transitions in physical systems remains a\nfundamental challenge in the computational sciences. Traditional sampling\nmethods like molecular dynamics (MD) or MCMC often struggle with the\nhigh-dimensional nature of molecular systems and the high energy barriers of\ntransitions between stable states. While these transitions are rare events in\nsimulation timescales, they often represent the most biologically significant\nprocesses - for example, the conformational change of an ion channel protein\nfrom its closed to open state, which controls cellular ion flow and is crucial\nfor neural signaling. Such transitions in real systems may take milliseconds to\nseconds but could require months or years of continuous simulation to observe\neven once. We present a method that reformulates transition path generation as\na continuous optimization problem solved through physics-informed neural\nnetworks (PINNs) inspired by string methods for minimum-energy path (MEP)\ngeneration. By representing transition paths as implicit neural functions and\nleveraging automatic differentiation with differentiable molecular dynamics\nforce fields, our method enables the efficient discovery of physically\nrealistic transition pathways without requiring expensive path sampling. We\ndemonstrate our method's effectiveness on two proteins, including an explicitly\nhydrated bovine pancreatic trypsin inhibitor (BPTI) system with over 8,300\natoms."}
{"id": "2410.14464", "pdf": "https://arxiv.org/pdf/2410.14464", "abs": "https://arxiv.org/abs/2410.14464", "authors": ["Jialu Tang", "Tong Xia", "Yuan Lu", "Cecilia Mascolo", "Aaqib Saeed"], "title": "Electrocardiogram-Language Model for Few-Shot Question Answering with Meta Learning", "categories": ["cs.LG"], "comment": "Accepted at AHLI CHIL 2025", "summary": "Electrocardiogram (ECG) interpretation requires specialized expertise, often\ninvolving synthesizing insights from ECG signals with complex clinical queries\nposed in natural language. The scarcity of labeled ECG data coupled with the\ndiverse nature of clinical inquiries presents a significant challenge for\ndeveloping robust and adaptable ECG diagnostic systems. This work introduces a\nnovel multimodal meta-learning method for few-shot ECG question answering,\naddressing the challenge of limited labeled data while leveraging the rich\nknowledge encoded within large language models (LLMs). Our LLM-agnostic\napproach integrates a pre-trained ECG encoder with a frozen LLM (e.g., LLaMA\nand Gemma) via a trainable fusion module, enabling the language model to reason\nabout ECG data and generate clinically meaningful answers. Extensive\nexperiments demonstrate superior generalization to unseen diagnostic tasks\ncompared to supervised baselines, achieving notable performance even with\nlimited ECG leads. For instance, in a 5-way 5-shot setting, our method using\nLLaMA-3.1-8B achieves an accuracy of 84.6%, 77.3%, and 69.6% on single verify,\nchoose and query question types, respectively. These results highlight the\npotential of our method to enhance clinical ECG interpretation by combining\nsignal processing with the nuanced language understanding capabilities of LLMs,\nparticularly in data-constrained scenarios."}
{"id": "2505.02784", "pdf": "https://arxiv.org/pdf/2505.02784", "abs": "https://arxiv.org/abs/2505.02784", "authors": ["Vladyslav Zalevskyi", "Thomas Sanchez", "Misha Kaandorp", "Margaux Roulet", "Diego Fajardo-Rojas", "Liu Li", "Jana Hutter", "Hongwei Bran Li", "Matthew Barkovich", "Hui Ji", "Luca Wilhelmi", "Aline Dändliker", "Céline Steger", "Mériam Koob", "Yvan Gomez", "Anton Jakovčić", "Melita Klaić", "Ana Adžić", "Pavel Marković", "Gracia Grabarić", "Milan Rados", "Jordina Aviles Verdera", "Gregor Kasprian", "Gregor Dovjak", "Raphael Gaubert-Rachmühl", "Maurice Aschwanden", "Qi Zeng", "Davood Karimi", "Denis Peruzzo", "Tommaso Ciceri", "Giorgio Longari", "Rachika E. Hamadache", "Amina Bouzid", "Xavier Lladó", "Simone Chiarella", "Gerard Martí-Juan", "Miguel Ángel González Ballester", "Marco Castellaro", "Marco Pinamonti", "Valentina Visani", "Robin Cremese", "Keïn Sam", "Fleur Gaudfernau", "Param Ahir", "Mehul Parikh", "Maximilian Zenk", "Michael Baumgartner", "Klaus Maier-Hein", "Li Tianhong", "Yang Hong", "Zhao Longfei", "Domen Preloznik", "Žiga Špiclin", "Jae Won Choi", "Muyang Li", "Jia Fu", "Guotai Wang", "Jingwen Jiang", "Lyuyang Tong", "Bo Du", "Andrea Gondova", "Sungmin You", "Kiho Im", "Abdul Qayyum", "Moona Mazher", "Steven A Niederer", "Andras Jakab", "Roxane Licandro", "Kelly Payette", "Meritxell Bach Cuadra"], "title": "Advances in Automated Fetal Brain MRI Segmentation and Biometry: Insights from the FeTA 2024 Challenge", "categories": ["cs.CV"], "comment": null, "summary": "Accurate fetal brain tissue segmentation and biometric analysis are essential\nfor studying brain development in utero. The FeTA Challenge 2024 advanced\nautomated fetal brain MRI analysis by introducing biometry prediction as a new\ntask alongside tissue segmentation. For the first time, our diverse\nmulti-centric test set included data from a new low-field (0.55T) MRI dataset.\nEvaluation metrics were also expanded to include the topology-specific Euler\ncharacteristic difference (ED). Sixteen teams submitted segmentation methods,\nmost of which performed consistently across both high- and low-field scans.\nHowever, longitudinal trends indicate that segmentation accuracy may be\nreaching a plateau, with results now approaching inter-rater variability. The\nED metric uncovered topological differences that were missed by conventional\nmetrics, while the low-field dataset achieved the highest segmentation scores,\nhighlighting the potential of affordable imaging systems when paired with\nhigh-quality reconstruction. Seven teams participated in the biometry task, but\nmost methods failed to outperform a simple baseline that predicted measurements\nbased solely on gestational age, underscoring the challenge of extracting\nreliable biometric estimates from image data alone. Domain shift analysis\nidentified image quality as the most significant factor affecting model\ngeneralization, with super-resolution pipelines also playing a substantial\nrole. Other factors, such as gestational age, pathology, and acquisition site,\nhad smaller, though still measurable, effects. Overall, FeTA 2024 offers a\ncomprehensive benchmark for multi-class segmentation and biometry estimation in\nfetal brain MRI, underscoring the need for data-centric approaches, improved\ntopological evaluation, and greater dataset diversity to enable clinically\nrobust and generalizable AI tools."}
{"id": "2505.00455", "pdf": "https://arxiv.org/pdf/2505.00455", "abs": "https://arxiv.org/abs/2505.00455", "authors": ["Sungbok Shin", "Hyeon Jeon", "Sanghyun Hong", "Niklas Elmqvist"], "title": "Data Therapist: Eliciting Domain Knowledge from Subject Matter Experts Using Large Language Models", "categories": ["cs.HC", "cs.AI"], "comment": "Submitted to IEEE VIS2025", "summary": "Effective data visualization requires not only technical proficiency but also\na deep understanding of the domain-specific context in which data exists. This\ncontext often includes tacit knowledge about data provenance, quality, and\nintended use, which is rarely explicit in the dataset itself. We present the\nData Therapist, a web-based tool that helps domain experts externalize this\nimplicit knowledge through a mixed-initiative process combining iterative Q&A\nwith interactive annotation. Powered by a large language model, the system\nanalyzes user-supplied datasets, prompts users with targeted questions, and\nallows annotation at varying levels of granularity. The resulting structured\nknowledge base can inform both human and automated visualization design. We\nevaluated the tool in a qualitative study involving expert pairs from Molecular\nBiology, Accounting, Political Science, and Usable Security. The study revealed\nrecurring patterns in how experts reason about their data and highlights areas\nwhere AI support can improve visualization design."}
{"id": "2411.01742", "pdf": "https://arxiv.org/pdf/2411.01742", "abs": "https://arxiv.org/abs/2411.01742", "authors": ["Dohyun Kim", "Pedro Sandoval-Segura"], "title": "Learning from Convolution-based Unlearnable Datasets", "categories": ["cs.LG", "cs.CV"], "comment": "Accepted to AdvML-Frontiers Workshop at NeurIPS 2024. Code available\n  at https://github.com/aseriesof-tubes/RSK", "summary": "The construction of large datasets for deep learning has raised concerns\nregarding unauthorized use of online data, leading to increased interest in\nprotecting data from third-parties who want to use it for training. The\nConvolution-based Unlearnable DAtaset (CUDA) method aims to make data\nunlearnable by applying class-wise blurs to every image in the dataset so that\nneural networks learn relations between blur kernels and labels, as opposed to\ninformative features for classifying clean data. In this work, we evaluate\nwhether CUDA data remains unlearnable after image sharpening and frequency\nfiltering, finding that this combination of simple transforms improves the\nutility of CUDA data for training. In particular, we observe a substantial\nincrease in test accuracy over adversarial training for models trained with\nCUDA unlearnable data from CIFAR-10, CIFAR-100, and ImageNet-100. In training\nmodels to high accuracy using unlearnable data, we underscore the need for\nongoing refinement in data poisoning techniques to ensure data privacy. Our\nmethod opens new avenues for enhancing the robustness of unlearnable datasets\nby highlighting that simple methods such as sharpening and frequency filtering\nare capable of breaking convolution-based unlearnable datasets."}
{"id": "2505.03149", "pdf": "https://arxiv.org/pdf/2505.03149", "abs": "https://arxiv.org/abs/2505.03149", "authors": ["Joseph Kettelkamp", "Ludovica Romanin", "Sarv Priya", "Mathews Jacob"], "title": "Motion-compensated cardiac MRI using low-rank diffeomorphic flow (DMoCo)", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "We introduce an unsupervised motion-compensated image reconstruction\nalgorithm for free-breathing and ungated 3D cardiac magnetic resonance imaging\n(MRI). We express the image volume corresponding to each specific motion phase\nas the deformation of a single static image template. The main contribution of\nthe work is the low-rank model for the compact joint representation of the\nfamily of diffeomorphisms, parameterized by the motion phases. The\ndiffeomorphism at a specific motion phase is obtained by integrating a\nparametric velocity field along a path connecting the reference template phase\nto the motion phase. The velocity field at different phases is represented\nusing a low-rank model. The static template and the low-rank motion model\nparameters are learned directly from the k-space data in an unsupervised\nfashion. The more constrained motion model is observed to offer improved\nrecovery compared to current motion-resolved and motion-compensated algorithms\nfor free-breathing 3D cine MRI."}
{"id": "2505.02417", "pdf": "https://arxiv.org/pdf/2505.02417", "abs": "https://arxiv.org/abs/2505.02417", "authors": ["Yunfeng Ge", "Jiawei Li", "Yiji Zhao", "Haomin Wen", "Zhao Li", "Meikang Qiu", "Hongyan Li", "Ming Jin", "Shirui Pan"], "title": "T2S: High-resolution Time Series Generation with Text-to-Series Diffusion Models", "categories": ["cs.LG", "cs.AI"], "comment": "Accepted by the 34th International Joint Conference on Artificial\n  Intelligence (IJCAI 2025)", "summary": "Text-to-Time Series generation holds significant potential to address\nchallenges such as data sparsity, imbalance, and limited availability of\nmultimodal time series datasets across domains. While diffusion models have\nachieved remarkable success in Text-to-X (e.g., vision and audio data)\ngeneration, their use in time series generation remains in its nascent stages.\nExisting approaches face two critical limitations: (1) the lack of systematic\nexploration of general-proposed time series captions, which are often\ndomain-specific and struggle with generalization; and (2) the inability to\ngenerate time series of arbitrary lengths, limiting their applicability to\nreal-world scenarios. In this work, we first categorize time series captions\ninto three levels: point-level, fragment-level, and instance-level.\nAdditionally, we introduce a new fragment-level dataset containing over 600,000\nhigh-resolution time series-text pairs. Second, we propose Text-to-Series\n(T2S), a diffusion-based framework that bridges the gap between natural\nlanguage and time series in a domain-agnostic manner. T2S employs a\nlength-adaptive variational autoencoder to encode time series of varying\nlengths into consistent latent embeddings. On top of that, T2S effectively\naligns textual representations with latent embeddings by utilizing Flow\nMatching and employing Diffusion Transformer as the denoiser. We train T2S in\nan interleaved paradigm across multiple lengths, allowing it to generate\nsequences of any desired length. Extensive evaluations demonstrate that T2S\nachieves state-of-the-art performance across 13 datasets spanning 12 domains."}
{"id": "2411.10729", "pdf": "https://arxiv.org/pdf/2411.10729", "abs": "https://arxiv.org/abs/2411.10729", "authors": ["Luciano S. Martinez-Rau", "Yuxuan Zhang", "Bengt Oelmann", "Sebastian Bader"], "title": "On-device Anomaly Detection in Conveyor Belt Operations", "categories": ["cs.LG", "cs.CE", "eess.SP"], "comment": "Preprint submitted to IEEE Sensors Journal", "summary": "Conveyor belts are crucial in mining operations by enabling the continuous\nand efficient movement of bulk materials over long distances, which directly\nimpacts productivity. While detecting anomalies in specific conveyor belt\ncomponents has been widely studied, identifying the root causes of these\nfailures, such as changing production conditions and operator errors, remains\ncritical. Continuous monitoring of mining conveyor belt work cycles is still at\nan early stage and requires robust solutions. Recently, an anomaly detection\nmethod for duty cycle operations of a mining conveyor belt has been proposed.\nBased on its limited performance and unevaluated long-term proper operation,\nthis study proposes two novel methods for classifying normal and abnormal duty\ncycles. The proposed approaches are pattern recognition systems that make use\nof threshold-based duty-cycle detection mechanisms, manually extracted\nfeatures, pattern-matching, and supervised tiny machine learning models. The\nexplored low-computational models include decision tree, random forest, extra\ntrees, extreme gradient boosting, Gaussian naive Bayes, and multi-layer\nperceptron. A comprehensive evaluation of the former and proposed approaches is\ncarried out on two datasets. Both proposed methods outperform the former\nmethod, with the best-performing approach being dataset-dependent. The\nheuristic rule-based approach achieves the highest performance in the same\ndataset used for algorithm training, with 97.3% for normal cycles and 80.2% for\nabnormal cycles. The ML-based approach performs better on a dataset including\nthe effects of machine aging, scoring 91.3% for normal cycles and 67.9% for\nabnormal cycles. Implemented on two low-power microcontrollers, the methods\ndemonstrate efficient, real-time operation with energy consumption of 13.3 and\n20.6 ${\\mu}$J during inference. These results offer valuable insights for\ndetecting ..."}
{"id": "2505.04105", "pdf": "https://arxiv.org/pdf/2505.04105", "abs": "https://arxiv.org/abs/2505.04105", "authors": ["Andrew Zhang", "Hao Wang", "Shuchang Ye", "Michael Fulham", "Jinman Kim"], "title": "MAISY: Motion-Aware Image SYnthesis for Medical Image Motion Correction", "categories": ["cs.CV"], "comment": null, "summary": "Patient motion during medical image acquisition causes blurring, ghosting,\nand distorts organs, which makes image interpretation challenging. Current\nstate-of-the-art algorithms using Generative Adversarial Network (GAN)-based\nmethods with their ability to learn the mappings between corrupted images and\ntheir ground truth via Structural Similarity Index Measure (SSIM) loss\neffectively generate motion-free images. However, we identified the following\nlimitations: (i) they mainly focus on global structural characteristics and\ntherefore overlook localized features that often carry critical pathological\ninformation, and (ii) the SSIM loss function struggles to handle images with\nvarying pixel intensities, luminance factors, and variance. In this study, we\npropose Motion-Aware Image SYnthesis (MAISY) which initially characterize\nmotion and then uses it for correction by: (a) leveraging the foundation model\nSegment Anything Model (SAM), to dynamically learn spatial patterns along\nanatomical boundaries where motion artifacts are most pronounced and, (b)\nintroducing the Variance-Selective SSIM (VS-SSIM) loss which adaptively\nemphasizes spatial regions with high pixel variance to preserve essential\nanatomical details during artifact correction. Experiments on chest and head CT\ndatasets demonstrate that our model outperformed the state-of-the-art\ncounterparts, with Peak Signal-to-Noise Ratio (PSNR) increasing by 40%, SSIM by\n10%, and Dice by 16%."}
{"id": "2505.03694", "pdf": "https://arxiv.org/pdf/2505.03694", "abs": "https://arxiv.org/abs/2505.03694", "authors": ["Parv Kapoor", "Ian Higgins", "Nikhil Keetha", "Jay Patrikar", "Brady Moon", "Zelin Ye", "Yao He", "Ivan Cisneros", "Yaoyu Hu", "Changliu Liu", "Eunsuk Kang", "Sebastian Scherer"], "title": "Demonstrating ViSafe: Vision-enabled Safety for High-speed Detect and Avoid", "categories": ["cs.RO", "cs.AI"], "comment": "13 pages, RSS 2025 Demo track, https://theairlab.org/visafe/", "summary": "Assured safe-separation is essential for achieving seamless high-density\noperation of airborne vehicles in a shared airspace. To equip\nresource-constrained aerial systems with this safety-critical capability, we\npresent ViSafe, a high-speed vision-only airborne collision avoidance system.\nViSafe offers a full-stack solution to the Detect and Avoid (DAA) problem by\ntightly integrating a learning-based edge-AI framework with a custom\nmulti-camera hardware prototype designed under SWaP-C constraints. By\nleveraging perceptual input-focused control barrier functions (CBF) to design,\nencode, and enforce safety thresholds, ViSafe can provide provably safe runtime\nguarantees for self-separation in high-speed aerial operations. We evaluate\nViSafe's performance through an extensive test campaign involving both\nsimulated digital twins and real-world flight scenarios. By independently\nvarying agent types, closure rates, interaction geometries, and environmental\nconditions (e.g., weather and lighting), we demonstrate that ViSafe\nconsistently ensures self-separation across diverse scenarios. In\nfirst-of-its-kind real-world high-speed collision avoidance tests with closure\nrates reaching 144 km/h, ViSafe sets a new benchmark for vision-only autonomous\ncollision avoidance, establishing a new standard for safety in high-speed\naerial navigation."}
{"id": "2412.00613", "pdf": "https://arxiv.org/pdf/2412.00613", "abs": "https://arxiv.org/abs/2412.00613", "authors": ["Xunye Tian", "Liuhua Peng", "Zhijian Zhou", "Mingming Gong", "Arthur Gretton", "Feng Liu"], "title": "A Unified Data Representation Learning for Non-parametric Two-sample Testing", "categories": ["cs.LG", "stat.ML"], "comment": "19 pages, 3 figures. To appear in Proceedings of the Fourty-First\n  Conference on Uncertainty in Artificial Intelligence (UAI 2025)", "summary": "Learning effective data representations has been crucial in non-parametric\ntwo-sample testing. Common approaches will first split data into training and\ntest sets and then learn data representations purely on the training set.\nHowever, recent theoretical studies have shown that, as long as the sample\nindexes are not used during the learning process, the whole data can be used to\nlearn data representations, meanwhile ensuring control of Type-I errors. The\nabove fact motivates us to use the test set (but without sample indexes) to\nfacilitate the data representation learning in the testing. To this end, we\npropose a representation-learning two-sample testing (RL-TST) framework. RL-TST\nfirst performs purely self-supervised representation learning on the entire\ndataset to capture inherent representations (IRs) that reflect the underlying\ndata manifold. A discriminative model is then trained on these IRs to learn\ndiscriminative representations (DRs), enabling the framework to leverage both\nthe rich structural information from IRs and the discriminative power of DRs.\nExtensive experiments demonstrate that RL-TST outperforms representative\napproaches by simultaneously using data manifold information in the test set\nand enhancing test power via finding the DRs with the training set."}
{"id": "2505.04150", "pdf": "https://arxiv.org/pdf/2505.04150", "abs": "https://arxiv.org/abs/2505.04150", "authors": ["Yu Yamaoka", "Weng Ian Chan", "Shigeto Seno", "Soichiro Fukada", "Hideo Matsuda"], "title": "Learning from Similarity Proportion Loss for Classifying Skeletal Muscle Recovery Stages", "categories": ["cs.CV", "cs.LG"], "comment": "MICCAI2024 workshop ADSMI in Morocco (oral) [Peer-reviewed]", "summary": "Evaluating the regeneration process of damaged muscle tissue is a fundamental\nanalysis in muscle research to measure experimental effect sizes and uncover\nmechanisms behind muscle weakness due to aging and disease. The conventional\napproach to assessing muscle tissue regeneration involves whole-slide imaging\nand expert visual inspection of the recovery stages based on the morphological\ninformation of cells and fibers. There is a need to replace these tasks with\nautomated methods incorporating machine learning techniques to ensure a\nquantitative and objective analysis. Given the limited availability of fully\nlabeled data, a possible approach is Learning from Label Proportions (LLP), a\nweakly supervised learning method using class label proportions. However,\ncurrent LLP methods have two limitations: (1) they cannot adapt the feature\nextractor for muscle tissues, and (2) they treat the classes representing\nrecovery stages and cell morphological changes as nominal, resulting in the\nloss of ordinal information. To address these issues, we propose Ordinal Scale\nLearning from Similarity Proportion (OSLSP), which uses a similarity proportion\nloss derived from two bag combinations. OSLSP can update the feature extractor\nby using class proportion attention to the ordinal scale of the class. Our\nmodel with OSLSP outperforms large-scale pre-trained and fine-tuning models in\nclassification tasks of skeletal muscle recovery stages."}
{"id": "2505.03750", "pdf": "https://arxiv.org/pdf/2505.03750", "abs": "https://arxiv.org/abs/2505.03750", "authors": ["Jinhai Hu", "Wang Ling Goh", "Yuan Gao"], "title": "AI-Powered Agile Analog Circuit Design and Optimization", "categories": ["cs.AR", "cs.AI"], "comment": "3 pages, 5 figures, AI4X, 2025", "summary": "Artificial intelligence (AI) techniques are transforming analog circuit\ndesign by automating device-level tuning and enabling system-level\nco-optimization. This paper integrates two approaches: (1) AI-assisted\ntransistor sizing using Multi-Objective Bayesian Optimization (MOBO) for direct\ncircuit parameter optimization, demonstrated on a linearly tunable\ntransconductor; and (2) AI-integrated circuit transfer function modeling for\nsystem-level optimization in a keyword spotting (KWS) application, demonstrated\nby optimizing an analog bandpass filter within a machine learning training\nloop. The combined insights highlight how AI can improve analog performance,\nreduce design iteration effort, and jointly optimize analog components and\napplication-level metrics."}
{"id": "2412.15496", "pdf": "https://arxiv.org/pdf/2412.15496", "abs": "https://arxiv.org/abs/2412.15496", "authors": ["Zhongtian Ma", "Qiaosheng Zhang", "Bocheng Zhou", "Yexin Zhang", "Shuyue Hu", "Zhen Wang"], "title": "Graph Attention is Not Always Beneficial: A Theoretical Analysis of Graph Attention Mechanisms via Contextual Stochastic Block Models", "categories": ["cs.LG", "stat.ML"], "comment": "Accepted by ICML 2025", "summary": "Despite the growing popularity of graph attention mechanisms, their\ntheoretical understanding remains limited. This paper aims to explore the\nconditions under which these mechanisms are effective in node classification\ntasks through the lens of Contextual Stochastic Block Models (CSBMs). Our\ntheoretical analysis reveals that incorporating graph attention mechanisms is\n\\emph{not universally beneficial}. Specifically, by appropriately defining\n\\emph{structure noise} and \\emph{feature noise} in graphs, we show that graph\nattention mechanisms can enhance classification performance when structure\nnoise exceeds feature noise. Conversely, when feature noise predominates,\nsimpler graph convolution operations are more effective. Furthermore, we\nexamine the over-smoothing phenomenon and show that, in the high\nsignal-to-noise ratio (SNR) regime, graph convolutional networks suffer from\nover-smoothing, whereas graph attention mechanisms can effectively resolve this\nissue. Building on these insights, we propose a novel multi-layer Graph\nAttention Network (GAT) architecture that significantly outperforms\nsingle-layer GATs in achieving \\emph{perfect node classification} in CSBMs,\nrelaxing the SNR requirement from $ \\omega(\\sqrt{\\log n}) $ to $\n\\omega(\\sqrt{\\log n} / \\sqrt[3]{n}) $. To our knowledge, this is the first\nstudy to delineate the conditions for perfect node classification using\nmulti-layer GATs. Our theoretical contributions are corroborated by extensive\nexperiments on both synthetic and real-world datasets, highlighting the\npractical implications of our findings."}
{"id": "2505.04485", "pdf": "https://arxiv.org/pdf/2505.04485", "abs": "https://arxiv.org/abs/2505.04485", "authors": ["Ali Alawieh", "Alexandru P. Condurache"], "title": "FA-KPConv: Introducing Euclidean Symmetries to KPConv via Frame Averaging", "categories": ["cs.CV"], "comment": "8 pages, 2 figures, accepted at IJCNN 2025", "summary": "We present Frame-Averaging Kernel-Point Convolution (FA-KPConv), a neural\nnetwork architecture built on top of the well-known KPConv, a widely adopted\nbackbone for 3D point cloud analysis. Even though invariance and/or\nequivariance to Euclidean transformations are required for many common tasks,\nKPConv-based networks can only approximately achieve such properties when\ntraining on large datasets or with significant data augmentations. Using Frame\nAveraging, we allow to flexibly customize point cloud neural networks built\nwith KPConv layers, by making them exactly invariant and/or equivariant to\ntranslations, rotations and/or reflections of the input point clouds. By simply\nwrapping around an existing KPConv-based network, FA-KPConv embeds geometrical\nprior knowledge into it while preserving the number of learnable parameters and\nnot compromising any input information. We showcase the benefit of such an\nintroduced bias for point cloud classification and point cloud registration,\nespecially in challenging cases such as scarce training data or randomly\nrotated test data."}
{"id": "2505.04165", "pdf": "https://arxiv.org/pdf/2505.04165", "abs": "https://arxiv.org/abs/2505.04165", "authors": ["Kairong Yu", "Tianqing Zhang", "Qi Xu", "Gang Pan", "Hongwei Wang"], "title": "TS-SNN: Temporal Shift Module for Spiking Neural Networks", "categories": ["cs.NE", "cs.AI"], "comment": "Accepted by ICML2025", "summary": "Spiking Neural Networks (SNNs) are increasingly recognized for their\nbiological plausibility and energy efficiency, positioning them as strong\nalternatives to Artificial Neural Networks (ANNs) in neuromorphic computing\napplications. SNNs inherently process temporal information by leveraging the\nprecise timing of spikes, but balancing temporal feature utilization with low\nenergy consumption remains a challenge. In this work, we introduce Temporal\nShift module for Spiking Neural Networks (TS-SNN), which incorporates a novel\nTemporal Shift (TS) module to integrate past, present, and future spike\nfeatures within a single timestep via a simple yet effective shift operation. A\nresidual combination method prevents information loss by integrating shifted\nand original features. The TS module is lightweight, requiring only one\nadditional learnable parameter, and can be seamlessly integrated into existing\narchitectures with minimal additional computational cost. TS-SNN achieves\nstate-of-the-art performance on benchmarks like CIFAR-10 (96.72\\%), CIFAR-100\n(80.28\\%), and ImageNet (70.61\\%) with fewer timesteps, while maintaining low\nenergy consumption. This work marks a significant step forward in developing\nefficient and accurate SNN architectures."}
{"id": "2501.19040", "pdf": "https://arxiv.org/pdf/2501.19040", "abs": "https://arxiv.org/abs/2501.19040", "authors": ["Huanran Chen", "Yinpeng Dong", "Zeming Wei", "Hang Su", "Jun Zhu"], "title": "Towards the Worst-case Robustness of Large Language Models", "categories": ["cs.LG"], "comment": null, "summary": "Recent studies have revealed the vulnerability of large language models to\nadversarial attacks, where adversaries craft specific input sequences to induce\nharmful, violent, private, or incorrect outputs. In this work, we study their\nworst-case robustness, i.e., whether an adversarial example exists that leads\nto such undesirable outputs. We upper bound the worst-case robustness using\nstronger white-box attacks, indicating that most current deterministic defenses\nachieve nearly 0\\% worst-case robustness. We propose a general tight lower\nbound for randomized smoothing using fractional knapsack solvers or 0-1\nknapsack solvers, and using them to bound the worst-case robustness of all\nstochastic defenses. Based on these solvers, we provide theoretical lower\nbounds for several previous empirical defenses. For example, we certify the\nrobustness of a specific case, smoothing using a uniform kernel, against\n\\textit{any possible attack} with an average $\\ell_0$ perturbation of 2.02 or\nan average suffix length of 6.41."}
{"id": "2505.04497", "pdf": "https://arxiv.org/pdf/2505.04497", "abs": "https://arxiv.org/abs/2505.04497", "authors": ["Aditi Ramaswamy", "Hana Chockler", "Melane Navaratnarajah"], "title": "Defining and Quantifying Creative Behavior in Popular Image Generators", "categories": ["cs.CV", "cs.AI", "I.4.m; I.2.m"], "comment": null, "summary": "Creativity of generative AI models has been a subject of scientific debate in\nthe last years, without a conclusive answer. In this paper, we study creativity\nfrom a practical perspective and introduce quantitative measures that help the\nuser to choose a suitable AI model for a given task. We evaluated our measures\non a number of popular image-to-image generation models, and the results of\nthis suggest that our measures conform to human intuition."}
{"id": "2503.01876", "pdf": "https://arxiv.org/pdf/2503.01876", "abs": "https://arxiv.org/abs/2503.01876", "authors": ["Zhanpeng He", "Yifeng Cao", "Matei Ciocarlie"], "title": "Uncertainty Comes for Free: Human-in-the-Loop Policies with Diffusion Models", "categories": ["cs.LG", "cs.RO"], "comment": null, "summary": "Human-in-the-loop (HitL) robot deployment has gained significant attention in\nboth academia and industry as a semi-autonomous paradigm that enables human\noperators to intervene and adjust robot behaviors at deployment time, improving\nsuccess rates. However, continuous human monitoring and intervention can be\nhighly labor-intensive and impractical when deploying a large number of robots.\nTo address this limitation, we propose a method that allows diffusion policies\nto actively seek human assistance only when necessary, reducing reliance on\nconstant human oversight. To achieve this, we leverage the generative process\nof diffusion policies to compute an uncertainty-based metric based on which the\nautonomous agent can decide to request operator assistance at deployment time,\nwithout requiring any operator interaction during training. Additionally, we\nshow that the same method can be used for efficient data collection for\nfine-tuning diffusion policies in order to improve their autonomous\nperformance. Experimental results from simulated and real-world environments\ndemonstrate that our approach enhances policy performance during deployment for\na variety of scenarios."}
{"id": "2505.04512", "pdf": "https://arxiv.org/pdf/2505.04512", "abs": "https://arxiv.org/abs/2505.04512", "authors": ["Teng Hu", "Zhentao Yu", "Zhengguang Zhou", "Sen Liang", "Yuan Zhou", "Qin Lin", "Qinglin Lu"], "title": "HunyuanCustom: A Multimodal-Driven Architecture for Customized Video Generation", "categories": ["cs.CV"], "comment": null, "summary": "Customized video generation aims to produce videos featuring specific\nsubjects under flexible user-defined conditions, yet existing methods often\nstruggle with identity consistency and limited input modalities. In this paper,\nwe propose HunyuanCustom, a multi-modal customized video generation framework\nthat emphasizes subject consistency while supporting image, audio, video, and\ntext conditions. Built upon HunyuanVideo, our model first addresses the\nimage-text conditioned generation task by introducing a text-image fusion\nmodule based on LLaVA for enhanced multi-modal understanding, along with an\nimage ID enhancement module that leverages temporal concatenation to reinforce\nidentity features across frames. To enable audio- and video-conditioned\ngeneration, we further propose modality-specific condition injection\nmechanisms: an AudioNet module that achieves hierarchical alignment via spatial\ncross-attention, and a video-driven injection module that integrates\nlatent-compressed conditional video through a patchify-based feature-alignment\nnetwork. Extensive experiments on single- and multi-subject scenarios\ndemonstrate that HunyuanCustom significantly outperforms state-of-the-art open-\nand closed-source methods in terms of ID consistency, realism, and text-video\nalignment. Moreover, we validate its robustness across downstream tasks,\nincluding audio and video-driven customized video generation. Our results\nhighlight the effectiveness of multi-modal conditioning and identity-preserving\nstrategies in advancing controllable video generation. All the code and models\nare available at https://hunyuancustom.github.io."}
{"id": "2503.13764", "pdf": "https://arxiv.org/pdf/2503.13764", "abs": "https://arxiv.org/abs/2503.13764", "authors": ["Mohammad Partohaghighi", "Roummel Marcia", "YangQuan Chen"], "title": "Effective Dimension Aware Fractional-Order Stochastic Gradient Descent for Convex Optimization Problems", "categories": ["cs.LG", "math.OC"], "comment": "IEEE L-CSS submitted", "summary": "Fractional-order stochastic gradient descent (FOSGD) leverages fractional\nexponents to capture long-memory effects in optimization. However, its utility\nis often limited by the difficulty of tuning and stabilizing these exponents.\nWe propose 2SED Fractional-Order Stochastic Gradient Descent (2SEDFOSGD), which\nintegrates the Two-Scale Effective Dimension (2SED) algorithm with FOSGD to\nadapt the fractional exponent in a data-driven manner. By tracking model\nsensitivity and effective dimensionality, 2SEDFOSGD dynamically modulates the\nexponent to mitigate oscillations and hasten convergence. Theoretically, this\napproach preserves the advantages of fractional memory without the sluggish or\nunstable behavior observed in na\\\"ive fractional SGD. Empirical evaluations in\nGaussian and $\\alpha$-stable noise scenarios using an autoregressive (AR)\nmodel\\textcolor{red}{, as well as on the MNIST and CIFAR-100 datasets for image\nclassification,} highlight faster convergence and more robust parameter\nestimates compared to baseline methods, underscoring the potential of\ndimension-aware fractional techniques for advanced modeling and estimation\ntasks."}
{"id": "2505.04594", "pdf": "https://arxiv.org/pdf/2505.04594", "abs": "https://arxiv.org/abs/2505.04594", "authors": ["Zhihao Zhang", "Abhinav Kumar", "Girish Chandar Ganesan", "Xiaoming Liu"], "title": "MonoCoP: Chain-of-Prediction for Monocular 3D Object Detection", "categories": ["cs.CV"], "comment": null, "summary": "Accurately predicting 3D attributes is crucial for monocular 3D object\ndetection (Mono3D), with depth estimation posing the greatest challenge due to\nthe inherent ambiguity in mapping 2D images to 3D space. While existing methods\nleverage multiple depth cues (e.g., estimating depth uncertainty, modeling\ndepth error) to improve depth accuracy, they overlook that accurate depth\nprediction requires conditioning on other 3D attributes, as these attributes\nare intrinsically inter-correlated through the 3D to 2D projection, which\nultimately limits overall accuracy and stability. Inspired by Chain-of-Thought\n(CoT) in large language models (LLMs), this paper proposes MonoCoP, which\nleverages a Chain-of-Prediction (CoP) to predict attributes sequentially and\nconditionally via three key designs. First, it employs a lightweight\nAttributeNet (AN) for each 3D attribute to learn attribute-specific features.\nNext, MonoCoP constructs an explicit chain to propagate these learned features\nfrom one attribute to the next. Finally, MonoCoP uses a residual connection to\naggregate features for each attribute along the chain, ensuring that later\nattribute predictions are conditioned on all previously processed attributes\nwithout forgetting the features of earlier ones. Experimental results show that\nour MonoCoP achieves state-of-the-art (SoTA) performance on the KITTI\nleaderboard without requiring additional data and further surpasses existing\nmethods on the Waymo and nuScenes frontal datasets."}
{"id": "2503.16659", "pdf": "https://arxiv.org/pdf/2503.16659", "abs": "https://arxiv.org/abs/2503.16659", "authors": ["Viet Thanh Duy Nguyen", "Truong-Son Hy"], "title": "Advances in Protein Representation Learning: Methods, Applications, and Future Directions", "categories": ["cs.LG", "q-bio.BM"], "comment": null, "summary": "Proteins are complex biomolecules that play a central role in various\nbiological processes, making them critical targets for breakthroughs in\nmolecular biology, medical research, and drug discovery. Deciphering their\nintricate, hierarchical structures, and diverse functions is essential for\nadvancing our understanding of life at the molecular level. Protein\nRepresentation Learning (PRL) has emerged as a transformative approach,\nenabling the extraction of meaningful computational representations from\nprotein data to address these challenges. In this paper, we provide a\ncomprehensive review of PRL research, categorizing methodologies into five key\nareas: feature-based, sequence-based, structure-based, multimodal, and\ncomplex-based approaches. To support researchers in this rapidly evolving\nfield, we introduce widely used databases for protein sequences, structures,\nand functions, which serve as essential resources for model development and\nevaluation. We also explore the diverse applications of these approaches in\nmultiple domains, demonstrating their broad impact. Finally, we discuss\npressing technical challenges and outline future directions to advance PRL,\noffering insights to inspire continued innovation in this foundational field."}
{"id": "2309.14630", "pdf": "https://arxiv.org/pdf/2309.14630", "abs": "https://arxiv.org/abs/2309.14630", "authors": ["Florian Gunsilius", "David Van Dijcke"], "title": "Free Discontinuity Regression: With an Application to the Economic Effects of Internet Shutdowns", "categories": ["econ.EM", "cs.CV", "math.ST", "stat.AP", "stat.ME", "stat.TH"], "comment": "24 pages, 3 figures, 2 tables; authors listed alphabetically; code\n  available at https://github.com/Davidvandijcke/fdr", "summary": "Sharp, multidimensional changepoints-abrupt shifts in a regression surface\nwhose locations and magnitudes are unknown-arise in settings as varied as\ngene-expression profiling, financial covariance breaks, climate-regime\ndetection, and urban socioeconomic mapping. Despite their prevalence, there are\nno current approaches that jointly estimate the location and size of the\ndiscontinuity set in a one-shot approach with statistical guarantees. We\ntherefore introduce Free Discontinuity Regression (FDR), a fully nonparametric\nestimator that simultaneously (i) smooths a regression surface, (ii) segments\nit into contiguous regions, and (iii) provably recovers the precise locations\nand sizes of its jumps. By extending a convex relaxation of the Mumford-Shah\nfunctional to random spatial sampling and correlated noise, FDR overcomes the\nfixed-grid and i.i.d. noise assumptions of classical image-segmentation\napproaches, thus enabling its application to real-world data of any dimension.\nThis yields the first identification and uniform consistency results for\nmultivariate jump surfaces: under mild SBV regularity, the estimated function,\nits discontinuity set, and all jump sizes converge to their true population\ncounterparts. Hyperparameters are selected automatically from the data using\nStein's Unbiased Risk Estimate, and large-scale simulations up to three\ndimensions validate the theoretical results and demonstrate good finite-sample\nperformance. Applying FDR to an internet shutdown in India reveals a 25-35%\nreduction in economic activity around the estimated shutdown boundaries-much\nlarger than previous estimates. By unifying smoothing, segmentation, and\neffect-size recovery in a general statistical setting, FDR turns\nfree-discontinuity ideas into a practical tool with formal guarantees for\nmodern multivariate data."}
{"id": "2503.22480", "pdf": "https://arxiv.org/pdf/2503.22480", "abs": "https://arxiv.org/abs/2503.22480", "authors": ["Wangtao Sun", "Xiang Cheng", "Xing Yu", "Haotian Xu", "Zhao Yang", "Shizhu He", "Jun Zhao", "Kang Liu"], "title": "Probabilistic Uncertain Reward Model", "categories": ["cs.LG"], "comment": null, "summary": "Reinforcement Learning from Human Feedback (RLHF) has emerged as a critical\ntechnique for training large language models. However, reward hacking-a\nphenomenon where models exploit flaws in the reward model-remains a significant\nbarrier to achieving robust and scalable intelligence through long-term\ntraining. Existing studies have proposed the uncertain reward models to address\nreward hacking, however, they often lack systematic or theoretical foundations,\nfailing to model the uncertainty intrinsically emerging from preference data,\nand thus cannot sufficiently mitigate reward hacking to sustain prolonged RLHF\ntraining and exploration. In this paper, we propose a Probabilistic Uncertain\nReward Model (PURM), a natural generalization of the classical Bradley-Terry\nreward model, that can directly learn the reward distribution emerged from the\npreference data. We theoretically derived PURM's loss function and the\nuncertainty of the reward distribution. To mitigate reward hacking with PURM,\nwe further introduce an uncertainty-aware penalty into Proximal Policy\nOptimization (PPO), which leverages the learned uncertainty to dynamically\nbalance reward optimization and exploration. Experimental results demonstrate\nthat PURM significantly delays the onset of reward hacking while improving\nfinal performance compared with existing methods. We also find that PURM\ngenuinely produce sound reward and uncertainty estimations. The data and code\nof this paper can be found at\nhttps://anonymous.4open.science/r/Probabilistic-Uncertain-Reward-Model/"}
{"id": "2408.17237", "pdf": "https://arxiv.org/pdf/2408.17237", "abs": "https://arxiv.org/abs/2408.17237", "authors": ["John M. Ball", "Christopher L. Horner"], "title": "A nonlinear elasticity model in computer vision", "categories": ["math.AP", "cs.CV", "94A08, 74B20"], "comment": "The paper has been substantially revised. In particular the section\n  on metrics has been rewritten to correct an error, and a new result added on\n  the existence of discrete morphing sequences in the mass-conserving case. In\n  the mass-conserving case there is a new formulation of the question\n  concerning whether the minimizing deformation for affinely related images is\n  the corresponding affine map", "summary": "The purpose of this paper is to analyze a nonlinear elasticity model\nintroduced by the authors for comparing two images, regarded as bounded open\nsubsets of $\\R^n$ together with associated vector-valued intensity maps.\nOptimal transformations between the images are sought as minimisers of an\nintegral functional among orientation-preserving homeomorphisms. The existence\nof minimisers is proved under natural coercivity and polyconvexity conditions,\nassuming only that the intensity functions are bounded measurable. Variants of\nthe existence theorem are also proved, first under the constraint that finite\nsets of landmark points in the two images are mapped one to the other, and\nsecond when one image is to be compared to an unknown part of another.\n  The question is studied as to whether for images related by an affine mapping\nthe unique minimiser is given by that affine mapping. For a natural class of\nfunctional integrands an example is given guaranteeing that this property holds\nfor pairs of images in which the second is a scaling of the first by a constant\nfactor. However for the property to hold for arbitrary pairs of affinely\nrelated images it is shown that the integrand has to depend on the gradient of\nthe transformation as a convex function of its determinant alone. This suggests\na new model in which the integrand depends also on second derivatives of the\ntransformation, and an example is given for which both existence of minimisers\nis assured and the above property holds for all pairs of affinely related\nimages."}
{"id": "2504.09246", "pdf": "https://arxiv.org/pdf/2504.09246", "abs": "https://arxiv.org/abs/2504.09246", "authors": ["Niels Mündler", "Jingxuan He", "Hao Wang", "Koushik Sen", "Dawn Song", "Martin Vechev"], "title": "Type-Constrained Code Generation with Language Models", "categories": ["cs.LG", "cs.PL"], "comment": null, "summary": "Large language models (LLMs) have achieved notable success in code\ngeneration. However, they still frequently produce uncompilable output because\ntheir next-token inference procedure does not model formal aspects of code.\nAlthough constrained decoding is a promising approach to alleviate this issue,\nit has only been applied to handle either domain-specific languages or\nsyntactic features of general-purpose programming languages. However, LLMs\nfrequently generate code with typing errors, which are beyond the domain of\nsyntax and generally hard to adequately constrain. To address this challenge,\nwe introduce a type-constrained decoding approach that leverages type systems\nto guide code generation. For this purpose, we develop novel prefix automata\nand a search over inhabitable types, forming a sound approach to enforce\nwell-typedness on LLM-generated code. We formalize our approach on a\nfoundational simply-typed language and extend it to TypeScript to demonstrate\npracticality. Our evaluation on the HumanEval and MBPP datasets shows that our\napproach reduces compilation errors by more than half and significantly\nincreases functional correctness in code synthesis, translation, and repair\ntasks across LLMs of various sizes and model families, including\nstate-of-the-art open-weight models with more than 30B parameters. The results\ndemonstrate the generality and effectiveness of our approach in constraining\nLLM code generation with formal rules of type systems."}
{"id": "2409.16111", "pdf": "https://arxiv.org/pdf/2409.16111", "abs": "https://arxiv.org/abs/2409.16111", "authors": ["Yannik Blei", "Michael Krawez", "Nisarga Nilavadi", "Tanja Katharina Kaiser", "Wolfram Burgard"], "title": "CloudTrack: Scalable UAV Tracking with Cloud Semantics", "categories": ["cs.RO", "cs.CV"], "comment": "7 pages, 3 figures", "summary": "Nowadays, unmanned aerial vehicles (UAVs) are commonly used in search and\nrescue scenarios to gather information in the search area. The automatic\nidentification of the person searched for in aerial footage could increase the\nautonomy of such systems, reduce the search time, and thus increase the missed\nperson's chances of survival. In this paper, we present a novel approach to\nperform semantically conditioned open vocabulary object tracking that is\nspecifically designed to cope with the limitations of UAV hardware. Our\napproach has several advantages. It can run with verbal descriptions of the\nmissing person, e.g., the color of the shirt, it does not require dedicated\ntraining to execute the mission and can efficiently track a potentially moving\nperson. Our experimental results demonstrate the versatility and efficacy of\nour approach."}
{"id": "2504.17664", "pdf": "https://arxiv.org/pdf/2504.17664", "abs": "https://arxiv.org/abs/2504.17664", "authors": ["Grégory Bournassenko"], "title": "On Multivariate Financial Time Series Classification", "categories": ["cs.LG"], "comment": null, "summary": "This article investigates the use of Machine Learning and Deep Learning\nmodels in multivariate time series analysis within financial markets. It\ncompares small and big data approaches, focusing on their distinct challenges\nand the benefits of scaling. Traditional methods such as SVMs are contrasted\nwith modern architectures like ConvTimeNet. The results show the importance of\nusing and understanding Big Data in depth in the analysis and prediction of\nfinancial time series."}
{"id": "2411.07848", "pdf": "https://arxiv.org/pdf/2411.07848", "abs": "https://arxiv.org/abs/2411.07848", "authors": ["Sonia Raychaudhuri", "Duy Ta", "Katrina Ashton", "Angel X. Chang", "Jiuguang Wang", "Bernadette Bucher"], "title": "Zero-shot Object-Centric Instruction Following: Integrating Foundation Models with Traditional Navigation", "categories": ["cs.RO", "cs.CV"], "comment": null, "summary": "Large scale scenes such as multifloor homes can be robustly and efficiently\nmapped with a 3D graph of landmarks estimated jointly with robot poses in a\nfactor graph, a technique commonly used in commercial robots such as drones and\nrobot vacuums. In this work, we propose Language-Inferred Factor Graph for\nInstruction Following (LIFGIF), a zero-shot method to ground natural language\ninstructions in such a map. LIFGIF also includes a policy for following natural\nlanguage navigation instructions in a novel environment while the map is\nconstructed, enabling robust navigation performance in the physical world. To\nevaluate LIFGIF, we present a new dataset, Object-Centric VLN (OC-VLN), in\norder to evaluate grounding of object-centric natural language navigation\ninstructions. We compare to two state-of-the-art zero-shot baselines from\nrelated tasks, Object Goal Navigation and Vision Language Navigation, to\ndemonstrate that LIFGIF outperforms them across all our evaluation metrics on\nOCVLN. Finally, we successfully demonstrate the effectiveness of LIFGIF for\nperforming zero-shot object-centric instruction following in the real world on\na Boston Dynamics Spot robot."}
{"id": "2505.02974", "pdf": "https://arxiv.org/pdf/2505.02974", "abs": "https://arxiv.org/abs/2505.02974", "authors": ["Fabien Casenave", "Xavier Roynard", "Brian Staber", "William Piat", "Michele Alessandro Bucci", "Nissrine Akkari", "Abbas Kabalan", "Xuan Minh Vuong Nguyen", "Luca Saverio", "Raphaël Carpintero Perez", "Anthony Kalaydjian", "Samy Fouché", "Thierry Gonon", "Ghassan Najjar", "Emmanuel Menier", "Matthieu Nastorg", "Giovanni Catalani", "Christian Rey"], "title": "Physics-Learning AI Datamodel (PLAID) datasets: a collection of physics simulations for machine learning", "categories": ["cs.LG"], "comment": null, "summary": "Machine learning-based surrogate models have emerged as a powerful tool to\naccelerate simulation-driven scientific workflows. However, their widespread\nadoption is hindered by the lack of large-scale, diverse, and standardized\ndatasets tailored to physics-based simulations. While existing initiatives\nprovide valuable contributions, many are limited in scope-focusing on specific\nphysics domains, relying on fragmented tooling, or adhering to overly\nsimplistic datamodels that restrict generalization. To address these\nlimitations, we introduce PLAID (Physics-Learning AI Datamodel), a flexible and\nextensible framework for representing and sharing datasets of physics\nsimulations. PLAID defines a unified standard for describing simulation data\nand is accompanied by a library for creating, reading, and manipulating complex\ndatasets across a wide range of physical use cases (gitlab.com/drti/plaid). We\nrelease six carefully crafted datasets under the PLAID standard, covering\nstructural mechanics and computational fluid dynamics, and provide baseline\nbenchmarks using representative learning methods. Benchmarking tools are made\navailable on Hugging Face, enabling direct participation by the community and\ncontribution to ongoing evaluation efforts (huggingface.co/PLAIDcompetitions)."}
{"id": "2411.08777", "pdf": "https://arxiv.org/pdf/2411.08777", "abs": "https://arxiv.org/abs/2411.08777", "authors": ["Pit Henrich", "Franziska Mathis-Ullrich", "Paul Maria Scheikl"], "title": "LUDO: Low-Latency Understanding of Deformable Objects using Point Cloud Occupancy Functions", "categories": ["cs.RO", "cs.CV"], "comment": null, "summary": "Accurately determining the shape of objects and the location of their\ninternal structures within deformable objects is crucial for medical tasks that\nrequire precise targeting, such as robotic biopsies. We introduce LUDO, a\nmethod for accurate low-latency understanding of deformable objects. LUDO\nreconstructs objects in their deformed state, including their internal\nstructures, from a single-view point cloud observation in under 30 ms using\noccupancy networks. LUDO provides uncertainty estimates for its predictions.\nAdditionally, it provides explainability by highlighting key features in its\ninput observations. Both uncertainty and explainability are important for\nsafety-critical applications such as surgical interventions. We demonstrate\nLUDO's abilities for autonomous targeting of internal regions of interest\n(ROIs) in deformable objects. We evaluate LUDO in real-world robotic\nexperiments, achieving a success rate of 98.9% for puncturing various ROIs\ninside deformable objects. LUDO demonstrates the potential to interact with\ndeformable objects without the need for deformable registration methods."}
{"id": "2505.03519", "pdf": "https://arxiv.org/pdf/2505.03519", "abs": "https://arxiv.org/abs/2505.03519", "authors": ["Sy-Tuyen Ho", "Koh Jun Hao", "Ngoc-Bao Nguyen", "Alexander Binder", "Ngai-Man Cheung"], "title": "Uncovering the Limitations of Model Inversion Evaluation -- Benchmarks and Connection to Type-I Adversarial Attacks", "categories": ["cs.LG"], "comment": "Our dataset and code are available in the Supp", "summary": "Model Inversion (MI) attacks aim to reconstruct information of private\ntraining data by exploiting access to machine learning models. The most common\nevaluation framework for MI attacks/defenses relies on an evaluation model that\nhas been utilized to assess progress across almost all MI attacks and defenses\nproposed in recent years. In this paper, for the first time, we present an\nin-depth study of MI evaluation. Firstly, we construct the first comprehensive\nhuman-annotated dataset of MI attack samples, based on 28 setups of different\nMI attacks, defenses, private and public datasets. Secondly, using our dataset,\nwe examine the accuracy of the MI evaluation framework and reveal that it\nsuffers from a significant number of false positives. These findings raise\nquestions about the previously reported success rates of SOTA MI attacks.\nThirdly, we analyze the causes of these false positives, design controlled\nexperiments, and discover the surprising effect of Type I adversarial features\non MI evaluation, as well as adversarial transferability, highlighting a\nrelationship between two previously distinct research areas. Our findings\nsuggest that the performance of SOTA MI attacks has been overestimated, with\nthe actual privacy leakage being significantly less than previously reported.\nIn conclusion, we highlight critical limitations in the widely used MI\nevaluation framework and present our methods to mitigate false positive rates.\nWe remark that prior research has shown that Type I adversarial attacks are\nvery challenging, with no existing solution. Therefore, we urge to consider\nhuman evaluation as a primary MI evaluation framework rather than merely a\nsupplement as in previous MI research. We also encourage further work on\ndeveloping more robust and reliable automatic evaluation frameworks."}
{"id": "2501.04597", "pdf": "https://arxiv.org/pdf/2501.04597", "abs": "https://arxiv.org/abs/2501.04597", "authors": ["Boyang Sun", "Hanzhi Chen", "Stefan Leutenegger", "Cesar Cadena", "Marc Pollefeys", "Hermann Blum"], "title": "FrontierNet: Learning Visual Cues to Explore", "categories": ["cs.RO", "cs.CV"], "comment": null, "summary": "Exploration of unknown environments is crucial for autonomous robots; it\nallows them to actively reason and decide on what new data to acquire for\ndifferent tasks, such as mapping, object discovery, and environmental\nassessment. Existing solutions, such as frontier-based exploration approaches,\nrely heavily on 3D map operations, which are limited by map quality and, more\ncritically, often overlook valuable context from visual cues. This work aims at\nleveraging 2D visual cues for efficient autonomous exploration, addressing the\nlimitations of extracting goal poses from a 3D map. We propose a visual-only\nfrontier-based exploration system, with FrontierNet as its core component.\nFrontierNet is a learning-based model that (i) proposes frontiers, and (ii)\npredicts their information gain, from posed RGB images enhanced by monocular\ndepth priors. Our approach provides an alternative to existing 3D-dependent\ngoal-extraction approaches, achieving a 15\\% improvement in early-stage\nexploration efficiency, as validated through extensive simulations and\nreal-world experiments. The project is available at\nhttps://github.com/cvg/FrontierNet."}
{"id": "2505.03677", "pdf": "https://arxiv.org/pdf/2505.03677", "abs": "https://arxiv.org/abs/2505.03677", "authors": ["Emanuele Zappala", "Alice Giola", "Andreas Kramer", "Enrico Greco"], "title": "Neural Integral Operators for Inverse problems in Spectroscopy", "categories": ["cs.LG"], "comment": "13 pages. v2: Link to code repository added. Comments are welcome", "summary": "Deep learning has shown high performance on spectroscopic inverse problems\nwhen sufficient data is available. However, it is often the case that data in\nspectroscopy is scarce, and this usually causes severe overfitting problems\nwith deep learning methods. Traditional machine learning methods are viable\nwhen datasets are smaller, but the accuracy and applicability of these methods\nis generally more limited. We introduce a deep learning method for\nclassification of molecular spectra based on learning integral operators via\nintegral equations of the first kind, which results in an algorithm that is\nless affected by overfitting issues on small datasets, compared to other deep\nlearning models. The problem formulation of the deep learning approach is based\non inverse problems, which have traditionally found important applications in\nspectroscopy. We perform experiments on real world data to showcase our\nalgorithm. It is seen that the model outperforms traditional machine learning\napproaches such as decision tree and support vector machine, and for small\ndatasets it outperforms other deep learning models. Therefore, our methodology\nleverages the power of deep learning, still maintaining the performance when\nthe available data is very limited, which is one of the main issues that deep\nlearning faces in spectroscopy, where datasets are often times of small size."}
{"id": "2505.04590", "pdf": "https://arxiv.org/pdf/2505.04590", "abs": "https://arxiv.org/abs/2505.04590", "authors": ["Alexandre Binninger", "Ruben Wiersma", "Philipp Herholz", "Olga Sorkine-Hornung"], "title": "TetWeave: Isosurface Extraction using On-The-Fly Delaunay Tetrahedral Grids for Gradient-Based Mesh Optimization", "categories": ["cs.GR", "cs.CV", "I.3.5"], "comment": "ACM Trans. Graph. 44, 4. SIGGRAPH 2025. 19 pages, 21 figures", "summary": "We introduce TetWeave, a novel isosurface representation for gradient-based\nmesh optimization that jointly optimizes the placement of a tetrahedral grid\nused for Marching Tetrahedra and a novel directional signed distance at each\npoint. TetWeave constructs tetrahedral grids on-the-fly via Delaunay\ntriangulation, enabling increased flexibility compared to predefined grids. The\nextracted meshes are guaranteed to be watertight, two-manifold and\nintersection-free. The flexibility of TetWeave enables a resampling strategy\nthat places new points where reconstruction error is high and allows to\nencourage mesh fairness without compromising on reconstruction error. This\nleads to high-quality, adaptive meshes that require minimal memory usage and\nfew parameters to optimize. Consequently, TetWeave exhibits near-linear memory\nscaling relative to the vertex count of the output mesh - a substantial\nimprovement over predefined grids. We demonstrate the applicability of TetWeave\nto a broad range of challenging tasks in computer graphics and vision, such as\nmulti-view 3D reconstruction, mesh compression and geometric texture\ngeneration."}
{"id": "2505.03777", "pdf": "https://arxiv.org/pdf/2505.03777", "abs": "https://arxiv.org/abs/2505.03777", "authors": ["LG AI Research", "Sehyun Chun", "Jiye Kim", "Ahra Jo", "Yeonsik Jo", "Seungyul Oh", "Seungjun Lee", "Kwangrok Ryoo", "Jongmin Lee", "Seung Hwan Kim", "Byung Jun Kang", "Soonyoung Lee", "Jun Ha Park", "Chanwoo Moon", "Jiwon Ham", "Haein Lee", "Heejae Han", "Jaeseung Byun", "Soojong Do", "Minju Ha", "Dongyun Kim", "Kyunghoon Bae", "Woohyung Lim", "Edward Hwayoung Lee", "Yongmin Park", "Jeongsang Yu", "Gerrard Jeongwon Jo", "Yeonjung Hong", "Kyungjae Yoo", "Sehui Han", "Jaewan Lee", "Changyoung Park", "Kijeong Jeon", "Sihyuk Yi"], "title": "MolMole: Molecule Mining from Scientific Literature", "categories": ["cs.LG"], "comment": "15 pages, 12 figures", "summary": "The extraction of molecular structures and reaction data from scientific\ndocuments is challenging due to their varied, unstructured chemical formats and\ncomplex document layouts. To address this, we introduce MolMole, a vision-based\ndeep learning framework that unifies molecule detection, reaction diagram\nparsing, and optical chemical structure recognition (OCSR) into a single\npipeline for automating the extraction of chemical data directly from\npage-level documents. Recognizing the lack of a standard page-level benchmark\nand evaluation metric, we also present a testset of 550 pages annotated with\nmolecule bounding boxes, reaction labels, and MOLfiles, along with a novel\nevaluation metric. Experimental results demonstrate that MolMole outperforms\nexisting toolkits on both our benchmark and public datasets. The benchmark\ntestset will be publicly available, and the MolMole toolkit will be accessible\nsoon through an interactive demo on the LG AI Research website. For commercial\ninquiries, please contact us at\n\\href{mailto:contact_ddu@lgresearch.ai}{contact\\_ddu@lgresearch.ai}."}
{"id": "2505.04005", "pdf": "https://arxiv.org/pdf/2505.04005", "abs": "https://arxiv.org/abs/2505.04005", "authors": ["Devan Selvaraj"], "title": "Iterative Orthogonalization Scaling Laws", "categories": ["cs.LG", "68T07"], "comment": null, "summary": "The muon optimizer has picked up much attention as of late as a possible\nreplacement to the seemingly omnipresent Adam optimizer. Recently, care has\nbeen taken to document the scaling laws of hyper-parameters under muon such as\nweight decay and learning rate. However, at much larger scales the iterative\northogonalization procedure present in muon may suffer a possible issue as the\nsingular values of random matrices shrink with scale. This paper shows this\nscaling behavior theoretically and empirically on random matrices but does not\nsuggest what to do about it."}
{"id": "2305.11774", "pdf": "https://arxiv.org/pdf/2305.11774", "abs": "https://arxiv.org/abs/2305.11774", "authors": ["Ben Tu", "Nikolas Kantas", "Robert M. Lee", "Behrang Shafei"], "title": "Multi-objective optimisation via the R2 utilities", "categories": ["math.OC", "cs.LG", "stat.ML"], "comment": "SIAM Review 2025. 47 pages. Code is available at:\n  https://github.com/benmltu/scalarize", "summary": "The goal of multi-objective optimisation is to identify a collection of\npoints which describe the best possible trade-offs between the multiple\nobjectives. In order to solve this vector-valued optimisation problem,\npractitioners often appeal to the use of scalarisation functions in order to\ntransform the multi-objective problem into a collection of single-objective\nproblems. This set of scalarised problems can then be solved using traditional\nsingle-objective optimisation techniques. In this work, we formalise this\nconvention into a general mathematical framework. We show how this strategy\neffectively recasts the original multi-objective optimisation problem into a\nsingle-objective optimisation problem defined over sets. An appropriate class\nof objective functions for this new problem are the R2 utilities, which are\nutility functions that are defined as a weighted integral over the scalarised\noptimisation problems. As part of our work, we show that these utilities are\nmonotone and submodular set functions which can be optimised effectively using\ngreedy optimisation algorithms. We then analyse the performance of these greedy\nalgorithms both theoretically and empirically. Our analysis largely focusses on\nBayesian optimisation, which is a popular probabilistic framework for black-box\noptimisation."}
{"id": "2310.10976", "pdf": "https://arxiv.org/pdf/2310.10976", "abs": "https://arxiv.org/abs/2310.10976", "authors": ["Hristo G. Chipilski"], "title": "Exact nonlinear state estimation", "categories": ["stat.ME", "cs.CE", "cs.LG", "math.DS", "physics.ao-ph"], "comment": null, "summary": "The majority of data assimilation (DA) methods in the geosciences are based\non Gaussian assumptions. While these assumptions facilitate efficient\nalgorithms, they cause analysis biases and subsequent forecast degradations.\nNon-parametric, particle-based DA algorithms have superior accuracy, but their\napplication to high-dimensional models still poses operational challenges.\nDrawing inspiration from recent advances in the field of generative artificial\nintelligence (AI), this article introduces a new nonlinear estimation theory\nwhich attempts to bridge the existing gap in DA methodology. Specifically, a\nConjugate Transform Filter (CTF) is derived and shown to generalize the\ncelebrated Kalman filter to arbitrarily non-Gaussian distributions. The new\nfilter has several desirable properties, such as its ability to preserve\nstatistical relationships in the prior state and convergence to highly accurate\nobservations. An ensemble approximation of the new theory (ECTF) is also\npresented and validated using idealized statistical experiments that feature\nbounded quantities with non-Gaussian distributions, a prevalent challenge in\nEarth system models. Results from these experiments indicate that the greatest\nbenefits from ECTF occur when observation errors are small relative to the\nforecast uncertainty and when state variables exhibit strong nonlinear\ndependencies. Ultimately, the new filtering theory offers exciting avenues for\nimproving conventional DA algorithms through their principled integration with\nAI techniques."}
{"id": "2312.11797", "pdf": "https://arxiv.org/pdf/2312.11797", "abs": "https://arxiv.org/abs/2312.11797", "authors": ["Min Dai", "Yuchao Dong", "Yanwei Jia", "Xun Yu Zhou"], "title": "Data-Driven Merton's Strategies via Policy Randomization", "categories": ["q-fin.PM", "cs.LG", "q-fin.CP"], "comment": "43 pages, 5 figures, 2 tables", "summary": "We study Merton's expected utility maximization problem in an incomplete\nmarket, characterized by a factor process in addition to the stock price\nprocess, where all the model primitives are unknown. The agent under\nconsideration is a price taker who has access only to the stock and factor\nvalue processes and the instantaneous volatility. We propose an auxiliary\nproblem in which the agent can invoke policy randomization according to a\nspecific class of Gaussian distributions, and prove that the mean of its\noptimal Gaussian policy solves the original Merton problem. With randomized\npolicies, we are in the realm of continuous-time reinforcement learning (RL)\nrecently developed in Wang et al. (2020) and Jia and Zhou (2022a, 2022b, 2023),\nenabling us to solve the auxiliary problem in a data-driven way without having\nto estimate the model primitives. Specifically, we establish a policy\nimprovement theorem based on which we design both online and offline\nactor-critic RL algorithms for learning Merton's strategies. A key insight from\nthis study is that RL in general and policy randomization in particular are\nuseful beyond the purpose for exploration -- they can be employed as a\ntechnical tool to solve a problem that cannot be otherwise solved by mere\ndeterministic policies. At last, we carry out both simulation and empirical\nstudies in a stochastic volatility environment to demonstrate the decisive\noutperformance of the devised RL algorithms in comparison to the conventional\nmodel-based, plug-in method."}
{"id": "2404.15342", "pdf": "https://arxiv.org/pdf/2404.15342", "abs": "https://arxiv.org/abs/2404.15342", "authors": ["Yan Pei", "Wei Luo"], "title": "WaveSleepNet: An Interpretable Network for Expert-like Sleep Staging", "categories": ["eess.SP", "cs.LG"], "comment": "17 pages, 6 figures", "summary": "Although deep learning algorithms have proven their efficiency in automatic\nsleep staging, the widespread skepticism about their \"black-box\" nature has\nlimited its clinical acceptance. In this study, we propose WaveSleepNet, an\ninterpretable neural network for sleep staging that reasons in a similar way to\nsleep experts. In this network, we utilize the latent space representations\ngenerated during training to identify characteristic wave prototypes\ncorresponding to different sleep stages. The feature representation of an input\nsignal is segmented into patches within the latent space, each of which is\ncompared against the learned wave prototypes. The proximity between these\npatches and the wave prototypes is quantified through scores, indicating the\nprototypes' presence and relative proportion within the signal. The scores are\nserved as the decision-making criteria for final sleep staging. During\ntraining, an ensemble of loss functions is employed for the prototypes'\ndiversity and robustness. Furthermore, the learned wave prototypes are\nvisualized by analysing occlusion sensitivity. The efficacy of WaveSleepNet is\nvalidated across three public datasets, achieving sleep staging performance\nthat are on par with the state-of-the-art models when several WaveSleepNets are\ncombine into a larger network. A detailed case study examined the\ndecision-making process of the WaveSleepNet which aligns closely with American\nAcademy of Sleep Medicine (AASM) manual guidelines. Another case study\nsystematically explained the misidentified reason behind each sleep stage.\nWaveSleepNet's transparent process provides specialists with direct access to\nthe physiological significance of its criteria, allowing for future adaptation\nor enrichment by sleep experts."}
{"id": "2405.00781", "pdf": "https://arxiv.org/pdf/2405.00781", "abs": "https://arxiv.org/abs/2405.00781", "authors": ["Martin Larocca", "Supanut Thanasilp", "Samson Wang", "Kunal Sharma", "Jacob Biamonte", "Patrick J. Coles", "Lukasz Cincio", "Jarrod R. McClean", "Zoë Holmes", "M. Cerezo"], "title": "Barren Plateaus in Variational Quantum Computing", "categories": ["quant-ph", "cs.LG", "stat.ML"], "comment": "24 pages, 10 boxes, updated to published version", "summary": "Variational quantum computing offers a flexible computational paradigm with\napplications in diverse areas. However, a key obstacle to realizing their\npotential is the Barren Plateau (BP) phenomenon. When a model exhibits a BP,\nits parameter optimization landscape becomes exponentially flat and featureless\nas the problem size increases. Importantly, all the moving pieces of an\nalgorithm -- choices of ansatz, initial state, observable, loss function and\nhardware noise -- can lead to BPs when ill-suited. Due to the significant\nimpact of BPs on trainability, researchers have dedicated considerable effort\nto develop theoretical and heuristic methods to understand and mitigate their\neffects. As a result, the study of BPs has become a thriving area of research,\ninfluencing and cross-fertilizing other fields such as quantum optimal control,\ntensor networks, and learning theory. This article provides a comprehensive\nreview of the current understanding of the BP phenomenon."}
{"id": "2405.12258", "pdf": "https://arxiv.org/pdf/2405.12258", "abs": "https://arxiv.org/abs/2405.12258", "authors": ["Abbi Abdel-Rehim", "Hector Zenil", "Oghenejokpeme Orhobor", "Marie Fisher", "Ross J. Collins", "Elizabeth Bourne", "Gareth W. Fearnley", "Emma Tate", "Holly X. Smith", "Larisa N. Soldatova", "Ross D. King"], "title": "Scientific Hypothesis Generation by a Large Language Model: Laboratory Validation in Breast Cancer Treatment", "categories": ["q-bio.QM", "cs.LG", "q-bio.CB"], "comment": "12 pages, 6 tables, 1 figure. Supplementary information available", "summary": "Large language models LLMs have transformed AI and achieved breakthrough\nperformance on a wide range of tasks In science the most interesting\napplication of LLMs is for hypothesis formation A feature of LLMs which results\nfrom their probabilistic structure is that the output text is not necessarily a\nvalid inference from the training text These are termed hallucinations and are\nharmful in many applications In science some hallucinations may be useful novel\nhypotheses whose validity may be tested by laboratory experiments Here we\nexperimentally test the application of LLMs as a source of scientific\nhypotheses using the domain of breast cancer treatment We applied the LLM GPT4\nto hypothesize novel synergistic pairs of FDA-approved noncancer drugs that\ntarget the MCF7 breast cancer cell line relative to the nontumorigenic breast\ncell line MCF10A In the first round of laboratory experiments GPT4 succeeded in\ndiscovering three drug combinations out of twelve tested with synergy scores\nabove the positive controls GPT4 then generated new combinations based on its\ninitial results this generated three more combinations with positive synergy\nscores out of four tested We conclude that LLMs are a valuable source of\nscientific hypotheses."}
{"id": "2405.18686", "pdf": "https://arxiv.org/pdf/2405.18686", "abs": "https://arxiv.org/abs/2405.18686", "authors": ["Alexander Soen", "Hisham Husain", "Philip Schulz", "Vu Nguyen"], "title": "Rejection via Learning Density Ratios", "categories": ["stat.ML", "cs.LG"], "comment": "Compared to published version, a typo in Appendix Section E has been\n  fixed", "summary": "Classification with rejection emerges as a learning paradigm which allows\nmodels to abstain from making predictions. The predominant approach is to alter\nthe supervised learning pipeline by augmenting typical loss functions, letting\nmodel rejection incur a lower loss than an incorrect prediction. Instead, we\npropose a different distributional perspective, where we seek to find an\nidealized data distribution which maximizes a pretrained model's performance.\nThis can be formalized via the optimization of a loss's risk with a\n$\\varphi$-divergence regularization term. Through this idealized distribution,\na rejection decision can be made by utilizing the density ratio between this\ndistribution and the data distribution. We focus on the setting where our\n$\\varphi$-divergences are specified by the family of $\\alpha$-divergence. Our\nframework is tested empirically over clean and noisy datasets."}
{"id": "2409.07014", "pdf": "https://arxiv.org/pdf/2409.07014", "abs": "https://arxiv.org/abs/2409.07014", "authors": ["Peizhi Wu", "Haoshu Xu", "Ryan Marcus", "Zachary G. Ives"], "title": "A Practical Theory of Generalization in Selectivity Learning", "categories": ["stat.ML", "cs.DB", "cs.LG"], "comment": "15 pages. Technical Report (Extended Version)", "summary": "Query-driven machine learning models have emerged as a promising estimation\ntechnique for query selectivities. Yet, surprisingly little is known about the\nefficacy of these techniques from a theoretical perspective, as there exist\nsubstantial gaps between practical solutions and state-of-the-art (SOTA) theory\nbased on the Probably Approximately Correct (PAC) learning framework. In this\npaper, we aim to bridge the gaps between theory and practice. First, we\ndemonstrate that selectivity predictors induced by signed measures are\nlearnable, which relaxes the reliance on probability measures in SOTA theory.\nMore importantly, beyond the PAC learning framework (which only allows us to\ncharacterize how the model behaves when both training and test workloads are\ndrawn from the same distribution), we establish, under mild assumptions, that\nselectivity predictors from this class exhibit favorable out-of-distribution\n(OOD) generalization error bounds.\n  These theoretical advances provide us with a better understanding of both the\nin-distribution and OOD generalization capabilities of query-driven selectivity\nlearning, and facilitate the design of two general strategies to improve OOD\ngeneralization for existing query-driven selectivity models. We empirically\nverify that our techniques help query-driven selectivity models generalize\nsignificantly better to OOD queries both in terms of prediction accuracy and\nquery latency performance, while maintaining their superior in-distribution\ngeneralization performance."}
{"id": "2411.11838", "pdf": "https://arxiv.org/pdf/2411.11838", "abs": "https://arxiv.org/abs/2411.11838", "authors": ["Elie Azeraf"], "title": "Pairwise Markov Chains for Volatility Forecasting", "categories": ["stat.ML", "cs.LG"], "comment": "14 pages, 9 figures", "summary": "The Pairwise Markov Chain (PMC) is a probabilistic graphical model extending\nthe well-known Hidden Markov Model. This model, although highly effective for\nmany tasks, has been scarcely utilized for continuous value prediction. This is\nmainly due to the issue of modeling observations inherent in generative\nprobabilistic models. In this paper, we introduce a new algorithm for\nprediction with the PMC. On the one hand, this algorithm allows circumventing\nthe feature problem, thus fully exploiting the capabilities of the PMC. On the\nother hand, it enables the PMC to extend any predictive model by introducing\nhidden states, updated at each time step, and allowing the introduction of\nnon-stationarity for any model. We apply the PMC with its new algorithm for\nvolatility forecasting, which we compare to the highly popular GARCH(1,1) and\nfeedforward neural models across numerous pairs. This is particularly relevant\ngiven the regime changes that we can observe in volatility. For each scenario,\nour algorithm enhances the performance of the extended model, demonstrating the\nvalue of our approach."}
{"id": "2411.18964", "pdf": "https://arxiv.org/pdf/2411.18964", "abs": "https://arxiv.org/abs/2411.18964", "authors": ["Luke Bhan", "Peijia Qin", "Miroslav Krstic", "Yuanyuan Shi"], "title": "Neural Operators for Predictor Feedback Control of Nonlinear Delay Systems", "categories": ["eess.SY", "cs.LG", "cs.SY", "math.DS", "math.OC"], "comment": "26 pages. Learning for Dynamics and Control 2025", "summary": "Predictor feedback designs are critical for delay-compensating controllers in\nnonlinear systems. However, these designs are limited in practical applications\nas predictors cannot be directly implemented, but require numerical\napproximation schemes, which become computationally prohibitive when system\ndynamics are expensive to compute. To address this challenge, we recast the\npredictor design as an operator learning problem, and learn the predictor\nmapping via a neural operator. We prove the existence of an arbitrarily\naccurate neural operator approximation of the predictor operator. Under the\napproximated predictor, we achieve semiglobal practical stability of the\nclosed-loop nonlinear delay system. The estimate is semiglobal in a unique\nsense - one can enlarge the set of initial states as desired, though this\nincreases the difficulty of training a neural operator, which appears\npractically in the stability estimate. Furthermore, our analysis holds for any\nblack-box predictor satisfying the universal approximation error bound. We\ndemonstrate the approach by controlling a 5-link robotic manipulator with\ndifferent neural operator models, achieving significant speedups compared to\nclassic predictor feedback schemes while maintaining closed-loop stability."}
{"id": "2412.02529", "pdf": "https://arxiv.org/pdf/2412.02529", "abs": "https://arxiv.org/abs/2412.02529", "authors": ["Andrew Wagenmaker", "Lu Mi", "Marton Rozsa", "Matthew S. Bull", "Karel Svoboda", "Kayvon Daie", "Matthew D. Golub", "Kevin Jamieson"], "title": "Active learning of neural population dynamics using two-photon holographic optogenetics", "categories": ["q-bio.NC", "cs.LG", "stat.ML"], "comment": "NeurIPS 2024", "summary": "Recent advances in techniques for monitoring and perturbing neural\npopulations have greatly enhanced our ability to study circuits in the brain.\nIn particular, two-photon holographic optogenetics now enables precise\nphotostimulation of experimenter-specified groups of individual neurons, while\nsimultaneous two-photon calcium imaging enables the measurement of ongoing and\ninduced activity across the neural population. Despite the enormous space of\npotential photostimulation patterns and the time-consuming nature of\nphotostimulation experiments, very little algorithmic work has been done to\ndetermine the most effective photostimulation patterns for identifying the\nneural population dynamics. Here, we develop methods to efficiently select\nwhich neurons to stimulate such that the resulting neural responses will best\ninform a dynamical model of the neural population activity. Using neural\npopulation responses to photostimulation in mouse motor cortex, we demonstrate\nthe efficacy of a low-rank linear dynamical systems model, and develop an\nactive learning procedure which takes advantage of low-rank structure to\ndetermine informative photostimulation patterns. We demonstrate our approach on\nboth real and synthetic data, obtaining in some cases as much as a two-fold\nreduction in the amount of data required to reach a given predictive power. Our\nactive stimulation design method is based on a novel active learning procedure\nfor low-rank regression, which may be of independent interest."}
{"id": "2501.14095", "pdf": "https://arxiv.org/pdf/2501.14095", "abs": "https://arxiv.org/abs/2501.14095", "authors": ["Kelly Ramsay", "Dylan Spicker"], "title": "Improved subsample-and-aggregate via the private modified winsorized mean", "categories": ["stat.ME", "cs.LG", "62G35, 68P27", "G.3.7; C.2.0"], "comment": "45 pages, 6 figures", "summary": "We develop a univariate, differentially private mean estimator, called the\nprivate modified winsorized mean, designed to be used as the aggregator in\nsubsample-and-aggregate. We demonstrate, via real data analysis, that common\ndifferentially private multivariate mean estimators may not perform well as the\naggregator, even in large datasets, motivating our developments.We show that\nthe modified winsorized mean is minimax optimal for several, large classes of\ndistributions, even under adversarial contamination. We also demonstrate that,\nempirically, the private modified winsorized mean performs well compared to\nother private mean estimates. We consider the modified winsorized mean as the\naggregator in subsample-and-aggregate, deriving a finite sample deviations\nbound for a subsample-and-aggregate estimate generated with the new aggregator.\nThis result yields two important insights: (i) the optimal choice of subsamples\ndepends on the bias of the estimator computed on the subsamples, and (ii) the\nrate of convergence of the subsample-and-aggregate estimator depends on the\nrobustness of the estimator computed on the subsamples."}
{"id": "2502.06978", "pdf": "https://arxiv.org/pdf/2502.06978", "abs": "https://arxiv.org/abs/2502.06978", "authors": ["Guancheng Qiu", "Mathieu Tanneau", "Pascal Van Hentenryck"], "title": "Dual Conic Proxy for Semidefinite Relaxation of AC Optimal Power Flow", "categories": ["math.OC", "cs.LG"], "comment": null, "summary": "The nonlinear, non-convex AC Optimal Power Flow (AC-OPF) problem is\nfundamental for power systems operations. The intrinsic complexity of AC-OPF\nhas fueled a growing interest in the development of optimization proxies for\nthe problem, i.e., machine learning models that predict high-quality,\nclose-to-optimal solutions. More recently, dual conic proxy architectures have\nbeen proposed, which combine machine learning and convex relaxations of AC-OPF,\nto provide valid certificates of optimality using learning-based methods.\nBuilding on this methodology, this paper proposes, for the first time, a dual\nconic proxy architecture for the semidefinite (SDP) relaxation of AC-OPF\nproblems. Although the SDP relaxation is stronger than the second-order cone\nrelaxation considered in previous work, its practical use has been hindered by\nits computational cost. The proposed method combines a neural network with a\ndifferentiable dual completion strategy that leverages the structure of the\ndual SDP problem. This approach guarantees dual feasibility, and therefore\nvalid dual bounds, while providing orders of magnitude of speedups compared to\ninterior-point algorithms. The paper also leverages self-supervised learning,\nwhich alleviates the need for time-consuming data generation and allows to\ntrain the proposed models efficiently. Numerical experiments are presented on\nseveral power grid benchmarks with up to 500 buses. The results demonstrate\nthat the proposed SDP-based proxies can outperform weaker conic relaxations,\nwhile providing several orders of magnitude speedups compared to a\nstate-of-the-art interior-point SDP solver."}
{"id": "2502.12674", "pdf": "https://arxiv.org/pdf/2502.12674", "abs": "https://arxiv.org/abs/2502.12674", "authors": ["Peizhuo Li", "Hongyi Li", "Ge Sun", "Jin Cheng", "Xinrong Yang", "Guillaume Bellegarda", "Milad Shafiee", "Yuhong Cao", "Auke Ijspeert", "Guillaume Sartoretti"], "title": "SATA: Safe and Adaptive Torque-Based Locomotion Policies Inspired by Animal Learning", "categories": ["cs.RO", "cs.LG"], "comment": null, "summary": "Despite recent advances in learning-based controllers for legged robots,\ndeployments in human-centric environments remain limited by safety concerns.\nMost of these approaches use position-based control, where policies output\ntarget joint angles that must be processed by a low-level controller (e.g., PD\nor impedance controllers) to compute joint torques. Although impressive results\nhave been achieved in controlled real-world scenarios, these methods often\nstruggle with compliance and adaptability when encountering environments or\ndisturbances unseen during training, potentially resulting in extreme or unsafe\nbehaviors. Inspired by how animals achieve smooth and adaptive movements by\ncontrolling muscle extension and contraction, torque-based policies offer a\npromising alternative by enabling precise and direct control of the actuators\nin torque space. In principle, this approach facilitates more effective\ninteractions with the environment, resulting in safer and more adaptable\nbehaviors. However, challenges such as a highly nonlinear state space and\ninefficient exploration during training have hindered their broader adoption.\nTo address these limitations, we propose SATA, a bio-inspired framework that\nmimics key biomechanical principles and adaptive learning mechanisms observed\nin animal locomotion. Our approach effectively addresses the inherent\nchallenges of learning torque-based policies by significantly improving\nearly-stage exploration, leading to high-performance final policies.\nRemarkably, our method achieves zero-shot sim-to-real transfer. Our\nexperimental results indicate that SATA demonstrates remarkable compliance and\nsafety, even in challenging environments such as soft/slippery terrain or\nnarrow passages, and under significant external disturbances, highlighting its\npotential for practical deployments in human-centric and safety-critical\nscenarios."}
{"id": "2503.01116", "pdf": "https://arxiv.org/pdf/2503.01116", "abs": "https://arxiv.org/abs/2503.01116", "authors": ["Jianzhe Xue", "Dongcheng Yuan", "Zhanxi Ma", "Tiankai Jiang", "Yu Sun", "Haibo Zhou", "Xuemin Shen"], "title": "Large AI Model for Delay-Doppler Domain Channel Prediction in 6G OTFS-Based Vehicular Networks", "categories": ["eess.SP", "cs.LG"], "comment": "This manuscript has been accepted by SCIENCE CHINA Information\n  Sciences", "summary": "Channel prediction is crucial for high-mobility vehicular networks, as it\nenables the anticipation of future channel conditions and the proactive\nadjustment of communication strategies. However, achieving accurate vehicular\nchannel prediction is challenging due to significant Doppler effects and rapid\nchannel variations resulting from high-speed vehicle movement and complex\npropagation environments. In this paper, we propose a novel delay-Doppler (DD)\ndomain channel prediction framework tailored for high-mobility vehicular\nnetworks. By transforming the channel representation into the DD domain, we\nobtain an intuitive, sparse, and stable depiction that closely aligns with the\nunderlying physical propagation processes, effectively reducing the complex\nvehicular channel to a set of time-series parameters with enhanced\npredictability. Furthermore, we leverage the large artificial intelligence (AI)\nmodel to predict these DD-domain time-series parameters, capitalizing on their\nadvanced ability to model temporal correlations. The zero-shot capability of\nthe pre-trained large AI model facilitates accurate channel predictions without\nrequiring task-specific training, while subsequent fine-tuning on specific\nvehicular channel data further improves prediction accuracy. Extensive\nsimulation results demonstrate the effectiveness of our DD-domain channel\nprediction framework and the superior accuracy of the large AI model in\npredicting time-series channel parameters, thereby highlighting the potential\nof our approach for robust vehicular communication systems."}
{"id": "2503.23810", "pdf": "https://arxiv.org/pdf/2503.23810", "abs": "https://arxiv.org/abs/2503.23810", "authors": ["Ilayda Yaman", "Guoda Tian", "Dino Pjanic", "Fredrik Tufvesson", "Ove Edfors", "Zhengya Zhang", "Liang Liu"], "title": "Adaptive Attention-Based Model for 5G Radio-based Outdoor Localization", "categories": ["eess.SP", "cs.LG"], "comment": "6 pages, 6 figures", "summary": "Radio-based localization in dynamic environments, such as urban and vehicular\nsettings, requires systems that efficiently adapt to varying signal conditions\nand environmental changes. Factors like multipath interference and obstructions\nintroduce different levels of complexity that affect the accuracy of the\nlocalization. Although generalized models offer broad applicability, they often\nstruggle to capture the nuances of specific environments, leading to suboptimal\nperformance in real-world deployments. In contrast, specialized models can be\ntailored to particular conditions, enabling more precise localization by\neffectively handling domain-specific variations, which also results in reduced\nexecution time and smaller model size. However, deploying multiple specialized\nmodels requires an efficient mechanism to select the most appropriate one for a\ngiven scenario. In this work, we develop an adaptive localization framework\nthat combines shallow attention-based models with a router/switching mechanism\nbased on a single-layer perceptron. This enables seamless transitions between\nspecialized localization models optimized for different conditions, balancing\naccuracy and computational complexity. We design three low-complex models\ntailored for distinct scenarios, and a router that dynamically selects the most\nsuitable model based on real-time input characteristics. The proposed framework\nis validated using real-world vehicle localization data collected from a\nmassive MIMO base station and compared to more general models."}
{"id": "2504.02288", "pdf": "https://arxiv.org/pdf/2504.02288", "abs": "https://arxiv.org/abs/2504.02288", "authors": ["Edward DongBo Cui", "Lu Zhang", "William Ping-hsun Lee"], "title": "Shallow AutoEncoding Recommender with Cold Start Handling via Side Features", "categories": ["cs.IR", "cs.LG"], "comment": "Preparing submission to CIKM 2025; 2 Figures; 4 Tables; 13 pages;\n  Python code implementation example", "summary": "User and item cold starts present significant challenges in industrial\napplications of recommendation systems. Supplementing user-item interaction\ndata with metadata is a common solution-but often at the cost of introducing\nadditional biases. In this work, we introduce an augmented EASE model, i.e.\nFEASE, that seamlessly integrates both user and item side information to\naddress these cold start issues. Our straightforward, autoencoder-based method\nproduces a closed-form solution that leverages rich content signals for cold\nitems while refining user representations in data-sparse environments.\nImportantly, our method strikes a balance by effectively recommending cold\nstart items and handling cold start users without incurring extra bias, and it\nmaintains strong performance in warm settings. Experimental results demonstrate\nimproved recommendation accuracy and robustness compared to previous\ncollaborative filtering approaches. Moreover, our model serves as a strong\nbaseline for future comparative studies."}
{"id": "2504.08989", "pdf": "https://arxiv.org/pdf/2504.08989", "abs": "https://arxiv.org/abs/2504.08989", "authors": ["Han Liao", "Shuaishuai Zu"], "title": "RouterKT: Mixture-of-Experts for Knowledge Tracing", "categories": ["cs.CY", "cs.IR", "cs.LG"], "comment": "data leakage in experiments", "summary": "Knowledge Tracing (KT) is a fundamental task in Intelligent Tutoring Systems\n(ITS), which aims to model the dynamic knowledge states of students based on\ntheir interaction histories. However, existing KT models often rely on a global\nforgetting decay mechanism for capturing learning patterns, assuming that\nstudents' performance is predominantly influenced by their most recent\ninteractions. Such approaches fail to account for the diverse and complex\nlearning patterns arising from individual differences and varying learning\nstages. To address this limitation, we propose RouterKT, a novel\nMixture-of-Experts (MoE) architecture designed to capture heterogeneous\nlearning patterns by enabling experts to specialize in different patterns\nwithout any handcrafted learning pattern bias such as forgetting decay.\nSpecifically, RouterKT introduces a \\textbf{person-wise routing mechanism} to\neffectively model individual-specific learning behaviors and employs\n\\textbf{multi-heads as experts} to enhance the modeling of complex and diverse\npatterns. Comprehensive experiments on ten benchmark datasets demonstrate that\nRouterKT exhibits significant flexibility and improves the performance of\nvarious KT backbone models, with a maximum average AUC improvement of 3.29\\%\nacross different backbones and datasets, outperforming other state-of-the-art\nmodels. Moreover, RouterKT demonstrates consistently superior inference\nefficiency compared to existing approaches based on handcrafted learning\npattern bias, highlighting its usability for real-world educational\napplications. The source code is available at\nhttps://github.com/ringotc/RouterKT.git."}
{"id": "2504.13232", "pdf": "https://arxiv.org/pdf/2504.13232", "abs": "https://arxiv.org/abs/2504.13232", "authors": ["Sayed Pouria Talebi", "Clive Cheong Took", "Danilo P. Mandic"], "title": "A Quantum of Learning: Using Quaternion Algebra to Model Learning on Quantum Devices", "categories": ["quant-ph", "cs.LG", "math.QA", "stat.ML"], "comment": null, "summary": "This article considers the problem of designing adaption and optimisation\ntechniques for training quantum learning machines. To this end, the division\nalgebra of quaternions is used to derive an effective model for representing\ncomputation and measurement operations on qubits. In turn, the derived model,\nserves as the foundation for formulating an adaptive learning problem on\nprincipal quantum learning units, thereby establishing quantum information\nprocessing units akin to that of neurons in classical approaches. Then,\nleveraging the modern HR-calculus, a comprehensive training framework for\nlearning on quantum machines is developed. The quaternion-valued model\naccommodates mathematical tractability and establishment of performance\ncriteria, such as convergence conditions."}
