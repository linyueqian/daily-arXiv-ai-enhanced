{"id": "2504.15349", "pdf": "https://arxiv.org/pdf/2504.15349", "abs": "https://arxiv.org/abs/2504.15349", "authors": ["William Bruns"], "title": "Exploring Compositional Generalization (in ReCOGS_pos) by Transformers using Restricted Access Sequence Processing (RASP)", "categories": ["cs.CL"], "comment": "8 pages main text with 3 figures and 1 table; limitations page and\n  references separate; 4 more figures, 1 image, and 1 more table in the\n  appendices supplement the work. 29 pages of appendix content", "summary": "Humans understand new combinations of words encountered if they are\ncombinations of words recognized from different contexts, an ability called\nCompositional Generalization. The COGS benchmark (Kim and Linzen, 2020)\narXiv:2010.05465 reports 0% accuracy for Transformer models on some structural\ngeneralizations. We use (Weiss et al., 2021) arXiv:2106.06981's Restricted\nAccess Sequence Processing (RASP), a Transformer-equivalent programming\nlanguage, to prove by construction that a Transformer encoder-decoder can\nperform the semantically equivalent ReCOGS_pos (Wu et al., 2024)\narXiv:2303.13716 variant of COGS systematically and compositionally: Our RASP\nmodel attains 100% semantic exact match on the ReCOGS test set and 100% SEM on\nall generalization splits except obj_pp_to_subj_pp which gets 92%. Furthermore,\nour RASP model shows the ReCOGS_pos task does not require a hierarchical or\ntree-structured solution: we use word-level tokens with an \"embedding\" layer\nthat tags with possible parts of speech, applying just once per encoder pass 19\nattention-head compatible flat pattern-matching rules, shown using grammar\ncoverage (Zeller et al., 2023) to be learnable from the training data, plus\ngeneral prepositional phrase (pp) handling and sentential complement (cp)\nhandling logic, and output the next logical form (LF) token (repeating until\nthe LF is complete). The model does not apply recursive, tree-structured rules\nlike 'np_det pp np -> np_pp -> np', but scores 100% semantic and string exact\nmatch on pp recursion, cp recursion using the decoder loop."}
{"id": "2504.15392", "pdf": "https://arxiv.org/pdf/2504.15392", "abs": "https://arxiv.org/abs/2504.15392", "authors": ["Myrthe Reuver", "Indira Sen", "Matteo Melis", "Gabriella Lapesa"], "title": "Tell Me What You Know About Sexism: Expert-LLM Interaction Strategies and Co-Created Definitions for Zero-Shot Sexism Detection", "categories": ["cs.CL", "cs.CY"], "comment": "Accepted and published at Findings of NAACL 2025: cite published\n  version whenever possible", "summary": "This paper investigates hybrid intelligence and collaboration between\nresearchers of sexism and Large Language Models (LLMs), with a four-component\npipeline. First, nine sexism researchers answer questions about their knowledge\nof sexism and of LLMs. They then participate in two interactive experiments\ninvolving an LLM (GPT3.5). The first experiment has experts assessing the\nmodel's knowledge about sexism and suitability for use in research. The second\nexperiment tasks them with creating three different definitions of sexism: an\nexpert-written definition, an LLM-written one, and a co-created definition.\nLastly, zero-shot classification experiments use the three definitions from\neach expert in a prompt template for sexism detection, evaluating GPT4o on\n2.500 texts sampled from five sexism benchmarks. We then analyze the resulting\n67.500 classification decisions. The LLM interactions lead to longer and more\ncomplex definitions of sexism. Expert-written definitions on average perform\npoorly compared to LLM-generated definitions. However, some experts do improve\nclassification performance with their co-created definitions of sexism, also\nexperts who are inexperienced in using LLMs."}
{"id": "2504.15431", "pdf": "https://arxiv.org/pdf/2504.15431", "abs": "https://arxiv.org/abs/2504.15431", "authors": ["Sungjun Han", "Juyoung Suk", "Suyeong An", "Hyungguk Kim", "Kyuseok Kim", "Wonsuk Yang", "Seungtaek Choi", "Jamin Shin"], "title": "Trillion 7B Technical Report", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "Preview version", "summary": "We introduce Trillion-7B, the most token-efficient Korean-centric\nmultilingual LLM available. Our novel Cross-lingual Document Attention (XLDA)\nmechanism enables highly efficient and effective knowledge transfer from\nEnglish to target languages like Korean and Japanese. Combined with optimized\ndata mixtures, language-specific filtering, and tailored tokenizer\nconstruction, Trillion-7B achieves competitive performance while dedicating\nonly 10\\% of its 2T training tokens to multilingual data and requiring just\n59.4K H100 GPU hours (\\$148K) for full training. Comprehensive evaluations\nacross 27 benchmarks in four languages demonstrate Trillion-7B's robust\nmultilingual performance and exceptional cross-lingual consistency."}
{"id": "2504.15301", "pdf": "https://arxiv.org/pdf/2504.15301", "abs": "https://arxiv.org/abs/2504.15301", "authors": ["Zoi Lygizou", "Dimitris Kalles"], "title": "A biologically Inspired Trust Model for Open Multi-Agent Systems that is Resilient to Rapid Performance Fluctuations", "categories": ["cs.MA", "cs.AI", "cs.DC"], "comment": null, "summary": "Trust management provides an alternative solution for securing open, dynamic,\nand distributed multi-agent systems, where conventional cryptographic methods\nprove to be impractical. However, existing trust models face challenges related\nto agent mobility, changing behaviors, and the cold start problem. To address\nthese issues we introduced a biologically inspired trust model in which\ntrustees assess their own capabilities and store trust data locally. This\ndesign improves mobility support, reduces communication overhead, resists\ndisinformation, and preserves privacy. Despite these advantages, prior\nevaluations revealed limitations of our model in adapting to provider\npopulation changes and continuous performance fluctuations. This study proposes\na novel algorithm, incorporating a self-classification mechanism for providers\nto detect performance drops potentially harmful for the service consumers.\nSimulation results demonstrate that the new algorithm outperforms its original\nversion and FIRE, a well-known trust and reputation model, particularly in\nhandling dynamic trustee behavior. While FIRE remains competitive under extreme\nenvironmental changes, the proposed algorithm demonstrates greater adaptability\nacross various conditions. In contrast to existing trust modeling research,\nthis study conducts a comprehensive evaluation of our model using widely\nrecognized trust model criteria, assessing its resilience against common\ntrust-related attacks while identifying strengths, weaknesses, and potential\ncountermeasures. Finally, several key directions for future research are\nproposed."}
{"id": "2504.15432", "pdf": "https://arxiv.org/pdf/2504.15432", "abs": "https://arxiv.org/abs/2504.15432", "authors": ["Yucheng Lu", "Kazimier Smith"], "title": "Feeding LLM Annotations to BERT Classifiers at Your Own Risk", "categories": ["cs.CL"], "comment": null, "summary": "Using LLM-generated labels to fine-tune smaller encoder-only models for text\nclassification has gained popularity in various settings. While this approach\nmay be justified in simple and low-stakes applications, we conduct empirical\nanalysis to demonstrate how the perennial curse of training on synthetic data\nmanifests itself in this specific setup. Compared to models trained on gold\nlabels, we observe not only the expected performance degradation in accuracy\nand F1 score, but also increased instability across training runs and premature\nperformance plateaus. These findings cast doubts on the reliability of such\napproaches in real-world applications. We contextualize the observed phenomena\nthrough the lens of error propagation and offer several practical mitigation\nstrategies, including entropy-based filtering and ensemble techniques. Although\nthese heuristics offer partial relief, they do not fully resolve the inherent\nrisks of propagating non-random errors from LLM annotations to smaller\nclassifiers, underscoring the need for caution when applying this workflow in\nhigh-stakes text classification tasks."}
{"id": "2504.15676", "pdf": "https://arxiv.org/pdf/2504.15676", "abs": "https://arxiv.org/abs/2504.15676", "authors": ["Fernando Castillo", "Oscar Castillo", "Eduardo Brito", "Simon Espinola"], "title": "Trustworthy Decentralized Autonomous Machines: A New Paradigm in Automation Economy", "categories": ["cs.MA", "cs.CR"], "comment": "To be published in IEEE International Workshop on Decentralized\n  Physical Infrastructure Networks 2025, in conjunction with ICBC'25. 7 pages.\n  3 figures", "summary": "Decentralized Autonomous Machines (DAMs) represent a transformative paradigm\nin automation economy, integrating artificial intelligence (AI), blockchain\ntechnology, and Internet of Things (IoT) devices to create self-governing\neconomic agents participating in Decentralized Physical Infrastructure Networks\n(DePIN). Capable of managing both digital and physical assets and unlike\ntraditional Decentralized Autonomous Organizations (DAOs), DAMs extend autonomy\ninto the physical world, enabling trustless systems for Real and Digital World\nAssets (RDWAs). In this paper, we explore the technological foundations, and\nchallenges of DAMs and argue that DAMs are pivotal in transitioning from\ntrust-based to trustless economic models, offering scalable, transparent, and\nequitable solutions for asset management. The integration of AI-driven\ndecision-making, IoT-enabled operational autonomy, and blockchain-based\ngovernance allows DAMs to decentralize ownership, optimize resource allocation,\nand democratize access to economic opportunities. Therefore, in this research,\nwe highlight the potential of DAMs to address inefficiencies in centralized\nsystems, reduce wealth disparities, and foster a post-labor economy."}
{"id": "2504.15575", "pdf": "https://arxiv.org/pdf/2504.15575", "abs": "https://arxiv.org/abs/2504.15575", "authors": ["Haohe Liu", "Thomas Deacon", "Wenwu Wang", "Matt Paradis", "Mark D. Plumbley"], "title": "Exploring the User Experience of AI-Assisted Sound Searching Systems for Creative Workflows", "categories": ["eess.AS", "cs.SD"], "comment": null, "summary": "Locating the right sound effect efficiently is an important yet challenging\ntopic for audio production. Most current sound-searching systems rely on\npre-annotated audio labels created by humans, which can be time-consuming to\nproduce and prone to inaccuracies, limiting the efficiency of audio production.\nFollowing the recent advancement of contrastive language-audio pre-training\n(CLAP) models, we explore an alternative CLAP-based sound-searching system\n(CLAP-UI) that does not rely on human annotations. To evaluate the\neffectiveness of CLAP-UI, we conducted comparative experiments with a widely\nused sound effect searching platform, the BBC Sound Effect Library. Our study\nevaluates user performance, cognitive load, and satisfaction through\necologically valid tasks based on professional sound-searching workflows. Our\nresult shows that CLAP-UI demonstrated significantly enhanced productivity and\nreduced frustration while maintaining comparable cognitive demands. We also\nqualitatively analyzed the participants' feedback, which offered valuable\nperspectives on the design of future AI-assisted sound search systems."}
{"id": "2504.15822", "pdf": "https://arxiv.org/pdf/2504.15822", "abs": "https://arxiv.org/abs/2504.15822", "authors": ["Scott Wellington", "Xuechen Liu", "Junichi Yamagishi"], "title": "Quantifying Source Speaker Leakage in One-to-One Voice Conversion", "categories": ["cs.SD", "cs.CR", "eess.AS"], "comment": "Accepted at IEEE 23rd International Conference of the Biometrics\n  Special Interest Group (BIOSIG 2024)", "summary": "Using a multi-accented corpus of parallel utterances for use with commercial\nspeech devices, we present a case study to show that it is possible to quantify\na degree of confidence about a source speaker's identity in the case of\none-to-one voice conversion. Following voice conversion using a HiFi-GAN\nvocoder, we compare information leakage for a range speaker characteristics;\nassuming a \"worst-case\" white-box scenario, we quantify our confidence to\nperform inference and narrow the pool of likely source speakers, reinforcing\nthe regulatory obligation and moral duty that providers of synthetic voices\nhave to ensure the privacy of their speakers' data."}
{"id": "2504.15304", "pdf": "https://arxiv.org/pdf/2504.15304", "abs": "https://arxiv.org/abs/2504.15304", "authors": ["Kangyu Wang"], "title": "Can Machine Learning Agents Deal with Hard Choices?", "categories": ["cs.AI"], "comment": "22 pages excluding bibliography, 27 pagas including bibliography, 3\n  figures", "summary": "Machine Learning ML agents have been increasingly used in decision-making\nacross a wide range of tasks and environments. These ML agents are typically\ndesigned to balance multiple objectives when making choices. Understanding how\ntheir decision-making processes align with or diverge from human reasoning is\nessential. Human agents often encounter hard choices, that is, situations where\noptions are incommensurable; neither option is preferred, yet the agent is not\nindifferent between them. In such cases, human agents can identify hard choices\nand resolve them through deliberation. In contrast, current ML agents, due to\nfundamental limitations in Multi-Objective Optimisation or MOO methods, cannot\nidentify hard choices, let alone resolve them. Neither Scalarised Optimisation\nnor Pareto Optimisation, the two principal MOO approaches, can capture\nincommensurability. This limitation generates three distinct alignment\nproblems: the alienness of ML decision-making behaviour from a human\nperspective; the unreliability of preference-based alignment strategies for\nhard choices; and the blockage of alignment strategies pursuing multiple\nobjectives. Evaluating two potential technical solutions, I recommend an\nensemble solution that appears most promising for enabling ML agents to\nidentify hard choices and mitigate alignment problems. However, no known\ntechnique allows ML agents to resolve hard choices through deliberation, as\nthey cannot autonomously change their goals. This underscores the\ndistinctiveness of human agency and urges ML researchers to reconceptualise\nmachine autonomy and develop frameworks and methods that can better address\nthis fundamental gap."}
{"id": "2504.15300", "pdf": "https://arxiv.org/pdf/2504.15300", "abs": "https://arxiv.org/abs/2504.15300", "authors": ["Chaoyue Niu", "Yucheng Ding", "Junhui Lu", "Zhengxiang Huang", "Hang Zeng", "Yutong Dai", "Xuezhen Tu", "Chengfei Lv", "Fan Wu", "Guihai Chen"], "title": "Collaborative Learning of On-Device Small Model and Cloud-Based Large Model: Advances and Future Directions", "categories": ["cs.LG", "cs.DC", "cs.MA"], "comment": null, "summary": "The conventional cloud-based large model learning framework is increasingly\nconstrained by latency, cost, personalization, and privacy concerns. In this\nsurvey, we explore an emerging paradigm: collaborative learning between\non-device small model and cloud-based large model, which promises low-latency,\ncost-efficient, and personalized intelligent services while preserving user\nprivacy. We provide a comprehensive review across hardware, system, algorithm,\nand application layers. At each layer, we summarize key problems and recent\nadvances from both academia and industry. In particular, we categorize\ncollaboration algorithms into data-based, feature-based, and parameter-based\nframeworks. We also review publicly available datasets and evaluation metrics\nwith user-level or device-level consideration tailored to collaborative\nlearning settings. We further highlight real-world deployments, ranging from\nrecommender systems and mobile livestreaming to personal intelligent\nassistants. We finally point out open research directions to guide future\ndevelopment in this rapidly evolving field."}
{"id": "2504.15309", "pdf": "https://arxiv.org/pdf/2504.15309", "abs": "https://arxiv.org/abs/2504.15309", "authors": ["Anran Yu", "Wei Feng", "Yaochen Zhang", "Xiang Li", "Lei Meng", "Lei Wu", "Xiangxu Meng"], "title": "LLM-Enabled Style and Content Regularization for Personalized Text-to-Image Generation", "categories": ["cs.CV"], "comment": null, "summary": "The personalized text-to-image generation has rapidly advanced with the\nemergence of Stable Diffusion. Existing methods, which typically fine-tune\nmodels using embedded identifiers, often struggle with insufficient stylization\nand inaccurate image content due to reduced textual controllability. In this\npaper, we propose style refinement and content preservation strategies. The\nstyle refinement strategy leverages the semantic information of visual\nreasoning prompts and reference images to optimize style embeddings, allowing a\nmore precise and consistent representation of style information. The content\npreservation strategy addresses the content bias problem by preserving the\nmodel's generalization capabilities, ensuring enhanced textual controllability\nwithout compromising stylization. Experimental results verify that our approach\nachieves superior performance in generating consistent and personalized\ntext-to-image outputs."}
{"id": "2504.15376", "pdf": "https://arxiv.org/pdf/2504.15376", "abs": "https://arxiv.org/abs/2504.15376", "authors": ["Zhiqiu Lin", "Siyuan Cen", "Daniel Jiang", "Jay Karhade", "Hewei Wang", "Chancharik Mitra", "Tiffany Ling", "Yuhan Huang", "Sifan Liu", "Mingyu Chen", "Rushikesh Zawar", "Xue Bai", "Yilun Du", "Chuang Gan", "Deva Ramanan"], "title": "Towards Understanding Camera Motions in Any Video", "categories": ["cs.CV", "cs.AI", "cs.CL", "cs.LG", "cs.MM"], "comment": "Project site: https://linzhiqiu.github.io/papers/camerabench/", "summary": "We introduce CameraBench, a large-scale dataset and benchmark designed to\nassess and improve camera motion understanding. CameraBench consists of ~3,000\ndiverse internet videos, annotated by experts through a rigorous multi-stage\nquality control process. One of our contributions is a taxonomy of camera\nmotion primitives, designed in collaboration with cinematographers. We find,\nfor example, that some motions like \"follow\" (or tracking) require\nunderstanding scene content like moving subjects. We conduct a large-scale\nhuman study to quantify human annotation performance, revealing that domain\nexpertise and tutorial-based training can significantly enhance accuracy. For\nexample, a novice may confuse zoom-in (a change of intrinsics) with translating\nforward (a change of extrinsics), but can be trained to differentiate the two.\nUsing CameraBench, we evaluate Structure-from-Motion (SfM) and Video-Language\nModels (VLMs), finding that SfM models struggle to capture semantic primitives\nthat depend on scene content, while VLMs struggle to capture geometric\nprimitives that require precise estimation of trajectories. We then fine-tune a\ngenerative VLM on CameraBench to achieve the best of both worlds and showcase\nits applications, including motion-augmented captioning, video question\nanswering, and video-text retrieval. We hope our taxonomy, benchmark, and\ntutorials will drive future efforts towards the ultimate goal of understanding\ncamera motions in any video."}
{"id": "2504.15311", "pdf": "https://arxiv.org/pdf/2504.15311", "abs": "https://arxiv.org/abs/2504.15311", "authors": ["Fei Shang", "Haohua Du", "Dawei Yan", "Panlong Yang", "Xiang-Yang Li"], "title": "RINN: One Sample Radio Frequency Imaging based on Physics Informed Neural Network", "categories": ["eess.IV", "cs.AI"], "comment": null, "summary": "Due to its ability to work in non-line-of-sight and low-light environments,\nradio frequency (RF) imaging technology is expected to bring new possibilities\nfor embodied intelligence and multimodal sensing. However, widely used RF\ndevices (such as Wi-Fi) often struggle to provide high-precision\nelectromagnetic measurements and large-scale datasets, hindering the\napplication of RF imaging technology. In this paper, we combine the ideas of\nPINN to design the RINN network, using physical constraints instead of true\nvalue comparison constraints and adapting it with the characteristics of\nubiquitous RF signals, allowing the RINN network to achieve RF imaging using\nonly one sample without phase and with amplitude noise. Our numerical\nevaluation results show that compared with 5 classic algorithms based on phase\ndata for imaging results, RINN's imaging results based on phaseless data are\ngood, with indicators such as RRMSE (0.11) performing similarly well. RINN\nprovides new possibilities for the universal development of radio frequency\nimaging technology."}
{"id": "2504.15471", "pdf": "https://arxiv.org/pdf/2504.15471", "abs": "https://arxiv.org/abs/2504.15471", "authors": ["Tyler A. Chang", "Benjamin K. Bergen"], "title": "Bigram Subnetworks: Mapping to Next Tokens in Transformer Language Models", "categories": ["cs.CL"], "comment": null, "summary": "In Transformer language models, activation vectors transform from current\ntoken embeddings to next token predictions as they pass through the model. To\nisolate a minimal form of this transformation, we identify language model\nsubnetworks that make bigram predictions, naive next token predictions based\nonly on the current token. We find that bigram subnetworks can be found in\nfully trained language models up to 1B parameters, and these subnetworks are\ncritical for model performance even when they consist of less than 0.2% of\nmodel parameters. Bigram subnetworks are concentrated in the first Transformer\nMLP layer, and they overlap significantly with subnetworks trained to optimally\nprune a given model. Mechanistically, the bigram subnetworks often recreate a\npattern from the full models where the first layer induces a sharp change that\naligns activations with next token predictions rather than current token\nrepresentations. Our results demonstrate that bigram subnetworks comprise a\nminimal subset of parameters that are both necessary and sufficient for basic\nnext token predictions in language models, and they help drive the\ntransformation from current to next token activations in the residual stream.\nThese subnetworks can lay a foundation for studying language model circuits by\nbuilding up from a minimal circuit rather than the traditional approach of\nablating circuits from a full model."}
{"id": "2504.16010", "pdf": "https://arxiv.org/pdf/2504.16010", "abs": "https://arxiv.org/abs/2504.16010", "authors": ["Tuong Manh Vu", "Ernesto Carrella", "Robert Axtell", "Omar A. Guerrero"], "title": "The Formation of Production Networks: How Supply Chains Arise from Simple Learning with Minimal Information", "categories": ["cs.MA", "cs.LG", "econ.GN", "q-fin.EC"], "comment": null, "summary": "We develop a model where firms determine the price at which they sell their\ndifferentiable goods, the volume that they produce, and the inputs (types and\namounts) that they purchase from other firms. A steady-state production network\nemerges endogenously without resorting to assumptions such as equilibrium or\nperfect knowledge about production technologies. Through a simple version of\nreinforcement learning, firms with heterogeneous technologies cope with\nuncertainty and maximize profits. Due to this learning process, firms can adapt\nto shocks such as demand shifts, suppliers/clients closure, productivity\nchanges, and production technology modifications; effectively reshaping the\nproduction network. To demonstrate the potential of this model, we analyze the\nupstream and downstream impact of demand and productivity shocks."}
{"id": "2504.15663", "pdf": "https://arxiv.org/pdf/2504.15663", "abs": "https://arxiv.org/abs/2504.15663", "authors": ["Ju Yeon Kang", "Ji Won Yoon", "Semin Kim", "Min Hyun Han", "Nam Soo Kim"], "title": "FADEL: Uncertainty-aware Fake Audio Detection with Evidential Deep Learning", "categories": ["eess.AS", "cs.AI"], "comment": "Accepted at ICASSP 2025", "summary": "Recently, fake audio detection has gained significant attention, as\nadvancements in speech synthesis and voice conversion have increased the\nvulnerability of automatic speaker verification (ASV) systems to spoofing\nattacks. A key challenge in this task is generalizing models to detect unseen,\nout-of-distribution (OOD) attacks. Although existing approaches have shown\npromising results, they inherently suffer from overconfidence issues due to the\nusage of softmax for classification, which can produce unreliable predictions\nwhen encountering unpredictable spoofing attempts. To deal with this\nlimitation, we propose a novel framework called fake audio detection with\nevidential learning (FADEL). By modeling class probabilities with a Dirichlet\ndistribution, FADEL incorporates model uncertainty into its predictions,\nthereby leading to more robust performance in OOD scenarios. Experimental\nresults on the ASVspoof2019 Logical Access (LA) and ASVspoof2021 LA datasets\nindicate that the proposed method significantly improves the performance of\nbaseline models. Furthermore, we demonstrate the validity of uncertainty\nestimation by analyzing a strong correlation between average uncertainty and\nequal error rate (EER) across different spoofing algorithms."}
{"id": "2504.15509", "pdf": "https://arxiv.org/pdf/2504.15509", "abs": "https://arxiv.org/abs/2504.15509", "authors": ["Keqi Deng", "Wenxi Chen", "Xie Chen", "Philip C. Woodland"], "title": "SimulS2S-LLM: Unlocking Simultaneous Inference of Speech LLMs for Speech-to-Speech Translation", "categories": ["cs.CL", "cs.SD", "eess.AS"], "comment": null, "summary": "Simultaneous speech translation (SST) outputs translations in parallel with\nstreaming speech input, balancing translation quality and latency. While large\nlanguage models (LLMs) have been extended to handle the speech modality,\nstreaming remains challenging as speech is prepended as a prompt for the entire\ngeneration process. To unlock LLM streaming capability, this paper proposes\nSimulS2S-LLM, which trains speech LLMs offline and employs a test-time policy\nto guide simultaneous inference. SimulS2S-LLM alleviates the mismatch between\ntraining and inference by extracting boundary-aware speech prompts that allows\nit to be better matched with text input data. SimulS2S-LLM achieves\nsimultaneous speech-to-speech translation (Simul-S2ST) by predicting discrete\noutput speech tokens and then synthesising output speech using a pre-trained\nvocoder. An incremental beam search is designed to expand the search space of\nspeech token prediction without increasing latency. Experiments on the CVSS\nspeech data show that SimulS2S-LLM offers a better translation quality-latency\ntrade-off than existing methods that use the same training data, such as\nimproving ASR-BLEU scores by 3 points at similar latency."}
{"id": "2504.15313", "pdf": "https://arxiv.org/pdf/2504.15313", "abs": "https://arxiv.org/abs/2504.15313", "authors": ["Yajie Yu", "Yue Feng"], "title": "PolicyEvol-Agent: Evolving Policy via Environment Perception and Self-Awareness with Theory of Mind", "categories": ["cs.AI", "cs.LG"], "comment": null, "summary": "Multi-agents has exhibited significant intelligence in real-word simulations\nwith Large language models (LLMs) due to the capabilities of social cognition\nand knowledge retrieval. However, existing research on agents equipped with\neffective cognition chains including reasoning, planning, decision-making and\nreflecting remains limited, especially in the dynamically interactive\nscenarios. In addition, unlike human, prompt-based responses face challenges in\npsychological state perception and empirical calibration during uncertain\ngaming process, which can inevitably lead to cognition bias. In light of above,\nwe introduce PolicyEvol-Agent, a comprehensive LLM-empowered framework\ncharacterized by systematically acquiring intentions of others and adaptively\noptimizing irrational strategies for continual enhancement. Specifically,\nPolicyEvol-Agent first obtains reflective expertise patterns and then\nintegrates a range of cognitive operations with Theory of Mind alongside\ninternal and external perspectives. Simulation results, outperforming RL-based\nmodels and agent-based methods, demonstrate the superiority of PolicyEvol-Agent\nfor final gaming victory. Moreover, the policy evolution mechanism reveals the\neffectiveness of dynamic guideline adjustments in both automatic and human\nevaluation."}
{"id": "2504.15310", "pdf": "https://arxiv.org/pdf/2504.15310", "abs": "https://arxiv.org/abs/2504.15310", "authors": ["Syeda Tahreem Zahra", "Syed Kashif Imdad", "Sohail Khan", "Sohail Khalid", "Nauman Anwar Baig"], "title": "Power Transformer Health Index and Life Span Assessment: A Comprehensive Review of Conventional and Machine Learning based Approaches", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Power transformers play a critical role within the electrical power system,\nmaking their health assessment and the prediction of their remaining lifespan\nparamount for the purpose of ensuring efficient operation and facilitating\neffective maintenance planning. This paper undertakes a comprehensive\nexamination of existent literature, with a primary focus on both conventional\nand cutting-edge techniques employed within this domain. The merits and\ndemerits of recent methodologies and techniques are subjected to meticulous\nscrutiny and explication. Furthermore, this paper expounds upon intelligent\nfault diagnosis methodologies and delves into the most widely utilized\nintelligent algorithms for the assessment of transformer conditions. Diverse\nArtificial Intelligence (AI) approaches, including Artificial Neural Networks\n(ANN) and Convolutional Neural Network (CNN), Support Vector Machine (SVM),\nRandom Forest (RF), Genetic Algorithm (GA), and Particle Swarm Optimization\n(PSO), are elucidated offering pragmatic solutions for enhancing the\nperformance of transformer fault diagnosis. The amalgamation of multiple AI\nmethodologies and the exploration of timeseries analysis further contribute to\nthe augmentation of diagnostic precision and the early detection of faults in\ntransformers. By furnishing a comprehensive panorama of AI applications in the\nfield of transformer fault diagnosis, this study lays the groundwork for future\nresearch endeavors and the progression of this critical area of study."}
{"id": "2504.15362", "pdf": "https://arxiv.org/pdf/2504.15362", "abs": "https://arxiv.org/abs/2504.15362", "authors": ["Yuan-Hong Liao", "Sven Elflein", "Liu He", "Laura Leal-Taixé", "Yejin Choi", "Sanja Fidler", "David Acuna"], "title": "LongPerceptualThoughts: Distilling System-2 Reasoning for System-1 Perception", "categories": ["cs.CV", "cs.CL", "cs.LG"], "comment": "24 pages, 10 figures, in submission. Project page:\n  https://andrewliao11.github.io/LongPerceptualThoughts", "summary": "Recent reasoning models through test-time scaling have demonstrated that long\nchain-of-thoughts can unlock substantial performance boosts in hard reasoning\ntasks such as math and code. However, the benefit of such long thoughts for\nsystem-2 reasoning is relatively less explored in other domains such as\nperceptual tasks where shallower, system-1 reasoning seems sufficient. In this\npaper, we introduce LongPerceptualThoughts, a new synthetic dataset with 30K\nlong-thought traces for perceptual tasks. The key challenges in synthesizing\nelaborate reasoning thoughts for perceptual tasks are that off-the-shelf models\nare not yet equipped with such thinking behavior and that it is not\nstraightforward to build a reliable process verifier for perceptual tasks.\nThus, we propose a novel three-stage data synthesis framework that first\nsynthesizes verifiable multiple-choice questions from dense image descriptions,\nthen extracts simple CoTs from VLMs for those verifiable problems, and finally\nexpands those simple thoughts to elaborate long thoughts via frontier reasoning\nmodels. In controlled experiments with a strong instruction-tuned 7B model, we\ndemonstrate notable improvements over existing visual reasoning data-generation\nmethods. Our model, trained on the generated dataset, achieves an average +3.4\npoints improvement over 5 vision-centric benchmarks, including +11.8 points on\nV$^*$ Bench. Notably, despite being tuned for vision tasks, it also improves\nperformance on the text reasoning benchmark, MMLU-Pro, by +2 points."}
{"id": "2410.07369", "pdf": "https://arxiv.org/pdf/2410.07369", "abs": "https://arxiv.org/abs/2410.07369", "authors": ["Sam Gunn", "Xuandong Zhao", "Dawn Song"], "title": "An Undetectable Watermark for Generative Image Models", "categories": ["cs.CR", "cs.AI", "cs.LG", "cs.MM"], "comment": "ICLR 2025", "summary": "We present the first undetectable watermarking scheme for generative image\nmodels. Undetectability ensures that no efficient adversary can distinguish\nbetween watermarked and un-watermarked images, even after making many adaptive\nqueries. In particular, an undetectable watermark does not degrade image\nquality under any efficiently computable metric. Our scheme works by selecting\nthe initial latents of a diffusion model using a pseudorandom error-correcting\ncode (Christ and Gunn, 2024), a strategy which guarantees undetectability and\nrobustness. We experimentally demonstrate that our watermarks are\nquality-preserving and robust using Stable Diffusion 2.1. Our experiments\nverify that, in contrast to every prior scheme we tested, our watermark does\nnot degrade image quality. Our experiments also demonstrate robustness:\nexisting watermark removal attacks fail to remove our watermark from images\nwithout significantly degrading the quality of the images. Finally, we find\nthat we can robustly encode 512 bits in our watermark, and up to 2500 bits when\nthe images are not subjected to watermark removal attacks. Our code is\navailable at https://github.com/XuandongZhao/PRC-Watermark."}
{"id": "2504.15317", "pdf": "https://arxiv.org/pdf/2504.15317", "abs": "https://arxiv.org/abs/2504.15317", "authors": ["Meher Boulaabi", "Takwa Ben Aïcha Gader", "Afef Kacem Echi", "Zied Bouraoui"], "title": "Enhancing DR Classification with Swin Transformer and Shifted Window Attention", "categories": ["eess.IV", "cs.AI", "cs.CV"], "comment": null, "summary": "Diabetic retinopathy (DR) is a leading cause of blindness worldwide,\nunderscoring the importance of early detection for effective treatment.\nHowever, automated DR classification remains challenging due to variations in\nimage quality, class imbalance, and pixel-level similarities that hinder model\ntraining. To address these issues, we propose a robust preprocessing pipeline\nincorporating image cropping, Contrast-Limited Adaptive Histogram Equalization\n(CLAHE), and targeted data augmentation to improve model generalization and\nresilience. Our approach leverages the Swin Transformer, which utilizes\nhierarchical token processing and shifted window attention to efficiently\ncapture fine-grained features while maintaining linear computational\ncomplexity. We validate our method on the Aptos and IDRiD datasets for\nmulti-class DR classification, achieving accuracy rates of 89.65% and 97.40%,\nrespectively. These results demonstrate the effectiveness of our model,\nparticularly in detecting early-stage DR, highlighting its potential for\nimproving automated retinal screening in clinical settings."}
{"id": "2504.15475", "pdf": "https://arxiv.org/pdf/2504.15475", "abs": "https://arxiv.org/abs/2504.15475", "authors": ["Szymon Kobus", "Deniz Gündüz"], "title": "Speculative Sampling via Exponential Races", "categories": ["cs.CL", "cs.IT", "math.IT"], "comment": null, "summary": "Speculative decoding accelerates large language model inference using a\nsmaller draft model. In this paper, we establish a surprising connection\nbetween speculative decoding and channel simulation, which aims at simulating a\nnoisy channel using as few bits as possible. This connection allows us to\nprovide an information-theoretic analysis of the speed up that can be achieved\nby speculative decoding. Leveraging this link, we derive an explicit relation\nbetween generation speed-up and the number of tokens $k$ generated by the draft\nmodel for large $k$, which serves as an upper bound for all $k$. We also\npropose a novel speculative decoding method via exponential race ERSD that\nmatches state-of-the-art performance."}
{"id": "2504.15425", "pdf": "https://arxiv.org/pdf/2504.15425", "abs": "https://arxiv.org/abs/2504.15425", "authors": ["Songyuan Zhang", "Oswin So", "Mitchell Black", "Zachary Serlin", "Chuchu Fan"], "title": "Solving Multi-Agent Safe Optimal Control with Distributed Epigraph Form MARL", "categories": ["cs.RO", "cs.AI", "cs.LG", "cs.MA", "math.OC"], "comment": "28 pages, 16 figures; Accepted by Robotics: Science and Systems 2025", "summary": "Tasks for multi-robot systems often require the robots to collaborate and\ncomplete a team goal while maintaining safety. This problem is usually\nformalized as a constrained Markov decision process (CMDP), which targets\nminimizing a global cost and bringing the mean of constraint violation below a\nuser-defined threshold. Inspired by real-world robotic applications, we define\nsafety as zero constraint violation. While many safe multi-agent reinforcement\nlearning (MARL) algorithms have been proposed to solve CMDPs, these algorithms\nsuffer from unstable training in this setting. To tackle this, we use the\nepigraph form for constrained optimization to improve training stability and\nprove that the centralized epigraph form problem can be solved in a distributed\nfashion by each agent. This results in a novel centralized training distributed\nexecution MARL algorithm named Def-MARL. Simulation experiments on 8 different\ntasks across 2 different simulators show that Def-MARL achieves the best\noverall performance, satisfies safety constraints, and maintains stable\ntraining. Real-world hardware experiments on Crazyflie quadcopters demonstrate\nthe ability of Def-MARL to safely coordinate agents to complete complex\ncollaborative tasks compared to other methods."}
{"id": "2504.12867", "pdf": "https://arxiv.org/pdf/2504.12867", "abs": "https://arxiv.org/abs/2504.12867", "authors": ["Guanrou Yang", "Chen Yang", "Qian Chen", "Ziyang Ma", "Wenxi Chen", "Wen Wang", "Tianrui Wang", "Yifan Yang", "Zhikang Niu", "Wenrui Liu", "Fan Yu", "Zhihao Du", "Zhifu Gao", "ShiLiang Zhang", "Xie Chen"], "title": "EmoVoice: LLM-based Emotional Text-To-Speech Model with Freestyle Text Prompting", "categories": ["eess.AS", "cs.AI", "cs.CL"], "comment": null, "summary": "Human speech goes beyond the mere transfer of information; it is a profound\nexchange of emotions and a connection between individuals. While Text-to-Speech\n(TTS) models have made huge progress, they still face challenges in controlling\nthe emotional expression in the generated speech. In this work, we propose\nEmoVoice, a novel emotion-controllable TTS model that exploits large language\nmodels (LLMs) to enable fine-grained freestyle natural language emotion\ncontrol, and a phoneme boost variant design that makes the model output phoneme\ntokens and audio tokens in parallel to enhance content consistency, inspired by\nchain-of-thought (CoT) and chain-of-modality (CoM) techniques. Besides, we\nintroduce EmoVoice-DB, a high-quality 40-hour English emotion dataset featuring\nexpressive speech and fine-grained emotion labels with natural language\ndescriptions. EmoVoice achieves state-of-the-art performance on the English\nEmoVoice-DB test set using only synthetic training data, and on the Chinese\nSecap test set using our in-house data. We further investigate the reliability\nof existing emotion evaluation metrics and their alignment with human\nperceptual preferences, and explore using SOTA multimodal LLMs GPT-4o-audio and\nGemini to assess emotional speech. Demo samples are available at\nhttps://yanghaha0908.github.io/EmoVoice/. Dataset, code, and checkpoints will\nbe released."}
{"id": "2405.17615", "pdf": "https://arxiv.org/pdf/2405.17615", "abs": "https://arxiv.org/abs/2405.17615", "authors": ["Francesco Paissan", "Luca Della Libera", "Mirco Ravanelli", "Cem Subakan"], "title": "Listenable Maps for Zero-Shot Audio Classifiers", "categories": ["cs.SD", "cs.LG", "eess.AS", "eess.SP"], "comment": "Accepted to NeurIPS 2024", "summary": "Interpreting the decisions of deep learning models, including audio\nclassifiers, is crucial for ensuring the transparency and trustworthiness of\nthis technology. In this paper, we introduce LMAC-ZS (Listenable Maps for Audio\nClassifiers in the Zero-Shot context), which, to the best of our knowledge, is\nthe first decoder-based post-hoc interpretation method for explaining the\ndecisions of zero-shot audio classifiers. The proposed method utilizes a novel\nloss function that maximizes the faithfulness to the original similarity\nbetween a given text-and-audio pair. We provide an extensive evaluation using\nthe Contrastive Language-Audio Pretraining (CLAP) model to showcase that our\ninterpreter remains faithful to the decisions in a zero-shot classification\ncontext. Moreover, we qualitatively show that our method produces meaningful\nexplanations that correlate well with different text prompts."}
{"id": "2504.15360", "pdf": "https://arxiv.org/pdf/2504.15360", "abs": "https://arxiv.org/abs/2504.15360", "authors": ["Javier Fumanal-Idocin", "Javier Andreu-Perez"], "title": "Reliable Classification with Conformal Learning and Interval-Type 2 Fuzzy Sets", "categories": ["cs.AI"], "comment": null, "summary": "Classical machine learning classifiers tend to be overconfident can be\nunreliable outside of the laboratory benchmarks. Properly assessing the\nreliability of the output of the model per sample is instrumental for real-life\nscenarios where these systems are deployed. Because of this, different\ntechniques have been employed to properly quantify the quality of prediction\nfor a given model. These are most commonly Bayesian statistics and, more\nrecently, conformal learning. Given a calibration set, conformal learning can\nproduce outputs that are guaranteed to cover the target class with a desired\nsignificance level, and are more reliable than the standard confidence\nintervals used by Bayesian methods. In this work, we propose to use conformal\nlearning with fuzzy rule-based systems in classification and show some metrics\nof their performance. Then, we discuss how the use of type 2 fuzzy sets can\nimprove the quality of the output of the system compared to both fuzzy and\ncrisp rules. Finally, we also discuss how the fine-tuning of the system can be\nadapted to improve the quality of the conformal prediction."}
{"id": "2504.15312", "pdf": "https://arxiv.org/pdf/2504.15312", "abs": "https://arxiv.org/abs/2504.15312", "authors": ["Muhammad Mursil", "Hatem A. Rashwan", "Luis Santos-Calderon", "Pere Cavalle-Busquets", "Michelle M. Murphy", "Domenec Puig"], "title": "M-TabNet: A Multi-Encoder Transformer Model for Predicting Neonatal Birth Weight from Multimodal Data", "categories": ["cs.LG"], "comment": null, "summary": "Birth weight (BW) is a key indicator of neonatal health, with low birth\nweight (LBW) linked to increased mortality and morbidity. Early prediction of\nBW enables timely interventions; however, current methods like ultrasonography\nhave limitations, including reduced accuracy before 20 weeks and operator\ndependent variability. Existing models often neglect nutritional and genetic\ninfluences, focusing mainly on physiological and lifestyle factors. This study\npresents an attention-based transformer model with a multi-encoder architecture\nfor early (less than 12 weeks of gestation) BW prediction. Our model\neffectively integrates diverse maternal data such as physiological, lifestyle,\nnutritional, and genetic, addressing limitations seen in prior attention-based\nmodels such as TabNet. The model achieves a Mean Absolute Error (MAE) of 122\ngrams and an R-squared value of 0.94, demonstrating high predictive accuracy\nand interoperability with our in-house private dataset. Independent validation\nconfirms generalizability (MAE: 105 grams, R-squared: 0.95) with the IEEE\nchildren dataset. To enhance clinical utility, predicted BW is classified into\nlow and normal categories, achieving a sensitivity of 97.55% and a specificity\nof 94.48%, facilitating early risk stratification. Model interpretability is\nreinforced through feature importance and SHAP analyses, highlighting\nsignificant influences of maternal age, tobacco exposure, and vitamin B12\nstatus, with genetic factors playing a secondary role. Our results emphasize\nthe potential of advanced deep-learning models to improve early BW prediction,\noffering clinicians a robust, interpretable, and personalized tool for\nidentifying pregnancies at risk and optimizing neonatal outcomes."}
{"id": "2504.15371", "pdf": "https://arxiv.org/pdf/2504.15371", "abs": "https://arxiv.org/abs/2504.15371", "authors": ["Wei Fang", "Priyadarshini Panda"], "title": "Event2Vec: Processing neuromorphic events directly by representations in vector space", "categories": ["cs.CV", "cs.NE"], "comment": null, "summary": "The neuromorphic event cameras have overwhelming advantages in temporal\nresolution, power efficiency, and dynamic range compared to traditional\ncameras. However, the event cameras output asynchronous, sparse, and irregular\nevents, which are not compatible with mainstream computer vision and deep\nlearning methods. Various methods have been proposed to solve this issue but at\nthe cost of long preprocessing procedures, losing temporal resolutions, or\nbeing incompatible with massively parallel computation. Inspired by the great\nsuccess of the word to vector, we summarize the similarities between words and\nevents, then propose the first event to vector (event2vec) representation. We\nvalidate event2vec on classifying the ASL-DVS dataset, showing impressive\nparameter efficiency, accuracy, and speed than previous graph/image/voxel-based\nrepresentations. Beyond task performance, the most attractive advantage of\nevent2vec is that it aligns events to the domain of natural language\nprocessing, showing the promising prospect of integrating events into large\nlanguage and multimodal models. Our codes, models, and training logs are\navailable at https://github.com/fangwei123456/event2vec."}
{"id": "2504.10150", "pdf": "https://arxiv.org/pdf/2504.10150", "abs": "https://arxiv.org/abs/2504.10150", "authors": ["Chen Zhang", "Bo Hu", "Weidong Chen", "Zhendong Mao"], "title": "HistLLM: A Unified Framework for LLM-Based Multimodal Recommendation with User History Encoding and Compression", "categories": ["cs.IR", "cs.MM"], "comment": "We want to withdraw this paper and revise its experimental details.\n  The revised version will be uploaded after further verification", "summary": "While large language models (LLMs) have proven effective in leveraging\ntextual data for recommendations, their application to multimodal\nrecommendation tasks remains relatively underexplored. Although LLMs can\nprocess multimodal information through projection functions that map visual\nfeatures into their semantic space, recommendation tasks often require\nrepresenting users' history interactions through lengthy prompts combining text\nand visual elements, which not only hampers training and inference efficiency\nbut also makes it difficult for the model to accurately capture user\npreferences from complex and extended prompts, leading to reduced\nrecommendation performance. To address this challenge, we introduce HistLLM, an\ninnovative multimodal recommendation framework that integrates textual and\nvisual features through a User History Encoding Module (UHEM), compressing\nmultimodal user history interactions into a single token representation,\neffectively facilitating LLMs in processing user preferences. Extensive\nexperiments demonstrate the effectiveness and efficiency of our proposed\nmechanism."}
{"id": "2504.15390", "pdf": "https://arxiv.org/pdf/2504.15390", "abs": "https://arxiv.org/abs/2504.15390", "authors": ["Nikola Janjusevic", "Amirhoussein Khalilian-Gourtani", "Yao Wang", "Li Feng"], "title": "Learned Primal Dual Splitting for Self-Supervised Noise-Adaptive MRI Reconstruction", "categories": ["eess.IV"], "comment": "4 pages, 3 figures, 1 table", "summary": "Magnetic resonance imaging (MRI) reconstruction has largely been dominated by\ndeep neural networks (DNN); however, many state-of-the-art architectures use\nblack-box structures, which hinder interpretability and improvement. Here, we\npropose an interpretable DNN architecture for self-supervised MRI\nreconstruction and denoising by directly parameterizing and learning the\nclassical primal-dual splitting, dubbed LPDSNet. This splitting algorithm\nallows us to decouple the observation model from the signal prior.\nExperimentally, we show other interpretable architectures without this\ndecoupling property exhibit failure in the self-supervised learning regime. We\nreport state-of-the-art self-supervised joint MRI reconstruction and denoising\nperformance and novel noise-level generalization capabilities, where in\ncontrast black-box networks fail to generalize."}
{"id": "2504.15521", "pdf": "https://arxiv.org/pdf/2504.15521", "abs": "https://arxiv.org/abs/2504.15521", "authors": ["Minghao Wu", "Weixuan Wang", "Sinuo Liu", "Huifeng Yin", "Xintong Wang", "Yu Zhao", "Chenyang Lyu", "Longyue Wang", "Weihua Luo", "Kaifu Zhang"], "title": "The Bitter Lesson Learned from 2,000+ Multilingual Benchmarks", "categories": ["cs.CL"], "comment": "work in progress; 22 pages, 8 figures, 3 tables;", "summary": "As large language models (LLMs) continue to advance in linguistic\ncapabilities, robust multilingual evaluation has become essential for promoting\nequitable technological progress. This position paper examines over 2,000\nmultilingual (non-English) benchmarks from 148 countries, published between\n2021 and 2024, to evaluate past, present, and future practices in multilingual\nbenchmarking. Our findings reveal that, despite significant investments\namounting to tens of millions of dollars, English remains significantly\noverrepresented in these benchmarks. Additionally, most benchmarks rely on\noriginal language content rather than translations, with the majority sourced\nfrom high-resource countries such as China, India, Germany, the UK, and the\nUSA. Furthermore, a comparison of benchmark performance with human judgments\nhighlights notable disparities. STEM-related tasks exhibit strong correlations\nwith human evaluations (0.70 to 0.85), while traditional NLP tasks like\nquestion answering (e.g., XQuAD) show much weaker correlations (0.11 to 0.30).\nMoreover, translating English benchmarks into other languages proves\ninsufficient, as localized benchmarks demonstrate significantly higher\nalignment with local human judgments (0.68) than their translated counterparts\n(0.47). This underscores the importance of creating culturally and\nlinguistically tailored benchmarks rather than relying solely on translations.\nThrough this comprehensive analysis, we highlight six key limitations in\ncurrent multilingual evaluation practices, propose the guiding principles\naccordingly for effective multilingual benchmarking, and outline five critical\nresearch directions to drive progress in the field. Finally, we call for a\nglobal collaborative effort to develop human-aligned benchmarks that prioritize\nreal-world applications."}
{"id": "2504.15970", "pdf": "https://arxiv.org/pdf/2504.15970", "abs": "https://arxiv.org/abs/2504.15970", "authors": ["Baichuan Zeng"], "title": "Recent Advances and Future Directions in Extended Reality (XR): Exploring AI-Powered Spatial Intelligence", "categories": ["cs.HC", "cs.CV", "cs.MA"], "comment": "7 pages,4 figures", "summary": "Extended Reality (XR), encompassing Augmented Reality (AR), Virtual Reality\n(VR) and Mixed Reality (MR), is a transformative technology bridging the\nphysical and virtual world and it has diverse potential which will be\nubiquitous in the future. This review examines XR's evolution through\nfoundational framework - hardware ranging from monitors to sensors and software\nranging from visual tasks to user interface; highlights state of the art (SOTA)\nXR products with the comparison and analysis of performance based on their\nfoundational framework; discusses how commercial XR devices can support the\ndemand of high-quality performance focusing on spatial intelligence. For future\ndirections, attention should be given to the integration of multi-modal AI and\nIoT-driven digital twins to enable adaptive XR systems. With the concept of\nspatial intelligence, future XR should establish a new digital space with\nrealistic experience that benefits humanity. This review underscores the\npivotal role of AI in unlocking XR as the next frontier in human-computer\ninteraction."}
{"id": "2309.13259", "pdf": "https://arxiv.org/pdf/2309.13259", "abs": "https://arxiv.org/abs/2309.13259", "authors": ["Monan Zhou", "Xiaobing Li", "Feng Yu", "Wei Li"], "title": "EMelodyGen: Emotion-Conditioned Melody Generation in ABC Notation with the Musical Feature Template", "categories": ["cs.IR", "cs.AI", "cs.SD", "eess.AS"], "comment": "6 pages, 4 figures, accepted by ICMEW2025", "summary": "The EMelodyGen system focuses on emotional melody generation in ABC notation\ncontrolled by the musical feature template. Owing to the scarcity of\nwell-structured and emotionally labeled sheet music, we designed a template for\ncontrolling emotional melody generation by statistical correlations between\nmusical features and emotion labels derived from small-scale emotional symbolic\nmusic datasets and music psychology conclusions. We then automatically\nannotated a large, well-structured sheet music collection with rough emotional\nlabels by the template, converted them into ABC notation, and reduced label\nimbalance by data augmentation, resulting in a dataset named Rough4Q. Our\nsystem backbone pre-trained on Rough4Q can achieve up to 99% music21 parsing\nrate and melodies generated by our template can lead to a 91% alignment on\nemotional expressions in blind listening tests. Ablation studies further\nvalidated the effectiveness of the feature controls in the template. Available\ncode and demos are at https://github.com/monetjoe/EMelodyGen."}
{"id": "2409.03597", "pdf": "https://arxiv.org/pdf/2409.03597", "abs": "https://arxiv.org/abs/2409.03597", "authors": ["Yucong Zhang", "Xin Zou", "Jinshan Yang", "Wenjun Chen", "Juan Liu", "Faya Liang", "Ming Li"], "title": "Multimodal Laryngoscopic Video Analysis for Assisted Diagnosis of Vocal Fold Paralysis", "categories": ["cs.SD", "cs.AI", "eess.AS"], "comment": "Submitted to CSL", "summary": "This paper presents the Multimodal Laryngoscopic Video Analyzing System\n(MLVAS), a novel system that leverages both audio and video data to\nautomatically extract key video segments and metrics from raw laryngeal\nvideostroboscopic videos for assisted clinical assessment. The system\nintegrates video-based glottis detection with an audio keyword spotting method\nto analyze both video and audio data, identifying patient vocalizations and\nrefining video highlights to ensure optimal inspection of vocal fold movements.\nBeyond key video segment extraction from the raw laryngeal videos, MLVAS is\nable to generate effective audio and visual features for Vocal Fold Paralysis\n(VFP) detection. Pre-trained audio encoders are utilized to encode the patient\nvoice to get the audio features. Visual features are generated by measuring the\nangle deviation of both the left and right vocal folds to the estimated glottal\nmidline on the segmented glottis masks. To get better masks, we introduce a\ndiffusion-based refinement that follows traditional U-Net segmentation to\nreduce false positives. We conducted several ablation studies to demonstrate\nthe effectiveness of each module and modalities in the proposed MLVAS. The\nexperimental results on a public segmentation dataset show the effectiveness of\nour proposed segmentation module. In addition, unilateral VFP classification\nresults on a real-world clinic dataset demonstrate MLVAS's ability of providing\nreliable and objective metrics as well as visualization for assisted clinical\ndiagnosis."}
{"id": "2504.15364", "pdf": "https://arxiv.org/pdf/2504.15364", "abs": "https://arxiv.org/abs/2504.15364", "authors": ["Junyoung Park", "Dalton Jones", "Matt Morse", "Raghavv Goel", "Mingu Lee", "Chris Lott"], "title": "KeDiff: Key Similarity-Based KV Cache Eviction for Long-Context LLM Inference in Resource-Constrained Environments", "categories": ["cs.AI"], "comment": "8 pages, 14 figures", "summary": "In this work, we demonstrate that distinctive keys during LLM inference tend\nto have high attention scores. We explore this phenomenon and propose KeyDiff,\na training-free KV cache eviction method based on key similarity. This method\nfacilitates the deployment of LLM-based application requiring long input\nprompts in resource-constrained environments with limited memory and compute\nbudgets. Unlike other KV cache eviction methods, KeyDiff can process\narbitrarily long prompts within strict resource constraints and efficiently\ngenerate responses. We demonstrate that KeyDiff computes the optimal solution\nto a KV cache selection problem that maximizes key diversity, providing a\ntheoretical understanding of KeyDiff. Notably,KeyDiff does not rely on\nattention scores, allowing the use of optimized attention mechanisms like\nFlashAttention. We demonstrate the effectiveness of KeyDiff across diverse\ntasks and models, illustrating a performance gap of less than 0.04\\% with 8K\ncache budget ($\\sim$ 23\\% KV cache reduction) from the non-evicting baseline on\nthe LongBench benchmark for Llama 3.1-8B and Llama 3.2-3B."}
{"id": "2504.15315", "pdf": "https://arxiv.org/pdf/2504.15315", "abs": "https://arxiv.org/abs/2504.15315", "authors": ["Noa Cohen", "Rotem Dror", "Itzik Klein"], "title": "Diffusion-Driven Inertial Generated Data for Smartphone Location Classification", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Despite the crucial role of inertial measurements in motion tracking and\nnavigation systems, the time-consuming and resource-intensive nature of\ncollecting extensive inertial data has hindered the development of robust\nmachine learning models in this field. In recent years, diffusion models have\nemerged as a revolutionary class of generative models, reshaping the landscape\nof artificial data generation. These models surpass generative adversarial\nnetworks and other state-of-the-art approaches to complex tasks. In this work,\nwe propose diffusion-driven specific force-generated data for smartphone\nlocation recognition. We provide a comprehensive evaluation methodology by\ncomparing synthetic and real recorded specific force data across multiple\nmetrics. Our results demonstrate that our diffusion-based generative model\nsuccessfully captures the distinctive characteristics of specific force signals\nacross different smartphone placement conditions. Thus, by creating diverse,\nrealistic synthetic data, we can reduce the burden of extensive data collection\nwhile providing high-quality training data for machine learning models."}
{"id": "2504.15378", "pdf": "https://arxiv.org/pdf/2504.15378", "abs": "https://arxiv.org/abs/2504.15378", "authors": ["Scott Sorensen", "Wayne Treible", "Robert Wagner", "Andrew D. Gilliam", "Todd Rovito", "Joseph L. Mundy"], "title": "Physics Driven Image Simulation from Commercial Satellite Imagery", "categories": ["cs.CV"], "comment": "15 pages, 9 figures", "summary": "Physics driven image simulation allows for the modeling and creation of\nrealistic imagery beyond what is afforded by typical rendering pipelines. We\naim to automatically generate a physically realistic scene for simulation of a\ngiven region using satellite imagery to model the scene geometry, drive\nmaterial estimates, and populate the scene with dynamic elements. We present\nautomated techniques to utilize satellite imagery throughout the simulated\nscene to expedite scene construction and decrease manual overhead. Our\ntechnique does not use lidar, enabling simulations that could not be\nconstructed previously. To develop a 3D scene, we model the various components\nof the real location, addressing the terrain, modelling man-made structures,\nand populating the scene with smaller elements such as vegetation and vehicles.\nTo create the scene we begin with a Digital Surface Model, which serves as the\nbasis for scene geometry, and allows us to reason about the real location in a\ncommon 3D frame of reference. These simulated scenes can provide increased\nfidelity with less manual intervention for novel locations on earth, and can\nfacilitate algorithm development, and processing pipelines for imagery ranging\nfrom UV to LWIR $(200nm-20\\mu m)$."}
{"id": "2504.15481", "pdf": "https://arxiv.org/pdf/2504.15481", "abs": "https://arxiv.org/abs/2504.15481", "authors": ["Michel Berthier", "Nicoletta Prencipe", "Edoardo Provenzi"], "title": "Split-quaternions for perceptual white balance", "categories": ["eess.IV", "cs.CV"], "comment": null, "summary": "We propose a perceptual chromatic adaptation transform for white balance that\nmakes use of split-quaternions. The novelty of the present work, which is\nmotivated by a recently developed quantum-like model of color perception,\nconsists at stressing the link between the algebraic structures appearing in\nthis model and a certain sub-algebra of the split-quaternions. We show the\npotentiality of this approach for color image processing applications by\nproposing a chromatic adaptation transform, implemented via an appropriate use\nof the split-quaternion multiplication. Moreover, quantitative comparisons with\nthe widely used state-of-the art von Kries chromatic adaptation transform are\nprovided."}
{"id": "2504.15524", "pdf": "https://arxiv.org/pdf/2504.15524", "abs": "https://arxiv.org/abs/2504.15524", "authors": ["Qiyao Wang", "Guhong Chen", "Hongbo Wang", "Huaren Liu", "Minghui Zhu", "Zhifei Qin", "Linwei Li", "Yilin Yue", "Shiqiang Wang", "Jiayan Li", "Yihang Wu", "Ziqiang Liu", "Longze Chen", "Run Luo", "Liyang Fan", "Jiaming Li", "Lei Zhang", "Kan Xu", "Hongfei Lin", "Hamid Alinejad-Rokny", "Shiwen Ni", "Yuan Lin", "Min Yang"], "title": "IPBench: Benchmarking the Knowledge of Large Language Models in Intellectual Property", "categories": ["cs.CL", "cs.AI"], "comment": "89 pages, 75 figures, 55 tables", "summary": "Intellectual Property (IP) is a unique domain that integrates technical and\nlegal knowledge, making it inherently complex and knowledge-intensive. As large\nlanguage models (LLMs) continue to advance, they show great potential for\nprocessing IP tasks, enabling more efficient analysis, understanding, and\ngeneration of IP-related content. However, existing datasets and benchmarks\neither focus narrowly on patents or cover limited aspects of the IP field,\nlacking alignment with real-world scenarios. To bridge this gap, we introduce\nthe first comprehensive IP task taxonomy and a large, diverse bilingual\nbenchmark, IPBench, covering 8 IP mechanisms and 20 tasks. This benchmark is\ndesigned to evaluate LLMs in real-world intellectual property applications,\nencompassing both understanding and generation. We benchmark 16 LLMs, ranging\nfrom general-purpose to domain-specific models, and find that even the\nbest-performing model achieves only 75.8% accuracy, revealing substantial room\nfor improvement. Notably, open-source IP and law-oriented models lag behind\nclosed-source general-purpose models. We publicly release all data and code of\nIPBench and will continue to update it with additional IP-related tasks to\nbetter reflect real-world challenges in the intellectual property domain."}
{"id": "2504.16028", "pdf": "https://arxiv.org/pdf/2504.16028", "abs": "https://arxiv.org/abs/2504.16028", "authors": ["Tigran Bakaryan", "Christoph Aoun", "Ricardo de Lima Ribeiro", "Naira Hovakimyan", "Diogo Gomes"], "title": "Hessian Riemannian Flow For Multi-Population Wardrop Equilibrium", "categories": ["eess.SY", "cs.MA", "cs.SY", "math.OC"], "comment": null, "summary": "In this paper, we address the problem of optimizing flows on generalized\ngraphs that feature multiple entry points and multiple populations, each with\nvarying cost structures. We tackle this problem by considering the\nmulti-population Wardrop equilibrium, defined through variational inequalities.\nWe rigorously analyze the existence and uniqueness of the Wardrop equilibrium.\nFurthermore, we introduce an efficient numerical method to find the solution.\nIn particular, we reformulate the equilibrium problem as a distributed\noptimization problem over subgraphs and introduce a novel Hessian Riemannian\nflow method, a Riemannian-manifold-projected Hessian flow, to efficiently\ncompute a solution. Finally, we demonstrate the effectiveness of our approach\nthrough examples in urban traffic management, including routing for diverse\nvehicle types and strategies for minimizing emissions in congested\nenvironments."}
{"id": "2410.21897", "pdf": "https://arxiv.org/pdf/2410.21897", "abs": "https://arxiv.org/abs/2410.21897", "authors": ["Yifu Sun", "Xulong Zhang", "Monan Zhou", "Wei Li"], "title": "Semi-Supervised Self-Learning Enhanced Music Emotion Recognition", "categories": ["cs.SD", "cs.AI", "eess.AS"], "comment": "12 pages, 2 figures", "summary": "Music emotion recognition (MER) aims to identify the emotions conveyed in a\ngiven musical piece. However, currently, in the field of MER, the available\npublic datasets have limited sample sizes. Recently, segment-based methods for\nemotion-related tasks have been proposed, which train backbone networks on\nshorter segments instead of entire audio clips, thereby naturally augmenting\ntraining samples without requiring additional resources. Then, the predicted\nsegment-level results are aggregated to obtain the entire song prediction. The\nmost commonly used method is that the segment inherits the label of the clip\ncontaining it, but music emotion is not constant during the whole clip. Doing\nso will introduce label noise and make the training easy to overfit. To handle\nthe noisy label issue, we propose a semi-supervised self-learning (SSSL)\nmethod, which can differentiate between samples with correct and incorrect\nlabels in a self-learning manner, thus effectively utilizing the augmented\nsegment-level data. Experiments on three public emotional datasets demonstrate\nthat the proposed method can achieve better or comparable performance."}
{"id": "2412.11272", "pdf": "https://arxiv.org/pdf/2412.11272", "abs": "https://arxiv.org/abs/2412.11272", "authors": ["Rongxiang Wang", "Zhiming Xu", "Felix Xiaozhu Lin"], "title": "WhisperFlow: speech foundation models in real time", "categories": ["cs.SD", "eess.AS"], "comment": null, "summary": "Speech foundation models, such as OpenAI's Whisper, become the state of the\nart in speech understanding due to their strong accuracy and generalizability.\nYet, their applications are mostly limited to processing pre-recorded speech,\nwhereas processing of streaming speech, in particular doing it efficiently,\nremains rudimentary. Behind this inefficiency are multiple fundamental reasons:\n(1) speech foundation models are trained to process long, fixed-length voice\ninputs (often 30 seconds); (2) encoding each voice input requires encoding as\nmany as 1,500 tokens with tens of transformer layers; (3) decoding each output\nentails an irregular, complex beam search. As such, streaming speech processing\non resource-constrained client devices is more expensive than other AI tasks,\ne.g., text generation.\n  To this end, we present a novel framework, WhisperFlow, which embodies both\nmodel and system optimizations. (1) Hush word as a short, learnable audio\nsegment; appended to a voice input, a hush word gracefully stops the speech\nmodel from processing more input without hallucination; (2) Beam pruning, which\naligns streaming audio buffers over time and reuses results from earlier\ndecoding rounds, therefore significantly accelerating decoding; and (3) CPU/GPU\npipelining, which not only maps to the encoding/decoding stages dynamically,\nbut also tunes to an optimal resource ratio, respecting the encoding/decoding\nspeed that varies across voice inputs, models, and hardware.\n  We test WhisperFlow on commodity ARM platforms with 4-12 CPU cores and 10-30\nGPU cores. It reduces per-word latency by 1.6x-4.7x to as low as 0.5 second,\nwhile seeing negligible accuracy degradation. On an entry-level MacBook Air,\nWhisperFlow can keep the per-word latency around 1 second, with the whole\ndevice drawing only 7 Watts in total."}
{"id": "2504.15434", "pdf": "https://arxiv.org/pdf/2504.15434", "abs": "https://arxiv.org/abs/2504.15434", "authors": ["Sarath Shekkizhar", "Romain Cosentino"], "title": "AGI Is Coming... Right After AI Learns to Play Wordle", "categories": ["cs.AI", "cs.CV"], "comment": null, "summary": "This paper investigates multimodal agents, in particular, OpenAI's\nComputer-User Agent (CUA), trained to control and complete tasks through a\nstandard computer interface, similar to humans. We evaluated the agent's\nperformance on the New York Times Wordle game to elicit model behaviors and\nidentify shortcomings. Our findings revealed a significant discrepancy in the\nmodel's ability to recognize colors correctly depending on the context. The\nmodel had a $5.36\\%$ success rate over several hundred runs across a week of\nWordle. Despite the immense enthusiasm surrounding AI agents and their\npotential to usher in Artificial General Intelligence (AGI), our findings\nreinforce the fact that even simple tasks present substantial challenges for\ntoday's frontier AI models. We conclude with a discussion of the potential\nunderlying causes, implications for future development, and research directions\nto improve these AI systems."}
{"id": "2504.15322", "pdf": "https://arxiv.org/pdf/2504.15322", "abs": "https://arxiv.org/abs/2504.15322", "authors": ["Xiao Zhou", "Yuze Sun", "Jie Wu", "Xiaomeng Huang"], "title": "How to systematically develop an effective AI-based bias correction model?", "categories": ["cs.LG", "cs.AI", "physics.ao-ph"], "comment": null, "summary": "This study introduces ReSA-ConvLSTM, an artificial intelligence (AI)\nframework for systematic bias correction in numerical weather prediction (NWP).\nWe propose three innovations by integrating dynamic climatological\nnormalization, ConvLSTM with temporal causality constraints, and residual\nself-attention mechanisms. The model establishes a physics-aware nonlinear\nmapping between ECMWF forecasts and ERA5 reanalysis data. Using 41 years\n(1981-2021) of global atmospheric data, the framework reduces systematic biases\nin 2-m air temperature (T2m), 10-m winds (U10/V10), and sea-level pressure\n(SLP), achieving up to 20% RMSE reduction over 1-7 day forecasts compared to\noperational ECMWF outputs. The lightweight architecture (10.6M parameters)\nenables efficient generalization to multiple variables and downstream\napplications, reducing retraining time by 85% for cross-variable correction\nwhile improving ocean model skill through bias-corrected boundary conditions.\nThe ablation experiments demonstrate that our innovations significantly improve\nthe model's correction performance, suggesting that incorporating variable\ncharacteristics into the model helps enhance forecasting skills."}
{"id": "2504.15380", "pdf": "https://arxiv.org/pdf/2504.15380", "abs": "https://arxiv.org/abs/2504.15380", "authors": ["Huimin Zeng", "Jiacheng Li", "Zhiwei Xiong"], "title": "Plug-and-Play Versatile Compressed Video Enhancement", "categories": ["cs.CV"], "comment": "Accepted to CVPR 2025", "summary": "As a widely adopted technique in data transmission, video compression\neffectively reduces the size of files, making it possible for real-time cloud\ncomputing. However, it comes at the cost of visual quality, posing challenges\nto the robustness of downstream vision models. In this work, we present a\nversatile codec-aware enhancement framework that reuses codec information to\nadaptively enhance videos under different compression settings, assisting\nvarious downstream vision tasks without introducing computation bottleneck.\nSpecifically, the proposed codec-aware framework consists of a\ncompression-aware adaptation (CAA) network that employs a hierarchical\nadaptation mechanism to estimate parameters of the frame-wise enhancement\nnetwork, namely the bitstream-aware enhancement (BAE) network. The BAE network\nfurther leverages temporal and spatial priors embedded in the bitstream to\neffectively improve the quality of compressed input frames. Extensive\nexperimental results demonstrate the superior quality enhancement performance\nof our framework over existing enhancement methods, as well as its versatility\nin assisting multiple downstream tasks on compressed videos as a plug-and-play\nmodule. Code and models are available at\nhttps://huimin-zeng.github.io/PnP-VCVE/."}
{"id": "2504.15545", "pdf": "https://arxiv.org/pdf/2504.15545", "abs": "https://arxiv.org/abs/2504.15545", "authors": ["Zizhi Chen", "Xinyu Zhang", "Minghao Han", "Yizhou Liu", "Ziyun Qian", "Weifeng Zhang", "Xukun Zhang", "Jingwei Wei", "Lihua Zhang"], "title": "VLM-based Prompts as the Optimal Assistant for Unpaired Histopathology Virtual Staining", "categories": ["eess.IV", "cs.CV"], "comment": null, "summary": "In histopathology, tissue sections are typically stained using common H&E\nstaining or special stains (MAS, PAS, PASM, etc.) to clearly visualize specific\ntissue structures. The rapid advancement of deep learning offers an effective\nsolution for generating virtually stained images, significantly reducing the\ntime and labor costs associated with traditional histochemical staining.\nHowever, a new challenge arises in separating the fundamental visual\ncharacteristics of tissue sections from the visual differences induced by\nstaining agents. Additionally, virtual staining often overlooks essential\npathological knowledge and the physical properties of staining, resulting in\nonly style-level transfer. To address these issues, we introduce, for the first\ntime in virtual staining tasks, a pathological vision-language large model\n(VLM) as an auxiliary tool. We integrate contrastive learnable prompts,\nfoundational concept anchors for tissue sections, and staining-specific concept\nanchors to leverage the extensive knowledge of the pathological VLM. This\napproach is designed to describe, frame, and enhance the direction of virtual\nstaining. Furthermore, we have developed a data augmentation method based on\nthe constraints of the VLM. This method utilizes the VLM's powerful image\ninterpretation capabilities to further integrate image style and structural\ninformation, proving beneficial in high-precision pathological diagnostics.\nExtensive evaluations on publicly available multi-domain unpaired staining\ndatasets demonstrate that our method can generate highly realistic images and\nenhance the accuracy of downstream tasks, such as glomerular detection and\nsegmentation. Our code is available at:\nhttps://github.com/CZZZZZZZZZZZZZZZZZ/VPGAN-HARBOR"}
{"id": "2504.15527", "pdf": "https://arxiv.org/pdf/2504.15527", "abs": "https://arxiv.org/abs/2504.15527", "authors": ["Sophia Maria"], "title": "Compass-V2 Technical Report", "categories": ["cs.CL"], "comment": null, "summary": "Predominant LLMs focus on high-resource languages while leaving low-resource\nlanguages, particularly those in Southeast Asia (SEA), underrepresented. In\naddition, those models are general-purpose and pay limited attention to the\ne-commerce domain. To overcome these limitations, we introduce Compass-v2, a\nlightweight Mixture-of-Experts (MoE) model specifically designed for Southeast\nAsian languages and e-commerce applications. To balance model performance and\ninference cost, the model is designed with 30B total parameters and 5B active\nparameters, incorporating both fine-grained and shared expert modules. To\nenhance multilingual performance, we curated and constructed a high-quality,\nindustry-leading SEA dataset, to the best of our knowledge. To boost\nperformance in the e-commerce domain, we built a dataset comprising hundreds of\nbillions of tokens, sourced through external data mining and internal platform\ncollection. Besides, we pioneered a hybrid reasoning model that supports both\nfast thinking and deep thinking within a unified framework to enhance the\nreasoning capabilities, diverging from the conventional industry practice of\ndeploying two separate models. Through extensive experimental evaluations, our\nmodel demonstrates state-of-the-art SEA multilingual and e-commerce performance\namong sub-30B models, while maintaining significantly lower inference cost."}
{"id": "2504.10915", "pdf": "https://arxiv.org/pdf/2504.10915", "abs": "https://arxiv.org/abs/2504.10915", "authors": ["Rajesh Ranjan", "Shailja Gupta", "Surya Narayan Singh"], "title": "LOKA Protocol: A Decentralized Framework for Trustworthy and Ethical AI Agent Ecosystems", "categories": ["cs.MA", "cs.AI", "cs.CY"], "comment": "4 Figures, 1 Table", "summary": "The rise of autonomous AI agents, capable of perceiving, reasoning, and\nacting independently, signals a profound shift in how digital ecosystems\noperate, govern, and evolve. As these agents proliferate beyond centralized\ninfrastructures, they expose foundational gaps in identity, accountability, and\nethical alignment. Three critical questions emerge: Identity: Who or what is\nthe agent? Accountability: Can its actions be verified, audited, and trusted?\nEthical Consensus: Can autonomous systems reliably align with human values and\nprevent harmful emergent behaviors? We present the novel LOKA Protocol (Layered\nOrchestration for Knowledgeful Agents), a unified, systems-level architecture\nfor building ethically governed, interoperable AI agent ecosystems. LOKA\nintroduces a proposed Universal Agent Identity Layer (UAIL) for decentralized,\nverifiable identity; intent-centric communication protocols for semantic\ncoordination across diverse agents; and a Decentralized Ethical Consensus\nProtocol (DECP) that could enable agents to make context-aware decisions\ngrounded in shared ethical baselines. Anchored in emerging standards such as\nDecentralized Identifiers (DIDs), Verifiable Credentials (VCs), and\npost-quantum cryptography, LOKA proposes a scalable, future-resilient blueprint\nfor multi-agent AI governance. By embedding identity, trust, and ethics into\nthe protocol layer itself, LOKA proposes the foundation for a new era of\nresponsible, transparent, and autonomous AI ecosystems operating across digital\nand physical domains."}
{"id": "2412.15726", "pdf": "https://arxiv.org/pdf/2412.15726", "abs": "https://arxiv.org/abs/2412.15726", "authors": ["Vincenzo Timmel", "Claudio Paonessa", "Reza Kakooee", "Manfred Vogel", "Daniel Perruchoud"], "title": "Fine-tuning Whisper on Low-Resource Languages for Real-World Applications", "categories": ["cs.CL", "eess.AS"], "comment": null, "summary": "This paper presents a new approach to fine-tuning OpenAI's Whisper model for\nlow-resource languages by introducing a novel data generation method that\nconverts sentence-level data into a long-form corpus, using Swiss German as a\ncase study. Non-sentence-level data, which could improve the performance of\nlong-form audio, is difficult to obtain and often restricted by copyright laws.\nOur method bridges this gap by transforming more accessible sentence-level data\ninto a format that preserves the model's ability to handle long-form audio and\nperform segmentation without requiring non-sentence-level data. Our data\ngeneration process improves performance in several real-world applications and\nleads to the development of a new state-of-the-art speech-to-text (STT) model\nfor Swiss German. We compare our model with a non-fine-tuned Whisper and our\nprevious state-of-the-art Swiss German STT models, where our new model achieves\nhigher BLEU scores. Our results also indicate that the proposed method is\nadaptable to other low-resource languages, supported by written guidance and\ncode that allows the creation of fine-tuned Whisper models, which keep\nsegmentation capabilities and allow the transcription of longer audio files\nusing only sentence-level data with high quality."}
{"id": "2504.02407", "pdf": "https://arxiv.org/pdf/2504.02407", "abs": "https://arxiv.org/abs/2504.02407", "authors": ["Xiaohui Sun", "Ruitong Xiao", "Jianye Mo", "Bowen Wu", "Qun Yu", "Baoxun Wang"], "title": "F5R-TTS: Improving Flow-Matching based Text-to-Speech with Group Relative Policy Optimization", "categories": ["cs.SD", "eess.AS"], "comment": null, "summary": "We present F5R-TTS, a novel text-to-speech (TTS) system that integrates Group\nRelative Policy Optimization (GRPO) into a flow-matching based architecture. By\nreformulating the deterministic outputs of flow-matching TTS into probabilistic\nGaussian distributions, our approach enables seamless integration of\nreinforcement learning algorithms. During pretraining, we train a\nprobabilistically reformulated flow-matching based model which is derived from\nF5-TTS with an open-source dataset. In the subsequent reinforcement learning\n(RL) phase, we employ a GRPO-driven enhancement stage that leverages dual\nreward metrics: word error rate (WER) computed via automatic speech recognition\nand speaker similarity (SIM) assessed by verification models. Experimental\nresults on zero-shot voice cloning demonstrate that F5R-TTS achieves\nsignificant improvements in both speech intelligibility (a 29.5% relative\nreduction in WER) and speaker similarity (a 4.6% relative increase in SIM\nscore) compared to conventional flow-matching based TTS systems. Audio samples\nare available at https://frontierlabs.github.io/F5R."}
{"id": "2504.15457", "pdf": "https://arxiv.org/pdf/2504.15457", "abs": "https://arxiv.org/abs/2504.15457", "authors": ["Paresh Chaudhary", "Yancheng Liang", "Daphne Chen", "Simon S. Du", "Natasha Jaques"], "title": "Improving Human-AI Coordination through Adversarial Training and Generative Models", "categories": ["cs.AI"], "comment": null, "summary": "Being able to cooperate with new people is an important component of many\neconomically valuable AI tasks, from household robotics to autonomous driving.\nHowever, generalizing to novel humans requires training on data that captures\nthe diversity of human behaviors. Adversarial training is one avenue for\nsearching for such data and ensuring that agents are robust. However, it is\ndifficult to apply in the cooperative setting because adversarial policies\nintentionally learn to sabotage the task instead of simulating valid\ncooperation partners. To address this challenge, we propose a novel strategy\nfor overcoming self-sabotage that combines a pre-trained generative model to\nsimulate valid cooperative agent policies with adversarial training to maximize\nregret. We call our method GOAT: Generative Online Adversarial Training. In\nthis framework, the GOAT dynamically searches for and generates coordination\nstrategies where the learning policy -- the Cooperator agent -- underperforms.\nGOAT enables better generalization by exposing the Cooperator to various\nchallenging interaction scenarios. We maintain realistic coordination\nstrategies by updating only the generative model's embedding while keeping its\nparameters frozen, thus avoiding adversarial exploitation. We evaluate GOAT\nwith real human partners, and the results demonstrate state-of-the-art\nperformance on the Overcooked benchmark, highlighting its effectiveness in\ngeneralizing to diverse human behaviors."}
{"id": "2504.15323", "pdf": "https://arxiv.org/pdf/2504.15323", "abs": "https://arxiv.org/abs/2504.15323", "authors": ["Donggyun Kim", "Chanwoo Kim", "Seunghoon Hong"], "title": "HyperFlow: Gradient-Free Emulation of Few-Shot Fine-Tuning", "categories": ["cs.LG", "cs.AI", "cs.CV"], "comment": null, "summary": "While test-time fine-tuning is beneficial in few-shot learning, the need for\nmultiple backpropagation steps can be prohibitively expensive in real-time or\nlow-resource scenarios. To address this limitation, we propose an approach that\nemulates gradient descent without computing gradients, enabling efficient\ntest-time adaptation. Specifically, we formulate gradient descent as an Euler\ndiscretization of an ordinary differential equation (ODE) and train an\nauxiliary network to predict the task-conditional drift using only the few-shot\nsupport set. The adaptation then reduces to a simple numerical integration\n(e.g., via the Euler method), which requires only a few forward passes of the\nauxiliary network -- no gradients or forward passes of the target model are\nneeded. In experiments on cross-domain few-shot classification using the\nMeta-Dataset and CDFSL benchmarks, our method significantly improves\nout-of-domain performance over the non-fine-tuned baseline while incurring only\n6\\% of the memory cost and 0.02\\% of the computation time of standard\nfine-tuning, thus establishing a practical middle ground between direct\ntransfer and fully fine-tuned approaches."}
{"id": "2504.15384", "pdf": "https://arxiv.org/pdf/2504.15384", "abs": "https://arxiv.org/abs/2504.15384", "authors": ["Chen Zhao", "Anjum Shaik", "Joyce H. Keyak", "Nancy E. Lane", "Jeffrey D. Deng", "Kuan-Jui Su", "Qiuying Sha", "Hui Shen", "Hong-Wen Deng", "Weihua Zhou"], "title": "ICGM-FRAX: Iterative Cross Graph Matching for Hip Fracture Risk Assessment using Dual-energy X-ray Absorptiometry Images", "categories": ["cs.CV"], "comment": "23 pages, 4 figures", "summary": "Hip fractures represent a major health concern, particularly among the\nelderly, often leading decreased mobility and increased mortality. Early and\naccurate detection of at risk individuals is crucial for effective\nintervention. In this study, we propose Iterative Cross Graph Matching for Hip\nFracture Risk Assessment (ICGM-FRAX), a novel approach for predicting hip\nfractures using Dual-energy X-ray Absorptiometry (DXA) images. ICGM-FRAX\ninvolves iteratively comparing a test (subject) graph with multiple template\ngraphs representing the characteristics of hip fracture subjects to assess the\nsimilarity and accurately to predict hip fracture risk. These graphs are\nobtained as follows. The DXA images are separated into multiple regions of\ninterest (RoIs), such as the femoral head, shaft, and lesser trochanter.\nRadiomic features are then calculated for each RoI, with the central\ncoordinates used as nodes in a graph. The connectivity between nodes is\nestablished according to the Euclidean distance between these coordinates. This\nprocess transforms each DXA image into a graph, where each node represents a\nRoI, and edges derived by the centroids of RoIs capture the spatial\nrelationships between them. If the test graph closely matches a set of template\ngraphs representing subjects with incident hip fractures, it is classified as\nindicating high hip fracture risk. We evaluated our method using 547 subjects\nfrom the UK Biobank dataset, and experimental results show that ICGM-FRAX\nachieved a sensitivity of 0.9869, demonstrating high accuracy in predicting hip\nfractures."}
{"id": "2504.15649", "pdf": "https://arxiv.org/pdf/2504.15649", "abs": "https://arxiv.org/abs/2504.15649", "authors": ["Biao Wu", "Diankai Zhang", "Shaoli Liu", "Si Gao", "Chengjian Zheng", "Ning Wang"], "title": "RepNet-VSR: Reparameterizable Architecture for High-Fidelity Video Super-Resolution", "categories": ["eess.IV", "cs.CV"], "comment": "Champion Solution for CVPR 2025 MAI VSR Track", "summary": "As a fundamental challenge in visual computing, video super-resolution (VSR)\nfocuses on reconstructing highdefinition video sequences from their degraded\nlowresolution counterparts. While deep convolutional neural networks have\ndemonstrated state-of-the-art performance in spatial-temporal super-resolution\ntasks, their computationally intensive nature poses significant deployment\nchallenges for resource-constrained edge devices, particularly in real-time\nmobile video processing scenarios where power efficiency and latency\nconstraints coexist. In this work, we propose a Reparameterizable Architecture\nfor High Fidelity Video Super Resolution method, named RepNet-VSR, for\nreal-time 4x video super-resolution. On the REDS validation set, the proposed\nmodel achieves 27.79 dB PSNR when processing 180p to 720p frames in 103 ms per\n10 frames on a MediaTek Dimensity NPU. The competition results demonstrate an\nexcellent balance between restoration quality and deployment efficiency. The\nproposed method scores higher than the previous champion algorithm of MAI video\nsuper-resolution challenge."}
{"id": "2504.15544", "pdf": "https://arxiv.org/pdf/2504.15544", "abs": "https://arxiv.org/abs/2504.15544", "authors": ["Issa Sugiura", "Kouta Nakayama", "Yusuke Oda"], "title": "llm-jp-modernbert: A ModernBERT Model Trained on a Large-Scale Japanese Corpus with Long Context Length", "categories": ["cs.CL"], "comment": "9 pages, 5 figures", "summary": "Encoder-only transformer models like BERT are widely adopted as a pre-trained\nbackbone for tasks like sentence classification and retrieval. However,\npretraining of encoder models with large-scale corpora and long contexts has\nbeen relatively underexplored compared to decoder-only transformers. In this\nwork, we present llm-jp-modernbert, a ModernBERT model trained on a publicly\navailable, massive Japanese corpus with a context length of 8192 tokens. While\nour model does not surpass existing baselines on downstream tasks, it achieves\ngood results on fill-mask test evaluations. We also analyze the effect of\ncontext length expansion through pseudo-perplexity experiments. Furthermore, we\ninvestigate sentence embeddings in detail, analyzing their transitions during\ntraining and comparing them with those from other existing models, confirming\nsimilar trends with models sharing the same architecture. To support\nreproducibility and foster the development of long-context BERT, we release our\nmodel, along with the training and evaluation code."}
{"id": "2503.17436", "pdf": "https://arxiv.org/pdf/2503.17436", "abs": "https://arxiv.org/abs/2503.17436", "authors": ["Lars Kröger", "Cristian Cioflan", "Victor Kartsch", "Luca Benini"], "title": "On-Device Federated Continual Learning on RISC-V-based Ultra-Low-Power SoC for Intelligent Nano-Drone Swarms", "categories": ["cs.LG", "cs.CV", "cs.MA", "I.2.11; I.2.6; C.5.3; I.4.9"], "comment": "2 pages, 2 tables, 1 figure. Accepted as a poster at the RISC-V\n  Summit Europe 2025", "summary": "RISC-V-based architectures are paving the way for efficient On-Device\nLearning (ODL) in smart edge devices. When applied across multiple nodes, ODL\nenables the creation of intelligent sensor networks that preserve data privacy.\nHowever, developing ODL-capable, battery-operated embedded platforms presents\nsignificant challenges due to constrained computational resources and limited\ndevice lifetime, besides intrinsic learning issues such as catastrophic\nforgetting. We face these challenges by proposing a regularization-based\nOn-Device Federated Continual Learning algorithm tailored for multiple\nnano-drones performing face recognition tasks. We demonstrate our approach on a\nRISC-V-based 10-core ultra-low-power SoC, optimizing the ODL computational\nrequirements. We improve the classification accuracy by 24% over naive\nfine-tuning, requiring 178 ms per local epoch and 10.5 s per global epoch,\ndemonstrating the effectiveness of the architecture for this task."}
{"id": "2502.15430", "pdf": "https://arxiv.org/pdf/2502.15430", "abs": "https://arxiv.org/abs/2502.15430", "authors": ["David Valdivia", "Marien Renaud", "Elsa Cazelles", "Cédric Févotte"], "title": "Audio signal interpolation using optimal transportation of spectrograms", "categories": ["eess.SP", "cs.SD", "eess.AS"], "comment": null, "summary": "We present a novel approach for generating an artificial audio signal that\ninterpolates between given source and target sounds. Our approach relies on the\ncomputation of Wasserstein barycenters of the source and target spectrograms,\nfollowed by phase reconstruction and inversion. In contrast with previous\nworks, our new method considers the spectrograms globally and does not operate\non a temporal frame-to-frame basis. Another contribution is to endow the\ntransportation cost matrix with a specific structure that prohibits remote\ndisplacements of energy along the time axis, and for which optimal transport is\nmade possible by leveraging the unbalanced transport framework. The proposed\ncost matrix makes sense from the audio perspective and also allows to reduce\nthe computation load. Results with synthetic musical notes and real\nenvironmental sounds illustrate the potential of our novel approach."}
{"id": "2504.08365", "pdf": "https://arxiv.org/pdf/2504.08365", "abs": "https://arxiv.org/abs/2504.08365", "authors": ["Xueping Zhang", "Yaxiong Chen", "Ruilin Yao", "Yunfei Zi", "Shengwu Xiong"], "title": "Location-Oriented Sound Event Localization and Detection with Spatial Mapping and Regression Localization", "categories": ["cs.SD", "eess.AS"], "comment": null, "summary": "Sound Event Localization and Detection (SELD) combines the Sound Event\nDetection (SED) with the corresponding Direction Of Arrival (DOA). Recently,\nadopted event oriented multi-track methods affect the generality in polyphonic\nenvironments due to the limitation of the number of tracks. To enhance the\ngenerality in polyphonic environments, we propose Spatial Mapping and\nRegression Localization for SELD (SMRL-SELD). SMRL-SELD segments the 3D spatial\nspace, mapping it to a 2D plane, and a new regression localization loss is\nproposed to help the results converge toward the location of the corresponding\nevent. SMRL-SELD is location-oriented, allowing the model to learn event\nfeatures based on orientation. Thus, the method enables the model to process\npolyphonic sounds regardless of the number of overlapping events. We conducted\nexperiments on STARSS23 and STARSS22 datasets and our proposed SMRL-SELD\noutperforms the existing SELD methods in overall evaluation and polyphony\nenvironments."}
{"id": "2504.15466", "pdf": "https://arxiv.org/pdf/2504.15466", "abs": "https://arxiv.org/abs/2504.15466", "authors": ["Jiayi Pan", "Xiuyu Li", "Long Lian", "Charlie Snell", "Yifei Zhou", "Adam Yala", "Trevor Darrell", "Kurt Keutzer", "Alane Suhr"], "title": "Learning Adaptive Parallel Reasoning with Language Models", "categories": ["cs.AI", "cs.CL"], "comment": "Code, model, and data are available at\n  https://github.com/Parallel-Reasoning/APR. The first three authors\n  contributed equally to this work", "summary": "Scaling inference-time computation has substantially improved the reasoning\ncapabilities of language models. However, existing methods have significant\nlimitations: serialized chain-of-thought approaches generate overly long\noutputs, leading to increased latency and exhausted context windows, while\nparallel methods such as self-consistency suffer from insufficient\ncoordination, resulting in redundant computations and limited performance\ngains. To address these shortcomings, we propose Adaptive Parallel Reasoning\n(APR), a novel reasoning framework that enables language models to orchestrate\nboth serialized and parallel computations end-to-end. APR generalizes existing\nreasoning methods by enabling adaptive multi-threaded inference using spawn()\nand join() operations. A key innovation is our end-to-end reinforcement\nlearning strategy, optimizing both parent and child inference threads to\nenhance task success rate without requiring predefined reasoning structures.\nExperiments on the Countdown reasoning task demonstrate significant benefits of\nAPR: (1) higher performance within the same context window (83.4% vs. 60.0% at\n4k context); (2) superior scalability with increased computation (80.1% vs.\n66.6% at 20k total tokens); (3) improved accuracy at equivalent latency (75.2%\nvs. 57.3% at approximately 5,000ms). APR represents a step towards enabling\nlanguage models to autonomously optimize their reasoning processes through\nadaptive allocation of computation."}
{"id": "2504.15325", "pdf": "https://arxiv.org/pdf/2504.15325", "abs": "https://arxiv.org/abs/2504.15325", "authors": ["Alberto Casagrande", "Francesco Fabris", "Rossano Girometti", "Roberto Pagliarini"], "title": "Significativity Indices for Agreement Values", "categories": ["cs.LG", "cs.AI", "stat.ML"], "comment": "27 pages, 6 figures", "summary": "Agreement measures, such as Cohen's kappa or intraclass correlation, gauge\nthe matching between two or more classifiers. They are used in a wide range of\ncontexts from medicine, where they evaluate the effectiveness of medical\ntreatments and clinical trials, to artificial intelligence, where they can\nquantify the approximation due to the reduction of a classifier. The\nconsistency of different classifiers to a golden standard can be compared\nsimply by using the order induced by their agreement measure with respect to\nthe golden standard itself. Nevertheless, labelling an approach as good or bad\nexclusively by using the value of an agreement measure requires a scale or a\nsignificativity index. Some quality scales have been proposed in the literature\nfor Cohen's kappa, but they are mainly naive, and their boundaries are\narbitrary. This work proposes a general approach to evaluate the\nsignificativity of any agreement value between two classifiers and introduces\ntwo significativity indices: one dealing with finite data sets, the other one\nhandling classification probability distributions. Moreover, this manuscript\nconsiders the computational issues of evaluating such indices and identifies\nsome efficient algorithms to evaluate them."}
{"id": "2504.15397", "pdf": "https://arxiv.org/pdf/2504.15397", "abs": "https://arxiv.org/abs/2504.15397", "authors": ["Ankit Dhiman", "Manan Shah", "R Venkatesh Babu"], "title": "MirrorVerse: Pushing Diffusion Models to Realistically Reflect the World", "categories": ["cs.CV"], "comment": "Accepted to CVPR 2025. Project Page: https://mirror-verse.github.io/", "summary": "Diffusion models have become central to various image editing tasks, yet they\noften fail to fully adhere to physical laws, particularly with effects like\nshadows, reflections, and occlusions. In this work, we address the challenge of\ngenerating photorealistic mirror reflections using diffusion-based generative\nmodels. Despite extensive training data, existing diffusion models frequently\noverlook the nuanced details crucial to authentic mirror reflections. Recent\napproaches have attempted to resolve this by creating synhetic datasets and\nframing reflection generation as an inpainting task; however, they struggle to\ngeneralize across different object orientations and positions relative to the\nmirror. Our method overcomes these limitations by introducing key augmentations\ninto the synthetic data pipeline: (1) random object positioning, (2) randomized\nrotations, and (3) grounding of objects, significantly enhancing generalization\nacross poses and placements. To further address spatial relationships and\nocclusions in scenes with multiple objects, we implement a strategy to pair\nobjects during dataset generation, resulting in a dataset robust enough to\nhandle these complex scenarios. Achieving generalization to real-world scenes\nremains a challenge, so we introduce a three-stage training curriculum to\ndevelop the MirrorFusion 2.0 model to improve real-world performance. We\nprovide extensive qualitative and quantitative evaluations to support our\napproach. The project page is available at: https://mirror-verse.github.io/."}
{"id": "2504.15667", "pdf": "https://arxiv.org/pdf/2504.15667", "abs": "https://arxiv.org/abs/2504.15667", "authors": ["Jingchen Zou", "Jianqiang Li", "Gabriel Jimenez", "Qing Zhao", "Daniel Racoceanu", "Matias Cosarinsky", "Enzo Ferrante", "Guanghui Fu"], "title": "Performance Estimation for Supervised Medical Image Segmentation Models on Unlabeled Data Using UniverSeg", "categories": ["eess.IV", "cs.CV"], "comment": null, "summary": "The performance of medical image segmentation models is usually evaluated\nusing metrics like the Dice score and Hausdorff distance, which compare\npredicted masks to ground truth annotations. However, when applying the model\nto unseen data, such as in clinical settings, it is often impractical to\nannotate all the data, making the model's performance uncertain. To address\nthis challenge, we propose the Segmentation Performance Evaluator (SPE), a\nframework for estimating segmentation models' performance on unlabeled data.\nThis framework is adaptable to various evaluation metrics and model\narchitectures. Experiments on six publicly available datasets across six\nevaluation metrics including pixel-based metrics such as Dice score and\ndistance-based metrics like HD95, demonstrated the versatility and\neffectiveness of our approach, achieving a high correlation (0.956$\\pm$0.046)\nand low MAE (0.025$\\pm$0.019) compare with real Dice score on the independent\ntest set. These results highlight its ability to reliably estimate model\nperformance without requiring annotations. The SPE framework integrates\nseamlessly into any model training process without adding training overhead,\nenabling performance estimation and facilitating the real-world application of\nmedical image segmentation algorithms. The source code is publicly available"}
{"id": "2504.15548", "pdf": "https://arxiv.org/pdf/2504.15548", "abs": "https://arxiv.org/abs/2504.15548", "authors": ["Elyas Meguellati", "Assaad Zeghina", "Shazia Sadiq", "Gianluca Demartini"], "title": "LLM-based Semantic Augmentation for Harmful Content Detection", "categories": ["cs.CL", "cs.CY"], "comment": null, "summary": "Recent advances in large language models (LLMs) have demonstrated strong\nperformance on simple text classification tasks, frequently under zero-shot\nsettings. However, their efficacy declines when tackling complex social media\nchallenges such as propaganda detection, hateful meme classification, and\ntoxicity identification. Much of the existing work has focused on using LLMs to\ngenerate synthetic training data, overlooking the potential of LLM-based text\npreprocessing and semantic augmentation. In this paper, we introduce an\napproach that prompts LLMs to clean noisy text and provide context-rich\nexplanations, thereby enhancing training sets without substantial increases in\ndata volume. We systematically evaluate on the SemEval 2024 multi-label\nPersuasive Meme dataset and further validate on the Google Jigsaw toxic\ncomments and Facebook hateful memes datasets to assess generalizability. Our\nresults reveal that zero-shot LLM classification underperforms on these\nhigh-context tasks compared to supervised models. In contrast, integrating\nLLM-based semantic augmentation yields performance on par with approaches that\nrely on human-annotated data, at a fraction of the cost. These findings\nunderscore the importance of strategically incorporating LLMs into machine\nlearning (ML) pipeline for social media classification tasks, offering broad\nimplications for combating harmful content online."}
{"id": "2504.13920", "pdf": "https://arxiv.org/pdf/2504.13920", "abs": "https://arxiv.org/abs/2504.13920", "authors": ["Martina Vanelli", "Giacomo Como", "Fabio Fagnani"], "title": "How competitive are pay-as-bid auction games?", "categories": ["math.OC", "cs.GT", "cs.MA", "cs.SY", "eess.SY"], "comment": null, "summary": "We study the pay-as-bid auction game, a supply function model with\ndiscriminatory pricing and asymmetric firms. In this game, strategies are\nnon-decreasing supply functions relating pric to quantity and the exact choice\nof the strategy space turns out to be a crucial issue: when it includes all\nnon-decreasing continuous functions, pure-strategy Nash equilibria often fail\nto exist. To overcome this, we restrict the strategy space to the set of\nLipschitz-continuous functions and we prove that Nash equilibria always exist\n(under standard concavity assumptions) and consist of functions that are affine\non their own support and have slope equal to the maximum allowed Lipschitz\nconstant. We further show that the Nash equilibrium is unique up to the\nmarket-clearing price when the demand is affine and the asymmetric marginal\nproduction costs are homogeneous in zero. For quadratic production costs, we\nderive a closed-form expression and we compute the limit as the allowed\nLipschitz constant grows to infinity. Our results show that in the limit the\npay-as-bid auction game achieves perfect competition with efficient allocation\nand induces a lower market-clearing price compared to supply function models\nbased on uniform price auctions."}
{"id": "2504.04060", "pdf": "https://arxiv.org/pdf/2504.04060", "abs": "https://arxiv.org/abs/2504.04060", "authors": ["Yuhao Wang", "Heyang Liu", "Ziyang Cheng", "Ronghua Wu", "Qunshan Gu", "Yanfeng Wang", "Yu Wang"], "title": "VocalNet: Speech LLM with Multi-Token Prediction for Faster and High-Quality Generation", "categories": ["cs.CL", "cs.AI", "cs.SD", "eess.AS"], "comment": null, "summary": "Speech large language models (LLMs) have emerged as a prominent research\nfocus in speech processing. We introduce VocalNet-1B and VocalNet-8B, a series\nof high-performance, low-latency speech LLMs enabled by a scalable and\nmodel-agnostic training framework designed for real-time voice interaction.\nCentral to our contribution is the first application of multi-token prediction\n(MTP) to speech LLMs. This approach represents a paradigm shift from standard\nnext-token prediction (NTP), offering simultaneous improvements in generation\nspeed and quality. Informed by analysis of MTP's effect on speech generation\nand experimental comparisons, we designed a straightforward and highly\neffective MTP implementation. Experiments demonstrate that VocalNet performs on\npar with mainstream Omni LLMs even with limited training data, and\nsignificantly surpasses existing open-source speech LLMs. To foster\nreproducibility and community advancement, all model weights, inference code,\ntraining data, and framework implementations have been made publicly available\nat https://github.com/SJTU-OmniAgent/VocalNet"}
{"id": "2504.15214", "pdf": "https://arxiv.org/pdf/2504.15214", "abs": "https://arxiv.org/abs/2504.15214", "authors": ["Amirmohammad Mohammadi", "Davelle Carreiro", "Alexandra Van Dine", "Joshua Peeples"], "title": "Histogram-based Parameter-efficient Tuning for Passive Sonar Classification", "categories": ["cs.LG", "cs.SD"], "comment": "5 pages, 4 figures. Under Review", "summary": "Parameter-efficient transfer learning (PETL) methods adapt large artificial\nneural networks to downstream tasks without fine-tuning the entire model.\nHowever, existing additive methods, such as adapters, sometimes struggle to\ncapture distributional shifts in intermediate feature embeddings. We propose a\nnovel histogram-based parameter-efficient tuning (HPT) technique that captures\nthe statistics of the target domain and modulates the embeddings. Experimental\nresults on three downstream passive sonar datasets (ShipsEar, DeepShip, VTUAD)\ndemonstrate that HPT outperforms conventional adapters. Notably, HPT achieves\n91.8% vs. 89.8% accuracy on VTUAD. Furthermore, HPT trains faster and yields\nfeature representations closer to those of fully fine-tuned models. Overall,\nHPT balances parameter savings and performance, providing a distribution-aware\nalternative to existing adapters and shows a promising direction for scalable\ntransfer learning in resource-constrained environments. The code is publicly\navailable:\nhttps://github.com/Advanced-Vision-and-Learning-Lab/HLAST_DeepShip_ParameterEfficient."}
{"id": "2504.15552", "pdf": "https://arxiv.org/pdf/2504.15552", "abs": "https://arxiv.org/abs/2504.15552", "authors": ["Gengxian Cao", "Fengyuan Li", "Hong Duan", "Ye Yang", "Bofeng Wang", "Donghe Li"], "title": "A Multi-Agent Framework for Automated Qinqiang Opera Script Generation Using Large Language Models", "categories": ["cs.AI"], "comment": "17 pages,7 figures,1 tables", "summary": "This paper introduces a novel multi-Agent framework that automates the end to\nend production of Qinqiang opera by integrating Large Language Models , visual\ngeneration, and Text to Speech synthesis. Three specialized agents collaborate\nin sequence: Agent1 uses an LLM to craft coherent, culturally grounded\nscripts;Agent2 employs visual generation models to render contextually accurate\nstage scenes; and Agent3 leverages TTS to produce synchronized, emotionally\nexpressive vocal performances. In a case study on Dou E Yuan, the system\nachieved expert ratings of 3.8 for script fidelity, 3.5 for visual coherence,\nand 3.8 for speech accuracy-culminating in an overall score of 3.6, a 0.3 point\nimprovement over a Single Agent baseline. Ablation experiments demonstrate that\nremoving Agent2 or Agent3 leads to drops of 0.4 and 0.5 points, respectively,\nunderscoring the value of modular collaboration. This work showcases how AI\ndriven pipelines can streamline and scale the preservation of traditional\nperforming arts, and points toward future enhancements in cross modal\nalignment, richer emotional nuance, and support for additional opera genres."}
{"id": "2504.15328", "pdf": "https://arxiv.org/pdf/2504.15328", "abs": "https://arxiv.org/abs/2504.15328", "authors": ["Usevalad Milasheuski", "Luca Barbieri", "Sanaz Kianoush", "Monica Nicoli", "Stefano Savazzi"], "title": "Bayesian Federated Learning for Continual Training", "categories": ["cs.LG", "cs.AI", "stat.ML"], "comment": null, "summary": "Bayesian Federated Learning (BFL) enables uncertainty quantification and\nrobust adaptation in distributed learning. In contrast to the frequentist\napproach, it estimates the posterior distribution of a global model, offering\ninsights into model reliability. However, current BFL methods neglect continual\nlearning challenges in dynamic environments where data distributions shift over\ntime. We propose a continual BFL framework applied to human sensing with radar\ndata collected over several days. Using Stochastic Gradient Langevin Dynamics\n(SGLD), our approach sequentially updates the model, leveraging past posteriors\nto construct the prior for the new tasks. We assess the accuracy, the expected\ncalibration error (ECE) and the convergence speed of our approach against\nseveral baselines. Results highlight the effectiveness of continual Bayesian\nupdates in preserving knowledge and adapting to evolving data."}
{"id": "2504.15404", "pdf": "https://arxiv.org/pdf/2504.15404", "abs": "https://arxiv.org/abs/2504.15404", "authors": ["Tajamul Ashraf", "Rajes Manna", "Partha Sarathi Purkayastha", "Tavaheed Tariq", "Janibul Bashir"], "title": "Context Aware Grounded Teacher for Source Free Object Detection", "categories": ["cs.CV"], "comment": null, "summary": "We focus on the Source Free Object Detection (SFOD) problem, when source data\nis unavailable during adaptation, and the model must adapt to the unlabeled\ntarget domain. In medical imaging, several approaches have leveraged a\nsemi-supervised student-teacher architecture to bridge domain discrepancy.\nContext imbalance in labeled training data and significant domain shifts\nbetween domains can lead to biased teacher models that produce inaccurate\npseudolabels, degrading the student model's performance and causing a mode\ncollapse. Class imbalance, particularly when one class significantly outnumbers\nanother, leads to contextual bias. To tackle the problem of context bias and\nthe significant performance drop of the student model in the SFOD setting, we\nintroduce Grounded Teacher (GT) as a standard framework. In this study, we\nmodel contextual relationships using a dedicated relational context module and\nleverage it to mitigate inherent biases in the model. This approach enables us\nto apply augmentations to closely related classes, across and within domains,\nenhancing the performance of underrepresented classes while keeping the effect\non dominant classes minimal. We further improve the quality of predictions by\nimplementing an expert foundational branch to supervise the student model. We\nvalidate the effectiveness of our approach in mitigating context bias under the\nSFOD setting through experiments on three medical datasets supported by\ncomprehensive ablation studies. All relevant resources, including preprocessed\ndata, trained model weights, and code, are publicly available at this\nhttps://github.com/Tajamul21/Grounded_Teacher."}
{"id": "2504.15473", "pdf": "https://arxiv.org/pdf/2504.15473", "abs": "https://arxiv.org/abs/2504.15473", "authors": ["Berk Tinaz", "Zalan Fabian", "Mahdi Soltanolkotabi"], "title": "Emergence and Evolution of Interpretable Concepts in Diffusion Models", "categories": ["cs.CV", "cs.LG", "eess.IV", "I.2.6; I.2.10"], "comment": "32 pages, 32 figures, preliminary version", "summary": "Diffusion models have become the go-to method for text-to-image generation,\nproducing high-quality images from noise through a process called reverse\ndiffusion. Understanding the dynamics of the reverse diffusion process is\ncrucial in steering the generation and achieving high sample quality. However,\nthe inner workings of diffusion models is still largely a mystery due to their\nblack-box nature and complex, multi-step generation process. Mechanistic\nInterpretability (MI) techniques, such as Sparse Autoencoders (SAEs), aim at\nuncovering the operating principles of models through granular analysis of\ntheir internal representations. These MI techniques have been successful in\nunderstanding and steering the behavior of large language models at scale.\nHowever, the great potential of SAEs has not yet been applied toward gaining\ninsight into the intricate generative process of diffusion models. In this\nwork, we leverage the SAE framework to probe the inner workings of a popular\ntext-to-image diffusion model, and uncover a variety of human-interpretable\nconcepts in its activations. Interestingly, we find that even before the first\nreverse diffusion step is completed, the final composition of the scene can be\npredicted surprisingly well by looking at the spatial distribution of activated\nconcepts. Moreover, going beyond correlational analysis, we show that the\ndiscovered concepts have a causal effect on the model output and can be\nleveraged to steer the generative process. We design intervention techniques\naimed at manipulating image composition and style, and demonstrate that (1) in\nearly stages of diffusion image composition can be effectively controlled, (2)\nin the middle stages of diffusion image composition is finalized, however\nstylistic interventions are effective, and (3) in the final stages of diffusion\nonly minor textural details are subject to change."}
{"id": "2504.15573", "pdf": "https://arxiv.org/pdf/2504.15573", "abs": "https://arxiv.org/abs/2504.15573", "authors": ["Yuxin Jiang", "Yufei Wang", "Chuhan Wu", "Xinyi Dai", "Yan Xu", "Weinan Gan", "Yasheng Wang", "Xin Jiang", "Lifeng Shang", "Ruiming Tang", "Wei Wang"], "title": "Instruction-Tuning Data Synthesis from Scratch via Web Reconstruction", "categories": ["cs.CL"], "comment": "15 pages, 11 figures, 9 tables", "summary": "The improvement of LLMs' instruction-following capabilities depends\ncritically on the availability of high-quality instruction-response pairs.\nWhile existing automatic data synthetic methods alleviate the burden of manual\ncuration, they often rely heavily on either the quality of seed data or strong\nassumptions about the structure and content of web documents. To tackle these\nchallenges, we propose Web Reconstruction (WebR), a fully automated framework\nfor synthesizing high-quality instruction-tuning (IT) data directly from raw\nweb documents with minimal assumptions. Leveraging the inherent diversity of\nraw web content, we conceptualize web reconstruction as an instruction-tuning\ndata synthesis task via a novel dual-perspective paradigm--Web as Instruction\nand Web as Response--where each web document is designated as either an\ninstruction or a response to trigger the reconstruction process. Comprehensive\nexperiments show that datasets generated by WebR outperform state-of-the-art\nbaselines by up to 16.65% across four instruction-following benchmarks.\nNotably, WebR demonstrates superior compatibility, data efficiency, and\nscalability, enabling enhanced domain adaptation with minimal effort. The data\nand code are publicly available at https://github.com/YJiangcm/WebR."}
{"id": "2504.15610", "pdf": "https://arxiv.org/pdf/2504.15610", "abs": "https://arxiv.org/abs/2504.15610", "authors": ["Md Millat", "Md Motiur"], "title": "A LoRA-Based Approach to Fine-Tuning LLMs for Educational Guidance in Resource-Constrained Settings", "categories": ["cs.AI", "68T05 (Learning and adaptive systems), 68T07 (Artificial\n  intelligence and education)"], "comment": "18 pages, 6 figures (3 graphs + 3 flowchart/architecture diagrams),\n  submitted as a preprint for review consideration in AI for Education or\n  Machine Learning applications in low-resource settings. Includes detailed\n  experiments with LoRA and quantization methods for efficient LLM fine-tuning", "summary": "The current study describes a cost-effective method for adapting large\nlanguage models (LLMs) for academic advising with study-abroad contexts in mind\nand for application in low-resource methods for acculturation. With the\nMistral-7B-Instruct model applied with a Low-Rank Adaptation (LoRA) method and\na 4-bit quantization method, the model underwent training in two distinct\nstages related to this study's purpose to enhance domain specificity while\nmaintaining computational efficiency. In Phase 1, the model was conditioned\nwith a synthetic dataset via the Gemini Pro API, and in Phase 2, it was trained\nwith manually curated datasets from the StudyAbroadGPT project to achieve\nenhanced, contextualized responses. Technical innovations entailed\nmemory-efficient quantization, parameter-efficient adaptation, and continuous\ntraining analytics via Weights & Biases. After training, this study\ndemonstrated a reduction in training loss by 52.7%, 92% accuracy in\ndomain-specific recommendations, achieved 95% markdown-based formatting\nsupport, and a median run-rate of 100 samples per second on off-the-shelf GPU\nequipment. These findings support the effective application of\ninstruction-tuned LLMs within educational advisers, especially in low-resource\ninstitutional scenarios. Limitations included decreased generalizability and\nthe application of a synthetically generated dataset, but this framework is\nscalable for adding new multilingual-augmented and real-time academic advising\nprocesses. Future directions may include plans for the integration of\nretrieval-augmented generation, applying dynamic quantization routines, and\nconnecting to real-time academic databases to increase adaptability and\naccuracy."}
{"id": "2504.15366", "pdf": "https://arxiv.org/pdf/2504.15366", "abs": "https://arxiv.org/abs/2504.15366", "authors": ["Qifan Yan", "Andrew Liu", "Shiqi He", "Mathias Lécuyer", "Ivan Beschastnikh"], "title": "FedFetch: Faster Federated Learning with Adaptive Downstream Prefetching", "categories": ["cs.LG", "cs.DC"], "comment": "Accepted at INFOCOM 2025", "summary": "Federated learning (FL) is a machine learning paradigm that facilitates\nmassively distributed model training with end-user data on edge devices\ndirected by a central server. However, the large number of heterogeneous\nclients in FL deployments leads to a communication bottleneck between the\nserver and the clients. This bottleneck is made worse by straggling clients,\nany one of which will further slow down training. To tackle these challenges,\nresearchers have proposed techniques like client sampling and update\ncompression. These techniques work well in isolation but combine poorly in the\ndownstream, server-to-client direction. This is because unselected clients have\noutdated local model states and need to synchronize these states with the\nserver first.\n  We introduce FedFetch, a strategy to mitigate the download time overhead\ncaused by combining client sampling and compression techniques. FedFetch\nachieves this with an efficient prefetch schedule for clients to prefetch model\nstates multiple rounds before a stated training round. We empirically show that\nadding FedFetch to communication efficient FL techniques reduces end-to-end\ntraining time by 1.26$\\times$ and download time by 4.49$\\times$ across\ncompression techniques with heterogeneous client settings. Our implementation\nis available at https://github.com/DistributedML/FedFetch"}
{"id": "2504.15415", "pdf": "https://arxiv.org/pdf/2504.15415", "abs": "https://arxiv.org/abs/2504.15415", "authors": ["David Ma", "Yuanxing Zhang", "Jincheng Ren", "Jarvis Guo", "Yifan Yao", "Zhenlin Wei", "Zhenzhu Yang", "Zhongyuan Peng", "Boyu Feng", "Jun Ma", "Xiao Gu", "Zhoufutu Wen", "King Zhu", "Yancheng He", "Meng Cao", "Shiwen Ni", "Jiaheng Liu", "Wenhao Huang", "Ge Zhang", "Xiaojie Jin"], "title": "IV-Bench: A Benchmark for Image-Grounded Video Perception and Reasoning in Multimodal LLMs", "categories": ["cs.CV", "cs.CL"], "comment": null, "summary": "Existing evaluation frameworks for Multimodal Large Language Models (MLLMs)\nprimarily focus on image reasoning or general video understanding tasks,\nlargely overlooking the significant role of image context in video\ncomprehension. To bridge this gap, we propose IV-Bench, the first comprehensive\nbenchmark for evaluating Image-Grounded Video Perception and Reasoning.\nIV-Bench consists of 967 videos paired with 2,585 meticulously annotated\nimage-text queries across 13 tasks (7 perception and 6 reasoning tasks) and 5\nrepresentative categories. Extensive evaluations of state-of-the-art\nopen-source (e.g., InternVL2.5, Qwen2.5-VL) and closed-source (e.g., GPT-4o,\nGemini2-Flash and Gemini2-Pro) MLLMs demonstrate that current models\nsubstantially underperform in image-grounded video Perception and Reasoning,\nmerely achieving at most 28.9% accuracy. Further analysis reveals key factors\ninfluencing model performance on IV-Bench, including inference pattern, frame\nnumber, and resolution. Additionally, through a simple data synthesis approach,\nwe demonstratethe challenges of IV- Bench extend beyond merely aligning the\ndata format in the training proecss. These findings collectively provide\nvaluable insights for future research. Our codes and data are released in\nhttps://github.com/multimodal-art-projection/IV-Bench."}
{"id": "2504.15496", "pdf": "https://arxiv.org/pdf/2504.15496", "abs": "https://arxiv.org/abs/2504.15496", "authors": ["Eammon A. Littler", "Emmanuel A. Mannoh", "Ethan P. M. LaRochelle"], "title": "Fluorescence Reference Target Quantitative Analysis Library", "categories": ["physics.med-ph", "cs.CV", "eess.IV", "q-bio.QM"], "comment": "12 pages, 1 table, 4 figures. Code available:\n  https://github.com/QUEL-Imaging/quel-qal), PyPi: quel-qal", "summary": "Standardized performance evaluation of fluorescence imaging systems remains a\ncritical unmet need in the field of fluorescence-guided surgery (FGS). While\nthe American Association of Physicists in Medicine (AAPM) TG311 report and\nrecent FDA draft guidance provide recommended metrics for system\ncharacterization, practical tools for extracting these metrics remain limited,\ninconsistent, and often inaccessible. We present QUEL-QAL, an open-source\nPython library designed to streamline and standardize the quantitative analysis\nof fluorescence images using solid reference targets. The library provides a\nmodular, reproducible workflow that includes region of interest (ROI)\ndetection, statistical analysis, and visualization capabilities. QUEL-QAL\nsupports key metrics such as response linearity, limit of detection, depth\nsensitivity, and spatial resolution, in alignment with regulatory and academic\nguidance. Built on widely adopted Python packages, the library is designed to\nbe extensible, enabling users to adapt it to novel target designs and analysis\nprotocols. By promoting transparency, reproducibility, and regulatory\nalignment, QUEL-QAL offers a foundational tool to support standardized\nbenchmarking and accelerate the development and evaluation of fluorescence\nimaging systems."}
{"id": "2504.15604", "pdf": "https://arxiv.org/pdf/2504.15604", "abs": "https://arxiv.org/abs/2504.15604", "authors": ["Pavan Yadav", "Nikhil Khandalkar", "Krishna Shinde", "Lokesh B. Ramegowda", "Rajarshi Das"], "title": "Exploring Next Token Prediction in Theory of Mind (ToM) Tasks: Comparative Experiments with GPT-2 and LLaMA-2 AI Models", "categories": ["cs.CL", "cs.AI"], "comment": "75 pages, 60 figures", "summary": "Language models have made significant progress in generating coherent text\nand predicting next tokens based on input prompts. This study compares the\nnext-token prediction performance of two well-known models: OpenAI's GPT-2 and\nMeta's Llama-2-7b-chat-hf on Theory of Mind (ToM) tasks. To evaluate their\ncapabilities, we built a dataset from 10 short stories sourced from the Explore\nToM Dataset. We enhanced these stories by programmatically inserting additional\nsentences (infills) using GPT-4, creating variations that introduce different\nlevels of contextual complexity. This setup enables analysis of how increasing\ncontext affects model performance. We tested both models under four temperature\nsettings (0.01, 0.5, 1.0, 2.0) and evaluated their ability to predict the next\ntoken across three reasoning levels. Zero-order reasoning involves tracking the\nstate, either current (ground truth) or past (memory). First-order reasoning\nconcerns understanding another's mental state (e.g., \"Does Anne know the apple\nis salted?\"). Second-order reasoning adds recursion (e.g., \"Does Anne think\nthat Charles knows the apple is salted?\").\n  Our results show that adding more infill sentences slightly reduces\nprediction accuracy, as added context increases complexity and ambiguity.\nLlama-2 consistently outperforms GPT-2 in prediction accuracy, especially at\nlower temperatures, demonstrating greater confidence in selecting the most\nprobable token. As reasoning complexity rises, model responses diverge more.\nNotably, GPT-2 and Llama-2 display greater variability in predictions during\nfirst- and second-order reasoning tasks. These findings illustrate how model\narchitecture, temperature, and contextual complexity influence next-token\nprediction, contributing to a better understanding of the strengths and\nlimitations of current language models."}
{"id": "2504.15668", "pdf": "https://arxiv.org/pdf/2504.15668", "abs": "https://arxiv.org/abs/2504.15668", "authors": ["Mir Md Sajid Sarwar", "Rajarshi Ray"], "title": "Exploring Inevitable Waypoints for Unsolvability Explanation in Hybrid Planning Problems", "categories": ["cs.AI", "cs.FL", "I.2.0; F.4.3"], "comment": null, "summary": "Explaining unsolvability of planning problems is of significant research\ninterest in Explainable AI Planning. AI planning literature has reported\nseveral research efforts on generating explanations of solutions to planning\nproblems. However, explaining the unsolvability of planning problems remains a\nlargely open and understudied problem. A widely practiced approach to plan\ngeneration and automated problem solving, in general, is to decompose tasks\ninto sub-problems that help progressively converge towards the goal. In this\npaper, we propose to adopt the same philosophy of sub-problem identification as\na mechanism for analyzing and explaining unsolvability of planning problems in\nhybrid systems. In particular, for a given unsolvable planning problem, we\npropose to identify common waypoints, which are universal obstacles to plan\nexistence; in other words, they appear on every plan from the source to the\nplanning goal. This work envisions such waypoints as sub-problems of the\nplanning problem and the unreachability of any of these waypoints as an\nexplanation for the unsolvability of the original planning problem. We propose\na novel method of waypoint identification by casting the problem as an instance\nof the longest common subsequence problem, a widely popular problem in computer\nscience, typically considered as an illustrative example for the dynamic\nprogramming paradigm. Once the waypoints are identified, we perform symbolic\nreachability analysis on them to identify the earliest unreachable waypoint and\nreport it as the explanation of unsolvability. We present experimental results\non unsolvable planning problems in hybrid domains."}
{"id": "2504.15369", "pdf": "https://arxiv.org/pdf/2504.15369", "abs": "https://arxiv.org/abs/2504.15369", "authors": ["Calvin Luo", "Zilai Zeng", "Yilun Du", "Chen Sun"], "title": "Solving New Tasks by Adapting Internet Video Knowledge", "categories": ["cs.LG", "cs.AI", "cs.RO"], "comment": "ICLR 2025. Project Webpage:\n  https://diffusion-supervision.github.io/adapt2act/", "summary": "Video generative models demonstrate great promise in robotics by serving as\nvisual planners or as policy supervisors. When pretrained on internet-scale\ndata, such video models intimately understand alignment with natural language,\nand can thus facilitate generalization to novel downstream behavior through\ntext-conditioning. However, they may not be sensitive to the specificities of\nthe particular environment the agent inhabits. On the other hand, training\nvideo models on in-domain examples of robotic behavior naturally encodes\nenvironment-specific intricacies, but the scale of available demonstrations may\nnot be sufficient to support generalization to unseen tasks via natural\nlanguage specification. In this work, we investigate different adaptation\ntechniques that integrate in-domain information with large-scale pretrained\nvideo models, and explore the extent to which they enable novel\ntext-conditioned generalization for robotic tasks, while also considering their\nindependent data and resource considerations. We successfully demonstrate\nacross robotic environments that adapting powerful video models with small\nscales of example data can successfully facilitate generalization to novel\nbehaviors. In particular, we present a novel adaptation strategy, termed\nInverse Probabilistic Adaptation, that not only consistently achieves strong\ngeneralization performance across robotic tasks and settings, but also exhibits\nrobustness to the quality of adaptation data, successfully solving novel tasks\neven when only suboptimal in-domain demonstrations are available."}
{"id": "2504.15470", "pdf": "https://arxiv.org/pdf/2504.15470", "abs": "https://arxiv.org/abs/2504.15470", "authors": ["Jonathan Brokman", "Amit Giloni", "Omer Hofman", "Roman Vainshtein", "Hisashi Kojima", "Guy Gilboa"], "title": "Manifold Induced Biases for Zero-shot and Few-shot Detection of Generated Images", "categories": ["cs.CV"], "comment": "Accepted to ICLR 2025 (The International Conference on Learning\n  Representations)", "summary": "Distinguishing between real and AI-generated images, commonly referred to as\n'image detection', presents a timely and significant challenge. Despite\nextensive research in the (semi-)supervised regime, zero-shot and few-shot\nsolutions have only recently emerged as promising alternatives. Their main\nadvantage is in alleviating the ongoing data maintenance, which quickly becomes\noutdated due to advances in generative technologies. We identify two main gaps:\n(1) a lack of theoretical grounding for the methods, and (2) significant room\nfor performance improvements in zero-shot and few-shot regimes. Our approach is\nfounded on understanding and quantifying the biases inherent in generated\ncontent, where we use these quantities as criteria for characterizing generated\nimages. Specifically, we explore the biases of the implicit probability\nmanifold, captured by a pre-trained diffusion model. Through score-function\nanalysis, we approximate the curvature, gradient, and bias towards points on\nthe probability manifold, establishing criteria for detection in the zero-shot\nregime. We further extend our contribution to the few-shot setting by employing\na mixture-of-experts methodology. Empirical results across 20 generative models\ndemonstrate that our method outperforms current approaches in both zero-shot\nand few-shot settings. This work advances the theoretical understanding and\npractical usage of generated content biases through the lens of manifold\nanalysis."}
{"id": "2504.15756", "pdf": "https://arxiv.org/pdf/2504.15756", "abs": "https://arxiv.org/abs/2504.15756", "authors": ["Qirui Yang", "Fangpu Zhang", "Yeying Jin", "Qihua Cheng", "Pengtao Jiang", "Huanjing Yue", "Jingyu Yang"], "title": "DSDNet: Raw Domain Demoiréing via Dual Color-Space Synergy", "categories": ["cs.CV", "eess.IV"], "comment": null, "summary": "With the rapid advancement of mobile imaging, capturing screens using\nsmartphones has become a prevalent practice in distance learning and conference\nrecording. However, moir\\'e artifacts, caused by frequency aliasing between\ndisplay screens and camera sensors, are further amplified by the image signal\nprocessing pipeline, leading to severe visual degradation. Existing sRGB domain\ndemoir\\'eing methods struggle with irreversible information loss, while recent\ntwo-stage raw domain approaches suffer from information bottlenecks and\ninference inefficiency. To address these limitations, we propose a single-stage\nraw domain demoir\\'eing framework, Dual-Stream Demoir\\'eing Network (DSDNet),\nwhich leverages the synergy of raw and YCbCr images to remove moir\\'e while\npreserving luminance and color fidelity. Specifically, to guide luminance\ncorrection and moir\\'e removal, we design a raw-to-YCbCr mapping pipeline and\nintroduce the Synergic Attention with Dynamic Modulation (SADM) module. This\nmodule enriches the raw-to-sRGB conversion with cross-domain contextual\nfeatures. Furthermore, to better guide color fidelity, we develop a\nLuminance-Chrominance Adaptive Transformer (LCAT), which decouples luminance\nand chrominance representations. Extensive experiments demonstrate that DSDNet\noutperforms state-of-the-art methods in both visual quality and quantitative\nevaluation, and achieves an inference speed $\\mathrm{\\textbf{2.4x}}$ faster\nthan the second-best method, highlighting its practical advantages. We provide\nan anonymous online demo at https://xxxxxxxxdsdnet.github.io/DSDNet/."}
{"id": "2504.15630", "pdf": "https://arxiv.org/pdf/2504.15630", "abs": "https://arxiv.org/abs/2504.15630", "authors": ["Xiaowei Yuan", "Zhao Yang", "Ziyang Huang", "Yequan Wang", "Siqi Fan", "Yiming Ju", "Jun Zhao", "Kang Liu"], "title": "Exploiting Contextual Knowledge in LLMs through V-usable Information based Layer Enhancement", "categories": ["cs.CL"], "comment": null, "summary": "Large Language Models (LLMs) have demonstrated remarkable capabilities in\nvarious tasks, yet they often struggle with context-faithfulness generations\nthat properly reflect contextual knowledge. While existing approaches focus on\nenhancing the decoding strategies, they ignore the fundamental mechanism of how\ncontextual information is processed within LLMs' internal states. As a result,\nLLMs remain limited in their ability to fully leverage contextual knowledge. In\nthis paper, we propose Context-aware Layer Enhancement (CaLE), a novel\nintervention method that enhances the utilization of contextual knowledge\nwithin LLMs' internal representations. By employing V-usable information\nanalysis, CaLE strategically amplifies the growth of contextual information at\nan optimal layer, thereby enriching representations in the final layer. Our\nexperiments demonstrate that CaLE effectively improves context-faithful\ngeneration in Question-Answering tasks, particularly in scenarios involving\nunknown or conflicting contextual knowledge."}
{"id": "2504.15699", "pdf": "https://arxiv.org/pdf/2504.15699", "abs": "https://arxiv.org/abs/2504.15699", "authors": ["Ning Wang", "Zihan Yan", "Weiyang Li", "Chuan Ma", "He Chen", "Tao Xiang"], "title": "Advancing Embodied Agent Security: From Safety Benchmarks to Input Moderation", "categories": ["cs.AI"], "comment": "9 pages", "summary": "Embodied agents exhibit immense potential across a multitude of domains,\nmaking the assurance of their behavioral safety a fundamental prerequisite for\ntheir widespread deployment. However, existing research predominantly\nconcentrates on the security of general large language models, lacking\nspecialized methodologies for establishing safety benchmarks and input\nmoderation tailored to embodied agents. To bridge this gap, this paper\nintroduces a novel input moderation framework, meticulously designed to\nsafeguard embodied agents. This framework encompasses the entire pipeline,\nincluding taxonomy definition, dataset curation, moderator architecture, model\ntraining, and rigorous evaluation. Notably, we introduce EAsafetyBench, a\nmeticulously crafted safety benchmark engineered to facilitate both the\ntraining and stringent assessment of moderators specifically designed for\nembodied agents. Furthermore, we propose Pinpoint, an innovative\nprompt-decoupled input moderation scheme that harnesses a masked attention\nmechanism to effectively isolate and mitigate the influence of functional\nprompts on moderation tasks. Extensive experiments conducted on diverse\nbenchmark datasets and models validate the feasibility and efficacy of the\nproposed approach. The results demonstrate that our methodologies achieve an\nimpressive average detection accuracy of 94.58%, surpassing the performance of\nexisting state-of-the-art techniques, alongside an exceptional moderation\nprocessing time of merely 0.002 seconds per instance."}
{"id": "2504.15399", "pdf": "https://arxiv.org/pdf/2504.15399", "abs": "https://arxiv.org/abs/2504.15399", "authors": ["Guy Zamir", "Aryan Dokania", "Bo Zhao", "Rose Yu"], "title": "Improving Learning to Optimize Using Parameter Symmetries", "categories": ["cs.LG"], "comment": "Accepted at the ICLR Workshop on Neural Network Weights as a New Data\n  Modality 2025", "summary": "We analyze a learning-to-optimize (L2O) algorithm that exploits parameter\nspace symmetry to enhance optimization efficiency. Prior work has shown that\njointly learning symmetry transformations and local updates improves\nmeta-optimizer performance. Supporting this, our theoretical analysis\ndemonstrates that even without identifying the optimal group element, the\nmethod locally resembles Newton's method. We further provide an example where\nthe algorithm provably learns the correct symmetry transformation during\ntraining. To empirically evaluate L2O with teleportation, we introduce a\nbenchmark, analyze its success and failure cases, and show that enhancements\nlike momentum further improve performance. Our results highlight the potential\nof leveraging neural network parameter space symmetry to advance\nmeta-optimization."}
{"id": "2504.15485", "pdf": "https://arxiv.org/pdf/2504.15485", "abs": "https://arxiv.org/abs/2504.15485", "authors": ["Atin Pothiraj", "Elias Stengel-Eskin", "Jaemin Cho", "Mohit Bansal"], "title": "CAPTURe: Evaluating Spatial Reasoning in Vision Language Models via Occluded Object Counting", "categories": ["cs.CV", "cs.AI", "cs.CL"], "comment": "Code and data: https://github.com/atinpothiraj/CAPTURe", "summary": "Recognizing and reasoning about occluded (partially or fully hidden) objects\nis vital to understanding visual scenes, as occlusions frequently occur in\nreal-world environments and act as obstacles for spatial comprehension. To test\nmodels' ability to reason about multiple occluded objects, we introduce a novel\ntask, Counting Amodally for Patterns Through Unseen REgions (CAPTURe), which\nrequires a model to count objects arranged in a pattern by inferring how the\npattern continues behind an occluder (an object which blocks parts of the\nscene). CAPTURe requires both recognizing visual patterns and reasoning, making\nit a useful testbed for evaluating vision-language models (VLMs) on whether\nthey understand occluded patterns and possess spatial understanding skills. By\nrequiring models to reason about occluded objects, CAPTURe also tests VLMs'\nability to form world models that would allow them to fill in missing\ninformation. CAPTURe consists of two parts: (1) CAPTURe-real, with manually\nfiltered images of real objects in patterns and (2) CAPTURe-synthetic, a\ncontrolled diagnostic with generated patterned images. We evaluate four strong\nVLMs (GPT-4o, Intern-VL2, Molmo, and Qwen2-VL) on CAPTURe, finding that models\nstruggle to count on both occluded and unoccluded patterns. Crucially, we find\nthat models perform worse with occlusion, suggesting that VLMs are also\ndeficient in inferring unseen spatial relationships: even the strongest VLMs\nlike GPT-4o fail to count with occlusion. In contrast, we find that humans\nachieve very little error on CAPTURe. We also find that providing auxiliary\ninformation of occluded object locations increases performance, underscoring\nthat the model error comes both from an inability to handle occlusion as well\nas difficulty counting in images."}
{"id": "2312.15676", "pdf": "https://arxiv.org/pdf/2312.15676", "abs": "https://arxiv.org/abs/2312.15676", "authors": ["Yingtai Li", "Xueming Fu", "Han Li", "Shang Zhao", "Ruiyang Jin", "S. Kevin Zhou"], "title": "3DGR-CT: Sparse-View CT Reconstruction with a 3D Gaussian Representation", "categories": ["eess.IV", "cs.CV"], "comment": null, "summary": "Sparse-view computed tomography (CT) reduces radiation exposure by acquiring\nfewer projections, making it a valuable tool in clinical scenarios where\nlow-dose radiation is essential. However, this often results in increased noise\nand artifacts due to limited data. In this paper we propose a novel 3D Gaussian\nrepresentation (3DGR) based method for sparse-view CT reconstruction. Inspired\nby recent success in novel view synthesis driven by 3D Gaussian splatting, we\nleverage the efficiency and expressiveness of 3D Gaussian representation as an\nalternative to implicit neural representation. To unleash the potential of 3DGR\nfor CT imaging scenario, we propose two key innovations: (i) FBP-image-guided\nGuassian initialization and (ii) efficient integration with a differentiable CT\nprojector. Extensive experiments and ablations on diverse datasets demonstrate\nthe proposed 3DGR-CT consistently outperforms state-of-the-art counterpart\nmethods, achieving higher reconstruction accuracy with faster convergence.\nFurthermore, we showcase the potential of 3DGR-CT for real-time physical\nsimulation, which holds important clinical applications while challenging for\nimplicit neural representations."}
{"id": "2504.15640", "pdf": "https://arxiv.org/pdf/2504.15640", "abs": "https://arxiv.org/abs/2504.15640", "authors": ["Hongtao Wang", "Taiyan Zhang", "Renchi Yang", "Jianliang Xu"], "title": "Cost-Effective Text Clustering with Large Language Models", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Text clustering aims to automatically partition a collection of text\ndocuments into distinct clusters based on linguistic features. In the\nliterature, this task is usually framed as metric clustering based on text\nembeddings from pre-trained encoders or a graph clustering problem upon\npairwise similarities from an oracle, e.g., a large ML model. Recently, large\nlanguage models (LLMs) bring significant advancement in this field by offering\ncontextualized text embeddings and highly accurate similarity scores, but\nmeanwhile, present grand challenges to cope with substantial computational\nand/or financial overhead caused by numerous API-based queries or inference\ncalls to the models.\n  In response, this paper proposes TECL, a cost-effective framework that taps\ninto the feedback from LLMs for accurate text clustering within a limited\nbudget of queries to LLMs. Under the hood, TECL adopts our EdgeLLM or\nTriangleLLM to construct must-link/cannot-link constraints for text pairs, and\nfurther leverages such constraints as supervision signals input to our weighted\nconstrained clustering approach to generate clusters. Particularly, EdgeLLM\n(resp. TriangleLLM) enables the identification of informative text pairs (resp.\ntriplets) for querying LLMs via well-thought-out greedy algorithms and accurate\nextraction of pairwise constraints through carefully-crafted prompting\ntechniques. Our experiments on multiple benchmark datasets exhibit that TECL\nconsistently and considerably outperforms existing solutions in unsupervised\ntext clustering under the same query cost for LLMs."}
{"id": "2504.15716", "pdf": "https://arxiv.org/pdf/2504.15716", "abs": "https://arxiv.org/abs/2504.15716", "authors": ["Jie Zhu", "Qian Chen", "Huaixia Dou", "Junhui Li", "Lifan Guo", "Feng Chen", "Chi Zhang"], "title": "DianJin-R1: Evaluating and Enhancing Financial Reasoning in Large Language Models", "categories": ["cs.AI"], "comment": null, "summary": "Effective reasoning remains a core challenge for large language models (LLMs)\nin the financial domain, where tasks often require domain-specific knowledge,\nprecise numerical calculations, and strict adherence to compliance rules. We\npropose DianJin-R1, a reasoning-enhanced framework designed to address these\nchallenges through reasoning-augmented supervision and reinforcement learning.\nCentral to our approach is DianJin-R1-Data, a high-quality dataset constructed\nfrom CFLUE, FinQA, and a proprietary compliance corpus (Chinese Compliance\nCheck, CCC), combining diverse financial reasoning scenarios with verified\nannotations. Our models, DianJin-R1-7B and DianJin-R1-32B, are fine-tuned from\nQwen2.5-7B-Instruct and Qwen2.5-32B-Instruct using a structured format that\ngenerates both reasoning steps and final answers. To further refine reasoning\nquality, we apply Group Relative Policy Optimization (GRPO), a reinforcement\nlearning method that incorporates dual reward signals: one encouraging\nstructured outputs and another rewarding answer correctness. We evaluate our\nmodels on five benchmarks: three financial datasets (CFLUE, FinQA, and CCC) and\ntwo general reasoning benchmarks (MATH-500 and GPQA-Diamond). Experimental\nresults show that DianJin-R1 models consistently outperform their non-reasoning\ncounterparts, especially on complex financial tasks. Moreover, on the\nreal-world CCC dataset, our single-call reasoning models match or even surpass\nthe performance of multi-agent systems that require significantly more\ncomputational cost. These findings demonstrate the effectiveness of DianJin-R1\nin enhancing financial reasoning through structured supervision and\nreward-aligned learning, offering a scalable and practical solution for\nreal-world applications."}
{"id": "2504.15439", "pdf": "https://arxiv.org/pdf/2504.15439", "abs": "https://arxiv.org/abs/2504.15439", "authors": ["Hao Zhuo", "Yicheng Yang", "Kewen Peng"], "title": "Combating Toxic Language: A Review of LLM-Based Strategies for Software Engineering", "categories": ["cs.LG", "cs.SE"], "comment": null, "summary": "Large Language Models (LLMs) have become integral to software engineering\n(SE), where they are increasingly used in development workflows. However, their\nwidespread use raises concerns about the presence and propagation of toxic\nlanguage--harmful or offensive content that can foster exclusionary\nenvironments. This paper provides a comprehensive review of recent research on\ntoxicity detection and mitigation, focusing on both SE-specific and\ngeneral-purpose datasets. We examine annotation and preprocessing techniques,\nassess detection methodologies, and evaluate mitigation strategies,\nparticularly those leveraging LLMs. Additionally, we conduct an ablation study\ndemonstrating the effectiveness of LLM-based rewriting for reducing toxicity.\nBy synthesizing existing work and identifying open challenges, this review\nhighlights key areas for future research to ensure the responsible deployment\nof LLMs in SE and beyond."}
{"id": "2504.15513", "pdf": "https://arxiv.org/pdf/2504.15513", "abs": "https://arxiv.org/abs/2504.15513", "authors": ["Yixuan Zhu", "Haolin Wang", "Ao Li", "Wenliang Zhao", "Yansong Tang", "Jingxuan Niu", "Lei Chen", "Jie Zhou", "Jiwen Lu"], "title": "InstaRevive: One-Step Image Enhancement via Dynamic Score Matching", "categories": ["cs.CV"], "comment": "Accepted by ICLR 2025", "summary": "Image enhancement finds wide-ranging applications in real-world scenarios due\nto complex environments and the inherent limitations of imaging devices. Recent\ndiffusion-based methods yield promising outcomes but necessitate prolonged and\ncomputationally intensive iterative sampling. In response, we propose\nInstaRevive, a straightforward yet powerful image enhancement framework that\nemploys score-based diffusion distillation to harness potent generative\ncapability and minimize the sampling steps. To fully exploit the potential of\nthe pre-trained diffusion model, we devise a practical and effective diffusion\ndistillation pipeline using dynamic control to address inaccuracies in updating\ndirection during score matching. Our control strategy enables a dynamic\ndiffusing scope, facilitating precise learning of denoising trajectories within\nthe diffusion model and ensuring accurate distribution matching gradients\nduring training. Additionally, to enrich guidance for the generative power, we\nincorporate textual prompts via image captioning as auxiliary conditions,\nfostering further exploration of the diffusion model. Extensive experiments\nsubstantiate the efficacy of our framework across a diverse array of\nchallenging tasks and datasets, unveiling the compelling efficacy and\nefficiency of InstaRevive in delivering high-quality and visually appealing\nresults. Code is available at https://github.com/EternalEvan/InstaRevive."}
{"id": "2412.04802", "pdf": "https://arxiv.org/pdf/2412.04802", "abs": "https://arxiv.org/abs/2412.04802", "authors": ["Songcheng Du", "Yang Zou", "Zixu Wang", "Xingyuan Li", "Ying Li", "Changjing Shang", "Qiang Shen"], "title": "Unsupervised Hyperspectral and Multispectral Image Fusion via Self-Supervised Modality Decoupling", "categories": ["eess.IV", "cs.CV"], "comment": null, "summary": "Hyperspectral and Multispectral Image Fusion (HMIF) aims to fuse\nlow-resolution hyperspectral images (LR-HSIs) and high-resolution multispectral\nimages (HR-MSIs) to reconstruct high spatial and high spectral resolution\nimages. Current methods typically apply direct fusion from the two modalities\nwithout effective supervision, leading to an incomplete perception of deep\nmodality-complementary information and a limited understanding of\ninter-modality correlations. To address these issues, we propose a simple yet\neffective solution for unsupervised HMIF, revealing that modality decoupling is\nkey to improving fusion performance. Specifically, we propose an end-to-end\nself-supervised \\textbf{Mo}dality-Decoupled \\textbf{S}patial-\\textbf{S}pectral\nFusion (\\textbf{MossFuse}) framework that decouples shared and complementary\ninformation across modalities and aggregates a concise representation of both\nLR-HSIs and HR-MSIs to reduce modality redundancy. Also, we introduce the\nsubspace clustering loss as a clear guide to decouple modality-shared features\nfrom modality-complementary ones. Systematic experiments over multiple datasets\ndemonstrate that our simple and effective approach consistently outperforms the\nexisting HMIF methods while requiring considerably fewer parameters with\nreduced inference time. The anonymous source code is in\n\\href{https://github.com/dusongcheng/MossFuse}{MossFuse}."}
{"id": "2504.15642", "pdf": "https://arxiv.org/pdf/2504.15642", "abs": "https://arxiv.org/abs/2504.15642", "authors": ["Gerhard Jäger"], "title": "Computational Typology", "categories": ["cs.CL", "q-bio.PE"], "comment": "19 pages, s5 figure", "summary": "Typology is a subfield of linguistics that focuses on the study and\nclassification of languages based on their structural features. Unlike\ngenealogical classification, which examines the historical relationships\nbetween languages, typology seeks to understand the diversity of human\nlanguages by identifying common properties and patterns, known as universals.\nIn recent years, computational methods have played an increasingly important\nrole in typological research, enabling the analysis of large-scale linguistic\ndata and the testing of hypotheses about language structure and evolution. This\narticle provides an illustration of the benefits of computational statistical\nmodeling in typology."}
{"id": "2504.15719", "pdf": "https://arxiv.org/pdf/2504.15719", "abs": "https://arxiv.org/abs/2504.15719", "authors": ["Anna Karnysheva", "Christian Drescher", "Dietrich Klakow"], "title": "Implementing Rational Choice Functions with LLMs and Measuring their Alignment with User Preferences", "categories": ["cs.AI"], "comment": null, "summary": "As large language models (LLMs) become integral to intelligent user\ninterfaces (IUIs), their role as decision-making agents raises critical\nconcerns about alignment. Although extensive research has addressed issues such\nas factuality, bias, and toxicity, comparatively little attention has been paid\nto measuring alignment to preferences, i.e., the relative desirability of\ndifferent alternatives, a concept used in decision making, economics, and\nsocial choice theory. However, a reliable decision-making agent makes choices\nthat align well with user preferences.\n  In this paper, we generalize existing methods that exploit LLMs for ranking\nalternative outcomes by addressing alignment with the broader and more flexible\nconcept of user preferences, which includes both strict preferences and\nindifference among alternatives. To this end, we put forward design principles\nfor using LLMs to implement rational choice functions, and provide the\nnecessary tools to measure preference satisfaction. We demonstrate the\napplicability of our approach through an empirical study in a practical\napplication of an IUI in the automotive domain."}
{"id": "2504.15458", "pdf": "https://arxiv.org/pdf/2504.15458", "abs": "https://arxiv.org/abs/2504.15458", "authors": ["Brandon Le", "Dustin Keller"], "title": "Compton Form Factor Extraction using Quantum Deep Neural Networks", "categories": ["cs.LG", "nucl-th", "quant-ph"], "comment": null, "summary": "Extraction tests of Compton Form Factors are performed using pseudodata based\non experimental data from Deeply Virtual Compton Scattering experiments\nconducted at Jefferson Lab. The standard Belitsky, Kirchner, and Muller\nformalism at twist-two is employed, along with a fitting procedure designed to\nreduce model dependency similar to traditional local fits. The extraction of\nthe Compton Form Factors is performed using both Classical Deep Neural Networks\n(CDNNs) and Quantum Deep Neural Networks (QDNNs). Comparative studies reveal\nthat QDNNs outperform CDNNs for this application, demonstrating improved\npredictive accuracy and precision even for limited model complexity. The\nresults demonstrate the potential of QDNNs for future studies in which quantum\nalgorithms can be fully optimized."}
{"id": "2504.15599", "pdf": "https://arxiv.org/pdf/2504.15599", "abs": "https://arxiv.org/abs/2504.15599", "authors": ["Shichen Li", "Chenhui Shao"], "title": "Multi-Modal Fusion of In-Situ Video Data and Process Parameters for Online Forecasting of Cookie Drying Readiness", "categories": ["cs.CV", "cs.LG"], "comment": "17 pages, 12 figures", "summary": "Food drying is essential for food production, extending shelf life, and\nreducing transportation costs. Accurate real-time forecasting of drying\nreadiness is crucial for minimizing energy consumption, improving productivity,\nand ensuring product quality. However, this remains challenging due to the\ndynamic nature of drying, limited data availability, and the lack of effective\npredictive analytical methods. To address this gap, we propose an end-to-end\nmulti-modal data fusion framework that integrates in-situ video data with\nprocess parameters for real-time food drying readiness forecasting. Our\napproach leverages a new encoder-decoder architecture with modality-specific\nencoders and a transformer-based decoder to effectively extract features while\npreserving the unique structure of each modality. We apply our approach to\nsugar cookie drying, where time-to-ready is predicted at each timestamp.\nExperimental results demonstrate that our model achieves an average prediction\nerror of only 15 seconds, outperforming state-of-the-art data fusion methods by\n65.69% and a video-only model by 11.30%. Additionally, our model balances\nprediction accuracy, model size, and computational efficiency, making it\nwell-suited for heterogenous industrial datasets. The proposed model is\nextensible to various other industrial modality fusion tasks for online\ndecision-making."}
{"id": "2403.19001", "pdf": "https://arxiv.org/pdf/2403.19001", "abs": "https://arxiv.org/abs/2403.19001", "authors": ["Yui Lo", "Yuqian Chen", "Dongnan Liu", "Wan Liu", "Leo Zekelman", "Fan Zhang", "Yogesh Rathi", "Nikos Makris", "Alexandra J. Golby", "Weidong Cai", "Lauren J. O'Donnell"], "title": "Cross-domain Fiber Cluster Shape Analysis for Language Performance Cognitive Score Prediction", "categories": ["cs.CV", "cs.AI", "eess.IV", "q-bio.NC"], "comment": "This paper has been accepted for presentation at The 27th Intl. Conf.\n  on Medical Image Computing and Computer Assisted Intervention (MICCAI 2024)\n  Workshop on Computational Diffusion MRI (CDMRI). 11 pages, 2 figures", "summary": "Shape plays an important role in computer graphics, offering informative\nfeatures to convey an object's morphology and functionality. Shape analysis in\nbrain imaging can help interpret structural and functionality correlations of\nthe human brain. In this work, we investigate the shape of the brain's 3D white\nmatter connections and its potential predictive relationship to human cognitive\nfunction. We reconstruct brain connections as sequences of 3D points using\ndiffusion magnetic resonance imaging (dMRI) tractography. To describe each\nconnection, we extract 12 shape descriptors in addition to traditional dMRI\nconnectivity and tissue microstructure features. We introduce a novel\nframework, Shape--fused Fiber Cluster Transformer (SFFormer), that leverages a\nmulti-head cross-attention feature fusion module to predict subject-specific\nlanguage performance based on dMRI tractography. We assess the performance of\nthe method on a large dataset including 1065 healthy young adults. The results\ndemonstrate that both the transformer-based SFFormer model and its inter/intra\nfeature fusion with shape, microstructure, and connectivity are informative,\nand together, they improve the prediction of subject-specific language\nperformance scores. Overall, our results indicate that the shape of the brain's\nconnections is predictive of human language function."}
{"id": "2504.15683", "pdf": "https://arxiv.org/pdf/2504.15683", "abs": "https://arxiv.org/abs/2504.15683", "authors": ["Simon Jehnen", "Joaquín Ordieres-Meré", "Javier Villalba-Díez"], "title": "FinTextSim: Enhancing Financial Text Analysis with BERTopic", "categories": ["cs.CL", "cs.LG", "econ.GN", "q-fin.EC", "q-fin.GN", "68T50", "I.2.7; I.5.1; J.4"], "comment": null, "summary": "Recent advancements in information availability and computational\ncapabilities have transformed the analysis of annual reports, integrating\ntraditional financial metrics with insights from textual data. To extract\nvaluable insights from this wealth of textual data, automated review processes,\nsuch as topic modeling, are crucial. This study examines the effectiveness of\nBERTopic, a state-of-the-art topic model relying on contextual embeddings, for\nanalyzing Item 7 and Item 7A of 10-K filings from S&P 500 companies\n(2016-2022). Moreover, we introduce FinTextSim, a finetuned\nsentence-transformer model optimized for clustering and semantic search in\nfinancial contexts. Compared to all-MiniLM-L6-v2, the most widely used\nsentence-transformer, FinTextSim increases intratopic similarity by 81% and\nreduces intertopic similarity by 100%, significantly enhancing organizational\nclarity. We assess BERTopic's performance using embeddings from both FinTextSim\nand all-MiniLM-L6-v2. Our findings reveal that BERTopic only forms clear and\ndistinct economic topic clusters when paired with FinTextSim's embeddings.\nWithout FinTextSim, BERTopic struggles with misclassification and overlapping\ntopics. Thus, FinTextSim is pivotal for advancing financial text analysis.\nFinTextSim's enhanced contextual embeddings, tailored for the financial domain,\nelevate the quality of future research and financial information. This improved\nquality of financial information will enable stakeholders to gain a competitive\nadvantage, streamlining resource allocation and decision-making processes.\nMoreover, the improved insights have the potential to leverage business\nvaluation and stock price prediction models."}
{"id": "2504.15780", "pdf": "https://arxiv.org/pdf/2504.15780", "abs": "https://arxiv.org/abs/2504.15780", "authors": ["Daocheng Fu", "Zijun Chen", "Renqiu Xia", "Qi Liu", "Yuan Feng", "Hongbin Zhou", "Renrui Zhang", "Shiyang Feng", "Peng Gao", "Junchi Yan", "Botian Shi", "Bo Zhang", "Yu Qiao"], "title": "TrustGeoGen: Scalable and Formal-Verified Data Engine for Trustworthy Multi-modal Geometric Problem Solving", "categories": ["cs.AI", "cs.CL"], "comment": null, "summary": "Mathematical geometric problem solving (GPS) often requires effective\nintegration of multimodal information and verifiable logical coherence. Despite\nthe fast development of large language models in general problem solving, it\nremains unresolved regarding with both methodology and benchmarks, especially\ngiven the fact that exiting synthetic GPS benchmarks are often not\nself-verified and contain noise and self-contradicted information due to the\nillusion of LLMs. In this paper, we propose a scalable data engine called\nTrustGeoGen for problem generation, with formal verification to provide a\nprincipled benchmark, which we believe lays the foundation for the further\ndevelopment of methods for GPS. The engine synthesizes geometric data through\nfour key innovations: 1) multimodal-aligned generation of diagrams, textual\ndescriptions, and stepwise solutions; 2) formal verification ensuring\nrule-compliant reasoning paths; 3) a bootstrapping mechanism enabling\ncomplexity escalation via recursive state generation and 4) our devised\nGeoExplore series algorithms simultaneously produce multi-solution variants and\nself-reflective backtracking traces. By formal logical verification,\nTrustGeoGen produces GeoTrust-200K dataset with guaranteed modality integrity,\nalong with GeoTrust-test testset. Experiments reveal the state-of-the-art\nmodels achieve only 49.17\\% accuracy on GeoTrust-test, demonstrating its\nevaluation stringency. Crucially, models trained on GeoTrust achieve OOD\ngeneralization on GeoQA, significantly reducing logical inconsistencies\nrelative to pseudo-label annotated by OpenAI-o1. Our code is available at\nhttps://github.com/Alpha-Innovator/TrustGeoGen"}
{"id": "2504.15477", "pdf": "https://arxiv.org/pdf/2504.15477", "abs": "https://arxiv.org/abs/2504.15477", "authors": ["Junda Wu", "Rohan Surana", "Zhouhang Xie", "Yiran Shen", "Yu Xia", "Tong Yu", "Ryan A. Rossi", "Prithviraj Ammanabrolu", "Julian McAuley"], "title": "In-context Ranking Preference Optimization", "categories": ["cs.LG"], "comment": "10 pages", "summary": "Recent developments in Direct Preference Optimization (DPO) allow large\nlanguage models (LLMs) to function as implicit ranking models by maximizing the\nmargin between preferred and non-preferred responses. In practice, user\nfeedback on such lists typically involves identifying a few relevant items in\ncontext rather than providing detailed pairwise comparisons for every possible\nitem pair. Moreover, many complex information retrieval tasks, such as\nconversational agents and summarization systems, critically depend on ranking\nthe highest-quality outputs at the top, emphasizing the need to support natural\nand flexible forms of user feedback. To address the challenge of limited and\nsparse pairwise feedback in the in-context setting, we propose an In-context\nRanking Preference Optimization (IRPO) framework that directly optimizes LLMs\nbased on ranking lists constructed during inference. To further capture\nflexible forms of feedback, IRPO extends the DPO objective by incorporating\nboth the relevance of items and their positions in the list. Modeling these\naspects jointly is non-trivial, as ranking metrics are inherently discrete and\nnon-differentiable, making direct optimization difficult. To overcome this,\nIRPO introduces a differentiable objective based on positional aggregation of\npairwise item preferences, enabling effective gradient-based optimization of\ndiscrete ranking metrics. We further provide theoretical insights showing that\nIRPO (i) automatically emphasizes items with greater disagreement between the\nmodel and the reference ranking, and (ii) links its gradient to an importance\nsampling estimator, yielding an unbiased estimator with reduced variance.\nEmpirical results show IRPO outperforms standard DPO approaches in ranking\nperformance, highlighting its effectiveness in aligning LLMs with direct\nin-context ranking preferences."}
{"id": "2504.15609", "pdf": "https://arxiv.org/pdf/2504.15609", "abs": "https://arxiv.org/abs/2504.15609", "authors": ["Yunfeng Li", "Bo Wang", "Jiahao Wan", "Xueyi Wu", "Ye Li"], "title": "SonarT165: A Large-scale Benchmark and STFTrack Framework for Acoustic Object Tracking", "categories": ["cs.CV"], "comment": null, "summary": "Underwater observation systems typically integrate optical cameras and\nimaging sonar systems. When underwater visibility is insufficient, only sonar\nsystems can provide stable data, which necessitates exploration of the\nunderwater acoustic object tracking (UAOT) task. Previous studies have explored\ntraditional methods and Siamese networks for UAOT. However, the absence of a\nunified evaluation benchmark has significantly constrained the value of these\nmethods. To alleviate this limitation, we propose the first large-scale UAOT\nbenchmark, SonarT165, comprising 165 square sequences, 165 fan sequences, and\n205K high-quality annotations. Experimental results demonstrate that SonarT165\nreveals limitations in current state-of-the-art SOT trackers. To address these\nlimitations, we propose STFTrack, an efficient framework for acoustic object\ntracking. It includes two novel modules, a multi-view template fusion module\n(MTFM) and an optimal trajectory correction module (OTCM). The MTFM module\nintegrates multi-view feature of both the original image and the binary image\nof the dynamic template, and introduces a cross-attention-like layer to fuse\nthe spatio-temporal target representations. The OTCM module introduces the\nacoustic-response-equivalent pixel property and proposes normalized pixel\nbrightness response scores, thereby suppressing suboptimal matches caused by\ninaccurate Kalman filter prediction boxes. To further improve the model\nfeature, STFTrack introduces a acoustic image enhancement method and a\nFrequency Enhancement Module (FEM) into its tracking pipeline. Comprehensive\nexperiments show the proposed STFTrack achieves state-of-the-art performance on\nthe proposed benchmark. The code is available at\nhttps://github.com/LiYunfengLYF/SonarT165."}
{"id": "2504.07758", "pdf": "https://arxiv.org/pdf/2504.07758", "abs": "https://arxiv.org/abs/2504.07758", "authors": ["Shuangfan Zhou", "Chu Zhou", "Youwei Lyu", "Heng Guo", "Zhanyu Ma", "Boxin Shi", "Imari Sato"], "title": "PIDSR: Complementary Polarized Image Demosaicing and Super-Resolution", "categories": ["cs.CV", "eess.IV"], "comment": "CVPR 2025", "summary": "Polarization cameras can capture multiple polarized images with different\npolarizer angles in a single shot, bringing convenience to polarization-based\ndownstream tasks. However, their direct outputs are color-polarization filter\narray (CPFA) raw images, requiring demosaicing to reconstruct full-resolution,\nfull-color polarized images; unfortunately, this necessary step introduces\nartifacts that make polarization-related parameters such as the degree of\npolarization (DoP) and angle of polarization (AoP) prone to error. Besides,\nlimited by the hardware design, the resolution of a polarization camera is\noften much lower than that of a conventional RGB camera. Existing polarized\nimage demosaicing (PID) methods are limited in that they cannot enhance\nresolution, while polarized image super-resolution (PISR) methods, though\ndesigned to obtain high-resolution (HR) polarized images from the demosaicing\nresults, tend to retain or even amplify errors in the DoP and AoP introduced by\ndemosaicing artifacts. In this paper, we propose PIDSR, a joint framework that\nperforms complementary Polarized Image Demosaicing and Super-Resolution,\nshowing the ability to robustly obtain high-quality HR polarized images with\nmore accurate DoP and AoP from a CPFA raw image in a direct manner. Experiments\nshow our PIDSR not only achieves state-of-the-art performance on both synthetic\nand real data, but also facilitates downstream tasks."}
{"id": "2504.15688", "pdf": "https://arxiv.org/pdf/2504.15688", "abs": "https://arxiv.org/abs/2504.15688", "authors": ["Mandy Cartner", "Matthew Kogan", "Nikolas Webster", "Matthew Wagers", "Ivy Sichel"], "title": "Subject islands do not reduce to construction-specific discourse function", "categories": ["cs.CL"], "comment": null, "summary": "The term islands in linguistics refers to phrases from which extracting an\nelement results in ungrammaticality (Ross, 1967). Grammatical subjects are\nconsidered islands because extracting a sub-part of a subject results in an\nill-formed sentence, despite having a clear intended meaning (e.g., \"Which\ntopic did the article about inspire you?\"). The generative tradition, which\nviews syntax as autonomous of meaning and function, attributes this\nungrammaticality to the abstract movement dependency between the wh-phrase and\nthe subject-internal position with which it is associated for interpretation.\nHowever, research on language that emphasizes its communicative function\nsuggests instead that syntactic constraints, including islands, can be\nexplained based on the way different constructions package information.\nAccordingly, Abeill\\'e et al. (2020) suggest that the islandhood of subjects is\nspecific to the information structure of wh-questions, and propose that\nsubjects are not islands for movement, but for focusing, due to their\ndiscourse-backgroundedness. This predicts that other constructions that differ\nin their information structure from wh-questions, but still involve movement,\nshould not create a subject island effect. We test this prediction in three\nlarge-scale acceptability studies, using a super-additive design that singles\nout subject island violations, in three different constructions: wh-questions,\nrelative clauses, and topicalization. We report evidence for a subject island\neffect in each construction type, despite only wh-questions introducing what\nAbeill\\'e et al. (2020) call \"a clash in information structure.\" We argue that\nthis motivates an account of islands in terms of abstract, syntactic\nrepresentations, independent of the communicative function associated with the\nconstructions."}
{"id": "2504.15785", "pdf": "https://arxiv.org/pdf/2504.15785", "abs": "https://arxiv.org/abs/2504.15785", "authors": ["Siyu Zhou", "Tianyi Zhou", "Yijun Yang", "Guodong Long", "Deheng Ye", "Jing Jiang", "Chengqi Zhang"], "title": "WALL-E 2.0: World Alignment by NeuroSymbolic Learning improves World Model-based LLM Agents", "categories": ["cs.AI"], "comment": "Code is available at https://github.com/elated-sawyer/WALL-E", "summary": "Can we build accurate world models out of large language models (LLMs)? How\ncan world models benefit LLM agents? The gap between the prior knowledge of\nLLMs and the specified environment's dynamics usually bottlenecks LLMs'\nperformance as world models. To bridge the gap, we propose a training-free\n\"world alignment\" that learns an environment's symbolic knowledge complementary\nto LLMs. The symbolic knowledge covers action rules, knowledge graphs, and\nscene graphs, which are extracted by LLMs from exploration trajectories and\nencoded into executable codes to regulate LLM agents' policies. We further\npropose an RL-free, model-based agent \"WALL-E 2.0\" through the model-predictive\ncontrol (MPC) framework. Unlike classical MPC requiring costly optimization on\nthe fly, we adopt an LLM agent as an efficient look-ahead optimizer of future\nsteps' actions by interacting with the neurosymbolic world model. While the LLM\nagent's strong heuristics make it an efficient planner in MPC, the quality of\nits planned actions is also secured by the accurate predictions of the aligned\nworld model. They together considerably improve learning efficiency in a new\nenvironment. On open-world challenges in Mars (Minecraft like) and ALFWorld\n(embodied indoor environments), WALL-E 2.0 significantly outperforms existing\nmethods, e.g., surpassing baselines in Mars by 16.1%-51.6% of success rate and\nby at least 61.7% in score. In ALFWorld, it achieves a new record 98% success\nrate after only 4 iterations."}
{"id": "2504.15479", "pdf": "https://arxiv.org/pdf/2504.15479", "abs": "https://arxiv.org/abs/2504.15479", "authors": ["Jeremy Goldwasser", "Giles Hooker"], "title": "Unifying Image Counterfactuals and Feature Attributions with Latent-Space Adversarial Attacks", "categories": ["cs.LG", "cs.CV"], "comment": null, "summary": "Counterfactuals are a popular framework for interpreting machine learning\npredictions. These what if explanations are notoriously challenging to create\nfor computer vision models: standard gradient-based methods are prone to\nproduce adversarial examples, in which imperceptible modifications to image\npixels provoke large changes in predictions. We introduce a new,\neasy-to-implement framework for counterfactual images that can flexibly adapt\nto contemporary advances in generative modeling. Our method, Counterfactual\nAttacks, resembles an adversarial attack on the representation of the image\nalong a low-dimensional manifold. In addition, given an auxiliary dataset of\nimage descriptors, we show how to accompany counterfactuals with feature\nattribution that quantify the changes between the original and counterfactual\nimages. These importance scores can be aggregated into global counterfactual\nexplanations that highlight the overall features driving model predictions.\nWhile this unification is possible for any counterfactual method, it has\nparticular computational efficiency for ours. We demonstrate the efficacy of\nour approach with the MNIST and CelebA datasets."}
{"id": "2504.15612", "pdf": "https://arxiv.org/pdf/2504.15612", "abs": "https://arxiv.org/abs/2504.15612", "authors": ["Hongxing Peng", "Kang Lin", "Huanai Liu"], "title": "HS-Mamba: Full-Field Interaction Multi-Groups Mamba for Hyperspectral Image Classification", "categories": ["cs.CV"], "comment": null, "summary": "Hyperspectral image (HSI) classification has been one of the hot topics in\nremote sensing fields. Recently, the Mamba architecture based on selective\nstate-space models (S6) has demonstrated great advantages in long sequence\nmodeling. However, the unique properties of hyperspectral data, such as high\ndimensionality and feature inlining, pose challenges to the application of\nMamba to HSI classification. To compensate for these shortcomings, we propose\nan full-field interaction multi-groups Mamba framework (HS-Mamba), which adopts\na strategy different from pixel-patch based or whole-image based, but combines\nthe advantages of both. The patches cut from the whole image are sent to\nmulti-groups Mamba, combined with positional information to perceive local\ninline features in the spatial and spectral domains, and the whole image is\nsent to a lightweight attention module to enhance the global feature\nrepresentation ability. Specifically, HS-Mamba consists of a dual-channel\nspatial-spectral encoder (DCSS-encoder) module and a lightweight global inline\nattention (LGI-Att) branch. The DCSS-encoder module uses multiple groups of\nMamba to decouple and model the local features of dual-channel sequences with\nnon-overlapping patches. The LGI-Att branch uses a lightweight compressed and\nextended attention module to perceive the global features of the spatial and\nspectral domains of the unsegmented whole image. By fusing local and global\nfeatures, high-precision classification of hyperspectral images is achieved.\nExtensive experiments demonstrate the superiority of the proposed HS-Mamba,\noutperforming state-of-the-art methods on four benchmark HSI datasets."}
{"id": "2504.15777", "pdf": "https://arxiv.org/pdf/2504.15777", "abs": "https://arxiv.org/abs/2504.15777", "authors": ["Shangshang Wang", "Julian Asilis", "Ömer Faruk Akgül", "Enes Burak Bilgin", "Ollie Liu", "Willie Neiswanger"], "title": "Tina: Tiny Reasoning Models via LoRA", "categories": ["cs.CL", "cs.LG"], "comment": null, "summary": "How cost-effectively can strong reasoning abilities be achieved in language\nmodels? Driven by this fundamental question, we present Tina, a family of tiny\nreasoning models achieved with high cost-efficiency. Notably, Tina demonstrates\nthat substantial reasoning performance can be developed using only minimal\nresources, by applying parameter-efficient updates during reinforcement\nlearning (RL), using low-rank adaptation (LoRA), to an already tiny 1.5B\nparameter base model. This minimalist approach produces models that achieve\nreasoning performance which is competitive with, and sometimes surpasses, SOTA\nRL reasoning models built upon the same base model. Crucially, this is achieved\nat a tiny fraction of the computational post-training cost employed by existing\nSOTA models. In fact, the best Tina model achieves a >20\\% reasoning\nperformance increase and 43.33\\% Pass@1 accuracy on AIME24, at only \\$9 USD\npost-training and evaluation cost (i.e., an estimated 260x cost reduction). Our\nwork reveals the surprising effectiveness of efficient RL reasoning via LoRA.\nWe validate this across multiple open-source reasoning datasets and various\nablation settings starting with a single, fixed set of hyperparameters.\nFurthermore, we hypothesize that this effectiveness and efficiency stem from\nLoRA rapidly adapting the model to the structural format of reasoning rewarded\nby RL, while largely preserving the base model's underlying knowledge. In\nservice of accessibility and open research, we fully open-source all code,\ntraining logs, and model weights \\& checkpoints."}
{"id": "2504.15791", "pdf": "https://arxiv.org/pdf/2504.15791", "abs": "https://arxiv.org/abs/2504.15791", "authors": ["Raquel Fernandez-Peralta", "Javier Fumanal-Idocin", "Javier Andreu-Perez"], "title": "Crisp complexity of fuzzy classifiers", "categories": ["cs.AI"], "comment": null, "summary": "Rule-based systems are a very popular form of explainable AI, particularly in\nthe fuzzy community, where fuzzy rules are widely used for control and\nclassification problems. However, fuzzy rule-based classifiers struggle to\nreach bigger traction outside of fuzzy venues, because users sometimes do not\nknow about fuzzy and because fuzzy partitions are not so easy to interpret in\nsome situations. In this work, we propose a methodology to reduce fuzzy\nrule-based classifiers to crisp rule-based classifiers. We study different\npossible crisp descriptions and implement an algorithm to obtain them. Also, we\nanalyze the complexity of the resulting crisp classifiers. We believe that our\nresults can help both fuzzy and non-fuzzy practitioners understand better the\nway in which fuzzy rule bases partition the feature space and how easily one\nsystem can be translated to another and vice versa. Our complexity metric can\nalso help to choose between different fuzzy classifiers based on what the\nequivalent crisp partitions look like."}
{"id": "2504.15487", "pdf": "https://arxiv.org/pdf/2504.15487", "abs": "https://arxiv.org/abs/2504.15487", "authors": ["Moein Darman", "Pedram Hassanzadeh", "Laure Zanna", "Ashesh Chattopadhyay"], "title": "Fourier analysis of the physics of transfer learning for data-driven subgrid-scale models of ocean turbulence", "categories": ["cs.LG", "nlin.CD", "physics.ao-ph", "physics.geo-ph"], "comment": null, "summary": "Transfer learning (TL) is a powerful tool for enhancing the performance of\nneural networks (NNs) in applications such as weather and climate prediction\nand turbulence modeling. TL enables models to generalize to out-of-distribution\ndata with minimal training data from the new system. In this study, we employ a\n9-layer convolutional NN to predict the subgrid forcing in a two-layer ocean\nquasi-geostrophic system and examine which metrics best describe its\nperformance and generalizability to unseen dynamical regimes. Fourier analysis\nof the NN kernels reveals that they learn low-pass, Gabor, and high-pass\nfilters, regardless of whether the training data are isotropic or anisotropic.\nBy analyzing the activation spectra, we identify why NNs fail to generalize\nwithout TL and how TL can overcome these limitations: the learned weights and\nbiases from one dataset underestimate the out-of-distribution sample spectra as\nthey pass through the network, leading to an underestimation of output spectra.\nBy re-training only one layer with data from the target system, this\nunderestimation is corrected, enabling the NN to produce predictions that match\nthe target spectra. These findings are broadly applicable to data-driven\nparameterization of dynamical systems."}
{"id": "2504.15619", "pdf": "https://arxiv.org/pdf/2504.15619", "abs": "https://arxiv.org/abs/2504.15619", "authors": ["Jinda Lu", "Jinghan Li", "Yuan Gao", "Junkang Wu", "Jiancan Wu", "Xiang Wang", "Xiangnan He"], "title": "AdaViP: Aligning Multi-modal LLMs via Adaptive Vision-enhanced Preference Optimization", "categories": ["cs.CV"], "comment": null, "summary": "Preference alignment through Direct Preference Optimization (DPO) has\ndemonstrated significant effectiveness in aligning multimodal large language\nmodels (MLLMs) with human preferences. However, existing methods focus\nprimarily on language preferences while neglecting the critical visual context.\nIn this paper, we propose an Adaptive Vision-enhanced Preference optimization\n(AdaViP) that addresses these limitations through two key innovations: (1)\nvision-based preference pair construction, which integrates multiple visual\nfoundation models to strategically remove key visual elements from the image,\nenhancing MLLMs' sensitivity to visual details; and (2) adaptive preference\noptimization that dynamically balances vision- and language-based preferences\nfor more accurate alignment. Extensive evaluations across different benchmarks\ndemonstrate our effectiveness. Notably, our AdaViP-7B achieves 93.7% and 96.4%\nreductions in response-level and mentioned-level hallucination respectively on\nthe Object HalBench, significantly outperforming current state-of-the-art\nmethods."}
{"id": "2504.15784", "pdf": "https://arxiv.org/pdf/2504.15784", "abs": "https://arxiv.org/abs/2504.15784", "authors": ["Ruizhe Li", "Chiwei Zhu", "Benfeng Xu", "Xiaorui Wang", "Zhendong Mao"], "title": "Automated Creativity Evaluation for Large Language Models: A Reference-Based Approach", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Creative writing is a key capability of Large Language Models (LLMs), with\npotential applications in literature, storytelling, and various creative\ndomains. However, evaluating the creativity of machine-generated texts remains\na significant challenge, as existing methods either rely on costly manual\nannotations or fail to align closely with human assessments. In this paper, we\npropose an effective automated evaluation method based on the Torrance Test of\nCreative Writing (TTCW), which evaluates creativity as product. Our method\nemploys a reference-based Likert-style approach, scoring generated creative\ntexts relative to high-quality reference texts across various tests.\nExperimental results demonstrate that our method significantly improves the\nalignment between LLM evaluations and human assessments, achieving a pairwise\naccuracy of 0.75 (+15\\%)."}
{"id": "2504.15829", "pdf": "https://arxiv.org/pdf/2504.15829", "abs": "https://arxiv.org/abs/2504.15829", "authors": ["Modhurita Mitra", "Martine G. de Vos", "Nicola Cortinovis", "Dawa Ometto"], "title": "Generative AI for Research Data Processing: Lessons Learnt From Three Use Cases", "categories": ["cs.AI", "68T50", "I.2.7"], "comment": "10 pages, 4 figures, 6 tables. Published in Proceedings of the 2024\n  IEEE 20th International Conference on e-Science (e-Science), Osaka, Japan", "summary": "There has been enormous interest in generative AI since ChatGPT was launched\nin 2022. However, there are concerns about the accuracy and consistency of the\noutputs of generative AI. We have carried out an exploratory study on the\napplication of this new technology in research data processing. We identified\ntasks for which rule-based or traditional machine learning approaches were\ndifficult to apply, and then performed these tasks using generative AI.\n  We demonstrate the feasibility of using the generative AI model Claude 3 Opus\nin three research projects involving complex data processing tasks:\n  1) Information extraction: We extract plant species names from historical\nseedlists (catalogues of seeds) published by botanical gardens.\n  2) Natural language understanding: We extract certain data points (name of\ndrug, name of health indication, relative effectiveness, cost-effectiveness,\netc.) from documents published by Health Technology Assessment organisations in\nthe EU.\n  3) Text classification: We assign industry codes to projects on the\ncrowdfunding website Kickstarter.\n  We share the lessons we learnt from these use cases: How to determine if\ngenerative AI is an appropriate tool for a given data processing task, and if\nso, how to maximise the accuracy and consistency of the results obtained."}
{"id": "2504.15491", "pdf": "https://arxiv.org/pdf/2504.15491", "abs": "https://arxiv.org/abs/2504.15491", "authors": ["Tengda Tang", "Jianhua Yao", "Yixian Wang", "Qiuwu Sha", "Hanrui Feng", "Zhen Xu"], "title": "Application of Deep Generative Models for Anomaly Detection in Complex Financial Transactions", "categories": ["cs.LG"], "comment": null, "summary": "This study proposes an algorithm for detecting suspicious behaviors in large\npayment flows based on deep generative models. By combining Generative\nAdversarial Networks (GAN) and Variational Autoencoders (VAE), the algorithm is\ndesigned to detect abnormal behaviors in financial transactions. First, the GAN\nis used to generate simulated data that approximates normal payment flows. The\ndiscriminator identifies anomalous patterns in transactions, enabling the\ndetection of potential fraud and money laundering behaviors. Second, a VAE is\nintroduced to model the latent distribution of payment flows, ensuring that the\ngenerated data more closely resembles real transaction features, thus improving\nthe model's detection accuracy. The method optimizes the generative\ncapabilities of both GAN and VAE, ensuring that the model can effectively\ncapture suspicious behaviors even in sparse data conditions. Experimental\nresults show that the proposed method significantly outperforms traditional\nmachine learning algorithms and other deep learning models across various\nevaluation metrics, especially in detecting rare fraudulent behaviors.\nFurthermore, this study provides a detailed comparison of performance in\nrecognizing different transaction patterns (such as normal, money laundering,\nand fraud) in large payment flows, validating the advantages of generative\nmodels in handling complex financial data."}
{"id": "2504.15624", "pdf": "https://arxiv.org/pdf/2504.15624", "abs": "https://arxiv.org/abs/2504.15624", "authors": ["Jingzhi Li", "Changjiang Luo", "Ruoyu Chen", "Hua Zhang", "Wenqi Ren", "Jianhou Gan", "Xiaochun Cao"], "title": "FaceInsight: A Multimodal Large Language Model for Face Perception", "categories": ["cs.CV"], "comment": null, "summary": "Recent advances in multimodal large language models (MLLMs) have demonstrated\nstrong capabilities in understanding general visual content. However, these\ngeneral-domain MLLMs perform poorly in face perception tasks, often producing\ninaccurate or misleading responses to face-specific queries. To address this\ngap, we propose FaceInsight, the versatile face perception MLLM that provides\nfine-grained facial information. Our approach introduces visual-textual\nalignment of facial knowledge to model both uncertain dependencies and\ndeterministic relationships among facial information, mitigating the\nlimitations of language-driven reasoning. Additionally, we incorporate face\nsegmentation maps as an auxiliary perceptual modality, enriching the visual\ninput with localized structural cues to enhance semantic understanding.\nComprehensive experiments and analyses across three face perception tasks\ndemonstrate that FaceInsight consistently outperforms nine compared MLLMs under\nboth training-free and fine-tuned settings."}
{"id": "2504.15801", "pdf": "https://arxiv.org/pdf/2504.15801", "abs": "https://arxiv.org/abs/2504.15801", "authors": ["Valeria Lerman", "Yaniv Dover"], "title": "A closer look at how large language models trust humans: patterns and biases", "categories": ["cs.CL", "cs.AI", "cs.CY"], "comment": null, "summary": "As large language models (LLMs) and LLM-based agents increasingly interact\nwith humans in decision-making contexts, understanding the trust dynamics\nbetween humans and AI agents becomes a central concern. While considerable\nliterature studies how humans trust AI agents, it is much less understood how\nLLM-based agents develop effective trust in humans. LLM-based agents likely\nrely on some sort of implicit effective trust in trust-related contexts (e.g.,\nevaluating individual loan applications) to assist and affect decision making.\nUsing established behavioral theories, we develop an approach that studies\nwhether LLMs trust depends on the three major trustworthiness dimensions:\ncompetence, benevolence and integrity of the human subject. We also study how\ndemographic variables affect effective trust. Across 43,200 simulated\nexperiments, for five popular language models, across five different scenarios\nwe find that LLM trust development shows an overall similarity to human trust\ndevelopment. We find that in most, but not all cases, LLM trust is strongly\npredicted by trustworthiness, and in some cases also biased by age, religion\nand gender, especially in financial scenarios. This is particularly true for\nscenarios common in the literature and for newer models. While the overall\npatterns align with human-like mechanisms of effective trust formation,\ndifferent models exhibit variation in how they estimate trust; in some cases,\ntrustworthiness and demographic factors are weak predictors of effective trust.\nThese findings call for a better understanding of AI-to-human trust dynamics\nand monitoring of biases and trust development patterns to prevent unintended\nand potentially harmful outcomes in trust-sensitive applications of AI."}
{"id": "2504.15847", "pdf": "https://arxiv.org/pdf/2504.15847", "abs": "https://arxiv.org/abs/2504.15847", "authors": ["Xiang Liu", "Hau Chan", "Minming Li", "Xianlong Zeng", "Chenchen Fu", "Weiwei Wu"], "title": "CARE: Compatibility-Aware Incentive Mechanisms for Federated Learning with Budgeted Requesters", "categories": ["cs.AI"], "comment": null, "summary": "Federated learning (FL) is a promising approach that allows requesters (\\eg,\nservers) to obtain local training models from workers (e.g., clients). Since\nworkers are typically unwilling to provide training services/models freely and\nvoluntarily, many incentive mechanisms in FL are designed to incentivize\nparticipation by offering monetary rewards from requesters. However, existing\nstudies neglect two crucial aspects of real-world FL scenarios. First, workers\ncan possess inherent incompatibility characteristics (e.g., communication\nchannels and data sources), which can lead to degradation of FL efficiency\n(e.g., low communication efficiency and poor model generalization). Second, the\nrequesters are budgeted, which limits the amount of workers they can hire for\ntheir tasks. In this paper, we investigate the scenario in FL where multiple\nbudgeted requesters seek training services from incompatible workers with\nprivate training costs. We consider two settings: the cooperative budget\nsetting where requesters cooperate to pool their budgets to improve their\noverall utility and the non-cooperative budget setting where each requester\noptimizes their utility within their own budgets. To address efficiency\ndegradation caused by worker incompatibility, we develop novel\ncompatibility-aware incentive mechanisms, CARE-CO and CARE-NO, for both\nsettings to elicit true private costs and determine workers to hire for\nrequesters and their rewards while satisfying requester budget constraints. Our\nmechanisms guarantee individual rationality, truthfulness, budget feasibility,\nand approximation performance. We conduct extensive experiments using\nreal-world datasets to show that the proposed mechanisms significantly\noutperform existing baselines."}
{"id": "2504.15525", "pdf": "https://arxiv.org/pdf/2504.15525", "abs": "https://arxiv.org/abs/2504.15525", "authors": ["Chengjun Yu", "Yixin Ran", "Yangyi Xia", "Jia Wu", "Xiaojing Liu"], "title": "Federated Latent Factor Learning for Recovering Wireless Sensor Networks Signal with Privacy-Preserving", "categories": ["cs.LG"], "comment": "Accepted By ICAIS&ISAS 2025", "summary": "Wireless Sensor Networks (WSNs) are a cutting-edge domain in the field of\nintelligent sensing. Due to sensor failures and energy-saving strategies, the\ncollected data often have massive missing data, hindering subsequent analysis\nand decision-making. Although Latent Factor Learning (LFL) has been proven\neffective in recovering missing data, it fails to sufficiently consider data\nprivacy protection. To address this issue, this paper innovatively proposes a\nfederated latent factor learning (FLFL) based spatial signal recovery (SSR)\nmodel, named FLFL-SSR. Its main idea is two-fold: 1) it designs a sensor-level\nfederated learning framework, where each sensor uploads only gradient updates\ninstead of raw data to optimize the global model, and 2) it proposes a local\nspatial sharing strategy, allowing sensors within the same spatial region to\nshare their latent feature vectors, capturing spatial correlations and\nenhancing recovery accuracy. Experimental results on two real-world WSNs\ndatasets demonstrate that the proposed model outperforms existing federated\nmethods in terms of recovery performance."}
{"id": "2504.15627", "pdf": "https://arxiv.org/pdf/2504.15627", "abs": "https://arxiv.org/abs/2504.15627", "authors": ["Doanh C. Bui", "Hoai Luan Pham", "Vu Trung Duong Le", "Tuan Hai Vu", "Van Duy Tran", "Yasuhiko Nakashima"], "title": "ZeroSlide: Is Zero-Shot Classification Adequate for Lifelong Learning in Whole-Slide Image Analysis in the Era of Pathology Vision-Language Foundation Models?", "categories": ["cs.CV"], "comment": "10 pages, 3 figures, 1 table, conference submission", "summary": "Lifelong learning for whole slide images (WSIs) poses the challenge of\ntraining a unified model to perform multiple WSI-related tasks, such as cancer\nsubtyping and tumor classification, in a distributed, continual fashion. This\nis a practical and applicable problem in clinics and hospitals, as WSIs are\nlarge, require storage, processing, and transfer time. Training new models\nwhenever new tasks are defined is time-consuming. Recent work has applied\nregularization- and rehearsal-based methods to this setting. However, the rise\nof vision-language foundation models that align diagnostic text with pathology\nimages raises the question: are these models alone sufficient for lifelong WSI\nlearning using zero-shot classification, or is further investigation into\ncontinual learning strategies needed to improve performance? To our knowledge,\nthis is the first study to compare conventional continual-learning approaches\nwith vision-language zero-shot classification for WSIs. Our source code and\nexperimental results will be available soon."}
{"id": "2504.15815", "pdf": "https://arxiv.org/pdf/2504.15815", "abs": "https://arxiv.org/abs/2504.15815", "authors": ["Michael A. Hedderich", "Anyi Wang", "Raoyuan Zhao", "Florian Eichin", "Barbara Plank"], "title": "What's the Difference? Supporting Users in Identifying the Effects of Prompt and Model Changes Through Token Patterns", "categories": ["cs.CL", "cs.HC", "cs.LG"], "comment": null, "summary": "Prompt engineering for large language models is challenging, as even small\nprompt perturbations or model changes can significantly impact the generated\noutput texts. Existing evaluation methods, either automated metrics or human\nevaluation, have limitations, such as providing limited insights or being\nlabor-intensive. We propose Spotlight, a new approach that combines both\nautomation and human analysis. Based on data mining techniques, we\nautomatically distinguish between random (decoding) variations and systematic\ndifferences in language model outputs. This process provides token patterns\nthat describe the systematic differences and guide the user in manually\nanalyzing the effects of their prompt and model changes efficiently. We create\nthree benchmarks to quantitatively test the reliability of token pattern\nextraction methods and demonstrate that our approach provides new insights into\nestablished prompt data. From a human-centric perspective, through\ndemonstration studies and a user study, we show that our token pattern approach\nhelps users understand the systematic differences of language model outputs,\nand we are able to discover relevant differences caused by prompt and model\nchanges (e.g. related to gender or culture), thus supporting the prompt\nengineering process and human-centric model behavior research."}
{"id": "2504.15903", "pdf": "https://arxiv.org/pdf/2504.15903", "abs": "https://arxiv.org/abs/2504.15903", "authors": ["Nikhil Khandalkar", "Pavan Yadav", "Krishna Shinde", "Lokesh B. Ramegowda", "Rajarshi Das"], "title": "Impact of Noise on LLM-Models Performance in Abstraction and Reasoning Corpus (ARC) Tasks with Model Temperature Considerations", "categories": ["cs.AI"], "comment": "60 pages, 25 figures", "summary": "Recent advancements in Large Language Models (LLMs) have generated growing\ninterest in their structured reasoning capabilities, particularly in tasks\ninvolving abstraction and pattern recognition. The Abstraction and Reasoning\nCorpus (ARC) benchmark plays a crucial role in evaluating these capabilities by\ntesting how well AI models generalize to novel problems. While GPT-4o\ndemonstrates strong performance by solving all ARC tasks under zero-noise\nconditions, other models like DeepSeek R1 and LLaMA 3.2 fail to solve any,\nsuggesting limitations in their ability to reason beyond simple pattern\nmatching. To explore this gap, we systematically evaluate these models across\ndifferent noise levels and temperature settings. Our results reveal that the\nintroduction of noise consistently impairs model performance, regardless of\narchitecture. This decline highlights a shared vulnerability: current LLMs,\ndespite showing signs of abstract reasoning, remain highly sensitive to input\nperturbations. Such fragility raises concerns about their real-world\napplicability, where noise and uncertainty are common. By comparing how\ndifferent model architectures respond to these challenges, we offer insights\ninto the structural weaknesses of modern LLMs in reasoning tasks. This work\nunderscores the need for developing more robust and adaptable AI systems\ncapable of handling the ambiguity and variability inherent in real-world\nscenarios. Our findings aim to guide future research toward enhancing model\ngeneralization, robustness, and alignment with human-like cognitive\nflexibility."}
{"id": "2504.15539", "pdf": "https://arxiv.org/pdf/2504.15539", "abs": "https://arxiv.org/abs/2504.15539", "authors": ["Ryan J. Miller", "Alexander E. Dashuta", "Brayden Rudisill", "David Van Vranken", "Pierre Baldi"], "title": "Interpretable Deep Learning for Polar Mechanistic Reaction Prediction", "categories": ["cs.LG"], "comment": null, "summary": "Accurately predicting chemical reactions is essential for driving innovation\nin synthetic chemistry, with broad applications in medicine, manufacturing, and\nagriculture. At the same time, reaction prediction is a complex problem which\ncan be both time-consuming and resource-intensive for chemists to solve. Deep\nlearning methods offer an appealing solution by enabling high-throughput\nreaction prediction. However, many existing models are trained on the US Patent\nOffice dataset and treat reactions as overall transformations: mapping\nreactants directly to products with limited interpretability or mechanistic\ninsight. To address this, we introduce PMechRP (Polar Mechanistic Reaction\nPredictor), a system that trains machine learning models on the PMechDB\ndataset, which represents reactions as polar elementary steps that capture\nelectron flow and mechanistic detail. To further expand model coverage and\nimprove generalization, we augment PMechDB with a diverse set of\ncombinatorially generated reactions. We train and compare a range of machine\nlearning models, including transformer-based, graph-based, and two-step siamese\narchitectures. Our best-performing approach was a hybrid model, which combines\na 5-ensemble of Chemformer models with a two-step Siamese framework to leverage\nthe accuracy of transformer architectures, while filtering away \"alchemical\"\nproducts using the two-step network predictions. For evaluation, we use a test\nsplit of the PMechDB dataset and additionally curate a human benchmark dataset\nconsisting of complete mechanistic pathways extracted from an organic chemistry\ntextbook. Our hybrid model achieves a top-10 accuracy of 94.9% on the PMechDB\ntest set and a target recovery rate of 84.9% on the pathway dataset."}
{"id": "2504.15650", "pdf": "https://arxiv.org/pdf/2504.15650", "abs": "https://arxiv.org/abs/2504.15650", "authors": ["Dengyang Jiang", "Mengmeng Wang", "Teli Ma", "Hengzhuang Li", "Yong liu", "Guang Dai", "Lei Zhang"], "title": "AffordanceSAM: Segment Anything Once More in Affordance Grounding", "categories": ["cs.CV"], "comment": "SAM Meets Affordance Grounding", "summary": "Improving the generalization ability of an affordance grounding model to\nrecognize regions for unseen objects and affordance functions is crucial for\nreal-world application. However, current models are still far away from such\nstandards. To address this problem, we introduce AffordanceSAM, an effective\napproach that extends SAM's generalization capacity to the domain of affordance\ngrounding. For the purpose of thoroughly transferring SAM's robust performance\nin segmentation to affordance, we initially propose an affordance-adaption\nmodule in order to help modify SAM's segmentation output to be adapted to the\nspecific functional regions required for affordance grounding. We concurrently\nmake a coarse-to-fine training recipe to make SAM first be aware of affordance\nobjects and actions coarsely, and then be able to generate affordance heatmaps\nfinely. Both quantitative and qualitative experiments show the strong\ngeneralization capacity of our AffordanceSAM, which not only surpasses previous\nmethods under AGD20K benchmark but also shows evidence to handle the task with\nnovel objects and affordance functions."}
{"id": "2504.15843", "pdf": "https://arxiv.org/pdf/2504.15843", "abs": "https://arxiv.org/abs/2504.15843", "authors": ["Junshu Pan", "Wei Shen", "Shulin Huang", "Qiji Zhou", "Yue Zhang"], "title": "Pre-DPO: Improving Data Utilization in Direct Preference Optimization Using a Guiding Reference Model", "categories": ["cs.CL"], "comment": null, "summary": "Direct Preference Optimization (DPO) simplifies reinforcement learning from\nhuman feedback (RLHF) for large language models (LLMs) by directly optimizing\nhuman preferences without an explicit reward model. We find that during DPO\ntraining, the reference model plays the role of a data weight adjuster.\nHowever, the common practice of initializing the policy and reference models\nidentically in DPO can lead to inefficient data utilization and impose a\nperformance ceiling. Meanwhile, the lack of a reference model in Simple\nPreference Optimization (SimPO) reduces training robustness and necessitates\nstricter conditions to prevent catastrophic forgetting. In this work, we\npropose Pre-DPO, a simple yet effective DPO-based training paradigm that\nenhances preference optimization performance by leveraging a guiding reference\nmodel. This reference model provides foresight into the optimal policy state\nachievable through the training preference data, serving as a guiding mechanism\nthat adaptively assigns higher weights to samples more suitable for the model\nand lower weights to those less suitable. Extensive experiments on AlpacaEval\n2.0 and Arena-Hard v0.1 benchmarks demonstrate that Pre-DPO consistently\nimproves the performance of both DPO and SimPO, without relying on external\nmodels or additional data."}
{"id": "2504.16042", "pdf": "https://arxiv.org/pdf/2504.16042", "abs": "https://arxiv.org/abs/2504.16042", "authors": ["Ismaïl Baaj"], "title": "Approximate matrices of systems of max-min fuzzy relational equations", "categories": ["cs.AI", "cs.LO"], "comment": null, "summary": "In this article, we address the inconsistency of a system of max-min fuzzy\nrelational equations by minimally modifying the matrix governing the system in\norder to achieve consistency. Our method yields consistent systems that\napproximate the original inconsistent system in the following sense: the\nright-hand side vector of each consistent system is that of the inconsistent\nsystem, and the coefficients of the matrix governing each consistent system are\nobtained by modifying, exactly and minimally, the entries of the original\nmatrix that must be corrected to achieve consistency, while leaving all other\nentries unchanged.\n  To obtain a consistent system that closely approximates the considered\ninconsistent system, we study the distance (in terms of a norm among $L_1$,\n$L_2$ or $L_\\infty$) between the matrix of the inconsistent system and the set\nformed by the matrices of consistent systems that use the same right-hand side\nvector as the inconsistent system. We show that our method allows us to\ndirectly compute matrices of consistent systems that use the same right-hand\nside vector as the inconsistent system whose distance in terms of $L_\\infty$\nnorm to the matrix of the inconsistent system is minimal (the computational\ncosts are higher when using $L_1$ norm or $L_2$ norm). We also give an explicit\nanalytical formula for computing this minimal $L_\\infty$ distance. Finally, we\ntranslate our results for systems of min-max fuzzy relational equations and\npresent some potential applications."}
{"id": "2504.15562", "pdf": "https://arxiv.org/pdf/2504.15562", "abs": "https://arxiv.org/abs/2504.15562", "authors": ["Dip Roy"], "title": "Bayesian Autoencoder for Medical Anomaly Detection: Uncertainty-Aware Approach for Brain 2 MRI Analysis", "categories": ["cs.LG", "cs.CV"], "comment": "16 pages, 6 figures", "summary": "In medical imaging, anomaly detection is a vital element of healthcare\ndiagnostics, especially for neurological conditions which can be\nlife-threatening. Conventional deterministic methods often fall short when it\ncomes to capturing the inherent uncertainty of anomaly detection tasks. This\npaper introduces a Bayesian Variational Autoencoder (VAE) equipped with\nmulti-head attention mechanisms for detecting anomalies in brain magnetic\nresonance imaging (MRI). For the purpose of improving anomaly detection\nperformance, we incorporate both epistemic and aleatoric uncertainty estimation\nthrough Bayesian inference. The model was tested on the BraTS2020 dataset, and\nthe findings were a 0.83 ROC AUC and a 0.83 PR AUC. The data in our paper\nsuggests that modeling uncertainty is an essential component of anomaly\ndetection, enhancing both performance and interpretability and providing\nconfidence estimates, as well as anomaly predictions, for clinicians to\nleverage in making medical decisions."}
{"id": "2504.15661", "pdf": "https://arxiv.org/pdf/2504.15661", "abs": "https://arxiv.org/abs/2504.15661", "authors": ["Xian Wu", "Chang Liu"], "title": "DiTPainter: Efficient Video Inpainting with Diffusion Transformers", "categories": ["cs.CV"], "comment": null, "summary": "Many existing video inpainting algorithms utilize optical flows to construct\nthe corresponding maps and then propagate pixels from adjacent frames to\nmissing areas by mapping. Despite the effectiveness of the propagation\nmechanism, they might encounter blurry and inconsistencies when dealing with\ninaccurate optical flows or large masks. Recently, Diffusion Transformer (DiT)\nhas emerged as a revolutionary technique for video generation tasks. However,\npretrained DiT models for video generation all contain a large amount of\nparameters, which makes it very time consuming to apply to video inpainting\ntasks. In this paper, we present DiTPainter, an end-to-end video inpainting\nmodel based on Diffusion Transformer (DiT). DiTPainter uses an efficient\ntransformer network designed for video inpainting, which is trained from\nscratch instead of initializing from any large pretrained models. DiTPainter\ncan address videos with arbitrary lengths and can be applied to video\ndecaptioning and video completion tasks with an acceptable time cost.\nExperiments show that DiTPainter outperforms existing video inpainting\nalgorithms with higher quality and better spatial-temporal consistency."}
{"id": "2504.15848", "pdf": "https://arxiv.org/pdf/2504.15848", "abs": "https://arxiv.org/abs/2504.15848", "authors": ["Luwei Xiao", "Rui Mao", "Shuai Zhao", "Qika Lin", "Yanhao Jia", "Liang He", "Erik Cambria"], "title": "Exploring Cognitive and Aesthetic Causality for Multimodal Aspect-Based Sentiment Analysis", "categories": ["cs.CL"], "comment": "Accepted by TAFFC 2025", "summary": "Multimodal aspect-based sentiment classification (MASC) is an emerging task\ndue to an increase in user-generated multimodal content on social platforms,\naimed at predicting sentiment polarity toward specific aspect targets (i.e.,\nentities or attributes explicitly mentioned in text-image pairs). Despite\nextensive efforts and significant achievements in existing MASC, substantial\ngaps remain in understanding fine-grained visual content and the cognitive\nrationales derived from semantic content and impressions (cognitive\ninterpretations of emotions evoked by image content). In this study, we present\nChimera: a cognitive and aesthetic sentiment causality understanding framework\nto derive fine-grained holistic features of aspects and infer the fundamental\ndrivers of sentiment expression from both semantic perspectives and\naffective-cognitive resonance (the synergistic effect between emotional\nresponses and cognitive interpretations). Specifically, this framework first\nincorporates visual patch features for patch-word alignment. Meanwhile, it\nextracts coarse-grained visual features (e.g., overall image representation)\nand fine-grained visual regions (e.g., aspect-related regions) and translates\nthem into corresponding textual descriptions (e.g., facial, aesthetic).\nFinally, we leverage the sentimental causes and impressions generated by a\nlarge language model (LLM) to enhance the model's awareness of sentimental cues\nevoked by semantic content and affective-cognitive resonance. Experimental\nresults on standard MASC datasets demonstrate the effectiveness of the proposed\nmodel, which also exhibits greater flexibility to MASC compared to LLMs such as\nGPT-4o. We have publicly released the complete implementation and dataset at\nhttps://github.com/Xillv/Chimera"}
{"id": "2504.15286", "pdf": "https://arxiv.org/pdf/2504.15286", "abs": "https://arxiv.org/abs/2504.15286", "authors": ["Daniele Gorla", "Shivam Kumar", "Pietro Nicolaus Roselli Lorenzini", "Alireza Alipourfaz"], "title": "CUBETESTERAI: Automated JUnit Test Generation using the LLaMA Model", "categories": ["cs.SE", "cs.AI"], "comment": "Accepted to ICST 2025 Industry Track", "summary": "This paper presents an approach to automating JUnit test generation for Java\napplications using the Spring Boot framework, leveraging the LLaMA (Large\nLanguage Model Architecture) model to enhance the efficiency and accuracy of\nthe testing process. The resulting tool, called CUBETESTERAI, includes a\nuser-friendly web interface and the integration of a CI/CD pipeline using\nGitLab and Docker. These components streamline the automated test generation\nprocess, allowing developers to generate JUnit tests directly from their code\nsnippets with minimal manual intervention. The final implementation executes\nthe LLaMA models through RunPod, an online GPU service, which also enhances the\nprivacy of our tool. Using the advanced natural language processing\ncapabilities of the LLaMA model, CUBETESTERAI is able to generate test cases\nthat provide high code coverage and accurate validation of software\nfunctionalities in Java-based Spring Boot applications. Furthermore, it\nefficiently manages resource-intensive operations and refines the generated\ntests to address common issues like missing imports and handling of private\nmethods. By comparing CUBETESTERAI with some state-of-the-art tools, we show\nthat our proposal consistently demonstrates competitive and, in many cases,\nbetter performance in terms of code coverage in different real-life Java\nprograms."}
{"id": "2504.15582", "pdf": "https://arxiv.org/pdf/2504.15582", "abs": "https://arxiv.org/abs/2504.15582", "authors": ["Jason Hartline", "Yifan Wu", "Yunran Yang"], "title": "Smooth Calibration and Decision Making", "categories": ["cs.LG", "cs.DS", "stat.ML"], "comment": "In FORC 2025", "summary": "Calibration requires predictor outputs to be consistent with their Bayesian\nposteriors. For machine learning predictors that do not distinguish between\nsmall perturbations, calibration errors are continuous in predictions, e.g.,\nsmooth calibration error (Foster and Hart, 2018), Distance to Calibration\n(Blasiok et al., 2023a). On the contrary, decision-makers who use predictions\nmake optimal decisions discontinuously in probabilistic space, experiencing\nloss from miscalibration discontinuously. Calibration errors for\ndecision-making are thus discontinuous, e.g., Expected Calibration Error\n(Foster and Vohra, 1997), and Calibration Decision Loss (Hu and Wu, 2024).\nThus, predictors with a low calibration error for machine learning may suffer a\nhigh calibration error for decision-making, i.e., they may not be trustworthy\nfor decision-makers optimizing assuming their predictions are correct. It is\nnatural to ask if post-processing a predictor with a low calibration error for\nmachine learning is without loss to achieve a low calibration error for\ndecision-making. In our paper, we show that post-processing an online predictor\nwith $\\epsilon$ distance to calibration achieves $O(\\sqrt{\\epsilon})$ ECE and\nCDL, which is asymptotically optimal. The post-processing algorithm adds noise\nto make predictions differentially private. The optimal bound from low distance\nto calibration predictors from post-processing is non-optimal compared with\nexisting online calibration algorithms that directly optimize for ECE and CDL."}
{"id": "2504.15665", "pdf": "https://arxiv.org/pdf/2504.15665", "abs": "https://arxiv.org/abs/2504.15665", "authors": ["Pei Liu", "Yisi Luo", "Wenzhen Wang", "Xiangyong Cao"], "title": "Motion-Enhanced Nonlocal Similarity Implicit Neural Representation for Infrared Dim and Small Target Detection", "categories": ["cs.CV"], "comment": null, "summary": "Infrared dim and small target detection presents a significant challenge due\nto dynamic multi-frame scenarios and weak target signatures in the infrared\nmodality. Traditional low-rank plus sparse models often fail to capture dynamic\nbackgrounds and global spatial-temporal correlations, which results in\nbackground leakage or target loss. In this paper, we propose a novel\nmotion-enhanced nonlocal similarity implicit neural representation (INR)\nframework to address these challenges. We first integrate motion estimation via\noptical flow to capture subtle target movements, and propose multi-frame fusion\nto enhance motion saliency. Second, we leverage nonlocal similarity to\nconstruct patch tensors with strong low-rank properties, and propose an\ninnovative tensor decomposition-based INR model to represent the nonlocal patch\ntensor, effectively encoding both the nonlocal low-rankness and\nspatial-temporal correlations of background through continuous neural\nrepresentations. An alternating direction method of multipliers is developed\nfor the nonlocal INR model, which enjoys theoretical fixed-point convergence.\nExperimental results show that our approach robustly separates dim targets from\ncomplex infrared backgrounds, outperforming state-of-the-art methods in\ndetection accuracy and robustness."}
{"id": "2504.15895", "pdf": "https://arxiv.org/pdf/2504.15895", "abs": "https://arxiv.org/abs/2504.15895", "authors": ["Chenxu Yang", "Qingyi Si", "Yongjie Duan", "Zheliang Zhu", "Chenyu Zhu", "Zheng Lin", "Li Cao", "Weiping Wang"], "title": "Dynamic Early Exit in Reasoning Models", "categories": ["cs.CL", "cs.AI"], "comment": "19 pages, 11 figures", "summary": "Recent advances in large reasoning language models (LRLMs) rely on test-time\nscaling, which extends long chain-of-thought (CoT) generation to solve complex\ntasks. However, overthinking in long CoT not only slows down the efficiency of\nproblem solving, but also risks accuracy loss due to the extremely detailed or\nredundant reasoning steps. We propose a simple yet effective method that allows\nLLMs to self-truncate CoT sequences by early exit during generation. Instead of\nrelying on fixed heuristics, the proposed method monitors model behavior at\npotential reasoning transition points (e.g.,\"Wait\" tokens) and dynamically\nterminates the next reasoning chain's generation when the model exhibits high\nconfidence in a trial answer. Our method requires no additional training and\ncan be seamlessly integrated into existing o1-like reasoning LLMs. Experiments\non multiple reasoning benchmarks MATH-500, AMC 2023, GPQA Diamond and AIME 2024\nshow that the proposed method is consistently effective on deepseek-series\nreasoning LLMs, reducing the length of CoT sequences by an average of 31% to\n43% while improving accuracy by 1.7% to 5.7%."}
{"id": "2504.15296", "pdf": "https://arxiv.org/pdf/2504.15296", "abs": "https://arxiv.org/abs/2504.15296", "authors": ["Yihong Jin", "Ze Yang"], "title": "Scalability Optimization in Cloud-Based AI Inference Services: Strategies for Real-Time Load Balancing and Automated Scaling", "categories": ["cs.DC", "cs.AI", "F.2.2; I.2.8"], "comment": "Accepted to BDICN 2025", "summary": "The rapid expansion of AI inference services in the cloud necessitates a\nrobust scalability solution to manage dynamic workloads and maintain high\nperformance. This study proposes a comprehensive scalability optimization\nframework for cloud AI inference services, focusing on real-time load balancing\nand autoscaling strategies. The proposed model is a hybrid approach that\ncombines reinforcement learning for adaptive load distribution and deep neural\nnetworks for accurate demand forecasting. This multi-layered approach enables\nthe system to anticipate workload fluctuations and proactively adjust\nresources, ensuring maximum resource utilisation and minimising latency.\nFurthermore, the incorporation of a decentralised decision-making process\nwithin the model serves to enhance fault tolerance and reduce response time in\nscaling operations. Experimental results demonstrate that the proposed model\nenhances load balancing efficiency by 35\\ and reduces response delay by 28\\,\nthereby exhibiting a substantial optimization effect in comparison with\nconventional scalability solutions."}
{"id": "2504.15587", "pdf": "https://arxiv.org/pdf/2504.15587", "abs": "https://arxiv.org/abs/2504.15587", "authors": ["Zimo Yan", "Jie Zhang", "Zheng Xie", "Chang Liu", "Yizhen Liu", "Yiping Song"], "title": "MetaMolGen: A Neural Graph Motif Generation Model for De Novo Molecular Design", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Molecular generation plays an important role in drug discovery and materials\nscience, especially in data-scarce scenarios where traditional generative\nmodels often struggle to achieve satisfactory conditional generalization. To\naddress this challenge, we propose MetaMolGen, a first-order\nmeta-learning-based molecular generator designed for few-shot and\nproperty-conditioned molecular generation. MetaMolGen standardizes the\ndistribution of graph motifs by mapping them to a normalized latent space, and\nemploys a lightweight autoregressive sequence model to generate SMILES\nsequences that faithfully reflect the underlying molecular structure. In\naddition, it supports conditional generation of molecules with target\nproperties through a learnable property projector integrated into the\ngenerative process.Experimental results demonstrate that MetaMolGen\nconsistently generates valid and diverse SMILES sequences under low-data\nregimes, outperforming conventional baselines. This highlights its advantage in\nfast adaptation and efficient conditional generation for practical molecular\ndesign."}
{"id": "2504.15669", "pdf": "https://arxiv.org/pdf/2504.15669", "abs": "https://arxiv.org/abs/2504.15669", "authors": ["Wei Zhuo", "Zhiyue Tang", "Wufeng Xue", "Hao Ding", "Linlin Shen"], "title": "DINOv2-powered Few-Shot Semantic Segmentation: A Unified Framework via Cross-Model Distillation and 4D Correlation Mining", "categories": ["cs.CV"], "comment": null, "summary": "Few-shot semantic segmentation has gained increasing interest due to its\ngeneralization capability, i.e., segmenting pixels of novel classes requiring\nonly a few annotated images. Prior work has focused on meta-learning for\nsupport-query matching, with extensive development in both prototype-based and\naggregation-based methods. To address data scarcity, recent approaches have\nturned to foundation models to enhance representation transferability for novel\nclass segmentation. Among them, a hybrid dual-modal framework including both\nDINOv2 and SAM has garnered attention due to their complementary capabilities.\nWe wonder \"can we build a unified model with knowledge from both foundation\nmodels?\" To this end, we propose FS-DINO, with only DINOv2's encoder and a\nlightweight segmenter. The segmenter features a bottleneck adapter, a\nmeta-visual prompt generator based on dense similarities and semantic\nembeddings, and a decoder. Through coarse-to-fine cross-model distillation, we\neffectively integrate SAM's knowledge into our lightweight segmenter, which can\nbe further enhanced by 4D correlation mining on support-query pairs. Extensive\nexperiments on COCO-20i, PASCAL-5i, and FSS-1000 demonstrate the effectiveness\nand superiority of our method."}
{"id": "2504.15900", "pdf": "https://arxiv.org/pdf/2504.15900", "abs": "https://arxiv.org/abs/2504.15900", "authors": ["Cheng Wen", "Tingwei Guo", "Shuaijiang Zhao", "Wei Zou", "Xiangang Li"], "title": "SARI: Structured Audio Reasoning via Curriculum-Guided Reinforcement Learning", "categories": ["cs.CL"], "comment": null, "summary": "Recent work shows that reinforcement learning(RL) can markedly sharpen the\nreasoning ability of large language models (LLMs) by prompting them to \"think\nbefore answering.\" Yet whether and how these gains transfer to audio-language\nreasoning remains largely unexplored. We extend the Group-Relative Policy\nOptimization (GRPO) framework from DeepSeek-R1 to a Large Audio-Language Model\n(LALM), and construct a 32k sample multiple-choice corpus. Using a two-stage\nregimen supervised fine-tuning on structured and unstructured\nchains-of-thought, followed by curriculum-guided GRPO, we systematically\ncompare implicit vs. explicit, and structured vs. free form reasoning under\nidentical architectures. Our structured audio reasoning model, SARI (Structured\nAudio Reasoning via Curriculum-Guided Reinforcement Learning), achieves a\n16.35% improvement in average accuracy over the base model\nQwen2-Audio-7B-Instruct. Furthermore, the variant built upon Qwen2.5-Omni\nreaches state-of-the-art performance of 67.08% on the MMAU test-mini benchmark.\nAblation experiments show that on the base model we use: (i) SFT warm-up is\nimportant for stable RL training, (ii) structured chains yield more robust\ngeneralization than unstructured ones, and (iii) easy-to-hard curricula\naccelerate convergence and improve final performance. These findings\ndemonstrate that explicit, structured reasoning and curriculum learning\nsubstantially enhances audio-language understanding."}
{"id": "2504.15299", "pdf": "https://arxiv.org/pdf/2504.15299", "abs": "https://arxiv.org/abs/2504.15299", "authors": ["Haodong Wang", "Qihua Zhou", "Zicong Hong", "Song Guo"], "title": "D$^{2}$MoE: Dual Routing and Dynamic Scheduling for Efficient On-Device MoE-based LLM Serving", "categories": ["cs.DC", "cs.AI"], "comment": "Accepted by MobiCom 2025", "summary": "The mixture of experts (MoE) model is a sparse variant of large language\nmodels (LLMs), designed to hold a better balance between intelligent capability\nand computational overhead. Despite its benefits, MoE is still too expensive to\ndeploy on resource-constrained edge devices, especially with the demands of\non-device inference services. Recent research efforts often apply model\ncompression techniques, such as quantization, pruning and merging, to restrict\nMoE complexity. Unfortunately, due to their predefined static model\noptimization strategies, they cannot always achieve the desired\nquality-overhead trade-off when handling multiple requests, finally degrading\nthe on-device quality of service. These limitations motivate us to propose the\nD$^2$MoE, an algorithm-system co-design framework that matches diverse task\nrequirements by dynamically allocating the most proper bit-width to each\nexpert. Specifically, inspired by the nested structure of matryoshka dolls, we\npropose the matryoshka weight quantization (MWQ) to progressively compress\nexpert weights in a bit-nested manner and reduce the required runtime memory.\nOn top of it, we further optimize the I/O-computation pipeline and design a\nheuristic scheduling algorithm following our hottest-expert-bit-first (HEBF)\nprinciple, which maximizes the expert parallelism between I/O and computation\nqueue under constrained memory budgets, thus significantly reducing the idle\ntemporal bubbles waiting for the experts to load. Evaluations on real edge\ndevices show that D$^2$MoE improves the overall inference throughput by up to\n1.39$\\times$ and reduces the peak memory footprint by up to 53% over the latest\non-device inference frameworks, while still preserving comparable serving\naccuracy as its INT8 counterparts."}
{"id": "2504.15594", "pdf": "https://arxiv.org/pdf/2504.15594", "abs": "https://arxiv.org/abs/2504.15594", "authors": ["Tatsuhito Hasegawa", "Shunsuke Sakai"], "title": "Analytical Softmax Temperature Setting from Feature Dimensions for Model- and Domain-Robust Classification", "categories": ["cs.LG", "cs.CV"], "comment": "22 pages, 11 figures, under review", "summary": "In deep learning-based classification tasks, the softmax function's\ntemperature parameter $T$ critically influences the output distribution and\noverall performance. This study presents a novel theoretical insight that the\noptimal temperature $T^*$ is uniquely determined by the dimensionality of the\nfeature representations, thereby enabling training-free determination of $T^*$.\nDespite this theoretical grounding, empirical evidence reveals that $T^*$\nfluctuates under practical conditions owing to variations in models, datasets,\nand other confounding factors. To address these influences, we propose and\noptimize a set of temperature determination coefficients that specify how $T^*$\nshould be adjusted based on the theoretical relationship to feature\ndimensionality. Additionally, we insert a batch normalization layer immediately\nbefore the output layer, effectively stabilizing the feature space. Building on\nthese coefficients and a suite of large-scale experiments, we develop an\nempirical formula to estimate $T^*$ without additional training while also\nintroducing a corrective scheme to refine $T^*$ based on the number of classes\nand task complexity. Our findings confirm that the derived temperature not only\naligns with the proposed theoretical perspective but also generalizes\neffectively across diverse tasks, consistently enhancing classification\nperformance and offering a practical, training-free solution for determining\n$T^*$."}
{"id": "2504.15681", "pdf": "https://arxiv.org/pdf/2504.15681", "abs": "https://arxiv.org/abs/2504.15681", "authors": ["Vidi Team", "Celong Liu", "Chia-Wen Kuo", "Dawei Du", "Fan Chen", "Guang Chen", "Jiamin Yuan", "Lingxi Zhang", "Lu Guo", "Lusha Li", "Longyin Wen", "Qingyu Chen", "Rachel Deng", "Sijie Zhu", "Stuart Siew", "Tong Jin", "Wei Lu", "Wen Zhong", "Xiaohui Shen", "Xin Gu", "Xing Mei", "Xueqiong Qu"], "title": "Vidi: Large Multimodal Models for Video Understanding and Editing", "categories": ["cs.CV"], "comment": null, "summary": "Humans naturally share information with those they are connected to, and\nvideo has become one of the dominant mediums for communication and expression\non the Internet. To support the creation of high-quality large-scale video\ncontent, a modern pipeline requires a comprehensive understanding of both the\nraw input materials (e.g., the unedited footage captured by cameras) and the\nediting components (e.g., visual effects). In video editing scenarios, models\nmust process multiple modalities (e.g., vision, audio, text) with strong\nbackground knowledge and handle flexible input lengths (e.g., hour-long raw\nvideos), which poses significant challenges for traditional models. In this\nreport, we introduce Vidi, a family of Large Multimodal Models (LMMs) for a\nwide range of video understand editing scenarios. The first release focuses on\ntemporal retrieval, i.e., identifying the time ranges within the input videos\ncorresponding to a given text query, which plays a critical role in intelligent\nediting. The model is capable of processing hour-long videos with strong\ntemporal understanding capability, e.g., retrieve time ranges for certain\nqueries. To support a comprehensive evaluation in real-world scenarios, we also\npresent the VUE-TR benchmark, which introduces five key advancements. 1) Video\nduration: significantly longer than existing temporal retrival datasets, 2)\nAudio support: includes audio-based queries, 3) Query format: diverse query\nlengths/formats, 4) Annotation quality: ground-truth time ranges are manually\nannotated. 5) Evaluation metric: a refined IoU metric to support evaluation\nover multiple time ranges. Remarkably, Vidi significantly outperforms leading\nproprietary models, e.g., GPT-4o and Gemini, on the temporal retrieval task,\nindicating its superiority in video editing scenarios."}
{"id": "2504.15941", "pdf": "https://arxiv.org/pdf/2504.15941", "abs": "https://arxiv.org/abs/2504.15941", "authors": ["Fanny Jourdan", "Yannick Chevalier", "Cécile Favre"], "title": "FairTranslate: An English-French Dataset for Gender Bias Evaluation in Machine Translation by Overcoming Gender Binarity", "categories": ["cs.CL", "cs.AI"], "comment": "FAccT 2025", "summary": "Large Language Models (LLMs) are increasingly leveraged for translation tasks\nbut often fall short when translating inclusive language -- such as texts\ncontaining the singular 'they' pronoun or otherwise reflecting fair linguistic\nprotocols. Because these challenges span both computational and societal\ndomains, it is imperative to critically evaluate how well LLMs handle inclusive\ntranslation with a well-founded framework.\n  This paper presents FairTranslate, a novel, fully human-annotated dataset\ndesigned to evaluate non-binary gender biases in machine translation systems\nfrom English to French. FairTranslate includes 2418 English-French sentence\npairs related to occupations, annotated with rich metadata such as the\nstereotypical alignment of the occupation, grammatical gender indicator\nambiguity, and the ground-truth gender label (male, female, or inclusive).\n  We evaluate four leading LLMs (Gemma2-2B, Mistral-7B, Llama3.1-8B,\nLlama3.3-70B) on this dataset under different prompting procedures. Our results\nreveal substantial biases in gender representation across LLMs, highlighting\npersistent challenges in achieving equitable outcomes in machine translation.\nThese findings underscore the need for focused strategies and interventions\naimed at ensuring fair and inclusive language usage in LLM-based translation\nsystems.\n  We make the FairTranslate dataset publicly available on Hugging Face, and\ndisclose the code for all experiments on GitHub."}
{"id": "2504.15303", "pdf": "https://arxiv.org/pdf/2504.15303", "abs": "https://arxiv.org/abs/2504.15303", "authors": ["Yi Xiong", "Jinqi Huang", "Wenjie Huang", "Xuebing Yu", "Entong Li", "Zhixiong Ning", "Jinhua Zhou", "Li Zeng", "Xin Chen"], "title": "High-Throughput LLM inference on Heterogeneous Clusters", "categories": ["cs.DC", "cs.AI"], "comment": null, "summary": "Nowadays, many companies possess various types of AI accelerators, forming\nheterogeneous clusters. Efficiently leveraging these clusters for\nhigh-throughput large language model (LLM) inference services can significantly\nreduce costs and expedite task processing. However, LLM inference on\nheterogeneous clusters presents two main challenges. Firstly, different\ndeployment configurations can result in vastly different performance. The\nnumber of possible configurations is large, and evaluating the effectiveness of\na specific setup is complex. Thus, finding an optimal configuration is not an\neasy task. Secondly, LLM inference instances within a heterogeneous cluster\npossess varying processing capacities, leading to different processing speeds\nfor handling inference requests. Evaluating these capacities and designing a\nrequest scheduling algorithm that fully maximizes the potential of each\ninstance is challenging. In this paper, we propose a high-throughput inference\nservice system on heterogeneous clusters. First, the deployment configuration\nis optimized by modeling the resource amount and expected throughput and using\nthe exhaustive search method. Second, a novel mechanism is proposed to schedule\nrequests among instances, which fully considers the different processing\ncapabilities of various instances. Extensive experiments show that the proposed\nscheduler improves throughput by 122.5% and 33.6% on two heterogeneous\nclusters, respectively."}
{"id": "2504.15613", "pdf": "https://arxiv.org/pdf/2504.15613", "abs": "https://arxiv.org/abs/2504.15613", "authors": ["Minglian Han"], "title": "Learning Dynamic Graphs via Tensorized and Lightweight Graph Convolutional Networks", "categories": ["cs.LG"], "comment": null, "summary": "A dynamic graph (DG) is frequently encountered in numerous real-world\nscenarios. Consequently, A dynamic graph convolutional network (DGCN) has been\nsuccessfully applied to perform precise representation learning on a DG.\nHowever, conventional DGCNs typically consist of a static GCN coupled with a\nsequence neural network (SNN) to model spatial and temporal patterns\nseparately. This decoupled modeling mechanism inherently disrupts the intricate\nspatio-temporal dependencies. To address the issue, this study proposes a novel\nTensorized Lightweight Graph Convolutional Network (TLGCN) for accurate dynamic\ngraph learning. It mainly contains the following two key concepts: a) designing\na novel spatio-temporal information propagation method for joint propagation of\nspatio-temporal information based on the tensor M-product framework; b)\nproposing a tensorized lightweight graph convolutional network based on the\nabove method, which significantly reduces the memory occupation of the model by\nomitting complex feature transformation and nonlinear activation. Numerical\nexperiments on four real-world datasets demonstrate that the proposed TLGCN\noutperforms the state-of-the-art models in the weight estimation task on DGs."}
{"id": "2504.15694", "pdf": "https://arxiv.org/pdf/2504.15694", "abs": "https://arxiv.org/abs/2504.15694", "authors": ["Jun Dong", "Wenli Wu", "Jintao Cheng", "Xiaoyu Tang"], "title": "You Sense Only Once Beneath: Ultra-Light Real-Time Underwater Object Detection", "categories": ["cs.CV"], "comment": null, "summary": "Despite the remarkable achievements in object detection, the model's accuracy\nand efficiency still require further improvement under challenging underwater\nconditions, such as low image quality and limited computational resources. To\naddress this, we propose an Ultra-Light Real-Time Underwater Object Detection\nframework, You Sense Only Once Beneath (YSOOB). Specifically, we utilize a\nMulti-Spectrum Wavelet Encoder (MSWE) to perform frequency-domain encoding on\nthe input image, minimizing the semantic loss caused by underwater optical\ncolor distortion. Furthermore, we revisit the unique characteristics of\neven-sized and transposed convolutions, allowing the model to dynamically\nselect and enhance key information during the resampling process, thereby\nimproving its generalization ability. Finally, we eliminate model redundancy\nthrough a simple yet effective channel compression and reconstructed large\nkernel convolution (RLKC) to achieve model lightweight. As a result, forms a\nhigh-performance underwater object detector YSOOB with only 1.2 million\nparameters. Extensive experimental results demonstrate that, with the fewest\nparameters, YSOOB achieves mAP50 of 83.1% and 82.9% on the URPC2020 and DUO\ndatasets, respectively, comparable to the current SOTA detectors. The inference\nspeed reaches 781.3 FPS and 57.8 FPS on the T4 GPU (TensorRT FP16) and the edge\ncomputing device Jetson Xavier NX (TensorRT FP16), surpassing YOLOv12-N by\n28.1% and 22.5%, respectively."}
{"id": "2504.15983", "pdf": "https://arxiv.org/pdf/2504.15983", "abs": "https://arxiv.org/abs/2504.15983", "authors": ["Shang Wang"], "title": "W-PCA Based Gradient-Free Proxy for Efficient Search of Lightweight Language Models", "categories": ["cs.CL", "cs.AI"], "comment": "ICLR 2025", "summary": "The demand for efficient natural language processing (NLP) systems has led to\nthe development of lightweight language models. Previous work in this area has\nprimarily focused on manual design or training-based neural architecture search\n(NAS) methods. Recently, zero-shot NAS methods have been proposed for\nevaluating language models without the need for training. However, prevailing\napproaches to zero-shot NAS often face challenges such as biased evaluation\nmetrics and computational inefficiencies. In this paper, we introduce\nweight-weighted PCA (W-PCA), a novel zero-shot NAS method specifically tailored\nfor lightweight language models. Our approach utilizes two evaluation proxies:\nthe parameter count and the number of principal components with cumulative\ncontribution exceeding $\\eta$ in the feed-forward neural (FFN) layer.\nAdditionally, by eliminating the need for gradient computations, we optimize\nthe evaluation time, thus enhancing the efficiency of designing and evaluating\nlightweight language models. We conduct a comparative analysis on the GLUE and\nSQuAD datasets to evaluate our approach. The results demonstrate that our\nmethod significantly reduces training time compared to one-shot NAS methods and\nachieves higher scores in the testing phase compared to previous\nstate-of-the-art training-based methods. Furthermore, we perform ranking\nevaluations on a dataset sampled from the FlexiBERT search space. Our approach\nexhibits superior ranking correlation and further reduces solving time compared\nto other zero-shot NAS methods that require gradient computation."}
{"id": "2504.15324", "pdf": "https://arxiv.org/pdf/2504.15324", "abs": "https://arxiv.org/abs/2504.15324", "authors": ["Vuong M. Ngo", "Edward Bolger", "Stan Goodwin", "John O'Sullivan", "Dinh Viet Cuong", "Mark Roantree"], "title": "A Graph Based Raman Spectral Processing Technique for Exosome Classification", "categories": ["q-bio.QM", "cs.AI", "cs.IT", "cs.LG", "math.IT"], "comment": "The 23rd International Conference on Artificial Intelligence in\n  Medicine (AIME 2025), LNAI, Springer, 11 pages", "summary": "Exosomes are small vesicles crucial for cell signaling and disease\nbiomarkers. Due to their complexity, an \"omics\" approach is preferable to\nindividual biomarkers. While Raman spectroscopy is effective for exosome\nanalysis, it requires high sample concentrations and has limited sensitivity to\nlipids and proteins. Surface-enhanced Raman spectroscopy helps overcome these\nchallenges. In this study, we leverage Neo4j graph databases to organize 3,045\nRaman spectra of exosomes, enhancing data generalization. To further refine\nspectral analysis, we introduce a novel spectral filtering process that\nintegrates the PageRank Filter with optimal Dimensionality Reduction. This\nmethod improves feature selection, resulting in superior classification\nperformance. Specifically, the Extra Trees model, using our spectral processing\napproach, achieves 0.76 and 0.857 accuracy in classifying hyperglycemic,\nhypoglycemic, and normal exosome samples based on Raman spectra and surface,\nrespectively, with group 10-fold cross-validation. Our results show that\ngraph-based spectral filtering combined with optimal dimensionality reduction\nsignificantly improves classification accuracy by reducing noise while\npreserving key biomarker signals. This novel framework enhances Raman-based\nexosome analysis, expanding its potential for biomedical applications, disease\ndiagnostics, and biomarker discovery."}
{"id": "2504.15615", "pdf": "https://arxiv.org/pdf/2504.15615", "abs": "https://arxiv.org/abs/2504.15615", "authors": ["Jingwu Tang", "Jiayun Wu", "Zhiwei Steven Wu", "Jiahao Zhang"], "title": "Dimension-Free Decision Calibration for Nonlinear Loss Functions", "categories": ["cs.LG", "stat.ML"], "comment": null, "summary": "When model predictions inform downstream decision making, a natural question\nis under what conditions can the decision-makers simply respond to the\npredictions as if they were the true outcomes. Calibration suffices to\nguarantee that simple best-response to predictions is optimal. However,\ncalibration for high-dimensional prediction outcome spaces requires exponential\ncomputational and statistical complexity. The recent relaxation known as\ndecision calibration ensures the optimality of the simple best-response rule\nwhile requiring only polynomial sample complexity in the dimension of outcomes.\nHowever, known results on calibration and decision calibration crucially rely\non linear loss functions for establishing best-response optimality. A natural\napproach to handle nonlinear losses is to map outcomes $y$ into a feature space\n$\\phi(y)$ of dimension $m$, then approximate losses with linear functions of\n$\\phi(y)$. Unfortunately, even simple classes of nonlinear functions can demand\nexponentially large or infinite feature dimensions $m$. A key open problem is\nwhether it is possible to achieve decision calibration with sample complexity\nindependent of~$m$. We begin with a negative result: even verifying decision\ncalibration under standard deterministic best response inherently requires\nsample complexity polynomial in~$m$. Motivated by this lower bound, we\ninvestigate a smooth version of decision calibration in which decision-makers\nfollow a smooth best-response. This smooth relaxation enables dimension-free\ndecision calibration algorithms. We introduce algorithms that, given\n$\\mathrm{poly}(|A|,1/\\epsilon)$ samples and any initial predictor~$p$, can\nefficiently post-process it to satisfy decision calibration without worsening\naccuracy. Our algorithms apply broadly to function classes that can be\nwell-approximated by bounded-norm functions in (possibly infinite-dimensional)\nseparable RKHS."}
{"id": "2504.15707", "pdf": "https://arxiv.org/pdf/2504.15707", "abs": "https://arxiv.org/abs/2504.15707", "authors": ["Yannic Neuhaus", "Matthias Hein"], "title": "RePOPE: Impact of Annotation Errors on the POPE Benchmark", "categories": ["cs.CV", "cs.AI", "cs.LG"], "comment": null, "summary": "Since data annotation is costly, benchmark datasets often incorporate labels\nfrom established image datasets. In this work, we assess the impact of label\nerrors in MSCOCO on the frequently used object hallucination benchmark POPE. We\nre-annotate the benchmark images and identify an imbalance in annotation errors\nacross different subsets. Evaluating multiple models on the revised labels,\nwhich we denote as RePOPE, we observe notable shifts in model rankings,\nhighlighting the impact of label quality. Code and data are available at\nhttps://github.com/YanNeu/RePOPE ."}
{"id": "2504.15987", "pdf": "https://arxiv.org/pdf/2504.15987", "abs": "https://arxiv.org/abs/2504.15987", "authors": ["Zhenkai Qin", "Dongze Wu", "Yuxin Liu", "Guifang Yang"], "title": "Few-shot Hate Speech Detection Based on the MindSpore Framework", "categories": ["cs.CL", "cs.CY"], "comment": null, "summary": "The proliferation of hate speech on social media poses a significant threat\nto online communities, requiring effective detection systems. While deep\nlearning models have shown promise, their performance often deteriorates in\nfew-shot or low-resource settings due to reliance on large annotated corpora.\nTo address this, we propose MS-FSLHate, a prompt-enhanced neural framework for\nfew-shot hate speech detection implemented on the MindSpore deep learning\nplatform. The model integrates learnable prompt embeddings, a CNN-BiLSTM\nbackbone with attention pooling, and synonym-based adversarial data\naugmentation to improve generalization. Experimental results on two benchmark\ndatasets-HateXplain and HSOL-demonstrate that our approach outperforms\ncompetitive baselines in precision, recall, and F1-score. Additionally, the\nframework shows high efficiency and scalability, suggesting its suitability for\ndeployment in resource-constrained environments. These findings highlight the\npotential of combining prompt-based learning with adversarial augmentation for\nrobust and adaptable hate speech detection in few-shot scenarios."}
{"id": "2504.15330", "pdf": "https://arxiv.org/pdf/2504.15330", "abs": "https://arxiv.org/abs/2504.15330", "authors": ["Mohit Gupta", "Akiko Aizawa", "Rajiv Ratn Shah"], "title": "Med-CoDE: Medical Critique based Disagreement Evaluation Framework", "categories": ["cs.IR", "cs.AI", "cs.CL"], "comment": "8 pages, 4 figures, NAACL SRW 2025", "summary": "The emergence of large language models (LLMs) has significantly influenced\nnumerous fields, including healthcare, by enhancing the capabilities of\nautomated systems to process and generate human-like text. However, despite\ntheir advancements, the reliability and accuracy of LLMs in medical contexts\nremain critical concerns. Current evaluation methods often lack robustness and\nfail to provide a comprehensive assessment of LLM performance, leading to\npotential risks in clinical settings. In this work, we propose Med-CoDE, a\nspecifically designed evaluation framework for medical LLMs to address these\nchallenges. The framework leverages a critique-based approach to quantitatively\nmeasure the degree of disagreement between model-generated responses and\nestablished medical ground truths. This framework captures both accuracy and\nreliability in medical settings. The proposed evaluation framework aims to fill\nthe existing gap in LLM assessment by offering a systematic method to evaluate\nthe quality and trustworthiness of medical LLMs. Through extensive experiments\nand case studies, we illustrate the practicality of our framework in providing\na comprehensive and reliable evaluation of medical LLMs."}
{"id": "2504.15616", "pdf": "https://arxiv.org/pdf/2504.15616", "abs": "https://arxiv.org/abs/2504.15616", "authors": ["Kai Chen", "Xiaodong Zhao", "Yujie Huang", "Guoyu Fang", "Xiao Song", "Ruiping Wang", "Ziyuan Wang"], "title": "SocialMOIF: Multi-Order Intention Fusion for Pedestrian Trajectory Prediction", "categories": ["cs.LG", "cs.CV"], "comment": "11 pages,6 figures", "summary": "The analysis and prediction of agent trajectories are crucial for\ndecision-making processes in intelligent systems, with precise short-term\ntrajectory forecasting being highly significant across a range of applications.\nAgents and their social interactions have been quantified and modeled by\nresearchers from various perspectives; however, substantial limitations exist\nin the current work due to the inherent high uncertainty of agent intentions\nand the complex higher-order influences among neighboring groups. SocialMOIF is\nproposed to tackle these challenges, concentrating on the higher-order\nintention interactions among neighboring groups while reinforcing the primary\nrole of first-order intention interactions between neighbors and the target\nagent. This method develops a multi-order intention fusion model to achieve a\nmore comprehensive understanding of both direct and indirect intention\ninformation. Within SocialMOIF, a trajectory distribution approximator is\ndesigned to guide the trajectories toward values that align more closely with\nthe actual data, thereby enhancing model interpretability. Furthermore, a\nglobal trajectory optimizer is introduced to enable more accurate and efficient\nparallel predictions. By incorporating a novel loss function that accounts for\ndistance and direction during training, experimental results demonstrate that\nthe model outperforms previous state-of-the-art baselines across multiple\nmetrics in both dynamic and static datasets."}
{"id": "2504.15723", "pdf": "https://arxiv.org/pdf/2504.15723", "abs": "https://arxiv.org/abs/2504.15723", "authors": ["Dasol Jeong", "Donggoo Kang", "Jiwon Park", "Hyebean Lee", "Joonki Paik"], "title": "Structure-Preserving Zero-Shot Image Editing via Stage-Wise Latent Injection in Diffusion Models", "categories": ["cs.CV"], "comment": null, "summary": "We propose a diffusion-based framework for zero-shot image editing that\nunifies text-guided and reference-guided approaches without requiring\nfine-tuning. Our method leverages diffusion inversion and timestep-specific\nnull-text embeddings to preserve the structural integrity of the source image.\nBy introducing a stage-wise latent injection strategy-shape injection in early\nsteps and attribute injection in later steps-we enable precise, fine-grained\nmodifications while maintaining global consistency. Cross-attention with\nreference latents facilitates semantic alignment between the source and\nreference. Extensive experiments across expression transfer, texture\ntransformation, and style infusion demonstrate state-of-the-art performance,\nconfirming the method's scalability and adaptability to diverse image editing\nscenarios."}
{"id": "2504.16005", "pdf": "https://arxiv.org/pdf/2504.16005", "abs": "https://arxiv.org/abs/2504.16005", "authors": ["Tom Zehle", "Moritz Schlager", "Timo Heiß", "Matthias Feurer"], "title": "CAPO: Cost-Aware Prompt Optimization", "categories": ["cs.CL", "cs.AI", "cs.NE", "stat.ML"], "comment": "Submitted to AutoML 2025", "summary": "Large language models (LLMs) have revolutionized natural language processing\nby solving a wide range of tasks simply guided by a prompt. Yet their\nperformance is highly sensitive to prompt formulation. While automated prompt\noptimization addresses this challenge by finding optimal prompts, current\nmethods require a substantial number of LLM calls and input tokens, making\nprompt optimization expensive. We introduce CAPO (Cost-Aware Prompt\nOptimization), an algorithm that enhances prompt optimization efficiency by\nintegrating AutoML techniques. CAPO is an evolutionary approach with LLMs as\noperators, incorporating racing to save evaluations and multi-objective\noptimization to balance performance with prompt length. It jointly optimizes\ninstructions and few-shot examples while leveraging task descriptions for\nimproved robustness. Our extensive experiments across diverse datasets and LLMs\ndemonstrate that CAPO outperforms state-of-the-art discrete prompt optimization\nmethods in 11/15 cases with improvements up to 21%p. Our algorithm achieves\nbetter performances already with smaller budgets, saves evaluations through\nracing, and decreases average prompt length via a length penalty, making it\nboth cost-efficient and cost-aware. Even without few-shot examples, CAPO\noutperforms its competitors and generally remains robust to initial prompts.\nCAPO represents an important step toward making prompt optimization more\npowerful and accessible by improving cost-efficiency."}
{"id": "2504.15417", "pdf": "https://arxiv.org/pdf/2504.15417", "abs": "https://arxiv.org/abs/2504.15417", "authors": ["Van-Giang Trinh", "Belaid Benhamou", "Sylvain Soliman", "François Fages"], "title": "On the Boolean Network Theory of Datalog$^\\neg$", "categories": ["cs.LO", "cs.AI"], "comment": "48 pages, 7 figures", "summary": "Datalog$^\\neg$ is a central formalism used in a variety of domains ranging\nfrom deductive databases and abstract argumentation frameworks to answer set\nprogramming. Its model theory is the finite counterpart of the logical\nsemantics developed for normal logic programs, mainly based on the notions of\nClark's completion and two-valued or three-valued canonical models including\nsupported, stable, regular and well-founded models. In this paper we establish\na formal link between Datalog$^\\neg$ and Boolean network theory, which was\ninitially introduced by Stuart Kaufman and Ren\\'e Thomas to reason about gene\nregulatory networks. We use previous results from Boolean network theory to\nprove that in the absence of odd cycles in a Datalog$^\\neg$ program, the\nregular models coincide with the stable models, which entails the existence of\nstable models, and in the absence of even cycles, we show the uniqueness of\nstable partial models, which entails the uniqueness of regular models. These\nresults on regular models have been claimed by You and Yuan in 1994 for normal\nlogic programs but we show problems in their definition of well-founded\nstratification and in their proofs that we can fix for negative normal logic\nprograms only. We also give upper bounds on the numbers of stable partial,\nregular, and stable models of a Datalog$^\\neg$ program using the cardinality of\na feedback vertex set in its atom dependency graph. Interestingly, our\nconnection to Boolean network theory also points us to the notion of trap\nspaces for Datalog$^\\neg$ programs. We relate the notions of supported or\nstable trap spaces to the other semantics of Datalog$^\\neg$, and show the\nequivalence between subset-minimal stable trap spaces and regular models."}
{"id": "2504.15623", "pdf": "https://arxiv.org/pdf/2504.15623", "abs": "https://arxiv.org/abs/2504.15623", "authors": ["Xiucheng Wang", "Qiming Zhang", "Nan Cheng", "Ruijin Sun", "Zan Li", "Shuguang Cui", "Xuemin Shen"], "title": "RadioDiff-$k^2$: Helmholtz Equation Informed Generative Diffusion Model for Multi-Path Aware Radio Map Construction", "categories": ["cs.LG", "cs.SY", "eess.SY"], "comment": null, "summary": "In this paper, we propose a novel physics-informed generative learning\napproach, termed RadioDiff-$\\bm{k^2}$, for accurate and efficient\nmultipath-aware radio map (RM) construction. As wireless communication evolves\ntowards environment-aware paradigms, driven by the increasing demand for\nintelligent and proactive optimization in sixth-generation (6G) networks,\naccurate construction of RMs becomes crucial yet highly challenging.\nConventional electromagnetic (EM)-based methods, such as full-wave solvers and\nray-tracing approaches, exhibit substantial computational overhead and limited\nadaptability to dynamic scenarios. Although, existing neural network (NN)\napproaches have efficient inferencing speed, they lack sufficient consideration\nof the underlying physics of EM wave propagation, limiting their effectiveness\nin accurately modeling critical EM singularities induced by complex multipath\nenvironments. To address these fundamental limitations, we propose a novel\nphysics-inspired RM construction method guided explicitly by the Helmholtz\nequation, which inherently governs EM wave propagation. Specifically, we\ntheoretically establish a direct correspondence between EM singularities, which\ncorrespond to the critical spatial features influencing wireless propagation,\nand regions defined by negative wave numbers in the Helmholtz equation. Based\non this insight, we design an innovative dual generative diffusion model (DM)\nframework comprising one DM dedicated to accurately inferring EM singularities\nand another DM responsible for reconstructing the complete RM using these\nsingularities along with environmental contextual information. Our\nphysics-informed approach uniquely combines the efficiency advantages of\ndata-driven methods with rigorous physics-based EM modeling, significantly\nenhancing RM accuracy, particularly in complex propagation environments\ndominated by multipath effects."}
{"id": "2504.15728", "pdf": "https://arxiv.org/pdf/2504.15728", "abs": "https://arxiv.org/abs/2504.15728", "authors": ["Manjunath D", "Aniruddh Sikdar", "Prajwal Gurunath", "Sumanth Udupa", "Suresh Sundaram"], "title": "SAGA: Semantic-Aware Gray color Augmentation for Visible-to-Thermal Domain Adaptation across Multi-View Drone and Ground-Based Vision Systems", "categories": ["cs.CV"], "comment": "Accepted at CVPR-W PBVS 2025", "summary": "Domain-adaptive thermal object detection plays a key role in facilitating\nvisible (RGB)-to-thermal (IR) adaptation by reducing the need for co-registered\nimage pairs and minimizing reliance on large annotated IR datasets. However,\ninherent limitations of IR images, such as the lack of color and texture cues,\npose challenges for RGB-trained models, leading to increased false positives\nand poor-quality pseudo-labels. To address this, we propose Semantic-Aware Gray\ncolor Augmentation (SAGA), a novel strategy for mitigating color bias and\nbridging the domain gap by extracting object-level features relevant to IR\nimages. Additionally, to validate the proposed SAGA for drone imagery, we\nintroduce the IndraEye, a multi-sensor (RGB-IR) dataset designed for diverse\napplications. The dataset contains 5,612 images with 145,666 instances,\ncaptured from diverse angles, altitudes, backgrounds, and times of day,\noffering valuable opportunities for multimodal learning, domain adaptation for\nobject detection and segmentation, and exploration of sensor-specific strengths\nand weaknesses. IndraEye aims to enhance the development of more robust and\naccurate aerial perception systems, especially in challenging environments.\nExperimental results show that SAGA significantly improves RGB-to-IR adaptation\nfor autonomous driving and IndraEye dataset, achieving consistent performance\ngains of +0.4% to +7.6% (mAP) when integrated with state-of-the-art domain\nadaptation techniques. The dataset and codes are available at\nhttps://github.com/airliisc/IndraEye."}
{"id": "2504.16007", "pdf": "https://arxiv.org/pdf/2504.16007", "abs": "https://arxiv.org/abs/2504.16007", "authors": ["Igor Rozhkov", "Natalia Loukachevitch"], "title": "Methods for Recognizing Nested Terms", "categories": ["cs.CL"], "comment": "To be published in Computational Linguistics and Intellectual\n  Technologies proceedings", "summary": "In this paper, we describe our participation in the RuTermEval competition\ndevoted to extracting nested terms. We apply the Binder model, which was\npreviously successfully applied to the recognition of nested named entities, to\nextract nested terms. We obtained the best results of term recognition in all\nthree tracks of the RuTermEval competition. In addition, we study the new task\nof recognition of nested terms from flat training data annotated with terms\nwithout nestedness. We can conclude that several approaches we proposed in this\nwork are viable enough to retrieve nested terms effectively without nested\nlabeling of them."}
{"id": "2504.15424", "pdf": "https://arxiv.org/pdf/2504.15424", "abs": "https://arxiv.org/abs/2504.15424", "authors": ["Nishath Rajiv Ranasinghe", "Shawn M. Jones", "Michal Kucer", "Ayan Biswas", "Daniel O'Malley", "Alexander Buschmann Most", "Selma Liliane Wanna", "Ajay Sreekumar"], "title": "LLM-Assisted Translation of Legacy FORTRAN Codes to C++: A Cross-Platform Study", "categories": ["cs.SE", "cs.AI", "I.2.2; I.2.7; D.2.3; D.2.4"], "comment": "12 pages, 7 figures, 2 tables", "summary": "Large Language Models (LLMs) are increasingly being leveraged for generating\nand translating scientific computer codes by both domain-experts and non-domain\nexperts. Fortran has served as one of the go to programming languages in legacy\nhigh-performance computing (HPC) for scientific discoveries. Despite growing\nadoption, LLM-based code translation of legacy code-bases has not been\nthoroughly assessed or quantified for its usability. Here, we studied the\napplicability of LLM-based translation of Fortran to C++ as a step towards\nbuilding an agentic-workflow using open-weight LLMs on two different\ncomputational platforms. We statistically quantified the compilation accuracy\nof the translated C++ codes, measured the similarity of the LLM translated code\nto the human translated C++ code, and statistically quantified the output\nsimilarity of the Fortran to C++ translation."}
{"id": "2504.15634", "pdf": "https://arxiv.org/pdf/2504.15634", "abs": "https://arxiv.org/abs/2504.15634", "authors": ["Peizheng Liu", "Hitoshi Iba"], "title": "Enhancing Reinforcement learning in 3-Dimensional Hydrophobic-Polar Protein Folding Model with Attention-based layers", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Transformer-based architectures have recently propelled advances in sequence\nmodeling across domains, but their application to the hydrophobic-hydrophilic\n(H-P) model for protein folding remains relatively unexplored. In this work, we\nadapt a Deep Q-Network (DQN) integrated with attention mechanisms\n(Transformers) to address the 3D H-P protein folding problem. Our system\nformulates folding decisions as a self-avoiding walk in a reinforced\nenvironment, and employs a specialized reward function based on favorable\nhydrophobic interactions. To improve performance, the method incorporates\nvalidity check including symmetry-breaking constraints, dueling and double\nQ-learning, and prioritized replay to focus learning on critical transitions.\nExperimental evaluations on standard benchmark sequences demonstrate that our\napproach achieves several known best solutions for shorter sequences, and\nobtains near-optimal results for longer chains. This study underscores the\npromise of attention-based reinforcement learning for protein folding, and\ncreated a prototype of Transformer-based Q-network structure for 3-dimensional\nlattice models."}
{"id": "2504.15751", "pdf": "https://arxiv.org/pdf/2504.15751", "abs": "https://arxiv.org/abs/2504.15751", "authors": ["Menan Velayuthan", "Asiri Gawesha", "Purushoth Velayuthan", "Nuwan Kodagoda", "Dharshana Kasthurirathna", "Pradeepa Samarasinghe"], "title": "GADS: A Super Lightweight Model for Head Pose Estimation", "categories": ["cs.CV"], "comment": "16 pages, 5 tables, 10 figures, not submitted to any conference or\n  journal", "summary": "In human-computer interaction, head pose estimation profoundly influences\napplication functionality. Although utilizing facial landmarks is valuable for\nthis purpose, existing landmark-based methods prioritize precision over\nsimplicity and model size, limiting their deployment on edge devices and in\ncompute-poor environments. To bridge this gap, we propose \\textbf{Grouped\nAttention Deep Sets (GADS)}, a novel architecture based on the Deep Set\nframework. By grouping landmarks into regions and employing small Deep Set\nlayers, we reduce computational complexity. Our multihead attention mechanism\nextracts and combines inter-group information, resulting in a model that is\n$7.5\\times$ smaller and executes $25\\times$ faster than the current lightest\nstate-of-the-art model. Notably, our method achieves an impressive reduction,\nbeing $4321\\times$ smaller than the best-performing model. We introduce vanilla\nGADS and Hybrid-GADS (landmarks + RGB) and evaluate our models on three\nbenchmark datasets -- AFLW2000, BIWI, and 300W-LP. We envision our architecture\nas a robust baseline for resource-constrained head pose estimation methods."}
{"id": "2504.16046", "pdf": "https://arxiv.org/pdf/2504.16046", "abs": "https://arxiv.org/abs/2504.16046", "authors": ["Jingyu Zhang", "Jiacan Yu", "Marc Marone", "Benjamin Van Durme", "Daniel Khashabi"], "title": "Certified Mitigation of Worst-Case LLM Copyright Infringement", "categories": ["cs.CL"], "comment": null, "summary": "The exposure of large language models (LLMs) to copyrighted material during\npre-training raises concerns about unintentional copyright infringement post\ndeployment. This has driven the development of \"copyright takedown\" methods,\npost-training approaches aimed at preventing models from generating content\nsubstantially similar to copyrighted ones. While current mitigation approaches\nare somewhat effective for average-case risks, we demonstrate that they\noverlook worst-case copyright risks exhibits by the existence of long, verbatim\nquotes from copyrighted sources. We propose BloomScrub, a remarkably simple yet\nhighly effective inference-time approach that provides certified copyright\ntakedown. Our method repeatedly interleaves quote detection with rewriting\ntechniques to transform potentially infringing segments. By leveraging\nefficient data sketches (Bloom filters), our approach enables scalable\ncopyright screening even for large-scale real-world corpora. When quotes beyond\na length threshold cannot be removed, the system can abstain from responding,\noffering certified risk reduction. Experimental results show that BloomScrub\nreduces infringement risk, preserves utility, and accommodates different levels\nof enforcement stringency with adaptive abstention. Our results suggest that\nlightweight, inference-time methods can be surprisingly effective for copyright\nprevention."}
{"id": "2504.15440", "pdf": "https://arxiv.org/pdf/2504.15440", "abs": "https://arxiv.org/abs/2504.15440", "authors": ["Andrey Fradkin"], "title": "Demand for LLMs: Descriptive Evidence on Substitution, Market Expansion, and Multihoming", "categories": ["cs.CY", "cs.AI", "econ.GN", "q-fin.EC", "K.4; I.2"], "comment": null, "summary": "This paper documents three stylized facts about the demand for Large Language\nModels (LLMs) using data from OpenRouter, a prominent LLM marketplace. First,\nnew models experience rapid initial adoption that stabilizes within weeks.\nSecond, model releases differ substantially in whether they primarily attract\nnew users or substitute demand from competing models. Third, multihoming, using\nmultiple models simultaneously, is common among apps. These findings suggest\nsignificant horizontal and vertical differentiation in the LLM market, implying\nopportunities for providers to maintain demand and pricing power despite rapid\ntechnological advances."}
{"id": "2504.15664", "pdf": "https://arxiv.org/pdf/2504.15664", "abs": "https://arxiv.org/abs/2504.15664", "authors": ["Phuong Quynh Le", "Jörg Schlötterer", "Christin Seifert"], "title": "An XAI-based Analysis of Shortcut Learning in Neural Networks", "categories": ["cs.LG", "cs.CV"], "comment": "Accepted at The World Conference on eXplainable Artificial\n  Intelligence 2025 (XAI-2025)", "summary": "Machine learning models tend to learn spurious features - features that\nstrongly correlate with target labels but are not causal. Existing approaches\nto mitigate models' dependence on spurious features work in some cases, but\nfail in others. In this paper, we systematically analyze how and where neural\nnetworks encode spurious correlations. We introduce the neuron spurious score,\nan XAI-based diagnostic measure to quantify a neuron's dependence on spurious\nfeatures. We analyze both convolutional neural networks (CNNs) and vision\ntransformers (ViTs) using architecture-specific methods. Our results show that\nspurious features are partially disentangled, but the degree of disentanglement\nvaries across model architectures. Furthermore, we find that the assumptions\nbehind existing mitigation methods are incomplete. Our results lay the\ngroundwork for the development of novel methods to mitigate spurious\ncorrelations and make AI models safer to use in practice."}
{"id": "2504.15770", "pdf": "https://arxiv.org/pdf/2504.15770", "abs": "https://arxiv.org/abs/2504.15770", "authors": ["Lei Xu", "Mehmet Yamac", "Mete Ahishali", "Moncef Gabbouj"], "title": "Multi-Scale Tensorial Summation and Dimensional Reduction Guided Neural Network for Edge Detection", "categories": ["cs.CV"], "comment": null, "summary": "Edge detection has attracted considerable attention thanks to its exceptional\nability to enhance performance in downstream computer vision tasks. In recent\nyears, various deep learning methods have been explored for edge detection\ntasks resulting in a significant performance improvement compared to\nconventional computer vision algorithms. In neural networks, edge detection\ntasks require considerably large receptive fields to provide satisfactory\nperformance. In a typical convolutional operation, such a large receptive field\ncan be achieved by utilizing a significant number of consecutive layers, which\nyields deep network structures. Recently, a Multi-scale Tensorial Summation\n(MTS) factorization operator was presented, which can achieve very large\nreceptive fields even from the initial layers. In this paper, we propose a\nnovel MTS Dimensional Reduction (MTS-DR) module guided neural network,\nMTS-DR-Net, for the edge detection task. The MTS-DR-Net uses MTS layers, and\ncorresponding MTS-DR blocks as a new backbone to remove redundant information\ninitially. Such a dimensional reduction module enables the neural network to\nfocus specifically on relevant information (i.e., necessary subspaces).\nFinally, a weight U-shaped refinement module follows MTS-DR blocks in the\nMTS-DR-Net. We conducted extensive experiments on two benchmark edge detection\ndatasets: BSDS500 and BIPEDv2 to verify the effectiveness of our model. The\nimplementation of the proposed MTS-DR-Net can be found at\nhttps://github.com/LeiXuAI/MTS-DR-Net.git."}
{"id": "2504.16053", "pdf": "https://arxiv.org/pdf/2504.16053", "abs": "https://arxiv.org/abs/2504.16053", "authors": ["Zhifan Ye", "Kejing Xia", "Yonggan Fu", "Xin Dong", "Jihoon Hong", "Xiangchi Yuan", "Shizhe Diao", "Jan Kautz", "Pavlo Molchanov", "Yingyan Celine Lin"], "title": "LongMamba: Enhancing Mamba's Long Context Capabilities via Training-Free Receptive Field Enlargement", "categories": ["cs.CL", "cs.AI"], "comment": "Accepted by ICLR 2025", "summary": "State space models (SSMs) have emerged as an efficient alternative to\nTransformer models for language modeling, offering linear computational\ncomplexity and constant memory usage as context length increases. However,\ndespite their efficiency in handling long contexts, recent studies have shown\nthat SSMs, such as Mamba models, generally underperform compared to\nTransformers in long-context understanding tasks. To address this significant\nshortfall and achieve both efficient and accurate long-context understanding,\nwe propose LongMamba, a training-free technique that significantly enhances the\nlong-context capabilities of Mamba models. LongMamba builds on our discovery\nthat the hidden channels in Mamba can be categorized into local and global\nchannels based on their receptive field lengths, with global channels primarily\nresponsible for long-context capability. These global channels can become the\nkey bottleneck as the input context lengthens. Specifically, when input lengths\nlargely exceed the training sequence length, global channels exhibit\nlimitations in adaptively extend their receptive fields, leading to Mamba's\npoor long-context performance. The key idea of LongMamba is to mitigate the\nhidden state memory decay in these global channels by preventing the\naccumulation of unimportant tokens in their memory. This is achieved by first\nidentifying critical tokens in the global channels and then applying token\nfiltering to accumulate only those critical tokens. Through extensive\nbenchmarking across synthetic and real-world long-context scenarios, LongMamba\nsets a new standard for Mamba's long-context performance, significantly\nextending its operational range without requiring additional training. Our code\nis available at https://github.com/GATECH-EIC/LongMamba."}
{"id": "2504.15497", "pdf": "https://arxiv.org/pdf/2504.15497", "abs": "https://arxiv.org/abs/2504.15497", "authors": ["Noah Subedar", "Taeui Kim", "Saathwick Venkataramalingam"], "title": "Scalable APT Malware Classification via Parallel Feature Extraction and GPU-Accelerated Learning", "categories": ["cs.CR", "cs.AI", "I.2.0; I.2.6; K.6.5"], "comment": "26 pages, 54 figures, 14 tables", "summary": "This paper presents an underlying framework for both automating and\naccelerating malware classification, more specifically, mapping malicious\nexecutables to known Advanced Persistent Threat (APT) groups. The main feature\nof this analysis is the assembly-level instructions present in executables\nwhich are also known as opcodes. The collection of such opcodes on many\nmalicious samples is a lengthy process; hence, open-source reverse engineering\ntools are used in tandem with scripts that leverage parallel computing to\nanalyze multiple files at once. Traditional and deep learning models are\napplied to create models capable of classifying malware samples. One-gram and\ntwo-gram datasets are constructed and used to train models such as SVM, KNN,\nand Decision Tree; however, they struggle to provide adequate results without\nrelying on metadata to support n-gram sequences. The computational limitations\nof such models are overcome with convolutional neural networks (CNNs) and\nheavily accelerated using graphical compute unit (GPU) resources."}
{"id": "2504.15686", "pdf": "https://arxiv.org/pdf/2504.15686", "abs": "https://arxiv.org/abs/2504.15686", "authors": ["Phuong Quynh Le", "Christin Seifert", "Jörg Schlötterer"], "title": "Invariant Learning with Annotation-free Environments", "categories": ["cs.LG"], "comment": "Accepted at NeurIPS 2024 Workshop UniReps", "summary": "Invariant learning is a promising approach to improve domain generalization\ncompared to Empirical Risk Minimization (ERM). However, most invariant learning\nmethods rely on the assumption that training examples are pre-partitioned into\ndifferent known environments. We instead infer environments without the need\nfor additional annotations, motivated by observations of the properties within\nthe representation space of a trained ERM model. We show the preliminary\neffectiveness of our approach on the ColoredMNIST benchmark, achieving\nperformance comparable to methods requiring explicit environment labels and on\npar with an annotation-free method that poses strong restrictions on the ERM\nreference model."}
{"id": "2504.15776", "pdf": "https://arxiv.org/pdf/2504.15776", "abs": "https://arxiv.org/abs/2504.15776", "authors": ["Quentin Herau", "Nathan Piasco", "Moussab Bennehar", "Luis Rolado", "Dzmitry Tsishkou", "Bingbing Liu", "Cyrille Migniot", "Pascal Vasseur", "Cédric Demonceaux"], "title": "Pose Optimization for Autonomous Driving Datasets using Neural Rendering Models", "categories": ["cs.CV", "cs.RO"], "comment": "under review", "summary": "Autonomous driving systems rely on accurate perception and localization of\nthe ego car to ensure safety and reliability in challenging real-world driving\nscenarios. Public datasets play a vital role in benchmarking and guiding\nadvancement in research by providing standardized resources for model\ndevelopment and evaluation. However, potential inaccuracies in sensor\ncalibration and vehicle poses within these datasets can lead to erroneous\nevaluations of downstream tasks, adversely impacting the reliability and\nperformance of the autonomous systems. To address this challenge, we propose a\nrobust optimization method based on Neural Radiance Fields (NeRF) to refine\nsensor poses and calibration parameters, enhancing the integrity of dataset\nbenchmarks. To validate improvement in accuracy of our optimized poses without\nground truth, we present a thorough evaluation process, relying on reprojection\nmetrics, Novel View Synthesis rendering quality, and geometric alignment. We\ndemonstrate that our method achieves significant improvements in sensor pose\naccuracy. By optimizing these critical parameters, our approach not only\nimproves the utility of existing datasets but also paves the way for more\nreliable autonomous driving models. To foster continued progress in this field,\nwe make the optimized sensor poses publicly available, providing a valuable\nresource for the research community."}
{"id": "2504.16056", "pdf": "https://arxiv.org/pdf/2504.16056", "abs": "https://arxiv.org/abs/2504.16056", "authors": ["Daniel Hendriks", "Philipp Spitzer", "Niklas Kühl", "Gerhard Satzger"], "title": "Honey, I Shrunk the Language Model: Impact of Knowledge Distillation Methods on Performance and Explainability", "categories": ["cs.CL"], "comment": null, "summary": "Artificial Intelligence (AI) has increasingly influenced modern society,\nrecently in particular through significant advancements in Large Language\nModels (LLMs). However, high computational and storage demands of LLMs still\nlimit their deployment in resource-constrained environments. Knowledge\ndistillation addresses this challenge by training a small student model from a\nlarger teacher model. Previous research has introduced several distillation\nmethods for both generating training data and for training the student model.\nDespite their relevance, the effects of state-of-the-art distillation methods\non model performance and explainability have not been thoroughly investigated\nand compared. In this work, we enlarge the set of available methods by applying\ncritique-revision prompting to distillation for data generation and by\nsynthesizing existing methods for training. For these methods, we provide a\nsystematic comparison based on the widely used Commonsense Question-Answering\n(CQA) dataset. While we measure performance via student model accuracy, we\nemploy a human-grounded study to evaluate explainability. We contribute new\ndistillation methods and their comparison in terms of both performance and\nexplainability. This should further advance the distillation of small language\nmodels and, thus, contribute to broader applicability and faster diffusion of\nLLM technology."}
{"id": "2504.15499", "pdf": "https://arxiv.org/pdf/2504.15499", "abs": "https://arxiv.org/abs/2504.15499", "authors": ["James Mickens", "Sarah Radway", "Ravi Netravali"], "title": "Guillotine: Hypervisors for Isolating Malicious AIs", "categories": ["cs.CR", "cs.AI", "cs.OS"], "comment": "To be published in the ACM SIGOPS 2025 Workshop on Hot Topics in\n  Operating Systems", "summary": "As AI models become more embedded in critical sectors like finance,\nhealthcare, and the military, their inscrutable behavior poses ever-greater\nrisks to society. To mitigate this risk, we propose Guillotine, a hypervisor\narchitecture for sandboxing powerful AI models -- models that, by accident or\nmalice, can generate existential threats to humanity. Although Guillotine\nborrows some well-known virtualization techniques, Guillotine must also\nintroduce fundamentally new isolation mechanisms to handle the unique threat\nmodel posed by existential-risk AIs. For example, a rogue AI may try to\nintrospect upon hypervisor software or the underlying hardware substrate to\nenable later subversion of that control plane; thus, a Guillotine hypervisor\nrequires careful co-design of the hypervisor software and the CPUs, RAM, NIC,\nand storage devices that support the hypervisor software, to thwart side\nchannel leakage and more generally eliminate mechanisms for AI to exploit\nreflection-based vulnerabilities. Beyond such isolation at the software,\nnetwork, and microarchitectural layers, a Guillotine hypervisor must also\nprovide physical fail-safes more commonly associated with nuclear power plants,\navionic platforms, and other types of mission critical systems. Physical\nfail-safes, e.g., involving electromechanical disconnection of network cables,\nor the flooding of a datacenter which holds a rogue AI, provide defense in\ndepth if software, network, and microarchitectural isolation is compromised and\na rogue AI must be temporarily shut down or permanently destroyed."}
{"id": "2504.15736", "pdf": "https://arxiv.org/pdf/2504.15736", "abs": "https://arxiv.org/abs/2504.15736", "authors": ["Jiawen Wu", "Bingguang Chen", "Yuyi Zhou", "Qi Meng", "Rongchan Zhu", "Zhi-Ming Ma"], "title": "Riemannian Neural Geodesic Interpolant", "categories": ["cs.LG", "stat.ML"], "comment": null, "summary": "Stochastic interpolants are efficient generative models that bridge two\narbitrary probability density functions in finite time, enabling flexible\ngeneration from the source to the target distribution or vice versa. These\nmodels are primarily developed in Euclidean space, and are therefore limited in\ntheir application to many distribution learning problems defined on Riemannian\nmanifolds in real-world scenarios. In this work, we introduce the Riemannian\nNeural Geodesic Interpolant (RNGI) model, which interpolates between two\nprobability densities on a Riemannian manifold along the stochastic geodesics,\nand then samples from one endpoint as the final state using the continuous flow\noriginating from the other endpoint. We prove that the temporal marginal\ndensity of RNGI solves a transport equation on the Riemannian manifold. After\ntraining the model's the neural velocity and score fields, we propose the\nEmbedding Stochastic Differential Equation (E-SDE) algorithm for stochastic\nsampling of RNGI. E-SDE significantly improves the sampling quality by reducing\nthe accumulated error caused by the excessive intrinsic discretization of\nRiemannian Brownian motion in the classical Geodesic Random Walk (GRW)\nalgorithm. We also provide theoretical bounds on the generative bias measured\nin terms of KL-divergence. Finally, we demonstrate the effectiveness of the\nproposed RNGI and E-SDE through experiments conducted on both collected and\nsynthetic distributions on S2 and SO(3)."}
{"id": "2504.15782", "pdf": "https://arxiv.org/pdf/2504.15782", "abs": "https://arxiv.org/abs/2504.15782", "authors": ["Daniele Baieri", "Riccardo Cicciarella", "Michael Krützen", "Emanuele Rodolà", "Silvia Zuffi"], "title": "Model-based Metric 3D Shape and Motion Reconstruction of Wild Bottlenose Dolphins in Drone-Shot Videos", "categories": ["cs.CV", "cs.GR", "I.4.8; J.3"], "comment": "9 pages, 7 figures", "summary": "We address the problem of estimating the metric 3D shape and motion of wild\ndolphins from monocular video, with the aim of assessing their body condition.\nWhile considerable progress has been made in reconstructing 3D models of\nterrestrial quadrupeds, aquatic animals remain unexplored due to the difficulty\nof observing them in their natural underwater environment. To address this, we\npropose a model-based approach that incorporates a transmission model to\naccount for water-induced occlusion. We apply our method to video captured\nunder different sea conditions. We estimate mass and volume, and compare our\nresults to a manual 2D measurements-based method."}
{"id": "2504.16060", "pdf": "https://arxiv.org/pdf/2504.16060", "abs": "https://arxiv.org/abs/2504.16060", "authors": ["Ziqiao Ma", "Jing Ding", "Xuejun Zhang", "Dezhi Luo", "Jiahe Ding", "Sihan Xu", "Yuchen Huang", "Run Peng", "Joyce Chai"], "title": "Vision-Language Models Are Not Pragmatically Competent in Referring Expression Generation", "categories": ["cs.CL"], "comment": "Homepage: https://vlm-reg.github.io/", "summary": "Referring Expression Generation (REG) is a core task for evaluating the\npragmatic competence of vision-language systems, requiring not only accurate\nsemantic grounding but also adherence to principles of cooperative\ncommunication (Grice, 1975). However, current evaluations of vision-language\nmodels (VLMs) often overlook the pragmatic dimension, reducing REG to a\nregion-based captioning task and neglecting Gricean maxims. In this work, we\nrevisit REG from a pragmatic perspective, introducing a new dataset (RefOI) of\n1.5k images annotated with both written and spoken referring expressions.\nThrough a systematic evaluation of state-of-the-art VLMs, we identify three key\nfailures of pragmatic competence: (1) failure to uniquely identify the\nreferent, (2) inclusion of excessive or irrelevant information, and (3)\nmisalignment with human pragmatic preference, such as the underuse of minimal\nspatial cues. We also show that standard automatic evaluations fail to capture\nthese pragmatic violations, reinforcing superficial cues rather than genuine\nreferential success. Our findings call for a renewed focus on pragmatically\ninformed models and evaluation frameworks that align with real human\ncommunication."}
{"id": "2504.15515", "pdf": "https://arxiv.org/pdf/2504.15515", "abs": "https://arxiv.org/abs/2504.15515", "authors": ["Wuchen Li"], "title": "Transport f divergences", "categories": ["math.ST", "cs.AI", "cs.IT", "math.IT", "stat.TH"], "comment": "Comments are welcome", "summary": "We define a class of divergences to measure differences between probability\ndensity functions in one-dimensional sample space. The construction is based on\nthe convex function with the Jacobi operator of mapping function that\npushforwards one density to the other. We call these information measures {\\em\ntransport $f$-divergences}. We present several properties of transport\n$f$-divergences, including invariances, convexities, variational formulations,\nand Taylor expansions in terms of mapping functions. Examples of transport\n$f$-divergences in generative models are provided."}
{"id": "2504.15758", "pdf": "https://arxiv.org/pdf/2504.15758", "abs": "https://arxiv.org/abs/2504.15758", "authors": ["Andrew Gracyk"], "title": "Observability conditions for neural state-space models with eigenvalues and their roots of unity", "categories": ["cs.LG", "cs.SY", "eess.SY", "math.DS", "math.OC"], "comment": "First version", "summary": "We operate through the lens of ordinary differential equations and control\ntheory to study the concept of observability in the context of neural\nstate-space models and the Mamba architecture. We develop strategies to enforce\nobservability, which are tailored to a learning context, specifically where the\nhidden states are learnable at initial time, in conjunction to over its\ncontinuum, and high-dimensional. We also highlight our methods emphasize\neigenvalues, roots of unity, or both. Our methods effectuate computational\nefficiency when enforcing observability, sometimes at great scale. We formulate\nobservability conditions in machine learning based on classical control theory\nand discuss their computational complexity. Our nontrivial results are\nfivefold. We discuss observability through the use of permutations in neural\napplications with learnable matrices without high precision. We present two\nresults built upon the Fourier transform that effect observability with high\nprobability up to the randomness in the learning. These results are worked with\nthe interplay of representations in Fourier space and their eigenstructure,\nnonlinear mappings, and the observability matrix. We present a result for Mamba\nthat is similar to a Hautus-type condition, but instead employs an argument\nusing a Vandermonde matrix instead of eigenvectors. Our final result is a\nshared-parameter construction of the Mamba system, which is computationally\nefficient in high exponentiation. We develop a training algorithm with this\ncoupling, showing it satisfies a Robbins-Monro condition under certain\northogonality, while a more classical training procedure fails to satisfy a\ncontraction with high Lipschitz constant."}
{"id": "2504.15783", "pdf": "https://arxiv.org/pdf/2504.15783", "abs": "https://arxiv.org/abs/2504.15783", "authors": ["Johan Öfverstedt", "Elin Lundström", "Håkan Ahlström", "Joel Kullberg"], "title": "Towards prediction of morphological heart age from computed tomography angiography", "categories": ["cs.CV"], "comment": "24 pages", "summary": "Age prediction from medical images or other health-related non-imaging data\nis an important approach to data-driven aging research, providing knowledge of\nhow much information a specific tissue or organ carries about the chronological\nage of the individual. In this work, we studied the prediction of age from\ncomputed tomography angiography (CTA) images, which provide detailed\nrepresentations of the heart morphology, with the goals of (i) studying the\nrelationship between morphology and aging, and (ii) developing a novel\n\\emph{morphological heart age} biomarker. We applied an image\nregistration-based method that standardizes the images from the whole cohort\ninto a single space. We then extracted supervoxels (using unsupervised\nsegmentation), and corresponding robust features of density and local volume,\nwhich provide a detailed representation of the heart morphology while being\nrobust to registration errors. Machine learning models are then trained to fit\nregression models from these features to the chronological age. We applied the\nmethod to a subset of the images from the Swedish CArdioPulomonary bioImage\nStudy (SCAPIS) dataset, consisting of 721 females and 666 males. We observe a\nmean absolute error of $2.74$ years for females and $2.77$ years for males. The\npredictions from different sub-regions of interest were observed to be more\nhighly correlated with the predictions from the whole heart, compared to the\nchronological age, revealing a high consistency in the predictions from\nmorphology. Saliency analysis was also performed on the prediction models to\nstudy what regions are associated positively and negatively with the predicted\nage. This resulted in detailed association maps where the density and volume of\nknown, as well as some novel sub-regions of interest, are determined to be\nimportant. The saliency analysis aids in the interpretability of the models and\ntheir predictions."}
{"id": "2504.16063", "pdf": "https://arxiv.org/pdf/2504.16063", "abs": "https://arxiv.org/abs/2504.16063", "authors": ["A. Fronzetti Colladon", "R. Vestrelli"], "title": "A Python Tool for Reconstructing Full News Text from GDELT", "categories": ["cs.CL", "cs.DB", "cs.IR", "I.2.7; H.2.8; H.3.1"], "comment": null, "summary": "News data have become an essential resource across various disciplines,\nincluding economics, finance, management, social sciences, and computer\nscience. Researchers leverage newspaper articles to study economic trends,\nmarket dynamics, corporate strategies, public perception, political discourse,\nand the evolution of public opinion. Additionally, news datasets have been\ninstrumental in training large-scale language models, with applications in\nsentiment analysis, fake news detection, and automated news summarization.\nDespite their significance, access to comprehensive news corpora remains a key\nchallenge. Many full-text news providers, such as Factiva and LexisNexis,\nrequire costly subscriptions, while free alternatives often suffer from\nincomplete data and transparency issues. This paper presents a novel approach\nto obtaining full-text newspaper articles at near-zero cost by leveraging data\nfrom the Global Database of Events, Language, and Tone (GDELT). Specifically,\nwe focus on the GDELT Web News NGrams 3.0 dataset, which provides\nhigh-frequency updates of n-grams extracted from global online news sources. We\nprovide Python code to reconstruct full-text articles from these n-grams by\nidentifying overlapping textual fragments and intelligently merging them. Our\nmethod enables researchers to access structured, large-scale newspaper data for\ntext analysis while overcoming the limitations of existing proprietary\ndatasets. The proposed approach enhances the accessibility of news data for\nempirical research, facilitating applications in economic forecasting,\ncomputational social science, and natural language processing."}
{"id": "2504.15546", "pdf": "https://arxiv.org/pdf/2504.15546", "abs": "https://arxiv.org/abs/2504.15546", "authors": ["Jayachandu Bandlamudi", "Ritwik Chaudhuri", "Neelamadhav Gantayat", "Kushal Mukherjee", "Prerna Agarwal", "Renuka Sindhgatta", "Sameep Mehta"], "title": "A Framework for Testing and Adapting REST APIs as LLM Tools", "categories": ["cs.SE", "cs.AI", "I.2.7"], "comment": null, "summary": "Large Language Models (LLMs) are enabling autonomous agents to perform\ncomplex workflows using external tools or functions, often provided via REST\nAPIs in enterprise systems. However, directly utilizing these APIs as tools\nposes challenges due to their complex input schemas, elaborate responses, and\noften ambiguous documentation. Current benchmarks for tool testing do not\nadequately address these complexities, leading to a critical gap in evaluating\nAPI readiness for agent-driven automation. In this work, we present a novel\ntesting framework aimed at evaluating and enhancing the readiness of REST APIs\nto function as tools for LLM-based agents. Our framework transforms apis as\ntools, generates comprehensive test cases for the APIs, translates tests cases\ninto natural language instructions suitable for agents, enriches tool\ndefinitions and evaluates the agent's ability t correctly invoke the API and\nprocess its inputs and responses. To provide actionable insights, we analyze\nthe outcomes of 750 test cases, presenting a detailed taxonomy of errors,\nincluding input misinterpretation, output handling inconsistencies, and schema\nmismatches. Additionally, we classify these test cases to streamline debugging\nand refinement of tool integrations. This work offers a foundational step\ntoward enabling enterprise APIs as tools, improving their usability in\nagent-based applications."}
{"id": "2504.15771", "pdf": "https://arxiv.org/pdf/2504.15771", "abs": "https://arxiv.org/abs/2504.15771", "authors": ["Assaf Gerner", "Netta Madvil", "Nadav Barak", "Alex Zaikman", "Jonatan Liberman", "Liron Hamra", "Rotem Brazilay", "Shay Tsadok", "Yaron Friedman", "Neal Harow", "Noam Bresler", "Shir Chorev", "Philip Tannor"], "title": "Grounded in Context: Retrieval-Based Method for Hallucination Detection", "categories": ["cs.LG"], "comment": null, "summary": "Despite advancements in grounded content generation, production Large\nLanguage Models (LLMs) based applications still suffer from hallucinated\nanswers. We present \"Grounded in Context\" - Deepchecks' hallucination detection\nframework, designed for production-scale long-context data and tailored to\ndiverse use cases, including summarization, data extraction, and RAG. Inspired\nby RAG architecture, our method integrates retrieval and Natural Language\nInference (NLI) models to predict factual consistency between premises and\nhypotheses using an encoder-based model with only a 512-token context window.\nOur framework identifies unsupported claims with an F1 score of 0.83 in\nRAGTruth's response-level classification task, matching methods that trained on\nthe dataset, and outperforming all comparable frameworks using similar-sized\nmodels."}
{"id": "2504.15786", "pdf": "https://arxiv.org/pdf/2504.15786", "abs": "https://arxiv.org/abs/2504.15786", "authors": ["Ningli Xu", "Rongjun Qin"], "title": "Satellite to GroundScape -- Large-scale Consistent Ground View Generation from Satellite Views", "categories": ["cs.CV"], "comment": "8 figures", "summary": "Generating consistent ground-view images from satellite imagery is\nchallenging, primarily due to the large discrepancies in viewing angles and\nresolution between satellite and ground-level domains. Previous efforts mainly\nconcentrated on single-view generation, often resulting in inconsistencies\nacross neighboring ground views. In this work, we propose a novel cross-view\nsynthesis approach designed to overcome these challenges by ensuring\nconsistency across ground-view images generated from satellite views. Our\nmethod, based on a fixed latent diffusion model, introduces two conditioning\nmodules: satellite-guided denoising, which extracts high-level scene layout to\nguide the denoising process, and satellite-temporal denoising, which captures\ncamera motion to maintain consistency across multiple generated views. We\nfurther contribute a large-scale satellite-ground dataset containing over\n100,000 perspective pairs to facilitate extensive ground scene or video\ngeneration. Experimental results demonstrate that our approach outperforms\nexisting methods on perceptual and temporal metrics, achieving high\nphotorealism and consistency in multi-view outputs."}
{"id": "2504.16073", "pdf": "https://arxiv.org/pdf/2504.16073", "abs": "https://arxiv.org/abs/2504.16073", "authors": ["Zhiyuan Hu", "Shiyun Xiong", "Yifan Zhang", "See-Kiong Ng", "Anh Tuan Luu", "Bo An", "Shuicheng Yan", "Bryan Hooi"], "title": "Guiding VLM Agents with Process Rewards at Inference Time for GUI Navigation", "categories": ["cs.CL"], "comment": null, "summary": "Recent advancements in visual language models (VLMs) have notably enhanced\ntheir capabilities in handling complex Graphical User Interface (GUI)\ninteraction tasks. Despite these improvements, current frameworks often\nstruggle to generate correct actions in challenging GUI environments.\nState-of-the-art commercial VLMs are black-boxes, and fine-tuning open-source\nVLMs for GUI tasks requires significant resources. Additionally, existing\ntrajectory-level evaluation and refinement techniques frequently fall short due\nto delayed feedback and local optimization issues. To address these challenges,\nwe propose an approach that guides VLM agents with process supervision by a\nreward model during GUI navigation and control at inference time. This guidance\nallows the VLM agent to optimize actions at each inference step, thereby\nimproving performance in both static and dynamic environments. In particular,\nour method demonstrates significant performance gains in three GUI navigation\ntasks, achieving a 3.4% improvement in single step action accuracy for static\nenvironments, along with a around 33% increase in task success rate in one\ndynamic environment. With further integration of trajectory reflection and\nretry mechanisms, we also demonstrate even greater enhancement in task success."}
{"id": "2504.15549", "pdf": "https://arxiv.org/pdf/2504.15549", "abs": "https://arxiv.org/abs/2504.15549", "authors": ["Anjali Khurana", "Xiaotian Su", "April Yi Wang", "Parmit K Chilana"], "title": "Do It For Me vs. Do It With Me: Investigating User Perceptions of Different Paradigms of Automation in Copilots for Feature-Rich Software", "categories": ["cs.HC", "cs.AI", "cs.LG"], "comment": "Accepted for publication in the CHI Conference on Human Factors in\n  Computing Systems (CHI 2025), April 26 - May 1, 2025, Yokohama, Japan", "summary": "Large Language Model (LLM)-based in-application assistants, or copilots, can\nautomate software tasks, but users often prefer learning by doing, raising\nquestions about the optimal level of automation for an effective user\nexperience. We investigated two automation paradigms by designing and\nimplementing a fully automated copilot (AutoCopilot) and a semi-automated\ncopilot (GuidedCopilot) that automates trivial steps while offering\nstep-by-step visual guidance. In a user study (N=20) across data analysis and\nvisual design tasks, GuidedCopilot outperformed AutoCopilot in user control,\nsoftware utility, and learnability, especially for exploratory and creative\ntasks, while AutoCopilot saved time for simpler visual tasks. A follow-up\ndesign exploration (N=10) enhanced GuidedCopilot with task-and state-aware\nfeatures, including in-context preview clips and adaptive instructions. Our\nfindings highlight the critical role of user control and tailored guidance in\ndesigning the next generation of copilots that enhance productivity, support\ndiverse skill levels, and foster deeper software engagement."}
{"id": "2504.15773", "pdf": "https://arxiv.org/pdf/2504.15773", "abs": "https://arxiv.org/abs/2504.15773", "authors": ["Cong Liu", "Sharvaree Vadgama", "David Ruhe", "Erik Bekkers", "Patrick Forrè"], "title": "Clifford Group Equivariant Diffusion Models for 3D Molecular Generation", "categories": ["cs.LG", "cs.AI"], "comment": "7 pages, 1 figure, 1 table", "summary": "This paper explores leveraging the Clifford algebra's expressive power for\n$\\E(n)$-equivariant diffusion models. We utilize the geometric products between\nClifford multivectors and the rich geometric information encoded in Clifford\nsubspaces in \\emph{Clifford Diffusion Models} (CDMs). We extend the diffusion\nprocess beyond just Clifford one-vectors to incorporate all higher-grade\nmultivector subspaces. The data is embedded in grade-$k$ subspaces, allowing us\nto apply latent diffusion across complete multivectors. This enables CDMs to\ncapture the joint distribution across different subspaces of the algebra,\nincorporating richer geometric information through higher-order features. We\nprovide empirical results for unconditional molecular generation on the QM9\ndataset, showing that CDMs provide a promising avenue for generative modeling."}
{"id": "2504.15792", "pdf": "https://arxiv.org/pdf/2504.15792", "abs": "https://arxiv.org/abs/2504.15792", "authors": ["Dinh Nam Pham", "Torsten Rahne"], "title": "Development and evaluation of a deep learning algorithm for German word recognition from lip movements", "categories": ["cs.CV"], "comment": "English version of journal article in HNO 2022", "summary": "When reading lips, many people benefit from additional visual information\nfrom the lip movements of the speaker, which is, however, very error prone.\nAlgorithms for lip reading with artificial intelligence based on artificial\nneural networks significantly improve word recognition but are not available\nfor the German language. A total of 1806 video clips with only one\nGerman-speaking person each were selected, split into word segments, and\nassigned to word classes using speech-recognition software. In 38,391 video\nsegments with 32 speakers, 18 polysyllabic, visually distinguishable words were\nused to train and validate a neural network. The 3D Convolutional Neural\nNetwork and Gated Recurrent Units models and a combination of both models\n(GRUConv) were compared, as were different image sections and color spaces of\nthe videos. The accuracy was determined in 5000 training epochs. Comparison of\nthe color spaces did not reveal any relevant different correct classification\nrates in the range from 69% to 72%. With a cut to the lips, a significantly\nhigher accuracy of 70% was achieved than when cut to the entire speaker's face\n(34%). With the GRUConv model, the maximum accuracies were 87% with known\nspeakers and 63% in the validation with unknown speakers. The neural network\nfor lip reading, which was first developed for the German language, shows a\nvery high level of accuracy, comparable to English-language algorithms. It\nworks with unknown speakers as well and can be generalized with more word\nclasses."}
{"id": "2504.16074", "pdf": "https://arxiv.org/pdf/2504.16074", "abs": "https://arxiv.org/abs/2504.16074", "authors": ["Shi Qiu", "Shaoyang Guo", "Zhuo-Yang Song", "Yunbo Sun", "Zeyu Cai", "Jiashen Wei", "Tianyu Luo", "Yixuan Yin", "Haoxu Zhang", "Yi Hu", "Chenyang Wang", "Chencheng Tang", "Haoling Chang", "Qi Liu", "Ziheng Zhou", "Tianyu Zhang", "Jingtian Zhang", "Zhangyi Liu", "Minghao Li", "Yuku Zhang", "Boxuan Jing", "Xianqi Yin", "Yutong Ren", "Zizhuo Fu", "Weike Wang", "Xudong Tian", "Anqi Lv", "Laifu Man", "Jianxiang Li", "Feiyu Tao", "Qihua Sun", "Zhou Liang", "Yushu Mu", "Zhongxuan Li", "Jing-Jun Zhang", "Shutao Zhang", "Xiaotian Li", "Xingqi Xia", "Jiawei Lin", "Zheyu Shen", "Jiahang Chen", "Qiuhao Xiong", "Binran Wang", "Fengyuan Wang", "Ziyang Ni", "Bohan Zhang", "Fan Cui", "Changkun Shao", "Qing-Hong Cao", "Ming-xing Luo", "Muhan Zhang", "Hua Xing Zhu"], "title": "PHYBench: Holistic Evaluation of Physical Perception and Reasoning in Large Language Models", "categories": ["cs.CL"], "comment": "21 pages ,8 figures, 4 tables", "summary": "We introduce PHYBench, a novel, high-quality benchmark designed for\nevaluating reasoning capabilities of large language models (LLMs) in physical\ncontexts. PHYBench consists of 500 meticulously curated physics problems based\non real-world physical scenarios, designed to assess the ability of models to\nunderstand and reason about realistic physical processes. Covering mechanics,\nelectromagnetism, thermodynamics, optics, modern physics, and advanced physics,\nthe benchmark spans difficulty levels from high school exercises to\nundergraduate problems and Physics Olympiad challenges. Additionally, we\npropose the Expression Edit Distance (EED) Score, a novel evaluation metric\nbased on the edit distance between mathematical expressions, which effectively\ncaptures differences in model reasoning processes and results beyond\ntraditional binary scoring methods. We evaluate various LLMs on PHYBench and\ncompare their performance with human experts. Our results reveal that even\nstate-of-the-art reasoning models significantly lag behind human experts,\nhighlighting their limitations and the need for improvement in complex physical\nreasoning scenarios. Our benchmark results and dataset are publicly available\nat https://phybench-official.github.io/phybench-demo/."}
{"id": "2504.15564", "pdf": "https://arxiv.org/pdf/2504.15564", "abs": "https://arxiv.org/abs/2504.15564", "authors": ["Musfiqur Rahman", "SayedHassan Khatoonabadi", "Emad Shihab"], "title": "A Large-scale Class-level Benchmark Dataset for Code Generation with LLMs", "categories": ["cs.SE", "cs.AI", "cs.LG"], "comment": "This paper was submitted to the 29th International Conference on\n  Evaluation and Assessment in Software Engineering (EASE 2025) AI models/data\n  track", "summary": "Recent advancements in large language models (LLMs) have demonstrated\npromising capabilities in code generation tasks. However, most existing\nbenchmarks focus on isolated functions and fail to capture the complexity of\nreal-world, class-level software structures. To address this gap, we introduce\na large-scale, Python class-level dataset curated from $13{,}174$ real-world\nopen-source projects. The dataset contains over 842,000 class skeletons, each\nincluding class and method signatures, along with associated docstrings when\navailable. We preserve structural and contextual dependencies critical to\nrealistic software development scenarios and enrich the dataset with static\ncode metrics to support downstream analysis. To evaluate the usefulness of this\ndataset, we use extracted class skeletons as prompts for GPT-4 to generate full\nclass implementations. Results show that the LLM-generated classes exhibit\nstrong lexical and structural similarity to human-written counterparts, with\naverage ROUGE@L, BLEU, and TSED scores of 0.80, 0.59, and 0.73, respectively.\nThese findings confirm that well-structured prompts derived from real-world\nclass skeletons significantly enhance LLM performance in class-level code\ngeneration. This dataset offers a valuable resource for benchmarking, training,\nand improving LLMs in realistic software engineering contexts."}
{"id": "2504.15806", "pdf": "https://arxiv.org/pdf/2504.15806", "abs": "https://arxiv.org/abs/2504.15806", "authors": ["Kai Luo", "Juan Tang", "Mingchao Cai", "Xiaoqing Zeng", "Manqi Xie", "Ming Yan"], "title": "DAE-KAN: A Kolmogorov-Arnold Network Model for High-Index Differential-Algebraic Equations", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Kolmogorov-Arnold Networks (KANs) have emerged as a promising alternative to\nMulti-Layer Perceptrons (MLPs) due to their superior function-fitting abilities\nin data-driven modeling. In this paper, we propose a novel framework, DAE-KAN,\nfor solving high-index differential-algebraic equations (DAEs) by integrating\nKANs with Physics-Informed Neural Networks (PINNs). This framework not only\npreserves the ability of traditional PINNs to model complex systems governed by\nphysical laws but also enhances their performance by leveraging the\nfunction-fitting strengths of KANs. Numerical experiments demonstrate that for\nDAE systems ranging from index-1 to index-3, DAE-KAN reduces the absolute\nerrors of both differential and algebraic variables by 1 to 2 orders of\nmagnitude compared to traditional PINNs. To assess the effectiveness of this\napproach, we analyze the drift-off error and find that both PINNs and DAE-KAN\noutperform classical numerical methods in controlling this phenomenon. Our\nresults highlight the potential of neural network methods, particularly\nDAE-KAN, in solving high-index DAEs with substantial computational accuracy and\ngeneralization, offering a promising solution for challenging partial\ndifferential-algebraic equations."}
{"id": "2504.15796", "pdf": "https://arxiv.org/pdf/2504.15796", "abs": "https://arxiv.org/abs/2504.15796", "authors": ["Jiaqi Tang", "Yinsong Xu", "Qingchao Chen"], "title": "Locating and Mitigating Gradient Conflicts in Point Cloud Domain Adaptation via Saliency Map Skewness", "categories": ["cs.CV", "cs.LG"], "comment": null, "summary": "Object classification models utilizing point cloud data are fundamental for\n3D media understanding, yet they often struggle with unseen or\nout-of-distribution (OOD) scenarios. Existing point cloud unsupervised domain\nadaptation (UDA) methods typically employ a multi-task learning (MTL) framework\nthat combines primary classification tasks with auxiliary self-supervision\ntasks to bridge the gap between cross-domain feature distributions. However,\nour further experiments demonstrate that not all gradients from\nself-supervision tasks are beneficial and some may negatively impact the\nclassification performance. In this paper, we propose a novel solution, termed\nSaliency Map-based Data Sampling Block (SM-DSB), to mitigate these gradient\nconflicts. Specifically, our method designs a new scoring mechanism based on\nthe skewness of 3D saliency maps to estimate gradient conflicts without\nrequiring target labels. Leveraging this, we develop a sample selection\nstrategy that dynamically filters out samples whose self-supervision gradients\nare not beneficial for the classification. Our approach is scalable,\nintroducing modest computational overhead, and can be integrated into all the\npoint cloud UDA MTL frameworks. Extensive evaluations demonstrate that our\nmethod outperforms state-of-the-art approaches. In addition, we provide a new\nperspective on understanding the UDA problem through back-propagation analysis."}
{"id": "2504.16084", "pdf": "https://arxiv.org/pdf/2504.16084", "abs": "https://arxiv.org/abs/2504.16084", "authors": ["Yuxin Zuo", "Kaiyan Zhang", "Shang Qu", "Li Sheng", "Xuekai Zhu", "Biqing Qi", "Youbang Sun", "Ganqu Cui", "Ning Ding", "Bowen Zhou"], "title": "TTRL: Test-Time Reinforcement Learning", "categories": ["cs.CL", "cs.LG"], "comment": null, "summary": "This paper investigates Reinforcement Learning (RL) on data without explicit\nlabels for reasoning tasks in Large Language Models (LLMs). The core challenge\nof the problem is reward estimation during inference while not having access to\nground-truth information. While this setting appears elusive, we find that\ncommon practices in Test-Time Scaling (TTS), such as majority voting, yield\nsurprisingly effective rewards suitable for driving RL training. In this work,\nwe introduce Test-Time Reinforcement Learning (TTRL), a novel method for\ntraining LLMs using RL on unlabeled data. TTRL enables self-evolution of LLMs\nby utilizing the priors in the pre-trained models. Our experiments demonstrate\nthat TTRL consistently improves performance across a variety of tasks and\nmodels. Notably, TTRL boosts the pass@1 performance of Qwen-2.5-Math-7B by\napproximately 159% on the AIME 2024 with only unlabeled test data. Furthermore,\nalthough TTRL is only supervised by the Maj@N metric, TTRL has demonstrated\nperformance to consistently surpass the upper limit of the initial model, and\napproach the performance of models trained directly on test data with\nground-truth labels. Our experimental findings validate the general\neffectiveness of TTRL across various tasks, and highlight TTRL's potential for\nbroader tasks and domains. GitHub: https://github.com/PRIME-RL/TTRL"}
{"id": "2504.15585", "pdf": "https://arxiv.org/pdf/2504.15585", "abs": "https://arxiv.org/abs/2504.15585", "authors": ["Kun Wang", "Guibin Zhang", "Zhenhong Zhou", "Jiahao Wu", "Miao Yu", "Shiqian Zhao", "Chenlong Yin", "Jinhu Fu", "Yibo Yan", "Hanjun Luo", "Liang Lin", "Zhihao Xu", "Haolang Lu", "Xinye Cao", "Xinyun Zhou", "Weifei Jin", "Fanci Meng", "Junyuan Mao", "Hao Wu", "Minghe Wang", "Fan Zhang", "Junfeng Fang", "Chengwei Liu", "Yifan Zhang", "Qiankun Li", "Chongye Guo", "Yalan Qin", "Yi Ding", "Donghai Hong", "Jiaming Ji", "Xinfeng Li", "Yifan Jiang", "Dongxia Wang", "Yihao Huang", "Yufei Guo", "Jen-tse Huang", "Yanwei Yue", "Wenke Huang", "Guancheng Wan", "Tianlin Li", "Lei Bai", "Jie Zhang", "Qing Guo", "Jingyi Wang", "Tianlong Chen", "Joey Tianyi Zhou", "Xiaojun Jia", "Weisong Sun", "Cong Wu", "Jing Chen", "Xuming Hu", "Yiming Li", "Xiao Wang", "Ningyu Zhang", "Luu Anh Tuan", "Guowen Xu", "Tianwei Zhang", "Xingjun Ma", "Xiang Wang", "Bo An", "Jun Sun", "Mohit Bansal", "Shirui Pan", "Yuval Elovici", "Bhavya Kailkhura", "Bo Li", "Yaodong Yang", "Hongwei Li", "Wenyuan Xu", "Yizhou Sun", "Wei Wang", "Qing Li", "Ke Tang", "Yu-Gang Jiang", "Felix Juefei-Xu", "Hui Xiong", "Xiaofeng Wang", "Shuicheng Yan", "Dacheng Tao", "Philip S. Yu", "Qingsong Wen", "Yang Liu"], "title": "A Comprehensive Survey in LLM(-Agent) Full Stack Safety: Data, Training and Deployment", "categories": ["cs.CR", "cs.AI", "cs.CL", "cs.LG"], "comment": null, "summary": "The remarkable success of Large Language Models (LLMs) has illuminated a\npromising pathway toward achieving Artificial General Intelligence for both\nacademic and industrial communities, owing to their unprecedented performance\nacross various applications. As LLMs continue to gain prominence in both\nresearch and commercial domains, their security and safety implications have\nbecome a growing concern, not only for researchers and corporations but also\nfor every nation. Currently, existing surveys on LLM safety primarily focus on\nspecific stages of the LLM lifecycle, e.g., deployment phase or fine-tuning\nphase, lacking a comprehensive understanding of the entire \"lifechain\" of LLMs.\nTo address this gap, this paper introduces, for the first time, the concept of\n\"full-stack\" safety to systematically consider safety issues throughout the\nentire process of LLM training, deployment, and eventual commercialization.\nCompared to the off-the-shelf LLM safety surveys, our work demonstrates several\ndistinctive advantages: (I) Comprehensive Perspective. We define the complete\nLLM lifecycle as encompassing data preparation, pre-training, post-training,\ndeployment and final commercialization. To our knowledge, this represents the\nfirst safety survey to encompass the entire lifecycle of LLMs. (II) Extensive\nLiterature Support. Our research is grounded in an exhaustive review of over\n800+ papers, ensuring comprehensive coverage and systematic organization of\nsecurity issues within a more holistic understanding. (III) Unique Insights.\nThrough systematic literature analysis, we have developed reliable roadmaps and\nperspectives for each chapter. Our work identifies promising research\ndirections, including safety in data generation, alignment techniques, model\nediting, and LLM-based agent systems. These insights provide valuable guidance\nfor researchers pursuing future work in this field."}
{"id": "2504.15812", "pdf": "https://arxiv.org/pdf/2504.15812", "abs": "https://arxiv.org/abs/2504.15812", "authors": ["Xuchuang Wang", "Qirun Zeng", "Jinhang Zuo", "Xutong Liu", "Mohammad Hajiesmaili", "John C. S. Lui", "Adam Wierman"], "title": "Fusing Reward and Dueling Feedback in Stochastic Bandits", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "This paper investigates the fusion of absolute (reward) and relative\n(dueling) feedback in stochastic bandits, where both feedback types are\ngathered in each decision round. We derive a regret lower bound, demonstrating\nthat an efficient algorithm may incur only the smaller among the reward and\ndueling-based regret for each individual arm. We propose two fusion approaches:\n(1) a simple elimination fusion algorithm that leverages both feedback types to\nexplore all arms and unifies collected information by sharing a common\ncandidate arm set, and (2) a decomposition fusion algorithm that selects the\nmore effective feedback to explore the corresponding arms and randomly assigns\none feedback type for exploration and the other for exploitation in each round.\nThe elimination fusion experiences a suboptimal multiplicative term of the\nnumber of arms in regret due to the intrinsic suboptimality of dueling\nelimination. In contrast, the decomposition fusion achieves regret matching the\nlower bound up to a constant under a common assumption. Extensive experiments\nconfirm the efficacy of our algorithms and theoretical results."}
{"id": "2504.15823", "pdf": "https://arxiv.org/pdf/2504.15823", "abs": "https://arxiv.org/abs/2504.15823", "authors": ["Songyan Xie", "Jinghang Wen", "Encheng Su", "Qiucheng Yu"], "title": "Human-Imperceptible Physical Adversarial Attack for NIR Face Recognition Models", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Near-infrared (NIR) face recognition systems, which can operate effectively\nin low-light conditions or in the presence of makeup, exhibit vulnerabilities\nwhen subjected to physical adversarial attacks. To further demonstrate the\npotential risks in real-world applications, we design a novel, stealthy, and\npractical adversarial patch to attack NIR face recognition systems in a\nblack-box setting. We achieved this by utilizing human-imperceptible\ninfrared-absorbing ink to generate multiple patches with digitally optimized\nshapes and positions for infrared images. To address the optimization mismatch\nbetween digital and real-world NIR imaging, we develop a light reflection model\nfor human skin to minimize pixel-level discrepancies by simulating NIR light\nreflection.\n  Compared to state-of-the-art (SOTA) physical attacks on NIR face recognition\nsystems, the experimental results show that our method improves the attack\nsuccess rate in both digital and physical domains, particularly maintaining\neffectiveness across various face postures. Notably, the proposed approach\noutperforms SOTA methods, achieving an average attack success rate of 82.46% in\nthe physical domain across different models, compared to 64.18% for existing\nmethods. The artifact is available at\nhttps://anonymous.4open.science/r/Human-imperceptible-adversarial-patch-0703/."}
{"id": "2504.15448", "pdf": "https://arxiv.org/pdf/2504.15448", "abs": "https://arxiv.org/abs/2504.15448", "authors": ["Yanampally Abhiram Reddy", "Siddhi Agarwal", "Vikram Parashar", "Arshiya Arora"], "title": "Real-Time Sentiment Insights from X Using VADER, DistilBERT, and Web-Scraped Data", "categories": ["econ.GN", "cs.CL", "q-fin.EC"], "comment": "19 pages, 2 figures", "summary": "In the age of social media, understanding public sentiment toward major\ncorporations is crucial for investors, policymakers, and researchers. This\npaper presents a comprehensive sentiment analysis system tailored for corporate\nreputation monitoring, combining Natural Language Processing (NLP) and machine\nlearning techniques to accurately interpret public opinion in real time. The\nmethodology integrates a hybrid sentiment detection framework leveraging both\nrule-based models (VADER) and transformer-based deep learning models\n(DistilBERT), applied to social media data from multiple platforms. The system\nbegins with robust preprocessing involving noise removal and text\nnormalization, followed by sentiment classification using an ensemble approach\nto ensure both interpretability and contextual accuracy. Results are visualized\nthrough sentiment distribution plots, comparative analyses, and temporal\nsentiment trends for enhanced interpretability. Our analysis reveals\nsignificant disparities in public sentiment across major corporations, with\ncompanies like Amazon (81.2) and Samsung (45.8) receiving excellent sentiment\nscores, while Microsoft (21.7) and Walmart (21.9) exhibit poor sentiment\nprofiles. These findings demonstrate the utility of our multi-source sentiment\nframework in providing actionable insights regarding corporate public\nperception, enabling stakeholders to make informed strategic decisions based on\ncomprehensive sentiment analysis."}
{"id": "2504.15637", "pdf": "https://arxiv.org/pdf/2504.15637", "abs": "https://arxiv.org/abs/2504.15637", "authors": ["Farnaz Behrang", "Zhizhou Zhang", "Georgian-Vlad Saioc", "Peng Liu", "Milind Chabbi"], "title": "DR.FIX: Automatically Fixing Data Races at Industry Scale", "categories": ["cs.DC", "cs.AI", "cs.LG", "cs.PL", "cs.SE"], "comment": "To appear in PLDI 2025", "summary": "Data races are a prevalent class of concurrency bugs in shared-memory\nparallel programs, posing significant challenges to software reliability and\nreproducibility. While there is an extensive body of research on detecting data\nraces and a wealth of practical detection tools across various programming\nlanguages, considerably less effort has been directed toward automatically\nfixing data races at an industrial scale. In large codebases, data races are\ncontinuously introduced and exhibit myriad patterns, making automated fixing\nparticularly challenging.\n  In this paper, we tackle the problem of automatically fixing data races at an\nindustrial scale. We present Dr.Fix, a tool that combines large language models\n(LLMs) with program analysis to generate fixes for data races in real-world\nsettings, effectively addressing a broad spectrum of racy patterns in complex\ncode contexts. Implemented for Go--the programming language widely used in\nmodern microservice architectures where concurrency is pervasive and data races\nare common--Dr.Fix seamlessly integrates into existing development workflows.\nWe detail the design of Dr.Fix and examine how individual design choices\ninfluence the quality of the fixes produced. Over the past 18 months, Dr.Fix\nhas been integrated into developer workflows at Uber demonstrating its\npractical utility. During this period, Dr.Fix produced patches for 224 (55%)\nfrom a corpus of 404 data races spanning various categories; 193 of these\npatches (86%) were accepted by more than a hundred developers via code reviews\nand integrated into the codebase."}
{"id": "2504.15827", "pdf": "https://arxiv.org/pdf/2504.15827", "abs": "https://arxiv.org/abs/2504.15827", "authors": ["Xuyang Zhong", "Haochen Luo", "Chen Liu"], "title": "DualOptim: Enhancing Efficacy and Stability in Machine Unlearning with Dual Optimizers", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Existing machine unlearning (MU) approaches exhibit significant sensitivity\nto hyperparameters, requiring meticulous tuning that limits practical\ndeployment. In this work, we first empirically demonstrate the instability and\nsuboptimal performance of existing popular MU methods when deployed in\ndifferent scenarios. To address this issue, we propose Dual Optimizer\n(DualOptim), which incorporates adaptive learning rate and decoupled momentum\nfactors. Empirical and theoretical evidence demonstrates that DualOptim\ncontributes to effective and stable unlearning. Through extensive experiments,\nwe show that DualOptim can significantly boost MU efficacy and stability across\ndiverse tasks, including image classification, image generation, and large\nlanguage models, making it a versatile approach to empower existing MU\nalgorithms."}
{"id": "2504.15835", "pdf": "https://arxiv.org/pdf/2504.15835", "abs": "https://arxiv.org/abs/2504.15835", "authors": ["Yiqian Wu", "Malte Prinzler", "Xiaogang Jin", "Siyu Tang"], "title": "Text-based Animatable 3D Avatars with Morphable Model Alignment", "categories": ["cs.CV"], "comment": null, "summary": "The generation of high-quality, animatable 3D head avatars from text has\nenormous potential in content creation applications such as games, movies, and\nembodied virtual assistants. Current text-to-3D generation methods typically\ncombine parametric head models with 2D diffusion models using score\ndistillation sampling to produce 3D-consistent results. However, they struggle\nto synthesize realistic details and suffer from misalignments between the\nappearance and the driving parametric model, resulting in unnatural animation\nresults. We discovered that these limitations stem from ambiguities in the 2D\ndiffusion predictions during 3D avatar distillation, specifically: i) the\navatar's appearance and geometry is underconstrained by the text input, and ii)\nthe semantic alignment between the predictions and the parametric head model is\ninsufficient because the diffusion model alone cannot incorporate information\nfrom the parametric model. In this work, we propose a novel framework,\nAnimPortrait3D, for text-based realistic animatable 3DGS avatar generation with\nmorphable model alignment, and introduce two key strategies to address these\nchallenges. First, we tackle appearance and geometry ambiguities by utilizing\nprior information from a pretrained text-to-3D model to initialize a 3D avatar\nwith robust appearance, geometry, and rigging relationships to the morphable\nmodel. Second, we refine the initial 3D avatar for dynamic expressions using a\nControlNet that is conditioned on semantic and normal maps of the morphable\nmodel to ensure accurate alignment. As a result, our method outperforms\nexisting approaches in terms of synthesis quality, alignment, and animation\nfidelity. Our experiments show that the proposed method advances the state of\nthe art in text-based, animatable 3D head avatar generation."}
{"id": "2504.15629", "pdf": "https://arxiv.org/pdf/2504.15629", "abs": "https://arxiv.org/abs/2504.15629", "authors": ["Harsh Maheshwari", "Srikanth Tenneti", "Alwarappan Nakkiran"], "title": "CiteFix: Enhancing RAG Accuracy Through Post-Processing Citation Correction", "categories": ["cs.IR", "cs.CL"], "comment": null, "summary": "Retrieval Augmented Generation (RAG) has emerged as a powerful application of\nLarge Language Models (LLMs), revolutionizing information search and\nconsumption. RAG systems combine traditional search capabilities with LLMs to\ngenerate comprehensive answers to user queries, ideally with accurate\ncitations. However, in our experience of developing a RAG product, LLMs often\nstruggle with source attribution, aligning with other industry studies\nreporting citation accuracy rates of only about 74% for popular generative\nsearch engines. To address this, we present efficient post-processing\nalgorithms to improve citation accuracy in LLM-generated responses, with\nminimal impact on latency and cost. Our approaches cross-check generated\ncitations against retrieved articles using methods including keyword + semantic\nmatching, fine tuned model with BERTScore, and a lightweight LLM-based\ntechnique. Our experimental results demonstrate a relative improvement of\n15.46% in the overall accuracy metrics of our RAG system. This significant\nenhancement potentially enables a shift from our current larger language model\nto a relatively smaller model that is approximately 12x more cost-effective and\n3x faster in inference time, while maintaining comparable performance. This\nresearch contributes to enhancing the reliability and trustworthiness of\nAI-generated content in information retrieval and summarization tasks which is\ncritical to gain customer trust especially in commercial products."}
{"id": "2504.15654", "pdf": "https://arxiv.org/pdf/2504.15654", "abs": "https://arxiv.org/abs/2504.15654", "authors": ["Md Abdul Baset Sarker", "Art Nguyen", "Sigmond Kukla", "Kevin Fite", "Masudul H. Imtiaz"], "title": "A Vision-Enabled Prosthetic Hand for Children with Upper Limb Disabilities", "categories": ["cs.RO", "cs.AI", "cs.CV", "cs.LG"], "comment": null, "summary": "This paper introduces a novel AI vision-enabled pediatric prosthetic hand\ndesigned to assist children aged 10-12 with upper limb disabilities. The\nprosthesis features an anthropomorphic appearance, multi-articulating\nfunctionality, and a lightweight design that mimics a natural hand, making it\nboth accessible and affordable for low-income families. Using 3D printing\ntechnology and integrating advanced machine vision, sensing, and embedded\ncomputing, the prosthetic hand offers a low-cost, customizable solution that\naddresses the limitations of current myoelectric prostheses. A micro camera is\ninterfaced with a low-power FPGA for real-time object detection and assists\nwith precise grasping. The onboard DL-based object detection and grasp\nclassification models achieved accuracies of 96% and 100% respectively. In the\nforce prediction, the mean absolute error was found to be 0.018. The features\nof the proposed prosthetic hand can thus be summarized as: a) a wrist-mounted\nmicro camera for artificial sensing, enabling a wide range of hand-based tasks;\nb) real-time object detection and distance estimation for precise grasping; and\nc) ultra-low-power operation that delivers high performance within constrained\npower and resource limits."}
{"id": "2504.15846", "pdf": "https://arxiv.org/pdf/2504.15846", "abs": "https://arxiv.org/abs/2504.15846", "authors": ["Jonah Ekelund", "Savvas Raptis", "Vicki Toy-Edens", "Wenli Mo", "Drew L. Turner", "Ian J. Cohen", "Stefano Markidis"], "title": "Adaptive PCA-Based Outlier Detection for Multi-Feature Time Series in Space Missions", "categories": ["cs.LG", "physics.space-ph"], "comment": "Accepted to ICCS 2025", "summary": "Analyzing multi-featured time series data is critical for space missions\nmaking efficient event detection, potentially onboard, essential for automatic\nanalysis. However, limited onboard computational resources and data downlink\nconstraints necessitate robust methods for identifying regions of interest in\nreal time. This work presents an adaptive outlier detection algorithm based on\nthe reconstruction error of Principal Component Analysis (PCA) for feature\nreduction, designed explicitly for space mission applications. The algorithm\nadapts dynamically to evolving data distributions by using Incremental PCA,\nenabling deployment without a predefined model for all possible conditions. A\npre-scaling process normalizes each feature's magnitude while preserving\nrelative variance within feature types. We demonstrate the algorithm's\neffectiveness in detecting space plasma events, such as distinct space\nenvironments, dayside and nightside transients phenomena, and transition layers\nthrough NASA's MMS mission observations. Additionally, we apply the method to\nNASA's THEMIS data, successfully identifying a dayside transient using\nonboard-available measurements."}
{"id": "2504.15863", "pdf": "https://arxiv.org/pdf/2504.15863", "abs": "https://arxiv.org/abs/2504.15863", "authors": ["Diego de Oliveira Hitzges", "Suman Ghosh", "Guillermo Gallego"], "title": "DERD-Net: Learning Depth from Event-based Ray Densities", "categories": ["cs.CV", "cs.LG", "cs.RO", "eess.SP"], "comment": "13 pages, 3 figures, 14 tables. Project page:\n  https://github.com/tub-rip/DERD-Net", "summary": "Event cameras offer a promising avenue for multi-view stereo depth estimation\nand Simultaneous Localization And Mapping (SLAM) due to their ability to detect\nblur-free 3D edges at high-speed and over broad illumination conditions.\nHowever, traditional deep learning frameworks designed for conventional cameras\nstruggle with the asynchronous, stream-like nature of event data, as their\narchitectures are optimized for discrete, image-like inputs. We propose a\nscalable, flexible and adaptable framework for pixel-wise depth estimation with\nevent cameras in both monocular and stereo setups. The 3D scene structure is\nencoded into disparity space images (DSIs), representing spatial densities of\nrays obtained by back-projecting events into space via known camera poses. Our\nneural network processes local subregions of the DSIs combining 3D convolutions\nand a recurrent structure to recognize valuable patterns for depth prediction.\nLocal processing enables fast inference with full parallelization and ensures\nconstant ultra-low model complexity and memory costs, regardless of camera\nresolution. Experiments on standard benchmarks (MVSEC and DSEC datasets)\ndemonstrate unprecedented effectiveness: (i) using purely monocular data, our\nmethod achieves comparable results to existing stereo methods; (ii) when\napplied to stereo data, it strongly outperforms all state-of-the-art (SOTA)\napproaches, reducing the mean absolute error by at least 42%; (iii) our method\nalso allows for increases in depth completeness by more than 3-fold while still\nyielding a reduction in median absolute error of at least 30%. Given its\nremarkable performance and effective processing of event-data, our framework\nholds strong potential to become a standard approach for using deep learning\nfor event-based depth estimation and SLAM. Project page:\nhttps://github.com/tub-rip/DERD-Net"}
{"id": "2504.15659", "pdf": "https://arxiv.org/pdf/2504.15659", "abs": "https://arxiv.org/abs/2504.15659", "authors": ["Anjiang Wei", "Huanmi Tan", "Tarun Suresh", "Daniel Mendoza", "Thiago S. F. X. Teixeira", "Ke Wang", "Caroline Trippel", "Alex Aiken"], "title": "VeriCoder: Enhancing LLM-Based RTL Code Generation through Functional Correctness Validation", "categories": ["cs.AR", "cs.AI", "cs.CL", "cs.LG", "cs.SE"], "comment": null, "summary": "Recent advances in Large Language Models (LLMs) have sparked growing interest\nin applying them to Electronic Design Automation (EDA) tasks, particularly\nRegister Transfer Level (RTL) code generation. While several RTL datasets have\nbeen introduced, most focus on syntactic validity rather than functional\nvalidation with tests, leading to training examples that compile but may not\nimplement the intended behavior. We present VERICODER, a model for RTL code\ngeneration fine-tuned on a dataset validated for functional correctness. This\nfine-tuning dataset is constructed using a novel methodology that combines unit\ntest generation with feedback-directed refinement. Given a natural language\nspecification and an initial RTL design, we prompt a teacher model\n(GPT-4o-mini) to generate unit tests and iteratively revise the RTL design\nbased on its simulation results using the generated tests. If necessary, the\nteacher model also updates the tests to ensure they comply with the natural\nlanguage specification. As a result of this process, every example in our\ndataset is functionally validated, consisting of a natural language\ndescription, an RTL implementation, and passing tests. Fine-tuned on this\ndataset of over 125,000 examples, VERICODER achieves state-of-the-art metrics\nin functional correctness on VerilogEval and RTLLM, with relative gains of up\nto 71.7% and 27.4% respectively. An ablation study further shows that models\ntrained on our functionally validated dataset outperform those trained on\nfunctionally non-validated datasets, underscoring the importance of\nhigh-quality datasets in RTL code generation."}
{"id": "2504.15724", "pdf": "https://arxiv.org/pdf/2504.15724", "abs": "https://arxiv.org/abs/2504.15724", "authors": ["Yiannis Papageorgiou", "Yannis Thomas", "Alexios Filippakopoulos", "Ramin Khalili", "Iordanis Koutsopoulos"], "title": "Collaborative Split Federated Learning with Parallel Training and Aggregation", "categories": ["cs.DC", "cs.AI"], "comment": null, "summary": "Federated learning (FL) operates based on model exchanges between the server\nand the clients, and it suffers from significant client-side computation and\ncommunication burden. Split federated learning (SFL) arises a promising\nsolution by splitting the model into two parts, that are trained sequentially:\nthe clients train the first part of the model (client-side model) and transmit\nit to the server that trains the second (server-side model). Existing SFL\nschemes though still exhibit long training delays and significant communication\noverhead, especially when clients of different computing capability\nparticipate. Thus, we propose Collaborative-Split Federated Learning~(C-SFL), a\nnovel scheme that splits the model into three parts, namely the model parts\ntrained at the computationally weak clients, the ones trained at the\ncomputationally strong clients, and the ones at the server. Unlike existing\nworks, C-SFL enables parallel training and aggregation of model's parts at the\nclients and at the server, resulting in reduced training delays and\ncommmunication overhead while improving the model's accuracy. Experiments\nverify the multiple gains of C-SFL against the existing schemes."}
{"id": "2504.15854", "pdf": "https://arxiv.org/pdf/2504.15854", "abs": "https://arxiv.org/abs/2504.15854", "authors": ["Georgios Mavroudeas", "Malik Magdon-Ismail", "Kristin P. Bennett", "Jason Kuruzovich"], "title": "Consistent Causal Inference of Group Effects in Non-Targeted Trials with Finitely Many Effect Levels", "categories": ["cs.LG", "stat.ML"], "comment": null, "summary": "A treatment may be appropriate for some group (the ``sick\" group) on whom it\nhas a positive effect, but it can also have a detrimental effect on subjects\nfrom another group (the ``healthy\" group). In a non-targeted trial both sick\nand healthy subjects may be treated, producing heterogeneous effects within the\ntreated group. Inferring the correct treatment effect on the sick population is\nthen difficult, because the effects on the different groups get tangled. We\npropose an efficient nonparametric approach to estimating the group effects,\ncalled {\\bf PCM} (pre-cluster and merge). We prove its asymptotic consistency\nin a general setting and show, on synthetic data, more than a 10x improvement\nin accuracy over existing state-of-the-art. Our approach applies more generally\nto consistent estimation of functions with a finite range."}
{"id": "2504.15865", "pdf": "https://arxiv.org/pdf/2504.15865", "abs": "https://arxiv.org/abs/2504.15865", "authors": ["Lotfi Abdelkrim Mecharbat", "Ibrahim Elmakky", "Martin Takac", "Mohammed Yaqub"], "title": "MedNNS: Supernet-based Medical Task-Adaptive Neural Network Search", "categories": ["cs.CV", "cs.AI", "cs.LG"], "comment": null, "summary": "Deep learning (DL) has achieved remarkable progress in the field of medical\nimaging. However, adapting DL models to medical tasks remains a significant\nchallenge, primarily due to two key factors: (1) architecture selection, as\ndifferent tasks necessitate specialized model designs, and (2) weight\ninitialization, which directly impacts the convergence speed and final\nperformance of the models. Although transfer learning from ImageNet is a widely\nadopted strategy, its effectiveness is constrained by the substantial\ndifferences between natural and medical images. To address these challenges, we\nintroduce Medical Neural Network Search (MedNNS), the first Neural Network\nSearch framework for medical imaging applications. MedNNS jointly optimizes\narchitecture selection and weight initialization by constructing a meta-space\nthat encodes datasets and models based on how well they perform together. We\nbuild this space using a Supernetwork-based approach, expanding the model zoo\nsize by 51x times over previous state-of-the-art (SOTA) methods. Moreover, we\nintroduce rank loss and Fr\\'echet Inception Distance (FID) loss into the\nconstruction of the space to capture inter-model and inter-dataset\nrelationships, thereby achieving more accurate alignment in the meta-space.\nExperimental results across multiple datasets demonstrate that MedNNS\nsignificantly outperforms both ImageNet pre-trained DL models and SOTA Neural\nArchitecture Search (NAS) methods, achieving an average accuracy improvement of\n1.7% across datasets while converging substantially faster. The code and the\nprocessed meta-space is available at https://github.com/BioMedIA-MBZUAI/MedNNS."}
{"id": "2504.16000", "pdf": "https://arxiv.org/pdf/2504.16000", "abs": "https://arxiv.org/abs/2504.16000", "authors": ["Soham Bonnerjee", "Zhen Wei", "Yeon", "Anna Asch", "Sagnik Nandy", "Promit Ghosal"], "title": "How Private is Your Attention? Bridging Privacy with In-Context Learning", "categories": ["stat.ML", "cs.AI", "cs.CL", "cs.CR", "cs.LG"], "comment": null, "summary": "In-context learning (ICL)-the ability of transformer-based models to perform\nnew tasks from examples provided at inference time-has emerged as a hallmark of\nmodern language models. While recent works have investigated the mechanisms\nunderlying ICL, its feasibility under formal privacy constraints remains\nlargely unexplored. In this paper, we propose a differentially private\npretraining algorithm for linear attention heads and present the first\ntheoretical analysis of the privacy-accuracy trade-off for ICL in linear\nregression. Our results characterize the fundamental tension between\noptimization and privacy-induced noise, formally capturing behaviors observed\nin private training via iterative methods. Additionally, we show that our\nmethod is robust to adversarial perturbations of training prompts, unlike\nstandard ridge regression. All theoretical findings are supported by extensive\nsimulations across diverse settings."}
{"id": "2504.15743", "pdf": "https://arxiv.org/pdf/2504.15743", "abs": "https://arxiv.org/abs/2504.15743", "authors": ["Seung Gyu Jeong", "Sung Woo Nam", "Seong Kwan Jung", "Seong-Eun Kim"], "title": "iMedic: Towards Smartphone-based Self-Auscultation Tool for AI-Powered Pediatric Respiratory Assessment", "categories": ["cs.HC", "cs.AI", "cs.LG"], "comment": null, "summary": "Respiratory auscultation is crucial for early detection of pediatric\npneumonia, a condition that can quickly worsen without timely intervention. In\nareas with limited physician access, effective auscultation is challenging. We\npresent a smartphone-based system that leverages built-in microphones and\nadvanced deep learning algorithms to detect abnormal respiratory sounds\nindicative of pneumonia risk. Our end-to-end deep learning framework employs\ndomain generalization to integrate a large electronic stethoscope dataset with\na smaller smartphone-derived dataset, enabling robust feature learning for\naccurate respiratory assessments without expensive equipment. The accompanying\nmobile application guides caregivers in collecting high-quality lung sound\nsamples and provides immediate feedback on potential pneumonia risks. User\nstudies show strong classification performance and high acceptance,\ndemonstrating the system's ability to facilitate proactive interventions and\nreduce preventable childhood pneumonia deaths. By seamlessly integrating into\nubiquitous smartphones, this approach offers a promising avenue for more\nequitable and comprehensive remote pediatric care."}
{"id": "2504.15897", "pdf": "https://arxiv.org/pdf/2504.15897", "abs": "https://arxiv.org/abs/2504.15897", "authors": ["Zherui Yang", "Zhengyang Xue", "Ligang Liu"], "title": "SUPRA: Subspace Parameterized Attention for Neural Operator on General Domains", "categories": ["cs.LG"], "comment": null, "summary": "Neural operators are efficient surrogate models for solving partial\ndifferential equations (PDEs), but their key components face challenges: (1) in\norder to improve accuracy, attention mechanisms suffer from computational\ninefficiency on large-scale meshes, and (2) spectral convolutions rely on the\nFast Fourier Transform (FFT) on regular grids and assume a flat geometry, which\ncauses accuracy degradation on irregular domains. To tackle these problems, we\nregard the matrix-vector operations in the standard attention mechanism on\nvectors in Euclidean space as bilinear forms and linear operators in vector\nspaces and generalize the attention mechanism to function spaces. This new\nattention mechanism is fully equivalent to the standard attention but\nimpossible to compute due to the infinite dimensionality of function spaces. To\naddress this, inspired by model reduction techniques, we propose a Subspace\nParameterized Attention (SUPRA) neural operator, which approximates the\nattention mechanism within a finite-dimensional subspace. To construct a\nsubspace on irregular domains for SUPRA, we propose using the Laplacian\neigenfunctions, which naturally adapt to domains' geometry and guarantee the\noptimal approximation for smooth functions. Experiments show that the SUPRA\nneural operator reduces error rates by up to 33% on various PDE datasets while\nmaintaining state-of-the-art computational efficiency."}
{"id": "2504.15883", "pdf": "https://arxiv.org/pdf/2504.15883", "abs": "https://arxiv.org/abs/2504.15883", "authors": ["Farida Mohsen", "Samir Belhaouari", "Zubair Shah"], "title": "Integrating Non-Linear Radon Transformation for Diabetic Retinopathy Grading", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Diabetic retinopathy is a serious ocular complication that poses a\nsignificant threat to patients' vision and overall health. Early detection and\naccurate grading are essential to prevent vision loss. Current automatic\ngrading methods rely heavily on deep learning applied to retinal fundus images,\nbut the complex, irregular patterns of lesions in these images, which vary in\nshape and distribution, make it difficult to capture subtle changes. This study\nintroduces RadFuse, a multi-representation deep learning framework that\nintegrates non-linear RadEx-transformed sinogram images with traditional fundus\nimages to enhance diabetic retinopathy detection and grading. Our RadEx\ntransformation, an optimized non-linear extension of the Radon transform,\ngenerates sinogram representations to capture complex retinal lesion patterns.\nBy leveraging both spatial and transformed domain information, RadFuse enriches\nthe feature set available to deep learning models, improving the\ndifferentiation of severity levels. We conducted extensive experiments on two\nbenchmark datasets, APTOS-2019 and DDR, using three convolutional neural\nnetworks (CNNs): ResNeXt-50, MobileNetV2, and VGG19. RadFuse showed significant\nimprovements over fundus-image-only models across all three CNN architectures\nand outperformed state-of-the-art methods on both datasets. For severity\ngrading across five stages, RadFuse achieved a quadratic weighted kappa of\n93.24%, an accuracy of 87.07%, and an F1-score of 87.17%. In binary\nclassification between healthy and diabetic retinopathy cases, the method\nreached an accuracy of 99.09%, precision of 98.58%, and recall of 99.6%,\nsurpassing previously established models. These results demonstrate RadFuse's\ncapacity to capture complex non-linear features, advancing diabetic retinopathy\nclassification and promoting the integration of advanced mathematical\ntransforms in medical image analysis."}
{"id": "2504.16081", "pdf": "https://arxiv.org/pdf/2504.16081", "abs": "https://arxiv.org/abs/2504.16081", "authors": ["Yimu Wang", "Xuye Liu", "Wei Pang", "Li Ma", "Shuai Yuan", "Paul Debevec", "Ning Yu"], "title": "Survey of Video Diffusion Models: Foundations, Implementations, and Applications", "categories": ["cs.CV", "cs.CL"], "comment": null, "summary": "Recent advances in diffusion models have revolutionized video generation,\noffering superior temporal consistency and visual quality compared to\ntraditional generative adversarial networks-based approaches. While this\nemerging field shows tremendous promise in applications, it faces significant\nchallenges in motion consistency, computational efficiency, and ethical\nconsiderations. This survey provides a comprehensive review of diffusion-based\nvideo generation, examining its evolution, technical foundations, and practical\napplications. We present a systematic taxonomy of current methodologies,\nanalyze architectural innovations and optimization strategies, and investigate\napplications across low-level vision tasks such as denoising and\nsuper-resolution. Additionally, we explore the synergies between diffusionbased\nvideo generation and related domains, including video representation learning,\nquestion answering, and retrieval. Compared to the existing surveys (Lei et\nal., 2024a;b; Melnik et al., 2024; Cao et al., 2023; Xing et al., 2024c) which\nfocus on specific aspects of video generation, such as human video synthesis\n(Lei et al., 2024a) or long-form content generation (Lei et al., 2024b), our\nwork provides a broader, more updated, and more fine-grained perspective on\ndiffusion-based approaches with a special section for evaluation metrics,\nindustry solutions, and training engineering techniques in video generation.\nThis survey serves as a foundational resource for researchers and practitioners\nworking at the intersection of diffusion models and video generation, providing\ninsights into both the theoretical frameworks and practical implementations\nthat drive this rapidly evolving field. A structured list of related works\ninvolved in this survey is also available on\nhttps://github.com/Eyeline-Research/Survey-Video-Diffusion."}
{"id": "2504.15766", "pdf": "https://arxiv.org/pdf/2504.15766", "abs": "https://arxiv.org/abs/2504.15766", "authors": ["Tobias Demmler", "Lennart Hartung", "Andreas Tamke", "Thao Dang", "Alexander Hegai", "Karsten Haug", "Lars Mikelsons"], "title": "Dynamic Intent Queries for Motion Transformer-based Trajectory Prediction", "categories": ["cs.RO", "cs.AI"], "comment": null, "summary": "In autonomous driving, accurately predicting the movements of other traffic\nparticipants is crucial, as it significantly influences a vehicle's planning\nprocesses. Modern trajectory prediction models strive to interpret complex\npatterns and dependencies from agent and map data. The Motion Transformer (MTR)\narchitecture and subsequent work define the most accurate methods in common\nbenchmarks such as the Waymo Open Motion Benchmark. The MTR model employs\npre-generated static intention points as initial goal points for trajectory\nprediction. However, the static nature of these points frequently leads to\nmisalignment with map data in specific traffic scenarios, resulting in\nunfeasible or unrealistic goal points. Our research addresses this limitation\nby integrating scene-specific dynamic intention points into the MTR model. This\nadaptation of the MTR model was trained and evaluated on the Waymo Open Motion\nDataset. Our findings demonstrate that incorporating dynamic intention points\nhas a significant positive impact on trajectory prediction accuracy, especially\nfor predictions over long time horizons. Furthermore, we analyze the impact on\nground truth trajectories which are not compliant with the map data or are\nillegal maneuvers."}
{"id": "2504.15905", "pdf": "https://arxiv.org/pdf/2504.15905", "abs": "https://arxiv.org/abs/2504.15905", "authors": ["Wenjing Xiao", "Chenglong Shi", "Miaojiang Chen", "Zhiquan Liu", "Min Chen", "H. Herbert Song"], "title": "GraphEdge: Dynamic Graph Partition and Task Scheduling for GNNs Computing in Edge Network", "categories": ["cs.LG", "cs.AI"], "comment": "17 pages,12 figures", "summary": "With the exponential growth of Internet of Things (IoT) devices, edge\ncomputing (EC) is gradually playing an important role in providing\ncost-effective services. However, existing approaches struggle to perform well\nin graph-structured scenarios where user data is correlated, such as traffic\nflow prediction and social relationship recommender systems. In particular,\ngraph neural network (GNN)-based approaches lead to expensive server\ncommunication cost. To address this problem, we propose GraphEdge, an efficient\nGNN-based EC architecture. It considers the EC system of GNN tasks, where there\nare associations between users and it needs to take into account the task data\nof its neighbors when processing the tasks of a user. Specifically, the\narchitecture first perceives the user topology and represents their data\nassociations as a graph layout at each time step. Then the graph layout is\noptimized by calling our proposed hierarchical traversal graph cut algorithm\n(HiCut), which cuts the graph layout into multiple weakly associated subgraphs\nbased on the aggregation characteristics of GNN, and the communication cost\nbetween different subgraphs during GNN inference is minimized. Finally, based\non the optimized graph layout, our proposed deep reinforcement learning (DRL)\nbased graph offloading algorithm (DRLGO) is executed to obtain the optimal\noffloading strategy for the tasks of users, the offloading strategy is\nsubgraph-based, it tries to offload user tasks in a subgraph to the same edge\nserver as possible while minimizing the task processing time and energy\nconsumption of the EC system. Experimental results show the good effectiveness\nand dynamic adaptation of our proposed architecture and it also performs well\neven in dynamic scenarios."}
{"id": "2504.15888", "pdf": "https://arxiv.org/pdf/2504.15888", "abs": "https://arxiv.org/abs/2504.15888", "authors": ["Zhiqiang Wei", "Lianqing Zheng", "Jianan Liu", "Tao Huang", "Qing-Long Han", "Wenwen Zhang", "Fengdeng Zhang"], "title": "MS-Occ: Multi-Stage LiDAR-Camera Fusion for 3D Semantic Occupancy Prediction", "categories": ["cs.CV"], "comment": "8 pages, 5 figures", "summary": "Accurate 3D semantic occupancy perception is essential for autonomous driving\nin complex environments with diverse and irregular objects. While\nvision-centric methods suffer from geometric inaccuracies, LiDAR-based\napproaches often lack rich semantic information. To address these limitations,\nMS-Occ, a novel multi-stage LiDAR-camera fusion framework which includes\nmiddle-stage fusion and late-stage fusion, is proposed, integrating LiDAR's\ngeometric fidelity with camera-based semantic richness via hierarchical\ncross-modal fusion. The framework introduces innovations at two critical\nstages: (1) In the middle-stage feature fusion, the Gaussian-Geo module\nleverages Gaussian kernel rendering on sparse LiDAR depth maps to enhance 2D\nimage features with dense geometric priors, and the Semantic-Aware module\nenriches LiDAR voxels with semantic context via deformable cross-attention; (2)\nIn the late-stage voxel fusion, the Adaptive Fusion (AF) module dynamically\nbalances voxel features across modalities, while the High Classification\nConfidence Voxel Fusion (HCCVF) module resolves semantic inconsistencies using\nself-attention-based refinement. Experiments on the nuScenes-OpenOccupancy\nbenchmark show that MS-Occ achieves an Intersection over Union (IoU) of 32.1%\nand a mean IoU (mIoU) of 25.3%, surpassing the state-of-the-art by +0.7% IoU\nand +2.4% mIoU. Ablation studies further validate the contribution of each\nmodule, with substantial improvements in small-object perception, demonstrating\nthe practical value of MS-Occ for safety-critical autonomous driving scenarios."}
{"id": "2212.09409", "pdf": "https://arxiv.org/pdf/2212.09409", "abs": "https://arxiv.org/abs/2212.09409", "authors": ["Dustin Wright", "Isabelle Augenstein"], "title": "Aggregating Soft Labels from Crowd Annotations Improves Uncertainty Estimation Under Distribution Shift", "categories": ["cs.CL"], "comment": "Accepted to PLOS One; 29 pages, 16 figures, 4 tables", "summary": "Selecting an effective training signal for machine learning tasks is\ndifficult: expert annotations are expensive, and crowd-sourced annotations may\nnot be reliable. Recent work has demonstrated that learning from a distribution\nover labels acquired from crowd annotations can be effective both for\nperformance and uncertainty estimation. However, this has mainly been studied\nusing a limited set of soft-labeling methods in an in-domain setting.\nAdditionally, no one method has been shown to consistently perform well across\ntasks, making it difficult to know a priori which to choose. To fill these\ngaps, this paper provides the first large-scale empirical study on learning\nfrom crowd labels in the out-of-domain setting, systematically analyzing 8\nsoft-labeling methods on 4 language and vision tasks. Additionally, we propose\nto aggregate soft-labels via a simple average in order to achieve consistent\nperformance across tasks. We demonstrate that this yields classifiers with\nimproved predictive uncertainty estimation in most settings while maintaining\nconsistent raw performance compared to learning from individual soft-labeling\nmethods or taking a majority vote of the annotations. We additionally highlight\nthat in regimes with abundant or minimal training data, the selection of soft\nlabeling method is less important, while for highly subjective labels and\nmoderate amounts of training data, aggregation yields significant improvements\nin uncertainty estimation over individual methods. Code can be found at\nhttps://github.com/copenlu/aggregating-crowd-annotations-ood."}
{"id": "2504.15779", "pdf": "https://arxiv.org/pdf/2504.15779", "abs": "https://arxiv.org/abs/2504.15779", "authors": ["Aaron J. Gutknecht", "Fernando E. Rosas", "David A. Ehrlich", "Abdullah Makkeh", "Pedro A. M. Mediano", "Michael Wibral"], "title": "Shannon invariants: A scalable approach to information decomposition", "categories": ["cs.IT", "cs.AI", "cs.LG", "math.IT", "nlin.AO", "physics.data-an"], "comment": "16 pages, 4 Figures", "summary": "Distributed systems, such as biological and artificial neural networks,\nprocess information via complex interactions engaging multiple subsystems,\nresulting in high-order patterns with distinct properties across scales.\nInvestigating how these systems process information remains challenging due to\ndifficulties in defining appropriate multivariate metrics and ensuring their\nscalability to large systems. To address these challenges, we introduce a novel\nframework based on what we call \"Shannon invariants\" -- quantities that capture\nessential properties of high-order information processing in a way that depends\nonly on the definition of entropy and can be efficiently calculated for large\nsystems. Our theoretical results demonstrate how Shannon invariants can be used\nto resolve long-standing ambiguities regarding the interpretation of widely\nused multivariate information-theoretic measures. Moreover, our practical\nresults reveal distinctive information-processing signatures of various deep\nlearning architectures across layers, which lead to new insights into how these\nsystems process information and how this evolves during training. Overall, our\nframework resolves fundamental limitations in analyzing high-order phenomena\nand offers broad opportunities for theoretical developments and empirical\nanalyses."}
{"id": "2504.15920", "pdf": "https://arxiv.org/pdf/2504.15920", "abs": "https://arxiv.org/abs/2504.15920", "authors": ["Xiang Li", "Haobing Liu", "Jianpeng Qi", "Yuan Cao", "Guoqing Chao", "Yanwei Yu"], "title": "ScaleGNN: Towards Scalable Graph Neural Networks via Adaptive High-order Neighboring Feature Fusion", "categories": ["cs.LG"], "comment": null, "summary": "Graph Neural Networks (GNNs) have demonstrated strong performance across\nvarious graph-based tasks by effectively capturing relational information\nbetween nodes. These models rely on iterative message passing to propagate node\nfeatures, enabling nodes to aggregate information from their neighbors. Recent\nresearch has significantly improved the message-passing mechanism, enhancing\nGNN scalability on large-scale graphs. However, GNNs still face two main\nchallenges: over-smoothing, where excessive message passing results in\nindistinguishable node representations, especially in deep networks\nincorporating high-order neighbors; and scalability issues, as traditional\narchitectures suffer from high model complexity and increased inference time\ndue to redundant information aggregation. This paper proposes a novel framework\nfor large-scale graphs named ScaleGNN that simultaneously addresses both\nchallenges by adaptively fusing multi-level graph features. We first construct\nneighbor matrices for each order, learning their relative information through\ntrainable weights through an adaptive high-order feature fusion module. This\nallows the model to selectively emphasize informative high-order neighbors\nwhile reducing unnecessary computational costs. Additionally, we introduce a\nHigh-order redundant feature masking mechanism based on a Local Contribution\nScore (LCS), which enables the model to retain only the most relevant neighbors\nat each order, preventing redundant information propagation. Furthermore,\nlow-order enhanced feature aggregation adaptively integrates low-order and\nhigh-order features based on task relevance, ensuring effective capture of both\nlocal and global structural information without excessive complexity. Extensive\nexperiments on real-world datasets demonstrate that our approach consistently\noutperforms state-of-the-art GNN models in both accuracy and computational\nefficiency."}
{"id": "2504.15918", "pdf": "https://arxiv.org/pdf/2504.15918", "abs": "https://arxiv.org/abs/2504.15918", "authors": ["Chang Zong", "Bin Li", "Shoujun Zhou", "Jian Wan", "Lei Zhang"], "title": "Ask2Loc: Learning to Locate Instructional Visual Answers by Asking Questions", "categories": ["cs.CV", "cs.AI", "cs.HC", "68T45, 68T20"], "comment": "16 pages, 8 figures", "summary": "Locating specific segments within an instructional video is an efficient way\nto acquire guiding knowledge. Generally, the task of obtaining video segments\nfor both verbal explanations and visual demonstrations is known as visual\nanswer localization (VAL). However, users often need multiple interactions to\nobtain answers that align with their expectations when using the system. During\nthese interactions, humans deepen their understanding of the video content by\nasking themselves questions, thereby accurately identifying the location.\nTherefore, we propose a new task, named In-VAL, to simulate the multiple\ninteractions between humans and videos in the procedure of obtaining visual\nanswers. The In-VAL task requires interactively addressing several semantic gap\nissues, including 1) the ambiguity of user intent in the input questions, 2)\nthe incompleteness of language in video subtitles, and 3) the fragmentation of\ncontent in video segments. To address these issues, we propose Ask2Loc, a\nframework for resolving In-VAL by asking questions. It includes three key\nmodules: 1) a chatting module to refine initial questions and uncover clear\nintentions, 2) a rewriting module to generate fluent language and create\ncomplete descriptions, and 3) a searching module to broaden local context and\nprovide integrated content. We conduct extensive experiments on three\nreconstructed In-VAL datasets. Compared to traditional end-to-end and two-stage\nmethods, our proposed Ask2Loc can improve performance by up to 14.91 (mIoU) on\nthe In-VAL task. Our code and datasets can be accessed at\nhttps://github.com/changzong/Ask2Loc."}
{"id": "2407.04615", "pdf": "https://arxiv.org/pdf/2407.04615", "abs": "https://arxiv.org/abs/2407.04615", "authors": ["Sergey Troshin", "Vlad Niculae", "Antske Fokkens"], "title": "On the Low-Rank Parametrization of Reward Models for Controlled Language Generation", "categories": ["cs.CL"], "comment": null, "summary": "Language models trained on large amounts of data are known to produce\ninappropriate content in some cases and require careful tuning to be used in\nthe real world. We revisit an effective and modular approach for\ncontrollability of the language models, when an external expert model guides\nthe decoding. Particularly, we zoom in into the parametrization choice of an\nexternal expert, highlighting the difference between low-rank and higher-rank\nparametrizations. Higher-rank experts are designed to support high flexibility\nwhen representing the rewards, leading to higher computational costs during\ndecoding. However, we demonstrate that they might not use their full\nflexibility. By analyzing the recently proposed reward-augmented decoding\napproach (RAD), which uses a higher-rank expert model, we introduce a simpler\nbut more efficient low-rank parametrization of the expert model enabling fast\nand effective guided decoding. We empirically show that the low-rank RAD\nperforms on par with the more flexible RAD on a detoxification and a sentiment\ncontrol task, while requiring only a single reward model call per generated\ntoken."}
{"id": "2504.15804", "pdf": "https://arxiv.org/pdf/2504.15804", "abs": "https://arxiv.org/abs/2504.15804", "authors": ["Ning Wang", "Bingkun Yao", "Jie Zhou", "Yuchen Hu", "Xi Wang", "Nan Guan", "Zhe Jiang"], "title": "Insights from Verification: Training a Verilog Generation LLM with Reinforcement Learning with Testbench Feedback", "categories": ["cs.AR", "cs.AI"], "comment": null, "summary": "Large language models (LLMs) have shown strong performance in Verilog\ngeneration from natural language description. However, ensuring the functional\ncorrectness of the generated code remains a significant challenge. This paper\nintroduces a method that integrates verification insights from testbench into\nthe training of Verilog generation LLMs, aligning the training with the\nfundamental goal of hardware design: functional correctness. The main obstacle\nin using LLMs for Verilog code generation is the lack of sufficient functional\nverification data, particularly testbenches paired with design specifications\nand code. To address this problem, we introduce an automatic testbench\ngeneration pipeline that decomposes the process and uses feedback from the\nVerilog compiler simulator (VCS) to reduce hallucination and ensure\ncorrectness. We then use the testbench to evaluate the generated codes and\ncollect them for further training, where verification insights are introduced.\nOur method applies reinforcement learning (RL), specifically direct preference\noptimization (DPO), to align Verilog code generation with functional\ncorrectness by training preference pairs based on testbench outcomes. In\nevaluations on VerilogEval-Machine, VerilogEval-Human, RTLLM v1.1, RTLLM v2,\nand VerilogEval v2, our approach consistently outperforms state-of-the-art\nbaselines in generating functionally correct Verilog code. We open source all\ntraining code, data, and models at\nhttps://anonymous.4open.science/r/VeriPrefer-E88B."}
{"id": "2504.15924", "pdf": "https://arxiv.org/pdf/2504.15924", "abs": "https://arxiv.org/abs/2504.15924", "authors": ["Alycia Carey", "Xintao Wu"], "title": "Achieving Distributive Justice in Federated Learning via Uncertainty Quantification", "categories": ["cs.LG", "cs.AI", "stat.ML", "68T01", "I.2.0"], "comment": "21 pages, 1 figure, 7 tables", "summary": "Client-level fairness metrics for federated learning are used to ensure that\nall clients in a federation either: a) have similar final performance on their\nlocal data distributions (i.e., client parity), or b) obtain final performance\non their local data distributions relative to their contribution to the\nfederated learning process (i.e., contribution fairness). While a handful of\nworks that propose either client-parity or contribution-based fairness metrics\nground their definitions and decisions in social theories of equality -- such\nas distributive justice -- most works arbitrarily choose what notion of\nfairness to align with which makes it difficult for practitioners to choose\nwhich fairness metric aligns best with their fairness ethics. In this work, we\npropose UDJ-FL (Uncertainty-based Distributive Justice for Federated Learning),\na flexible federated learning framework that can achieve multiple distributive\njustice-based client-level fairness metrics. Namely, by utilizing techniques\ninspired by fair resource allocation, in conjunction with performing aleatoric\nuncertainty-based client weighing, our UDJ-FL framework is able to achieve\negalitarian, utilitarian, Rawls' difference principle, or desert-based\nclient-level fairness. We empirically show the ability of UDJ-FL to achieve all\nfour defined distributive justice-based client-level fairness metrics in\naddition to providing fairness equivalent to (or surpassing) other popular fair\nfederated learning works. Further, we provide justification for why aleatoric\nuncertainty weighing is necessary to the construction of our UDJ-FL framework\nas well as derive theoretical guarantees for the generalization bounds of\nUDJ-FL. Our code is publicly available at\nhttps://github.com/alycia-noel/UDJ-FL."}
{"id": "2504.15921", "pdf": "https://arxiv.org/pdf/2504.15921", "abs": "https://arxiv.org/abs/2504.15921", "authors": ["Jian Hu", "Dimitrios Korkinof", "Shaogang Gong", "Mariano Beguerisse-Diaz"], "title": "ViSMaP: Unsupervised Hour-long Video Summarisation by Meta-Prompting", "categories": ["cs.CV"], "comment": null, "summary": "We introduce ViSMap: Unsupervised Video Summarisation by Meta Prompting, a\nsystem to summarise hour long videos with no-supervision. Most existing video\nunderstanding models work well on short videos of pre-segmented events, yet\nthey struggle to summarise longer videos where relevant events are sparsely\ndistributed and not pre-segmented. Moreover, long-form video understanding\noften relies on supervised hierarchical training that needs extensive\nannotations which are costly, slow and prone to inconsistency. With ViSMaP we\nbridge the gap between short videos (where annotated data is plentiful) and\nlong ones (where it's not). We rely on LLMs to create optimised\npseudo-summaries of long videos using segment descriptions from short ones.\nThese pseudo-summaries are used as training data for a model that generates\nlong-form video summaries, bypassing the need for expensive annotations of long\nvideos. Specifically, we adopt a meta-prompting strategy to iteratively\ngenerate and refine creating pseudo-summaries of long videos. The strategy\nleverages short clip descriptions obtained from a supervised short video model\nto guide the summary. Each iteration uses three LLMs working in sequence: one\nto generate the pseudo-summary from clip descriptions, another to evaluate it,\nand a third to optimise the prompt of the generator. This iteration is\nnecessary because the quality of the pseudo-summaries is highly dependent on\nthe generator prompt, and varies widely among videos. We evaluate our summaries\nextensively on multiple datasets; our results show that ViSMaP achieves\nperformance comparable to fully supervised state-of-the-art models while\ngeneralising across domains without sacrificing performance. Code will be\nreleased upon publication."}
{"id": "2408.06276", "pdf": "https://arxiv.org/pdf/2408.06276", "abs": "https://arxiv.org/abs/2408.06276", "authors": ["Jieyong Kim", "Hyunseo Kim", "Hyunjin Cho", "SeongKu Kang", "Buru Chang", "Jinyoung Yeo", "Dongha Lee"], "title": "Review-driven Personalized Preference Reasoning with Large Language Models for Recommendation", "categories": ["cs.CL"], "comment": "Accepted to SIGIR 2025", "summary": "Recent advancements in Large Language Models (LLMs) have demonstrated\nexceptional performance across a wide range of tasks, generating significant\ninterest in their application to recommendation systems. However, existing\nmethods have not fully capitalized on the potential of LLMs, often constrained\nby limited input information or failing to fully utilize their advanced\nreasoning capabilities. To address these limitations, we introduce EXP3RT, a\nnovel LLM-based recommender designed to leverage rich preference information\ncontained in user and item reviews. EXP3RT is basically fine-tuned through\ndistillation from a teacher LLM to perform three key tasks in order: EXP3RT\nfirst extracts and encapsulates essential subjective preferences from raw\nreviews, aggregates and summarizes them according to specific criteria to\ncreate user and item profiles. It then generates detailed step-by-step\nreasoning followed by predicted rating, i.e., reasoning-enhanced rating\nprediction, by considering both subjective and objective information from\nuser/item profiles and item descriptions. This personalized preference\nreasoning from EXP3RT enhances rating prediction accuracy and also provides\nfaithful and reasonable explanations for recommendation. Extensive experiments\nshow that EXP3RT outperforms existing methods on both rating prediction and\ncandidate item reranking for top-k recommendation, while significantly\nenhancing the explainability of recommendation systems."}
{"id": "2504.15876", "pdf": "https://arxiv.org/pdf/2504.15876", "abs": "https://arxiv.org/abs/2504.15876", "authors": ["Qizhen Wu Lei Chen", "Kexin Liu", "Jinhu Lü"], "title": "Bidirectional Task-Motion Planning Based on Hierarchical Reinforcement Learning for Strategic Confrontation", "categories": ["cs.RO", "cs.AI"], "comment": null, "summary": "In swarm robotics, confrontation scenarios, including strategic\nconfrontations, require efficient decision-making that integrates discrete\ncommands and continuous actions. Traditional task and motion planning methods\nseparate decision-making into two layers, but their unidirectional structure\nfails to capture the interdependence between these layers, limiting\nadaptability in dynamic environments. Here, we propose a novel bidirectional\napproach based on hierarchical reinforcement learning, enabling dynamic\ninteraction between the layers. This method effectively maps commands to task\nallocation and actions to path planning, while leveraging cross-training\ntechniques to enhance learning across the hierarchical framework. Furthermore,\nwe introduce a trajectory prediction model that bridges abstract task\nrepresentations with actionable planning goals. In our experiments, it achieves\nover 80\\% in confrontation win rate and under 0.01 seconds in decision time,\noutperforming existing approaches. Demonstrations through large-scale tests and\nreal-world robot experiments further emphasize the generalization capabilities\nand practical applicability of our method."}
{"id": "2504.15930", "pdf": "https://arxiv.org/pdf/2504.15930", "abs": "https://arxiv.org/abs/2504.15930", "authors": ["Yinmin Zhong", "Zili Zhang", "Xiaoniu Song", "Hanpeng Hu", "Chao Jin", "Bingyang Wu", "Nuo Chen", "Yukun Chen", "Yu Zhou", "Changyi Wan", "Hongyu Zhou", "Yimin Jiang", "Yibo Zhu", "Daxin Jiang"], "title": "StreamRL: Scalable, Heterogeneous, and Elastic RL for LLMs with Disaggregated Stream Generation", "categories": ["cs.LG", "cs.DC"], "comment": null, "summary": "Reinforcement learning (RL) has become the core post-training technique for\nlarge language models (LLMs). RL for LLMs involves two stages: generation and\ntraining. The LLM first generates samples online, which are then used to derive\nrewards for training. The conventional view holds that the colocated\narchitecture, where the two stages share resources via temporal multiplexing,\noutperforms the disaggregated architecture, in which dedicated resources are\nassigned to each stage. However, in real-world deployments, we observe that the\ncolocated architecture suffers from resource coupling, where the two stages are\nconstrained to use the same resources. This coupling compromises the\nscalability and cost-efficiency of colocated RL in large-scale training. In\ncontrast, the disaggregated architecture allows for flexible resource\nallocation, supports heterogeneous training setups, and facilitates\ncross-datacenter deployment.\n  StreamRL is designed with disaggregation from first principles and fully\nunlocks its potential by addressing two types of performance bottlenecks in\nexisting disaggregated RL frameworks: pipeline bubbles, caused by stage\ndependencies, and skewness bubbles, resulting from long-tail output length\ndistributions. To address pipeline bubbles, StreamRL breaks the traditional\nstage boundary in synchronous RL algorithms through stream generation and\nachieves full overlapping in asynchronous RL. To address skewness bubbles,\nStreamRL employs an output-length ranker model to identify long-tail samples\nand reduces generation time via skewness-aware dispatching and scheduling.\nExperiments show that StreamRL improves throughput by up to 2.66x compared to\nexisting state-of-the-art systems, and improves cost-effectiveness by up to\n1.33x in a heterogeneous, cross-datacenter setting."}
{"id": "2504.15928", "pdf": "https://arxiv.org/pdf/2504.15928", "abs": "https://arxiv.org/abs/2504.15928", "authors": ["Meng Wang", "Tian Lin", "Qingshan Hou", "Aidi Lin", "Jingcheng Wang", "Qingsheng Peng", "Truong X. Nguyen", "Danqi Fang", "Ke Zou", "Ting Xu", "Cancan Xue", "Ten Cheer Quek", "Qinkai Yu", "Minxin Liu", "Hui Zhou", "Zixuan Xiao", "Guiqin He", "Huiyu Liang", "Tingkun Shi", "Man Chen", "Linna Liu", "Yuanyuan Peng", "Lianyu Wang", "Qiuming Hu", "Junhong Chen", "Zhenhua Zhang", "Cheng Chen", "Yitian Zhao", "Dianbo Liu", "Jianhua Wu", "Xinjian Chen", "Changqing Zhang", "Triet Thanh Nguyen", "Yanda Meng", "Yalin Zheng", "Yih Chung Tham", "Carol Y. Cheung", "Huazhu Fu", "Haoyu Chen", "Ching-Yu Cheng"], "title": "A Clinician-Friendly Platform for Ophthalmic Image Analysis Without Technical Barriers", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Artificial intelligence (AI) shows remarkable potential in medical imaging\ndiagnostics, but current models typically require retraining when deployed\nacross different clinical centers, limiting their widespread adoption. We\nintroduce GlobeReady, a clinician-friendly AI platform that enables ocular\ndisease diagnosis without retraining/fine-tuning or technical expertise.\nGlobeReady achieves high accuracy across imaging modalities: 93.9-98.5% for an\n11-category fundus photo dataset and 87.2-92.7% for a 15-category OCT dataset.\nThrough training-free local feature augmentation, it addresses domain shifts\nacross centers and populations, reaching an average accuracy of 88.9% across\nfive centers in China, 86.3% in Vietnam, and 90.2% in the UK. The built-in\nconfidence-quantifiable diagnostic approach further boosted accuracy to\n94.9-99.4% (fundus) and 88.2-96.2% (OCT), while identifying out-of-distribution\ncases at 86.3% (49 CFP categories) and 90.6% (13 OCT categories). Clinicians\nfrom multiple countries rated GlobeReady highly (average 4.6 out of 5) for its\nusability and clinical relevance. These results demonstrate GlobeReady's\nrobust, scalable diagnostic capability and potential to support ophthalmic care\nwithout technical barriers."}
{"id": "2409.18110", "pdf": "https://arxiv.org/pdf/2409.18110", "abs": "https://arxiv.org/abs/2409.18110", "authors": ["Hung-Ting Chen", "Eunsol Choi"], "title": "Open-World Evaluation for Retrieving Diverse Perspectives", "categories": ["cs.CL", "cs.IR"], "comment": null, "summary": "We study retrieving a set of documents that covers various perspectives on a\ncomplex and contentious question (e.g., will ChatGPT do more harm than good?).\nWe curate a Benchmark for Retrieval Diversity for Subjective questions (BERDS),\nwhere each example consists of a question and diverse perspectives associated\nwith the question, sourced from survey questions and debate websites. On this\ndata, retrievers paired with a corpus are evaluated to surface a document set\nthat contains diverse perspectives. Our framing diverges from most retrieval\ntasks in that document relevancy cannot be decided by simple string matches to\nreferences. Instead, we build a language model-based automatic evaluator that\ndecides whether each retrieved document contains a perspective. This allows us\nto evaluate the performance of three different types of corpus (Wikipedia, web\nsnapshot, and corpus constructed on the fly with retrieved pages from the\nsearch engine) paired with retrievers. Retrieving diverse documents remains\nchallenging, with the outputs from existing retrievers covering all\nperspectives on only 40% of the examples. We further study the effectiveness of\nquery expansion and diversity-focused reranking approaches and analyze\nretriever sycophancy."}
{"id": "2504.15894", "pdf": "https://arxiv.org/pdf/2504.15894", "abs": "https://arxiv.org/abs/2504.15894", "authors": ["Chengbo Zheng", "Tim Miller", "Alina Bialkowski", "H Peter Soyer", "Monika Janda"], "title": "Supporting Data-Frame Dynamics in AI-assisted Decision Making", "categories": ["cs.HC", "cs.AI"], "comment": "Presented at the 2025 ACM Workshop on Human-AI Interaction for\n  Augmented Reasoning, Report Number: CHI25-WS-AUGMENTED-REASONING", "summary": "High stakes decision-making often requires a continuous interplay between\nevolving evidence and shifting hypotheses, a dynamic that is not well supported\nby current AI decision support systems. In this paper, we introduce a\nmixed-initiative framework for AI assisted decision making that is grounded in\nthe data-frame theory of sensemaking and the evaluative AI paradigm. Our\napproach enables both humans and AI to collaboratively construct, validate, and\nadapt hypotheses. We demonstrate our framework with an AI-assisted skin cancer\ndiagnosis prototype that leverages a concept bottleneck model to facilitate\ninterpretable interactions and dynamic updates to diagnostic hypotheses."}
{"id": "2504.15956", "pdf": "https://arxiv.org/pdf/2504.15956", "abs": "https://arxiv.org/abs/2504.15956", "authors": ["Jerry Yao-Chieh Hu", "Hude Liu", "Hong-Yu Chen", "Weimin Wu", "Han Liu"], "title": "Universal Approximation with Softmax Attention", "categories": ["cs.LG", "cs.AI", "stat.ML"], "comment": null, "summary": "We prove that with linear transformations, both (i) two-layer self-attention\nand (ii) one-layer self-attention followed by a softmax function are universal\napproximators for continuous sequence-to-sequence functions on compact domains.\nOur main technique is a new interpolation-based method for analyzing\nattention's internal mechanism. This leads to our key insight: self-attention\nis able to approximate a generalized version of ReLU to arbitrary precision,\nand hence subsumes many known universal approximators. Building on these, we\nshow that two-layer multi-head attention alone suffices as a\nsequence-to-sequence universal approximator. In contrast, prior works rely on\nfeed-forward networks to establish universal approximation in Transformers.\nFurthermore, we extend our techniques to show that, (softmax-)attention-only\nlayers are capable of approximating various statistical models in-context. We\nbelieve these techniques hold independent interest."}
{"id": "2504.15929", "pdf": "https://arxiv.org/pdf/2504.15929", "abs": "https://arxiv.org/abs/2504.15929", "authors": ["Saban Ozturk", "Melih B. Yilmaz", "Muti Kara", "M. Talat Yavuz", "Aykut Koç", "Tolga Çukur"], "title": "Meta-Entity Driven Triplet Mining for Aligning Medical Vision-Language Models", "categories": ["cs.CV", "cs.AI"], "comment": "18 pages, 7 figures, 6 tables", "summary": "Diagnostic imaging relies on interpreting both images and radiology reports,\nbut the growing data volumes place significant pressure on medical experts,\nyielding increased errors and workflow backlogs. Medical vision-language models\n(med-VLMs) have emerged as a powerful framework to efficiently process\nmultimodal imaging data, particularly in chest X-ray (CXR) evaluations, albeit\ntheir performance hinges on how well image and text representations are\naligned. Existing alignment methods, predominantly based on contrastive\nlearning, prioritize separation between disease classes over segregation of\nfine-grained pathology attributes like location, size or severity, leading to\nsuboptimal representations. Here, we propose MedTrim (Meta-entity-driven\nTriplet mining), a novel method that enhances image-text alignment through\nmultimodal triplet learning synergistically guided by disease class as well as\nadjectival and directional pathology descriptors. Unlike common alignment\nmethods that separate broad disease classes, MedTrim leverages structured\nmeta-entity information to preserve subtle but clinically significant\nintra-class variations. For this purpose, we first introduce an ontology-based\nentity recognition module that extracts pathology-specific meta-entities from\nCXR reports, as annotations on pathology attributes are rare in public\ndatasets. For refined sample selection in triplet mining, we then introduce a\nnovel score function that captures an aggregate measure of inter-sample\nsimilarity based on disease classes and adjectival/directional descriptors.\nLastly, we introduce a multimodal triplet alignment objective for explicit\nwithin- and cross-modal alignment between samples sharing detailed pathology\ncharacteristics. Our demonstrations indicate that MedTrim improves performance\nin downstream retrieval and classification tasks compared to state-of-the-art\nalignment methods."}
{"id": "2410.02355", "pdf": "https://arxiv.org/pdf/2410.02355", "abs": "https://arxiv.org/abs/2410.02355", "authors": ["Junfeng Fang", "Houcheng Jiang", "Kun Wang", "Yunshan Ma", "Shi Jie", "Xiang Wang", "Xiangnan He", "Tat-seng Chua"], "title": "AlphaEdit: Null-Space Constrained Knowledge Editing for Language Models", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Large language models (LLMs) often exhibit hallucinations due to incorrect or\noutdated knowledge. Hence, model editing methods have emerged to enable\ntargeted knowledge updates. To achieve this, a prevailing paradigm is the\nlocating-then-editing approach, which first locates influential parameters and\nthen edits them by introducing a perturbation. While effective, current studies\nhave demonstrated that this perturbation inevitably disrupt the originally\npreserved knowledge within LLMs, especially in sequential editing scenarios. To\naddress this, we introduce AlphaEdit, a novel solution that projects\nperturbation onto the null space of the preserved knowledge before applying it\nto the parameters. We theoretically prove that this projection ensures the\noutput of post-edited LLMs remains unchanged when queried about the preserved\nknowledge, thereby mitigating the issue of disruption. Extensive experiments on\nvarious LLMs, including LLaMA3, GPT2-XL, and GPT-J, show that AlphaEdit boosts\nthe performance of most locating-then-editing methods by an average of 36.7%\nwith a single line of additional code for projection solely. Our code is\navailable at: https://github.com/jianghoucheng/AlphaEdit."}
{"id": "2504.15912", "pdf": "https://arxiv.org/pdf/2504.15912", "abs": "https://arxiv.org/abs/2504.15912", "authors": ["Riley Pierson", "Armin Moin"], "title": "Automated Bug Report Prioritization in Large Open-Source Projects", "categories": ["cs.SE", "cs.AI"], "comment": null, "summary": "Large open-source projects receive a large number of issues (known as bugs),\nincluding software defect (i.e., bug) reports and new feature requests from\ntheir user and developer communities at a fast rate. The often limited project\nresources do not allow them to deal with all issues. Instead, they have to\nprioritize them according to the project's priorities and the issues'\nseverities. In this paper, we propose a novel approach to automated bug\nprioritization based on the natural language text of the bug reports that are\nstored in the open bug repositories of the issue-tracking systems. We conduct\ntopic modeling using a variant of LDA called TopicMiner-MTM and text\nclassification with the BERT large language model to achieve a higher\nperformance level compared to the state-of-the-art. Experimental results using\nan existing reference dataset containing 85,156 bug reports of the Eclipse\nPlatform project indicate that we outperform existing approaches in terms of\nAccuracy, Precision, Recall, and F1-measure of the bug report priority\nprediction."}
{"id": "2504.15995", "pdf": "https://arxiv.org/pdf/2504.15995", "abs": "https://arxiv.org/abs/2504.15995", "authors": ["Sindhuja Madabushi", "Ahmad Faraz Khan", "Haider Ali", "Jin-Hee Cho"], "title": "OPUS-VFL: Incentivizing Optimal Privacy-Utility Tradeoffs in Vertical Federated Learning", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Vertical Federated Learning (VFL) enables organizations with disjoint feature\nspaces but shared user bases to collaboratively train models without sharing\nraw data. However, existing VFL systems face critical limitations: they often\nlack effective incentive mechanisms, struggle to balance privacy-utility\ntradeoffs, and fail to accommodate clients with heterogeneous resource\ncapabilities. These challenges hinder meaningful participation, degrade model\nperformance, and limit practical deployment. To address these issues, we\npropose OPUS-VFL, an Optimal Privacy-Utility tradeoff Strategy for VFL.\nOPUS-VFL introduces a novel, privacy-aware incentive mechanism that rewards\nclients based on a principled combination of model contribution, privacy\npreservation, and resource investment. It employs a lightweight leave-one-out\n(LOO) strategy to quantify feature importance per client, and integrates an\nadaptive differential privacy mechanism that enables clients to dynamically\ncalibrate noise levels to optimize their individual utility. Our framework is\ndesigned to be scalable, budget-balanced, and robust to inference and poisoning\nattacks. Extensive experiments on benchmark datasets (MNIST, CIFAR-10, and\nCIFAR-100) demonstrate that OPUS-VFL significantly outperforms state-of-the-art\nVFL baselines in both efficiency and robustness. It reduces label inference\nattack success rates by up to 20%, increases feature inference reconstruction\nerror (MSE) by over 30%, and achieves up to 25% higher incentives for clients\nthat contribute meaningfully while respecting privacy and cost constraints.\nThese results highlight the practicality and innovation of OPUS-VFL as a\nsecure, fair, and performance-driven solution for real-world VFL."}
{"id": "2504.15931", "pdf": "https://arxiv.org/pdf/2504.15931", "abs": "https://arxiv.org/abs/2504.15931", "authors": ["Ekaterina Kondrateva", "Sandzhi Barg", "Mikhail Vasiliev"], "title": "Benchmarking the Reproducibility of Brain MRI Segmentation Across Scanners and Time", "categories": ["cs.CV"], "comment": null, "summary": "Accurate and reproducible brain morphometry from structural MRI is critical\nfor monitoring neuroanatomical changes across time and across imaging domains.\nAlthough deep learning has accelerated segmentation workflows, scanner-induced\nvariability and reproducibility limitations remain-especially in longitudinal\nand multi-site settings. In this study, we benchmark two modern segmentation\npipelines, FastSurfer and SynthSeg, both integrated into FreeSurfer, one of the\nmost widely adopted tools in neuroimaging.\n  Using two complementary datasets - a 17-year longitudinal cohort (SIMON) and\na 9-site test-retest cohort (SRPBS)-we quantify inter-scan segmentation\nvariability using Dice coefficient, Surface Dice, Hausdorff Distance (HD95),\nand Mean Absolute Percentage Error (MAPE). Our results reveal up to 7-8% volume\nvariation in small subcortical structures such as the amygdala and ventral\ndiencephalon, even under controlled test-retest conditions. This raises a key\nquestion: is it feasible to detect subtle longitudinal changes on the order of\n5-10% in pea-sized brain regions, given the magnitude of domain-induced\nmorphometric noise?\n  We further analyze the effects of registration templates and interpolation\nmodes, and propose surface-based quality filtering to improve segmentation\nreliability. This study provides a reproducible benchmark for morphometric\nreproducibility and emphasizes the need for harmonization strategies in\nreal-world neuroimaging studies.\n  Code and figures: https://github.com/kondratevakate/brain-mri-segmentation"}
{"id": "2410.19503", "pdf": "https://arxiv.org/pdf/2410.19503", "abs": "https://arxiv.org/abs/2410.19503", "authors": ["Jahyun Koo", "Yerin Hwang", "Yongil Kim", "Taegwan Kang", "Hyunkyung Bae", "Kyomin Jung"], "title": "SWITCH: Studying with Teacher for Knowledge Distillation of Large Language Models", "categories": ["cs.CL"], "comment": "NAACL 2025 Findings", "summary": "Despite the success of Large Language Models (LLMs), they still face\nchallenges related to high inference costs and memory requirements. To address\nthese issues, Knowledge Distillation (KD) has emerged as a popular method for\nmodel compression, with student-generated outputs (SGOs) as training data being\nparticularly notable for reducing the mismatch between training and inference.\nHowever, SGOs often produce noisy and biased sequences, which can lead to\nmisguidance from the teacher model, especially in long sequences. To mitigate\nthese challenges, we propose SWITCH (Studying WIth TeaCHer for Knowledge\nDistillation), a novel approach that strategically incorporates the teacher\nmodel during the student's sequence generation. SWITCH identifies discrepancies\nbetween the token probabilities of the teacher and student models, allowing the\nteacher to intervene selectively, particularly in long sequences that are more\nprone to teacher misguidance. Extensive experimental results across three model\nfamilies and five instruction-following datasets show that SWITCH surpasses\ntraditional KD methods, particularly excelling in the generation of long\nsequential data."}
{"id": "2504.15927", "pdf": "https://arxiv.org/pdf/2504.15927", "abs": "https://arxiv.org/abs/2504.15927", "authors": ["Ling Cheng", "Jiashu Pu", "Ruicheng Liang", "Qian Shao", "Hezhe Qiao", "Feida Zhu"], "title": "New Recipe for Semi-supervised Community Detection: Clique Annealing under Crystallization Kinetics", "categories": ["cs.SI", "cs.AI"], "comment": "arXiv admin note: text overlap with arXiv:2203.05898 by other authors", "summary": "Semi-supervised community detection methods are widely used for identifying\nspecific communities due to the label scarcity. Existing semi-supervised\ncommunity detection methods typically involve two learning stages learning in\nboth initial identification and subsequent adjustment, which often starts from\nan unreasonable community core candidate. Moreover, these methods encounter\nscalability issues because they depend on reinforcement learning and generative\nadversarial networks, leading to higher computational costs and restricting the\nselection of candidates. To address these limitations, we draw a parallel\nbetween crystallization kinetics and community detection to integrate the\nspontaneity of the annealing process into community detection. Specifically, we\nliken community detection to identifying a crystal subgrain (core) that expands\ninto a complete grain (community) through a process similar to annealing. Based\non this finding, we propose CLique ANNealing (CLANN), which applies kinetics\nconcepts to community detection by integrating these principles into the\noptimization process to strengthen the consistency of the community core.\nSubsequently, a learning-free Transitive Annealer was employed to refine the\nfirst-stage candidates by merging neighboring cliques and repositioning the\ncommunity core, enabling a spontaneous growth process that enhances\nscalability. Extensive experiments on \\textbf{43} different network settings\ndemonstrate that CLANN outperforms state-of-the-art methods across multiple\nreal-world datasets, showcasing its exceptional efficacy and efficiency in\ncommunity detection."}
{"id": "2504.16020", "pdf": "https://arxiv.org/pdf/2504.16020", "abs": "https://arxiv.org/abs/2504.16020", "authors": ["Soham Sane"], "title": "AlphaGrad: Non-Linear Gradient Normalization Optimizer", "categories": ["cs.LG", "cs.AI", "cs.NE", "stat.ML"], "comment": null, "summary": "We introduce AlphaGrad, a memory-efficient, conditionally stateless optimizer\naddressing the memory overhead and hyperparameter complexity of adaptive\nmethods like Adam. AlphaGrad enforces scale invariance via tensor-wise L2\ngradient normalization followed by a smooth hyperbolic tangent transformation,\n$g' = \\tanh(\\alpha \\cdot \\tilde{g})$, controlled by a single steepness\nparameter $\\alpha$. Our contributions include: (1) the AlphaGrad algorithm\nformulation; (2) a formal non-convex convergence analysis guaranteeing\nstationarity; (3) extensive empirical evaluation on diverse RL benchmarks (DQN,\nTD3, PPO). Compared to Adam, AlphaGrad demonstrates a highly context-dependent\nperformance profile. While exhibiting instability in off-policy DQN, it\nprovides enhanced training stability with competitive results in TD3 (requiring\ncareful $\\alpha$ tuning) and achieves substantially superior performance in\non-policy PPO. These results underscore the critical importance of empirical\n$\\alpha$ selection, revealing strong interactions between the optimizer's\ndynamics and the underlying RL algorithm. AlphaGrad presents a compelling\nalternative optimizer for memory-constrained scenarios and shows significant\npromise for on-policy learning regimes where its stability and efficiency\nadvantages can be particularly impactful."}
{"id": "2504.15932", "pdf": "https://arxiv.org/pdf/2504.15932", "abs": "https://arxiv.org/abs/2504.15932", "authors": ["Wang Lin", "Liyu Jia", "Wentao Hu", "Kaihang Pan", "Zhongqi Yue", "Wei Zhao", "Jingyuan Chen", "Fei Wu", "Hanwang Zhang"], "title": "Reasoning Physical Video Generation with Diffusion Timestep Tokens via Reinforcement Learning", "categories": ["cs.CV"], "comment": null, "summary": "Despite recent progress in video generation, producing videos that adhere to\nphysical laws remains a significant challenge. Traditional diffusion-based\nmethods struggle to extrapolate to unseen physical conditions (eg, velocity)\ndue to their reliance on data-driven approximations. To address this, we\npropose to integrate symbolic reasoning and reinforcement learning to enforce\nphysical consistency in video generation. We first introduce the Diffusion\nTimestep Tokenizer (DDT), which learns discrete, recursive visual tokens by\nrecovering visual attributes lost during the diffusion process. The recursive\nvisual tokens enable symbolic reasoning by a large language model. Based on it,\nwe propose the Phys-AR framework, which consists of two stages: The first stage\nuses supervised fine-tuning to transfer symbolic knowledge, while the second\nstage applies reinforcement learning to optimize the model's reasoning\nabilities through reward functions based on physical conditions. Our approach\nallows the model to dynamically adjust and improve the physical properties of\ngenerated videos, ensuring adherence to physical laws. Experimental results\ndemonstrate that PhysAR can generate videos that are physically consistent."}
{"id": "2411.04223", "pdf": "https://arxiv.org/pdf/2411.04223", "abs": "https://arxiv.org/abs/2411.04223", "authors": ["Weiliang Zhao", "Daniel Ben-Levi", "Wei Hao", "Junfeng Yang", "Chengzhi Mao"], "title": "Diversity Helps Jailbreak Large Language Models", "categories": ["cs.CL"], "comment": null, "summary": "We have uncovered a powerful jailbreak technique that leverages large\nlanguage models' ability to diverge from prior context, enabling them to bypass\nsafety constraints and generate harmful outputs. By simply instructing the LLM\nto deviate and obfuscate previous attacks, our method dramatically outperforms\nexisting approaches, achieving up to a 62.83% higher success rate in\ncompromising ten leading chatbots, including GPT-4, Gemini, and Llama, while\nusing only 12.9% of the queries. This revelation exposes a critical flaw in\ncurrent LLM safety training, suggesting that existing methods may merely mask\nvulnerabilities rather than eliminate them. Our findings sound an urgent alarm\nfor the need to revolutionize testing methodologies to ensure robust and\nreliable LLM security."}
{"id": "2504.15972", "pdf": "https://arxiv.org/pdf/2504.15972", "abs": "https://arxiv.org/abs/2504.15972", "authors": ["Sophie C. Pope", "Andrew Barovic", "Armin Moin"], "title": "Bug Destiny Prediction in Large Open-Source Software Repositories through Sentiment Analysis and BERT Topic Modeling", "categories": ["cs.SE", "cs.AI"], "comment": null, "summary": "This study explores a novel approach to predicting key bug-related outcomes,\nincluding the time to resolution, time to fix, and ultimate status of a bug,\nusing data from the Bugzilla Eclipse Project. Specifically, we leverage\nfeatures available before a bug is resolved to enhance predictive accuracy. Our\nmethodology incorporates sentiment analysis to derive both an emotionality\nscore and a sentiment classification (positive or negative). Additionally, we\nintegrate the bug's priority level and its topic, extracted using a BERTopic\nmodel, as features for a Convolutional Neural Network (CNN) and a Multilayer\nPerceptron (MLP). Our findings indicate that the combination of BERTopic and\nsentiment analysis can improve certain model performance metrics. Furthermore,\nwe observe that balancing model inputs enhances practical applicability, albeit\nat the cost of a significant reduction in accuracy in most cases. To address\nour primary objectives, predicting time-to-resolution, time-to-fix, and bug\ndestiny, we employ both binary classification and exact time value predictions,\nallowing for a comparative evaluation of their predictive effectiveness.\nResults demonstrate that sentiment analysis serves as a valuable predictor of a\nbug's eventual outcome, particularly in determining whether it will be fixed.\nHowever, its utility is less pronounced when classifying bugs into more complex\nor unconventional outcome categories."}
{"id": "2504.16032", "pdf": "https://arxiv.org/pdf/2504.16032", "abs": "https://arxiv.org/abs/2504.16032", "authors": ["Yazan Otoum", "Arghavan Asad", "Amiya Nayak"], "title": "LLMs meet Federated Learning for Scalable and Secure IoT Management", "categories": ["cs.LG", "cs.AI", "cs.ET"], "comment": "This work has been submitted to the IEEE Global Communications\n  Conference (GLOBECOM) 2025 for possible publication", "summary": "The rapid expansion of IoT ecosystems introduces severe challenges in\nscalability, security, and real-time decision-making. Traditional centralized\narchitectures struggle with latency, privacy concerns, and excessive resource\nconsumption, making them unsuitable for modern large-scale IoT deployments.\nThis paper presents a novel Federated Learning-driven Large Language Model\n(FL-LLM) framework, designed to enhance IoT system intelligence while ensuring\ndata privacy and computational efficiency. The framework integrates Generative\nIoT (GIoT) models with a Gradient Sensing Federated Strategy (GSFS),\ndynamically optimizing model updates based on real-time network conditions. By\nleveraging a hybrid edge-cloud processing architecture, our approach balances\nintelligence, scalability, and security in distributed IoT environments.\nEvaluations on the IoT-23 dataset demonstrate that our framework improves model\naccuracy, reduces response latency, and enhances energy efficiency,\noutperforming traditional FL techniques (i.e., FedAvg, FedOpt). These findings\nhighlight the potential of integrating LLM-powered federated learning into\nlarge-scale IoT ecosystems, paving the way for more secure, scalable, and\nadaptive IoT management solutions."}
{"id": "2504.15958", "pdf": "https://arxiv.org/pdf/2504.15958", "abs": "https://arxiv.org/abs/2504.15958", "authors": ["Zebin Yao", "Lei Ren", "Huixing Jiang", "Chen Wei", "Xiaojie Wang", "Ruifan Li", "Fangxiang Feng"], "title": "FreeGraftor: Training-Free Cross-Image Feature Grafting for Subject-Driven Text-to-Image Generation", "categories": ["cs.CV"], "comment": null, "summary": "Subject-driven image generation aims to synthesize novel scenes that\nfaithfully preserve subject identity from reference images while adhering to\ntextual guidance, yet existing methods struggle with a critical trade-off\nbetween fidelity and efficiency. Tuning-based approaches rely on time-consuming\nand resource-intensive subject-specific optimization, while zero-shot methods\nfail to maintain adequate subject consistency. In this work, we propose\nFreeGraftor, a training-free framework that addresses these limitations through\ncross-image feature grafting. Specifically, FreeGraftor employs semantic\nmatching and position-constrained attention fusion to transfer visual details\nfrom reference subjects to the generated image. Additionally, our framework\nincorporates a novel noise initialization strategy to preserve geometry priors\nof reference subjects for robust feature matching. Extensive qualitative and\nquantitative experiments demonstrate that our method enables precise subject\nidentity transfer while maintaining text-aligned scene synthesis. Without\nrequiring model fine-tuning or additional training, FreeGraftor significantly\noutperforms existing zero-shot and training-free approaches in both subject\nfidelity and text alignment. Furthermore, our framework can seamlessly extend\nto multi-subject generation, making it practical for real-world deployment. Our\ncode is available at https://github.com/Nihukat/FreeGraftor."}
{"id": "2412.12639", "pdf": "https://arxiv.org/pdf/2412.12639", "abs": "https://arxiv.org/abs/2412.12639", "authors": ["Xiangxiang Gao", "Weisheng Xie", "Yiwei Xiang", "Feng Ji"], "title": "Falcon: Faster and Parallel Inference of Large Language Models through Enhanced Semi-Autoregressive Drafting and Custom-Designed Decoding Tree", "categories": ["cs.CL", "cs.AI"], "comment": "AAAI 2025 Accepted", "summary": "Striking an optimal balance between minimal drafting latency and high\nspeculation accuracy to enhance the inference speed of Large Language Models\nremains a significant challenge in speculative decoding. In this paper, we\nintroduce Falcon, an innovative semi-autoregressive speculative decoding\nframework fashioned to augment both the drafter's parallelism and output\nquality. Falcon incorporates the Coupled Sequential Glancing Distillation\ntechnique, which fortifies inter-token dependencies within the same block,\nleading to increased speculation accuracy. We offer a comprehensive theoretical\nanalysis to illuminate the underlying mechanisms. Additionally, we introduce a\nCustom-Designed Decoding Tree, which permits the drafter to generate multiple\ntokens in a single forward pass and accommodates multiple forward passes as\nneeded, thereby boosting the number of drafted tokens and significantly\nimproving the overall acceptance rate. Comprehensive evaluations on benchmark\ndatasets such as MT-Bench, HumanEval, and GSM8K demonstrate Falcon's superior\nacceleration capabilities. The framework achieves a lossless speedup ratio\nranging from 2.91x to 3.51x when tested on the Vicuna and LLaMA2-Chat model\nseries. These results outstrip existing speculative decoding methods for LLMs,\nincluding Eagle, Medusa, Lookahead, SPS, and PLD, while maintaining a compact\ndrafter architecture equivalent to merely two Transformer layers."}
{"id": "2504.16021", "pdf": "https://arxiv.org/pdf/2504.16021", "abs": "https://arxiv.org/abs/2504.16021", "authors": ["Dinithi Dissanayake", "Suranga Nanayakkara"], "title": "Navigating the State of Cognitive Flow: Context-Aware AI Interventions for Effective Reasoning Support", "categories": ["cs.HC", "cs.AI"], "comment": "Presented at the 2025 ACM Workshop on Human-AI Interaction for\n  Augmented Reasoning, Report Number: CHI25-WS-AUGMENTED-REASONING", "summary": "Flow theory describes an optimal cognitive state where individuals experience\ndeep focus and intrinsic motivation when a task's difficulty aligns with their\nskill level. In AI-augmented reasoning, interventions that disrupt the state of\ncognitive flow can hinder rather than enhance decision-making. This paper\nproposes a context-aware cognitive augmentation framework that adapts\ninterventions based on three key contextual factors: type, timing, and scale.\nBy leveraging multimodal behavioral cues (e.g., gaze behavior, typing\nhesitation, interaction speed), AI can dynamically adjust cognitive support to\nmaintain or restore flow. We introduce the concept of cognitive flow, an\nextension of flow theory in AI-augmented reasoning, where interventions are\npersonalized, adaptive, and minimally intrusive. By shifting from static\ninterventions to context-aware augmentation, our approach ensures that AI\nsystems support deep engagement in complex decision-making and reasoning\nwithout disrupting cognitive immersion."}
{"id": "2504.16041", "pdf": "https://arxiv.org/pdf/2504.16041", "abs": "https://arxiv.org/abs/2504.16041", "authors": ["Amund Tveit", "Bjørn Remseth", "Arve Skogvold"], "title": "Muon Optimizer Accelerates Grokking", "categories": ["cs.LG", "cs.AI", "I.2"], "comment": "8 pages, 4 figures", "summary": "This paper investigates the impact of different optimizers on the grokking\nphenomenon, where models exhibit delayed generalization. We conducted\nexperiments across seven numerical tasks (primarily modular arithmetic) using a\nmodern Transformer architecture. The experimental configuration systematically\nvaried the optimizer (Muon vs. AdamW) and the softmax activation function\n(standard softmax, stablemax, and sparsemax) to assess their combined effect on\nlearning dynamics. Our empirical evaluation reveals that the Muon optimizer,\ncharacterized by its use of spectral norm constraints and second-order\ninformation, significantly accelerates the onset of grokking compared to the\nwidely used AdamW optimizer. Specifically, Muon reduced the mean grokking epoch\nfrom 153.09 to 102.89 across all configurations, a statistically significant\ndifference (t = 5.0175, p = 6.33e-08). This suggests that the optimizer choice\nplays a crucial role in facilitating the transition from memorization to\ngeneralization."}
{"id": "2504.15991", "pdf": "https://arxiv.org/pdf/2504.15991", "abs": "https://arxiv.org/abs/2504.15991", "authors": ["Leonardo Olivi", "Edoardo Santero Mormile", "Enzo Tartaglione"], "title": "Efficient Adaptation of Deep Neural Networks for Semantic Segmentation in Space Applications", "categories": ["cs.CV", "cs.LG"], "comment": null, "summary": "In recent years, the application of Deep Learning techniques has shown\nremarkable success in various computer vision tasks, paving the way for their\ndeployment in extraterrestrial exploration. Transfer learning has emerged as a\npowerful strategy for addressing the scarcity of labeled data in these novel\nenvironments. This paper represents one of the first efforts in evaluating the\nfeasibility of employing adapters toward efficient transfer learning for rock\nsegmentation in extraterrestrial landscapes, mainly focusing on lunar and\nmartian terrains. Our work suggests that the use of adapters, strategically\nintegrated into a pre-trained backbone model, can be successful in reducing\nboth bandwidth and memory requirements for the target extraterrestrial device.\nIn this study, we considered two memory-saving strategies: layer fusion (to\nreduce to zero the inference overhead) and an ``adapter ranking'' (to also\nreduce the transmission cost). Finally, we evaluate these results in terms of\ntask performance, memory, and computation on embedded devices, evidencing\ntrade-offs that open the road to more research in the field."}
{"id": "2412.14354", "pdf": "https://arxiv.org/pdf/2412.14354", "abs": "https://arxiv.org/abs/2412.14354", "authors": ["Zhichao Xu", "Jinghua Yan", "Ashim Gupta", "Vivek Srikumar"], "title": "State Space Models are Strong Text Rerankers", "categories": ["cs.CL", "cs.IR"], "comment": "Accepted to RepL4NLP 2025. The first two authors contributed equally,\n  order decided randomly", "summary": "Transformers dominate NLP and IR; but their inference inefficiencies and\nchallenges in extrapolating to longer contexts have sparked interest in\nalternative model architectures. Among these, state space models (SSMs) like\nMamba offer promising advantages, particularly $O(1)$ time complexity in\ninference. Despite their potential, SSMs' effectiveness at text reranking -- a\ntask requiring fine-grained query-document interaction and long-context\nunderstanding -- remains underexplored. This study benchmarks SSM-based\narchitectures (specifically, Mamba-1 and Mamba-2) against transformer-based\nmodels across various scales, architectures, and pre-training objectives,\nfocusing on performance and efficiency in text reranking tasks. We find that\n(1) Mamba architectures achieve competitive text ranking performance,\ncomparable to transformer-based models of similar size; (2) they are less\nefficient in training and inference compared to transformers with flash\nattention; and (3) Mamba-2 outperforms Mamba-1 in both performance and\nefficiency. These results underscore the potential of state space models as a\ntransformer alternative and highlight areas for improvement in future IR\napplications."}
{"id": "2504.16026", "pdf": "https://arxiv.org/pdf/2504.16026", "abs": "https://arxiv.org/abs/2504.16026", "authors": ["Konstantin F. Pilz", "James Sanders", "Robi Rahman", "Lennart Heim"], "title": "Trends in AI Supercomputers", "categories": ["cs.CY", "cs.AI"], "comment": null, "summary": "Frontier AI development relies on powerful AI supercomputers, yet analysis of\nthese systems is limited. We create a dataset of 500 AI supercomputers from\n2019 to 2025 and analyze key trends in performance, power needs, hardware cost,\nownership, and global distribution. We find that the computational performance\nof AI supercomputers has doubled every nine months, while hardware acquisition\ncost and power needs both doubled every year. The leading system in March 2025,\nxAI's Colossus, used 200,000 AI chips, had a hardware cost of \\$7B, and\nrequired 300 MW of power, as much as 250,000 households. As AI supercomputers\nevolved from tools for science to industrial machines, companies rapidly\nexpanded their share of total AI supercomputer performance, while the share of\ngovernments and academia diminished. Globally, the United States accounts for\nabout 75% of total performance in our dataset, with China in second place at\n15%. If the observed trends continue, the leading AI supercomputer in 2030 will\nachieve $2\\times10^{22}$ 16-bit FLOP/s, use two million AI chips, have a\nhardware cost of \\$200 billion, and require 9 GW of power. Our analysis\nprovides visibility into the AI supercomputer landscape, allowing policymakers\nto assess key AI trends like resource needs, ownership, and national\ncompetitiveness."}
{"id": "2504.16054", "pdf": "https://arxiv.org/pdf/2504.16054", "abs": "https://arxiv.org/abs/2504.16054", "authors": ["Physical Intelligence", "Kevin Black", "Noah Brown", "James Darpinian", "Karan Dhabalia", "Danny Driess", "Adnan Esmail", "Michael Equi", "Chelsea Finn", "Niccolo Fusai", "Manuel Y. Galliker", "Dibya Ghosh", "Lachy Groom", "Karol Hausman", "Brian Ichter", "Szymon Jakubczak", "Tim Jones", "Liyiming Ke", "Devin LeBlanc", "Sergey Levine", "Adrian Li-Bell", "Mohith Mothukuri", "Suraj Nair", "Karl Pertsch", "Allen Z. Ren", "Lucy Xiaoyang Shi", "Laura Smith", "Jost Tobias Springenberg", "Kyle Stachowicz", "James Tanner", "Quan Vuong", "Homer Walke", "Anna Walling", "Haohuan Wang", "Lili Yu", "Ury Zhilinsky"], "title": "$π_{0.5}$: a Vision-Language-Action Model with Open-World Generalization", "categories": ["cs.LG", "cs.RO"], "comment": null, "summary": "In order for robots to be useful, they must perform practically relevant\ntasks in the real world, outside of the lab. While vision-language-action (VLA)\nmodels have demonstrated impressive results for end-to-end robot control, it\nremains an open question how far such models can generalize in the wild. We\ndescribe $\\pi_{0.5}$, a new model based on $\\pi_{0}$ that uses co-training on\nheterogeneous tasks to enable broad generalization. $\\pi_{0.5}$\\ uses data from\nmultiple robots, high-level semantic prediction, web data, and other sources to\nenable broadly generalizable real-world robotic manipulation. Our system uses a\ncombination of co-training and hybrid multi-modal examples that combine image\nobservations, language commands, object detections, semantic subtask\nprediction, and low-level actions. Our experiments show that this kind of\nknowledge transfer is essential for effective generalization, and we\ndemonstrate for the first time that an end-to-end learning-enabled robotic\nsystem can perform long-horizon and dexterous manipulation skills, such as\ncleaning a kitchen or bedroom, in entirely new homes."}
{"id": "2504.16003", "pdf": "https://arxiv.org/pdf/2504.16003", "abs": "https://arxiv.org/abs/2504.16003", "authors": ["Yachun Mi", "Yu Li", "Weicheng Meng", "Chaofeng Chen", "Chen Hui", "Shaohui Liu"], "title": "MVQA: Mamba with Unified Sampling for Efficient Video Quality Assessment", "categories": ["cs.CV"], "comment": null, "summary": "The rapid growth of long-duration, high-definition videos has made efficient\nvideo quality assessment (VQA) a critical challenge. Existing research\ntypically tackles this problem through two main strategies: reducing model\nparameters and resampling inputs. However, light-weight Convolution Neural\nNetworks (CNN) and Transformers often struggle to balance efficiency with high\nperformance due to the requirement of long-range modeling capabilities.\nRecently, the state-space model, particularly Mamba, has emerged as a promising\nalternative, offering linear complexity with respect to sequence length.\nMeanwhile, efficient VQA heavily depends on resampling long sequences to\nminimize computational costs, yet current resampling methods are often weak in\npreserving essential semantic information. In this work, we present MVQA, a\nMamba-based model designed for efficient VQA along with a novel Unified\nSemantic and Distortion Sampling (USDS) approach. USDS combines semantic patch\nsampling from low-resolution videos and distortion patch sampling from\noriginal-resolution videos. The former captures semantically dense regions,\nwhile the latter retains critical distortion details. To prevent computation\nincrease from dual inputs, we propose a fusion mechanism using pre-defined\nmasks, enabling a unified sampling strategy that captures both semantic and\nquality information without additional computational burden. Experiments show\nthat the proposed MVQA, equipped with USDS, achieve comparable performance to\nstate-of-the-art methods while being $2\\times$ as fast and requiring only $1/5$\nGPU memory."}
{"id": "2412.15993", "pdf": "https://arxiv.org/pdf/2412.15993", "abs": "https://arxiv.org/abs/2412.15993", "authors": ["Lynn Greschner", "Roman Klinger"], "title": "Fearful Falcons and Angry Llamas: Emotion Category Annotations of Arguments by Humans and LLMs", "categories": ["cs.CL"], "comment": "accepted to NLP4DH 2025", "summary": "Arguments evoke emotions, influencing the effect of the argument itself. Not\nonly the emotional intensity but also the category influence the argument's\neffects, for instance, the willingness to adapt stances. While binary\nemotionality has been studied in arguments, there is no work on discrete\nemotion categories (e.g., \"Anger\") in such data. To fill this gap, we\ncrowdsource subjective annotations of emotion categories in a German argument\ncorpus and evaluate automatic LLM-based labeling methods. Specifically, we\ncompare three prompting strategies (zero-shot, one-shot, chain-of-thought) on\nthree large instruction-tuned language models (Falcon-7b-instruct,\nLlama-3.1-8B-instruct, GPT-4o-mini). We further vary the definition of the\noutput space to be binary (is there emotionality in the argument?),\nclosed-domain (which emotion from a given label set is in the argument?), or\nopen-domain (which emotion is in the argument?). We find that emotion\ncategories enhance the prediction of emotionality in arguments, emphasizing the\nneed for discrete emotion annotations in arguments. Across all prompt settings\nand models, automatic predictions show a high recall but low precision for\npredicting anger and fear, indicating a strong bias toward negative emotions."}
{"id": "2504.16027", "pdf": "https://arxiv.org/pdf/2504.16027", "abs": "https://arxiv.org/abs/2504.16027", "authors": ["Ahmed R. Sadik", "Siddhata Govind"], "title": "Benchmarking LLM for Code Smells Detection: OpenAI GPT-4.0 vs DeepSeek-V3", "categories": ["cs.SE", "cs.AI", "cs.LG", "cs.PL"], "comment": null, "summary": "Determining the most effective Large Language Model for code smell detection\npresents a complex challenge. This study introduces a structured methodology\nand evaluation matrix to tackle this issue, leveraging a curated dataset of\ncode samples consistently annotated with known smells. The dataset spans four\nprominent programming languages Java, Python, JavaScript, and C++; allowing for\ncross language comparison. We benchmark two state of the art LLMs, OpenAI GPT\n4.0 and DeepSeek-V3, using precision, recall, and F1 score as evaluation\nmetrics. Our analysis covers three levels of detail: overall performance,\ncategory level performance, and individual code smell type performance.\nAdditionally, we explore cost effectiveness by comparing the token based\ndetection approach of GPT 4.0 with the pattern-matching techniques employed by\nDeepSeek V3. The study also includes a cost analysis relative to traditional\nstatic analysis tools such as SonarQube. The findings offer valuable guidance\nfor practitioners in selecting an efficient, cost effective solution for\nautomated code smell detection"}
{"id": "2504.16078", "pdf": "https://arxiv.org/pdf/2504.16078", "abs": "https://arxiv.org/abs/2504.16078", "authors": ["Thomas Schmied", "Jörg Bornschein", "Jordi Grau-Moya", "Markus Wulfmeier", "Razvan Pascanu"], "title": "LLMs are Greedy Agents: Effects of RL Fine-tuning on Decision-Making Abilities", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "The success of Large Language Models (LLMs) has sparked interest in various\nagentic applications. A key hypothesis is that LLMs, leveraging common sense\nand Chain-of-Thought (CoT) reasoning, can effectively explore and efficiently\nsolve complex domains. However, LLM agents have been found to suffer from\nsub-optimal exploration and the knowing-doing gap, the inability to effectively\nact on knowledge present in the model. In this work, we systematically study\nwhy LLMs perform sub-optimally in decision-making scenarios. In particular, we\nclosely examine three prevalent failure modes: greediness, frequency bias, and\nthe knowing-doing gap. We propose mitigation of these shortcomings by\nfine-tuning via Reinforcement Learning (RL) on self-generated CoT rationales.\nOur experiments across multi-armed bandits, contextual bandits, and\nTic-tac-toe, demonstrate that RL fine-tuning enhances the decision-making\nabilities of LLMs by increasing exploration and narrowing the knowing-doing\ngap. Finally, we study both classic exploration mechanisms, such as\n$\\epsilon$-greedy, and LLM-specific approaches, such as self-correction and\nself-consistency, to enable more effective fine-tuning of LLMs for\ndecision-making."}
{"id": "2504.16016", "pdf": "https://arxiv.org/pdf/2504.16016", "abs": "https://arxiv.org/abs/2504.16016", "authors": ["Xinyuan Song", "Yangfan He", "Sida Li", "Jianhui Wang", "Hongyang He", "Xinhang Yuan", "Ruoyu Wang", "Jiaqi Chen", "Keqin Li", "Kuan Lu", "Menghao Huo", "Binxu Li", "Pei Liu"], "title": "Efficient Temporal Consistency in Diffusion-Based Video Editing with Adaptor Modules: A Theoretical Framework", "categories": ["cs.CV"], "comment": "arXiv admin note: substantial text overlap with arXiv:2501.04606", "summary": "Adapter-based methods are commonly used to enhance model performance with\nminimal additional complexity, especially in video editing tasks that require\nframe-to-frame consistency. By inserting small, learnable modules into\npretrained diffusion models, these adapters can maintain temporal coherence\nwithout extensive retraining. Approaches that incorporate prompt learning with\nboth shared and frame-specific tokens are particularly effective in preserving\ncontinuity across frames at low training cost. In this work, we want to provide\na general theoretical framework for adapters that maintain frame consistency in\nDDIM-based models under a temporal consistency loss. First, we prove that the\ntemporal consistency objective is differentiable under bounded feature norms,\nand we establish a Lipschitz bound on its gradient. Second, we show that\ngradient descent on this objective decreases the loss monotonically and\nconverges to a local minimum if the learning rate is within an appropriate\nrange. Finally, we analyze the stability of modules in the DDIM inversion\nprocedure, showing that the associated error remains controlled. These\ntheoretical findings will reinforce the reliability of diffusion-based video\nediting methods that rely on adapter strategies and provide theoretical\ninsights in video generation tasks."}
{"id": "2502.13053", "pdf": "https://arxiv.org/pdf/2502.13053", "abs": "https://arxiv.org/abs/2502.13053", "authors": ["Yurun Chen", "Xavier Hu", "Keting Yin", "Juncheng Li", "Shengyu Zhang"], "title": "Evaluating the Robustness of Multimodal Agents Against Active Environmental Injection Attacks", "categories": ["cs.CL"], "comment": null, "summary": "As researchers continue to optimize AI agents for more effective task\nexecution within operating systems, they often overlook a critical security\nconcern: the ability of these agents to detect \"impostors\" within their\nenvironment. Through an analysis of the agents' operational context, we\nidentify a significant threat-attackers can disguise malicious attacks as\nenvironmental elements, injecting active disturbances into the agents'\nexecution processes to manipulate their decision-making. We define this novel\nthreat as the Active Environment Injection Attack (AEIA). Focusing on the\ninteraction mechanisms of the Android OS, we conduct a risk assessment of AEIA\nand identify two critical security vulnerabilities: (1) Adversarial content\ninjection in multimodal interaction interfaces, where attackers embed\nadversarial instructions within environmental elements to mislead agent\ndecision-making; and (2) Reasoning gap vulnerabilities in the agent's task\nexecution process, which increase susceptibility to AEIA attacks during\nreasoning. To evaluate the impact of these vulnerabilities, we propose AEIA-MN,\nan attack scheme that exploits interaction vulnerabilities in mobile operating\nsystems to assess the robustness of MLLM-based agents. Experimental results\nshow that even advanced MLLMs are highly vulnerable to this attack, achieving a\nmaximum attack success rate of 93% on the AndroidWorld benchmark by combining\ntwo vulnerabilities."}
{"id": "2504.16047", "pdf": "https://arxiv.org/pdf/2504.16047", "abs": "https://arxiv.org/abs/2504.16047", "authors": ["Frank Li", "Hari Trivedi", "Bardia Khosravi", "Theo Dapamede", "Mohammadreza Chavoshi", "Abdulhameed Dere", "Rohan Satya Isaac", "Aawez Mansuri", "Janice Newsome", "Saptarshi Purkayastha", "Judy Gichoya"], "title": "Evaluating Vision Language Models (VLMs) for Radiology: A Comprehensive Analysis", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Foundation models, trained on vast amounts of data using self-supervised\ntechniques, have emerged as a promising frontier for advancing artificial\nintelligence (AI) applications in medicine. This study evaluates three\ndifferent vision-language foundation models (RAD-DINO, CheXagent, and\nBiomedCLIP) on their ability to capture fine-grained imaging features for\nradiology tasks. The models were assessed across classification, segmentation,\nand regression tasks for pneumothorax and cardiomegaly on chest radiographs.\nSelf-supervised RAD-DINO consistently excelled in segmentation tasks, while\ntext-supervised CheXagent demonstrated superior classification performance.\nBiomedCLIP showed inconsistent performance across tasks. A custom segmentation\nmodel that integrates global and local features substantially improved\nperformance for all foundation models, particularly for challenging\npneumothorax segmentation. The findings highlight that pre-training methodology\nsignificantly influences model performance on specific downstream tasks. For\nfine-grained segmentation tasks, models trained without text supervision\nperformed better, while text-supervised models offered advantages in\nclassification and interpretability. These insights provide guidance for\nselecting foundation models based on specific clinical applications in\nradiology."}
{"id": "2504.15284", "pdf": "https://arxiv.org/pdf/2504.15284", "abs": "https://arxiv.org/abs/2504.15284", "authors": ["Weichen Li", "Albert Jan", "Baishakhi Ray", "Chengzhi Mao", "Junfeng Yang", "Kexin Pei"], "title": "EditLord: Learning Code Transformation Rules for Code Editing", "categories": ["cs.SE", "cs.CR", "cs.LG"], "comment": null, "summary": "Code editing is a foundational task in software development, where its\neffectiveness depends on whether it introduces desired code property changes\nwithout changing the original code's intended functionality. Existing\napproaches often formulate code editing as an implicit end-to-end task,\nomitting the fact that code-editing procedures inherently consist of discrete\nand explicit steps. Thus, they suffer from suboptimal performance and lack of\nrobustness and generalization. We introduce EditLord, a code editing framework\nthat makes the code transformation steps explicit. Our key insight is to employ\na language model (LM) as an inductive learner to extract code editing rules\nfrom the training code pairs as concise meta-rule sets. Such rule sets will be\nmanifested for each training sample to augment them for finetuning or assist in\nprompting- and iterative-based code editing. EditLordoutperforms the\nstate-of-the-art by an average of 22.7% in editing performance and 58.1% in\nrobustness while achieving 20.2% higher functional correctness across critical\nsoftware engineering and security applications, LM models, and editing modes."}
{"id": "2504.16023", "pdf": "https://arxiv.org/pdf/2504.16023", "abs": "https://arxiv.org/abs/2504.16023", "authors": ["Song Wang", "Xiaolu Liu", "Lingdong Kong", "Jianyun Xu", "Chunyong Hu", "Gongfan Fang", "Wentong Li", "Jianke Zhu", "Xinchao Wang"], "title": "PointLoRA: Low-Rank Adaptation with Token Selection for Point Cloud Learning", "categories": ["cs.CV"], "comment": "Accepted by CVPR2025", "summary": "Self-supervised representation learning for point cloud has demonstrated\neffectiveness in improving pre-trained model performance across diverse tasks.\nHowever, as pre-trained models grow in complexity, fully fine-tuning them for\ndownstream applications demands substantial computational and storage\nresources. Parameter-efficient fine-tuning (PEFT) methods offer a promising\nsolution to mitigate these resource requirements, yet most current approaches\nrely on complex adapter and prompt mechanisms that increase tunable parameters.\nIn this paper, we propose PointLoRA, a simple yet effective method that\ncombines low-rank adaptation (LoRA) with multi-scale token selection to\nefficiently fine-tune point cloud models. Our approach embeds LoRA layers\nwithin the most parameter-intensive components of point cloud transformers,\nreducing the need for tunable parameters while enhancing global feature\ncapture. Additionally, multi-scale token selection extracts critical local\ninformation to serve as prompts for downstream fine-tuning, effectively\ncomplementing the global context captured by LoRA. The experimental results\nacross various pre-trained models and three challenging public datasets\ndemonstrate that our approach achieves competitive performance with only 3.43%\nof the trainable parameters, making it highly effective for\nresource-constrained applications. Source code is available at:\nhttps://github.com/songw-zju/PointLoRA."}
{"id": "2503.04797", "pdf": "https://arxiv.org/pdf/2503.04797", "abs": "https://arxiv.org/abs/2503.04797", "authors": ["Rahul Raja", "Arpita Vats"], "title": "Parallel Corpora for Machine Translation in Low-resource Indic Languages: A Comprehensive Review", "categories": ["cs.CL", "cs.LG"], "comment": "Accepted in NACCL", "summary": "Parallel corpora play an important role in training machine translation (MT)\nmodels, particularly for low-resource languages where high-quality bilingual\ndata is scarce. This review provides a comprehensive overview of available\nparallel corpora for Indic languages, which span diverse linguistic families,\nscripts, and regional variations. We categorize these corpora into\ntext-to-text, code-switched, and various categories of multimodal datasets,\nhighlighting their significance in the development of robust multilingual MT\nsystems. Beyond resource enumeration, we critically examine the challenges\nfaced in corpus creation, including linguistic diversity, script variation,\ndata scarcity, and the prevalence of informal textual content.We also discuss\nand evaluate these corpora in various terms such as alignment quality and\ndomain representativeness. Furthermore, we address open challenges such as data\nimbalance across Indic languages, the trade-off between quality and quantity,\nand the impact of noisy, informal, and dialectal data on MT performance.\nFinally, we outline future directions, including leveraging cross-lingual\ntransfer learning, expanding multilingual datasets, and integrating multimodal\nresources to enhance translation quality. To the best of our knowledge, this\npaper presents the first comprehensive review of parallel corpora specifically\ntailored for low-resource Indic languages in the context of machine\ntranslation."}
{"id": "2504.16061", "pdf": "https://arxiv.org/pdf/2504.16061", "abs": "https://arxiv.org/abs/2504.16061", "authors": ["Sangeet Khemlani", "Tyler Tran", "Nathaniel Gyory", "Anthony M. Harrison", "Wallace E. Lawson", "Ravenna Thielstrom", "Hunter Thompson", "Taaren Singh", "J. Gregory Trafton"], "title": "Vision language models are unreliable at trivial spatial cognition", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Vision language models (VLMs) are designed to extract relevant visuospatial\ninformation from images. Some research suggests that VLMs can exhibit humanlike\nscene understanding, while other investigations reveal difficulties in their\nability to process relational information. To achieve widespread applicability,\nVLMs must perform reliably, yielding comparable competence across a wide\nvariety of related tasks. We sought to test how reliable these architectures\nare at engaging in trivial spatial cognition, e.g., recognizing whether one\nobject is left of another in an uncluttered scene. We developed a benchmark\ndataset -- TableTest -- whose images depict 3D scenes of objects arranged on a\ntable, and used it to evaluate state-of-the-art VLMs. Results show that\nperformance could be degraded by minor variations of prompts that use logically\nequivalent descriptions. These analyses suggest limitations in how VLMs may\nreason about spatial relations in real-world applications. They also reveal\nnovel opportunities for bolstering image caption corpora for more efficient\ntraining and testing."}
{"id": "2504.15327", "pdf": "https://arxiv.org/pdf/2504.15327", "abs": "https://arxiv.org/abs/2504.15327", "authors": ["Tianliang Yao", "Bo Lu", "Markus Kowarschik", "Yixuan Yuan", "Hubin Zhao", "Sebastien Ourselin", "Kaspar Althoefer", "Junbo Ge", "Peng Qi"], "title": "Advancing Embodied Intelligence in Robotic-Assisted Endovascular Procedures: A Systematic Review of AI Solutions", "categories": ["cs.RO", "cs.LG"], "comment": "24 pages, 7 figures, submitted to IEEE", "summary": "Endovascular procedures have revolutionized the treatment of vascular\ndiseases thanks to minimally invasive solutions that significantly reduce\npatient recovery time and enhance clinical outcomes. However, the precision and\ndexterity required during these procedures poses considerable challenges for\ninterventionists. Robotic systems have emerged offering transformative\nsolutions, addressing issues such as operator fatigue, radiation exposure, and\nthe inherent limitations of human precision. The integration of Embodied\nIntelligence (EI) into these systems signifies a paradigm shift, enabling\nrobots to navigate complex vascular networks and adapt to dynamic physiological\nconditions. Data-driven approaches, advanced computer vision, medical image\nanalysis, and machine learning techniques, are at the forefront of this\nevolution. These methods augment procedural intelligence by facilitating\nreal-time vessel segmentation, device tracking, and anatomical landmark\ndetection. Reinforcement learning and imitation learning further refine\nnavigation strategies and replicate experts' techniques. This review\nsystematically examines the integration of EI principles into robotic\ntechnologies, in relation to endovascular procedures. We discuss recent\nadvancements in intelligent perception and data-driven control, and their\npractical applications in robot-assisted endovascular procedures. By critically\nevaluating current limitations and emerging opportunities, this review\nestablishes a framework for future developments, emphasizing the potential for\ngreater autonomy and improved clinical outcomes. Emerging trends and specific\nareas of research, such as federated learning for medical data sharing,\nexplainable AI for clinical decision support, and advanced human-robot\ncollaboration paradigms, are also explored, offering insights into the future\ndirection of this rapidly evolving field."}
{"id": "2504.16030", "pdf": "https://arxiv.org/pdf/2504.16030", "abs": "https://arxiv.org/abs/2504.16030", "authors": ["Joya Chen", "Ziyun Zeng", "Yiqi Lin", "Wei Li", "Zejun Ma", "Mike Zheng Shou"], "title": "LiveCC: Learning Video LLM with Streaming Speech Transcription at Scale", "categories": ["cs.CV"], "comment": "CVPR 2025. If any references are missing, please contact\n  joyachen@u.nus.edu", "summary": "Recent video large language models (Video LLMs) often depend on costly human\nannotations or proprietary model APIs (e.g., GPT-4o) to produce training data,\nwhich limits their training at scale. In this paper, we explore large-scale\ntraining for Video LLM with cheap automatic speech recognition (ASR)\ntranscripts. Specifically, we propose a novel streaming training approach that\ndensely interleaves the ASR words and video frames according to their\ntimestamps. Compared to previous studies in vision-language representation with\nASR, our method naturally fits the streaming characteristics of ASR, thus\nenabling the model to learn temporally-aligned, fine-grained vision-language\nmodeling. To support the training algorithm, we introduce a data production\npipeline to process YouTube videos and their closed captions (CC, same as ASR),\nresulting in Live-CC-5M dataset for pre-training and Live-WhisperX-526K dataset\nfor high-quality supervised fine-tuning (SFT). Remarkably, even without SFT,\nthe ASR-only pre-trained LiveCC-7B-Base model demonstrates competitive general\nvideo QA performance and exhibits a new capability in real-time video\ncommentary. To evaluate this, we carefully design a new LiveSports-3K\nbenchmark, using LLM-as-a-judge to measure the free-form commentary.\nExperiments show our final LiveCC-7B-Instruct model can surpass advanced 72B\nmodels (Qwen2.5-VL-72B-Instruct, LLaVA-Video-72B) in commentary quality even\nworking in a real-time mode. Meanwhile, it achieves state-of-the-art results at\nthe 7B/8B scale on popular video QA benchmarks such as VideoMME and OVOBench,\ndemonstrating the broad generalizability of our approach. All resources of this\npaper have been released at https://showlab.github.io/livecc."}
{"id": "2503.09572", "pdf": "https://arxiv.org/pdf/2503.09572", "abs": "https://arxiv.org/abs/2503.09572", "authors": ["Lutfi Eren Erdogan", "Nicholas Lee", "Sehoon Kim", "Suhong Moon", "Hiroki Furuta", "Gopala Anumanchipalli", "Kurt Keutzer", "Amir Gholami"], "title": "Plan-and-Act: Improving Planning of Agents for Long-Horizon Tasks", "categories": ["cs.CL"], "comment": null, "summary": "Large language models (LLMs) have shown remarkable advancements in enabling\nlanguage agents to tackle simple tasks. However, applying them for complex,\nmulti-step, long-horizon tasks remains a challenge. Recent work have found\nsuccess by separating high-level planning from low-level execution, which\nenables the model to effectively balance high-level planning objectives and\nlow-level execution details. However, generating accurate plans remains\ndifficult since LLMs are not inherently trained for this task. To address this,\nwe propose Plan-and-Act, a novel framework that incorporates explicit planning\ninto LLM-based agents and introduces a scalable method to enhance plan\ngeneration through a novel synthetic data generation method. Plan-and-Act\nconsists of a Planner model which generates structured, high-level plans to\nachieve user goals, and an Executor model that translates these plans into\nenvironment-specific actions. To train the Planner effectively, we introduce a\nsynthetic data generation method that annotates ground-truth trajectories with\nfeasible plans, augmented with diverse and extensive examples to enhance\ngeneralization. We evaluate Plan-and-Act using web navigation as a\nrepresentative long-horizon planning environment, demonstrating a\nstate-of-the-art 57.58% success rate on the WebArena-Lite benchmark as well as\na text-only state-of-the-art 81.36% success rate on WebVoyager."}
{"id": "2504.16072", "pdf": "https://arxiv.org/pdf/2504.16072", "abs": "https://arxiv.org/abs/2504.16072", "authors": ["Long Lian", "Yifan Ding", "Yunhao Ge", "Sifei Liu", "Hanzi Mao", "Boyi Li", "Marco Pavone", "Ming-Yu Liu", "Trevor Darrell", "Adam Yala", "Yin Cui"], "title": "Describe Anything: Detailed Localized Image and Video Captioning", "categories": ["cs.CV", "cs.AI"], "comment": "Project page: https://describe-anything.github.io/", "summary": "Generating detailed and accurate descriptions for specific regions in images\nand videos remains a fundamental challenge for vision-language models. We\nintroduce the Describe Anything Model (DAM), a model designed for detailed\nlocalized captioning (DLC). DAM preserves both local details and global context\nthrough two key innovations: a focal prompt, which ensures high-resolution\nencoding of targeted regions, and a localized vision backbone, which integrates\nprecise localization with its broader context. To tackle the scarcity of\nhigh-quality DLC data, we propose a Semi-supervised learning (SSL)-based Data\nPipeline (DLC-SDP). DLC-SDP starts with existing segmentation datasets and\nexpands to unlabeled web images using SSL. We introduce DLC-Bench, a benchmark\ndesigned to evaluate DLC without relying on reference captions. DAM sets new\nstate-of-the-art on 7 benchmarks spanning keyword-level, phrase-level, and\ndetailed multi-sentence localized image and video captioning."}
{"id": "2504.15370", "pdf": "https://arxiv.org/pdf/2504.15370", "abs": "https://arxiv.org/abs/2504.15370", "authors": ["Juno Nam", "Miguel Steiner", "Max Misterka", "Soojung Yang", "Avni Singhal", "Rafael Gómez-Bombarelli"], "title": "Transferable Learning of Reaction Pathways from Geometric Priors", "categories": ["physics.chem-ph", "cs.LG"], "comment": "14 pages, 6 figures; Supporting Information in ancillary files", "summary": "Identifying minimum-energy paths (MEPs) is crucial for understanding chemical\nreaction mechanisms but remains computationally demanding. We introduce MEPIN,\na scalable machine-learning method for efficiently predicting MEPs from\nreactant and product configurations, without relying on transition-state\ngeometries or pre-optimized reaction paths during training. The task is defined\nas predicting deviations from geometric interpolations along reaction\ncoordinates. We address this task with a continuous reaction path model based\non a symmetry-broken equivariant neural network that generates a flexible\nnumber of intermediate structures. The model is trained using an energy-based\nobjective, with efficiency enhanced by incorporating geometric priors from\ngeodesic interpolation as initial interpolations or pre-training objectives.\nOur approach generalizes across diverse chemical reactions and achieves\naccurate alignment with reference intrinsic reaction coordinates, as\ndemonstrated on various small molecule reactions and [3+2] cycloadditions. Our\nmethod enables the exploration of large chemical reaction spaces with\nefficient, data-driven predictions of reaction pathways."}
{"id": "2504.16064", "pdf": "https://arxiv.org/pdf/2504.16064", "abs": "https://arxiv.org/abs/2504.16064", "authors": ["Theodoros Kouzelis", "Efstathios Karypidis", "Ioannis Kakogeorgiou", "Spyros Gidaris", "Nikos Komodakis"], "title": "Boosting Generative Image Modeling via Joint Image-Feature Synthesis", "categories": ["cs.CV"], "comment": null, "summary": "Latent diffusion models (LDMs) dominate high-quality image generation, yet\nintegrating representation learning with generative modeling remains a\nchallenge. We introduce a novel generative image modeling framework that\nseamlessly bridges this gap by leveraging a diffusion model to jointly model\nlow-level image latents (from a variational autoencoder) and high-level\nsemantic features (from a pretrained self-supervised encoder like DINO). Our\nlatent-semantic diffusion approach learns to generate coherent image-feature\npairs from pure noise, significantly enhancing both generative quality and\ntraining efficiency, all while requiring only minimal modifications to standard\nDiffusion Transformer architectures. By eliminating the need for complex\ndistillation objectives, our unified design simplifies training and unlocks a\npowerful new inference strategy: Representation Guidance, which leverages\nlearned semantics to steer and refine image generation. Evaluated in both\nconditional and unconditional settings, our method delivers substantial\nimprovements in image quality and training convergence speed, establishing a\nnew direction for representation-aware generative modeling."}
{"id": "2503.11816", "pdf": "https://arxiv.org/pdf/2503.11816", "abs": "https://arxiv.org/abs/2503.11816", "authors": ["Neusha Javidnia", "Bita Darvish Rouhani", "Farinaz Koushanfar"], "title": "Key, Value, Compress: A Systematic Exploration of KV Cache Compression Techniques", "categories": ["cs.CL"], "comment": "Presented at IEEE Custom Integrated Circuits Conference (CICC) 2025", "summary": "Large language models (LLMs) have demonstrated exceptional capabilities in\ngenerating text, images, and video content. However, as context length grows,\nthe computational cost of attention increases quadratically with the number of\ntokens, presenting significant efficiency challenges. This paper presents an\nanalysis of various Key-Value (KV) cache compression strategies, offering a\ncomprehensive taxonomy that categorizes these methods by their underlying\nprinciples and implementation techniques. Furthermore, we evaluate their impact\non performance and inference latency, providing critical insights into their\neffectiveness. Our findings highlight the trade-offs involved in KV cache\ncompression and its influence on handling long-context scenarios, paving the\nway for more efficient LLM implementations."}
{"id": "2309.13218", "pdf": "https://arxiv.org/pdf/2309.13218", "abs": "https://arxiv.org/abs/2309.13218", "authors": ["Pivithuru Thejan Amarasinghe", "Su Nguyen", "Yuan Sun", "Damminda Alahakoon"], "title": "Language Models for Business Optimisation with a Real World Case Study in Production Scheduling", "categories": ["cs.AI"], "comment": null, "summary": "Business optimisation has been used extensively to determine optimal\nsolutions for challenging business operations. Problem formulation is an\nimportant part of business optimisation as it influences both the validity of\nsolutions and the efficiency of the optimisation process. While different\noptimisation modelling languages have been developed, problem formulation is\nstill not a trivial task and usually requires optimisation expertise and\nproblem-domain knowledge. Recently, Large Language Models (LLMs) have\ndemonstrated outstanding performance across different language-related tasks.\nSince problem formulation can be viewed as a translation task, there is a\npotential to leverage LLMs to automate problem formulation. However, developing\nan LLM for problem formulation is challenging, due to limited training data,\nand the complexity of real-world optimisation problems. Several prompt\nengineering methods have been proposed in the literature to automate problem\nformulation with LLMs. While the initial results are encouraging, the accuracy\nof formulations generated by these methods can still be significantly improved.\nIn this paper, we present an LLM-based framework for automating problem\nformulation in business optimization. Our approach introduces a method for\nfine-tuning cost-efficient LLMs specifically tailored to specialized business\noptimization challenges. The experiment results demonstrate that our framework\ncan generate accurate formulations for conventional and real-world business\noptimisation problems in production scheduling. Extensive analyses show the\neffectiveness and the convergence of the proposed fine-tuning method. The\nproposed method also shows very competitive performance when compared with the\nstate-of-the-art prompt engineering methods in the literature when tested on\ngeneral linear programming problems."}
{"id": "2504.15375", "pdf": "https://arxiv.org/pdf/2504.15375", "abs": "https://arxiv.org/abs/2504.15375", "authors": ["Bradley Boswell", "Seth Barrett", "Swarnamugi Rajaganapathy", "Gokila Dorai"], "title": "FLARE: Feature-based Lightweight Aggregation for Robust Evaluation of IoT Intrusion Detection", "categories": ["cs.CR", "cs.LG"], "comment": "23 pages, 19 tables, 2 algorithms, 2 figures, submitted to\n  SecureComm25", "summary": "The proliferation of Internet of Things (IoT) devices has expanded the attack\nsurface, necessitating efficient intrusion detection systems (IDSs) for network\nprotection. This paper presents FLARE, a feature-based lightweight aggregation\nfor robust evaluation of IoT intrusion detection to address the challenges of\nsecuring IoT environments through feature aggregation techniques. FLARE\nutilizes a multilayered processing approach, incorporating session, flow, and\ntime-based sliding-window data aggregation to analyze network behavior and\ncapture vital features from IoT network traffic data. We perform extensive\nevaluations on IoT data generated from our laboratory experimental setup to\nassess the effectiveness of the proposed aggregation technique. To classify\nattacks in IoT IDS, we employ four supervised learning models and two deep\nlearning models. We validate the performance of these models in terms of\naccuracy, precision, recall, and F1-score. Our results reveal that\nincorporating the FLARE aggregation technique as a foundational step in feature\nengineering, helps lay a structured representation, and enhances the\nperformance of complex end-to-end models, making it a crucial step in IoT IDS\npipeline. Our findings highlight the potential of FLARE as a valuable technique\nto improve performance and reduce computational costs of end-to-end IDS\nimplementations, thereby fostering more robust IoT intrusion detection systems."}
{"id": "2504.16080", "pdf": "https://arxiv.org/pdf/2504.16080", "abs": "https://arxiv.org/abs/2504.16080", "authors": ["Le Zhuo", "Liangbing Zhao", "Sayak Paul", "Yue Liao", "Renrui Zhang", "Yi Xin", "Peng Gao", "Mohamed Elhoseiny", "Hongsheng Li"], "title": "From Reflection to Perfection: Scaling Inference-Time Optimization for Text-to-Image Diffusion Models via Reflection Tuning", "categories": ["cs.CV"], "comment": "All code, checkpoints, and datasets are available at\n  \\url{https://diffusion-cot.github.io/reflection2perfection}", "summary": "Recent text-to-image diffusion models achieve impressive visual quality\nthrough extensive scaling of training data and model parameters, yet they often\nstruggle with complex scenes and fine-grained details. Inspired by the\nself-reflection capabilities emergent in large language models, we propose\nReflectionFlow, an inference-time framework enabling diffusion models to\niteratively reflect upon and refine their outputs. ReflectionFlow introduces\nthree complementary inference-time scaling axes: (1) noise-level scaling to\noptimize latent initialization; (2) prompt-level scaling for precise semantic\nguidance; and most notably, (3) reflection-level scaling, which explicitly\nprovides actionable reflections to iteratively assess and correct previous\ngenerations. To facilitate reflection-level scaling, we construct GenRef, a\nlarge-scale dataset comprising 1 million triplets, each containing a\nreflection, a flawed image, and an enhanced image. Leveraging this dataset, we\nefficiently perform reflection tuning on state-of-the-art diffusion\ntransformer, FLUX.1-dev, by jointly modeling multimodal inputs within a unified\nframework. Experimental results show that ReflectionFlow significantly\noutperforms naive noise-level scaling methods, offering a scalable and\ncompute-efficient solution toward higher-quality image synthesis on challenging\ntasks."}
{"id": "2504.00021", "pdf": "https://arxiv.org/pdf/2504.00021", "abs": "https://arxiv.org/abs/2504.00021", "authors": ["Rahul Raja", "Arpita Vats"], "title": "FUSE : A Ridge and Random Forest-Based Metric for Evaluating MT in Indigenous Languages", "categories": ["cs.CL"], "comment": "NACCL 2025", "summary": "This paper presents the winning submission of the RaaVa team to the\nAmericasNLP 2025 Shared Task 3 on Automatic Evaluation Metrics for Machine\nTranslation (MT) into Indigenous Languages of America, where our system ranked\nfirst overall based on average Pearson correlation with the human annotations.\nWe introduce Feature-Union Scorer (FUSE) for Evaluation, FUSE integrates Ridge\nregression and Gradient Boosting to model translation quality. In addition to\nFUSE, we explore five alternative approaches leveraging different combinations\nof linguistic similarity features and learning paradigms. FUSE Score highlights\nthe effectiveness of combining lexical, phonetic, semantic, and fuzzy token\nsimilarity with learning-based modeling to improve MT evaluation for\nmorphologically rich and low-resource languages. MT into Indigenous languages\nposes unique challenges due to polysynthesis, complex morphology, and\nnon-standardized orthography. Conventional automatic metrics such as BLEU, TER,\nand ChrF often fail to capture deeper aspects like semantic adequacy and\nfluency. Our proposed framework, formerly referred to as FUSE, incorporates\nmultilingual sentence embeddings and phonological encodings to better align\nwith human evaluation. We train supervised models on human-annotated\ndevelopment sets and evaluate held-out test data. Results show that FUSE\nconsistently achieves higher Pearson and Spearman correlations with human\njudgments, offering a robust and linguistically informed solution for MT\nevaluation in low-resource settings."}
{"id": "2402.15929", "pdf": "https://arxiv.org/pdf/2402.15929", "abs": "https://arxiv.org/abs/2402.15929", "authors": ["Isha Chaudhary", "Vedaant V. Jain", "Gagandeep Singh"], "title": "Certifying Knowledge Comprehension in LLMs", "categories": ["cs.AI", "cs.CL", "cs.LG"], "comment": null, "summary": "Large Language Models (LLMs) are increasingly deployed in safety-critical\nsystems where they provide answers based on in-context information derived from\nknowledge bases. As LLMs are increasingly envisioned as superhuman agents,\ntheir proficiency in knowledge comprehension-extracting relevant information\nand reasoning over it to answer questions, a key facet of human\nintelligence-becomes crucial. However, existing evaluations of LLMs on\nknowledge comprehension are typically conducted on small test sets, but these\ndatasets represent only a tiny fraction of the vast number of possible queries.\nSimple empirical evaluations on these limited test sets raises concerns about\nthe reliability and generalizability of the results. In this work, we introduce\nthe first specification and certification framework for knowledge comprehension\nin LLMs, providing formal probabilistic guarantees for reliability. Instead of\na fixed dataset, we design novel specifications that mathematically represent\nprohibitively large probability distributions of knowledge comprehension\nprompts with natural noise, using knowledge graphs. From these specifications,\nwe generate quantitative certificates that offer high-confidence, tight bounds\non the probability that a given LLM correctly answers any question drawn from\nthe specification distribution. We apply our framework to certify SOTA LLMs in\ntwo domains: precision medicine and general question-answering. Our results\nreveal previously unrecognized vulnerabilities in SOTA LLMs due to natural\nnoise in the prompts. Additionally, we establish performance hierarchies with\nformal guarantees among the SOTA LLMs, particularly in the context of precision\nmedicine question-answering."}
{"id": "2504.15386", "pdf": "https://arxiv.org/pdf/2504.15386", "abs": "https://arxiv.org/abs/2504.15386", "authors": ["Rebecca Knowlton", "Layla Parast"], "title": "Assessing Surrogate Heterogeneity in Real World Data Using Meta-Learners", "categories": ["stat.ME", "cs.LG", "stat.ML"], "comment": null, "summary": "Surrogate markers are most commonly studied within the context of randomized\nclinical trials. However, the need for alternative outcomes extends beyond\nthese settings and may be more pronounced in real-world public health and\nsocial science research, where randomized trials are often impractical.\nResearch on identifying surrogates in real-world non-randomized data is scarce,\nas available statistical approaches for evaluating surrogate markers tend to\nrely on the assumption that treatment is randomized. While the few methods that\nallow for non-randomized treatment/exposure appropriately handle confounding\nindividual characteristics, they do not offer a way to examine surrogate\nheterogeneity with respect to patient characteristics. In this paper, we\npropose a framework to assess surrogate heterogeneity in real-world, i.e.,\nnon-randomized, data and implement this framework using various meta-learners.\nOur approach allows us to quantify heterogeneity in surrogate strength with\nrespect to patient characteristics while accommodating confounders through the\nuse of flexible, off-the-shelf machine learning methods. In addition, we use\nour framework to identify individuals for whom the surrogate is a valid\nreplacement of the primary outcome. We examine the performance of our methods\nvia a simulation study and application to examine heterogeneity in the\nsurrogacy of hemoglobin A1c as a surrogate for fasting plasma glucose."}
{"id": "2504.16082", "pdf": "https://arxiv.org/pdf/2504.16082", "abs": "https://arxiv.org/abs/2504.16082", "authors": ["Ziqi Pang", "Yu-Xiong Wang"], "title": "MR. Video: \"MapReduce\" is the Principle for Long Video Understanding", "categories": ["cs.CV"], "comment": "Preprint", "summary": "We propose MR. Video, an agentic long video understanding framework that\ndemonstrates the simple yet effective MapReduce principle for processing long\nvideos: (1) Map: independently and densely perceiving short video clips, and\n(2) Reduce: jointly aggregating information from all clips. Compared with\nsequence-to-sequence vision-language models (VLMs), MR. Video performs detailed\nshort video perception without being limited by context length. Compared with\nexisting video agents that typically rely on sequential key segment selection,\nthe Map operation enables simpler and more scalable sequence parallel\nperception of short video segments. Its Reduce step allows for more\ncomprehensive context aggregation and reasoning, surpassing explicit key\nsegment retrieval. This MapReduce principle is applicable to both VLMs and\nvideo agents, and we use LLM agents to validate its effectiveness.\n  In practice, MR. Video employs two MapReduce stages: (A) Captioning:\ngenerating captions for short video clips (map), then standardizing repeated\ncharacters and objects into shared names (reduce); (B) Analysis: for each user\nquestion, analyzing relevant information from individual short videos (map),\nand integrating them into a final answer (reduce). MR. Video achieves over 10%\naccuracy improvement on the challenging LVBench compared to state-of-the-art\nVLMs and video agents.\n  Code is available at: https://github.com/ziqipang/MR-Video"}
{"id": "2504.01801", "pdf": "https://arxiv.org/pdf/2504.01801", "abs": "https://arxiv.org/abs/2504.01801", "authors": ["Zhijun Wang", "Jiahuan Li", "Hao Zhou", "Rongxiang Weng", "Jingang Wang", "Xin Huang", "Xue Han", "Junlan Feng", "Chao Deng", "Shujian Huang"], "title": "Investigating and Scaling up Code-Switching for Multilingual Language Model Pre-Training", "categories": ["cs.CL"], "comment": null, "summary": "Large language models (LLMs) exhibit remarkable multilingual capabilities\ndespite the extreme language imbalance in the pre-training data. In this paper,\nwe closely examine the reasons behind this phenomenon, focusing on the\npre-training corpus. We find that the existence of code-switching, alternating\nbetween different languages within a context, is key to multilingual\ncapabilities. We conduct an analysis to investigate code-switching in the\npre-training corpus, examining its presence and categorizing it into four types\nwithin two quadrants. We then assess its impact on multilingual performance.\nThese types of code-switching data are unbalanced in proportions and\ndemonstrate different effects on facilitating language transfer. To better\nexplore the power of code-switching for language alignment during pre-training,\nwe investigate the strategy of synthetic code-switching. We continuously scale\nup the synthetic code-switching data and observe remarkable improvements in\nboth benchmarks and representation space. Extensive experiments indicate that\nincorporating synthetic code-switching data enables better language alignment\nand generalizes well to high, medium, and low-resource languages with\npre-training corpora of varying qualities."}
{"id": "2405.18780", "pdf": "https://arxiv.org/pdf/2405.18780", "abs": "https://arxiv.org/abs/2405.18780", "authors": ["Isha Chaudhary", "Qian Hu", "Manoj Kumar", "Morteza Ziyadi", "Rahul Gupta", "Gagandeep Singh"], "title": "Certifying Counterfactual Bias in LLMs", "categories": ["cs.AI", "cs.LG"], "comment": "Published at ICLR 2025", "summary": "Large Language Models (LLMs) can produce biased responses that can cause\nrepresentational harms. However, conventional studies are insufficient to\nthoroughly evaluate biases across LLM responses for different demographic\ngroups (a.k.a. counterfactual bias), as they do not scale to large number of\ninputs and do not provide guarantees. Therefore, we propose the first\nframework, LLMCert-B that certifies LLMs for counterfactual bias on\ndistributions of prompts. A certificate consists of high-confidence bounds on\nthe probability of unbiased LLM responses for any set of counterfactual prompts\n- prompts differing by demographic groups, sampled from a distribution. We\nillustrate counterfactual bias certification for distributions of\ncounterfactual prompts created by applying prefixes sampled from prefix\ndistributions, to a given set of prompts. We consider prefix distributions\nconsisting random token sequences, mixtures of manual jailbreaks, and\nperturbations of jailbreaks in LLM's embedding space. We generate non-trivial\ncertificates for SOTA LLMs, exposing their vulnerabilities over distributions\nof prompts generated from computationally inexpensive prefix distributions."}
{"id": "2504.15388", "pdf": "https://arxiv.org/pdf/2504.15388", "abs": "https://arxiv.org/abs/2504.15388", "authors": ["Tianyi Ma", "Tengyao Wang", "Richard J. Samworth"], "title": "Deep learning with missing data", "categories": ["stat.ME", "cs.LG", "math.ST", "stat.ML", "stat.TH", "62C20, 62D10, 62G08"], "comment": "49 pages, 9 figures", "summary": "In the context of multivariate nonparametric regression with missing\ncovariates, we propose Pattern Embedded Neural Networks (PENNs), which can be\napplied in conjunction with any existing imputation technique. In addition to a\nneural network trained on the imputed data, PENNs pass the vectors of\nobservation indicators through a second neural network to provide a compact\nrepresentation. The outputs are then combined in a third neural network to\nproduce final predictions. Our main theoretical result exploits an assumption\nthat the observation patterns can be partitioned into cells on which the Bayes\nregression function behaves similarly, and belongs to a compositional H\\\"older\nclass. It provides a finite-sample excess risk bound that holds for an\narbitrary missingness mechanism, and in combination with a complementary\nminimax lower bound, demonstrates that our PENN estimator attains in typical\ncases the minimax rate of convergence as if the cells of the partition were\nknown in advance, up to a poly-logarithmic factor in the sample size. Numerical\nexperiments on simulated, semi-synthetic and real data confirm that the PENN\nestimator consistently improves, often dramatically, on standard neural\nnetworks without pattern embedding. Code to reproduce our experiments, as well\nas a tutorial on how to apply our method, is publicly available."}
{"id": "2504.16083", "pdf": "https://arxiv.org/pdf/2504.16083", "abs": "https://arxiv.org/abs/2504.16083", "authors": ["Yucheng Li", "Huiqiang Jiang", "Chengruidong Zhang", "Qianhui Wu", "Xufang Luo", "Surin Ahn", "Amir H. Abdi", "Dongsheng Li", "Jianfeng Gao", "Yuqing Yang", "Lili Qiu"], "title": "MMInference: Accelerating Pre-filling for Long-Context VLMs via Modality-Aware Permutation Sparse Attention", "categories": ["cs.CV", "cs.LG"], "comment": null, "summary": "The integration of long-context capabilities with visual understanding\nunlocks unprecedented potential for Vision Language Models (VLMs). However, the\nquadratic attention complexity during the pre-filling phase remains a\nsignificant obstacle to real-world deployment. To overcome this limitation, we\nintroduce MMInference (Multimodality Million tokens Inference), a dynamic\nsparse attention method that accelerates the prefilling stage for long-context\nmulti-modal inputs. First, our analysis reveals that the temporal and spatial\nlocality of video input leads to a unique sparse pattern, the Grid pattern.\nSimultaneously, VLMs exhibit markedly different sparse distributions across\ndifferent modalities. We introduce a permutation-based method to leverage the\nunique Grid pattern and handle modality boundary issues. By offline search the\noptimal sparse patterns for each head, MMInference constructs the sparse\ndistribution dynamically based on the input. We also provide optimized GPU\nkernels for efficient sparse computations. Notably, MMInference integrates\nseamlessly into existing VLM pipelines without any model modifications or\nfine-tuning. Experiments on multi-modal benchmarks-including Video QA,\nCaptioning, VisionNIAH, and Mixed-Modality NIAH-with state-of-the-art\nlong-context VLMs (LongVila, LlavaVideo, VideoChat-Flash, Qwen2.5-VL) show that\nMMInference accelerates the pre-filling stage by up to 8.3x at 1M tokens while\nmaintaining accuracy. Our code is available at https://aka.ms/MMInference."}
{"id": "2504.07989", "pdf": "https://arxiv.org/pdf/2504.07989", "abs": "https://arxiv.org/abs/2504.07989", "authors": ["Nirvan Patil", "Malhar Abhay Inamdar", "Agnivo Gosai", "Guruprasad Pathak", "Anish Joshi", "Aryan Sagavekar", "Anish Joshirao", "Raj Dandekar", "Rajat Dandekar", "Sreedath Panat"], "title": "Regional Tiny Stories: Using Small Models to Compare Language Learning and Tokenizer Performance", "categories": ["cs.CL", "cs.AI"], "comment": "34 pages, 24 figures, 16 tables", "summary": "Small Language Models (SLMs) offer efficient alternatives to LLMs for\nspecific domains. The 2023 TinyStories study developed an English dataset that\nallows SLMs with 1 to 10 million parameters to produce coherent outputs. Our\nresearch expands this framework by translating the original dataset into Indian\nlanguages and creating synthetic data using LLMs. We focus on Hindi, Marathi,\nand Bengali, evaluating SLMs for regional language processing and understanding\nlinguistic complexity. We show that SLMs efficiently process regional languages\nwith significantly fewer parameters than LLMs, providing a complementary\nframework for ``inference based evaluation\" of tokenization strategies and\nlinguistic complexity. Our analysis shows that language-specific tokenizers\noutperform general-purpose ones for Indian languages. Empirical validations,\nsupported by information-theoretic and morphological analyses, provides\nfundamental understanding behind the better performance of Hindi models over\nMarathi and Bengali. Additionally, we show that synthetic datasets outperform\ntranslated content for training SLMs. Correlation analyses reveal\ncross-linguistic patterns and language-specific relationships between\ncreativity, grammatical precision, and narrative completeness. These findings\nadvance both the practical application of SLMs to underserved languages and our\ntheoretical understanding of neural language development."}
{"id": "2408.12871", "pdf": "https://arxiv.org/pdf/2408.12871", "abs": "https://arxiv.org/abs/2408.12871", "authors": ["Zhou Xiaochen", "Liang Xingzhou", "Zou Hui", "Lu Yi", "Qu Jingjing"], "title": "DeepDiveAI: Identifying AI Related Documents in Large Scale Literature Data", "categories": ["cs.AI"], "comment": null, "summary": "In this paper, we propose a method to automatically classify AI-related\ndocuments from large-scale literature databases, leading to the creation of an\nAI-related literature dataset, named DeepDiveAI. The dataset construction\napproach integrates expert knowledge with the capabilities of advanced models,\nstructured across two global stages. In the first stage, expert-curated\nclassification datasets are used to train an LSTM model, which classifies\ncoarse AI related records from large-scale datasets. In the second stage, we\nuse Qwen2.5 Plus to annotate a random 10% of the coarse AI-related records,\nwhich are then used to train a BERT binary classifier. This step further\nrefines the coarse AI related record set to obtain the final DeepDiveAI\ndataset. Evaluation results demonstrate that the entire workflow can\nefficiently and accurately identify AI-related literature from large-scale\ndatasets."}
{"id": "2504.15414", "pdf": "https://arxiv.org/pdf/2504.15414", "abs": "https://arxiv.org/abs/2504.15414", "authors": ["Dylan Khor", "Bowen Weng"], "title": "Post-Convergence Sim-to-Real Policy Transfer: A Principled Alternative to Cherry-Picking", "categories": ["cs.RO", "cs.LG"], "comment": null, "summary": "Learning-based approaches, particularly reinforcement learning (RL), have\nbecome widely used for developing control policies for autonomous agents, such\nas locomotion policies for legged robots. RL training typically maximizes a\npredefined reward (or minimizes a corresponding cost/loss) by iteratively\noptimizing policies within a simulator. Starting from a randomly initialized\npolicy, the empirical expected reward follows a trajectory with an overall\nincreasing trend. While some policies become temporarily stuck in local optima,\na well-defined training process generally converges to a reward level with\nnoisy oscillations. However, selecting a policy for real-world deployment is\nrarely an analytical decision (i.e., simply choosing the one with the highest\nreward) and is instead often performed through trial and error. To improve\nsim-to-real transfer, most research focuses on the pre-convergence stage,\nemploying techniques such as domain randomization, multi-fidelity training,\nadversarial training, and architectural innovations. However, these methods do\nnot eliminate the inevitable convergence trajectory and noisy oscillations of\nrewards, leading to heuristic policy selection or cherry-picking. This paper\naddresses the post-convergence sim-to-real transfer problem by introducing a\nworst-case performance transference optimization approach, formulated as a\nconvex quadratic-constrained linear programming problem. Extensive experiments\ndemonstrate its effectiveness in transferring RL-based locomotion policies from\nsimulation to real-world laboratory tests."}
{"id": "2504.09697", "pdf": "https://arxiv.org/pdf/2504.09697", "abs": "https://arxiv.org/abs/2504.09697", "authors": ["Kenan Tang", "Yanhong Li", "Yao Qin"], "title": "SPICE: A Synergistic, Precise, Iterative, and Customizable Image Editing Workflow", "categories": ["cs.GR", "cs.CV", "cs.LG"], "comment": "24 pages, 21 figures. Figure 9(b) has been accepted by CVPR AI Art\n  Gallery 2025", "summary": "Recent prompt-based image editing models have demonstrated impressive\nprompt-following capability at structural editing tasks. However, existing\nmodels still fail to perform local edits, follow detailed editing prompts, or\nmaintain global image quality beyond a single editing step. To address these\nchallenges, we introduce SPICE, a training-free workflow that accepts arbitrary\nresolutions and aspect ratios, accurately follows user requirements, and\nimproves image quality consistently during more than 100 editing steps. By\nsynergizing the strengths of a base diffusion model and a Canny edge ControlNet\nmodel, SPICE robustly handles free-form editing instructions from the user.\nSPICE outperforms state-of-the-art baselines on a challenging realistic\nimage-editing dataset consisting of semantic editing (object addition, removal,\nreplacement, and background change), stylistic editing (texture changes), and\nstructural editing (action change) tasks. Not only does SPICE achieve the\nhighest quantitative performance according to standard evaluation metrics, but\nit is also consistently preferred by users over existing image-editing methods.\nWe release the workflow implementation for popular diffusion model Web UIs to\nsupport further research and artistic exploration."}
{"id": "2504.10284", "pdf": "https://arxiv.org/pdf/2504.10284", "abs": "https://arxiv.org/abs/2504.10284", "authors": ["Weiqi Wang", "Jiefu Ou", "Yangqiu Song", "Benjamin Van Durme", "Daniel Khashabi"], "title": "Can LLMs Generate Tabular Summaries of Science Papers? Rethinking the Evaluation Protocol", "categories": ["cs.CL"], "comment": null, "summary": "Literature review tables are essential for summarizing and comparing\ncollections of scientific papers. We explore the task of generating tables that\nbest fulfill a user's informational needs given a collection of scientific\npapers. Building on recent work (Newman et al., 2024), we extend prior\napproaches to address real-world complexities through a combination of\nLLM-based methods and human annotations. Our contributions focus on three key\nchallenges encountered in real-world use: (i) User prompts are often\nunder-specified; (ii) Retrieved candidate papers frequently contain irrelevant\ncontent; and (iii) Task evaluation should move beyond shallow text similarity\ntechniques and instead assess the utility of inferred tables for\ninformation-seeking tasks (e.g., comparing papers). To support reproducible\nevaluation, we introduce ARXIV2TABLE, a more realistic and challenging\nbenchmark for this task, along with a novel approach to improve literature\nreview table generation in real-world scenarios. Our extensive experiments on\nthis benchmark show that both open-weight and proprietary LLMs struggle with\nthe task, highlighting its difficulty and the need for further advancements.\nOur dataset and code are available at https://github.com/JHU-CLSP/arXiv2Table."}
{"id": "2410.21131", "pdf": "https://arxiv.org/pdf/2410.21131", "abs": "https://arxiv.org/abs/2410.21131", "authors": ["Marharyta Domnich", "Julius Välja", "Rasmus Moorits Veski", "Giacomo Magnifico", "Kadi Tulver", "Eduard Barbu", "Raul Vicente"], "title": "Towards Unifying Evaluation of Counterfactual Explanations: Leveraging Large Language Models for Human-Centric Assessments", "categories": ["cs.AI", "cs.CL"], "comment": "This paper extends the AAAI-2025 version by including the Appendix", "summary": "As machine learning models evolve, maintaining transparency demands more\nhuman-centric explainable AI techniques. Counterfactual explanations, with\nroots in human reasoning, identify the minimal input changes needed to obtain a\ngiven output and, hence, are crucial for supporting decision-making. Despite\ntheir importance, the evaluation of these explanations often lacks grounding in\nuser studies and remains fragmented, with existing metrics not fully capturing\nhuman perspectives. To address this challenge, we developed a diverse set of 30\ncounterfactual scenarios and collected ratings across 8 evaluation metrics from\n206 respondents. Subsequently, we fine-tuned different Large Language Models\n(LLMs) to predict average or individual human judgment across these metrics.\nOur methodology allowed LLMs to achieve an accuracy of up to 63% in zero-shot\nevaluations and 85% (over a 3-classes prediction) with fine-tuning across all\nmetrics. The fine-tuned models predicting human ratings offer better\ncomparability and scalability in evaluating different counterfactual\nexplanation frameworks."}
{"id": "2504.15465", "pdf": "https://arxiv.org/pdf/2504.15465", "abs": "https://arxiv.org/abs/2504.15465", "authors": ["Patrick H. Coppock", "Brian Zhang", "Eliot H. Solomon", "Vasilis Kypriotis", "Leon Yang", "Bikash Sharma", "Dan Schatzberg", "Todd C. Mowry", "Dimitrios Skarlatos"], "title": "LithOS: An Operating System for Efficient Machine Learning on GPUs", "categories": ["cs.OS", "cs.LG"], "comment": null, "summary": "The surging demand for GPUs in datacenters for machine learning (ML) has made\nefficient GPU utilization crucial. However, meeting the diverse needs of ML\nmodels while optimizing resource usage is challenging. To enable transparent,\nfine-grained GPU management that maximizes utilization and energy efficiency\nwhile maintaining strong isolation, an operating system (OS) approach is\nneeded. This paper introduces LithOS, a first step toward a GPU OS. LithOS\nincludes the following new abstractions and mechanisms for efficient GPU\nresource management: (i) a novel TPC Scheduler that supports spatial scheduling\nat the granularity of individual TPCs, unlocking efficient TPC stealing between\nworkloads; (ii) transparent kernel atomization to reduce head-of-line blocking\nand enable dynamic resource reallocation mid-execution; (iii) a lightweight\nhardware right-sizing mechanism that determines the minimal TPC resources\nneeded per atom; and (iv) a transparent power management mechanism that reduces\npower consumption based on in-flight work behavior. We implement LithOS in Rust\nand evaluate its performance across extensive ML environments, comparing it to\nstate-of-the-art solutions from NVIDIA and prior research. For inference\nstacking, LithOS reduces tail latencies by 13x compared to MPS; compared to the\nbest SotA, it reduces tail latencies by 3x while improving aggregate throughput\nby 1.6x. In hybrid inference-training stacking, LithOS reduces tail latencies\nby 4.7x compared to MPS; compared to the best SotA, it reduces tail latencies\n1.18x while improving aggregate throughput by 1.35x. Finally, for a modest\nperformance hit under 4%, LithOS's right-sizing provides a quarter of GPU\ncapacity savings on average, while for a 7% hit, its power management yields a\nquarter of a GPU's energy savings. Overall, LithOS increases GPU efficiency,\nestablishing a foundation for future OS research on GPUs."}
{"id": "2504.15305", "pdf": "https://arxiv.org/pdf/2504.15305", "abs": "https://arxiv.org/abs/2504.15305", "authors": ["Abhishek Tyagi", "Charu Gaur"], "title": "SLAM-Based Navigation and Fault Resilience in a Surveillance Quadcopter with Embedded Vision Systems", "categories": ["cs.RO", "cs.CV", "cs.SY", "eess.SY", "68T40, 68U10, 70Q05", "I.2.9; I.4.8; I.2.10; C.3"], "comment": "18 pages, 21 figures, 4 tables. Onboard processing using Raspberry Pi\n  4 and Arduino Nano. Includes ORB-SLAM3-based navigation, LQR control, rotor\n  fault recovery, object detection, and PCA face recognition. Real-world and\n  simulation tests included. Designed for GPS-denied autonomous UAV\n  surveillance", "summary": "We present an autonomous aerial surveillance platform, Veg, designed as a\nfault-tolerant quadcopter system that integrates visual SLAM for\nGPS-independent navigation, advanced control architecture for dynamic\nstability, and embedded vision modules for real-time object and face\nrecognition. The platform features a cascaded control design with an LQR\ninner-loop and PD outer-loop trajectory control. It leverages ORB-SLAM3 for\n6-DoF localization and loop closure, and supports waypoint-based navigation\nthrough Dijkstra path planning over SLAM-derived maps. A real-time Failure\nDetection and Identification (FDI) system detects rotor faults and executes\nemergency landing through re-routing. The embedded vision system, based on a\nlightweight CNN and PCA, enables onboard object detection and face recognition\nwith high precision. The drone operates fully onboard using a Raspberry Pi 4\nand Arduino Nano, validated through simulations and real-world testing. This\nwork consolidates real-time localization, fault recovery, and embedded AI on a\nsingle platform suitable for constrained environments."}
{"id": "2504.11972", "pdf": "https://arxiv.org/pdf/2504.11972", "abs": "https://arxiv.org/abs/2504.11972", "authors": ["Xanh Ho", "Jiahao Huang", "Florian Boudin", "Akiko Aizawa"], "title": "LLM-as-a-Judge: Reassessing the Performance of LLMs in Extractive QA", "categories": ["cs.CL"], "comment": "17 pages; code and data are available at\n  https://github.com/Alab-NII/llm-judge-extract-qa", "summary": "Extractive reading comprehension question answering (QA) datasets are\ntypically evaluated using Exact Match (EM) and F1-score, but these metrics\noften fail to fully capture model performance. With the success of large\nlanguage models (LLMs), they have been employed in various tasks, including\nserving as judges (LLM-as-a-judge). In this paper, we reassess the performance\nof QA models using LLM-as-a-judge across four reading comprehension QA\ndatasets. We examine different families of LLMs and various answer types to\nevaluate the effectiveness of LLM-as-a-judge in these tasks. Our results show\nthat LLM-as-a-judge is highly correlated with human judgments and can replace\ntraditional EM/F1 metrics. By using LLM-as-a-judge, the correlation with human\njudgments improves significantly, from 0.22 (EM) and 0.40 (F1-score) to 0.85.\nThese findings confirm that EM and F1 metrics underestimate the true\nperformance of the QA models. While LLM-as-a-judge is not perfect for more\ndifficult answer types (e.g., job), it still outperforms EM/F1, and we observe\nno bias issues, such as self-preference, when the same model is used for both\nthe QA and judgment tasks."}
{"id": "2412.09385", "pdf": "https://arxiv.org/pdf/2412.09385", "abs": "https://arxiv.org/abs/2412.09385", "authors": ["Fabrizio Davide", "Pietro Torre", "Leonardo Ercolani", "Andrea Gaggioli"], "title": "AI Predicts AGI: Leveraging AGI Forecasting and Peer Review to Explore LLMs' Complex Reasoning Capabilities", "categories": ["cs.AI", "I.2.7"], "comment": "47 pages, 8 figures, 17 tables, appendix with data and code", "summary": "We tasked 16 state-of-the-art large language models (LLMs) with estimating\nthe likelihood of Artificial General Intelligence (AGI) emerging by 2030. To\nassess the quality of these forecasts, we implemented an automated peer review\nprocess (LLM-PR). The LLMs' estimates varied widely, ranging from 3% (Reka-\nCore) to 47.6% (GPT-4o), with a median of 12.5%. These estimates closely align\nwith a recent expert survey that projected a 10% likelihood of AGI by 2027,\nunderscoring the relevance of LLMs in forecasting complex, speculative\nscenarios. The LLM-PR process demonstrated strong reliability, evidenced by a\nhigh Intraclass Correlation Coefficient (ICC = 0.79), reflecting notable\nconsistency in scoring across the models. Among the models, Pplx-70b-online\nemerged as the top performer, while Gemini-1.5-pro-api ranked the lowest. A\ncross-comparison with external benchmarks, such as LMSYS Chatbot Arena,\nrevealed that LLM rankings remained consistent across different evaluation\nmethods, suggesting that existing benchmarks may not encapsulate some of the\nskills relevant for AGI prediction. We further explored the use of weighting\nschemes based on external benchmarks, optimizing the alignment of LLMs'\npredictions with human expert forecasts. This analysis led to the development\nof a new, 'AGI benchmark' designed to highlight performance differences in\nAGI-related tasks. Our findings offer insights into LLMs' capabilities in\nspeculative, interdisciplinary forecasting tasks and emphasize the growing need\nfor innovative evaluation frameworks for assessing AI performance in complex,\nuncertain real-world scenarios."}
{"id": "2504.15472", "pdf": "https://arxiv.org/pdf/2504.15472", "abs": "https://arxiv.org/abs/2504.15472", "authors": ["Pingcheng Jian", "Xiao Wei", "Yanbaihui Liu", "Samuel A. Moore", "Michael M. Zavlanos", "Boyuan Chen"], "title": "LAPP: Large Language Model Feedback for Preference-Driven Reinforcement Learning", "categories": ["cs.RO", "cs.LG", "cs.SY", "eess.SY"], "comment": null, "summary": "We introduce Large Language Model-Assisted Preference Prediction (LAPP), a\nnovel framework for robot learning that enables efficient, customizable, and\nexpressive behavior acquisition with minimum human effort. Unlike prior\napproaches that rely heavily on reward engineering, human demonstrations,\nmotion capture, or expensive pairwise preference labels, LAPP leverages large\nlanguage models (LLMs) to automatically generate preference labels from raw\nstate-action trajectories collected during reinforcement learning (RL). These\nlabels are used to train an online preference predictor, which in turn guides\nthe policy optimization process toward satisfying high-level behavioral\nspecifications provided by humans. Our key technical contribution is the\nintegration of LLMs into the RL feedback loop through trajectory-level\npreference prediction, enabling robots to acquire complex skills including\nsubtle control over gait patterns and rhythmic timing. We evaluate LAPP on a\ndiverse set of quadruped locomotion and dexterous manipulation tasks and show\nthat it achieves efficient learning, higher final performance, faster\nadaptation, and precise control of high-level behaviors. Notably, LAPP enables\nrobots to master highly dynamic and expressive tasks such as quadruped\nbackflips, which remain out of reach for standard LLM-generated or handcrafted\nrewards. Our results highlight LAPP as a promising direction for scalable\npreference-driven robot learning."}
{"id": "2504.15329", "pdf": "https://arxiv.org/pdf/2504.15329", "abs": "https://arxiv.org/abs/2504.15329", "authors": ["Yike Zhang", "Eduardo Davalos", "Jack Noble"], "title": "Vision6D: 3D-to-2D Interactive Visualization and Annotation Tool for 6D Pose Estimation", "categories": ["cs.GR", "cs.CV", "cs.HC", "cs.RO"], "comment": null, "summary": "Accurate 6D pose estimation has gained more attention over the years for\nrobotics-assisted tasks that require precise interaction with physical objects.\nThis paper presents an interactive 3D-to-2D visualization and annotation tool\nto support the 6D pose estimation research community. To the best of our\nknowledge, the proposed work is the first tool that allows users to visualize\nand manipulate 3D objects interactively on a 2D real-world scene, along with a\ncomprehensive user study. This system supports robust 6D camera pose annotation\nby providing both visual cues and spatial relationships to determine object\nposition and orientation in various environments. The annotation feature in\nVision6D is particularly helpful in scenarios where the transformation matrix\nbetween the camera and world objects is unknown, as it enables accurate\nannotation of these objects' poses using only the camera intrinsic matrix. This\ncapability serves as a foundational step in developing and training advanced\npose estimation models across various domains. We evaluate Vision6D's\neffectiveness by utilizing widely-used open-source pose estimation datasets\nLinemod and HANDAL through comparisons between the default ground-truth camera\nposes with manual annotations. A user study was performed to show that Vision6D\ngenerates accurate pose annotations via visual cues in an intuitive 3D user\ninterface. This approach aims to bridge the gap between 2D scene projections\nand 3D scenes, offering an effective way for researchers and developers to\nsolve 6D pose annotation related problems. The software is open-source and\npublicly available at https://github.com/InteractiveGL/vision6D."}
{"id": "2504.12915", "pdf": "https://arxiv.org/pdf/2504.12915", "abs": "https://arxiv.org/abs/2504.12915", "authors": ["Ebrahim Norouzi", "Sven Hertling", "Harald Sack"], "title": "ConExion: Concept Extraction with Large Language Models", "categories": ["cs.CL", "cs.IR"], "comment": null, "summary": "In this paper, an approach for concept extraction from documents using\npre-trained large language models (LLMs) is presented. Compared with\nconventional methods that extract keyphrases summarizing the important\ninformation discussed in a document, our approach tackles a more challenging\ntask of extracting all present concepts related to the specific domain, not\njust the important ones. Through comprehensive evaluations of two widely used\nbenchmark datasets, we demonstrate that our method improves the F1 score\ncompared to state-of-the-art techniques. Additionally, we explore the potential\nof using prompts within these models for unsupervised concept extraction. The\nextracted concepts are intended to support domain coverage evaluation of\nontologies and facilitate ontology learning, highlighting the effectiveness of\nLLMs in concept extraction tasks. Our source code and datasets are publicly\navailable at https://github.com/ISE-FIZKarlsruhe/concept_extraction."}
{"id": "2412.11373", "pdf": "https://arxiv.org/pdf/2412.11373", "abs": "https://arxiv.org/abs/2412.11373", "authors": ["Matthew Stephenson", "Matthew Sidji", "Benoît Ronval"], "title": "Codenames as a Benchmark for Large Language Models", "categories": ["cs.AI", "cs.CL"], "comment": "12 pages, 2 figures, 2 tables", "summary": "In this paper, we propose the use of the popular word-based board game\nCodenames as a suitable benchmark for evaluating the reasoning capabilities of\nLarge Language Models (LLMs). Codenames presents a highly interesting challenge\nfor achieving successful AI performance, requiring both a sophisticated\nunderstanding of language, theory of mind, and epistemic reasoning\ncapabilities. Prior attempts to develop agents for Codenames have largely\nrelied on word embedding techniques, which have a limited vocabulary range and\nperform poorly when paired with differing approaches. LLMs have demonstrated\nenhanced reasoning and comprehension capabilities for language-based tasks, but\ncan still suffer in lateral thinking challenges. We evaluate the capabilities\nof several state-of-the-art LLMs, including GPT-4o, Gemini 1.5, Claude 3.5\nSonnet, and Llama 3.1, across a variety of board setups. Our results indicate\nthat while certain LLMs perform better than others overall, different models\nexhibit varying emergent behaviours during gameplay and excel at specific\nroles. We also evaluate the performance of different combinations of LLMs when\nplaying cooperatively together, demonstrating that LLM agents are more\ngeneralisable to a wider range of teammates than prior techniques."}
{"id": "2504.15512", "pdf": "https://arxiv.org/pdf/2504.15512", "abs": "https://arxiv.org/abs/2504.15512", "authors": ["Siyuan Liang", "Jiayang Liu", "Jiecheng Zhai", "Tianmeng Fang", "Rongcheng Tu", "Aishan Liu", "Xiaochun Cao", "Dacheng Tao"], "title": "T2VShield: Model-Agnostic Jailbreak Defense for Text-to-Video Models", "categories": ["cs.CR", "cs.LG"], "comment": "25 pages, 5 figures", "summary": "The rapid development of generative artificial intelligence has made text to\nvideo models essential for building future multimodal world simulators.\nHowever, these models remain vulnerable to jailbreak attacks, where specially\ncrafted prompts bypass safety mechanisms and lead to the generation of harmful\nor unsafe content. Such vulnerabilities undermine the reliability and security\nof simulation based applications. In this paper, we propose T2VShield, a\ncomprehensive and model agnostic defense framework designed to protect text to\nvideo models from jailbreak threats. Our method systematically analyzes the\ninput, model, and output stages to identify the limitations of existing\ndefenses, including semantic ambiguities in prompts, difficulties in detecting\nmalicious content in dynamic video outputs, and inflexible model centric\nmitigation strategies. T2VShield introduces a prompt rewriting mechanism based\non reasoning and multimodal retrieval to sanitize malicious inputs, along with\na multi scope detection module that captures local and global inconsistencies\nacross time and modalities. The framework does not require access to internal\nmodel parameters and works with both open and closed source systems. Extensive\nexperiments on five platforms show that T2VShield can reduce jailbreak success\nrates by up to 35 percent compared to strong baselines. We further develop a\nhuman centered audiovisual evaluation protocol to assess perceptual safety,\nemphasizing the importance of visual level defense in enhancing the\ntrustworthiness of next generation multimodal simulators."}
{"id": "2504.13914", "pdf": "https://arxiv.org/pdf/2504.13914", "abs": "https://arxiv.org/abs/2504.13914", "authors": ["ByteDance Seed", ":", "Jiaze Chen", "Tiantian Fan", "Xin Liu", "Lingjun Liu", "Zhiqi Lin", "Mingxuan Wang", "Chengyi Wang", "Xiangpeng Wei", "Wenyuan Xu", "Yufeng Yuan", "Yu Yue", "Lin Yan", "Qiying Yu", "Xiaochen Zuo", "Chi Zhang", "Ruofei Zhu", "Zhecheng An", "Zhihao Bai", "Yu Bao", "Xingyan Bin", "Jiangjie Chen", "Feng Chen", "Hongmin Chen", "Riwei Chen", "Liangqiang Chen", "Zixin Chen", "Jinsong Chen", "Siyan Chen", "Kaiyuan Chen", "Zhi Chen", "Jin Chen", "Jiecao Chen", "Jinxin Chi", "Weinan Dai", "Ning Dai", "Jiahui Dai", "Shihan Dou", "Yantao Du", "Zhengyin Du", "Jianhui Duan", "Chen Dun", "Ting-Han Fan", "Jiazhan Feng", "Junda Feng", "Ziyuan Feng", "Yuwei Fu", "Wenqi Fu", "Hanjie Fu", "Hao Ge", "Hongyi Guo", "Mingji Han", "Li Han", "Wenhao Hao", "Xintong Hao", "Qianyu He", "Jerry He", "Feng He", "Wen Heng", "Zehua Hong", "Qi Hou", "Liang Hu", "Shengding Hu", "Nan Hu", "Kai Hua", "Qi Huang", "Ziyue Huang", "Hongzhi Huang", "Zihao Huang", "Ting Huang", "Wenhao Huang", "Wei Jia", "Bin Jia", "Xiaoying Jia", "Yuhua Jiang", "Haobin Jiang", "Ziheng Jiang", "Kaihua Jiang", "Chengquan Jiang", "Jianpeng Jiao", "Xiaoran Jin", "Xing Jin", "Xunhao Lai", "Zheng Li", "Xiang Li", "Liyi Li", "Hongkai Li", "Zheng Li", "Shengxian Wan", "Ya Wang", "Yunshui Li", "Chenggang Li", "Niuniu Li", "Siyu Li", "Xi Li", "Xiao Li", "Aoyan Li", "Yuntao Li", "Nianning Liang", "Xinnian Liang", "Haibin Lin", "Weijian Lin", "Ye Lin", "Zhicheng Liu", "Guanlin Liu", "Guanlin Liu", "Chenxiao Liu", "Yan Liu", "Gaohong Liu", "Juncai Liu", "Chundian Liu", "Deyi Liu", "Kaibo Liu", "Siyao Liu", "Qi Liu", "Yongfei Liu", "Kang Liu", "Gan Liu", "Boyi Liu", "Rui Long", "Weiqiang Lou", "Chenwei Lou", "Xiang Luo", "Yao Luo", "Caiping Lv", "Heyang Lv", "Bole Ma", "Qianli Ma", "Hongzhi Ma", "Yiyuan Ma", "Jin Ma", "Wenchang Ma", "Tingting Ma", "Chen Mao", "Qiyang Min", "Zhe Nan", "Guanghan Ning", "Jinxiang Ou", "Haojie Pan", "Renming Pang", "Yanghua Peng", "Tao Peng", "Lihua Qian", "Lihua Qian", "Mu Qiao", "Meng Qu", "Cheng Ren", "Hongbin Ren", "Yong Shan", "Wei Shen", "Ke Shen", "Kai Shen", "Guangming Sheng", "Jinlong Shi", "Wenlei Shi", "Guang Shi", "Shuai Shuai Cao", "Yuxin Song", "Zuquan Song", "Jing Su", "Yifan Sun", "Tao Sun", "Zewei Sun", "Borui Wan", "Zihan Wang", "Xiaohui Wang", "Xi Wang", "Shuguang Wang", "Jun Wang", "Qinlong Wang", "Chenyuan Wang", "Shuai Wang", "Zihan Wang", "Changbao Wang", "Jiaqiang Wang", "Shihang Wang", "Xuwu Wang", "Zaiyuan Wang", "Yuxuan Wang", "Wenqi Wang", "Taiqing Wang", "Chengzhi Wei", "Houmin Wei", "Ziyun Wei", "Shufa Wei", "Zheng Wu", "Yonghui Wu", "Yangjun Wu", "Bohong Wu", "Shuang Wu", "Jingqiao Wu", "Ning Wu", "Shuangzhi Wu", "Jianmin Wu", "Chenguang Xi", "Fan Xia", "Yuqiao Xian", "Liang Xiang", "Boren Xiang", "Bowen Xiao", "Zhen Xiao", "Xia Xiao", "Yongsheng Xiao", "Chao Xin", "Shulin Xin", "Yuwen Xiong", "Jingjing Xu", "Ziwen Xu", "Chenyin Xu", "Jiayi Xu", "Yifan Xu", "Wei Xu", "Yufei Xu", "Shikun Xu", "Shipeng Yan", "Shen Yan", "Qingping Yang", "Xi Yang", "Tianhao Yang", "Yuehang Yang", "Yuan Yang", "Ximing Yang", "Zeyu Yang", "Guang Yang", "Yifan Yang", "Xuesong Yao", "Bairen Yi", "Fan Yin", "Jianian Yin", "Ziqiang Ying", "Xiangyu Yu", "Hongli Yu", "Song Yu", "Menghan Yu", "Huan Yu", "Siyu Yuan", "Jun Yuan", "Yutao Zeng", "Tianyang Zhan", "Zheng Zhang", "Yun Zhang", "Mofan Zhang", "Wang Zhang", "Ru Zhang", "Zhi Zhang", "Tianqi Zhang", "Xinyi Zhang", "Zhexi Zhang", "Sijun Zhang", "Wenqiang Zhang", "Xiangxiang Zhang", "Yongtao Zhang", "Yuyu Zhang", "Ge Zhang", "He Zhang", "Yue Zhang", "Renjie Zheng", "Ningxin Zheng", "Zhuolin Zheng", "Yaowei Zheng", "Chen Zheng", "Xiaoyun Zhi", "Wanjun Zhong", "Cheng Zhong", "Zheng Zhong", "Baoquan Zhong", "Xun Zhou", "Na Zhou", "Huan Zhou", "Hang Zhu", "Defa Zhu", "Wenjia Zhu", "Lei Zuo"], "title": "Seed-Thinking-v1.5: Advancing Superb Reasoning Models with Reinforcement Learning", "categories": ["cs.CL"], "comment": null, "summary": "We introduce Seed-Thinking-v1.5, capable of reasoning through thinking before\nresponding, resulting in improved performance on a wide range of benchmarks.\nSeed-Thinking-v1.5 achieves 86.7 on AIME 2024, 55.0 on Codeforces and 77.3 on\nGPQA, demonstrating excellent reasoning abilities in STEM and coding. Beyond\nreasoning tasks, the method demonstrates notable generalization across diverse\ndomains. For instance, it surpasses DeepSeek R1 by 8% in win rate on\nnon-reasoning tasks, indicating its broader applicability. Compared to other\nstate-of-the-art reasoning models, Seed-Thinking-v1.5 is a Mixture-of-Experts\n(MoE) model with a relatively small size, featuring 20B activated and 200B\ntotal parameters. As part of our effort to assess generalized reasoning, we\ndevelop two internal benchmarks, BeyondAIME and Codeforces, both of which will\nbe publicly released to support future research."}
{"id": "2412.11761", "pdf": "https://arxiv.org/pdf/2412.11761", "abs": "https://arxiv.org/abs/2412.11761", "authors": ["Timothée Anne", "Noah Syrkis", "Meriem Elhosni", "Florian Turati", "Franck Legendre", "Alain Jaquier", "Sebastian Risi"], "title": "Harnessing Language for Coordination: A Framework and Benchmark for LLM-Driven Multi-Agent Control", "categories": ["cs.AI"], "comment": null, "summary": "Large Language Models (LLMs) have demonstrated remarkable performance across\nvarious tasks. Their potential to facilitate human coordination with many\nagents is a promising but largely under-explored area. Such capabilities would\nbe helpful in disaster response, urban planning, and real-time strategy\nscenarios. In this work, we introduce (1) a real-time strategy game benchmark\ndesigned to evaluate these abilities and (2) a novel framework we term HIVE.\nHIVE empowers a single human to coordinate swarms of up to 2,000 agents through\na natural language dialog with an LLM. We present promising results on this\nmulti-agent benchmark, with our hybrid approach solving tasks such as\ncoordinating agent movements, exploiting unit weaknesses, leveraging human\nannotations, and understanding terrain and strategic points. Our findings also\nhighlight critical limitations of current models, including difficulties in\nprocessing spatial visual information and challenges in formulating long-term\nstrategic plans. This work sheds light on the potential and limitations of LLMs\nin human-swarm coordination, paving the way for future research in this area.\nThe HIVE project page, hive.syrkis.com, includes videos of the system in\naction."}
{"id": "2504.15517", "pdf": "https://arxiv.org/pdf/2504.15517", "abs": "https://arxiv.org/abs/2504.15517", "authors": ["Mingchen Song", "Xiang Deng", "Guoqiang Zhong", "Qi Lv", "Jia Wan", "Yinchuan Li", "Jianye Hao", "Weili Guan"], "title": "Few-Shot Vision-Language Action-Incremental Policy Learning", "categories": ["cs.RO", "cs.LG"], "comment": null, "summary": "Recently, Transformer-based robotic manipulation methods utilize multi-view\nspatial representations and language instructions to learn robot motion\ntrajectories by leveraging numerous robot demonstrations. However, the\ncollection of robot data is extremely challenging, and existing methods lack\nthe capability for continuous learning on new tasks with only a few\ndemonstrations. In this paper, we formulate these challenges as the Few-Shot\nAction-Incremental Learning (FSAIL) task, and accordingly design a Task-prOmpt\ngraPh evolutIon poliCy (TOPIC) to address these issues. Specifically, to\naddress the data scarcity issue in robotic imitation learning, TOPIC learns\nTask-Specific Prompts (TSP) through the deep interaction of multi-modal\ninformation within few-shot demonstrations, thereby effectively extracting the\ntask-specific discriminative information. On the other hand, to enhance the\ncapability for continual learning on new tasks and mitigate the issue of\ncatastrophic forgetting, TOPIC adopts a Continuous Evolution Strategy (CES).\nCES leverages the intrinsic relationships between tasks to construct a task\nrelation graph, which effectively facilitates the adaptation of new tasks by\nreusing skills learned from previous tasks. TOPIC pioneers few-shot continual\nlearning in the robotic manipulation task, and extensive experimental results\ndemonstrate that TOPIC outperforms state-of-the-art baselines by over 26$\\%$ in\nsuccess rate, significantly enhancing the continual learning capabilities of\nexisting Transformer-based policies."}
{"id": "2504.15899", "pdf": "https://arxiv.org/pdf/2504.15899", "abs": "https://arxiv.org/abs/2504.15899", "authors": ["Blerim Abdullai", "Tony Wang", "Xinyuan Qiao", "Florian Shkurti", "Timothy D. Barfoot"], "title": "RaSCL: Radar to Satellite Crossview Localization", "categories": ["cs.RO", "cs.CV"], "comment": null, "summary": "GNSS is unreliable, inaccurate, and insufficient in many real-time autonomous\nfield applications. In this work, we present a GNSS-free global localization\nsolution that contains a method of registering imaging radar on the ground with\noverhead RGB imagery, with joint optimization of relative poses from odometry\nand global poses from our overhead registration. Previous works have used\nvarious combinations of ground sensors and overhead imagery, and different\nfeature extraction and matching methods. These include various handcrafted and\ndeep-learning-based methods for extracting features from overhead imagery. Our\nwork presents insights on extracting essential features from RGB overhead\nimages for effective global localization against overhead imagery using only\nground radar and a single georeferenced initial guess. We motivate our method\nby evaluating it on datasets in diverse geographic conditions and robotic\nplatforms, including on an Unmanned Surface Vessel (USV) as well as urban and\nsuburban driving datasets."}
{"id": "2409.13221", "pdf": "https://arxiv.org/pdf/2409.13221", "abs": "https://arxiv.org/abs/2409.13221", "authors": ["Yinmin Zhong", "Zili Zhang", "Bingyang Wu", "Shengyu Liu", "Yukun Chen", "Changyi Wan", "Hanpeng Hu", "Lei Xia", "Ranchen Ming", "Yibo Zhu", "Xin Jin"], "title": "Optimizing RLHF Training for Large Language Models with Stage Fusion", "categories": ["cs.LG", "cs.CL", "cs.DC"], "comment": null, "summary": "We present RLHFuse, an efficient training system with stage fusion for\nReinforcement Learning from Human Feedback (RLHF). Due to the intrinsic nature\nof RLHF training, i.e., the data skewness in the generation stage and the\npipeline bubbles in the training stage, existing RLHF systems suffer from low\nGPU utilization. RLHFuse breaks the traditional view of RLHF workflow as a\ncomposition of individual tasks, splitting each task into finer-grained\nsubtasks, and performing stage fusion to improve GPU utilization. RLHFuse\ncontains two key ideas. First, for generation and inference tasks, RLHFuse\nsplits them into sample-level subtasks, enabling efficient inter-stage fusion\nto overlap the execution of generation and inference stages, thus mitigating\nthe original generation bottleneck dominated by long-tailed samples. Second,\nfor training tasks, RLHFuse breaks them into subtasks of micro-batches and\nperforms intra-stage fusion to concurrently execute these subtasks in the\ntraining stage with a fused pipeline schedule, effectively mitigating the\npipeline bubbles. The experiments show that RLHFuse increases the training\nthroughput by up to $3.7\\times$, compared to existing systems."}
{"id": "2504.09597", "pdf": "https://arxiv.org/pdf/2504.09597", "abs": "https://arxiv.org/abs/2504.09597", "authors": ["Zhixuan Pan", "Shaowen Wang", "Jian Li"], "title": "Understanding LLM Behaviors via Compression: Data Generation, Knowledge Acquisition and Scaling Laws", "categories": ["cs.AI", "cs.IT", "cs.LG", "math.IT"], "comment": null, "summary": "Large Language Models (LLMs) have demonstrated remarkable capabilities across\nnumerous tasks, yet principled explanations for their underlying mechanisms and\nseveral phenomena, such as scaling laws, hallucinations, and related behaviors,\nremain elusive. In this work, we revisit the classical relationship between\ncompression and prediction, grounded in Kolmogorov complexity and Shannon\ninformation theory, to provide deeper insights into LLM behaviors. By\nleveraging the Kolmogorov Structure Function and interpreting LLM compression\nas a two-part coding process, we offer a detailed view of how LLMs acquire and\nstore information across increasing model and data scales -- from pervasive\nsyntactic patterns to progressively rarer knowledge elements. Motivated by this\ntheoretical perspective and natural assumptions inspired by Heap's and Zipf's\nlaws, we introduce a simplified yet representative hierarchical data-generation\nframework called the Syntax-Knowledge model. Under the Bayesian setting, we\nshow that prediction and compression within this model naturally lead to\ndiverse learning and scaling behaviors of LLMs. In particular, our theoretical\nanalysis offers intuitive and principled explanations for both data and model\nscaling laws, the dynamics of knowledge acquisition during training and\nfine-tuning, factual knowledge hallucinations in LLMs. The experimental results\nvalidate our theoretical predictions."}
{"id": "2504.15541", "pdf": "https://arxiv.org/pdf/2504.15541", "abs": "https://arxiv.org/abs/2504.15541", "authors": ["Qichao Liu", "Heye Huang", "Shiyue Zhao", "Lei Shi", "Soyoung Ahn", "Xiaopeng Li"], "title": "RiskNet: Interaction-Aware Risk Forecasting for Autonomous Driving in Long-Tail Scenarios", "categories": ["cs.RO", "cs.LG"], "comment": "24 pages, 14 figures", "summary": "Ensuring the safety of autonomous vehicles (AVs) in long-tail scenarios\nremains a critical challenge, particularly under high uncertainty and complex\nmulti-agent interactions. To address this, we propose RiskNet, an\ninteraction-aware risk forecasting framework, which integrates deterministic\nrisk modeling with probabilistic behavior prediction for comprehensive risk\nassessment. At its core, RiskNet employs a field-theoretic model that captures\ninteractions among ego vehicle, surrounding agents, and infrastructure via\ninteraction fields and force. This model supports multidimensional risk\nevaluation across diverse scenarios (highways, intersections, and roundabouts),\nand shows robustness under high-risk and long-tail settings. To capture the\nbehavioral uncertainty, we incorporate a graph neural network (GNN)-based\ntrajectory prediction module, which learns multi-modal future motion\ndistributions. Coupled with the deterministic risk field, it enables dynamic,\nprobabilistic risk inference across time, enabling proactive safety assessment\nunder uncertainty. Evaluations on the highD, inD, and rounD datasets, spanning\nlane changes, turns, and complex merges, demonstrate that our method\nsignificantly outperforms traditional approaches (e.g., TTC, THW, RSS, NC\nField) in terms of accuracy, responsiveness, and directional sensitivity, while\nmaintaining strong generalization across scenarios. This framework supports\nreal-time, scenario-adaptive risk forecasting and demonstrates strong\ngeneralization across uncertain driving environments. It offers a unified\nfoundation for safety-critical decision-making in long-tail scenarios."}
{"id": "2504.15953", "pdf": "https://arxiv.org/pdf/2504.15953", "abs": "https://arxiv.org/abs/2504.15953", "authors": ["Chance J. Hamilton", "Alfredo Weitzenfeld"], "title": "Visual Place Cell Encoding: A Computational Model for Spatial Representation and Cognitive Mapping", "categories": ["cs.RO", "cs.CV"], "comment": null, "summary": "This paper presents the Visual Place Cell Encoding (VPCE) model, a\nbiologically inspired computational framework for simulating place cell-like\nactivation using visual input. Drawing on evidence that visual landmarks play a\ncentral role in spatial encoding, the proposed VPCE model activates visual\nplace cells by clustering high-dimensional appearance features extracted from\nimages captured by a robot-mounted camera. Each cluster center defines a\nreceptive field, and activation is computed based on visual similarity using a\nradial basis function. We evaluate whether the resulting activation patterns\ncorrelate with key properties of biological place cells, including spatial\nproximity, orientation alignment, and boundary differentiation. Experiments\ndemonstrate that the VPCE can distinguish between visually similar yet\nspatially distinct locations and adapt to environment changes such as the\ninsertion or removal of walls. These results suggest that structured visual\ninput, even in the absence of motion cues or reward-driven learning, is\nsufficient to generate place-cell-like spatial representations and support\nbiologically inspired cognitive mapping."}
{"id": "2410.02064", "pdf": "https://arxiv.org/pdf/2410.02064", "abs": "https://arxiv.org/abs/2410.02064", "authors": ["Christopher Ackerman", "Nina Panickssery"], "title": "Inspection and Control of Self-Generated-Text Recognition Ability in Llama3-8b-Instruct", "categories": ["cs.LG", "cs.CL"], "comment": "10 pages, 13 figs, 2 tables, accepted as conference paper to ICLR\n  2025", "summary": "It has been reported that LLMs can recognize their own writing. As this has\npotential implications for AI safety, yet is relatively understudied, we\ninvestigate the phenomenon, seeking to establish whether it robustly occurs at\nthe behavioral level, how the observed behavior is achieved, and whether it can\nbe controlled. First, we find that the Llama3-8b-Instruct chat model - but not\nthe base Llama3-8b model - can reliably distinguish its own outputs from those\nof humans, and present evidence that the chat model is likely using its\nexperience with its own outputs, acquired during post-training, to succeed at\nthe writing recognition task. Second, we identify a vector in the residual\nstream of the model that is differentially activated when the model makes a\ncorrect self-written-text recognition judgment, show that the vector activates\nin response to information relevant to self-authorship, present evidence that\nthe vector is related to the concept of \"self\" in the model, and demonstrate\nthat the vector is causally related to the model's ability to perceive and\nassert self-authorship. Finally, we show that the vector can be used to control\nboth the model's behavior and its perception, steering the model to claim or\ndisclaim authorship by applying the vector to the model's output as it\ngenerates it, and steering the model to believe or disbelieve it wrote\narbitrary texts by applying the vector to them as the model reads them."}
{"id": "2504.14128", "pdf": "https://arxiv.org/pdf/2504.14128", "abs": "https://arxiv.org/abs/2504.14128", "authors": ["Christopher Zhang Cui", "Xingdi Yuan", "Ziang Xiao", "Prithviraj Ammanabrolu", "Marc-Alexandre Côté"], "title": "TALES: Text Adventure Learning Environment Suite", "categories": ["cs.AI", "cs.CL"], "comment": null, "summary": "Reasoning is an essential skill to enable Large Language Models (LLMs) to\ninteract with the world. As tasks become more complex, they demand increasingly\nsophisticated and diverse reasoning capabilities for sequential\ndecision-making, requiring structured reasoning over the context history to\ndetermine the next best action. We introduce TALES, a diverse collection of\nsynthetic and human-written text-adventure games designed to challenge and\nevaluate diverse reasoning capabilities. We present results over a range of\nLLMs, open- and closed-weights, performing a qualitative analysis on the top\nperforming models. Despite an impressive showing on synthetic games, even the\ntop LLM-driven agents fail to achieve 15% on games designed for human\nenjoyment. Code and visualization of the experiments can be found at\nhttps://microsoft.github.io/tales."}
{"id": "2504.15561", "pdf": "https://arxiv.org/pdf/2504.15561", "abs": "https://arxiv.org/abs/2504.15561", "authors": ["Jingkai Xu", "Xiangli Nie"], "title": "SPECI: Skill Prompts based Hierarchical Continual Imitation Learning for Robot Manipulation", "categories": ["cs.RO", "cs.LG"], "comment": null, "summary": "Real-world robot manipulation in dynamic unstructured environments requires\nlifelong adaptability to evolving objects, scenes and tasks. Traditional\nimitation learning relies on static training paradigms, which are ill-suited\nfor lifelong adaptation. Although Continual Imitation Learnin (CIL) enables\nincremental task adaptation while preserving learned knowledge, current CIL\nmethods primarily overlook the intrinsic skill characteristics of robot\nmanipulation or depend on manually defined and rigid skills, leading to\nsuboptimal cross-task knowledge transfer. To address these issues, we propose\nSkill Prompts-based HiErarchical Continual Imitation Learning (SPECI), a novel\nend-to-end hierarchical CIL policy architecture for robot manipulation. The\nSPECI framework consists of a multimodal perception and fusion module for\nheterogeneous sensory information encoding, a high-level skill inference module\nfor dynamic skill extraction and selection, and a low-level action execution\nmodule for precise action generation. To enable efficient knowledge transfer on\nboth skill and task levels, SPECI performs continual implicit skill acquisition\nand reuse via an expandable skill codebook and an attention-driven skill\nselection mechanism. Furthermore, we introduce mode approximation to augment\nthe last two modules with task-specific and task-sharing parameters, thereby\nenhancing task-level knowledge transfer. Extensive experiments on diverse\nmanipulation task suites demonstrate that SPECI consistently outperforms\nstate-of-the-art CIL methods across all evaluated metrics, revealing\nexceptional bidirectional knowledge transfer and superior overall performance."}
{"id": "2504.15975", "pdf": "https://arxiv.org/pdf/2504.15975", "abs": "https://arxiv.org/abs/2504.15975", "authors": ["Peter Fletcher"], "title": "A New Graph Grammar Formalism for Robust Syntactic Pattern Recognition", "categories": ["cs.FL", "cs.CV", "F.4.2; F.4.3"], "comment": "64 pages, 23 figures", "summary": "I introduce a formalism for representing the syntax of recursively structured\ngraph-like patterns. It does not use production rules, like a conventional\ngraph grammar, but represents the syntactic structure in a more direct and\ndeclarative way. The grammar and the pattern are both represented as networks,\nand parsing is seen as the construction of a homomorphism from the pattern to\nthe grammar. The grammars can represent iterative, hierarchical and nested\nrecursive structure in more than one dimension.\n  This supports a highly parallel style of parsing, in which all aspects of\npattern recognition (feature detection, segmentation, parsing, filling in\nmissing symbols, top-down and bottom-up inference) are integrated into a single\nprocess, to exploit the synergy between them.\n  The emphasis of this paper is on underlying theoretical issues, but I also\ngive some example runs to illustrate the error-tolerant parsing of complex\nrecursively structured patterns of 50-1000 symbols, involving variability in\ngeometric relationships, blurry and indistinct symbols, overlapping symbols,\ncluttered images, and erased patches."}
{"id": "2410.13828", "pdf": "https://arxiv.org/pdf/2410.13828", "abs": "https://arxiv.org/abs/2410.13828", "authors": ["Hui Yuan", "Yifan Zeng", "Yue Wu", "Huazheng Wang", "Mengdi Wang", "Liu Leqi"], "title": "A Common Pitfall of Margin-based Language Model Alignment: Gradient Entanglement", "categories": ["cs.LG", "cs.AI", "cs.CL"], "comment": null, "summary": "Reinforcement Learning from Human Feedback (RLHF) has become the predominant\napproach for language model (LM) alignment. At its core, RLHF uses a\nmargin-based loss for preference optimization, specifying ideal LM behavior\nonly by the difference between preferred and dispreferred responses. In this\npaper, we identify a common pitfall of margin-based methods -- the\nunder-specification of ideal LM behavior on preferred and dispreferred\nresponses individually, which leads to two unintended consequences as the\nmargin increases: (1) The probability of dispreferred (e.g., unsafe) responses\nmay increase, resulting in potential safety alignment failures. (2) The\nprobability of preferred responses may decrease, even when those responses are\nideal. We demystify the reasons behind these problematic behaviors:\nmargin-based losses couple the change in the preferred probability to the\ngradient of the dispreferred one, and vice versa, often preventing the\npreferred probability from increasing while the dispreferred one decreases, and\nthus causing a synchronized increase or decrease in both probabilities. We term\nthis effect, inherent in margin-based objectives, gradient entanglement.\nFormally, we derive conditions for general margin-based alignment objectives\nunder which gradient entanglement becomes concerning: the inner product of the\ngradients of preferred and dispreferred log-probabilities is large relative to\nthe individual gradient norms. We theoretically investigate why such inner\nproducts can be large when aligning language models and empirically validate\nour findings. Empirical implications of our framework extend to explaining\nimportant differences in the training dynamics of various preference\noptimization algorithms, and suggesting potential algorithm designs to mitigate\nthe under-specification issue of margin-based methods and thereby improving\nlanguage model alignment."}
{"id": "2504.14325", "pdf": "https://arxiv.org/pdf/2504.14325", "abs": "https://arxiv.org/abs/2504.14325", "authors": ["Alessio Buscemi", "Daniele Proverbio", "Alessandro Di Stefano", "The Anh Han", "German Castignani", "Pietro Liò"], "title": "FAIRGAME: a Framework for AI Agents Bias Recognition using Game Theory", "categories": ["cs.AI"], "comment": null, "summary": "Letting AI agents interact in multi-agent applications adds a layer of\ncomplexity to the interpretability and prediction of AI outcomes, with profound\nimplications for their trustworthy adoption in research and society. Game\ntheory offers powerful models to capture and interpret strategic interaction\namong agents, but requires the support of reproducible, standardized and\nuser-friendly IT frameworks to enable comparison and interpretation of results.\nTo this end, we present FAIRGAME, a Framework for AI Agents Bias Recognition\nusing Game Theory. We describe its implementation and usage, and we employ it\nto uncover biased outcomes in popular games among AI agents, depending on the\nemployed Large Language Model (LLM) and used language, as well as on the\npersonality trait or strategic knowledge of the agents. Overall, FAIRGAME\nallows users to reliably and easily simulate their desired games and scenarios\nand compare the results across simulation campaigns and with game-theoretic\npredictions, enabling the systematic discovery of biases, the anticipation of\nemerging behavior out of strategic interplays, and empowering further research\ninto strategic decision-making using LLM agents."}
{"id": "2504.15577", "pdf": "https://arxiv.org/pdf/2504.15577", "abs": "https://arxiv.org/abs/2504.15577", "authors": ["Qingyuan He", "Chang Liu", "Juecen Zhan", "Weiqiang Huang", "Ran Hao"], "title": "State-Aware IoT Scheduling Using Deep Q-Networks and Edge-Based Coordination", "categories": ["cs.NI", "cs.LG"], "comment": null, "summary": "This paper addresses the challenge of energy efficiency management faced by\nintelligent IoT devices in complex application environments. A novel\noptimization method is proposed, combining Deep Q-Network (DQN) with an edge\ncollaboration mechanism. The method builds a state-action-reward interaction\nmodel and introduces edge nodes as intermediaries for state aggregation and\npolicy scheduling. This enables dynamic resource coordination and task\nallocation among multiple devices. During the modeling process, device status,\ntask load, and network resources are jointly incorporated into the state space.\nThe DQN is used to approximate and learn the optimal scheduling strategy. To\nenhance the model's ability to perceive inter-device relationships, a\ncollaborative graph structure is introduced to model the multi-device\nenvironment and assist in decision optimization. Experiments are conducted\nusing real-world IoT data collected from the FastBee platform. Several\ncomparative and validation tests are performed, including energy efficiency\ncomparisons across different scheduling strategies, robustness analysis under\nvarying task loads, and evaluation of state dimension impacts on policy\nconvergence speed. The results show that the proposed method outperforms\nexisting baseline approaches in terms of average energy consumption, processing\nlatency, and resource utilization. This confirms its effectiveness and\npracticality in intelligent IoT scenarios."}
{"id": "2504.16062", "pdf": "https://arxiv.org/pdf/2504.16062", "abs": "https://arxiv.org/abs/2504.16062", "authors": ["Hardik Shah", "Jiaxu Xing", "Nico Messikommer", "Boyang Sun", "Marc Pollefeys", "Davide Scaramuzza"], "title": "ForesightNav: Learning Scene Imagination for Efficient Exploration", "categories": ["cs.RO", "cs.CV"], "comment": null, "summary": "Understanding how humans leverage prior knowledge to navigate unseen\nenvironments while making exploratory decisions is essential for developing\nautonomous robots with similar abilities. In this work, we propose\nForesightNav, a novel exploration strategy inspired by human imagination and\nreasoning. Our approach equips robotic agents with the capability to predict\ncontextual information, such as occupancy and semantic details, for unexplored\nregions. These predictions enable the robot to efficiently select meaningful\nlong-term navigation goals, significantly enhancing exploration in unseen\nenvironments. We validate our imagination-based approach using the Structured3D\ndataset, demonstrating accurate occupancy prediction and superior performance\nin anticipating unseen scene geometry. Our experiments show that the\nimagination module improves exploration efficiency in unseen environments,\nachieving a 100% completion rate for PointNav and an SPL of 67% for ObjectNav\non the Structured3D Validation split. These contributions demonstrate the power\nof imagination-driven reasoning for autonomous systems to enhance generalizable\nand efficient exploration."}
{"id": "2410.14669", "pdf": "https://arxiv.org/pdf/2410.14669", "abs": "https://arxiv.org/abs/2410.14669", "authors": ["Baiqi Li", "Zhiqiu Lin", "Wenxuan Peng", "Jean de Dieu Nyandwi", "Daniel Jiang", "Zixian Ma", "Simran Khanuja", "Ranjay Krishna", "Graham Neubig", "Deva Ramanan"], "title": "NaturalBench: Evaluating Vision-Language Models on Natural Adversarial Samples", "categories": ["cs.CV", "cs.CL"], "comment": "Accepted to NeurIPS 24; We open-source our dataset at:\n  https://huggingface.co/datasets/BaiqiL/NaturalBench ; Project page at:\n  https://linzhiqiu.github.io/papers/naturalbench/", "summary": "Vision-language models (VLMs) have made significant progress in recent\nvisual-question-answering (VQA) benchmarks that evaluate complex\nvisio-linguistic reasoning. However, are these models truly effective? In this\nwork, we show that VLMs still struggle with natural images and questions that\nhumans can easily answer, which we term natural adversarial samples. We also\nfind it surprisingly easy to generate these VQA samples from natural image-text\ncorpora using off-the-shelf models like CLIP and ChatGPT. We propose a\nsemi-automated approach to collect a new benchmark, NaturalBench, for reliably\nevaluating VLMs with 10,000 human-verified VQA samples. Crucially, we adopt a\n$\\textbf{vision-centric}$ design by pairing each question with two images that\nyield different answers, preventing blind solutions from answering without\nusing the images. This makes NaturalBench more challenging than previous\nbenchmarks that can be solved with commonsense priors. We evaluate 53\nstate-of-the-art VLMs on NaturalBench, showing that models like\nLLaVA-OneVision, Cambrian-1, Llama3.2-Vision, Molmo, Qwen2-VL, and even GPT-4o\nlag 50%-70% behind human performance (over 90%). We analyze why NaturalBench is\nhard from two angles: (1) Compositionality: Solving NaturalBench requires\ndiverse visio-linguistic skills, including understanding attribute bindings,\nobject relationships, and advanced reasoning like logic and counting. To this\nend, unlike prior work that uses a single tag per sample, we tag each\nNaturalBench sample with 1 to 8 skill tags for fine-grained evaluation. (2)\nBiases: NaturalBench exposes severe biases in VLMs, as models often choose the\nsame answer regardless of the image. Lastly, we apply our benchmark curation\nmethod to diverse data sources, including long captions (over 100 words) and\nnon-English languages like Chinese and Hindi, highlighting its potential for\ndynamic evaluations of VLMs."}
{"id": "2504.14350", "pdf": "https://arxiv.org/pdf/2504.14350", "abs": "https://arxiv.org/abs/2504.14350", "authors": ["Yi Sun", "Han Wang", "Jiaqiang Li", "Jiacheng Liu", "Xiangyu Li", "Hao Wen", "Huiwen Zheng", "Yan Liang", "Yuanchun Li", "Yunxin Liu"], "title": "Time's Up! An Empirical Study of LLM Reasoning Ability Under Output Length Constraint", "categories": ["cs.AI"], "comment": null, "summary": "Recent work has demonstrated the remarkable potential of Large Language\nModels (LLMs) in test-time scaling. By making the models think before\nanswering, they are able to achieve much higher accuracy with extra inference\ncomputation. However, in many real-world scenarios, models are used under time\nconstraints, where an answer should be given to the user within a certain\noutput length. It is unclear whether and how the reasoning abilities of LLMs\nremain effective under such constraints. We take a first look at this problem\nby conducting an in-depth empirical study. Specifically, we test more than 25\nLLMs on common reasoning datasets under a wide range of output length budgets,\nand we analyze the correlation between the inference accuracy and various\nproperties including model type, model size, prompt style, etc. We also\nconsider the mappings between the token budgets and the actual on-device\nlatency budgets. The results have demonstrated several interesting findings\nregarding the budget-aware LLM reasoning that differ from the unconstrained\nsituation, e.g. the optimal choices of model sizes and prompts change under\ndifferent budgets. These findings offer practical guidance for users to deploy\nLLMs under real-world latency constraints."}
{"id": "2504.15578", "pdf": "https://arxiv.org/pdf/2504.15578", "abs": "https://arxiv.org/abs/2504.15578", "authors": ["Ian Mikesell", "Samuel Filgueira da Silva", "Mehmet Fatih Ozkan", "Faissal El Idrissi", "Prashanth Ramesh", "Marcello Canova"], "title": "Real-Time Optimal Design of Experiment for Parameter Identification of Li-Ion Cell Electrochemical Model", "categories": ["eess.SY", "cs.LG", "cs.SY"], "comment": null, "summary": "Accurately identifying the parameters of electrochemical models of li-ion\nbattery (LiB) cells is a critical task for enhancing the fidelity and\npredictive ability. Traditional parameter identification methods often require\nextensive data collection experiments and lack adaptability in dynamic\nenvironments. This paper describes a Reinforcement Learning (RL) based approach\nthat dynamically tailors the current profile applied to a LiB cell to optimize\nthe parameters identifiability of the electrochemical model. The proposed\nframework is implemented in real-time using a Hardware-in-the-Loop (HIL) setup,\nwhich serves as a reliable testbed for evaluating the RL-based design strategy.\nThe HIL validation confirms that the RL-based experimental design outperforms\nconventional test protocols used for parameter identification in terms of both\nreducing the modeling errors on a verification test and minimizing the duration\nof the experiment used for parameter identification."}
{"id": "2304.07647", "pdf": "https://arxiv.org/pdf/2304.07647", "abs": "https://arxiv.org/abs/2304.07647", "authors": ["Jiani Huang", "Ziyang Li", "Mayur Naik", "Ser-Nam Lim"], "title": "LASER: A Neuro-Symbolic Framework for Learning Spatial-Temporal Scene Graphs with Weak Supervision", "categories": ["cs.CV", "cs.LG", "cs.LO"], "comment": null, "summary": "Supervised approaches for learning spatio-temporal scene graphs (STSG) from\nvideo are greatly hindered due to their reliance on STSG-annotated videos,\nwhich are labor-intensive to construct at scale. Is it feasible to instead use\nreadily available video captions as weak supervision? To address this question,\nwe propose LASER, a neuro-symbolic framework to enable training STSG generators\nusing only video captions. LASER employs large language models to first extract\nlogical specifications with rich spatio-temporal semantic information from\nvideo captions. LASER then trains the underlying STSG generator to align the\npredicted STSG with the specification. The alignment algorithm overcomes the\nchallenges of weak supervision by leveraging a differentiable symbolic reasoner\nand using a combination of contrastive, temporal, and semantics losses. The\noverall approach efficiently trains low-level perception models to extract a\nfine-grained STSG that conforms to the video caption. In doing so, it enables a\nnovel methodology for learning STSGs without tedious annotations. We evaluate\nour method on three video datasets: OpenPVSG, 20BN, and MUGEN. Our approach\ndemonstrates substantial improvements over fully-supervised baselines,\nachieving a unary predicate prediction accuracy of 27.78% (+12.65%) and a\nbinary recall@5 of 0.42 (+0.22) on OpenPVSG. Additionally, LASER exceeds\nbaselines by 7% on 20BN and 5.2% on MUGEN in terms of overall predicate\nprediction accuracy."}
{"id": "2502.19676", "pdf": "https://arxiv.org/pdf/2502.19676", "abs": "https://arxiv.org/abs/2502.19676", "authors": ["Zhangdie Yuan", "Zifeng Ding", "Andreas Vlachos"], "title": "FOReCAst: The Future Outcome Reasoning and Confidence Assessment Benchmark", "categories": ["cs.LG", "cs.CL"], "comment": null, "summary": "Forecasting is an important task in many domains, such as technology and\neconomics. However existing forecasting benchmarks largely lack comprehensive\nconfidence assessment, focus on limited question types, and often consist of\nartificial questions that do not align with real-world human forecasting needs.\nTo address these gaps, we introduce FOReCAst (Future Outcome Reasoning and\nConfidence Assessment), a benchmark that evaluates models' ability to make\npredictions and their confidence in them. FOReCAst spans diverse forecasting\nscenarios involving Boolean questions, timeframe prediction, and quantity\nestimation, enabling a comprehensive evaluation of both prediction accuracy and\nconfidence calibration for real-world applications."}
{"id": "2504.14706", "pdf": "https://arxiv.org/pdf/2504.14706", "abs": "https://arxiv.org/abs/2504.14706", "authors": ["Shin-nosuke Ishikawa", "Atsushi Yoshino"], "title": "AI with Emotions: Exploring Emotional Expressions in Large Language Models", "categories": ["cs.AI"], "comment": "14 pages, 8 figures, accepted to the Natural Language Processing for\n  Digital Humanities (NLP4DH) workshop at NAACL 2025", "summary": "The human-level performance of Large Language Models (LLMs) across various\ntasks has raised expectations for the potential of Artificial Intelligence (AI)\nto possess emotions someday. To explore the capability of current LLMs to\nexpress emotions in their outputs, we conducted an experiment using several\nLLMs (OpenAI GPT, Google Gemini, Meta Llama3, and Cohere Command R+) to\nrole-play as agents answering questions with specified emotional states. We\ndefined the emotional states using Russell's Circumplex model, a\nwell-established framework that characterizes emotions along the\nsleepy-activated (arousal) and pleasure-displeasure (valence) axes. We chose\nthis model for its simplicity, utilizing two continuous parameters, which\nallows for better controllability in applications involving continuous changes\nin emotional states. The responses generated were evaluated using a sentiment\nanalysis model, independent of the LLMs, trained on the GoEmotions dataset. The\nevaluation showed that the emotional states of the generated answers were\nconsistent with the specifications, demonstrating the LLMs' capability for\nemotional expression. This indicates the potential for LLM-based AI agents to\nsimulate emotions, opening up a wide range of applications for emotion-based\ninteractions, such as advisors or consultants who can provide advice or\nopinions with a personal touch."}
{"id": "2504.15580", "pdf": "https://arxiv.org/pdf/2504.15580", "abs": "https://arxiv.org/abs/2504.15580", "authors": ["Chengyuan Deng", "Jie Gao", "Jalaj Upadhyay", "Chen Wang", "Samson Zhou"], "title": "On the Price of Differential Privacy for Hierarchical Clustering", "categories": ["cs.DS", "cs.CR", "cs.LG"], "comment": "ICLR 2025", "summary": "Hierarchical clustering is a fundamental unsupervised machine learning task\nwith the aim of organizing data into a hierarchy of clusters. Many applications\nof hierarchical clustering involve sensitive user information, therefore\nmotivating recent studies on differentially private hierarchical clustering\nunder the rigorous framework of Dasgupta's objective. However, it has been\nshown that any privacy-preserving algorithm under edge-level differential\nprivacy necessarily suffers a large error. To capture practical applications of\nthis problem, we focus on the weight privacy model, where each edge of the\ninput graph is at least unit weight. We present a novel algorithm in the weight\nprivacy model that shows significantly better approximation than known\nimpossibility results in the edge-level DP setting. In particular, our\nalgorithm achieves $O(\\log^{1.5}n/\\varepsilon)$ multiplicative error for\n$\\varepsilon$-DP and runs in polynomial time, where $n$ is the size of the\ninput graph, and the cost is never worse than the optimal additive error in\nexisting work. We complement our algorithm by showing if the unit-weight\nconstraint does not apply, the lower bound for weight-level DP hierarchical\nclustering is essentially the same as the edge-level DP, i.e.\n$\\Omega(n^2/\\varepsilon)$ additive error. As a result, we also obtain a new\nlower bound of $\\tilde{\\Omega}(1/\\varepsilon)$ additive error for balanced\nsparsest cuts in the weight-level DP model, which may be of independent\ninterest. Finally, we evaluate our algorithm on synthetic and real-world\ndatasets. Our experimental results show that our algorithm performs well in\nterms of extra cost and has good scalability to large graphs."}
{"id": "2403.08728", "pdf": "https://arxiv.org/pdf/2403.08728", "abs": "https://arxiv.org/abs/2403.08728", "authors": ["Asad Aali", "Giannis Daras", "Brett Levac", "Sidharth Kumar", "Alexandros G. Dimakis", "Jonathan I. Tamir"], "title": "Ambient Diffusion Posterior Sampling: Solving Inverse Problems with Diffusion Models Trained on Corrupted Data", "categories": ["cs.CV", "cs.AI", "cs.LG"], "comment": null, "summary": "We provide a framework for solving inverse problems with diffusion models\nlearned from linearly corrupted data. Firstly, we extend the Ambient Diffusion\nframework to enable training directly from measurements corrupted in the\nFourier domain. Subsequently, we train diffusion models for MRI with access\nonly to Fourier subsampled multi-coil measurements at acceleration factors R=\n2,4,6,8. Secondly, we propose Ambient Diffusion Posterior Sampling (A-DPS), a\nreconstruction algorithm that leverages generative models pre-trained on one\ntype of corruption (e.g. image inpainting) to perform posterior sampling on\nmeasurements from a different forward process (e.g. image blurring). For MRI\nreconstruction in high acceleration regimes, we observe that A-DPS models\ntrained on subsampled data are better suited to solving inverse problems than\nmodels trained on fully sampled data. We also test the efficacy of A-DPS on\nnatural image datasets (CelebA, FFHQ, and AFHQ) and show that A-DPS can\nsometimes outperform models trained on clean data for several image restoration\ntasks in both speed and performance."}
{"id": "2504.09723", "pdf": "https://arxiv.org/pdf/2504.09723", "abs": "https://arxiv.org/abs/2504.09723", "authors": ["Dakuo Wang", "Ting-Yao Hsu", "Yuxuan Lu", "Hansu Gu", "Limeng Cui", "Yaochen Xie", "William Headean", "Bingsheng Yao", "Akash Veeragouni", "Jiapeng Liu", "Sreyashi Nag", "Jessie Wang"], "title": "AgentA/B: Automated and Scalable Web A/BTesting with Interactive LLM Agents", "categories": ["cs.HC", "cs.CL"], "comment": null, "summary": "A/B testing experiment is a widely adopted method for evaluating UI/UX design\ndecisions in modern web applications. Yet, traditional A/B testing remains\nconstrained by its dependence on the large-scale and live traffic of human\nparticipants, and the long time of waiting for the testing result. Through\nformative interviews with six experienced industry practitioners, we identified\ncritical bottlenecks in current A/B testing workflows. In response, we present\nAgentA/B, a novel system that leverages Large Language Model-based autonomous\nagents (LLM Agents) to automatically simulate user interaction behaviors with\nreal webpages. AgentA/B enables scalable deployment of LLM agents with diverse\npersonas, each capable of navigating the dynamic webpage and interactively\nexecuting multi-step interactions like search, clicking, filtering, and\npurchasing. In a demonstrative controlled experiment, we employ AgentA/B to\nsimulate a between-subject A/B testing with 1,000 LLM agents Amazon.com, and\ncompare agent behaviors with real human shopping behaviors at a scale. Our\nfindings suggest AgentA/B can emulate human-like behavior patterns."}
{"id": "2504.15046", "pdf": "https://arxiv.org/pdf/2504.15046", "abs": "https://arxiv.org/abs/2504.15046", "authors": ["Shilin Zhang", "Zican Hu", "Wenhao Wu", "Xinyi Xie", "Jianxiang Tang", "Chunlin Chen", "Daoyi Dong", "Yu Cheng", "Zhenhong Sun", "Zhi Wang"], "title": "Text-to-Decision Agent: Learning Generalist Policies from Natural Language Supervision", "categories": ["cs.AI"], "comment": "18 pages, 8 figures", "summary": "RL systems usually tackle generalization by inferring task beliefs from\nhigh-quality samples or warmup explorations. The restricted form limits their\ngenerality and usability since these supervision signals are expensive and even\ninfeasible to acquire in advance for unseen tasks. Learning directly from the\nraw text about decision tasks is a promising alternative to leverage a much\nbroader source of supervision. In the paper, we propose Text-to-Decision Agent\n(T2DA), a simple and scalable framework that supervises generalist policy\nlearning with natural language. We first introduce a generalized world model to\nencode multi-task decision data into a dynamics-aware embedding space. Then,\ninspired by CLIP, we predict which textual description goes with which decision\nembedding, effectively bridging their semantic gap via contrastive\nlanguage-decision pre-training and aligning the text embeddings to comprehend\nthe environment dynamics. After training the text-conditioned generalist\npolicy, the agent can directly realize zero-shot text-to-decision generation in\nresponse to language instructions. Comprehensive experiments on MuJoCo and\nMeta-World benchmarks show that T2DA facilitates high-capacity zero-shot\ngeneralization and outperforms various types of baselines."}
{"id": "2504.15632", "pdf": "https://arxiv.org/pdf/2504.15632", "abs": "https://arxiv.org/abs/2504.15632", "authors": ["Seyed Shayan Daneshvar", "Da Tan", "Shaowei Wang", "Carson Leung"], "title": "A Study On Mixup-inspired Augmentation Methods For Software Vulnerability Detection", "categories": ["cs.SE", "cs.CR", "cs.LG"], "comment": "Accepted at EASE 2025, Istanbul, Turkey", "summary": "Various Deep Learning (DL) methods have recently been utilized to detect\nsoftware vulnerabilities. Real-world software vulnerability datasets are rare\nand hard to acquire as there's no simple metric for classifying vulnerability.\nSuch datasets are heavily imbalanced, and none of the current datasets are\nconsidered huge for DL models. To tackle these problems a recent work has tried\nto augment the dataset using the source code and generate realistic\nsingle-statement vulnerabilities which is not quite practical and requires\nmanual checking of the generated vulnerabilities. In this regard, we aim to\nexplore the augmentation of vulnerabilities at the representation level to help\ncurrent models learn better which has never been done before to the best of our\nknowledge. We implement and evaluate the 5 augmentation techniques that augment\nthe embedding of the data and recently have been used for code search which is\na completely different software engineering task. We also introduced a\nconditioned version of those augmentation methods, which ensures the\naugmentation does not change the vulnerable section of the vector\nrepresentation. We show that such augmentation methods can be helpful and\nincrease the f1-score by up to 9.67%, yet they cannot beat Random Oversampling\nwhen balancing datasets which increases the f1-score by 10.82%!"}
{"id": "2404.12803", "pdf": "https://arxiv.org/pdf/2404.12803", "abs": "https://arxiv.org/abs/2404.12803", "authors": ["Jingqun Tang", "Chunhui Lin", "Zhen Zhao", "Shu Wei", "Binghong Wu", "Qi Liu", "Hao Feng", "Yang Li", "Siqi Wang", "Lei Liao", "Wei Shi", "Yuliang Liu", "Hao Liu", "Yuan Xie", "Xiang Bai", "Can Huang"], "title": "TextSquare: Scaling up Text-Centric Visual Instruction Tuning", "categories": ["cs.CV", "cs.LG"], "comment": null, "summary": "Text-centric visual question answering (VQA) has made great strides with the\ndevelopment of Multimodal Large Language Models (MLLMs), yet open-source models\nstill fall short of leading models like GPT4V and Gemini, partly due to a lack\nof extensive, high-quality instruction tuning data. To this end, we introduce a\nnew approach for creating a massive, high-quality instruction-tuning dataset,\nSquare-10M, which is generated using closed-source MLLMs. The data construction\nprocess, termed Square, consists of four steps: Self-Questioning, Answering,\nReasoning, and Evaluation. Our experiments with Square-10M led to three key\nfindings: 1) Our model, TextSquare, considerably surpasses open-source previous\nstate-of-the-art Text-centric MLLMs and sets a new standard on OCRBench(62.2%).\nIt even outperforms top-tier models like GPT4V and Gemini in 6 of 10\ntext-centric benchmarks. 2) Additionally, we demonstrate the critical role of\nVQA reasoning data in offering comprehensive contextual insights for specific\nquestions. This not only improves accuracy but also significantly mitigates\nhallucinations. Specifically, TextSquare scores an average of 75.1% across four\ngeneral VQA and hallucination evaluation datasets, outperforming previous\nstate-of-the-art models. 3) Notably, the phenomenon observed in scaling\ntext-centric VQA datasets reveals a vivid pattern: the exponential increase of\ninstruction tuning data volume is directly proportional to the improvement in\nmodel performance, thereby validating the necessity of the dataset scale and\nthe high quality of Square-10M."}
{"id": "2504.14945", "pdf": "https://arxiv.org/pdf/2504.14945", "abs": "https://arxiv.org/abs/2504.14945", "authors": ["Jianhao Yan", "Yafu Li", "Zican Hu", "Zhi Wang", "Ganqu Cui", "Xiaoye Qu", "Yu Cheng", "Yue Zhang"], "title": "Learning to Reason under Off-Policy Guidance", "categories": ["cs.LG", "cs.AI", "cs.CL"], "comment": "Work in progress", "summary": "Recent advances in large reasoning models (LRMs) demonstrate that\nsophisticated behaviors such as multi-step reasoning and self-reflection can\nemerge via reinforcement learning (RL) with simple rule-based rewards. However,\nexisting zero-RL approaches are inherently ``on-policy'', limiting learning to\na model's own outputs and failing to acquire reasoning abilities beyond its\ninitial capabilities. We introduce LUFFY (Learning to reason Under oFF-policY\nguidance), a framework that augments zero-RL with off-policy reasoning traces.\nLUFFY dynamically balances imitation and exploration by combining off-policy\ndemonstrations with on-policy rollouts during training. Notably, we propose\npolicy shaping via regularized importance sampling to avoid superficial and\nrigid imitation during mixed-policy training. Remarkably, LUFFY achieves an\nover +7.0 average gain across six math benchmarks and an advantage of over +6.2\npoints in out-of-distribution tasks. It also substantially surpasses\nimitation-based supervised fine-tuning (SFT), particularly in generalization.\nAnalysis shows LUFFY not only imitates effectively but also explores beyond\ndemonstrations, offering a scalable path to train generalizable reasoning\nmodels with off-policy guidance."}
{"id": "2504.15188", "pdf": "https://arxiv.org/pdf/2504.15188", "abs": "https://arxiv.org/abs/2504.15188", "authors": ["Yizhu Jiao", "Xuchao Zhang", "Zhaoyang Wang", "Yubo Ma", "Zhun Deng", "Rujia Wang", "Chetan Bansal", "Saravan Rajmohan", "Jiawei Han", "Huaxiu Yao"], "title": "Synergistic Weak-Strong Collaboration by Aligning Preferences", "categories": ["cs.AI"], "comment": null, "summary": "Current Large Language Models (LLMs) excel in general reasoning yet struggle\nwith specialized tasks requiring proprietary or domain-specific knowledge.\nFine-tuning large models for every niche application is often infeasible due to\nblack-box constraints and high computational overhead. To address this, we\npropose a collaborative framework that pairs a specialized weak model with a\ngeneral strong model. The weak model, tailored to specific domains, produces\ninitial drafts and background information, while the strong model leverages its\nadvanced reasoning to refine these drafts, extending LLMs' capabilities to\ncritical yet specialized tasks. To optimize this collaboration, we introduce a\ncollaborative feedback to fine-tunes the weak model, which quantifies the\ninfluence of the weak model's contributions in the collaboration procedure and\nestablishes preference pairs to guide preference tuning of the weak model. We\nvalidate our framework through experiments on three domains. We find that the\ncollaboration significantly outperforms each model alone by leveraging\ncomplementary strengths. Moreover, aligning the weak model with the\ncollaborative preference further enhances overall performance."}
{"id": "2504.15657", "pdf": "https://arxiv.org/pdf/2504.15657", "abs": "https://arxiv.org/abs/2504.15657", "authors": ["Yibo Liu", "Paul Kry", "Kenny Erleben", "Noam Aigerman", "Sune Darkner", "Teseo Schneider"], "title": "Neural Kinematic Bases for Fluids", "categories": ["cs.GR", "cs.LG", "physics.flu-dyn"], "comment": null, "summary": "We propose mesh-free fluid simulations that exploit a kinematic neural basis\nfor velocity fields represented by an MLP. We design a set of losses that\nensures that these neural bases satisfy fundamental physical properties such as\northogonality, divergence-free, boundary alignment, and smoothness. Our neural\nbases can then be used to fit an input sketch of a flow, which will inherit the\nsame fundamental properties from the bases. We then can animate such flow in\nreal-time using standard time integrators. Our neural bases can accommodate\ndifferent domains and naturally extend to three dimensions."}
{"id": "2406.04861", "pdf": "https://arxiv.org/pdf/2406.04861", "abs": "https://arxiv.org/abs/2406.04861", "authors": ["Aarya Patel", "Hamid Laga", "Ojaswa Sharma"], "title": "Normal-guided Detail-Preserving Neural Implicit Function for High-Fidelity 3D Surface Reconstruction", "categories": ["cs.CV", "cs.GR", "I.3.5"], "comment": "Accepted at ACM SIGGRAPH I3D 2025. Published in PACMCGIT journal.\n  Project page with images and code:\n  https://graphics-research-group.github.io/sn-nir", "summary": "Neural implicit representations have emerged as a powerful paradigm for 3D\nreconstruction. However, despite their success, existing methods fail to\ncapture fine geometric details and thin structures, especially in scenarios\nwhere only sparse multi-view RGB images of the objects of interest are\navailable. This paper shows that training neural representations with\nfirst-order differential properties (surface normals) leads to highly accurate\n3D surface reconstruction, even with as few as two RGB images. Using input RGB\nimages, we compute approximate ground-truth surface normals from depth maps\nproduced by an off-the-shelf monocular depth estimator. During training, we\ndirectly locate the surface point of the SDF network and supervise its normal\nwith the one estimated from the depth map. Extensive experiments demonstrate\nthat our method achieves state-of-the-art reconstruction accuracy with a\nminimal number of views, capturing intricate geometric details and thin\nstructures that were previously challenging to capture."}
{"id": "2311.17059", "pdf": "https://arxiv.org/pdf/2311.17059", "abs": "https://arxiv.org/abs/2311.17059", "authors": ["Jun Wang", "Hosein Hasanbeig", "Kaiyuan Tan", "Zihe Sun", "Yiannis Kantaros"], "title": "Mission-driven Exploration for Accelerated Deep Reinforcement Learning with Temporal Logic Task Specifications", "categories": ["cs.RO", "cs.AI", "cs.LG"], "comment": null, "summary": "This paper addresses the problem of designing control policies for agents\nwith unknown stochastic dynamics and control objectives specified using Linear\nTemporal Logic (LTL). Recent Deep Reinforcement Learning (DRL) algorithms have\naimed to compute policies that maximize the satisfaction probability of LTL\nformulas, but they often suffer from slow learning performance. To address\nthis, we introduce a novel Deep Q-learning algorithm that significantly\nimproves learning speed. The enhanced sample efficiency stems from a\nmission-driven exploration strategy that prioritizes exploration towards\ndirections likely to contribute to mission success. Identifying these\ndirections relies on an automaton representation of the LTL task as well as a\nlearned neural network that partially models the agent-environment interaction.\nWe provide comparative experiments demonstrating the efficiency of our\nalgorithm on robot navigation tasks in unseen environments."}
{"id": "2504.15674", "pdf": "https://arxiv.org/pdf/2504.15674", "abs": "https://arxiv.org/abs/2504.15674", "authors": ["Yanbo Dai", "Songze Li", "Zihan Gan", "Xueluan Gong"], "title": "TrojanDam: Detection-Free Backdoor Defense in Federated Learning through Proactive Model Robustification utilizing OOD Data", "categories": ["cs.CR", "cs.LG"], "comment": null, "summary": "Federated learning (FL) systems allow decentralized data-owning clients to\njointly train a global model through uploading their locally trained updates to\na centralized server. The property of decentralization enables adversaries to\ncraft carefully designed backdoor updates to make the global model misclassify\nonly when encountering adversary-chosen triggers. Existing defense mechanisms\nmainly rely on post-training detection after receiving updates. These methods\neither fail to identify updates which are deliberately fabricated statistically\nclose to benign ones, or show inconsistent performance in different FL training\nstages. The effect of unfiltered backdoor updates will accumulate in the global\nmodel, and eventually become functional. Given the difficulty of ruling out\nevery backdoor update, we propose a backdoor defense paradigm, which focuses on\nproactive robustification on the global model against potential backdoor\nattacks. We first reveal that the successful launching of backdoor attacks in\nFL stems from the lack of conflict between malicious and benign updates on\nredundant neurons of ML models. We proceed to prove the feasibility of\nactivating redundant neurons utilizing out-of-distribution (OOD) samples in\ncentralized settings, and migrating to FL settings to propose a novel backdoor\ndefense mechanism, TrojanDam. The proposed mechanism has the FL server\ncontinuously inject fresh OOD mappings into the global model to activate\nredundant neurons, canceling the effect of backdoor updates during aggregation.\nWe conduct systematic and extensive experiments to illustrate the superior\nperformance of TrojanDam, over several SOTA backdoor defense methods across a\nwide range of FL settings."}
{"id": "2406.12407", "pdf": "https://arxiv.org/pdf/2406.12407", "abs": "https://arxiv.org/abs/2406.12407", "authors": ["Pit Henrich", "Franziska Mathis-Ullrich"], "title": "LOOC: Localizing Organs using Occupancy Networks and Body Surface Depth Images", "categories": ["cs.CV"], "comment": "Published in IEEE Access", "summary": "We introduce a novel approach for the precise localization of 67 anatomical\nstructures from single depth images captured from the exterior of the human\nbody. Our method uses a multi-class occupancy network, trained using segmented\nCT scans augmented with body-pose changes, and incorporates a specialized\nsampling strategy to handle densely packed internal organs. Our contributions\ninclude the application of occupancy networks for occluded structure\nlocalization, a robust method for estimating anatomical positions from depth\nimages, and the creation of detailed, individualized 3D anatomical atlases. We\noutperform localization using template matching and provide qualitative\nreal-world reconstructions. This method promises improvements in automated\nmedical imaging and diagnostic procedures by offering accurate, non-invasive\nlocalization of critical anatomical structures."}
{"id": "2404.00247", "pdf": "https://arxiv.org/pdf/2404.00247", "abs": "https://arxiv.org/abs/2404.00247", "authors": ["Runze Lin", "Junghui Chen", "Lei Xie", "Hongye Su"], "title": "Facilitating Reinforcement Learning for Process Control Using Transfer Learning: Overview and Perspectives", "categories": ["eess.SY", "cs.AI", "cs.LG", "cs.SY"], "comment": "Chinese Control and Decision Conference (CCDC 2025), Oral, Regular\n  Paper & Asian Control Conference (ASCC 2024), Oral, Position Paper", "summary": "In the context of Industry 4.0 and smart manufacturing, the field of process\nindustry optimization and control is also undergoing a digital transformation.\nWith the rise of Deep Reinforcement Learning (DRL), its application in process\ncontrol has attracted widespread attention. However, the extremely low sample\nefficiency and the safety concerns caused by exploration in DRL hinder its\npractical implementation in industrial settings. Transfer learning offers an\neffective solution for DRL, enhancing its generalization and adaptability in\nmulti-mode control scenarios. This paper provides insights into the use of DRL\nfor process control from the perspective of transfer learning. We analyze the\nchallenges of applying DRL in the process industry and the necessity of\nintroducing transfer learning. Furthermore, recommendations and prospects are\nprovided for future research directions on how transfer learning can be\nintegrated with DRL to enhance process control. This paper aims to offer a set\nof promising, user-friendly, easy-to-implement, and scalable approaches to\nartificial intelligence-facilitated industrial control for scholars and\nengineers in the process industry."}
{"id": "2504.15679", "pdf": "https://arxiv.org/pdf/2504.15679", "abs": "https://arxiv.org/abs/2504.15679", "authors": ["Brandon Panos", "Ivan Milic"], "title": "Policy-Based Radiative Transfer: Solving the $2$-Level Atom Non-LTE Problem using Soft Actor-Critic Reinforcement Learning", "categories": ["astro-ph.SR", "cs.LG"], "comment": null, "summary": "We present a novel reinforcement learning (RL) approach for solving the\nclassical 2-level atom non-LTE radiative transfer problem by framing it as a\ncontrol task in which an RL agent learns a depth-dependent source function\n$S(\\tau)$ that self-consistently satisfies the equation of statistical\nequilibrium (SE). The agent's policy is optimized entirely via reward-based\ninteractions with a radiative transfer engine, without explicit knowledge of\nthe ground truth. This method bypasses the need for constructing approximate\nlambda operators ($\\Lambda^*$) common in accelerated iterative schemes.\nAdditionally, it requires no extensive precomputed labeled datasets to extract\na supervisory signal, and avoids backpropagating gradients through the complex\nRT solver itself. Finally, we show through experiment that a simple feedforward\nneural network trained greedily cannot solve for SE, possibly due to the moving\ntarget nature of the problem. Our $\\Lambda^*-\\text{Free}$ method offers\npotential advantages for complex scenarios (e.g., atmospheres with enhanced\nvelocity fields, multi-dimensional geometries, or complex microphysics) where\n$\\Lambda^*$ construction or solver differentiability is challenging.\nAdditionally, the agent can be incentivized to find more efficient policies by\nmanipulating the discount factor, leading to a reprioritization of immediate\nrewards. If demonstrated to generalize past its training data, this RL\nframework could serve as an alternative or accelerated formalism to achieve SE.\nTo the best of our knowledge, this study represents the first application of\nreinforcement learning in solar physics that directly solves for a fundamental\nphysical constraint."}
{"id": "2407.20090", "pdf": "https://arxiv.org/pdf/2407.20090", "abs": "https://arxiv.org/abs/2407.20090", "authors": ["Jinmiao Zhao", "Zelin Shi", "Chuang Yu", "Yunpeng Liu", "Yimian Dai"], "title": "Towards Robust Infrared Small Target Detection: A Feature-Enhanced and Sensitivity-Tunable Framework", "categories": ["cs.CV"], "comment": null, "summary": "Recently, single-frame infrared small target (SIRST) detection technology has\nattracted wide-spread attention. However, due to the intrinsic feature scarcity\nin infrared small targets, precise segmentation of small targets from complex\nbackgrounds remains a significant challenge. Different from most existing deep\nlearning-based methods that focus on improving network architectures, we\npropose a feature-enhanced and sensitivity-tunable (FEST) framework, which is\ncompatible with existing SIRST detection networks and further enhances their\ndetection performance. The FEST framework improves the model's robustness from\ntwo aspects: feature enhancement and target confidence regulation. For feature\nenhancement, on the one hand, we adopt a multi-scale fusion strategy, which can\neffectively improve the model's perception and adaptability to multi-scale\nfeatures of multi-size targets. On the other hand, we construct an edge\nenhancement difficulty mining (EEDM) loss based on the analysis of the task\ncharacteristics, which helps guide the network to continuously focus on\nchallenging target regions and edge features during training. For target\nconfidence regulation, we design an adjustable sensitivity (AS) strategy for\nnetwork post-processing. This strategy not only enhances the adaptability of\nthe network in complex scenarios, but also significantly improves the detection\nrate of infrared small targets while maintaining segmentation accuracy.\nExtensive experimental results show that our FEST framework can significantly\nenhance the performance of existing SIRST detection networks. Notably, the\nmulti-scale direction-aware network (MSDA-Net) equipped with the FEST framework\nwon the first prize in the PRCV 2024 wide-area infrared small target detection\ncompetition."}
{"id": "2405.18471", "pdf": "https://arxiv.org/pdf/2405.18471", "abs": "https://arxiv.org/abs/2405.18471", "authors": ["Shehu AbdusSalam", "Steve Abel", "Miguel Crispim Romao"], "title": "Symbolic Regression for Beyond the Standard Model Physics", "categories": ["hep-ph", "cs.AI", "cs.LG", "hep-th", "physics.comp-ph"], "comment": "Version accepted for publication in PRD. 8 pages, 10 figures. For\n  associated code and symbolic expressions see\n  https://gitlab.com/miguel.romao/symbolic-regression-bsm", "summary": "We propose symbolic regression as a powerful tool for studying Beyond the\nStandard Model physics. As a benchmark model, we consider the so-called\nConstrained Minimal Supersymmetric Standard Model, which has a four-dimensional\nparameter space defined at the GUT scale. We provide a set of analytical\nexpressions that reproduce three low-energy observables of interest in terms of\nthe parameters of the theory: the Higgs mass, the contribution to the anomalous\nmagnetic moment of the muon, and the cold dark matter relic density. To\ndemonstrate the power of the approach, we employ the symbolic expressions in a\nglobal fits analysis to derive the posterior probability densities of the\nparameters, which are obtained extremely rapidly in comparison with\nconventional methods."}
{"id": "2504.15691", "pdf": "https://arxiv.org/pdf/2504.15691", "abs": "https://arxiv.org/abs/2504.15691", "authors": ["Mingliang Ma Abolfazl Safikhani"], "title": "Transfer Learning for High-dimensional Reduced Rank Time Series Models", "categories": ["stat.ML", "cs.LG"], "comment": "29 pages accepted by AISTATS2025", "summary": "The objective of transfer learning is to enhance estimation and inference in\na target data by leveraging knowledge gained from additional sources. Recent\nstudies have explored transfer learning for independent observations in\ncomplex, high-dimensional models assuming sparsity, yet research on time series\nmodels remains limited. Our focus is on transfer learning for sequences of\nobservations with temporal dependencies and a more intricate model parameter\nstructure. Specifically, we investigate the vector autoregressive model (VAR),\na widely recognized model for time series data, where the transition matrix can\nbe deconstructed into a combination of a sparse matrix and a low-rank one. We\npropose a new transfer learning algorithm tailored for estimating\nhigh-dimensional VAR models characterized by low-rank and sparse structures.\nAdditionally, we present a novel approach for selecting informative\nobservations from auxiliary datasets. Theoretical guarantees are established,\nencompassing model parameter consistency, informative set selection, and the\nasymptotic distribution of estimators under mild conditions. The latter\nfacilitates the construction of entry-wise confidence intervals for model\nparameters. Finally, we demonstrate the empirical efficacy of our methodologies\nthrough both simulated and real-world datasets."}
{"id": "2408.08645", "pdf": "https://arxiv.org/pdf/2408.08645", "abs": "https://arxiv.org/abs/2408.08645", "authors": ["Kai Li", "Yupeng Deng", "Jingbo Chen", "Yu Meng", "Zhihao Xi", "Junxian Ma", "Chenhao Wang", "Maolin Wang", "Xiangyu Zhao"], "title": "PolyFootNet: Extracting Polygonal Building Footprints in Off-Nadir Remote Sensing Images", "categories": ["cs.CV"], "comment": null, "summary": "Extracting polygonal building footprints from off-nadir imagery is crucial\nfor diverse applications. Current deep-learning-based extraction approaches\npredominantly rely on semantic segmentation paradigms and post-processing\nalgorithms, limiting their boundary precision and applicability. However,\nexisting polygonal extraction methodologies are inherently designed for\nnear-nadir imagery and fail under the geometric complexities introduced by\noff-nadir viewing angles. To address these challenges, this paper introduces\nPolygonal Footprint Network (PolyFootNet), a novel deep-learning framework that\ndirectly outputs polygonal building footprints without requiring external\npost-processing steps. PolyFootNet employs a High-Quality Mask Prompter to\ngenerate precise roof masks, which guide polygonal vertex extraction in a\nunified model pipeline. A key contribution of PolyFootNet is introducing the\nSelf Offset Attention mechanism, grounded in Nadaraya-Watson regression, to\neffectively mitigate the accuracy discrepancy observed between low-rise and\nhigh-rise buildings. This approach allows low-rise building predictions to\nleverage angular corrections learned from high-rise building offsets,\nsignificantly enhancing overall extraction accuracy. Additionally, motivated by\nthe inherent ambiguity of building footprint extraction tasks, we\nsystematically investigate alternative extraction paradigms and demonstrate\nthat a combined approach of building masks and offsets achieves superior\npolygonal footprint results. Extensive experiments validate PolyFootNet's\neffectiveness, illustrating its promising potential as a robust, generalizable,\nand precise polygonal building footprint extraction method from challenging\noff-nadir imagery. To facilitate further research, we will release pre-trained\nweights of our offset prediction module at\nhttps://github.com/likaiucas/PolyFootNet."}
{"id": "2407.10834", "pdf": "https://arxiv.org/pdf/2407.10834", "abs": "https://arxiv.org/abs/2407.10834", "authors": ["Quang H. Nguyen", "Thinh Dao", "Duy C. Hoang", "Juliette Decugis", "Saurav Manchanda", "Nitesh V. Chawla", "Khoa D. Doan"], "title": "MetaLLM: A High-performant and Cost-efficient Dynamic Framework for Wrapping LLMs", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "The rapid progress in machine learning (ML) has brought forth many large\nlanguage models (LLMs) that excel in various tasks and areas. These LLMs come\nwith different abilities and costs in terms of computation or pricing. Since\nthe demand for each query can vary, e.g., because of the queried domain or its\ncomplexity, defaulting to one LLM in an application is not usually the best\nchoice, whether it is the biggest, priciest, or even the one with the best\naverage test performance. Consequently, picking the right LLM that is both\naccurate and cost-effective for an application is necessary yet remains a\nchallenge. In this paper, we introduce MetaLLM, a framework that dynamically\nand intelligently routes each query to the optimal LLM (among several available\nLLMs) for classification and multi-choice question-answering tasks, achieving\nsignificantly improved accuracy and cost-effectiveness. By framing the\nselection problem as a multi-armed bandit, MetaLLM balances prediction accuracy\nand cost efficiency under uncertainty. Our experiments, conducted on popular\nLLM platforms such as OpenAI and Together AI, as well as open-source LLM,\nshowcase MetaLLM's efficacy in real-world scenarios, laying the groundwork for\nfuture extensions."}
{"id": "2504.15722", "pdf": "https://arxiv.org/pdf/2504.15722", "abs": "https://arxiv.org/abs/2504.15722", "authors": ["Zhe Huang", "Simone Rossi", "Rui Yuan", "Thomas Hannagan"], "title": "From predictions to confidence intervals: an empirical study of conformal prediction methods for in-context learning", "categories": ["stat.ML", "cs.LG"], "comment": null, "summary": "Transformers have become a standard architecture in machine learning,\ndemonstrating strong in-context learning (ICL) abilities that allow them to\nlearn from the prompt at inference time. However, uncertainty quantification\nfor ICL remains an open challenge, particularly in noisy regression tasks. This\npaper investigates whether ICL can be leveraged for distribution-free\nuncertainty estimation, proposing a method based on conformal prediction to\nconstruct prediction intervals with guaranteed coverage. While traditional\nconformal methods are computationally expensive due to repeated model fitting,\nwe exploit ICL to efficiently generate confidence intervals in a single forward\npass. Our empirical analysis compares this approach against ridge\nregression-based conformal methods, showing that conformal prediction with\nin-context learning (CP with ICL) achieves robust and scalable uncertainty\nestimates. Additionally, we evaluate its performance under distribution shifts\nand establish scaling laws to guide model training. These findings bridge ICL\nand conformal prediction, providing a theoretically grounded and new framework\nfor uncertainty quantification in transformer-based models."}
{"id": "2409.03901", "pdf": "https://arxiv.org/pdf/2409.03901", "abs": "https://arxiv.org/abs/2409.03901", "authors": ["Thanh-Dung Le", "Vu Nguyen Ha", "Ti Ti Nguyen", "Geoffrey Eappen", "Prabhu Thiruvasagam", "Hong-fu Chou", "Duc-Dung Tran", "Hung Nguyen-Kha", "Luis M. Garces-Socarras", "Jorge L. Gonzalez-Rios", "Juan Carlos Merlano-Duncan", "Symeon Chatzinotas"], "title": "Onboard Satellite Image Classification for Earth Observation: A Comparative Study of ViT Models", "categories": ["cs.CV", "eess.SP"], "comment": null, "summary": "This study focuses on identifying the most effective pre-trained model for\nland use classification in onboard satellite processing, emphasizing achieving\nhigh accuracy, computational efficiency, and robustness against noisy data\nconditions commonly encountered during satellite-based inference. Through\nextensive experimentation, we compare the performance of traditional CNN-based,\nResNet-based, and various pre-trained vision Transformer models. Our findings\ndemonstrate that pre-trained Vision Transformer (ViT) models, particularly\nMobileViTV2 and EfficientViT-M2, outperform models trained from scratch in\nterms of accuracy and efficiency. These models achieve high performance with\nreduced computational requirements and exhibit greater resilience during\ninference under noisy conditions. While MobileViTV2 has excelled on clean\nvalidation data, EfficientViT-M2 has proved more robust when handling noise,\nmaking it the most suitable model for onboard satellite EO tasks. Our\nexperimental results demonstrate that EfficientViT-M2 is the optimal choice for\nreliable and efficient RS-IC in satellite operations, achieving 98.76 % of\naccuracy, precision, and recall. Precisely, EfficientViT-M2 delivers the\nhighest performance across all metrics, excels in training efficiency (1,000s)\nand inference time (10s), and demonstrates greater robustness (overall\nrobustness score of 0.79). Consequently, EfficientViT-M2 consumes 63.93 % less\npower than MobileViTV2 (79.23 W) and 73.26 % less power than SwinTransformer\n(108.90 W). This highlights its significant advantage in energy efficiency."}
{"id": "2409.07200", "pdf": "https://arxiv.org/pdf/2409.07200", "abs": "https://arxiv.org/abs/2409.07200", "authors": ["Rongfeng Lu", "Hangyu Chen", "Zunjie Zhu", "Yuhang Qin", "Ming Lu", "Le Zhang", "Chenggang Yan", "Anke Xue"], "title": "ThermalGaussian: Thermal 3D Gaussian Splatting", "categories": ["cs.CV", "cs.AI"], "comment": "10 pages, 7 figures", "summary": "Thermography is especially valuable for the military and other users of\nsurveillance cameras. Some recent methods based on Neural Radiance Fields\n(NeRF) are proposed to reconstruct the thermal scenes in 3D from a set of\nthermal and RGB images. However, unlike NeRF, 3D Gaussian splatting (3DGS)\nprevails due to its rapid training and real-time rendering. In this work, we\npropose ThermalGaussian, the first thermal 3DGS approach capable of rendering\nhigh-quality images in RGB and thermal modalities. We first calibrate the RGB\ncamera and the thermal camera to ensure that both modalities are accurately\naligned. Subsequently, we use the registered images to learn the multimodal 3D\nGaussians. To prevent the overfitting of any single modality, we introduce\nseveral multimodal regularization constraints. We also develop smoothing\nconstraints tailored to the physical characteristics of the thermal modality.\nBesides, we contribute a real-world dataset named RGBT-Scenes, captured by a\nhand-hold thermal-infrared camera, facilitating future research on thermal\nscene reconstruction. We conduct comprehensive experiments to show that\nThermalGaussian achieves photorealistic rendering of thermal images and\nimproves the rendering quality of RGB images. With the proposed multimodal\nregularization constraints, we also reduced the model's storage cost by 90%.\nOur project page is at https://thermalgaussian.github.io/."}
{"id": "2504.15753", "pdf": "https://arxiv.org/pdf/2504.15753", "abs": "https://arxiv.org/abs/2504.15753", "authors": ["Alexis M. H. Teter", "Wenqing Wang", "Sachin Shivakumar", "Abhishek Halder"], "title": "Markov Kernels, Distances and Optimal Control: A Parable of Linear Quadratic Non-Gaussian Distribution Steering", "categories": ["math.OC", "cs.LG", "cs.SY", "eess.SY", "math.PR", "math.ST", "stat.TH"], "comment": null, "summary": "For a controllable linear time-varying (LTV) pair\n$(\\boldsymbol{A}_t,\\boldsymbol{B}_t)$ and $\\boldsymbol{Q}_{t}$ positive\nsemidefinite, we derive the Markov kernel for the It\\^{o} diffusion\n${\\mathrm{d}}\\boldsymbol{x}_{t}=\\boldsymbol{A}_{t}\\boldsymbol{x}_t {\\mathrm{d}}\nt + \\sqrt{2}\\boldsymbol{B}_{t}{\\mathrm{d}}\\boldsymbol{w}_{t}$ with an\naccompanying killing of probability mass at rate\n$\\frac{1}{2}\\boldsymbol{x}^{\\top}\\boldsymbol{Q}_{t}\\boldsymbol{x}$. This Markov\nkernel is the Green's function for an associated linear\nreaction-advection-diffusion partial differential equation. Our result\ngeneralizes the recently derived kernel for the special case\n$\\left(\\boldsymbol{A}_t,\\boldsymbol{B}_t\\right)=\\left(\\boldsymbol{0},\\boldsymbol{I}\\right)$,\nand depends on the solution of an associated Riccati matrix ODE. A consequence\nof this result is that the linear quadratic non-Gaussian Schr\\\"{o}dinger bridge\nis exactly solvable. This means that the problem of steering a controlled LTV\ndiffusion from a given non-Gaussian distribution to another over a fixed\ndeadline while minimizing an expected quadratic cost can be solved using\ndynamic Sinkhorn recursions performed with the derived kernel. Our derivation\nfor the\n$\\left(\\boldsymbol{A}_t,\\boldsymbol{B}_t,\\boldsymbol{Q}_t\\right)$-parametrized\nkernel pursues a new idea that relies on finding a state-time dependent\ndistance-like functional given by the solution of a deterministic optimal\ncontrol problem. This technique breaks away from existing methods, such as\ngeneralizing Hermite polynomials or Weyl calculus, which have seen limited\nsuccess in the reaction-diffusion context. Our technique uncovers a new\nconnection between Markov kernels, distances, and optimal control. This\nconnection is of interest beyond its immediate application in solving the\nlinear quadratic Schr\\\"{o}dinger bridge problem."}
{"id": "2410.10783", "pdf": "https://arxiv.org/pdf/2410.10783", "abs": "https://arxiv.org/abs/2410.10783", "authors": ["Nimrod Shabtay", "Felipe Maia Polo", "Sivan Doveh", "Wei Lin", "M. Jehanzeb Mirza", "Leshem Chosen", "Mikhail Yurochkin", "Yuekai Sun", "Assaf Arbelle", "Leonid Karlinsky", "Raja Giryes"], "title": "LiveXiv -- A Multi-Modal Live Benchmark Based on Arxiv Papers Content", "categories": ["cs.CV"], "comment": null, "summary": "The large-scale training of multi-modal models on data scraped from the web\nhas shown outstanding utility in infusing these models with the required world\nknowledge to perform effectively on multiple downstream tasks. However, one\ndownside of scraping data from the web can be the potential sacrifice of the\nbenchmarks on which the abilities of these models are often evaluated. To\nsafeguard against test data contamination and to truly test the abilities of\nthese foundation models we propose LiveXiv: A scalable evolving live benchmark\nbased on scientific ArXiv papers. LiveXiv accesses domain-specific manuscripts\nat any given timestamp and proposes to automatically generate visual\nquestion-answer pairs (VQA). This is done without any human-in-the-loop, using\nthe multi-modal content in the manuscripts, like graphs, charts, and tables.\nMoreover, we introduce an efficient evaluation approach that estimates the\nperformance of all models on the evolving benchmark using evaluations of only a\nsubset of models. This significantly reduces the overall evaluation cost. We\nbenchmark multiple open and proprietary Large Multi-modal Models (LMMs) on the\nfirst version of our benchmark, showing its challenging nature and exposing the\nmodels true abilities, avoiding contamination. Lastly, in our commitment to\nhigh quality, we have collected and evaluated a manually verified subset. By\ncomparing its overall results to our automatic annotations, we have found that\nthe performance variance is indeed minimal (<2.5%). Our dataset is available\nonline on HuggingFace, and our code will be available here."}
{"id": "2410.05056", "pdf": "https://arxiv.org/pdf/2410.05056", "abs": "https://arxiv.org/abs/2410.05056", "authors": ["Attila Lovas"], "title": "Transition of $α$-mixing in Random Iterations with Applications in Queuing Theory", "categories": ["math.ST", "cs.AI", "math.PR", "stat.TH", "60K37, 60K25, 60J05, 60J20", "G.3; I.6.5; C.4"], "comment": "39 pages, 1 figure", "summary": "Nonlinear time series models with exogenous regressors are essential in\neconometrics, queuing theory, and machine learning, though their statistical\nanalysis remains incomplete. Key results, such as the law of large numbers and\nthe functional central limit theorem, are known for weakly dependent variables.\nWe demonstrate the transfer of mixing properties from the exogenous regressor\nto the response via coupling arguments. Additionally, we study Markov chains in\nrandom environments with drift and minorization conditions, even under\nnon-stationary environments with favorable mixing properties, and apply this\nframework to single-server queuing models."}
{"id": "2504.15826", "pdf": "https://arxiv.org/pdf/2504.15826", "abs": "https://arxiv.org/abs/2504.15826", "authors": ["Xinru Mu", "Omar M. Saad", "Tariq Alkhalifah"], "title": "Full waveform inversion with CNN-based velocity representation extension", "categories": ["physics.geo-ph", "cs.LG"], "comment": "16 pages, 15 figures, Scientific paper", "summary": "Full waveform inversion (FWI) updates the velocity model by minimizing the\ndiscrepancy between observed and simulated data. However, discretization errors\nin numerical modeling and incomplete seismic data acquisition can introduce\nnoise, which propagates through the adjoint operator and affects the accuracy\nof the velocity gradient, thereby impacting the FWI inversion accuracy. To\nmitigate the influence of noise on the gradient, we employ a convolutional\nneural network (CNN) to refine the velocity model before performing the forward\nsimulation, aiming to reduce noise and provide a more accurate velocity update\ndirection. We use the same data misfit loss to update both the velocity and\nnetwork parameters, thereby forming a self-supervised learning procedure. We\npropose two implementation schemes, which differ in whether the velocity update\npasses through the CNN. In both methodologies, the velocity representation is\nextended (VRE) by using a neural network in addition to the grid-based\nvelocities. Thus, we refer to this general approach as VRE-FWI. Synthetic and\nreal data tests demonstrate that the proposed VRE-FWI achieves higher velocity\ninversion accuracy compared to traditional FWI, at a marginal additional\ncomputational cost of approximately 1%."}
{"id": "2411.17820", "pdf": "https://arxiv.org/pdf/2411.17820", "abs": "https://arxiv.org/abs/2411.17820", "authors": ["Xinhao Liu", "Jintong Li", "Yicheng Jiang", "Niranjan Sujay", "Zhicheng Yang", "Juexiao Zhang", "John Abanes", "Jing Zhang", "Chen Feng"], "title": "CityWalker: Learning Embodied Urban Navigation from Web-Scale Videos", "categories": ["cs.CV", "cs.RO"], "comment": "Accepted to CVPR 2025", "summary": "Navigating dynamic urban environments presents significant challenges for\nembodied agents, requiring advanced spatial reasoning and adherence to\ncommon-sense norms. Despite progress, existing visual navigation methods\nstruggle in map-free or off-street settings, limiting the deployment of\nautonomous agents like last-mile delivery robots. To overcome these obstacles,\nwe propose a scalable, data-driven approach for human-like urban navigation by\ntraining agents on thousands of hours of in-the-wild city walking and driving\nvideos sourced from the web. We introduce a simple and scalable data processing\npipeline that extracts action supervision from these videos, enabling\nlarge-scale imitation learning without costly annotations. Our model learns\nsophisticated navigation policies to handle diverse challenges and critical\nscenarios. Experimental results show that training on large-scale, diverse\ndatasets significantly enhances navigation performance, surpassing current\nmethods. This work shows the potential of using abundant online video data to\ndevelop robust navigation policies for embodied agents in dynamic urban\nsettings. Project homepage is at https://ai4ce.github.io/CityWalker/."}
{"id": "2410.05295", "pdf": "https://arxiv.org/pdf/2410.05295", "abs": "https://arxiv.org/abs/2410.05295", "authors": ["Xiaogeng Liu", "Peiran Li", "Edward Suh", "Yevgeniy Vorobeychik", "Zhuoqing Mao", "Somesh Jha", "Patrick McDaniel", "Huan Sun", "Bo Li", "Chaowei Xiao"], "title": "AutoDAN-Turbo: A Lifelong Agent for Strategy Self-Exploration to Jailbreak LLMs", "categories": ["cs.CR", "cs.AI", "cs.LG"], "comment": "ICLR 2025 Spotlight. Project Page:\n  https://autodans.github.io/AutoDAN-Turbo Code:\n  https://github.com/SaFoLab-WISC/AutoDAN-Turbo", "summary": "In this paper, we propose AutoDAN-Turbo, a black-box jailbreak method that\ncan automatically discover as many jailbreak strategies as possible from\nscratch, without any human intervention or predefined scopes (e.g., specified\ncandidate strategies), and use them for red-teaming. As a result, AutoDAN-Turbo\ncan significantly outperform baseline methods, achieving a 74.3% higher average\nattack success rate on public benchmarks. Notably, AutoDAN-Turbo achieves an\n88.5 attack success rate on GPT-4-1106-turbo. In addition, AutoDAN-Turbo is a\nunified framework that can incorporate existing human-designed jailbreak\nstrategies in a plug-and-play manner. By integrating human-designed strategies,\nAutoDAN-Turbo can even achieve a higher attack success rate of 93.4 on\nGPT-4-1106-turbo."}
{"id": "2504.15933", "pdf": "https://arxiv.org/pdf/2504.15933", "abs": "https://arxiv.org/abs/2504.15933", "authors": ["Anh Truong", "Ahmed H. Mahmoud", "Mina Konaković Luković", "Justin Solomon"], "title": "Low-Rank Adaptation of Neural Fields", "categories": ["cs.GR", "cs.LG"], "comment": null, "summary": "Processing visual data often involves small adjustments or sequences of\nchanges, such as in image filtering, surface smoothing, and video storage.\nWhile established graphics techniques like normal mapping and video compression\nexploit redundancy to encode such small changes efficiently, the problem of\nencoding small changes to neural fields (NF) -- neural network\nparameterizations of visual or physical functions -- has received less\nattention.\n  We propose a parameter-efficient strategy for updating neural fields using\nlow-rank adaptations (LoRA). LoRA, a method from the parameter-efficient\nfine-tuning LLM community, encodes small updates to pre-trained models with\nminimal computational overhead. We adapt LoRA to instance-specific neural\nfields, avoiding the need for large pre-trained models yielding a pipeline\nsuitable for low-compute hardware.\n  We validate our approach with experiments in image filtering, video\ncompression, and geometry editing, demonstrating its effectiveness and\nversatility for representing neural field updates."}
{"id": "2411.18473", "pdf": "https://arxiv.org/pdf/2411.18473", "abs": "https://arxiv.org/abs/2411.18473", "authors": ["Lei Liu", "Zhenghao Chen", "Wei Jiang", "Wei Wang", "Dong Xu"], "title": "HEMGS: A Hybrid Entropy Model for 3D Gaussian Splatting Data Compression", "categories": ["cs.CV"], "comment": null, "summary": "In this work, we propose a novel compression framework for 3D Gaussian\nSplatting (3DGS) data. Building on anchor-based 3DGS methodologies, our\napproach compresses all attributes within each anchor by introducing a novel\nHybrid Entropy Model for 3D Gaussian Splatting (HEMGS) to achieve hybrid\nlossy-lossless compression. It consists of three main components: a\nvariable-rate predictor, a hyperprior network, and an autoregressive network.\nFirst, unlike previous methods that adopt multiple models to achieve multi-rate\nlossy compression, thereby increasing training overhead, our variable-rate\npredictor enables variable-rate compression with a single model and a\nhyperparameter $\\lambda$ by producing a learned Quantization Step feature for\nversatile lossy compression. Second, to improve lossless compression, the\nhyperprior network captures both scene-agnostic and scene-specific features to\ngenerate a prior feature, while the autoregressive network employs an adaptive\ncontext selection algorithm with flexible receptive fields to produce a\ncontextual feature. By integrating these two features, HEMGS can accurately\nestimate the distribution of the current coding element within each attribute,\nenabling improved entropy coding and reduced storage. We integrate HEMGS into a\ncompression framework, and experimental results on four benchmarks indicate\nthat HEMGS achieves about a 40% average reduction in size while maintaining\nrendering quality over baseline methods and achieving state-of-the-art\ncompression results."}
{"id": "2410.16739", "pdf": "https://arxiv.org/pdf/2410.16739", "abs": "https://arxiv.org/abs/2410.16739", "authors": ["Yanjun Chen", "Xinming Zhang", "Xianghui Wang", "Zhiqiang Xu", "Xiaoyu Shen", "Wei Zhang"], "title": "Rethinking Soft Actor-Critic in High-Dimensional Action Spaces: The Cost of Ignoring Distribution Shift", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Soft Actor-Critic algorithm is widely recognized for its robust performance\nacross a range of deep reinforcement learning tasks, where it leverages the\ntanh transformation to constrain actions within bounded limits. However, this\ntransformation induces a distribution shift, distorting the original Gaussian\naction distribution and potentially leading the policy to select suboptimal\nactions, particularly in high-dimensional action spaces. In this paper, we\nconduct a comprehensive theoretical and empirical analysis of this distribution\nshift, deriving the precise probability density function (PDF) for actions\nfollowing the tanh transformation to clarify the misalignment introduced\nbetween the transformed distribution's mode and the intended action output. We\nsubstantiate these theoretical insights through extensive experiments on\nhigh-dimensional tasks within the HumanoidBench benchmark. Our findings\nindicate that accounting for this distribution shift substantially enhances\nSAC's performance, resulting in notable improvements in cumulative rewards,\nsample efficiency, and reliability across tasks. These results underscore a\ncritical consideration for SAC and similar algorithms: addressing\ntransformation-induced distribution shifts is essential to optimizing policy\neffectiveness in high-dimensional deep reinforcement learning environments,\nthereby expanding the robustness and applicability of SAC in complex control\ntasks."}
{"id": "2504.15942", "pdf": "https://arxiv.org/pdf/2504.15942", "abs": "https://arxiv.org/abs/2504.15942", "authors": ["Erik Imgrund", "Thorsten Eisenhofer", "Konrad Rieck"], "title": "Adversarial Observations in Weather Forecasting", "categories": ["cs.CR", "cs.LG"], "comment": null, "summary": "AI-based systems, such as Google's GenCast, have recently redefined the state\nof the art in weather forecasting, offering more accurate and timely\npredictions of both everyday weather and extreme events. While these systems\nare on the verge of replacing traditional meteorological methods, they also\nintroduce new vulnerabilities into the forecasting process. In this paper, we\ninvestigate this threat and present a novel attack on autoregressive diffusion\nmodels, such as those used in GenCast, capable of manipulating weather\nforecasts and fabricating extreme events, including hurricanes, heat waves, and\nintense rainfall. The attack introduces subtle perturbations into weather\nobservations that are statistically indistinguishable from natural noise and\nchange less than 0.1% of the measurements - comparable to tampering with data\nfrom a single meteorological satellite. As modern forecasting integrates data\nfrom nearly a hundred satellites and many other sources operated by different\ncountries, our findings highlight a critical security risk with the potential\nto cause large-scale disruptions and undermine public trust in weather\nprediction."}
{"id": "2412.02856", "pdf": "https://arxiv.org/pdf/2412.02856", "abs": "https://arxiv.org/abs/2412.02856", "authors": ["Piotr Teterwak", "Kuniaki Saito", "Theodoros Tsiligkaridis", "Bryan A. Plummer", "Kate Saenko"], "title": "Is Large-Scale Pretraining the Secret to Good Domain Generalization?", "categories": ["cs.CV", "cs.LG"], "comment": "Accepted at ICLR 2025", "summary": "Multi-Source Domain Generalization (DG) is the task of training on multiple\nsource domains and achieving high classification performance on unseen target\ndomains. Recent methods combine robust features from web-scale pretrained\nbackbones with new features learned from source data, and this has dramatically\nimproved benchmark results. However, it remains unclear if DG finetuning\nmethods are becoming better over time, or if improved benchmark performance is\nsimply an artifact of stronger pre-training. Prior studies have shown that\nperceptual similarity to pre-training data correlates with zero-shot\nperformance, but we find the effect limited in the DG setting. Instead, we\nposit that having perceptually similar data in pretraining is not enough; and\nthat it is how well these data were learned that determines performance. This\nleads us to introduce the Alignment Hypothesis, which states that the final DG\nperformance will be high if and only if alignment of image and class label text\nembeddings is high. Our experiments confirm the Alignment Hypothesis is true,\nand we use it as an analysis tool of existing DG methods evaluated on DomainBed\ndatasets by splitting evaluation data into In-pretraining (IP) and\nOut-of-pretraining (OOP). We show that all evaluated DG methods struggle on\nDomainBed-OOP, while recent methods excel on DomainBed-IP. Put together, our\nfindings highlight the need for DG methods which can generalize beyond\npretraining alignment."}
{"id": "2411.07007", "pdf": "https://arxiv.org/pdf/2411.07007", "abs": "https://arxiv.org/abs/2411.07007", "authors": ["Arnav Kumar Jain", "Harley Wiltzer", "Jesse Farebrother", "Irina Rish", "Glen Berseth", "Sanjiban Choudhury"], "title": "Non-Adversarial Inverse Reinforcement Learning via Successor Feature Matching", "categories": ["cs.LG", "cs.AI"], "comment": "Accepted to ICLR 2025", "summary": "In inverse reinforcement learning (IRL), an agent seeks to replicate expert\ndemonstrations through interactions with the environment. Traditionally, IRL is\ntreated as an adversarial game, where an adversary searches over reward models,\nand a learner optimizes the reward through repeated RL procedures. This\ngame-solving approach is both computationally expensive and difficult to\nstabilize. In this work, we propose a novel approach to IRL by direct policy\noptimization: exploiting a linear factorization of the return as the inner\nproduct of successor features and a reward vector, we design an IRL algorithm\nby policy gradient descent on the gap between the learner and expert features.\nOur non-adversarial method does not require learning a reward function and can\nbe solved seamlessly with existing actor-critic RL algorithms. Remarkably, our\napproach works in state-only settings without expert action labels, a setting\nwhich behavior cloning (BC) cannot solve. Empirical results demonstrate that\nour method learns from as few as a single expert demonstration and achieves\nimproved performance on various control tasks."}
{"id": "2504.15979", "pdf": "https://arxiv.org/pdf/2504.15979", "abs": "https://arxiv.org/abs/2504.15979", "authors": ["Zhiyuan Zheng", "Jianpeng Qi", "Jiantao Li", "Guoqing Chao", "Junyu Dong", "Yanwei Yu"], "title": "Efficient Discovery of Motif Transition Process for Large-Scale Temporal Graphs", "categories": ["cs.DB", "cs.LG"], "comment": null, "summary": "Understanding the dynamic transition of motifs in temporal graphs is\nessential for revealing how graph structures evolve over time, identifying\ncritical patterns, and predicting future behaviors, yet existing methods often\nfocus on predefined motifs, limiting their ability to comprehensively capture\ntransitions and interrelationships. We propose a parallel motif transition\nprocess discovery algorithm, PTMT, a novel parallel method for discovering\nmotif transition processes in large-scale temporal graphs. PTMT integrates a\ntree-based framework with the temporal zone partitioning (TZP) strategy, which\npartitions temporal graphs by time and structure while preserving lossless\nmotif transitions and enabling massive parallelism. PTMT comprises three\nphases: growth zone parallel expansion, overlap-aware result aggregation, and\ndeterministic encoding of motif transitions, ensuring accurate tracking of\ndynamic transitions and interactions. Results on 10 real-world datasets\ndemonstrate that PTMT achieves speedups ranging from 12.0$\\times$ to\n50.3$\\times$ compared to the SOTA method."}
{"id": "2412.07608", "pdf": "https://arxiv.org/pdf/2412.07608", "abs": "https://arxiv.org/abs/2412.07608", "authors": ["Chengbo Wang", "Guozheng Ma", "Yifei Xue", "Yizhen Lao"], "title": "Faster and Better 3D Splatting via Group Training", "categories": ["cs.CV"], "comment": null, "summary": "3D Gaussian Splatting (3DGS) has emerged as a powerful technique for novel\nview synthesis, demonstrating remarkable capability in high-fidelity scene\nreconstruction through its Gaussian primitive representations. However, the\ncomputational overhead induced by the massive number of primitives poses a\nsignificant bottleneck to training efficiency. To overcome this challenge, we\npropose Group Training, a simple yet effective strategy that organizes Gaussian\nprimitives into manageable groups, optimizing training efficiency and improving\nrendering quality. This approach shows universal compatibility with existing\n3DGS frameworks, including vanilla 3DGS and Mip-Splatting, consistently\nachieving accelerated training while maintaining superior synthesis quality.\nExtensive experiments reveal that our straightforward Group Training strategy\nachieves up to 30% faster convergence and improved rendering quality across\ndiverse scenarios."}
{"id": "2501.01005", "pdf": "https://arxiv.org/pdf/2501.01005", "abs": "https://arxiv.org/abs/2501.01005", "authors": ["Zihao Ye", "Lequn Chen", "Ruihang Lai", "Wuwei Lin", "Yineng Zhang", "Stephanie Wang", "Tianqi Chen", "Baris Kasikci", "Vinod Grover", "Arvind Krishnamurthy", "Luis Ceze"], "title": "FlashInfer: Efficient and Customizable Attention Engine for LLM Inference Serving", "categories": ["cs.DC", "cs.AI", "cs.LG"], "comment": "Accepted by MLSys 2025, code available at\n  http://github.com/flashinfer-ai/flashinfer", "summary": "Transformers, driven by attention mechanisms, form the foundation of large\nlanguage models (LLMs). As these models scale up, efficient GPU attention\nkernels become essential for high-throughput and low-latency inference. Diverse\nLLM applications demand flexible and high-performance attention solutions. We\npresent FlashInfer: a customizable and efficient attention engine for LLM\nserving. FlashInfer tackles KV-cache storage heterogeneity using block-sparse\nformat and composable formats to optimize memory access and reduce redundancy.\nIt also offers a customizable attention template, enabling adaptation to\nvarious settings through Just-In-Time (JIT) compilation. Additionally,\nFlashInfer's load-balanced scheduling algorithm adjusts to dynamism of user\nrequests while maintaining compatibility with CUDAGraph which requires static\nconfiguration. FlashInfer have been integrated into leading LLM serving\nframeworks like SGLang, vLLM and MLC-Engine. Comprehensive kernel-level and\nend-to-end evaluations demonstrate FlashInfer's ability to significantly boost\nkernel performance across diverse inference scenarios: compared to\nstate-of-the-art LLM serving solutions, FlashInfer achieve 29-69%\ninter-token-latency reduction compared to compiler backends for LLM serving\nbenchmark, 28-30% latency reduction for long-context inference, and 13-17%\nspeedup for LLM serving with parallel generation."}
{"id": "2504.15993", "pdf": "https://arxiv.org/pdf/2504.15993", "abs": "https://arxiv.org/abs/2504.15993", "authors": ["Oliver Summerell", "Gerardo Aragon-Camarasa", "Stephanie Ordonez Sanchez"], "title": "Benchmarking machine learning models for predicting aerofoil performance", "categories": ["physics.flu-dyn", "cs.LG"], "comment": "9 pages, 10 figures, submitted to EWTEC", "summary": "This paper investigates the capability of Neural Networks (NNs) as\nalternatives to the traditional methods to analyse the performance of aerofoils\nused in the wind and tidal energy industry. The current methods used to assess\nthe characteristic lift and drag coefficients include Computational Fluid\nDynamics (CFD), thin aerofoil and panel methods, all face trade-offs between\ncomputational speed and the accuracy of the results and as such NNs have been\ninvestigated as an alternative with the aim that it would perform both quickly\nand accurately. As such, this paper provides a benchmark for the windAI_bench\ndataset published by the National Renewable Energy Laboratory (NREL) in the\nUSA. In order to validate the methodology of the benchmarking, the AirfRANS\n{\\tt arXiv:2212.07564v3} dataset is used as both a starting point and a point\nof comparison. This study evaluates four neural networks (MLP, PointNet,\nGraphSAGE, GUNet) trained on a range aerofoils at 25 angles of attack\n(4$^\\circ$ to 20$^\\circ$). to predict fluid flow and calculate lift\ncoefficients ($C_L$) via the panel method. GraphSAGE and GUNet performed well\nduring the testing phase, but underperformed during validation. Accordingly,\nthis paper has identified PointNet and MLP as the two strongest models tested,\nhowever whilst the results from MLP are more commonly correct for predicting\nthe behaviour of the fluid, the results from PointNet provide the more accurate\nresults for calculating $C_L$."}
{"id": "2412.14015", "pdf": "https://arxiv.org/pdf/2412.14015", "abs": "https://arxiv.org/abs/2412.14015", "authors": ["Haotong Lin", "Sida Peng", "Jingxiao Chen", "Songyou Peng", "Jiaming Sun", "Minghuan Liu", "Hujun Bao", "Jiashi Feng", "Xiaowei Zhou", "Bingyi Kang"], "title": "Prompting Depth Anything for 4K Resolution Accurate Metric Depth Estimation", "categories": ["cs.CV"], "comment": "CVPR 2025, Project page: https://PromptDA.github.io/", "summary": "Prompts play a critical role in unleashing the power of language and vision\nfoundation models for specific tasks. For the first time, we introduce\nprompting into depth foundation models, creating a new paradigm for metric\ndepth estimation termed Prompt Depth Anything. Specifically, we use a low-cost\nLiDAR as the prompt to guide the Depth Anything model for accurate metric depth\noutput, achieving up to 4K resolution. Our approach centers on a concise prompt\nfusion design that integrates the LiDAR at multiple scales within the depth\ndecoder. To address training challenges posed by limited datasets containing\nboth LiDAR depth and precise GT depth, we propose a scalable data pipeline that\nincludes synthetic data LiDAR simulation and real data pseudo GT depth\ngeneration. Our approach sets new state-of-the-arts on the ARKitScenes and\nScanNet++ datasets and benefits downstream applications, including 3D\nreconstruction and generalized robotic grasping."}
{"id": "2501.04444", "pdf": "https://arxiv.org/pdf/2501.04444", "abs": "https://arxiv.org/abs/2501.04444", "authors": ["Dana A Abdullah", "Dana Rasul Hamad", "Ismail Y. Maolood", "Hakem Beitollahi", "Aso K. Ameen", "Sirwan A. Aula", "Abdulhady Abas Abdulla", "Mohammed Y. Shakorf", "Sabat Salih Muhamad"], "title": "A novel Facial Recognition technique with Focusing on Masked Faces", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Recognizing the same faces with and without masks is important for ensuring\nconsistent identification in security, access control, and public safety. This\ncapability is crucial in scenarios like law enforcement, healthcare, and\nsurveillance, where accurate recognition must be maintained despite facial\nocclusion. This research focuses on the challenge of recognizing the same faces\nwith and without masks by employing cosine similarity as the primary technique.\nWith the increased use of masks, traditional facial recognition systems face\nsignificant accuracy issues, making it crucial to develop methods that can\nreliably identify individuals in masked conditions. For that reason, this study\nproposed Masked-Unmasked Face Matching Model (MUFM). This model employs\ntransfer learning using the Visual Geometry Group (VGG16) model to extract\nsignificant facial features, which are subsequently classified utilizing the\nK-Nearest Neighbors (K-NN) algorithm. The cosine similarity metric is employed\nto compare masked and unmasked faces of the same individuals. This approach\nrepresents a novel contribution, as the task of recognizing the same individual\nwith and without a mask using cosine similarity has not been previously\naddressed. By integrating these advanced methodologies, the research\ndemonstrates effective identification of individuals despite the presence of\nmasks, addressing a significant limitation in traditional systems. Using data\nis another essential part of this work, by collecting and preparing an image\ndataset from three different sources especially some of those data are real\nprovided a comprehensive power of this research. The image dataset used were\nalready collected in three different datasets of masked and unmasked for the\nsame faces."}
{"id": "2504.16068", "pdf": "https://arxiv.org/pdf/2504.16068", "abs": "https://arxiv.org/abs/2504.16068", "authors": ["Chuin Wei Tan", "Marc L. Descoteaux", "Mit Kotak", "Gabriel de Miranda Nascimento", "Seán R. Kavanagh", "Laura Zichi", "Menghang Wang", "Aadit Saluja", "Yizhong R. Hu", "Tess Smidt", "Anders Johansson", "William C. Witt", "Boris Kozinsky", "Albert Musaelian"], "title": "High-performance training and inference for deep equivariant interatomic potentials", "categories": ["physics.comp-ph", "cs.LG", "physics.chem-ph"], "comment": null, "summary": "Machine learning interatomic potentials, particularly those based on deep\nequivariant neural networks, have demonstrated state-of-the-art accuracy and\ncomputational efficiency in atomistic modeling tasks like molecular dynamics\nand high-throughput screening. The size of datasets and demands of downstream\nworkflows are growing rapidly, making robust and scalable software essential.\nThis work presents a major overhaul of the NequIP framework focusing on\nmulti-node parallelism, computational performance, and extensibility. The\nredesigned framework supports distributed training on large datasets and\nremoves barriers preventing full utilization of the PyTorch 2.0 compiler at\ntrain time. We demonstrate this acceleration in a case study by training\nAllegro models on the SPICE 2 dataset of organic molecular systems. For\ninference, we introduce the first end-to-end infrastructure that uses the\nPyTorch Ahead-of-Time Inductor compiler for machine learning interatomic\npotentials. Additionally, we implement a custom kernel for the Allegro model's\nmost expensive operation, the tensor product. Together, these advancements\nspeed up molecular dynamics calculations on system sizes of practical relevance\nby up to a factor of 18."}
{"id": "2412.18386", "pdf": "https://arxiv.org/pdf/2412.18386", "abs": "https://arxiv.org/abs/2412.18386", "authors": ["Sagnik Majumder", "Tushar Nagarajan", "Ziad Al-Halah", "Kristen Grauman"], "title": "Switch-a-View: View Selection Learned from Unlabeled In-the-wild Videos", "categories": ["cs.CV"], "comment": null, "summary": "We introduce SWITCH-A-VIEW, a model that learns to automatically select the\nviewpoint to display at each timepoint when creating a how-to video. The key\ninsight of our approach is how to train such a model from unlabeled -- but\nhuman-edited -- video samples. We pose a pretext task that pseudo-labels\nsegments in the training videos for their primary viewpoint (egocentric or\nexocentric), and then discovers the patterns between the visual and spoken\ncontent in a how-to video on the one hand and its view-switch moments on the\nother hand. Armed with this predictor, our model can be applied to new\nmulti-view video settings for orchestrating which viewpoint should be displayed\nwhen, even when such settings come with limited labels. We demonstrate our idea\non a variety of real-world videos from HowTo100M and Ego-Exo4D, and rigorously\nvalidate its advantages. Project:\nhttps://vision.cs.utexas.edu/projects/switch_a_view/."}
{"id": "2501.05496", "pdf": "https://arxiv.org/pdf/2501.05496", "abs": "https://arxiv.org/abs/2501.05496", "authors": ["Yanbing Zhou", "Xiangmou Qu", "Chenlong You", "Jiyang Zhou", "Jingyue Tang", "Xin Zheng", "Chunmao Cai", "Yingbo Wu"], "title": "FedSA: A Unified Representation Learning via Semantic Anchors for Prototype-based Federated Learning", "categories": ["cs.LG", "cs.AI"], "comment": "Accepted by AAAI2025", "summary": "Prototype-based federated learning has emerged as a promising approach that\nshares lightweight prototypes to transfer knowledge among clients with data\nheterogeneity in a model-agnostic manner. However, existing methods often\ncollect prototypes directly from local models, which inevitably introduce\ninconsistencies into representation learning due to the biased data\ndistributions and differing model architectures among clients. In this paper,\nwe identify that both statistical and model heterogeneity create a vicious\ncycle of representation inconsistency, classifier divergence, and skewed\nprototype alignment, which negatively impacts the performance of clients. To\nbreak the vicious cycle, we propose a novel framework named Federated Learning\nvia Semantic Anchors (FedSA) to decouple the generation of prototypes from\nlocal representation learning. We introduce a novel perspective that uses\nsimple yet effective semantic anchors serving as prototypes to guide local\nmodels in learning consistent representations. By incorporating semantic\nanchors, we further propose anchor-based regularization with margin-enhanced\ncontrastive learning and anchor-based classifier calibration to correct feature\nextractors and calibrate classifiers across clients, achieving intra-class\ncompactness and inter-class separability of prototypes while ensuring\nconsistent decision boundaries. We then update the semantic anchors with these\nconsistent and discriminative prototypes, which iteratively encourage clients\nto collaboratively learn a unified data representation with robust\ngeneralization. Extensive experiments under both statistical and model\nheterogeneity settings show that FedSA significantly outperforms existing\nprototype-based FL methods on various classification tasks."}
{"id": "2504.16075", "pdf": "https://arxiv.org/pdf/2504.16075", "abs": "https://arxiv.org/abs/2504.16075", "authors": ["Joshua S. Harvey", "Joshua Rosaler", "Mingshu Li", "Dhruv Desai", "Dhagash Mehta"], "title": "Explainable Unsupervised Anomaly Detection with Random Forest", "categories": ["stat.ML", "cs.LG"], "comment": "14 pages, 5 figures", "summary": "We describe the use of an unsupervised Random Forest for similarity learning\nand improved unsupervised anomaly detection. By training a Random Forest to\ndiscriminate between real data and synthetic data sampled from a uniform\ndistribution over the real data bounds, a distance measure is obtained that\nanisometrically transforms the data, expanding distances at the boundary of the\ndata manifold. We show that using distances recovered from this transformation\nimproves the accuracy of unsupervised anomaly detection, compared to other\ncommonly used detectors, demonstrated over a large number of benchmark\ndatasets. As well as improved performance, this method has advantages over\nother unsupervised anomaly detection methods, including minimal requirements\nfor data preprocessing, native handling of missing data, and potential for\nvisualizations. By relating outlier scores to partitions of the Random Forest,\nwe develop a method for locally explainable anomaly predictions in terms of\nfeature importance."}
{"id": "2412.19504", "pdf": "https://arxiv.org/pdf/2412.19504", "abs": "https://arxiv.org/abs/2412.19504", "authors": ["Jing Li", "Bo Wang"], "title": "Hear the Scene: Audio-Enhanced Text Spotting", "categories": ["cs.CV"], "comment": null, "summary": "Recent advancements in scene text spotting have focused on end-to-end\nmethodologies that heavily rely on precise location annotations, which are\noften costly and labor-intensive to procure. In this study, we introduce an\ninnovative approach that leverages only transcription annotations for training\ntext spotting models, substantially reducing the dependency on elaborate\nannotation processes. Our methodology employs a query-based paradigm that\nfacilitates the learning of implicit location features through the interaction\nbetween text queries and image embeddings. These features are later refined\nduring the text recognition phase using an attention activation map. Addressing\nthe challenges associated with training a weakly-supervised model from scratch,\nwe implement a circular curriculum learning strategy to enhance model\nconvergence. Additionally, we introduce a coarse-to-fine cross-attention\nlocalization mechanism for more accurate text instance localization. Notably,\nour framework supports audio-based annotation, which significantly diminishes\nannotation time and provides an inclusive alternative for individuals with\ndisabilities. Our approach achieves competitive performance against existing\nbenchmarks, demonstrating that high accuracy in text spotting can be attained\nwithout extensive location annotations."}
{"id": "2502.00043", "pdf": "https://arxiv.org/pdf/2502.00043", "abs": "https://arxiv.org/abs/2502.00043", "authors": ["Hao Lyu", "Yanyong Guo", "Pan Liu", "Nan Zheng", "Ting Wang", "Quansheng Yue"], "title": "Mitigating Traffic Oscillations in Mixed Traffic Flow with Scalable Deep Koopman Predictive Control", "categories": ["eess.SY", "cs.AI", "cs.SY"], "comment": null, "summary": "The use of connected automated vehicle (CAV) is advocated to mitigate traffic\noscillations in mixed traffic flow consisting of CAVs and human driven vehicles\n(HDVs). This study proposes an adaptive deep Koopman predictive control\nframework (AdapKoopPC) for regulating mixed traffic flow. Firstly, a Koopman\ntheory-based adaptive trajectory prediction deep network (AdapKoopnet) is\ndesigned for modeling HDVs car-following behavior. AdapKoopnet enables the\nrepresentation of HDVs behavior by a linear model in a high-dimensional space.\nSecondly, the model predictive control is employed to smooth the mixed traffic\nflow, where the combination of the linear dynamic model of CAVs and linear\nprediction blocks from AdapKoopnet is embedded as the predictive model into the\nAdapKoopPC. Finally, the predictive performance of the prosed AdapKoopnet is\nverified using the HighD naturalistic driving dataset. Furthermore, the control\nperformance of AdapKoopPC is validated by the numerical simulations. Results\ndemonstrate that the AdapKoopnet provides more accuracy HDVs predicted\ntrajectories than the baseline nonlinear models. Moreover, the proposed\nAdapKoopPC exhibits more effective control performance with less computation\ncost compared with baselines in mitigating traffic oscillations, especially at\nthe low CAVs penetration rates. The code of proposed AdapKoopPC is open source."}
{"id": "2108.05974", "pdf": "https://arxiv.org/pdf/2108.05974", "abs": "https://arxiv.org/abs/2108.05974", "authors": ["Saber Malekmohammadi", "Kiarash Shaloudegi", "Zeou Hu", "Yaoliang Yu"], "title": "An Operator Splitting View of Federated Learning", "categories": ["cs.LG"], "comment": "30 pages, 28 figures", "summary": "Over the past few years, the federated learning ($\\texttt{FL}$) community has\nwitnessed a proliferation of new $\\texttt{FL}$ algorithms. However, our\nunderstating of the theory of $\\texttt{FL}$ is still fragmented, and a\nthorough, formal comparison of these algorithms remains elusive. Motivated by\nthis gap, we show that many of the existing $\\texttt{FL}$ algorithms can be\nunderstood from an operator splitting point of view. This unification allows us\nto compare different algorithms with ease, to refine previous convergence\nresults and to uncover new algorithmic variants. In particular, our analysis\nreveals the vital role played by the step size in $\\texttt{FL}$ algorithms. The\nunification also leads to a streamlined and economic way to accelerate\n$\\texttt{FL}$ algorithms, without incurring any communication overhead. We\nperform numerical experiments on both convex and nonconvex models to validate\nour findings."}
{"id": "2501.02811", "pdf": "https://arxiv.org/pdf/2501.02811", "abs": "https://arxiv.org/abs/2501.02811", "authors": ["Bin Wang", "Li Jing"], "title": "First-place Solution for Streetscape Shop Sign Recognition Competition", "categories": ["cs.CV"], "comment": "technical report", "summary": "Text recognition technology applied to street-view storefront signs is\nincreasingly utilized across various practical domains, including map\nnavigation, smart city planning analysis, and business value assessments in\ncommercial districts. This technology holds significant research and commercial\npotential. Nevertheless, it faces numerous challenges. Street view images often\ncontain signboards with complex designs and diverse text styles, complicating\nthe text recognition process. A notable advancement in this field was\nintroduced by our team in a recent competition. We developed a novel multistage\napproach that integrates multimodal feature fusion, extensive self-supervised\ntraining, and a Transformer-based large model. Furthermore, innovative\ntechniques such as BoxDQN, which relies on reinforcement learning, and text\nrectification methods were employed, leading to impressive outcomes.\nComprehensive experiments have validated the effectiveness of these methods,\nshowcasing our potential to enhance text recognition capabilities in complex\nurban environments."}
{"id": "2502.00443", "pdf": "https://arxiv.org/pdf/2502.00443", "abs": "https://arxiv.org/abs/2502.00443", "authors": ["Cédric Join", "Emmanuel Delaleau", "Michel Fliess"], "title": "Model-Free Predictive Control: Introductory Algebraic Calculations, and a Comparison with HEOL and ANNs", "categories": ["eess.SY", "cs.AI", "cs.SY", "49J99", "I.2.8"], "comment": "Joint IFAC Conference: SSSC, TDS, COSY -- Gif-sur-Vette, France, 30\n  June-2 July 2025", "summary": "Model predictive control (MPC) is a popular control engineering practice, but\nrequires a sound knowledge of the model. Model-free predictive control (MFPC),\na burning issue today, also related to reinforcement learning (RL) in AI, is\nreformulated here via a linear differential equation with constant\ncoefficients, thanks to a new perspective on optimal control combined with\nrecent advances in the field of model-free control (MFC). It is replacing\nDynamic Programming, the Hamilton-Jacobi-Bellman equation, and Pontryagin's\nMaximum Principle. The computing burden is low. The implementation is\nstraightforward. Two nonlinear examples, a chemical reactor and a two tank\nsystem, are illustrating our approach. A comparison with the HEOL setting,\nwhere some expertise of the process model is needed, shows only a slight\nsuperiority of the later. A recent identification of the two tank system via a\ncomplex ANN architecture might indicate that a full modeling and the\ncorresponding machine learning mechanism are not always necessary neither in\ncontrol, nor, more generally, in AI."}
{"id": "2308.04404", "pdf": "https://arxiv.org/pdf/2308.04404", "abs": "https://arxiv.org/abs/2308.04404", "authors": ["Sajjad Emdadi Mahdimahalleh"], "title": "Revolutionizing Wireless Networks with Federated Learning: A Comprehensive Review", "categories": ["cs.LG", "cs.NI"], "comment": null, "summary": "These days with the rising computational capabilities of wireless user\nequipment such as smart phones, tablets, and vehicles, along with growing\nconcerns about sharing private data, a novel machine learning model called\nfederated learning (FL) has emerged. FL enables the separation of data\nacquisition and computation at the central unit, which is different from\ncentralized learning that occurs in a data center. FL is typically used in a\nwireless edge network where communication resources are limited and unreliable.\nBandwidth constraints necessitate scheduling only a subset of UEs for updates\nin each iteration, and because the wireless medium is shared, transmissions are\nsusceptible to interference and are not assured. The article discusses the\nsignificance of Machine Learning in wireless communication and highlights\nFederated Learning (FL) as a novel approach that could play a vital role in\nfuture mobile networks, particularly 6G and beyond."}
{"id": "2501.03173", "pdf": "https://arxiv.org/pdf/2501.03173", "abs": "https://arxiv.org/abs/2501.03173", "authors": ["Alexandru Buburuzan", "Anuj Sharma", "John Redford", "Puneet K. Dokania", "Romain Mueller"], "title": "MObI: Multimodal Object Inpainting Using Diffusion Models", "categories": ["cs.CV"], "comment": "8 pages; Project page at https://alexbubu.com/mobi", "summary": "Safety-critical applications, such as autonomous driving, require extensive\nmultimodal data for rigorous testing. Methods based on synthetic data are\ngaining prominence due to the cost and complexity of gathering real-world data\nbut require a high degree of realism and controllability in order to be useful.\nThis paper introduces MObI, a novel framework for Multimodal Object Inpainting\nthat leverages a diffusion model to create realistic and controllable object\ninpaintings across perceptual modalities, demonstrated for both camera and\nlidar simultaneously. Using a single reference RGB image, MObI enables objects\nto be seamlessly inserted into existing multimodal scenes at a 3D location\nspecified by a bounding box, while maintaining semantic consistency and\nmultimodal coherence. Unlike traditional inpainting methods that rely solely on\nedit masks, our 3D bounding box conditioning gives objects accurate spatial\npositioning and realistic scaling. As a result, our approach can be used to\ninsert novel objects flexibly into multimodal scenes, providing significant\nadvantages for testing perception models."}
{"id": "2502.09436", "pdf": "https://arxiv.org/pdf/2502.09436", "abs": "https://arxiv.org/abs/2502.09436", "authors": ["Dario Spoljaric", "Yashuai Yan", "Dongheui Lee"], "title": "Variable Stiffness for Robust Locomotion through Reinforcement Learning", "categories": ["cs.RO", "cs.AI"], "comment": "accepted to IFAC Joint Symposia on Mechatronics & Robotics", "summary": "Reinforcement-learned locomotion enables legged robots to perform highly\ndynamic motions but often accompanies time-consuming manual tuning of joint\nstiffness. This paper introduces a novel control paradigm that integrates\nvariable stiffness into the action space alongside joint positions, enabling\ngrouped stiffness control such as per-joint stiffness (PJS), per-leg stiffness\n(PLS) and hybrid joint-leg stiffness (HJLS). We show that variable stiffness\npolicies, with grouping in per-leg stiffness (PLS), outperform position-based\ncontrol in velocity tracking and push recovery. In contrast, HJLS excels in\nenergy efficiency. Despite the fact that our policy is trained on flat floor\nonly, our method showcases robust walking behaviour on diverse outdoor\nterrains, indicating robust sim-to-real transfer. Our approach simplifies\ndesign by eliminating per-joint stiffness tuning while keeping competitive\nresults with various metrics."}
{"id": "2312.10235", "pdf": "https://arxiv.org/pdf/2312.10235", "abs": "https://arxiv.org/abs/2312.10235", "authors": ["Carlos E. Pérez De Jesús", "Alec J. Linot", "Michael D. Graham"], "title": "Building symmetries into data-driven manifold dynamics models for complex flows: application to two-dimensional Kolmogorov flow", "categories": ["cs.LG", "nlin.CD"], "comment": null, "summary": "Data-driven reduced-order models of the dynamics of complex flows are\nimportant for tasks related to design, understanding, prediction, and control.\nMany flows obey symmetries, and the present work illustrates how these can be\nexploited to yield highly efficient low-dimensional data-driven models for\nchaotic flows. In particular, incorporating symmetries both guarantees that the\nreduced order model automatically respects them and dramatically increases the\neffective density of data sampling. Given data for the long-time dynamics of a\nsystem, and knowing the set of continuous and discrete symmetries it obeys, the\nfirst step in the methodology is to identify a \"fundamental chart\", a region in\nthe state space of the flow to which all other regions can be mapped by a\nsymmetry operation, and a set of criteria indicating what mapping takes each\npoint in state space into that chart. We then find a low-dimensional coordinate\nrepresentation of the data in the fundamental chart with the use of an\nautoencoder architecture that also provides an estimate of the dimension of the\ninvariant manifold where data lie. Finally, we learn dynamics on this manifold\nwith the use of neural ordinary differential equations. We apply this method,\ndenoted \"symmetry charting\" to simulation data from two-dimensional Kolmogorov\nflow in a chaotic bursting regime. This system has a continuous translation\nsymmetry, and discrete rotation and shift-reflect symmetries. With this\nframework we observe that less data is needed to learn accurate data-driven\nmodels, more robust estimates of the manifold dimension are obtained,\nequivariance of the NSE is satisfied, better short-time tracking with respect\nto the true data is observed, and long-time statistics are correctly captured."}
{"id": "2501.10640", "pdf": "https://arxiv.org/pdf/2501.10640", "abs": "https://arxiv.org/abs/2501.10640", "authors": ["Dhruv Parikh", "Jacob Fein-Ashley", "Tian Ye", "Rajgopal Kannan", "Viktor Prasanna"], "title": "ClusterViG: Efficient Globally Aware Vision GNNs via Image Partitioning", "categories": ["cs.CV", "cs.DC"], "comment": "IEEE MCNA 2025", "summary": "Convolutional Neural Networks (CNN) and Vision Transformers (ViT) have\ndominated the field of Computer Vision (CV). Graph Neural Networks (GNN) have\nperformed remarkably well across diverse domains because they can represent\ncomplex relationships via unstructured graphs. However, the applicability of\nGNNs for visual tasks was unexplored till the introduction of Vision GNNs\n(ViG). Despite the success of ViGs, their performance is severely bottlenecked\ndue to the expensive $k$-Nearest Neighbors ($k$-NN) based graph construction.\nRecent works addressing this bottleneck impose constraints on the flexibility\nof GNNs to build unstructured graphs, undermining their core advantage while\nintroducing additional inefficiencies. To address these issues, in this paper,\nwe propose a novel method called Dynamic Efficient Graph Convolution (DEGC) for\ndesigning efficient and globally aware ViGs. DEGC partitions the input image\nand constructs graphs in parallel for each partition, improving graph\nconstruction efficiency. Further, DEGC integrates local intra-graph and global\ninter-graph feature learning, enabling enhanced global context awareness. Using\nDEGC as a building block, we propose a novel CNN-GNN architecture, ClusterViG,\nfor CV tasks. Extensive experiments indicate that ClusterViG reduces end-to-end\ninference latency for vision tasks by up to $5\\times$ when compared against a\nsuite of models such as ViG, ViHGNN, PVG, and GreedyViG, with a similar model\nparameter count. Additionally, ClusterViG reaches state-of-the-art performance\non image classification, object detection, and instance segmentation tasks,\ndemonstrating the effectiveness of the proposed globally aware learning\nstrategy. Finally, input partitioning performed by DEGC enables ClusterViG to\nbe trained efficiently on higher-resolution images, underscoring the\nscalability of our approach."}
{"id": "2502.13055", "pdf": "https://arxiv.org/pdf/2502.13055", "abs": "https://arxiv.org/abs/2502.13055", "authors": ["Xingzhi Qian", "Xinran Zheng", "Yiling He", "Shuo Yang", "Lorenzo Cavallaro"], "title": "LAMD: Context-driven Android Malware Detection and Classification with LLMs", "categories": ["cs.CR", "cs.AI", "cs.LG"], "comment": "accepted by 2025 46th IEEE Symposium on Security and Privacy\n  Workshops (SPW)", "summary": "The rapid growth of mobile applications has escalated Android malware\nthreats. Although there are numerous detection methods, they often struggle\nwith evolving attacks, dataset biases, and limited explainability. Large\nLanguage Models (LLMs) offer a promising alternative with their zero-shot\ninference and reasoning capabilities. However, applying LLMs to Android malware\ndetection presents two key challenges: (1)the extensive support code in Android\napplications, often spanning thousands of classes, exceeds LLMs' context limits\nand obscures malicious behavior within benign functionality; (2)the structural\ncomplexity and interdependencies of Android applications surpass LLMs'\nsequence-based reasoning, fragmenting code analysis and hindering malicious\nintent inference. To address these challenges, we propose LAMD, a practical\ncontext-driven framework to enable LLM-based Android malware detection. LAMD\nintegrates key context extraction to isolate security-critical code regions and\nconstruct program structures, then applies tier-wise code reasoning to analyze\napplication behavior progressively, from low-level instructions to high-level\nsemantics, providing final prediction and explanation. A well-designed factual\nconsistency verification mechanism is equipped to mitigate LLM hallucinations\nfrom the first tier. Evaluation in real-world settings demonstrates LAMD's\neffectiveness over conventional detectors, establishing a feasible basis for\nLLM-driven malware analysis in dynamic threat landscapes."}
{"id": "2405.18100", "pdf": "https://arxiv.org/pdf/2405.18100", "abs": "https://arxiv.org/abs/2405.18100", "authors": ["Onno Eberhard", "Claire Vernade", "Michael Muehlebach"], "title": "A Pontryagin Perspective on Reinforcement Learning", "categories": ["cs.LG", "math.OC"], "comment": null, "summary": "Reinforcement learning has traditionally focused on learning state-dependent\npolicies to solve optimal control problems in a closed-loop fashion. In this\nwork, we introduce the paradigm of open-loop reinforcement learning where a\nfixed action sequence is learned instead. We present three new algorithms: one\nrobust model-based method and two sample-efficient model-free methods. Rather\nthan basing our algorithms on Bellman's equation from dynamic programming, our\nwork builds on Pontryagin's principle from the theory of open-loop optimal\ncontrol. We provide convergence guarantees and evaluate all methods empirically\non a pendulum swing-up task, as well as on two high-dimensional MuJoCo tasks,\nsignificantly outperforming existing baselines."}
{"id": "2502.20650", "pdf": "https://arxiv.org/pdf/2502.20650", "abs": "https://arxiv.org/abs/2502.20650", "authors": ["Yu Pan", "Bingrong Dai", "Jiahao Chen", "Lin Wang", "Yi Du", "Jiao Liu"], "title": "Gungnir: Exploiting Stylistic Features in Images for Backdoor Attacks on Diffusion Models", "categories": ["cs.CV", "cs.CR"], "comment": null, "summary": "In recent years, Diffusion Models (DMs) have demonstrated significant\nadvances in the field of image generation. However, according to current\nresearch, DMs are vulnerable to backdoor attacks, which allow attackers to\ncontrol the model's output by inputting data containing covert triggers, such\nas a specific visual patch or phrase. Existing defense strategies are well\nequipped to thwart such attacks through backdoor detection and trigger\ninversion because previous attack methods are constrained by limited input\nspaces and low-dimensional triggers. For example, visual triggers are easily\nobserved by defenders, text-based or attention-based triggers are more\nsusceptible to neural network detection. To explore more possibilities of\nbackdoor attack in DMs, we propose Gungnir, a novel method that enables\nattackers to activate the backdoor in DMs through style triggers within input\nimages. Our approach proposes using stylistic features as triggers for the\nfirst time and implements backdoor attacks successfully in image-to-image tasks\nby introducing Reconstructing-Adversarial Noise (RAN) and Short-Term\nTimesteps-Retention (STTR). Our technique generates trigger-embedded images\nthat are perceptually indistinguishable from clean images, thus bypassing both\nmanual inspection and automated detection neural networks. Experiments\ndemonstrate that Gungnir can easily bypass existing defense methods. Among\nexisting DM defense frameworks, our approach achieves a 0 backdoor detection\nrate (BDR). Our codes are available at https://github.com/paoche11/Gungnir."}
{"id": "2503.01411", "pdf": "https://arxiv.org/pdf/2503.01411", "abs": "https://arxiv.org/abs/2503.01411", "authors": ["Peng Yan", "Ahmed Abdulkadir", "Gerrit A. Schatte", "Giulia Aguzzi", "Joonsu Gha", "Nikola Pascher", "Matthias Rosenthal", "Yunlong Gao", "Benjamin F. Grewe", "Thilo Stadelmann"], "title": "Learning Actionable World Models for Industrial Process Control", "categories": ["cs.LG", "cs.AI", "cs.SY", "eess.SY", "I.2.0; I.2.4"], "comment": "Accepted by SDS 2025", "summary": "To go from (passive) process monitoring to active process control, an\neffective AI system must learn about the behavior of the complex system from\nvery limited training data, forming an ad-hoc digital twin with respect to\nprocess inputs and outputs that captures the consequences of actions on the\nprocess's world. We propose a novel methodology based on learning world models\nthat disentangles process parameters in the learned latent representation,\nallowing for fine-grained control. Representation learning is driven by the\nlatent factors influencing the processes through contrastive learning within a\njoint embedding predictive architecture. This makes changes in representations\npredictable from changes in inputs and vice versa, facilitating\ninterpretability of key factors responsible for process variations, paving the\nway for effective control actions to keep the process within operational\nbounds. The effectiveness of our method is validated on the example of plastic\ninjection molding, demonstrating practical relevance in proposing specific\ncontrol actions for a notoriously unstable process."}
{"id": "2406.02584", "pdf": "https://arxiv.org/pdf/2406.02584", "abs": "https://arxiv.org/abs/2406.02584", "authors": ["Kazuki Sakamoto", "Connor T. Jerzak", "Adel Daoud"], "title": "A Scoping Review of Earth Observation and Machine Learning for Causal Inference: Implications for the Geography of Poverty", "categories": ["cs.LG", "cs.CV", "stat.ME", "stat.ML", "62H11", "I.2.6; I.5.4"], "comment": "To appear as: Sakamoto, Kazuki, Connor T. Jerzak, and Adel Daoud. \"A\n  Scoping Review of Earth Observation and Machine Learning for Causal\n  Inference: Implications for the Geography of Poverty.\" In Geography of\n  Poverty, edited by Ola Hall and Ibrahim Wahab. Edward Elgar Publishing\n  (Cheltenham, UK), 2025", "summary": "Earth observation (EO) data such as satellite imagery can have far-reaching\nimpacts on our understanding of the geography of poverty, especially when\ncoupled with machine learning (ML) and computer vision. Early research used\ncomputer vision to predict living conditions in areas with limited data, but\nrecent studies increasingly focus on causal analysis. Despite this shift, the\nuse of EO-ML methods for causal inference lacks thorough documentation, and\nbest practices are still developing. Through a comprehensive scoping review, we\ncatalog the current literature on EO-ML methods in causal analysis. We\nsynthesize five principal approaches to incorporating EO data in causal\nworkflows: (1) outcome imputation for downstream causal analysis, (2) EO image\ndeconfounding, (3) EO-based treatment effect heterogeneity, (4) EO-based\ntransportability analysis, and (5) image-informed causal discovery. Building on\nthese findings, we provide a detailed protocol guiding researchers in\nintegrating EO data into causal analysis -- covering data requirements,\ncomputer vision model selection, and evaluation metrics. While our focus\ncenters on health and living conditions outcomes, our protocol is adaptable to\nother sustainable development domains utilizing EO data."}
{"id": "2503.06223", "pdf": "https://arxiv.org/pdf/2503.06223", "abs": "https://arxiv.org/abs/2503.06223", "authors": ["Ruofan Wang", "Xiang Zheng", "Xiaosen Wang", "Cong Wang", "Xingjun Ma"], "title": "Red Team Diffuser: Exposing Toxic Continuation Vulnerabilities in Vision-Language Models via Reinforcement Learning", "categories": ["cs.CV"], "comment": null, "summary": "The growing deployment of large Vision-Language Models (VLMs) exposes\ncritical safety gaps in their alignment mechanisms. While existing jailbreak\nstudies primarily focus on VLMs' susceptibility to harmful instructions, we\nreveal a fundamental yet overlooked vulnerability: toxic text continuation,\nwhere VLMs produce highly toxic completions when prompted with harmful text\nprefixes paired with semantically adversarial images. To systematically study\nthis threat, we propose Red Team Diffuser (RTD), the first red teaming\ndiffusion model that coordinates adversarial image generation and toxic\ncontinuation through reinforcement learning. Our key innovations include\ndynamic cross-modal attack and stealth-aware optimization. For toxic text\nprefixes from an LLM safety benchmark, we conduct greedy search to identify\noptimal image prompts that maximally induce toxic completions. The discovered\nimage prompts then drive RL-based diffusion model fine-tuning, producing\nsemantically aligned adversarial images that boost toxicity rates.\nStealth-aware optimization introduces joint adversarial rewards that balance\ntoxicity maximization (via Detoxify classifier) and stealthiness (via\nBERTScore), circumventing traditional noise-based adversarial patterns.\nExperimental results demonstrate the effectiveness of RTD, increasing the\ntoxicity rate of LLaVA outputs by 10.69% over text-only baselines on the\noriginal attack set and 8.91% on an unseen set, proving generalization\ncapability. Moreover, RTD exhibits strong cross-model transferability, raising\nthe toxicity rate by 5.1% on Gemini and 26.83% on LLaMA. Our findings expose\ntwo critical flaws in current VLM alignment: (1) failure to prevent toxic\ncontinuation from harmful prefixes, and (2) overlooking cross-modal attack\nvectors. These results necessitate a paradigm shift toward multimodal red\nteaming in safety evaluations."}
{"id": "2503.03563", "pdf": "https://arxiv.org/pdf/2503.03563", "abs": "https://arxiv.org/abs/2503.03563", "authors": ["Florian Plötzky", "Katarina Britz", "Wolf-Tilo Balke"], "title": "A Conceptual Model for Attributions in Event-Centric Knowledge Graphs", "categories": ["cs.DB", "cs.AI"], "comment": "Accepted by Data & Knowledge Engineering, 22 pages, 9 figures", "summary": "The use of narratives as a means of fusing information from knowledge graphs\n(KGs) into a coherent line of argumentation has been the subject of recent\ninvestigation. Narratives are especially useful in event-centric knowledge\ngraphs in that they provide a means to connect different real-world events and\ncategorize them by well-known narrations. However, specifically for\ncontroversial events, a problem in information fusion arises, namely, multiple\nviewpoints regarding the validity of certain event aspects, e.g., regarding the\nrole a participant takes in an event, may exist. Expressing those viewpoints in\nKGs is challenging because disputed information provided by different\nviewpoints may introduce inconsistencies. Hence, most KGs only feature a single\nview on the contained information, hampering the effectiveness of narrative\ninformation access. This paper is an extension of our original work and\nintroduces attributions, i.e., parameterized predicates that allow for the\nrepresentation of facts that are only valid in a specific viewpoint. For this,\nwe develop a conceptual model that allows for the representation of\nviewpoint-dependent information. As an extension, we enhance the model by a\nconception of viewpoint-compatibility. Based on this, we deepen our original\ndeliberations on the model's effects on information fusion and provide\nadditional grounding in the literature."}
{"id": "2406.14388", "pdf": "https://arxiv.org/pdf/2406.14388", "abs": "https://arxiv.org/abs/2406.14388", "authors": ["Oisin Nolan", "Tristan S. W. Stevens", "Wessel L. van Nierop", "Ruud J. G. van Sloun"], "title": "Active Diffusion Subsampling", "categories": ["cs.LG"], "comment": "27 pages, 16 figures", "summary": "Subsampling is commonly used to mitigate costs associated with data\nacquisition, such as time or energy requirements, motivating the development of\nalgorithms for estimating the fully-sampled signal of interest $x$ from\npartially observed measurements $y$. In maximum entropy sampling, one selects\nmeasurement locations that are expected to have the highest entropy, so as to\nminimize uncertainty about $x$. This approach relies on an accurate model of\nthe posterior distribution over future measurements, given the measurements\nobserved so far. Recently, diffusion models have been shown to produce\nhigh-quality posterior samples of high-dimensional signals using guided\ndiffusion. In this work, we propose Active Diffusion Subsampling (ADS), a\nmethod for designing intelligent subsampling masks using guided diffusion in\nwhich the model tracks a distribution of beliefs over the true state of $x$\nthroughout the reverse diffusion process, progressively decreasing its\nuncertainty by actively choosing to acquire measurements with maximum expected\nentropy, ultimately producing the posterior distribution $p(x \\mid y)$. ADS can\nbe applied using pre-trained diffusion models for any subsampling rate, and\ndoes not require task-specific retraining - just the specification of a\nmeasurement model. Furthermore, the maximum entropy sampling policy employed by\nADS is interpretable, enhancing transparency relative to existing methods using\nblack-box policies. Code is available at\nhttps://active-diffusion-subsampling.github.io/."}
{"id": "2503.21979", "pdf": "https://arxiv.org/pdf/2503.21979", "abs": "https://arxiv.org/abs/2503.21979", "authors": ["Size Wu", "Wenwei Zhang", "Lumin Xu", "Sheng Jin", "Zhonghua Wu", "Qingyi Tao", "Wentao Liu", "Wei Li", "Chen Change Loy"], "title": "Harmonizing Visual Representations for Unified Multimodal Understanding and Generation", "categories": ["cs.CV"], "comment": null, "summary": "Unifying visual understanding and generation within a single multimodal\nframework remains a significant challenge, as the two inherently heterogeneous\ntasks require representations at different levels of granularity. Current\napproaches that utilize vector quantization (VQ) or variational autoencoders\n(VAE) for unified visual representation prioritize intrinsic imagery features\nover semantics, compromising understanding performance. In this work, we take\ninspiration from masked image modelling (MIM) that learns rich semantics via a\nmask-and-reconstruct pre-training and its successful extension to masked\nautoregressive (MAR) image generation. A preliminary study on the MAR encoder's\nrepresentation reveals exceptional linear probing accuracy and precise feature\nresponse to visual concepts, which indicates MAR's potential for visual\nunderstanding tasks beyond its original generation role. Based on these\ninsights, we present \\emph{Harmon}, a unified autoregressive framework that\nharmonizes understanding and generation tasks with a shared MAR encoder.\nThrough a three-stage training procedure that progressively optimizes\nunderstanding and generation capabilities, Harmon achieves state-of-the-art\nimage generation results on the GenEval, MJHQ30K and WISE benchmarks while\nmatching the performance of methods with dedicated semantic encoders (e.g.,\nJanus) on image understanding benchmarks. Our code and models will be available\nat https://github.com/wusize/Harmon."}
{"id": "2503.11917", "pdf": "https://arxiv.org/pdf/2503.11917", "abs": "https://arxiv.org/abs/2503.11917", "authors": ["Mikel Rodriguez", "Raluca Ada Popa", "Four Flynn", "Lihao Liang", "Allan Dafoe", "Anna Wang"], "title": "A Framework for Evaluating Emerging Cyberattack Capabilities of AI", "categories": ["cs.CR", "cs.AI"], "comment": null, "summary": "As frontier AI models become more capable, evaluating their potential to\nenable cyberattacks is crucial for ensuring the safe development of Artificial\nGeneral Intelligence (AGI). Current cyber evaluation efforts are often ad-hoc,\nlacking systematic analysis of attack phases and guidance on targeted defenses.\nThis work introduces a novel evaluation framework that addresses these\nlimitations by: (1) examining the end-to-end attack chain, (2) identifying gaps\nin AI threat evaluation, and (3) helping defenders prioritize targeted\nmitigations and conduct AI-enabled adversary emulation for red teaming. Our\napproach adapts existing cyberattack chain frameworks for AI systems. We\nanalyzed over 12,000 real-world instances of AI involvement in cyber incidents,\ncatalogued by Google's Threat Intelligence Group, to curate seven\nrepresentative attack chain archetypes. Through a bottleneck analysis on these\narchetypes, we pinpointed phases most susceptible to AI-driven disruption. We\nthen identified and utilized externally developed cybersecurity model\nevaluations focused on these critical phases. We report on AI's potential to\namplify offensive capabilities across specific attack stages, and offer\nrecommendations for prioritizing defenses. We believe this represents the most\ncomprehensive AI cyber risk evaluation framework published to date."}
{"id": "2406.19711", "pdf": "https://arxiv.org/pdf/2406.19711", "abs": "https://arxiv.org/abs/2406.19711", "authors": ["Ziming Zhao", "Zhenwei Wang", "Tiehua Zhang", "Zhishu Shen", "Hai Dong", "Zhen Lei", "Xingjun Ma", "Gaowei Xu", "Zhijun Ding", "Yun Yang"], "title": "CHASE: A Causal Hypergraph based Framework for Root Cause Analysis in Multimodal Microservice Systems", "categories": ["cs.LG"], "comment": null, "summary": "In recent years, the widespread adoption of distributed microservice\narchitectures within the industry has significantly increased the demand for\nenhanced system availability and robustness. Due to the complex service\ninvocation paths and dependencies in enterprise-level microservice systems, it\nis challenging to locate the anomalies promptly during service invocations,\nthus causing intractable issues for normal system operations and maintenance.\nIn this paper, we propose a Causal Heterogeneous grAph baSed framEwork for root\ncause analysis, namely CHASE, for microservice systems with multimodal data,\nincluding traces, logs, and system monitoring metrics. Specifically, related\ninformation is encoded into representative embeddings and further modeled by a\nmultimodal invocation graph. Following that, anomaly detection is performed on\neach instance node with attentive heterogeneous message passing from its\nadjacent metric and log nodes. Finally, CHASE learns from the constructed\nhypergraph with hyperedges representing the flow of causality and performs root\ncause localization. We evaluate the proposed framework on two public\nmicroservice datasets with distinct attributes and compare with the\nstate-of-the-art methods. The results show that CHASE achieves the average\nperformance gain up to 36.2%(A@1) and 29.4%(Percentage@1), respectively to its\nbest counterpart."}
{"id": "2504.02812", "pdf": "https://arxiv.org/pdf/2504.02812", "abs": "https://arxiv.org/abs/2504.02812", "authors": ["Van Nguyen Nguyen", "Stephen Tyree", "Andrew Guo", "Mederic Fourmy", "Anas Gouda", "Taeyeop Lee", "Sungphill Moon", "Hyeontae Son", "Lukas Ranftl", "Jonathan Tremblay", "Eric Brachmann", "Bertram Drost", "Vincent Lepetit", "Carsten Rother", "Stan Birchfield", "Jiri Matas", "Yann Labbe", "Martin Sundermeyer", "Tomas Hodan"], "title": "BOP Challenge 2024 on Model-Based and Model-Free 6D Object Pose Estimation", "categories": ["cs.CV"], "comment": "arXiv admin note: text overlap with arXiv:2403.09799", "summary": "We present the evaluation methodology, datasets and results of the BOP\nChallenge 2024, the 6th in a series of public competitions organized to capture\nthe state of the art in 6D object pose estimation and related tasks. In 2024,\nour goal was to transition BOP from lab-like setups to real-world scenarios.\nFirst, we introduced new model-free tasks, where no 3D object models are\navailable and methods need to onboard objects just from provided reference\nvideos. Second, we defined a new, more practical 6D object detection task where\nidentities of objects visible in a test image are not provided as input. Third,\nwe introduced new BOP-H3 datasets recorded with high-resolution sensors and\nAR/VR headsets, closely resembling real-world scenarios. BOP-H3 include 3D\nmodels and onboarding videos to support both model-based and model-free tasks.\nParticipants competed on seven challenge tracks. Notably, the best 2024 method\nfor model-based 6D localization of unseen objects (FreeZeV2.1) achieves 22%\nhigher accuracy on BOP-Classic-Core than the best 2023 method (GenFlow), and is\nonly 4% behind the best 2023 method for seen objects (GPose2023) although being\nsignificantly slower (24.9 vs 2.7s per image). A more practical 2024 method for\nthis task is Co-op which takes only 0.8s per image and is 13% more accurate\nthan GenFlow. Methods have similar rankings on 6D detection as on 6D\nlocalization but higher run time. On model-based 2D detection of unseen\nobjects, the best 2024 method (MUSE) achieves 21--29% relative improvement\ncompared to the best 2023 method (CNOS). However, the 2D detection accuracy for\nunseen objects is still -35% behind the accuracy for seen objects (GDet2023),\nand the 2D detection stage is consequently the main bottleneck of existing\npipelines for 6D localization/detection of unseen objects. The online\nevaluation system stays open and is available at http://bop.felk.cvut.cz/"}
{"id": "2504.02698", "pdf": "https://arxiv.org/pdf/2504.02698", "abs": "https://arxiv.org/abs/2504.02698", "authors": ["Shengrui XU", "Tianchi Lu", "Zikun Wang", "Jixiu Zhai", "Jingwan Wang"], "title": "SCMPPI: Supervised Contrastive Multimodal Framework for Predicting Protein-Protein Interactions", "categories": ["cs.LG", "cs.AI", "q-bio.QM", "92C40, 68T07", "I.2.6; J.3"], "comment": "19 pages,9 figures,conference", "summary": "Protein-protein interaction (PPI) prediction plays a pivotal role in\ndeciphering cellular functions and disease mechanisms. To address the\nlimitations of traditional experimental methods and existing computational\napproaches in cross-modal feature fusion and false-negative suppression, we\npropose SCMPPI-a novel supervised contrastive multimodal framework. By\neffectively integrating sequence-based features (AAC, DPC, ESMC-CKSAAP) with\nnetwork topology (Node2Vec embeddings) and incorporating an enhanced\ncontrastive learning strategy with negative sample filtering, SCMPPI achieves\nsuperior prediction performance. Extensive experiments on eight benchmark\ndatasets demonstrate its state-of-the-art accuracy(98.13%) and AUC(99.69%),\nalong with excellent cross-species generalization (AUC>99%). Successful\napplications in CD9 networks, Wnt pathway analysis, and cancer-specific\nnetworks further highlight its potential for disease target discovery,\nestablishing SCMPPI as a powerful tool for multimodal biological data analysis."}
{"id": "2409.15267", "pdf": "https://arxiv.org/pdf/2409.15267", "abs": "https://arxiv.org/abs/2409.15267", "authors": ["Shreyas Chaudhari", "Srinivasa Pranav", "Emile Anand", "José M. F. Moura"], "title": "Peer-to-Peer Learning Dynamics of Wide Neural Networks", "categories": ["cs.LG", "cs.SY", "eess.SY"], "comment": "Published at IEEE International Conference on Acoustics, Speech, and\n  Signal Processing (ICASSP), Hyderabad, India, 2025", "summary": "Peer-to-peer learning is an increasingly popular framework that enables\nbeyond-5G distributed edge devices to collaboratively train deep neural\nnetworks in a privacy-preserving manner without the aid of a central server.\nNeural network training algorithms for emerging environments, e.g., smart\ncities, have many design considerations that are difficult to tune in\ndeployment settings -- such as neural network architectures and\nhyperparameters. This presents a critical need for characterizing the training\ndynamics of distributed optimization algorithms used to train highly nonconvex\nneural networks in peer-to-peer learning environments. In this work, we provide\nan explicit characterization of the learning dynamics of wide neural networks\ntrained using popular distributed gradient descent (DGD) algorithms. Our\nresults leverage both recent advancements in neural tangent kernel (NTK) theory\nand extensive previous work on distributed learning and consensus. We validate\nour analytical results by accurately predicting the parameter and error\ndynamics of wide neural networks trained for classification tasks."}
{"id": "2504.06116", "pdf": "https://arxiv.org/pdf/2504.06116", "abs": "https://arxiv.org/abs/2504.06116", "authors": ["Davide Sferrazza", "Gabriele Berton", "Gabriele Trivigno", "Carlo Masone"], "title": "To Match or Not to Match: Revisiting Image Matching for Reliable Visual Place Recognition", "categories": ["cs.CV"], "comment": "CVPRW 2025", "summary": "Visual Place Recognition (VPR) is a critical task in computer vision,\ntraditionally enhanced by re-ranking retrieval results with image matching.\nHowever, recent advancements in VPR methods have significantly improved\nperformance, challenging the necessity of re-ranking. In this work, we show\nthat modern retrieval systems often reach a point where re-ranking can degrade\nresults, as current VPR datasets are largely saturated. We propose using image\nmatching as a verification step to assess retrieval confidence, demonstrating\nthat inlier counts can reliably predict when re-ranking is beneficial. Our\nfindings shift the paradigm of retrieval pipelines, offering insights for more\nrobust and adaptive VPR systems. The code is available at\nhttps://github.com/FarInHeight/To-Match-or-Not-to-Match."}
{"id": "2504.09809", "pdf": "https://arxiv.org/pdf/2504.09809", "abs": "https://arxiv.org/abs/2504.09809", "authors": ["Zhimin Li", "Haichao Miao", "Xinyuan Yan", "Valerio Pascucci", "Matthew Berger", "Shusen Liu"], "title": "See or Recall: A Sanity Check for the Role of Vision in Solving Visualization Question Answer Tasks with Multimodal LLMs", "categories": ["cs.HC", "cs.AI"], "comment": null, "summary": "Recent developments in multimodal large language models (MLLM) have equipped\nlanguage models to reason about vision and language jointly. This permits MLLMs\nto both perceive and answer questions about data visualization across a variety\nof designs and tasks. Applying MLLMs to a broad range of visualization tasks\nrequires us to properly evaluate their capabilities, and the most common way to\nconduct evaluation is through measuring a model's visualization reasoning\ncapability, analogous to how we would evaluate human understanding of\nvisualizations (e.g., visualization literacy). However, we found that in the\ncontext of visualization question answering (VisQA), how an MLLM perceives and\nreasons about visualizations can be fundamentally different from how humans\napproach the same problem. During the evaluation, even without visualization,\nthe model could correctly answer a substantial portion of the visualization\ntest questions, regardless of whether any selection options were provided. We\nhypothesize that the vast amount of knowledge encoded in the language model\npermits factual recall that supersedes the need to seek information from the\nvisual signal. It raises concerns that the current VisQA evaluation may not\nfully capture the models' visualization reasoning capabilities. To address\nthis, we propose a comprehensive sanity check framework that integrates a\nrule-based decision tree and a sanity check table to disentangle the effects of\n\"seeing\" (visual processing) and \"recall\" (reliance on prior knowledge). This\nvalidates VisQA datasets for evaluation, highlighting where models are truly\n\"seeing\", positively or negatively affected by the factual recall, or relying\non inductive biases for question answering. Our study underscores the need for\ncareful consideration in designing future visualization understanding studies\nwhen utilizing MLLMs."}
{"id": "2410.22564", "pdf": "https://arxiv.org/pdf/2410.22564", "abs": "https://arxiv.org/abs/2410.22564", "authors": ["Pedro Valdeira", "Shiqiang Wang", "Yuejie Chi"], "title": "Vertical Federated Learning with Missing Features During Training and Inference", "categories": ["cs.LG", "cs.DC", "cs.DS", "math.OC"], "comment": "Accepted to ICLR 2025", "summary": "Vertical federated learning trains models from feature-partitioned datasets\nacross multiple clients, who collaborate without sharing their local data.\nStandard approaches assume that all feature partitions are available during\nboth training and inference. Yet, in practice, this assumption rarely holds, as\nfor many samples only a subset of the clients observe their partition. However,\nnot utilizing incomplete samples during training harms generalization, and not\nsupporting them during inference limits the utility of the model. Moreover, if\nany client leaves the federation after training, its partition becomes\nunavailable, rendering the learned model unusable. Missing feature blocks are\ntherefore a key challenge limiting the applicability of vertical federated\nlearning in real-world scenarios. To address this, we propose LASER-VFL, a\nvertical federated learning method for efficient training and inference of\nsplit neural network-based models that is capable of handling arbitrary sets of\npartitions. Our approach is simple yet effective, relying on the sharing of\nmodel parameters and on task-sampling to train a family of predictors. We show\nthat LASER-VFL achieves a $\\mathcal{O}({1}/{\\sqrt{T}})$ convergence rate for\nnonconvex objectives and, under the Polyak-{\\L}ojasiewicz inequality, it\nachieves linear convergence to a neighborhood of the optimum. Numerical\nexperiments show improved performance of LASER-VFL over the baselines.\nRemarkably, this is the case even in the absence of missing features. For\nexample, for CIFAR-100, we see an improvement in accuracy of $19.3\\%$ when each\nof four feature blocks is observed with a probability of 0.5 and of $9.5\\%$\nwhen all features are observed. The code for this work is available at\nhttps://github.com/Valdeira/LASER-VFL."}
{"id": "2504.09448", "pdf": "https://arxiv.org/pdf/2504.09448", "abs": "https://arxiv.org/abs/2504.09448", "authors": ["Lin Zhu", "Xinbing Wang", "Chenghu Zhou", "Nanyang Ye"], "title": "Bayesian Cross-Modal Alignment Learning for Few-Shot Out-of-Distribution Generalization", "categories": ["cs.CV", "cs.LG"], "comment": "Accepted by AAAI2023", "summary": "Recent advances in large pre-trained models showed promising results in\nfew-shot learning. However, their generalization ability on two-dimensional\nOut-of-Distribution (OoD) data, i.e., correlation shift and diversity shift,\nhas not been thoroughly investigated. Researches have shown that even with a\nsignificant amount of training data, few methods can achieve better performance\nthan the standard empirical risk minimization method (ERM) in OoD\ngeneralization. This few-shot OoD generalization dilemma emerges as a\nchallenging direction in deep neural network generalization research, where the\nperformance suffers from overfitting on few-shot examples and OoD\ngeneralization errors. In this paper, leveraging a broader supervision source,\nwe explore a novel Bayesian cross-modal image-text alignment learning method\n(Bayes-CAL) to address this issue. Specifically, the model is designed as only\ntext representations are fine-tuned via a Bayesian modelling approach with\ngradient orthogonalization loss and invariant risk minimization (IRM) loss. The\nBayesian approach is essentially introduced to avoid overfitting the base\nclasses observed during training and improve generalization to broader unseen\nclasses. The dedicated loss is introduced to achieve better image-text\nalignment by disentangling the causal and non-casual parts of image features.\nNumerical experiments demonstrate that Bayes-CAL achieved state-of-the-art OoD\ngeneralization performances on two-dimensional distribution shifts. Moreover,\ncompared with CLIP-like models, Bayes-CAL yields more stable generalization\nperformances on unseen classes. Our code is available at\nhttps://github.com/LinLLLL/BayesCAL."}
{"id": "2504.09865", "pdf": "https://arxiv.org/pdf/2504.09865", "abs": "https://arxiv.org/abs/2504.09865", "authors": ["Isabel O. Gallegos", "Chen Shani", "Weiyan Shi", "Federico Bianchi", "Izzy Gainsburg", "Dan Jurafsky", "Robb Willer"], "title": "Labeling Messages as AI-Generated Does Not Reduce Their Persuasive Effects", "categories": ["cs.CY", "cs.AI", "cs.HC"], "comment": null, "summary": "As generative artificial intelligence (AI) enables the creation and\ndissemination of information at massive scale and speed, it is increasingly\nimportant to understand how people perceive AI-generated content. One prominent\npolicy proposal requires explicitly labeling AI-generated content to increase\ntransparency and encourage critical thinking about the information, but prior\nresearch has not yet tested the effects of such labels. To address this gap, we\nconducted a survey experiment (N=1601) on a diverse sample of Americans,\npresenting participants with an AI-generated message about several public\npolicies (e.g., allowing colleges to pay student-athletes), randomly assigning\nwhether participants were told the message was generated by (a) an expert AI\nmodel, (b) a human policy expert, or (c) no label. We found that messages were\ngenerally persuasive, influencing participants' views of the policies by 9.74\npercentage points on average. However, while 94.6% of participants assigned to\nthe AI and human label conditions believed the authorship labels, labels had no\nsignificant effects on participants' attitude change toward the policies,\njudgments of message accuracy, nor intentions to share the message with others.\nThese patterns were robust across a variety of participant characteristics,\nincluding prior knowledge of the policy, prior experience with AI, political\nparty, education level, or age. Taken together, these results imply that, while\nauthorship labels would likely enhance transparency, they are unlikely to\nsubstantially affect the persuasiveness of the labeled content, highlighting\nthe need for alternative strategies to address challenges posed by AI-generated\ninformation."}
{"id": "2411.06500", "pdf": "https://arxiv.org/pdf/2411.06500", "abs": "https://arxiv.org/abs/2411.06500", "authors": ["Agatha Schmidt", "Henrik Zunker", "Alexander Heinlein", "Martin J. Kühn"], "title": "Graph Neural Network Surrogates to leverage Mechanistic Expert Knowledge towards Reliable and Immediate Pandemic Response", "categories": ["cs.LG", "q-bio.PE", "68T07, 92B20, 92B05"], "comment": "27 pages, 9 figures", "summary": "During the COVID-19 crisis, mechanistic models have guided evidence-based\ndecision making. However, time-critical decisions in a dynamical environment\nlimit the time available to gather supporting evidence. Infectious disease\ndynamics are often heterogeneous on a spatial or demographic scale, requiring\nappropriately resolved models. In addition, with a large number of potential\ninterventions, all scenarios can barely be computed on time, even when using\nsupercomputing facilities. We suggest to couple complex mechanistic models with\ndata-driven surrogate models to allow for on-the-fly model adaptations by\npublic health experts and decision makers. We build upon a spatially and\ndemographically resolved infectious disease metapopulation model and train a\ngraph neural network for data sets representing prevaccination phases of a\npandemic. The resulting networks reached an execution time of a fraction of a\nsecond, a speeding up the metapopulation up to four orders of magnitude. The\napproach yields large potential for on-the-fly execution and, thus, facilitates\nintegration into low-barrier web applications for use in pandemic\ndecision-making."}
{"id": "2504.10852", "pdf": "https://arxiv.org/pdf/2504.10852", "abs": "https://arxiv.org/abs/2504.10852", "authors": ["Pengxiao Han", "Changkun Ye", "Jinguang Tong", "Cuicui Jiang", "Jie Hong", "Li Fang", "Xuesong Li"], "title": "Enhancing Features in Long-tailed Data Using Large Vision Model", "categories": ["cs.CV"], "comment": null, "summary": "Language-based foundation models, such as large language models (LLMs) or\nlarge vision-language models (LVLMs), have been widely studied in long-tailed\nrecognition. However, the need for linguistic data is not applicable to all\npractical tasks. In this study, we aim to explore using large vision models\n(LVMs) or visual foundation models (VFMs) to enhance long-tailed data features\nwithout any language information. Specifically, we extract features from the\nLVM and fuse them with features in the baseline network's map and latent space\nto obtain the augmented features. Moreover, we design several prototype-based\nlosses in the latent space to further exploit the potential of the augmented\nfeatures. In the experimental section, we validate our approach on two\nbenchmark datasets: ImageNet-LT and iNaturalist2018."}
{"id": "2504.13945", "pdf": "https://arxiv.org/pdf/2504.13945", "abs": "https://arxiv.org/abs/2504.13945", "authors": ["Zhanglin Wu", "Tengfei Song", "Ning Xie", "Weidong Zhang", "Mengli Zhu", "Shuang Wu", "Shiliang Sun", "Hao Yang"], "title": "Evaluating Menu OCR and Translation: A Benchmark for Aligning Human and Automated Evaluations in Large Vision-Language Models", "categories": ["cs.LG", "cs.AI"], "comment": "12 pages, 5 figures, 5 Tables", "summary": "The rapid advancement of large vision-language models (LVLMs) has\nsignificantly propelled applications in document understanding, particularly in\noptical character recognition (OCR) and multilingual translation. However,\ncurrent evaluations of LVLMs, like the widely used OCRBench, mainly focus on\nverifying the correctness of their short-text responses and long-text responses\nwith simple layout, while the evaluation of their ability to understand long\ntexts with complex layout design is highly significant but largely overlooked.\nIn this paper, we propose Menu OCR and Translation Benchmark (MOTBench), a\nspecialized evaluation framework emphasizing the pivotal role of menu\ntranslation in cross-cultural communication. MOTBench requires LVLMs to\naccurately recognize and translate each dish, along with its price and unit\nitems on a menu, providing a comprehensive assessment of their visual\nunderstanding and language processing capabilities. Our benchmark is comprised\nof a collection of Chinese and English menus, characterized by intricate\nlayouts, a variety of fonts, and culturally specific elements across different\nlanguages, along with precise human annotations. Experiments show that our\nautomatic evaluation results are highly consistent with professional human\nevaluation. We evaluate a range of publicly available state-of-the-art LVLMs,\nand through analyzing their output to identify the strengths and weaknesses in\ntheir performance, offering valuable insights to guide future advancements in\nLVLM development. MOTBench is available at https://github.com/gitwzl/MOTBench."}
{"id": "2411.07729", "pdf": "https://arxiv.org/pdf/2411.07729", "abs": "https://arxiv.org/abs/2411.07729", "authors": ["Sungyoon Kim", "Aaron Mishkin", "Mert Pilanci"], "title": "Exploring the loss landscape of regularized neural networks via convex duality", "categories": ["cs.LG"], "comment": "Updated accepted version", "summary": "We discuss several aspects of the loss landscape of regularized neural\nnetworks: the structure of stationary points, connectivity of optimal\nsolutions, path with nonincreasing loss to arbitrary global optimum, and the\nnonuniqueness of optimal solutions, by casting the problem into an equivalent\nconvex problem and considering its dual. Starting from two-layer neural\nnetworks with scalar output, we first characterize the solution set of the\nconvex problem using its dual and further characterize all stationary points.\nWith the characterization, we show that the topology of the global optima goes\nthrough a phase transition as the width of the network changes, and construct\ncounterexamples where the problem may have a continuum of optimal solutions.\nFinally, we show that the solution set characterization and connectivity\nresults can be extended to different architectures, including two-layer\nvector-valued neural networks and parallel three-layer neural networks."}
{"id": "2504.10972", "pdf": "https://arxiv.org/pdf/2504.10972", "abs": "https://arxiv.org/abs/2504.10972", "authors": ["Yihang Liu", "Lianghua He", "Ying Wen", "Longzhen Yang", "Hongzhou Chen"], "title": "AFiRe: Anatomy-Driven Self-Supervised Learning for Fine-Grained Representation in Radiographic Images", "categories": ["cs.CV"], "comment": null, "summary": "Current self-supervised methods, such as contrastive learning, predominantly\nfocus on global discrimination, neglecting the critical fine-grained anatomical\ndetails required for accurate radiographic analysis. To address this challenge,\nwe propose an Anatomy-driven self-supervised framework for enhancing\nFine-grained Representation in radiographic image analysis (AFiRe). The core\nidea of AFiRe is to align the anatomical consistency with the unique\ntoken-processing characteristics of Vision Transformer. Specifically, AFiRe\nsynergistically performs two self-supervised schemes: (i) Token-wise\nanatomy-guided contrastive learning, which aligns image tokens based on\nstructural and categorical consistency, thereby enhancing fine-grained\nspatial-anatomical discrimination; (ii) Pixel-level anomaly-removal\nrestoration, which particularly focuses on local anomalies, thereby refining\nthe learned discrimination with detailed geometrical information. Additionally,\nwe propose Synthetic Lesion Mask to enhance anatomical diversity while\npreserving intra-consistency, which is typically corrupted by traditional data\naugmentations, such as Cropping and Affine transformations. Experimental\nresults show that AFiRe: (i) provides robust anatomical discrimination,\nachieving more cohesive feature clusters compared to state-of-the-art\ncontrastive learning methods; (ii) demonstrates superior generalization,\nsurpassing 7 radiography-specific self-supervised methods in multi-label\nclassification tasks with limited labeling; and (iii) integrates fine-grained\ninformation, enabling precise anomaly detection using only image-level\nannotations."}
{"id": "2504.14320", "pdf": "https://arxiv.org/pdf/2504.14320", "abs": "https://arxiv.org/abs/2504.14320", "authors": ["Nimisha Karnatak", "Adrien Baranes", "Rob Marchant", "Huinan Zeng", "Tríona Butler", "Kristen Olson"], "title": "Expanding the Generative AI Design Space through Structured Prompting and Multimodal Interfaces", "categories": ["cs.HC", "cs.AI"], "comment": "Accepted at CHI'25 Workshop on Designing and Developing User\n  Interfaces with AI", "summary": "Text-based prompting remains the predominant interaction paradigm in\ngenerative AI, yet it often introduces friction for novice users such as small\nbusiness owners (SBOs), who struggle to articulate creative goals in\ndomain-specific contexts like advertising. Through a formative study with six\nSBOs in the United Kingdom, we identify three key challenges: difficulties in\nexpressing brand intuition through prompts, limited opportunities for\nfine-grained adjustment and refinement during and after content generation, and\nthe frequent production of generic content that lacks brand specificity. In\nresponse, we present ACAI (AI Co-Creation for Advertising and Inspiration), a\nmultimodal generative AI tool designed to support novice designers by moving\nbeyond traditional prompt interfaces. ACAI features a structured input system\ncomposed of three panels: Branding, Audience and Goals, and the Inspiration\nBoard. These inputs allow users to convey brand-relevant context and visual\npreferences. This work contributes to HCI research on generative systems by\nshowing how structured interfaces can foreground user-defined context, improve\nalignment, and enhance co-creative control in novice creative workflows."}
{"id": "2412.04404", "pdf": "https://arxiv.org/pdf/2412.04404", "abs": "https://arxiv.org/abs/2412.04404", "authors": ["Tom Overman", "Diego Klabjan"], "title": "Federated Automated Feature Engineering", "categories": ["cs.LG", "cs.DC"], "comment": "Preliminary Work", "summary": "Automated feature engineering (AutoFE) is used to automatically create new\nfeatures from original features to improve predictive performance without\nneeding significant human intervention and domain expertise. Many algorithms\nexist for AutoFE, but very few approaches exist for the federated learning (FL)\nsetting where data is gathered across many clients and is not shared between\nclients or a central server. We introduce AutoFE algorithms for the horizontal,\nvertical, and hybrid FL settings, which differ in how the data is gathered\nacross clients. To the best of our knowledge, we are the first to develop\nAutoFE algorithms for the horizontal and hybrid FL cases, and we show that the\ndownstream test scores of our federated AutoFE algorithms is close in\nperformance to the case where data is held centrally and AutoFE is performed\ncentrally."}
{"id": "2504.12157", "pdf": "https://arxiv.org/pdf/2504.12157", "abs": "https://arxiv.org/abs/2504.12157", "authors": ["Xiaojun Ye", "Chun Wang", "Yiren Song", "Sheng Zhou", "Liangcheng Li", "Jiajun Bu"], "title": "FocusedAD: Character-centric Movie Audio Description", "categories": ["cs.CV", "I.2.10"], "comment": "Code and Demo link: https://github.com/Thorin215/FocusedAD", "summary": "Movie Audio Description (AD) aims to narrate visual content during\ndialogue-free segments, particularly benefiting blind and visually impaired\n(BVI) audiences. Compared with general video captioning, AD demands\nplot-relevant narration with explicit character name references, posing unique\nchallenges in movie understanding.To identify active main characters and focus\non storyline-relevant regions, we propose FocusedAD, a novel framework that\ndelivers character-centric movie audio descriptions. It includes: (i) a\nCharacter Perception Module(CPM) for tracking character regions and linking\nthem to names; (ii) a Dynamic Prior Module(DPM) that injects contextual cues\nfrom prior ADs and subtitles via learnable soft prompts; and (iii) a Focused\nCaption Module(FCM) that generates narrations enriched with plot-relevant\ndetails and named characters. To overcome limitations in character\nidentification, we also introduce an automated pipeline for building character\nquery banks. FocusedAD achieves state-of-the-art performance on multiple\nbenchmarks, including strong zero-shot results on MAD-eval-Named and our newly\nproposed Cinepile-AD dataset. Code and data will be released at\nhttps://github.com/Thorin215/FocusedAD ."}
{"id": "2504.14411", "pdf": "https://arxiv.org/pdf/2504.14411", "abs": "https://arxiv.org/abs/2504.14411", "authors": ["Xiang Zhang", "Yongfeng Zhang"], "title": "Planet as a Brain: Towards Internet of AgentSites based on AIOS Server", "categories": ["cs.NI", "cs.AI"], "comment": null, "summary": "The internet is undergoing a historical transformation from the \"Internet of\nWebsites\" to the \"Internet of AgentSites.\" While traditional Websites served as\nthe foundation for information hosting and dissemination, a new frontier is\nemerging where AgentSites serve as the hubs of the internet, where each\nAgentSite hosts one or more AI agents that receive tasks, address them, and\ndeliver actionable solutions, marking a significant shift in the digital\nlandscape and representing the next generation of online ecosystems. Under this\nvision, AIOS, the AI Agent Operating System, serves as the server for the\ndevelopment, deployment and execution of AI agents, which is a fundamental\ninfrastructure for the Internet of Agentsites.\n  In this paper, we introduce AIOS Server, a runtime framework to host agents\nand enable global-scale collaboration among decentralized agents. AIOS Server\nprovides a communication protocol leveraging the Model Context Protocol (MCP)\nand JSON-RPC to enable agent-agent or human-agent interactions. Each AIOS node\noperates as a server to host and execute agents, while supporting peer-to-peer\ncoordination without reliance on centralized orchestration. Based on AIOS\nServer, we further present the world's first practically deployed Internet of\nAgentsites (AIOS-IoA), including AgentHub for agent registration and discovery\nand AgentChat for interactive communication, at https://planet.aios.foundation.\nThe agent discovery mechanism based on Distributed Hash Tables (DHT) and a\nGossip protocol serves as the search engine for the internet of agentsites.\nThis work provides a practical foundation for building the Internet of\nAgentsites-a new paradigm where autonomous agents become first-class citizens\nof the web. The implementation is available at\nhttps://github.com/agiresearch/AIOS.Server and will be integrated into the AIOS\nmain branch at https://github.com/agiresearch/AIOS."}
{"id": "2412.17908", "pdf": "https://arxiv.org/pdf/2412.17908", "abs": "https://arxiv.org/abs/2412.17908", "authors": ["Orson Mengara"], "title": "Trading Devil RL: Backdoor attack via Stock market, Bayesian Optimization and Reinforcement Learning", "categories": ["cs.LG", "cs.CE", "physics.comp-ph", "physics.soc-ph"], "comment": "End of data poisoning research!: Navier-stokes equations (3D;\n  update); Reinforcement Learning (RL); HFT (High Frequency Trading); Limit\n  Order Markets and backdoor attack detection", "summary": "With the rapid development of generative artificial intelligence,\nparticularly large language models a number of sub-fields of deep learning have\nmade significant progress and are now very useful in everyday applications. For\nexample,financial institutions simulate a wide range of scenarios for various\nmodels created by their research teams using reinforcement learning, both\nbefore production and after regular operations. In this work, we propose a\nbackdoor attack that focuses solely on data poisoning and a method of detection\nby dynamic systems and statistical analysis of the distribution of data. This\nparticular backdoor attack is classified as an attack without prior\nconsideration or trigger, and we name it FinanceLLMsBackRL. Our aim is to\nexamine the potential effects of large language models that use reinforcement\nlearning systems for text production or speech recognition, finance, physics,\nor the ecosystem of contemporary artificial intelligence models."}
{"id": "2504.14348", "pdf": "https://arxiv.org/pdf/2504.14348", "abs": "https://arxiv.org/abs/2504.14348", "authors": ["Le Wang", "Zonghao Ying", "Tianyuan Zhang", "Siyuan Liang", "Shengshan Hu", "Mingchuan Zhang", "Aishan Liu", "Xianglong Liu"], "title": "Manipulating Multimodal Agents via Cross-Modal Prompt Injection", "categories": ["cs.CV"], "comment": "17 pages, 5 figures", "summary": "The emergence of multimodal large language models has redefined the agent\nparadigm by integrating language and vision modalities with external data\nsources, enabling agents to better interpret human instructions and execute\nincreasingly complex tasks. However, in this work, we identify a critical yet\npreviously overlooked security vulnerability in multimodal agents: cross-modal\nprompt injection attacks. To exploit this vulnerability, we propose\nCrossInject, a novel attack framework in which attackers embed adversarial\nperturbations across multiple modalities to align with target malicious\ncontent, allowing external instructions to hijack the agent's decision-making\nprocess and execute unauthorized tasks. Our approach consists of two key\ncomponents. First, we introduce Visual Latent Alignment, where we optimize\nadversarial features to the malicious instructions in the visual embedding\nspace based on a text-to-image generative model, ensuring that adversarial\nimages subtly encode cues for malicious task execution. Subsequently, we\npresent Textual Guidance Enhancement, where a large language model is leveraged\nto infer the black-box defensive system prompt through adversarial meta\nprompting and generate an malicious textual command that steers the agent's\noutput toward better compliance with attackers' requests. Extensive experiments\ndemonstrate that our method outperforms existing injection attacks, achieving\nat least a +26.4% increase in attack success rates across diverse tasks.\nFurthermore, we validate our attack's effectiveness in real-world multimodal\nautonomous agents, highlighting its potential implications for safety-critical\napplications."}
{"id": "2504.15041", "pdf": "https://arxiv.org/pdf/2504.15041", "abs": "https://arxiv.org/abs/2504.15041", "authors": ["Shiben Liu", "Huijie Fan", "Qiang Wang", "Baojie Fan", "Yandong Tang", "Liangqiong Qu"], "title": "Distribution-aware Forgetting Compensation for Exemplar-Free Lifelong Person Re-identification", "categories": ["cs.CV", "cs.AI"], "comment": "12 pages, 5 figures", "summary": "Lifelong Person Re-identification (LReID) suffers from a key challenge in\npreserving old knowledge while adapting to new information. The existing\nsolutions include rehearsal-based and rehearsal-free methods to address this\nchallenge. Rehearsal-based approaches rely on knowledge distillation,\ncontinuously accumulating forgetting during the distillation process.\nRehearsal-free methods insufficiently learn the distribution of each domain,\nleading to forgetfulness over time. To solve these issues, we propose a novel\nDistribution-aware Forgetting Compensation (DAFC) model that explores\ncross-domain shared representation learning and domain-specific distribution\nintegration without using old exemplars or knowledge distillation. We propose a\nText-driven Prompt Aggregation (TPA) that utilizes text features to enrich\nprompt elements and guide the prompt model to learn fine-grained\nrepresentations for each instance. This can enhance the differentiation of\nidentity information and establish the foundation for domain distribution\nawareness. Then, Distribution-based Awareness and Integration (DAI) is designed\nto capture each domain-specific distribution by a dedicated expert network and\nadaptively consolidate them into a shared region in high-dimensional space. In\nthis manner, DAI can consolidate and enhance cross-domain shared representation\nlearning while alleviating catastrophic forgetting. Furthermore, we develop a\nKnowledge Consolidation Mechanism (KCM) that comprises instance-level\ndiscrimination and cross-domain consistency alignment strategies to facilitate\nmodel adaptive learning of new knowledge from the current domain and promote\nknowledge consolidation learning between acquired domain-specific\ndistributions, respectively. Experimental results show that our DAFC\noutperforms state-of-the-art methods. Our code is available at\nhttps://github.com/LiuShiBen/DAFC."}
{"id": "2501.05842", "pdf": "https://arxiv.org/pdf/2501.05842", "abs": "https://arxiv.org/abs/2501.05842", "authors": ["Bendegúz M. Györök", "Jan H. Hoekstra", "Johan Kon", "Tamás Péni", "Maarten Schoukens", "Roland Tóth"], "title": "Orthogonal projection-based regularization for efficient model augmentation", "categories": ["cs.LG", "cs.SY", "eess.SY"], "comment": "Accepted for L4DC 2025", "summary": "Deep-learning-based nonlinear system identification has shown the ability to\nproduce reliable and highly accurate models in practice. However, these\nblack-box models lack physical interpretability, and a considerable part of the\nlearning effort is often spent on capturing already expected/known behavior of\nthe system, that can be accurately described by first-principles laws of\nphysics. A potential solution is to directly integrate such prior physical\nknowledge into the model structure, combining the strengths of physics-based\nmodeling and deep-learning-based identification. The most common approach is to\nuse an additive model augmentation structure, where the physics-based and the\nmachine-learning (ML) components are connected in parallel, i.e., additively.\nHowever, such models are overparametrized, training them is challenging,\npotentially causing the physics-based part to lose interpretability. To\novercome this challenge, this paper proposes an orthogonal projection-based\nregularization technique to enhance parameter learning and even model accuracy\nin learning-based augmentation of nonlinear baseline models."}
{"id": "2504.14621", "pdf": "https://arxiv.org/pdf/2504.14621", "abs": "https://arxiv.org/abs/2504.14621", "authors": ["Zhenkui Yang", "Zeyi Huang", "Ge Wang", "Han Ding", "Tony Xiao Han", "Fei Wang"], "title": "Talk is Not Always Cheap: Promoting Wireless Sensing Models with Text Prompts", "categories": ["cs.CV"], "comment": "10 pages", "summary": "Wireless signal-based human sensing technologies, such as WiFi,\nmillimeter-wave (mmWave) radar, and Radio Frequency Identification (RFID),\nenable the detection and interpretation of human presence, posture, and\nactivities, thereby providing critical support for applications in public\nsecurity, healthcare, and smart environments. These technologies exhibit\nnotable advantages due to their non-contact operation and environmental\nadaptability; however, existing systems often fail to leverage the textual\ninformation inherent in datasets. To address this, we propose an innovative\ntext-enhanced wireless sensing framework, WiTalk, that seamlessly integrates\nsemantic knowledge through three hierarchical prompt strategies-label-only,\nbrief description, and detailed action description-without requiring\narchitectural modifications or incurring additional data costs. We rigorously\nvalidate this framework across three public benchmark datasets: XRF55 for human\naction recognition (HAR), and WiFiTAL and XRFV2 for WiFi temporal action\nlocalization (TAL). Experimental results demonstrate significant performance\nimprovements: on XRF55, accuracy for WiFi, RFID, and mmWave increases by 3.9%,\n2.59%, and 0.46%, respectively; on WiFiTAL, the average performance of WiFiTAD\nimproves by 4.98%; and on XRFV2, the mean average precision gains across\nvarious methods range from 4.02% to 13.68%. Our codes have been included in\nhttps://github.com/yangzhenkui/WiTalk."}
{"id": "2501.11268", "pdf": "https://arxiv.org/pdf/2501.11268", "abs": "https://arxiv.org/abs/2501.11268", "authors": ["Ahmad Mousavi", "Ramin Zandvakili"], "title": "Sparse L0-norm based Kernel-free Quadratic Surface Support Vector Machines", "categories": ["cs.LG", "stat.ML"], "comment": null, "summary": "Kernel-free quadratic surface support vector machine (SVM) models have gained\nsignificant attention in machine learning. However, introducing a quadratic\nclassifier increases the model's complexity by quadratically expanding the\nnumber of parameters relative to the dimensionality of the data, exacerbating\noverfitting. Hence, we propose sparse $\\ell_0$-norm based Kernel-free quadratic\nsurface SVMs, designed to mitigate overfitting and enhance interpretability.\nGiven the intractable nature of these models, we present a penalty\ndecomposition algorithm to obtain first-order optimality points efficiently. We\ndemonstrate that the subproblems in our framework either admit closed-form\nsolutions or can leverage duality theory to improve computational efficiency.\nThrough empirical evaluations on real-world datasets, we demonstrate the\nefficacy and robustness of our approach, showcasing its potential to advance\nKernel-free quadratic surface SVMs in practical applications while addressing\noverfitting concerns. All the implemented models and experiment codes are\navailable at https://github.com/raminzandvakili/L0-QSVM."}
{"id": "2504.14658", "pdf": "https://arxiv.org/pdf/2504.14658", "abs": "https://arxiv.org/abs/2504.14658", "authors": ["Jing Zhang", "Dan Guo", "Zhangbin Li", "Meng Wang"], "title": "EmoSEM: Segment and Explain Emotion Stimuli in Visual Art", "categories": ["cs.CV"], "comment": null, "summary": "This paper focuses on a key challenge in visual art understanding: given an\nart image, the model pinpoints pixel regions that trigger a specific human\nemotion, and generates linguistic explanations for the emotional arousal.\nDespite recent advances in art understanding, pixel-level emotion understanding\nstill faces a dual challenge: first, the subjectivity of emotion makes it\ndifficult for general segmentation models like SAM to adapt to emotion-oriented\nsegmentation tasks; and second, the abstract nature of art expression makes it\ndifficult for captioning models to balance pixel-level semantic understanding\nand emotion reasoning. To solve the above problems, this paper proposes the\nEmotion stimuli Segmentation and Explanation Model (EmoSEM) to endow the\nsegmentation model SAM with emotion comprehension capability. First, to enable\nthe model to perform segmentation under the guidance of emotional intent well,\nwe introduce an emotional prompt with a learnable mask token as the conditional\ninput for segmentation decoding. Then, we design an emotion projector to\nestablish the association between emotion and visual features. Next, more\nimportantly, to address emotion-visual stimuli alignment, we develop a\nlightweight prefix projector, a module that fuses the learned emotional mask\nwith the corresponding emotion into a unified representation compatible with\nthe language model. Finally, we input the joint visual, mask, and emotional\ntokens into the language model and output the emotional explanations. It\nensures that the generated interpretations remain semantically and emotionally\ncoherent with the visual stimuli. The method innovatively realizes end-to-end\nmodeling from low-level pixel features to high-level emotion interpretation,\nproviding the first interpretable fine-grained analysis framework for artistic\nemotion computing. Extensive experiments validate the effectiveness of our\nmodel."}
{"id": "2502.02150", "pdf": "https://arxiv.org/pdf/2502.02150", "abs": "https://arxiv.org/abs/2502.02150", "authors": ["Ruiqi Feng", "Tailin Wu", "Chenglei Yu", "Wenhao Deng", "Peiyan Hu"], "title": "On the Guidance of Flow Matching", "categories": ["cs.LG"], "comment": "35 pages, 7 figures", "summary": "Flow matching has shown state-of-the-art performance in various generative\ntasks, ranging from image generation to decision-making, where guided\ngeneration is pivotal. However, the guidance of flow matching is more general\nthan and thus substantially different from that of its predecessor, diffusion\nmodels. Therefore, the challenge in guidance for general flow matching remains\nlargely underexplored. In this paper, we propose the first framework of general\nguidance for flow matching. From this framework, we derive a family of guidance\ntechniques that can be applied to general flow matching. These include a new\ntraining-free asymptotically exact guidance, novel training losses for\ntraining-based guidance, and two classes of approximate guidance that cover\nclassical gradient guidance methods as special cases. We theoretically\ninvestigate these different methods to give a practical guideline for choosing\nsuitable methods in different scenarios. Experiments on synthetic datasets,\nimage inverse problems, and offline reinforcement learning demonstrate the\neffectiveness of our proposed guidance methods and verify the correctness of\nour flow matching guidance framework. Code to reproduce the experiments can be\nfound at https://github.com/AI4Science-WestlakeU/flow_guidance."}
{"id": "2504.15095", "pdf": "https://arxiv.org/pdf/2504.15095", "abs": "https://arxiv.org/abs/2504.15095", "authors": ["Mingxia Zhan", "Li Zhang", "Xiaomeng Chu", "Beibei Wang"], "title": "VistaDepth: Frequency Modulation With Bias Reweighting For Enhanced Long-Range Depth Estimation", "categories": ["cs.CV"], "comment": "8 pages, 6 figures, 4 tables", "summary": "Monocular depth estimation (MDE) aims to predict per-pixel depth values from\na single RGB image. Recent advancements have positioned diffusion models as\neffective MDE tools by framing the challenge as a conditional image generation\ntask. Despite their progress, these methods often struggle with accurately\nreconstructing distant depths, due largely to the imbalanced distribution of\ndepth values and an over-reliance on spatial-domain features. To overcome these\nlimitations, we introduce VistaDepth, a novel framework that integrates\nadaptive frequency-domain feature enhancements with an adaptive\nweight-balancing mechanism into the diffusion process. Central to our approach\nis the Latent Frequency Modulation (LFM) module, which dynamically refines\nspectral responses in the latent feature space, thereby improving the\npreservation of structural details and reducing noisy artifacts. Furthermore,\nwe implement an adaptive weighting strategy that modulates the diffusion loss\nin real-time, enhancing the model's sensitivity towards distant depth\nreconstruction. These innovations collectively result in superior depth\nperception performance across both distance and detail. Experimental\nevaluations confirm that VistaDepth achieves state-of-the-art performance among\ndiffusion-based MDE techniques, particularly excelling in the accurate\nreconstruction of distant regions."}
{"id": "2502.02417", "pdf": "https://arxiv.org/pdf/2502.02417", "abs": "https://arxiv.org/abs/2502.02417", "authors": ["Matthias Wolff", "Florian Eilers", "Xiaoyi Jiang"], "title": "CVKAN: Complex-Valued Kolmogorov-Arnold Networks", "categories": ["cs.LG"], "comment": "accepted at IEEE International Joint Conference on Neural Networks\n  (IJCNN) 2025", "summary": "In this work we propose CVKAN, a complex-valued Kolmogorov-Arnold Network\n(KAN), to join the intrinsic interpretability of KANs and the advantages of\nComplex-Valued Neural Networks (CVNNs). We show how to transfer a KAN and the\nnecessary associated mechanisms into the complex domain. To confirm that CVKAN\nmeets expectations we conduct experiments on symbolic complex-valued function\nfitting and physically meaningful formulae as well as on a more realistic\ndataset from knot theory. Our proposed CVKAN is more stable and performs on par\nor better than real-valued KANs while requiring less parameters and a shallower\nnetwork architecture, making it more explainable."}
{"id": "2504.15278", "pdf": "https://arxiv.org/pdf/2504.15278", "abs": "https://arxiv.org/abs/2504.15278", "authors": ["Hongchi Xia", "Entong Su", "Marius Memmel", "Arhan Jain", "Raymond Yu", "Numfor Mbiziwo-Tiapo", "Ali Farhadi", "Abhishek Gupta", "Shenlong Wang", "Wei-Chiu Ma"], "title": "DRAWER: Digital Reconstruction and Articulation With Environment Realism", "categories": ["cs.CV", "cs.RO"], "comment": "Project page: https://drawer-art.github.io/", "summary": "Creating virtual digital replicas from real-world data unlocks significant\npotential across domains like gaming and robotics. In this paper, we present\nDRAWER, a novel framework that converts a video of a static indoor scene into a\nphotorealistic and interactive digital environment. Our approach centers on two\nmain contributions: (i) a reconstruction module based on a dual scene\nrepresentation that reconstructs the scene with fine-grained geometric details,\nand (ii) an articulation module that identifies articulation types and hinge\npositions, reconstructs simulatable shapes and appearances and integrates them\ninto the scene. The resulting virtual environment is photorealistic,\ninteractive, and runs in real time, with compatibility for game engines and\nrobotic simulation platforms. We demonstrate the potential of DRAWER by using\nit to automatically create an interactive game in Unreal Engine and to enable\nreal-to-sim-to-real transfer for robotics applications."}
{"id": "2502.11986", "pdf": "https://arxiv.org/pdf/2502.11986", "abs": "https://arxiv.org/abs/2502.11986", "authors": ["Wooseong Jeong", "Kuk-Jin Yoon"], "title": "Selective Task Group Updates for Multi-Task Optimization", "categories": ["cs.LG"], "comment": "Accepted at ICLR 2025", "summary": "Multi-task learning enables the acquisition of task-generic knowledge by\ntraining multiple tasks within a unified architecture. However, training all\ntasks together in a single architecture can lead to performance degradation,\nknown as negative transfer, which is a main concern in multi-task learning.\nPrevious works have addressed this issue by optimizing the multi-task network\nthrough gradient manipulation or weighted loss adjustments. However, their\noptimization strategy focuses on addressing task imbalance in shared\nparameters, neglecting the learning of task-specific parameters. As a result,\nthey show limitations in mitigating negative transfer, since the learning of\nshared space and task-specific information influences each other during\noptimization. To address this, we propose a different approach to enhance\nmulti-task performance by selectively grouping tasks and updating them for each\nbatch during optimization. We introduce an algorithm that adaptively determines\nhow to effectively group tasks and update them during the learning process. To\ntrack inter-task relations and optimize multi-task networks simultaneously, we\npropose proximal inter-task affinity, which can be measured during the\noptimization process. We provide a theoretical analysis on how dividing tasks\ninto multiple groups and updating them sequentially significantly affects\nmulti-task performance by enhancing the learning of task-specific parameters.\nOur methods substantially outperform previous multi-task optimization\napproaches and are scalable to different architectures and various numbers of\ntasks."}
{"id": "2412.05053", "pdf": "https://arxiv.org/pdf/2412.05053", "abs": "https://arxiv.org/abs/2412.05053", "authors": ["Kaizhen Sun", "Jinghang Li", "Kuan Dai", "Bangyan Liao", "Wei Xiong", "Yi Zhou"], "title": "EvTTC: An Event Camera Dataset for Time-to-Collision Estimation", "categories": ["cs.RO", "cs.CV"], "comment": "8 pages, 7 figures, 5 tables", "summary": "Time-to-Collision (TTC) estimation lies in the core of the forward collision\nwarning (FCW) functionality, which is key to all Automatic Emergency Braking\n(AEB) systems. Although the success of solutions using frame-based cameras\n(e.g., Mobileye's solutions) has been witnessed in normal situations, some\nextreme cases, such as the sudden variation in the relative speed of leading\nvehicles and the sudden appearance of pedestrians, still pose significant risks\nthat cannot be handled. This is due to the inherent imaging principles of\nframe-based cameras, where the time interval between adjacent exposures\nintroduces considerable system latency to AEB. Event cameras, as a novel\nbio-inspired sensor, offer ultra-high temporal resolution and can\nasynchronously report brightness changes at the microsecond level. To explore\nthe potential of event cameras in the above-mentioned challenging cases, we\npropose EvTTC, which is, to the best of our knowledge, the first multi-sensor\ndataset focusing on TTC tasks under high-relative-speed scenarios. EvTTC\nconsists of data collected using standard cameras and event cameras, covering\nvarious potential collision scenarios in daily driving and involving multiple\ncollision objects. Additionally, LiDAR and GNSS/INS measurements are provided\nfor the calculation of ground-truth TTC. Considering the high cost of testing\nTTC algorithms on full-scale mobile platforms, we also provide a small-scale\nTTC testbed for experimental validation and data augmentation. All the data and\nthe design of the testbed are open sourced, and they can serve as a benchmark\nthat will facilitate the development of vision-based TTC techniques."}
{"id": "2503.08976", "pdf": "https://arxiv.org/pdf/2503.08976", "abs": "https://arxiv.org/abs/2503.08976", "authors": ["Zirui Gong", "Yanjun Zhang", "Leo Yu Zhang", "Zhaoxi Zhang", "Yong Xiang", "Shirui Pan"], "title": "Not All Edges are Equally Robust: Evaluating the Robustness of Ranking-Based Federated Learning", "categories": ["cs.LG", "cs.CR", "cs.DC"], "comment": "18 pages. To appear in the IEEE Symposium on Security and Privacy\n  2025", "summary": "Federated Ranking Learning (FRL) is a state-of-the-art FL framework that\nstands out for its communication efficiency and resilience to poisoning\nattacks. It diverges from the traditional FL framework in two ways: 1) it\nleverages discrete rankings instead of gradient updates, significantly reducing\ncommunication costs and limiting the potential space for malicious updates, and\n2) it uses majority voting on the server side to establish the global ranking,\nensuring that individual updates have minimal influence since each client\ncontributes only a single vote. These features enhance the system's scalability\nand position FRL as a promising paradigm for FL training.\n  However, our analysis reveals that FRL is not inherently robust, as certain\nedges are particularly vulnerable to poisoning attacks. Through a theoretical\ninvestigation, we prove the existence of these vulnerable edges and establish a\nlower bound and an upper bound for identifying them in each layer. Based on\nthis finding, we introduce a novel local model poisoning attack against FRL,\nnamely the Vulnerable Edge Manipulation (VEM) attack. The VEM attack focuses on\nidentifying and perturbing the most vulnerable edges in each layer and\nleveraging an optimization-based approach to maximize the attack's impact.\nThrough extensive experiments on benchmark datasets, we demonstrate that our\nattack achieves an overall 53.23% attack impact and is 3.7x more impactful than\nexisting methods. Our findings highlight significant vulnerabilities in\nranking-based FL systems and underline the urgency for the development of new\nrobust FL frameworks."}
{"id": "2504.07677", "pdf": "https://arxiv.org/pdf/2504.07677", "abs": "https://arxiv.org/abs/2504.07677", "authors": ["Hye-Min Won", "Jieun Lee", "Jiyong Oh"], "title": "Localization Meets Uncertainty: Uncertainty-Aware Multi-Modal Localization", "categories": ["cs.RO", "cs.CV"], "comment": "13 pages, 6 figures", "summary": "Reliable localization is critical for robot navigation in complex indoor\nenvironments. In this paper, we propose an uncertainty-aware localization\nmethod that enhances the reliability of localization outputs without modifying\nthe prediction model itself. This study introduces a percentile-based rejection\nstrategy that filters out unreliable 3-DoF pose predictions based on aleatoric\nand epistemic uncertainties the network estimates. We apply this approach to a\nmulti-modal end-to-end localization that fuses RGB images and 2D LiDAR data,\nand we evaluate it across three real-world datasets collected using a\ncommercialized serving robot. Experimental results show that applying stricter\nuncertainty thresholds consistently improves pose accuracy. Specifically, the\nmean position error is reduced by 41.0%, 56.7%, and 69.4%, and the mean\norientation error by 55.6%, 65.7%, and 73.3%, when applying 90%, 80%, and 70%\nthresholds, respectively. Furthermore, the rejection strategy effectively\nremoves extreme outliers, resulting in better alignment with ground truth\ntrajectories. To the best of our knowledge, this is the first study to\nquantitatively demonstrate the benefits of percentile-based uncertainty\nrejection in multi-modal end-to-end localization tasks. Our approach provides a\npractical means to enhance the reliability and accuracy of localization systems\nin real-world deployments."}
{"id": "2503.15766", "pdf": "https://arxiv.org/pdf/2503.15766", "abs": "https://arxiv.org/abs/2503.15766", "authors": ["Peter Sharpe", "Rishikesh Ranade", "Kaustubh Tangsali", "Mohammad Amin Nabian", "Ram Cherukuri", "Sanjay Choudhry"], "title": "Accelerating Transient CFD through Machine Learning-Based Flow Initialization", "categories": ["cs.LG", "physics.flu-dyn"], "comment": "17 pages, 8 figures", "summary": "Transient computational fluid dynamics (CFD) simulations are essential for\nmany industrial applications, but a significant portion of their computational\ncost stems from the time needed to reach statistical steadiness from initial\nconditions. We present a novel machine learning-based initialization method\nthat reduces the cost of this subsequent transient solve substantially,\nachieving a 50% reduction in time-to-convergence compared to traditional\nuniform and potential flow-based initializations. Through a case study in\nautomotive aerodynamics using a 16.7M-cell unsteady RANS simulation, we\nevaluate three ML-based initialization strategies. Two of these strategies are\nrecommended for general use: (1) a physics-informed hybrid method combining ML\npredictions with potential flow solutions, and (2) a more versatile approach\nintegrating ML predictions with uniform flow. Both strategies enable CFD\nsolvers to achieve convergence times comparable to computationally expensive\nsteady RANS initializations, while requiring only seconds of computation. We\ndevelop a robust statistical convergence metric based on windowed\ntime-averaging for performance comparison between initialization strategies.\nNotably, these improvements are achieved using an ML model trained on a\ndifferent dataset of automotive geometries, demonstrating strong generalization\ncapabilities. The proposed methods integrate seamlessly with existing CFD\nworkflows without requiring modifications to the underlying flow solver,\nproviding a practical approach to accelerating industrial CFD simulations\nthrough improved ML-based initialization strategies."}
{"id": "2504.14257", "pdf": "https://arxiv.org/pdf/2504.14257", "abs": "https://arxiv.org/abs/2504.14257", "authors": ["Yilin Liu", "Duoteng Xu", "Xingyao Yu", "Xiang Xu", "Daniel Cohen-Or", "Hao Zhang", "Hui Huang"], "title": "HoLa: B-Rep Generation using a Holistic Latent Representation", "categories": ["cs.GR", "cs.CV"], "comment": "ACM TOG and SIGGRAPH 2025 (Patent Protected); Project page:\n  https://vcc.tech/research/2025/HolaBrep", "summary": "We introduce a novel representation for learning and generating\nComputer-Aided Design (CAD) models in the form of $\\textit{boundary\nrepresentations}$ (B-Reps). Our representation unifies the continuous geometric\nproperties of B-Rep primitives in different orders (e.g., surfaces and curves)\nand their discrete topological relations in a $\\textit{holistic latent}$ (HoLa)\nspace. This is based on the simple observation that the topological connection\nbetween two surfaces is intrinsically tied to the geometry of their\nintersecting curve. Such a prior allows us to reformulate topology learning in\nB-Reps as a geometric reconstruction problem in Euclidean space. Specifically,\nwe eliminate the presence of curves, vertices, and all the topological\nconnections in the latent space by learning to distinguish and derive curve\ngeometries from a pair of surface primitives via a neural intersection network.\nTo this end, our holistic latent space is only defined on surfaces but encodes\na full B-Rep model, including the geometry of surfaces, curves, vertices, and\ntheir topological relations. Our compact and holistic latent space facilitates\nthe design of a first diffusion-based generator to take on a large variety of\ninputs including point clouds, single/multi-view images, 2D sketches, and text\nprompts. Our method significantly reduces ambiguities, redundancies, and\nincoherences among the generated B-Rep primitives, as well as training\ncomplexities inherent in prior multi-step B-Rep learning pipelines, while\nachieving greatly improved validity rate over current state of the art: 82% vs.\n$\\approx$50%."}
{"id": "2504.05138", "pdf": "https://arxiv.org/pdf/2504.05138", "abs": "https://arxiv.org/abs/2504.05138", "authors": ["Haoran Zhang", "Zejun Gong", "Zekai Li", "Marie Siew", "Carlee Joe-Wong", "Rachid El-Azouzi"], "title": "Towards Optimal Heterogeneous Client Sampling in Multi-Model Federated Learning", "categories": ["cs.LG", "cs.DC", "I.2.11"], "comment": "29 pages with full proofs", "summary": "Federated learning (FL) allows edge devices to collaboratively train models\nwithout sharing local data. As FL gains popularity, clients may need to train\nmultiple unrelated FL models, but communication constraints limit their ability\nto train all models simultaneously. While clients could train FL models\nsequentially, opportunistically having FL clients concurrently train different\nmodels -- termed multi-model federated learning (MMFL) -- can reduce the\noverall training time. Prior work uses simple client-to-model assignments that\ndo not optimize the contribution of each client to each model over the course\nof its training. Prior work on single-model FL shows that intelligent client\nselection can greatly accelerate convergence, but na\\\"ive extensions to MMFL\ncan violate heterogeneous resource constraints at both the server and the\nclients. In this work, we develop a novel convergence analysis of MMFL with\narbitrary client sampling methods, theoretically demonstrating the strengths\nand limitations of previous well-established gradient-based methods. Motivated\nby this analysis, we propose MMFL-LVR, a loss-based sampling method that\nminimizes training variance while explicitly respecting communication limits at\nthe server and reducing computational costs at the clients. We extend this to\nMMFL-StaleVR, which incorporates stale updates for improved efficiency and\nstability, and MMFL-StaleVRE, a lightweight variant suitable for low-overhead\ndeployment. Experiments show our methods improve average accuracy by up to\n19.1% over random sampling, with only a 5.4% gap from the theoretical optimum\n(full client participation)."}
{"id": "2504.05490", "pdf": "https://arxiv.org/pdf/2504.05490", "abs": "https://arxiv.org/abs/2504.05490", "authors": ["Sasan Vakili", "Manuel Mazo Jr.", "Peyman Mohajerin Esfahani"], "title": "Optimal Bayesian Affine Estimator and Active Learning for the Wiener Model", "categories": ["cs.LG", "cs.SY", "eess.SY"], "comment": "23 pages, 4 figures", "summary": "This paper presents a Bayesian estimation framework for Wiener models,\nfocusing on learning nonlinear output functions under known linear state\ndynamics. We derive a closed-form optimal affine estimator for the unknown\nparameters, characterized by the so-called \"dynamic basis statistics\" (DBS).\nSeveral features of the proposed estimator are studied, including Bayesian\nunbiasedness, closed-form posterior statistics, error monotonicity in\ntrajectory length, and consistency condition (also known as persistent\nexcitation). In the special case of Fourier basis functions, we demonstrate\nthat the closed-form description is computationally available, as the Fourier\nDBS enjoys explicit expressions. Furthermore, we identify an inherent\ninconsistency in the Fourier bases for single-trajectory measurements,\nregardless of the input excitation. Leveraging the closed-form estimation\nerror, we develop an active learning algorithm synthesizing input signals to\nminimize estimation error. Numerical experiments validate the efficacy of our\napproach, showing significant improvements over traditional regularized\nleast-squares methods."}
{"id": "2504.07307", "pdf": "https://arxiv.org/pdf/2504.07307", "abs": "https://arxiv.org/abs/2504.07307", "authors": ["Jingxin Zhan", "Yuchen Xin", "Zhihua Zhang"], "title": "Follow-the-Perturbed-Leader Approaches Best-of-Both-Worlds for the m-Set Semi-Bandit Problems", "categories": ["cs.LG", "stat.ML"], "comment": null, "summary": "We consider a common case of the combinatorial semi-bandit problem, the\n$m$-set semi-bandit, where the learner exactly selects $m$ arms from the total\n$d$ arms. In the adversarial setting, the best regret bound, known to be\n$\\mathcal{O}(\\sqrt{nmd})$ for time horizon $n$, is achieved by the well-known\nFollow-the-Regularized-Leader (FTRL) policy. However, this requires to\nexplicitly compute the arm-selection probabilities via optimizing problems at\neach time step and sample according to them. This problem can be avoided by the\nFollow-the-Perturbed-Leader (FTPL) policy, which simply pulls the $m$ arms that\nrank among the $m$ smallest (estimated) loss with random perturbation. In this\npaper, we show that FTPL with a Fr\\'echet perturbation also enjoys the near\noptimal regret bound $\\mathcal{O}(\\sqrt{nmd\\log(d)})$ in the adversarial\nsetting and approaches best-of-both-world regret bounds, i.e., achieves a\nlogarithmic regret for the stochastic setting."}
{"id": "2504.09192", "pdf": "https://arxiv.org/pdf/2504.09192", "abs": "https://arxiv.org/abs/2504.09192", "authors": ["Zhiyong Wang"], "title": "Towards More Efficient, Robust, Instance-adaptive, and Generalizable Online Learning", "categories": ["cs.LG"], "comment": "Ph.D. Thesis", "summary": "The primary goal of my Ph.D. study is to develop provably efficient and\npractical algorithms for data-driven online sequential decision-making under\nuncertainty. My work focuses on reinforcement learning (RL), multi-armed\nbandits, and their applications, including recommendation systems, computer\nnetworks, video analytics, and large language models (LLMs). Online learning\nmethods, such as bandits and RL, have demonstrated remarkable success - ranging\nfrom outperforming human players in complex games like Atari and Go to\nadvancing robotics, recommendation systems, and fine-tuning LLMs. Despite these\nsuccesses, many established algorithms rely on idealized models that can fail\nunder model misspecifications or adversarial perturbations, particularly in\nsettings where accurate prior knowledge of the underlying model class is\nunavailable or where malicious users operate within dynamic systems. These\nchallenges are pervasive in real-world applications, where robust and adaptive\nsolutions are critical. Furthermore, while worst-case guarantees provide\ntheoretical reliability, they often fail to capture instance-dependent\nperformance, which can lead to more efficient and practical solutions. Another\nkey challenge lies in generalizing to new, unseen environments, a crucial\nrequirement for deploying these methods in dynamic and unpredictable settings.\nTo address these limitations, my research aims to develop more efficient,\nrobust, instance-adaptive, and generalizable online learning algorithms for\nboth reinforcement learning and bandits. Towards this end, I focus on\ndeveloping more efficient, robust, instance-adaptive, and generalizable for\nboth general reinforcement learning (RL) and bandits."}
{"id": "2504.09940", "pdf": "https://arxiv.org/pdf/2504.09940", "abs": "https://arxiv.org/abs/2504.09940", "authors": ["Guowen Li", "Xintong Liu", "Shilei Cao", "Haoyuan Liang", "Mengxuan Chen", "Lixian Zhang", "Jinxiao Zhang", "Jiuke Wang", "Meng Jin", "Juepeng Zheng", "Haohuan Fu"], "title": "TianQuan-Climate: A Subseasonal-to-Seasonal Global Weather Model via Incorporate Climatology State", "categories": ["cs.LG"], "comment": null, "summary": "Subseasonal forecasting serves as an important support for Sustainable\nDevelopment Goals (SDGs), such as climate challenges, agricultural yield and\nsustainable energy production. However, subseasonal forecasting is a complex\ntask in meteorology due to dissipating initial conditions and delayed external\nforces. Although AI models are increasingly pushing the boundaries of this\nforecasting limit, they face two major challenges: error accumulation and\nSmoothness. To address these two challenges, we propose Climate Furnace\nSubseasonal-to-Seasonal (TianQuan-Climate), a novel machine learning model\ndesigned to provide global daily mean forecasts up to 45 days, covering five\nupper-air atmospheric variables at 13 pressure levels and two surface\nvariables. Our proposed TianQuan-Climate has two advantages: 1) it utilizes a\nmulti-model prediction strategy to reduce system error impacts in long-term\nsubseasonal forecasts; 2) it incorporates a Content Fusion Module for\nclimatological integration and extends ViT with uncertainty blocks (UD-ViT) to\nimprove generalization by learning from uncertainty. We demonstrate the\neffectiveness of TianQuan-Climate on benchmarks for weather forecasting and\nclimate projections within the 15 to 45-day range, where TianQuan-Climate\noutperforms existing numerical and AI methods."}
{"id": "2504.12675", "pdf": "https://arxiv.org/pdf/2504.12675", "abs": "https://arxiv.org/abs/2504.12675", "authors": ["Pengtao Dang", "Tingbo Guo", "Melissa Fishel", "Guang Lin", "Wenzhuo Wu", "Sha Cao", "Chi Zhang"], "title": "Physics Informed Constrained Learning of Dynamics from Static Data", "categories": ["cs.LG", "physics.bio-ph", "q-bio.MN"], "comment": "39 pages, 10 figures", "summary": "A physics-informed neural network (PINN) models the dynamics of a system by\nintegrating the governing physical laws into the architecture of a neural\nnetwork. By enforcing physical laws as constraints, PINN overcomes challenges\nwith data scarsity and potentially high dimensionality. Existing PINN\nframeworks rely on fully observed time-course data, the acquisition of which\ncould be prohibitive for many systems. In this study, we developed a new PINN\nlearning paradigm, namely Constrained Learning, that enables the approximation\nof first-order derivatives or motions using non-time course or partially\nobserved data. Computational principles and a general mathematical formulation\nof Constrained Learning were developed. We further introduced MPOCtrL (Message\nPassing Optimization-based Constrained Learning) an optimization approach\ntailored for the Constrained Learning framework that strives to balance the\nfitting of physical models and observed data. Its code is available at github\nlink: https://github.com/ptdang1001/MPOCtrL Experiments on synthetic and\nreal-world data demonstrated that MPOCtrL can effectively detect the nonlinear\ndependency between observed data and the underlying physical properties of the\nsystem. In particular, on the task of metabolic flux analysis, MPOCtrL\noutperforms all existing data-driven flux estimators."}
{"id": "2504.12875", "pdf": "https://arxiv.org/pdf/2504.12875", "abs": "https://arxiv.org/abs/2504.12875", "authors": ["Phung Lai", "Guanxiong Liu", "NhatHai Phan", "Issa Khalil", "Abdallah Khreishah", "Xintao Wu"], "title": "A Client-level Assessment of Collaborative Backdoor Poisoning in Non-IID Federated Learning", "categories": ["cs.LG"], "comment": null, "summary": "Federated learning (FL) enables collaborative model training using\ndecentralized private data from multiple clients. While FL has shown robustness\nagainst poisoning attacks with basic defenses, our research reveals new\nvulnerabilities stemming from non-independent and identically distributed\n(non-IID) data among clients. These vulnerabilities pose a substantial risk of\nmodel poisoning in real-world FL scenarios.\n  To demonstrate such vulnerabilities, we develop a novel collaborative\nbackdoor poisoning attack called CollaPois. In this attack, we distribute a\nsingle pre-trained model infected with a Trojan to a group of compromised\nclients. These clients then work together to produce malicious gradients,\ncausing the FL model to consistently converge towards a low-loss region\ncentered around the Trojan-infected model. Consequently, the impact of the\nTrojan is amplified, especially when the benign clients have diverse local data\ndistributions and scattered local gradients. CollaPois stands out by achieving\nits goals while involving only a limited number of compromised clients, setting\nit apart from existing attacks. Also, CollaPois effectively avoids noticeable\nshifts or degradation in the FL model's performance on legitimate data samples,\nallowing it to operate stealthily and evade detection by advanced robust FL\nalgorithms.\n  Thorough theoretical analysis and experiments conducted on various benchmark\ndatasets demonstrate the superiority of CollaPois compared to state-of-the-art\nbackdoor attacks. Notably, CollaPois bypasses existing backdoor defenses,\nespecially in scenarios where clients possess diverse data distributions.\nMoreover, the results show that CollaPois remains effective even when involving\na small number of compromised clients. Notably, clients whose local data is\nclosely aligned with compromised clients experience higher risks of backdoor\ninfections."}
{"id": "2504.12988", "pdf": "https://arxiv.org/pdf/2504.12988", "abs": "https://arxiv.org/abs/2504.12988", "authors": ["Yannis Montreuil", "Axel Carlier", "Lai Xing Ng", "Wei Tsang Ooi"], "title": "Why Ask One When You Can Ask $k$? Two-Stage Learning-to-Defer to the Top-$k$ Experts", "categories": ["cs.LG", "stat.ML"], "comment": null, "summary": "Learning-to-Defer (L2D) enables decision-making systems to improve\nreliability by selectively deferring uncertain predictions to more competent\nagents. However, most existing approaches focus exclusively on single-agent\ndeferral, which is often inadequate in high-stakes scenarios that require\ncollective expertise. We propose Top-$k$ Learning-to-Defer, a generalization of\nthe classical two-stage L2D framework that allocates each query to the $k$ most\nconfident agents instead of a single one. To further enhance flexibility and\ncost-efficiency, we introduce Top-$k(x)$ Learning-to-Defer, an adaptive\nextension that learns the optimal number of agents to consult for each query,\nbased on input complexity, agent competency distributions, and consultation\ncosts. For both settings, we derive a novel surrogate loss and prove that it is\nBayes-consistent and $(\\mathcal{R}, \\mathcal{G})$-consistent, ensuring\nconvergence to the Bayes-optimal allocation. Notably, we show that the\nwell-established model cascades paradigm arises as a restricted instance of our\nTop-$k$ and Top-$k(x)$ formulations. Extensive experiments across diverse\nbenchmarks demonstrate the effectiveness of our framework on both\nclassification and regression tasks."}
{"id": "2504.13633", "pdf": "https://arxiv.org/pdf/2504.13633", "abs": "https://arxiv.org/abs/2504.13633", "authors": ["Samuel Wertz", "Arnaud Vandaele", "Nicolas Gillis"], "title": "Efficient algorithms for the Hadamard decomposition", "categories": ["cs.LG", "eess.SP", "math.OC", "stat.ML"], "comment": "7 pages, preprint submitted to IEEE MLSP 2025, code available from\n  https://github.com/WertzSamuel/HadamardDecompositions", "summary": "The Hadamard decomposition is a powerful technique for data analysis and\nmatrix compression, which decomposes a given matrix into the element-wise\nproduct of two or more low-rank matrices. In this paper, we develop an\nefficient algorithm to solve this problem, leveraging an alternating\noptimization approach that decomposes the global non-convex problem into a\nseries of convex sub-problems. To improve performance, we explore advanced\ninitialization strategies inspired by the singular value decomposition (SVD)\nand incorporate acceleration techniques by introducing momentum-based updates.\nBeyond optimizing the two-matrix case, we also extend the Hadamard\ndecomposition framework to support more than two low-rank matrices, enabling\napproximations with higher effective ranks while preserving computational\nefficiency. Finally, we conduct extensive experiments to compare our method\nwith the existing gradient descent-based approaches for the Hadamard\ndecomposition and with traditional low-rank approximation techniques. The\nresults highlight the effectiveness of our proposed method across diverse\ndatasets."}
{"id": "2504.13768", "pdf": "https://arxiv.org/pdf/2504.13768", "abs": "https://arxiv.org/abs/2504.13768", "authors": ["Vinay Sharma", "Rémi Tanguy Oddon", "Pietro Tesini", "Jens Ravesloot", "Cees Taal", "Olga Fink"], "title": "Equi-Euler GraphNet: An Equivariant, Temporal-Dynamics Informed Graph Neural Network for Dual Force and Trajectory Prediction in Multi-Body Systems", "categories": ["cs.LG", "cs.CE", "physics.comp-ph"], "comment": "permission not yet received for arXiv", "summary": "Accurate real-time modeling of multi-body dynamical systems is essential for\nenabling digital twin applications across industries. While many data-driven\napproaches aim to learn system dynamics, jointly predicting internal loads and\nsystem trajectories remains a key challenge. This dual prediction is especially\nimportant for fault detection and predictive maintenance, where internal\nloads-such as contact forces-act as early indicators of faults, reflecting wear\nor misalignment before affecting motion. These forces also serve as inputs to\ndegradation models (e.g., crack growth), enabling damage prediction and\nremaining useful life estimation. We propose Equi-Euler GraphNet, a\nphysics-informed graph neural network (GNN) that simultaneously predicts\ninternal forces and global trajectories in multi-body systems. In this\nmesh-free framework, nodes represent system components and edges encode\ninteractions. Equi-Euler GraphNet introduces two inductive biases: (1) an\nequivariant message-passing scheme, interpreting edge messages as interaction\nforces consistent under Euclidean transformations; and (2) a temporal-aware\niterative node update mechanism, based on Euler integration, to capture\ninfluence of distant interactions over time. Tailored for cylindrical roller\nbearings, it decouples ring dynamics from constrained motion of rolling\nelements. Trained on high-fidelity multiphysics simulations, Equi-Euler\nGraphNet generalizes beyond the training distribution, accurately predicting\nloads and trajectories under unseen speeds, loads, and configurations. It\noutperforms state-of-the-art GNNs focused on trajectory prediction, delivering\nstable rollouts over thousands of time steps with minimal error accumulation.\nAchieving up to a 200x speedup over conventional solvers while maintaining\ncomparable accuracy, it serves as an efficient reduced-order model for digital\ntwins, design, and maintenance."}
{"id": "2504.14286", "pdf": "https://arxiv.org/pdf/2504.14286", "abs": "https://arxiv.org/abs/2504.14286", "authors": ["Xiaojiang Zhang", "Jinghui Wang", "Zifei Cheng", "Wenhao Zhuang", "Zheng Lin", "Minglei Zhang", "Shaojie Wang", "Yinghan Cui", "Chao Wang", "Junyi Peng", "Shimiao Jiang", "Shiqi Kuang", "Shouyu Yin", "Chaohang Wen", "Haotian Zhang", "Bin Chen", "Bing Yu"], "title": "SRPO: A Cross-Domain Implementation of Large-Scale Reinforcement Learning on LLM", "categories": ["cs.LG"], "comment": null, "summary": "Recent advances of reasoning models, exemplified by OpenAI's o1 and\nDeepSeek's R1, highlight the significant potential of Reinforcement Learning\n(RL) to enhance the reasoning capabilities of Large Language Models (LLMs).\nHowever, replicating these advancements across diverse domains remains\nchallenging due to limited methodological transparency. In this work, we\npresent two-Staged history-Resampling Policy Optimization (SRPO), which\nsurpasses the performance of DeepSeek-R1-Zero-32B on the AIME24 and\nLiveCodeBench benchmarks. SRPO achieves this using the same base model as\nDeepSeek (i.e. Qwen2.5-32B), using only about 1/10 of the training steps\nrequired by DeepSeek-R1-Zero-32B, demonstrating superior efficiency. Building\nupon Group Relative Policy Optimization (GRPO), we introduce two key\nmethodological innovations: (1) a two-stage cross-domain training paradigm\ndesigned to balance the development of mathematical reasoning and coding\nproficiency, and (2) History Resampling (HR), a technique to address\nineffective samples. Our comprehensive experiments validate the effectiveness\nof our approach, offering valuable insights into scaling LLM reasoning\ncapabilities across diverse tasks."}
{"id": "2303.03984", "pdf": "https://arxiv.org/pdf/2303.03984", "abs": "https://arxiv.org/abs/2303.03984", "authors": ["Feihu Huang", "Chunyu Xuan", "Xinrui Wang", "Siqi Zhang", "Songcan Chen"], "title": "Enhanced Adaptive Gradient Algorithms for Nonconvex-PL Minimax Optimization", "categories": ["math.OC", "cs.LG", "cs.NA", "math.NA"], "comment": "Published in AISTATS 2025", "summary": "Minimax optimization recently is widely applied in many machine learning\ntasks such as generative adversarial networks, robust learning and\nreinforcement learning. In the paper, we study a class of nonconvex-nonconcave\nminimax optimization with nonsmooth regularization, where the objective\nfunction is possibly nonconvex on primal variable $x$, and it is nonconcave and\nsatisfies the Polyak-Lojasiewicz (PL) condition on dual variable $y$. Moreover,\nwe propose a class of enhanced momentum-based gradient descent ascent methods\n(i.e., MSGDA and AdaMSGDA) to solve these stochastic nonconvex-PL minimax\nproblems. In particular, our AdaMSGDA algorithm can use various adaptive\nlearning rates in updating the variables $x$ and $y$ without relying on any\nspecifical types. Theoretically, we prove that our methods have the best known\nsample complexity of $\\tilde{O}(\\epsilon^{-3})$ only requiring one sample at\neach loop in finding an $\\epsilon$-stationary solution. Some numerical\nexperiments on PL-game and Wasserstein-GAN demonstrate the efficiency of our\nproposed methods."}
{"id": "2304.03069", "pdf": "https://arxiv.org/pdf/2304.03069", "abs": "https://arxiv.org/abs/2304.03069", "authors": ["Jarek Duda"], "title": "Adaptive Student's t-distribution with method of moments moving estimator for nonstationary time series", "categories": ["stat.ME", "cs.LG", "econ.EM", "stat.ML"], "comment": "7 pages, 10 figures", "summary": "The real life time series are usually nonstationary, bringing a difficult\nquestion of model adaptation. Classical approaches like ARMA-ARCH assume\narbitrary type of dependence. To avoid their bias, we will focus on recently\nproposed agnostic philosophy of moving estimator: in time $t$ finding\nparameters optimizing e.g. $F_t=\\sum_{\\tau<t} (1-\\eta)^{t-\\tau} \\ln(\\rho_\\theta\n(x_\\tau))$ moving log-likelihood, evolving in time. It allows for example to\nestimate parameters using inexpensive exponential moving averages (EMA), like\nabsolute central moments $m_p=E[|x-\\mu|^p]$ evolving for one or multiple powers\n$p\\in\\mathbb{R}^+$ using $m_{p,t+1} = m_{p,t} + \\eta (|x_t-\\mu_t|^p-m_{p,t})$.\nApplication of such general adaptive methods of moments will be presented on\nStudent's t-distribution, popular especially in economical applications, here\napplied to log-returns of DJIA companies. While standard ARMA-ARCH approaches\nprovide evolution of $\\mu$ and $\\sigma$, here we also get evolution of $\\nu$\ndescribing $\\rho(x)\\sim |x|^{-\\nu-1}$ tail shape, probability of extreme events\n- which might turn out catastrophic, destabilizing the market."}
{"id": "2305.00044", "pdf": "https://arxiv.org/pdf/2305.00044", "abs": "https://arxiv.org/abs/2305.00044", "authors": ["Patrick Bajari", "Zhihao Cen", "Victor Chernozhukov", "Manoj Manukonda", "Suhas Vijaykumar", "Jin Wang", "Ramon Huerta", "Junbo Li", "Ling Leng", "George Monokroussos", "Shan Wan"], "title": "Hedonic Prices and Quality Adjusted Price Indices Powered by AI", "categories": ["econ.GN", "cs.LG", "q-fin.EC"], "comment": "Revised CEMMAP Working Paper (CWP08/23)", "summary": "We develop empirical models that efficiently process large amounts of\nunstructured product data (text, images, prices, quantities) to produce\naccurate hedonic price estimates and derived indices. To achieve this, we\ngenerate abstract product attributes (or ``features'') from descriptions and\nimages using deep neural networks. These attributes are then used to estimate\nthe hedonic price function. To demonstrate the effectiveness of this approach,\nwe apply the models to Amazon's data for first-party apparel sales, and\nestimate hedonic prices. The resulting models have a very high out-of-sample\npredictive accuracy, with $R^2$ ranging from $80\\%$ to $90\\%$. Finally, we\nconstruct the AI-based hedonic Fisher price index, chained at the\nyear-over-year frequency, and contrast it with the CPI and other electronic\nindices."}
{"id": "2307.03034", "pdf": "https://arxiv.org/pdf/2307.03034", "abs": "https://arxiv.org/abs/2307.03034", "authors": ["Keqin Liu", "Qizhen Jia", "Chengzhong Zhang"], "title": "PCL-Indexability and Whittle Index for Restless Bandits with General Observation Models", "categories": ["stat.ML", "cs.LG"], "comment": null, "summary": "In this paper, we consider a general observation model for restless\nmulti-armed bandit problems. The operation of the player needs to be based on\ncertain feedback mechanism that is error-prone due to resource constraints or\nenvironmental or intrinsic noises. By establishing a general probabilistic\nmodel for dynamics of feedback/observation, we formulate the problem as a\nrestless bandit with a countable belief state space starting from an arbitrary\ninitial belief (a priori information). We apply the achievable region method\nwith partial conservation law (PCL) to the infinite-state problem and analyze\nits indexability and priority index (Whittle index). Finally, we propose an\napproximation process to transform the problem into which the AG algorithm of\nNi\\~no-Mora and Bertsimas for finite-state problems can be applied to.\nNumerical experiments show that our algorithm has an excellent performance."}
{"id": "2406.09194", "pdf": "https://arxiv.org/pdf/2406.09194", "abs": "https://arxiv.org/abs/2406.09194", "authors": ["Honam Wong", "Wendao Wu", "Fanghui Liu", "Yiping Lu"], "title": "Benign overfitting in Fixed Dimension via Physics-Informed Learning with Smooth Inductive Bias", "categories": ["stat.ML", "cs.IT", "cs.LG", "cs.NA", "math.IT", "math.NA", "math.ST", "stat.TH"], "comment": null, "summary": "Recent advances in machine learning have inspired a surge of research into\nreconstructing specific quantities of interest from measurements that comply\nwith certain physical laws. These efforts focus on inverse problems that are\ngoverned by partial differential equations (PDEs). In this work, we develop an\nasymptotic Sobolev norm learning curve for kernel ridge(less) regression when\naddressing (elliptical) linear inverse problems. Our results show that the PDE\noperators in the inverse problem can stabilize the variance and even behave\nbenign overfitting for fixed-dimensional problems, exhibiting different\nbehaviors from regression problems. Besides, our investigation also\ndemonstrates the impact of various inductive biases introduced by minimizing\ndifferent Sobolev norms as a form of implicit regularization. For the\nregularized least squares estimator, we find that all considered inductive\nbiases can achieve the optimal convergence rate, provided the regularization\nparameter is appropriately chosen. The convergence rate is actually independent\nto the choice of (smooth enough) inductive bias for both ridge and ridgeless\nregression. Surprisingly, our smoothness requirement recovered the condition\nfound in Bayesian setting and extend the conclusion to the minimum norm\ninterpolation estimators."}
{"id": "2408.16683", "pdf": "https://arxiv.org/pdf/2408.16683", "abs": "https://arxiv.org/abs/2408.16683", "authors": ["Gianmario Voria", "Giulia Sellitto", "Carmine Ferrara", "Francesco Abate", "Andrea De Lucia", "Filomena Ferrucci", "Gemma Catolino", "Fabio Palomba"], "title": "A Catalog of Fairness-Aware Practices in Machine Learning Engineering", "categories": ["cs.SE", "cs.LG"], "comment": null, "summary": "Machine learning's widespread adoption in decision-making processes raises\nconcerns about fairness, particularly regarding the treatment of sensitive\nfeatures and potential discrimination against minorities. The software\nengineering community has responded by developing fairness-oriented metrics,\nempirical studies, and approaches. However, there remains a gap in\nunderstanding and categorizing practices for engineering fairness throughout\nthe machine learning lifecycle. This paper presents a novel catalog of\npractices for addressing fairness in machine learning derived from a systematic\nmapping study. The study identifies and categorizes 28 practices from existing\nliterature, mapping them onto different stages of the machine learning\nlifecycle. From this catalog, the authors extract actionable items and\nimplications for both researchers and practitioners in software engineering.\nThis work aims to provide a comprehensive resource for integrating fairness\nconsiderations into the development and deployment of machine learning systems,\nenhancing their reliability, accountability, and credibility."}
{"id": "2409.00035", "pdf": "https://arxiv.org/pdf/2409.00035", "abs": "https://arxiv.org/abs/2409.00035", "authors": ["Biplov Paneru", "Bipul Thapa", "Bishwash Paneru", "Sanjog Chhetri Sapkota"], "title": "EEG Right & Left Voluntary Hand Movement-based Virtual Brain-Computer Interfacing Keyboard Using Hybrid Deep Learning Approach", "categories": ["eess.SP", "cs.HC", "cs.LG", "cs.NE", "q-bio.NC"], "comment": "Please note: This is the preprint version of the manuscript. The\n  final peer-reviewed version has been published in Advanced Engineering\n  Informatics, Volume 65, Part D, 2025, and is available at:\n  https://doi.org/10.1016/j.aei.2025.103304 Please cite the published journal\n  version for referencing this work", "summary": "Brain-machine interfaces (BMIs), particularly those based on\nelectroencephalography (EEG), offer promising solutions for assisting\nindividuals with motor disabilities. However, challenges in reliably\ninterpreting EEG signals for specific tasks, such as simulating keystrokes,\npersist due to the complexity and variability of brain activity. Current\nEEG-based BMIs face limitations in adaptability, usability, and robustness,\nespecially in applications like virtual keyboards, as traditional\nmachine-learning models struggle to handle high-dimensional EEG data\neffectively. To address these gaps, we developed an EEG-based BMI system\ncapable of accurately identifying voluntary keystrokes, specifically leveraging\nright and left voluntary hand movements. Using a publicly available EEG\ndataset, the signals were pre-processed with band-pass filtering, segmented\ninto 22-electrode arrays, and refined into event-related potential (ERP)\nwindows, resulting in a 19x200 feature array categorized into three classes:\nresting state (0), 'd' key press (1), and 'l' key press (2). Our approach\nemploys a hybrid neural network architecture with BiGRU-Attention as the\nproposed model for interpreting EEG signals, achieving superior test accuracy\nof 90% and a mean accuracy of 91% in 10-fold stratified cross-validation. This\nperformance outperforms traditional ML methods like Support Vector Machines\n(SVMs) and Naive Bayes, as well as advanced architectures such as Transformers,\nCNN-Transformer hybrids, and EEGNet. Finally, the BiGRU-Attention model is\nintegrated into a real-time graphical user interface (GUI) to simulate and\npredict keystrokes from brain activity. Our work demonstrates how deep learning\ncan advance EEG-based BMI systems by addressing the challenges of signal\ninterpretation and classification."}
{"id": "2409.05598", "pdf": "https://arxiv.org/pdf/2409.05598", "abs": "https://arxiv.org/abs/2409.05598", "authors": ["Tomoyuki Obuchi", "Toshiyuki Tanaka"], "title": "When resampling/reweighting improves feature learning in imbalanced classification?: A toy-model study", "categories": ["stat.ML", "cond-mat.dis-nn", "cs.IT", "cs.LG", "math.IT"], "comment": "33 pages, 14 figures", "summary": "A toy model of binary classification is studied with the aim of clarifying\nthe class-wise resampling/reweighting effect on the feature learning\nperformance under the presence of class imbalance. In the analysis, a\nhigh-dimensional limit of the input space is taken while keeping the ratio of\nthe dataset size against the input dimension finite and the non-rigorous\nreplica method from statistical mechanics is employed. The result shows that\nthere exists a case in which the no resampling/reweighting situation gives the\nbest feature learning performance irrespectively of the choice of losses or\nclassifiers, supporting recent findings in Cao et al. (2019); Kang et al.\n(2019). It is also revealed that the key of the result is the symmetry of the\nloss and the problem setting. Inspired by this, we propose a further simplified\nmodel exhibiting the same property in the multiclass setting. These clarify\nwhen the class-wise resampling/reweighting becomes effective in imbalanced\nclassification."}
{"id": "2410.21635", "pdf": "https://arxiv.org/pdf/2410.21635", "abs": "https://arxiv.org/abs/2410.21635", "authors": ["Andrew Zhao"], "title": "Learning the structure of any Hamiltonian from minimal assumptions", "categories": ["quant-ph", "cs.DS", "cs.LG"], "comment": "45 pages", "summary": "We study the problem of learning an unknown quantum many-body Hamiltonian $H$\nfrom black-box queries to its time evolution $e^{-\\mathrm{i} H t}$. Prior\nproposals for solving this task either impose some assumptions on $H$, such as\nits interaction structure or locality, or otherwise use an exponential amount\nof computational postprocessing. In this paper, we present algorithms to learn\nany $n$-qubit Hamiltonian, which do not need to know the Hamiltonian terms in\nadvance, nor are they restricted to local interactions. Our algorithms are\nefficient as long as the number of terms $m$ is polynomially bounded in the\nsystem size $n$. We consider two models of control over the time evolution:~the\nfirst has access to time reversal ($t < 0$), enabling an algorithm that outputs\nan $\\epsilon$-accurate classical description of $H$ after querying its dynamics\nfor a total of $\\widetilde{\\mathcal{O}}(m/\\epsilon)$ evolution time. The second\naccess model is more conventional, allowing only forward-time evolutions;~our\nalgorithm requires $\\widetilde{\\mathcal{O}}(\\|H\\|^3/\\epsilon^4)$ evolution time\nin this setting. Central to our results is the recently introduced concept of a\npseudo-Choi state of $H$. We extend the utility of this learning resource by\nshowing how to use it to learn the Fourier spectrum of $H$, how to achieve\nnearly Heisenberg-limited scaling with it, and how to prepare it even under our\nmore restricted access models."}
{"id": "2411.10258", "pdf": "https://arxiv.org/pdf/2411.10258", "abs": "https://arxiv.org/abs/2411.10258", "authors": ["Qi Liu", "Yanchen Liu", "Ruifeng Li", "Chenhong Cao", "Yufeng Li", "Xingyu Li", "Peng Wang", "Runhan Feng", "Shiyang Bu"], "title": "MDHP-Net: Detecting an Emerging Time-exciting Threat in IVN", "categories": ["cs.CR", "cs.LG", "cs.NI"], "comment": "Previously this version appeared as arXiv:2504.11867 which was\n  submitted as a new work by accident", "summary": "The integration of intelligent and connected technologies in modern vehicles,\nwhile offering enhanced functionalities through Electronic Control Unit (ECU)\nand interfaces like OBD-II and telematics, also exposes the vehicle's\nin-vehicle network (IVN) to potential cyberattacks. Unlike prior work, we\nidentify a new time-exciting threat model against IVN. These attacks inject\nmalicious messages that exhibit a time-exciting effect, gradually manipulating\nnetwork traffic to disrupt vehicle operations and compromise safety-critical\nfunctions. We systematically analyze the characteristics of the threat:\ndynamism, time-exciting impact, and low prior knowledge dependency. To validate\nits practicality, we replicate the attack on a real Advanced Driver Assistance\nSystem via Controller Area Network (CAN), exploiting Unified Diagnostic Service\nvulnerabilities and proposing four attack strategies. While CAN's integrity\nchecks mitigate attacks, Ethernet migration (e.g., DoIP/SOME/IP) introduces new\nsurfaces. We further investigate the feasibility of time-exciting threat under\nSOME/IP. To detect time-exciting threat, we introduce MDHP-Net, leveraging\nMulti-Dimentional Hawkes Process (MDHP) and temporal and message-wise feature\nextracting structures. Meanwhile, to estimate MDHP parameters, we developed the\nfirst GPU-optimized gradient descent solver for MDHP (MDHP-GDS). These modules\nsignificantly improves the detection rate under time-exciting attacks in\nmulti-ECU IVN system. To address data scarcity, we release STEIA9, the first\nopen-source dataset for time-exciting attacks, covering 9 Ethernet-based attack\nscenarios. Extensive experiments on STEIA9 (9 attack scenarios) show MDHP-Net\noutperforms 3 baselines, confirming attack feasibility and detection efficacy."}
{"id": "2412.07959", "pdf": "https://arxiv.org/pdf/2412.07959", "abs": "https://arxiv.org/abs/2412.07959", "authors": ["Lorenzo Vianello", "Clément Lhoste", "Emek Barış Küçüktabak", "Matthew Short", "Levi Hargrove", "Jose L. Pons"], "title": "Deep-Learning Control of Lower-Limb Exoskeletons via simplified Therapist Input", "categories": ["cs.RO", "cs.LG"], "comment": "Accepted to the INTERNATIONAL CONSORTIUM FOR REHABILITATION ROBOTICS\n  2025", "summary": "Partial-assistance exoskeletons hold significant potential for gait\nrehabilitation by promoting active participation during (re)learning of\nnormative walking patterns. Typically, the control of interaction torques in\npartial-assistance exoskeletons relies on a hierarchical control structure.\nThese approaches require extensive calibration due to the complexity of the\ncontroller and user-specific parameter tuning, especially for activities like\nstair or ramp navigation. To address the limitations of hierarchical control in\nexoskeletons, this work proposes a three-step, data-driven approach: (1) using\nrecent sensor data to probabilistically infer locomotion states (landing step\nlength, landing step height, walking velocity, step clearance, gait phase), (2)\nallowing therapists to modify these features via a user interface, and (3)\nusing the adjusted locomotion features to predict the desired joint posture and\nmodel stiffness in a spring-damper system based on prediction uncertainty. We\nevaluated the proposed approach with two healthy participants engaging in\ntreadmill walking and stair ascent and descent at varying speeds, with and\nwithout external modification of the gait features through a user interface.\nResults showed a variation in kinematics according to the gait characteristics\nand a negative interaction power suggesting exoskeleton assistance across the\ndifferent conditions."}
{"id": "2501.05170", "pdf": "https://arxiv.org/pdf/2501.05170", "abs": "https://arxiv.org/abs/2501.05170", "authors": ["Robin Burke", "Gediminas Adomavicius", "Toine Bogers", "Tommaso Di Noia", "Dominik Kowald", "Julia Neidhardt", "Özlem Özgöbek", "Maria Soledad Pera", "Nava Tintarev", "Jürgen Ziegler"], "title": "De-centering the (Traditional) User: Multistakeholder Evaluation of Recommender Systems", "categories": ["cs.IR", "cs.LG"], "comment": "Preprint in revision at Elsevier, \"Re-centering the User in\n  Recommender System Research\" special issue of the International Journal of\n  Human-Computer Studies (IJHCS)", "summary": "Multistakeholder recommender systems are those that account for the impacts\nand preferences of multiple groups of individuals, not just the end users\nreceiving recommendations. Due to their complexity, these systems cannot be\nevaluated strictly by the overall utility of a single stakeholder, as is often\nthe case of more mainstream recommender system applications. In this article,\nwe focus our discussion on the challenges of multistakeholder evaluation of\nrecommender systems. We bring attention to the different aspects involved --\nfrom the range of stakeholders involved (including but not limited to providers\nand consumers) to the values and specific goals of each relevant stakeholder.\nWe discuss how to move from theoretical principles to practical implementation,\nproviding specific use case examples. Finally, we outline open research\ndirections for the RecSys community to explore. We aim to provide guidance to\nresearchers and practitioners about incorporating these complex and\ndomain-dependent issues of evaluation in the course of designing, developing,\nand researching applications with multistakeholder aspects."}
{"id": "2502.12804", "pdf": "https://arxiv.org/pdf/2502.12804", "abs": "https://arxiv.org/abs/2502.12804", "authors": ["Michael Doherty", "Robin Matzner", "Rasoul Sadeghi", "Polina Bayvel", "Alejandra Beghelli"], "title": "Reinforcement Learning for Dynamic Resource Allocation in Optical Networks: Hype or Hope?", "categories": ["cs.NI", "cs.LG", "cs.SY", "eess.SY"], "comment": null, "summary": "The application of reinforcement learning (RL) to dynamic resource allocation\nin optical networks has been the focus of intense research activity in recent\nyears, with almost 100 peer-reviewed papers. We present a review of progress in\nthe field, and identify significant gaps in benchmarking practices and\nreproducibility. To determine the strongest benchmark algorithms, we\nsystematically evaluate several heuristics across diverse network topologies.\nWe find that path count and sort criteria for path selection significantly\naffect the benchmark performance. We meticulously recreate the problems from\nfive landmark papers and apply the improved benchmarks. Our comparisons\ndemonstrate that simple heuristics consistently match or outperform the\npublished RL solutions, often with an order of magnitude lower blocking\nprobability. Furthermore, we present empirical lower bounds on network blocking\nusing a novel defragmentation-based method, revealing that potential\nimprovements over the benchmark heuristics are limited to 19-36% increased\ntraffic load for the same blocking performance in our examples. We make our\nsimulation framework and results publicly available to promote reproducible\nresearch and standardized evaluation https://doi.org/10.5281/zenodo.12594495."}
{"id": "2503.20507", "pdf": "https://arxiv.org/pdf/2503.20507", "abs": "https://arxiv.org/abs/2503.20507", "authors": ["Rakesh Nadig", "Vamanan Arulchelvan", "Rahul Bera", "Taha Shahroodi", "Gagandeep Singh", "Andreas Kakolyris", "Mohammad Sadrosadati", "Jisung Park", "Onur Mutlu"], "title": "Harmonia: A Multi-Agent Reinforcement Learning Approach to Data Placement and Migration in Hybrid Storage Systems", "categories": ["cs.AR", "cs.DC", "cs.LG"], "comment": null, "summary": "Hybrid storage systems (HSS) combine multiple storage devices with diverse\ncharacteristics to achieve high performance and capacity at low cost. The\nperformance of an HSS highly depends on the effectiveness of two key policies:\n(1) the data-placement policy, which determines the best-fit storage device for\nincoming data, and (2) the data-migration policy, which rearranges stored data\nacross the devices to sustain high HSS performance. Prior works focus on\nimproving only data placement or only data migration in HSS, which leads to\nrelatively low HSS performance. Unfortunately, no prior work tries to optimize\nboth policies together. Our goal is to design a holistic data-management\ntechnique that optimizes both data-placement and data-migration policies to\nfully exploit the potential of an HSS, and thus significantly improve system\nperformance. We demonstrate the need for multiple reinforcement learning (RL)\nagents to accomplish our goal. We propose Harmonia, a multi-agent RL-based\ndata-management technique that employs two lightweight autonomous RL agents, a\ndata-placement agent and a data-migration agent, which adapt their policies for\nthe current workload and HSS configuration, and coordinate with each other to\nimprove overall HSS performance. We evaluate Harmonia on a real HSS with up to\nfour heterogeneous and diverse storage devices. Our evaluation using 17\ndata-intensive workloads on performance-optimized (cost-optimized) HSS with two\nstorage devices shows that, on average, Harmonia outperforms the\nbest-performing prior approach by 49.5% (31.7%). On an HSS with three (four)\ndevices, Harmonia outperforms the best-performing prior work by 37.0% (42.0%).\nHarmonia's performance benefits come with low latency (240ns for inference) and\nstorage overheads (206 KiB in DRAM for both RL agents together). We will\nopen-source Harmonia's implementation to aid future research on HSS."}
{"id": "2504.07481", "pdf": "https://arxiv.org/pdf/2504.07481", "abs": "https://arxiv.org/abs/2504.07481", "authors": ["Tian Xie", "Menghui Jiang", "Huanfeng Shen", "Huifang Li", "Chao Zeng", "Jun Ma", "Guanhao Zhang", "Liangpei Zhang"], "title": "A Mechanism-Learning Deeply Coupled Model for Remote Sensing Retrieval of Global Land Surface Temperature", "categories": ["physics.ao-ph", "cs.LG"], "comment": null, "summary": "Land surface temperature (LST) retrieval from remote sensing data is pivotal\nfor analyzing climate processes and surface energy budgets. However, LST\nretrieval is an ill-posed inverse problem, which becomes particularly severe\nwhen only a single band is available. In this paper, we propose a deeply\ncoupled framework integrating mechanistic modeling and machine learning to\nenhance the accuracy and generalizability of single-channel LST retrieval.\nTraining samples are generated using a physically-based radiative transfer\nmodel and a global collection of 5810 atmospheric profiles. A physics-informed\nmachine learning framework is proposed to systematically incorporate the first\nprinciples from classical physical inversion models into the learning workflow,\nwith optimization constrained by radiative transfer equations. Global\nvalidation demonstrated a 30% reduction in root-mean-square error versus\nstandalone methods. Under extreme humidity, the mean absolute error decreased\nfrom 4.87 K to 2.29 K (53% improvement). Continental-scale tests across five\ncontinents confirmed the superior generalizability of this model."}
{"id": "2504.08989", "pdf": "https://arxiv.org/pdf/2504.08989", "abs": "https://arxiv.org/abs/2504.08989", "authors": ["Han Liao", "Shuaishuai Zu"], "title": "RouterKT: Mixture-of-Experts for Knowledge Tracing", "categories": ["cs.CY", "cs.IR", "cs.LG"], "comment": "10 pages", "summary": "Knowledge Tracing (KT) is a fundamental task in Intelligent Tutoring Systems\n(ITS), which aims to model the dynamic knowledge states of students based on\ntheir interaction histories. However, existing KT models often rely on a global\nforgetting decay mechanism for capturing learning patterns, assuming that\nstudents' performance is predominantly influenced by their most recent\ninteractions. Such approaches fail to account for the diverse and complex\nlearning patterns arising from individual differences and varying learning\nstages. To address this limitation, we propose RouterKT, a novel\nMixture-of-Experts (MoE) architecture designed to capture heterogeneous\nlearning patterns by enabling experts to specialize in different patterns\nwithout any handcrafted learning pattern bias such as forgetting decay.\nSpecifically, RouterKT introduces a \\textbf{person-wise routing mechanism} to\neffectively model individual-specific learning behaviors and employs\n\\textbf{multi-heads as experts} to enhance the modeling of complex and diverse\npatterns. Comprehensive experiments on ten benchmark datasets demonstrate that\nRouterKT exhibits significant flexibility and improves the performance of\nvarious KT backbone models, with a maximum average AUC improvement of 3.29\\%\nacross different backbones and datasets, outperforming other state-of-the-art\nmodels. Moreover, RouterKT demonstrates consistently superior inference\nefficiency compared to existing approaches based on handcrafted learning\npattern bias, highlighting its usability for real-world educational\napplications. The source code is available at\nhttps://github.com/ringotc/RouterKT.git."}
{"id": "2504.13397", "pdf": "https://arxiv.org/pdf/2504.13397", "abs": "https://arxiv.org/abs/2504.13397", "authors": ["Yu Gan", "Mohadeseh Azari", "Nitish Kumar Chandra", "Xin Jin", "Jinglei Cheng", "Kaushik P. Seshadreesan", "Junyu Liu"], "title": "Quantum repeaters enhanced by vacuum beam guides", "categories": ["quant-ph", "cs.DC", "cs.LG", "cs.NI"], "comment": "10 pages", "summary": "The development of large-scale quantum communication networks faces critical\nchallenges due to photon loss and decoherence in optical fiber channels. These\nfundamentally limit transmission distances and demand dense networks of\nrepeater stations. This work investigates using vacuum beam guides (VBGs)-a\npromising ultra-low-loss transmission platform-as an alternative to traditional\nfiber links. By incorporating VBGs into repeater-based architectures, we\ndemonstrate that the inter-repeater spacing can be substantially extended,\nresulting in fewer required nodes and significantly reducing hardware and\noperational complexity. We perform a cost-function analysis to quantify\nperformance trade-offs across first, second, and third-generation repeaters.\nOur results show that first-generation repeaters reduce costs dramatically by\neliminating entanglement purification. Third-generation repeaters benefit from\nimproved link transmission success, which is crucial for quantum error\ncorrection. In contrast, second-generation repeaters exhibit a more nuanced\nresponse; although transmission loss is reduced, their performance remains\nprimarily limited by logical gate errors rather than channel loss. These\nfindings highlight that while all repeater generations benefit from reduced\nphoton loss, the magnitude of improvement depends critically on the underlying\nerror mechanisms. Vacuum beam guides thus emerge as a powerful enabler for\nscalable, high-performance quantum networks, particularly in conjunction with\nnear-term quantum hardware capabilities."}
