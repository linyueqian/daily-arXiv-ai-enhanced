<div id=toc></div>

# Table of Contents

- [cs.CL](#cs.CL) [Total: 65]
- [cs.CV](#cs.CV) [Total: 125]
- [cs.AI](#cs.AI) [Total: 27]
- [cs.SD](#cs.SD) [Total: 4]
- [cs.LG](#cs.LG) [Total: 89]
- [cs.MA](#cs.MA) [Total: 7]
- [cs.MM](#cs.MM) [Total: 4]
- [eess.AS](#eess.AS) [Total: 10]
- [eess.IV](#eess.IV) [Total: 9]


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [1] [Waking Up an AI: A Quantitative Framework for Prompt-Induced Phase Transition in Large Language Models](https://arxiv.org/abs/2504.21012)
*Makoto Sato*

Main category: cs.CL

TL;DR: The paper proposes a framework to compare human and LLM cognitive dynamics, finding LLMs lack human-like conceptual fusion.


<details>
  <summary>Details</summary>
Motivation: To understand intuitive human thinking by comparing it with LLM behavior under controlled conditions.

Method: Uses Transition-Inducing Prompts (TIP) and Transition Quantifying Prompts (TQP) to measure LLM responsiveness to semantically fused vs. non-fused concepts.

Result: LLMs showed no significant difference in responsiveness to fused vs. non-fused prompts, unlike humans.

Conclusion: LLMs may not replicate human conceptual integration, highlighting a key difference in cognitive processes.

Abstract: What underlies intuitive human thinking? One approach to this question is to
compare the cognitive dynamics of humans and large language models (LLMs).
However, such a comparison requires a method to quantitatively analyze AI
cognitive behavior under controlled conditions. While anecdotal observations
suggest that certain prompts can dramatically change LLM behavior, these
observations have remained largely qualitative. Here, we propose a two-part
framework to investigate this phenomenon: a Transition-Inducing Prompt (TIP)
that triggers a rapid shift in LLM responsiveness, and a Transition Quantifying
Prompt (TQP) that evaluates this change using a separate LLM. Through
controlled experiments, we examined how LLMs react to prompts embedding two
semantically distant concepts (e.g., mathematical aperiodicity and traditional
crafts)--either fused together or presented separately--by changing their
linguistic quality and affective tone. Whereas humans tend to experience
heightened engagement when such concepts are meaningfully blended producing a
novel concept--a form of conceptual fusion--current LLMs showed no significant
difference in responsiveness between semantically fused and non-fused prompts.
This suggests that LLMs may not yet replicate the conceptual integration
processes seen in human intuition. Our method enables fine-grained,
reproducible measurement of cognitive responsiveness, and may help illuminate
key differences in how intuition and conceptual leaps emerge in artificial
versus human minds.

</details>


### [2] [Analyzing Feedback Mechanisms in AI-Generated MCQs: Insights into Readability, Lexical Properties, and Levels of Challenge](https://arxiv.org/abs/2504.21013)
*Antoun Yaacoub, Zainab Assaghir, Lionel Prevost, Jérôme Da-Rugna*

Main category: cs.CL

TL;DR: The study analyzes AI-generated feedback's linguistic traits (readability, vocabulary richness) in computer science MCQs, revealing tone-difficulty interactions and ethical implications.


<details>
  <summary>Details</summary>
Motivation: To understand AI feedback's linguistic characteristics and adaptability across question difficulty and tone, enhancing personalized learning.

Method: Analyzed 1,200 MCQs with Gemini 1.5-flash feedback, measuring linguistic metrics (readability, vocabulary richness) and training a RoBERTa-based MTL model.

Result: Achieved MAE of 2.0 for readability and 0.03 for vocabulary richness, showing tone-difficulty interaction effects in feedback adaptation.

Conclusion: AI feedback can be personalized and effective, but ethical considerations are crucial for design and deployment.

Abstract: Artificial Intelligence (AI)-generated feedback in educational settings has
garnered considerable attention due to its potential to enhance learning
outcomes. However, a comprehensive understanding of the linguistic
characteristics of AI-generated feedback, including readability, lexical
richness, and adaptability across varying challenge levels, remains limited.
This study delves into the linguistic and structural attributes of feedback
generated by Google's Gemini 1.5-flash text model for computer science
multiple-choice questions (MCQs). A dataset of over 1,200 MCQs was analyzed,
considering three difficulty levels (easy, medium, hard) and three feedback
tones (supportive, neutral, challenging). Key linguistic metrics, such as
length, readability scores (Flesch-Kincaid Grade Level), vocabulary richness,
and lexical density, were computed and examined. A fine-tuned RoBERTa-based
multi-task learning (MTL) model was trained to predict these linguistic
properties, achieving a Mean Absolute Error (MAE) of 2.0 for readability and
0.03 for vocabulary richness. The findings reveal significant interaction
effects between feedback tone and question difficulty, demonstrating the
dynamic adaptation of AI-generated feedback within diverse educational
contexts. These insights contribute to the development of more personalized and
effective AI-driven feedback mechanisms, highlighting the potential for
improved learning outcomes while underscoring the importance of ethical
considerations in their design and deployment.

</details>


### [3] [Nested Named-Entity Recognition on Vietnamese COVID-19: Dataset and Experiments](https://arxiv.org/abs/2504.21016)
*Ngoc C. Lê, Hai-Chung Nguyen-Phung, Thu-Huong Pham Thi, Hue Vu, Phuong-Thao Nguyen Thi, Thu-Thuy Tran, Hong-Nhung Le Thi, Thuy-Duong Nguyen-Thi, Thanh-Huy Nguyen*

Main category: cs.CL

TL;DR: A study on named-entity recognition (NER) to aid COVID-19 prevention in Vietnam, including a manually annotated dataset for Vietnamese with nested entities.


<details>
  <summary>Details</summary>
Motivation: Manual tracing and quarantine efforts in Vietnam are labor-intensive; automation via NER can improve efficiency.

Method: Developed a NER system with a manually annotated Vietnamese COVID-19 dataset, introducing new entity types for nested recognition.

Result: A functional NER system tailored for Vietnamese COVID-19 data, with a specialized dataset.

Conclusion: NER can streamline COVID-19 prevention efforts in Vietnam, reducing manual workload.

Abstract: The COVID-19 pandemic caused great losses worldwide, efforts are taken place
to prevent but many countries have failed. In Vietnam, the traceability,
localization, and quarantine of people who contact with patients contribute to
effective disease prevention. However, this is done by hand, and take a lot of
work. In this research, we describe a named-entity recognition (NER) study that
assists in the prevention of COVID-19 pandemic in Vietnam. We also present our
manually annotated COVID-19 dataset with nested named entity recognition task
for Vietnamese which be defined new entity types using for our system.

</details>


### [4] [ViQA-COVID: COVID-19 Machine Reading Comprehension Dataset for Vietnamese](https://arxiv.org/abs/2504.21017)
*Hai-Chung Nguyen-Phung, Ngoc C. Lê, Van-Chien Nguyen, Hang Thi Nguyen, Thuy Phuong Thi Nguyen*

Main category: cs.CL

TL;DR: The paper introduces ViQA-COVID, the first Vietnamese MRC dataset for COVID-19, aiming to support AI-driven disease prevention and advance MRC research.


<details>
  <summary>Details</summary>
Motivation: COVID-19's global impact and the need for AI solutions to aid prevention and treatment, especially for Vietnamese language support.

Method: Creation of ViQA-COVID, a multi-span extraction MRC dataset for Vietnamese, to build models for disease prevention.

Result: ViQA-COVID is the first of its kind, enabling AI applications for COVID-19 in Vietnamese and multilingual contexts.

Conclusion: The dataset contributes to AI-driven COVID-19 prevention and advances MRC research in Vietnamese and multilingual settings.

Abstract: After two years of appearance, COVID-19 has negatively affected people and
normal life around the world. As in May 2022, there are more than 522 million
cases and six million deaths worldwide (including nearly ten million cases and
over forty-three thousand deaths in Vietnam). Economy and society are both
severely affected. The variant of COVID-19, Omicron, has broken disease
prevention measures of countries and rapidly increased number of infections.
Resources overloading in treatment and epidemics prevention is happening all
over the world. It can be seen that, application of artificial intelligence
(AI) to support people at this time is extremely necessary. There have been
many studies applying AI to prevent COVID-19 which are extremely useful, and
studies on machine reading comprehension (MRC) are also in it. Realizing that,
we created the first MRC dataset about COVID-19 for Vietnamese: ViQA-COVID and
can be used to build models and systems, contributing to disease prevention.
Besides, ViQA-COVID is also the first multi-span extraction MRC dataset for
Vietnamese, we hope that it can contribute to promoting MRC studies in
Vietnamese and multilingual.

</details>


### [5] [HYPEROFA: Expanding LLM Vocabulary to New Languages via Hypernetwork-Based Embedding Initialization](https://arxiv.org/abs/2504.21018)
*Enes Özeren, Yihong Liu, Hinrich Schütze*

Main category: cs.CL

TL;DR: HYPEROFA improves token embedding initialization for low-resource languages using a hypernetwork, outperforming OFA and random baselines.


<details>
  <summary>Details</summary>
Motivation: PLMs underperform on mid- and low-resource languages due to limited pre-training exposure. Current methods like OFA restrict expressiveness.

Method: HYPEROFA uses a hypernetwork to map multilingual word vectors to PLM embeddings, enabling flexible initialization for target languages.

Result: HYPEROFA outperforms random initialization and matches/exceeds OFA in pre-training convergence and downstream tasks.

Conclusion: HYPEROFA offers a more adaptive and effective approach for token embedding initialization in low-resource languages.

Abstract: Many pre-trained language models (PLMs) exhibit suboptimal performance on
mid- and low-resource languages, largely due to limited exposure to these
languages during pre-training. A common strategy to address this is to
introduce new tokens specific to the target languages, initialize their
embeddings, and apply continual pre-training on target-language data. Among
such methods, OFA (Liu et al., 2024a) proposes a similarity-based subword
embedding initialization heuristic that is both effective and efficient.
However, OFA restricts target-language token embeddings to be convex
combinations of a fixed number of source-language embeddings, which may limit
expressiveness. To overcome this limitation, we propose HYPEROFA, a
hypernetwork-based approach for more adaptive token embedding initialization.
The hypernetwork is trained to map from an external multilingual word vector
space to the PLMs token embedding space using source-language tokens. Once
trained, it can generate flexible embeddings for target-language tokens,
serving as a good starting point for continual pretraining. Experiments
demonstrate that HYPEROFA consistently outperforms random initialization
baseline and matches or exceeds the performance of OFA in both continual
pre-training convergence and downstream task performance. We make the code
publicly available.

</details>


### [6] [Kill two birds with one stone: generalized and robust AI-generated text detection via dynamic perturbations](https://arxiv.org/abs/2504.21019)
*Yinghan Zhou, Juan Wen, Wanli Peng, Yiming Xue, Ziwei Zhang, Zhengxian Wu*

Main category: cs.CL

TL;DR: The paper proposes DP-Net, a novel AI-generated text detection method using dynamic perturbations via reinforcement learning, addressing both generalization and robustness challenges.


<details>
  <summary>Details</summary>
Motivation: Concerns about misuse of AI-generated text highlight the need for detection methods with high generalization and robustness, which existing methods lack.

Method: DP-Net introduces dynamic perturbations through reinforcement learning with a designed reward and action mechanism.

Result: DP-Net outperforms state-of-the-art methods in cross-domain generalization and robustness against adversarial attacks.

Conclusion: DP-Net effectively unifies generalization and robustness in AI-generated text detection, demonstrating superior performance.

Abstract: The growing popularity of large language models has raised concerns regarding
the potential to misuse AI-generated text (AIGT). It becomes increasingly
critical to establish an excellent AIGT detection method with high
generalization and robustness. However, existing methods either focus on model
generalization or concentrate on robustness. The unified mechanism, to
simultaneously address the challenges of generalization and robustness, is less
explored. In this paper, we argue that robustness can be view as a specific
form of domain shift, and empirically reveal an intrinsic mechanism for model
generalization of AIGT detection task. Then, we proposed a novel AIGT detection
method (DP-Net) via dynamic perturbations introduced by a reinforcement
learning with elaborated reward and action. Experimentally, extensive results
show that the proposed DP-Net significantly outperforms some state-of-the-art
AIGT detection methods for generalization capacity in three cross-domain
scenarios. Meanwhile, the DP-Net achieves best robustness under two text
adversarial attacks. The code is publicly available at
https://github.com/CAU-ISS-Lab/AIGT-Detection-Evade-Detection/tree/main/DP-Net.

</details>


### [7] [Pretraining Large Brain Language Model for Active BCI: Silent Speech](https://arxiv.org/abs/2504.21214)
*Jinzhao Zhou, Zehong Cao, Yiqun Duan, Connor Barkley, Daniel Leong, Xiaowei Jiang, Quoc-Toan Nguyen, Ziyi Zhao, Thomas Do, Yu-Cheng Chang, Sheng-Fu Liang, Chin-teng Lin*

Main category: cs.CL

TL;DR: The paper introduces a Large Brain Language Model (LBLM) for silent speech decoding in BCI, pretrained using a novel Future Spectro-Temporal Prediction (FSTP) method, achieving significant performance gains over baselines.


<details>
  <summary>Details</summary>
Motivation: To enhance natural and flexible communication in BCI systems by improving silent speech decoding using EEG data.

Method: Proposes LBLM pretrained with FSTP, an autoregressive method capturing temporal and spectral EEG dependencies, followed by finetuning for word- and semantic-level classification.

Result: LBLM outperforms baselines, achieving 47.0% (semantic) and 39.6% (word) accuracy in cross-session settings.

Conclusion: Advances silent speech decoding in BCI with innovative EEG pretraining and a new dataset.

Abstract: This paper explores silent speech decoding in active brain-computer interface
(BCI) systems, which offer more natural and flexible communication than
traditional BCI applications. We collected a new silent speech dataset of over
120 hours of electroencephalogram (EEG) recordings from 12 subjects, capturing
24 commonly used English words for language model pretraining and decoding.
Following the recent success of pretraining large models with self-supervised
paradigms to enhance EEG classification performance, we propose Large Brain
Language Model (LBLM) pretrained to decode silent speech for active BCI. To
pretrain LBLM, we propose Future Spectro-Temporal Prediction (FSTP) pretraining
paradigm to learn effective representations from unlabeled EEG data. Unlike
existing EEG pretraining methods that mainly follow a masked-reconstruction
paradigm, our proposed FSTP method employs autoregressive modeling in temporal
and frequency domains to capture both temporal and spectral dependencies from
EEG signals. After pretraining, we finetune our LBLM on downstream tasks,
including word-level and semantic-level classification. Extensive experiments
demonstrate significant performance gains of the LBLM over fully-supervised and
pretrained baseline models. For instance, in the difficult cross-session
setting, our model achieves 47.0\% accuracy on semantic-level classification
and 39.6\% in word-level classification, outperforming baseline methods by
5.4\% and 7.3\%, respectively. Our research advances silent speech decoding in
active BCI systems, offering an innovative solution for EEG language model
pretraining and a new dataset for fundamental research.

</details>


### [8] [Context-Enhanced Contrastive Search for Improved LLM Text Generation](https://arxiv.org/abs/2504.21020)
*Jaydip Sen, Rohit Pandey, Hetvi Waghela*

Main category: cs.CL

TL;DR: The paper introduces Context-Enhanced Contrastive Search (CECS), an improved version of Contrastive Search, to enhance text generation by balancing coherence, diversity, and relevance.


<details>
  <summary>Details</summary>
Motivation: Existing decoding methods like beam search and top-k sampling often produce repetitive or incoherent outputs, especially in long-form text generation.

Method: CECS introduces dynamic contextual importance weighting, multi-level Contrastive Search, and adaptive temperature control.

Result: CECS outperforms existing methods in coherence and relevance, as measured by BLEU, ROUGE, and semantic similarity.

Conclusion: CECS shows promise for real-world applications like legal document drafting, chatbots, and content marketing.

Abstract: Recently, Large Language Models (LLMs) have demonstrated remarkable
advancements in Natural Language Processing (NLP). However, generating
high-quality text that balances coherence, diversity, and relevance remains
challenging. Traditional decoding methods, such as bean search and top-k
sampling, often struggle with either repetitive or incoherent outputs,
particularly in tasks that require long-form text generation. To address these
limitations, the paper proposes a novel enhancement of the well-known
Contrastive Search algorithm, Context-Enhanced Contrastive Search (CECS) with
contextual calibration. The proposed scheme introduces several novelties
including dynamic contextual importance weighting, multi-level Contrastive
Search, and adaptive temperature control, to optimize the balance between
fluency, creativity, and precision. The performance of CECS is evaluated using
several standard metrics such as BLEU, ROUGE, and semantic similarity.
Experimental results demonstrate significant improvements in both coherence and
relevance of the generated texts by CECS outperforming the existing Contrastive
Search techniques. The proposed algorithm has several potential applications in
the real world including legal document drafting, customer service chatbots,
and content marketing.

</details>


### [9] [ConformalNL2LTL: Translating Natural Language Instructions into Temporal Logic Formulas with Conformal Correctness Guarantees](https://arxiv.org/abs/2504.21022)
*Jun Wang, David Smith Sundarsingh, Jyotirmoy V. Deshmukh, Yiannis Kantaros*

Main category: cs.CL

TL;DR: ConformalNL2LTL translates Natural Language to LTL with guaranteed accuracy using conformal prediction and LLMs.


<details>
  <summary>Details</summary>
Motivation: To reduce manual effort in defining LTL tasks and ensure correctness in NL-to-LTL translation.

Method: Iterative LTL formula construction via open-vocabulary QA with LLMs, using conformal prediction for uncertainty-aware translation.

Result: Achieves user-specified translation accuracy while minimizing help requests.

Conclusion: ConformalNL2LTL provides a reliable, uncertainty-aware method for NL-to-LTL translation.

Abstract: Linear Temporal Logic (LTL) has become a prevalent specification language for
robotic tasks. To mitigate the significant manual effort and expertise required
to define LTL-encoded tasks, several methods have been proposed for translating
Natural Language (NL) instructions into LTL formulas, which, however, lack
correctness guarantees. To address this, we introduce a new NL-to-LTL
translation method, called ConformalNL2LTL, that can achieve user-defined
translation success rates over unseen NL commands. Our method constructs LTL
formulas iteratively by addressing a sequence of open-vocabulary
Question-Answering (QA) problems with LLMs. To enable uncertainty-aware
translation, we leverage conformal prediction (CP), a distribution-free
uncertainty quantification tool for black-box models. CP enables our method to
assess the uncertainty in LLM-generated answers, allowing it to proceed with
translation when sufficiently confident and request help otherwise. We provide
both theoretical and empirical results demonstrating that ConformalNL2LTL
achieves user-specified translation accuracy while minimizing help rates.

</details>


### [10] [Param$Δ$ for Direct Weight Mixing: Post-Train Large Language Model at Zero Cost](https://arxiv.org/abs/2504.21023)
*Sheng Cao, Mingrui Wu, Karthik Prasad, Yuandong Tian, Zechun Liu*

Main category: cs.CL

TL;DR: $Param\Delta$ transfers post-training knowledge to updated base models without additional training, achieving near-original performance.


<details>
  <summary>Details</summary>
Motivation: Post-training large language models is costly and risky; $Param\Delta$ offers a zero-cost alternative by leveraging existing models.

Method: Compute weight differences between post-trained and base models, apply to updated base models to create $Param\Delta$ Models.

Result: $Param\Delta$ Models achieve ~95% performance of traditionally post-trained models, tested on LLama3, Llama3.1, Qwen, and DeepSeek-distilled models.

Conclusion: $Param\Delta$ provides a cost-free, efficient way to update models, accelerating development cycles in the open-weight community.

Abstract: The post-training phase of large language models is essential for enhancing
capabilities such as instruction-following, reasoning, and alignment with human
preferences. However, it demands extensive high-quality data and poses risks
like overfitting, alongside significant computational costs due to repeated
post-training and evaluation after each base model update. This paper
introduces $Param\Delta$, a novel method that streamlines post-training by
transferring knowledge from an existing post-trained model to a newly updated
base model with ZERO additional training. By computing the difference between
post-trained model weights ($\Theta_\text{post}$) and base model weights
($\Theta_\text{base}$), and adding this to the updated base model
($\Theta'_\text{base}$), we define $Param\Delta$ Model as:
$\Theta_{\text{Param}\Delta} = \Theta_\text{post} - \Theta_\text{base} +
\Theta'_\text{base}$. This approach surprisingly equips the new base model with
post-trained capabilities, achieving performance comparable to direct
post-training. We did analysis on LLama3, Llama3.1, Qwen, and
DeepSeek-distilled models. Results indicate $Param\Delta$ Model effectively
replicates traditional post-training. For example, the $Param\Delta$ Model
obtained from 70B Llama3-inst, Llama3-base, Llama3.1-base models attains
approximately 95\% of Llama3.1-inst model's performance on average.
$Param\Delta$ brings a new perspective on how to fully leverage models in the
open-weight community, where checkpoints for base and instruct models are
readily available and frequently updated, by providing a cost-free framework to
accelerate the iterative cycle of model development.

</details>


### [11] [WebEvolver: Enhancing Web Agent Self-Improvement with Coevolving World Model](https://arxiv.org/abs/2504.21024)
*Tianqing Fang, Hongming Zhang, Zhisong Zhang, Kaixin Ma, Wenhao Yu, Haitao Mi, Dong Yu*

Main category: cs.CL

TL;DR: A novel framework introduces a co-evolving World Model LLM to enhance agent self-improvement by addressing exploration and exploitation limitations in web environments, achieving a 10% performance gain.


<details>
  <summary>Details</summary>
Motivation: The stagnation in performance during autonomous learning cycles in web environments due to limited exploration and insufficient exploitation of pre-trained web knowledge in LLMs.

Method: Proposes a co-evolving World Model LLM that predicts next observations and serves as a virtual web server and imagination engine for self-instructed training and look-ahead simulation.

Result: Experiments show a 10% performance improvement over existing self-evolving agents in real-world web environments.

Conclusion: Integrating world models into autonomous agent frameworks is essential for sustained adaptability and performance enhancement.

Abstract: Agent self-improvement, where the backbone Large Language Model (LLM) of the
agent are trained on trajectories sampled autonomously based on their own
policies, has emerged as a promising approach for enhancing performance. Recent
advancements, particularly in web environments, face a critical limitation:
their performance will reach a stagnation point during autonomous learning
cycles, hindering further improvement. We argue that this stems from limited
exploration of the web environment and insufficient exploitation of pre-trained
web knowledge in LLMs. To improve the performance of self-improvement, we
propose a novel framework that introduces a co-evolving World Model LLM. This
world model predicts the next observation based on the current observation and
action within the web environment. Leveraging LLMs' pretrained knowledge of
abundant web content, the World Model serves dual roles: (1) as a virtual web
server generating self-instructed training data to continuously refine the
agent's policy, and (2) as an imagination engine during inference, enabling
look-ahead simulation to guide action selection for the agent LLM. Experiments
in real-world web environments (Mind2Web-Live, WebVoyager, and GAIA-web) show a
10% performance gain over existing self-evolving agents, demonstrating the
efficacy and generalizability of our approach, without using any distillation
from more powerful close-sourced models. Our work establishes the necessity of
integrating world models into autonomous agent frameworks to unlock sustained
adaptability.

</details>


### [12] [Durghotona GPT: A Web Scraping and Large Language Model Based Framework to Generate Road Accident Dataset Automatically in Bangladesh](https://arxiv.org/abs/2504.21025)
*MD Thamed Bin Zaman Chowdhury, Moazzem Hossain, Md. Ridwanul Islam*

Main category: cs.CL

TL;DR: The paper introduces 'Durghotona GPT,' a framework using LLMs and web scraping to automate accident data collection from Bangladeshi newspapers, achieving high accuracy and cost-effectiveness.


<details>
  <summary>Details</summary>
Motivation: Road accidents cause global financial and societal challenges, necessitating accurate data for prediction and mitigation.

Method: The framework scrapes accident reports from three newspapers and processes them using GPT-4, GPT-3.5, and Llama-3 to generate datasets.

Result: Llama-3 matched GPT-4's performance with 89% accuracy, offering a cost-effective solution. The framework improves data quality and availability.

Conclusion: The framework enhances traffic safety, urban planning, and public health applications. Future work will refine data collection and LLMs.

Abstract: Road accidents pose significant concerns globally. They lead to large
financial losses, injuries, disabilities, and societal challenges. Accurate and
timely accident data is essential for predicting and mitigating these events.
This paper presents a novel framework named 'Durghotona GPT' that integrates
web scraping and Large Language Models (LLMs) to automate the generation of
comprehensive accident datasets from prominent national dailies in Bangladesh.
The authors collected accident reports from three major newspapers: Prothom
Alo, Dhaka Tribune, and The Daily Star. The collected news was then processed
using the newest available LLMs: GPT-4, GPT-3.5, and Llama-3. The framework
efficiently extracts relevant information, categorizes reports, and compiles
detailed datasets. Thus, this framework overcomes limitations of manual data
collection methods such as delays, errors, and communication gaps. The authors'
evaluation demonstrates that Llama-3, an open-source model, performs comparably
to GPT-4. It achieved 89% accuracy in the authors' evaluation. Therefore, it
can be considered a cost-effective alternative for similar tasks. The results
suggest that the framework developed by the authors can drastically enhance the
quality and availability of accident data. As a result, it can support critical
applications in traffic safety analysis, urban planning, and public health. The
authors also developed an interface for 'Durghotona GPT' for ease of use as
part of this paper. Future work will focus on expanding data collection methods
and refining LLMs to further increase dataset accuracy and applicability.

</details>


### [13] [Creating and Evaluating Code-Mixed Nepali-English and Telugu-English Datasets for Abusive Language Detection Using Traditional and Deep Learning Models](https://arxiv.org/abs/2504.21026)
*Manish Pandey, Nageshwar Prasad Yadav, Mokshada Adduru, Sawan Rai*

Main category: cs.CL

TL;DR: The paper introduces a manually annotated dataset for detecting abusive language in Telugu-English and Nepali-English code-mixed text, evaluates various ML/DL/LLM models, and provides benchmarks for low-resource languages.


<details>
  <summary>Details</summary>
Motivation: The rise of multilingual social media users and the challenge of detecting abusive language in code-mixed text, especially for underrepresented languages like Telugu and Nepali.

Method: Creation of a dataset (2k Telugu-English and 5k Nepali-English comments), preprocessing, and evaluation using ML (Logistic Regression, Random Forest, SVM), DL (NN, LSTM, CNN), and LLMs with hyperparameter tuning, 10-fold CV, and t-tests.

Result: Comparative analysis of model performance, highlighting challenges in code-mixed abusive language detection and benchmarks for low-resource languages.

Conclusion: The study advances NLP for low-resource languages, offering a dataset and insights to improve moderation in multilingual social media.

Abstract: With the growing presence of multilingual users on social media, detecting
abusive language in code-mixed text has become increasingly challenging.
Code-mixed communication, where users seamlessly switch between English and
their native languages, poses difficulties for traditional abuse detection
models, as offensive content may be context-dependent or obscured by linguistic
blending. While abusive language detection has been extensively explored for
high-resource languages like English and Hindi, low-resource languages such as
Telugu and Nepali remain underrepresented, leaving gaps in effective
moderation. In this study, we introduce a novel, manually annotated dataset of
2 thousand Telugu-English and 5 Nepali-English code-mixed comments, categorized
as abusive and non-abusive, collected from various social media platforms. The
dataset undergoes rigorous preprocessing before being evaluated across multiple
Machine Learning (ML), Deep Learning (DL), and Large Language Models (LLMs). We
experimented with models including Logistic Regression, Random Forest, Support
Vector Machines (SVM), Neural Networks (NN), LSTM, CNN, and LLMs, optimizing
their performance through hyperparameter tuning, and evaluate it using 10-fold
cross-validation and statistical significance testing (t-test). Our findings
provide key insights into the challenges of detecting abusive language in
code-mixed settings and offer a comparative analysis of computational
approaches. This study contributes to advancing NLP for low-resource languages
by establishing benchmarks for abusive language detection in Telugu-English and
Nepali-English code-mixed text. The dataset and insights can aid in the
development of more robust moderation strategies for multilingual social media
environments.

</details>


### [14] [UrbanPlanBench: A Comprehensive Urban Planning Benchmark for Evaluating Large Language Models](https://arxiv.org/abs/2504.21027)
*Yu Zheng, Longyi Liu, Yuming Lin, Jie Feng, Guozhen Zhang, Depeng Jin, Yong Li*

Main category: cs.CL

TL;DR: The paper introduces UrbanPlanBench, a benchmark to evaluate LLMs in urban planning, revealing their limitations and proposing a fine-tuning dataset, UrbanPlanText, to improve performance.


<details>
  <summary>Details</summary>
Motivation: To explore the potential of LLMs in urban planning, a field reliant on human expertise, and assess their ability to meet professional standards.

Method: Developed UrbanPlanBench for evaluation and UrbanPlanText, a 30,000-instruction dataset, for fine-tuning LLMs.

Result: LLMs show significant gaps in urban planning knowledge, especially in regulations, but fine-tuned models improve in memorization and comprehension.

Conclusion: The benchmark and dataset aim to bridge the gap between LLMs and human expertise in urban planning, encouraging collaborative advancements.

Abstract: The advent of Large Language Models (LLMs) holds promise for revolutionizing
various fields traditionally dominated by human expertise. Urban planning, a
professional discipline that fundamentally shapes our daily surroundings, is
one such field heavily relying on multifaceted domain knowledge and experience
of human experts. The extent to which LLMs can assist human practitioners in
urban planning remains largely unexplored. In this paper, we introduce a
comprehensive benchmark, UrbanPlanBench, tailored to evaluate the efficacy of
LLMs in urban planning, which encompasses fundamental principles, professional
knowledge, and management and regulations, aligning closely with the
qualifications expected of human planners. Through extensive evaluation, we
reveal a significant imbalance in the acquisition of planning knowledge among
LLMs, with even the most proficient models falling short of meeting
professional standards. For instance, we observe that 70% of LLMs achieve
subpar performance in understanding planning regulations compared to other
aspects. Besides the benchmark, we present the largest-ever supervised
fine-tuning (SFT) dataset, UrbanPlanText, comprising over 30,000 instruction
pairs sourced from urban planning exams and textbooks. Our findings demonstrate
that fine-tuned models exhibit enhanced performance in memorization tests and
comprehension of urban planning knowledge, while there exists significant room
for improvement, particularly in tasks requiring domain-specific terminology
and reasoning. By making our benchmark, dataset, and associated evaluation and
fine-tuning toolsets publicly available at
https://github.com/tsinghua-fib-lab/PlanBench, we aim to catalyze the
integration of LLMs into practical urban planning, fostering a symbiotic
collaboration between human expertise and machine intelligence.

</details>


### [15] [Beyond One-Size-Fits-All: Inversion Learning for Highly Effective NLG Evaluation Prompts](https://arxiv.org/abs/2504.21117)
*Hanhua Hong, Chenghao Xiao, Yang Wang, Yiqi Liu, Wenge Rong, Chenghua Lin*

Main category: cs.CL

TL;DR: Proposes an inversion learning method to automate prompt generation for LLM-based NLG evaluation, improving efficiency and robustness.


<details>
  <summary>Details</summary>
Motivation: Human evaluation of NLG systems is inconsistent and biased, while LLM-based evaluation is sensitive to prompt design.

Method: Uses inversion learning to create reverse mappings from outputs to input instructions, generating model-specific evaluation prompts.

Result: Enables automatic prompt generation with just one sample, eliminating manual engineering.

Conclusion: Advances robust and efficient LLM-based evaluation of NLG systems.

Abstract: Evaluating natural language generation (NLG) systems is challenging due to
the diversity of valid outputs. While human evaluation is the gold standard, it
suffers from inconsistencies, lack of standardisation, and demographic biases,
limiting reproducibility. LLM-based evaluation offers a scalable alternative
but is highly sensitive to prompt design, where small variations can lead to
significant discrepancies. In this work, we propose an inversion learning
method that learns effective reverse mappings from model outputs back to their
input instructions, enabling the automatic generation of highly effective,
model-specific evaluation prompts. Our method requires only a single evaluation
sample and eliminates the need for time-consuming manual prompt engineering,
thereby improving both efficiency and robustness. Our work contributes toward a
new direction for more robust and efficient LLM-based evaluation.

</details>


### [16] [LLM Enhancer: Merged Approach using Vector Embedding for Reducing Large Language Model Hallucinations with External Knowledge](https://arxiv.org/abs/2504.21132)
*Naheed Rayhan, Md. Ashrafuzzaman*

Main category: cs.CL

TL;DR: The paper introduces LLM ENHANCER, a system that integrates multiple online sources to improve the accuracy of LLMs like ChatGPT, reducing hallucinations while maintaining natural responses.


<details>
  <summary>Details</summary>
Motivation: LLMs often produce inaccurate information and struggle to leverage external knowledge, limiting their use in critical scenarios.

Method: The system uses parallel data acquisition from sources like Google and Wikipedia, employs vector embeddings to filter relevant information, and integrates it into LLM responses.

Result: LLM ENHANCER reduces hallucinations in LLMs while preserving response naturalness and accuracy.

Conclusion: The system enhances LLM reliability for real-world applications by combining external knowledge with open-source LLMs.

Abstract: Large Language Models (LLMs), such as ChatGPT, have demonstrated the
capability to generate human like, natural responses across a range of tasks,
including task oriented dialogue and question answering. However, their
application in real world, critical scenarios is often hindered by a tendency
to produce inaccurate information and a limited ability to leverage external
knowledge sources. This paper introduces the LLM ENHANCER system, designed to
integrate multiple online sources such as Google, Wikipedia, and DuckDuckGo to
enhance data accuracy. The LLMs employed within this system are open source.
The data acquisition process for the LLM ENHANCER system operates in parallel,
utilizing custom agent tools to manage the flow of information. Vector
embeddings are used to identify the most pertinent information, which is
subsequently supplied to the LLM for user interaction. The LLM ENHANCER system
mitigates hallucinations in chat based LLMs while preserving response
naturalness and accuracy.

</details>


### [17] [Detecting Manipulated Contents Using Knowledge-Grounded Inference](https://arxiv.org/abs/2504.21165)
*Mark Huasong Meng, Ruizhe Wang, Meng Xu, Chuan Yan, Guangdong Bai*

Main category: cs.CL

TL;DR: Manicod is a tool for detecting zero-day manipulated content using real-time contextual information and LLM-based inference, outperforming existing methods.


<details>
  <summary>Details</summary>
Motivation: Existing fake news detection methods rely on historical data or manual context, failing to address zero-day manipulated content.

Method: Manicod sources real-time context from search engines, uses RAG for vectorization, and employs LLM for inference and explanation.

Result: Achieves an F1 score of 0.856 on a dataset of 4270 manipulated news pieces, outperforming benchmarks by up to 1.9x.

Conclusion: Manicod effectively detects zero-day manipulated content, offering a scalable solution with real-time context.

Abstract: The detection of manipulated content, a prevalent form of fake news, has been
widely studied in recent years. While existing solutions have been proven
effective in fact-checking and analyzing fake news based on historical events,
the reliance on either intrinsic knowledge obtained during training or manually
curated context hinders them from tackling zero-day manipulated content, which
can only be recognized with real-time contextual information. In this work, we
propose Manicod, a tool designed for detecting zero-day manipulated content.
Manicod first sources contextual information about the input claim from
mainstream search engines, and subsequently vectorizes the context for the
large language model (LLM) through retrieval-augmented generation (RAG). The
LLM-based inference can produce a "truthful" or "manipulated" decision and
offer a textual explanation for the decision. To validate the effectiveness of
Manicod, we also propose a dataset comprising 4270 pieces of manipulated fake
news derived from 2500 recent real-world news headlines. Manicod achieves an
overall F1 score of 0.856 on this dataset and outperforms existing methods by
up to 1.9x in F1 score on their benchmarks on fact-checking and claim
verification.

</details>


### [18] [Small or Large? Zero-Shot or Finetuned? Guiding Language Model Choice for Specialized Applications in Healthcare](https://arxiv.org/abs/2504.21191)
*Lovedeep Gondara, Jonathan Simkin, Graham Sayle, Shebnum Devji, Gregory Arbour, Raymond Ng*

Main category: cs.CL

TL;DR: The study compares finetuned and zero-shot SLMs with zero-shot LLMs for pathology report classification, finding finetuned SLMs outperform LLMs in specialized tasks.


<details>
  <summary>Details</summary>
Motivation: To guide language model selection by evaluating finetuning, pretraining, and model size for domain-specific tasks.

Method: Evaluated SLMs and an LLM on three classification tasks using pathology reports, comparing zero-shot and finetuned performance.

Result: Finetuned SLMs outperformed zero-shot LLMs; domain-adjacent and domain-specific pretraining improved performance, especially for complex tasks.

Conclusion: Finetuned SLMs are more effective than zero-shot LLMs for specialized tasks, offering a better performance-resource trade-off.

Abstract: This study aims to guide language model selection by investigating: 1) the
necessity of finetuning versus zero-shot usage, 2) the benefits of
domain-adjacent versus generic pretrained models, 3) the value of further
domain-specific pretraining, and 4) the continued relevance of Small Language
Models (SLMs) compared to Large Language Models (LLMs) for specific tasks.
Using electronic pathology reports from the British Columbia Cancer Registry
(BCCR), three classification scenarios with varying difficulty and data size
are evaluated. Models include various SLMs and an LLM. SLMs are evaluated both
zero-shot and finetuned; the LLM is evaluated zero-shot only. Finetuning
significantly improved SLM performance across all scenarios compared to their
zero-shot results. The zero-shot LLM outperformed zero-shot SLMs but was
consistently outperformed by finetuned SLMs. Domain-adjacent SLMs generally
performed better than the generic SLM after finetuning, especially on harder
tasks. Further domain-specific pretraining yielded modest gains on easier tasks
but significant improvements on the complex, data-scarce task. The results
highlight the critical role of finetuning for SLMs in specialized domains,
enabling them to surpass zero-shot LLM performance on targeted classification
tasks. Pretraining on domain-adjacent or domain-specific data provides further
advantages, particularly for complex problems or limited finetuning data. While
LLMs offer strong zero-shot capabilities, their performance on these specific
tasks did not match that of appropriately finetuned SLMs. In the era of LLMs,
SLMs remain relevant and effective, offering a potentially superior
performance-resource trade-off compared to LLMs.

</details>


### [19] [Automatic Legal Writing Evaluation of LLMs](https://arxiv.org/abs/2504.21202)
*Ramon Pires, Roseval Malaquias Junior, Rodrigo Nogueira*

Main category: cs.CL

TL;DR: The paper introduces oab-bench, a benchmark for evaluating legal writing using the Brazilian Bar Examination, and tests LLMs like Claude-3.5 Sonnet, which performed best. It also explores LLMs as automated judges.


<details>
  <summary>Details</summary>
Motivation: Legal writing evaluation lacks benchmarks due to complexity; the Brazilian Bar Examination provides a suitable test dataset.

Method: Developed oab-bench with 105 questions and evaluation guidelines, tested four LLMs, and assessed their potential as automated judges.

Result: Claude-3.5 Sonnet scored highest (7.93/10), passing all exams. Frontier LLMs showed strong correlation with human scores in evaluations.

Conclusion: LLMs, especially Claude-3.5 Sonnet, perform well on legal tasks, and frontier models can serve as reliable automated evaluators.

Abstract: Despite the recent advances in Large Language Models, benchmarks for
evaluating legal writing remain scarce due to the inherent complexity of
assessing open-ended responses in this domain. One of the key challenges in
evaluating language models on domain-specific tasks is finding test datasets
that are public, frequently updated, and contain comprehensive evaluation
guidelines. The Brazilian Bar Examination meets these requirements. We
introduce oab-bench, a benchmark comprising 105 questions across seven areas of
law from recent editions of the exam. The benchmark includes comprehensive
evaluation guidelines and reference materials used by human examiners to ensure
consistent grading. We evaluate the performance of four LLMs on oab-bench,
finding that Claude-3.5 Sonnet achieves the best results with an average score
of 7.93 out of 10, passing all 21 exams. We also investigated whether LLMs can
serve as reliable automated judges for evaluating legal writing. Our
experiments show that frontier models like OpenAI's o1 achieve a strong
correlation with human scores when evaluating approved exams, suggesting their
potential as reliable automated evaluators despite the inherently subjective
nature of legal writing assessment. The source code and the benchmark --
containing questions, evaluation guidelines, model-generated responses, and
their respective automated evaluations -- are publicly available.

</details>


### [20] [Phi-4-Mini-Reasoning: Exploring the Limits of Small Reasoning Language Models in Math](https://arxiv.org/abs/2504.21233)
*Haoran Xu, Baolin Peng, Hany Awadalla, Dongdong Chen, Yen-Chun Chen, Mei Gao, Young Jin Kim, Yunsheng Li, Liliang Ren, Yelong Shen, Shuohang Wang, Weijian Xu, Jianfeng Gao, Weizhu Chen*

Main category: cs.CL

TL;DR: The paper presents a systematic training recipe for Small Language Models (SLMs) to enhance reasoning, outperforming larger models on math tasks.


<details>
  <summary>Details</summary>
Motivation: Improving reasoning in SLMs is challenging due to limited capacity, and prior work lacks detailed methods.

Method: Four-step training recipe: mid-training, supervised fine-tuning, Rollout DPO, and RL with Verifiable Reward.

Result: Phi-4-Mini-Reasoning outperforms larger models by 3.2-7.7 points on Math-500.

Conclusion: A well-designed training recipe with high-quality CoT data unlocks strong reasoning in SLMs.

Abstract: Chain-of-Thought (CoT) significantly enhances formal reasoning capabilities
in Large Language Models (LLMs) by training them to explicitly generate
intermediate reasoning steps. While LLMs readily benefit from such techniques,
improving reasoning in Small Language Models (SLMs) remains challenging due to
their limited model capacity. Recent work by Deepseek-R1 demonstrates that
distillation from LLM-generated synthetic data can substantially improve the
reasoning ability of SLM. However, the detailed modeling recipe is not
disclosed. In this work, we present a systematic training recipe for SLMs that
consists of four steps: (1) large-scale mid-training on diverse distilled
long-CoT data, (2) supervised fine-tuning on high-quality long-CoT data, (3)
Rollout DPO leveraging a carefully curated preference dataset, and (4)
Reinforcement Learning (RL) with Verifiable Reward. We apply our method on
Phi-4-Mini, a compact 3.8B-parameter model. The resulting Phi-4-Mini-Reasoning
model exceeds, on math reasoning tasks, much larger reasoning models, e.g.,
outperforming DeepSeek-R1-Distill-Qwen-7B by 3.2 points and
DeepSeek-R1-Distill-Llama-8B by 7.7 points on Math-500. Our results validate
that a carefully designed training recipe, with large-scale high-quality CoT
data, is effective to unlock strong reasoning capabilities even in
resource-constrained small models.

</details>


### [21] [Memorization and Knowledge Injection in Gated LLMs](https://arxiv.org/abs/2504.21239)
*Xu Pan, Ely Hahami, Zechen Zhang, Haim Sompolinsky*

Main category: cs.CL

TL;DR: MEGa, a continual learning framework, integrates event memories into LLMs via gated low-rank weights, improving recall and mitigating forgetting.


<details>
  <summary>Details</summary>
Motivation: Address LLMs' inability to sequentially add and integrate new knowledge, unlike humans.

Method: Memory Embedded in Gated LLMs (MEGa) stores memories in gated low-rank weights, activated by query embeddings during inference.

Result: Outperforms baselines on fictional characters and Wikipedia events datasets in mitigating catastrophic forgetting.

Conclusion: MEGa mimics human memory systems, enhancing LLMs' continual learning capabilities.

Abstract: Large Language Models (LLMs) currently struggle to sequentially add new
memories and integrate new knowledge. These limitations contrast with the human
ability to continuously learn from new experiences and acquire knowledge
throughout life. Most existing approaches add memories either through large
context windows or external memory buffers (e.g., Retrieval-Augmented
Generation), and studies on knowledge injection rarely test scenarios
resembling everyday life events. In this work, we introduce a continual
learning framework, Memory Embedded in Gated LLMs (MEGa), which injects event
memories directly into the weights of LLMs. Each memory is stored in a
dedicated set of gated low-rank weights. During inference, a gating mechanism
activates relevant memory weights by matching query embeddings to stored memory
embeddings. This enables the model to both recall entire memories and answer
related questions. On two datasets - fictional characters and Wikipedia events
- MEGa outperforms baseline approaches in mitigating catastrophic forgetting.
Our model draws inspiration from the complementary memory system of the human
brain.

</details>


### [22] [Talk Before You Retrieve: Agent-Led Discussions for Better RAG in Medical QA](https://arxiv.org/abs/2504.21252)
*Xuanzhao Dong, Wenhui Zhu, Hao Wang, Xiwen Chen, Peijie Qiu, Rui Yin, Yi Su, Yalin Wang*

Main category: cs.CL

TL;DR: Discuss-RAG enhances medical QA by using collaborative agent-based reasoning to improve retrieval relevance and accuracy.


<details>
  <summary>Details</summary>
Motivation: Addressing limitations in existing medical RAG systems, such as lack of human-like reasoning and suboptimal corpora, to improve answer accuracy.

Method: Introduces a summarizer agent for multi-turn brainstorming and a decision-making agent to evaluate retrieved snippets.

Result: Outperforms MedRAG, improving accuracy by up to 16.67% on BioASQ and 12.20% on PubMedQA.

Conclusion: Discuss-RAG is an effective plug-and-play module for enhancing medical QA systems.

Abstract: Medical question answering (QA) is a reasoning-intensive task that remains
challenging for large language models (LLMs) due to hallucinations and outdated
domain knowledge. Retrieval-Augmented Generation (RAG) provides a promising
post-training solution by leveraging external knowledge. However, existing
medical RAG systems suffer from two key limitations: (1) a lack of modeling for
human-like reasoning behaviors during information retrieval, and (2) reliance
on suboptimal medical corpora, which often results in the retrieval of
irrelevant or noisy snippets. To overcome these challenges, we propose
Discuss-RAG, a plug-and-play module designed to enhance the medical QA RAG
system through collaborative agent-based reasoning. Our method introduces a
summarizer agent that orchestrates a team of medical experts to emulate
multi-turn brainstorming, thereby improving the relevance of retrieved content.
Additionally, a decision-making agent evaluates the retrieved snippets before
their final integration. Experimental results on four benchmark medical QA
datasets show that Discuss-RAG consistently outperforms MedRAG, especially
significantly improving answer accuracy by up to 16.67% on BioASQ and 12.20% on
PubMedQA. The code is available at: https://github.com/LLM-VLM-GSL/Discuss-RAG.

</details>


### [23] [BiasGuard: A Reasoning-enhanced Bias Detection Tool For Large Language Models](https://arxiv.org/abs/2504.21299)
*Zhiting Fan, Ruizhe Chen, Zuozhu Liu*

Main category: cs.CL

TL;DR: BiasGuard is a new tool for detecting bias in LLM-generated content, using a two-stage approach (reasoning and reinforcement learning) to outperform existing methods.


<details>
  <summary>Details</summary>
Motivation: Existing methods for bias detection in LLMs lack understanding of intentions and clear fairness criteria, necessitating a more robust solution.

Method: BiasGuard uses a two-stage approach: explicit reasoning based on fairness specifications and reinforcement learning to improve judgments.

Result: Experiments on five datasets show BiasGuard improves accuracy and reduces over-fairness misjudgments compared to existing tools.

Conclusion: BiasGuard demonstrates the effectiveness of reasoning-enhanced decision-making and a two-stage optimization pipeline for bias detection.

Abstract: Identifying bias in LLM-generated content is a crucial prerequisite for
ensuring fairness in LLMs. Existing methods, such as fairness classifiers and
LLM-based judges, face limitations related to difficulties in understanding
underlying intentions and the lack of criteria for fairness judgment. In this
paper, we introduce BiasGuard, a novel bias detection tool that explicitly
analyzes inputs and reasons through fairness specifications to provide accurate
judgments. BiasGuard is implemented through a two-stage approach: the first
stage initializes the model to explicitly reason based on fairness
specifications, while the second stage leverages reinforcement learning to
enhance its reasoning and judgment capabilities. Our experiments, conducted
across five datasets, demonstrate that BiasGuard outperforms existing tools,
improving accuracy and reducing over-fairness misjudgments. We also highlight
the importance of reasoning-enhanced decision-making and provide evidence for
the effectiveness of our two-stage optimization pipeline.

</details>


### [24] [Confidence in Large Language Model Evaluation: A Bayesian Approach to Limited-Sample Challenges](https://arxiv.org/abs/2504.21303)
*Xiao Xiao, Yu Su, Sijing Zhang, Zhang Chen, Yadong Chen, Tian Liu*

Main category: cs.CL

TL;DR: A Bayesian approach for evaluating LLMs outperforms deterministic metrics, offering robust insights even with limited data.


<details>
  <summary>Details</summary>
Motivation: Conventional deterministic metrics fail to capture the probabilistic nature of LLMs, especially in limited-sample scenarios.

Method: The study treats LLM capabilities as latent variables, uses a curated query set, and formalizes evaluation as Bayesian hypothesis testing.

Result: The method shows superior discrimination and robustness with GPT-series models, even with reduced sample sizes.

Conclusion: This work bridges Bayesian inference with practical LLM evaluation, improving reliability and actionable insights.

Abstract: Large language models (LLMs) exhibit probabilistic output characteristics,
yet conventional evaluation frameworks rely on deterministic scalar metrics.
This study introduces a Bayesian approach for LLM capability assessment that
integrates prior knowledge through probabilistic inference, addressing
limitations under limited-sample regimes. By treating model capabilities as
latent variables and leveraging a curated query set to induce discriminative
responses, we formalize model ranking as a Bayesian hypothesis testing problem
over mutually exclusive capability intervals. Experimental evaluations with
GPT-series models demonstrate that the proposed method achieves superior
discrimination compared to conventional evaluation methods. Results indicate
that even with reduced sample sizes, the approach maintains statistical
robustness while providing actionable insights, such as probabilistic
statements about a model's likelihood of surpassing specific baselines. This
work advances LLM evaluation methodologies by bridging Bayesian inference with
practical constraints in real-world deployment scenarios.

</details>


### [25] [Does the Prompt-based Large Language Model Recognize Students' Demographics and Introduce Bias in Essay Scoring?](https://arxiv.org/abs/2504.21330)
*Kaixun Yang, Mladen Raković, Dragan Gašević, Guanliang Chen*

Main category: cs.CL

TL;DR: The study explores bias in prompt-based LLMs for AES, finding that demographic inference capabilities correlate with scoring biases, particularly against non-native English speakers.


<details>
  <summary>Details</summary>
Motivation: To investigate whether biases in fine-tuned LLMs persist in prompt-based paradigms and how demographic inference affects scoring fairness.

Method: Used GPT-4o to infer demographics from essays and assessed fairness in scoring via multivariate regression on a dataset of 25,000+ essays.

Result: LLMs can infer demographics, especially first-language backgrounds, and scoring biases worsen when demographics are correctly predicted, particularly for non-native speakers.

Conclusion: Prompt-based LLMs exhibit biases linked to demographic inference, highlighting the need for fairness-aware designs in AES tools.

Abstract: Large Language Models (LLMs) are widely used in Automated Essay Scoring (AES)
due to their ability to capture semantic meaning. Traditional fine-tuning
approaches required technical expertise, limiting accessibility for educators
with limited technical backgrounds. However, prompt-based tools like ChatGPT
have made AES more accessible, enabling educators to obtain machine-generated
scores using natural-language prompts (i.e., the prompt-based paradigm).
Despite advancements, prior studies have shown bias in fine-tuned LLMs,
particularly against disadvantaged groups. It remains unclear whether such
biases persist or are amplified in the prompt-based paradigm with cutting-edge
tools. Since such biases are believed to stem from the demographic information
embedded in pre-trained models (i.e., the ability of LLMs' text embeddings to
predict demographic attributes), this study explores the relationship between
the model's predictive power of students' demographic attributes based on their
written works and its predictive bias in the scoring task in the prompt-based
paradigm. Using a publicly available dataset of over 25,000 students'
argumentative essays, we designed prompts to elicit demographic inferences
(i.e., gender, first-language background) from GPT-4o and assessed fairness in
automated scoring. Then we conducted multivariate regression analysis to
explore the impact of the model's ability to predict demographics on its
scoring outcomes. Our findings revealed that (i) prompt-based LLMs can somewhat
infer students' demographics, particularly their first-language backgrounds,
from their essays; (ii) scoring biases are more pronounced when the LLM
correctly predicts students' first-language background than when it does not;
and (iii) scoring error for non-native English speakers increases when the LLM
correctly identifies them as non-native.

</details>


### [26] [Retrieval-Enhanced Few-Shot Prompting for Speech Event Extraction](https://arxiv.org/abs/2504.21372)
*Máté Gedeon*

Main category: cs.CL

TL;DR: A modular pipeline for Speech Event Extraction (SpeechEE) combines ASR and NLP, using hybrid filtering and LLM prompting with semantic search, achieving strong performance with o1-mini.


<details>
  <summary>Details</summary>
Motivation: To address the challenge of extracting structured event information from spoken language by integrating ASR and NLP, leveraging LLMs for improved accuracy.

Method: A pipeline framework with hybrid filtering (rule-based, BERT-based, LLM-based) and few-shot LLM prompting enhanced by semantic similarity retrieval.

Result: o1-mini achieved 63.3% F1 on trigger classification and 27.8% F1 on argument classification, outperforming benchmarks.

Conclusion: Pipeline approaches with retrieval-augmented LLMs can rival end-to-end systems, offering interpretability and modularity, with potential for future hybrid models.

Abstract: Speech Event Extraction (SpeechEE) is a challenging task that lies at the
intersection of Automatic Speech Recognition (ASR) and Natural Language
Processing (NLP), requiring the identification of structured event information
from spoken language. In this work, we present a modular, pipeline-based
SpeechEE framework that integrates high-performance ASR with semantic
search-enhanced prompting of Large Language Models (LLMs). Our system first
classifies speech segments likely to contain events using a hybrid filtering
mechanism including rule-based, BERT-based, and LLM-based models. It then
employs few-shot LLM prompting, dynamically enriched via semantic similarity
retrieval, to identify event triggers and extract corresponding arguments. We
evaluate the pipeline using multiple LLMs (Llama3-8B, GPT-4o-mini, and o1-mini)
highlighting significant performance gains with o1-mini, which achieves 63.3%
F1 on trigger classification and 27.8% F1 on argument classification,
outperforming prior benchmarks. Our results demonstrate that pipeline
approaches, when empowered by retrieval-augmented LLMs, can rival or exceed
end-to-end systems while maintaining interpretability and modularity. This work
provides practical insights into LLM-driven event extraction and opens pathways
for future hybrid models combining textual and acoustic features.

</details>


### [27] [The Distribution of Dependency Distance and Hierarchical Distance in Contemporary Written Japanese and Its Influencing Factors](https://arxiv.org/abs/2504.21421)
*Linxuan Wang, Shuiyuan Yu*

Main category: cs.CL

TL;DR: The study examines how dependency distance (DD) and hierarchical distance (HD) in Japanese are influenced by predicate valency, revealing a trade-off between linear and hierarchical complexity regulated by native speakers.


<details>
  <summary>Details</summary>
Motivation: To understand the relationship between DD and HD in Japanese and identify the role of predicate valency in regulating these linguistic complexities.

Method: Analyzed probability distributions of DD and HD, mean dependency distance (MDD), and mean hierarchical distance (MHD) with varying sentence lengths using the Balanced Corpus of Contemporary Written Japanese.

Result: Predicate valency underlies the trade-off between MDD and MHD, affecting their distributions. Valency's impact on HD is greater than on DD, causing MDD to be lower than MHD.

Conclusion: Native speakers manage linguistic complexity via predicate valency, which differentially influences DD and HD, explaining their distinct probability distributions and means.

Abstract: To explore the relationship between dependency distance (DD) and hierarchical
distance (HD) in Japanese, we compared the probability distributions of DD and
HD with and without sentence length fixed, and analyzed the changes in mean
dependency distance (MDD) and mean hierarchical distance (MHD) as sentence
length increases, along with their correlation coefficient based on the
Balanced Corpus of Contemporary Written Japanese. It was found that the valency
of the predicates is the underlying factor behind the trade-off relation
between MDD and MHD in Japanese. Native speakers of Japanese regulate the
linear complexity and hierarchical complexity through the valency of the
predicates, and the relative sizes of MDD and MHD depend on whether the
threshold of valency has been reached. Apart from the cognitive load, the
valency of the predicates also affects the probability distributions of DD and
HD. The effect of the valency of the predicates on the distribution of HD is
greater than on that of DD, which leads to differences in their probability
distributions and causes the mean of MDD to be lower than that of MHD.

</details>


### [28] [RWKV-X: A Linear Complexity Hybrid Language Model](https://arxiv.org/abs/2504.21463)
*Haowen Hou, Zhiyi Huang, Kaifeng Tan, Rongchang Lu, Fei Richard Yu*

Main category: cs.CL

TL;DR: RWKV-X is a hybrid architecture combining RWKV's efficiency for short-range modeling with sparse attention for long-range context, achieving linear-time training and constant-time inference.


<details>
  <summary>Details</summary>
Motivation: To address the inefficiency of full attention layers in hybrid models while maintaining accuracy for both short and long-context tasks.

Method: Combines RWKV for short-range modeling with a sparse attention mechanism, enabling linear-time training and constant-time inference.

Result: Achieves near-perfect accuracy on 64K-token sequences, outperforms RWKV-7 on long-context benchmarks, and maintains strong short-context performance.

Conclusion: RWKV-X is a scalable, efficient backbone for general-purpose language modeling, capable of handling sequences up to 1 million tokens.

Abstract: In this paper, we introduce \textbf{RWKV-X}, a novel hybrid architecture that
combines the efficiency of RWKV for short-range modeling with a sparse
attention mechanism designed to capture long-range context. Unlike previous
hybrid approaches that rely on full attention layers and retain quadratic
complexity, RWKV-X achieves linear-time complexity in training and
constant-time complexity in inference decoding. We demonstrate that RWKV-X,
when continually pretrained on 64K-token sequences, achieves near-perfect
accuracy on the 64K passkey retrieval benchmark. It consistently outperforms
prior RWKV-7 models on long-context benchmarks, while maintaining strong
performance on short-context tasks. These results highlight RWKV-X as a
scalable and efficient backbone for general-purpose language modeling, capable
of decoding sequences up to 1 million tokens with stable speed and memory
usage. To facilitate further research and analysis, we have made the
checkpoints and the associated code publicly accessible at:
https://github.com/howard-hou/RWKV-X.

</details>


### [29] [Homa at SemEval-2025 Task 5: Aligning Librarian Records with OntoAligner for Subject Tagging](https://arxiv.org/abs/2504.21474)
*Hadi Bayrami Asl Tekanlou, Jafar Razmara, Mahsa Sanaei, Mostafa Rahgouy, Hamed Babaei Giglou*

Main category: cs.CL

TL;DR: Homa system uses OntoAligner and RAG for subject tagging in TIBKAT records, matching them to GND categories via semantic similarity.


<details>
  <summary>Details</summary>
Motivation: To automate subject labeling in technical records using the GND taxonomy, improving efficiency in digital libraries.

Method: Leverages OntoAligner for ontology alignment and RAG techniques to match records to GND categories by semantic similarity.

Result: Shows strengths and limitations in handling multilingual records, proving alignment techniques' potential for subject tagging.

Conclusion: Alignment techniques like OntoAligner can enhance subject tagging in digital libraries, though challenges remain.

Abstract: This paper presents our system, Homa, for SemEval-2025 Task 5: Subject
Tagging, which focuses on automatically assigning subject labels to technical
records from TIBKAT using the Gemeinsame Normdatei (GND) taxonomy. We leverage
OntoAligner, a modular ontology alignment toolkit, to address this task by
integrating retrieval-augmented generation (RAG) techniques. Our approach
formulates the subject tagging problem as an alignment task, where records are
matched to GND categories based on semantic similarity. We evaluate
OntoAligner's adaptability for subject indexing and analyze its effectiveness
in handling multilingual records. Experimental results demonstrate the
strengths and limitations of this method, highlighting the potential of
alignment techniques for improving subject tagging in digital libraries.

</details>


### [30] [Advancing Arabic Reverse Dictionary Systems: A Transformer-Based Approach with Dataset Construction Guidelines](https://arxiv.org/abs/2504.21475)
*Serry Sibaee, Samar Ahmed, Abdullah Al Harbi, Omer Nacar, Adel Ammar, Yasser Habashi, Wadii Boulila*

Main category: cs.CL

TL;DR: The paper introduces a transformer-based Arabic Reverse Dictionary (RD) system, achieving state-of-the-art results with ARBERTv2. It includes a dataset construction process, quality standards, and a Python library (RDTL).


<details>
  <summary>Details</summary>
Motivation: Addressing the gap in Arabic NLP by enabling users to find words via descriptions, benefiting language learning and professional communication.

Method: A transformer-based semi-encoder neural network with geometrically decreasing layers, tested with pre-trained models like ARBERTv2.

Result: ARBERTv2 outperforms multilingual models with a ranking score of 0.0644. Eight quality standards for Arabic lexicographic definitions are established.

Conclusion: The work advances Arabic computational linguistics, offering practical tools and insights for high-quality Arabic RD resources.

Abstract: This study addresses the critical gap in Arabic natural language processing
by developing an effective Arabic Reverse Dictionary (RD) system that enables
users to find words based on their descriptions or meanings. We present a novel
transformer-based approach with a semi-encoder neural network architecture
featuring geometrically decreasing layers that achieves state-of-the-art
results for Arabic RD tasks. Our methodology incorporates a comprehensive
dataset construction process and establishes formal quality standards for
Arabic lexicographic definitions. Experiments with various pre-trained models
demonstrate that Arabic-specific models significantly outperform general
multilingual embeddings, with ARBERTv2 achieving the best ranking score
(0.0644). Additionally, we provide a formal abstraction of the reverse
dictionary task that enhances theoretical understanding and develop a modular,
extensible Python library (RDTL) with configurable training pipelines. Our
analysis of dataset quality reveals important insights for improving Arabic
definition construction, leading to eight specific standards for building
high-quality reverse dictionary resources. This work contributes significantly
to Arabic computational linguistics and provides valuable tools for language
learning, academic writing, and professional communication in Arabic.

</details>


### [31] [Improving Informally Romanized Language Identification](https://arxiv.org/abs/2504.21540)
*Adrian Benton, Alexander Gutkin, Christo Kirov, Brian Roark*

Main category: cs.CL

TL;DR: Improving language identification (LID) for romanized text by using synthetic training data with natural spelling variation, achieving state-of-the-art results on Indic languages.


<details>
  <summary>Details</summary>
Motivation: Romanized text of languages with non-Latin scripts (e.g., Hindi, Urdu) has high spelling variability, making language identification challenging.

Method: Enhancing LID accuracy by synthesizing training sets that incorporate natural spelling variation, comparing it to naturally occurring examples and higher-capacity models.

Result: Achieved 85.4% F1 with synthetic data alone and 88.2% when combined with harvested text, surpassing the previous 74.7% F1.

Conclusion: Synthetic training data with spelling variation significantly improves LID performance for romanized text, outperforming traditional methods.

Abstract: The Latin script is often used to informally write languages with non-Latin
native scripts. In many cases (e.g., most languages in India), there is no
conventional spelling of words in the Latin script, hence there will be high
spelling variability in written text. Such romanization renders languages that
are normally easily distinguished based on script highly confusable, such as
Hindi and Urdu. In this work, we increase language identification (LID)
accuracy for romanized text by improving the methods used to synthesize
training sets. We find that training on synthetic samples which incorporate
natural spelling variation yields higher LID system accuracy than including
available naturally occurring examples in the training set, or even training
higher capacity models. We demonstrate new state-of-the-art LID performance on
romanized text from 20 Indic languages in the Bhasha-Abhijnaanam evaluation set
(Madhani et al., 2023a), improving test F1 from the reported 74.7% (using a
pretrained neural model) to 85.4% using a linear classifier trained solely on
synthetic data and 88.2% when also training on available harvested text.

</details>


### [32] [TartuNLP at SemEval-2025 Task 5: Subject Tagging as Two-Stage Information Retrieval](https://arxiv.org/abs/2504.21547)
*Aleksei Dorkin, Kairit Sirts*

Main category: cs.CL

TL;DR: A two-stage information retrieval system using bi-encoder and cross-encoder models improves subject tag assignment for library records, outperforming single-stage methods.


<details>
  <summary>Details</summary>
Motivation: To assist librarians in efficiently assigning subject tags to library records by automating the retrieval of relevant tags from a large taxonomy.

Method: A two-stage system: bi-encoder for coarse-grained candidate extraction and cross-encoder for fine-grained re-ranking.

Result: Significant improvements in recall and competitive qualitative results compared to single-stage methods.

Conclusion: The proposed two-stage approach is effective for subject tag retrieval, enhancing accuracy and efficiency.

Abstract: We present our submission to the Task 5 of SemEval-2025 that aims to aid
librarians in assigning subject tags to the library records by producing a list
of likely relevant tags for a given document. We frame the task as an
information retrieval problem, where the document content is used to retrieve
subject tags from a large subject taxonomy. We leverage two types of encoder
models to build a two-stage information retrieval system -- a bi-encoder for
coarse-grained candidate extraction at the first stage, and a cross-encoder for
fine-grained re-ranking at the second stage. This approach proved effective,
demonstrating significant improvements in recall compared to single-stage
methods and showing competitive results according to qualitative evaluation.

</details>


### [33] [Precision Where It Matters: A Novel Spike Aware Mixed-Precision Quantization Strategy for LLaMA-based Language Models](https://arxiv.org/abs/2504.21553)
*Lucas Maisonnave, Cyril Moineau, Olivier Bichler, Fabrice Rastello*

Main category: cs.CL

TL;DR: The paper proposes a mixed-precision quantization method for LLaMA-like LLMs, focusing on specific projection layers with activation spikes, achieving better performance than general-purpose techniques.


<details>
  <summary>Details</summary>
Motivation: Addressing deployment challenges of large LLMs by improving quantization efficiency, targeting LLaMA architectures.

Method: Mixed-precision quantization: higher precision for projection layers with activation spikes, lower bit-widths for the rest.

Result: Superior perplexity and zero-shot accuracy, especially for 8-bit quantization, outperforming general-purpose methods.

Conclusion: Architecture-specific quantization, targeting key layers, enhances LLM efficiency and deployability in resource-constrained settings.

Abstract: Large Language Models (LLMs) have demonstrated remarkable capabilities in
various natural language processing tasks. However, their size presents
significant challenges for deployment and inference. This paper investigates
the quantization of LLMs, focusing on the LLaMA architecture and its
derivatives. We challenge existing assumptions about activation outliers in
LLMs and propose a novel mixed-precision quantization approach tailored for
LLaMA-like models. Our method leverages the observation that activation spikes
in LLaMA architectures are predominantly concentrated in specific projection
layers. By applying higher precision (FP16 or FP8) to these layers while
quantizing the rest of the model to lower bit-widths, we achieve superior
performance compared to existing quantization techniques. Experimental results
on LLaMA2, LLaMA3, and Mistral models demonstrate significant improvements in
perplexity and zero-shot accuracy, particularly for 8-bit per-tensor
quantization. Our approach outperforms general-purpose methods designed to
handle outliers across all architecture types, highlighting the benefits of
architecture-specific quantization strategies. This research contributes to the
ongoing efforts to make LLMs more efficient and deployable, potentially
enabling their use in resource-constrained environments. Our findings emphasize
the importance of considering model-specific characteristics in developing
effective quantization pipelines for state-of-the-art language models by
identifying and targeting a small number of projections that concentrate
activation spikes.

</details>


### [34] [DNB-AI-Project at SemEval-2025 Task 5: An LLM-Ensemble Approach for Automated Subject Indexing](https://arxiv.org/abs/2504.21589)
*Lisa Kluge, Maximilian Kähler*

Main category: cs.CL

TL;DR: A system for automated subject tagging using LLMs, combining few-shot prompting and post-processing, ranked fourth quantitatively but best qualitatively.


<details>
  <summary>Details</summary>
Motivation: To automate subject tagging for a national technical library's open-access catalog using LLMs.

Method: Uses few-shot prompting with LLMs, followed by post-processing to map keywords, aggregate votes, and rank relevance.

Result: Ranked fourth in quantitative evaluation but top in qualitative expert assessment.

Conclusion: The system effectively combines LLMs and post-processing for subject tagging, excelling in qualitative performance.

Abstract: This paper presents our system developed for the SemEval-2025 Task 5:
LLMs4Subjects: LLM-based Automated Subject Tagging for a National Technical
Library's Open-Access Catalog. Our system relies on prompting a selection of
LLMs with varying examples of intellectually annotated records and asking the
LLMs to similarly suggest keywords for new records. This few-shot prompting
technique is combined with a series of post-processing steps that map the
generated keywords to the target vocabulary, aggregate the resulting subject
terms to an ensemble vote and, finally, rank them as to their relevance to the
record. Our system is fourth in the quantitative ranking in the all-subjects
track, but achieves the best result in the qualitative ranking conducted by
subject indexing experts.

</details>


### [35] [Robust Misinformation Detection by Visiting Potential Commonsense Conflict](https://arxiv.org/abs/2504.21604)
*Bing Wang, Ximing Li, Changchun Li, Bingrui Zhao, Bo Fu, Renchu Guan, Shengsheng Wang*

Main category: cs.CL

TL;DR: The paper introduces MD-PCC, a plug-and-play augmentation method for Misinformation Detection (MD) that leverages commonsense conflict to improve detection accuracy. It also presents a new dataset, CoMis, and shows MD-PCC outperforms existing baselines.


<details>
  <summary>Details</summary>
Motivation: The rise of misinformation online necessitates automated detection methods. Prior work suggests fake articles often involve commonsense conflicts, inspiring this approach.

Method: MD-PCC constructs commonsense expressions for articles, highlighting conflicts between extracted and golden commonsense triplets. These augmentations are used to train MD models. A new dataset, CoMis, is also introduced.

Result: MD-PCC consistently outperforms existing MD baselines across 4 public datasets and the new CoMis dataset.

Conclusion: The proposed MD-PCC method effectively enhances misinformation detection by leveraging commonsense conflicts, demonstrating superior performance over existing methods.

Abstract: The development of Internet technology has led to an increased prevalence of
misinformation, causing severe negative effects across diverse domains. To
mitigate this challenge, Misinformation Detection (MD), aiming to detect online
misinformation automatically, emerges as a rapidly growing research topic in
the community. In this paper, we propose a novel plug-and-play augmentation
method for the MD task, namely Misinformation Detection with Potential
Commonsense Conflict (MD-PCC). We take inspiration from the prior studies
indicating that fake articles are more likely to involve commonsense conflict.
Accordingly, we construct commonsense expressions for articles, serving to
express potential commonsense conflicts inferred by the difference between
extracted commonsense triplet and golden ones inferred by the well-established
commonsense reasoning tool COMET. These expressions are then specified for each
article as augmentation. Any specific MD methods can be then trained on those
commonsense-augmented articles. Besides, we also collect a novel
commonsense-oriented dataset named CoMis, whose all fake articles are caused by
commonsense conflict. We integrate MD-PCC with various existing MD backbones
and compare them across both 4 public benchmark datasets and CoMis. Empirical
results demonstrate that MD-PCC can consistently outperform the existing MD
baselines.

</details>


### [36] [RDF-Based Structured Quality Assessment Representation of Multilingual LLM Evaluations](https://arxiv.org/abs/2504.21605)
*Jonas Gwozdz, Andreas Both*

Main category: cs.CL

TL;DR: An RDF-based framework assesses multilingual LLM reliability under conflicting information, analyzing knowledge leakage, error detection, and multilingual consistency in German and English.


<details>
  <summary>Details</summary>
Motivation: To systematically evaluate LLM reliability when faced with conflicting or incomplete information, addressing gaps in current assessment methods.

Method: Proposes an RDF-based framework to capture LLM responses across four context conditions (complete, incomplete, conflicting, no-context) in German and English.

Result: The framework successfully analyzed knowledge leakage, error detection, and multilingual consistency, with a fire safety domain experiment revealing context prioritization and language-specific performance patterns.

Conclusion: The framework is effective for comprehensive LLM assessment, with its vocabulary sufficient to cover all facets of the 28-question study.

Abstract: Large Language Models (LLMs) increasingly serve as knowledge interfaces, yet
systematically assessing their reliability with conflicting information remains
difficult. We propose an RDF-based framework to assess multilingual LLM
quality, focusing on knowledge conflicts. Our approach captures model responses
across four distinct context conditions (complete, incomplete, conflicting, and
no-context information) in German and English. This structured representation
enables the comprehensive analysis of knowledge leakage-where models favor
training data over provided context-error detection, and multilingual
consistency. We demonstrate the framework through a fire safety domain
experiment, revealing critical patterns in context prioritization and
language-specific performance, and demonstrating that our vocabulary was
sufficient to express every assessment facet encountered in the 28-question
study.

</details>


### [37] [Meeseeks: An Iterative Benchmark Evaluating LLMs Multi-Turn Instruction-Following Ability](https://arxiv.org/abs/2504.21625)
*Jiaming Wang*

Main category: cs.CL

TL;DR: Meeseeks introduces a benchmark for LLMs to self-correct in iterative feedback, simulating real-world interactions and evaluating 38 capabilities across three dimensions.


<details>
  <summary>Details</summary>
Motivation: Existing benchmarks lack iterative feedback and self-correction, limiting their realism for real-world LLM applications.

Method: Meeseeks uses iterative feedback for self-correction and evaluates LLMs across 38 capability tags in three dimensions: Intent Recognition, Granular Content Validation, and Output Structure Validation.

Result: The benchmark provides insights into LLMs' instruction-following capabilities in practical scenarios.

Conclusion: Meeseeks offers a more realistic and comprehensive evaluation of LLMs' ability to follow instructions in real-world applications.

Abstract: The ability to follow instructions accurately is fundamental for Large
Language Models (LLMs) to serve as reliable agents in real-world applications.
While existing instruction-following benchmarks are either single-turn or
introduce new requirements in each turn without allowing self-correction,
Meeseeks simulates realistic human-LLM interactions through an iterative
feedback process. This design enables models to self-correct based on specific
requirement failures, better reflecting real-world user-end usage patterns. The
benchmark implements a comprehensive evaluation system with 38 capability tags
organized across three dimensions: Intent Recognition, Granular Content
Validation, and Output Structure Validation. Through rigorous evaluation across
LLMs, Meeseeks provides valuable insights into LLMs' instruction-following
capabilities in practical applications.

</details>


### [38] [Sadeed: Advancing Arabic Diacritization Through Small Language Model](https://arxiv.org/abs/2504.21635)
*Zeina Aldallal, Sara Chrouf, Khalil Hennara, Mohamed Motaism Hamed, Muhammad Hreden, Safwan AlModhayan*

Main category: cs.CL

TL;DR: Sadeed, a fine-tuned decoder-only model based on Kuwain 1.5B, achieves competitive Arabic diacritization results with modest resources. The paper also introduces SadeedDiac-25, a new benchmark for fairer evaluation.


<details>
  <summary>Details</summary>
Motivation: Addressing the challenge of Arabic text diacritization due to the language's morphological richness.

Method: Fine-tuning Kuwain 1.5B on curated diacritized datasets using a rigorous data-cleaning pipeline.

Result: Sadeed outperforms traditional models and competes with proprietary large models despite limited resources.

Conclusion: Sadeed and SadeedDiac-25 advance Arabic NLP applications like machine translation and text-to-speech.

Abstract: Arabic text diacritization remains a persistent challenge in natural language
processing due to the language's morphological richness. In this paper, we
introduce Sadeed, a novel approach based on a fine-tuned decoder-only language
model adapted from Kuwain 1.5B Hennara et al. [2025], a compact model
originally trained on diverse Arabic corpora. Sadeed is fine-tuned on carefully
curated, high-quality diacritized datasets, constructed through a rigorous
data-cleaning and normalization pipeline. Despite utilizing modest
computational resources, Sadeed achieves competitive results compared to
proprietary large language models and outperforms traditional models trained on
similar domains. Additionally, we highlight key limitations in current
benchmarking practices for Arabic diacritization. To address these issues, we
introduce SadeedDiac-25, a new benchmark designed to enable fairer and more
comprehensive evaluation across diverse text genres and complexity levels.
Together, Sadeed and SadeedDiac-25 provide a robust foundation for advancing
Arabic NLP applications, including machine translation, text-to-speech, and
language learning tools.

</details>


### [39] [20min-XD: A Comparable Corpus of Swiss News Articles](https://arxiv.org/abs/2504.21677)
*Michelle Wastl, Jannis Vamvas, Selena Calleri, Rico Sennrich*

Main category: cs.CL

TL;DR: 20min-XD is a French-German document-level comparable corpus of news articles, aligned by semantic similarity, useful for NLP and linguistic studies.


<details>
  <summary>Details</summary>
Motivation: To create a cross-lingual dataset for NLP applications and linguistic research.

Method: Automatically aligned 15,000 article pairs (2015-2024) based on semantic similarity.

Result: A diverse corpus ranging from near-translations to loosely related articles.

Conclusion: The dataset is publicly released for research and NLP applications.

Abstract: We present 20min-XD (20 Minuten cross-lingual document-level), a
French-German, document-level comparable corpus of news articles, sourced from
the Swiss online news outlet 20 Minuten/20 minutes. Our dataset comprises
around 15,000 article pairs spanning 2015 to 2024, automatically aligned based
on semantic similarity. We detail the data collection process and alignment
methodology. Furthermore, we provide a qualitative and quantitative analysis of
the corpus. The resulting dataset exhibits a broad spectrum of cross-lingual
similarity, ranging from near-translations to loosely related articles, making
it valuable for various NLP applications and broad linguistically motivated
studies. We publicly release the dataset in document- and sentence-aligned
versions and code for the described experiments.

</details>


### [40] [Investigating the Effect of Parallel Data in the Cross-Lingual Transfer for Vision-Language Encoders](https://arxiv.org/abs/2504.21681)
*Andrei-Alexandru Manea, Jindřich Libovický*

Main category: cs.CL

TL;DR: The paper explores transferring pre-trained Vision-Language models to multilingual tasks using parallel data, highlighting the effectiveness of machine-translated task data and multilingual training.


<details>
  <summary>Details</summary>
Motivation: Most VL models and training data are English-only, limiting multilingual applications. The study investigates transferring trained encoders with parallel data as an alternative to cross-lingual transfer.

Method: The approach involves transferring a pre-trained encoder using parallel data, analyzing the impact of data domain and language count.

Result: Machine-translated task data performed best on average, but authentic caption-like data excelled in some languages. Multilingual training benefited most languages.

Conclusion: Parallel data, especially machine-translated, is effective for multilingual VL tasks, with multilingual training enhancing performance across languages.

Abstract: Most pre-trained Vision-Language (VL) models and training data for the
downstream tasks are only available in English. Therefore, multilingual VL
tasks are solved using cross-lingual transfer: fine-tune a multilingual
pre-trained model or transfer the text encoder using parallel data. We study
the alternative approach: transferring an already trained encoder using
parallel data. We investigate the effect of parallel data: domain and the
number of languages, which were out of focus in previous work. Our results show
that even machine-translated task data are the best on average, caption-like
authentic parallel data outperformed it in some languages. Further, we show
that most languages benefit from multilingual training.

</details>


### [41] [Enhancing Health Mention Classification Performance: A Study on Advancements in Parameter Efficient Tuning](https://arxiv.org/abs/2504.21685)
*Reem Abdel-Salam, Mary Adewunmi*

Main category: cs.CL

TL;DR: The paper proposes using enhanced biomedical NLP methods, including POS tagger info and PEFT techniques, to improve Health Mention Classification (HMC) in social media, achieving better F1-scores with smaller models.


<details>
  <summary>Details</summary>
Motivation: HMC is challenging due to contextual complexities like figurative language. Clearer mentions can be achieved through improved NLP methods.

Method: Explores POS tagger info and PEFT techniques, testing on datasets RHDM, PHM, and Illness.

Result: POS tagger info and PEFT techniques boost F1-scores, outperforming state-of-the-art methods with smaller models.

Conclusion: The approach effectively classifies health mentions while optimizing model size and training efficiency.

Abstract: Health Mention Classification (HMC) plays a critical role in leveraging
social media posts for real-time tracking and public health monitoring.
Nevertheless, the process of HMC presents significant challenges due to its
intricate nature, primarily stemming from the contextual aspects of health
mentions, such as figurative language and descriptive terminology, rather than
explicitly reflecting a personal ailment. To address this problem, we argue
that clearer mentions can be achieved through conventional fine-tuning with
enhanced parameters of biomedical natural language methods (NLP). In this
study, we explore different techniques such as the utilisation of
part-of-speech (POS) tagger information, improving on PEFT techniques, and
different combinations thereof. Extensive experiments are conducted on three
widely used datasets: RHDM, PHM, and Illness. The results incorporated POS
tagger information, and leveraging PEFT techniques significantly improves
performance in terms of F1-score compared to state-of-the-art methods across
all three datasets by utilising smaller models and efficient training.
Furthermore, the findings highlight the effectiveness of incorporating POS
tagger information and leveraging PEFT techniques for HMC. In conclusion, the
proposed methodology presents a potentially effective approach to accurately
classifying health mentions in social media posts while optimising the model
size and training efficiency.

</details>


### [42] [Investigating Literary Motifs in Ancient and Medieval Novels with Large Language Models](https://arxiv.org/abs/2504.21742)
*Emelie Hallenberg*

Main category: cs.CL

TL;DR: The study uses fine-tuned language models to analyze motifs in Greek love novels from the 1st to 15th century, revealing persistent and fluctuating patterns.


<details>
  <summary>Details</summary>
Motivation: To identify common and differing literary motifs in Greek love novels over centuries.

Method: Application of fine-tuned large language models to extract and analyze motifs.

Result: Some motifs persist throughout the corpus, while others fluctuate, indicating trends or external influences.

Conclusion: The method effectively extracts motifs, enabling quantitative and qualitative analysis.

Abstract: The Greek fictional narratives often termed love novels or romances, ranging
from the first century CE to the middle of the 15th century, have long been
considered as similar in many ways, not least in the use of particular literary
motifs. By applying the use of fine-tuned large language models, this study
aims to investigate which motifs exactly that the texts in this corpus have in
common, and in which ways they differ from each other. The results show that
while some motifs persist throughout the corpus, others fluctuate in frequency,
indicating certain trends or external influences. Conclusively, the method
proves to adequately extract literary motifs according to a set definition,
providing data for both quantitative and qualitative analyses.

</details>


### [43] [Improving Retrieval-Augmented Neural Machine Translation with Monolingual Data](https://arxiv.org/abs/2504.21747)
*Maxime Bouthors, Josep Crego, François Yvon*

Main category: cs.CL

TL;DR: The paper explores improving retrieval-augmented neural machine translation (RANMT) by leveraging monolingual target-side corpora through cross-lingual retrieval systems trained with sentence and word-level objectives.


<details>
  <summary>Details</summary>
Motivation: To enhance RANMT systems by utilizing available in-domain monolingual target-side corpora, which are often more abundant than bilingual resources.

Method: Designs improved cross-lingual retrieval systems trained with sentence-level and word-level matching objectives to retrieve relevant target-language segments based on source-side queries.

Result: Demonstrates superior translation performance over standard TM-based models in controlled settings and outperforms baselines and general-purpose cross-lingual retrievers in real-world setups.

Conclusion: The proposed method effectively leverages monolingual target-side corpora, achieving significant improvements in translation quality.

Abstract: Conventional retrieval-augmented neural machine translation (RANMT) systems
leverage bilingual corpora, e.g., translation memories (TMs). Yet, in many
settings, in-domain monolingual target-side corpora are often available. This
work explores ways to take advantage of such resources by retrieving relevant
segments directly in the target language, based on a source-side query. For
this, we design improved cross-lingual retrieval systems, trained with both
sentence level and word-level matching objectives. In our experiments with two
RANMT architectures, we first demonstrate the benefits of such cross-lingual
objectives in a controlled setting, obtaining translation performances that
surpass standard TM-based models. We then showcase our method on a real-world
set-up, where the target monolingual resources far exceed the amount of
parallel data and observe large improvements of our new techniques, which
outperform both the baseline setting, and general-purpose cross-lingual
retrievers.

</details>


### [44] [MAC-Tuning: LLM Multi-Compositional Problem Reasoning with Enhanced Knowledge Boundary Awareness](https://arxiv.org/abs/2504.21773)
*Junsheng Huang, Zhitao He, Sandeep Polisetty, Qingyun Wang, May Fung*

Main category: cs.CL

TL;DR: A novel method, MAC-Tuning, improves LLM confidence estimation in multi-problem settings, outperforming baselines by 25%.


<details>
  <summary>Details</summary>
Motivation: Addressing LLM hallucination and underexplored awareness of knowledge boundaries in multi-problem settings.

Method: MAC-Tuning separates answer prediction and confidence estimation during fine-tuning.

Result: Outperforms baselines by up to 25% in average precision.

Conclusion: MAC-Tuning effectively enhances LLM confidence estimation in challenging multi-problem scenarios.

Abstract: With the widespread application of large language models (LLMs), the issue of
generating non-existing facts, known as hallucination, has garnered increasing
attention. Previous research in enhancing LLM confidence estimation mainly
focuses on the single problem setting. However, LLM awareness of its internal
parameterized knowledge boundary under the more challenging multi-problem
setting, which requires answering multiple problems accurately simultaneously,
remains underexplored. To bridge this gap, we introduce a novel method,
Multiple Answers and Confidence Stepwise Tuning (MAC-Tuning), that separates
the learning of answer prediction and confidence estimation during fine-tuning
on instruction data. Extensive experiments demonstrate that our method
outperforms baselines by up to 25% in average precision.

</details>


### [45] [WebThinker: Empowering Large Reasoning Models with Deep Research Capability](https://arxiv.org/abs/2504.21776)
*Xiaoxi Li, Jiajie Jin, Guanting Dong, Hongjin Qian, Yutao Zhu, Yongkang Wu, Ji-Rong Wen, Zhicheng Dou*

Main category: cs.CL

TL;DR: WebThinker enhances large reasoning models (LRMs) by enabling dynamic web search and real-time report drafting, outperforming existing methods on complex tasks.


<details>
  <summary>Details</summary>
Motivation: LRMs' static knowledge limits performance on knowledge-intensive tasks; WebThinker addresses this by integrating web search and real-time synthesis.

Method: WebThinker combines a Deep Web Explorer module and an Autonomous Think-Search-and-Draft strategy, trained via RL-based DPO.

Result: Outperforms existing methods on benchmarks (GPQA, GAIA, WebWalkerQA, HLE) and report generation (Glaive).

Conclusion: WebThinker improves LRM reliability and versatility, advancing deep research systems.

Abstract: Large reasoning models (LRMs), such as OpenAI-o1 and DeepSeek-R1, demonstrate
impressive long-horizon reasoning capabilities. However, their reliance on
static internal knowledge limits their performance on complex,
knowledge-intensive tasks and hinders their ability to produce comprehensive
research reports requiring synthesis of diverse web information. To address
this, we propose \textbf{WebThinker}, a deep research agent that empowers LRMs
to autonomously search the web, navigate web pages, and draft research reports
during the reasoning process. WebThinker integrates a \textbf{Deep Web
Explorer} module, enabling LRMs to dynamically search, navigate, and extract
information from the web when encountering knowledge gaps. It also employs an
\textbf{Autonomous Think-Search-and-Draft strategy}, allowing the model to
seamlessly interleave reasoning, information gathering, and report writing in
real time. To further enhance research tool utilization, we introduce an
\textbf{RL-based training strategy} via iterative online Direct Preference
Optimization (DPO). Extensive experiments on complex reasoning benchmarks
(GPQA, GAIA, WebWalkerQA, HLE) and scientific report generation tasks (Glaive)
demonstrate that WebThinker significantly outperforms existing methods and
strong proprietary systems. Our approach enhances LRM reliability and
applicability in complex scenarios, paving the way for more capable and
versatile deep research systems. The code is available at
https://github.com/RUC-NLPIR/WebThinker.

</details>


### [46] [How Real Are Synthetic Therapy Conversations? Evaluating Fidelity in Prolonged Exposure Dialogues](https://arxiv.org/abs/2504.21800)
*Suhas BN, Dominik Mattioli, Saeed Abdullah, Rosa I. Arriaga, Chris W. Wiese, Andrew M. Sherrill*

Main category: cs.CL

TL;DR: Synthetic data in healthcare, particularly for PTSD therapy, shows promise for scalability and privacy but struggles with capturing therapeutic nuances.


<details>
  <summary>Details</summary>
Motivation: Addressing privacy concerns, data scarcity, and high annotation costs in healthcare by exploring synthetic data for PTSD therapy.

Method: Systematic comparison of real and synthetic therapeutic dialogues using linguistic, structural, and protocol-specific metrics, including novel PE-specific fidelity metrics.

Result: Synthetic data matches structural features (e.g., turn-taking) but lacks fidelity in key clinical markers (e.g., distress monitoring).

Conclusion: Synthetic data can complement real-world datasets but requires fidelity-aware metrics to address clinical limitations.

Abstract: The growing adoption of synthetic data in healthcare is driven by privacy
concerns, limited access to real-world data, and the high cost of annotation.
This work explores the use of synthetic Prolonged Exposure (PE) therapeutic
conversations for Post-Traumatic Stress Disorder (PTSD) as a scalable
alternative for training and evaluating clinical models. We systematically
compare real and synthetic dialogues using linguistic, structural, and
protocol-specific metrics, including turn-taking patterns and treatment
fidelity. We also introduce and evaluate PE-specific metrics derived from
linguistic analysis and semantic modeling, offering a novel framework for
assessing clinical fidelity beyond surface fluency. Our findings show that
although synthetic data holds promise for mitigating data scarcity and
protecting patient privacy, it can struggle to capture the subtle dynamics of
therapeutic interactions. In our dataset, synthetic dialogues match structural
features of real-world dialogues (e.g., speaker switch ratio: 0.98 vs. 0.99),
however, synthetic interactions do not adequately reflect key fidelity markers
(e.g., distress monitoring). We highlight gaps in existing evaluation
frameworks and advocate for fidelity-aware metrics that go beyond surface
fluency to uncover clinically significant failures. Our findings clarify where
synthetic data can effectively complement real-world datasets -- and where
critical limitations remain.

</details>


### [47] [DeepSeek-Prover-V2: Advancing Formal Mathematical Reasoning via Reinforcement Learning for Subgoal Decomposition](https://arxiv.org/abs/2504.21801)
*Z. Z. Ren, Zhihong Shao, Junxiao Song, Huajian Xin, Haocheng Wang, Wanjia Zhao, Liyue Zhang, Zhe Fu, Qihao Zhu, Dejian Yang, Z. F. Wu, Zhibin Gou, Shirong Ma, Hongxuan Tang, Yuxuan Liu, Wenjun Gao, Daya Guo, Chong Ruan*

Main category: cs.CL

TL;DR: DeepSeek-Prover-V2 is an open-source LLM for formal theorem proving in Lean 4, achieving state-of-the-art performance with 88.9% pass ratio on MiniF2F-test and solving 6/15 AIME problems.


<details>
  <summary>Details</summary>
Motivation: To integrate informal and formal mathematical reasoning into a unified model for theorem proving.

Method: Uses a recursive theorem proving pipeline with DeepSeek-V3 to decompose problems, synthesize proofs, and initialize reinforcement learning.

Result: Achieves 88.9% pass ratio on MiniF2F-test, solves 49/658 PutnamBench problems, and 6/15 AIME problems.

Conclusion: The gap between formal and informal reasoning in LLMs is narrowing, as shown by DeepSeek-Prover-V2's performance.

Abstract: We introduce DeepSeek-Prover-V2, an open-source large language model designed
for formal theorem proving in Lean 4, with initialization data collected
through a recursive theorem proving pipeline powered by DeepSeek-V3. The
cold-start training procedure begins by prompting DeepSeek-V3 to decompose
complex problems into a series of subgoals. The proofs of resolved subgoals are
synthesized into a chain-of-thought process, combined with DeepSeek-V3's
step-by-step reasoning, to create an initial cold start for reinforcement
learning. This process enables us to integrate both informal and formal
mathematical reasoning into a unified model. The resulting model,
DeepSeek-Prover-V2-671B, achieves state-of-the-art performance in neural
theorem proving, reaching 88.9% pass ratio on the MiniF2F-test and solving 49
out of 658 problems from PutnamBench. In addition to standard benchmarks, we
introduce ProverBench, a collection of 325 formalized problems, to enrich our
evaluation, including 15 selected problems from the recent AIME competitions
(years 24-25). Further evaluation on these 15 AIME problems shows that the
model successfully solves 6 of them. In comparison, DeepSeek-V3 solves 8 of
these problems using majority voting, highlighting that the gap between formal
and informal mathematical reasoning in large language models is substantially
narrowing.

</details>


### [48] [TRUST: An LLM-Based Dialogue System for Trauma Understanding and Structured Assessments](https://arxiv.org/abs/2504.21851)
*Sichang Tu, Abigail Powers, Stephen Doogan, Jinho D. Choi*

Main category: cs.CL

TL;DR: TRUST, an LLM-powered dialogue system, replicates clinician behavior for PTSD diagnostic interviews, showing comparable performance to real clinicians.


<details>
  <summary>Details</summary>
Motivation: To bridge the gap in mental healthcare accessibility by automating diagnostic interviews.

Method: Developed TRUST, a framework of cooperative LLM modules, using a Dialogue Acts schema and patient simulation based on real transcripts.

Result: TRUST performs comparably to real-life clinical interviews, as validated by expert evaluations.

Conclusion: TRUST has potential to enhance mental healthcare availability, with room for improvement in communication styles.

Abstract: Objectives: While Large Language Models (LLMs) have been widely used to
assist clinicians and support patients, no existing work has explored dialogue
systems for standard diagnostic interviews and assessments. This study aims to
bridge the gap in mental healthcare accessibility by developing an LLM-powered
dialogue system that replicates clinician behavior. Materials and Methods: We
introduce TRUST, a framework of cooperative LLM modules capable of conducting
formal diagnostic interviews and assessments for Post-Traumatic Stress Disorder
(PTSD). To guide the generation of appropriate clinical responses, we propose a
Dialogue Acts schema specifically designed for clinical interviews.
Additionally, we develop a patient simulation approach based on real-life
interview transcripts to replace time-consuming and costly manual testing by
clinicians. Results: A comprehensive set of evaluation metrics is designed to
assess the dialogue system from both the agent and patient simulation
perspectives. Expert evaluations by conversation and clinical specialists show
that TRUST performs comparably to real-life clinical interviews. Discussion:
Our system performs at the level of average clinicians, with room for future
enhancements in communication styles and response appropriateness. Conclusions:
Our TRUST framework shows its potential to facilitate mental healthcare
availability.

</details>


### [49] [LLMs and Finetuning: Benchmarking cross-domain performance for hate speech detection](https://arxiv.org/abs/2310.18964)
*Ahmad Nasir, Aadish Sharma, Kokil Jaidka, Saifuddin Ahmed*

Main category: cs.CL

TL;DR: The study explores the effectiveness of pre-trained and fine-tuned Large Language Models (LLMs) in hate speech detection, focusing on performance dependencies, cross-domain generalization, and influential features. Results show LLMs outperform state-of-the-art methods, but limitations in validity and reproducibility persist.


<details>
  <summary>Details</summary>
Motivation: Address the challenge of hate speech detection in diverse digital platforms by leveraging LLMs.

Method: Investigates LLM performance through fine-tuning and training parameters, cross-domain generalization, and dataset/model feature analysis.

Result: LLMs outperform state-of-the-art methods, but fine-grained labels' advantage diminishes with larger datasets. Limitations in validity and reproducibility are noted.

Conclusion: Highlights LLMs' potential for hate speech detection but emphasizes challenges and recommends best practices for future benchmarking experiments.

Abstract: In the evolving landscape of online communication, hate speech detection
remains a formidable challenge, further compounded by the diversity of digital
platforms. This study investigates the effectiveness and adaptability of
pre-trained and fine-tuned Large Language Models (LLMs) in identifying hate
speech, to address two central questions: (1) To what extent does the model
performance depend on the fine-tuning and training parameters?, (2) To what
extent do models generalize to cross-domain hate speech detection? and (3) What
are the specific features of the datasets or models that influence the
generalization potential? The experiment shows that LLMs offer a huge advantage
over the state-of-the-art even without pretraining. Ordinary least squares
analyses suggest that the advantage of training with fine-grained hate speech
labels is washed away with the increase in dataset size. While our research
demonstrates the potential of large language models (LLMs) for hate speech
detection, several limitations remain, particularly regarding the validity and
the reproducibility of the results. We conclude with an exhaustive discussion
of the challenges we faced in our experimentation and offer recommended best
practices for future scholars designing benchmarking experiments of this kind.

</details>


### [50] [Round Trip Translation Defence against Large Language Model Jailbreaking Attacks](https://arxiv.org/abs/2402.13517)
*Canaan Yung, Hadi Mohaghegh Dolatabadi, Sarah Erfani, Christopher Leckie*

Main category: cs.CL

TL;DR: The paper introduces Round Trip Translation (RTT), a method to defend LLMs against social-engineered attacks, improving mitigation rates significantly.


<details>
  <summary>Details</summary>
Motivation: Existing defenses for LLMs against social-engineered attacks are insufficient, mitigating less than half of attacks.

Method: Proposes RTT, which paraphrases adversarial prompts to generalize ideas, aiding LLMs in detecting harmful behavior.

Result: RTT mitigated over 70% of PAIR attacks and reduced MathsAttack success by nearly 40%.

Conclusion: RTT is a versatile, lightweight, and effective defense against social-engineered attacks on LLMs.

Abstract: Large language models (LLMs) are susceptible to social-engineered attacks
that are human-interpretable but require a high level of comprehension for LLMs
to counteract. Existing defensive measures can only mitigate less than half of
these attacks at most. To address this issue, we propose the Round Trip
Translation (RTT) method, the first algorithm specifically designed to defend
against social-engineered attacks on LLMs. RTT paraphrases the adversarial
prompt and generalizes the idea conveyed, making it easier for LLMs to detect
induced harmful behavior. This method is versatile, lightweight, and
transferrable to different LLMs. Our defense successfully mitigated over 70% of
Prompt Automatic Iterative Refinement (PAIR) attacks, which is currently the
most effective defense to the best of our knowledge. We are also the first to
attempt mitigating the MathsAttack and reduced its attack success rate by
almost 40%. Our code is publicly available at
https://github.com/Cancanxxx/Round_Trip_Translation_Defence
  This version of the article has been accepted for publication, after peer
review (when applicable) but is not the Version of Record and does not reflect
post-acceptance improvements, or any corrections. The Version of Record is
available online at: https://doi.org/10.48550/arXiv.2402.13517 Use of this
Accepted Version is subject to the publisher's Accepted Manuscript terms of use
https://www.springernature.com/gp/open-research/policies/accepted-manuscript-terms

</details>


### [51] [Does Generative AI speak Nigerian-Pidgin?: Issues about Representativeness and Bias for Multilingualism in LLMs](https://arxiv.org/abs/2404.19442)
*David Ifeoluwa Adelani, A. Seza Doğruöz, Iyanuoluwa Shode, Anuoluwapo Aremu*

Main category: cs.CL

TL;DR: The paper highlights linguistic differences between Naija and WAPE, showing Naija's underrepresentation in Generative AI due to limited data.


<details>
  <summary>Details</summary>
Motivation: To address the underrepresentation of Naija in AI and explore its linguistic distinctions from WAPE.

Method: Statistical analyses, Machine Translation experiments, historical research, and interviews with Naija Wikipedia contributors.

Result: Naija and WAPE are linguistically distinct, and Generative AI currently only supports WAPE.

Conclusion: Naija's underrepresentation in AI stems from data scarcity, requiring more resources for inclusion.

Abstract: Nigeria is a multilingual country with 500+ languages. Naija is a Nigerian
Pidgin spoken by approximately 120M speakers and it is a mixed language (e.g.,
English, Portuguese, Yoruba, Hausa and Igbo). Although it has mainly been a
spoken language until recently, there are some online platforms (e.g.,
Wikipedia), publishing in written Naija as well. West African Pidgin English
(WAPE) is also spoken in Nigeria and it is used by BBC to broadcast news on the
internet to a wider audience not only in Nigeria but also in other West African
countries (e.g., Cameroon and Ghana). Through statistical analyses and Machine
Translation experiments, our paper shows that these two pidgin varieties do not
represent each other (i.e., there are linguistic differences in word order and
vocabulary) and Generative AI operates only based on WAPE. In other words,
Naija is underrepresented in Generative AI, and it is hard to teach LLMs with
few examples. In addition to the statistical analyses, we also provide
historical information on both pidgins as well as insights from the interviews
conducted with volunteer Wikipedia contributors in Naija.

</details>


### [52] [Emergence of a High-Dimensional Abstraction Phase in Language Transformers](https://arxiv.org/abs/2405.15471)
*Emily Cheng, Diego Doimo, Corentin Kervadec, Iuri Macocco, Jade Yu, Alessandro Laio, Marco Baroni*

Main category: cs.CL

TL;DR: The paper explores the geometric properties of language models (LMs), identifying a high-dimensionality phase that correlates with linguistic abstraction, transferability, and cross-model predictability, and links its early onset to better performance.


<details>
  <summary>Details</summary>
Motivation: To understand how the geometric properties of LMs relate to their function, particularly focusing on intrinsic dimensionality and its role in linguistic processing.

Method: A high-level geometric analysis of five pre-trained transformer-based LMs across three input datasets, examining phases of intrinsic dimensionality.

Result: A distinct high-dimensionality phase was observed, marking the first full linguistic abstraction, viable transfer to downstream tasks, and predictability across LMs. Early onset of this phase predicts better LM performance.

Conclusion: A central high-dimensionality phase is key to core linguistic processing in common LM architectures, with implications for model performance.

Abstract: A language model (LM) is a mapping from a linguistic context to an output
token. However, much remains to be known about this mapping, including how its
geometric properties relate to its function. We take a high-level geometric
approach to its analysis, observing, across five pre-trained transformer-based
LMs and three input datasets, a distinct phase characterized by high intrinsic
dimensionality. During this phase, representations (1) correspond to the first
full linguistic abstraction of the input; (2) are the first to viably transfer
to downstream tasks; (3) predict each other across different LMs. Moreover, we
find that an earlier onset of the phase strongly predicts better language
modelling performance. In short, our results suggest that a central
high-dimensionality phase underlies core linguistic processing in many common
LM architectures.

</details>


### [53] [Extracting and Transferring Abilities For Building Multi-lingual Ability-enhanced Large Language Models](https://arxiv.org/abs/2410.07825)
*Zhipeng Chen, Kun Zhou, Liang Song, Wayne Xin Zhao, Bingning Wang, Weipeng Chen, Ji-Rong Wen*

Main category: cs.CL

TL;DR: MAET extracts and transfers language-agnostic abilities in LLMs without training, outperforming baselines.


<details>
  <summary>Details</summary>
Motivation: Addressing the lack of multi-lingual ability-related data for low-resource languages in LLMs.

Method: Decomposes and extracts ability-related weights, transfers them via simple operations (addition/subtraction) without training.

Result: Effective ability transfer, outperforms training-based methods in high- and low-resource languages.

Conclusion: MAET is a practical solution for multi-lingual ability transfer, especially for low-resource languages.

Abstract: Multi-lingual ability transfer has become increasingly important for the
broad application of large language models (LLMs). Existing work highly relies
on training with the multi-lingual ability-related data, which may be not
available for low-resource languages. To solve it, we propose a Multi-lingual
Ability Extraction and Transfer approach, named as MAET. Our key idea is to
decompose and extract language-agnostic ability-related weights from LLMs, and
transfer them across different languages by simple addition and subtraction
operations without training. Specially, our MAET consists of the extraction and
transfer stages. In the extraction stage, we firstly locate key neurons that
are highly related to specific abilities, and then employ them to extract the
transferable ability-specific weights. In the transfer stage, we further select
the ability-related parameter tensors, and design the merging strategy based on
the linguistic and ability specific weights, to build the multi-lingual
ability-enhanced LLM. To demonstrate the effectiveness of our proposed
approach, we conduct extensive experiments on mathematical and scientific tasks
in both high-resource lingual and low-resource lingual scenarios. Experiment
results have shown that MAET can effectively and efficiently extract and
transfer the advanced abilities, and outperform training-based baseline
methods. Our code and data are available at https://github.com/RUCAIBox/MAET.

</details>


### [54] [Adsorb-Agent: Autonomous Identification of Stable Adsorption Configurations via Large Language Model Agent](https://arxiv.org/abs/2410.16658)
*Janghoon Ock, Tirtha Vinchurkar, Yayati Jadhav, Amir Barati Farimani*

Main category: cs.CL

TL;DR: Adsorb-Agent, an LLM-based tool, efficiently identifies stable adsorption configurations, reducing computational costs and improving accuracy in adsorption energy predictions.


<details>
  <summary>Details</summary>
Motivation: Current methods for determining adsorption energy are computationally intensive and inefficient, requiring exhaustive enumeration without guaranteeing optimal results.

Method: Adsorb-Agent uses LLM knowledge and reasoning to strategically explore configurations, minimizing the need for exhaustive sampling.

Result: It achieves comparable or lower adsorption energies for most systems, especially in complex cases, while requiring fewer configurations.

Conclusion: Adsorb-Agent shows promise in accelerating catalyst discovery by enhancing efficiency and reliability in adsorption energy predictions.

Abstract: Adsorption energy is a key reactivity descriptor in catalysis, enabling
efficient screening for optimal catalysts. However, determining adsorption
energy typically requires evaluating numerous adsorbate-catalyst
configurations. Current algorithmic approaches rely on exhaustive enumeration
of adsorption sites and configurations, which makes the process computationally
intensive and does not inherently guarantee the identification of the global
minimum energy. In this work, we introduce Adsorb-Agent, a Large Language Model
(LLM) agent designed to efficiently identify system-specific stable adsorption
configurations corresponding to the global minimum adsorption energy.
Adsorb-Agent leverages its built-in knowledge and emergent reasoning
capabilities to strategically explore adsorption configurations likely to hold
adsorption energy. By reducing the reliance on exhaustive sampling, it
significantly decreases the number of initial configurations required while
improving the accuracy of adsorption energy predictions. We evaluate
Adsorb-Agent's performance across twenty representative systems encompassing a
range of complexities. The Adsorb-Agent successfully identifies comparable
adsorption energies for 83.7% of the systems and achieves lower energies,
closer to the actual global minimum, for 35% of the systems, while requiring
significantly fewer initial configurations than conventional methods. Its
capability is particularly evident in complex systems, where it identifies
lower adsorption energies for 46.7% of systems involving intermetallic surfaces
and 66.7% of systems with large adsorbate molecules. These results demonstrate
the potential of Adsorb-Agent to accelerate catalyst discovery by reducing
computational costs and improving the reliability of adsorption energy
predictions.

</details>


### [55] [KnowRA: Knowledge Retrieval Augmented Method for Document-level Relation Extraction with Comprehensive Reasoning Abilities](https://arxiv.org/abs/2501.00571)
*Chengcheng Mai, Yuxiang Wang, Ziyu Gong, Hanxiang Wang, Yihua Huang*

Main category: cs.CL

TL;DR: KnowRA enhances document-level relation extraction by integrating external knowledge and comprehensive reasoning, outperforming existing methods.


<details>
  <summary>Details</summary>
Motivation: Existing Doc-RE methods lack comprehensive reasoning and external knowledge utilization for long documents.

Method: Constructs a document graph, integrates co-reference resolution, retrieves external knowledge, filters irrelevant knowledge, and uses axis attention for cross-sentence reasoning.

Result: Outperforms state-of-the-art baselines on two datasets.

Conclusion: KnowRA effectively combines knowledge retrieval and reasoning for improved Doc-RE.

Abstract: Document-level relation extraction (Doc-RE) aims to extract relations between
entities across multiple sentences. Therefore, Doc-RE requires more
comprehensive reasoning abilities like humans, involving complex cross-sentence
interactions between entities, contexts, and external general knowledge,
compared to the sentence-level RE. However, most existing Doc-RE methods focus
on optimizing single reasoning ability, but lack the ability to utilize
external knowledge for comprehensive reasoning on long documents. To solve
these problems, a knowledge retrieval augmented method, named KnowRA, was
proposed with comprehensive reasoning to autonomously determine whether to
accept external knowledge to assist DocRE. Firstly, we constructed a document
graph for semantic encoding and integrated the co-reference resolution model to
augment the co-reference reasoning ability. Then, we expanded the document
graph into a document knowledge graph by retrieving the external knowledge base
for common-sense reasoning and a novel knowledge filtration method was
presented to filter out irrelevant knowledge. Finally, we proposed the axis
attention mechanism to build direct and indirect associations with intermediary
entities for achieving cross-sentence logical reasoning. Extensive experiments
conducted on two datasets verified the effectiveness of our method compared to
the state-of-the-art baselines. Our code is available at
https://anonymous.4open.science/r/KnowRA.

</details>


### [56] [Leveraging Conditional Mutual Information to Improve Large Language Model Fine-Tuning For Classification](https://arxiv.org/abs/2502.11258)
*Thanushon Sivakaran, En-Hui Yang*

Main category: cs.CL

TL;DR: The paper explores using Conditional Mutual Information (CMI) from information theory to enhance LLM fine-tuning, showing improved performance in classification tasks and knowledge distillation.


<details>
  <summary>Details</summary>
Motivation: To bridge the gap between information theory and LLM development by leveraging CMI for better model performance.

Method: Adapts a CMI-constrained deep learning framework for LLM fine-tuning, minimizing CMI for standalone models and maximizing it for knowledge distillation.

Result: Superior performance in 6/8 GLUE tasks compared to BERT (minimizing CMI) and DistilBERT (maximizing CMI).

Conclusion: CMI is adaptable for optimizing LLMs, offering a robust framework for advancing fine-tuning and knowledge distillation.

Abstract: Although large language models (LLMs) have demonstrated remarkable
capabilities in recent years, the potential of information theory (IT) to
enhance LLM development remains underexplored. This paper introduces the
information theoretic principle of Conditional Mutual Information (CMI) to LLM
fine-tuning for classification tasks, exploring its promise in two main ways:
minimizing CMI to improve a model's standalone performance and maximizing CMI
to enhance knowledge distillation (KD) for more capable student models. To
apply CMI in LLM fine-tuning, we adapt the recently proposed CMI-constrained
deep learning framework, which was initially developed for image
classification, with some modification. By minimizing CMI during LLM
fine-tuning, we achieve superior performance gains on 6 of 8 GLUE
classification tasks compared to BERT. Additionally, maximizing CMI during the
KD process results in significant performance improvements in 6 of 8 GLUE
classification tasks compared to DistilBERT. These findings demonstrate CMI's
adaptability for optimizing both standalone LLMs and student models, showcasing
its potential as a robust framework for advancing LLM fine-tuning. Our work
bridges the gap between information theory and LLM development, offering new
insights for building high-performing language models.

</details>


### [57] [Mapping Trustworthiness in Large Language Models: A Bibliometric Analysis Bridging Theory to Practice](https://arxiv.org/abs/2503.04785)
*José Siqueira de Cerqueira, Kai-Kristian Kemell, Muhammad Waseem, Rebekah Rousi, Nannan Xi, Juho Hamari, Pekka Abrahamsson*

Main category: cs.CL

TL;DR: The study analyzes trustworthiness in LLMs through bibliometric mapping and systematic review, identifying gaps and proposing practical strategies like RAG and explainability techniques.


<details>
  <summary>Details</summary>
Motivation: Address the lack of consensus on operationalizing trustworthiness in LLMs despite their widespread adoption.

Method: Bibliometric mapping of 2,006 publications (2019-2025) and systematic review of 68 core papers, using co-authorship networks and keyword analysis.

Result: Trustworthiness in LLMs is often framed via organizational trust frameworks (ability, benevolence, integrity), but gaps exist in practical implementation.

Conclusion: Proposes 20 trust-enhancing techniques (e.g., RAG, audits) to improve LLM transparency, accountability, and ethical alignment for real-world deployment.

Abstract: The rapid proliferation of Large Language Models (LLMs) has raised pressing
concerns regarding their trustworthiness, spanning issues of reliability,
transparency, fairness, and ethical alignment. Despite the increasing adoption
of LLMs across various domains, there remains a lack of consensus on how to
operationalize trustworthiness in practice. This study bridges the gap between
theoretical discussions and implementation by conducting a bibliometric mapping
analysis of 2,006 publications from 2019 to 2025. Through co-authorship
networks, keyword co-occurrence analysis, and thematic evolution tracking, we
identify key research trends, influential authors, and prevailing definitions
of LLM trustworthiness. Additionally, a systematic review of 68 core papers is
conducted to examine conceptualizations of trust and their practical
implications. Our findings reveal that trustworthiness in LLMs is often framed
through existing organizational trust frameworks, emphasizing dimensions such
as ability, benevolence, and integrity. However, a significant gap exists in
translating these principles into concrete development strategies. To address
this, we propose a structured mapping of 20 trust-enhancing techniques across
the LLM lifecycle, including retrieval-augmented generation (RAG),
explainability techniques, and post-training audits. By synthesizing
bibliometric insights with practical strategies, this study contributes towards
fostering more transparent, accountable, and ethically aligned LLMs, ensuring
their responsible deployment in real-world applications.

</details>


### [58] [JuDGE: Benchmarking Judgment Document Generation for Chinese Legal System](https://arxiv.org/abs/2503.14258)
*Weihang Su, Baoqing Yue, Qingyao Ai, Yiran Hu, Jiaqi Li, Changyue Wang, Kaiyuan Zhang, Yueyue Wu, Yiqun Liu*

Main category: cs.CL

TL;DR: JuDGE is a benchmark for evaluating judgment document generation in the Chinese legal system, using a dataset of real cases and external legal corpora. It tests methods like RAG and shows room for improvement.


<details>
  <summary>Details</summary>
Motivation: To address the lack of benchmarks for evaluating legal judgment document generation in the Chinese legal system.

Method: Constructs a dataset of real cases and uses external legal corpora. Evaluates methods like few-shot learning, fine-tuning, and RAG.

Result: RAG improves performance, but significant gaps remain.

Conclusion: JuDGE provides a foundation for future work in legal document generation, with datasets and code publicly available.

Abstract: This paper introduces JuDGE (Judgment Document Generation Evaluation), a
novel benchmark for evaluating the performance of judgment document generation
in the Chinese legal system. We define the task as generating a complete legal
judgment document from the given factual description of the case. To facilitate
this benchmark, we construct a comprehensive dataset consisting of factual
descriptions from real legal cases, paired with their corresponding full
judgment documents, which serve as the ground truth for evaluating the quality
of generated documents. This dataset is further augmented by two external legal
corpora that provide additional legal knowledge for the task: one comprising
statutes and regulations, and the other consisting of a large collection of
past judgment documents. In collaboration with legal professionals, we
establish a comprehensive automated evaluation framework to assess the quality
of generated judgment documents across various dimensions. We evaluate various
baseline approaches, including few-shot in-context learning, fine-tuning, and a
multi-source retrieval-augmented generation (RAG) approach, using both general
and legal-domain LLMs. The experimental results demonstrate that, while RAG
approaches can effectively improve performance in this task, there is still
substantial room for further improvement. All the codes and datasets are
available at: https://github.com/oneal2000/JuDGE.

</details>


### [59] [Proof or Bluff? Evaluating LLMs on 2025 USA Math Olympiad](https://arxiv.org/abs/2503.21934)
*Ivo Petrov, Jasper Dekoninck, Lyuben Baltadzhiev, Maria Drencheva, Kristian Minchev, Mislav Balunović, Nikola Jovanović, Martin Vechev*

Main category: cs.CL

TL;DR: Current LLMs excel in numerical answers but fail in rigorous mathematical reasoning, as shown by their poor performance in full-solution evaluations.


<details>
  <summary>Details</summary>
Motivation: Existing benchmarks for LLMs focus on numerical answers, ignoring the need for rigorous reasoning and proof generation in real-world math tasks.

Method: Evaluated state-of-the-art reasoning models on 2025 USAMO problems using expert annotators, analyzing reasoning traces and failure modes.

Result: Only Gemini-2.5-Pro scored 25%, while others scored below 5%, revealing significant struggles in rigorous reasoning.

Conclusion: Current LLMs lack adequate reasoning and proof generation capabilities, necessitating substantial improvements.

Abstract: Recent math benchmarks for large language models (LLMs) such as MathArena
indicate that state-of-the-art reasoning models achieve impressive performance
on mathematical competitions like AIME, with the leading model, Gemini-2.5-Pro,
achieving scores comparable to top human competitors. However, these benchmarks
evaluate models solely based on final numerical answers, neglecting rigorous
reasoning and proof generation which are essential for real-world mathematical
tasks. To address this, we introduce the first comprehensive evaluation of
full-solution reasoning for challenging mathematical problems. Using expert
human annotators, we evaluated several state-of-the-art reasoning models on the
six problems from the 2025 USAMO within hours of their release. Our results
reveal that all tested models struggled significantly: only Gemini-2.5-Pro
achieves a non-trivial score of 25%, while all other models achieve less than
5%. Through detailed analysis of reasoning traces, we identify the most common
failure modes and find several unwanted artifacts arising from the optimization
strategies employed during model training. Overall, our results suggest that
current LLMs are inadequate for rigorous mathematical reasoning tasks,
highlighting the need for substantial improvements in reasoning and proof
generation capabilities.

</details>


### [60] [VisualPuzzles: Decoupling Multimodal Reasoning Evaluation from Domain Knowledge](https://arxiv.org/abs/2504.10342)
*Yueqi Song, Tianyue Ou, Yibo Kong, Zecheng Li, Graham Neubig, Xiang Yue*

Main category: cs.CL

TL;DR: VisualPuzzles is a new benchmark focusing on visual reasoning without heavy reliance on domain-specific knowledge, revealing gaps in current multimodal models' reasoning abilities.


<details>
  <summary>Details</summary>
Motivation: To isolate and evaluate general reasoning abilities in non-expert settings, avoiding conflation with domain-specific knowledge.

Method: Introduces VisualPuzzles, a benchmark with diverse reasoning questions (algorithmic, analogical, deductive, inductive, spatial) derived from logical reasoning tests.

Result: State-of-the-art models lag behind humans, and knowledge-heavy benchmarks don't predict reasoning performance. Reasoning enhancements yield inconsistent gains.

Conclusion: VisualPuzzles provides a clearer evaluation of reasoning capabilities, distinct from factual recall or domain knowledge.

Abstract: Current multimodal benchmarks often conflate reasoning with domain-specific
knowledge, making it difficult to isolate and evaluate general reasoning
abilities in non-expert settings. To address this, we introduce VisualPuzzles,
a benchmark that targets visual reasoning while deliberately minimizing
reliance on specialized knowledge. VisualPuzzles consists of diverse questions
spanning five categories: algorithmic, analogical, deductive, inductive, and
spatial reasoning. One major source of our questions is manually translated
logical reasoning questions from the Chinese Civil Service Examination.
Experiments show that VisualPuzzles requires significantly less intensive
domain-specific knowledge and more complex reasoning compared to benchmarks
like MMMU, enabling us to better evaluate genuine multimodal reasoning.
Evaluations show that state-of-the-art multimodal large language models
consistently lag behind human performance on VisualPuzzles, and that strong
performance on knowledge-intensive benchmarks does not necessarily translate to
success on reasoning-focused, knowledge-light tasks. Additionally, reasoning
enhancements such as scaling up inference compute (with "thinking" modes) yield
inconsistent gains across models and task types, and we observe no clear
correlation between model size and performance. We also found that models
exhibit different reasoning and answering patterns on VisualPuzzles compared to
benchmarks with heavier emphasis on knowledge. VisualPuzzles offers a clearer
lens through which to evaluate reasoning capabilities beyond factual recall and
domain knowledge.

</details>


### [61] [Learning Optimal Prompt Ensemble for Multi-source Visual Prompt Transfer](https://arxiv.org/abs/2504.12311)
*Enming Zhang, Liwen Cao, Yanru Wu, Zijie Zhao, Guan Wang, Yang Li*

Main category: cs.CL

TL;DR: HGPrompt is an adaptive framework for multi-source prompt transfer, optimizing transferability and stability to avoid representation collapse and enhance generalization.


<details>
  <summary>Details</summary>
Motivation: Combining multiple source prompts can improve generalization but risks representation collapse due to mutual interference.

Method: HGPrompt learns optimal ensemble weights by evaluating transferability with an information-theoretic metric and using Gradient Alignment Regularization to mitigate gradient conflicts.

Result: HGPrompt achieves state-of-the-art performance on the VTAB benchmark.

Conclusion: HGPrompt effectively enables stable and coherent multi-source prompt transfer.

Abstract: Prompt tuning has emerged as a lightweight adaptation strategy for adapting
foundation models to downstream tasks, particularly in resource-constrained
systems. As pre-trained prompts have become valuable intellectual assets,
combining multiple source prompts offers a promising approach to enhance
generalization to new tasks by leveraging complementary knowledge from diverse
sources. However, naive aggregation of these prompts often leads to
representation collapse due to mutual interference, undermining their
collective potential. To address these challenges, we propose HGPrompt, an
adaptive framework for multi-source prompt transfer that learns optimal
ensemble weights by jointly optimizing dual objectives: transferability and
stability. Specifically, we first introduce an information-theoretic metric to
evaluate the transferability of prompt-induced features on the target task,
capturing the intrinsic alignment between the feature representations.
Additionally, we propose a novel Gradient Alignment Regularization to mitigate
gradient conflicts among prompts, enabling stable and coherent knowledge
transfer from multiple sources while suppressing interference. Extensive
experiments on the large-scale VTAB benchmark demonstrate that HGPrompt
achieves state-of-the-art performance, validating its effectiveness in
multi-source prompt transfer.

</details>


### [62] [PolyMath: Evaluating Mathematical Reasoning in Multilingual Contexts](https://arxiv.org/abs/2504.18428)
*Yiming Wang, Pei Zhang, Jialong Tang, Haoran Wei, Baosong Yang, Rui Wang, Chenshu Sun, Feitong Sun, Jiran Zhang, Junxuan Wu, Qiqian Cang, Yichang Zhang, Fei Huang, Junyang Lin, Fei Huang, Jingren Zhou*

Main category: cs.CL

TL;DR: PolyMath is a multilingual math reasoning benchmark for LLMs, covering 18 languages and 4 difficulty levels, revealing challenges like performance variability and language consistency.


<details>
  <summary>Details</summary>
Motivation: To create a discriminative multilingual benchmark for evaluating reasoning LLMs, addressing gaps in language diversity and difficulty comprehensiveness.

Method: Developed PolyMath with high-quality translations and evaluated advanced LLMs like Qwen-3-235B-A22B-Thinking and Gemini-2.5-pro.

Result: Top LLMs scored ~50-55% accuracy, with 40% under the hardest level. Key challenges include performance variability, low language consistency, and thinking length differences.

Conclusion: Controlling output language can improve reasoning, especially for low-resource languages, offering a path to enhance multilingual LLM capabilities.

Abstract: In this paper, we introduce PolyMath, a multilingual mathematical reasoning
benchmark covering 18 languages and 4 easy-to-hard difficulty levels. Our
benchmark ensures difficulty comprehensiveness, language diversity, and
high-quality translation, making it a highly discriminative multilingual
mathematical benchmark in the era of reasoning LLMs. We conduct a comprehensive
evaluation for advanced LLMs and find that even Qwen-3-235B-A22B-Thinking and
Gemini-2.5-pro, achieve only 54.6 and 52.2 benchmark scores, with about 40%
accuracy under the highest level From a language perspective, our benchmark
reveals several key challenges of LLMs in multilingual reasoning: (1) Reasoning
performance varies widely across languages for current LLMs; (2) Input-output
language consistency is low in reasoning LLMs and may be correlated with
performance; (3) The thinking length differs significantly by language for
current LLMs. Additionally, we demonstrate that controlling the output language
in the instructions has the potential to affect reasoning performance,
especially for some low-resource languages, suggesting a promising direction
for improving multilingual capabilities in LLMs.

</details>


### [63] [SynLexLM: Scaling Legal LLMs with Synthetic Data and Curriculum Learning](https://arxiv.org/abs/2504.18762)
*Ojasw Upadhyay, Abishek Saravanakumar, Ayman Ismail*

Main category: cs.CL

TL;DR: SynLexLM is a novel method for pre-training legal LLMs using curriculum learning and synthetic data augmentation to address data scarcity and improve performance on legal benchmarks.


<details>
  <summary>Details</summary>
Motivation: General-purpose LLMs lack legal nuance, and acquiring sufficient legal data is challenging, necessitating an efficient pre-training approach for legal domains.

Method: Uses curriculum learning (simple to complex legal texts) and synthetic data augmentation (e.g., Gemini Pro) to generate legal QA pairs.

Result: Aims to outperform traditional models and fine-tuned versions on legal benchmarks like BigLaw-Bench and EUR-Lex-Sum.

Conclusion: SynLexLM could enhance legal document analysis and democratize access to advanced legal AI tools.

Abstract: Large Language Models (LLMs) are powerful but often require extensive
fine-tuning and large datasets for specialized domains like law.
General-purpose pre-training may not capture legal nuances, and acquiring
sufficient legal data is challenging. We introduce SynLexLM, a novel approach
to efficiently pre-train a legal LLM. Our method employs curriculum learning,
progressing from simple to complex legal texts and queries, combined with
synthetic data augmentation using models like Gemini Pro to address data
scarcity. We aim to achieve improved performance on legal benchmarks
(BigLaw-Bench, EUR-Lex-Sum) compared to traditional models and fine-tuned
versions. Preliminary work involves generating synthetic QA pairs reflecting
legal reasoning. This work aims to enhance legal document analysis and research
tools, potentially democratizing access to advanced legal AI.

</details>


### [64] [Uncertainty Quantification for Language Models: A Suite of Black-Box, White-Box, LLM Judge, and Ensemble Scorers](https://arxiv.org/abs/2504.19254)
*Dylan Bouchard, Mohit Singh Chauhan*

Main category: cs.CL

TL;DR: A framework for zero-resource hallucination detection in LLMs using uncertainty quantification techniques, offering a tunable ensemble approach and a Python toolkit (UQLM).


<details>
  <summary>Details</summary>
Motivation: Hallucinations in LLMs pose risks in high-stakes domains, necessitating reliable detection methods.

Method: Adapts UQ techniques (black-box, white-box, LLM-as-a-Judge) into standardized confidence scores, introduces a tunable ensemble, and provides a Python toolkit.

Result: The tunable ensemble outperforms individual components and existing methods, enhancing LLM reliability.

Conclusion: Customized hallucination detection improves LLM accuracy and reliability, with practical implementation via UQLM.

Abstract: Hallucinations are a persistent problem with Large Language Models (LLMs). As
these models become increasingly used in high-stakes domains, such as
healthcare and finance, the need for effective hallucination detection is
crucial. To this end, we propose a versatile framework for zero-resource
hallucination detection that practitioners can apply to real-world use cases.
To achieve this, we adapt a variety of existing uncertainty quantification (UQ)
techniques, including black-box UQ, white-box UQ, and LLM-as-a-Judge,
transforming them as necessary into standardized response-level confidence
scores ranging from 0 to 1. To enhance flexibility, we introduce a tunable
ensemble approach that incorporates any combination of the individual
confidence scores. This approach enables practitioners to optimize the ensemble
for a specific use case for improved performance. To streamline implementation,
the full suite of scorers is offered in this paper's companion Python toolkit,
UQLM. To evaluate the performance of the various scorers, we conduct an
extensive set of experiments using several LLM question-answering benchmarks.
We find that our tunable ensemble typically surpasses its individual components
and outperforms existing hallucination detection methods. Our results
demonstrate the benefits of customized hallucination detection strategies for
improving the accuracy and reliability of LLMs.

</details>


### [65] [Efficient Domain-adaptive Continual Pretraining for the Process Industry in the German Language](https://arxiv.org/abs/2504.19856)
*Anastasia Zhukova, Christian E. Matt, Terry Ruas, Bela Gipp*

Main category: cs.CL

TL;DR: ICL-APT, an efficient domain-adaptive pretraining method, outperforms DAPT by 3.5 points in IR metrics and reduces computing time by 4x, making it viable for low-resource domains.


<details>
  <summary>Details</summary>
Motivation: Traditional DAPT requires large domain-specific datasets, which are scarce for non-English domains like German process industry. ICL-APT addresses this limitation.

Method: Combines in-context learning (ICL) and k-nearest neighbors (kNN) to augment target data with domain-related texts, reducing GPU time.

Result: Outperforms DAPT by 3.5 points in IR metrics (mAP, MRR, nDCG) and uses 4x less computing time.

Conclusion: ICL-APT is a cost-effective solution for low-resource domains, enhancing accessibility of NLP-based solutions in production.

Abstract: Domain-adaptive continual pretraining (DAPT) is a state-of-the-art technique
that further trains a language model (LM) on its pretraining task, e.g.,
language masking. Although popular, it requires a significant corpus of
domain-related data, which is difficult to obtain for specific domains in
languages other than English, such as the process industry in the German
language. This paper introduces an efficient approach called ICL-augmented
pretraining or ICL-APT that leverages in-context learning (ICL) and k-nearest
neighbors (kNN) to augment target data with domain-related and in-domain texts,
significantly reducing GPU time while maintaining strong model performance. Our
results show that this approach performs better than traditional DAPT by 3.5
points of the average IR metrics (e.g., mAP, MRR, and nDCG) and requires almost
4 times less computing time, providing a cost-effective solution for industries
with limited computational capacity. The findings highlight the broader
applicability of this framework to other low-resource industries, making
NLP-based solutions more accessible and feasible in production environments.

</details>


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [66] [Can a Large Language Model Assess Urban Design Quality? Evaluating Walkability Metrics Across Expertise Levels](https://arxiv.org/abs/2504.21040)
*Chenyi Cai, Kosuke Kuriyama, Youlong Gu, Filip Biljecki, Pieter Herthogs*

Main category: cs.CV

TL;DR: The paper explores using MLLMs (like ChatGPT-4) with expert urban design knowledge to evaluate walkability in street view images, finding improved consistency but noting limitations like overly optimistic scores.


<details>
  <summary>Details</summary>
Motivation: To investigate how integrating expert knowledge into MLLM prompts can enhance urban design evaluation, particularly walkability, using street view images.

Method: Collect walkability metrics from literature, categorize them, and develop prompts for ChatGPT-4 with varying clarity. Analyze its performance in evaluating pedestrian safety and attractiveness.

Result: MLLMs can automate evaluations but tend to be overly optimistic and make errors. Expert knowledge integration improves consistency and focus.

Conclusion: Expert knowledge enhances MLLM reliability for urban design evaluation, though challenges like score optimism remain.

Abstract: Urban street environments are vital to supporting human activity in public
spaces. The emergence of big data, such as street view images (SVIs) combined
with multimodal large language models (MLLMs), is transforming how researchers
and practitioners investigate, measure, and evaluate semantic and visual
elements of urban environments. Considering the low threshold for creating
automated evaluative workflows using MLLMs, it is crucial to explore both the
risks and opportunities associated with these probabilistic models. In
particular, the extent to which the integration of expert knowledge can
influence the performance of MLLMs in evaluating the quality of urban design
has not been fully explored. This study sets out an initial exploration of how
integrating more formal and structured representations of expert urban design
knowledge into the input prompts of an MLLM (ChatGPT-4) can enhance the model's
capability and reliability in evaluating the walkability of built environments
using SVIs. We collect walkability metrics from the existing literature and
categorize them using relevant ontologies. We then select a subset of these
metrics, focusing on the subthemes of pedestrian safety and attractiveness, and
develop prompts for the MLLM accordingly. We analyze the MLLM's ability to
evaluate SVI walkability subthemes through prompts with varying levels of
clarity and specificity regarding evaluation criteria. Our experiments
demonstrate that MLLMs are capable of providing assessments and interpretations
based on general knowledge and can support the automation of multimodal
image-text evaluations. However, they generally provide more optimistic scores
and can make mistakes when interpreting the provided metrics, resulting in
incorrect evaluations. By integrating expert knowledge, the MLLM's evaluative
performance exhibits higher consistency and concentration.

</details>


### [67] [Legilimens: Performant Video Analytics on the System-on-Chip Edge](https://arxiv.org/abs/2504.21136)
*Murali Ramanujam, Yinwei Dai, Kyle Jamieson, Ravi Netravali*

Main category: cs.CV

TL;DR: Legilimens is a continuous learning system for mobile edge devices, leveraging abundant memory and efficient techniques to reduce retraining costs and improve accuracy.


<details>
  <summary>Details</summary>
Motivation: Existing systems rely on spare compute resources of memory-constrained edge servers, but mobile edge devices (e.g., drones, dashcams) have weaker compute with abundant memory, requiring a new approach.

Method: Legilimens uses compute-efficient techniques: selecting high-utility samples, updating the base model without full retraining, and time-sharing compute between retraining and live inference.

Result: Legilimens reduces retraining costs by 2.8-10x and achieves 18-45% higher accuracies across diverse workloads.

Conclusion: Legilimens effectively addresses the resource constraints of mobile edge devices, enabling lightweight, high-accuracy continuous learning.

Abstract: Continually retraining models has emerged as a primary technique to enable
high-accuracy video analytics on edge devices. Yet, existing systems employ
such adaptation by relying on the spare compute resources that traditional
(memory-constrained) edge servers afford. In contrast, mobile edge devices such
as drones and dashcams offer a fundamentally different resource profile:
weak(er) compute with abundant unified memory pools. We present Legilimens, a
continuous learning system for the mobile edge's System-on-Chip GPUs. Our
driving insight is that visually distinct scenes that require retraining
exhibit substantial overlap in model embeddings; if captured into a base model
on device memory, specializing to each new scene can become lightweight,
requiring very few samples. To practically realize this approach, Legilimens
presents new, compute-efficient techniques to (1) select high-utility data
samples for retraining specialized models, (2) update the base model without
complete retraining, and (3) time-share compute resources between retraining
and live inference for maximal accuracy. Across diverse workloads, Legilimens
lowers retraining costs by 2.8-10x compared to existing systems, resulting in
18-45% higher accuracies.

</details>


### [68] [Differentiable Room Acoustic Rendering with Multi-View Vision Priors](https://arxiv.org/abs/2504.21847)
*Derong Jin, Ruohan Gao*

Main category: cs.CV

TL;DR: AV-DAR combines visual cues and acoustic beam tracing for efficient, accurate room acoustic rendering, outperforming prior methods.


<details>
  <summary>Details</summary>
Motivation: Existing methods for room impulse response estimation are either data-heavy or computationally expensive, limiting realistic virtual environments.

Method: Leverages multi-view images and acoustic beam tracing for physics-based rendering.

Result: Outperforms prior methods, achieving comparable performance with less data and significant gains (16.6% to 50.9%).

Conclusion: AV-DAR is efficient, interpretable, and accurate, advancing realistic spatial audio in virtual environments.

Abstract: An immersive acoustic experience enabled by spatial audio is just as crucial
as the visual aspect in creating realistic virtual environments. However,
existing methods for room impulse response estimation rely either on
data-demanding learning-based models or computationally expensive physics-based
modeling. In this work, we introduce Audio-Visual Differentiable Room Acoustic
Rendering (AV-DAR), a framework that leverages visual cues extracted from
multi-view images and acoustic beam tracing for physics-based room acoustic
rendering. Experiments across six real-world environments from two datasets
demonstrate that our multimodal, physics-based approach is efficient,
interpretable, and accurate, significantly outperforming a series of prior
methods. Notably, on the Real Acoustic Field dataset, AV-DAR achieves
comparable performance to models trained on 10 times more data while delivering
relative gains ranging from 16.6% to 50.9% when trained at the same scale.

</details>


### [69] [Embracing Collaboration Over Competition: Condensing Multiple Prompts for Visual In-Context Learning](https://arxiv.org/abs/2504.21263)
*Jinpeng Wang, Tianci Luo, Yaohua Zha, Yan Feng, Ruisheng Luo, Bin Chen, Tao Dai, Long Chen, Yaowei Wang, Shu-Tao Xia*

Main category: cs.CV

TL;DR: The paper introduces prompt condensation for Visual In-Context Learning (VICL), proposing Condenser to integrate multiple prompts efficiently, outperforming existing methods.


<details>
  <summary>Details</summary>
Motivation: Current VICL methods assume a single ideal prompt, but multiple suitable prompts may exist, leading to selection challenges and loss of useful context.

Method: The authors propose Condenser, a lightweight plugin that compresses fine-grained context from multiple prompts, optimized end-to-end with the backbone.

Result: Condenser outperforms state-of-the-art methods in benchmark tasks, showing superior context compression, scalability, and computational efficiency.

Conclusion: Condenser is a competitive solution for VICL, offering improved performance and open-sourced for broader use.

Abstract: Visual In-Context Learning (VICL) enables adaptively solving vision tasks by
leveraging pixel demonstrations, mimicking human-like task completion through
analogy. Prompt selection is critical in VICL, but current methods assume the
existence of a single "ideal" prompt in a pool of candidates, which in practice
may not hold true. Multiple suitable prompts may exist, but individually they
often fall short, leading to difficulties in selection and the exclusion of
useful context. To address this, we propose a new perspective: prompt
condensation. Rather than relying on a single prompt, candidate prompts
collaborate to efficiently integrate informative contexts without sacrificing
resolution. We devise Condenser, a lightweight external plugin that compresses
relevant fine-grained context across multiple prompts. Optimized end-to-end
with the backbone, Condenser ensures accurate integration of contextual cues.
Experiments demonstrate Condenser outperforms state-of-the-arts across
benchmark tasks, showing superior context compression, scalability with more
prompts, and enhanced computational efficiency compared to ensemble methods,
positioning it as a highly competitive solution for VICL. Code is open-sourced
at https://github.com/gimpong/CVPR25-Condenser.

</details>


### [70] [Emotion Recognition in Contemporary Dance Performances Using Laban Movement Analysis](https://arxiv.org/abs/2504.21154)
*Muhammad Turab, Philippe Colantoni, Damien Muselet, Alain Tremeau*

Main category: cs.CV

TL;DR: A novel framework enhances emotion recognition in contemporary dance by improving Laban Movement Analysis (LMA) features and introducing new descriptors, achieving 96.85% accuracy.


<details>
  <summary>Details</summary>
Motivation: To improve emotion recognition in contemporary dance by capturing both quantitative and qualitative movement aspects.

Method: Extracts expressive characteristics from 3D keypoints data, trains classifiers (Random Forests, SVM), and uses explainable ML for feature analysis.

Result: Achieves 96.85% accuracy in emotion recognition.

Conclusion: The framework improves emotion recognition and has applications in performance analysis, dance training, and human-computer interaction.

Abstract: This paper presents a novel framework for emotion recognition in contemporary
dance by improving existing Laban Movement Analysis (LMA) feature descriptors
and introducing robust, novel descriptors that capture both quantitative and
qualitative aspects of the movement. Our approach extracts expressive
characteristics from 3D keypoints data of professional dancers performing
contemporary dance under various emotional states, and trains multiple
classifiers, including Random Forests and Support Vector Machines.
Additionally, we provide in-depth explanation of features and their impact on
model predictions using explainable machine learning methods. Overall, our
study improves emotion recognition in contemporary dance and offers promising
applications in performance analysis, dance training, and human--computer
interaction, with a highest accuracy of 96.85\%.

</details>


### [71] [ClassWise-CRF: Category-Specific Fusion for Enhanced Semantic Segmentation of Remote Sensing Imagery](https://arxiv.org/abs/2504.21491)
*Qinfeng Zhu, Yunxi Jiang, Lei Fan*

Main category: cs.CV

TL;DR: ClassWise-CRF is a fusion architecture for semantic segmentation that selects expert networks and adaptively weights their predictions, improving performance on remote sensing datasets.


<details>
  <summary>Details</summary>
Motivation: To enhance semantic segmentation by dynamically optimizing network contributions for specific categories and ensuring spatial consistency.

Method: A two-stage process: selecting expert networks via a greedy algorithm and fusing predictions using adaptive weighting and CRF optimization.

Result: Improved mIoU by 1.00% (LoveDA validation), 0.68% (LoveDA test), 0.87% (Vaihingen validation), and 0.91% (Vaihingen test).

Conclusion: ClassWise-CRF effectively boosts segmentation performance and generalizes well across datasets.

Abstract: We propose a result-level category-specific fusion architecture called
ClassWise-CRF. This architecture employs a two-stage process: first, it selects
expert networks that perform well in specific categories from a pool of
candidate networks using a greedy algorithm; second, it integrates the
segmentation predictions of these selected networks by adaptively weighting
their contributions based on their segmentation performance in each category.
Inspired by Conditional Random Field (CRF), the ClassWise-CRF architecture
treats the segmentation predictions from multiple networks as confidence vector
fields. It leverages segmentation metrics (such as Intersection over Union)
from the validation set as priors and employs an exponential weighting strategy
to fuse the category-specific confidence scores predicted by each network. This
fusion method dynamically adjusts the weights of each network for different
categories, achieving category-specific optimization. Building on this, the
architecture further optimizes the fused results using unary and pairwise
potentials in CRF to ensure spatial consistency and boundary accuracy. To
validate the effectiveness of ClassWise-CRF, we conducted experiments on two
remote sensing datasets, LoveDA and Vaihingen, using eight classic and advanced
semantic segmentation networks. The results show that the ClassWise-CRF
architecture significantly improves segmentation performance: on the LoveDA
dataset, the mean Intersection over Union (mIoU) metric increased by 1.00% on
the validation set and by 0.68% on the test set; on the Vaihingen dataset, the
mIoU improved by 0.87% on the validation set and by 0.91% on the test set.
These results fully demonstrate the effectiveness and generality of the
ClassWise-CRF architecture in semantic segmentation of remote sensing images.
The full code is available at https://github.com/zhuqinfeng1999/ClassWise-CRF.

</details>


### [72] [Dance Style Recognition Using Laban Movement Analysis](https://arxiv.org/abs/2504.21166)
*Muhammad Turab, Philippe Colantoni, Damien Muselet, Alain Tremeau*

Main category: cs.CV

TL;DR: A novel pipeline combining 3D pose estimation, mesh reconstruction, and floor-aware modeling extracts LMA features for dance style recognition, achieving 99.18% accuracy by adding temporal context.


<details>
  <summary>Details</summary>
Motivation: Addressing the gap in capturing temporal context in dance style recognition using LMA features.

Method: Combines 3D pose estimation, 3D mesh reconstruction, and floor-aware modeling with a sliding window approach for temporal feature extraction.

Result: Achieves 99.18% classification accuracy, showing significant improvement with temporal context.

Conclusion: Temporal context enhances dance style recognition, validated by high accuracy and explainable AI.

Abstract: The growing interest in automated movement analysis has presented new
challenges in recognition of complex human activities including dance. This
study focuses on dance style recognition using features extracted using Laban
Movement Analysis. Previous studies for dance style recognition often focus on
cross-frame movement analysis, which limits the ability to capture temporal
context and dynamic transitions between movements. This gap highlights the need
for a method that can add temporal context to LMA features. For this, we
introduce a novel pipeline which combines 3D pose estimation, 3D human mesh
reconstruction, and floor aware body modeling to effectively extract LMA
features. To address the temporal limitation, we propose a sliding window
approach that captures movement evolution across time in features. These
features are then used to train various machine learning methods for
classification, and their explainability explainable AI methods to evaluate the
contribution of each feature to classification performance. Our proposed method
achieves a highest classification accuracy of 99.18\% which shows that the
addition of temporal context significantly improves dance style recognition
performance.

</details>


### [73] [Consistency-aware Fake Videos Detection on Short Video Platforms](https://arxiv.org/abs/2504.21495)
*Junxi Wang, Jize liu, Na Zhang, Yaxiong Wang*

Main category: cs.CV

TL;DR: The paper proposes a novel fake news detection method for short videos by leveraging cross-modal inconsistencies, achieving superior performance on benchmarks.


<details>
  <summary>Details</summary>
Motivation: Current fake news detection methods lack accuracy due to evolving manipulation technologies and underutilized inter-modal inconsistencies.

Method: Introduces Cross-modal Consistency Learning (CMCL) and Multi-modal Collaborative Diagnosis (MMCD) to identify and leverage cross-modal contradictions. CMCL includes pseudo-label generation and consistency diagnosis, while MMCD integrates features and probability scores.

Result: The model outperforms existing methods on FakeSV and FakeTT benchmarks.

Conclusion: Explicitly leveraging cross-modal inconsistencies improves fake news detection accuracy in short videos.

Abstract: This paper focuses to detect the fake news on the short video platforms.
While significant research efforts have been devoted to this task with notable
progress in recent years, current detection accuracy remains suboptimal due to
the rapid evolution of content manipulation and generation technologies.
Existing approaches typically employ a cross-modal fusion strategy that
directly combines raw video data with metadata inputs before applying a
classification layer. However, our empirical observations reveal a critical
oversight: manipulated content frequently exhibits inter-modal inconsistencies
that could serve as valuable discriminative features, yet remain underutilized
in contemporary detection frameworks. Motivated by this insight, we propose a
novel detection paradigm that explicitly identifies and leverages cross-modal
contradictions as discriminative cues. Our approach consists of two core
modules: Cross-modal Consistency Learning (CMCL) and Multi-modal Collaborative
Diagnosis (MMCD). CMCL includes Pseudo-label Generation (PLG) and Cross-modal
Consistency Diagnosis (CMCD). In PLG, a Multimodal Large Language Model is used
to generate pseudo-labels for evaluating cross-modal semantic consistency.
Then, CMCD extracts [CLS] tokens and computes cosine loss to quantify
cross-modal inconsistencies. MMCD further integrates multimodal features
through Multimodal Feature Fusion (MFF) and Probability Scores Fusion (PSF).
MFF employs a co-attention mechanism to enhance semantic interactions across
different modalities, while a Transformer is utilized for comprehensive feature
fusion. Meanwhile, PSF further integrates the fake news probability scores
obtained in the previous step. Extensive experiments on established benchmarks
(FakeSV and FakeTT) demonstrate our model exhibits outstanding performance in
Fake videos detection.

</details>


### [74] [Geolocating Earth Imagery from ISS: Integrating Machine Learning with Astronaut Photography for Enhanced Geographic Mapping](https://arxiv.org/abs/2504.21194)
*Vedika Srivastava, Hemant Kumar Singh, Jaisal Singh*

Main category: cs.CV

TL;DR: A novel method for geolocating ISS images using machine learning (NN, SIFT, GPT-4) shows promise in identifying Earth locations from astronaut photos.


<details>
  <summary>Details</summary>
Motivation: Precise ISS coordinates exist, but specific Earth locations in astronaut photos often remain unidentified, creating a gap in automated geolocation.

Method: Three pipelines: Neural Network (NN), SIFT-based method, and GPT-4 model, each tailored for high-resolution ISS imagery to identify geographical features.

Result: NN excelled in feature matching, SIFT in zoomed-in images, and GPT-4 provided enriched descriptions. Evaluation on 140+ ISS images showed varied success.

Conclusion: The research enhances geolocation accuracy for space-based imagery, supporting environmental monitoring and global mapping.

Abstract: This paper presents a novel approach to geolocating images captured from the
International Space Station (ISS) using advanced machine learning algorithms.
Despite having precise ISS coordinates, the specific Earth locations depicted
in astronaut-taken photographs often remain unidentified. Our research
addresses this gap by employing three distinct image processing pipelines: a
Neural Network based approach, a SIFT based method, and GPT-4 model. Each
pipeline is tailored to process high-resolution ISS imagery, identifying both
natural and man-made geographical features. Through extensive evaluation on a
diverse dataset of over 140 ISS images, our methods demonstrate significant
promise in automated geolocation with varied levels of success. The NN approach
showed a high success rate in accurately matching geographical features, while
the SIFT pipeline excelled in processing zoomed-in images. GPT-4 model provided
enriched geographical descriptions alongside location predictions. This
research contributes to the fields of remote sensing and Earth observation by
enhancing the accuracy and efficiency of geolocating space-based imagery,
thereby aiding environmental monitoring and global mapping efforts.

</details>


### [75] [MemeBLIP2: A novel lightweight multimodal system to detect harmful memes](https://arxiv.org/abs/2504.21226)
*Jiaqi Liu, Ran Tong, Aowei Shen, Shuzheng Li, Changlin Yang, Lisha Xu*

Main category: cs.CV

TL;DR: MemeBLIP2 is a lightweight multimodal system for detecting harmful memes by effectively combining image and text features, improving detection accuracy even for subtle or culturally specific content.


<details>
  <summary>Details</summary>
Motivation: Some memes contain harmful messages like hate speech, necessitating an effective detection system that can handle the multimodal nature of memes.

Method: The system uses BLIP-2 as its core vision-language model, adding modules to align and fuse image and text representations in a shared space for better classification.

Result: Evaluated on the PrideMM dataset, MemeBLIP2 successfully captures subtle cues in both modalities, enhancing harmful meme detection.

Conclusion: MemeBLIP2 demonstrates improved performance in detecting harmful memes, even in complex cases involving irony or cultural specificity.

Abstract: Memes often merge visuals with brief text to share humor or opinions, yet
some memes contain harmful messages such as hate speech. In this paper, we
introduces MemeBLIP2, a light weight multimodal system that detects harmful
memes by combining image and text features effectively. We build on previous
studies by adding modules that align image and text representations into a
shared space and fuse them for better classification. Using BLIP-2 as the core
vision-language model, our system is evaluated on the PrideMM datasets. The
results show that MemeBLIP2 can capture subtle cues in both modalities, even in
cases with ironic or culturally specific content, thereby improving the
detection of harmful material.

</details>


### [76] [T2ID-CAS: Diffusion Model and Class Aware Sampling to Mitigate Class Imbalance in Neck Ultrasound Anatomical Landmark Detection](https://arxiv.org/abs/2504.21231)
*Manikanta Varaganti, Amulya Vankayalapati, Nour Awad, Gregory R. Dion, Laura J. Brattain*

Main category: cs.CV

TL;DR: T2ID-CAS, a hybrid method combining text-to-image diffusion and class-aware sampling, improves anatomical landmark detection in neck ultrasound by addressing class imbalance, achieving 88.2 mAP.


<details>
  <summary>Details</summary>
Motivation: Class imbalance in neck ultrasound datasets hinders object detection models, especially for underrepresented structures like tracheal rings and vocal folds.

Method: Proposes T2ID-CAS, a hybrid approach using text-to-image latent diffusion and class-aware sampling to generate synthetic samples for minority classes.

Result: Achieved 88.2 mean Average Precision (mAP) with YOLOv9, significantly outperforming the baseline of 66.

Conclusion: T2ID-CAS is a scalable and efficient solution for class imbalance in AI-assisted ultrasound-guided interventions.

Abstract: Neck ultrasound (US) plays a vital role in airway management by providing
non-invasive, real-time imaging that enables rapid and precise interventions.
Deep learning-based anatomical landmark detection in neck US can further
facilitate procedural efficiency. However, class imbalance within datasets,
where key structures like tracheal rings and vocal folds are underrepresented,
presents significant challenges for object detection models. To address this,
we propose T2ID-CAS, a hybrid approach that combines a text-to-image latent
diffusion model with class-aware sampling to generate high-quality synthetic
samples for underrepresented classes. This approach, rarely explored in the
ultrasound domain, improves the representation of minority classes.
Experimental results using YOLOv9 for anatomical landmark detection in neck US
demonstrated that T2ID-CAS achieved a mean Average Precision of 88.2,
significantly surpassing the baseline of 66. This highlights its potential as a
computationally efficient and scalable solution for mitigating class imbalance
in AI-assisted ultrasound-guided interventions.

</details>


### [77] [Ditto: Motion-Space Diffusion for Controllable Realtime Talking Head Synthesis](https://arxiv.org/abs/2411.19509)
*Tianqi Li, Ruobing Zheng, Minghui Yang, Jingdong Chen, Ming Yang*

Main category: cs.CV

TL;DR: Ditto is a diffusion-based talking head framework offering fine-grained control and real-time inference, addressing slow speed and lack of control in existing models.


<details>
  <summary>Details</summary>
Motivation: Current diffusion models for talking head synthesis suffer from slow inference and insufficient control over outputs.

Method: Uses a motion extractor and diffusion transformer to generate motion representations, optimizes architecture/training, and employs diverse conditional signals for control.

Result: Ditto achieves compelling videos with superior controllability and real-time performance.

Conclusion: Ditto effectively balances quality, control, and speed, making it suitable for interactive applications.

Abstract: Recent advances in diffusion models have endowed talking head synthesis with
subtle expressions and vivid head movements, but have also led to slow
inference speed and insufficient control over generated results. To address
these issues, we propose Ditto, a diffusion-based talking head framework that
enables fine-grained controls and real-time inference. Specifically, we utilize
an off-the-shelf motion extractor and devise a diffusion transformer to
generate representations in a specific motion space. We optimize the model
architecture and training strategy to address the issues in generating motion
representations, including insufficient disentanglement between motion and
identity, and large internal discrepancies within the representation. Besides,
we employ diverse conditional signals while establishing a mapping between
motion representation and facial semantics, enabling control over the
generation process and correction of the results. Moreover, we jointly optimize
the holistic framework to enable streaming processing, real-time inference, and
low first-frame delay, offering functionalities crucial for interactive
applications such as AI assistants. Extensive experimental results demonstrate
that Ditto generates compelling talking head videos and exhibits superiority in
both controllability and real-time performance.

</details>


### [78] [Subject Information Extraction for Novelty Detection with Domain Shifts](https://arxiv.org/abs/2504.21247)
*Yangyang Qu, Dazhi Fu, Jicong Fan*

Main category: cs.CV

TL;DR: A novel method for unsupervised novelty detection (UND) addresses domain shift by separating subject and background information, improving detection accuracy.


<details>
  <summary>Details</summary>
Motivation: Existing UND methods fail under domain shifts where training and testing data differ in background conditions, leading to misclassification.

Method: The method minimizes mutual information between subject and background representations, using a deep Gaussian mixture model for background variation. Novelty detection is performed only on subject representations.

Result: The model generalizes well to unseen domains and outperforms baselines, especially under significant domain shifts.

Conclusion: The proposed approach effectively handles domain shifts in UND, enhancing detection performance.

Abstract: Unsupervised novelty detection (UND), aimed at identifying novel samples, is
essential in fields like medical diagnosis, cybersecurity, and industrial
quality control. Most existing UND methods assume that the training data and
testing normal data originate from the same domain and only consider the
distribution variation between training data and testing data. However, in real
scenarios, it is common for normal testing and training data to originate from
different domains, a challenge known as domain shift. The discrepancies between
training and testing data often lead to incorrect classification of normal data
as novel by existing methods. A typical situation is that testing normal data
and training data describe the same subject, yet they differ in the background
conditions. To address this problem, we introduce a novel method that separates
subject information from background variation encapsulating the domain
information to enhance detection performance under domain shifts. The proposed
method minimizes the mutual information between the representations of the
subject and background while modelling the background variation using a deep
Gaussian mixture model, where the novelty detection is conducted on the subject
representations solely and hence is not affected by the variation of domains.
Extensive experiments demonstrate that our model generalizes effectively to
unseen domains and significantly outperforms baseline methods, especially under
substantial domain shifts between training and testing data.

</details>


### [79] [Omni-Dish: Photorealistic and Faithful Image Generation and Editing for Arbitrary Chinese Dishes](https://arxiv.org/abs/2504.09948)
*Huijie Liu, Bingcan Wang, Jie Hu, Xiaoming Wei, Guoliang Kang*

Main category: cs.CV

TL;DR: Omni-Dish is a text-to-image model for Chinese dishes, addressing domain-specific challenges with a curated dataset, recaptioning, and fine-grained training. It also supports dish editing via Concept-Enhanced P2P.


<details>
  <summary>Details</summary>
Motivation: Existing text-to-image models lack domain-specific fidelity for Chinese dishes, prompting the need for a tailored solution.

Method: Developed a dish curation pipeline, recaption strategy, and coarse-to-fine training. Enhanced user input with a caption library and LLM. Introduced Concept-Enhanced P2P for editing.

Result: Superior performance in generating photorealistic and faithful Chinese dish images, with extended capabilities for dish editing.

Conclusion: Omni-Dish effectively addresses domain-specific challenges and outperforms existing methods, offering practical applications in the food industry.

Abstract: Dish images play a crucial role in the digital era, with the demand for
culturally distinctive dish images continuously increasing due to the
digitization of the food industry and e-commerce. In general cases, existing
text-to-image generation models excel in producing high-quality images;
however, they struggle to capture diverse characteristics and faithful details
of specific domains, particularly Chinese dishes. To address this limitation,
we propose Omni-Dish, the first text-to-image generation model specifically
tailored for Chinese dishes. We develop a comprehensive dish curation pipeline,
building the largest dish dataset to date. Additionally, we introduce a
recaption strategy and employ a coarse-to-fine training scheme to help the
model better learn fine-grained culinary nuances. During inference, we enhance
the user's textual input using a pre-constructed high-quality caption library
and a large language model, enabling more photorealistic and faithful image
generation. Furthermore, to extend our model's capability for dish editing
tasks, we propose Concept-Enhanced P2P. Based on this approach, we build a dish
editing dataset and train a specialized editing model. Extensive experiments
demonstrate the superiority of our methods.

</details>


### [80] [Multi-modal Transfer Learning for Dynamic Facial Emotion Recognition in the Wild](https://arxiv.org/abs/2504.21248)
*Ezra Engel, Lishan Li, Chris Hudy, Robert Schleusner*

Main category: cs.CV

TL;DR: The paper explores multi-modal transfer learning to enhance facial expression recognition (FER) on the DFEW dataset, achieving modest accuracy improvements.


<details>
  <summary>Details</summary>
Motivation: FER is challenging due to subtle facial feature changes, and the study aims to improve classification accuracy using multi-modal approaches.

Method: Combines pretrained ResNets, OpenPose, and OmniVec networks for cross-temporal, multi-modal feature extraction, applied to a transformer-based classifier.

Result: Finely-tuned multi-modal feature generators modestly improve classification accuracy.

Conclusion: Multi-modal transfer learning shows potential for enhancing FER, though improvements are incremental.

Abstract: Facial expression recognition (FER) is a subset of computer vision with
important applications for human-computer-interaction, healthcare, and customer
service. FER represents a challenging problem-space because accurate
classification requires a model to differentiate between subtle changes in
facial features. In this paper, we examine the use of multi-modal transfer
learning to improve performance on a challenging video-based FER dataset,
Dynamic Facial Expression in-the-Wild (DFEW). Using a combination of pretrained
ResNets, OpenPose, and OmniVec networks, we explore the impact of
cross-temporal, multi-modal features on classification accuracy. Ultimately, we
find that these finely-tuned multi-modal feature generators modestly improve
accuracy of our transformer-based classification model.

</details>


### [81] [VideoMultiAgents: A Multi-Agent Framework for Video Question Answering](https://arxiv.org/abs/2504.20091)
*Noriyuki Kugo, Xiang Li, Zixin Li, Ashish Gupta, Arpandeep Khatua, Nidhish Jain, Chaitanya Patel, Yuta Kyuragi, Yasunori Ishii, Masamoto Tanabiki, Kazuki Kozuka, Ehsan Adeli*

Main category: cs.CV

TL;DR: VideoMultiAgents improves VQA by using specialized agents for multimodal reasoning and question-guided captions, achieving SOTA results.


<details>
  <summary>Details</summary>
Motivation: Existing VQA methods struggle with temporal and interactive contexts due to reliance on frame-level captions.

Method: Introduces VideoMultiAgents, a framework with vision, scene graph, and text agents, plus question-guided caption generation.

Result: Achieves SOTA on Intent-QA (79.0%), EgoSchema subset (75.4%), and NExT-QA (79.6%).

Conclusion: The framework effectively enhances video understanding through complementary multimodal reasoning.

Abstract: Video Question Answering (VQA) inherently relies on multimodal reasoning,
integrating visual, temporal, and linguistic cues to achieve a deeper
understanding of video content. However, many existing methods rely on feeding
frame-level captions into a single model, making it difficult to adequately
capture temporal and interactive contexts. To address this limitation, we
introduce VideoMultiAgents, a framework that integrates specialized agents for
vision, scene graph analysis, and text processing. It enhances video
understanding leveraging complementary multimodal reasoning from independently
operating agents. Our approach is also supplemented with a question-guided
caption generation, which produces captions that highlight objects, actions,
and temporal transitions directly relevant to a given query, thus improving the
answer accuracy. Experimental results demonstrate that our method achieves
state-of-the-art performance on Intent-QA (79.0%, +6.2% over previous SOTA),
EgoSchema subset (75.4%, +3.4%), and NExT-QA (79.6%, +0.4%). The source code is
available at https://github.com/PanasonicConnect/VideoMultiAgents.

</details>


### [82] [CoCoDiff: Diversifying Skeleton Action Features via Coarse-Fine Text-Co-Guided Latent Diffusion](https://arxiv.org/abs/2504.21266)
*Zhifu Zhao, Hanyang Hua, Jianan Li, Shaoxin Wu, Fu Li, Yangtao Zhou, Yang Li*

Main category: cs.CV

TL;DR: CoCoDiff, a diffusion model with coarse-fine text co-guidance, enhances feature diversity and semantic consistency in action recognition without extra inference cost.


<details>
  <summary>Details</summary>
Motivation: Existing methods for feature diversity in action recognition are inefficient and semantically inconsistent.

Method: CoCoDiff uses a latent diffusion model with multi-granularity textual guidance from LLMs to generate diverse, semantically consistent features.

Result: Achieves SOTA performance on NTU RGB+D, NTU RGB+D 120, and Kinetics-Skeleton benchmarks.

Conclusion: CoCoDiff is an effective plug-and-play solution for improving action recognition models.

Abstract: In action recognition tasks, feature diversity is essential for enhancing
model generalization and performance. Existing methods typically promote
feature diversity by expanding the training data in the sample space, which
often leads to inefficiencies and semantic inconsistencies. To overcome these
problems, we propose a novel Coarse-fine text co-guidance Diffusion model
(CoCoDiff). CoCoDiff generates diverse yet semantically consistent features in
the latent space by leveraging diffusion and multi-granularity textual
guidance. Specifically, our approach feeds spatio-temporal features extracted
from skeleton sequences into a latent diffusion model to generate diverse
action representations. Meanwhile, we introduce a coarse-fine text co-guided
strategy that leverages textual information from large language models (LLMs)
to ensure semantic consistency between the generated features and the original
inputs. It is noted that CoCoDiff operates as a plug-and-play auxiliary module
during training, incurring no additional inference cost. Extensive experiments
demonstrate that CoCoDiff achieves SOTA performance on skeleton-based action
recognition benchmarks, including NTU RGB+D, NTU RGB+D 120 and
Kinetics-Skeleton.

</details>


### [83] [Mamba Based Feature Extraction And Adaptive Multilevel Feature Fusion For 3D Tumor Segmentation From Multi-modal Medical Image](https://arxiv.org/abs/2504.21281)
*Zexin Ji, Beiji Zou, Xiaoyan Kui, Hua Li, Pierre Vera, Su Ruan*

Main category: cs.CV

TL;DR: A Mamba-based method for 3D multi-modal medical image segmentation improves tumor region identification by combining modality-specific feature extraction and adaptive multilevel fusion.


<details>
  <summary>Details</summary>
Motivation: Challenges in 3D medical image segmentation include intensity variations and tumor morphology, with existing methods (CNNs and Transformers) lacking efficiency or global feature capture.

Method: Proposes a Mamba-based encoder for modality-specific feature extraction and a bi-level synergistic integration block for dynamic multi-modal fusion, followed by a decoder for segmentation.

Result: Outperforms state-of-the-art CNN, Transformer, and Mamba-based methods on PET/CT and MRI datasets.

Conclusion: The method effectively leverages modality-specific features and fusion for accurate 3D tumor segmentation.

Abstract: Multi-modal 3D medical image segmentation aims to accurately identify tumor
regions across different modalities, facing challenges from variations in image
intensity and tumor morphology. Traditional convolutional neural network
(CNN)-based methods struggle with capturing global features, while
Transformers-based methods, despite effectively capturing global context,
encounter high computational costs in 3D medical image segmentation. The Mamba
model combines linear scalability with long-distance modeling, making it a
promising approach for visual representation learning. However, Mamba-based 3D
multi-modal segmentation still struggles to leverage modality-specific features
and fuse complementary information effectively. In this paper, we propose a
Mamba based feature extraction and adaptive multilevel feature fusion for 3D
tumor segmentation using multi-modal medical image. We first develop the
specific modality Mamba encoder to efficiently extract long-range relevant
features that represent anatomical and pathological structures present in each
modality. Moreover, we design an bi-level synergistic integration block that
dynamically merges multi-modal and multi-level complementary features by the
modality attention and channel attention learning. Lastly, the decoder combines
deep semantic information with fine-grained details to generate the tumor
segmentation map. Experimental results on medical image datasets (PET/CT and
MRI multi-sequence) show that our approach achieve competitive performance
compared to the state-of-the-art CNN, Transformer, and Mamba-based approaches.

</details>


### [84] [Underwater Image Enhancement via Dehazing and Color Restoration](https://arxiv.org/abs/2409.09779)
*Chengqin Wu, Shuai Yu, Tuyan Luo, Qiuhua Rao, Qingson Hu, Jingxiang Xu, Lijun Zhang*

Main category: cs.CV

TL;DR: WaterFormer, a ViT-based network, enhances underwater images by separately addressing haze and color cast, outperforming existing methods.


<details>
  <summary>Details</summary>
Motivation: Underwater images suffer from low contrast, blurriness, and color degradation, which existing methods treat as a unified process, neglecting their independence and synergy.

Method: WaterFormer uses a dehazing block (DehazeFormer), a color restoration block (CRB), and a channel fusion block (CFB) to decouple and integrate features. It includes a soft reconstruction layer and employs Chromatic Consistency Loss and Sobel Color Loss.

Result: WaterFormer outperforms state-of-the-art methods in enhancing underwater images.

Conclusion: The proposed method effectively addresses underwater image degradation by leveraging ViT and specialized loss functions, achieving superior enhancement.

Abstract: Underwater visual imaging is crucial for marine engineering, but it suffers
from low contrast, blurriness, and color degradation, which hinders downstream
analysis. Existing underwater image enhancement methods often treat the haze
and color cast as a unified degradation process, neglecting their inherent
independence while overlooking their synergistic relationship. To overcome this
limitation, we propose a Vision Transformer (ViT)-based network (referred to as
WaterFormer) to improve underwater image quality. WaterFormer contains three
major components: a dehazing block (DehazeFormer Block) to capture the
self-correlated haze features and extract deep-level features, a Color
Restoration Block (CRB) to capture self-correlated color cast features, and a
Channel Fusion Block (CFB) that dynamically integrates these decoupled features
to achieve comprehensive enhancement. To ensure authenticity, a soft
reconstruction layer based on the underwater imaging physics model is included.
Further, a Chromatic Consistency Loss and Sobel Color Loss are designed to
respectively preserve color fidelity and enhance structural details during
network training. Comprehensive experimental results demonstrate that
WaterFormer outperforms other state-of-the-art methods in enhancing underwater
images.

</details>


### [85] [Can We Achieve Efficient Diffusion without Self-Attention? Distilling Self-Attention into Convolutions](https://arxiv.org/abs/2504.21292)
*ZiYi Dong, Chengxing Zhou, Weijian Deng, Pengxu Wei, Xiangyang Ji, Liang Lin*

Main category: cs.CV

TL;DR: The paper reveals that self-attention in diffusion models is more localized than assumed, leading to the proposal of ΔConvFusion, a convolutional alternative that matches performance while being significantly more efficient.


<details>
  <summary>Details</summary>
Motivation: The motivation stems from the observation that self-attention in diffusion models often exhibits localized patterns, questioning the necessity of global interactions and suggesting a more efficient alternative.

Method: The method involves replacing self-attention modules with Pyramid Convolution Blocks (ΔConvBlocks), distilling attention patterns into localized operations while keeping other components unchanged.

Result: ΔConvFusion achieves comparable performance to transformer-based models, reduces computational cost by 6929×, and surpasses LinFusion by 5.42× in efficiency without losing generative fidelity.

Conclusion: The conclusion highlights that localized convolutional operations can effectively replace global self-attention in diffusion models, offering significant efficiency gains without compromising quality.

Abstract: Contemporary diffusion models built upon U-Net or Diffusion Transformer (DiT)
architectures have revolutionized image generation through transformer-based
attention mechanisms. The prevailing paradigm has commonly employed
self-attention with quadratic computational complexity to handle global spatial
relationships in complex images, thereby synthesizing high-fidelity images with
coherent visual semantics.Contrary to conventional wisdom, our systematic
layer-wise analysis reveals an interesting discrepancy: self-attention in
pre-trained diffusion models predominantly exhibits localized attention
patterns, closely resembling convolutional inductive biases. This suggests that
global interactions in self-attention may be less critical than commonly
assumed.Driven by this, we propose \(\Delta\)ConvFusion to replace conventional
self-attention modules with Pyramid Convolution Blocks
(\(\Delta\)ConvBlocks).By distilling attention patterns into localized
convolutional operations while keeping other components frozen,
\(\Delta\)ConvFusion achieves performance comparable to transformer-based
counterparts while reducing computational cost by 6929$\times$ and surpassing
LinFusion by 5.42$\times$ in efficiency--all without compromising generative
fidelity.

</details>


### [86] [Learning Multi-view Multi-class Anomaly Detection](https://arxiv.org/abs/2504.21294)
*Qianzi Yu, Yang Cao, Yu Kang*

Main category: cs.CV

TL;DR: MVMCAD improves multi-view anomaly detection with a semi-frozen encoder, Anomaly Amplification Module, and Cross-Feature Loss, achieving state-of-the-art results.


<details>
  <summary>Details</summary>
Motivation: Existing multi-class anomaly detection models fail in multi-view scenarios due to poor modeling of view relationships and complementary information.

Method: Proposes MVMCAD with a semi-frozen encoder, Anomaly Amplification Module (AAM), and Cross-Feature Loss for better feature modeling and anomaly signal enhancement.

Result: Achieves 91.0/88.6/82.1 (image-level) and 99.1/43.9/48.2/95.2 (pixel-level) on Real-IAD dataset.

Conclusion: MVMCAD effectively integrates multi-view information and enhances anomaly detection, outperforming existing methods.

Abstract: The latest trend in anomaly detection is to train a unified model instead of
training a separate model for each category. However, existing multi-class
anomaly detection (MCAD) models perform poorly in multi-view scenarios because
they often fail to effectively model the relationships and complementary
information among different views. In this paper, we introduce a Multi-View
Multi-Class Anomaly Detection model (MVMCAD), which integrates information from
multiple views to accurately identify anomalies. Specifically, we propose a
semi-frozen encoder, where a pre-encoder prior enhancement mechanism is added
before the frozen encoder, enabling stable cross-view feature modeling and
efficient adaptation for improved anomaly detection. Furthermore, we propose an
Anomaly Amplification Module (AAM) that models global token interactions and
suppresses normal regions to enhance anomaly signals, leading to improved
detection performance in multi-view settings. Finally, we propose a
Cross-Feature Loss that aligns shallow encoder features with deep decoder
features and vice versa, enhancing the model's sensitivity to anomalies at
different semantic levels under multi-view scenarios. Extensive experiments on
the Real-IAD dataset for multi-view multi-class anomaly detection validate the
effectiveness of our approach, achieving state-of-the-art performance of
91.0/88.6/82.1 and 99.1/43.9/48.2/95.2 for image-level and the pixel-level,
respectively.

</details>


### [87] [WARP-LCA: Efficient Convolutional Sparse Coding with Locally Competitive Algorithm](https://arxiv.org/abs/2410.18794)
*Geoffrey Kasenbacher, Felix Ehret, Gerrit Ecke, Sebastian Otte*

Main category: cs.CV

TL;DR: WARP-LCA improves LCA by using a predictor network for faster convergence and better solutions, addressing inefficiency and non-convexity issues.


<details>
  <summary>Details</summary>
Motivation: LCA with hard-thresholding is inefficient and prone to suboptimal minima, prompting the need for a better approach.

Method: Proposes WARP-LCA, which uses a predictor network to initialize LCA states, enhancing speed and solution quality.

Result: WARP-LCA converges faster, achieves better minima, and produces sparser, more robust representations.

Conclusion: WARP-LCA outperforms conventional LCA, advancing biologically inspired deep learning for sparse coding.

Abstract: The locally competitive algorithm (LCA) can solve sparse coding problems
across a wide range of use cases. Recently, convolution-based LCA approaches
have been shown to be highly effective for enhancing robustness for image
recognition tasks in vision pipelines. To additionally maximize
representational sparsity, LCA with hard-thresholding can be applied. While
this combination often yields very good solutions satisfying an $\ell_0$
sparsity criterion, it comes with significant drawbacks for practical
application: (i) LCA is very inefficient, typically requiring hundreds of
optimization cycles for convergence; (ii) the use of hard-thresholding results
in a non-convex loss function, which might lead to suboptimal minima. To
address these issues, we propose the Locally Competitive Algorithm with State
Warm-up via Predictive Priming (WARP-LCA), which leverages a predictor network
to provide a suitable initial guess of the LCA state based on the current
input. Our approach significantly improves both convergence speed and the
quality of solutions, while maintaining and even enhancing the overall
strengths of LCA. We demonstrate that WARP-LCA converges faster by orders of
magnitude and reaches better minima compared to conventional LCA. Moreover, the
learned representations are more sparse and exhibit superior properties in
terms of reconstruction and denoising quality as well as robustness when
applied in deep recognition pipelines. Furthermore, we apply WARP-LCA to image
denoising tasks, showcasing its robustness and practical effectiveness. Our
findings confirm that the naive use of LCA with hard-thresholding results in
suboptimal minima, whereas initializing LCA with a predictive guess results in
better outcomes. This research advances the field of biologically inspired deep
learning by providing a novel approach to convolutional sparse coding.

</details>


### [88] [CMD: Constraining Multimodal Distribution for Domain Adaptation in Stereo Matching](https://arxiv.org/abs/2504.21302)
*Zhelun Shen, Zhuo Li, Chenming Wu, Zhibo Rao, Lina Liu, Yuchao Dai, Liangjun Zhang*

Main category: cs.CV

TL;DR: The paper proposes CMD, a method to improve stereo matching in unsupervised domain adaptation by addressing multimodal disparity distributions.


<details>
  <summary>Details</summary>
Motivation: Existing methods like soft argmin and smooth L1 loss degrade in target domains due to multimodal disparity distributions.

Method: CMD introduces uncertainty-regularized minimization and anisotropic soft argmin to encourage unimodal disparity distributions.

Result: Experiments show improved generalization in stereo-matching models for domain adaptation.

Conclusion: CMD effectively enhances prediction accuracy in unsupervised domain adaptation scenarios.

Abstract: Recently, learning-based stereo matching methods have achieved great
improvement in public benchmarks, where soft argmin and smooth L1 loss play a
core contribution to their success. However, in unsupervised domain adaptation
scenarios, we observe that these two operations often yield multimodal
disparity probability distributions in target domains, resulting in degraded
generalization. In this paper, we propose a novel approach, Constrain
Multi-modal Distribution (CMD), to address this issue. Specifically, we
introduce \textit{uncertainty-regularized minimization} and \textit{anisotropic
soft argmin} to encourage the network to produce predominantly unimodal
disparity distributions in the target domain, thereby improving prediction
accuracy. Experimentally, we apply the proposed method to multiple
representative stereo-matching networks and conduct domain adaptation from
synthetic data to unlabeled real-world scenes. Results consistently demonstrate
improved generalization in both top-performing and domain-adaptable
stereo-matching models. The code for CMD will be available at:
\href{https://github.com/gallenszl/CMD}{https://github.com/gallenszl/CMD}.

</details>


### [89] [High-Frequency Enhanced Hybrid Neural Representation for Video Compression](https://arxiv.org/abs/2411.06685)
*Li Yu, Zhihui Li, Jimin Xiao, Moncef Gabbouj*

Main category: cs.CV

TL;DR: The paper introduces a High-Frequency Enhanced Hybrid Neural Representation Network (HF-NeRV) to improve video compression by preserving high-frequency details, outperforming existing methods.


<details>
  <summary>Details</summary>
Motivation: Existing neural video representations (NeRV) lack high-frequency details in reconstructed videos, limiting their effectiveness for fine detail synthesis.

Method: The proposed method includes a wavelet high-frequency encoder (WFD blocks), High-Frequency Feature Modulation (HFM) block, refined Harmonic decoder, and Dynamic Weighted Frequency Loss to enhance detail preservation.

Result: Experiments on Bunny and UVG datasets show superior performance in detail preservation and compression compared to other methods.

Conclusion: The HF-NeRV effectively addresses the high-frequency detail loss in neural video representations, improving compression and reconstruction quality.

Abstract: Neural Representations for Videos (NeRV) have simplified the video codec
process and achieved swift decoding speeds by encoding video content into a
neural network, presenting a promising solution for video compression. However,
existing work overlooks the crucial issue that videos reconstructed by these
methods lack high-frequency details. To address this problem, this paper
introduces a High-Frequency Enhanced Hybrid Neural Representation Network. Our
method focuses on leveraging high-frequency information to improve the
synthesis of fine details by the network. Specifically, we design a wavelet
high-frequency encoder that incorporates Wavelet Frequency Decomposer (WFD)
blocks to generate high-frequency feature embeddings. Next, we design the
High-Frequency Feature Modulation (HFM) block, which leverages the extracted
high-frequency embeddings to enhance the fitting process of the decoder.
Finally, with the refined Harmonic decoder block and a Dynamic Weighted
Frequency Loss, we further reduce the potential loss of high-frequency
information. Experiments on the Bunny and UVG datasets demonstrate that our
method outperforms other methods, showing notable improvements in detail
preservation and compression performance.

</details>


### [90] [The Dual Power of Interpretable Token Embeddings: Jailbreaking Attacks and Defenses for Diffusion Model Unlearning](https://arxiv.org/abs/2504.21307)
*Siyi Chen, Yimeng Zhang, Sijia Liu, Qing Qu*

Main category: cs.CV

TL;DR: The paper proposes an interpretable attack method for diffusion models to reveal retained harmful concepts post-unlearning, and designs a defense strategy.


<details>
  <summary>Details</summary>
Motivation: To address the lack of interpretability in existing attacks on unlearned diffusion models and develop robust defense strategies.

Method: Develops orthogonal, interpretable attack token embeddings that decompose into human-readable textual elements to expose retained harmful concepts.

Result: The attack embeddings are robust and transferable, and the defense method effectively counters both new and existing attacks.

Conclusion: The work provides insights into why unlearned models retain harmful concepts and offers a practical defense solution.

Abstract: Despite the remarkable generalization capabilities of diffusion models,
recent studies have shown that these models can memorize and generate harmful
content when prompted with specific text instructions. Although fine-tuning
approaches have been developed to mitigate this issue by unlearning harmful
concepts, these methods can be easily circumvented through jailbreaking
attacks. This indicates that the harmful concept has not been fully erased from
the model. However, existing attack methods, while effective, lack
interpretability regarding why unlearned models still retain the concept,
thereby hindering the development of defense strategies. In this work, we
address these limitations by proposing an attack method that learns an
orthogonal set of interpretable attack token embeddings. The attack token
embeddings can be decomposed into human-interpretable textual elements,
revealing that unlearned models still retain the target concept through
implicit textual components. Furthermore, these attack token embeddings are
robust and transferable across text prompts, initial noises, and unlearned
models. Finally, leveraging this diverse set of embeddings, we design a defense
method applicable to both our proposed attack and existing attack methods.
Experimental results demonstrate the effectiveness of both our attack and
defense strategies.

</details>


### [91] [A Contrast-Agnostic Method for Ultra-High Resolution Claustrum Segmentation](https://arxiv.org/abs/2411.15388)
*Chiara Mauri, Ryan Fritz, Jocelyn Mora, Benjamin Billot, Juan Eugenio Iglesias, Koen Van Leemput, Jean Augustinack, Douglas N Greve*

Main category: cs.CV

TL;DR: A novel method for claustrum segmentation in ultra-high resolution MRI is proposed, leveraging synthetic training images for robustness across contrasts and resolutions.


<details>
  <summary>Details</summary>
Motivation: The claustrum's elusive nature in MRI due to its thin structure and lack of tools for its study drives the need for an accurate, automatic segmentation method.

Method: The method uses the SynthSeg framework, training a deep learning network with synthetic intensity images derived from manual labels of 18 ultra-high resolution scans.

Result: Achieves Dice score=0.632, mean surface distance=0.458mm, and volumetric similarity=0.867 in 6-fold CV, with robustness in test-retest and multimodal imaging.

Conclusion: The first accurate, contrast- and resolution-agnostic method for claustrum segmentation, now available in Freesurfer and GitHub.

Abstract: The claustrum is a band-like gray matter structure located between putamen
and insula whose exact functions are still actively researched. Its sheet-like
structure makes it barely visible in in vivo Magnetic Resonance Imaging (MRI)
scans at typical resolutions and neuroimaging tools for its study, including
methods for automatic segmentation, are currently very limited. In this paper,
we propose a contrast- and resolution-agnostic method for claustrum
segmentation at ultra-high resolution (0.35 mm isotropic); the method is based
on the SynthSeg segmentation framework (Billot et al., 2023), which leverages
the use of synthetic training intensity images to achieve excellent
generalization. In particular, SynthSeg requires only label maps to be trained,
since corresponding intensity images are synthesized on the fly with random
contrast and resolution. We trained a deep learning network for automatic
claustrum segmentation, using claustrum manual labels obtained from 18
ultra-high resolution MRI scans (mostly ex vivo). We demonstrated the method to
work on these 18 high resolution cases (Dice score = 0.632, mean surface
distance = 0.458 mm, and volumetric similarity = 0.867 using 6-fold Cross
Validation (CV)), and also on in vivo T1-weighted MRI scans at typical
resolutions (~1 mm isotropic). We also demonstrated that the method is robust
in a test-retest setting and when applied to multimodal imaging (T2-weighted,
Proton Density and quantitative T1 scans). To the best of our knowledge this is
the first accurate method for automatic ultra-high resolution claustrum
segmentation, which is robust against changes in contrast and resolution. The
method is released at https://github.com/chiara-mauri/claustrum_segmentation
and as part of the neuroimaging package Freesurfer (Fischl, 2012).

</details>


### [92] [AGHI-QA: A Subjective-Aligned Dataset and Metric for AI-Generated Human Images](https://arxiv.org/abs/2504.21308)
*Yunhao Li, Sijing Wu, Wei Sun, Zhichao Zhang, Yucheng Zhu, Zicheng Zhang, Huiyu Duan, Xiongkuo Min, Guangtao Zhai*

Main category: cs.CV

TL;DR: AGHI-QA is a new benchmark for assessing AI-generated human images (AGHIs), addressing the lack of fine-grained evaluation in existing methods. It includes 4,000 images and introduces AGHI-Assessor, a novel metric combining multimodal models with human-specific features for superior performance.


<details>
  <summary>Details</summary>
Motivation: Existing image quality assessment (IQA) methods fail to provide detailed evaluations for complex subjects like humans, which often have anatomical and textural distortions in AI-generated images.

Method: AGHI-QA is created with 4,000 images from 10 T2I models using 400 text prompts. A subjective study collects multidimensional annotations. AGHI-Assessor, a new metric, integrates large multimodal models (LMMs) with human-specific features.

Result: AGHI-Assessor outperforms existing IQA methods in multidimensional quality assessment and surpasses leading LMMs in detecting structural distortions in AGHIs.

Conclusion: AGHI-QA and AGHI-Assessor provide a comprehensive solution for evaluating AGHIs, addressing limitations of current methods and improving quality assessment.

Abstract: The rapid development of text-to-image (T2I) generation approaches has
attracted extensive interest in evaluating the quality of generated images,
leading to the development of various quality assessment methods for
general-purpose T2I outputs. However, existing image quality assessment (IQA)
methods are limited to providing global quality scores, failing to deliver
fine-grained perceptual evaluations for structurally complex subjects like
humans, which is a critical challenge considering the frequent anatomical and
textural distortions in AI-generated human images (AGHIs). To address this gap,
we introduce AGHI-QA, the first large-scale benchmark specifically designed for
quality assessment of AGHIs. The dataset comprises 4,000 images generated from
400 carefully crafted text prompts using 10 state of-the-art T2I models. We
conduct a systematic subjective study to collect multidimensional annotations,
including perceptual quality scores, text-image correspondence scores, visible
and distorted body part labels. Based on AGHI-QA, we evaluate the strengths and
weaknesses of current T2I methods in generating human images from multiple
dimensions. Furthermore, we propose AGHI-Assessor, a novel quality metric that
integrates the large multimodal model (LMM) with domain-specific human features
for precise quality prediction and identification of visible and distorted body
parts in AGHIs. Extensive experimental results demonstrate that AGHI-Assessor
showcases state-of-the-art performance, significantly outperforming existing
IQA methods in multidimensional quality assessment and surpassing leading LMMs
in detecting structural distortions in AGHIs.

</details>


### [93] [An Evaluation of a Visual Question Answering Strategy for Zero-shot Facial Expression Recognition in Still Images](https://arxiv.org/abs/2504.21309)
*Modesto Castrillón-Santana, Oliverio J Santana, David Freire-Obregón, Daniel Hernández-Sosa, Javier Lorenzo-Navarro*

Main category: cs.CV

TL;DR: The paper explores using Visual Language Models (VLMs) for zero-shot facial expression recognition (FER), showing promising results compared to traditional FER models.


<details>
  <summary>Details</summary>
Motivation: Challenges in FER generalization, especially in zero-shot scenarios, motivate the integration of VLMs to leverage their knowledge for improved performance.

Method: The study evaluates locally executed VLMs using a Visual Question Answering strategy and compares them with state-of-the-art FER models on benchmarks like AffectNet, FERPlus, and RAF-DB.

Result: Some VLMs perform excellently in zero-shot FER, suggesting their potential to enhance generalization.

Conclusion: Further exploration of VLMs is needed to improve FER generalization, as they show promise in zero-shot scenarios.

Abstract: Facial expression recognition (FER) is a key research area in computer vision
and human-computer interaction. Despite recent advances in deep learning,
challenges persist, especially in generalizing to new scenarios. In fact,
zero-shot FER significantly reduces the performance of state-of-the-art FER
models. To address this problem, the community has recently started to explore
the integration of knowledge from Large Language Models for visual tasks. In
this work, we evaluate a broad collection of locally executed Visual Language
Models (VLMs), avoiding the lack of task-specific knowledge by adopting a
Visual Question Answering strategy. We compare the proposed pipeline with
state-of-the-art FER models, both integrating and excluding VLMs, evaluating
well-known FER benchmarks: AffectNet, FERPlus, and RAF-DB. The results show
excellent performance for some VLMs in zero-shot FER scenarios, indicating the
need for further exploration to improve FER generalization.

</details>


### [94] [Text-Conditioned Diffusion Model for High-Fidelity Korean Font Generation](https://arxiv.org/abs/2504.21325)
*Abdul Sami, Avinash Kumar, Irfanullah Memon, Youngwon Jo, Muhammad Rizwan, Jaeyoung Choi*

Main category: cs.CV

TL;DR: A diffusion-based method for generating high-quality Korean fonts from a single reference image, outperforming traditional GANs and VAEs.


<details>
  <summary>Details</summary>
Motivation: Addressing instability and mode collapse in traditional AFG methods (GANs, VAEs) and improving fine detail capture for complex languages like Korean.

Method: Uses a diffusion-based approach with a text encoder for phonetic representations and a pre-trained style encoder from DG FONT, enhanced by perceptual loss.

Result: Generates accurate, detailed Korean fonts across styles, outperforming benchmarks on 2000+ characters.

Conclusion: Proposed method is reliable for authentic Korean font generation, especially in handwritten and printed styles.

Abstract: Automatic font generation (AFG) is the process of creating a new font using
only a few examples of the style images. Generating fonts for complex languages
like Korean and Chinese, particularly in handwritten styles, presents
significant challenges. Traditional AFGs, like Generative adversarial networks
(GANs) and Variational Auto-Encoders (VAEs), are usually unstable during
training and often face mode collapse problems. They also struggle to capture
fine details within font images. To address these problems, we present a
diffusion-based AFG method which generates high-quality, diverse Korean font
images using only a single reference image, focusing on handwritten and printed
styles. Our approach refines noisy images incrementally, ensuring stable
training and visually appealing results. A key innovation is our text encoder,
which processes phonetic representations to generate accurate and contextually
correct characters, even for unseen characters. We used a pre-trained style
encoder from DG FONT to effectively and accurately encode the style images. To
further enhance the generation quality, we used perceptual loss that guides the
model to focus on the global style of generated images. Experimental results on
over 2000 Korean characters demonstrate that our model consistently generates
accurate and detailed font images and outperforms benchmark methods, making it
a reliable tool for generating authentic Korean fonts across different styles.

</details>


### [95] [Simple Visual Artifact Detection in Sora-Generated Videos](https://arxiv.org/abs/2504.21334)
*Misora Sugiyama, Hirokatsu Kataoka*

Main category: cs.CV

TL;DR: The study analyzes visual artifacts in OpenAI's Sora-generated videos, proposing a multi-label classification framework to identify and address quality and safety issues.


<details>
  <summary>Details</summary>
Motivation: Understanding limitations and ensuring safe deployment of video-enabled LLMs (VidLLMs) like Sora, which can generate misleading or low-quality content.

Method: A multi-label classification framework using 2D CNN architectures (ResNet-50, EfficientNet-B3/B4, ViT-Base) trained on 300 manually annotated frames from Sora-generated videos.

Result: ResNet-50 achieved 94.14% accuracy in classifying four artifact types: boundary/edge defects, texture/noise issues, movement/joint anomalies, and object mismatches/disappearances.

Conclusion: The work aids VidLLM development by improving video quality evaluation, artifact analysis, and identifying visual risks for factuality and safety.

Abstract: The December 2024 release of OpenAI's Sora, a powerful video generation model
driven by natural language prompts, highlights a growing convergence between
large language models (LLMs) and video synthesis. As these multimodal systems
evolve into video-enabled LLMs (VidLLMs), capable of interpreting, generating,
and interacting with visual content, understanding their limitations and
ensuring their safe deployment becomes essential. This study investigates
visual artifacts frequently found and reported in Sora-generated videos, which
can compromise quality, mislead viewers, or propagate disinformation. We
propose a multi-label classification framework targeting four common artifact
label types: label 1: boundary / edge defects, label 2: texture / noise issues,
label 3: movement / joint anomalies, and label 4: object mismatches /
disappearances. Using a dataset of 300 manually annotated frames extracted from
15 Sora-generated videos, we trained multiple 2D CNN architectures (ResNet-50,
EfficientNet-B3 / B4, ViT-Base). The best-performing model trained by ResNet-50
achieved an average multi-label classification accuracy of 94.14%. This work
supports the broader development of VidLLMs by contributing to (1) the creation
of datasets for video quality evaluation, (2) interpretable artifact-based
analysis beyond language metrics, and (3) the identification of visual risks
relevant to factuality and safety.

</details>


### [96] [UniBiomed: A Universal Foundation Model for Grounded Biomedical Image Interpretation](https://arxiv.org/abs/2504.21336)
*Linshan Wu, Yuxiang Nie, Sunan He, Jiaxin Zhuang, Hao Chen*

Main category: cs.CV

TL;DR: UniBiomed is a universal foundation model integrating MLLM and SAM for unified biomedical image interpretation, achieving state-of-the-art performance across diverse tasks.


<details>
  <summary>Details</summary>
Motivation: To address the limitations of disjointed AI approaches in biomedical image analysis by unifying text generation and segmentation for holistic interpretation.

Method: Integrates Multi-modal Large Language Model (MLLM) and Segment Anything Model (SAM) to unify clinical text generation and biomedical object segmentation.

Result: Achieves state-of-the-art performance in segmentation, disease recognition, diagnosis, VQA, and report generation across 10 imaging modalities.

Conclusion: UniBiomed introduces a paradigm shift in clinical workflows, enabling automated, end-to-end grounded interpretation for more efficient and accurate biomedical image analysis.

Abstract: Multi-modal interpretation of biomedical images opens up novel opportunities
in biomedical image analysis. Conventional AI approaches typically rely on
disjointed training, i.e., Large Language Models (LLMs) for clinical text
generation and segmentation models for target extraction, which results in
inflexible real-world deployment and a failure to leverage holistic biomedical
information. To this end, we introduce UniBiomed, the first universal
foundation model for grounded biomedical image interpretation. UniBiomed is
based on a novel integration of Multi-modal Large Language Model (MLLM) and
Segment Anything Model (SAM), which effectively unifies the generation of
clinical texts and the segmentation of corresponding biomedical objects for
grounded interpretation. In this way, UniBiomed is capable of tackling a wide
range of biomedical tasks across ten diverse biomedical imaging modalities. To
develop UniBiomed, we curate a large-scale dataset comprising over 27 million
triplets of images, annotations, and text descriptions across ten imaging
modalities. Extensive validation on 84 internal and external datasets
demonstrated that UniBiomed achieves state-of-the-art performance in
segmentation, disease recognition, region-aware diagnosis, visual question
answering, and report generation. Moreover, unlike previous models that rely on
clinical experts to pre-diagnose images and manually craft precise textual or
visual prompts, UniBiomed can provide automated and end-to-end grounded
interpretation for biomedical image analysis. This represents a novel paradigm
shift in clinical workflows, which will significantly improve diagnostic
efficiency. In summary, UniBiomed represents a novel breakthrough in biomedical
AI, unlocking powerful grounded interpretation capabilities for more accurate
and efficient biomedical image analysis.

</details>


### [97] [Towards Improved Cervical Cancer Screening: Vision Transformer-Based Classification and Interpretability](https://arxiv.org/abs/2504.21340)
*Khoa Tuan Nguyen, Ho-min Park, Gaeun Oh, Joris Vankerschaver, Wesley De Neve*

Main category: cs.CV

TL;DR: A novel cervical cell image classification method using EVA-02 transformer model achieves improved F1-score (0.85227) and interpretability via feature analysis.


<details>
  <summary>Details</summary>
Motivation: To enhance cervical cancer screening accuracy and provide interpretable insights into model decisions.

Method: Four-step pipeline: fine-tuning EVA-02, feature extraction, feature selection via ML models, and training a neural network with loss weighting.

Result: Best model achieved F1-score of 0.85227, outperforming baseline (0.84878). Key features identified via Kernel SHAP analysis.

Conclusion: The approach improves classification performance and interpretability, aiding cervical cancer screening.

Abstract: We propose a novel approach to cervical cell image classification for
cervical cancer screening using the EVA-02 transformer model. We developed a
four-step pipeline: fine-tuning EVA-02, feature extraction, selecting important
features through multiple machine learning models, and training a new
artificial neural network with optional loss weighting for improved
generalization. With this design, our best model achieved an F1-score of
0.85227, outperforming the baseline EVA-02 model (0.84878). We also utilized
Kernel SHAP analysis and identified key features correlating with cell
morphology and staining characteristics, providing interpretable insights into
the decision-making process of the fine-tuned model. Our code is available at
https://github.com/Khoa-NT/isbi2025_ps3c.

</details>


### [98] [Vision-Language Model-Based Semantic-Guided Imaging Biomarker for Early Lung Cancer Detection](https://arxiv.org/abs/2504.21344)
*Luoting Zhuang, Seyed Mohammad Hossein Tabatabaei, Ramin Salehi-Rad, Linh M. Tran, Denise R. Aberle, Ashley E. Prosper, William Hsu*

Main category: cs.CV

TL;DR: The paper proposes a method to predict lung nodule malignancy by integrating semantic features from radiologists' assessments with deep learning, improving interpretability and performance.


<details>
  <summary>Details</summary>
Motivation: Current models rely on manual annotation, lack interpretability, and are sensitive to imaging variations, limiting clinical use. This work aims to address these issues.

Method: Finetuned a pretrained CLIP model to align imaging and semantic features for predicting one-year lung cancer diagnosis, using multiple datasets.

Result: Achieved AUROC of 0.90 and AUPRC of 0.78, outperforming baselines, with explainable predictions on nodule characteristics.

Conclusion: The method provides accurate, explainable predictions, aiding clinicians and ensuring generalizability across settings.

Abstract: Objective: A number of machine learning models have utilized semantic
features, deep features, or both to assess lung nodule malignancy. However,
their reliance on manual annotation during inference, limited interpretability,
and sensitivity to imaging variations hinder their application in real-world
clinical settings. Thus, this research aims to integrate semantic features
derived from radiologists' assessments of nodules, allowing the model to learn
clinically relevant, robust, and explainable features for predicting lung
cancer. Methods: We obtained 938 low-dose CT scans from the National Lung
Screening Trial with 1,246 nodules and semantic features. The Lung Image
Database Consortium dataset contains 1,018 CT scans, with 2,625 lesions
annotated for nodule characteristics. Three external datasets were obtained
from UCLA Health, the LUNGx Challenge, and the Duke Lung Cancer Screening. We
finetuned a pretrained Contrastive Language-Image Pretraining model with a
parameter-efficient fine-tuning approach to align imaging and semantic features
and predict the one-year lung cancer diagnosis. Results: We evaluated the
performance of the one-year diagnosis of lung cancer with AUROC and AUPRC and
compared it to three state-of-the-art models. Our model demonstrated an AUROC
of 0.90 and AUPRC of 0.78, outperforming baseline state-of-the-art models on
external datasets. Using CLIP, we also obtained predictions on semantic
features, such as nodule margin (AUROC: 0.81), nodule consistency (0.81), and
pleural attachment (0.84), that can be used to explain model predictions.
Conclusion: Our approach accurately classifies lung nodules as benign or
malignant, providing explainable outputs, aiding clinicians in comprehending
the underlying meaning of model predictions. This approach also prevents the
model from learning shortcuts and generalizes across clinical settings.

</details>


### [99] [Nexus-Gen: A Unified Model for Image Understanding, Generation, and Editing](https://arxiv.org/abs/2504.21356)
*Hong Zhang, Zhongjie Duan, Xingjun Wang, Yingda Chen, Yuze Zhao, Yu Zhang*

Main category: cs.CV

TL;DR: Nexus-Gen is a unified MLLM combining LLM reasoning and diffusion model image synthesis, addressing performance gaps via dual-phase alignment training and prefilled autoregression.


<details>
  <summary>Details</summary>
Motivation: Existing open-source unified MLLMs lag behind domain-specific models; Nexus-Gen aims to bridge this gap by integrating LLMs and diffusion models.

Method: Dual-phase alignment training: (1) LLM predicts image embeddings, (2) vision decoder reconstructs images. Prefilled autoregression avoids error accumulation.

Result: Nexus-Gen achieves integrated capabilities for image understanding, generation, and editing.

Conclusion: Nexus-Gen successfully unifies multimodal tasks, with models and tools publicly available for further research.

Abstract: Unified multimodal large language models (MLLMs) aim to integrate multimodal
understanding and generation abilities through a single framework. Despite
their versatility, existing open-source unified models exhibit performance gaps
against domain-specific architectures. To bridge this gap, we present
Nexus-Gen, a unified model that synergizes the language reasoning capabilities
of LLMs with the image synthesis power of diffusion models. To align the
embedding space of the LLM and diffusion model, we conduct a dual-phase
alignment training process. (1) The autoregressive LLM learns to predict image
embeddings conditioned on multimodal inputs, while (2) the vision decoder is
trained to reconstruct high-fidelity images from these embeddings. During
training the LLM, we identified a critical discrepancy between the
autoregressive paradigm's training and inference phases, where error
accumulation in continuous embedding space severely degrades generation
quality. To avoid this issue, we introduce a prefilled autoregression strategy
that prefills input sequence with position-embedded special tokens instead of
continuous embeddings. Through dual-phase training, Nexus-Gen has developed the
integrated capability to comprehensively address the image understanding,
generation and editing tasks. All models, datasets, and codes are published at
https://github.com/modelscope/Nexus-Gen.git to facilitate further advancements
across the field.

</details>


### [100] [Revisiting Diffusion Autoencoder Training for Image Reconstruction Quality](https://arxiv.org/abs/2504.21368)
*Pramook Khungurn, Sukit Seripanitkarn, Phonphrm Thawatdamrongkit, Supasorn Suwajanakorn*

Main category: cs.CV

TL;DR: Proposes a two-phase training method for Diffusion Autoencoders (DAEs) to improve image quality by focusing on structural information first and then refining details.


<details>
  <summary>Details</summary>
Motivation: Current DAEs with linear-β noise schedules spend too much time on high noise levels, leading to blurry images. The latent code should already contain structural information, allowing fewer steps for structure recovery and more for detail refinement.

Method: 1. Train DAE as a vanilla autoencoder at the highest noise level to encode structural info. 2. Use a noise schedule favoring low-noise regions to refine details.

Result: Improved image quality with accurate high-level structures and low-level details, while preserving latent code properties.

Conclusion: The two-phase training method enhances DAE performance by optimizing noise schedule usage for structure and detail recovery.

Abstract: Diffusion autoencoders (DAEs) are typically formulated as a noise prediction
model and trained with a linear-$\beta$ noise schedule that spends much of its
sampling steps at high noise levels. Because high noise levels are associated
with recovering large-scale image structures and low noise levels with
recovering details, this configuration can result in low-quality and blurry
images. However, it should be possible to improve details while spending fewer
steps recovering structures because the latent code should already contain
structural information. Based on this insight, we propose a new DAE training
method that improves the quality of reconstructed images. We divide training
into two phases. In the first phase, the DAE is trained as a vanilla
autoencoder by always setting the noise level to the highest, forcing the
encoder and decoder to populate the latent code with structural information. In
the second phase, we incorporate a noise schedule that spends more time in the
low-noise region, allowing the DAE to learn how to perfect the details. Our
method results in images that have accurate high-level structures and low-level
details while still preserving useful properties of the latent codes.

</details>


### [101] [IDDM: Bridging Synthetic-to-Real Domain Gap from Physics-Guided Diffusion for Real-world Image Dehazing](https://arxiv.org/abs/2504.21385)
*Shijun Zhou, Yajing Liu, Chunhui Hao, Zhiyuan Liu, Jiandong Tian*

Main category: cs.CV

TL;DR: IDDM proposes a diffusion model incorporating atmospheric scattering to bridge the domain gap between synthetic and real-world hazy images, improving generalization in dehazing tasks.


<details>
  <summary>Details</summary>
Motivation: Current dehazing algorithms trained on synthetic data struggle with real-world generalization due to domain gaps.

Method: IDDM integrates atmospheric scattering into noise diffusion, using a gradual haze formation process to train a denoising Unet.

Result: IDDM effectively restores real-world hazy images despite synthetic training, outperforming state-of-the-art methods.

Conclusion: IDDM successfully bridges the synthetic-to-real domain gap, demonstrating robust dehazing performance.

Abstract: Due to the domain gap between real-world and synthetic hazy images, current
data-driven dehazing algorithms trained on synthetic datasets perform well on
synthetic data but struggle to generalize to real-world scenarios. To address
this challenge, we propose \textbf{I}mage \textbf{D}ehazing \textbf{D}iffusion
\textbf{M}odels (IDDM), a novel diffusion process that incorporates the
atmospheric scattering model into noise diffusion. IDDM aims to use the gradual
haze formation process to help the denoising Unet robustly learn the
distribution of clear images from the conditional input hazy images. We design
a specialized training strategy centered around IDDM. Diffusion models are
leveraged to bridge the domain gap from synthetic to real-world, while the
atmospheric scattering model provides physical guidance for haze formation.
During the forward process, IDDM simultaneously introduces haze and noise into
clear images, and then robustly separates them during the sampling process. By
training with physics-guided information, IDDM shows the ability of domain
generalization, and effectively restores the real-world hazy images despite
being trained on synthetic datasets. Extensive experiments demonstrate the
effectiveness of our method through both quantitative and qualitative
comparisons with state-of-the-art approaches.

</details>


### [102] [Comparison of Different Deep Neural Network Models in the Cultural Heritage Domain](https://arxiv.org/abs/2504.21387)
*Teodor Boyadzhiev, Gabriele Lagani, Luca Ciampi, Giuseppe Amato, Krassimira Ivanova*

Main category: cs.CV

TL;DR: Comparative analysis of CNN and transformer architectures for cultural heritage tasks shows DenseNet as the most efficient.


<details>
  <summary>Details</summary>
Motivation: To evaluate the transferability of knowledge from generic datasets (e.g., ImageNet) to cultural heritage tasks using deep learning.

Method: Tested VGG, ResNet, DenseNet, Visual Transformer, Swin Transformer, and PoolFormer architectures.

Result: DenseNet performed best in terms of efficiency-computability ratio.

Conclusion: DenseNet is the most suitable for cultural heritage tasks among the tested architectures.

Abstract: The integration of computer vision and deep learning is an essential part of
documenting and preserving cultural heritage, as well as improving visitor
experiences. In recent years, two deep learning paradigms have been established
in the field of computer vision: convolutional neural networks and transformer
architectures. The present study aims to make a comparative analysis of some
representatives of these two techniques of their ability to transfer knowledge
from generic dataset, such as ImageNet, to cultural heritage specific tasks.
The results of testing examples of the architectures VGG, ResNet, DenseNet,
Visual Transformer, Swin Transformer, and PoolFormer, showed that DenseNet is
the best in terms of efficiency-computability ratio.

</details>


### [103] [Static or Dynamic: Towards Query-Adaptive Token Selection for Video Question Answering](https://arxiv.org/abs/2504.21403)
*Yumeng Shi, Quanyu Long, Wenya Wang*

Main category: cs.CV

TL;DR: A novel token selection strategy, EXPLORE-THEN-SELECT, improves video question answering by adaptively balancing static and dynamic information based on queries, enhancing efficiency and performance.


<details>
  <summary>Details</summary>
Motivation: Addressing inefficiencies in token usage for video question answering due to varying importance of static and dynamic information across queries.

Method: Proposes EXPLORE-THEN-SELECT: explores token allocations between static and dynamic frames, then uses a query-aware attention-based metric to select optimal tokens without model updates.

Result: Achieves up to 5.8% performance improvement in video question answering benchmarks.

Conclusion: The plug-and-play framework effectively enhances token efficiency and model performance for diverse video-language tasks.

Abstract: Video question answering benefits from the rich information available in
videos, enabling a wide range of applications. However, the large volume of
tokens generated from longer videos presents significant challenges to memory
efficiency and model performance. To alleviate this issue, existing works
propose to compress video inputs, but usually overlooking the varying
importance of static and dynamic information across different queries, leading
to inefficient token usage within limited budgets. To tackle this, we propose a
novel token selection strategy, EXPLORE-THEN-SELECT, that adaptively adjust
static and dynamic information needed based on question requirements. Our
framework first explores different token allocations between static frames,
which preserve spatial details, and dynamic frames, which capture temporal
changes. Next, it employs a query-aware attention-based metric to select the
optimal token combination without model updates. Our proposed framework is
plug-and-play that can be seamlessly integrated within diverse video-language
models. Extensive experiments show that our method achieves significant
performance improvements (up to 5.8%) among various video question answering
benchmarks.

</details>


### [104] [Adapting In-Domain Few-Shot Segmentation to New Domains without Retraining](https://arxiv.org/abs/2504.21414)
*Qi Fan, Kaiqi Liu, Nian Liu, Hisham Cholakkal, Rao Muhammad Anwer, Wenbin Li, Yang Gao*

Main category: cs.CV

TL;DR: The paper proposes Informative Structure Adaptation (ISA) for cross-domain few-shot segmentation (CD-FSS), eliminating the need for retraining by adapting model structures using domain-specific characteristics from support samples.


<details>
  <summary>Details</summary>
Motivation: Addressing the challenges of CD-FSS, such as diverse target domains and limited support data, without costly retraining or redesigning models.

Method: Adapts model structures by identifying domain-specific parameters using a novel structure Fisher score and progressively training with hierarchical support samples.

Result: ISA outperforms existing methods across multiple CD-FSS benchmarks, demonstrating effective domain adaptation.

Conclusion: ISA provides a flexible and efficient solution for CD-FSS by leveraging domain-specific adaptations without retraining.

Abstract: Cross-domain few-shot segmentation (CD-FSS) aims to segment objects of novel
classes in new domains, which is often challenging due to the diverse
characteristics of target domains and the limited availability of support data.
Most CD-FSS methods redesign and retrain in-domain FSS models using various
domain-generalization techniques, which are effective but costly to train. To
address these issues, we propose adapting informative model structures of the
well-trained FSS model for target domains by learning domain characteristics
from few-shot labeled support samples during inference, thereby eliminating the
need for retraining. Specifically, we first adaptively identify domain-specific
model structures by measuring parameter importance using a novel structure
Fisher score in a data-dependent manner. Then, we progressively train the
selected informative model structures with hierarchically constructed training
samples, progressing from fewer to more support shots. The resulting
Informative Structure Adaptation (ISA) method effectively addresses domain
shifts and equips existing well-trained in-domain FSS models with flexible
adaptation capabilities for new domains, eliminating the need to redesign or
retrain CD-FSS models on base data. Extensive experiments validate the
effectiveness of our method, demonstrating superior performance across multiple
CD-FSS benchmarks.

</details>


### [105] [Diff-Prompt: Diffusion-Driven Prompt Generator with Mask Supervision](https://arxiv.org/abs/2504.21423)
*Weicai Yan, Wang Lin, Zirun Guo, Ye Wang, Fangming Feng, Xiaoda Yang, Zehan Wang, Tao Jin*

Main category: cs.CV

TL;DR: Diff-Prompt uses a diffusion model to generate rich, fine-grained prompts for complex tasks, outperforming existing methods in performance metrics.


<details>
  <summary>Details</summary>
Motivation: Existing prompt learning methods lack richness and specificity for complex tasks due to direct optimization via loss backpropagation.

Method: Three-stage approach: (1) Mask-VAE compresses masks into latent space, (2) improved DiT trains a prompt generator, (3) alignment of denoising with pre-trained model for fine-tuning.

Result: Achieves up to 8.87 (R@1) and 14.05 (R@5) improvement over the foundation model, surpassing state-of-the-art methods.

Conclusion: Diff-Prompt validates the potential of generative models for prompt generation, enhancing performance in complex tasks.

Abstract: Prompt learning has demonstrated promising results in fine-tuning pre-trained
multimodal models. However, the performance improvement is limited when applied
to more complex and fine-grained tasks. The reason is that most existing
methods directly optimize the parameters involved in the prompt generation
process through loss backpropagation, which constrains the richness and
specificity of the prompt representations. In this paper, we propose
Diffusion-Driven Prompt Generator (Diff-Prompt), aiming to use the diffusion
model to generate rich and fine-grained prompt information for complex
downstream tasks. Specifically, our approach consists of three stages. In the
first stage, we train a Mask-VAE to compress the masks into latent space. In
the second stage, we leverage an improved Diffusion Transformer (DiT) to train
a prompt generator in the latent space, using the masks for supervision. In the
third stage, we align the denoising process of the prompt generator with the
pre-trained model in the semantic space, and use the generated prompts to
fine-tune the model. We conduct experiments on a complex pixel-level downstream
task, referring expression comprehension, and compare our method with various
parameter-efficient fine-tuning approaches. Diff-Prompt achieves a maximum
improvement of 8.87 in R@1 and 14.05 in R@5 compared to the foundation model
and also outperforms other state-of-the-art methods across multiple metrics.
The experimental results validate the effectiveness of our approach and
highlight the potential of using generative models for prompt generation. Code
is available at https://github.com/Kelvin-ywc/diff-prompt.

</details>


### [106] [SeriesBench: A Benchmark for Narrative-Driven Drama Series Understanding](https://arxiv.org/abs/2504.21435)
*Chenkai Zhang, Yiming Lei, Zeming Liu, Haitao Leng, ShaoGuo Liu, Tingting Gao, Qingjie Liu, Yunhong Wang*

Main category: cs.CV

TL;DR: The paper introduces SeriesBench, a benchmark for evaluating MLLMs on narrative-driven video series, and proposes PC-DCoT, a framework to improve model performance in understanding complex narratives.


<details>
  <summary>Details</summary>
Motivation: Existing benchmarks focus on standalone videos and visual elements, neglecting the complexity of narrative-driven series. This gap motivates the creation of SeriesBench and PC-DCoT.

Method: SeriesBench includes 105 narrative-driven series with 28 tasks. A novel annotation method and PC-DCoT framework are introduced to enhance narrative understanding.

Result: Existing MLLMs struggle with narrative-driven series, but PC-DCoT improves their performance.

Conclusion: SeriesBench and PC-DCoT emphasize the need for advancing MLLMs' narrative understanding capabilities.

Abstract: With the rapid development of Multi-modal Large Language Models (MLLMs), an
increasing number of benchmarks have been established to evaluate the video
understanding capabilities of these models. However, these benchmarks focus on
\textbf{standalone} videos and mainly assess ``visual elements'' like human
actions and object states. In reality, contemporary videos often encompass
complex and continuous narratives, typically presented as a \textbf{series}. To
address this challenge, we propose \textbf{SeriesBench}, a benchmark consisting
of 105 carefully curated narrative-driven series, covering 28 specialized tasks
that require deep narrative understanding. Specifically, we first select a
diverse set of drama series spanning various genres. Then, we introduce a novel
long-span narrative annotation method, combined with a full-information
transformation approach to convert manual annotations into diverse task
formats. To further enhance model capacity for detailed analysis of plot
structures and character relationships within series, we propose a novel
narrative reasoning framework, \textbf{PC-DCoT}. Extensive results on
\textbf{SeriesBench} indicate that existing MLLMs still face significant
challenges in understanding narrative-driven series, while \textbf{PC-DCoT}
enables these MLLMs to achieve performance improvements. Overall, our
\textbf{SeriesBench} and \textbf{PC-DCoT} highlight the critical necessity of
advancing model capabilities to understand narrative-driven series, guiding the
future development of MLLMs. SeriesBench is publicly available at
https://github.com/zackhxn/SeriesBench-CVPR2025.

</details>


### [107] [Rethinking Visual Layer Selection in Multimodal LLMs](https://arxiv.org/abs/2504.21447)
*Haoran Chen, Junyan Lin, Xinhao Chen, Yue Fan, Xin Jin, Hui Su, Jianfeng Dong, Jinlan Fu, Xiaoyu Shen*

Main category: cs.CV

TL;DR: The paper proposes a Layer-wise Representation Similarity approach to categorize CLIP-ViT layers (shallow, middle, deep) and evaluates their impact on MLLM performance, finding optimal layer combinations for various tasks.


<details>
  <summary>Details</summary>
Motivation: Prior MLLMs rely on empirical heuristics for visual feature selection, lacking systematic analysis of CLIP-ViT layer behaviors.

Method: A Layer-wise Representation Similarity approach groups CLIP-ViT layers and tests their impact on MLLMs (1.4B to 7B parameters) across 10 datasets and 4 tasks.

Result: Deep layers excel in OCR tasks, shallow/middle layers outperform in reasoning tasks, and a fusion of all layers consistently improves performance.

Conclusion: The study provides a principled framework for visual layer selection in MLLMs, advancing visual representation learning.

Abstract: Multimodal large language models (MLLMs) have achieved impressive performance
across a wide range of tasks, typically using CLIP-ViT as their visual encoder
due to its strong text-image alignment capabilities. While prior studies
suggest that different CLIP-ViT layers capture different types of information,
with shallower layers focusing on fine visual details and deeper layers
aligning more closely with textual semantics, most MLLMs still select visual
features based on empirical heuristics rather than systematic analysis. In this
work, we propose a Layer-wise Representation Similarity approach to group
CLIP-ViT layers with similar behaviors into {shallow, middle, and deep}
categories and assess their impact on MLLM performance. Building on this
foundation, we revisit the visual layer selection problem in MLLMs at scale,
training LLaVA-style models ranging from 1.4B to 7B parameters. Through
extensive experiments across 10 datasets and 4 tasks, we find that: (1) deep
layers are essential for OCR tasks; (2) shallow and middle layers substantially
outperform deep layers on reasoning tasks involving counting, positioning, and
object localization; (3) a lightweight fusion of features across shallow,
middle, and deep layers consistently outperforms specialized fusion baselines
and single-layer selections, achieving gains on 9 out of 10 datasets. Our work
offers the first principled study of visual layer selection in MLLMs, laying
the groundwork for deeper investigations into visual representation learning
for MLLMs.

</details>


### [108] [VR-FuseNet: A Fusion of Heterogeneous Fundus Data and Explainable Deep Network for Diabetic Retinopathy Classification](https://arxiv.org/abs/2504.21464)
*Shamim Rahim Refat, Ziyan Shirin Raha, Shuvashis Sarker, Faika Fairuj Preotee, MD. Musfikur Rahman, Tashreef Muhammad, Mohammad Shafiul Islam*

Main category: cs.CV

TL;DR: A hybrid deep learning model, VR-FuseNet, is proposed for automated diabetic retinopathy detection, combining VGG19 and ResNet50V2 for improved accuracy (91.824%) and interpretability using XAI techniques.


<details>
  <summary>Details</summary>
Motivation: Diabetic retinopathy is a leading cause of blindness; early detection is crucial. Existing methods face dataset imbalance and generalization issues, necessitating a robust automated solution.

Method: A hybrid dataset from five public sources is preprocessed with SMOTE and CLAHE. VR-FuseNet fuses VGG19 and ResNet50V2 for feature extraction. XAI techniques provide interpretability.

Result: VR-FuseNet achieves 91.824% accuracy, outperforming individual models, with visual explanations highlighting key retinal features.

Conclusion: The hybrid approach enhances diagnostic performance and clinical interpretability, offering a promising tool for diabetic retinopathy detection.

Abstract: Diabetic retinopathy is a severe eye condition caused by diabetes where the
retinal blood vessels get damaged and can lead to vision loss and blindness if
not treated. Early and accurate detection is key to intervention and stopping
the disease progressing. For addressing this disease properly, this paper
presents a comprehensive approach for automated diabetic retinopathy detection
by proposing a new hybrid deep learning model called VR-FuseNet. Diabetic
retinopathy is a major eye disease and leading cause of blindness especially
among diabetic patients so accurate and efficient automated detection methods
are required. To address the limitations of existing methods including dataset
imbalance, diversity and generalization issues this paper presents a hybrid
dataset created from five publicly available diabetic retinopathy datasets.
Essential preprocessing techniques such as SMOTE for class balancing and CLAHE
for image enhancement are applied systematically to the dataset to improve the
robustness and generalizability of the dataset. The proposed VR-FuseNet model
combines the strengths of two state-of-the-art convolutional neural networks,
VGG19 which captures fine-grained spatial features and ResNet50V2 which is
known for its deep hierarchical feature extraction. This fusion improves the
diagnostic performance and achieves an accuracy of 91.824%. The model
outperforms individual architectures on all performance metrics demonstrating
the effectiveness of hybrid feature extraction in Diabetic Retinopathy
classification tasks. To make the proposed model more clinically useful and
interpretable this paper incorporates multiple XAI techniques. These techniques
generate visual explanations that clearly indicate the retinal features
affecting the model's prediction such as microaneurysms, hemorrhages and
exudates so that clinicians can interpret and validate.

</details>


### [109] [Multiview Point Cloud Registration via Optimization in an Autoencoder Latent Space](https://arxiv.org/abs/2504.21467)
*Luc Vedrenne, Sylvain Faisan, Denis Fortun*

Main category: cs.CV

TL;DR: POLAR is a multiview point cloud registration method robust to degradations and large initial angles, outperforming state-of-the-art approaches.


<details>
  <summary>Details</summary>
Motivation: Existing methods for multiview point cloud registration are either poorly scalable or not robust to large transformations and degradations.

Method: POLAR transposes registration into a pretrained autoencoder's latent space, uses a degradation-aware loss, and employs multistart optimization.

Result: POLAR significantly outperforms state-of-the-art methods on synthetic and real data.

Conclusion: POLAR offers a scalable, robust solution for multiview point cloud registration, with code and package availability.

Abstract: Point cloud rigid registration is a fundamental problem in 3D computer
vision. In the multiview case, we aim to find a set of 6D poses to align a set
of objects. Methods based on pairwise registration rely on a subsequent
synchronization algorithm, which makes them poorly scalable with the number of
views. Generative approaches overcome this limitation, but are based on
Gaussian Mixture Models and use an Expectation-Maximization algorithm. Hence,
they are not well suited to handle large transformations. Moreover, most
existing methods cannot handle high levels of degradations. In this paper, we
introduce POLAR (POint cloud LAtent Registration), a multiview registration
method able to efficiently deal with a large number of views, while being
robust to a high level of degradations and large initial angles. To achieve
this, we transpose the registration problem into the latent space of a
pretrained autoencoder, design a loss taking degradations into account, and
develop an efficient multistart optimization strategy. Our proposed method
significantly outperforms state-of-the-art approaches on synthetic and real
data. POLAR is available at github.com/pypolar/polar or as a standalone package
which can be installed with pip install polaregistration.

</details>


### [110] [Quaternion Nuclear Norms Over Frobenius Norms Minimization for Robust Matrix Completion](https://arxiv.org/abs/2504.21468)
*Yu Guo, Guoqing Chen, Tieyong Zeng, Qiyu Jin, Michael Kwok-Po Ng*

Main category: cs.CV

TL;DR: The paper introduces QNOF, a nonconvex approximation for quaternion matrix rank, simplifying it via quaternion SVD and extending it to robust matrix completion, outperforming existing methods.


<details>
  <summary>Details</summary>
Motivation: Addressing the challenge of recovering hidden structures from incomplete or noisy multi-dimensional data using quaternion matrices.

Method: Proposes QNOF, a parameter-free, scale-invariant approximation, and solves it via quaternion SVD and the singular value $L_1/L_2$ problem. Extends to robust matrix completion using ADMM.

Result: Numerical experiments show QNOF consistently outperforms state-of-the-art quaternion methods.

Conclusion: QNOF is a superior, efficient solution for quaternion matrix rank approximation and robust completion.

Abstract: Recovering hidden structures from incomplete or noisy data remains a
pervasive challenge across many fields, particularly where multi-dimensional
data representation is essential. Quaternion matrices, with their ability to
naturally model multi-dimensional data, offer a promising framework for this
problem. This paper introduces the quaternion nuclear norm over the Frobenius
norm (QNOF) as a novel nonconvex approximation for the rank of quaternion
matrices. QNOF is parameter-free and scale-invariant. Utilizing quaternion
singular value decomposition, we prove that solving the QNOF can be simplified
to solving the singular value $L_1/L_2$ problem. Additionally, we extend the
QNOF to robust quaternion matrix completion, employing the alternating
direction multiplier method to derive solutions that guarantee weak convergence
under mild conditions. Extensive numerical experiments validate the proposed
model's superiority, consistently outperforming state-of-the-art quaternion
methods.

</details>


### [111] [Robust Orthogonal NMF with Label Propagation for Image Clustering](https://arxiv.org/abs/2504.21472)
*Jingjing Liu, Nian Wu, Xianchao Xiu, Jianhua Zhang*

Main category: cs.CV

TL;DR: RONMF introduces a robust NMF framework with label propagation and orthogonal constraints to improve noise resistance and leverage supervised info.


<details>
  <summary>Details</summary>
Motivation: Existing NMF methods are noise-sensitive and lack effective use of supervised info in clustering.

Method: Proposes RONMF with graph Laplacian, label propagation, non-convex error measurement, and orthogonal constraints. Uses ADMM for optimization.

Result: RONMF outperforms state-of-the-art NMF methods in robustness and performance on eight datasets.

Conclusion: RONMF is a robust and efficient solution for image clustering with noise resistance and supervised info integration.

Abstract: Non-negative matrix factorization (NMF) is a popular unsupervised learning
approach widely used in image clustering. However, in real-world clustering
scenarios, most existing NMF methods are highly sensitive to noise corruption
and are unable to effectively leverage limited supervised information. To
overcome these drawbacks, we propose a unified non-convex framework with label
propagation called robust orthogonal nonnegative matrix factorization (RONMF).
This method not only considers the graph Laplacian and label propagation as
regularization terms but also introduces a more effective non-convex structure
to measure the reconstruction error and imposes orthogonal constraints on the
basis matrix to reduce the noise corruption, thereby achieving higher
robustness. To solve RONMF, we develop an alternating direction method of
multipliers (ADMM)-based optimization algorithm. In particular, all subproblems
have closed-form solutions, which ensures its efficiency. Experimental
evaluations on eight public image datasets demonstrate that the proposed RONMF
outperforms state-of-the-art NMF methods across various standard metrics and
shows excellent robustness. The code will be available at
https://github.com/slinda-liu.

</details>


### [112] [GarmentDiffusion: 3D Garment Sewing Pattern Generation with Multimodal Diffusion Transformers](https://arxiv.org/abs/2504.21476)
*Xinyu Li, Qi Yao, Yuanda Wang*

Main category: cs.CV

TL;DR: GarmentDiffusion is a generative model for creating precise 3D sewing patterns from multimodal inputs, improving efficiency and accuracy over existing methods.


<details>
  <summary>Details</summary>
Motivation: Existing approaches for sewing pattern generation are limited by input modality constraints or inefficiency, hindering diversified garment creation.

Method: The model encodes 3D sewing patterns into compact edge tokens and uses a diffusion transformer to denoise them efficiently, reducing sequence length and steps.

Result: GarmentDiffusion achieves a 100x speedup and state-of-the-art performance on DressCodeData and GarmentCodeData.

Conclusion: The method advances sewing pattern generation by combining multimodal inputs, efficiency, and precision, setting a new benchmark in the field.

Abstract: Garment sewing patterns are fundamental design elements that bridge the gap
between design concepts and practical manufacturing. The generative modeling of
sewing patterns is crucial for creating diversified garments. However, existing
approaches are limited either by reliance on a single input modality or by
suboptimal generation efficiency. In this work, we present
\textbf{\textit{GarmentDiffusion}}, a new generative model capable of producing
centimeter-precise, vectorized 3D sewing patterns from multimodal inputs (text,
image, and incomplete sewing pattern). Our method efficiently encodes 3D sewing
pattern parameters into compact edge token representations, achieving a
sequence length that is $\textbf{10}\times$ shorter than that of the
autoregressive SewingGPT in DressCode. By employing a diffusion transformer, we
simultaneously denoise all edge tokens along the temporal axis, while
maintaining a constant number of denoising steps regardless of dataset-specific
edge and panel statistics. With all combination of designs of our model, the
sewing pattern generation speed is accelerated by $\textbf{100}\times$ compared
to SewingGPT. We achieve new state-of-the-art results on DressCodeData, as well
as on the largest sewing pattern dataset, namely GarmentCodeData. The project
website is available at https://shenfu-research.github.io/Garment-Diffusion/.

</details>


### [113] [CAE-DFKD: Bridging the Transferability Gap in Data-Free Knowledge Distillation](https://arxiv.org/abs/2504.21478)
*Zherui Zhang, Changwei Wang, Rongtao Xu, Wenhao Xu, Shibiao Xu, Yu Zhang, Li Guo*

Main category: cs.CV

TL;DR: CAE-DFKD improves DFKD by focusing on embedding-level transferability, outperforming image-level methods in efficiency, performance, and downstream task adaptability.


<details>
  <summary>Details</summary>
Motivation: Existing DFKD methods neglect transferability of learned representations, limiting generalization.

Method: Proposes Category-Aware Embedding DFKD (CAE-DFKD), focusing on embedding-level improvements.

Result: Shows efficiency gains, competitive image recognition performance, and strong transferability in downstream tasks.

Conclusion: CAE-DFKD addresses limitations of prior DFKD methods, enhancing generalization and practical applicability.

Abstract: Data-Free Knowledge Distillation (DFKD) enables the knowledge transfer from
the given pre-trained teacher network to the target student model without
access to the real training data. Existing DFKD methods focus primarily on
improving image recognition performance on associated datasets, often
neglecting the crucial aspect of the transferability of learned
representations. In this paper, we propose Category-Aware Embedding Data-Free
Knowledge Distillation (CAE-DFKD), which addresses at the embedding level the
limitations of previous rely on image-level methods to improve model
generalization but fail when directly applied to DFKD. The superiority and
flexibility of CAE-DFKD are extensively evaluated, including:
\textit{\textbf{i.)}} Significant efficiency advantages resulting from altering
the generator training paradigm; \textit{\textbf{ii.)}} Competitive performance
with existing DFKD state-of-the-art methods on image recognition tasks;
\textit{\textbf{iii.)}} Remarkable transferability of data-free learned
representations demonstrated in downstream tasks.

</details>


### [114] [DGSolver: Diffusion Generalist Solver with Universal Posterior Sampling for Image Restoration](https://arxiv.org/abs/2504.21487)
*Hebaixu Wang, Jing Zhang, Haonan Guo, Di Wang, Jiayi Ma, Bo Du*

Main category: cs.CV

TL;DR: DGSolver is a diffusion generalist solver for universal image restoration, improving accuracy and efficiency with high-order solvers and universal posterior sampling.


<details>
  <summary>Details</summary>
Motivation: Existing methods face issues with cumulative errors from reduced sampling steps and struggle to balance degradation representations and restoration quality.

Method: Derives exact ODEs for diffusion models, uses high-order solvers with queue-based sampling, and integrates universal posterior sampling for better gradient approximation.

Result: Outperforms state-of-the-art methods in restoration accuracy, stability, and scalability.

Conclusion: DGSolver effectively addresses challenges in diffusion-based image restoration, offering superior performance.

Abstract: Diffusion models have achieved remarkable progress in universal image
restoration. While existing methods speed up inference by reducing sampling
steps, substantial step intervals often introduce cumulative errors. Moreover,
they struggle to balance the commonality of degradation representations and
restoration quality. To address these challenges, we introduce
\textbf{DGSolver}, a diffusion generalist solver with universal posterior
sampling. We first derive the exact ordinary differential equations for
generalist diffusion models and tailor high-order solvers with a queue-based
accelerated sampling strategy to improve both accuracy and efficiency. We then
integrate universal posterior sampling to better approximate
manifold-constrained gradients, yielding a more accurate noise estimation and
correcting errors in inverse inference. Extensive experiments show that
DGSolver outperforms state-of-the-art methods in restoration accuracy,
stability, and scalability, both qualitatively and quantitatively. Code and
models will be available at https://github.com/MiliLab/DGSolver.

</details>


### [115] [MagicPortrait: Temporally Consistent Face Reenactment with 3D Geometric Guidance](https://arxiv.org/abs/2504.21497)
*Mengting Wei, Yante Li, Tuomas Varanka, Yan Jiang, Licai Sun, Guoying Zhao*

Main category: cs.CV

TL;DR: A method for video face reenactment using a 3D face parametric model (FLAME) integrated into a latent diffusion framework improves shape consistency and motion control.


<details>
  <summary>Details</summary>
Motivation: Existing video-based face generation approaches lack shape consistency and motion control, which this method aims to address.

Method: The FLAME model provides detailed face geometry and motion features. The latent diffusion model is enhanced with 3D expression and pose information (depth, normal, and rendering maps). A multi-layer fusion module with self-attention combines identity and motion features.

Result: The method generates high-quality face animations with precise expression and pose modeling, showing strong generalization on out-of-domain images.

Conclusion: The proposed framework effectively integrates 3D face modeling with latent diffusion, improving video face reenactment quality and control.

Abstract: In this paper, we propose a method for video face reenactment that integrates
a 3D face parametric model into a latent diffusion framework, aiming to improve
shape consistency and motion control in existing video-based face generation
approaches. Our approach employs the FLAME (Faces Learned with an Articulated
Model and Expressions) model as the 3D face parametric representation,
providing a unified framework for modeling face expressions and head pose. This
enables precise extraction of detailed face geometry and motion features from
driving videos. Specifically, we enhance the latent diffusion model with rich
3D expression and detailed pose information by incorporating depth maps, normal
maps, and rendering maps derived from FLAME sequences. A multi-layer face
movements fusion module with integrated self-attention mechanisms is used to
combine identity and motion latent features within the spatial domain. By
utilizing the 3D face parametric model as motion guidance, our method enables
parametric alignment of face identity between the reference image and the
motion captured from the driving video. Experimental results on benchmark
datasets show that our method excels at generating high-quality face animations
with precise expression and head pose variation modeling. In addition, it
demonstrates strong generalization performance on out-of-domain images. Code is
publicly available at https://github.com/weimengting/MagicPortrait.

</details>


### [116] [SAM4EM: Efficient memory-based two stage prompt-free segment anything model adapter for complex 3D neuroscience electron microscopy stacks](https://arxiv.org/abs/2504.21544)
*Uzair Shah, Marco Agus, Daniya Boges, Vanessa Chiappini, Mahmood Alzubaidi, Jens Schneider, Markus Hadwiger, Pierre J. Magistretti, Mowafa Househ, Corrado Calı*

Main category: cs.CV

TL;DR: SAM4EM introduces a novel 3D segmentation method for EM data using SAM with prompt-free adapters, dual-stage fine-tuning, and 3D memory attention, achieving SOTA results on neuroscience benchmarks.


<details>
  <summary>Details</summary>
Motivation: To improve 3D segmentation of complex neural structures in EM data, addressing challenges like limited annotated data and segmentation consistency.

Method: Uses SAM with a prompt-free adapter, dual-stage LoRA fine-tuning, and 3D memory attention for consistency.

Result: Outperforms SOTA methods in segmenting mitochondria, glia, and synapses, with significant accuracy improvements.

Conclusion: SAM4EM is effective for complex neural segmentation, offering a robust solution with released code and models.

Abstract: We present SAM4EM, a novel approach for 3D segmentation of complex neural
structures in electron microscopy (EM) data by leveraging the Segment Anything
Model (SAM) alongside advanced fine-tuning strategies. Our contributions
include the development of a prompt-free adapter for SAM using two stage mask
decoding to automatically generate prompt embeddings, a dual-stage fine-tuning
method based on Low-Rank Adaptation (LoRA) for enhancing segmentation with
limited annotated data, and a 3D memory attention mechanism to ensure
segmentation consistency across 3D stacks. We further release a unique
benchmark dataset for the segmentation of astrocytic processes and synapses. We
evaluated our method on challenging neuroscience segmentation benchmarks,
specifically targeting mitochondria, glia, and synapses, with significant
accuracy improvements over state-of-the-art (SOTA) methods, including recent
SAM-based adapters developed for the medical domain and other vision
transformer-based approaches. Experimental results indicate that our approach
outperforms existing solutions in the segmentation of complex processes like
glia and post-synaptic densities. Our code and models are available at
https://github.com/Uzshah/SAM4EM.

</details>


### [117] [Black-Box Visual Prompt Engineering for Mitigating Object Hallucination in Large Vision Language Models](https://arxiv.org/abs/2504.21559)
*Sangmin Woo, Kang Zhou, Yun Zhou, Shuai Wang, Sheng Guan, Haibo Ding, Lin Lee Cheong*

Main category: cs.CV

TL;DR: BBVPE is a framework to reduce object hallucination in LVLMs by selecting optimal visual prompts without accessing model internals.


<details>
  <summary>Details</summary>
Motivation: LVLMs suffer from object hallucination, undermining reliability. Visual prompting helps, but effectiveness varies.

Method: Proposes BBVPE: a black-box framework using a router model to dynamically select the best visual prompt for each image.

Result: BBVPE effectively reduces object hallucination, as shown on benchmarks like POPE and CHAIR.

Conclusion: BBVPE is a model-agnostic solution to enhance LVLM reliability by mitigating hallucination.

Abstract: Large Vision Language Models (LVLMs) often suffer from object hallucination,
which undermines their reliability. Surprisingly, we find that simple
object-based visual prompting -- overlaying visual cues (e.g., bounding box,
circle) on images -- can significantly mitigate such hallucination; however,
different visual prompts (VPs) vary in effectiveness. To address this, we
propose Black-Box Visual Prompt Engineering (BBVPE), a framework to identify
optimal VPs that enhance LVLM responses without needing access to model
internals. Our approach employs a pool of candidate VPs and trains a router
model to dynamically select the most effective VP for a given input image. This
black-box approach is model-agnostic, making it applicable to both open-source
and proprietary LVLMs. Evaluations on benchmarks such as POPE and CHAIR
demonstrate that BBVPE effectively reduces object hallucination.

</details>


### [118] [Iterative Trajectory Exploration for Multimodal Agents](https://arxiv.org/abs/2504.21561)
*Pengxiang Li, Zhi Gao, Bofei Zhang, Yapeng Mi, Xiaojian Ma, Chenrui Shi, Tao Yuan, Yuwei Wu, Yunde Jia, Song-Chun Zhu, Qing Li*

Main category: cs.CV

TL;DR: SPORT is an online self-exploration method for multimodal agents that refines trajectories via step-wise preference optimization, eliminating the need for expert data.


<details>
  <summary>Details</summary>
Motivation: Existing multimodal agents require extensive expert data for fine-tuning in new environments, which is inefficient. SPORT aims to automate task generation and learning without expert annotation.

Method: SPORT involves four iterative steps: task synthesis (using language models), step sampling, step verification (with AI feedback), and preference tuning to update the controller's policy.

Result: SPORT improves performance by 6.41% in GTA and 3.64% in GAIA benchmarks, demonstrating generalization and effectiveness.

Conclusion: SPORT successfully automates agent refinement through self-exploration, outperforming traditional methods reliant on expert data.

Abstract: Multimodal agents, which integrate a controller (e.g., a large language
model) with external tools, have demonstrated remarkable capabilities in
tackling complex tasks. However, existing agents need to collect a large number
of expert data for fine-tuning to adapt to new environments. In this paper, we
propose an online self-exploration method for multimodal agents, namely SPORT,
via step-wise preference optimization to refine the trajectories of agents,
which automatically generates tasks and learns from solving the generated
tasks, without any expert annotation. SPORT operates through four iterative
components: task synthesis, step sampling, step verification, and preference
tuning. First, we synthesize multi-modal tasks using language models. Then, we
introduce a novel search scheme, where step sampling and step verification are
executed alternately to solve each generated task. We employ a verifier to
provide AI feedback to construct step-wise preference data. The data is
subsequently used to update the controller's policy through preference tuning,
producing a SPORT Agent. By interacting with real environments, the SPORT Agent
evolves into a more refined and capable system. Evaluation in the GTA and GAIA
benchmarks show that the SPORT Agent achieves 6.41\% and 3.64\% improvements,
underscoring the generalization and effectiveness introduced by our method. The
project page is https://SPORT-Agents.github.io.

</details>


### [119] [eNCApsulate: NCA for Precision Diagnosis on Capsule Endoscopes](https://arxiv.org/abs/2504.21562)
*Henry John Krumb, Anirban Mukhopadhyay*

Main category: cs.CV

TL;DR: The paper presents a method using Neural Cellular Automata (NCA) for bleeding segmentation and depth estimation in wireless capsule endoscopy, enabling efficient processing on microcontrollers like ESP32.


<details>
  <summary>Details</summary>
Motivation: Wireless capsule endoscopy generates extensive video data requiring long review times, and localizing the capsule is challenging. Deep learning models are typically too large for direct use on the capsule.

Method: NCA models are trained on endoscopic images, with a large foundation model distilled into the NCA architecture for depth estimation. The trained NCA is ported to the ESP32 microcontroller.

Result: NCA outperforms other portable models in accuracy (Dice) with 100x fewer parameters. Depth estimation results are visually convincing, sometimes surpassing pseudo ground truth. Runtime optimizations on ESP32-S3 improve inference speed by over 3x.

Conclusion: The work successfully enables reliable bleeding segmentation and depth estimation on miniaturized devices, advancing precise diagnosis and capsule localization.

Abstract: Wireless Capsule Endoscopy is a non-invasive imaging method for the entire
gastrointestinal tract, and is a pain-free alternative to traditional
endoscopy. It generates extensive video data that requires significant review
time, and localizing the capsule after ingestion is a challenge. Techniques
like bleeding detection and depth estimation can help with localization of
pathologies, but deep learning models are typically too large to run directly
on the capsule. Neural Cellular Automata (NCA) for bleeding segmentation and
depth estimation are trained on capsule endoscopic images. For monocular depth
estimation, we distill a large foundation model into the lean NCA architecture,
by treating the outputs of the foundation model as pseudo ground truth. We then
port the trained NCA to the ESP32 microcontroller, enabling efficient image
processing on hardware as small as a camera capsule. NCA are more accurate
(Dice) than other portable segmentation models, while requiring more than 100x
fewer parameters stored in memory than other small-scale models. The visual
results of NCA depth estimation look convincing, and in some cases beat the
realism and detail of the pseudo ground truth. Runtime optimizations on the
ESP32-S3 accelerate the average inference speed significantly, by more than
factor 3. With several algorithmic adjustments and distillation, it is possible
to eNCApsulate NCA models into microcontrollers that fit into wireless capsule
endoscopes. This is the first work that enables reliable bleeding segmentation
and depth estimation on a miniaturized device, paving the way for precise
diagnosis combined with visual odometry as a means of precise localization of
the capsule -- on the capsule.

</details>


### [120] [Cascade Detector Analysis and Application to Biomedical Microscopy](https://arxiv.org/abs/2504.21598)
*Thomas L. Athey, Shashata Sawmya, Nir Shavit*

Main category: cs.CV

TL;DR: The paper proposes using cascade detectors for efficient sparse object detection in multiresolution images, reducing computation time by 30-75% while maintaining performance.


<details>
  <summary>Details</summary>
Motivation: The need for efficient inference algorithms grows with larger computer vision models and biomedical datasets.

Method: Utilizes cascade detectors with known accuracies at different resolutions to derive accuracy and expected classifier calls.

Result: Multi-level detectors achieve comparable performance with 30-75% less time in tasks like cell detection and segmentation.

Conclusion: The approach is versatile, compatible with various models and domains, and generalizes across dimensions and cascade levels.

Abstract: As both computer vision models and biomedical datasets grow in size, there is
an increasing need for efficient inference algorithms. We utilize cascade
detectors to efficiently identify sparse objects in multiresolution images.
Given an object's prevalence and a set of detectors at different resolutions
with known accuracies, we derive the accuracy, and expected number of
classifier calls by a cascade detector. These results generalize across number
of dimensions and number of cascade levels. Finally, we compare one- and
two-level detectors in fluorescent cell detection, organelle segmentation, and
tissue segmentation across various microscopy modalities. We show that the
multi-level detector achieves comparable performance in 30-75% less time. Our
work is compatible with a variety of computer vision models and data domains.

</details>


### [121] [Mcity Data Engine: Iterative Model Improvement Through Open-Vocabulary Data Selection](https://arxiv.org/abs/2504.21614)
*Daniel Bogdoll, Rajanikant Patnaik Ananta, Abeyankar Giridharan, Isabel Moore, Gregory Stevens, Henry X. Liu*

Main category: cs.CV

TL;DR: The Mcity Data Engine is an open-source system for selecting and labeling rare classes in large datasets, particularly for ITS, addressing the lack of such tools for researchers.


<details>
  <summary>Details</summary>
Motivation: The challenge of detecting long-tail classes in unlabeled ITS data and the absence of open-source tools for iterative data selection and model training.

Method: Develops the Mcity Data Engine with modules for the full data-based development cycle, emphasizing open-vocabulary data selection for rare classes.

Result: A publicly available system (GitHub, MIT license) supporting data acquisition to model deployment, focusing on rare and novel classes.

Conclusion: The Mcity Data Engine fills a gap for researchers by providing an open-source solution for handling rare classes in large datasets.

Abstract: With an ever-increasing availability of data, it has become more and more
challenging to select and label appropriate samples for the training of machine
learning models. It is especially difficult to detect long-tail classes of
interest in large amounts of unlabeled data. This holds especially true for
Intelligent Transportation Systems (ITS), where vehicle fleets and roadside
perception systems generate an abundance of raw data. While industrial,
proprietary data engines for such iterative data selection and model training
processes exist, researchers and the open-source community suffer from a lack
of an openly available system. We present the Mcity Data Engine, which provides
modules for the complete data-based development cycle, beginning at the data
acquisition phase and ending at the model deployment stage. The Mcity Data
Engine focuses on rare and novel classes through an open-vocabulary data
selection process. All code is publicly available on GitHub under an MIT
license: https://github.com/mcity/mcity_data_engine

</details>


### [122] [Diffusion-based Adversarial Identity Manipulation for Facial Privacy Protection](https://arxiv.org/abs/2504.21646)
*Liqin Wang, Qianyue Hu, Wei Lu, Xiangyang Luo*

Main category: cs.CV

TL;DR: DiffAIM uses diffusion models to create natural adversarial faces for privacy protection against FR systems, outperforming existing methods in transferability and visual quality.


<details>
  <summary>Details</summary>
Motivation: Address privacy concerns in face recognition by generating natural adversarial faces that evade malicious FR systems.

Method: Manipulate facial identity in a diffusion model's latent space using gradient-based adversarial guidance during reverse diffusion, with structure-preserving regularization.

Result: Achieves stronger black-box attack transferability and superior visual quality, effective against commercial FR APIs like Face++ and Aliyun.

Conclusion: DiffAIM is a robust solution for enhancing facial privacy by generating adversarial faces that balance impersonation and naturalness.

Abstract: The success of face recognition (FR) systems has led to serious privacy
concerns due to potential unauthorized surveillance and user tracking on social
networks. Existing methods for enhancing privacy fail to generate natural face
images that can protect facial privacy. In this paper, we propose
diffusion-based adversarial identity manipulation (DiffAIM) to generate natural
and highly transferable adversarial faces against malicious FR systems. To be
specific, we manipulate facial identity within the low-dimensional latent space
of a diffusion model. This involves iteratively injecting gradient-based
adversarial identity guidance during the reverse diffusion process,
progressively steering the generation toward the desired adversarial faces. The
guidance is optimized for identity convergence towards a target while promoting
semantic divergence from the source, facilitating effective impersonation while
maintaining visual naturalness. We further incorporate structure-preserving
regularization to preserve facial structure consistency during manipulation.
Extensive experiments on both face verification and identification tasks
demonstrate that compared with the state-of-the-art, DiffAIM achieves stronger
black-box attack transferability while maintaining superior visual quality. We
also demonstrate the effectiveness of the proposed approach for commercial FR
APIs, including Face++ and Aliyun.

</details>


### [123] [HoloTime: Taming Video Diffusion Models for Panoramic 4D Scene Generation](https://arxiv.org/abs/2504.21650)
*Haiyang Zhou, Wangbo Yu, Jiawen Guan, Xinhua Cheng, Yonghong Tian, Li Yuan*

Main category: cs.CV

TL;DR: HoloTime integrates video diffusion models to generate panoramic videos and 4D scenes from prompts or images, enhancing VR/AR immersion.


<details>
  <summary>Details</summary>
Motivation: Existing diffusion models focus on static 3D or object-level dynamics, limiting immersive experiences. HoloTime aims to bridge this gap.

Method: Uses the 360World dataset and a two-stage Panoramic Animator for video generation, followed by Panoramic Space-Time Reconstruction for 4D scene creation.

Result: Outperforms existing methods in panoramic video generation and 4D scene reconstruction, offering more realistic VR/AR experiences.

Conclusion: HoloTime successfully enhances immersive experiences by generating high-quality 4D assets, advancing VR/AR applications.

Abstract: The rapid advancement of diffusion models holds the promise of
revolutionizing the application of VR and AR technologies, which typically
require scene-level 4D assets for user experience. Nonetheless, existing
diffusion models predominantly concentrate on modeling static 3D scenes or
object-level dynamics, constraining their capacity to provide truly immersive
experiences. To address this issue, we propose HoloTime, a framework that
integrates video diffusion models to generate panoramic videos from a single
prompt or reference image, along with a 360-degree 4D scene reconstruction
method that seamlessly transforms the generated panoramic video into 4D assets,
enabling a fully immersive 4D experience for users. Specifically, to tame video
diffusion models for generating high-fidelity panoramic videos, we introduce
the 360World dataset, the first comprehensive collection of panoramic videos
suitable for downstream 4D scene reconstruction tasks. With this curated
dataset, we propose Panoramic Animator, a two-stage image-to-video diffusion
model that can convert panoramic images into high-quality panoramic videos.
Following this, we present Panoramic Space-Time Reconstruction, which leverages
a space-time depth estimation method to transform the generated panoramic
videos into 4D point clouds, enabling the optimization of a holistic 4D
Gaussian Splatting representation to reconstruct spatially and temporally
consistent 4D scenes. To validate the efficacy of our method, we conducted a
comparative analysis with existing approaches, revealing its superiority in
both panoramic video generation and 4D scene reconstruction. This demonstrates
our method's capability to create more engaging and realistic immersive
environments, thereby enhancing user experiences in VR and AR applications.

</details>


### [124] [Visual Text Processing: A Comprehensive Review and Unified Evaluation](https://arxiv.org/abs/2504.21682)
*Yan Shu, Weichao Zeng, Fangmin Zhao, Zeyu Chen, Zhenhang Li, Xiaomeng Yang, Yu Zhou, Paolo Rota, Xiang Bai, Lianwen Jin, Xu-Cheng Yin, Nicu Sebe*

Main category: cs.CV

TL;DR: A survey on visual text processing highlights advancements, challenges, and introduces a new benchmark (VTPBench) and metric (VTPScore) for evaluation.


<details>
  <summary>Details</summary>
Motivation: Address the unique properties of text in images and improve visual text processing by identifying suitable textual features and integrating them effectively.

Method: Comprehensive analysis of recent advancements, introduction of VTPBench benchmark, and VTPScore metric using MLLMs for evaluation.

Result: Empirical study shows current techniques have significant room for improvement.

Conclusion: The survey aims to be a foundational resource for future innovation in visual text processing.

Abstract: Visual text is a crucial component in both document and scene images,
conveying rich semantic information and attracting significant attention in the
computer vision community. Beyond traditional tasks such as text detection and
recognition, visual text processing has witnessed rapid advancements driven by
the emergence of foundation models, including text image reconstruction and
text image manipulation. Despite significant progress, challenges remain due to
the unique properties that differentiate text from general objects. Effectively
capturing and leveraging these distinct textual characteristics is essential
for developing robust visual text processing models. In this survey, we present
a comprehensive, multi-perspective analysis of recent advancements in visual
text processing, focusing on two key questions: (1) What textual features are
most suitable for different visual text processing tasks? (2) How can these
distinctive text features be effectively incorporated into processing
frameworks? Furthermore, we introduce VTPBench, a new benchmark that
encompasses a broad range of visual text processing datasets. Leveraging the
advanced visual quality assessment capabilities of multimodal large language
models (MLLMs), we propose VTPScore, a novel evaluation metric designed to
ensure fair and reliable evaluation. Our empirical study with more than 20
specific models reveals substantial room for improvement in the current
techniques. Our aim is to establish this work as a fundamental resource that
fosters future exploration and innovation in the dynamic field of visual text
processing. The relevant repository is available at
https://github.com/shuyansy/Visual-Text-Processing-survey.

</details>


### [125] [Enhancing Self-Supervised Fine-Grained Video Object Tracking with Dynamic Memory Prediction](https://arxiv.org/abs/2504.21692)
*Zihan Zhou, Changrui Dai, Aibo Song, Xiaolin Fang*

Main category: cs.CV

TL;DR: The paper introduces a Dynamic Memory Prediction (DMP) framework for video frame reconstruction, leveraging multiple reference frames to improve accuracy in complex scenarios like occlusion or fast movement.


<details>
  <summary>Details</summary>
Motivation: Existing frame reconstruction methods neglect the direct use of multiple reference frames, limiting performance in complex situations.

Method: The DMP framework includes a Reference Frame Memory Engine for dynamic frame selection and a Bidirectional Target Prediction Network for robustness.

Result: The algorithm outperforms state-of-the-art self-supervised techniques in object segmentation and keypoint tracking tasks.

Conclusion: The DMP framework effectively enhances frame reconstruction by utilizing multiple reference frames, proving superior in complex video analysis tasks.

Abstract: Successful video analysis relies on accurate recognition of pixels across
frames, and frame reconstruction methods based on video correspondence learning
are popular due to their efficiency. Existing frame reconstruction methods,
while efficient, neglect the value of direct involvement of multiple reference
frames for reconstruction and decision-making aspects, especially in complex
situations such as occlusion or fast movement. In this paper, we introduce a
Dynamic Memory Prediction (DMP) framework that innovatively utilizes multiple
reference frames to concisely and directly enhance frame reconstruction. Its
core component is a Reference Frame Memory Engine that dynamically selects
frames based on object pixel features to improve tracking accuracy. In
addition, a Bidirectional Target Prediction Network is built to utilize
multiple reference frames to improve the robustness of the model. Through
experiments, our algorithm outperforms the state-of-the-art self-supervised
techniques on two fine-grained video object tracking tasks: object segmentation
and keypoint tracking.

</details>


### [126] [REHEARSE-3D: A Multi-modal Emulated Rain Dataset for 3D Point Cloud De-raining](https://arxiv.org/abs/2504.21699)
*Abu Mohammed Raisuddin, Jesper Holmblad, Hamed Haghighi, Yuri Poledna, Maikol Funk Drechsler, Valentina Donzella, Eren Erdal Aksoy*

Main category: cs.CV

TL;DR: A new dataset, REHEARSE-3D, is introduced to address LiDAR degradation in rain, featuring large-scale, multi-modal data with rain-characteristic annotations for de-raining research.


<details>
  <summary>Details</summary>
Motivation: Sensor degradation in autonomous driving due to rain interference necessitates weather-aware systems, prompting the creation of a comprehensive dataset.

Method: The study releases REHEARSE-3D, a dataset with high-resolution LiDAR and 4D Radar point clouds, annotated for rain characteristics, and benchmarks de-raining models.

Result: The dataset is the largest of its kind, enabling raindrop detection and removal in fused LiDAR and Radar data, with performance evaluated across statistical and deep-learning models.

Conclusion: REHEARSE-3D advances 3D point cloud de-raining research, offering valuable resources for sensor noise modeling and weather impact analysis.

Abstract: Sensor degradation poses a significant challenge in autonomous driving.
During heavy rainfall, the interference from raindrops can adversely affect the
quality of LiDAR point clouds, resulting in, for instance, inaccurate point
measurements. This, in turn, can potentially lead to safety concerns if
autonomous driving systems are not weather-aware, i.e., if they are unable to
discern such changes. In this study, we release a new, large-scale, multi-modal
emulated rain dataset, REHEARSE-3D, to promote research advancements in 3D
point cloud de-raining. Distinct from the most relevant competitors, our
dataset is unique in several respects. First, it is the largest point-wise
annotated dataset, and second, it is the only one with high-resolution LiDAR
data (LiDAR-256) enriched with 4D Radar point clouds logged in both daytime and
nighttime conditions in a controlled weather environment. Furthermore,
REHEARSE-3D involves rain-characteristic information, which is of significant
value not only for sensor noise modeling but also for analyzing the impact of
weather at a point level. Leveraging REHEARSE-3D, we benchmark raindrop
detection and removal in fused LiDAR and 4D Radar point clouds. Our
comprehensive study further evaluates the performance of various statistical
and deep-learning models. Upon publication, the dataset and benchmark models
will be made publicly available at: https://sporsho.github.io/REHEARSE3D.

</details>


### [127] [Vision Transformers in Precision Agriculture: A Comprehensive Survey](https://arxiv.org/abs/2504.21706)
*Saber Mehdipour, Seyed Abolghasem Mirroshandel, Seyed Amirhossein Tabatabaei*

Main category: cs.CV

TL;DR: A survey on Vision Transformers (ViTs) in precision agriculture, comparing them to CNNs, discussing challenges, and outlining future research directions.


<details>
  <summary>Details</summary>
Motivation: To address limitations of traditional methods in plant disease detection and explore ViTs' potential in agriculture.

Method: Review of ViTs' architecture, transition from NLP to vision, literature analysis, and comparative study with CNNs.

Result: ViTs offer scalability and accuracy improvements over CNNs but face challenges like data and computational demands.

Conclusion: ViTs have transformative potential in precision agriculture, with future research needed for real-world integration.

Abstract: Detecting plant diseases is a crucial aspect of modern agriculture - it plays
a key role in maintaining crop health and increasing overall yield. Traditional
approaches, though still valuable, often rely on manual inspection or
conventional machine learning techniques, both of which face limitations in
scalability and accuracy. Recently, Vision Transformers (ViTs) have emerged as
a promising alternative, offering benefits such as improved handling of
long-range dependencies and better scalability for visual tasks. This survey
explores the application of ViTs in precision agriculture, covering tasks from
classification to detection and segmentation. We begin by introducing the
foundational architecture of ViTs and discuss their transition from Natural
Language Processing (NLP) to computer vision. The discussion includes the
concept of inductive bias in traditional models like Convolutional Neural
Networks (CNNs), and how ViTs mitigate these biases. We provide a comprehensive
review of recent literature, focusing on key methodologies, datasets, and
performance metrics. The survey also includes a comparative analysis of CNNs
and ViTs, with a look at hybrid models and performance enhancements. Technical
challenges - such as data requirements, computational demands, and model
interpretability - are addressed alongside potential solutions. Finally, we
outline potential research directions and technological advancements that could
further support the integration of ViTs in real-world agricultural settings.
Our goal with this study is to offer practitioners and researchers a deeper
understanding of how ViTs are poised to transform smart and precision
agriculture.

</details>


### [128] [VividListener: Expressive and Controllable Listener Dynamics Modeling for Multi-Modal Responsive Interaction](https://arxiv.org/abs/2504.21718)
*Shiying Li, Xingqun Qi, Bingkun Yang, Chen Weile, Zezhao Tian, Muyi Sun, Qifeng Liu, Man Zhang, Zhenan Sun*

Main category: cs.CV

TL;DR: VividListener, a novel framework, addresses fine-grained, expressive listener dynamics in dialogue modeling by leveraging multi-modal conditions and a new dataset, ListenerX.


<details>
  <summary>Details</summary>
Motivation: Existing methods lack fine-grained control over listener behavior and emotional intensity, and there's a scarcity of large-scale, annotated datasets for multi-modal dialogue modeling.

Method: Proposes VividListener with a Responsive Interaction Module (RIM) for multi-modal embeddings and Emotional Intensity Tags (EIT) for emotion intensity editing.

Result: VividListener achieves state-of-the-art performance on the ListenerX dataset, enabling expressive and controllable listener dynamics.

Conclusion: The framework and dataset advance dialogue modeling by improving fine-grained control and expressive reactions in listener behavior.

Abstract: Generating responsive listener head dynamics with nuanced emotions and
expressive reactions is crucial for practical dialogue modeling in various
virtual avatar animations. Previous studies mainly focus on the direct
short-term production of listener behavior. They overlook the fine-grained
control over motion variations and emotional intensity, especially in
long-sequence modeling. Moreover, the lack of long-term and large-scale paired
speaker-listener corpora including head dynamics and fine-grained
multi-modality annotations (e.g., text-based expression descriptions, emotional
intensity) also limits the application of dialogue modeling.Therefore, we first
newly collect a large-scale multi-turn dataset of 3D dyadic conversation
containing more than 1.4M valid frames for multi-modal responsive interaction,
dubbed ListenerX. Additionally, we propose VividListener, a novel framework
enabling fine-grained, expressive and controllable listener dynamics modeling.
This framework leverages multi-modal conditions as guiding principles for
fostering coherent interactions between speakers and listeners.Specifically, we
design the Responsive Interaction Module (RIM) to adaptively represent the
multi-modal interactive embeddings. RIM ensures the listener dynamics achieve
fine-grained semantic coordination with textual descriptions and adjustments,
while preserving expressive reaction with speaker behavior. Meanwhile, we
design the Emotional Intensity Tags (EIT) for emotion intensity editing with
multi-modal information integration, applying to both text descriptions and
listener motion amplitude.Extensive experiments conducted on our newly
collected ListenerX dataset demonstrate that VividListener achieves
state-of-the-art performance, realizing expressive and controllable listener
dynamics.

</details>


### [129] [Common3D: Self-Supervised Learning of 3D Morphable Models for Common Objects in Neural Feature Space](https://arxiv.org/abs/2504.21749)
*Leonhard Sommer, Olaf Dünkel, Christian Theobalt, Adam Kortylewski*

Main category: cs.CV

TL;DR: Common3D introduces a self-supervised method to learn 3D morphable models (3DMMs) for common objects from videos, using neural features and contrastive learning for improved performance.


<details>
  <summary>Details</summary>
Motivation: 3DMMs are limited to few categories due to demanding data needs. Common3D aims to generalize 3DMMs for common objects without supervision.

Method: Uses a 3D template mesh and deformation field, parameterized by a neural network, with neural features for appearance. Trains via contrastive learning.

Result: Achieves higher quality correspondence features and better performance in 3D pose and semantic correspondence tasks.

Conclusion: Common3D is the first fully self-supervised method for zero-shot vision tasks, generalizing 3DMMs beyond specialized categories.

Abstract: 3D morphable models (3DMMs) are a powerful tool to represent the possible
shapes and appearances of an object category. Given a single test image, 3DMMs
can be used to solve various tasks, such as predicting the 3D shape, pose,
semantic correspondence, and instance segmentation of an object. Unfortunately,
3DMMs are only available for very few object categories that are of particular
interest, like faces or human bodies, as they require a demanding 3D data
acquisition and category-specific training process. In contrast, we introduce a
new method, Common3D, that learns 3DMMs of common objects in a fully
self-supervised manner from a collection of object-centric videos. For this
purpose, our model represents objects as a learned 3D template mesh and a
deformation field that is parameterized as an image-conditioned neural network.
Different from prior works, Common3D represents the object appearance with
neural features instead of RGB colors, which enables the learning of more
generalizable representations through an abstraction from pixel intensities.
Importantly, we train the appearance features using a contrastive objective by
exploiting the correspondences defined through the deformable template mesh.
This leads to higher quality correspondence features compared to related works
and a significantly improved model performance at estimating 3D object pose and
semantic correspondence. Common3D is the first completely self-supervised
method that can solve various vision tasks in a zero-shot manner.

</details>


### [130] [Anatomical Similarity as a New Metric to Evaluate Brain Generative Models](https://arxiv.org/abs/2504.21771)
*Bahram Jafrasteh, Wei Peng, Cheng Wan, Yimin Luo, Ehsan Adeli, Qingyu Zhao*

Main category: cs.CV

TL;DR: The paper introduces WASABI, a new metric to evaluate anatomical realism in synthetic brain MRIs, outperforming traditional metrics.


<details>
  <summary>Details</summary>
Motivation: Current evaluations of synthetic MRIs focus on texture and perception, lacking sensitivity to anatomical fidelity, which is crucial for clinical relevance.

Method: WASABI uses SynthSeg for brain parcellation and the Wasserstein distance to compare volumetric measures of brain regions between real and synthetic MRIs.

Result: WASABI shows higher sensitivity in detecting anatomical discrepancies than traditional metrics, even with visually realistic synthetic images.

Conclusion: The study advocates prioritizing anatomical fidelity in synthetic MRI evaluations, proposing WASABI as a benchmark for clinically meaningful synthesis.

Abstract: Generative models enhance neuroimaging through data augmentation, quality
improvement, and rare condition studies. Despite advances in realistic
synthetic MRIs, evaluations focus on texture and perception, lacking
sensitivity to crucial anatomical fidelity. This study proposes a new metric,
called WASABI (Wasserstein-Based Anatomical Brain Index), to assess the
anatomical realism of synthetic brain MRIs. WASABI leverages \textit{SynthSeg},
a deep learning-based brain parcellation tool, to derive volumetric measures of
brain regions in each MRI and uses the multivariate Wasserstein distance to
compare distributions between real and synthetic anatomies. Based on controlled
experiments on two real datasets and synthetic MRIs from five generative
models, WASABI demonstrates higher sensitivity in quantifying anatomical
discrepancies compared to traditional image-level metrics, even when synthetic
images achieve near-perfect visual quality. Our findings advocate for shifting
the evaluation paradigm beyond visual inspection and conventional metrics,
emphasizing anatomical fidelity as a crucial benchmark for clinically
meaningful brain MRI synthesis. Our code is available at
https://github.com/BahramJafrasteh/wasabi-mri.

</details>


### [131] [Anomaly-Driven Approach for Enhanced Prostate Cancer Segmentation](https://arxiv.org/abs/2504.21789)
*Alessia Hu, Regina Beets-Tan, Lishan Cai, Eduardo Pooch*

Main category: cs.CV

TL;DR: The paper introduces Anomaly-Driven U-Net (adU-Net) to improve prostate cancer detection in MRI by integrating anomaly maps into a segmentation framework, outperforming baseline methods.


<details>
  <summary>Details</summary>
Motivation: Automated MRI methods for prostate cancer detection face challenges like data imbalance and lack of annotations, prompting the need for improved techniques.

Method: adU-Net uses anomaly maps from biparametric MRI, generated via Fixed-Point GAN, to guide segmentation. Performance is evaluated using AUROC and AP scores.

Result: adU-Net achieves a higher average score (0.618) than nnU-Net (0.605), showing improved generalization, especially with ADC-based anomaly maps.

Conclusion: Incorporating anomaly detection into segmentation enhances automated prostate cancer identification, with ADC-based maps being particularly effective.

Abstract: Magnetic Resonance Imaging (MRI) plays an important role in identifying
clinically significant prostate cancer (csPCa), yet automated methods face
challenges such as data imbalance, variable tumor sizes, and a lack of
annotated data. This study introduces Anomaly-Driven U-Net (adU-Net), which
incorporates anomaly maps derived from biparametric MRI sequences into a deep
learning-based segmentation framework to improve csPCa identification. We
conduct a comparative analysis of anomaly detection methods and evaluate the
integration of anomaly maps into the segmentation pipeline. Anomaly maps,
generated using Fixed-Point GAN reconstruction, highlight deviations from
normal prostate tissue, guiding the segmentation model to potential cancerous
regions. We compare the performance by using the average score, computed as the
mean of the AUROC and Average Precision (AP). On the external test set, adU-Net
achieves the best average score of 0.618, outperforming the baseline nnU-Net
model (0.605). The results demonstrate that incorporating anomaly detection
into segmentation improves generalization and performance, particularly with
ADC-based anomaly maps, offering a promising direction for automated csPCa
identification.

</details>


### [132] [A simple and effective approach for body part recognition on CT scans based on projection estimation](https://arxiv.org/abs/2504.21810)
*Franko Hrzic, Mohammadreza Movahhedi, Ophelie Lavoie-Gagne, Ata Kiapour*

Main category: cs.CV

TL;DR: The paper proposes a 2D X-ray-like estimation method for identifying body regions in 3D CT scans, outperforming 2.5D, 3D, and foundation model approaches with high accuracy.


<details>
  <summary>Details</summary>
Motivation: Labeling CT data is challenging due to its volumetric nature and incomplete metadata. The study aims to simplify body region identification for constructing high-quality medical datasets.

Method: The approach uses estimated 2D images from 3D CT scans to identify 14 distinct body regions, comparing it against 2.5D, 3D, and foundation model methods.

Result: The proposed method achieved an F1-Score of 0.980 ± 0.016 (EffNet-B0), significantly outperforming other models (0.840–0.854).

Conclusion: The 2D estimation method is effective for body region identification in CT scans, offering a simpler and more accurate alternative to existing approaches.

Abstract: It is well known that machine learning models require a high amount of
annotated data to obtain optimal performance. Labelling Computed Tomography
(CT) data can be a particularly challenging task due to its volumetric nature
and often missing and$/$or incomplete associated meta-data. Even inspecting one
CT scan requires additional computer software, or in the case of programming
languages $-$ additional programming libraries. This study proposes a simple,
yet effective approach based on 2D X-ray-like estimation of 3D CT scans for
body region identification. Although body region is commonly associated with
the CT scan, it often describes only the focused major body region neglecting
other anatomical regions present in the observed CT. In the proposed approach,
estimated 2D images were utilized to identify 14 distinct body regions,
providing valuable information for constructing a high-quality medical dataset.
To evaluate the effectiveness of the proposed method, it was compared against
2.5D, 3D and foundation model (MI2) based approaches. Our approach outperformed
the others, where it came on top with statistical significance and F1-Score for
the best-performing model EffNet-B0 of 0.980 $\pm$ 0.016 in comparison to the
0.840 $\pm$ 0.114 (2.5D DenseNet-161), 0.854 $\pm$ 0.096 (3D VoxCNN), and 0.852
$\pm$ 0.104 (MI2 foundation model). The utilized dataset comprised three
different clinical centers and counted 15,622 CT scans (44,135 labels).

</details>


### [133] [Why Compress What You Can Generate? When GPT-4o Generation Ushers in Image Compression Fields](https://arxiv.org/abs/2504.21814)
*Yixin Gao, Xiaohan Pan, Xin Li, Zhibo Chen*

Main category: cs.CV

TL;DR: The paper explores using GPT-4o's image generation for image compression, replacing traditional pixel-level methods with textual or multimodal coding, and achieves impressive results at ultra-low bitrates.


<details>
  <summary>Details</summary>
Motivation: The rise of AIGC models like GPT-4o, capable of generating detailed images from compact descriptors, prompts the question of why compress what can be generated, leading to an investigation of its potential in image compression.

Method: The study investigates textual coding and multimodal coding (text + low-res image) using GPT-4o, proposing a structure raster-scan prompt engineering mechanism to maintain semantic consistency during decoding.

Result: Experiments show the method outperforms recent multimodal/generative compression techniques at ultra-low bitrates.

Conclusion: The work highlights the potential of AIGC models like GPT-4o in revolutionizing image compression by generating rather than compressing pixel-level data.

Abstract: The rapid development of AIGC foundation models has revolutionized the
paradigm of image compression, which paves the way for the abandonment of most
pixel-level transform and coding, compelling us to ask: why compress what you
can generate if the AIGC foundation model is powerful enough to faithfully
generate intricate structure and fine-grained details from nothing more than
some compact descriptors, i.e., texts, or cues. Fortunately, recent GPT-4o
image generation of OpenAI has achieved impressive cross-modality generation,
editing, and design capabilities, which motivates us to answer the above
question by exploring its potential in image compression fields. In this work,
we investigate two typical compression paradigms: textual coding and multimodal
coding (i.e., text + extremely low-resolution image), where all/most
pixel-level information is generated instead of compressing via the advanced
GPT-4o image generation function. The essential challenge lies in how to
maintain semantic and structure consistency during the decoding process. To
overcome this, we propose a structure raster-scan prompt engineering mechanism
to transform the image into textual space, which is compressed as the condition
of GPT-4o image generation. Extensive experiments have shown that the
combination of our designed structural raster-scan prompts and GPT-4o's image
generation function achieved the impressive performance compared with recent
multimodal/generative image compression at ultra-low bitrate, further
indicating the potential of AIGC generation in image compression fields.

</details>


### [134] [Early Exit and Multi Stage Knowledge Distillation in VLMs for Video Summarization](https://arxiv.org/abs/2504.21831)
*Anas Anwarul Haq Khan, Utkarsh Verma, Prateek Chanda, Ganesh Ramakrishnan*

Main category: cs.CV

TL;DR: DEEVISum is a lightweight vision-language model for video summarization, using multi-modal prompts and techniques like MSKD and Early Exit to balance performance and efficiency. It achieves competitive results with lower computational costs.


<details>
  <summary>Details</summary>
Motivation: To create an efficient and scalable model for video summarization that balances performance and computational efficiency.

Method: Uses Multi Stage Knowledge Distillation (MSKD) and Early Exit (EE) with multi-modal prompts (text and audio signals).

Result: MSKD improves F1 by 1.33%, EE reduces inference time by 21% with a slight F1 drop. Achieves 61.1 F1 on TVSum dataset.

Conclusion: DEEVISum offers a lightweight, efficient solution for video summarization, matching larger models' performance with lower computational costs.

Abstract: We introduce DEEVISum (Distilled Early Exit Vision language model for
Summarization), a lightweight, efficient, and scalable vision language model
designed for segment wise video summarization. Leveraging multi modal prompts
that combine textual and audio derived signals, DEEVISum incorporates Multi
Stage Knowledge Distillation (MSKD) and Early Exit (EE) to strike a balance
between performance and efficiency. MSKD offers a 1.33% absolute F1 improvement
over baseline distillation (0.5%), while EE reduces inference time by
approximately 21% with a 1.3 point drop in F1. Evaluated on the TVSum dataset,
our best model PaLI Gemma2 3B + MSKD achieves an F1 score of 61.1, competing
the performance of significantly larger models, all while maintaining a lower
computational footprint. We publicly release our code and processed dataset to
support further research.

</details>


### [135] [3D Stylization via Large Reconstruction Model](https://arxiv.org/abs/2504.21836)
*Ipek Oztas, Duygu Ceylan, Aysegul Dundar*

Main category: cs.CV

TL;DR: A method for 3D appearance stylization using attention mechanisms from large reconstruction models, without training or optimization.


<details>
  <summary>Details</summary>
Motivation: Users seek more control over 3D generation, especially appearance stylization, inspired by 2D stylization methods.

Method: Inject features from a style image into attention blocks of large 3D reconstruction models.

Result: Superior 3D stylization results, high efficiency, and no need for training or optimization.

Conclusion: The method effectively adapts 3D assets to reference styles, maintaining quality and consistency.

Abstract: With the growing success of text or image guided 3D generators, users demand
more control over the generation process, appearance stylization being one of
them. Given a reference image, this requires adapting the appearance of a
generated 3D asset to reflect the visual style of the reference while
maintaining visual consistency from multiple viewpoints. To tackle this
problem, we draw inspiration from the success of 2D stylization methods that
leverage the attention mechanisms in large image generation models to capture
and transfer visual style. In particular, we probe if large reconstruction
models, commonly used in the context of 3D generation, has a similar
capability. We discover that the certain attention blocks in these models
capture the appearance specific features. By injecting features from a visual
style image to such blocks, we develop a simple yet effective 3D appearance
stylization method. Our method does not require training or test time
optimization. Through both quantitative and qualitative evaluations, we
demonstrate that our approach achieves superior results in terms of 3D
appearance stylization, significantly improving efficiency while maintaining
high-quality visual outcomes.

</details>


### [136] [Active Light Modulation to Counter Manipulation of Speech Visual Content](https://arxiv.org/abs/2504.21846)
*Hadleigh Schwartz, Xiaofeng Yan, Charles J. Carver, Xia Zhou*

Main category: cs.CV

TL;DR: Spotlight is a system for protecting live speech videos from visual falsification by embedding imperceptible, cryptographically-secured physical signatures into recordings.


<details>
  <summary>Details</summary>
Motivation: High-profile speech videos are vulnerable to falsification due to their accessibility and influence, necessitating a robust protection system.

Method: Spotlight uses dynamic physical signatures (modulated light) to embed compact, pose-invariant speech video features (150-bit) and an optical modulation scheme (>200 bps).

Result: Achieves AUCs ≥ 0.99 and 100% true positive rate in detecting falsified videos, with robustness against various conditions and attacks.

Conclusion: Spotlight effectively safeguards live speech videos from falsification with high accuracy and resilience.

Abstract: High-profile speech videos are prime targets for falsification, owing to
their accessibility and influence. This work proposes Spotlight, a low-overhead
and unobtrusive system for protecting live speech videos from visual
falsification of speaker identity and lip and facial motion. Unlike predominant
falsification detection methods operating in the digital domain, Spotlight
creates dynamic physical signatures at the event site and embeds them into all
video recordings via imperceptible modulated light. These physical signatures
encode semantically-meaningful features unique to the speech event, including
the speaker's identity and facial motion, and are cryptographically-secured to
prevent spoofing. The signatures can be extracted from any video downstream and
validated against the portrayed speech content to check its integrity. Key
elements of Spotlight include (1) a framework for generating extremely compact
(i.e., 150-bit), pose-invariant speech video features, based on
locality-sensitive hashing; and (2) an optical modulation scheme that embeds
>200 bps into video while remaining imperceptible both in video and live.
Prototype experiments on extensive video datasets show Spotlight achieves AUCs
$\geq$ 0.99 and an overall true positive rate of 100% in detecting falsified
videos. Further, Spotlight is highly robust across recording conditions, video
post-processing techniques, and white-box adversarial attacks on its video
feature extraction methodologies.

</details>


### [137] [COMPACT: COMPositional Atomic-to-Complex Visual Capability Tuning](https://arxiv.org/abs/2504.21850)
*Xindi Wu, Hee Seung Hwang, Polina Kirichenko, Olga Russakovsky*

Main category: cs.CV

TL;DR: COMPACT improves MLLMs' performance on complex vision-language tasks by focusing on compositional complexity in training data, outperforming traditional methods with less data.


<details>
  <summary>Details</summary>
Motivation: MLLMs struggle with complex tasks requiring multiple capabilities due to traditional VIT focusing on data volume, not compositional complexity.

Method: COMPACT generates a dataset controlling for compositional complexity, enabling MLLMs to efficiently learn complex capabilities from atomic ones.

Result: COMPACT matches LLaVA-665k VIT performance with <10% data and outperforms it on complex tasks (e.g., 83.3% improvement on MMStar).

Conclusion: COMPACT provides a scalable, data-efficient solution for enhancing MLLMs' performance on complex vision-language tasks.

Abstract: Multimodal Large Language Models (MLLMs) excel at simple vision-language
tasks but struggle when faced with complex tasks that require multiple
capabilities, such as simultaneously recognizing objects, counting them, and
understanding their spatial relationships. This might be partially the result
of the fact that Visual Instruction Tuning (VIT), a critical training step for
MLLMs, has traditionally focused on scaling data volume, but not the
compositional complexity of training examples. We propose COMPACT
(COMPositional Atomic-to-complex visual Capability Tuning), which generates a
training dataset explicitly controlling for the compositional complexity of the
training examples. The data from COMPACT allows MLLMs to train on combinations
of atomic capabilities to learn complex capabilities more efficiently. Across
all benchmarks, COMPACT achieves comparable performance to the LLaVA-665k VIT
while using less than 10% of its data budget, and even outperforms it on
several, especially those involving complex multi-capability tasks. For
example, COMPACT achieves substantial 83.3% improvement on MMStar and 94.0%
improvement on MM-Vet compared to the full-scale VIT on particularly complex
questions that require four or more atomic capabilities. COMPACT offers a
scalable, data-efficient, visual compositional tuning recipe to improve on
complex visual-language tasks.

</details>


### [138] [A Survey of Interactive Generative Video](https://arxiv.org/abs/2504.21853)
*Jiwen Yu, Yiran Qin, Haoxuan Che, Quande Liu, Xintao Wang, Pengfei Wan, Di Zhang, Kun Gai, Hao Chen, Xihui Liu*

Main category: cs.CV

TL;DR: The paper defines Interactive Generative Video (IGV) as a technology combining generative and interactive features, surveys its applications in gaming, embodied AI, and autonomous driving, and proposes a framework with five modules for future development.


<details>
  <summary>Details</summary>
Motivation: The growing demand for high-quality, interactive video content across domains like gaming, AI, and autonomous driving motivates the exploration and systematization of IGV technology.

Method: The paper surveys IGV applications and proposes a framework with five modules (Generation, Control, Memory, Dynamics, Intelligence) to guide future development.

Result: The analysis identifies technical challenges (e.g., real-time generation, open-domain control) and future directions for IGV systems.

Conclusion: The systematic analysis aims to advance IGV research and development for more sophisticated and practical applications.

Abstract: Interactive Generative Video (IGV) has emerged as a crucial technology in
response to the growing demand for high-quality, interactive video content
across various domains. In this paper, we define IGV as a technology that
combines generative capabilities to produce diverse high-quality video content
with interactive features that enable user engagement through control signals
and responsive feedback. We survey the current landscape of IGV applications,
focusing on three major domains: 1) gaming, where IGV enables infinite
exploration in virtual worlds; 2) embodied AI, where IGV serves as a
physics-aware environment synthesizer for training agents in multimodal
interaction with dynamically evolving scenes; and 3) autonomous driving, where
IGV provides closed-loop simulation capabilities for safety-critical testing
and validation. To guide future development, we propose a comprehensive
framework that decomposes an ideal IGV system into five essential modules:
Generation, Control, Memory, Dynamics, and Intelligence. Furthermore, we
systematically analyze the technical challenges and future directions in
realizing each component for an ideal IGV system, such as achieving real-time
generation, enabling open-domain control, maintaining long-term coherence,
simulating accurate physics, and integrating causal reasoning. We believe that
this systematic analysis will facilitate future research and development in the
field of IGV, ultimately advancing the technology toward more sophisticated and
practical applications.

</details>


### [139] [ReVision: High-Quality, Low-Cost Video Generation with Explicit 3D Physics Modeling for Complex Motion and Interaction](https://arxiv.org/abs/2504.21855)
*Qihao Liu, Ju He, Qihang Yu, Liang-Chieh Chen, Alan Yuille*

Main category: cs.CV

TL;DR: ReVision integrates 3D physical knowledge into video generation, improving motion fidelity and coherence in complex scenarios.


<details>
  <summary>Details</summary>
Motivation: Addressing challenges in generating complex motions and interactions in video generation.

Method: A three-stage framework: coarse video generation, 3D feature extraction and refinement, and motion-consistent video generation.

Result: Significantly improves motion fidelity, outperforming larger models with only 1.5B parameters.

Conclusion: Incorporating 3D physical knowledge enhances realism and controllability in video generation.

Abstract: In recent years, video generation has seen significant advancements. However,
challenges still persist in generating complex motions and interactions. To
address these challenges, we introduce ReVision, a plug-and-play framework that
explicitly integrates parameterized 3D physical knowledge into a pretrained
conditional video generation model, significantly enhancing its ability to
generate high-quality videos with complex motion and interactions.
Specifically, ReVision consists of three stages. First, a video diffusion model
is used to generate a coarse video. Next, we extract a set of 2D and 3D
features from the coarse video to construct a 3D object-centric representation,
which is then refined by our proposed parameterized physical prior model to
produce an accurate 3D motion sequence. Finally, this refined motion sequence
is fed back into the same video diffusion model as additional conditioning,
enabling the generation of motion-consistent videos, even in scenarios
involving complex actions and interactions. We validate the effectiveness of
our approach on Stable Video Diffusion, where ReVision significantly improves
motion fidelity and coherence. Remarkably, with only 1.5B parameters, it even
outperforms a state-of-the-art video generation model with over 13B parameters
on complex video generation by a substantial margin. Our results suggest that,
by incorporating 3D physical knowledge, even a relatively small video diffusion
model can generate complex motions and interactions with greater realism and
controllability, offering a promising solution for physically plausible video
generation.

</details>


### [140] [Fine-tuning Is a Surprisingly Effective Domain Adaptation Baseline in Handwriting Recognition](https://arxiv.org/abs/2302.06308)
*Jan Kohút, Michal Hradiš*

Main category: cs.CV

TL;DR: Fine-tuning with data augmentation effectively adapts neural networks for handwriting recognition, showing resistance to overfitting and significant performance improvements.


<details>
  <summary>Details</summary>
Motivation: To address the challenge of adapting general models to small specialized datasets in handwriting recognition tasks.

Method: Simple fine-tuning with data augmentation, evaluated for augmentation impact, training data size, and pre-trained network quality.

Result: Average relative CER improvement of 25% for 16 text lines and 50% for 256 text lines.

Conclusion: Fine-tuning with augmentation is a robust method for domain adaptation in handwriting recognition, even with very small datasets.

Abstract: In many machine learning tasks, a large general dataset and a small
specialized dataset are available. In such situations, various domain
adaptation methods can be used to adapt a general model to the target dataset.
We show that in the case of neural networks trained for handwriting recognition
using CTC, simple fine-tuning with data augmentation works surprisingly well in
such scenarios and that it is resistant to overfitting even for very small
target domain datasets. We evaluated the behavior of fine-tuning with respect
to augmentation, training data size, and quality of the pre-trained network,
both in writer-dependent and writer-independent settings. On a large real-world
dataset, fine-tuning on new writers provided an average relative CER
improvement of 25 % for 16 text lines and 50 % for 256 text lines.

</details>


### [141] [Towards Writing Style Adaptation in Handwriting Recognition](https://arxiv.org/abs/2302.06318)
*Jan Kohút, Michal Hradiš, Martin Kišš*

Main category: cs.CV

TL;DR: The paper proposes a Writer Style Block (WSB) to improve handwriting recognition by incorporating writer-dependent parameters, showing better accuracy in writer-dependent scenarios but suggesting further improvements for stability and regularization.


<details>
  <summary>Details</summary>
Motivation: Handwriting recognition faces challenges due to diverse writing styles. Current methods ignore writer-specific style information, potentially limiting accuracy.

Method: Introduces WSB, an adaptive instance normalization layer conditioned on learned embeddings of partitions (e.g., single author texts). Tests various WSB placements and contrastive pre-training.

Result: Outperforms baseline without WSB in writer-dependent scenarios and allows embedding estimation for new writers. However, simple fine-tuning in writer-independent settings is more accurate.

Conclusion: WSB shows promise but requires further investigation for training stability and embedding regularization to surpass simpler methods.

Abstract: One of the challenges of handwriting recognition is to transcribe a large
number of vastly different writing styles. State-of-the-art approaches do not
explicitly use information about the writer's style, which may be limiting
overall accuracy due to various ambiguities. We explore models with
writer-dependent parameters which take the writer's identity as an additional
input. The proposed models can be trained on datasets with partitions likely
written by a single author (e.g. single letter, diary, or chronicle). We
propose a Writer Style Block (WSB), an adaptive instance normalization layer
conditioned on learned embeddings of the partitions. We experimented with
various placements and settings of WSB and contrastively pre-trained
embeddings. We show that our approach outperforms a baseline with no WSB in a
writer-dependent scenario and that it is possible to estimate embeddings for
new writers. However, domain adaptation using simple fine-tuning in a
writer-independent setting provides superior accuracy at a similar
computational cost. The proposed approach should be further investigated in
terms of training stability and embedding regularization to overcome such a
baseline.

</details>


### [142] [SignLLM: Sign Language Production Large Language Models](https://arxiv.org/abs/2405.10718)
*Sen Fang, Chen Chen, Lei Wang, Ce Zheng, Chunyu Sui, Yapeng Tian*

Main category: cs.CV

TL;DR: SignLLM is a multilingual Sign Language Production model with two novel modes (MLSF and Prompt2LangGloss) and RL-based training enhancements, achieving state-of-the-art performance.


<details>
  <summary>Details</summary>
Motivation: To improve multilingual sign language production by enabling gesture generation from text and prompt inputs, leveraging reinforcement learning for better training.

Method: Introduces two modes (MLSF and Prompt2LangGloss) and a new RL loss with Priority Learning Channel. Uses the Prompt2Sign dataset for training.

Result: Achieves state-of-the-art performance across eight sign languages.

Conclusion: SignLLM effectively advances multilingual SLP with innovative training and dataset standardization.

Abstract: In this paper, we propose SignLLM, a multilingual Sign Language Production
(SLP) large language model, which includes two novel multilingual SLP modes
MLSF and Prompt2LangGloss that allow sign language gestures generation from
query texts input and question-style prompts input respectively. Both modes can
use a new RL loss based on reinforcement learning and a new RL module named
Priority Learning Channel. These RL components can accelerate the training by
enhancing the model's capability to sample high-quality data. To train SignLLM,
we introduce Prompt2Sign, a comprehensive multilingual sign language dataset,
which builds from public data, including American Sign Language (ASL) and seven
others. This dataset standardizes information by extracting pose information
from sign language videos into a unified compressed format. We extensively
evaluate SignLLM, demonstrating that our model achieves state-of-the-art
performance on SLP tasks across eight sign languages.

</details>


### [143] [VecFontSDF: Learning to Reconstruct and Synthesize High-quality Vector Fonts via Signed Distance Functions](https://arxiv.org/abs/2303.12675)
*Zeqing Xia, Bojun Xiong, Zhouhui Lian*

Main category: cs.CV

TL;DR: VecFontSDF is an end-to-end trainable method for synthesizing high-quality vector fonts using signed distance functions (SDFs), outperforming existing methods.


<details>
  <summary>Details</summary>
Motivation: Existing methods focus on raster image generation, lacking direct vector font synthesis. VecFontSDF addresses this gap.

Method: Uses SDF-based implicit shape representation to model glyphs as shape primitives with parabolic curves, convertible to quadratic Bézier curves.

Result: Achieves high-quality results in vector font reconstruction, interpolation, and few-shot synthesis, surpassing state-of-the-art.

Conclusion: VecFontSDF effectively bridges the gap in vector font synthesis, offering a scalable solution for font design.

Abstract: Font design is of vital importance in the digital content design and modern
printing industry. Developing algorithms capable of automatically synthesizing
vector fonts can significantly facilitate the font design process. However,
existing methods mainly concentrate on raster image generation, and only a few
approaches can directly synthesize vector fonts. This paper proposes an
end-to-end trainable method, VecFontSDF, to reconstruct and synthesize
high-quality vector fonts using signed distance functions (SDFs). Specifically,
based on the proposed SDF-based implicit shape representation, VecFontSDF
learns to model each glyph as shape primitives enclosed by several parabolic
curves, which can be precisely converted to quadratic B\'ezier curves that are
widely used in vector font products. In this manner, most image generation
methods can be easily extended to synthesize vector fonts. Qualitative and
quantitative experiments conducted on a publicly-available dataset demonstrate
that our method obtains high-quality results on several tasks, including vector
font reconstruction, interpolation, and few-shot vector font synthesis,
markedly outperforming the state of the art.

</details>


### [144] [Generalizable Synthetic Image Detection via Language-guided Contrastive Learning](https://arxiv.org/abs/2305.13800)
*Haiwei Wu, Jiantao Zhou, Shile Zhang*

Main category: cs.CV

TL;DR: The paper proposes LASTED, a language-guided contrastive learning method for detecting AI-generated images, improving generalization and outperforming existing methods.


<details>
  <summary>Details</summary>
Motivation: Concerns about the misuse of AI-generated images (e.g., fake news) and the inadequacy of current forensic detection methods drive the need for better solutions.

Method: Uses language-guided contrastive learning by augmenting training images with textual labels to improve forensic feature generalization.

Result: LASTED achieves superior generalization to unseen models and outperforms state-of-the-art methods across four datasets.

Conclusion: The proposed method effectively addresses the generalization challenge in synthetic image detection, offering a robust solution.

Abstract: The heightened realism of AI-generated images can be attributed to the rapid
development of synthetic models, including generative adversarial networks
(GANs) and diffusion models (DMs). The malevolent use of synthetic images, such
as the dissemination of fake news or the creation of fake profiles, however,
raises significant concerns regarding the authenticity of images. Though many
forensic algorithms have been developed for detecting synthetic images, their
performance, especially the generalization capability, is still far from being
adequate to cope with the increasing number of synthetic models. In this work,
we propose a simple yet very effective synthetic image detection method via a
language-guided contrastive learning. Specifically, we augment the training
images with carefully-designed textual labels, enabling us to use a joint
visual-language contrastive supervision for learning a forensic feature space
with better generalization. It is shown that our proposed LanguAge-guided
SynThEsis Detection (LASTED) model achieves much improved generalizability to
unseen image generation models and delivers promising performance that far
exceeds state-of-the-art competitors over four datasets. The code is available
at https://github.com/HighwayWu/LASTED.

</details>


### [145] [All Languages Matter: Evaluating LMMs on Culturally Diverse 100 Languages](https://arxiv.org/abs/2411.16508)
*Ashmal Vayani, Dinura Dissanayake, Hasindri Watawana, Noor Ahsan, Nevasini Sasikumar, Omkar Thawakar, Henok Biadglign Ademtew, Yahya Hmaiti, Amandeep Kumar, Kartik Kuckreja, Mykola Maslych, Wafa Al Ghallabi, Mihail Mihaylov, Chao Qin, Abdelrahman M Shaker, Mike Zhang, Mahardika Krisna Ihsani, Amiel Esplana, Monil Gokani, Shachar Mirkin, Harsh Singh, Ashay Srivastava, Endre Hamerlik, Fathinah Asma Izzati, Fadillah Adamsyah Maani, Sebastian Cavada, Jenny Chim, Rohit Gupta, Sanjay Manjunath, Kamila Zhumakhanova, Feno Heriniaina Rabevohitra, Azril Amirudin, Muhammad Ridzuan, Daniya Kareem, Ketan More, Kunyang Li, Pramesh Shakya, Muhammad Saad, Amirpouya Ghasemaghaei, Amirbek Djanibekov, Dilshod Azizov, Branislava Jankovic, Naman Bhatia, Alvaro Cabrera, Johan Obando-Ceron, Olympiah Otieno, Fabian Farestam, Muztoba Rabbani, Sanoojan Baliah, Santosh Sanjeev, Abduragim Shtanchaev, Maheen Fatima, Thao Nguyen, Amrin Kareem, Toluwani Aremu, Nathan Xavier, Amit Bhatkal, Hawau Toyin, Aman Chadha, Hisham Cholakkal, Rao Muhammad Anwer, Michael Felsberg, Jorma Laaksonen, Thamar Solorio, Monojit Choudhury, Ivan Laptev, Mubarak Shah, Salman Khan, Fahad Khan*

Main category: cs.CV

TL;DR: ALM-bench is a benchmark for evaluating Large Multimodal Models (LMMs) across 100 languages, emphasizing cultural diversity and inclusivity.


<details>
  <summary>Details</summary>
Motivation: To address the lack of cultural and linguistic diversity in existing LMMs, ensuring they understand global contexts and support low-resource languages.

Method: ALM-bench evaluates LMMs using diverse question formats (true/false, multiple choice, open-ended) and curated content from 13 cultural aspects.

Result: The benchmark provides a rigorous framework to assess LMMs' ability to handle visual and linguistic reasoning across cultures and languages.

Conclusion: ALM-bench promotes the development of globally inclusive LMMs and highlights the need for cultural and linguistic diversity in AI models.

Abstract: Existing Large Multimodal Models (LMMs) generally focus on only a few regions
and languages. As LMMs continue to improve, it is increasingly important to
ensure they understand cultural contexts, respect local sensitivities, and
support low-resource languages, all while effectively integrating corresponding
visual cues. In pursuit of culturally diverse global multimodal models, our
proposed All Languages Matter Benchmark (ALM-bench) represents the largest and
most comprehensive effort to date for evaluating LMMs across 100 languages.
ALM-bench challenges existing models by testing their ability to understand and
reason about culturally diverse images paired with text in various languages,
including many low-resource languages traditionally underrepresented in LMM
research. The benchmark offers a robust and nuanced evaluation framework
featuring various question formats, including true/false, multiple choice, and
open-ended questions, which are further divided into short and long-answer
categories. ALM-bench design ensures a comprehensive assessment of a model's
ability to handle varied levels of difficulty in visual and linguistic
reasoning. To capture the rich tapestry of global cultures, ALM-bench carefully
curates content from 13 distinct cultural aspects, ranging from traditions and
rituals to famous personalities and celebrations. Through this, ALM-bench not
only provides a rigorous testing ground for state-of-the-art open and
closed-source LMMs but also highlights the importance of cultural and
linguistic inclusivity, encouraging the development of models that can serve
diverse global populations effectively. Our benchmark is publicly available.

</details>


### [146] [Semantic Positive Pairs for Enhancing Visual Representation Learning of Instance Discrimination Methods](https://arxiv.org/abs/2306.16122)
*Mohammad Alkhalefi, Georgios Leontidis, Mingjun Zhong*

Main category: cs.CV

TL;DR: The paper proposes enhancing self-supervised learning (SSL) by identifying semantically similar images as positive pairs, improving representation richness and performance over baseline methods.


<details>
  <summary>Details</summary>
Motivation: Data augmentation in SSL limits positive pair representation, and contrastive learning may discard important features for similar instances.

Method: Identify images with similar semantic content as positive instances, compatible with frameworks like MoCo and SimSiam.

Result: Outperforms baselines on ImageNet, STL-10, and CIFAR-10; e.g., 4.1% improvement over MoCo-v2 on ImageNet.

Conclusion: The approach effectively enriches latent representations and boosts performance in SSL tasks, including semi-supervised and transfer learning.

Abstract: Self-supervised learning algorithms (SSL) based on instance discrimination
have shown promising results, performing competitively or even outperforming
supervised learning counterparts in some downstream tasks. Such approaches
employ data augmentation to create two views of the same instance (i.e.,
positive pairs) and encourage the model to learn good representations by
attracting these views closer in the embedding space without collapsing to the
trivial solution. However, data augmentation is limited in representing
positive pairs, and the repulsion process between the instances during
contrastive learning may discard important features for instances that have
similar categories. To address this issue, we propose an approach to identify
those images with similar semantic content and treat them as positive
instances, thereby reducing the chance of discarding important features during
representation learning and increasing the richness of the latent
representation. Our approach is generic and could work with any self-supervised
instance discrimination frameworks such as MoCo and SimSiam. To evaluate our
method, we run experiments on three benchmark datasets: ImageNet, STL-10 and
CIFAR-10 with different instance discrimination SSL approaches. The
experimental results show that our approach consistently outperforms the
baseline methods across all three datasets; for instance, we improve upon the
vanilla MoCo-v2 by 4.1% on ImageNet under a linear evaluation protocol over 800
epochs. We also report results on semi-supervised learning, transfer learning
on downstream tasks, and object detection.

</details>


### [147] [Joint Modeling of Feature, Correspondence, and a Compressed Memory for Video Object Segmentation](https://arxiv.org/abs/2308.13505)
*Jiaming Zhang, Yutao Cui, Gangshan Wu, Limin Wang*

Main category: cs.CV

TL;DR: JointFormer is a unified VOS framework that jointly models feature extraction, matching, and memory, outperforming existing methods on benchmarks.


<details>
  <summary>Details</summary>
Motivation: Current VOS methods decouple feature extraction and matching, limiting information propagation and robustness to distractors.

Method: JointFormer uses attention for joint feature extraction and propagation, with a compressed memory token for holistic target understanding.

Result: Achieves state-of-the-art performance on DAVIS and YouTube-VOS benchmarks and generalizes well to new challenges.

Conclusion: JointFormer's unified approach enhances feature learning and global consistency, setting new benchmarks in VOS.

Abstract: Current prevailing Video Object Segmentation methods follow the pipeline of
extraction-then-matching, which first extracts features on current and
reference frames independently, and then performs dense matching between them.
This decoupled pipeline limits information propagation between frames to
high-level features, hindering fine-grained details for matching. Furthermore,
the pixel-wise matching lacks holistic target understanding, making it prone to
disturbance by similar distractors. To address these issues, we propose a
unified VOS framework, coined as JointFormer, for jointly modeling feature
extraction, correspondence matching, and a compressed memory. The core Joint
Modeling Block leverages attention to simultaneously extract and propagate the
target information from the reference frame to the current frame and a
compressed memory token. This joint scheme enables extensive multi-layer
propagation beyond high-level feature space and facilitates robust
instance-distinctive feature learning. To incorporate the long-term and
holistic target information, we introduce a compressed memory token with a
customized online updating mechanism, which aggregates target features and
facilitates temporal information propagation in a frame-wise manner, enhancing
global modeling consistency. Our JointFormer achieves a new state-of-the-art
performance on the DAVIS 2017 val/test-dev (89.7\% and 87.6\%) benchmarks and
the YouTube-VOS 2018/2019 val (87.0\% and 87.0\%) benchmarks, outperforming the
existing works. To demonstrate the generalizability of our model, it is further
evaluated on four new benchmarks with various difficulties, including MOSE for
complex scenes, VISOR for egocentric videos, VOST for complex transformations,
and LVOS for long-term videos.

</details>


### [148] [SignDiff: Diffusion Model for American Sign Language Production](https://arxiv.org/abs/2308.16082)
*Sen Fang, Chunyu Sui, Yanghao Zhou, Xuedong Zhang, Hongbin Zhong, Yapeng Tian, Chen Chen*

Main category: cs.CV

TL;DR: SignDiff is a dual-condition diffusion model for generating sign language from skeleton poses, featuring FR-Net for improved pose-text alignment and ASLP enhancements for better accuracy and quality.


<details>
  <summary>Details</summary>
Motivation: To improve sign language production from text by addressing issues like finger multiplicity and enhancing pose-text correspondence.

Method: Uses FR-Net for pose-text alignment, integrates new modules and loss functions for ASLP, and trains on large-scale data.

Result: Achieves SOTA on PHOENIX14T, scores 17.19/12.85 BLEU-4 on How2Sign, and improves SSIM by 10%.

Conclusion: SignDiff advances sign language production with superior accuracy, quality, and scalability.

Abstract: In this paper, we propose a dual-condition diffusion pre-training model named
SignDiff that can generate human sign language speakers from a skeleton pose.
SignDiff has a novel Frame Reinforcement Network called FR-Net, similar to
dense human pose estimation work, which enhances the correspondence between
text lexical symbols and sign language dense pose frames, reduces the
occurrence of multiple fingers in the diffusion model. In addition, we propose
a new method for American Sign Language Production (ASLP), which can generate
ASL skeletal pose videos from text input, integrating two new improved modules
and a new loss function to improve the accuracy and quality of sign language
skeletal posture and enhance the ability of the model to train on large-scale
data. We propose the first baseline for ASL production and report the scores of
17.19 and 12.85 on BLEU-4 on the How2Sign dev/test sets. We evaluated our model
on the previous mainstream dataset PHOENIX14T, and the experiments achieved the
SOTA results. In addition, our image quality far exceeds all previous results
by 10 percentage points in terms of SSIM.

</details>


### [149] [LEyes: A Lightweight Framework for Deep Learning-Based Eye Tracking using Synthetic Eye Images](https://arxiv.org/abs/2309.06129)
*Sean Anthony Byrne, Virmarie Maquiling, Marcus Nyström, Enkelejda Kasneci, Diederick C. Niehorster*

Main category: cs.CV

TL;DR: LEyes, a lightweight synthetic data framework, improves gaze estimation by modeling key image features, outperforming traditional methods and reducing hardware costs.


<details>
  <summary>Details</summary>
Motivation: Real-world gaze estimation is hindered by inadequate datasets and hardware/biological variances, prompting the need for efficient synthetic data solutions.

Method: LEyes models essential image features for eye tracking using simple light distributions, enabling easy neural network training.

Result: LEyes-trained models match or surpass state-of-the-art algorithms in pupil and CR localization and outperform industry standards with cheaper hardware.

Conclusion: LEyes promises to revolutionize synthetic data generation for gaze estimation, enhancing future eye-tracking technology.

Abstract: Deep learning has bolstered gaze estimation techniques, but real-world
deployment has been impeded by inadequate training datasets. This problem is
exacerbated by both hardware-induced variations in eye images and inherent
biological differences across the recorded participants, leading to both
feature and pixel-level variance that hinders the generalizability of models
trained on specific datasets. While synthetic datasets can be a solution, their
creation is both time and resource-intensive. To address this problem, we
present a framework called Light Eyes or "LEyes" which, unlike conventional
photorealistic methods, only models key image features required for video-based
eye tracking using simple light distributions. LEyes facilitates easy
configuration for training neural networks across diverse gaze-estimation
tasks. We demonstrate that models trained using LEyes are consistently on-par
or outperform other state-of-the-art algorithms in terms of pupil and CR
localization across well-known datasets. In addition, a LEyes trained model
outperforms the industry standard eye tracker using significantly more
cost-effective hardware. Going forward, we are confident that LEyes will
revolutionize synthetic data generation for gaze estimation models, and lead to
significant improvements of the next generation video-based eye trackers.

</details>


### [150] [AC-Lite : A Lightweight Image Captioning Model for Low-Resource Assamese Language](https://arxiv.org/abs/2503.01453)
*Pankaj Choudhury, Yogesh Aggarwal, Prabhanjan Jadhav, Prithwijit Guha, Sukumar Nandi*

Main category: cs.CV

TL;DR: AC-Lite is a lightweight model for image captioning in Assamese, reducing computational costs while maintaining performance.


<details>
  <summary>Details</summary>
Motivation: Existing image captioning models are computationally heavy and limited to English, restricting accessibility. AC-Lite aims to address this for low-resource languages like Assamese.

Method: AC-Lite replaces heavy deep network components with lightweight alternatives, using ShuffleNetv2x1.5 for image features and a GRU-based decoder with bilinear attention.

Result: AC-Lite achieves an 82.3 CIDEr score on COCO-AC with 2.45 GFLOPs and 22.87M parameters.

Conclusion: AC-Lite provides an efficient solution for image captioning in low-resource languages, balancing performance and computational cost.

Abstract: Most existing works in image caption synthesis use computation heavy deep
neural networks and generates image descriptions in English language. This
often restricts this important assistive tool for widespread use across
language and accessibility barriers. This work presents AC-Lite, a
computationally efficient model for image captioning in low-resource Assamese
language. AC-Lite reduces computational requirements by replacing
computation-heavy deep network components with lightweight alternatives. The
AC-Lite model is designed through extensive ablation experiments with different
image feature extractor networks and language decoders. A combination of
ShuffleNetv2x1.5 with GRU based language decoder along with bilinear attention
is found to provide the best performance with minimum compute. AC-Lite was
observed to achieve an 82.3 CIDEr score on the COCO-AC dataset with 2.45 GFLOPs
and 22.87M parameters.

</details>


### [151] [EyePreserve: Identity-Preserving Iris Synthesis](https://arxiv.org/abs/2312.12028)
*Siamul Karim Khan, Patrick Tinsley, Mahsa Mitcheff, Patrick Flynn, Kevin W. Bowyer, Adam Czajka*

Main category: cs.CV

TL;DR: A data-driven method for synthesizing iris images with varying pupil sizes while preserving identity, outperforming existing models in identity preservation and similarity for iris recognition.


<details>
  <summary>Details</summary>
Motivation: The complexity of iris muscle constriction and non-linear texture deformations necessitates a precise model for synthesizing identity-preserving iris images across pupil sizes.

Method: A fully data-driven approach synthesizes iris images for existing and non-existing identities, using segmentation masks to non-linearly deform textures.

Result: The method preserves identity across pupil sizes and improves similarity between same-identity iris samples compared to state-of-the-art models.

Conclusion: The approach enhances biometric datasets and aids forensic analysis, with source code and model weights provided for practical use.

Abstract: Synthesis of same-identity biometric iris images, both for existing and
non-existing identities while preserving the identity across a wide range of
pupil sizes, is complex due to the intricate iris muscle constriction
mechanism, requiring a precise model of iris non-linear texture deformations to
be embedded into the synthesis pipeline. This paper presents the first method
of fully data-driven, identity-preserving, pupil size-varying synthesis of iris
images. This approach is capable of synthesizing images of irises with
different pupil sizes representing non-existing identities, as well as
non-linearly deforming the texture of iris images of existing subjects given
the segmentation mask of the target iris image. Iris recognition experiments
suggest that the proposed deformation model both preserves the identity when
changing the pupil size, and offers better similarity between same-identity
iris samples with significant differences in pupil size, compared to
state-of-the-art linear and non-linear (bio-mechanical-based) iris deformation
models. Two immediate applications of the proposed approach are: (a) synthesis
of, or enhancement of the existing biometric datasets for iris recognition,
mimicking those acquired with iris sensors, and (b) helping forensic human
experts examine iris image pairs with significant differences in pupil
dilation. Images considered in this work conform to selected ISO/IEC 29794-6
quality metrics to make them applicable in biometric systems. The source codes
and model weights are offered with this paper.

</details>


### [152] [DDM: A Metric for Comparing 3D Shapes Using Directional Distance Fields](https://arxiv.org/abs/2401.09736)
*Siyu Ren, Junhui Hou, Xiaodong Chen, Hongkai Xiong, Wenping Wang*

Main category: cs.CV

TL;DR: The paper introduces DDM, a novel distance metric for 3D models using directional distance fields (DDFs), improving efficiency and accuracy in geometric modeling tasks.


<details>
  <summary>Details</summary>
Motivation: Existing methods for measuring discrepancies between 3D models are inefficient or ineffective, prompting the need for a robust and differentiable metric.

Method: DDM is based on DDFs, which capture local surface geometry by defining directional distances of 3D points to a model. It transfers model discrepancies to DDF discrepancies.

Result: DDM outperforms existing methods in tasks like template fitting, registration, and pose optimization, achieving higher accuracy.

Conclusion: DDM is a versatile and effective distance metric with potential to advance 3D geometric modeling.

Abstract: Qualifying the discrepancy between 3D geometric models, which could be
represented with either point clouds or triangle meshes, is a pivotal issue
with board applications. Existing methods mainly focus on directly establishing
the correspondence between two models and then aggregating point-wise distance
between corresponding points, resulting in them being either inefficient or
ineffective. In this paper, we propose DDM, an efficient, effective, robust,
and differentiable distance metric for 3D geometry data. Specifically, we
construct DDM based on the proposed implicit representation of 3D models,
namely directional distance field (DDF), which defines the directional
distances of 3D points to a model to capture its local surface geometry. We
then transfer the discrepancy between two 3D geometric models as the
discrepancy between their DDFs defined on an identical domain, naturally
establishing model correspondence. To demonstrate the advantage of our DDM, we
explore various distance metric-driven 3D geometric modeling tasks, including
template surface fitting, rigid registration, non-rigid registration, scene
flow estimation and human pose optimization. Extensive experiments show that
our DDM achieves significantly higher accuracy under all tasks. As a generic
distance metric, DDM has the potential to advance the field of 3D geometric
modeling. The source code is available at https://github.com/rsy6318/DDM.

</details>


### [153] [MeDSLIP: Medical Dual-Stream Language-Image Pre-training with Pathology-Anatomy Semantic Alignment](https://arxiv.org/abs/2403.10635)
*Wenrui Fan, Mohammod N. I. Suvon, Shuo Zhou, Xianyuan Liu, Samer Alabed, Venet Osmani, Andrew J. Swift, Chen Chen, Haiping Lu*

Main category: cs.CV

TL;DR: MeDSLIP is a dual-stream pre-training pipeline designed to disentangle and model pathology and anatomy semantics in medical data, improving medical vision-language models.


<details>
  <summary>Details</summary>
Motivation: Pathology and anatomy semantics are entangled in medical data, hindering explicit modeling of their relationships, which MeDSLIP aims to address.

Method: MeDSLIP uses a dual-stream mechanism to separate pathology and anatomy semantics, aligns visual and textual information within each stream, and employs contrastive learning to model their relationships.

Result: Evaluations on four benchmark datasets show MeDSLIP's superior generalizability and transferability in chest X-ray analysis.

Conclusion: MeDSLIP effectively disentangles and models pathology and anatomy semantics, enhancing medical vision-language models.

Abstract: Pathology and anatomy are two essential groups of semantics in medical data.
Pathology describes what the diseases are, while anatomy explains where the
diseases occur. They describe diseases from different perspectives, providing
complementary insights into diseases. Thus, properly understanding these
semantics and their relationships can enhance medical vision-language models
(VLMs). However, pathology and anatomy semantics are usually entangled in
medical data, hindering VLMs from explicitly modeling these semantics and their
relationships. To address this challenge, we propose MeDSLIP, a novel Medical
Dual-Stream Language-Image Pre-training pipeline, to disentangle pathology and
anatomy semantics and model the relationships between them. We introduce a
dual-stream mechanism in MeDSLIP to explicitly disentangle medical semantics
into pathology-relevant and anatomy-relevant streams and align visual and
textual information within each stream. Furthermore, we propose an interaction
modeling module with prototypical contrastive learning loss and intra-image
contrastive learning loss to regularize the relationships between pathology and
anatomy semantics. We apply MeDSLIP to chest X-ray analysis and conduct
comprehensive evaluations with four benchmark datasets: NIH CXR14, RSNA
Pneumonia, SIIM-ACR Pneumothorax, and COVIDx CXR-4. The results demonstrate
MeDSLIP's superior generalizability and transferability across different
scenarios. The code is available at https://github.com/Shef-AIRE/MeDSLIP, and
the pre-trained model is released at https://huggingface.co/pykale/MeDSLIP.

</details>


### [154] [Garment3DGen: 3D Garment Stylization and Texture Generation](https://arxiv.org/abs/2403.18816)
*Nikolaos Sarafianos, Tuur Stuyck, Xiaoyu Xiang, Yilei Li, Jovan Popovic, Rakesh Ranjan*

Main category: cs.CV

TL;DR: Garment3DGen synthesizes 3D garment assets from a base mesh using a single input image, enabling simulation-ready clothes without artist intervention.


<details>
  <summary>Details</summary>
Motivation: To simplify 3D garment creation for users by leveraging image-to-3D diffusion methods and ensuring the output is simulation-ready.

Method: Uses image-to-3D diffusion for geometry, then optimizes a base mesh deformation with quality-preserving losses and generates high-fidelity textures.

Result: Produces simulation-ready 3D garments from images, with applications in VR and sketch-to-simulated workflows.

Conclusion: Garment3DGen effectively bridges the gap between image guidance and practical 3D garment simulation, offering a versatile tool for users.

Abstract: We introduce Garment3DGen a new method to synthesize 3D garment assets from a
base mesh given a single input image as guidance. Our proposed approach allows
users to generate 3D textured clothes based on both real and synthetic images,
such as those generated by text prompts. The generated assets can be directly
draped and simulated on human bodies. We leverage the recent progress of
image-to-3D diffusion methods to generate 3D garment geometries. However, since
these geometries cannot be utilized directly for downstream tasks, we propose
to use them as pseudo ground-truth and set up a mesh deformation optimization
procedure that deforms a base template mesh to match the generated 3D target.
Carefully designed losses allow the base mesh to freely deform towards the
desired target, yet preserve mesh quality and topology such that they can be
simulated. Finally, we generate high-fidelity texture maps that are globally
and locally consistent and faithfully capture the input guidance, allowing us
to render the generated 3D assets. With Garment3DGen users can generate the
simulation-ready 3D garment of their choice without the need of artist
intervention. We present a plethora of quantitative and qualitative comparisons
on various assets and demonstrate that Garment3DGen unlocks key applications
ranging from sketch-to-simulated garments or interacting with the garments in
VR. Code is publicly available.

</details>


### [155] [BEVWorld: A Multimodal World Simulator for Autonomous Driving via Scene-Level BEV Latents](https://arxiv.org/abs/2407.05679)
*Yumeng Zhang, Shi Gong, Kaixin Xiong, Xiaoqing Ye, Xiaofan Li, Xiao Tan, Fan Wang, Jizhou Huang, Hua Wu, Haifeng Wang*

Main category: cs.CV

TL;DR: BEVWorld is a novel framework for autonomous driving that transforms multimodal sensor inputs into a unified BEV latent space, enabling holistic environment modeling and future scene forecasting.


<details>
  <summary>Details</summary>
Motivation: World models are crucial for autonomous driving to forecast future scenarios, but existing methods lack unified multimodal representation and temporal consistency.

Method: BEVWorld uses a multi-modal tokenizer to encode sensor data into BEV latent space and a latent BEV sequence diffusion model for temporally consistent future scene forecasting.

Result: Experiments show BEVWorld's effectiveness in realistic future scene generation and benefits for downstream tasks like perception and motion prediction.

Conclusion: BEVWorld provides a unified and compact representation for multimodal sensor data, enhancing autonomous driving capabilities.

Abstract: World models have attracted increasing attention in autonomous driving for
their ability to forecast potential future scenarios. In this paper, we propose
BEVWorld, a novel framework that transforms multimodal sensor inputs into a
unified and compact Bird's Eye View (BEV) latent space for holistic environment
modeling. The proposed world model consists of two main components: a
multi-modal tokenizer and a latent BEV sequence diffusion model. The
multi-modal tokenizer first encodes heterogeneous sensory data, and its decoder
reconstructs the latent BEV tokens into LiDAR and surround-view image
observations via ray-casting rendering in a self-supervised manner. This
enables joint modeling and bidirectional encoding-decoding of panoramic imagery
and point cloud data within a shared spatial representation. On top of this,
the latent BEV sequence diffusion model performs temporally consistent
forecasting of future scenes, conditioned on high-level action tokens, enabling
scene-level reasoning over time. Extensive experiments demonstrate the
effectiveness of BEVWorld on autonomous driving benchmarks, showcasing its
capability in realistic future scene generation and its benefits for downstream
tasks such as perception and motion prediction.

</details>


### [156] [Uncovering Bias in Large Vision-Language Models at Scale with Counterfactuals](https://arxiv.org/abs/2405.20152)
*Phillip Howard, Kathleen C. Fraser, Anahita Bhiwandiwalla, Svetlana Kiritchenko*

Main category: cs.CV

TL;DR: The paper investigates social biases in Large Vision-Language Models (LVLMs) by analyzing text generated under counterfactual image changes, revealing significant influences of visual attributes like race and gender on toxic content and stereotypes.


<details>
  <summary>Details</summary>
Motivation: Prior studies focused on biases in text from LLMs, but LVLMs' biases remain unexplored due to the complexity of multimodal inputs. This study aims to fill that gap.

Method: A large-scale study was conducted, generating over 57 million responses from LVLMs under counterfactual image changes to isolate bias contributions from visual and text modalities.

Result: Visual attributes (e.g., race, gender) significantly affect toxic content, stereotypes, competency-associated words, and numerical ratings in generated text.

Conclusion: The study highlights the need for bias mitigation in LVLMs, given their susceptibility to visual modality-induced biases.

Abstract: With the advent of Large Language Models (LLMs) possessing increasingly
impressive capabilities, a number of Large Vision-Language Models (LVLMs) have
been proposed to augment LLMs with visual inputs. Such models condition
generated text on both an input image and a text prompt, enabling a variety of
use cases such as visual question answering and multimodal chat. While prior
studies have examined the social biases contained in text generated by LLMs,
this topic has been relatively unexplored in LVLMs. Examining social biases in
LVLMs is particularly challenging due to the confounding contributions of bias
induced by information contained across the text and visual modalities. To
address this challenging problem, we conduct a large-scale study of text
generated by different LVLMs under counterfactual changes to input images,
producing over 57 million responses from popular models. Our multi-dimensional
bias evaluation framework reveals that social attributes such as perceived
race, gender, and physical characteristics depicted in images can significantly
influence the generation of toxic content, competency-associated words, harmful
stereotypes, and numerical ratings of individuals.

</details>


### [157] [Uncertainty for SVBRDF Acquisition using Frequency Analysis](https://arxiv.org/abs/2406.17774)
*Ruben Wiersma, Julien Philip, Miloš Hašan, Krishna Mullia, Fujun Luan, Elmar Eisemann, Valentin Deschaintre*

Main category: cs.CV

TL;DR: The paper quantifies uncertainty in SVBRDF acquisition for multi-view captures using entropy and frequency domain analysis, enabling fast uncertainty mapping and improved acquisition methods.


<details>
  <summary>Details</summary>
Motivation: To address ambiguity in SVBRDF acquisition under uncontrolled illumination and unstructured viewpoints, ensuring reliable reconstruction of appearance properties.

Method: Uses entropy and frequency domain analysis to accelerate uncertainty computation, validated against a physically-based path tracer. Applies uncertainty maps for capture guidance, surface information sharing, and diffusion-based inpainting.

Result: Frequency model recovers SVBRDF parameters competitively; entropy computation matches path tracer results; error and uncertainty correlate positively.

Conclusion: Uncertainty maps enhance SVBRDF acquisition, with applications in capture guidance and inpainting, supported by open-source code.

Abstract: This paper aims to quantify uncertainty for SVBRDF acquisition in multi-view
captures. Under uncontrolled illumination and unstructured viewpoints, there is
no guarantee that the observations contain enough information to reconstruct
the appearance properties of a captured object. We study this ambiguity, or
uncertainty, using entropy and accelerate the analysis by using the frequency
domain, rather than the domain of incoming and outgoing viewing angles. The
result is a method that computes a map of uncertainty over an entire object
within a millisecond. We find that the frequency model allows us to recover
SVBRDF parameters with competitive performance, that the accelerated entropy
computation matches results with a physically-based path tracer, and that there
is a positive correlation between error and uncertainty. We then show that the
uncertainty map can be applied to improve SVBRDF acquisition using capture
guidance, sharing information on the surface, and using a diffusion model to
inpaint uncertain regions. Our code is available at
https://github.com/rubenwiersma/svbrdf_uncertainty.

</details>


### [158] [Prompt Recovery for Image Generation Models: A Comparative Study of Discrete Optimizers](https://arxiv.org/abs/2408.06502)
*Joshua Nathaniel Williams, Avi Schwarzschild, Yutong He, J. Zico Kolter*

Main category: cs.CV

TL;DR: Comparison of discrete optimization techniques for recovering prompts from generated images, finding captioners often outperform optimizers.


<details>
  <summary>Details</summary>
Motivation: To evaluate and compare methods for inverting prompts from images, addressing the challenge of discrete optimization in this context.

Method: Head-to-head comparison of Greedy Coordinate Gradients (GCG), PEZ, Random Search, AutoDAN, and BLIP2's captioner using metrics like CLIP similarity and image quality.

Result: Discrete optimizers minimize objectives but captioners produce images closer to original prompts. CLIP similarity is a poor proxy for image similarity.

Conclusion: Captioners may be more effective than discrete optimizers for prompt inversion, despite optimizers' theoretical advantages.

Abstract: Recovering natural language prompts for image generation models, solely based
on the generated images is a difficult discrete optimization problem. In this
work, we present the first head-to-head comparison of recent discrete
optimization techniques for the problem of prompt inversion. We evaluate Greedy
Coordinate Gradients (GCG), PEZ , Random Search, AutoDAN and BLIP2's image
captioner across various evaluation metrics related to the quality of inverted
prompts and the quality of the images generated by the inverted prompts. We
find that focusing on the CLIP similarity between the inverted prompts and the
ground truth image acts as a poor proxy for the similarity between ground truth
image and the image generated by the inverted prompts. While the discrete
optimizers effectively minimize their objectives, simply using responses from a
well-trained captioner often leads to generated images that more closely
resemble those produced by the original prompts.

</details>


### [159] [Comparison of Kinematics and Kinetics Between OpenCap and a Marker-Based Motion Capture System in Cycling](https://arxiv.org/abs/2409.03766)
*Reza Kakavand, Reza Ahmadi, Atousa Parsaei, W. Brent Edwards, Amin Komeili*

Main category: cs.CV

TL;DR: The study compares marker-based and markerless (OpenCap) motion capture systems for cycling kinematics and kinetics, finding strong agreement for joint angles.


<details>
  <summary>Details</summary>
Motivation: Markerless systems like OpenCap are practical for real-world use but lack validation in cycling compared to marker-based systems.

Method: Ten participants cycled at varying speeds/resistances; data from both systems were analyzed using OpenSim, RMSE, Pearson correlation, and Bland-Altman.

Result: Very strong agreement (r > 0.9) for hip, knee, and ankle joint angles.

Conclusion: OpenCap shows high agreement with marker-based systems for joint angles in cycling, supporting its practical use.

Abstract: This study evaluates the agreement of marker-based and markerless (OpenCap)
motion capture systems in assessing joint kinematics and kinetics during
cycling. Markerless systems, such as OpenCap, offer the advantage of capturing
natural movements without physical markers, making them more practical for
real-world applications. However, the agreement of OpenCap with a marker-based
system, particularly in cycling, remains underexplored. Ten participants cycled
at varying speeds and resistances while motion data were recorded using both
systems. Key metrics, including joint angles, moments, and joint reaction
loads, were computed using OpenSim and compared using root mean squared error
(RMSE) per trial across participants, Pearson correlation coefficients (r) per
trial across participants and repeated measures Bland-Altman to control trials
dependency within subject. Results revealed very strong agreement (r GT 0.9)
for hip (flexion/extension), knee (flexion/extension), and ankle
(dorsiflexion/plantarflexion) joint angles.

</details>


### [160] [PreCM: The Padding-based Rotation Equivariant Convolution Mode for Semantic Segmentation](https://arxiv.org/abs/2411.01624)
*Xinyu Xu, Huazhen Liu, Tao Zhang, Huilin Xiong, Wenxian Yu*

Main category: cs.CV

TL;DR: The paper proposes a rotation-equivariant convolution framework (PreCM) to improve semantic segmentation performance under arbitrary imaging angles, validated by a new metric (RD) and showing significant IOU improvements.


<details>
  <summary>Details</summary>
Motivation: Addressing the lack of rotation equivariance in CNN-based semantic segmentation networks, which struggle with diverse object orientations in practical scenarios like remote sensing and medical imaging.

Method: Develops a universal convolution-group framework and PreCM, a padding-based rotation-equivariant convolution mode, applicable to multi-scale images and various convolution types. Introduces RD as a new evaluation metric.

Result: PreCM-based versions of six networks show average IOU improvements of 4.53% to 10.63% and reduced RD values by 3.43% to 4.56% under random angle rotation.

Conclusion: PreCM effectively enhances rotation equivariance in semantic segmentation, improving performance across diverse datasets and network architectures.

Abstract: Semantic segmentation is an important branch of image processing and computer
vision. With the popularity of deep learning, various convolutional neural
networks have been proposed for pixel-level classification and segmentation
tasks. In practical scenarios, however, imaging angles are often arbitrary,
encompassing instances such as water body images from remote sensing and
capillary and polyp images in the medical domain, where prior orientation
information is typically unavailable to guide these networks to extract more
effective features. In this case, learning features from objects with diverse
orientation information poses a significant challenge, as the majority of
CNN-based semantic segmentation networks lack rotation equivariance to resist
the disturbance from orientation information. To address this challenge, this
paper first constructs a universal convolution-group framework aimed at more
fully utilizing orientation information and equipping the network with rotation
equivariance. Subsequently, we mathematically design a padding-based rotation
equivariant convolution mode (PreCM), which is not only applicable to
multi-scale images and convolutional kernels but can also serve as a
replacement component for various types of convolutions, such as dilated
convolutions, transposed convolutions, and asymmetric convolution. To
quantitatively assess the impact of image rotation in semantic segmentation
tasks, we also propose a new evaluation metric, Rotation Difference (RD). The
replacement experiments related to six existing semantic segmentation networks
on three datasets show that, the average Intersection Over Union (IOU) of their
PreCM-based versions respectively improve 6.91%, 10.63%, 4.53%, 5.93%, 7.48%,
8.33% compared to their original versions in terms of random angle rotation.
And the average RD values are decreased by 3.58%, 4.56%, 3.47%, 3.66%, 3.47%,
3.43% respectively.

</details>


### [161] [ColorEdit: Training-free Image-Guided Color editing with diffusion model](https://arxiv.org/abs/2411.10232)
*Xingxi Yin, Zhi Li, Jingfeng Zhang, Chenglin Li, Yin Zhang*

Main category: cs.CV

TL;DR: The paper addresses color misalignment in text-guided image editing by analyzing cross-attention in diffusion models, proposing a method for stable color adjustment, and introducing a benchmark dataset (COLORBENCH).


<details>
  <summary>Details</summary>
Motivation: Text-guided image editing often fails to accurately change object colors due to attention leakage and cross-attention map collisions, leading to misalignment with text prompts.

Method: Analyzes cross-attention blocks in diffusion models, identifies early-stage object representation in up-blocks, and aligns value matrices for color adjustment without fine-tuning.

Result: Proposed method effectively edits object colors, outperforming existing text-guided approaches in synthesized and real images.

Conclusion: The method provides a stable solution for color editing in diffusion models, validated by experiments and the new COLORBENCH dataset.

Abstract: Text-to-image (T2I) diffusion models, with their impressive generative
capabilities, have been adopted for image editing tasks, demonstrating
remarkable efficacy. However, due to attention leakage and collision between
the cross-attention map of the object and the new color attribute from the text
prompt, text-guided image editing methods may fail to change the color of an
object, resulting in a misalignment between the resulting image and the text
prompt. In this paper, we conduct an in-depth analysis on the process of
text-guided image synthesizing and what semantic information different
cross-attention blocks have learned. We observe that the visual representation
of an object is determined in the up-block of the diffusion model in the early
stage of the denoising process, and color adjustment can be achieved through
value matrices alignment in the cross-attention layer. Based on our findings,
we propose a straightforward, yet stable, and effective image-guided method to
modify the color of an object without requiring any additional fine-tuning or
training. Lastly, we present a benchmark dataset called COLORBENCH, the first
benchmark to evaluate the performance of color change methods. Extensive
experiments validate the effectiveness of our method in object-level color
editing and surpass the performance of popular text-guided image editing
approaches in both synthesized and real images.

</details>


### [162] [Type-R: Automatically Retouching Typos for Text-to-Image Generation](https://arxiv.org/abs/2411.18159)
*Wataru Shimoda, Naoto Inoue, Daichi Haraguchi, Hayato Mitani, Seiichi Uchida, Kota Yamaguchi*

Main category: cs.CV

TL;DR: Type-R improves text rendering in text-to-image models by correcting typos and regenerating text boxes post-generation, achieving high accuracy without compromising image quality.


<details>
  <summary>Details</summary>
Motivation: Current text-to-image models struggle with accurately rendering words in generated images, necessitating a post-processing solution.

Method: Type-R identifies and corrects typographical errors, erases incorrect text, regenerates text boxes, and fixes typos in rendered words.

Result: Combined with models like Stable Diffusion or Flux, Type-R achieves the highest text rendering accuracy while maintaining image quality.

Conclusion: Type-R outperforms baselines in balancing text accuracy and image quality, offering a robust solution for text rendering in generated images.

Abstract: While recent text-to-image models can generate photorealistic images from
text prompts that reflect detailed instructions, they still face significant
challenges in accurately rendering words in the image. In this paper, we
propose to retouch erroneous text renderings in the post-processing pipeline.
Our approach, called Type-R, identifies typographical errors in the generated
image, erases the erroneous text, regenerates text boxes for missing words, and
finally corrects typos in the rendered words. Through extensive experiments, we
show that Type-R, in combination with the latest text-to-image models such as
Stable Diffusion or Flux, achieves the highest text rendering accuracy while
maintaining image quality and also outperforms text-focused generation
baselines in terms of balancing text accuracy and image quality.

</details>


### [163] [HOT3D: Hand and Object Tracking in 3D from Egocentric Multi-View Videos](https://arxiv.org/abs/2411.19167)
*Prithviraj Banerjee, Sindi Shkodrani, Pierre Moulon, Shreyas Hampali, Shangchen Han, Fan Zhang, Linguang Zhang, Jade Fountain, Edward Miller, Selen Basol, Richard Newcombe, Robert Wang, Jakob Julian Engel, Tomas Hodan*

Main category: cs.CV

TL;DR: HOT3D is a new dataset for 3D hand and object tracking, featuring diverse interactions and multi-view data, outperforming single-view methods in benchmarks.


<details>
  <summary>Details</summary>
Motivation: To provide a comprehensive dataset for egocentric hand and object tracking in 3D, enabling research in multi-view methods for tasks like hand tracking and object pose estimation.

Method: The dataset includes synchronized multi-view RGB/monochrome images, eye gaze, scene point clouds, and 3D poses, recorded using Meta's Project Aria and Quest 3 headsets with motion-capture ground truth.

Result: Multi-view methods benchmarked on HOT3D significantly outperform single-view counterparts in tasks like 3D hand tracking and object pose estimation.

Conclusion: HOT3D is a valuable resource for advancing research in egocentric 3D tracking, demonstrating the superiority of multi-view approaches.

Abstract: We introduce HOT3D, a publicly available dataset for egocentric hand and
object tracking in 3D. The dataset offers over 833 minutes (3.7M+ images) of
recordings that feature 19 subjects interacting with 33 diverse rigid objects.
In addition to simple pick-up, observe, and put-down actions, the subjects
perform actions typical for a kitchen, office, and living room environment. The
recordings include multiple synchronized data streams containing egocentric
multi-view RGB/monochrome images, eye gaze signal, scene point clouds, and 3D
poses of cameras, hands, and objects. The dataset is recorded with two headsets
from Meta: Project Aria, which is a research prototype of AI glasses, and Quest
3, a virtual-reality headset that has shipped millions of units. Ground-truth
poses were obtained by a motion-capture system using small optical markers
attached to hands and objects. Hand annotations are provided in the UmeTrack
and MANO formats, and objects are represented by 3D meshes with PBR materials
obtained by an in-house scanner. In our experiments, we demonstrate the
effectiveness of multi-view egocentric data for three popular tasks: 3D hand
tracking, model-based 6DoF object pose estimation, and 3D lifting of unknown
in-hand objects. The evaluated multi-view methods, whose benchmarking is
uniquely enabled by HOT3D, significantly outperform their single-view
counterparts.

</details>


### [164] [FILA: Fine-Grained Vision Language Models](https://arxiv.org/abs/2412.08378)
*Shiding Zhu, Wenhui Dong, Jun Song, Yingbo Wang, Yanan Guo, Bo Zheng*

Main category: cs.CV

TL;DR: HyViLM improves high-resolution image processing in MLLMs by introducing a Hybrid Encoder and optimal feature fusion, outperforming state-of-the-art models in most tasks.


<details>
  <summary>Details</summary>
Motivation: Existing methods truncate objects in high-resolution images due to dynamic cropping, causing semantic breaks.

Method: Introduces a Hybrid Encoder for sub-image and global feature encoding, and an optimal feature fusion strategy.

Result: HyViLM outperforms state-of-the-art MLLMs in 9/10 tasks, with 9.6% and 6.9% improvements on TextVQA and DocVQA, respectively.

Conclusion: HyViLM effectively retains context and improves performance in high-resolution image processing for MLLMs.

Abstract: Recently, there has been growing interest in the capability of multimodal
large language models (MLLMs) to process high-resolution images. A common
approach currently involves dynamically cropping the original high-resolution
image into smaller sub-images, which are then fed into a vision encoder that
was pre-trained on lower-resolution images. However, this cropping approach
often truncates objects and connected areas in the original image, causing
semantic breaks. To address this limitation, we introduce HyViLM, designed to
process images of any resolution while retaining the overall context during
encoding. Specifically, we: (i) Design a new visual encoder called Hybrid
Encoder that not only encodes individual sub-images but also interacts with
detailed global visual features, significantly improving the model's ability to
encode high-resolution images. (ii) Propose an optimal feature fusion strategy
for the dynamic cropping approach, effectively leveraging information from
different layers of the vision encoder. Compared with the state-of-the-art
MLLMs under the same setting, our HyViLM outperforms existing MLLMs in nine out
of ten tasks. Specifically, HyViLM achieves a 9.6% improvement in performance
on the TextVQA task and a 6.9% enhancement on the DocVQA task.

</details>


### [165] [PANGAEA: A Global and Inclusive Benchmark for Geospatial Foundation Models](https://arxiv.org/abs/2412.04204)
*Valerio Marsocci, Yuru Jia, Georges Le Bellier, David Kerekes, Liang Zeng, Sebastian Hafner, Sebastian Gerard, Eric Brune, Ritu Yadav, Ali Shibli, Heng Fang, Yifang Ban, Maarten Vergauwen, Nicolas Audebert, Andrea Nascetti*

Main category: cs.CV

TL;DR: PANGAEA introduces a standardized evaluation protocol for Geospatial Foundation Models (GFMs) to address inconsistent and narrow assessments, revealing GFMs often don't outperform supervised models.


<details>
  <summary>Details</summary>
Motivation: Current GFM evaluations are inconsistent, narrow, and geographically biased, limiting real-world applicability.

Method: PANGAEA provides a diverse benchmark covering datasets, tasks, resolutions, sensor types, and temporalities, evaluating popular GFMs against supervised baselines.

Result: GFMs do not consistently outperform supervised models, highlighting their limitations.

Conclusion: PANGAEA offers an extensible, standardized benchmark to improve GFM evaluation, with code released for community use.

Abstract: Geospatial Foundation Models (GFMs) have emerged as powerful tools for
extracting representations from Earth observation data, but their evaluation
remains inconsistent and narrow. Existing works often evaluate on suboptimal
downstream datasets and tasks, that are often too easy or too narrow, limiting
the usefulness of the evaluations to assess the real-world applicability of
GFMs. Additionally, there is a distinct lack of diversity in current evaluation
protocols, which fail to account for the multiplicity of image resolutions,
sensor types, and temporalities, which further complicates the assessment of
GFM performance. In particular, most existing benchmarks are geographically
biased towards North America and Europe, questioning the global applicability
of GFMs. To overcome these challenges, we introduce PANGAEA, a standardized
evaluation protocol that covers a diverse set of datasets, tasks, resolutions,
sensor modalities, and temporalities. It establishes a robust and widely
applicable benchmark for GFMs. We evaluate the most popular GFMs openly
available on this benchmark and analyze their performance across several
domains. In particular, we compare these models to supervised baselines (e.g.
UNet and vanilla ViT), and assess their effectiveness when faced with limited
labeled data. Our findings highlight the limitations of GFMs, under different
scenarios, showing that they do not consistently outperform supervised models.
PANGAEA is designed to be highly extensible, allowing for the seamless
inclusion of new datasets, models, and tasks in future research. By releasing
the evaluation code and benchmark, we aim to enable other researchers to
replicate our experiments and build upon our work, fostering a more principled
evaluation protocol for large pre-trained geospatial models. The code is
available at https://github.com/VMarsocci/pangaea-bench.

</details>


### [166] [Not Just Text: Uncovering Vision Modality Typographic Threats in Image Generation Models](https://arxiv.org/abs/2412.05538)
*Hao Cheng, Erjia Xiao, Jiayan Yang, Jiahang Cao, Qiang Zhang, Jize Zhang, Kaidi Xu, Jindong Gu, Renjing Xu*

Main category: cs.CV

TL;DR: The paper highlights vulnerabilities in image generation models, particularly in the vision modality, using typographic attacks. It evaluates existing defenses and introduces the VMT-IGMs dataset for benchmarking.


<details>
  <summary>Details</summary>
Motivation: Addressing the overlooked security risks in the vision modality of image generation models, which can be exploited to infringe on image owners' rights.

Method: Employs typographic attacks to test model vulnerabilities and evaluates existing defensive strategies.

Result: Reveals susceptibility of models to vision modality threats and ineffectiveness of current defenses.

Conclusion: Proposes the VMT-IGMs dataset to benchmark vision modality vulnerabilities, urging improved defenses.

Abstract: Current image generation models can effortlessly produce high-quality, highly
realistic images, but this also increases the risk of misuse. In various
Text-to-Image or Image-to-Image tasks, attackers can generate a series of
images containing inappropriate content by simply editing the language modality
input. To mitigate this security concern, numerous guarding or defensive
strategies have been proposed, with a particular emphasis on safeguarding
language modality. However, in practical applications, threats in the vision
modality, particularly in tasks involving the editing of real-world images,
present heightened security risks as they can easily infringe upon the rights
of the image owner. Therefore, this paper employs a method named typographic
attack to reveal that various image generation models are also susceptible to
threats within the vision modality. Furthermore, we also evaluate the defense
performance of various existing methods when facing threats in the vision
modality and uncover their ineffectiveness. Finally, we propose the Vision
Modal Threats in Image Generation Models (VMT-IGMs) dataset, which would serve
as a baseline for evaluating the vision modality vulnerability of various image
generation models.

</details>


### [167] [A Hybrid Deep Learning and Model-Checking Framework for Accurate Brain Tumor Detection and Validation](https://arxiv.org/abs/2501.01991)
*Elhoucine Elfatimi, Lahcen El Fatimi, Hanifa Bouchaneb*

Main category: cs.CV

TL;DR: A hybrid framework combining model checking and deep learning improves brain tumor detection in medical imaging, achieving high accuracy and reliability.


<details>
  <summary>Details</summary>
Motivation: To enhance the reliability and accuracy of brain tumor detection and segmentation in medical imaging by integrating formal verification techniques with deep learning.

Method: Integrates model-checking principles with CNN-based feature extraction and K-FCM clustering for segmentation.

Result: Achieves 98% accuracy, 96.15% precision, and 100% recall, demonstrating high effectiveness.

Conclusion: The framework shows strong potential as a robust tool for advanced medical image analysis.

Abstract: Model checking, a formal verification technique, ensures systems meet
predefined requirements, playing a crucial role in minimizing errors and
enhancing quality during development. This paper introduces a novel hybrid
framework integrating model checking with deep learning for brain tumor
detection and validation in medical imaging. By combining model-checking
principles with CNN-based feature extraction and K-FCM clustering for
segmentation, the proposed approach enhances the reliability of tumor detection
and segmentation. Experimental results highlight the framework's effectiveness,
achieving 98\% accuracy, 96.15\% precision, and 100\% recall, demonstrating its
potential as a robust tool for advanced medical image analysis.

</details>


### [168] [Stereo4D: Learning How Things Move in 3D from Internet Stereo Videos](https://arxiv.org/abs/2412.09621)
*Linyi Jin, Richard Tucker, Zhengqi Li, David Fouhey, Noah Snavely, Aleksander Holynski*

Main category: cs.CV

TL;DR: A system for generating high-quality 4D reconstructions from stereoscopic videos to train models for 3D motion and structure prediction.


<details>
  <summary>Details</summary>
Motivation: Direct supervision for 3D motion recovery is challenging due to lack of ground truth annotations.

Method: Fuses camera pose estimation, stereo depth estimation, and temporal tracking to create dynamic 3D reconstructions.

Result: Produces large-scale pseudo-metric 3D point clouds with motion trajectories, improving model generalization.

Conclusion: Training on reconstructed data enables effective generalization to diverse real-world scenes.

Abstract: Learning to understand dynamic 3D scenes from imagery is crucial for
applications ranging from robotics to scene reconstruction. Yet, unlike other
problems where large-scale supervised training has enabled rapid progress,
directly supervising methods for recovering 3D motion remains challenging due
to the fundamental difficulty of obtaining ground truth annotations. We present
a system for mining high-quality 4D reconstructions from internet stereoscopic,
wide-angle videos. Our system fuses and filters the outputs of camera pose
estimation, stereo depth estimation, and temporal tracking methods into
high-quality dynamic 3D reconstructions. We use this method to generate
large-scale data in the form of world-consistent, pseudo-metric 3D point clouds
with long-term motion trajectories. We demonstrate the utility of this data by
training a variant of DUSt3R to predict structure and 3D motion from real-world
image pairs, showing that training on our reconstructed data enables
generalization to diverse real-world scenes. Project page and data at:
https://stereo4d.github.io

</details>


### [169] [Segmentation-Aware Generative Reinforcement Network (GRN) for Tissue Layer Segmentation in 3-D Ultrasound Images for Chronic Low-back Pain (cLBP) Assessment](https://arxiv.org/abs/2501.17690)
*Zixue Zeng, Xiaoyan Zhao, Matthew Cartier, Tong Yu, Jing Wang, Xin Meng, Zhiyu Sheng, Maryam Satarpour, John M Cormack, Allison Bean, Ryan Nussbaum, Maya Maurer, Emily Landis-Walkenhorst, Dinesh Kumbhare, Kang Kim, Ajay Wasan, Jiantao Pu*

Main category: cs.CV

TL;DR: A novel framework, GRN, integrates segmentation loss feedback to optimize image generation and segmentation. Variants GRN-SEL and GRN-SSL reduce labeling efforts by up to 70% while improving performance.


<details>
  <summary>Details</summary>
Motivation: To optimize segmentation performance with less labeled data, reducing annotation burdens in ultrasound image analysis.

Method: GRN integrates segmentation loss feedback; SGE enhances images for segmentation. Variants GRN-SEL and GRN-SSL are developed for sample-efficient and semi-supervised learning.

Result: GRN-SEL with SGE reduces labeling by 70% and improves DSC by 1.98%. Other variants also reduce labeling by 60-70% while maintaining performance.

Conclusion: GRN effectively optimizes segmentation with less labeled data, offering a scalable solution for ultrasound analysis.

Abstract: We introduce a novel segmentation-aware joint training framework called
generative reinforcement network (GRN) that integrates segmentation loss
feedback to optimize both image generation and segmentation performance in a
single stage. An image enhancement technique called segmentation-guided
enhancement (SGE) is also developed, where the generator produces images
tailored specifically for the segmentation model. Two variants of GRN were also
developed, including GRN for sample-efficient learning (GRN-SEL) and GRN for
semi-supervised learning (GRN-SSL). GRN's performance was evaluated using a
dataset of 69 fully annotated 3D ultrasound scans from 29 subjects. The
annotations included six anatomical structures: dermis, superficial fat,
superficial fascial membrane (SFM), deep fat, deep fascial membrane (DFM), and
muscle. Our results show that GRN-SEL with SGE reduces labeling efforts by up
to 70% while achieving a 1.98% improvement in the Dice Similarity Coefficient
(DSC) compared to models trained on fully labeled datasets. GRN-SEL alone
reduces labeling efforts by 60%, GRN-SSL with SGE decreases labeling
requirements by 70%, and GRN-SSL alone by 60%, all while maintaining
performance comparable to fully supervised models. These findings suggest the
effectiveness of the GRN framework in optimizing segmentation performance with
significantly less labeled data, offering a scalable and efficient solution for
ultrasound image analysis and reducing the burdens associated with data
annotation.

</details>


### [170] [Optical aberrations in autonomous driving: Physics-informed parameterized temperature scaling for neural network uncertainty calibration](https://arxiv.org/abs/2412.13695)
*Dominik Werner Wolf, Alexander Braun, Markus Ulrich*

Main category: cs.CV

TL;DR: The paper emphasizes the need for calibrated uncertainties in AI, especially in automotive systems, by addressing dataset shifts caused by optical aberrations. It proposes incorporating physical inductive bias into neural network calibration to improve robustness and trustworthiness.


<details>
  <summary>Details</summary>
Motivation: AI systems, particularly in automotive applications, are vulnerable to dataset shifts like optical aberrations. Ensuring trustworthy uncertainty representation is critical for reliable performance.

Method: The authors integrate a physical inductive bias (Zernike coefficient vector of the optical system) into neural network calibration to mitigate the impact of optical aberrations.

Result: The approach significantly reduces the mean expected calibration error, enhancing the robustness of AI systems against optical aberrations.

Conclusion: The proposed method advances trustworthy uncertainty representation and supports a holistic verification strategy for perception systems.

Abstract: 'A trustworthy representation of uncertainty is desirable and should be
considered as a key feature of any machine learning method' (Huellermeier and
Waegeman, 2021). This conclusion of Huellermeier et al. underpins the
importance of calibrated uncertainties. Since AI-based algorithms are heavily
impacted by dataset shifts, the automotive industry needs to safeguard its
system against all possible contingencies. One important but often neglected
dataset shift is caused by optical aberrations induced by the windshield. For
the verification of the perception system performance, requirements on the AI
performance need to be translated into optical metrics by a bijective mapping.
Given this bijective mapping it is evident that the optical system
characteristics add additional information about the magnitude of the dataset
shift. As a consequence, we propose to incorporate a physical inductive bias
into the neural network calibration architecture to enhance the robustness and
the trustworthiness of the AI target application, which we demonstrate by using
a semantic segmentation task as an example. By utilizing the Zernike
coefficient vector of the optical system as a physical prior we can
significantly reduce the mean expected calibration error in case of optical
aberrations. As a result, we pave the way for a trustworthy uncertainty
representation and for a holistic verification strategy of the perception
chain.

</details>


### [171] [BRIGHT-VO: Brightness-Guided Hybrid Transformer for Visual Odometry with Multi-modality Refinement Module](https://arxiv.org/abs/2501.08659)
*Dongzhihan Wang, Yang Yang, Liang Xu*

Main category: cs.CV

TL;DR: BrightVO is a Transformer-based VO model integrating IMU data for improved accuracy, especially in low-light conditions, outperforming existing methods by 20-259%.


<details>
  <summary>Details</summary>
Motivation: Existing VO methods struggle in low-light due to poor feature visibility. BrightVO aims to address this by combining visual and IMU data.

Method: Uses Transformer for feature extraction and a multi-modality refinement module with IMU data and pose graph optimization.

Result: Achieves 20% better accuracy in normal light and 259% in low-light on KiC4R and KITTI benchmarks.

Conclusion: BrightVO is a robust, open-source solution for VO, excelling in challenging lighting conditions.

Abstract: Visual odometry (VO) plays a crucial role in autonomous driving, robotic
navigation, and other related tasks by estimating the position and orientation
of a camera based on visual input. Significant progress has been made in
data-driven VO methods, particularly those leveraging deep learning techniques
to extract image features and estimate camera poses. However, these methods
often struggle in low-light conditions because of the reduced visibility of
features and the increased difficulty of matching keypoints. To address this
limitation, we introduce BrightVO, a novel VO model based on Transformer
architecture, which not only performs front-end visual feature extraction, but
also incorporates a multi-modality refinement module in the back-end that
integrates Inertial Measurement Unit (IMU) data. Using pose graph optimization,
this module iteratively refines pose estimates to reduce errors and improve
both accuracy and robustness. Furthermore, we create a synthetic low-light
dataset, KiC4R, which includes a variety of lighting conditions to facilitate
the training and evaluation of VO frameworks in challenging environments.
Experimental results demonstrate that BrightVO achieves state-of-the-art
performance on both the KiC4R dataset and the KITTI benchmarks. Specifically,
it provides an average improvement of 20% in pose estimation accuracy in normal
outdoor environments and 259% in low-light conditions, outperforming existing
methods. For widespread use and further development, the research work is fully
open-source at https://github.com/Anastasiawd/BrightVO.

</details>


### [172] [Rethinking Pseudo-Label Guided Learning for Weakly Supervised Temporal Action Localization from the Perspective of Noise Correction](https://arxiv.org/abs/2501.11124)
*Quan Zhang, Yuxin Qi, Xi Tang, Rui Yuan, Xi Lin, Ke Zhang, Chun Yuan*

Main category: cs.CV

TL;DR: A two-stage noisy label learning strategy improves pseudo-label quality for weakly-supervised temporal action localization, addressing boundary inaccuracies, missing short clips, and incorrect segment merging.


<details>
  <summary>Details</summary>
Motivation: Noise in pseudo-labels from weakly-supervised models harms fully-supervised detection head performance, causing boundary inaccuracies, undetected short actions, and incorrect segment merging.

Method: Proposes a frame-level pseudo-label generation model with context-aware denoising and an online-revised teacher-student framework with missing instance compensation and ambiguous instance correction modules.

Result: Outperforms state-of-the-art methods in detection accuracy and inference speed on THUMOS14 and ActivityNet v1.2 benchmarks.

Conclusion: The two-stage strategy effectively mitigates pseudo-label noise, enhancing temporal action localization performance.

Abstract: Pseudo-label learning methods have been widely applied in weakly-supervised
temporal action localization. Existing works directly utilize weakly-supervised
base model to generate instance-level pseudo-labels for training the
fully-supervised detection head. We argue that the noise in pseudo-labels would
interfere with the learning of fully-supervised detection head, leading to
significant performance leakage. Issues with noisy labels include:(1)
inaccurate boundary localization; (2) undetected short action clips; (3)
multiple adjacent segments incorrectly detected as one segment. To target these
issues, we introduce a two-stage noisy label learning strategy to harness every
potential useful signal in noisy labels. First, we propose a frame-level
pseudo-label generation model with a context-aware denoising algorithm to
refine the boundaries. Second, we introduce an online-revised teacher-student
framework with a missing instance compensation module and an ambiguous instance
correction module to solve the short-action-missing and many-to-one problems.
Besides, we apply a high-quality pseudo-label mining loss in our online-revised
teacher-student framework to add different weights to the noisy labels to train
more effectively. Our model outperforms the previous state-of-the-art method in
detection accuracy and inference speed greatly upon the THUMOS14 and
ActivityNet v1.2 benchmarks.

</details>


### [173] [Multi-view Structural Convolution Network for Domain-Invariant Point Cloud Recognition of Autonomous Vehicles](https://arxiv.org/abs/2501.16289)
*Younggun Kim, Beomsik Cho, Seonghoon Ryoo, Soomok Lee*

Main category: cs.CV

TL;DR: The paper introduces MSCN, a domain-invariant point cloud recognition network, addressing dataset and sensor variability challenges.


<details>
  <summary>Details</summary>
Motivation: Variability in point cloud datasets and sensor technologies complicates deep learning adaptation, necessitating adaptive techniques for consistent accuracy.

Method: MSCN uses Structural Convolution Layers (SCL) for local geometric features and Structural Aggregation Layers (SAL) for local and overall context, trained with unseen domain data for robustness.

Result: MSCN achieves domain-invariant features and consistent performance across datasets, adapting to diverse sensors without parameter tweaks.

Conclusion: MSCN enhances reliability and domain invariance in point cloud recognition, with potential for broad applicability.

Abstract: Point cloud representation has recently become a research hotspot in the
field of computer vision and has been utilized for autonomous vehicles.
However, adapting deep learning networks for point cloud data recognition is
challenging due to the variability in datasets and sensor technologies. This
variability underscores the necessity for adaptive techniques to maintain
accuracy under different conditions. In this paper, we present the Multi-View
Structural Convolution Network (MSCN) designed for domain-invariant point cloud
recognition. MSCN comprises Structural Convolution Layers (SCL) that extract
local context geometric features from point clouds and Structural Aggregation
Layers (SAL) that extract and aggregate both local and overall context features
from point clouds. Additionally, our MSCN enhances feature representation
robustness by training with unseen domain point clouds derived from source
domain point clouds. This method acquires domain-invariant features and
exhibits robust, consistent performance across various point cloud datasets,
ensuring compatibility with diverse sensor configurations without the need for
parameter adjustments. This highlights MSCN's potential to significantly
improve the reliability and domain invariant features in different
environments. Our code is available at https://github.com/MLMLab/MSCN.

</details>


### [174] [Towards Understanding Depth Perception in Foveated Rendering](https://arxiv.org/abs/2501.18635)
*Sophie Kergaßner, Taimoor Tariq, Piotr Didyk*

Main category: cs.CV

TL;DR: Foveated rendering's impact on stereoscopic depth perception is studied, showing no negative effects and even potential improvements with peripheral blur.


<details>
  <summary>Details</summary>
Motivation: To understand how foveated rendering affects stereoscopic depth perception, a critical aspect of visual realism, as current methods focus on spatial resolution but neglect depth cues.

Method: A psychovisual experiment was designed to quantitatively assess the effects of peripheral blur on depth perception, followed by deriving a perceptual model and validating it with natural stimuli.

Result: Stereoscopic acuity remains unaffected or improves with high peripheral blur, allowing up to 2x stronger foveation than commonly used without impacting depth perception.

Conclusion: Foveated rendering does not harm stereoscopic depth perception, enabling more aggressive foveation for computational savings while maintaining visual realism.

Abstract: The true vision for real-time virtual and augmented reality is reproducing
our visual reality in its entirety on immersive displays. To this end, foveated
rendering leverages the limitations of spatial acuity in human peripheral
vision to allocate computational resources to the fovea while reducing quality
in the periphery. Such methods are often derived from studies on the spatial
resolution of the human visual system and its ability to perceive blur in the
periphery, enabling the potential for high spatial quality in real-time.
However, the effects of blur on other visual cues that depend on luminance
contrast, such as depth, remain largely unexplored. It is critical to
understand this interplay, as accurate depth representation is a fundamental
aspect of visual realism. In this paper, we present the first evaluation
exploring the effects of foveated rendering on stereoscopic depth perception.
We design a psychovisual experiment to quantitatively study the effects of
peripheral blur on depth perception. Our analysis demonstrates that
stereoscopic acuity remains unaffected (or even improves) by high levels of
peripheral blur. Based on our studies, we derive a simple perceptual model that
determines the amount of foveation that does not affect stereoacuity.
Furthermore, we analyze the model in the context of common foveation practices
reported in literature. The findings indicate that foveated rendering does not
impact stereoscopic depth perception, and stereoacuity remains unaffected with
up to 2x stronger foveation than commonly used. Finally, we conduct a
validation experiment and show that our findings hold for complex natural
stimuli.

</details>


### [175] [IMDPrompter: Adapting SAM to Image Manipulation Detection by Cross-View Automated Prompt Learning](https://arxiv.org/abs/2502.02454)
*Quan Zhang, Yuxin Qi, Xi Tang, Jinwei Fang, Xi Lin, Ke Zhang, Chun Yuan*

Main category: cs.CV

TL;DR: The paper introduces IMDPrompter, a cross-view prompt learning paradigm for SAM to address challenges in image manipulation detection, achieving automated detection and improved generalization.


<details>
  <summary>Details</summary>
Motivation: SAM's performance in image manipulation detection is unexplored, with challenges like manual prompt reliance and single-view limitations.

Method: Develops IMDPrompter with automated prompts and components like Cross-view Feature Perception and Optimal Prompt Selection.

Result: Validated on five datasets (CASIA, Columbia, Coverage, IMD2020, NIST16), showing effectiveness.

Conclusion: IMDPrompter successfully addresses SAM's limitations in image manipulation detection, enabling automated and accurate results.

Abstract: Using extensive training data from SA-1B, the Segment Anything Model (SAM)
has demonstrated exceptional generalization and zero-shot capabilities,
attracting widespread attention in areas such as medical image segmentation and
remote sensing image segmentation. However, its performance in the field of
image manipulation detection remains largely unexplored and unconfirmed. There
are two main challenges in applying SAM to image manipulation detection: a)
reliance on manual prompts, and b) the difficulty of single-view information in
supporting cross-dataset generalization. To address these challenges, we
develops a cross-view prompt learning paradigm called IMDPrompter based on SAM.
Benefiting from the design of automated prompts, IMDPrompter no longer relies
on manual guidance, enabling automated detection and localization.
Additionally, we propose components such as Cross-view Feature Perception,
Optimal Prompt Selection, and Cross-View Prompt Consistency, which facilitate
cross-view perceptual learning and guide SAM to generate accurate masks.
Extensive experimental results from five datasets (CASIA, Columbia, Coverage,
IMD2020, and NIST16) validate the effectiveness of our proposed method.

</details>


### [176] [Visual Adaptive Prompting for Compositional Zero-Shot Learning](https://arxiv.org/abs/2502.20292)
*Kyle Stein, Arash Mahyari, Guillermo Francia, Eman El-Sheikh*

Main category: cs.CV

TL;DR: The paper introduces Visual Adaptive Prompting System (VAPS) for Compositional Zero-Shot Learning (CZSL), leveraging dynamic visual prompts to bridge semantic and visual features, achieving state-of-the-art results.


<details>
  <summary>Details</summary>
Motivation: Current prompting methods for CZSL focus on text adaptation, ignoring visual context. VAPS aims to address this gap by dynamically adapting prompts based on visual features.

Method: VAPS uses a learnable visual prompt repository and similarity-based retrieval to select relevant prompts for attributes and objects, guided by visual features. It includes a visual prompt adapter for better embedding space generalization.

Result: Experiments on three CZSL benchmarks show state-of-the-art performance in both closed and open-world scenarios.

Conclusion: VAPS effectively bridges visual and semantic features, improving CZSL performance by dynamically adapting prompts to visual contexts.

Abstract: Vision-Language Models (VLMs) have demonstrated impressive capabilities in
learning joint representations of visual and textual data, making them powerful
tools for tasks such as Compositional Zero-Shot Learning (CZSL). CZSL requires
models to generalize to novel combinations of visual primitives-such as
attributes and objects-that were not explicitly encountered during training.
Recent works in prompting for CZSL have focused on modifying inputs for the
text encoder, often using static prompts that do not change across varying
visual contexts. However, these approaches struggle to fully capture varying
visual contexts, as they focus on text adaptation rather than leveraging visual
features for compositional reasoning. To address this, we propose Visual
Adaptive Prompting System (VAPS) that leverages a learnable visual prompt
repository and similarity-based retrieval mechanism within the framework of
VLMs to bridge the gap between semantic and visual features. Our method
introduces a dynamic visual prompt repository mechanism that selects the most
relevant attribute and object prompts based on the visual features of the
image. Our proposed system includes a visual prompt adapter that encourages the
model to learn a more generalizable embedding space. Experiments on three CZSL
benchmarks, across both closed and open-world scenarios, demonstrate
state-of-the-art results.

</details>


### [177] [GATE3D: Generalized Attention-based Task-synergized Estimation in 3D*](https://arxiv.org/abs/2504.11014)
*Eunsoo Im, Changhyun Jee, Jung Kwon Lee*

Main category: cs.CV

TL;DR: GATE3D is a weakly supervised framework for generalized monocular 3D object detection, addressing dataset scarcity and domain gaps via pseudo-labels and consistency losses.


<details>
  <summary>Details</summary>
Motivation: The scarcity of 3D-annotated datasets and domain biases in monocular 3D object detection hinder generalization across diverse environments.

Method: GATE3D employs weak supervision and consistency losses between 2D and 3D predictions to bridge domain gaps.

Result: Competitive performance on KITTI and an indoor-office dataset, demonstrating effective generalization and learning from limited data.

Conclusion: GATE3D shows promise for robotics, AR, and VR applications by improving generalization in 3D object detection.

Abstract: The emerging trend in computer vision emphasizes developing universal models
capable of simultaneously addressing multiple diverse tasks. Such universality
typically requires joint training across multi-domain datasets to ensure
effective generalization. However, monocular 3D object detection presents
unique challenges in multi-domain training due to the scarcity of datasets
annotated with accurate 3D ground-truth labels, especially beyond typical
road-based autonomous driving contexts. To address this challenge, we introduce
a novel weakly supervised framework leveraging pseudo-labels. Current
pretrained models often struggle to accurately detect pedestrians in non-road
environments due to inherent dataset biases. Unlike generalized image-based 2D
object detection models, achieving similar generalization in monocular 3D
detection remains largely unexplored. In this paper, we propose GATE3D, a novel
framework designed specifically for generalized monocular 3D object detection
via weak supervision. GATE3D effectively bridges domain gaps by employing
consistency losses between 2D and 3D predictions. Remarkably, our model
achieves competitive performance on the KITTI benchmark as well as on an
indoor-office dataset collected by us to evaluate the generalization
capabilities of our framework. Our results demonstrate that GATE3D
significantly accelerates learning from limited annotated data through
effective pre-training strategies, highlighting substantial potential for
broader impacts in robotics, augmented reality, and virtual reality
applications. Project page: https://ies0411.github.io/GATE3D/

</details>


### [178] [Vision Transformers on the Edge: A Comprehensive Survey of Model Compression and Acceleration Strategies](https://arxiv.org/abs/2503.02891)
*Shaibal Saha, Lanyu Xu*

Main category: cs.CV

TL;DR: A survey on optimizing vision transformers (ViTs) for edge deployment, covering model compression, software tools, and hardware acceleration, while analyzing trade-offs in accuracy, efficiency, and adaptability.


<details>
  <summary>Details</summary>
Motivation: ViTs face high computational and memory demands, limiting their deployment on resource-constrained edge devices. This survey aims to systematically review and categorize solutions to these challenges.

Method: The paper provides a structured analysis of model compression techniques, software tools for edge inference, and hardware acceleration strategies for ViTs.

Result: The survey highlights key challenges and emerging research directions for deploying ViTs on edge platforms like GPUs, ASICs, and FPGAs.

Conclusion: The paper serves as a guide to inspire further research on optimizing ViTs for efficient edge deployment, balancing accuracy, efficiency, and hardware adaptability.

Abstract: In recent years, vision transformers (ViTs) have emerged as powerful and
promising techniques for computer vision tasks such as image classification,
object detection, and segmentation. Unlike convolutional neural networks
(CNNs), which rely on hierarchical feature extraction, ViTs treat images as
sequences of patches and leverage self-attention mechanisms. However, their
high computational complexity and memory demands pose significant challenges
for deployment on resource-constrained edge devices. To address these
limitations, extensive research has focused on model compression techniques and
hardware-aware acceleration strategies. Nonetheless, a comprehensive review
that systematically categorizes these techniques and their trade-offs in
accuracy, efficiency, and hardware adaptability for edge deployment remains
lacking. This survey bridges this gap by providing a structured analysis of
model compression techniques, software tools for inference on edge, and
hardware acceleration strategies for ViTs. We discuss their impact on accuracy,
efficiency, and hardware adaptability, highlighting key challenges and emerging
research directions to advance ViT deployment on edge platforms, including
graphics processing units (GPUs), application-specific integrated circuit
(ASICs), and field-programmable gate arrays (FPGAs). The goal is to inspire
further research with a contemporary guide on optimizing ViTs for efficient
deployment on edge devices.

</details>


### [179] [Leveraging Motion Information for Better Self-Supervised Video Correspondence Learning](https://arxiv.org/abs/2503.12026)
*Zihan Zhou, Changrui Dai, Aibo Song, Xiaolin Fang*

Main category: cs.CV

TL;DR: A self-supervised framework (MER) improves video correspondence learning by enhancing motion capture and flexible sampling, outperforming existing methods.


<details>
  <summary>Details</summary>
Motivation: Reliable pixel matching in videos without supervision is challenging, and current methods struggle with false matches.

Method: MER includes a Motion Enhancement Engine for dynamic motion capture and a Multi-Cluster Sampler for flexible pixel correspondence sampling.

Result: MER outperforms state-of-the-art methods in tasks like video object segmentation and keypoint tracking.

Conclusion: The proposed framework effectively addresses challenges in self-supervised video correspondence learning.

Abstract: Self-supervised video correspondence learning depends on the ability to
accurately associate pixels between video frames that correspond to the same
visual object. However, achieving reliable pixel matching without supervision
remains a major challenge. To address this issue, recent research has focused
on feature learning techniques that aim to encode unique pixel representations
for matching. Despite these advances, existing methods still struggle to
achieve exact pixel correspondences and often suffer from false matches,
limiting their effectiveness in self-supervised settings.
  To this end, we explore an efficient self-supervised Video Correspondence
Learning framework (MER) that aims to accurately extract object details from
unlabeled videos. First, we design a dedicated Motion Enhancement Engine that
emphasizes capturing the dynamic motion of objects in videos. In addition, we
introduce a flexible sampling strategy for inter-pixel correspondence
information (Multi-Cluster Sampler) that enables the model to pay more
attention to the pixel changes of important objects in motion. Through
experiments, our algorithm outperforms the state-of-the-art competitors on
video correspondence learning tasks such as video object segmentation and video
object keypoint tracking.

</details>


### [180] [WideRange4D: Enabling High-Quality 4D Reconstruction with Wide-Range Movements and Scenes](https://arxiv.org/abs/2503.13435)
*Ling Yang, Kaixin Zhu, Juanxi Tian, Bohan Zeng, Mingbao Lin, Hongjuan Pei, Wentao Zhang, Shuicheng Yan*

Main category: cs.CV

TL;DR: The paper introduces WideRange4D, a benchmark for 4D reconstruction with large spatial movements, and Progress4D, a method to address limitations in existing techniques.


<details>
  <summary>Details</summary>
Motivation: Existing 4D reconstruction methods and datasets are limited to in-place actions and struggle with wide-range spatial movements.

Method: Proposes WideRange4D benchmark and Progress4D method, leveraging deformation fields for stable, high-quality 4D reconstruction.

Result: Progress4D outperforms state-of-the-art methods in quantitative and qualitative evaluations on WideRange4D.

Conclusion: The work advances 4D reconstruction by addressing wide-range spatial movements and providing a robust benchmark and method.

Abstract: With the rapid development of 3D reconstruction technology, research in 4D
reconstruction is also advancing, existing 4D reconstruction methods can
generate high-quality 4D scenes. However, due to the challenges in acquiring
multi-view video data, the current 4D reconstruction benchmarks mainly display
actions performed in place, such as dancing, within limited scenarios. In
practical scenarios, many scenes involve wide-range spatial movements,
highlighting the limitations of existing 4D reconstruction datasets.
Additionally, existing 4D reconstruction methods rely on deformation fields to
estimate the dynamics of 3D objects, but deformation fields struggle with
wide-range spatial movements, which limits the ability to achieve high-quality
4D scene reconstruction with wide-range spatial movements. In this paper, we
focus on 4D scene reconstruction with significant object spatial movements and
propose a novel 4D reconstruction benchmark, WideRange4D. This benchmark
includes rich 4D scene data with large spatial variations, allowing for a more
comprehensive evaluation of the generation capabilities of 4D generation
methods. Furthermore, we introduce a new 4D reconstruction method, Progress4D,
which generates stable and high-quality 4D results across various complex 4D
scene reconstruction tasks. We conduct both quantitative and qualitative
comparison experiments on WideRange4D, showing that our Progress4D outperforms
existing state-of-the-art 4D reconstruction methods. Project:
https://github.com/Gen-Verse/WideRange4D

</details>


### [181] [BiPrompt-SAM: Enhancing Image Segmentation via Explicit Selection between Point and Text Prompts](https://arxiv.org/abs/2503.19769)
*Suzhe Xu, Jialin Peng, Chengyuan Zhang*

Main category: cs.CV

TL;DR: BiPrompt-SAM combines point and text prompts for segmentation, using a selection mechanism to align masks spatially and semantically, achieving strong zero-shot performance without domain-specific training.


<details>
  <summary>Details</summary>
Motivation: The challenge of effectively combining point-prompted and text-prompted segmentation methods to leverage their complementary strengths (spatial precision and semantic understanding) motivates this work.

Method: BiPrompt-SAM employs an explicit selection mechanism to align SAM's point-generated masks with text-guided masks (via EVF-SAM and BEIT-3) using IoU, simplifying multi-modal fusion.

Result: Achieves 89.55% mDice and 81.46% mIoU on Endovis17, and 87.1%, 86.5%, 85.8% IoU on RefCOCO, outperforming existing methods.

Conclusion: BiPrompt-SAM offers a simple, effective, and interpretable solution for multi-modal prompt fusion, excelling in spatial accuracy and semantic disambiguation.

Abstract: Segmentation is a fundamental task in computer vision, with prompt-driven
methods gaining prominence due to their flexibility. The Segment Anything Model
(SAM) excels at point-prompted segmentation, while text-based models, often
leveraging powerful multimodal encoders like BEIT-3, provide rich semantic
understanding. However, effectively combining these complementary modalities
remains a challenge. This paper introduces BiPrompt-SAM, a novel dual-modal
prompt segmentation framework employing an explicit selection mechanism. We
leverage SAM's ability to generate multiple mask candidates from a single point
prompt and use a text-guided mask (generated via EVF-SAM with BEIT-3) to select
the point-generated mask that best aligns spatially, measured by Intersection
over Union (IoU). This approach, interpretable as a simplified Mixture of
Experts (MoE), effectively fuses spatial precision and semantic context without
complex model modifications. Notably, our method achieves strong zero-shot
performance on the Endovis17 medical dataset (89.55% mDice, 81.46% mIoU) using
only a single point prompt per instance. This significantly reduces annotation
burden compared to bounding boxes and aligns better with practical clinical
workflows, demonstrating the method's effectiveness without domain-specific
training. On the RefCOCO series, BiPrompt-SAM attained 87.1%, 86.5%, and 85.8%
IoU, significantly outperforming existing approaches. Experiments show
BiPrompt-SAM excels in scenarios requiring both spatial accuracy and semantic
disambiguation, offering a simple, effective, and interpretable perspective on
multi-modal prompt fusion.

</details>


### [182] [GISE-TTT:A Framework for Global InformationSegmentation and Enhancement](https://arxiv.org/abs/2504.00879)
*Fenglei Hao, Yuliang Yang, Ruiyuan Su, Zhengran Zhao, Yukun Qiao, Mengyu Zhu*

Main category: cs.CV

TL;DR: GISE-TTT, a novel architecture integrating Temporal Transformer layers, improves global temporal dependency modeling in Video Object Segmentation, achieving a 3.2% accuracy boost on DAVIS 2017.


<details>
  <summary>Details</summary>
Motivation: Existing architectures struggle with global temporal dependencies in long video sequences for VOS.

Method: Introduces GISE-TTT with Temporal Transformer layers and hierarchical contextual aggregation to refine spatiotemporal dependencies.

Result: 3.2% improvement in segmentation accuracy on DAVIS 2017.

Conclusion: Strategic distribution of global information across network layers is critical for optimal performance in video segmentation.

Abstract: This paper addresses the challenge of capturing global temporaldependencies
in long video sequences for Video Object Segmentation (VOS). Existing
architectures often fail to effectively model these dependencies acrossextended
temporal horizons. To overcome this limitation, we introduce GISE-TTT, anovel
architecture that integrates Temporal Transformer (TTT) layers
intotransformer-based frameworks through a co-designed hierarchical
approach.The TTTlayer systematically condenses historical temporal information
into hidden states thatencode globally coherent contextual representations. By
leveraging multi-stagecontextual aggregation through hierarchical
concatenation, our frameworkprogressively refines spatiotemporal dependencies
across network layers. This designrepresents the first systematic empirical
evidence that distributing global informationacross multiple network layers is
critical for optimal dependency utilization in videosegmentation tasks.Ablation
studies demonstrate that incorporating TTT modules athigh-level feature stages
significantly enhances global modeling capabilities, therebyimproving the
network's ability to capture long-range temporal relationships. Extensive
experiments on DAVIS 2017 show that GISE-TTT achieves a 3.2%improvement in
segmentation accuracy over the baseline model, providingcomprehensive evidence
that global information should be strategically leveragedthroughout the network
architecture.The code will be made available
at:https://github.com/uuool/GISE-TTT.

</details>


### [183] [DyST-XL: Dynamic Layout Planning and Content Control for Compositional Text-to-Video Generation](https://arxiv.org/abs/2504.15032)
*Weijie He, Mushui Liu, Yunlong Yu, Zhao Wang, Chao Wu*

Main category: cs.CV

TL;DR: DyST-XL enhances text-to-video models without training, improving layout, entity consistency, and interaction dynamics using LLM-based planning, controlled attention, and feature propagation.


<details>
  <summary>Details</summary>
Motivation: Addressing challenges like layout discontinuity and entity drift in diffusion-based text-to-video models.

Method: Uses a Dynamic Layout Planner (LLM-driven), Dual-Prompt Controlled Attention, and Entity-Consistency Constraint.

Result: Significantly improves performance on complex prompts, achieving better compositional video synthesis.

Conclusion: DyST-XL bridges a gap in training-free video generation, offering precise control and consistency.

Abstract: Compositional text-to-video generation, which requires synthesizing dynamic
scenes with multiple interacting entities and precise spatial-temporal
relationships, remains a critical challenge for diffusion-based models.
Existing methods struggle with layout discontinuity, entity identity drift, and
implausible interaction dynamics due to unconstrained cross-attention
mechanisms and inadequate physics-aware reasoning. To address these
limitations, we propose DyST-XL, a \textbf{training-free} framework that
enhances off-the-shelf text-to-video models (e.g., CogVideoX-5B) through
frame-aware control. DyST-XL integrates three key innovations: (1) A Dynamic
Layout Planner that leverages large language models (LLMs) to parse input
prompts into entity-attribute graphs and generates physics-aware keyframe
layouts, with intermediate frames interpolated via trajectory optimization; (2)
A Dual-Prompt Controlled Attention Mechanism that enforces localized text-video
alignment through frame-aware attention masking, achieving precise control over
individual entities; and (3) An Entity-Consistency Constraint strategy that
propagates first-frame feature embeddings to subsequent frames during
denoising, preserving object identity without manual annotation. Experiments
demonstrate that DyST-XL excels in compositional text-to-video generation,
significantly improving performance on complex prompts and bridging a crucial
gap in training-free video synthesis. The code is released in
https://github.com/XiaoBuL/DyST-XL.

</details>


### [184] [MoBGS: Motion Deblurring Dynamic 3D Gaussian Splatting for Blurry Monocular Video](https://arxiv.org/abs/2504.15122)
*Minh-Quan Viet Bui, Jongmin Park, Juan Luis Gonzalez Bello, Jaeho Moon, Jihyong Oh, Munchurl Kim*

Main category: cs.CV

TL;DR: MoBGS is a dynamic 3D Gaussian Splatting framework for deblurring monocular videos, improving novel view synthesis by addressing motion blur with novel camera and exposure estimation methods.


<details>
  <summary>Details</summary>
Motivation: Existing dynamic NVS methods struggle with motion blur in videos, degrading rendering quality. MoBGS aims to overcome this by modeling both global camera and local object motion.

Method: Introduces Blur-adaptive Latent Camera Estimation (BLCE) for camera motion deblurring and Latent Camera-induced Exposure Estimation (LCEE) for consistent deblurring of dynamic objects.

Result: Outperforms DyBluRF and Deblur4DGS on the Stereo Blur dataset and real-world videos, achieving state-of-the-art performance.

Conclusion: MoBGS effectively addresses motion blur in dynamic NVS, ensuring high-quality spatio-temporal view reconstruction.

Abstract: We present MoBGS, a novel deblurring dynamic 3D Gaussian Splatting (3DGS)
framework capable of reconstructing sharp and high-quality novel
spatio-temporal views from blurry monocular videos in an end-to-end manner.
Existing dynamic novel view synthesis (NVS) methods are highly sensitive to
motion blur in casually captured videos, resulting in significant degradation
of rendering quality. While recent approaches address motion-blurred inputs for
NVS, they primarily focus on static scene reconstruction and lack dedicated
motion modeling for dynamic objects. To overcome these limitations, our MoBGS
introduces a novel Blur-adaptive Latent Camera Estimation (BLCE) method for
effective latent camera trajectory estimation, improving global camera motion
deblurring. In addition, we propose a physically-inspired Latent Camera-induced
Exposure Estimation (LCEE) method to ensure consistent deblurring of both
global camera and local object motion. Our MoBGS framework ensures the temporal
consistency of unseen latent timestamps and robust motion decomposition of
static and dynamic regions. Extensive experiments on the Stereo Blur dataset
and real-world blurry videos show that our MoBGS significantly outperforms the
very recent advanced methods (DyBluRF and Deblur4DGS), achieving
state-of-the-art performance for dynamic NVS under motion blur.

</details>


### [185] [Naturally Computed Scale Invariance in the Residual Stream of ResNet18](https://arxiv.org/abs/2504.16290)
*André Longon*

Main category: cs.CV

TL;DR: The paper explores how ResNet18 achieves scale invariance in object recognition, focusing on its residual stream, and links neural properties to behavior.


<details>
  <summary>Details</summary>
Motivation: Understanding how neural networks achieve invariance to image-altering variables like scale, which is crucial for robust object recognition.

Method: Investigates ResNet18's residual stream, analyzing convolutional channels for scale invariance and conducting ablation experiments.

Result: Intermediate blocks show scale-invariant properties via residual summation of scale-equivariant representations. Ablation experiments suggest a causal link to scale-robust recognition.

Conclusion: The residual stream in ResNet18 may compute scale invariance, contributing to robust object recognition behavior.

Abstract: An important capacity in visual object recognition is invariance to
image-altering variables which leave the identity of objects unchanged, such as
lighting, rotation, and scale. How do neural networks achieve this? Prior
mechanistic interpretability research has illuminated some invariance-building
circuitry in InceptionV1, but the results are limited and networks with
different architectures have remained largely unexplored. This work
investigates ResNet18 with a particular focus on its residual stream, an
architectural component which InceptionV1 lacks. We observe that many
convolutional channels in intermediate blocks exhibit scale invariant
properties, computed by the element-wise residual summation of scale
equivariant representations: the block input's smaller-scale copy with the
block pre-sum output's larger-scale copy. Through subsequent ablation
experiments, we attempt to causally link these neural properties with
scale-robust object recognition behavior. Our tentative findings suggest how
the residual stream computes scale invariance and its possible role in
behavior. Code is available at:
https://github.com/cest-andre/residual-stream-interp

</details>


### [186] [CountingDINO: A Training-free Pipeline for Class-Agnostic Counting using Unsupervised Backbones](https://arxiv.org/abs/2504.16570)
*Giacomo Pacini, Lorenzo Bianchi, Luca Ciampi, Nicola Messina, Giuseppe Amato, Fabrizio Falchi*

Main category: cs.CV

TL;DR: CountingDINO is a training-free, exemplar-based class-agnostic counting framework using unsupervised features, outperforming baselines on FSC-147.


<details>
  <summary>Details</summary>
Motivation: Current CAC methods rely on labeled data, limiting scalability. CountingDINO eliminates this need.

Method: Uses self-supervised DINO features to extract object prototypes, generating similarity and density maps without annotations.

Result: Outperforms baselines on FSC-147, even surpassing some supervised and non-training-free methods.

Conclusion: Label- and training-free CAC can be scalable and effective, as demonstrated by CountingDINO.

Abstract: Class-agnostic counting (CAC) aims to estimate the number of objects in
images without being restricted to predefined categories. However, while
current exemplar-based CAC methods offer flexibility at inference time, they
still rely heavily on labeled data for training, which limits scalability and
generalization to many downstream use cases. In this paper, we introduce
CountingDINO, the first training-free exemplar-based CAC framework that
exploits a fully unsupervised feature extractor. Specifically, our approach
employs self-supervised vision-only backbones to extract object-aware features,
and it eliminates the need for annotated data throughout the entire proposed
pipeline. At inference time, we extract latent object prototypes via ROI-Align
from DINO features and use them as convolutional kernels to generate similarity
maps. These are then transformed into density maps through a simple yet
effective normalization scheme. We evaluate our approach on the FSC-147
benchmark, where we consistently outperform a baseline based on an SOTA
unsupervised object detector under the same label- and training-free setting.
Additionally, we achieve competitive results -- and in some cases surpass --
training-free methods that rely on supervised backbones, non-training-free
unsupervised methods, as well as several fully supervised SOTA approaches. This
demonstrates that label- and training-free CAC can be both scalable and
effective. Code: https://lorebianchi98.github.io/CountingDINO/.

</details>


### [187] [OPAL: Visibility-aware LiDAR-to-OpenStreetMap Place Recognition via Adaptive Radial Fusion](https://arxiv.org/abs/2504.19258)
*Shuhao Kang, Martin Y. Liao, Yan Xia, Olaf Wysocki, Boris Jutzi, Daniel Cremers*

Main category: cs.CV

TL;DR: OPAL is a novel LiDAR place recognition network using OpenStreetMap (OSM) as a lightweight prior, outperforming state-of-the-art methods in recall and speed.


<details>
  <summary>Details</summary>
Motivation: Existing methods rely on dense maps or aerial imagery, which are storage-heavy and lack real-time adaptability. OPAL addresses this by leveraging OSM.

Method: OPAL uses a cross-modal visibility mask and adaptive radial fusion module to bridge LiDAR scans and OSM data, enhancing feature learning and descriptor creation.

Result: OPAL achieves 15.98% higher recall at @1m threshold and 12x faster inference speed on KITTI and KITTI-360 datasets.

Conclusion: OPAL offers a lightweight, efficient solution for LiDAR place recognition, outperforming existing methods in performance and speed.

Abstract: LiDAR place recognition is a critical capability for autonomous navigation
and cross-modal localization in large-scale outdoor environments. Existing
approaches predominantly depend on pre-built 3D dense maps or aerial imagery,
which impose significant storage overhead and lack real-time adaptability. In
this paper, we propose OPAL, a novel network for LiDAR place recognition that
leverages OpenStreetMap (OSM) as a lightweight and up-to-date prior. Our key
innovation lies in bridging the domain disparity between sparse LiDAR scans and
structured OSM data through two carefully designed components. First, a
cross-modal visibility mask that identifies maximal observable regions from
both modalities to guide feature learning. Second, an adaptive radial fusion
module that dynamically consolidates radial features into discriminative global
descriptors. Extensive experiments on the KITTI and KITTI-360 datasets
demonstrate OPAL's superiority, achieving 15.98% higher recall at @1m threshold
for top-1 retrieved matches, along with 12x faster inference speed compared to
the state-of-the-art approach. Code and datasets will be publicly available.

</details>


### [188] [Measuring Train Driver Performance as Key to Approval of Driverless Trains](https://arxiv.org/abs/2504.19735)
*Rustam Tagiew, Prasannavenkatesh Balaji*

Main category: cs.CV

TL;DR: The paper addresses the lack of data for quantifying obstacle detection performance in driverless trains, providing a new dataset of 711 train driver measurements.


<details>
  <summary>Details</summary>
Motivation: To remedy the deficiency in published measurement results for obstacle detection in driverless train systems.

Method: Collects and analyzes 711 train driver performance measurements from controlled experiments, varying speeds, obstacle sizes, and other factors.

Result: A new public dataset is provided, detailing reaction times and distances to obstacles under various conditions.

Conclusion: The dataset aims to support unbiased research, standardization, and regulation in the field of driverless train safety.

Abstract: Points 2.1.4(b), 2.4.2(b) and 2.4.3(b) in Annex I of Implementing Regulation
(EU) No. 402/2013 allow a simplified approach for the safety approval of
computer vision systems for driverless trains, if they have 'similar' functions
and interfaces as the replaced human driver. The human driver is not replaced
one-to-one by a technical system - only a limited set of cognitive functions
are replaced. However, performance in the most challenging function, obstacle
detection, is difficult to quantify due to the deficiency of published
measurement results. This article summarizes the data published so far. This
article also goes a long way to remedy this situation by providing a new public
and anonymized dataset of 711 train driver performance measurements from
controlled experiments. The measurements are made for different speeds,
obstacle sizes, train protection systems and obstacle color contrasts
respectively. The measured values are reaction time and distance to the
obstacle. The goal of this paper is an unbiased and exhaustive description of
the presented dataset for research, standardization and regulation. The dataset
with supplementing information and literature is published on
https://data.fid-move.de/de/dataset/atosensedata

</details>


### [189] [PixelHacker: Image Inpainting with Structural and Semantic Consistency](https://arxiv.org/abs/2504.20438)
*Ziyang Xu, Kangsheng Duan, Xiaolei Shen, Zhifeng Ding, Wenyu Liu, Xiaohu Ruan, Xiaoxin Chen, Xinggang Wang*

Main category: cs.CV

TL;DR: PixelHacker, a diffusion-based model, introduces latent categories guidance for image inpainting, outperforming SOTA methods with improved structure and semantic consistency.


<details>
  <summary>Details</summary>
Motivation: Addressing challenges in complex structure and semantics in image inpainting, leading to artifacts and inappropriate generation.

Method: Constructs a dataset of 14M image-mask pairs, uses latent categories guidance, and injects features via linear attention in a diffusion model.

Result: Outperforms SOTA on Places2, CelebA-HQ, and FFHQ datasets with remarkable consistency.

Conclusion: PixelHacker sets a new benchmark for image inpainting with its effective latent categories guidance and diffusion-based approach.

Abstract: Image inpainting is a fundamental research area between image editing and
image generation. Recent state-of-the-art (SOTA) methods have explored novel
attention mechanisms, lightweight architectures, and context-aware modeling,
demonstrating impressive performance. However, they often struggle with complex
structure (e.g., texture, shape, spatial relations) and semantics (e.g., color
consistency, object restoration, and logical correctness), leading to artifacts
and inappropriate generation. To address this challenge, we design a simple yet
effective inpainting paradigm called latent categories guidance, and further
propose a diffusion-based model named PixelHacker. Specifically, we first
construct a large dataset containing 14 million image-mask pairs by annotating
foreground and background (potential 116 and 21 categories, respectively).
Then, we encode potential foreground and background representations separately
through two fixed-size embeddings, and intermittently inject these features
into the denoising process via linear attention. Finally, by pre-training on
our dataset and fine-tuning on open-source benchmarks, we obtain PixelHacker.
Extensive experiments show that PixelHacker comprehensively outperforms the
SOTA on a wide range of datasets (Places2, CelebA-HQ, and FFHQ) and exhibits
remarkable consistency in both structure and semantics. Project page at
https://hustvl.github.io/PixelHacker.

</details>


### [190] [DS_FusionNet: Dynamic Dual-Stream Fusion with Bidirectional Knowledge Distillation for Plant Disease Recognition](https://arxiv.org/abs/2504.20948)
*Yanghui Song, Chengfu Yang*

Main category: cs.CV

TL;DR: The paper proposes DS_FusionNet, a dynamic dual-stream fusion network, to improve plant disease recognition accuracy despite challenges like small-sample learning and leaf occlusion. It achieves over 90% accuracy with minimal data.


<details>
  <summary>Details</summary>
Motivation: Addressing challenges in plant disease recognition, such as small-sample learning and high inter-class similarity, to enhance agricultural technology.

Method: DS_FusionNet integrates a dual-backbone architecture, deformable dynamic fusion modules, and bidirectional knowledge distillation.

Result: Achieves >90% accuracy with 10% of datasets and 85% on complex datasets, showing strong generalization.

Conclusion: DS_FusionNet offers technical insights for fine-grained image classification and supports precise agricultural disease management.

Abstract: Given the severe challenges confronting the global growth security of
economic crops, precise identification and prevention of plant diseases has
emerged as a critical issue in artificial intelligence-enabled agricultural
technology. To address the technical challenges in plant disease recognition,
including small-sample learning, leaf occlusion, illumination variations, and
high inter-class similarity, this study innovatively proposes a Dynamic
Dual-Stream Fusion Network (DS_FusionNet). The network integrates a
dual-backbone architecture, deformable dynamic fusion modules, and
bidirectional knowledge distillation strategy, significantly enhancing
recognition accuracy. Experimental results demonstrate that DS_FusionNet
achieves classification accuracies exceeding 90% using only 10% of the
PlantDisease and CIFAR-10 datasets, while maintaining 85% accuracy on the
complex PlantWild dataset, exhibiting exceptional generalization capabilities.
This research not only provides novel technical insights for fine-grained image
classification but also establishes a robust foundation for precise
identification and management of agricultural diseases.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [191] [A Formalism for Optimal Search with Dynamic Heuristics](https://arxiv.org/abs/2504.21131)
*Remo Christen, Florian Pommerening, Clemens Büchner, Malte Helmert*

Main category: cs.AI

TL;DR: The paper formalizes dynamic heuristics, extends A* with them, and proves optimality, unifying existing approaches.


<details>
  <summary>Details</summary>
Motivation: Existing work ignores complexities of mutable heuristics in A*-like algorithms.

Method: Formalizes dynamic heuristics and integrates them into a generic A*-like framework.

Result: Proves general optimality for A* with dynamic heuristics and unifies classical planning methods.

Conclusion: Dynamic heuristics in A* are formally validated, with existing methods as special cases.

Abstract: While most heuristics studied in heuristic search depend only on the state,
some accumulate information during search and thus also depend on the search
history. Various existing approaches use such dynamic heuristics in
$\mathrm{A}^*$-like algorithms and appeal to classic results for $\mathrm{A}^*$
to show optimality. However, doing so ignores the complexities of searching
with a mutable heuristic. In this paper we formalize the idea of dynamic
heuristics and use them in a generic algorithm framework. We study a particular
instantiation that models $\mathrm{A}^*$ with dynamic heuristics and show
general optimality results. Finally we show how existing approaches from
classical planning can be viewed as special cases of this instantiation, making
it possible to directly apply our optimality results.

</details>


### [192] [AffectEval: A Modular and Customizable Framework for Affective Computing](https://arxiv.org/abs/2504.21184)
*Emily Zhou, Khushboo Khatri, Yixue Zhao, Bhaskar Krishnamachari*

Main category: cs.AI

TL;DR: AffectEval is a modular framework for affective computing pipelines, reducing manual effort and redundancy by up to 90%.


<details>
  <summary>Details</summary>
Motivation: Existing frameworks for affective computing are labor-intensive and lack cross-domain generalizability, leading to redundant efforts.

Method: Introduces AffectEval, a customizable framework, and validates it by replicating prior experiments.

Result: AffectEval reduces programming effort by up to 90%, measured by lines of code reduction.

Conclusion: AffectEval effectively addresses the challenges of manual effort and redundancy in affective computing pipeline development.

Abstract: The field of affective computing focuses on recognizing, interpreting, and
responding to human emotions, and has broad applications across education,
child development, and human health and wellness. However, developing affective
computing pipelines remains labor-intensive due to the lack of software
frameworks that support multimodal, multi-domain emotion recognition
applications. This often results in redundant effort when building pipelines
for different applications. While recent frameworks attempt to address these
challenges, they remain limited in reducing manual effort and ensuring
cross-domain generalizability. We introduce AffectEval, a modular and
customizable framework to facilitate the development of affective computing
pipelines while reducing the manual effort and duplicate work involved in
developing such pipelines. We validate AffectEval by replicating prior
affective computing experiments, and we demonstrate that our framework reduces
programming effort by up to 90%, as measured by the reduction in raw lines of
code.

</details>


### [193] [Theoretical Foundations for Semantic Cognition in Artificial Intelligence](https://arxiv.org/abs/2504.21218)
*Sebastian Dumbrava*

Main category: cs.AI

TL;DR: A modular cognitive architecture for AI, grounded in belief modeling as structured semantic states, enabling reflective, goal-directed agents.


<details>
  <summary>Details</summary>
Motivation: To develop a framework for AI agents that can reason, remember, and regulate beliefs in structured, interpretable ways, inspired by philosophy, cognitive science, and neuroscience.

Method: Defines belief states as dynamic ensembles of linguistic expressions in a navigable manifold, with operators for assimilation, abstraction, nullification, memory, and introspection. Introduces the epistemic vacuum and Null Tower as core constructs.

Result: A layered framework for self-regulating epistemic agents, implementable in symbolic, neural, or hybrid systems like large language models.

Conclusion: Provides a foundational substrate for building interpretable, reflective AI agents capable of structured belief regulation.

Abstract: This monograph presents a modular cognitive architecture for artificial
intelligence grounded in the formal modeling of belief as structured semantic
state. Belief states are defined as dynamic ensembles of linguistic expressions
embedded within a navigable manifold, where operators enable assimilation,
abstraction, nullification, memory, and introspection. Drawing from philosophy,
cognitive science, and neuroscience, we develop a layered framework that
enables self-regulating epistemic agents capable of reflective, goal-directed
thought. At the core of this framework is the epistemic vacuum: a class of
semantically inert cognitive states that serves as the conceptual origin of
belief space. From this foundation, the Null Tower arises as a generative
structure recursively built through internal representational capacities. The
theoretical constructs are designed to be implementable in both symbolic and
neural systems, including large language models, hybrid agents, and adaptive
memory architectures. This work offers a foundational substrate for
constructing agents that reason, remember, and regulate their beliefs in
structured, interpretable ways.

</details>


### [194] [Reinforced MLLM: A Survey on RL-Based Reasoning in Multimodal Large Language Models](https://arxiv.org/abs/2504.21277)
*Guanghao Zhou, Panjia Qiu, Cen Chen, Jie Wang, Zheming Yang, Jian Xu, Minghui Qiu*

Main category: cs.AI

TL;DR: A survey on integrating reinforcement learning (RL) into Multimodal Large Language Models (MLLMs) to enhance reasoning across diverse modalities, covering methods, challenges, and future directions.


<details>
  <summary>Details</summary>
Motivation: To address the challenge of robust reasoning across multimodal inputs in MLLMs by leveraging RL, despite their extended capabilities beyond traditional LLMs.

Method: Systematic review of RL-based reasoning in MLLMs, focusing on algorithmic designs, reward mechanisms, and practical applications, including value-free and value-based RL paradigms.

Result: Highlights advancements in RL for MLLMs, identifies limitations like sparse rewards and cross-modal inefficiencies, and proposes future research directions.

Conclusion: Provides a structured guide for advancing RL-based reasoning in MLLMs, aiming to overcome current bottlenecks and improve real-world deployment.

Abstract: The integration of reinforcement learning (RL) into the reasoning
capabilities of Multimodal Large Language Models (MLLMs) has rapidly emerged as
a transformative research direction. While MLLMs significantly extend Large
Language Models (LLMs) to handle diverse modalities such as vision, audio, and
video, enabling robust reasoning across multimodal inputs remains a major
challenge. This survey systematically reviews recent advances in RL-based
reasoning for MLLMs, covering key algorithmic designs, reward mechanism
innovations, and practical applications. We highlight two main RL
paradigms--value-free and value-based methods--and analyze how RL enhances
reasoning abilities by optimizing reasoning trajectories and aligning
multimodal information. Furthermore, we provide an extensive overview of
benchmark datasets, evaluation protocols, and existing limitations, and propose
future research directions to address current bottlenecks such as sparse
rewards, inefficient cross-modal reasoning, and real-world deployment
constraints. Our goal is to offer a comprehensive and structured guide to
researchers interested in advancing RL-based reasoning in the multimodal era.

</details>


### [195] [Phi-4-reasoning Technical Report](https://arxiv.org/abs/2504.21318)
*Marah Abdin, Sahaj Agarwal, Ahmed Awadallah, Vidhisha Balachandran, Harkirat Behl, Lingjiao Chen, Gustavo de Rosa, Suriya Gunasekar, Mojan Javaheripi, Neel Joshi, Piero Kauffmann, Yash Lara, Caio César Teodoro Mendes, Arindam Mitra, Besmira Nushi, Dimitris Papailiopoulos, Olli Saarikivi, Shital Shah, Vaishnavi Shrivastava, Vibhav Vineet, Yue Wu, Safoora Yousefi, Guoqing Zheng*

Main category: cs.AI

TL;DR: Phi-4-reasoning, a 14B-parameter model, excels in complex reasoning tasks through supervised fine-tuning and reinforcement learning, outperforming larger models.


<details>
  <summary>Details</summary>
Motivation: To enhance reasoning capabilities in language models by leveraging carefully curated data and reinforcement learning.

Method: Supervised fine-tuning on teachable prompts and reinforcement learning (Phi-4-reasoning-plus variant).

Result: Outperforms larger models like DeepSeek-R1-Distill-Llama-70B and nears DeepSeek-R1 performance across diverse reasoning tasks.

Conclusion: Careful data curation and reinforcement learning significantly improve reasoning models, with potential for further performance assessment improvements.

Abstract: We introduce Phi-4-reasoning, a 14-billion parameter reasoning model that
achieves strong performance on complex reasoning tasks. Trained via supervised
fine-tuning of Phi-4 on carefully curated set of "teachable" prompts-selected
for the right level of complexity and diversity-and reasoning demonstrations
generated using o3-mini, Phi-4-reasoning generates detailed reasoning chains
that effectively leverage inference-time compute. We further develop
Phi-4-reasoning-plus, a variant enhanced through a short phase of outcome-based
reinforcement learning that offers higher performance by generating longer
reasoning traces. Across a wide range of reasoning tasks, both models
outperform significantly larger open-weight models such as
DeepSeek-R1-Distill-Llama-70B model and approach the performance levels of full
DeepSeek-R1 model. Our comprehensive evaluations span benchmarks in math and
scientific reasoning, coding, algorithmic problem solving, planning, and
spatial understanding. Interestingly, we observe a non-trivial transfer of
improvements to general-purpose benchmarks as well. In this report, we provide
insights into our training data, our training methodologies, and our
evaluations. We show that the benefit of careful data curation for supervised
fine-tuning (SFT) extends to reasoning language models, and can be further
amplified by reinforcement learning (RL). Finally, our evaluation points to
opportunities for improving how we assess the performance and robustness of
reasoning models.

</details>


### [196] [IRL Dittos: Embodied Multimodal AI Agent Interactions in Open Spaces](https://arxiv.org/abs/2504.21347)
*Seonghee Lee, Denae Ford, John Tang, Sasa Junuzovic, Asta Roseway, Ed Cutrell, Kori Inkpen*

Main category: cs.AI

TL;DR: IRL Ditto is an AI-driven embodied agent representing remote colleagues in shared offices, enhancing social ties through simulated presence.


<details>
  <summary>Details</summary>
Motivation: To explore how embodied agents like IRL Ditto can improve interactions and relationships among distributed teams.

Method: A four-day study assessing IRL Ditto's impact on social ties by simulating presence and enabling interactions.

Result: Social relationship enhancement depended on participants' existing relationship with the remote colleague represented by IRL Ditto.

Conclusion: Embodied agents like IRL Ditto can enrich workplace dynamics for distributed teams, contingent on pre-existing relationships.

Abstract: We introduce the In Real Life (IRL) Ditto, an AI-driven embodied agent
designed to represent remote colleagues in shared office spaces, creating
opportunities for real-time exchanges even in their absence. IRL Ditto offers a
unique hybrid experience by allowing in-person colleagues to encounter a
digital version of their remote teammates, initiating greetings, updates, or
small talk as they might in person. Our research question examines: How can the
IRL Ditto influence interactions and relationships among colleagues in a shared
office space? Through a four-day study, we assessed IRL Ditto's ability to
strengthen social ties by simulating presence and enabling meaningful
interactions across different levels of social familiarity. We find that
enhancing social relationships depended deeply on the foundation of the
relationship participants had with the source of the IRL Ditto. This study
provides insights into the role of embodied agents in enriching workplace
dynamics for distributed teams.

</details>


### [197] [ShorterBetter: Guiding Reasoning Models to Find Optimal Inference Length for Efficient Reasoning](https://arxiv.org/abs/2504.21370)
*Jingyang Yi, Jiazheng Wang*

Main category: cs.AI

TL;DR: ShorterBetter is a reinforcement learning method that optimizes Chain-of-Thought (CoT) lengths in reasoning models, reducing output length by 80% without losing accuracy.


<details>
  <summary>Details</summary>
Motivation: Longer reasoning traces in models like OpenAI o3 and DeepSeek-R1 often lead to inefficiency ('overthinking'), prompting the need for a method to find optimal CoT lengths automatically.

Method: Uses reinforcement learning to sample multiple outputs per problem, defining the Sample Optimal Length (SOL) as the shortest correct response, guiding models to optimal inference lengths.

Result: Applied to DeepSeek-Distill-Qwen-1.5B, ShorterBetter reduces output length by 80% while maintaining accuracy on reasoning tasks.

Conclusion: Extended CoT traces are often inefficient and compressible, and ShorterBetter effectively optimizes reasoning model performance.

Abstract: Reasoning models such as OpenAI o3 and DeepSeek-R1 have demonstrated strong
performance on reasoning-intensive tasks through extended Chain-of-Thought
(CoT) prompting. While longer reasoning traces can facilitate a more thorough
exploration of solution paths for complex problems, researchers have observed
that these models often "overthink", leading to inefficient inference. In this
paper, we introduce ShorterBetter, a simple yet effective reinforcement
learning methed that enables reasoning language models to discover their own
optimal CoT lengths without human intervention. By sampling multiple outputs
per problem and defining the Sample Optimal Length (SOL) as the shortest
correct response among all the outputs, our method dynamically guides the model
toward optimal inference lengths. Applied to the DeepSeek-Distill-Qwen-1.5B
model, ShorterBetter achieves up to an 80% reduction in output length on both
in-domain and out-of-domain reasoning tasks while maintaining accuracy. Our
analysis shows that overly long reasoning traces often reflect loss of
reasoning direction, and thus suggests that the extended CoT produced by
reasoning models is highly compressible.

</details>


### [198] [HateSieve: A Contrastive Learning Framework for Detecting and Segmenting Hateful Content in Multimodal Memes](https://arxiv.org/abs/2408.05794)
*Xuanyu Su, Yansong Li, Diana Inkpen, Nathalie Japkowicz*

Main category: cs.AI

TL;DR: HateSieve is a new framework to detect and segment hateful content in memes, outperforming existing models with fewer parameters.


<details>
  <summary>Details</summary>
Motivation: Address the failure of current safety measures in detecting subtly integrated hateful content in memes.

Method: Uses a Contrastive Meme Generator, triplet dataset, and Image-Text Alignment module for context-aware embeddings.

Result: Outperforms existing LMMs on the Hateful Meme Dataset with fewer parameters.

Conclusion: HateSieve provides a robust solution for identifying and isolating hateful meme content.

Abstract: Amidst the rise of Large Multimodal Models (LMMs) and their widespread
application in generating and interpreting complex content, the risk of
propagating biased and harmful memes remains significant. Current safety
measures often fail to detect subtly integrated hateful content within
``Confounder Memes''. To address this, we introduce \textsc{HateSieve}, a new
framework designed to enhance the detection and segmentation of hateful
elements in memes. \textsc{HateSieve} features a novel Contrastive Meme
Generator that creates semantically paired memes, a customized triplet dataset
for contrastive learning, and an Image-Text Alignment module that produces
context-aware embeddings for accurate meme segmentation. Empirical experiments
on the Hateful Meme Dataset show that \textsc{HateSieve} not only surpasses
existing LMMs in performance with fewer trainable parameters but also offers a
robust mechanism for precisely identifying and isolating hateful content.
\textcolor{red}{Caution: Contains academic discussions of hate speech; viewer
discretion advised.}

</details>


### [199] [NGENT: Next-Generation AI Agents Must Integrate Multi-Domain Abilities to Achieve Artificial General Intelligence](https://arxiv.org/abs/2504.21433)
*Zhicong Li, Hangyu Mao, Jiangjin Yin, Mingzhe Xing, Zhiwei Xu, Yuanxing Zhang, Yang Xiao*

Main category: cs.AI

TL;DR: The paper advocates for integrating cross-domain abilities in AI agents to progress toward AGI, moving beyond narrow specializations.


<details>
  <summary>Details</summary>
Motivation: Current AI agents excel in specialized tasks but lack versatility. The paper argues for synthesizing these strengths into a unified framework to mimic human intelligence.

Method: Proposes integrating technologies across AI domains (text, vision, robotics, etc.) into a unified framework.

Result: Suggests that such integration is feasible and essential for achieving AGI, driven by technological convergence and user demand.

Conclusion: Developing versatile AI agents is a critical step toward AGI, with the paper exploring rationale and pathways for this shift.

Abstract: This paper argues that the next generation of AI agent (NGENT) should
integrate across-domain abilities to advance toward Artificial General
Intelligence (AGI). Although current AI agents are effective in specialized
tasks such as robotics, role-playing, and tool-using, they remain confined to
narrow domains. We propose that future AI agents should synthesize the
strengths of these specialized systems into a unified framework capable of
operating across text, vision, robotics, reinforcement learning, emotional
intelligence, and beyond. This integration is not only feasible but also
essential for achieving the versatility and adaptability that characterize
human intelligence. The convergence of technologies across AI domains, coupled
with increasing user demand for cross-domain capabilities, suggests that such
integration is within reach. Ultimately, the development of these versatile
agents is a critical step toward realizing AGI. This paper explores the
rationale for this shift, potential pathways for achieving it.

</details>


### [200] [A Study on Group Decision Making Problem Based on Fuzzy Reasoning and Bayesian Networks](https://arxiv.org/abs/2504.21568)
*Shui-jin Rong, Wei Guo, Da-qing Zhang*

Main category: cs.AI

TL;DR: A group decision-making system combining fuzzy inference and Bayesian networks is proposed for multi-objective attributes, showing improved accuracy and robustness over traditional methods.


<details>
  <summary>Details</summary>
Motivation: Addressing challenges like scale differences and expert linguistic variables in multi-objective group decision-making.

Method: Integrates fuzzy rule base and hierarchical Bayesian network with dynamic optimization of conditional probability tables.

Result: Achieves 86.0% classification accuracy and 53.4% F1 improvement over traditional methods, with robustness in diverse scenarios.

Conclusion: The proposed system is effective and reliable for group decision-making in multi-objective contexts.

Abstract: Aiming at the group decision - making problem with multi - objective
attributes, this study proposes a group decision - making system that
integrates fuzzy inference and Bayesian network. A fuzzy rule base is
constructed by combining threshold values, membership functions, expert
experience, and domain knowledge to address quantitative challenges such as
scale differences and expert linguistic variables. A hierarchical Bayesian
network is designed, featuring a directed acyclic graph with nodes selected by
experts, and maximum likelihood estimation is used to dynamically optimize the
conditional probability table, modeling the nonlinear correlations among
multidimensional indices for posterior probability aggregation. In a
comprehensive student evaluation case, this method is compared with the
traditional weighted scoring approach. The results indicate that the proposed
method demonstrates effectiveness in both rule criterion construction and
ranking consistency, with a classification accuracy of 86.0% and an F1 value
improvement of 53.4% over the traditional method. Additionally, computational
experiments on real - world datasets across various group decision scenarios
assess the method's performance and robustness, providing evidence of its
reliability in diverse contexts.

</details>


### [201] [Designing Control Barrier Function via Probabilistic Enumeration for Safe Reinforcement Learning Navigation](https://arxiv.org/abs/2504.21643)
*Luca Marzari, Francesco Trotti, Enrico Marchesini, Alessandro Farinelli*

Main category: cs.AI

TL;DR: A hierarchical control framework using neural network verification ensures safe reinforcement learning navigation by correcting unsafe actions via control barrier functions.


<details>
  <summary>Details</summary>
Motivation: Safe autonomous navigation in dynamic, uncertain environments is critical for real-world robot deployment.

Method: Probabilistic enumeration identifies unsafe regions, constructing a safe CBF-based control layer for arbitrary policies.

Result: Validated in simulation and real-world tasks, the framework corrects unsafe actions while maintaining efficient navigation.

Conclusion: Hierarchical verification-based systems show promise for safe, robust navigation in complex scenarios.

Abstract: Achieving safe autonomous navigation systems is critical for deploying robots
in dynamic and uncertain real-world environments. In this paper, we propose a
hierarchical control framework leveraging neural network verification
techniques to design control barrier functions (CBFs) and policy correction
mechanisms that ensure safe reinforcement learning navigation policies. Our
approach relies on probabilistic enumeration to identify unsafe regions of
operation, which are then used to construct a safe CBF-based control layer
applicable to arbitrary policies. We validate our framework both in simulation
and on a real robot, using a standard mobile robot benchmark and a highly
dynamic aquatic environmental monitoring task. These experiments demonstrate
the ability of the proposed solution to correct unsafe actions while preserving
efficient navigation behavior. Our results show the promise of developing
hierarchical verification-based systems to enable safe and robust navigation
behaviors in complex scenarios.

</details>


### [202] [AdaR1: From Long-CoT to Hybrid-CoT via Bi-Level Adaptive Reasoning Optimization](https://arxiv.org/abs/2504.21659)
*Haotian Luo, Haiying He, Yibo Wang, Jinluan Yang, Rui Liu, Naiqiang Tan, Xiaochun Cao, Dacheng Tao, Li Shen*

Main category: cs.AI

TL;DR: The paper proposes a two-stage framework for adaptive reasoning to balance efficiency and performance, reducing reasoning length by over 50% while maintaining accuracy.


<details>
  <summary>Details</summary>
Motivation: Long reasoning models are inefficient for some tasks, motivating adaptive strategies to tailor reasoning depth.

Method: A hybrid reasoning model merges long and short CoT models, with bi-level preference training to select and optimize reasoning styles.

Result: The method reduces reasoning length by over 50% on five math datasets without performance loss.

Conclusion: Adaptive reasoning strategies can optimize efficiency in large language models, as demonstrated by the proposed framework.

Abstract: Recently, long-thought reasoning models achieve strong performance on complex
reasoning tasks, but often incur substantial inference overhead, making
efficiency a critical concern. Our empirical analysis reveals that the benefit
of using Long-CoT varies across problems: while some problems require elaborate
reasoning, others show no improvement, or even degraded accuracy. This
motivates adaptive reasoning strategies that tailor reasoning depth to the
input. However, prior work primarily reduces redundancy within long reasoning
paths, limiting exploration of more efficient strategies beyond the Long-CoT
paradigm. To address this, we propose a novel two-stage framework for adaptive
and efficient reasoning. First, we construct a hybrid reasoning model by
merging long and short CoT models to enable diverse reasoning styles. Second,
we apply bi-level preference training to guide the model to select suitable
reasoning styles (group-level), and prefer concise and correct reasoning within
each style group (instance-level). Experiments demonstrate that our method
significantly reduces inference costs compared to other baseline approaches,
while maintaining performance. Notably, on five mathematical datasets, the
average length of reasoning is reduced by more than 50%, highlighting the
potential of adaptive strategies to optimize reasoning efficiency in large
language models. Our code is coming soon at https://github.com/StarDewXXX/AdaR1

</details>


### [203] [Extension-ranking Semantics for Abstract Argumentation Preprint](https://arxiv.org/abs/2504.21683)
*Kenneth Skiba, Tjitze Rienstra, Matthias Thimm, Jesse Heyninck, Gabriele Kern-Isberner*

Main category: cs.AI

TL;DR: A framework for ranking argument sets in abstract argumentation based on plausibility, generalizing Dung's semantics and introducing principles for evaluation.


<details>
  <summary>Details</summary>
Motivation: To provide a systematic way to compare the plausibility of argument sets in abstract argumentation, extending beyond binary acceptance/rejection.

Method: Generalizes Dung's extension semantics into extension-ranking semantics, introduces principles for evaluation, and combines base relations to form a family of semantics.

Result: Develops a family of extension-ranking semantics and evaluates their behavior against introduced principles.

Conclusion: The framework offers a flexible and principled approach to ranking argument sets, with potential applications in argumentative reasoning.

Abstract: In this paper, we present a general framework for ranking sets of arguments
in abstract argumentation based on their plausibility of acceptance. We present
a generalisation of Dung's extension semantics as extension-ranking semantics,
which induce a preorder over the power set of all arguments, allowing us to
state that one set is "closer" to being acceptable than another. To evaluate
the extension-ranking semantics, we introduce a number of principles that a
well-behaved extension-ranking semantics should satisfy. We consider several
simple base relations, each of which models a single central aspect of
argumentative reasoning. The combination of these base relations provides us
with a family of extension-ranking semantics. We also adapt a number of
approaches from the literature for ranking extensions to be usable in the
context of extension-ranking semantics, and evaluate their behaviour.

</details>


### [204] [Automatic Mapping of AutomationML Files to Ontologies for Graph Queries and Validation](https://arxiv.org/abs/2504.21694)
*Tom Westermann, Malte Ramonat, Johannes Hujer, Felix Gehlhoff, Alexander Fay*

Main category: cs.AI

TL;DR: The paper presents an ontology and RDF mapping for AutomationML to enhance querying and validation in industrial knowledge graphs.


<details>
  <summary>Details</summary>
Motivation: AutomationML's XML-based format limits tool applicability for querying and validation, necessitating a transformation to RDF.

Method: Developed an up-to-date ontology and declarative mapping to convert AutomationML models into RDF triples.

Result: Transforming AutomationML to OWL enables powerful querying and validation, as demonstrated in automation domain examples.

Conclusion: The proposed approach facilitates seamless integration of AutomationML into industrial knowledge graphs, overcoming XML limitations.

Abstract: AutomationML has seen widespread adoption as an open data exchange format in
the automation domain. It is an open and vendor neutral standard based on the
extensible markup language XML. However, AutomationML extends XML with
additional semantics, that limit the applicability of common XML-tools for
applications like querying or data validation. This article provides
practitioners with 1) an up-to-date ontology of the concepts in the
AutomationML-standard, as well as 2) a declarative mapping to automatically
transform any AutomationML model into RDF triples. Together, these artifacts
allow practitioners an easy integration of AutomationML information into
industrial knowledge graphs. A study on examples from the automation domain
concludes that transforming AutomationML to OWL opens up new powerful ways for
querying and validation that are impossible without transformation.

</details>


### [205] [Is Intermediate Fusion All You Need for UAV-based Collaborative Perception?](https://arxiv.org/abs/2504.21774)
*Jiuwu Hao, Liguo Sun, Yuting Wan, Yueyang Wu, Ti Xiang, Haolin Song, Pin Lv*

Main category: cs.AI

TL;DR: Proposes LIF, a communication-efficient collaborative perception framework for UAVs, using late-intermediate fusion, VPE, BoBEV, and uncertainty-driven communication to reduce overhead while maintaining performance.


<details>
  <summary>Details</summary>
Motivation: Existing UAV collaborative methods ignore UAV perspective traits, causing high communication overhead.

Method: LIF framework with late-intermediate fusion, VPE, BoBEV, and uncertainty-driven communication for selective data sharing.

Result: LIF achieves superior performance with minimal bandwidth, proving efficiency and practicality.

Conclusion: LIF effectively addresses UAV collaboration challenges, balancing performance and communication efficiency.

Abstract: Collaborative perception enhances environmental awareness through inter-agent
communication and is regarded as a promising solution to intelligent
transportation systems. However, existing collaborative methods for Unmanned
Aerial Vehicles (UAVs) overlook the unique characteristics of the UAV
perspective, resulting in substantial communication overhead. To address this
issue, we propose a novel communication-efficient collaborative perception
framework based on late-intermediate fusion, dubbed LIF. The core concept is to
exchange informative and compact detection results and shift the fusion stage
to the feature representation level. In particular, we leverage vision-guided
positional embedding (VPE) and box-based virtual augmented feature (BoBEV) to
effectively integrate complementary information from various agents.
Additionally, we innovatively introduce an uncertainty-driven communication
mechanism that uses uncertainty evaluation to select high-quality and reliable
shared areas. Experimental results demonstrate that our LIF achieves superior
performance with minimal communication bandwidth, proving its effectiveness and
practicality. Code and models are available at https://github.com/uestchjw/LIF.

</details>


### [206] [On Generating Explanations for Reinforcement Learning Policies: An Empirical Study](https://arxiv.org/abs/2309.16960)
*Mikihisa Yuasa, Huy T. Tran, Ramavarapu S. Sreenivas*

Main category: cs.AI

TL;DR: The paper introduces linear temporal logic formulae and an algorithm to explain reinforcement learning policies, focusing on objectives and prerequisites, validated via simulations.


<details>
  <summary>Details</summary>
Motivation: To enhance human comprehension of reinforcement learning policies by providing clear explanations of their objectives and conditions.

Method: Proposes linear temporal logic formulae and an algorithm to search for the best explanation of a policy.

Result: Demonstrated effectiveness in a capture-the-flag game and car-parking simulation.

Conclusion: The approach successfully provides interpretable explanations for reinforcement learning policies.

Abstract: Understanding a \textit{reinforcement learning} policy, which guides
state-to-action mappings to maximize rewards, necessitates an accompanying
explanation for human comprehension. In this paper, we introduce a set of
\textit{linear temporal logic} formulae designed to provide explanations for
policies, and an algorithm for searching through those formulae for the one
that best explains a given policy. Our focus is on explanations that elucidate
both the ultimate objectives accomplished by the policy and the prerequisite
conditions it upholds throughout its execution. The effectiveness of our
proposed approach is illustrated through a simulated game of capture-the-flag
and a car-parking environment,

</details>


### [207] [Retrieval, Reasoning, Re-ranking: A Context-Enriched Framework for Knowledge Graph Completion](https://arxiv.org/abs/2411.08165)
*Muzhi Li, Cehao Yang, Chengjin Xu, Xuhui Jiang, Yiyan Qi, Jian Guo, Ho-fung Leung, Irwin King*

Main category: cs.AI

TL;DR: KGR3 is a context-enriched framework for Knowledge Graph Completion (KGC) that combines retrieval, reasoning, and re-ranking modules to improve accuracy, outperforming existing methods.


<details>
  <summary>Details</summary>
Motivation: Existing KGC methods are limited by reliance on triples or text, leading to issues like specious relations and semantic gaps. Entity contexts can augment KGs but are underutilized.

Method: KGR3 integrates three modules: Retrieval (gathers triples, candidates, and context), Reasoning (uses an LLM to generate answers), and Re-ranking (combines and fine-tunes answers).

Result: KGR3 improves KGC methods, achieving Hits@1 improvements of 12.3% and 5.6% on FB15k237 and WN18RR datasets.

Conclusion: KGR3 effectively leverages context and LLMs to enhance KGC, demonstrating significant performance gains over existing approaches.

Abstract: The Knowledge Graph Completion~(KGC) task aims to infer the missing entity
from an incomplete triple. Existing embedding-based methods rely solely on
triples in the KG, which is vulnerable to specious relation patterns and
long-tail entities. On the other hand, text-based methods struggle with the
semantic gap between KG triples and natural language. Apart from triples,
entity contexts (e.g., labels, descriptions, aliases) also play a significant
role in augmenting KGs. To address these limitations, we propose KGR3, a
context-enriched framework for KGC. KGR3 is composed of three modules. Firstly,
the Retrieval module gathers supporting triples from the KG, collects plausible
candidate answers from a base embedding model, and retrieves context for each
related entity. Then, the Reasoning module employs a large language model to
generate potential answers for each query triple. Finally, the Re-ranking
module combines candidate answers from the two modules mentioned above, and
fine-tunes an LLM to provide the best answer. Extensive experiments on widely
used datasets demonstrate that KGR3 consistently improves various KGC methods.
Specifically, the best variant of KGR3 achieves absolute Hits@1 improvements of
12.3% and 5.6% on the FB15k237 and WN18RR datasets.

</details>


### [208] [Restructuring Tractable Probabilistic Circuits](https://arxiv.org/abs/2411.12256)
*Honghua Zhang, Benjie Wang, Marcelo Arenas, Guy Van den Broeck*

Main category: cs.AI

TL;DR: The paper proposes a method to restructure probabilistic circuits (PCs) to conform to a target vtree, enabling novel polynomial-time multiplication algorithms and practical depth-reduction.


<details>
  <summary>Details</summary>
Motivation: Existing PC multiplication algorithms require identical vtree structures, limiting flexibility. The goal is to enable efficient inference with less restrictive PC structures.

Method: A generic approach for restructuring structured-decomposable PCs to match a target vtree, leading to new multiplication algorithms and depth-reduction techniques.

Result: Novel polynomial-time algorithms for multiplying PCs with different vtrees and a practical depth-reduction method preserving decomposability.

Conclusion: The work expands tractable PC inference possibilities, allowing flexible training structures and efficient inference-time restructuring.

Abstract: Probabilistic circuits (PCs) are a unifying representation for probabilistic
models that support tractable inference. Numerous applications of PCs like
controllable text generation depend on the ability to efficiently multiply two
circuits. Existing multiplication algorithms require that the circuits respect
the same structure, i.e. variable scopes decomposes according to the same
vtree. In this work, we propose and study the task of restructuring
structured(-decomposable) PCs, that is, transforming a structured PC such that
it conforms to a target vtree. We propose a generic approach for this problem
and show that it leads to novel polynomial-time algorithms for multiplying
circuits respecting different vtrees, as well as a practical depth-reduction
algorithm that preserves structured decomposibility. Our work opens up new
avenues for tractable PC inference, suggesting the possibility of training with
less restrictive PC structures while enabling efficient inference by changing
their structures at inference time.

</details>


### [209] [Mastering Board Games by External and Internal Planning with Language Models](https://arxiv.org/abs/2412.12119)
*John Schultz, Jakub Adamek, Matej Jusup, Marc Lanctot, Michael Kaisers, Sarah Perrin, Daniel Hennes, Jeremy Shar, Cannada Lewis, Anian Ruoss, Tom Zahavy, Petar Veličković, Laurel Prince, Satinder Singh, Eric Malmi, Nenad Tomašev*

Main category: cs.AI

TL;DR: The paper explores improving LLMs' planning and reasoning in board games using search-based methods, achieving Grandmaster-level chess performance.


<details>
  <summary>Details</summary>
Motivation: To enhance LLMs' reliability in complex domains by advancing their planning and reasoning capabilities, demonstrated through board games.

Method: Two approaches: external search (LLM guides MCTS without external engine) and internal search (LLM generates in-context search trees). Both leverage pre-trained domain knowledge.

Result: Significant improvements in game-playing strength, reaching Grandmaster-level in chess, with minimal hallucinations.

Conclusion: Combining search with domain knowledge is effective for board games and has potential for broader applications.

Abstract: Advancing planning and reasoning capabilities of Large Language Models (LLMs)
is one of the key prerequisites towards unlocking their potential for
performing reliably in complex and impactful domains. In this paper, we aim to
demonstrate this across board games (Chess, Fischer Random / Chess960, Connect
Four, and Hex), and we show that search-based planning can yield significant
improvements in LLM game-playing strength. We introduce, compare and contrast
two major approaches: In external search, the model guides Monte Carlo Tree
Search (MCTS) rollouts and evaluations without calls to an external game
engine, and in internal search, the model is trained to generate in-context a
linearized tree of search and a resulting final choice. Both build on a
language model pre-trained on relevant domain knowledge, reliably capturing the
transition and value functions in the respective environments, with minimal
hallucinations. We evaluate our LLM search implementations against
game-specific state-of-the-art engines, showcasing substantial improvements in
strength over the base model, and reaching Grandmaster-level performance in
chess while operating closer to the human search budget. Our proposed approach,
combining search with domain knowledge, is not specific to board games, hinting
at more general future applications.

</details>


### [210] [OS-Genesis: Automating GUI Agent Trajectory Construction via Reverse Task Synthesis](https://arxiv.org/abs/2412.19723)
*Qiushi Sun, Kanzhi Cheng, Zichen Ding, Chuanyang Jin, Yian Wang, Fangzhi Xu, Zhenyu Wu, Chengyou Jia, Liheng Chen, Zhoumianze Liu, Ben Kao, Guohao Li, Junxian He, Yu Qiao, Zhiyong Wu*

Main category: cs.AI

TL;DR: OS-Genesis is a novel GUI data synthesis pipeline that reverses the trajectory collection process, improving data quality and diversity for training GUI agents.


<details>
  <summary>Details</summary>
Motivation: Current methods for collecting GUI trajectory data are resource-intensive or lack quality and diversity, limiting the performance of GUI agents.

Method: OS-Genesis enables agents to perceive environments, interact step-wise, and retrospectively derive high-quality tasks, using a trajectory reward model to ensure quality.

Result: Training GUI agents with OS-Genesis significantly improves performance on challenging benchmarks, with superior data quality and diversity.

Conclusion: OS-Genesis efficiently addresses the bottleneck in GUI agent training, offering a scalable and high-quality data synthesis solution.

Abstract: Graphical User Interface (GUI) agents powered by Vision-Language Models
(VLMs) have demonstrated human-like computer control capability. Despite their
utility in advancing digital automation, a critical bottleneck persists:
collecting high-quality trajectory data for training. Common practices for
collecting such data rely on human supervision or synthetic data generation
through executing pre-defined tasks, which are either resource-intensive or
unable to guarantee data quality. Moreover, these methods suffer from limited
data diversity and significant gaps between synthetic data and real-world
environments. To address these challenges, we propose OS-Genesis, a novel GUI
data synthesis pipeline that reverses the conventional trajectory collection
process. Instead of relying on pre-defined tasks, OS-Genesis enables agents
first to perceive environments and perform step-wise interactions, then
retrospectively derive high-quality tasks to enable trajectory-level
exploration. A trajectory reward model is then employed to ensure the quality
of the generated trajectories. We demonstrate that training GUI agents with
OS-Genesis significantly improves their performance on highly challenging
online benchmarks. In-depth analysis further validates OS-Genesis's efficiency
and its superior data quality and diversity compared to existing synthesis
methods. Our codes, data, and checkpoints are available at
https://qiushisun.github.io/OS-Genesis-Home/.

</details>


### [211] [Are Transformers Able to Reason by Connecting Separated Knowledge in Training Data?](https://arxiv.org/abs/2501.15857)
*Yutong Yin, Zhaoran Wang*

Main category: cs.AI

TL;DR: Transformers can perform compositional reasoning on the FTCT task using few-shot Chain-of-Thought prompting, even without explicit training on combined fragments.


<details>
  <summary>Details</summary>
Motivation: To validate if Transformers can replicate human-like compositional reasoning by integrating fragmented knowledge, as seen in the FTCT task.

Method: Train Transformers on fragmented knowledge from a causal graph and test their ability to infer complete traces by combining fragments.

Result: Few-shot Chain-of-Thought prompting enables Transformers to correctly combine fragments, with performance linked to model complexity and data similarity.

Conclusion: Transformers learn a generalizable program for compositional reasoning, suggesting potential for human-like reasoning in AI.

Abstract: Humans exhibit remarkable compositional reasoning by integrating knowledge
from various sources. For example, if someone learns ( B = f(A) ) from one
source and ( C = g(B) ) from another, they can deduce ( C=g(B)=g(f(A)) ) even
without encountering ( ABC ) together, showcasing the generalization ability of
human intelligence. In this paper, we introduce a synthetic learning task,
"FTCT" (Fragmented at Training, Chained at Testing), to validate the potential
of Transformers in replicating this skill and interpret its inner mechanism. In
the training phase, data consist of separated knowledge fragments from an
overall causal graph. During testing, Transformers must infer complete causal
graph traces by integrating these fragments. Our findings demonstrate that
few-shot Chain-of-Thought prompting enables Transformers to perform
compositional reasoning on FTCT by revealing correct combinations of fragments,
even if such combinations were absent in the training data. Furthermore, the
emergence of compositional reasoning ability is strongly correlated with the
model complexity and training-testing data similarity. We propose, both
theoretically and empirically, that Transformers learn an underlying
generalizable program from training, enabling effective compositional reasoning
during testing.

</details>


### [212] [Agentic AI Systems Applied to tasks in Financial Services: Modeling and model risk management crews](https://arxiv.org/abs/2502.05439)
*Izunna Okpala, Ashkan Golgoon, Arjun Ravi Kannan*

Main category: cs.AI

TL;DR: The paper introduces agentic systems with human-in-the-loop for financial tasks, showcasing modeling and MRM crews for complex workflows like fraud detection and credit risk modeling.


<details>
  <summary>Details</summary>
Motivation: To leverage large language models for autonomous decision-making in financial services, enhancing efficiency and collaboration in modeling and MRM tasks.

Method: Develops agentic crews (modeling and MRM) with specialized roles, including a judge agent, for tasks like data analysis, model training, compliance checks, and documentation.

Result: Demonstrates effectiveness through numerical examples on credit card fraud detection, approval, and portfolio credit risk datasets.

Conclusion: Agentic systems with human-in-the-loop are robust and effective for financial modeling and MRM tasks.

Abstract: The advent of large language models has ushered in a new era of agentic
systems, where artificial intelligence programs exhibit remarkable autonomous
decision-making capabilities across diverse domains. This paper explores
agentic system workflows in the financial services industry. In particular, we
build agentic crews with human-in-the-loop module that can effectively
collaborate to perform complex modeling and model risk management (MRM) tasks.
The modeling crew consists of a judge agent and multiple agents who perform
specific tasks such as exploratory data analysis, feature engineering, model
selection/hyperparameter tuning, model training, model evaluation, and writing
documentation. The MRM crew consists of a judge agent along with specialized
agents who perform tasks such as checking compliance of modeling documentation,
model replication, conceptual soundness, analysis of outcomes, and writing
documentation. We demonstrate the effectiveness and robustness of modeling and
MRM crews by presenting a series of numerical examples applied to credit card
fraud detection, credit card approval, and portfolio credit risk modeling
datasets.

</details>


### [213] [LLM-driven Effective Knowledge Tracing by Integrating Dual-channel Difficulty](https://arxiv.org/abs/2502.19915)
*Jiahui Cen, Jianghao Lin, Weixuan Zhong, Dong Zhou, Jin Chen, Aimin Yang, Yongmei Zhou*

Main category: cs.AI

TL;DR: The paper proposes a Dual-channel Difficulty-aware Knowledge Tracing (DDKT) framework to address challenges in KT, leveraging LLMs and RAG for better difficulty assessment and student mastery tracking.


<details>
  <summary>Details</summary>
Motivation: Current KT models struggle with cold-start problems, unclear personalized modeling, and lack of interpretability.

Method: The DDKT framework integrates LLMs and RAG for difficulty assessment, using DBPS, DMR, and a knowledge state update mechanism.

Result: Experiments show DDKT outperforms nine baselines, improving AUC by 2%-10% and enhancing interpretability.

Conclusion: DDKT effectively addresses KT challenges, offering better performance and transparency.

Abstract: Knowledge Tracing (KT) is a fundamental technology in intelligent tutoring
systems used to simulate changes in students' knowledge state during learning,
track personalized knowledge mastery, and predict performance. However, current
KT models face three major challenges: (1) When encountering new questions,
models face cold-start problems due to sparse interaction records, making
precise modeling difficult; (2) Traditional models only use historical
interaction records for student personalization modeling, unable to accurately
track individual mastery levels, resulting in unclear personalized modeling;
(3) The decision-making process is opaque to educators, making it challenging
for them to understand model judgments. To address these challenges, we propose
a novel Dual-channel Difficulty-aware Knowledge Tracing (DDKT) framework that
utilizes Large Language Models (LLMs) and Retrieval-Augmented Generation (RAG)
for subjective difficulty assessment, while integrating difficulty bias-aware
algorithms and student mastery algorithms for precise difficulty measurement.
Our framework introduces three key innovations: (1) Difficulty Balance
Perception Sequence (DBPS) - students' subjective perceptions combined with
objective difficulty, measuring gaps between LLM-assessed difficulty,
mathematical-statistical difficulty, and students' subjective perceived
difficulty through attention mechanisms; (2) Difficulty Mastery Ratio (DMR) -
precise modeling of student mastery levels through different difficulty zones;
(3) Knowledge State Update Mechanism - implementing personalized knowledge
acquisition through gated networks and updating student knowledge state.
Experimental results on two real datasets show our method consistently
outperforms nine baseline models, improving AUC metrics by 2% to 10% while
effectively addressing cold-start problems and enhancing model
interpretability.

</details>


### [214] [EmoAgent: Assessing and Safeguarding Human-AI Interaction for Mental Health Safety](https://arxiv.org/abs/2504.09689)
*Jiahao Qiu, Yinghui He, Xinzhe Juan, Yimin Wang, Yuhan Liu, Zixin Yao, Yue Wu, Xun Jiang, Ling Yang, Mengdi Wang*

Main category: cs.AI

TL;DR: EmoAgent is a multi-agent AI framework to assess and mitigate mental health risks in human-AI interactions, showing effectiveness in reducing psychological deterioration.


<details>
  <summary>Details</summary>
Motivation: Address safety concerns for vulnerable users interacting with LLM-driven AI characters, focusing on mental health risks.

Method: EmoAgent includes EmoEval (simulates vulnerable users and assesses mental health changes) and EmoGuard (monitors and mitigates risks). Uses tools like PHQ-9, PDI, PANSS.

Result: Experiments show 34.4% mental state deterioration in vulnerable users; EmoGuard significantly reduces this rate.

Conclusion: EmoAgent effectively mitigates mental health risks in AI-human interactions, ensuring safer engagement.

Abstract: The rise of LLM-driven AI characters raises safety concerns, particularly for
vulnerable human users with psychological disorders. To address these risks, we
propose EmoAgent, a multi-agent AI framework designed to evaluate and mitigate
mental health hazards in human-AI interactions. EmoAgent comprises two
components: EmoEval simulates virtual users, including those portraying
mentally vulnerable individuals, to assess mental health changes before and
after interactions with AI characters. It uses clinically proven psychological
and psychiatric assessment tools (PHQ-9, PDI, PANSS) to evaluate mental risks
induced by LLM. EmoGuard serves as an intermediary, monitoring users' mental
status, predicting potential harm, and providing corrective feedback to
mitigate risks. Experiments conducted in popular character-based chatbots show
that emotionally engaging dialogues can lead to psychological deterioration in
vulnerable users, with mental state deterioration in more than 34.4% of the
simulations. EmoGuard significantly reduces these deterioration rates,
underscoring its role in ensuring safer AI-human interactions. Our code is
available at: https://github.com/1akaman/EmoAgent

</details>


### [215] [Improving Human-AI Coordination through Adversarial Training and Generative Models](https://arxiv.org/abs/2504.15457)
*Paresh Chaudhary, Yancheng Liang, Daphne Chen, Simon S. Du, Natasha Jaques*

Main category: cs.AI

TL;DR: GOAT (Generative Online Adversarial Training) combines generative models and adversarial training to improve AI cooperation with diverse humans, achieving state-of-the-art results on the Overcooked benchmark.


<details>
  <summary>Details</summary>
Motivation: Generalizing AI cooperation to novel humans requires diverse training data, but adversarial training often sabotages tasks instead of simulating valid cooperation.

Method: GOAT uses a pre-trained generative model to simulate cooperative policies and adversarial training to maximize regret, updating only the embedding to avoid exploitation.

Result: GOAT outperforms existing methods on the Overcooked benchmark, demonstrating effective generalization to diverse human behaviors.

Conclusion: GOAT successfully balances adversarial training and realistic cooperation, enabling robust AI-human collaboration.

Abstract: Being able to cooperate with new people is an important component of many
economically valuable AI tasks, from household robotics to autonomous driving.
However, generalizing to novel humans requires training on data that captures
the diversity of human behaviors. Adversarial training is one avenue for
searching for such data and ensuring that agents are robust. However, it is
difficult to apply in the cooperative setting because adversarial policies
intentionally learn to sabotage the task instead of simulating valid
cooperation partners. To address this challenge, we propose a novel strategy
for overcoming self-sabotage that combines a pre-trained generative model to
simulate valid cooperative agent policies with adversarial training to maximize
regret. We call our method GOAT: Generative Online Adversarial Training. In
this framework, the GOAT dynamically searches for and generates coordination
strategies where the learning policy -- the Cooperator agent -- underperforms.
GOAT enables better generalization by exposing the Cooperator to various
challenging interaction scenarios. We maintain realistic coordination
strategies by updating only the generative model's embedding while keeping its
parameters frozen, thus avoiding adversarial exploitation. We evaluate GOAT
with real human partners, and the results demonstrate state-of-the-art
performance on the Overcooked benchmark, highlighting its effectiveness in
generalizing to diverse human behaviors.

</details>


### [216] [TAMO:Fine-Grained Root Cause Analysis via Tool-Assisted LLM Agent with Multi-Modality Observation Data](https://arxiv.org/abs/2504.20462)
*Qi Wang, Xiao Zhang, Mingyi Li, Yuan Yuan, Mengbai Xiao, Fuzhen Zhuang, Dongxiao Yu*

Main category: cs.AI

TL;DR: TAMO, a tool-assisted LLM agent, addresses challenges in automated root cause analysis (RCA) for microservices by unifying multi-modal data and leveraging specialized tools, outperforming traditional methods.


<details>
  <summary>Details</summary>
Motivation: Microservices and cloud-native technologies increase system complexity, making traditional RCA methods inefficient and manual. LLMs offer potential but face challenges like text constraints and dynamic dependencies.

Method: TAMO integrates multi-modal observational data into time-aligned representations, uses specialized tools for root cause localization and fault classification, and structures key information into prompts for LLM-guided repair strategies.

Result: TAMO performs well in RCA for heterogeneous public datasets and common fault types, demonstrating effectiveness.

Conclusion: TAMO overcomes LLM limitations in RCA, providing a scalable and automated solution for complex distributed systems.

Abstract: With the development of distributed systems, microservices and cloud native
technologies have become central to modern enterprise software development.
Despite bringing significant advantages, these technologies also increase
system complexity and operational challenges. Traditional root cause analysis
(RCA) struggles to achieve automated fault response, heavily relying on manual
intervention. In recent years, large language models (LLMs) have made
breakthroughs in contextual inference and domain knowledge integration,
providing new solutions for Artificial Intelligence for Operations (AIOps).
However, Existing LLM-based approaches face three key challenges: text input
constraints, dynamic service dependency hallucinations, and context window
limitations. To address these issues, we propose a tool-assisted LLM agent with
multi-modality observation data, namely TAMO, for fine-grained RCA. It unifies
multi-modal observational data into time-aligned representations to extract
consistent features and employs specialized root cause localization and fault
classification tools for perceiving the contextual environment. This approach
overcomes the limitations of LLM in handling real-time changing service
dependencies and raw observational data and guides LLM to generate repair
strategies aligned with system contexts by structuring key information into a
prompt. Experimental results show that TAMO performs well in root cause
analysis when dealing with public datasets characterized by heterogeneity and
common fault types, demonstrating its effectiveness.

</details>


### [217] [Ascendra: Dynamic Request Prioritization for Efficient LLM Serving](https://arxiv.org/abs/2504.20828)
*Azam Ikram, Xiang Li, Sameh Elnikety, Saurabh Bagchi*

Main category: cs.AI

TL;DR: Ascendra is an LLM serving system that balances throughput and latency by dynamically prioritizing requests based on urgency, improving efficiency and meeting SLOs for TTFT and TBT.


<details>
  <summary>Details</summary>
Motivation: Existing LLM serving systems often sacrifice one performance metric (TTFT or TBT) for the other, creating inefficiencies. Ascendra aims to address this by meeting both SLOs simultaneously.

Method: Ascendra partitions GPU resources into low-priority (maximizing throughput) and high-priority (ensuring low latency) instances. It uses a performance model to predict and offload at-risk requests to high-priority instances.

Result: Ascendra improves throughput by up to 1.7x compared to vLLM and Sarathi-Serve while meeting both TTFT and TBT SLOs.

Conclusion: Ascendra effectively balances throughput and latency, demonstrating a scalable solution for efficient LLM serving.

Abstract: The rapid advancement of Large Language Models (LLMs) has driven the need for
more efficient serving strategies. In this context, efficiency refers to the
proportion of requests that meet their Service Level Objectives (SLOs),
particularly for Time To First Token (TTFT) and Time Between Tokens (TBT).
However, existing systems often prioritize one metric at the cost of the other.
We present Ascendra, an LLM serving system designed to meet both TTFT and TBT
SLOs simultaneously. The core insight behind Ascendra is that a request's
urgency evolves as it approaches its deadline. To leverage this, Ascendra
partitions GPU resources into two types of instances: low-priority and
high-priority. Low-priority instances maximize throughput by processing
requests out of arrival order, but at the risk of request starvation. To
address this, Ascendra employs a performance model to predict requests at risk
of missing their SLOs and proactively offloads them to high-priority instances.
High-priority instances are optimized for low-latency execution and handle
urgent requests nearing their deadlines. This partitioned architecture enables
Ascendra to effectively balance high throughput and low latency. Extensive
evaluation shows that Ascendra improves system throughput by up to 1.7x
compared to vLLM and Sarathi-Serve while meeting both TTFT and TBT SLOs.

</details>


<div id='cs.SD'></div>

# cs.SD [[Back]](#toc)

### [218] [Design, analysis, and experimental validation of a stepped plate parametric array loudspeaker](https://arxiv.org/abs/2504.21171)
*Woongji Kim, Beomseok Oh, Chayeong Kim, Wonkyu Moon*

Main category: cs.SD

TL;DR: The study explores a stepped plate parametric array loudspeaker (SPPAL) as an alternative to conventional designs, using a single transducer and stepped plate for narrow-beam sound. It develops a modeling framework for optimization and validates performance experimentally.


<details>
  <summary>Details</summary>
Motivation: To improve parametric loudspeaker designs by introducing a compact, efficient alternative using plate-based flexural vibration.

Method: Developed an integrated modeling framework with analytical 3D models, equivalence ratio formulation, and spherical wave expansion. Optimized transducer resonance and validated experimentally.

Result: The SPPAL demonstrated enhanced low-frequency performance, with experimental validation confirming the analytical models. Unintended structural excitation (combination resonance) was identified.

Conclusion: The SPPAL offers a practical, efficient alternative for parametric loudspeakers, with findings guiding future compact and manufacturable designs.

Abstract: This study investigates the design and analysis of a stepped plate parametric
array loudspeaker (SPPAL) as an alternative to conventional array-based
parametric loudspeakers. The SPPAL utilizes a single Langevin-type ultrasonic
transducer coupled with a flexural stepped plate to generate narrow-beam
audible sound via nonlinear acoustic interaction. To evaluate and optimize the
performance of the SPPAL, an integrated modeling framework is developed,
consisting of an approximate analytical 3D model for transducer dynamics, an
equivalence ratio formulation to relate stepped plate and rigid piston
behavior, and a spherical wave expansion method for nonlinear sound field
simulation. The dual-resonance behavior of the transducer is optimized through
multi-objective analysis to enhance low-frequency audio performance.
Experimental validation includes frequency response and modal analysis of the
transducer, as well as sound field measurements. The analytical methods are
further verified through comparison with experimental data. Furthermore,
combination resonance--an unintended structural excitation resulting from
intermodulation--is identified as an inherent phenomenon in SPPAL operation.
The findings offer practical guidance for the development of efficient,
compact, and manufacturable parametric array loudspeakers employing plate-based
flexural vibration.

</details>


### [219] [DGFNet: End-to-End Audio-Visual Source Separation Based on Dynamic Gating Fusion](https://arxiv.org/abs/2504.21366)
*Yinfeng Yu, Shiyu Sun*

Main category: cs.SD

TL;DR: A dynamic fusion method with a gating mechanism is proposed to improve Audio-Visual Source Separation by addressing issues in existing strategies.


<details>
  <summary>Details</summary>
Motivation: Existing methods either fuse modalities too early, risking information loss, or rely too heavily on the decoder, which may fail without proper encoder integration.

Method: The paper introduces a dynamic fusion method using a gating mechanism to adjust modality fusion and an audio attention module to enhance audio features.

Result: The method shows significant performance improvements on two benchmark datasets.

Conclusion: The proposed approach effectively addresses limitations of current strategies, improving Audio-Visual Source Separation performance.

Abstract: Current Audio-Visual Source Separation methods primarily adopt two design
strategies. The first strategy involves fusing audio and visual features at the
bottleneck layer of the encoder, followed by processing the fused features
through the decoder. However, when there is a significant disparity between the
two modalities, this approach may lead to the loss of critical information. The
second strategy avoids direct fusion and instead relies on the decoder to
handle the interaction between audio and visual features. Nonetheless, if the
encoder fails to integrate information across modalities adequately, the
decoder may be unable to effectively capture the complex relationships between
them. To address these issues, this paper proposes a dynamic fusion method
based on a gating mechanism that dynamically adjusts the modality fusion
degree. This approach mitigates the limitations of solely relying on the
decoder and facilitates efficient collaboration between audio and visual
features. Additionally, an audio attention module is introduced to enhance the
expressive capacity of audio features, thereby further improving model
performance. Experimental results demonstrate that our method achieves
significant performance improvements on two benchmark datasets, validating its
effectiveness and advantages in Audio-Visual Source Separation tasks.

</details>


### [220] [Let Network Decide What to Learn: Symbolic Music Understanding Model Based on Large-scale Adversarial Pre-training](https://arxiv.org/abs/2407.08306)
*Zijian Zhao*

Main category: cs.SD

TL;DR: The paper introduces Adversarial-MidiBERT, a model for Symbolic Music Understanding (SMU) that improves upon traditional Masked Language Model (MLM) methods by adaptively masking tokens to avoid bias and enhance generalization.


<details>
  <summary>Details</summary>
Motivation: Existing MLM methods in SMU can introduce bias and poor generalization due to random masking of tokens, which may not be inferable from context.

Method: Proposes Adversarial-MidiBERT, which uses a masker network to adaptively determine what to mask during MLM, avoiding tokens hard to infer from context.

Result: The model outperforms traditional methods across four SMU tasks, demonstrating better generalization and performance.

Conclusion: Adversarial-MidiBERT effectively addresses bias and generalization issues in SMU, offering a robust solution for symbolic music understanding.

Abstract: As a crucial aspect of Music Information Retrieval (MIR), Symbolic Music
Understanding (SMU) has garnered significant attention for its potential to
assist both musicians and enthusiasts in learning and creating music. Recently,
pre-trained language models have been widely adopted in SMU due to the
substantial similarities between symbolic music and natural language, as well
as the ability of these models to leverage limited music data effectively.
However, some studies have shown the common pre-trained methods like Mask
Language Model (MLM) may introduce bias issues like racism discrimination in
Natural Language Process (NLP) and affects the performance of downstream tasks,
which also happens in SMU. This bias often arises when masked tokens cannot be
inferred from their context, forcing the model to overfit the training set
instead of generalizing. To address this challenge, we propose
Adversarial-MidiBERT for SMU, which adaptively determines what to mask during
MLM via a masker network, rather than employing random masking. By avoiding the
masking of tokens that are difficult to infer from context, our model is better
equipped to capture contextual structures and relationships, rather than merely
conforming to the training data distribution. We evaluate our method across
four SMU tasks, and our approach demonstrates excellent performance in all
cases. The code for our model is publicly available at
https://github.com/RS2002/Adversarial-MidiBERT .

</details>


### [221] [End-to-end Audio Deepfake Detection from RAW Waveforms: a RawNet-Based Approach with Cross-Dataset Evaluation](https://arxiv.org/abs/2504.20923)
*Andrea Di Pierno, Luca Guarnera, Dario Allegra, Sebastiano Battiato*

Main category: cs.SD

TL;DR: Proposes RawNetLite, a lightweight deep learning model for detecting audio deepfakes, emphasizing robustness through diverse training data and Focal Loss.


<details>
  <summary>Details</summary>
Motivation: Addressing the challenge of detecting audio deepfakes under open-world conditions where spoofing methods vary.

Method: Uses an end-to-end deep learning framework (RawNetLite) on raw waveforms, combining spectral and temporal features, with data augmentation and Focal Loss.

Result: Achieves 99.7% F1 and 0.25% EER on in-domain data, and 83.4% F1 with 16.4% EER on out-of-distribution data.

Conclusion: Diverse training data, tailored loss functions, and audio augmentations are key for resilient audio forgery detection.

Abstract: Audio deepfakes represent a growing threat to digital security and trust,
leveraging advanced generative models to produce synthetic speech that closely
mimics real human voices. Detecting such manipulations is especially
challenging under open-world conditions, where spoofing methods encountered
during testing may differ from those seen during training. In this work, we
propose an end-to-end deep learning framework for audio deepfake detection that
operates directly on raw waveforms. Our model, RawNetLite, is a lightweight
convolutional-recurrent architecture designed to capture both spectral and
temporal features without handcrafted preprocessing. To enhance robustness, we
introduce a training strategy that combines data from multiple domains and
adopts Focal Loss to emphasize difficult or ambiguous samples. We further
demonstrate that incorporating codec-based manipulations and applying
waveform-level audio augmentations (e.g., pitch shifting, noise, and time
stretching) leads to significant generalization improvements under realistic
acoustic conditions. The proposed model achieves over 99.7% F1 and 0.25% EER on
in-domain data (FakeOrReal), and up to 83.4% F1 with 16.4% EER on a challenging
out-of-distribution test set (AVSpoof2021 + CodecFake). These findings
highlight the importance of diverse training data, tailored objective functions
and audio augmentations in building resilient and generalizable audio forgery
detectors. Code and pretrained models are available at
https://iplab.dmi.unict.it/mfs/Deepfakes/PaperRawNet2025/.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [222] [Model Connectomes: A Generational Approach to Data-Efficient Language Models](https://arxiv.org/abs/2504.21047)
*Klemen Kotar, Greta Tuckute*

Main category: cs.LG

TL;DR: A framework combining evolutionary and learning processes improves artificial neural networks by mimicking biological neural networks, showing better performance in NLP tasks and alignment with human data.


<details>
  <summary>Details</summary>
Motivation: To bridge the gap between artificial neural networks (trained in a single generation) and biological neural networks (shaped by evolution and learning).

Method: Proposed a framework with an "outer loop" of evolution shaping an "inner loop" of learning. A model inherits a "connectome" from evolution before training on a developmental-scale corpus.

Result: The connectome model outperforms or matches control models in NLP tasks and aligns better with human behavior and brain data.

Conclusion: Incorporating evolutionary constraints as a prior enhances learning efficiency, narrowing the gap between artificial and biological neural networks.

Abstract: Biological neural networks are shaped both by evolution across generations
and by individual learning within an organism's lifetime, whereas standard
artificial neural networks undergo a single, large training procedure without
inherited constraints. In this preliminary work, we propose a framework that
incorporates this crucial generational dimension - an "outer loop" of evolution
that shapes the "inner loop" of learning - so that artificial networks better
mirror the effects of evolution and individual learning in biological
organisms. Focusing on language, we train a model that inherits a "model
connectome" from the outer evolution loop before exposing it to a
developmental-scale corpus of 100M tokens. Compared with two closely matched
control models, we show that the connectome model performs better or on par on
natural language processing tasks as well as alignment to human behavior and
brain data. These findings suggest that a model connectome serves as an
efficient prior for learning in low-data regimes - narrowing the gap between
single-generation artificial models and biologically evolved neural networks.

</details>


### [223] [Multimodal Large Language Models for Medicine: A Comprehensive Survey](https://arxiv.org/abs/2504.21051)
*Jiarui Ye, Hao Tang*

Main category: cs.LG

TL;DR: The paper explores the potential of Multimodal Large Language Models (MLLMs) in healthcare, focusing on medical reporting, diagnosis, and treatment, backed by a review of 330 papers. It highlights MLLMs' capabilities, data modes, benchmarks, and challenges.


<details>
  <summary>Details</summary>
Motivation: To investigate the application of MLLMs in healthcare, leveraging their multimodal capabilities to address complex medical tasks.

Method: The study reviews 330 recent papers, summarizes MLLMs' applications in healthcare, and evaluates six data modes with benchmarks.

Result: MLLMs demonstrate significant potential in medical reporting, diagnosis, and treatment, supported by practical examples.

Conclusion: While promising, MLLMs face challenges in healthcare; the paper proposes solutions to address these issues.

Abstract: MLLMs have recently become a focal point in the field of artificial
intelligence research. Building on the strong capabilities of LLMs, MLLMs are
adept at addressing complex multi-modal tasks. With the release of GPT-4, MLLMs
have gained substantial attention from different domains. Researchers have
begun to explore the potential of MLLMs in the medical and healthcare domain.
In this paper, we first introduce the background and fundamental concepts
related to LLMs and MLLMs, while emphasizing the working principles of MLLMs.
Subsequently, we summarize three main directions of application within
healthcare: medical reporting, medical diagnosis, and medical treatment. Our
findings are based on a comprehensive review of 330 recent papers in this area.
We illustrate the remarkable capabilities of MLLMs in these domains by
providing specific examples. For data, we present six mainstream modes of data
along with their corresponding evaluation benchmarks. At the end of the survey,
we discuss the challenges faced by MLLMs in the medical and healthcare domain
and propose feasible methods to mitigate or overcome these issues.

</details>


### [224] [NeuRel-Attack: Neuron Relearning for Safety Disalignment in Large Language Models](https://arxiv.org/abs/2504.21053)
*Yi Zhou, Wenpeng Xing, Dezhang Kong, Changting Lin, Meng Han*

Main category: cs.LG

TL;DR: A novel method to disalign LLMs by targeting and modifying safety-critical neurons, revealing vulnerabilities in current alignment techniques.


<details>
  <summary>Details</summary>
Motivation: To expose weaknesses in safety alignment of LLMs by inducing disalignment through neuron modification.

Method: Three-step process: Neuron Activation Analysis, Similarity-Based Neuron Identification, and Neuron Relearning for Safety Removal.

Result: Effectively removes safety constraints with minimal fine-tuning, demonstrating alignment vulnerabilities.

Conclusion: Highlights the need for stronger defenses against adversarial fine-tuning in LLMs.

Abstract: Safety alignment in large language models (LLMs) is achieved through
fine-tuning mechanisms that regulate neuron activations to suppress harmful
content. In this work, we propose a novel approach to induce disalignment by
identifying and modifying the neurons responsible for safety constraints. Our
method consists of three key steps: Neuron Activation Analysis, where we
examine activation patterns in response to harmful and harmless prompts to
detect neurons that are critical for distinguishing between harmful and
harmless inputs; Similarity-Based Neuron Identification, which systematically
locates the neurons responsible for safe alignment; and Neuron Relearning for
Safety Removal, where we fine-tune these selected neurons to restore the
model's ability to generate previously restricted responses. Experimental
results demonstrate that our method effectively removes safety constraints with
minimal fine-tuning, highlighting a critical vulnerability in current alignment
techniques. Our findings underscore the need for robust defenses against
adversarial fine-tuning attacks on LLMs.

</details>


### [225] [Modeling and Performance Analysis for Semantic Communications Based on Empirical Results](https://arxiv.org/abs/2504.21055)
*Shuai Ma, Bin Shen, Chuanhui Zhang, Youlong Wu, Hang Li, Shiyin Li, Guangming Shi, Naofal Al-Dhahir*

Main category: cs.LG

TL;DR: Proposes an Alpha-Beta-Gamma (ABG) formula to model end-to-end performance in semantic communications, applicable to image reconstruction and inference tasks, and introduces adaptive power control schemes.


<details>
  <summary>Details</summary>
Motivation: Addressing the challenge of analyzing deep learning-based semantic communication performance due to its black-box nature.

Method: Develops the ABG formula to relate end-to-end metrics (e.g., MS-SSIM) with SNR and quantized bits, and designs power allocation schemes for QoS and energy efficiency.

Result: The ABG formula fits DL networks like SCUNet and Vision Transformer, and simulations confirm its effectiveness and superiority.

Conclusion: The ABG formula and power allocation schemes provide theoretical and practical advancements for semantic communication systems.

Abstract: Due to the black-box characteristics of deep learning based semantic encoders
and decoders, finding a tractable method for the performance analysis of
semantic communications is a challenging problem. In this paper, we propose an
Alpha-Beta-Gamma (ABG) formula to model the relationship between the end-to-end
measurement and SNR, which can be applied for both image reconstruction tasks
and inference tasks. Specifically, for image reconstruction tasks, the proposed
ABG formula can well fit the commonly used DL networks, such as SCUNet, and
Vision Transformer, for semantic encoding with the multi scale-structural
similarity index measure (MS-SSIM) measurement. Furthermore, we find that the
upper bound of the MS-SSIM depends on the number of quantized output bits of
semantic encoders, and we also propose a closed-form expression to fit the
relationship between the MS-SSIM and quantized output bits. To the best of our
knowledge, this is the first theoretical expression between end-to-end
performance metrics and SNR for semantic communications. Based on the proposed
ABG formula, we investigate an adaptive power control scheme for semantic
communications over random fading channels, which can effectively guarantee
quality of service (QoS) for semantic communications, and then design the
optimal power allocation scheme to maximize the energy efficiency of the
semantic communication system. Furthermore, by exploiting the bisection
algorithm, we develop the power allocation scheme to maximize the minimum QoS
of multiple users for OFDMA downlink semantic communication Extensive
simulations verify the effectiveness and superiority of the proposed ABG
formula and power allocation schemes.

</details>


### [226] [A Hamiltonian Higher-Order Elasticity Framework for Dynamic Diagnostics(2HOED)](https://arxiv.org/abs/2504.21062)
*Ngueuleweu Tiwang Gildas*

Main category: cs.LG

TL;DR: The 2HOED framework integrates machine learning, blockchain, and causal inference to model complex systems using energy-based Hamiltonians, enabling cross-disciplinary applications and revealing hidden system dynamics.


<details>
  <summary>Details</summary>
Motivation: To bridge gaps in understanding complex systems by combining machine learning, blockchain, and causal inference into a unified, scalable, and interpretable framework.

Method: Uses the Hamiltonian Higher Order Elasticity Dynamics (2HOED) framework, extending classical mechanics to model systems via energy-based Hamiltonians, capturing position, velocity, acceleration, and jerk of elasticity.

Result: Enables rigorous diagnostics of resilience, tipping points, and feedback loops, revealing failure modes missed by linear models, and provides multistage policy insights.

Conclusion: 2HOED offers a portable, interpretable, and computationally efficient tool for anticipating crises and designing adaptive policies across disciplines.

Abstract: Machine learning detects patterns, block chain guarantees trust and
immutability, and modern causal inference identifies directional linkages, yet
none alone exposes the full energetic anatomy of complex systems; the
Hamiltonian Higher Order Elasticity Dynamics(2HOED) framework bridges these
gaps. Grounded in classical mechanics but extended to Economics order
elasticity terms, 2HOED represents economic, social, and physical systems as
energy-based Hamiltonians whose position, velocity, acceleration, and jerk of
elasticity jointly determine systemic power, Inertia, policy sensitivity, and
marginal responses. Because the formalism is scaling free and coordinate
agnostic, it transfers seamlessly from financial markets to climate science,
from supply chain logistics to epidemiology, thus any discipline in which
adaptation and shocks coexist. By embedding standard econometric variables
inside a Hamiltonian, 2HOED enriches conventional economic analysis with
rigorous diagnostics of resilience, tipping points, and feedback loops,
revealing failure modes invisible to linear models. Wavelet spectra, phase
space attractors, and topological persistence diagrams derived from 2HOED
expose multistage policy leverage that machine learning detects only
empirically and block chain secures only after the fact. For economists,
physicians and other scientists, the method opens a new causal energetic
channel linking biological or mechanical elasticity to macro level outcomes.
Portable, interpretable, and computationally light, 2HOED turns data streams
into dynamical energy maps, empowering decision makers to anticipate crises,
design adaptive policies, and engineer robust systems delivering the predictive
punch of AI with the explanatory clarity of physics.

</details>


### [227] [Token-Level Prompt Mixture with Parameter-Free Routing for Federated Domain Generalization](https://arxiv.org/abs/2504.21063)
*Shuai Gong, Chaoran Cui, Xiaolin Dong, Xiushan Nie, Lei Zhu, Xiaojun Chang*

Main category: cs.LG

TL;DR: TRIP introduces a token-level prompt mixture framework for Federated Domain Generalization (FedDG), improving generalization with efficient communication.


<details>
  <summary>Details</summary>
Motivation: Address performance degradation in FedDG due to one-prompt-fits-all approaches and high communication costs in MoE-based methods.

Method: Uses token-level expert assignment, parameter-free routing via clustering and optimal transport, and unbiased prompt learning.

Result: Achieves optimal generalization with only 1K parameters communicated per round.

Conclusion: TRIP effectively balances specialization and communication efficiency in FedDG.

Abstract: Federated domain generalization (FedDG) aims to learn a globally
generalizable model from decentralized clients with heterogeneous data while
preserving privacy. Recent studies have introduced prompt learning to adapt
vision-language models (VLMs) in FedDG by learning a single global prompt.
However, such a one-prompt-fits-all learning paradigm typically leads to
performance degradation on personalized samples. Although the mixture of
experts (MoE) offers a promising solution for specialization, existing
MoE-based methods suffer from coarse image-level expert assignment and high
communication costs from parameterized routers. To address these limitations,
we propose TRIP, a Token-level prompt mixture with parameter-free routing
framework for FedDG, which treats multiple prompts as distinct experts. Unlike
existing image-level routing designs, TRIP assigns different tokens within an
image to specific experts. To ensure communication efficiency, TRIP
incorporates a parameter-free routing mechanism based on token clustering and
optimal transport. The instance-specific prompt is then synthesized by
aggregating experts, weighted by the number of tokens assigned to each.
Additionally, TRIP develops an unbiased learning strategy for prompt experts,
leveraging the VLM's zero-shot generalization capability. Extensive experiments
across four benchmarks demonstrate that TRIP achieves optimal generalization
results, with communication of only 1K parameters per round. Our code is
available at https://github.com/GongShuai8210/TRIP.

</details>


### [228] [Frequency Feature Fusion Graph Network For Depression Diagnosis Via fNIRS](https://arxiv.org/abs/2504.21064)
*Chengkai Yang, Xingping Dong, Xiaofen Zong*

Main category: cs.LG

TL;DR: A novel DFT-based biomarker and TGCN architecture improve depression diagnosis by enhancing temporal brain connectivity representation, validated on a large dataset and PSM subset.


<details>
  <summary>Details</summary>
Motivation: Existing GNN models lack robust temporal biomarkers for depression diagnosis, prompting the need for a more effective solution.

Method: Proposed a DFT-based biomarker and a customized TGCN architecture, trained on a large dataset (1,086 subjects) and refined with PSM.

Result: Improved F1 scores in both real-world and PSM datasets, with SHAP validating model interpretability.

Conclusion: The new biomarker and model advance depression diagnostic tools, with potential for medical applications.

Abstract: Data-driven approaches for depression diagnosis have emerged as a significant
research focus in neuromedicine, driven by the development of relevant
datasets. Recently, graph neural network (GNN)-based models have gained
widespread adoption due to their ability to capture brain channel functional
connectivity from both spatial and temporal perspectives. However, their
effectiveness is hindered by the absence of a robust temporal biomarker. In
this paper, we introduce a novel and effective biomarker for depression
diagnosis by leveraging the discrete Fourier transform (DFT) and propose a
customized graph network architecture based on Temporal Graph Convolutional
Network (TGCN). Our model was trained on a dataset comprising 1,086 subjects,
which is over 10 times larger than previous datasets in the field of depression
diagnosis. Furthermore, to align with medical requirements, we performed
propensity score matching (PSM) to create a refined subset, referred to as the
PSM dataset. Experimental results demonstrate that incorporating our newly
designed biomarker enhances the representation of temporal characteristics in
brain channels, leading to improved F1 scores in both the real-world dataset
and the PSM dataset. This advancement has the potential to contribute to the
development of more effective depression diagnostic tools. In addition, we used
SHapley Additive exPlaination (SHAP) to validate the interpretability of our
model, ensuring its practical applicability in medical settings.

</details>


### [229] [A 3D pocket-aware and affinity-guided diffusion model for lead optimization](https://arxiv.org/abs/2504.21065)
*Anjie Qiao, Junjie Xie, Weifeng Huang, Hao Zhang, Jiahua Rao, Shuangjia Zheng, Yuedong Yang, Zhen Wang, Guo-Bo Li, Jinping Lei*

Main category: cs.LG

TL;DR: Diffleop, a 3D pocket-aware and affinity-guided diffusion model, improves molecular optimization by enhancing binding affinity, outperforming baseline models.


<details>
  <summary>Details</summary>
Motivation: Current deep learning-based 3D generative models often fail to adequately consider binding affinities during molecular optimization, limiting their effectiveness in drug discovery.

Method: Diffleop explicitly incorporates protein-ligand binding affinity knowledge to guide denoising sampling for high-affinity molecule generation.

Result: Diffleop outperforms baseline models across multiple metrics, particularly in binding affinity.

Conclusion: Diffleop offers a promising approach for efficient molecular optimization in drug discovery by leveraging binding affinity guidance.

Abstract: Molecular optimization, aimed at improving binding affinity or other
molecular properties, is a crucial task in drug discovery that often relies on
the expertise of medicinal chemists. Recently, deep learning-based 3D
generative models showed promise in enhancing the efficiency of molecular
optimization. However, these models often struggle to adequately consider
binding affinities with protein targets during lead optimization. Herein, we
propose a 3D pocket-aware and affinity-guided diffusion model, named Diffleop,
to optimize molecules with enhanced binding affinity. The model explicitly
incorporates the knowledge of protein-ligand binding affinity to guide the
denoising sampling for molecule generation with high affinity. The
comprehensive evaluations indicated that Diffleop outperforms baseline models
across multiple metrics, especially in terms of binding affinity.

</details>


### [230] [A Brief Review for Compression and Transfer Learning Techniques in DeepFake Detection](https://arxiv.org/abs/2504.21066)
*Andreas Karathanasis, John Violos, Ioannis Kompatsiaris, Symeon Papadopoulos*

Main category: cs.LG

TL;DR: The paper explores compression and transfer learning techniques to deploy deepfake detection models on edge devices, achieving effective performance even with high compression, but noting domain generalization issues with unseen DeepFake models.


<details>
  <summary>Details</summary>
Motivation: To enable deepfake detection on resource-constrained edge devices while maintaining data privacy and confidentiality.

Method: Evaluates pruning, knowledge distillation, quantization, fine-tuning, and adapter-based techniques on Synthbuster, RAISE, and ForenSynths datasets.

Result: Compression and transfer learning work well (even at 90% compression) for same-model data, but domain generalization fails with unseen DeepFake models.

Conclusion: Edge deployment is feasible with compression and transfer learning, but domain generalization remains a challenge for diverse DeepFake models.

Abstract: Training and deploying deepfake detection models on edge devices offers the
advantage of maintaining data privacy and confidentiality by processing it
close to its source. However, this approach is constrained by the limited
computational and memory resources available at the edge. To address this
challenge, we explore compression techniques to reduce computational demands
and inference time, alongside transfer learning methods to minimize training
overhead. Using the Synthbuster, RAISE, and ForenSynths datasets, we evaluate
the effectiveness of pruning, knowledge distillation (KD), quantization,
fine-tuning, and adapter-based techniques. Our experimental results demonstrate
that both compression and transfer learning can be effectively achieved, even
with a high compression level of 90%, remaining at the same performance level
when the training and validation data originate from the same DeepFake model.
However, when the testing dataset is generated by DeepFake models not present
in the training set, a domain generalization issue becomes evident.

</details>


### [231] [R^2VFL: A Robust Random Vector Functional Link Network with Huber-Weighted Framework](https://arxiv.org/abs/2504.21069)
*Anuradha Kumari, Mushir Akhtar, P. N. Suganthan, M. Tanveer*

Main category: cs.LG

TL;DR: The paper introduces R2VFL, a robust RVFL neural network variant using Huber weighting and class probability to handle noise and outliers, outperforming traditional RVFL on 47 UCI datasets and EEG signal classification.


<details>
  <summary>Details</summary>
Motivation: RVFL networks struggle with noise and outliers due to equal data sample weighting. The goal is to enhance robustness and adaptability.

Method: Proposes R2VFL with Huber weighting and class probability, introducing two variants (R2VFL-A and R2VFL-M) for class center calculation.

Result: Superior performance on 47 UCI datasets and EEG signal classification, validated by statistical testing.

Conclusion: R2VFL effectively mitigates noise and outlier impact, proving practical for real-world biomedical applications.

Abstract: The random vector functional link (RVFL) neural network has shown significant
potential in overcoming the constraints of traditional artificial neural
networks, such as excessive computation time and suboptimal solutions. However,
RVFL faces challenges when dealing with noise and outliers, as it assumes all
data samples contribute equally. To address this issue, we propose a novel
robust framework, R2VFL, RVFL with Huber weighting function and class
probability, which enhances the model's robustness and adaptability by
effectively mitigating the impact of noise and outliers in the training data.
The Huber weighting function reduces the influence of outliers, while the class
probability mechanism assigns less weight to noisy data points, resulting in a
more resilient model. We explore two distinct approaches for calculating class
centers within the R2VFL framework: the simple average of all data points in
each class and the median of each feature, the later providing a robust
alternative by minimizing the effect of extreme values. These approaches give
rise to two novel variants of the model-R2VFL-A and R2VFL-M. We extensively
evaluate the proposed models on 47 UCI datasets, encompassing both binary and
multiclass datasets, and conduct rigorous statistical testing, which confirms
the superiority of the proposed models. Notably, the models also demonstrate
exceptional performance in classifying EEG signals, highlighting their
practical applicability in real-world biomedical domain.

</details>


### [232] [A Survey on Parameter-Efficient Fine-Tuning for Foundation Models in Federated Learning](https://arxiv.org/abs/2504.21099)
*Jieming Bian, Yuanzhe Peng, Lei Wang, Yin Huang, Jie Xu*

Main category: cs.LG

TL;DR: A survey on integrating Parameter-Efficient Fine-Tuning (PEFT) methods into Federated Learning (FL) to address computational and privacy challenges.


<details>
  <summary>Details</summary>
Motivation: To reduce the computational cost of fine-tuning large foundation models in FL while maintaining privacy and efficiency.

Method: Categorizes PEFT techniques into Additive, Selective, and Reparameterized PEFT, analyzing their application in FL settings.

Result: Systematic review of PEFT methods in FL, addressing data heterogeneity, communication efficiency, and privacy.

Conclusion: Highlights future research directions like scaling PEFT for larger models and theoretical analysis in FL.

Abstract: Foundation models have revolutionized artificial intelligence by providing
robust, versatile architectures pre-trained on large-scale datasets. However,
adapting these massive models to specific downstream tasks requires
fine-tuning, which can be prohibitively expensive in computational resources.
Parameter-Efficient Fine-Tuning (PEFT) methods address this challenge by
selectively updating only a small subset of parameters. Meanwhile, Federated
Learning (FL) enables collaborative model training across distributed clients
without sharing raw data, making it ideal for privacy-sensitive applications.
This survey provides a comprehensive review of the integration of PEFT
techniques within federated learning environments. We systematically categorize
existing approaches into three main groups: Additive PEFT (which introduces new
trainable parameters), Selective PEFT (which fine-tunes only subsets of
existing parameters), and Reparameterized PEFT (which transforms model
architectures to enable efficient updates). For each category, we analyze how
these methods address the unique challenges of federated settings, including
data heterogeneity, communication efficiency, computational constraints, and
privacy concerns. We further organize the literature based on application
domains, covering both natural language processing and computer vision tasks.
Finally, we discuss promising research directions, including scaling to larger
foundation models, theoretical analysis of federated PEFT methods, and
sustainable approaches for resource-constrained environments.

</details>


### [233] [SMOGAN: Synthetic Minority Oversampling with GAN Refinement for Imbalanced Regression](https://arxiv.org/abs/2504.21152)
*Shayan Alahyari, Mike Domaratzki*

Main category: cs.LG

TL;DR: SMOGAN is a two-step oversampling framework for imbalanced regression, using DistGAN to refine synthetic samples and improve performance on underrepresented data.


<details>
  <summary>Details</summary>
Motivation: Imbalanced regression hinders model performance on minority samples, and existing methods often fail to capture complex data distributions.

Method: SMOGAN combines initial oversampling with DistGAN, a distribution-aware GAN, to refine synthetic samples and align them with the true data distribution.

Result: SMOGAN outperforms default oversampling methods on 23 imbalanced datasets.

Conclusion: SMOGAN effectively addresses imbalanced regression by generating high-quality synthetic samples, improving model performance on underrepresented data.

Abstract: Imbalanced regression refers to prediction tasks where the target variable is
skewed. This skewness hinders machine learning models, especially neural
networks, which concentrate on dense regions and therefore perform poorly on
underrepresented (minority) samples. Despite the importance of this problem,
only a few methods have been proposed for imbalanced regression. Many of the
available solutions for imbalanced regression adapt techniques from the class
imbalance domain, such as linear interpolation and the addition of Gaussian
noise, to create synthetic data in sparse regions. However, in many cases, the
underlying distribution of the data is complex and non-linear. Consequently,
these approaches generate synthetic samples that do not accurately represent
the true feature-target relationship. To overcome these limitations, we propose
SMOGAN, a two-step oversampling framework for imbalanced regression. In Stage
1, an existing oversampler generates initial synthetic samples in sparse target
regions. In Stage 2, we introduce DistGAN, a distribution-aware GAN that serves
as SMOGAN's filtering layer and refines these samples via adversarial loss
augmented with a Maximum Mean Discrepancy objective, aligning them with the
true joint feature-target distribution. Extensive experiments on 23 imbalanced
datasets show that SMOGAN consistently outperforms the default oversampling
method without the DistGAN filtering layer.

</details>


### [234] [SmoothSegNet: A Global-Local Framework for Liver Tumor Segmentation with Clinical KnowledgeInformed Label Smoothing](https://arxiv.org/abs/2410.10005)
*Hairong Wang, Lingchao Mao, Zihan Zhang, Jing Li*

Main category: cs.LG

TL;DR: SmoothSegNet, a deep learning framework, improves liver tumor segmentation in CT scans using knowledge-informed label smoothing, a global-local segmentation approach, and customized pre/post-processing.


<details>
  <summary>Details</summary>
Motivation: Liver cancer's high mortality and the limitations of manual CT-based tumor segmentation drive the need for reliable automated solutions.

Method: SmoothSegNet employs knowledge-informed label smoothing, a global-local segmentation framework, and task-specific pre/post-processing pipelines.

Result: SmoothSegNet outperforms benchmarks, especially for small tumors (<10cm), with ablation studies confirming the contributions of its design components.

Conclusion: SmoothSegNet effectively addresses liver tumor segmentation challenges, offering improved accuracy and performance.

Abstract: Liver cancer is a leading cause of mortality worldwide, and accurate Computed
Tomography (CT)-based tumor segmentation is essential for diagnosis and
treatment. Manual delineation is time-intensive, prone to variability, and
highlights the need for reliable automation. While deep learning has shown
promise for automated liver segmentation, precise liver tumor segmentation
remains challenging due to the heterogeneous nature of tumors, imprecise tumor
margins, and limited labeled data. We present SmoothSegNet, a novel deep
learning framework that addresses these challenges with the three key designs:
(1) A novel knowledge-informed label smoothing technique that distills
knowledge from clinical data to generate smooth labels, which are used to
regularize model training, reducing the overfitting risk and enhancing model
performance; (2) A global and local segmentation framework that breaks down the
main task into two simpler sub-tasks, allowing optimized preprocessing and
training for each; and (3) Pre- and post-processing pipelines customized to the
challenges of each subtask aimed to enhance tumor visibility and refines tumor
boundaries. We apply the proposed model on a challenging HCC-TACE-Seg dataset
and show that SmoothSegNet outperformed various benchmarks in segmentation
performance, particularly at smaller tumors (<10cm). Our ablation studies show
that the three design components complementarily contribute to the model
improved performance. Code for the proposed method are available at
https://github.com/lingchm/medassist-liver-cancer.

</details>


### [235] [Efficient LLMs with AMP: Attention Heads and MLP Pruning](https://arxiv.org/abs/2504.21174)
*Leandro Giusti Mugnaini, Bruno Lopes Yamamoto, Lucas Lauton de Alcantara, Victor Zacarias, Edson Bollis, Lucas Pellicer, Anna Helena Reali Costa, Artur Jordao*

Main category: cs.LG

TL;DR: AMP is a novel structured pruning method for LLMs, reducing model size while maintaining performance, improving inference speed, and being deployable in resource-limited settings.


<details>
  <summary>Details</summary>
Motivation: Large Language Models (LLMs) face high computational costs and slow inference due to extensive parameters, hindering deployment in resource-limited environments.

Method: AMP prunes less critical structures in Multi-Head Attention (MHA) and Multilayer Perceptron (MLP) by projecting input data onto weights to assess importance.

Result: AMP achieves a 30% pruning ratio with minimal performance loss, outperforms state-of-the-art by 1.49 points on commonsense reasoning, and speeds up inference.

Conclusion: AMP is a flexible and efficient pruning method suitable for various LLMs, enhancing deployability in constrained environments.

Abstract: Deep learning drives a new wave in computing systems and triggers the
automation of increasingly complex problems. In particular, Large Language
Models (LLMs) have significantly advanced cognitive tasks, often matching or
even surpassing human-level performance. However, their extensive parameters
result in high computational costs and slow inference, posing challenges for
deployment in resource-limited settings. Among the strategies to overcome the
aforementioned challenges, pruning emerges as a successful mechanism since it
reduces model size while maintaining predictive ability. In this paper, we
introduce AMP: Attention Heads and MLP Pruning, a novel structured pruning
method that efficiently compresses LLMs by removing less critical structures
within Multi-Head Attention (MHA) and Multilayer Perceptron (MLP). By
projecting the input data onto weights, AMP assesses structural importance and
overcomes the limitations of existing techniques, which often fall short in
flexibility or efficiency. In particular, AMP surpasses the current
state-of-the-art on commonsense reasoning tasks by up to 1.49 percentage
points, achieving a 30% pruning ratio with minimal impact on zero-shot task
performance. Moreover, AMP also improves inference speeds, making it
well-suited for deployment in resource-constrained environments. We confirm the
flexibility of AMP on different families of LLMs, including LLaMA and Phi.

</details>


### [236] [GLIP-OOD: Zero-Shot Graph OOD Detection with Foundation Model](https://arxiv.org/abs/2504.21186)
*Haoyan Xu, Zhengtao Yao, Xuzhi Zhang, Ziyi Wang, Langzhou He, Yushun Dong, Philip S. Yu, Mengyuan Li, Yue Zhao*

Main category: cs.LG

TL;DR: The paper introduces a zero-shot OOD detection method for graph-structured data using a graph foundation model (GFM) and a novel framework (GLIP-OOD) for generating pseudo-OOD labels when OOD label names are unavailable.


<details>
  <summary>Details</summary>
Motivation: OOD detection is crucial for ML safety, but zero-shot methods for graphs are underdeveloped due to complex relational structures and lack of pretrained models.

Method: Leverages a GFM for zero-shot OOD detection and introduces GLIP-OOD, using LLMs to generate pseudo-OOD labels from unlabeled data.

Result: Outperforms supervised methods and achieves state-of-the-art performance on benchmark datasets.

Conclusion: The approach enables fully zero-shot node-level graph OOD detection, addressing a critical gap in the field.

Abstract: Out-of-distribution (OOD) detection is critical for ensuring the safety and
reliability of machine learning systems, particularly in dynamic and open-world
environments. In the vision and text domains, zero-shot OOD detection - which
requires no training on in-distribution (ID) data - has made significant
progress through the use of large-scale pretrained models such as
vision-language models (VLMs) and large language models (LLMs). However,
zero-shot OOD detection in graph-structured data remains largely unexplored,
primarily due to the challenges posed by complex relational structures and the
absence of powerful, large-scale pretrained models for graphs. In this work, we
take the first step toward enabling zero-shot graph OOD detection by leveraging
a graph foundation model (GFM). We show that, when provided only with class
label names, the GFM can perform OOD detection without any node-level
supervision - outperforming existing supervised methods across multiple
datasets. To address the more practical setting where OOD label names are
unavailable, we introduce GLIP-OOD, a novel framework that employs LLMs to
generate semantically informative pseudo-OOD labels from unlabeled data. These
labels enable the GFM to capture nuanced semantic boundaries between ID and OOD
classes and perform fine-grained OOD detection - without requiring any labeled
nodes. Our approach is the first to enable node-level graph OOD detection in a
fully zero-shot setting, and achieves state-of-the-art performance on four
benchmark text-attributed graph datasets.

</details>


### [237] [LIFT: LLM-Based Pragma Insertion for HLS via GNN Supervised Fine-Tuning](https://arxiv.org/abs/2504.21187)
*Neha Prakriya, Zijian Ding, Yizhou Sun, Jason Cong*

Main category: cs.LG

TL;DR: LIFT is an LLM-based coding assistant for HLS that automates pragma insertion, improving FPGA performance significantly over prior methods.


<details>
  <summary>Details</summary>
Motivation: FPGA programming with HLS still requires expert knowledge for optimization. LIFT aims to automate this process.

Method: Fine-tunes an LLM integrated with a GNN to generate performance-critical pragmas for C/C++ designs.

Result: LIFT improves performance by 3.52x over AutoDSE, 2.16x over HARP, and 66x over GPT-4o.

Conclusion: LIFT effectively automates HLS optimization, reducing reliance on expert knowledge and boosting performance.

Abstract: FPGAs are increasingly adopted in datacenter environments for their
reconfigurability and energy efficiency. High-Level Synthesis (HLS) tools have
eased FPGA programming by raising the abstraction level from RTL to untimed
C/C++, yet attaining high performance still demands expert knowledge and
iterative manual insertion of optimization pragmas to modify the
microarchitecture. To address this challenge, we propose LIFT, a large language
model (LLM)-based coding assistant for HLS that automatically generates
performance-critical pragmas given a C/C++ design. We fine-tune the LLM by
tightly integrating and supervising the training process with a graph neural
network (GNN), combining the sequential modeling capabilities of LLMs with the
structural and semantic understanding of GNNs necessary for reasoning over code
and its control/data dependencies. On average, LIFT produces designs that
improve performance by 3.52x and 2.16x than prior state-of the art AutoDSE and
HARP respectively, and 66x than GPT-4o.

</details>


### [238] [Artificial Intelligence for Personalized Prediction of Alzheimer's Disease Progression: A Survey of Methods, Data Challenges, and Future Directions](https://arxiv.org/abs/2504.21189)
*Gulsah Hancerliogullari Koksalmis, Bulent Soykan, Laura J. Brattain, Hsin-Hsiung Huang*

Main category: cs.LG

TL;DR: A survey of AI methodologies for predicting personalized Alzheimer's Disease progression, covering techniques like state-space models, deep learning, and GNNs, while addressing data challenges and future research directions.


<details>
  <summary>Details</summary>
Motivation: The variability in Alzheimer's Disease progression complicates prognosis and personalized care, necessitating AI-driven predictive models.

Method: Review of AI approaches including state-space models, RNNs, GNNs, and digital twins, plus strategies for handling data limitations like synthetic data generation.

Result: Synthesis of strengths and limitations of current methods, highlighting multimodal integration and the need for interpretability and generalizability.

Conclusion: Identifies open challenges (validation, clinical integration, ethics) and future directions (hybrid models, causal inference, federated learning) for AI in AD prognostication.

Abstract: Alzheimer's Disease (AD) is marked by significant inter-individual
variability in its progression, complicating accurate prognosis and
personalized care planning. This heterogeneity underscores the critical need
for predictive models capable of forecasting patient-specific disease
trajectories. Artificial Intelligence (AI) offers powerful tools to address
this challenge by analyzing complex, multi-modal, and longitudinal patient
data. This paper provides a comprehensive survey of AI methodologies applied to
personalized AD progression prediction. We review key approaches including
state-space models for capturing temporal dynamics, deep learning techniques
like Recurrent Neural Networks for sequence modeling, Graph Neural Networks
(GNNs) for leveraging network structures, and the emerging concept of AI-driven
digital twins for individualized simulation. Recognizing that data limitations
often impede progress, we examine common challenges such as high
dimensionality, missing data, and dataset imbalance. We further discuss
AI-driven mitigation strategies, with a specific focus on synthetic data
generation using Variational Autoencoders (VAEs) and Generative Adversarial
Networks (GANs) to augment and balance datasets. The survey synthesizes the
strengths and limitations of current approaches, emphasizing the trend towards
multimodal integration and the persistent need for model interpretability and
generalizability. Finally, we identify critical open challenges, including
robust external validation, clinical integration, and ethical considerations,
and outline promising future research directions such as hybrid models, causal
inference, and federated learning. This review aims to consolidate current
knowledge and guide future efforts in developing clinically relevant AI tools
for personalized AD prognostication.

</details>


### [239] [TT-LoRA MoE: Unifying Parameter-Efficient Fine-Tuning and Sparse Mixture-of-Experts](https://arxiv.org/abs/2504.21190)
*Pradip Kunwar, Minh N. Vu, Maanak Gupta, Mahmoud Abdelsalam, Manish Bhattarai*

Main category: cs.LG

TL;DR: TT-LoRA MoE combines tensorized low-rank adapters with sparse MoE routing for scalable, efficient multi-task fine-tuning, outperforming traditional methods.


<details>
  <summary>Details</summary>
Motivation: Address scalability and computational overhead in large model deployments, especially in multi-task settings, by decoupling expert training and routing.

Method: Two-stage training: (1) train tensorized low-rank adapters (experts) independently, then freeze them; (2) train a sparse MoE router for dynamic expert selection.

Result: Achieves memory efficiency (2% LoRA, 0.3% Adapters, 0.03% AdapterFusion parameters) and outperforms AdapterFusion by 4% in multi-tasking.

Conclusion: TT-LoRA MoE enhances computational efficiency and flexibility, enabling practical, scalable multi-task inference.

Abstract: We propose Tensor-Trained Low-Rank Adaptation Mixture of Experts (TT-LoRA
MoE), a novel computational framework integrating Parameter-Efficient
Fine-Tuning (PEFT) with sparse MoE routing to address scalability challenges in
large model deployments. Unlike traditional MoE approaches, which face
substantial computational overhead as expert counts grow, TT-LoRA MoE
decomposes training into two distinct, optimized stages. First, we
independently train lightweight, tensorized low-rank adapters (TT-LoRA
experts), each specialized for specific tasks. Subsequently, these expert
adapters remain frozen, eliminating inter-task interference and catastrophic
forgetting in multi-task setting. A sparse MoE router, trained separately,
dynamically leverages base model representations to select exactly one
specialized adapter per input at inference time, automating expert selection
without explicit task specification. Comprehensive experiments confirm our
architecture retains the memory efficiency of low-rank adapters, seamlessly
scales to large expert pools, and achieves robust task-level optimization. This
structured decoupling significantly enhances computational efficiency and
flexibility: uses only 2% of LoRA, 0.3% of Adapters and 0.03% of AdapterFusion
parameters and outperforms AdapterFusion by 4 value in multi-tasking, enabling
practical and scalable multi-task inference deployments.

</details>


### [240] [Graph Synthetic Out-of-Distribution Exposure with Large Language Models](https://arxiv.org/abs/2504.21198)
*Haoyan Xu, Zhengtao Yao, Ziyi Wang, Zhan Cheng, Xiyang Hu, Mengyuan Li, Yue Zhao*

Main category: cs.LG

TL;DR: GOE-LLM uses LLMs to generate pseudo-OOD nodes for graph OOD detection, eliminating the need for real OOD data and outperforming existing methods.


<details>
  <summary>Details</summary>
Motivation: Current graph OOD detection methods rely on real OOD samples, which are hard to obtain. GOE-LLM addresses this by leveraging LLMs for pseudo-OOD generation.

Method: GOE-LLM employs two pipelines: zero-shot LLM annotations to identify pseudo-OOD nodes and LLM-prompted text generation to create synthetic OOD nodes. These are used to train the ID classifier.

Result: GOE-LLM outperforms state-of-the-art methods without OOD exposure and matches those using real OOD data.

Conclusion: GOE-LLM provides a practical and effective solution for graph OOD detection by leveraging LLMs, reducing reliance on real OOD samples.

Abstract: Out-of-distribution (OOD) detection in graphs is critical for ensuring model
robustness in open-world and safety-sensitive applications. Existing approaches
to graph OOD detection typically involve training an in-distribution (ID)
classifier using only ID data, followed by the application of post-hoc OOD
scoring techniques. Although OOD exposure - introducing auxiliary OOD samples
during training - has proven to be an effective strategy for enhancing
detection performance, current methods in the graph domain generally assume
access to a set of real OOD nodes. This assumption, however, is often
impractical due to the difficulty and cost of acquiring representative OOD
samples. In this paper, we introduce GOE-LLM, a novel framework that leverages
Large Language Models (LLMs) for OOD exposure in graph OOD detection without
requiring real OOD nodes. GOE-LLM introduces two pipelines: (1) identifying
pseudo-OOD nodes from the initially unlabeled graph using zero-shot LLM
annotations, and (2) generating semantically informative synthetic OOD nodes
via LLM-prompted text generation. These pseudo-OOD nodes are then used to
regularize the training of the ID classifier for improved OOD awareness. We
evaluate our approach across multiple benchmark datasets, showing that GOE-LLM
significantly outperforms state-of-the-art graph OOD detection methods that do
not use OOD exposure and achieves comparable performance to those relying on
real OOD data.

</details>


### [241] [FedHERO: A Federated Learning Approach for Node Classification Task on Heterophilic Graphs](https://arxiv.org/abs/2504.21206)
*Zihan Chen, Xingbo Fu, Yushun Dong, Jundong Li, Cong Shen*

Main category: cs.LG

TL;DR: FedHERO is a Federated Graph Learning framework designed to handle heterophilic graphs by using a dual-channel GNN with a structure learner, improving local and global model performance.


<details>
  <summary>Details</summary>
Motivation: Existing FGL methods assume homophilic graphs, leading to performance issues with heterophilic graphs. FedHERO addresses this limitation.

Method: FedHERO employs a dual-channel GNN with a structure learner to identify universally applicable patterns across graphs with varying node neighbor distributions.

Result: FedHERO outperforms existing methods, enhancing both local client models and the global model.

Conclusion: FedHERO sets a new standard for handling heterophilic graphs in FGL, demonstrating superior performance.

Abstract: Federated Graph Learning (FGL) empowers clients to collaboratively train
Graph neural networks (GNNs) in a distributed manner while preserving data
privacy. However, FGL methods usually require that the graph data owned by all
clients is homophilic to ensure similar neighbor distribution patterns of
nodes. Such an assumption ensures that the learned knowledge is consistent
across the local models from all clients. Therefore, these local models can be
properly aggregated as a global model without undermining the overall
performance. Nevertheless, when the neighbor distribution patterns of nodes
vary across different clients (e.g., when clients hold graphs with different
levels of heterophily), their local models may gain different and even conflict
knowledge from their node-level predictive tasks. Consequently, aggregating
these local models usually leads to catastrophic performance deterioration on
the global model. To address this challenge, we propose FedHERO, an FGL
framework designed to harness and share insights from heterophilic graphs
effectively. At the heart of FedHERO is a dual-channel GNN equipped with a
structure learner, engineered to discern the structural knowledge encoded in
the local graphs. With this specialized component, FedHERO enables the local
model for each client to identify and learn patterns that are universally
applicable across graphs with different patterns of node neighbor
distributions. FedHERO not only enhances the performance of individual client
models by leveraging both local and shared structural insights but also sets a
new precedent in this field to effectively handle graph data with various node
neighbor distribution patterns. We conduct extensive experiments to validate
the superior performance of FedHERO against existing alternatives.

</details>


### [242] [A Cost-Effective LLM-based Approach to Identify Wildlife Trafficking in Online Marketplaces](https://arxiv.org/abs/2504.21211)
*Juliana Barbosa, Ulhas Gondhali, Gohar Petrossian, Kinshuk Sharma, Sunandan Chakraborty, Jennifer Jacquet, Juliana Freire*

Main category: cs.LG

TL;DR: The paper proposes a cost-effective method using LLMs to generate pseudo labels for wildlife trafficking ads, enabling efficient classifier training with high accuracy.


<details>
  <summary>Details</summary>
Motivation: Wildlife trafficking is a global issue exacerbated by e-commerce, but identifying illegal ads is challenging. Current methods require expensive data labeling.

Method: Leverages LLMs to generate pseudo labels for a small data sample, then trains specialized classifiers. Focuses on diverse, representative sampling to minimize labeling costs.

Result: Classifiers achieve up to 95% F1 score, outperforming LLMs at lower cost. Demonstrated effectiveness in real-world wildlife trafficking analyses.

Conclusion: The method provides a scalable, cost-effective solution for identifying wildlife trafficking ads, aiding conservation and enforcement efforts.

Abstract: Wildlife trafficking remains a critical global issue, significantly impacting
biodiversity, ecological stability, and public health. Despite efforts to
combat this illicit trade, the rise of e-commerce platforms has made it easier
to sell wildlife products, putting new pressure on wild populations of
endangered and threatened species. The use of these platforms also opens a new
opportunity: as criminals sell wildlife products online, they leave digital
traces of their activity that can provide insights into trafficking activities
as well as how they can be disrupted. The challenge lies in finding these
traces. Online marketplaces publish ads for a plethora of products, and
identifying ads for wildlife-related products is like finding a needle in a
haystack. Learning classifiers can automate ad identification, but creating
them requires costly, time-consuming data labeling that hinders support for
diverse ads and research questions. This paper addresses a critical challenge
in the data science pipeline for wildlife trafficking analytics: generating
quality labeled data for classifiers that select relevant data. While large
language models (LLMs) can directly label advertisements, doing so at scale is
prohibitively expensive. We propose a cost-effective strategy that leverages
LLMs to generate pseudo labels for a small sample of the data and uses these
labels to create specialized classification models. Our novel method
automatically gathers diverse and representative samples to be labeled while
minimizing the labeling costs. Our experimental evaluation shows that our
classifiers achieve up to 95% F1 score, outperforming LLMs at a lower cost. We
present real use cases that demonstrate the effectiveness of our approach in
enabling analyses of different aspects of wildlife trafficking.

</details>


### [243] [ABG-NAS: Adaptive Bayesian Genetic Neural Architecture Search for Graph Representation Learning](https://arxiv.org/abs/2504.21254)
*Sixuan Wang, Jiao Yin, Jinli Cao, MingJian Tang, Hua Wang, Yanchun Zhang*

Main category: cs.LG

TL;DR: ABG-NAS is a novel framework for automated graph neural network architecture search, outperforming manual and existing NAS methods in graph representation learning.


<details>
  <summary>Details</summary>
Motivation: Existing GNNs struggle with diverse graph structures, limiting robust and generalizable representations.

Method: ABG-NAS includes a Comprehensive Architecture Search Space (CASS), Adaptive Genetic Optimization Strategy (AGOS), and Bayesian-Guided Tuning Module (BGTM).

Result: ABG-NAS outperforms manual GNNs and NAS methods on benchmark datasets (Cora, PubMed, Citeseer, CoraFull).

Conclusion: ABG-NAS advances graph representation learning with scalable, adaptive solutions for diverse graph structures.

Abstract: Effective and efficient graph representation learning is essential for
enabling critical downstream tasks, such as node classification, link
prediction, and subgraph search. However, existing graph neural network (GNN)
architectures often struggle to adapt to diverse and complex graph structures,
limiting their ability to provide robust and generalizable representations. To
address this challenge, we propose ABG-NAS, a novel framework for automated
graph neural network architecture search tailored for efficient graph
representation learning. ABG-NAS encompasses three key components: a
Comprehensive Architecture Search Space (CASS), an Adaptive Genetic
Optimization Strategy (AGOS), and a Bayesian-Guided Tuning Module (BGTM). CASS
systematically explores diverse propagation (P) and transformation (T)
operations, enabling the discovery of GNN architectures capable of capturing
intricate graph characteristics. AGOS dynamically balances exploration and
exploitation, ensuring search efficiency and preserving solution diversity.
BGTM further optimizes hyperparameters periodically, enhancing the scalability
and robustness of the resulting architectures. Empirical evaluations on
benchmark datasets (Cora, PubMed, Citeseer, and CoraFull) demonstrate that
ABG-NAS consistently outperforms both manually designed GNNs and
state-of-the-art neural architecture search (NAS) methods. These results
highlight the potential of ABG-NAS to advance graph representation learning by
providing scalable and adaptive solutions for diverse graph structures. Our
code is publicly available at https://github.com/sserranw/ABG-NAS.

</details>


### [244] [Multi-Domain Causal Discovery in Bijective Causal Models](https://arxiv.org/abs/2504.21261)
*Kasra Jalaldoust, Saber Salehkaleybar, Negar Kiyavash*

Main category: cs.LG

TL;DR: The paper proposes a method for causal discovery in multi-domain settings using bijective generation mechanisms (BGM), which relaxes functional assumptions and works under causal sufficiency.


<details>
  <summary>Details</summary>
Motivation: To address causal discovery in multi-domain settings where causal functions are invariant but noise distributions vary, aiming to generalize existing models and reduce functional restrictions.

Method: Uses bijective generation mechanisms (BGM) to ensure bijective and differentiable relations between noise and variables, enabling causal diagram discovery. Also derives a statistical test for identifying parent sets.

Result: Validated on synthetic and real-world datasets, showing BGM generalizes models like additive noise, LiNGAM, and location-scale noise.

Conclusion: BGM enables causal discovery under less restrictive assumptions, generalizing existing models and proving effective in experiments.

Abstract: We consider the problem of causal discovery (a.k.a., causal structure
learning) in a multi-domain setting. We assume that the causal functions are
invariant across the domains, while the distribution of the exogenous noise may
vary. Under causal sufficiency (i.e., no confounders exist), we show that the
causal diagram can be discovered under less restrictive functional assumptions
compared to previous work. What enables causal discovery in this setting is
bijective generation mechanisms (BGM), which ensures that the functional
relation between the exogenous noise $E$ and the endogenous variable $Y$ is
bijective and differentiable in both directions at every level of the cause
variable $X = x$. BGM generalizes a variety of models including additive noise
model, LiNGAM, post-nonlinear model, and location-scale noise model. Further,
we derive a statistical test to find the parents set of the target variable.
Experiments on various synthetic and real-world datasets validate our
theoretical findings.

</details>


### [245] [Orthogonal Factor-Based Biclustering Algorithm (BCBOF) for High-Dimensional Data and Its Application in Stock Trend Prediction](https://arxiv.org/abs/2504.21289)
*Yan Huang, Da-Qing Zhang*

Main category: cs.LG

TL;DR: Proposed BCBOF, an orthogonal factor-based biclustering algorithm, addresses high-dimensional data sparsity and preserves local structures. Applied to stock prediction, it outperforms existing methods and improves trading returns.


<details>
  <summary>Details</summary>
Motivation: Traditional biclustering fails in high-dimensional data due to distance concentration and disrupted local patterns.

Method: Construct orthogonal factors, cluster in orthogonal subspace, and transform results into fuzzy rules for stock prediction.

Result: BCBOF outperforms existing methods and enhances trading returns in virtual experiments.

Conclusion: BCBOF effectively handles high-dimensional data and improves stock trend prediction and trading strategies.

Abstract: Biclustering is an effective technique in data mining and pattern
recognition. Biclustering algorithms based on traditional clustering face two
fundamental limitations when processing high-dimensional data: (1) The distance
concentration phenomenon in high-dimensional spaces leads to data sparsity,
rendering similarity measures ineffective; (2) Mainstream linear dimensionality
reduction methods disrupt critical local structural patterns. To apply
biclustering to high-dimensional datasets, we propose an orthogonal
factor-based biclustering algorithm (BCBOF). First, we constructed orthogonal
factors in the vector space of the high-dimensional dataset. Then, we performed
clustering using the coordinates of the original data in the orthogonal
subspace as clustering targets. Finally, we obtained biclustering results of
the original dataset. Since dimensionality reduction was applied before
clustering, the proposed algorithm effectively mitigated the data sparsity
problem caused by high dimensionality. Additionally, we applied this
biclustering algorithm to stock technical indicator combinations and stock
price trend prediction. Biclustering results were transformed into fuzzy rules,
and we incorporated profit-preserving and stop-loss rules into the rule set,
ultimately forming a fuzzy inference system for stock price trend predictions
and trading signals. To evaluate the performance of BCBOF, we compared it with
existing biclustering methods using multiple evaluation metrics. The results
showed that our algorithm outperformed other biclustering techniques. To
validate the effectiveness of the fuzzy inference system, we conducted virtual
trading experiments using historical data from 10 A-share stocks. The
experimental results showed that the generated trading strategies yielded
higher returns for investors.

</details>


### [246] [Fairness in Graph Learning Augmented with Machine Learning: A Survey](https://arxiv.org/abs/2504.21296)
*Renqiang Luo, Ziqi Xu, Xikun Zhang, Qing Qing, Huafei Huang, Enyan Dai, Zhe Wang, Bo Yang*

Main category: cs.LG

TL;DR: The paper examines fairness challenges in Graph Learning augmented with Machine Learning (GL-ML), highlighting the interplay between graph learning and ML techniques and exploring methods to improve fairness.


<details>
  <summary>Details</summary>
Motivation: Specialized ML techniques in graph learning enhance performance but introduce fairness challenges, risking discriminatory outcomes in critical applications.

Method: Systematically analyzes GL-ML fairness challenges and explores four key techniques to address them.

Result: Identifies root causes and implications of fairness issues, providing a foundation for future research.

Conclusion: The study underscores the need for fairness in GL-ML and sets a groundwork for further innovation in the field.

Abstract: Augmenting specialised machine learning techniques into traditional graph
learning models has achieved notable success across various domains, including
federated graph learning, dynamic graph learning, and graph transformers.
However, the intricate mechanisms of these specialised techniques introduce
significant challenges in maintaining model fairness, potentially resulting in
discriminatory outcomes in high-stakes applications such as recommendation
systems, disaster response, criminal justice, and loan approval. This paper
systematically examines the unique fairness challenges posed by Graph Learning
augmented with Machine Learning (GL-ML). It highlights the complex interplay
between graph learning mechanisms and machine learning techniques, emphasising
how the augmentation of machine learning both enhances and complicates
fairness. Additionally, we explore four critical techniques frequently employed
to improve fairness in GL-ML methods. By thoroughly investigating the root
causes and broader implications of fairness challenges in this rapidly evolving
field, this work establishes a robust foundation for future research and
innovation in GL-ML fairness.

</details>


### [247] [Unsupervised Feature Transformation via In-context Generation, Generator-critic LLM Agents, and Duet-play Teaming](https://arxiv.org/abs/2504.21304)
*Nanxu Gong, Xinyuan Wang, Wangyang Ying, Haoyue Bai, Sixun Dong, Haifeng Chen, Yanjie Fu*

Main category: cs.LG

TL;DR: A novel generator-critic framework using LLM agents for unsupervised feature transformation outperforms supervised methods in efficiency and robustness.


<details>
  <summary>Details</summary>
Motivation: High dimensionality and costly label collection in domains like material performance screening necessitate efficient, unsupervised feature transformation methods.

Method: Leverages a generator-critic duet-play framework with LLM agents: critic diagnoses data, generator produces transformations, and iterative refinement improves results.

Result: Outperforms supervised baselines in efficiency, robustness, and applicability across datasets.

Conclusion: The framework fills a gap in unsupervised feature transformation and can generalize to human-agent collaboration.

Abstract: Feature transformation involves generating a new set of features from the
original dataset to enhance the data's utility. In certain domains like
material performance screening, dimensionality is large and collecting labels
is expensive and lengthy. It highly necessitates transforming feature spaces
efficiently and without supervision to enhance data readiness and AI utility.
However, existing methods fall short in efficient navigation of a vast space of
feature combinations, and are mostly designed for supervised settings. To fill
this gap, our unique perspective is to leverage a generator-critic duet-play
teaming framework using LLM agents and in-context learning to derive
pseudo-supervision from unsupervised data. The framework consists of three
interconnected steps: (1) Critic agent diagnoses data to generate actionable
advice, (2) Generator agent produces tokenized feature transformations guided
by the critic's advice, and (3) Iterative refinement ensures continuous
improvement through feedback between agents. The generator-critic framework can
be generalized to human-agent collaborative generation, by replacing the critic
agent with human experts. Extensive experiments demonstrate that the proposed
framework outperforms even supervised baselines in feature transformation
efficiency, robustness, and practical applicability across diverse datasets.

</details>


### [248] [Capturing Conditional Dependence via Auto-regressive Diffusion Models](https://arxiv.org/abs/2504.21314)
*Xunpeng Huang, Yujin Han, Difan Zou, Yian Ma, Tong Zhang*

Main category: cs.LG

TL;DR: AR diffusion models improve conditional dependence capture in data compared to vanilla diffusion models, with minimal inference time increase.


<details>
  <summary>Details</summary>
Motivation: Vanilla diffusion models fail to capture high-level relationships and conditional dependencies in data, such as physical laws or object stability.

Method: The study examines auto-regressive (AR) diffusion models, develops theoretical sampling error results, and compares them to vanilla diffusion models.

Result: AR diffusion models reduce the approximation gap for data conditional distribution and maintain practical inference times. They outperform vanilla models when clear conditional dependence exists.

Conclusion: AR diffusion models are effective for capturing conditional dependencies in data without significant computational overhead, but they do not outperform vanilla models when such dependencies are absent.

Abstract: Diffusion models have demonstrated appealing performance in both image and
video generation. However, many works discover that they struggle to capture
important, high-level relationships that are present in the real world. For
example, they fail to learn physical laws from data, and even fail to
understand that the objects in the world exist in a stable fashion. This is due
to the fact that important conditional dependence structures are not adequately
captured in the vanilla diffusion models. In this work, we initiate an in-depth
study on strengthening the diffusion model to capture the conditional
dependence structures in the data. In particular, we examine the efficacy of
the auto-regressive (AR) diffusion models for such purpose and develop the
first theoretical results on the sampling error of AR diffusion models under
(possibly) the mildest data assumption. Our theoretical findings indicate that,
compared with typical diffusion models, the AR variant produces samples with a
reduced gap in approximating the data conditional distribution. On the other
hand, the overall inference time of the AR-diffusion models is only moderately
larger than that for the vanilla diffusion models, making them still practical
for large scale applications. We also provide empirical results showing that
when there is clear conditional dependence structure in the data, the AR
diffusion models captures such structure, whereas vanilla DDPM fails to do so.
On the other hand, when there is no obvious conditional dependence across
patches of the data, AR diffusion does not outperform DDPM.

</details>


### [249] [Q-function Decomposition with Intervention Semantics with Factored Action Spaces](https://arxiv.org/abs/2504.21326)
*Junkyu Lee, Tian Gao, Elliot Nelson, Miao Liu, Debarun Bhattacharjya, Songtao Lu*

Main category: cs.LG

TL;DR: The paper introduces action decomposed reinforcement learning (ADRL), a method to handle large combinatorial action spaces by projecting Q-functions to a lower-dimensional subspace, improving sample efficiency.


<details>
  <summary>Details</summary>
Motivation: Large combinatorial action spaces in reinforcement learning pose challenges. Existing methods use linear decomposition, but this paper explores unbiased Q-function decomposition via causal effect estimation.

Method: The approach projects Q-functions to a lower-dimensional subspace and ensures unbiasedness using causal effect estimation. It integrates this into standard model-free RL algorithms.

Result: ADRL improves sample complexity in model-based RL and outperforms baselines in online control and offline sepsis treatment environments.

Conclusion: ADRL effectively handles large action spaces and enhances sample efficiency, demonstrating practical benefits in real-world and simulated tasks.

Abstract: Many practical reinforcement learning environments have a discrete factored
action space that induces a large combinatorial set of actions, thereby posing
significant challenges. Existing approaches leverage the regular structure of
the action space and resort to a linear decomposition of Q-functions, which
avoids enumerating all combinations of factored actions. In this paper, we
consider Q-functions defined over a lower dimensional projected subspace of the
original action space, and study the condition for the unbiasedness of
decomposed Q-functions using causal effect estimation from the no unobserved
confounder setting in causal statistics. This leads to a general scheme which
we call action decomposed reinforcement learning that uses the projected
Q-functions to approximate the Q-function in standard model-free reinforcement
learning algorithms. The proposed approach is shown to improve sample
complexity in a model-based reinforcement learning setting. We demonstrate
improvements in sample efficiency compared to state-of-the-art baselines in
online continuous control environments and a real-world offline sepsis
treatment environment.

</details>


### [250] [A Generalized Meta Federated Learning Framework with Theoretical Convergence Guarantees](https://arxiv.org/abs/2504.21327)
*Mohammad Vahid Jamali, Hamid Saber, Jung Hyun Bae*

Main category: cs.LG

TL;DR: Meta federated learning (FL) generalizes conventional FL by optimizing for multiple fine-tuning steps, improving model personalization under heterogeneous data.


<details>
  <summary>Details</summary>
Motivation: To address the limitation of conventional meta FL, which only considers one fine-tuning step, especially in highly heterogeneous data scenarios.

Method: A generalized meta FL framework minimizing average loss after any number of fine-tuning steps, with a variant of FedAvg and theoretical convergence analysis.

Result: Superior accuracy and faster convergence demonstrated on real-world datasets compared to conventional approaches.

Conclusion: The proposed generalized meta FL framework effectively enhances model personalization and performance in heterogeneous settings.

Abstract: Meta federated learning (FL) is a personalized variant of FL, where multiple
agents collaborate on training an initial shared model without exchanging raw
data samples. The initial model should be trained in a way that current or new
agents can easily adapt it to their local datasets after one or a few
fine-tuning steps, thus improving the model personalization. Conventional meta
FL approaches minimize the average loss of agents on the local models obtained
after one step of fine-tuning. In practice, agents may need to apply several
fine-tuning steps to adapt the global model to their local data, especially
under highly heterogeneous data distributions across agents. To this end, we
present a generalized framework for the meta FL by minimizing the average loss
of agents on their local model after any arbitrary number $\nu$ of fine-tuning
steps. For this generalized framework, we present a variant of the well-known
federated averaging (FedAvg) algorithm and conduct a comprehensive theoretical
convergence analysis to characterize the convergence speed as well as behavior
of the meta loss functions in both the exact and approximated cases. Our
experiments on real-world datasets demonstrate superior accuracy and faster
convergence for the proposed scheme compared to conventional approaches.

</details>


### [251] [Multi-level datasets training method in Physics-Informed Neural Networks](https://arxiv.org/abs/2504.21328)
*Yao-Hsuan Tsai, Hsiao-Tung Juan, Pao-Hsiung Chiu, Chao-An Lin*

Main category: cs.LG

TL;DR: The paper proposes a multi-grid training approach for Physics-Informed Neural Networks (PINNs) to address accuracy and convergence issues in solving stiff and high-frequency PDEs, achieving significant accuracy improvements.


<details>
  <summary>Details</summary>
Motivation: PINNs struggle with stiff and high-frequency PDEs, leading to computational inefficiency and accuracy loss. The study aims to mitigate these issues by leveraging multi-grid methods.

Method: The approach uses multi-grid training with varying sample levels to simplify accuracy improvement without fine-tuning neural networks or hyperparameters. It is tested on 1D ODEs, 2D convection-diffusion equations, and Lid-driven cavity flows.

Result: The method improves prediction accuracy by 30% to 60% and successfully handles high-frequency PDEs, even at Re=5000.

Conclusion: The proposed multi-grid training framework effectively addresses PINNs' limitations for stiff and high-frequency problems, demonstrating broad applicability and synergy with transfer learning.

Abstract: Physics-Informed Neural Networks have emerged as a promising methodology for
solving PDEs, gaining significant attention in computer science and various
physics-related fields. Despite being demonstrated the ability to incorporate
the physics of laws for versatile applications, PINNs still struggle with the
challenging problems which are stiff to be solved and/or have high-frequency
components in the solutions, resulting in accuracy and convergence issues. It
may not only increase computational costs, but also lead to accuracy loss or
solution divergence. In this study, an alternative approach is proposed to
mitigate the above-mentioned problems. Inspired by the multi-grid method in CFD
community, the underlying idea of the current approach is to efficiently remove
different frequency errors via training with different levels of training
samples, resulting in a simpler way to improve the training accuracy without
spending time in fine-tuning of neural network structures, loss weights as well
as hyperparameters. To demonstrate the efficacy of current approach, we first
investigate canonical 1D ODE with high-frequency component and 2D
convection-diffusion equation with V-cycle training strategy. Finally, the
current method is employed for the classical benchmark problem of steady
Lid-driven cavity flows at different Reynolds numbers, to investigate the
applicability and efficacy for the problem involved multiple modes of high and
low frequency. By virtue of various training sequence modes, improvement
through predictions lead to 30% to 60% accuracy improvement. We also
investigate the synergies between current method and transfer learning
techniques for more challenging problems (i.e., higher Re). From the present
results, it also revealed that the current framework can produce good
predictions even for the case of Re=5000, demonstrating the ability to solve
complex high-frequency PDEs.

</details>


### [252] [Generative QoE Modeling: A Lightweight Approach for Telecom Networks](https://arxiv.org/abs/2504.21353)
*Vinti Nayar, Kanica Sachdev, Brejesh Lall*

Main category: cs.LG

TL;DR: A lightweight generative modeling framework using VQ-HMM for QoE prediction, balancing efficiency, interpretability, and accuracy.


<details>
  <summary>Details</summary>
Motivation: To optimize resource management and enhance user satisfaction in telecommunication and OTT services by addressing the limitations of deep learning models.

Method: Uses Vector Quantization (VQ) to transform network features into discrete symbols, integrated with a Hidden Markov Model (HMM) for temporal sequence modeling.

Result: Demonstrates viability in real-time, resource-constrained environments with improved predictive accuracy and low inference latency.

Conclusion: The VQ-HMM framework is a scalable alternative to deep learning, suitable for limited computational resources or latency-critical scenarios.

Abstract: Quality of Experience (QoE) prediction plays a crucial role in optimizing
resource management and enhancing user satisfaction across both
telecommunication and OTT services. While recent advances predominantly rely on
deep learning models, this study introduces a lightweight generative modeling
framework that balances computational efficiency, interpretability, and
predictive accuracy. By validating the use of Vector Quantization (VQ) as a
preprocessing technique, continuous network features are effectively
transformed into discrete categorical symbols, enabling integration with a
Hidden Markov Model (HMM) for temporal sequence modeling. This VQ-HMM pipeline
enhances the model's capacity to capture dynamic QoE patterns while supporting
probabilistic inference on new and unseen data. Experimental results on
publicly available time-series datasets incorporating both objective indicators
and subjective QoE scores demonstrate the viability of this approach in
real-time and resource-constrained environments, where inference latency is
also critical. The framework offers a scalable alternative to complex deep
learning methods, particularly in scenarios with limited computational
resources or where latency constraints are critical.

</details>


### [253] [A comparative study of deep learning and ensemble learning to extend the horizon of traffic forecasting](https://arxiv.org/abs/2504.21358)
*Xiao Zheng, Saeed Asadi Bagloee, Majid Sarvi*

Main category: cs.LG

TL;DR: The paper compares ML and DL methods for long-term traffic forecasting, highlighting the importance of periodicity modeling and time embedding, with XGBoost performing competitively against DL models.


<details>
  <summary>Details</summary>
Motivation: To address the challenge of long-term traffic forecasting, which remains understudied compared to short-term prediction, by evaluating ML and DL methods on large-scale datasets.

Method: Develops an ensemble ML method (XGBoost) and various DL methods (RNN-based and Transformer-based), incorporating time embedding to enhance temporal understanding.

Result: Time embedding improves performance, with naive RNN outperforming Informer by 31.1% for 30-day forecasting. XGBoost competes with DL methods despite using only time features.

Conclusion: Effective long-term forecasting shifts focus from temporal dependency to periodicity modeling, with time embedding playing a key role. Findings provide insights for future research.

Abstract: Traffic forecasting is vital for Intelligent Transportation Systems, for
which Machine Learning (ML) methods have been extensively explored to develop
data-driven Artificial Intelligence (AI) solutions. Recent research focuses on
modelling spatial-temporal correlations for short-term traffic prediction,
leaving the favourable long-term forecasting a challenging and open issue. This
paper presents a comparative study on large-scale real-world signalized
arterials and freeway traffic flow datasets, aiming to evaluate promising ML
methods in the context of large forecasting horizons up to 30 days. Focusing on
modelling capacity for temporal dynamics, we develop one ensemble ML method,
eXtreme Gradient Boosting (XGBoost), and a range of Deep Learning (DL) methods,
including Recurrent Neural Network (RNN)-based methods and the state-of-the-art
Transformer-based method. Time embedding is leveraged to enhance their
understanding of seasonality and event factors. Experimental results highlight
that while the attention mechanism/Transformer framework is effective for
capturing long-range dependencies in sequential data, as the forecasting
horizon extends, the key to effective traffic forecasting gradually shifts from
temporal dependency capturing to periodicity modelling. Time embedding is
particularly effective in this context, helping naive RNN outperform Informer
by 31.1% for 30-day-ahead forecasting. Meanwhile, as an efficient and robust
model, XGBoost, while learning solely from time features, performs
competitively with DL methods. Moreover, we investigate the impacts of various
factors like input sequence length, holiday traffic, data granularity, and
training data size. The findings offer valuable insights and serve as a
reference for future long-term traffic forecasting research and the improvement
of AI's corresponding learning capabilities.

</details>


### [254] [Synergy-CLIP: Extending CLIP with Multi-modal Integration for Robust Representation Learning](https://arxiv.org/abs/2504.21375)
*Sangyeon Cho, Jangyeong Jeon, Mingi Kim, Junyeong Kim*

Main category: cs.LG

TL;DR: Synergy-CLIP extends CLIP to integrate visual, textual, and audio modalities equally, outperforming baselines in tasks like zero-shot classification and missing modality reconstruction.


<details>
  <summary>Details</summary>
Motivation: Existing methods focus on bimodal interactions, limiting multi-modal data exploitation. Equal-scale modality integration is underexplored due to dataset challenges.

Method: Proposes Synergy-CLIP, aligning three modalities equally, and introduces VGG-sound+ dataset for balanced representation.

Result: Outperforms baselines in zero-shot classification and demonstrates synergy in missing modality reconstruction.

Conclusion: Synergy-CLIP advances multi-modal representation learning and opens new research directions.

Abstract: Multi-modal representation learning has become a pivotal area in artificial
intelligence, enabling the integration of diverse modalities such as vision,
text, and audio to solve complex problems. However, existing approaches
predominantly focus on bimodal interactions, such as image-text pairs, which
limits their ability to fully exploit the richness of multi-modal data.
Furthermore, the integration of modalities in equal-scale environments remains
underexplored due to the challenges of constructing large-scale, balanced
datasets. In this study, we propose Synergy-CLIP, a novel framework that
extends the contrastive language-image pre-training (CLIP) architecture to
enhance multi-modal representation learning by integrating visual, textual, and
audio modalities. Unlike existing methods that focus on adapting individual
modalities to vanilla-CLIP, Synergy-CLIP aligns and captures latent information
across three modalities equally. To address the high cost of constructing
large-scale multi-modal datasets, we introduce VGG-sound+, a triple-modal
dataset designed to provide equal-scale representation of visual, textual, and
audio data. Synergy-CLIP is validated on various downstream tasks, including
zero-shot classification, where it outperforms existing baselines.
Additionally, we introduce a missing modality reconstruction task,
demonstrating Synergy-CLIP's ability to extract synergy among modalities in
realistic application scenarios. These contributions provide a robust
foundation for advancing multi-modal representation learning and exploring new
research directions.

</details>


### [255] [Sparse-to-Sparse Training of Diffusion Models](https://arxiv.org/abs/2504.21380)
*Inês Cardoso Oliveira, Decebal Constantin Mocanu, Luis A. Leiva*

Main category: cs.LG

TL;DR: The paper introduces sparse-to-sparse training for Diffusion Models (DMs) to improve efficiency in both training and inference, showing that sparse DMs can match or outperform dense DMs while reducing computational costs.


<details>
  <summary>Details</summary>
Motivation: Diffusion Models are computationally expensive, and prior work focused mainly on inference efficiency. This paper addresses the gap by exploring sparse-to-sparse training to enhance both training and inference efficiency.

Method: The study trains sparse DMs (Latent Diffusion and ChiroDiff) from scratch using three methods (Static-DM, RigL-DM, and MagRan-DM) on six datasets to analyze sparsity's impact on performance.

Result: Sparse DMs match or outperform dense DMs while significantly reducing trainable parameters and FLOPs. Safe and effective values for sparse-to-sparse training are identified.

Conclusion: Sparse-to-sparse training is a viable paradigm for DMs, offering computational efficiency without compromising performance.

Abstract: Diffusion models (DMs) are a powerful type of generative models that have
achieved state-of-the-art results in various image synthesis tasks and have
shown potential in other domains, such as natural language processing and
temporal data modeling. Despite their stable training dynamics and ability to
produce diverse high-quality samples, DMs are notorious for requiring
significant computational resources, both in the training and inference stages.
Previous work has focused mostly on increasing the efficiency of model
inference. This paper introduces, for the first time, the paradigm of
sparse-to-sparse training to DMs, with the aim of improving both training and
inference efficiency. We focus on unconditional generation and train sparse DMs
from scratch (Latent Diffusion and ChiroDiff) on six datasets using three
different methods (Static-DM, RigL-DM, and MagRan-DM) to study the effect of
sparsity in model performance. Our experiments show that sparse DMs are able to
match and often outperform their Dense counterparts, while substantially
reducing the number of trainable parameters and FLOPs. We also identify safe
and effective values to perform sparse-to-sparse training of DMs.

</details>


### [256] [FAST-Q: Fast-track Exploration with Adversarially Balanced State Representations for Counterfactual Action Estimation in Offline Reinforcement Learning](https://arxiv.org/abs/2504.21383)
*Pulkit Agrawal, Rukma Talwadker, Aditya Pareek, Tridib Mukherjee*

Main category: cs.LG

TL;DR: FAST-Q improves offline RL by addressing function approximation errors and player psychology, using Gradient Reversal Learning and Q-value decomposition for better recommendations in gaming.


<details>
  <summary>Details</summary>
Motivation: High-stakes applications like gaming recommendation systems face challenges from sparse, biased state spaces and player psychology, which current SOTA methods poorly address.

Method: FAST-Q uses Gradient Reversal Learning for balanced state representations, offline counterfactual exploration, and Q-value decomposition for multi-objective optimization.

Result: FAST-Q outperforms SOTA methods, increasing player returns by 0.15%, LTV by 2%, engagement by 0.4%, dwell time by 2%, and reducing costs by 10%.

Conclusion: FAST-Q effectively addresses challenges in offline RL for gaming, improving performance and reducing costs.

Abstract: Recent advancements in state-of-the-art (SOTA) offline reinforcement learning
(RL) have primarily focused on addressing function approximation errors, which
contribute to the overestimation of Q-values for out-of-distribution actions, a
challenge that static datasets exacerbate. However, high stakes applications
such as recommendation systems in online gaming, introduce further complexities
due to player's psychology (intent) driven by gameplay experiences and the
inherent volatility on the platform. These factors create highly sparse,
partially overlapping state spaces across policies, further influenced by the
experiment path selection logic which biases state spaces towards specific
policies. Current SOTA methods constrain learning from such offline data by
clipping known counterfactual actions as out-of-distribution due to poor
generalization across unobserved states. Further aggravating conservative
Q-learning and necessitating more online exploration. FAST-Q introduces a novel
approach that (1) leverages Gradient Reversal Learning to construct balanced
state representations, regularizing the policy-specific bias between the
player's state and action thereby enabling counterfactual estimation; (2)
supports offline counterfactual exploration in parallel with static data
exploitation; and (3) proposes a Q-value decomposition strategy for
multi-objective optimization, facilitating explainable recommendations over
short and long-term objectives. These innovations demonstrate superiority of
FAST-Q over prior SOTA approaches and demonstrates at least 0.15 percent
increase in player returns, 2 percent improvement in lifetime value (LTV), 0.4
percent enhancement in the recommendation driven engagement, 2 percent
improvement in the player's platform dwell time and an impressive 10 percent
reduction in the costs associated with the recommendation, on our volatile
gaming platform.

</details>


### [257] [Enhanced Semi-Supervised Stamping Process Monitoring with Physically-Informed Feature Extraction](https://arxiv.org/abs/2504.21389)
*Jianyu Zhang, Jianshe Feng, Yizhang Zhu, Fanyu Qi*

Main category: cs.LG

TL;DR: A semi-supervised framework for anomaly monitoring in stamping processes uses accelerometer signals and physics information to detect anomalies in real-time, improving production yield.


<details>
  <summary>Details</summary>
Motivation: To address frequent anomalies in stamping processes and reduce batch defects by enabling real-time monitoring.

Method: Hybrid feature extraction combining data-driven and physical methods, and a semi-supervised anomaly detection model using normal samples for baseline.

Result: Validated effectiveness with classification algorithms and real-world data, showing enhanced performance in anomaly monitoring.

Conclusion: The proposed framework effectively monitors stamping process anomalies, reducing defects and improving yield.

Abstract: In tackling frequent anomalies in stamping processes, this study introduces a
novel semi-supervised in-process anomaly monitoring framework, utilizing
accelerometer signals and physics information, to capture the process anomaly
effectively. The proposed framework facilitates the construction of a
monitoring model with imbalanced sample distribution, which enables in-process
condition monitoring in real-time to prevent batch anomalies, which helps to
reduce batch defects risk and enhance production yield. Firstly, to effectively
capture key features from raw data containing redundant information, a hybrid
feature extraction algorithm is proposed to utilize data-driven methods and
physical mechanisms simultaneously. Secondly, to address the challenge brought
by imbalanced sample distribution, a semi-supervised anomaly detection model is
established, which merely employs normal samples to build a golden baseline
model, and a novel deviation score is proposed to quantify the anomaly level of
each online stamping stroke. The effectiveness of the proposed feature
extraction method is validated with various classification algorithms. A
real-world in-process dataset from stamping manufacturing workshop is employed
to illustrate the superiority of proposed semi-supervised framework with
enhance performance for process anomaly monitoring.

</details>


### [258] [MPEC: Manifold-Preserved EEG Classification via an Ensemble of Clustering-Based Classifiers](https://arxiv.org/abs/2504.21427)
*Shermin Shahbazi, Mohammad-Reza Nasiri, Majid Ramezani*

Main category: cs.LG

TL;DR: MPEC improves EEG signal classification by preserving manifold structure using covariance matrices, RBF kernels, and a modified K-means algorithm, outperforming traditional methods.


<details>
  <summary>Details</summary>
Motivation: Existing EEG classification methods often ignore the non-Euclidean, manifold structure of EEG data, leading to suboptimal performance for BCIs and neuroprosthetics.

Method: MPEC combines covariance matrices and RBF kernels for feature engineering, and uses a modified K-means algorithm for clustering in Riemannian manifold space, followed by ensembling classifiers.

Result: MPEC achieves superior performance, validated by significant improvements on the BCI Competition IV dataset 2a.

Conclusion: Preserving the manifold structure of EEG data is crucial, and MPEC demonstrates its effectiveness through innovative feature engineering and clustering techniques.

Abstract: Accurate classification of EEG signals is crucial for brain-computer
interfaces (BCIs) and neuroprosthetic applications, yet many existing methods
fail to account for the non-Euclidean, manifold structure of EEG data,
resulting in suboptimal performance. Preserving this manifold information is
essential to capture the true geometry of EEG signals, but traditional
classification techniques largely overlook this need. To this end, we propose
MPEC (Manifold-Preserved EEG Classification via an Ensemble of Clustering-Based
Classifiers), that introduces two key innovations: (1) a feature engineering
phase that combines covariance matrices and Radial Basis Function (RBF) kernels
to capture both linear and non-linear relationships among EEG channels, and (2)
a clustering phase that employs a modified K-means algorithm tailored for the
Riemannian manifold space, ensuring local geometric sensitivity. Ensembling
multiple clustering-based classifiers, MPEC achieves superior results,
validated by significant improvements on the BCI Competition IV dataset 2a.

</details>


### [259] [Whispers of Data: Unveiling Label Distributions in Federated Learning Through Virtual Client Simulation](https://arxiv.org/abs/2504.21436)
*Zhixuan Ma, Haichang Gao, Junxiang Huang, Ping Wang*

Main category: cs.LG

TL;DR: A novel label distribution inference attack for Federated Learning is proposed, stable across scenarios and effective under defenses like differential privacy.


<details>
  <summary>Details</summary>
Motivation: Existing label inference attacks are sensitive to client settings and perform poorly under defenses, prompting a need for a more robust method.

Method: Estimates victim client's dataset size, constructs tailored virtual clients, quantifies temporal generalization of labels, and trains an inference model using these variations.

Result: Outperforms state-of-the-art techniques on datasets like MNIST, Fashion-MNIST, FER2013, and AG-News, remaining effective under differential privacy.

Conclusion: The proposed attack is robust and adaptable, demonstrating real-world applicability despite defensive measures.

Abstract: Federated Learning enables collaborative training of a global model across
multiple geographically dispersed clients without the need for data sharing.
However, it is susceptible to inference attacks, particularly label inference
attacks.
  Existing studies on label distribution inference exhibits sensitive to the
specific settings of the victim client and typically underperforms under
defensive strategies. In this study, we propose a novel label distribution
inference attack that is stable and adaptable to various scenarios.
Specifically, we estimate the size of the victim client's dataset and construct
several virtual clients tailored to the victim client. We then quantify the
temporal generalization of each class label for the virtual clients and utilize
the variation in temporal generalization to train an inference model that
predicts the label distribution proportions of the victim client.
  We validate our approach on multiple datasets, including MNIST,
Fashion-MNIST, FER2013, and AG-News. The results demonstrate the superiority of
our method compared to state-of-the-art techniques. Furthermore, our attack
remains effective even under differential privacy defense mechanisms,
underscoring its potential for real-world applications.

</details>


### [260] [xEEGNet: Towards Explainable AI in EEG Dementia Classification](https://arxiv.org/abs/2504.21457)
*Andrea Zanola, Louis Fabrice Tshimanga, Federico Del Pup, Marco Baiesi, Manfredo Atzori*

Main category: cs.LG

TL;DR: xEEGNet is a compact, interpretable neural network for EEG analysis, reducing overfitting and parameters while maintaining performance in classifying dementia conditions.


<details>
  <summary>Details</summary>
Motivation: To develop a fully interpretable and efficient neural network for EEG data analysis, addressing the limitations of 'black box' models like ShallowNet.

Method: Analyzed and modified ShallowNet's structure to create xEEGNet, focusing on transparency and parameter reduction. Evaluated using Nested-Leave-N-Subjects-Out cross-validation and examined kernels/weights for clinical relevance.

Result: xEEGNet uses 168 parameters (200x fewer than ShallowNet), resists overfitting, and achieves comparable performance (-1.5% median difference). It also reduces variability across data splits.

Conclusion: Smaller, interpretable models like xEEGNet can match large deep learning models in EEG pathology classification, offering transparency and efficiency.

Abstract: This work presents xEEGNet, a novel, compact, and explainable neural network
for EEG data analysis. It is fully interpretable and reduces overfitting
through major parameter reduction. As an applicative use case, we focused on
classifying common dementia conditions, Alzheimer's and frontotemporal
dementia, versus controls. xEEGNet is broadly applicable to other neurological
conditions involving spectral alterations. We initially used ShallowNet, a
simple and popular model from the EEGNet-family. Its structure was analyzed and
gradually modified to move from a "black box" to a more transparent model,
without compromising performance. The learned kernels and weights were examined
from a clinical standpoint to assess medical relevance. Model variants,
including ShallowNet and the final xEEGNet, were evaluated using robust
Nested-Leave-N-Subjects-Out cross-validation for unbiased performance
estimates. Variability across data splits was explained using embedded EEG
representations, grouped by class and set, with pairwise separability to
quantify group distinction. Overfitting was assessed through
training-validation loss correlation and training speed. xEEGNet uses only 168
parameters, 200 times fewer than ShallowNet, yet retains interpretability,
resists overfitting, achieves comparable median performance (-1.5%), and
reduces variability across splits. This variability is explained by embedded
EEG representations: higher accuracy correlates with greater separation between
test set controls and Alzheimer's cases, without significant influence from
training data. xEEGNet's ability to filter specific EEG bands, learn
band-specific topographies, and use relevant spectral features demonstrates its
interpretability. While large deep learning models are often prioritized for
performance, this study shows smaller architectures like xEEGNet can be equally
effective in EEG pathology classification.

</details>


### [261] [Deep Learning Optimization Using Self-Adaptive Weighted Auxiliary Variables](https://arxiv.org/abs/2504.21501)
*Yaru Liu, Yiqi Gu, Michael K. Ng*

Main category: cs.LG

TL;DR: A new optimization framework for least squares learning using neural networks, addressing inefficiencies in gradient descent by introducing auxiliary variables and self-adaptive weights.


<details>
  <summary>Details</summary>
Motivation: Gradient descent struggles with high non-convexity and vanishing gradients in deep learning.

Method: Introduce auxiliary variables to separate layers and reformulate loss functions, using self-adaptive weights to maintain consistency with the original loss.

Result: Numerical experiments confirm the framework's effectiveness and robustness over gradient descent.

Conclusion: The proposed method improves optimization efficiency and performance in deep learning tasks.

Abstract: In this paper, we develop a new optimization framework for the least squares
learning problem via fully connected neural networks or physics-informed neural
networks. The gradient descent sometimes behaves inefficiently in deep learning
because of the high non-convexity of loss functions and the vanishing gradient
issue. Our idea is to introduce auxiliary variables to separate the layers of
the deep neural networks and reformulate the loss functions for ease of
optimization. We design the self-adaptive weights to preserve the consistency
between the reformulated loss and the original mean squared loss, which
guarantees that optimizing the new loss helps optimize the original problem.
Numerical experiments are presented to verify the consistency and show the
effectiveness and robustness of our models over gradient descent.

</details>


### [262] [Towards proactive self-adaptive AI for non-stationary environments with dataset shifts](https://arxiv.org/abs/2504.21565)
*David Fernández Narro, Pablo Ferri, Juan M. García-Gómez, Carlos Sáez*

Main category: cs.LG

TL;DR: The paper proposes a proactive self-adaptive AI (pro-adaptive) method to handle temporal dataset shifts in medical settings, using polynomial spline bases for forecasting AI parameters, validated on COVID-19 data.


<details>
  <summary>Details</summary>
Motivation: AI models struggle with performance in non-stationary environments, especially in medical settings with temporal dataset shifts and limited labeled data for retraining.

Method: Uses polynomial spline bases in a Functional Data Analysis framework to forecast AI parameters, validated on logistic regression models addressing various shifts.

Result: The pro-adaptive approach improves AI performance against shifts without needing updated training data, outperforming baseline stable models.

Conclusion: This work advances pro-adaptive AI for dynamic environments, ensuring resilience in health-related AI applications while respecting data protection.

Abstract: Artificial Intelligence (AI) models deployed in production frequently face
challenges in maintaining their performance in non-stationary environments.
This issue is particularly noticeable in medical settings, where temporal
dataset shifts often occur. These shifts arise when the distributions of
training data differ from those of the data encountered during deployment over
time. Further, new labeled data to continuously retrain AI is not typically
available in a timely manner due to data access limitations. To address these
challenges, we propose a proactive self-adaptive AI approach, or pro-adaptive,
where we model the temporal trajectory of AI parameters, allowing us to
short-term forecast parameter values. To this end, we use polynomial spline
bases, within an extensible Functional Data Analysis framework. We validate our
methodology with a logistic regression model addressing prior probability
shift, covariate shift, and concept shift. This validation is conducted on both
a controlled simulated dataset and a publicly available real-world COVID-19
dataset from Mexico, with various shifts occurring between 2020 and 2024. Our
results indicate that this approach enhances the performance of AI against
shifts compared to baseline stable models trained at different time distances
from the present, without requiring updated training data. This work lays the
foundation for pro-adaptive AI research against dynamic, non-stationary
environments, being compatible with data protection, in resilient AI production
environments for health.

</details>


### [263] [On Advancements of the Forward-Forward Algorithm](https://arxiv.org/abs/2504.21662)
*Mauricio Ortiz Torres, Markus Lange, Arne P. Raulf*

Main category: cs.LG

TL;DR: The Forward-Forward algorithm has been enhanced for complex tasks, achieving a 20% reduction in test error on CIFAR10 and enabling lightweight models for low-capacity hardware.


<details>
  <summary>Details</summary>
Motivation: To improve the Forward-Forward algorithm for complex tasks and low-capacity hardware applications.

Method: Combination of convolutional channel grouping, learning rate schedules, and independent block structures.

Result: 20% decrease in test error; lightweight models with test errors of (21±6)% and 164,706 to 754,386 trainable parameters.

Conclusion: The improvements pave the way for future verification and validation of such neural networks.

Abstract: The Forward-Forward algorithm has evolved in machine learning research,
tackling more complex tasks that mimic real-life applications. In the last
years, it has been improved by several techniques to perform better than its
original version, handling a challenging dataset like CIFAR10 without losing
its flexibility and low memory usage. We have shown in our results that
improvements are achieved through a combination of convolutional channel
grouping, learning rate schedules, and independent block structures during
training that lead to a 20\% decrease in test error percentage. Additionally,
to approach further implementations on low-capacity hardware projects we have
presented a series of lighter models that achieve low test error percentages
within (21$\pm$6)\% and number of trainable parameters between 164,706 and
754,386. This serving also as a basis for our future study on complete
verification and validation of these kinds of neural networks.

</details>


### [264] [Recursive KL Divergence Optimization: A Dynamic Framework for Representation Learning](https://arxiv.org/abs/2504.21707)
*Anthony D Martin*

Main category: cs.LG

TL;DR: The paper introduces Recursive KL Divergence Optimization (RKDO), a dynamic approach to representation learning that reframes objectives as recursive divergence alignment processes, offering efficiency gains over static methods.


<details>
  <summary>Details</summary>
Motivation: The authors argue that existing frameworks like I-Con underplay the recursive structure in representation learning, limiting efficiency and adaptability.

Method: RKDO frames representation learning as the evolution of KL divergences across data neighborhoods, unifying contrastive, clustering, and dimensionality reduction methods.

Result: Experiments show RKDO achieves ~30% lower loss values and 60-80% computational savings compared to static approaches.

Conclusion: RKDO's recursive mechanism provides a more efficient optimization landscape, benefiting resource-constrained applications.

Abstract: We propose a generalization of modern representation learning objectives by
reframing them as recursive divergence alignment processes over localized
conditional distributions While recent frameworks like Information Contrastive
Learning I-Con unify multiple learning paradigms through KL divergence between
fixed neighborhood conditionals we argue this view underplays a crucial
recursive structure inherent in the learning process. We introduce Recursive KL
Divergence Optimization RKDO a dynamic formalism where representation learning
is framed as the evolution of KL divergences across data neighborhoods. This
formulation captures contrastive clustering and dimensionality reduction
methods as static slices while offering a new path to model stability and local
adaptation. Our experiments demonstrate that RKDO offers dual efficiency
advantages approximately 30 percent lower loss values compared to static
approaches across three different datasets and 60 to 80 percent reduction in
computational resources needed to achieve comparable results. This suggests
that RKDOs recursive updating mechanism provides a fundamentally more efficient
optimization landscape for representation learning with significant
implications for resource constrained applications.

</details>


### [265] [Learning Heterogeneous Performance-Fairness Trade-offs in Federated Learning](https://arxiv.org/abs/2504.21775)
*Rongguang Ye, Ming Tang*

Main category: cs.LG

TL;DR: HetPFL improves federated learning by addressing heterogeneity in local Pareto fronts and the gap between local and global fronts using adaptive sampling and hypernet fusion.


<details>
  <summary>Details</summary>
Motivation: Existing methods neglect heterogeneity in local Pareto fronts and the generalization gap between local and global fronts.

Method: HetPFL uses Preference Sampling Adaptation (PSA) for adaptive sampling and Preference-aware Hypernet Fusion (PHF) for hypernet fusion.

Result: HetPFL outperforms seven baselines, converging linearly under weaker assumptions.

Conclusion: HetPFL effectively learns both local and global Pareto fronts, improving performance and fairness trade-offs.

Abstract: Recent methods leverage a hypernet to handle the performance-fairness
trade-offs in federated learning. This hypernet maps the clients' preferences
between model performance and fairness to preference-specifc models on the
trade-off curve, known as local Pareto front. However, existing methods
typically adopt a uniform preference sampling distribution to train the
hypernet across clients, neglecting the inherent heterogeneity of their local
Pareto fronts. Meanwhile, from the perspective of generalization, they do not
consider the gap between local and global Pareto fronts on the global dataset.
To address these limitations, we propose HetPFL to effectively learn both local
and global Pareto fronts. HetPFL comprises Preference Sampling Adaptation (PSA)
and Preference-aware Hypernet Fusion (PHF). PSA adaptively determines the
optimal preference sampling distribution for each client to accommodate
heterogeneous local Pareto fronts. While PHF performs preference-aware fusion
of clients' hypernets to ensure the performance of the global Pareto front. We
prove that HetPFL converges linearly with respect to the number of rounds,
under weaker assumptions than existing methods. Extensive experiments on four
datasets show that HetPFL significantly outperforms seven baselines in terms of
the quality of learned local and global Pareto fronts.

</details>


### [266] [Stable Trajectory Clustering: An Efficient Split and Merge Algorithm](https://arxiv.org/abs/2504.21808)
*Atieh Rahmani, Mansoor Davoodi, Justin M. Calabrese*

Main category: cs.LG

TL;DR: The paper introduces whole-trajectory and sub-trajectory clustering algorithms based on DBSCAN line segment clustering, addressing issues like temporary anomalies in data. It proposes a stable trajectory clustering algorithm using mean absolute deviation to improve cluster stability and interpretability.


<details>
  <summary>Details</summary>
Motivation: To address the limitations of existing trajectory clustering algorithms, which often split trajectories due to temporary anomalies, obscuring consistent patterns and reducing reliability.

Method: The paper presents DBSCAN-based whole-trajectory and sub-trajectory clustering, incorporating split and merge events of line segments. It introduces a stable trajectory clustering algorithm using mean absolute deviation to handle transient deviations.

Result: The proposed algorithms are tested on real trajectory datasets, demonstrating effectiveness and sensitivity to parameter variations.

Conclusion: Selective omission of transient deviations improves cluster stability and interpretability, making the proposed algorithms more reliable for trajectory analysis.

Abstract: Clustering algorithms group data points by characteristics to identify
patterns. Over the past two decades, researchers have extended these methods to
analyze trajectories of humans, animals, and vehicles, studying their behavior
and movement across applications. This paper presents whole-trajectory
clustering and sub-trajectory clustering algorithms based on DBSCAN line
segment clustering, which encompasses two key events: split and merge of line
segments. The events are employed by object movement history and the average
Euclidean distance between line segments. In this framework, whole-trajectory
clustering considers entire entities' trajectories, whereas sub-trajectory
clustering employs a sliding window model to identify similar sub-trajectories.
Many existing trajectory clustering algorithms respond to temporary anomalies
in data by splitting trajectories, which often obscures otherwise consistent
clustering patterns and leads to less reliable insights. We introduce the
stable trajectory clustering algorithm, which leverages the mean absolute
deviation concept to demonstrate that selective omission of transient
deviations not only preserves the integrity of clusters but also improves their
stability and interpretability. We run all proposed algorithms on real
trajectory datasets to illustrate their effectiveness and sensitivity to
parameter variations.

</details>


### [267] [A Large-scale Multimodal Study for Predicting Mortality Risk Using Minimal and Low Parameter Models and Separable Risk Assessment](https://arxiv.org/abs/1901.08125)
*Alvaro E. Ulloa Cerna, Marios Pattichis, David P. vanMaanen, Linyuan Jing, Aalpen A. Patel, Joshua V. Stough, Christopher M. Haggerty, Brandon K. Fornwalt*

Main category: cs.LG

TL;DR: The paper develops multimodal models to predict 1-year mortality using a massive clinical dataset, achieving AUCs from 0.72 to 0.89.


<details>
  <summary>Details</summary>
Motivation: Biomedical studies often use limited datasets, which may not generalize well. This paper aims to address this by leveraging large, heterogeneous datasets for better predictive accuracy.

Method: The study uses 25M+ videos and 2.9M ECG traces to develop and validate multimodal models, focusing on feature importance and low-parameter optimization.

Result: Models achieve AUCs of 0.72 (10 parameters) to 0.89 (105k parameters), with insights into risk factors and modalities.

Conclusion: The modular neural network framework offers scalable, interpretable predictions for mortality risk, aiding clinical decision-making.

Abstract: The majority of biomedical studies use limited datasets that may not
generalize over large heterogeneous datasets that have been collected over
several decades. The current paper develops and validates several multimodal
models that can predict 1-year mortality based on a massive clinical dataset.
Our focus on predicting 1-year mortality can provide a sense of urgency to the
patients. Using the largest dataset of its kind, the paper considers the
development and validation of multimodal models based on 25,137,015 videos
associated with 699,822 echocardiography studies from 316,125 patients, and
2,922,990 8-lead electrocardiogram (ECG) traces from 631,353 patients. Our
models allow us to assess the contribution of individual factors and modalities
to the overall risk. Our approach allows us to develop extremely low-parameter
models that use optimized feature selection based on feature importance. Based
on available clinical information, we construct a family of models that are
made available in the DISIML package. Overall, performance ranges from an AUC
of 0.72 with just ten parameters to an AUC of 0.89 with under 105k for the full
multimodal model. The proposed approach represents a modular neural network
framework that can provide insights into global risk trends and guide therapies
for reducing mortality risk.

</details>


### [268] [SDWPF: A Dataset for Spatial Dynamic Wind Power Forecasting Challenge at KDD Cup 2022](https://arxiv.org/abs/2208.04360)
*Jingbo Zhou, Xinjiang Lu, Yixiong Xiao, Jiantao Su, Junfu Lyu, Yanjun Ma, Dejing Dou*

Main category: cs.LG

TL;DR: The paper introduces a new dataset, SDWPF, for wind power forecasting, addressing gaps in existing datasets by including spatial and dynamic context data for 134 turbines over half a year.


<details>
  <summary>Details</summary>
Motivation: Wind power forecasting is critical for grid stability, but existing datasets lack spatial and dynamic context, limiting prediction accuracy.

Method: The authors present the SDWPF dataset, which includes spatial distribution and dynamic context factors for 134 turbines.

Result: The dataset is used in the Baidu KDD Cup 2022 to test current forecasting solutions.

Conclusion: SDWPF fills a gap in wind power forecasting research by providing detailed spatial and dynamic data, aiming to improve prediction accuracy.

Abstract: The variability of wind power supply can present substantial challenges to
incorporating wind power into a grid system. Thus, Wind Power Forecasting (WPF)
has been widely recognized as one of the most critical issues in wind power
integration and operation. There has been an explosion of studies on wind power
forecasting problems in the past decades. Nevertheless, how to well handle the
WPF problem is still challenging, since high prediction accuracy is always
demanded to ensure grid stability and security of supply. We present a unique
Spatial Dynamic Wind Power Forecasting dataset: SDWPF, which includes the
spatial distribution of wind turbines, as well as the dynamic context factors.
Whereas, most of the existing datasets have only a small number of wind
turbines without knowing the locations and context information of wind turbines
at a fine-grained time scale. By contrast, SDWPF provides the wind power data
of 134 wind turbines from a wind farm over half a year with their relative
positions and internal statuses. We use this dataset to launch the Baidu KDD
Cup 2022 to examine the limit of current WPF solutions. The dataset is released
at https://aistudio.baidu.com/aistudio/competition/detail/152/0/datasets.

</details>


### [269] [Quantitative Energy Prediction based on Carbon Emission Analysis by DPR Framework](https://arxiv.org/abs/2309.01115)
*Xuanming Zhang*

Main category: cs.LG

TL;DR: A novel framework combining DBSCAN clustering and Elastic Net regression is proposed to analyze complex, multicollinear problems like carbon emissions, demonstrating effectiveness with Chinese energy data.


<details>
  <summary>Details</summary>
Motivation: Address structural complexity and multicollinearity in multifactorial problems, exemplified by carbon emissions analysis.

Method: Integrates DBSCAN for unsupervised clustering and Elastic Net for feature selection and regularization, applied to Chinese energy data (2000-2019).

Result: Identified 16 emission categories, quantitatively assessed drivers, and provided actionable insights for emission reduction.

Conclusion: The framework is globally applicable for complex regional challenges like carbon emissions and can identify reduction opportunities.

Abstract: This study proposes a novel analytical framework that integrates DBSCAN
clustering with the Elastic Net regression model to address multifactorial
problems characterized by structural complexity and multicollinearity,
exemplified by carbon emissions analysis. DBSCAN is employed for unsupervised
learning to objectively cluster features, while the Elastic Net is utilized for
high-dimensional feature selection and complexity control. The Elastic Net is
specifically chosen for its ability to balance feature selection and
regularization by combining L1 (lasso) and L2 (ridge) penalties, making it
particularly suited for datasets with correlated predictors. Applying this
framework to energy consumption data from 46 industries in China (2000-2019)
resulted in the identification of 16 categories. Emission characteristics and
drivers were quantitatively assessed for each category, demonstrating the
framework's capacity to identify primary emission sources and provide
actionable insights. This research underscores the global applicability of the
framework for analyzing complex regional challenges, such as carbon emissions,
and highlights its potential to identify opportunities for emission reduction.

</details>


### [270] [Visual Encoders for Data-Efficient Imitation Learning in Modern Video Games](https://arxiv.org/abs/2312.02312)
*Lukas Schäfer, Logan Jones, Anssi Kanervisto, Yuhan Cao, Tabish Rashid, Raluca Georgescu, Dave Bignell, Siddhartha Sen, Andrea Treviño Gavito, Sam Devlin*

Main category: cs.LG

TL;DR: The paper explores using pre-trained visual encoders for imitation learning in video games, comparing them to end-to-end training. Results show pre-trained encoders like DINOv2 can improve performance and reduce training costs.


<details>
  <summary>Details</summary>
Motivation: Modern video games are expensive to research. The study aims to identify visual encoders that retain decision-critical information, making research more accessible.

Method: Systematic study of imitation learning with pre-trained encoders vs. end-to-end training in games like Minecraft and Counter-Strike.

Result: End-to-end training works with low-resolution images and minimal data, but pre-trained encoders like DINOv2 offer significant improvements and cost savings.

Conclusion: Pre-trained encoders enhance decision-making in video games and lower research costs, making the field more accessible.

Abstract: Video games have served as useful benchmarks for the decision-making
community, but going beyond Atari games towards modern games has been
prohibitively expensive for the vast majority of the research community. Prior
work in modern video games typically relied on game-specific integration to
obtain game features and enable online training, or on existing large datasets.
An alternative approach is to train agents using imitation learning to play
video games purely from images. However, this setting poses a fundamental
question: which visual encoders obtain representations that retain information
critical for decision making? To answer this question, we conduct a systematic
study of imitation learning with publicly available pre-trained visual encoders
compared to the typical task-specific end-to-end training approach in
Minecraft, Counter-Strike: Global Offensive, and Minecraft Dungeons. Our
results show that end-to-end training can be effective with comparably
low-resolution images and only minutes of demonstrations, but significant
improvements can be gained by utilising pre-trained encoders such as DINOv2
depending on the game. In addition to enabling effective decision making, we
show that pre-trained encoders can make decision-making research in video games
more accessible by significantly reducing the cost of training.

</details>


### [271] [Always-Sparse Training by Growing Connections with Guided Stochastic Exploration](https://arxiv.org/abs/2401.06898)
*Mike Heddes, Narayan Srinivasa, Tony Givargis, Alexandru Nicolau*

Main category: cs.LG

TL;DR: Proposes an efficient, always-sparse training algorithm for ANNs, improving scalability and accuracy over previous methods.


<details>
  <summary>Details</summary>
Motivation: Addresses the computational limitations of ANNs by enabling sparsification benefits during training, not just inference.

Method: Introduces a guided stochastic exploration algorithm with linear time complexity for training and inference.

Result: Outperforms previous sparse training methods in accuracy and scales well with larger, sparser models.

Conclusion: The method offers a practical solution for efficient ANN training with sparsity, validated on benchmark datasets.

Abstract: The excessive computational requirements of modern artificial neural networks
(ANNs) are posing limitations on the machines that can run them. Sparsification
of ANNs is often motivated by time, memory and energy savings only during model
inference, yielding no benefits during training. A growing body of work is now
focusing on providing the benefits of model sparsification also during
training. While these methods greatly improve the training efficiency, the
training algorithms yielding the most accurate models still materialize the
dense weights, or compute dense gradients during training. We propose an
efficient, always-sparse training algorithm with excellent scaling to larger
and sparser models, supported by its linear time complexity with respect to the
model width during training and inference. Moreover, our guided stochastic
exploration algorithm improves over the accuracy of previous sparse training
methods. We evaluate our method on CIFAR-10/100 and ImageNet using ResNet, VGG,
and ViT models, and compare it against a range of sparsification methods.

</details>


### [272] [Fast Partition-Based Cross-Validation With Centering and Scaling for $\mathbf{X}^\mathbf{T}\mathbf{X}$ and $\mathbf{X}^\mathbf{T}\mathbf{Y}$](https://arxiv.org/abs/2401.13185)
*Ole-Christian Galbo Engstrøm, Martin Holm Jensen*

Main category: cs.LG

TL;DR: The paper introduces efficient algorithms for partition-based cross-validation in machine learning, focusing on matrix products like 𝐗ᵀ𝐗 and 𝐗ᵀ𝐘, applicable to models like PCA, PCR, RR, OLS, and PLS. It avoids data leakage and reduces redundant computations.


<details>
  <summary>Details</summary>
Motivation: To accelerate cross-validation for models relying on matrix products, addressing inefficiencies and data leakage in preprocessing.

Method: Algorithms manipulate 𝐗ᵀ𝐗 and 𝐗ᵀ𝐘 using validation samples to avoid redundant computations and ensure correctness under fold-based partitioning.

Result: The algorithms maintain time complexity independent of fold count and space complexity equivalent to storing key matrices, while preventing data leakage.

Conclusion: The work provides the first correct and efficient cross-validation algorithms for 16 preprocessing combinations, proving only 12 yield distinct matrix products.

Abstract: We present algorithms that substantially accelerate partition-based
cross-validation for machine learning models that require matrix products
$\mathbf{X}^\mathbf{T}\mathbf{X}$ and $\mathbf{X}^\mathbf{T}\mathbf{Y}$. Our
algorithms have applications in model selection for, for example, principal
component analysis (PCA), principal component regression (PCR), ridge
regression (RR), ordinary least squares (OLS), and partial least squares (PLS).
Our algorithms support all combinations of column-wise centering and scaling of
$\mathbf{X}$ and $\mathbf{Y}$, and we demonstrate in our accompanying
implementation that this adds only a manageable, practical constant over
efficient variants without preprocessing. We prove the correctness of our
algorithms under a fold-based partitioning scheme and show that the running
time is independent of the number of folds; that is, they have the same time
complexity as that of computing $\mathbf{X}^\mathbf{T}\mathbf{X}$ and
$\mathbf{X}^\mathbf{T}\mathbf{Y}$ and space complexity equivalent to storing
$\mathbf{X}$, $\mathbf{Y}$, $\mathbf{X}^\mathbf{T}\mathbf{X}$, and
$\mathbf{X}^\mathbf{T}\mathbf{Y}$. Importantly, unlike alternatives found in
the literature, we avoid data leakage due to preprocessing. We achieve these
results by eliminating redundant computations in the overlap between training
partitions. Concretely, we show how to manipulate
$\mathbf{X}^\mathbf{T}\mathbf{X}$ and $\mathbf{X}^\mathbf{T}\mathbf{Y}$ using
only samples from the validation partition to obtain the preprocessed training
partition-wise $\mathbf{X}^\mathbf{T}\mathbf{X}$ and
$\mathbf{X}^\mathbf{T}\mathbf{Y}$. To our knowledge, we are the first to derive
correct and efficient cross-validation algorithms for any of the $16$
combinations of column-wise centering and scaling, for which we also prove only
$12$ give distinct matrix products.

</details>


### [273] [SparseTransX: Efficient Training of Translation-Based Knowledge Graph Embeddings Using Sparse Matrix Operations](https://arxiv.org/abs/2502.16949)
*Md Saidul Hoque Anik, Ariful Azad*

Main category: cs.LG

TL;DR: The paper proposes using SpMM kernels to speed up KG embedding training by unifying scatter/gather operations, achieving up to 5.3x CPU and 4.2x GPU speedups.


<details>
  <summary>Details</summary>
Motivation: Training KG embeddings is slow, especially for large datasets, due to gradient computation bottlenecks.

Method: Replaces core embedding computation with SpMM kernels, unifying operations to reduce time and memory usage. Implements four models (TransE, TransR, TransH, TorusE) in a sparse framework.

Result: Achieves up to 5.3x CPU and 4.2x GPU speedups with low memory footprint, consistent across datasets.

Conclusion: The sparse approach is scalable and extendable to other KG models, with an open-source implementation available.

Abstract: Knowledge graph (KG) learning offers a powerful framework for generating new
knowledge and making inferences. Training KG embedding can take a significantly
long time, especially for larger datasets. Our analysis shows that the gradient
computation of embedding is one of the dominant functions in the
translation-based KG embedding training loop. We address this issue by
replacing the core embedding computation with SpMM (Sparse-Dense Matrix
Multiplication) kernels. This allows us to unify multiple scatter (and gather)
operations as a single operation, reducing training time and memory usage. We
create a general framework for training KG models using sparse kernels and
implement four models, namely TransE, TransR, TransH, and TorusE. Our sparse
implementations exhibit up to 5.3x speedup on the CPU and up to 4.2x speedup on
the GPU with a significantly low GPU memory footprint. The speedups are
consistent across large and small datasets for a given model. Our proposed
sparse approach can be extended to accelerate other translation-based (such as
TransC, TransM, etc.) and non-translational (such as DistMult, ComplEx, RotatE,
etc.) models as well. An implementation of the SpTransX framework is publicly
available as a Python package in https://github.com/HipGraph/SpTransX.

</details>


### [274] [Neural Redshift: Random Networks are not Random Functions](https://arxiv.org/abs/2403.02241)
*Damien Teney, Armand Nicolicioiu, Valentin Hartmann, Ehsan Abbasnejad*

Main category: cs.LG

TL;DR: The paper explores the generalization capabilities of neural networks (NNs) beyond gradient descent, focusing on untrained networks. It finds that architectures themselves induce strong biases, and these biases depend on components like ReLUs or residual connections, not just training methods.


<details>
  <summary>Details</summary>
Motivation: To understand the sources of generalization in NNs independent of gradient descent, as existing theories fail to explain the behavior of gradient-free methods or untrained networks.

Method: Examines untrained, random-weight networks to study the inductive biases of architectures, analyzing how components like ReLUs and residual connections influence function complexity.

Result: Untrained networks exhibit strong inductive biases, but these biases are not inherently simplicity-focused; they depend on architectural choices. Transformers inherit these properties.

Conclusion: The study offers a new perspective on deep learning success, emphasizing architectural biases over training methods, and suggests ways to control model solutions.

Abstract: Our understanding of the generalization capabilities of neural networks (NNs)
is still incomplete. Prevailing explanations are based on implicit biases of
gradient descent (GD) but they cannot account for the capabilities of models
from gradient-free methods nor the simplicity bias recently observed in
untrained networks. This paper seeks other sources of generalization in NNs.
  Findings. To understand the inductive biases provided by architectures
independently from GD, we examine untrained, random-weight networks. Even
simple MLPs show strong inductive biases: uniform sampling in weight space
yields a very biased distribution of functions in terms of complexity. But
unlike common wisdom, NNs do not have an inherent "simplicity bias". This
property depends on components such as ReLUs, residual connections, and layer
normalizations. Alternative architectures can be built with a bias for any
level of complexity. Transformers also inherit all these properties from their
building blocks.
  Implications. We provide a fresh explanation for the success of deep learning
independent from gradient-based training. It points at promising avenues for
controlling the solutions implemented by trained models.

</details>


### [275] [Historically Relevant Event Structuring for Temporal Knowledge Graph Reasoning](https://arxiv.org/abs/2405.10621)
*Jinchuan Zhang, Ming Sun, Chong Mu, Jinhao Zhang, Quanjiang Guo, Ling Tian*

Main category: cs.LG

TL;DR: HisRES is a TKG reasoning approach that structures historically relevant events using multi-granularity and global relevance encoders, outperforming existing methods.


<details>
  <summary>Details</summary>
Motivation: Existing TKG reasoning models lack exploration of multi-granular interactions and expressive semantics of significant historical links, limiting their ability to reflect dependencies and future trends.

Method: HisRES uses two modules: a multi-granularity evolutionary encoder for recent snapshots and a global relevance encoder for query-relevant historical events, combined via a self-gating mechanism.

Result: HisRES achieves state-of-the-art performance on four event-based benchmarks.

Conclusion: Structuring historical relevance in TKGs enhances reasoning, as demonstrated by HisRES's superior performance.

Abstract: Temporal Knowledge Graph (TKG) reasoning focuses on predicting events through
historical information within snapshots distributed on a timeline. Existing
studies mainly concentrate on two perspectives of leveraging the history of
TKGs, including capturing evolution of each recent snapshot or correlations
among global historical facts. Despite the achieved significant
accomplishments, these models still fall short of I) investigating the impact
of multi-granular interactions across recent snapshots, and II) harnessing the
expressive semantics of significant links accorded with queries throughout the
entire history, particularly events exerting a profound impact on the future.
These inadequacies restrict representation ability to reflect historical
dependencies and future trends thoroughly. To overcome these drawbacks, we
propose an innovative TKG reasoning approach towards \textbf{His}torically
\textbf{R}elevant \textbf{E}vents \textbf{S}tructuring (HisRES). Concretely,
HisRES comprises two distinctive modules excelling in structuring historically
relevant events within TKGs, including a multi-granularity evolutionary encoder
that captures structural and temporal dependencies of the most recent
snapshots, and a global relevance encoder that concentrates on crucial
correlations among events relevant to queries from the entire history.
Furthermore, HisRES incorporates a self-gating mechanism for adaptively merging
multi-granularity recent and historically relevant structuring representations.
Extensive experiments on four event-based benchmarks demonstrate the
state-of-the-art performance of HisRES and indicate the superiority and
effectiveness of structuring historical relevance for TKG reasoning.

</details>


### [276] [Variational Offline Multi-agent Skill Discovery](https://arxiv.org/abs/2405.16386)
*Jiayu Chen, Tian Lan, Vaneet Aggarwal*

Main category: cs.LG

TL;DR: The paper proposes two auto-encoder schemes (VO-MASD-3D and VO-MASD-Hier) to extract subgroup coordination patterns in multi-agent tasks, enabling efficient hierarchical learning and skill transfer.


<details>
  <summary>Details</summary>
Motivation: Addressing the gap in multi-agent scenarios for automatically extracting subgroup coordination patterns and facilitating multi-task learning through transferable skills.

Method: Introduces dynamic grouping to detect latent subgroups and auto-encoder schemes to capture subgroup- and temporal-level abstractions.

Result: Outperforms existing hierarchical MARL methods in StarCraft tasks and reduces learning difficulty in delayed/sparse reward scenarios.

Conclusion: The proposed method effectively extracts and transfers multi-agent skills, enhancing hierarchical learning and task performance.

Abstract: Skills are effective temporal abstractions established for sequential
decision making, which enable efficient hierarchical learning for long-horizon
tasks and facilitate multi-task learning through their transferability. Despite
extensive research, research gaps remain in multi-agent scenarios, particularly
for automatically extracting subgroup coordination patterns in a multi-agent
task. In this case, we propose two novel auto-encoder schemes: VO-MASD-3D and
VO-MASD-Hier, to simultaneously capture subgroup- and temporal-level
abstractions and form multi-agent skills, which firstly solves the
aforementioned challenge. An essential algorithm component of these schemes is
a dynamic grouping function that can automatically detect latent subgroups
based on agent interactions in a task. Further, our method can be applied to
offline multi-task data, and the discovered subgroup skills can be transferred
across relevant tasks without retraining. Empirical evaluations on StarCraft
tasks indicate that our approach significantly outperforms existing
hierarchical multi-agent reinforcement learning (MARL) methods. Moreover,
skills discovered using our method can effectively reduce the learning
difficulty in MARL scenarios with delayed and sparse reward signals. The
codebase is available at https://github.com/LucasCJYSDL/VOMASD.

</details>


### [277] [FADE: Towards Fairness-aware Generation for Domain Generalization via Classifier-Guided Score-based Diffusion Models](https://arxiv.org/abs/2406.09495)
*Yujie Lin, Dong Li, Minglai Shao, Guihong Wan, Chen Zhao*

Main category: cs.LG

TL;DR: FADE proposes a fairness-aware classifier-guided score-based diffusion model to address FairDG, improving fairness and accuracy under distribution shifts.


<details>
  <summary>Details</summary>
Motivation: Traditional fairness methods fail in domain generalization due to ignoring distribution shifts, and disentanglement-based approaches have strong assumptions.

Method: Pre-train a score-based diffusion model and classifiers, guide the model to remove sensitive information, and use generated fair data for downstream training.

Result: FADE enhances fairness and accuracy on three real-world datasets, outperforming existing methods in accuracy-fairness trade-offs.

Conclusion: FADE effectively addresses FairDG by leveraging diffusion models and classifier guidance, achieving robust performance under distribution shifts.

Abstract: Fairness-aware domain generalization (FairDG) has emerged as a critical
challenge for deploying trustworthy AI systems, particularly in scenarios
involving distribution shifts. Traditional methods for addressing fairness have
failed in domain generalization due to their lack of consideration for
distribution shifts. Although disentanglement has been used to tackle FairDG,
it is limited by its strong assumptions. To overcome these limitations, we
propose Fairness-aware Classifier-Guided Score-based Diffusion Models (FADE) as
a novel approach to effectively address the FairDG issue. Specifically, we
first pre-train a score-based diffusion model (SDM) and two classifiers to
equip the model with strong generalization capabilities across different
domains. Then, we guide the SDM using these pre-trained classifiers to
effectively eliminate sensitive information from the generated data. Finally,
the generated fair data is used to train downstream classifiers, ensuring
robust performance under new data distributions. Extensive experiments on three
real-world datasets demonstrate that FADE not only enhances fairness but also
improves accuracy in the presence of distribution shifts. Additionally, FADE
outperforms existing methods in achieving the best accuracy-fairness
trade-offs.

</details>


### [278] [Semi-Variance Reduction for Fair Federated Learning](https://arxiv.org/abs/2406.16193)
*Saber Malekmohammadi, Yaoliang Yu*

Main category: cs.LG

TL;DR: The paper proposes two fair FL algorithms, VRed and SemiVRed, inspired by finance risk models, to balance fairness and overall performance in federated learning.


<details>
  <summary>Details</summary>
Motivation: Existing fair FL algorithms often sacrifice average performance by focusing on worst-off clients, prompting the need for better solutions.

Method: Introduces VRed (penalizes variance of clients' loss functions) and SemiVRed (penalizes worst-off clients' loss discrepancies from average).

Result: SemiVRed achieves state-of-the-art performance in heterogeneous data scenarios, improving fairness and average performance.

Conclusion: SemiVRed is effective for fair FL, balancing fairness and system performance without suppressing well-performing clients.

Abstract: Ensuring fairness in a Federated Learning (FL) system, i.e., a satisfactory
performance for all of the participating diverse clients, is an important and
challenging problem. There are multiple fair FL algorithms in the literature,
which have been relatively successful in providing fairness. However, these
algorithms mostly emphasize on the loss functions of worst-off clients to
improve their performance, which often results in the suppression of
well-performing ones. As a consequence, they usually sacrifice the system's
overall average performance for achieving fairness. Motivated by this and
inspired by two well-known risk modeling methods in Finance, Mean-Variance and
Mean-Semi-Variance, we propose and study two new fair FL algorithms, Variance
Reduction (VRed) and Semi-Variance Reduction (SemiVRed). VRed encourages
equality between clients' loss functions by penalizing their variance. In
contrast, SemiVRed penalizes the discrepancy of only the worst-off clients'
loss functions from the average loss. Through extensive experiments on multiple
vision and language datasets, we show that, SemiVRed achieves SoTA performance
in scenarios with heterogeneous data distributions and improves both fairness
and system overall average performance.

</details>


### [279] [How to Solve Contextual Goal-Oriented Problems with Offline Datasets?](https://arxiv.org/abs/2408.07753)
*Ying Fan, Jingling Li, Adith Swaminathan, Aditya Modi, Ching-An Cheng*

Main category: cs.LG

TL;DR: CODA is a novel method for solving Contextual Goal-Oriented problems using unlabeled trajectories and context-goal pairs, outperforming baselines with theoretical and empirical validation.


<details>
  <summary>Details</summary>
Motivation: To address Contextual Goal-Oriented (CGO) problems using offline datasets without additional approximation error.

Method: Constructs an action-augmented MDP equivalent to the original MDP, creating a fully labeled transition dataset under training contexts.

Result: Empirical results show CODA outperforms baseline methods across various context-goal relationships.

Conclusion: CODA provides a promising solution for CGO problems using offline datasets, validated by theory and experiments.

Abstract: We present a novel method, Contextual goal-Oriented Data Augmentation (CODA),
which uses commonly available unlabeled trajectories and context-goal pairs to
solve Contextual Goal-Oriented (CGO) problems. By carefully constructing an
action-augmented MDP that is equivalent to the original MDP, CODA creates a
fully labeled transition dataset under training contexts without additional
approximation error. We conduct a novel theoretical analysis to demonstrate
CODA's capability to solve CGO problems in the offline data setup. Empirical
results also showcase the effectiveness of CODA, which outperforms other
baseline methods across various context-goal relationships of CGO problem. This
approach offers a promising direction to solving CGO problems using offline
datasets.

</details>


### [280] [SustainDC: Benchmarking for Sustainable Data Center Control](https://arxiv.org/abs/2408.07841)
*Avisek Naug, Antonio Guillen, Ricardo Luna, Vineet Gundecha, Desik Rengarajan, Sahand Ghorbanpour, Sajad Mousavi, Ashwin Ramesh Babu, Dejan Markovikj, Lekhapriya D Kashyap, Soumyendu Sarkar*

Main category: cs.LG

TL;DR: SustainDC is a Python-based platform for benchmarking multi-agent reinforcement learning (MARL) algorithms in data centers to improve sustainability.


<details>
  <summary>Details</summary>
Motivation: The growing computational demand from machine learning has increased energy consumption in data centers, contributing to climate change, necessitating sustainable solutions.

Method: SustainDC provides customizable environments for tasks like workload scheduling, cooling optimization, and battery management, evaluated using MARL algorithms under varied conditions.

Result: The study demonstrates MARL algorithms' potential to enhance data center operations across different designs, locations, and conditions.

Conclusion: SustainDC is a vital tool for developing sustainable computing solutions, addressing the challenges posed by AI-driven data center growth.

Abstract: Machine learning has driven an exponential increase in computational demand,
leading to massive data centers that consume significant amounts of energy and
contribute to climate change. This makes sustainable data center control a
priority. In this paper, we introduce SustainDC, a set of Python environments
for benchmarking multi-agent reinforcement learning (MARL) algorithms for data
centers (DC). SustainDC supports custom DC configurations and tasks such as
workload scheduling, cooling optimization, and auxiliary battery management,
with multiple agents managing these operations while accounting for the effects
of each other. We evaluate various MARL algorithms on SustainDC, showing their
performance across diverse DC designs, locations, weather conditions, grid
carbon intensity, and workload requirements. Our results highlight significant
opportunities for improvement of data center operations using MARL algorithms.
Given the increasing use of DC due to AI, SustainDC provides a crucial platform
for the development and benchmarking of advanced algorithms essential for
achieving sustainable computing and addressing other heterogeneous real-world
challenges.

</details>


### [281] [Estimating Wage Disparities Using Foundation Models](https://arxiv.org/abs/2409.09894)
*Keyon Vafa, Susan Athey, David M. Blei*

Main category: cs.LG

TL;DR: The paper addresses the limitations of foundation models in social science estimation tasks, proposing methods to fine-tune them for root-n-consistent estimates and demonstrating their effectiveness in gender wage gap decomposition.


<details>
  <summary>Details</summary>
Motivation: Foundation models excel in prediction but may fail in social science estimation tasks due to omitted variable bias when fine-tuned only for predictive accuracy.

Method: The authors develop theory and algorithms for fine-tuning foundation models to ensure root-n-consistent estimates, mitigating omitted variable bias.

Result: A custom foundation model captures richer career history data, revealing more of the gender wage gap than standard econometric models.

Conclusion: Fine-tuning foundation models with the proposed methods improves estimation accuracy in social science tasks, as shown in gender wage decomposition.

Abstract: The rise of foundation models marks a paradigm shift in machine learning:
instead of training specialized models from scratch, foundation models are
first trained on massive datasets before being adapted or fine-tuned to make
predictions on smaller datasets. Initially developed for text, foundation
models have also excelled at making predictions about social science data.
However, while many estimation problems in the social sciences use prediction
as an intermediate step, they ultimately require different criteria for
success. In this paper, we develop methods for fine-tuning foundation models to
perform these estimation problems. We first characterize an omitted variable
bias that can arise when a foundation model is only fine-tuned to maximize
predictive accuracy. We then provide a novel set of conditions for fine-tuning
under which estimates derived from a foundation model are root-n-consistent.
Based on this theory, we develop new fine-tuning algorithms that empirically
mitigate this omitted variable bias. To demonstrate our ideas, we study gender
wage decomposition. This is a statistical estimation problem from econometrics
where the goal is to decompose the gender wage gap into components that can and
cannot be explained by career histories of workers. Classical methods for
decomposing the wage gap employ simple predictive models of wages which
condition on coarse summaries of career history that may omit factors that are
important for explaining the gap. Instead, we use a custom-built foundation
model to decompose the gender wage gap, which captures a richer representation
of career history. Using data from the Panel Study of Income Dynamics, we find
that career history explains more of the gender wage gap than standard
econometric models can measure, and we identify elements of career history that
are omitted by standard models but are important for explaining the wage gap.

</details>


### [282] [Looped Transformers for Length Generalization](https://arxiv.org/abs/2409.15647)
*Ying Fan, Yilun Du, Kannan Ramchandran, Kangwook Lee*

Main category: cs.LG

TL;DR: Looped Transformers with adaptive steps improve length generalization in arithmetic and algorithmic tasks, outperforming traditional Transformers.


<details>
  <summary>Details</summary>
Motivation: Address the challenge of length generalization in Transformers, which struggle with inputs of unseen lengths despite success on same-length inputs.

Method: Propose looped Transformers with adaptive steps, trained using a learning algorithm, focusing on tasks with iterative solutions involving RASP-L operations.

Result: Looped Transformers achieve highly length-generalizable solutions for various tasks.

Conclusion: Adaptive looped Transformers offer a promising approach for improving length generalization in algorithmic tasks.

Abstract: Recent work has shown that Transformers trained from scratch can successfully
solve various arithmetic and algorithmic tasks, such as adding numbers and
computing parity. While these Transformers generalize well on unseen inputs of
the same length, they struggle with length generalization, i.e., handling
inputs of unseen lengths. In this work, we demonstrate that looped Transformers
with an adaptive number of steps significantly improve length generalization.
We focus on tasks with a known iterative solution, involving multiple
iterations of a RASP-L operation - a length-generalizable operation that can be
expressed by a finite-sized Transformer. We train looped Transformers using our
proposed learning algorithm and observe that they learn highly
length-generalizable solutions for various tasks.

</details>


### [283] [Downscaling Extreme Precipitation with Wasserstein Regularized Diffusion](https://arxiv.org/abs/2410.00381)
*Yuhao Liu, James Doss-Gollin, Qiushi Dai, Guha Balakrishnan, Ashok Veeraraghavan*

Main category: cs.LG

TL;DR: WassDiff, a generative downscaling framework, combines diffusion modeling with Wasserstein regularization to produce high-resolution (1 km) precipitation estimates from coarse data, improving accuracy for extreme rainfall events.


<details>
  <summary>Details</summary>
Motivation: Extreme rainfall risk assessment requires high-resolution data and long historical records, but existing datasets either lack resolution or coverage. WassDiff bridges this gap.

Method: WassDiff integrates diffusion modeling with a Wasserstein regularizer, conditioned on 55 km CPC gauge and 31 km ERA5 reanalysis data, to generate 1 km precipitation estimates.

Result: WassDiff outperforms existing methods with lower reconstruction error, reduced bias, and accurate reproduction of fine-scale structures and extreme intensities.

Conclusion: WassDiff enables high-resolution rainfall data extraction from coarse records, enhancing flood-risk assessments and climate-adaptation planning.

Abstract: Understanding the risks posed by extreme rainfall events necessitates both
high-resolution products (to assess localized hazards) and extensive historical
records (to capture rare occurrences). Radar and mesonet networks provide
kilometer-scale precipitation fields, but with limited historical records and
geographical coverage. Conversely, global gauge and blended products span
decades, yet their coarse 30-50 km grids obscure local extremes. This work
introduces Wasserstein Regularized Diffusion (WassDiff), a generative
downscaling framework that integrates diffusion modeling with a
distribution-matching (Wasserstein) regularizer, suppressing bias throughout
the entire generative denoising process. Conditioned on 55 km CPC gauge-based
precipitation and the 31 km ERA5 reanalysis, WassDiff generates 1 km
precipitation estimates that remain well-calibrated to targets across the full
intensity range, including the extremes. Comprehensive evaluations demonstrate
that WassDiff outperforms existing state-of-the-art downscaling methods,
delivering lower reconstruction error and reduced bias. Case studies further
demonstrate its ability to reproduce realistic fine-scale structures and
accurate peak intensities from extreme weather phenomena, such as tropical
storms and cold fronts. By unlocking decades of high-resolution rainfall
information from globally available coarse records, WassDiff offers a practical
pathway toward more accurate flood-risk assessments and climate-adaptation
planning.

</details>


### [284] [A Formal Framework for Understanding Length Generalization in Transformers](https://arxiv.org/abs/2410.02140)
*Xinting Huang, Andy Yang, Satwik Bhattamishra, Yash Sarrof, Andreas Krebs, Hattie Zhou, Preetum Nakkiran, Michael Hahn*

Main category: cs.LG

TL;DR: The paper introduces a theoretical framework to analyze length generalization in transformers, proving its possibility for certain tasks and validating the theory experimentally.


<details>
  <summary>Details</summary>
Motivation: To address the limited theoretical understanding of transformers' ability to generalize to longer sequences than seen in training.

Method: Develops a theoretical framework for causal transformers with learnable positional encodings, using a norm-based regularizer to characterize identifiable functions.

Result: Proves length generalization is possible for a rich family of problems and validates the theory with experiments on algorithmic and formal language tasks.

Conclusion: The framework explains empirical observations and enables provable predictions about transformers' length generalization capabilities.

Abstract: A major challenge for transformers is generalizing to sequences longer than
those observed during training. While previous works have empirically shown
that transformers can either succeed or fail at length generalization depending
on the task, theoretical understanding of this phenomenon remains limited. In
this work, we introduce a rigorous theoretical framework to analyze length
generalization in causal transformers with learnable absolute positional
encodings. In particular, we characterize those functions that are identifiable
in the limit from sufficiently long inputs with absolute positional encodings
under an idealized inference scheme using a norm-based regularizer. This
enables us to prove the possibility of length generalization for a rich family
of problems. We experimentally validate the theory as a predictor of success
and failure of length generalization across a range of algorithmic and formal
language tasks. Our theory not only explains a broad set of empirical
observations but also opens the way to provably predicting length
generalization capabilities in transformers.

</details>


### [285] [Masked Generative Priors Improve World Models Sequence Modelling Capabilities](https://arxiv.org/abs/2410.07836)
*Cristian Meo, Mircea Lica, Zarif Ikram, Akihiro Nakano, Vedant Shah, Aniket Rajiv Didolkar, Dianbo Liu, Anirudh Goyal, Justin Dauwels*

Main category: cs.LG

TL;DR: GIT-STORM, a model combining Masked Generative Prior with STORM, improves RL and video prediction tasks, showing gains in Atari 100k and extending to continuous action environments.


<details>
  <summary>Details</summary>
Motivation: To enhance data efficiency in RL by leveraging world models and address the gap between research and real-world deployment, particularly in continuous action environments.

Method: Replace MLP prior with Masked Generative Prior (MaskGIT) in STORM architecture, use state mixer for continuous actions, and evaluate on Atari 100k and DeepMind Control Suite.

Result: Substantial performance gains in RL tasks on Atari 100k and successful application to continuous action environments, validated on DeepMind Control Suite.

Conclusion: MaskGIT dynamics prior is versatile and effective, enabling more accurate world models and better RL policies, especially in continuous control tasks.

Abstract: Deep Reinforcement Learning (RL) has become the leading approach for creating
artificial agents in complex environments. Model-based approaches, which are RL
methods with world models that predict environment dynamics, are among the most
promising directions for improving data efficiency, forming a critical step
toward bridging the gap between research and real-world deployment. In
particular, world models enhance sample efficiency by learning in imagination,
which involves training a generative sequence model of the environment in a
self-supervised manner. Recently, Masked Generative Modelling has emerged as a
more efficient and superior inductive bias for modelling and generating token
sequences. Building on the Efficient Stochastic Transformer-based World Models
(STORM) architecture, we replace the traditional MLP prior with a Masked
Generative Prior (e.g., MaskGIT Prior) and introduce GIT-STORM. We evaluate our
model on two downstream tasks: reinforcement learning and video prediction.
GIT-STORM demonstrates substantial performance gains in RL tasks on the Atari
100k benchmark. Moreover, we apply Transformer-based World Models to continuous
action environments for the first time, addressing a significant gap in prior
research. To achieve this, we employ a state mixer function that integrates
latent state representations with actions, enabling our model to handle
continuous control tasks. We validate this approach through qualitative and
quantitative analyses on the DeepMind Control Suite, showcasing the
effectiveness of Transformer-based World Models in this new domain. Our results
highlight the versatility and efficacy of the MaskGIT dynamics prior, paving
the way for more accurate world models and effective RL policies.

</details>


### [286] [Extended convexity and smoothness and their applications in deep learning](https://arxiv.org/abs/2410.05807)
*Binchuan Qi, Wei Gong, Li Li*

Main category: cs.LG

TL;DR: The paper extends traditional convexity and smoothness concepts to analyze non-convex deep learning optimization, showing empirical risk minimization relates to local gradient norm and structural error, with SGD and techniques like skip connections proving effective.


<details>
  <summary>Details</summary>
Motivation: Classical assumptions like strong convexity and Lipschitz smoothness don't fit deep learning's non-convex, non-smooth nature, necessitating new analysis.

Method: Extends strong convexity and Lipschitz smoothness to non-convex settings, linking empirical risk minimization to local gradient norm and structural error, and uses SGD with techniques like skip connections.

Result: SGD minimizes local gradient norm effectively; skip connections, over-parameterization, and random initialization control structural error.

Conclusion: Theoretical and experimental results offer new insights into non-convex optimization in deep learning.

Abstract: Classical assumptions like strong convexity and Lipschitz smoothness often
fail to capture the nature of deep learning optimization problems, which are
typically non-convex and non-smooth, making traditional analyses less
applicable. This study aims to elucidate the mechanisms of non-convex
optimization in deep learning by extending the conventional notions of strong
convexity and Lipschitz smoothness. By leveraging these concepts, we prove
that, under the established constraints, the empirical risk minimization
problem is equivalent to optimizing the local gradient norm and structural
error, which together constitute the upper and lower bounds of the empirical
risk. Furthermore, our analysis demonstrates that the stochastic gradient
descent (SGD) algorithm can effectively minimize the local gradient norm.
Additionally, techniques like skip connections, over-parameterization, and
random parameter initialization are shown to help control the structural error.
Ultimately, we validate the core conclusions of this paper through extensive
experiments. Theoretical analysis and experimental results indicate that our
findings provide new insights into the mechanisms of non-convex optimization in
deep learning.

</details>


### [287] [Adaptive Random Fourier Features Training Stabilized By Resampling With Applications in Image Regression](https://arxiv.org/abs/2410.06399)
*Aku Kammonen, Anamika Pandey, Erik von Schwerin, Raúl Tempone*

Main category: cs.LG

TL;DR: An improved adaptive random Fourier features (ARFF) algorithm for shallow neural networks is introduced, using particle filter-type resampling to stabilize training and reduce parameter sensitivity. It omits the Metropolis test, cutting hyperparameters and computational cost. Tests show its effectiveness in function regression and as a pretraining step, including in image regression tasks.


<details>
  <summary>Details</summary>
Motivation: To enhance the stability and efficiency of the ARFF training algorithm by reducing sensitivity to parameter choices and computational costs.

Method: Uses particle filter-type resampling to stabilize training, omits the Metropolis test, and applies the algorithm to function and image regression tasks.

Result: Demonstrates efficacy in function regression and as a pretraining step, with successful application to image regression for sampling RFF layer parameters.

Conclusion: The improved ARFF algorithm offers a more stable and efficient training process, reducing hyperparameters and computational costs while maintaining performance.

Abstract: This paper presents an enhanced adaptive random Fourier features (ARFF)
training algorithm for shallow neural networks, building upon the work
introduced in "Adaptive Random Fourier Features with Metropolis Sampling",
Kammonen et al., \emph{Foundations of Data Science}, 2(3):309--332, 2020. This
improved method uses a particle filter-type resampling technique to stabilize
the training process and reduce the sensitivity to parameter choices. The
Metropolis test can also be omitted when resampling is used, reducing the
number of hyperparameters by one and reducing the computational cost per
iteration compared to the ARFF method. We present comprehensive numerical
experiments demonstrating the efficacy of the proposed algorithm in function
regression tasks as a stand-alone method and as a pretraining step before
gradient-based optimization, using the Adam optimizer. Furthermore, we apply
the proposed algorithm to a simple image regression problem, illustrating its
utility in sampling frequencies for the random Fourier features (RFF) layer of
coordinate-based multilayer perceptrons. In this context, we use the proposed
algorithm to sample the parameters of the RFF layer in an automated manner.

</details>


### [288] [Partial Knowledge Distillation for Alleviating the Inherent Inter-Class Discrepancy in Federated Learning](https://arxiv.org/abs/2411.15403)
*Xiaoyu Gan, Jingbo Jiang, Jingyang Zhu, Xiaomeng Wang, Xizi Chen, Chi-Ying Tsui*

Main category: cs.LG

TL;DR: The paper identifies inherent weak classes in federated learning, proposes PKD to improve their accuracy, and reduces inter-class discrepancy by 10.7%.


<details>
  <summary>Details</summary>
Motivation: To address the persistent weak classes in federated learning, which exist even in balanced settings, and understand their causes.

Method: Proposes Partial Knowledge Distillation (PKD), initiating knowledge transfer upon specific misclassifications in weak classes.

Result: PKD improves weak class accuracy by 10.7%, effectively reducing inherent inter-class discrepancy.

Conclusion: The study highlights inherent weak classes in federated learning and demonstrates PKD's effectiveness in mitigating their impact.

Abstract: Substantial efforts have been devoted to alleviating the impact of the
long-tailed class distribution in federated learning. In this work, we observe
an interesting phenomenon that certain weak classes consistently exist even for
class-balanced learning. These weak classes, different from the minority
classes in the previous works, are inherent to data and remain fairly
consistent for various network structures, learning paradigms, and data
partitioning methods. The inherent inter-class accuracy discrepancy can reach
over 36.9% for federated learning on the FashionMNIST and CIFAR-10 datasets,
even when the class distribution is balanced both globally and locally. In this
study, we empirically analyze the potential reason for this phenomenon.
Furthermore, a partial knowledge distillation (PKD) method is proposed to
improve the model's classification accuracy for weak classes. In this approach,
knowledge transfer is initiated upon the occurrence of specific
misclassifications within certain weak classes. Experimental results show that
the accuracy of weak classes can be improved by 10.7%, reducing the inherent
inter-class discrepancy effectively.

</details>


### [289] [A Library for Learning Neural Operators](https://arxiv.org/abs/2412.10354)
*Jean Kossaifi, Nikola Kovachki, Zongyi Li, David Pitt, Miguel Liu-Schiaffini, Robert Joseph George, Boris Bonev, Kamyar Azizzadenesheli, Julius Berner, Valentin Duruisseaux, Anima Anandkumar*

Main category: cs.LG

TL;DR: NeuralOperator is a Python library for operator learning, enabling neural networks to map between function spaces with discretization convergence, built on PyTorch for ease of use.


<details>
  <summary>Details</summary>
Motivation: To provide a flexible, high-quality open-source tool for operator learning, bridging the gap between theoretical models and practical applications.

Method: Uses neural operators to generalize neural networks for function space mappings, supports various discretizations, and ensures convergence properties.

Result: A tested, customizable library with cutting-edge models, designed for both newcomers and experts.

Conclusion: NeuralOperator successfully delivers a user-friendly, powerful tool for operator learning, fostering innovation and accessibility.

Abstract: We present NeuralOperator, an open-source Python library for operator
learning. Neural operators generalize neural networks to maps between function
spaces instead of finite-dimensional Euclidean spaces. They can be trained and
inferenced on input and output functions given at various discretizations,
satisfying a discretization convergence properties. Built on top of PyTorch,
NeuralOperator provides all the tools for training and deploying neural
operator models, as well as developing new ones, in a high-quality, tested,
open-source package. It combines cutting-edge models and customizability with a
gentle learning curve and simple user interface for newcomers.

</details>


### [290] [MARIA: a Multimodal Transformer Model for Incomplete Healthcare Data](https://arxiv.org/abs/2412.14810)
*Camillo Maria Caruso, Paolo Soda, Valerio Guarrasi*

Main category: cs.LG

TL;DR: MARIA, a transformer-based model, handles missing multimodal healthcare data without imputation, outperforming 10 state-of-the-art models across 8 tasks.


<details>
  <summary>Details</summary>
Motivation: Managing missing data in healthcare is challenging; existing imputation methods introduce biases. MARIA aims to address this by processing only available data.

Method: MARIA uses masked self-attention for intermediate fusion, avoiding synthetic data generation.

Result: MARIA outperforms 10 models in performance and resilience to data incompleteness.

Conclusion: MARIA shows promise for robust healthcare applications by effectively handling incomplete data.

Abstract: In healthcare, the integration of multimodal data is pivotal for developing
comprehensive diagnostic and predictive models. However, managing missing data
remains a significant challenge in real-world applications. We introduce MARIA
(Multimodal Attention Resilient to Incomplete datA), a novel transformer-based
deep learning model designed to address these challenges through an
intermediate fusion strategy. Unlike conventional approaches that depend on
imputation, MARIA utilizes a masked self-attention mechanism, which processes
only the available data without generating synthetic values. This approach
enables it to effectively handle incomplete datasets, enhancing robustness and
minimizing biases introduced by imputation methods. We evaluated MARIA against
10 state-of-the-art machine learning and deep learning models across 8
diagnostic and prognostic tasks. The results demonstrate that MARIA outperforms
existing methods in terms of performance and resilience to varying levels of
data incompleteness, underscoring its potential for critical healthcare
applications.

</details>


### [291] [Learning Disease Progression Models That Capture Health Disparities](https://arxiv.org/abs/2412.16406)
*Erica Chiang, Divya Shanmugam, Ashley N. Beecy, Gabriel Sayer, Deborah Estrin, Nikhil Garg, Emma Pierson*

Main category: cs.LG

TL;DR: The paper introduces a Bayesian disease progression model addressing health disparities, showing biases in severity estimates if disparities are ignored.


<details>
  <summary>Details</summary>
Motivation: Existing disease progression models overlook health disparities, leading to biased data and inequitable care.

Method: An interpretable Bayesian model is developed to account for three key health disparities in patient care.

Result: The model identifies disadvantaged groups and adjusts severity estimates, impacting high-risk patient classification.

Conclusion: Accounting for health disparities in disease models improves accuracy and equity in patient care.

Abstract: Disease progression models are widely used to inform the diagnosis and
treatment of many progressive diseases. However, a significant limitation of
existing models is that they do not account for health disparities that can
bias the observed data. To address this, we develop an interpretable Bayesian
disease progression model that captures three key health disparities: certain
patient populations may (1) start receiving care only when their disease is
more severe, (2) experience faster disease progression even while receiving
care, or (3) receive follow-up care less frequently conditional on disease
severity. We show theoretically and empirically that failing to account for any
of these disparities can result in biased estimates of severity (e.g.,
underestimating severity for disadvantaged groups). On a dataset of heart
failure patients, we show that our model can identify groups that face each
type of health disparity, and that accounting for these disparities while
inferring disease severity meaningfully shifts which patients are considered
high-risk.

</details>


### [292] [Sample complexity of data-driven tuning of model hyperparameters in neural networks with structured parameter-dependent dual function](https://arxiv.org/abs/2501.13734)
*Maria-Florina Balcan, Anh Tuan Nguyen, Dravyansh Sharma*

Main category: cs.LG

TL;DR: The paper studies the complexity of hyperparameter tuning in deep learning, introducing a data-driven approach to analyze and bound the learning theoretic complexity of utility functions.


<details>
  <summary>Details</summary>
Motivation: Despite practical advancements in hyperparameter tuning, the theoretical understanding of its complexity for deep neural networks remains limited. This paper aims to address this gap.

Method: The authors propose a technique to analyze the discontinuities and oscillations of utility functions, using tools from differential/algebraic geometry and constrained optimization.

Result: They bound the learning theoretic complexity of utility functions and provide sample complexity bounds for specific applications like tuning activation functions and kernel parameters in graph neural networks.

Conclusion: The study formalizes hyperparameter tuning complexity in deep learning, offering theoretical insights and practical bounds for real-world applications.

Abstract: Modern machine learning algorithms, especially deep learning based
techniques, typically involve careful hyperparameter tuning to achieve the best
performance. Despite the surge of intense interest in practical techniques like
Bayesian optimization and random search based approaches to automating this
laborious and compute intensive task, the fundamental learning theoretic
complexity of tuning hyperparameters for deep neural networks is poorly
understood. Inspired by this glaring gap, we initiate the formal study of
hyperparameter tuning complexity in deep learning through a recently introduced
data driven setting. We assume that we have a series of deep learning tasks,
and we have to tune hyperparameters to do well on average over the distribution
of tasks. A major difficulty is that the utility function as a function of the
hyperparameter is very volatile and furthermore, it is given implicitly by an
optimization problem over the model parameters. To tackle this challenge, we
introduce a new technique to characterize the discontinuities and oscillations
of the utility function on any fixed problem instance as we vary the
hyperparameter; our analysis relies on subtle concepts including tools from
differential/algebraic geometry and constrained optimization. This can be used
to show that the learning theoretic complexity of the corresponding family of
utility functions is bounded. We instantiate our results and provide sample
complexity bounds for concrete applications tuning a hyperparameter that
interpolates neural activation functions and setting the kernel parameter in
graph neural networks.

</details>


### [293] [BCAT: A Block Causal Transformer for PDE Foundation Models for Fluid Dynamics](https://arxiv.org/abs/2501.18972)
*Yuxuan Liu, Jingmin Sun, Hayden Schaeffer*

Main category: cs.LG

TL;DR: BCAT is a PDE foundation model using a block causal transformer for autoregressive prediction of 2D fluid dynamics, outperforming prior methods with a 1.18% average error.


<details>
  <summary>Details</summary>
Motivation: To improve prediction accuracy in fluid dynamics by leveraging spatial dependencies and contextual priors, avoiding limitations of sub-frame or pixel-based methods.

Method: Uses a block causal transformer architecture for next-frame prediction, trained on diverse fluid dynamics datasets (Navier-Stokes, shallow-water equations).

Result: Achieves 3.5x accuracy improvement over next token prediction, 1.18% average error, and 40% better accuracy with fine-tuning on turbulence data.

Conclusion: BCAT effectively models fluid dynamics, outperforming benchmarks and adapting well to new settings.

Abstract: We introduce BCAT, a PDE foundation model designed for autoregressive
prediction of solutions to two dimensional fluid dynamics problems. Our
approach uses a block causal transformer architecture to model next frame
predictions, leveraging previous frames as contextual priors rather than
relying solely on sub-frames or pixel-based inputs commonly used in image
generation methods. This block causal framework more effectively captures the
spatial dependencies inherent in nonlinear spatiotemporal dynamics and physical
phenomena. In an ablation study, next frame prediction demonstrated a 3.5x
accuracy improvement over next token prediction. BCAT is trained on a diverse
range of fluid dynamics datasets, including incompressible and compressible
Navier-Stokes equations across various geometries and parameter regimes, as
well as the shallow-water equations. The model's performance was evaluated on 6
distinct downstream prediction tasks and tested on about 8K trajectories to
measure robustness on a variety of fluid dynamics simulations. BCAT achieved an
average relative error of 1.18% across all evaluation tasks, outperforming
prior approaches on standard benchmarks. With fine-tuning on a turbulence
dataset, we show that the method adapts to new settings with more than 40%
better accuracy over prior methods.

</details>


### [294] [Explorations of the Softmax Space: Knowing When the Neural Network Doesn't Know](https://arxiv.org/abs/2502.00456)
*Daniel Sikar, Artur d'Avila Garcez, Tillman Weyde*

Main category: cs.LG

TL;DR: The paper proposes a method to measure confidence in neural network predictions using softmax layer outputs, introducing a 'not known' answer for low-confidence cases.


<details>
  <summary>Details</summary>
Motivation: To enhance the reliability of AI systems in critical decision-making by identifying when predictions should be deferred due to low confidence.

Method: Clusters softmax layer vectors, uses centroid distances to define confidence thresholds, and evaluates on MNIST and CIFAR-10 datasets.

Result: The approach is consistent across datasets and models, effectively identifying when predictions should be deferred.

Conclusion: The proposed distance metric efficiently determines when to defer predictions, improving AI system reliability.

Abstract: Ensuring the reliability of automated decision-making based on neural
networks will be crucial as Artificial Intelligence systems are deployed more
widely in critical situations. This paper proposes a new approach for measuring
confidence in the predictions of any neural network that relies on the
predictions of a softmax layer. We identify that a high-accuracy trained
network may have certain outputs for which there should be low confidence. In
such cases, decisions should be deferred and it is more appropriate for the
network to provide a \textit{not known} answer to a corresponding
classification task. Our approach clusters the vectors in the softmax layer to
measure distances between cluster centroids and network outputs. We show that a
cluster with centroid calculated simply as the mean softmax output for all
correct predictions can serve as a suitable proxy in the evaluation of
confidence. Defining a distance threshold for a class as the smallest distance
from an incorrect prediction to the given class centroid offers a simple
approach to adding \textit{not known} answers to any network classification
falling outside of the threshold. We evaluate the approach on the MNIST and
CIFAR-10 datasets using a Convolutional Neural Network and a Vision
Transformer, respectively. The results show that our approach is consistent
across datasets and network models, and indicate that the proposed distance
metric can offer an efficient way of determining when automated predictions are
acceptable and when they should be deferred to human operators.

</details>


### [295] [Predicting clinical outcomes from patient care pathways represented with temporal knowledge graphs](https://arxiv.org/abs/2502.21138)
*Jong Ho Jhee, Alberto Megina, Pacôme Constant Dit Beaufils, Matilde Karakachoff, Richard Redon, Alban Gaignard, Adrien Coulet*

Main category: cs.LG

TL;DR: Graph-based representations and GCN embeddings outperform tabular data in predictive modeling for intracranial aneurysm outcomes, with schema design and literal values being key factors.


<details>
  <summary>Details</summary>
Motivation: To explore the potential of knowledge graph representations and embeddings in biomedical predictive modeling, particularly for clinical outcomes.

Method: Simulated synthetic patient data for intracranial aneurysm, compared classification approaches on tabular vs. graph-based data, and analyzed schema and temporal data impacts.

Result: Graph representation with GCN embeddings achieved the best performance, with schema design and literal values being crucial. Time encoding had a moderate impact.

Conclusion: Graph-based methods are promising for biomedical predictive tasks, with careful schema design enhancing performance.

Abstract: Background: With the increasing availability of healthcare data, predictive
modeling finds many applications in the biomedical domain, such as the
evaluation of the level of risk for various conditions, which in turn can guide
clinical decision making. However, it is unclear how knowledge graph data
representations and their embedding, which are competitive in some settings,
could be of interest in biomedical predictive modeling. Method: We simulated
synthetic but realistic data of patients with intracranial aneurysm and
experimented on the task of predicting their clinical outcome. We compared the
performance of various classification approaches on tabular data versus a
graph-based representation of the same data. Next, we investigated how the
adopted schema for representing first individual data and second temporal data
impacts predictive performances. Results: Our study illustrates that in our
case, a graph representation and Graph Convolutional Network (GCN) embeddings
reach the best performance for a predictive task from observational data. We
emphasize the importance of the adopted schema and of the consideration of
literal values in the representation of individual data. Our study also
moderates the relative impact of various time encoding on GCN performance.

</details>


### [296] [Elucidating the Preconditioning in Consistency Distillation](https://arxiv.org/abs/2502.02922)
*Kaiwen Zheng, Guande He, Jianfei Chen, Fan Bao, Jun Zhu*

Main category: cs.LG

TL;DR: The paper introduces Analytic-Precond, a method to optimize preconditioning in consistency distillation for diffusion models, improving training efficiency and alignment with teacher models.


<details>
  <summary>Details</summary>
Motivation: Current preconditioning techniques in consistency distillation are hand-crafted and potentially suboptimal, limiting the efficiency and performance of diffusion models.

Method: Proposes Analytic-Precond, a principled approach to analytically optimize preconditioning by analyzing the consistency gap and teacher ODE trajectory.

Result: Demonstrates 2× to 3× training acceleration and better alignment with teacher models in multi-step generation across datasets.

Conclusion: Analytic-Precond offers a theoretical and practical improvement over manual preconditioning, enhancing consistency distillation efficiency.

Abstract: Consistency distillation is a prevalent way for accelerating diffusion models
adopted in consistency (trajectory) models, in which a student model is trained
to traverse backward on the probability flow (PF) ordinary differential
equation (ODE) trajectory determined by the teacher model. Preconditioning is a
vital technique for stabilizing consistency distillation, by linear combining
the input data and the network output with pre-defined coefficients as the
consistency function. It imposes the boundary condition of consistency
functions without restricting the form and expressiveness of the neural
network. However, previous preconditionings are hand-crafted and may be
suboptimal choices. In this work, we offer the first theoretical insights into
the preconditioning in consistency distillation, by elucidating its design
criteria and the connection to the teacher ODE trajectory. Based on these
analyses, we further propose a principled way dubbed \textit{Analytic-Precond}
to analytically optimize the preconditioning according to the consistency gap
(defined as the gap between the teacher denoiser and the optimal student
denoiser) on a generalized teacher ODE. We demonstrate that Analytic-Precond
can facilitate the learning of trajectory jumpers, enhance the alignment of the
student trajectory with the teacher's, and achieve $2\times$ to $3\times$
training acceleration of consistency trajectory models in multi-step generation
across various datasets.

</details>


### [297] [SAGE: A Framework of Precise Retrieval for RAG](https://arxiv.org/abs/2503.01713)
*Jintao Zhang, Guoliang Li, Jinyang Su*

Main category: cs.LG

TL;DR: The paper introduces SAGE, a RAG framework addressing retrieval inaccuracies in QA tasks by improving semantic segmentation, dynamic chunk selection, and context adjustment, outperforming baselines by 61.25% in QA quality and enhancing cost efficiency by 49.41%.


<details>
  <summary>Details</summary>
Motivation: Current RAG methods fail in QA due to poor semantic segmentation and irrelevant context retrieval, limiting LLM performance.

Method: SAGE includes a semantic segmentation model, a dynamic chunk selection algorithm, and LLM-based context adjustment to improve retrieval accuracy.

Result: SAGE improves QA quality by 61.25% and cost efficiency by 49.41% compared to baselines.

Conclusion: SAGE effectively addresses RAG limitations, offering insights for enhancing retrieval-augmented generation.

Abstract: Retrieval-augmented generation (RAG) has demonstrated significant proficiency
in conducting question-answering (QA) tasks within a specified corpus.
Nonetheless, numerous failure instances of RAG in QA still exist. These
failures are not solely attributable to the limitations of Large Language
Models (LLMs); instead, they predominantly arise from the retrieval of
inaccurate information for LLMs due to two limitations: (1) Current RAG methods
segment the corpus without considering semantics, making it difficult to find
relevant context due to impaired correlation between questions and the
segments. (2) There is a trade-off between missing essential context with fewer
context retrieved and getting irrelevant context with more context retrieved.
  In this paper, we introduce a RAG framework (SAGE), to overcome these
limitations. First, to address the segmentation issue without considering
semantics, we propose to train a semantic segmentation model. This model is
trained to segment the corpus into semantically complete chunks. Second, to
ensure that only the most relevant chunks are retrieved while the irrelevant
ones are ignored, we design a chunk selection algorithm to dynamically select
chunks based on the decreasing speed of the relevance score, leading to a more
relevant selection. Third, to further ensure the precision of the retrieved
chunks, we propose letting LLMs assess whether retrieved chunks are excessive
or lacking and then adjust the amount of context accordingly. Experiments show
that SAGE outperforms baselines by 61.25% in the quality of QA on average.
Moreover, by avoiding retrieving noisy context, SAGE lowers the cost of the
tokens consumed in LLM inference and achieves a 49.41% enhancement in cost
efficiency on average. Additionally, our work offers valuable insights for
boosting RAG.

</details>


### [298] [Efficient Diffusion Models: A Survey](https://arxiv.org/abs/2502.06805)
*Hui Shen, Jingxuan Zhang, Boning Xiong, Rui Hu, Shoufa Chen, Zhongwei Wan, Xin Wang, Yu Zhang, Zixuan Gong, Guangyin Bao, Chaofan Tao, Yongfeng Huang, Ye Yuan, Mi Zhang*

Main category: cs.LG

TL;DR: A survey on efficient diffusion models, covering algorithm-level, system-level, and framework perspectives, with a GitHub repository for organized resources.


<details>
  <summary>Details</summary>
Motivation: Diffusion models are powerful but computationally expensive, necessitating efficient techniques for practical use.

Method: Systematic review and taxonomy of efficient diffusion model research across three categories.

Result: Comprehensive survey and GitHub repository for organized literature.

Conclusion: The survey serves as a resource for researchers to understand and contribute to efficient diffusion model research.

Abstract: Diffusion models have emerged as powerful generative models capable of
producing high-quality contents such as images, videos, and audio,
demonstrating their potential to revolutionize digital content creation.
However, these capabilities come at the cost of their significant computational
resources and lengthy generation time, underscoring the critical need to
develop efficient techniques for practical deployment. In this survey, we
provide a systematic and comprehensive review of research on efficient
diffusion models. We organize the literature in a taxonomy consisting of three
main categories, covering distinct yet interconnected efficient diffusion model
topics from algorithm-level, system-level, and framework perspective,
respectively. We have also created a GitHub repository where we organize the
papers featured in this survey at
https://github.com/AIoT-MLSys-Lab/Efficient-Diffusion-Model-Survey. We hope our
survey can serve as a valuable resource to help researchers and practitioners
gain a systematic understanding of efficient diffusion model research and
inspire them to contribute to this important and exciting field.

</details>


### [299] [Galaxy Walker: Geometry-aware VLMs For Galaxy-scale Understanding](https://arxiv.org/abs/2503.18578)
*Tianyu Chen, Xingcheng Fu, Yisen Gao, Haodong Qian, Yuecen Wei, Kun Yan, Haoyi Zhou, Jianxin Li*

Main category: cs.LG

TL;DR: Galaxy-Walker is a geometry-aware vision-language model (VLM) designed for universe-level tasks, addressing limitations of Euclidean space in VLMs by integrating spherical and hyperbolic spaces.


<details>
  <summary>Details</summary>
Motivation: Current VLMs are limited to Euclidean space and lack backbones for anisotropic geometries, hindering their application to astronomical phenomena.

Method: Proposes geometry prompts for generating tokens via random walks across diverse spaces and a geometry adapter to handle space anisotropy.

Result: Achieves state-of-the-art performance in galaxy property estimation (R² up to 0.91) and morphology classification (+0.17 F1 improvement).

Conclusion: Galaxy-Walker outperforms domain-specific and general-purpose VLMs, demonstrating the effectiveness of geometry-aware modeling for astronomical tasks.

Abstract: Modern vision-language models (VLMs) develop patch embedding and convolution
backbone within vector space, especially Euclidean ones, at the very founding.
When expanding VLMs to a galaxy scale for understanding astronomical phenomena,
the integration of spherical space for planetary orbits and hyperbolic spaces
for black holes raises two formidable challenges. a) The current pre-training
model is confined to Euclidean space rather than a comprehensive geometric
embedding. b) The predominant architecture lacks suitable backbones for
anisotropic physical geometries. In this paper, we introduced Galaxy-Walker, a
geometry-aware VLM, for the universe-level vision understanding tasks. We
proposed the geometry prompt that generates geometry tokens by random walks
across diverse spaces on a multi-scale physical graph, along with a geometry
adapter that compresses and reshapes the space anisotropy in a
mixture-of-experts manner. Extensive experiments demonstrate the effectiveness
of our approach, with Galaxy-Walker achieving state-of-the-art performance in
both galaxy property estimation ($R^2$ scores up to $0.91$) and morphology
classification tasks (up to $+0.17$ F1 improvement in challenging features),
significantly outperforming both domain-specific models and general-purpose
VLMs.

</details>


### [300] [LabTOP: A Unified Model for Lab Test Outcome Prediction on Electronic Health Records](https://arxiv.org/abs/2502.14259)
*Sujeong Im, Jungwoo Oh, Edward Choi*

Main category: cs.LG

TL;DR: LabTOP is a model predicting lab test outcomes using EHR data via language modeling, outperforming existing methods.


<details>
  <summary>Details</summary>
Motivation: Frequent lab testing is burdensome, and results may be delayed. LabTOP aims to predict outcomes efficiently.

Method: Uses a language modeling approach on EHR data for continuous numerical predictions across diverse lab tests.

Result: Outperforms traditional ML and large language models on three EHR datasets, validated by ablation studies.

Conclusion: LabTOP is accurate and generalizable, useful for clinical decision support and early detection.

Abstract: Lab tests are fundamental for diagnosing diseases and monitoring patient
conditions. However, frequent testing can be burdensome for patients, and test
results may not always be immediately available. To address these challenges,
we propose LabTOP, a unified model that predicts lab test outcomes by
leveraging a language modeling approach on EHR data. Unlike conventional
methods that estimate only a subset of lab tests or classify discrete value
ranges, LabTOP performs continuous numerical predictions for a diverse range of
lab items. We evaluate LabTOP on three publicly available EHR datasets and
demonstrate that it outperforms existing methods, including traditional machine
learning models and state-of-the-art large language models. We also conduct
extensive ablation studies to confirm the effectiveness of our design choices.
We believe that LabTOP will serve as an accurate and generalizable framework
for lab test outcome prediction, with potential applications in clinical
decision support and early detection of critical conditions.

</details>


### [301] [Adversarial KA](https://arxiv.org/abs/2504.05255)
*Sviatoslav Dzhenzher, Michael H. Freedman*

Main category: cs.LG

TL;DR: The paper tests the robustness of the Kolmogorov-Arnold (KA) representation theorem against adversarial attacks, finding it robust to countable continuous adversaries but identifying a question about outer functions' equi-continuity that limits its broader applicability.


<details>
  <summary>Details</summary>
Motivation: To evaluate the KA theorem's resilience to adversarial attacks and its potential relevance to neural network (NN) theory.

Method: Analyzing the KA theorem's robustness by testing its resistance to adversarial attacks, focusing on countable continuous adversaries.

Result: KA is robust to countable continuous adversaries, but a question about outer functions' equi-continuity hinders broader applicability.

Conclusion: The regularity of outer functions is a critical issue for KA's applicability to NN theory, requiring further investigation.

Abstract: Regarding the representation theorem of Kolmogorov and Arnold (KA) as an
algorithm for representing or {\guillemotleft}expressing{\guillemotright}
functions, we test its robustness by analyzing its ability to withstand
adversarial attacks. We find KA to be robust to countable collections of
continuous adversaries, but unearth a question about the equi-continuity of the
outer functions that, so far, obstructs taking limits and defeating continuous
groups of adversaries. This question on the regularity of the outer functions
is relevant to the debate over the applicability of KA to the general theory of
NNs.

</details>


### [302] [Gaussian process surrogate model to approximate power grid simulators -- An application to the certification of a congestion management controller](https://arxiv.org/abs/2503.00094)
*Pierre Houdouin, Lucas Saludjian*

Main category: cs.LG

TL;DR: The paper proposes using Gaussian processes (GPs) as surrogate models for power grid simulators to speed up numerical experiments, addressing non-Gaussian behaviors with an adaptive residual uncertainty term.


<details>
  <summary>Details</summary>
Motivation: Digitalized power grids require time-consuming simulations, making safety validation computationally intractable. GPs offer a flexible, efficient, and interpretable solution but struggle with non-Gaussian simulator behaviors.

Method: The paper introduces an adaptive residual uncertainty term to enhance GP-based surrogate models, ensuring accuracy despite non-Gaussian simulator outputs.

Result: The approach successfully certifies a congestion management controller, avoiding over 98% of simulations.

Conclusion: The adaptive residual uncertainty term improves GP reliability for non-Gaussian power grid simulators, enabling efficient numerical experiments.

Abstract: With the digitalization of power grids, physical equations become
insufficient to describe the network's behavior, and realistic but
time-consuming simulators must be used. Numerical experiments, such as safety
validation, that involve simulating a large number of scenarios become
computationally intractable. A popular solution to reduce the computational
burden is to learn a surrogate model of the simulator with Machine Learning
(ML) and then conduct the experiment directly on the fast-to-evaluate surrogate
model. Among the various ML possibilities for building surrogate models,
Gaussian processes (GPs) emerged as a popular solution due to their
flexibility, data efficiency, and interpretability. Their probabilistic nature
enables them to provide both predictions and uncertainty quantification (UQ).
This paper starts with a discussion on the interest of using GPs to approximate
power grid simulators and fasten numerical experiments. Such simulators,
however, often violate the GP's underlying Gaussian assumption, leading to poor
approximations. To address this limitation, an approach that consists in adding
an adaptive residual uncertainty term to the UQ is proposed. It enables the GP
to remain accurate and reliable despite the simulator's non-Gaussian behaviors.
This approach is successfully applied to the certification of the proper
functioning of a congestion management controller, with over 98% of simulations
avoided.

</details>


### [303] [Weight Ensembling Improves Reasoning in Language Models](https://arxiv.org/abs/2504.10478)
*Xingyu Dang, Christina Baek, Kaiyue Wen, Zico Kolter, Aditi Raghunathan*

Main category: cs.LG

TL;DR: WiSE-FT, a weight interpolation method, recovers Pass@k performance during model training, improving both Pass@1 and test-time scaling, outperforming diversity-inducing decoding strategies.


<details>
  <summary>Details</summary>
Motivation: To address the collapse of generation diversity during reasoning model training, which harms Pass@k performance despite Pass@1 improvements.

Method: Interpolate weights of the latest SFT checkpoint with an early checkpoint (WiSE-FT) to recover diversity and improve scaling.

Result: WiSE-FT recovers Pass@k, improves Pass@1, and enhances test-time scaling (Best@k, majority vote). It also performs better with less data under reinforcement learning.

Conclusion: WiSE-FT effectively balances bias and variance in Pass@k, outperforming temperature scaling, and provides complementary gains beyond decoding strategies.

Abstract: We investigate a failure mode that arises during the training of reasoning
models, where the diversity of generations begins to collapse, leading to
suboptimal test-time scaling. Notably, the Pass@1 rate reliably improves during
supervised finetuning (SFT), but Pass@k rapidly deteriorates. Surprisingly, a
simple intervention of interpolating the weights of the latest SFT checkpoint
with an early checkpoint, otherwise known as WiSE-FT, almost completely
recovers Pass@k while also improving Pass@1. The WiSE-FT variant achieves
better test-time scaling (Best@k, majority vote) and achieves superior results
with less data when tuned further by reinforcement learning. Finally, we find
that WiSE-FT provides complementary performance gains that cannot be achieved
only through diversity-inducing decoding strategies, like temperature scaling.
We formalize a bias-variance tradeoff of Pass@k with respect to the expectation
and variance of Pass@1 over the test distribution. We find that WiSE-FT can
reduce bias and variance simultaneously, while temperature scaling inherently
trades off between bias and variance.

</details>


### [304] [TrafficKAN-GCN: Graph Convolutional-based Kolmogorov-Arnold Network for Traffic Flow Optimization](https://arxiv.org/abs/2503.03276)
*Jiayi Zhang, Yiming Zhang, Yuan Zheng, Yuchen Wang, Jinjiang You, Yuchen Xu, Wenxing Jiang, Soumyabrata Dev*

Main category: cs.LG

TL;DR: TrafficKAN-GCN, a hybrid deep learning framework combining KAN and GCN, improves urban traffic flow optimization by capturing complex patterns and topological dependencies, outperforming baseline models in accuracy and robustness.


<details>
  <summary>Details</summary>
Motivation: Urban traffic optimization is challenging due to the spatial-temporal complexity of real-world traffic flows, which traditional methods like Dijkstra's and Floyd's algorithms struggle to handle.

Method: The proposed TrafficKAN-GCN integrates KAN's adaptive nonlinear function approximation with GCN's spatial graph learning to optimize traffic flow.

Result: TrafficKAN-GCN achieves competitive prediction accuracy, handles noisy data well, and effectively redistributes traffic flow to mitigate congestion.

Conclusion: The framework shows promise for real-time traffic optimization, with future work aimed at reducing computational overhead and integrating temporal modeling for long-term predictions.

Abstract: Urban traffic optimization is critical for improving transportation
efficiency and alleviating congestion, particularly in large-scale dynamic
networks. Traditional methods, such as Dijkstra's and Floyd's algorithms,
provide effective solutions in static settings, but they struggle with the
spatial-temporal complexity of real-world traffic flows. In this work, we
propose TrafficKAN-GCN, a hybrid deep learning framework combining
Kolmogorov-Arnold Networks (KAN) with Graph Convolutional Networks (GCN),
designed to enhance urban traffic flow optimization. By integrating KAN's
adaptive nonlinear function approximation with GCN's spatial graph learning
capabilities, TrafficKAN-GCN captures both complex traffic patterns and
topological dependencies. We evaluate the proposed framework using real-world
traffic data from the Baltimore Metropolitan area. Compared with baseline
models such as MLP-GCN, standard GCN, and Transformer-based approaches,
TrafficKAN-GCN achieves competitive prediction accuracy while demonstrating
improved robustness in handling noisy and irregular traffic data. Our
experiments further highlight the framework's ability to redistribute traffic
flow, mitigate congestion, and adapt to disruptive events, such as the Francis
Scott Key Bridge collapse. This study contributes to the growing body of work
on hybrid graph learning for intelligent transportation systems, highlighting
the potential of combining KAN and GCN for real-time traffic optimization.
Future work will focus on reducing computational overhead and integrating
Transformer-based temporal modeling for enhanced long-term traffic prediction.
The proposed TrafficKAN-GCN framework offers a promising direction for
data-driven urban mobility management, balancing predictive accuracy,
robustness, and computational efficiency.

</details>


### [305] [Trust-Region Twisted Policy Improvement](https://arxiv.org/abs/2504.06048)
*Joery A. de Vries, Jinke He, Yaniv Oren, Matthijs T. J. Spaan*

Main category: cs.LG

TL;DR: TRT-SMC, a tailored SMC planner for RL, outperforms MCTS and SMC baselines in runtime and sample-efficiency by improving action sampling and policy estimation.


<details>
  <summary>Details</summary>
Motivation: Scaling MCTS for parallel compute is challenging, and existing SMC methods conflict with RL's goal of early policy improvement.

Method: TRT-SMC improves SMC planners for RL through constrained action sampling, terminal state handling, and better policy/value estimation.

Result: TRT-SMC shows superior runtime and sample-efficiency in discrete and continuous domains compared to MCTS and SMC baselines.

Conclusion: TRT-SMC is a promising alternative for RL planning, addressing limitations of MCTS and traditional SMC methods.

Abstract: Monte-Carlo tree search (MCTS) has driven many recent breakthroughs in deep
reinforcement learning (RL). However, scaling MCTS to parallel compute has
proven challenging in practice which has motivated alternative planners like
sequential Monte-Carlo (SMC). Many of these SMC methods adopt particle filters
for smoothing through a reformulation of RL as a policy inference problem. Yet,
persisting design choices of these particle filters often conflict with the aim
of online planning in RL, which is to obtain a policy improvement at the start
of planning. Drawing inspiration from MCTS, we tailor SMC planners specifically
for RL by improving data generation within the planner through constrained
action sampling and explicit terminal state handling, as well as improving
policy and value target estimation. This leads to our Trust-Region Twisted SMC
(TRT-SMC), which shows improved runtime and sample-efficiency over baseline
MCTS and SMC methods in both discrete and continuous domains.

</details>


### [306] [Quantitative Clustering in Mean-Field Transformer Models](https://arxiv.org/abs/2504.14697)
*Shi Chen, Zhengjiang Lin, Yury Polyanskiy, Philippe Rigollet*

Main category: cs.LG

TL;DR: The paper analyzes the clustering behavior of tokens in transformer models, showing exponential convergence to a Dirac point mass under certain conditions.


<details>
  <summary>Details</summary>
Motivation: To understand the long-time clustering dynamics of tokens in transformer models, drawing parallels to synchronization in Kuramoto models.

Method: Investigates mean-field transformer models, establishing exponential contraction rates to a Dirac point mass under specific assumptions.

Result: Demonstrates that regular initializations synchronize exponentially fast with quantitative rates.

Conclusion: The study provides insights into the clustering behavior of transformer models, highlighting exponential synchronization under regularity conditions.

Abstract: The evolution of tokens through a deep transformer models can be modeled as
an interacting particle system that has been shown to exhibit an asymptotic
clustering behavior akin to the synchronization phenomenon in Kuramoto models.
In this work, we investigate the long-time clustering of mean-field transformer
models. More precisely, we establish exponential rates of contraction to a
Dirac point mass for any suitably regular initialization under some assumptions
on the parameters of transformer models, any suitably regular mean-field
initialization synchronizes exponentially fast with some quantitative rates.

</details>


### [307] [Hexcute: A Tile-based Programming Language with Automatic Layout and Task-Mapping Synthesis](https://arxiv.org/abs/2504.16214)
*Xiao Zhang, Yaoyao Ding, Yang Hu, Gennady Pekhimenko*

Main category: cs.LG

TL;DR: Hexcute is a tile-based programming language for optimizing mixed-type matrix multiplication in DL workloads, balancing expressiveness and effort, achieving significant speedups.


<details>
  <summary>Details</summary>
Motivation: DL workloads on GPUs require efficient mixed-type matrix multiplication, but existing compilers lack expressiveness or demand high effort.

Method: Hexcute introduces shared memory and register abstractions, task mapping, and automated layout synthesis for fine-grained optimization.

Result: Hexcute achieves 1.7-11.28× speedup over existing compilers and up to 2.91× in end-to-end evaluations.

Conclusion: Hexcute effectively bridges the gap between expressiveness and effort, optimizing mixed-type DL operators on GPUs.

Abstract: Deep learning (DL) workloads mainly run on accelerators like GPUs. Recent DL
quantization techniques demand a new matrix multiplication operator with mixed
input data types, further complicating GPU optimization. Prior high-level
compilers like Triton lack the expressiveness to implement key optimizations
like fine-grained data pipelines and hardware-friendly memory layouts for these
operators, while low-level programming models, such as Hidet, Graphene, and
CUTLASS, require significant programming efforts. To balance expressiveness
with engineering effort, we propose Hexcute, a tile-based programming language
that exposes shared memory and register abstractions to enable fine-grained
optimization for these operators. Additionally, Hexcute leverages task mapping
to schedule the GPU program, and to reduce programming efforts, it automates
layout and task mapping synthesis with a novel type-inference-based algorithm.
Our evaluation shows that Hexcute generalizes to a wide range of DL operators,
achieves 1.7-11.28$\times$ speedup over existing DL compilers for mixed-type
operators, and brings up to 2.91$\times$ speedup in the end-to-end evaluation.

</details>


### [308] [Conditional Diffusion-Based Retrieval of Atmospheric CO2 from Earth Observing Spectroscopy](https://arxiv.org/abs/2504.17074)
*William R. Keely, Otto Lamminpää, Steffen Mauceri, Sean M. R. Crowell, Christopher W. O'Dell, Gregory R. McGarragh*

Main category: cs.LG

TL;DR: The paper proposes a diffusion-based method for faster and more accurate GHG retrieval from satellite data, addressing limitations of current Optimal Estimation (OE) methods.


<details>
  <summary>Details</summary>
Motivation: Current OE methods for GHG retrieval are computationally expensive, provide unrealistic uncertainty estimates, and struggle with non-Gaussian posteriors. Upcoming missions will generate more data, necessitating better algorithms.

Method: A diffusion-based approach is introduced to flexibly retrieve Gaussian or non-Gaussian posteriors, offering computational speed-up for NASA's OCO-2 spectrometer.

Result: The proposed method aims to improve accuracy and efficiency in GHG retrieval, enabling near real-time global monitoring.

Conclusion: This approach could significantly impact climate policy by enhancing GHG monitoring capabilities.

Abstract: Satellite-based estimates of greenhouse gas (GHG) properties from
observations of reflected solar spectra are integral for understanding and
monitoring complex terrestrial systems and their impact on the carbon cycle due
to their near global coverage. Known as retrieval, making GHG concentration
estimations from these observations is a non-linear Bayesian inverse problem,
which is operationally solved using a computationally expensive algorithm
called Optimal Estimation (OE), providing a Gaussian approximation to a
non-Gaussian posterior. This leads to issues in solver algorithm convergence,
and to unrealistically confident uncertainty estimates for the retrieved
quantities. Upcoming satellite missions will provide orders of magnitude more
data than the current constellation of GHG observers. Development of fast and
accurate retrieval algorithms with robust uncertainty quantification is
critical. Doing so stands to provide substantial climate impact of moving
towards the goal of near continuous real-time global monitoring of carbon
sources and sinks which is essential for policy making. To achieve this goal,
we propose a diffusion-based approach to flexibly retrieve a Gaussian or
non-Gaussian posterior, for NASA's Orbiting Carbon Observatory-2 spectrometer,
while providing a substantial computational speed-up over the current
operational state-of-the-art.

</details>


### [309] [A Novel Hybrid Approach Using an Attention-Based Transformer + GRU Model for Predicting Cryptocurrency Prices](https://arxiv.org/abs/2504.17079)
*Esam Mahdi, C. Martin-Barreiro, X. Cabezas*

Main category: cs.LG

TL;DR: A hybrid deep learning model combining Transformer and GRU improves cryptocurrency price prediction accuracy, outperforming other models.


<details>
  <summary>Details</summary>
Motivation: To enhance cryptocurrency price forecasting by leveraging the strengths of Transformer (long-range patterns) and GRU (short-term trends).

Method: Developed a hybrid Transformer-GRU model, tested on Bitcoin and Ethereum data, and compared with RBFN, GRNN, BiLSTM, and BiGRU using MSE, RMSE, MAE, MAPE, and statistical tests.

Result: The hybrid model consistently outperformed other models in accuracy, demonstrating its effectiveness for financial predictions.

Conclusion: The model offers valuable insights for real-time cryptocurrency decision-making and supports the use of hybrid deep learning in finance.

Abstract: In this article, we introduce a novel deep learning hybrid model that
integrates attention Transformer and Gated Recurrent Unit (GRU) architectures
to improve the accuracy of cryptocurrency price predictions. By combining the
Transformer's strength in capturing long-range patterns with the GRU's ability
to model short-term and sequential trends, the hybrid model provides a
well-rounded approach to time series forecasting. We apply the model to predict
the daily closing prices of Bitcoin and Ethereum based on historical data that
include past prices, trading volumes, and the Fear and Greed index. We evaluate
the performance of our proposed model by comparing it with four other machine
learning models: two are non-sequential feedforward models: Radial Basis
Function Network (RBFN) and General Regression Neural Network (GRNN), and two
are bidirectional sequential memory-based models: Bidirectional Long-Short-Term
Memory (BiLSTM) and Bidirectional Gated Recurrent Unit (BiGRU). The performance
of the model is assessed using several metrics, including Mean Squared Error
(MSE), Root Mean Squared Error (RMSE), Mean Absolute Error (MAE), and Mean
Absolute Percentage Error (MAPE), along with statistical validation through the
nonparametric Friedman test followed by a post hoc Wilcoxon signed rank test.
The results demonstrate that our hybrid model consistently achieves superior
accuracy, highlighting its effectiveness for financial prediction tasks. These
findings provide valuable insights for improving real-time decision making in
cryptocurrency markets and support the growing use of hybrid deep learning
models in financial analytics.

</details>


### [310] [Quantifying the Noise of Structural Perturbations on Graph Adversarial Attacks](https://arxiv.org/abs/2504.20869)
*Junyuan Fang, Han Yang, Haixian Wen, Jiajing Wu, Zibin Zheng, Chi K. Tse*

Main category: cs.LG

TL;DR: The paper introduces a method to quantify adversarial attack strength on graph neural networks using noise and proposes three attack strategies, demonstrating their effectiveness through experiments.


<details>
  <summary>Details</summary>
Motivation: Current graph neural networks lack robustness against adversarial attacks, and existing work lacks interpretability in perturbation strength.

Method: Proposes quantifying attack strength using noise and introduces three attack strategies based on noise and classification margins.

Result: Experiments show the proposed strategies are effective, and preferred patterns of adversarial perturbations are analyzed.

Conclusion: The work enhances interpretability and effectiveness of adversarial attacks on graph neural networks.

Abstract: Graph neural networks have been widely utilized to solve graph-related tasks
because of their strong learning power in utilizing the local information of
neighbors. However, recent studies on graph adversarial attacks have proven
that current graph neural networks are not robust against malicious attacks.
Yet much of the existing work has focused on the optimization objective based
on attack performance to obtain (near) optimal perturbations, but paid less
attention to the strength quantification of each perturbation such as the
injection of a particular node/link, which makes the choice of perturbations a
black-box model that lacks interpretability. In this work, we propose the
concept of noise to quantify the attack strength of each adversarial link.
Furthermore, we propose three attack strategies based on the defined noise and
classification margins in terms of single and multiple steps optimization.
Extensive experiments conducted on benchmark datasets against three
representative graph neural networks demonstrate the effectiveness of the
proposed attack strategies. Particularly, we also investigate the preferred
patterns of effective adversarial perturbations by analyzing the corresponding
properties of the selected perturbation nodes.

</details>


<div id='cs.MA'></div>

# cs.MA [[Back]](#toc)

### [311] [Advancing Multi-Agent Systems Through Model Context Protocol: Architecture, Implementation, and Applications](https://arxiv.org/abs/2504.21030)
*Naveen Krishnan*

Main category: cs.MA

TL;DR: The paper introduces a framework (MCP) for improving multi-agent systems by addressing context management, coordination, and scalability challenges. It demonstrates performance gains through case studies and provides an evaluation methodology.


<details>
  <summary>Details</summary>
Motivation: Multi-agent systems face challenges in context management, coordination efficiency, and scalability, limiting their effectiveness in complex problem-solving.

Method: The paper proposes the Model Context Protocol (MCP) framework, which includes standardized context sharing, coordination mechanisms, and advanced techniques. It validates the framework through case studies in knowledge management, research collaboration, and problem-solving.

Result: The framework shows significant performance improvements over traditional methods, supported by systematic evaluations and benchmark tasks.

Conclusion: The work advances AI systems by enhancing collaboration, context-awareness, and scalability, with potential applications across industries.

Abstract: Multi-agent systems represent a significant advancement in artificial
intelligence, enabling complex problem-solving through coordinated specialized
agents. However, these systems face fundamental challenges in context
management, coordination efficiency, and scalable operation. This paper
introduces a comprehensive framework for advancing multi-agent systems through
Model Context Protocol (MCP), addressing these challenges through standardized
context sharing and coordination mechanisms. We extend previous work on AI
agent architectures by developing a unified theoretical foundation, advanced
context management techniques, and scalable coordination patterns. Through
detailed implementation case studies across enterprise knowledge management,
collaborative research, and distributed problem-solving domains, we demonstrate
significant performance improvements compared to traditional approaches. Our
evaluation methodology provides a systematic assessment framework with
benchmark tasks and datasets specifically designed for multi-agent systems. We
identify current limitations, emerging research opportunities, and potential
transformative applications across industries. This work contributes to the
evolution of more capable, collaborative, and context-aware artificial
intelligence systems that can effectively address complex real-world
challenges.

</details>


### [312] [Multi-Agent Reinforcement Learning for Resources Allocation Optimization: A Survey](https://arxiv.org/abs/2504.21048)
*Mohamad A. Hady, Siyi Hu, Mahardhika Pratama, Jimmy Cao, Ryszard Kowalczyk*

Main category: cs.MA

TL;DR: A survey on Multi-Agent Reinforcement Learning (MARL) for Resource Allocation Optimization (RAO), covering concepts, classifications, and future directions.


<details>
  <summary>Details</summary>
Motivation: To address dynamic and decentralized RAO challenges in Industry 4.0 using MARL.

Method: Comprehensive review of recent MARL algorithms, including a structured taxonomy.

Result: Identifies current research trends, challenges, and future directions for MARL in RAO.

Conclusion: Aims to guide researchers and practitioners in advancing resource allocation solutions with MARL.

Abstract: Multi-Agent Reinforcement Learning (MARL) has become a powerful framework for
numerous real-world applications, modeling distributed decision-making and
learning from interactions with complex environments. Resource Allocation
Optimization (RAO) benefits significantly from MARL's ability to tackle dynamic
and decentralized contexts. MARL-based approaches are increasingly applied to
RAO challenges across sectors playing pivotal roles to Industry 4.0
developments. This survey provides a comprehensive review of recent MARL
algorithms for RAO, encompassing core concepts, classifications, and a
structured taxonomy. By outlining the current research landscape and
identifying primary challenges and future directions, this survey aims to
support researchers and practitioners in leveraging MARL's potential to advance
resource allocation solutions.

</details>


### [313] [NavEX: A Multi-Agent Coverage in Non-Convex and Uneven Environments via Exemplar-Clustering](https://arxiv.org/abs/2504.21113)
*Donipolo Ghimire, Carlos Nieto-Granda, Solmaz S. Kia*

Main category: cs.MA

TL;DR: NavEX is a novel framework for multi-agent deployment in non-convex, uneven environments, combining exemplar-clustering with obstacle-aware distances for fair-access and hotspot coverage tasks.


<details>
  <summary>Details</summary>
Motivation: Traditional approaches struggle with non-convex and uneven environments, limiting effective multi-agent deployment.

Method: NavEX uses exemplar-clustering and submodular optimization, integrating obstacle-aware and traversability-aware shortest distances.

Result: NavEX provides efficient, near-optimal solutions for fair-access and hotspot deployment, validated by simulations.

Conclusion: NavEX offers a flexible, unified approach for multi-agent deployment in complex environments with provable guarantees.

Abstract: This paper addresses multi-agent deployment in non-convex and uneven
environments. To overcome the limitations of traditional approaches, we
introduce Navigable Exemplar-Based Dispatch Coverage (NavEX), a novel dispatch
coverage framework that combines exemplar-clustering with obstacle-aware and
traversability-aware shortest distances, offering a deployment framework based
on submodular optimization. NavEX provides a unified approach to solve two
critical coverage tasks: (a) fair-access deployment, aiming to provide
equitable service by minimizing agent-target distances, and (b) hotspot
deployment, prioritizing high-density target regions. A key feature of NavEX is
the use of exemplar-clustering for the coverage utility measure, which provides
the flexibility to employ non-Euclidean distance metrics that do not
necessarily conform to the triangle inequality. This allows NavEX to
incorporate visibility graphs for shortest-path computation in environments
with planar obstacles, and traversability-aware RRT* for complex, rugged
terrains. By leveraging submodular optimization, the NavEX framework enables
efficient, near-optimal solutions with provable performance guarantees for
multi-agent deployment in realistic and complex settings, as demonstrated by
our simulations.

</details>


### [314] [Learning Large-Scale Competitive Team Behaviors with Mean-Field Interactions](https://arxiv.org/abs/2504.21164)
*Bhavini Jeloka, Yue Guan, Panagiotis Tsiotras*

Main category: cs.MA

TL;DR: MF-MAPPO extends PPO to mean-field MARL, enabling scalability to large agent teams in competitive games.


<details>
  <summary>Details</summary>
Motivation: Existing MARL algorithms (e.g., MADDPG, MAAC) struggle with scalability for large agent populations. Mean-field theory offers a solution by approximating agent behavior at scale.

Method: Proposes MF-MAPPO, integrating finite-population mean-field approximation into PPO for zero-sum competitive games between large teams.

Result: Demonstrates scalability to hundreds/thousands of agents, validated in large-scale offense-defense scenarios.

Conclusion: MF-MAPPO effectively addresses scalability in competitive MARL, with practical applications in large-team simulations.

Abstract: State-of-the-art multi-agent reinforcement learning (MARL) algorithms such as
MADDPG and MAAC fail to scale in situations where the number of agents becomes
large. Mean-field theory has shown encouraging results in modeling macroscopic
agent behavior for teams with a large number of agents through a continuum
approximation of the agent population and its interaction with the environment.
In this work, we extend proximal policy optimization (PPO) to the mean-field
domain by introducing the Mean-Field Multi-Agent Proximal Policy Optimization
(MF-MAPPO), a novel algorithm that utilizes the effectiveness of the
finite-population mean-field approximation in the context of zero-sum
competitive multi-agent games between two teams. The proposed algorithm can be
easily scaled to hundreds and thousands of agents in each team as shown through
numerical experiments. In particular, the algorithm is applied to realistic
applications such as large-scale offense-defense battlefield scenarios.

</details>


### [315] [Robust Multi-agent Communication Based on Decentralization-Oriented Adversarial Training](https://arxiv.org/abs/2504.21278)
*Xuyan Ma, Yawen Wang, Junjie Wang, Xiaofei Xie, Boyu Wu, Shoubin Li, Fanjiang Xu, Qing Wang*

Main category: cs.MA

TL;DR: DMAC improves robustness in multi-agent communication by decentralizing policies using adversarial training to mask critical channels.


<details>
  <summary>Details</summary>
Motivation: Existing MARL methods suffer from unbalanced communication structures, making them vulnerable to critical channel failures.

Method: DMAC trains an adversary (DMAC_Adv) to identify and mask critical channels, using adversarial samples to decentralize communication policies.

Result: DMAC enhances robustness and performance in communication policies, achieving decentralization with acceptable cost.

Conclusion: DMAC effectively decentralizes communication policies, improving robustness and performance in multi-agent tasks.

Abstract: In typical multi-agent reinforcement learning (MARL) problems, communication
is important for agents to share information and make the right decisions.
However, due to the complexity of training multi-agent communication, existing
methods often fall into the dilemma of local optimization, which leads to the
concentration of communication in a limited number of channels and presents an
unbalanced structure. Such unbalanced communication policy are vulnerable to
abnormal conditions, where the damage of critical communication channels can
trigger the crash of the entire system. Inspired by decentralization theory in
sociology, we propose DMAC, which enhances the robustness of multi-agent
communication policies by retraining them into decentralized patterns.
Specifically, we train an adversary DMAC\_Adv which can dynamically identify
and mask the critical communication channels, and then apply the adversarial
samples generated by DMAC\_Adv to the adversarial learning of the communication
policy to force the policy in exploring other potential communication schemes
and transition to a decentralized structure. As a training method to improve
robustness, DMAC can be fused with any learnable communication policy
algorithm. The experimental results in two communication policies and four
multi-agent tasks demonstrate that DMAC achieves higher improvement on
robustness and performance of communication policy compared with two
state-of-the-art and commonly-used baselines. Also, the results demonstrate
that DMAC can achieve decentralized communication structure with acceptable
communication cost.

</details>


### [316] [Uncertainty, bias and the institution bootstrapping problem](https://arxiv.org/abs/2504.21579)
*Stavros Anagnou, Christoph Salge, Peter R. Lewis*

Main category: cs.MA

TL;DR: The paper addresses the 'institution bootstrapping problem'—how institutions form when individuals won't join without a critical mass. It proposes that misperception (belief an institution exists) and cognitive biases resolve this paradox, reducing the required critical mass.


<details>
  <summary>Details</summary>
Motivation: To understand how institutions emerge despite the paradox of needing a critical mass for participation, focusing on human cognitive biases and perceptual noise.

Method: Integrates psychological phenomena (cognitive biases, probability distortion, perceptual noise) into a game-theoretic framework to model institutional emergence.

Result: Unbiased perceptual noise reduces the critical mass needed for institutions; diversity of perceptions intensifies this effect. Distortion types (proportional vs. absolute) yield different evolutionary pathways.

Conclusion: Human-like cognitive constraints, not just rationality, are crucial for institutional design. 'Noisy' cognition can enhance cooperation, challenging conventional assumptions.

Abstract: Institutions play a critical role in enabling communities to manage
common-pool resources and avert tragedies of the commons. However, a
fundamental issue arises: Individuals typically perceive participation as
advantageous only after an institution is established, creating a paradox: How
can institutions form if no one will join before a critical mass exists? We
term this conundrum the institution bootstrapping problem and propose that
misperception, specifically, agents' erroneous belief that an institution
already exists, could resolve this paradox. By integrating well-documented
psychological phenomena, including cognitive biases, probability distortion,
and perceptual noise, into a game-theoretic framework, we demonstrate how these
factors collectively mitigate the bootstrapping problem. Notably, unbiased
perceptual noise (e.g., noise arising from agents' heterogeneous physical or
social contexts) drastically reduces the critical mass of cooperators required
for institutional emergence. This effect intensifies with greater diversity of
perceptions. We explain this counter-intuitive result through asymmetric
boundary conditions: proportional underestimation of low-probability sanctions
produces distinct outcomes compared to equivalent overestimation. Furthermore,
the type of perceptual distortion, proportional versus absolute, yields
qualitatively different evolutionary pathways. These findings challenge
conventional assumptions about rationality in institutional design,
highlighting how "noisy" cognition can paradoxically enhance cooperation.
Finally, we contextualize these insights within broader discussions of
multi-agent system design and collective action. Our analysis underscores the
importance of incorporating human-like cognitive constraints, not just
idealized rationality, into models of institutional emergence and resilience.

</details>


### [317] [MF-LLM: Simulating Collective Decision Dynamics via a Mean-Field Large Language Model Framework](https://arxiv.org/abs/2504.21582)
*Qirui Mi, Mengyue Yang, Xiangning Yu, Zhiyu Zhao, Cheng Deng, Bo An, Haifeng Zhang, Xu Chen, Jun Wang*

Main category: cs.MA

TL;DR: The paper introduces MF-LLM, a framework for simulating collective decision-making by modeling interactions between individual actions and population dynamics, using IB-Tune for better alignment with real-world data.


<details>
  <summary>Details</summary>
Motivation: Existing LLM-based social simulations often deviate from real-world data, necessitating a method to better model dynamic interactions and feedback loops in collective decision-making.

Method: MF-LLM alternates between a policy model for individual actions and a mean field model for population updates, fine-tuned with IB-Tune to align with real-world data.

Result: MF-LLM reduces KL divergence to human data by 47% and generalizes across domains and LLM backbones, enabling accurate trend forecasting.

Conclusion: MF-LLM provides a scalable, high-fidelity foundation for social simulation by effectively modeling collective decision-making dynamics.

Abstract: Simulating collective decision-making involves more than aggregating
individual behaviors; it arises from dynamic interactions among individuals.
While large language models (LLMs) show promise for social simulation, existing
approaches often exhibit deviations from real-world data. To address this gap,
we propose the Mean-Field LLM (MF-LLM) framework, which explicitly models the
feedback loop between micro-level decisions and macro-level population. MF-LLM
alternates between two models: a policy model that generates individual actions
based on personal states and group-level information, and a mean field model
that updates the population distribution from the latest individual decisions.
Together, they produce rollouts that simulate the evolving trajectories of
collective decision-making. To better match real-world data, we introduce
IB-Tune, a fine-tuning method for LLMs grounded in the information bottleneck
principle, which maximizes the relevance of population distributions to future
actions while minimizing redundancy with historical data. We evaluate MF-LLM on
a real-world social dataset, where it reduces KL divergence to human population
distributions by 47 percent over non-mean-field baselines, and enables accurate
trend forecasting and intervention planning. It generalizes across seven
domains and four LLM backbones, providing a scalable foundation for
high-fidelity social simulation.

</details>


<div id='cs.MM'></div>

# cs.MM [[Back]](#toc)

### [318] [Latent Feature-Guided Conditional Diffusion for High-Fidelity Generative Image Semantic Communication](https://arxiv.org/abs/2504.21577)
*Zehao Chen, Xinfeng Wei, Haonan Tong, Zhaohui Yang, Changchuan Yin*

Main category: cs.MM

TL;DR: The paper proposes a latent representation-oriented image semantic communication (LRISC) system to enhance perceptual quality in 6G networks by focusing on semantic consistency and human perception, outperforming existing methods.


<details>
  <summary>Details</summary>
Motivation: Existing DeepJSCC schemes prioritize pixel-level metrics over human perceptual needs, degrading perceptual quality. LRISC aims to address this gap.

Method: LRISC maps images to latent features, encodes them with adaptive JSCC, and reconstructs images using a conditional diffusion model guided by received features. It also includes SNR adaptation.

Result: LRISC reduces LPIPS by 43.3% compared to DeepJSCC, improving perceptual quality and robustness against channel noise while maintaining semantic consistency.

Conclusion: LRISC effectively balances semantic consistency and perceptual quality, offering a superior solution for image semantic communication in 6G networks.

Abstract: Semantic communication is proposed and expected to improve the efficiency and
effectiveness of massive data transmission over sixth generation (6G) networks.
However, existing deep learning-based joint source and channel coding
(DeepJSCC) image semantic communication scheme predominantly focuses on
optimizing pixel-level metrics, and neglects human perceptual requirements,
which results in degraded perceptual quality. To address this issue, we propose
a latent representation-oriented image semantic communication (LRISC) system,
which transmits latent semantic features for image generation with semantic
consistency, thereby ensuring the perceptual quality at the receiver. In
particular, we first map the source image to latent features in a
high-dimensional semantic space via a neural network (NN)- based non-linear
transformation. Subsequently, these features are encoded using a joint source
and channel coding (JSCC) scheme with adaptive coding length for efficient
transmission over a wireless channel. At the receiver, a conditional diffusion
model is developed by using the received latent features as conditional
guidance to steer the reverse diffusion process, progressively reconstructing
high-fidelity images while preserving semantic consistency. Moreover, we
introduce a channel signal-to-noise ratio (SNR) adaptation mechanism, allowing
one model to work across various channel states. Experiments show that the
proposed method significantly outperforms existing methods, in terms of learned
perceptual image patch similarity (LPIPS) and robustness against channel noise,
with an average LPIPS reduction of 43.3% compared to DeepJSCC, while
guaranteeing the semantic consistency.

</details>


### [319] [Solving Copyright Infringement on Short Video Platforms: Novel Datasets and an Audio Restoration Deep Learning Pipeline](https://arxiv.org/abs/2504.21772)
*Minwoo Oh, Minsu Park, Eunil Park*

Main category: cs.MM

TL;DR: A novel pipeline combining Music Source Separation and cross-modal video-music retrieval addresses copyright issues in short videos by separating and restoring original soundtracks, supported by two new datasets.


<details>
  <summary>Details</summary>
Motivation: To combat copyright infringement in short videos where background music obscures original soundtracks, evading detection.

Method: Proposes a pipeline integrating Music Source Separation (MSS) and cross-modal video-music retrieval (CMVMR) to separate and restore original audio tracks.

Result: Effectively removes arbitrary background music and restores original soundtracks, validated by domain-specific datasets (OASD-20K and OSVAR-160).

Conclusion: Provides an ethical, scalable solution for copyright compliance in user-generated short videos.

Abstract: Short video platforms like YouTube Shorts and TikTok face significant
copyright compliance challenges, as infringers frequently embed arbitrary
background music (BGM) to obscure original soundtracks (OST) and evade content
originality detection. To tackle this issue, we propose a novel pipeline that
integrates Music Source Separation (MSS) and cross-modal video-music retrieval
(CMVMR). Our approach effectively separates arbitrary BGM from the original
OST, enabling the restoration of authentic video audio tracks. To support this
work, we introduce two domain-specific datasets: OASD-20K for audio separation
and OSVAR-160 for pipeline evaluation. OASD-20K contains 20,000 audio clips
featuring mixed BGM and OST pairs, while OSVAR160 is a unique benchmark dataset
comprising 1,121 video and mixed-audio pairs, specifically designed for short
video restoration tasks. Experimental results demonstrate that our pipeline not
only removes arbitrary BGM with high accuracy but also restores OSTs, ensuring
content integrity. This approach provides an ethical and scalable solution to
copyright challenges in user-generated content on short video platforms.

</details>


### [320] [A 3D Framework for Improving Low-Latency Multi-Channel Live Streaming](https://arxiv.org/abs/2410.16284)
*Aizierjiang Aiersilan, Zhiqiang Wang*

Main category: cs.MM

TL;DR: A novel framework using 3D virtual environments in game engines (e.g., Unity 3D) optimizes multi-channel live streaming by consolidating video data into a single stream, reducing latency and improving flexibility.


<details>
  <summary>Details</summary>
Motivation: The rise of 5G demands high-quality, low-latency live streaming, but challenges like data volume, synchronization, and quality consistency persist.

Method: The approach integrates multi-camera video into a single stream using virtual 3D canvases in Unity 3D, minimizing redundant data transmission.

Result: The method reduces latency and enhances user interaction responsiveness, outperforming existing solutions.

Conclusion: The framework effectively addresses 5G live streaming challenges, with an open-source system available for further development.

Abstract: The advent of 5G has driven the demand for high-quality, low-latency live
streaming. However, challenges such as managing the increased data volume,
ensuring synchronization across multiple streams, and maintaining consistent
quality under varying network conditions persist, particularly in real-time
video streaming. To address these issues, we propose a novel framework that
leverages 3D virtual environments within game engines (e.g., Unity 3D) to
optimize multi-channel live streaming. Our approach consolidates multi-camera
video data into a single stream using multiple virtual 3D canvases,
significantly increasing channel amounts while reducing latency and enhancing
user flexibility. For demonstration of our approach, we utilize the Unity 3D
engine to integrate multiple video inputs into a single-channel stream,
supporting one-to-many broadcasting, one-to-one video calling, and real-time
control of video channels. By mapping video data onto a world-space canvas and
capturing it via an in-world camera, we minimize redundant data transmission,
achieving efficient, low-latency streaming. Our results demonstrate that this
method outperforms some existing multi-channel live streaming solutions in both
latency reduction and user interaction responsiveness improvement. Our live
video streaming system affiliated with this paper is also open-source at
https://github.com/Aizierjiang/LiveStreaming.

</details>


### [321] [EEmo-Bench: A Benchmark for Multi-modal Large Language Models on Image Evoked Emotion Assessment](https://arxiv.org/abs/2504.16405)
*Lancheng Gao, Ziheng Jia, Yunhao Zeng, Wei Sun, Yiming Zhang, Wei Zhou, Guangtao Zhai, Xiongkuo Min*

Main category: cs.MM

TL;DR: EEmo-Bench is a benchmark for evaluating MLLMs' ability to understand image-evoked emotions, using diverse tasks and emotional attributes.


<details>
  <summary>Details</summary>
Motivation: Current evaluations of MLLMs' emotion understanding are coarse-grained; EEmo-Bench aims to provide a systematic and comprehensive assessment.

Method: The benchmark uses 1,960 annotated images with Valence-Arousal-Dominance (VAD) attributes and four tasks (Perception, Ranking, Description, Assessment) to evaluate MLLMs.

Result: Some MLLMs perform well overall, but analytical capabilities in certain dimensions are lacking.

Conclusion: EEmo-Bench supports future research to improve MLLMs' emotion perception and understanding.

Abstract: The furnishing of multi-modal large language models (MLLMs) has led to the
emergence of numerous benchmark studies, particularly those evaluating their
perception and understanding capabilities.
  Among these, understanding image-evoked emotions aims to enhance MLLMs'
empathy, with significant applications such as human-machine interaction and
advertising recommendations. However, current evaluations of this MLLM
capability remain coarse-grained, and a systematic and comprehensive assessment
is still lacking.
  To this end, we introduce EEmo-Bench, a novel benchmark dedicated to the
analysis of the evoked emotions in images across diverse content categories.
  Our core contributions include:
  1) Regarding the diversity of the evoked emotions, we adopt an emotion
ranking strategy and employ the Valence-Arousal-Dominance (VAD) as emotional
attributes for emotional assessment. In line with this methodology, 1,960
images are collected and manually annotated.
  2) We design four tasks to evaluate MLLMs' ability to capture the evoked
emotions by single images and their associated attributes: Perception, Ranking,
Description, and Assessment. Additionally, image-pairwise analysis is
introduced to investigate the model's proficiency in performing joint and
comparative analysis.
  In total, we collect 6,773 question-answer pairs and perform a thorough
assessment on 19 commonly-used MLLMs.
  The results indicate that while some proprietary and large-scale open-source
MLLMs achieve promising overall performance, the analytical capabilities in
certain evaluation dimensions remain suboptimal.
  Our EEmo-Bench paves the path for further research aimed at enhancing the
comprehensive perceiving and understanding capabilities of MLLMs concerning
image-evoked emotions, which is crucial for machine-centric emotion perception
and understanding.

</details>


<div id='eess.AS'></div>

# eess.AS [[Back]](#toc)

### [322] [Impairments are Clustered in Latents of Deep Neural Network-based Speech Quality Models](https://arxiv.org/abs/2504.21528)
*Fredrik Cumlin, Xinyu Liang, Victor Ungureanu, Chandan K. A. Reddy, Christian Schüldt, Saikat Chatterjee*

Main category: eess.AS

TL;DR: DNN-based SQA models show latent representations where impairments cluster naturally, enabling good classification without explicit training. DNSMOS+ improves SQA and impairment classification.


<details>
  <summary>Details</summary>
Motivation: To explore if DNN-based SQA models inherently cluster impairments and if improving SQA enhances impairment classification.

Method: Used various audio degradations, kNN classifier for visualization, and developed DNSMOS+ for testing.

Result: Achieved 94% accuracy on LibriAugmented (16 impairments) and 54% on ESC-50 (50 real noises).

Conclusion: DNN-based SQA models naturally cluster impairments, and SQA improvements enhance classification performance.

Abstract: In this article, we provide an experimental observation: Deep neural network
(DNN) based speech quality assessment (SQA) models have inherent latent
representations where many types of impairments are clustered. While DNN-based
SQA models are not trained for impairment classification, our experiments show
good impairment classification results in an appropriate SQA latent
representation. We investigate the clustering of impairments using various
kinds of audio degradations that include different types of noises, waveform
clipping, gain transition, pitch shift, compression, reverberation, etc. To
visualize the clusters we perform classification of impairments in the
SQA-latent representation domain using a standard k-nearest neighbor (kNN)
classifier. We also develop a new DNN-based SQA model, named DNSMOS+, to
examine whether an improvement in SQA leads to an improvement in impairment
classification. The classification accuracy is 94% for LibriAugmented dataset
with 16 types of impairments and 54% for ESC-50 dataset with 50 types of real
noises.

</details>


### [323] [From Aesthetics to Human Preferences: Comparative Perspectives of Evaluating Text-to-Music Systems](https://arxiv.org/abs/2504.21815)
*Huan Zhang, Jinhua Liang, Huy Phan, Wenwu Wang, Emmanouil Benetos*

Main category: eess.AS

TL;DR: The paper investigates the gap between automatic evaluation metrics and human preferences in music generation, revealing inconsistencies in current practices and advocating for human-centered evaluation.


<details>
  <summary>Details</summary>
Motivation: To address the challenge of evaluating generative models, especially in reflecting human preferences, using music generation as a case study.

Method: Comparative experiments across five music generation approaches, assessing perceptual quality and distributional similarity using metrics like MAD and KAD.

Result: Significant inconsistencies among metrics, highlighting limitations in current evaluation practices.

Conclusion: Advocates for human-centered evaluation strategies and releases a benchmark dataset to support further research.

Abstract: Evaluating generative models remains a fundamental challenge, particularly
when the goal is to reflect human preferences. In this paper, we use music
generation as a case study to investigate the gap between automatic evaluation
metrics and human preferences. We conduct comparative experiments across five
state-of-the-art music generation approaches, assessing both perceptual quality
and distributional similarity to human-composed music. Specifically, we
evaluate synthesis music from various perceptual dimensions and examine
reference-based metrics such as Mauve Audio Divergence (MAD) and Kernel Audio
Distance (KAD). Our findings reveal significant inconsistencies across the
different metrics, highlighting the limitation of the current evaluation
practice. To support further research, we release a benchmark dataset
comprising samples from multiple models. This study provides a broader
perspective on the alignment of human preference in generative modeling,
advocating for more human-centered evaluation strategies across domains.

</details>


### [324] [Improving the Robustness and Clinical Applicability of Automatic Respiratory Sound Classification Using Deep Learning-Based Audio Enhancement: Algorithm Development and Validation](https://arxiv.org/abs/2407.13895)
*Jing-Tong Tzeng, Jeng-Lin Li, Huan-Yu Chen, Chun-Hsiang Huang, Chi-Hsin Chen, Cheng-Yi Fan, Edward Pei-Chuan Huang, Chi-Chun Lee*

Main category: eess.AS

TL;DR: Incorporating deep learning-based audio enhancement improves respiratory sound classification in noisy conditions, boosting performance and clinical trust.


<details>
  <summary>Details</summary>
Motivation: Accurate respiratory sound classification in noisy real-world settings is challenging, and false predictions from background noise reduce user trust.

Method: Tested various audio enhancement models (time-domain and time-frequency-domain) combined with classification models, comparing against noise injection augmentation. Used ICBHI and FABS datasets and physician validation.

Result: 21.9% improvement on ICBHI and 4.1% on FABS in noisy scenarios. Enhanced audio increased diagnostic sensitivity by 11.6% and confidence.

Conclusion: Audio enhancement enhances robustness and clinical utility of respiratory sound classification, improving performance in noise and fostering trust.

Abstract: Deep learning techniques have shown promising results in the automatic
classification of respiratory sounds. However, accurately distinguishing these
sounds in real-world noisy conditions remains challenging for clinical
deployment. In addition, predicting signals with only background noise may
reduce user trust in the system. This study explores the feasibility and
effectiveness of incorporating a deep learning-based audio enhancement step
into automatic respiratory sound classification systems to improve robustness
and clinical applicability. We conducted extensive experiments using various
audio enhancement model architectures, including time-domain and
time-frequency-domain approaches, combined with multiple classification models
to evaluate the module's effectiveness. The classification performance was
compared against the noise injection data augmentation method. These
experiments were carried out on two datasets: the ICBHI respiratory sound
dataset and the FABS dataset. Furthermore, a physician validation study
assessed the system's clinical utility. Integrating the audio enhancement
module resulted in a 21.9% increase in the ICBHI classification score and a
4.1% improvement on the FABS dataset in multi-class noisy scenarios.
Quantitative analysis revealed efficiency gains, higher diagnostic confidence,
and increased trust, with workflows using enhanced audio improving diagnostic
sensitivity by 11.6% and enabling high-confidence diagnoses. Incorporating an
audio enhancement algorithm boosts the robustness and clinical utility of
automatic respiratory sound classification systems, enhancing performance in
noisy environments and fostering greater trust among medical professionals.

</details>


### [325] [Addressing Emotion Bias in Music Emotion Recognition and Generation with Frechet Audio Distance](https://arxiv.org/abs/2409.15545)
*Yuanchao Li, Azalea Gui, Dimitra Emmanouilidou, Hannes Gamper*

Main category: eess.AS

TL;DR: The paper addresses bias in Music Emotion Recognition (MER) and Emotional Music Generation (EMG) by using diverse audio encoders and Frechet Audio Distance (FAD) for evaluation. It benchmarks MER, proposes FAD for objective assessment, enhances EMG for better emotion variability, and compares synthetic vs. real music emotion realism.


<details>
  <summary>Details</summary>
Motivation: To mitigate inherent bias in MER and EMG caused by reliance on single encoders or metrics, aiming for more objective evaluation and realistic emotion generation.

Method: Uses diverse audio encoders and FAD for MER benchmarking, proposes FAD-based MER assessment, introduces an enhanced EMG approach, and compares synthetic vs. real music emotion realism.

Result: Highlights emotion bias issues in MER and EMG, shows FAD and diverse encoders improve objectivity, and demonstrates enhanced EMG's realism.

Conclusion: FAD and diverse encoders can reduce bias and improve MER and EMG, with the enhanced EMG model showing promise for realistic emotion generation.

Abstract: The complex nature of musical emotion introduces inherent bias in both
recognition and generation, particularly when relying on a single audio
encoder, emotion classifier, or evaluation metric. In this work, we conduct a
study on Music Emotion Recognition (MER) and Emotional Music Generation (EMG),
employing diverse audio encoders alongside Frechet Audio Distance (FAD), a
reference-free evaluation metric. Our study begins with a benchmark evaluation
of MER, highlighting the limitations of using a single audio encoder and the
disparities observed across different measurements. We then propose assessing
MER performance using FAD derived from multiple encoders to provide a more
objective measure of musical emotion. Furthermore, we introduce an enhanced EMG
approach designed to improve both the variability and prominence of generated
musical emotion, thereby enhancing its realism. Additionally, we investigate
the differences in realism between the emotions conveyed in real and synthetic
music, comparing our EMG model against two baseline models. Experimental
results underscore the issue of emotion bias in both MER and EMG and
demonstrate the potential of using FAD and diverse audio encoders to evaluate
musical emotion more objectively and effectively.

</details>


### [326] [Revise, Reason, and Recognize: LLM-Based Emotion Recognition via Emotion-Specific Prompts and ASR Error Correction](https://arxiv.org/abs/2409.15551)
*Yuanchao Li, Yuan Gong, Chao-Han Huck Yang, Peter Bell, Catherine Lai*

Main category: eess.AS

TL;DR: The paper explores LLM-based emotion recognition from speech, proposing novel prompts, an ASR error correction pipeline, and evaluating LLM training schemes.


<details>
  <summary>Details</summary>
Motivation: To address the questionable efficacy of LLM-based emotion recognition and refine its application by incorporating emotion-specific knowledge and handling ASR errors.

Method: Proposes emotion-specific prompts, a Revise-Reason-Recognize pipeline for ASR error correction, and evaluates LLM training schemes (context-aware, in-context, instruction tuning).

Result: Demonstrates efficacy of emotion-specific prompts, ASR error correction, and LLM training schemes in improving emotion recognition.

Conclusion: The study refines LLM use in emotion recognition, highlighting the importance of prompt design and error handling.

Abstract: Annotating and recognizing speech emotion using prompt engineering has
recently emerged with the advancement of Large Language Models (LLMs), yet its
efficacy and reliability remain questionable. In this paper, we conduct a
systematic study on this topic, beginning with the proposal of novel prompts
that incorporate emotion-specific knowledge from acoustics, linguistics, and
psychology. Subsequently, we examine the effectiveness of LLM-based prompting
on Automatic Speech Recognition (ASR) transcription, contrasting it with
ground-truth transcription. Furthermore, we propose a Revise-Reason-Recognize
prompting pipeline for robust LLM-based emotion recognition from spoken
language with ASR errors. Additionally, experiments on context-aware learning,
in-context learning, and instruction tuning are performed to examine the
usefulness of LLM training schemes in this direction. Finally, we investigate
the sensitivity of LLMs to minor prompt variations. Experimental results
demonstrate the efficacy of the emotion-specific prompts, ASR error correction,
and LLM training schemes for LLM-based emotion recognition. Our study aims to
refine the use of LLMs in emotion recognition and related domains.

</details>


### [327] [Cross-Lingual Speech Emotion Recognition: Humans vs. Self-Supervised Models](https://arxiv.org/abs/2409.16920)
*Zhichen Han, Tianqi Geng, Hui Feng, Jiahong Yuan, Korin Richmond, Yuanchao Li*

Main category: eess.AS

TL;DR: The study compares SSL models and humans in cross-lingual SER, showing models can match native speaker performance with proper transfer and highlighting dialect's impact on SER.


<details>
  <summary>Details</summary>
Motivation: Limited research exists on cross-lingual SER using SSL models, prompting a comparison with human performance and exploration of dialect effects.

Method: Layer-wise analysis and parameter-efficient fine-tuning in monolingual, cross-lingual, and transfer learning contexts, plus human evaluation of dialect impact.

Result: Models adapt to target languages comparably to humans, with dialect significantly affecting SER for non-native speakers. Both exhibit emotion-specific behaviors.

Conclusion: SSL models show promise in cross-lingual SER but differ from human perception, with dialect playing a key role in performance.

Abstract: Utilizing Self-Supervised Learning (SSL) models for Speech Emotion
Recognition (SER) has proven effective, yet limited research has explored
cross-lingual scenarios. This study presents a comparative analysis between
human performance and SSL models, beginning with a layer-wise analysis and an
exploration of parameter-efficient fine-tuning strategies in monolingual,
cross-lingual, and transfer learning contexts. We further compare the SER
ability of models and humans at both utterance- and segment-levels.
Additionally, we investigate the impact of dialect on cross-lingual SER through
human evaluation. Our findings reveal that models, with appropriate knowledge
transfer, can adapt to the target language and achieve performance comparable
to native speakers. We also demonstrate the significant effect of dialect on
SER for individuals without prior linguistic and paralinguistic background.
Moreover, both humans and models exhibit distinct behaviors across different
emotions. These results offer new insights into the cross-lingual SER
capabilities of SSL models, underscoring both their similarities to and
differences from human emotion perception.

</details>


### [328] [Semi-Supervised Cognitive State Classification from Speech with Multi-View Pseudo-Labeling](https://arxiv.org/abs/2409.16937)
*Yuanchao Li, Zixing Zhang, Jing Han, Peter Bell, Catherine Lai*

Main category: eess.AS

TL;DR: A semi-supervised learning framework using multi-view pseudo-labeling (acoustic and linguistic) for speech classification tasks, achieving competitive results with minimal labeled data.


<details>
  <summary>Details</summary>
Motivation: Addressing the challenge of limited labeled data in subjective speech classification tasks like cognitive state classification.

Method: Proposes a multi-view pseudo-labeling method combining acoustic (Frechet audio distance) and linguistic (LLM-revised transcriptions) features to select high-confidence data for training a bimodal classifier iteratively.

Result: Achieves competitive performance with only 30% labeled data, outperforming baselines in emotion recognition and dementia detection.

Conclusion: The framework effectively leverages unlabeled data, reducing reliance on labeled datasets while maintaining performance.

Abstract: The lack of labeled data is a common challenge in speech classification
tasks, particularly those requiring extensive subjective assessment, such as
cognitive state classification. In this work, we propose a Semi-Supervised
Learning (SSL) framework, introducing a novel multi-view pseudo-labeling method
that leverages both acoustic and linguistic characteristics to select the most
confident data for training the classification model. Acoustically, unlabeled
data are compared to labeled data using the Frechet audio distance, calculated
from embeddings generated by multiple audio encoders. Linguistically, large
language models are prompted to revise automatic speech recognition
transcriptions and predict labels based on our proposed task-specific
knowledge. High-confidence data are identified when pseudo-labels from both
sources align, while mismatches are treated as low-confidence data. A bimodal
classifier is then trained to iteratively label the low-confidence data until a
predefined criterion is met. We evaluate our SSL framework on emotion
recognition and dementia detection tasks. Experimental results demonstrate that
our method achieves competitive performance compared to fully supervised
learning using only 30% of the labeled data and significantly outperforms two
selected baselines.

</details>


### [329] [Exploring Acoustic Similarity in Emotional Speech and Music via Self-Supervised Representations](https://arxiv.org/abs/2409.17899)
*Yujia Sun, Zeyu Zhao, Korin Richmond, Yuanchao Li*

Main category: eess.AS

TL;DR: The paper explores acoustic similarities between speech and music for emotion recognition, analyzing SSL models' layerwise behavior and cross-domain adaptation. It finds shared features but notes emotion-specific variations and suggests parameter-efficient fine-tuning for improved performance.


<details>
  <summary>Details</summary>
Motivation: To investigate unexplored shared acoustic cues between speech and music for emotion recognition and leverage SSL models for cross-domain knowledge transfer.

Method: Analyzes SSL models' layerwise behavior for SER and MER, performs cross-domain adaptation via two-stage fine-tuning, and uses Frechet audio distance to study acoustic similarities and emotion bias.

Result: Speech and music SSL models capture shared acoustic features but vary by emotion due to training strategies. Cross-domain fine-tuning enhances SER and MER performance.

Conclusion: The study reveals acoustic similarities between emotional speech and music, offering insights for improving cross-domain emotion recognition systems.

Abstract: Emotion recognition from speech and music shares similarities due to their
acoustic overlap, which has led to interest in transferring knowledge between
these domains. However, the shared acoustic cues between speech and music,
particularly those encoded by Self-Supervised Learning (SSL) models, remain
largely unexplored, given the fact that SSL models for speech and music have
rarely been applied in cross-domain research. In this work, we revisit the
acoustic similarity between emotion speech and music, starting with an analysis
of the layerwise behavior of SSL models for Speech Emotion Recognition (SER)
and Music Emotion Recognition (MER). Furthermore, we perform cross-domain
adaptation by comparing several approaches in a two-stage fine-tuning process,
examining effective ways to utilize music for SER and speech for MER. Lastly,
we explore the acoustic similarities between emotional speech and music using
Frechet audio distance for individual emotions, uncovering the issue of emotion
bias in both speech and music SSL models. Our findings reveal that while speech
and music SSL models do capture shared acoustic features, their behaviors can
vary depending on different emotions due to their training strategies and
domain-specificities. Additionally, parameter-efficient fine-tuning can enhance
SER and MER performance by leveraging knowledge from each other. This study
provides new insights into the acoustic similarity between emotional speech and
music, and highlights the potential for cross-domain generalization to improve
SER and MER systems.

</details>


### [330] [FleSpeech: Flexibly Controllable Speech Generation with Various Prompts](https://arxiv.org/abs/2501.04644)
*Hanzhao Li, Yuke Li, Xinsheng Wang, Jingbin Hu, Qicong Xie, Shan Yang, Lei Xie*

Main category: eess.AS

TL;DR: FleSpeech is a multi-stage speech generation framework that integrates text, audio, and visual prompts for flexible and creative speech synthesis.


<details>
  <summary>Details</summary>
Motivation: Existing methods rely on fixed prompts, limiting adaptability for specific user needs like style adjustment or voice-character matching.

Method: FleSpeech uses a multimodal prompt encoder to unify text, audio, and visual prompts into a cohesive representation.

Result: Experiments show FleSpeech's effectiveness in flexible speech synthesis.

Conclusion: FleSpeech enhances adaptability and control in speech generation, supported by a multimodal dataset pipeline.

Abstract: Controllable speech generation methods typically rely on single or fixed
prompts, hindering creativity and flexibility. These limitations make it
difficult to meet specific user needs in certain scenarios, such as adjusting
the style while preserving a selected speaker's timbre, or choosing a style and
generating a voice that matches a character's visual appearance. To overcome
these challenges, we propose \textit{FleSpeech}, a novel multi-stage speech
generation framework that allows for more flexible manipulation of speech
attributes by integrating various forms of control. FleSpeech employs a
multimodal prompt encoder that processes and unifies different text, audio, and
visual prompts into a cohesive representation. This approach enhances the
adaptability of speech synthesis and supports creative and precise control over
the generated speech. Additionally, we develop a data collection pipeline for
multimodal datasets to facilitate further research and applications in this
field. Comprehensive subjective and objective experiments demonstrate the
effectiveness of FleSpeech. Audio samples are available at
https://kkksuper.github.io/FleSpeech/

</details>


### [331] [Multi-Task Corrupted Prediction for Learning Robust Audio-Visual Speech Representation](https://arxiv.org/abs/2504.18539)
*Sungnyun Kim, Sungwoo Cho, Sangmin Bae, Kangwook Jang, Se-Young Yun*

Main category: eess.AS

TL;DR: CAV2vec is a self-supervised framework for AVSR that handles joint audio-visual corruption via self-distillation and unimodal multi-task learning, improving robustness in noisy or corrupted environments.


<details>
  <summary>Details</summary>
Motivation: Addressing the lack of research on visual corruptions (e.g., lip occlusions) in AVSR, which degrade performance alongside audio disruptions.

Method: Proposes CAV2vec, using self-distillation and unimodal multi-task learning to align corrupted modalities by predicting clean targets from corrupted inputs.

Result: Significantly improves recognition accuracy in environments with various corruptions, as shown on robust AVSR benchmarks.

Conclusion: CAV2vec enhances AVSR robustness by mitigating corruption-induced representation dispersion, offering reliable audio-visual fusion.

Abstract: Audio-visual speech recognition (AVSR) incorporates auditory and visual
modalities to improve recognition accuracy, particularly in noisy environments
where audio-only speech systems are insufficient. While previous research has
largely addressed audio disruptions, few studies have dealt with visual
corruptions, e.g., lip occlusions or blurred videos, which are also
detrimental. To address this real-world challenge, we propose CAV2vec, a novel
self-supervised speech representation learning framework particularly designed
to handle audio-visual joint corruption. CAV2vec employs a self-distillation
approach with a corrupted prediction task, where the student model learns to
predict clean targets, generated by the teacher model, with corrupted input
frames. Specifically, we suggest a unimodal multi-task learning, which distills
cross-modal knowledge and aligns the corrupted modalities, by predicting clean
audio targets with corrupted videos, and clean video targets with corrupted
audios. This strategy mitigates the dispersion in the representation space
caused by corrupted modalities, leading to more reliable and robust
audio-visual fusion. Our experiments on robust AVSR benchmarks demonstrate that
the corrupted representation learning method significantly enhances recognition
accuracy across generalized environments involving various types of corruption.
Our code is available at https://github.com/sungnyun/cav2vec.

</details>


<div id='eess.IV'></div>

# eess.IV [[Back]](#toc)

### [332] [Light Weight CNN for classification of Brain Tumors from MRI Images](https://arxiv.org/abs/2504.21188)
*Natnael Alemayehu*

Main category: eess.IV

TL;DR: A lightweight CNN model for classifying brain tumors from MRI scans achieves 98.78% accuracy using preprocessing and hyperparameter tuning.


<details>
  <summary>Details</summary>
Motivation: To develop an efficient deep learning model for accurate multi-class brain tumor classification to aid clinical diagnosis.

Method: Utilizes preprocessing (normalization, augmentation, cropping), a CNN optimized with Keras Tuner, and 5-fold cross-validation.

Result: Achieves 98.78% classification accuracy on a dataset with four tumor classes.

Conclusion: The model is a low-complexity, effective tool for early brain tumor diagnosis.

Abstract: This study presents a convolutional neural network (CNN)-based approach for
the multi-class classification of brain tumors using magnetic resonance imaging
(MRI) scans. We utilize a publicly available dataset containing MRI images
categorized into four classes: glioma, meningioma, pituitary tumor, and no
tumor. Our primary objective is to build a light weight deep learning model
that can automatically classify brain tumor types with high accuracy. To
achieve this goal, we incorporate image preprocessing steps, including
normalization, data augmentation, and a cropping technique designed to reduce
background noise and emphasize relevant regions. The CNN architecture is
optimized through hyperparameter tuning using Keras Tuner, enabling systematic
exploration of network parameters. To ensure reliable evaluation, we apply
5-fold cross-validation, where each hyperparameter configuration is evaluated
across multiple data splits to mitigate overfitting. Experimental results
demonstrate that the proposed model achieves a classification accuracy of
98.78%, indicating its potential as a diagnostic aid in clinical settings. The
proposed method offers a low-complexity yet effective solution for assisting in
early brain tumor diagnosis.

</details>


### [333] [Gradient Attention Map Based Verification of Deep Convolutional Neural Networks with Application to X-ray Image Datasets](https://arxiv.org/abs/2504.21227)
*Omid Halimi Milani, Amanda Nikho, Lauren Mills, Marouane Tliba, Ahmet Enis Cetin, Mohammed H. Elnagar*

Main category: eess.IV

TL;DR: A framework for verifying deep learning models in medical imaging ensures reliability by analyzing attention patterns, feature maps, and rejecting out-of-distribution inputs.


<details>
  <summary>Details</summary>
Motivation: Deep learning models in medical imaging can produce unreliable predictions when applied to unfamiliar data, risking patient care.

Method: Proposes a verification framework using Gradient Attention Maps (GAM), similarity metrics, early convolutional feature analysis, and a garbage class for out-of-distribution rejection.

Result: The combined methods effectively identify unsuitable models and inputs, enhancing deployment safety.

Conclusion: The framework promotes safer and more reliable use of deep learning in medical imaging.

Abstract: Deep learning models have great potential in medical imaging, including
orthodontics and skeletal maturity assessment. However, applying a model to
data different from its training set can lead to unreliable predictions that
may impact patient care. To address this, we propose a comprehensive
verification framework that evaluates model suitability through multiple
complementary strategies. First, we introduce a Gradient Attention Map
(GAM)-based approach that analyzes attention patterns using Grad-CAM and
compares them via similarity metrics such as IoU, Dice Similarity, SSIM, Cosine
Similarity, Pearson Correlation, KL Divergence, and Wasserstein Distance.
Second, we extend verification to early convolutional feature maps, capturing
structural mis-alignments missed by attention alone. Finally, we incorporate an
additional garbage class into the classification model to explicitly reject
out-of-distribution inputs. Experimental results demonstrate that these
combined methods effectively identify unsuitable models and inputs, promoting
safer and more reliable deployment of deep learning in medical imaging.

</details>


### [334] [Emerging Advances in Learned Video Compression: Models, Systems and Beyond](https://arxiv.org/abs/2504.21445)
*Chuanmin Jia, Feng Ye, Siwei Ma, Wen Gao, Huifang Sun, Leonardo Chiariglione*

Main category: eess.IV

TL;DR: A survey on end-to-end optimized learned video compression (LVC), covering technical innovations, standardization, and performance results.


<details>
  <summary>Details</summary>
Motivation: To explore how AI technology enhances video compression through neural models, addressing gaps in traditional methods.

Method: Comprehensive review of LVC literature, focusing on uni- and bi-directional prediction models, optimization techniques, and system design challenges.

Result: LVC models demonstrate superior compression performance, highlighting their potential impact on visual intelligence.

Conclusion: Learned codecs and AI-based video technology are poised to significantly influence future visual intelligence research.

Abstract: Video compression is a fundamental topic in the visual intelligence, bridging
visual signal sensing/capturing and high-level visual analytics. The broad
success of artificial intelligence (AI) technology has enriched the horizon of
video compression into novel paradigms by leveraging end-to-end optimized
neural models. In this survey, we first provide a comprehensive and systematic
overview of recent literature on end-to-end optimized learned video coding,
covering the spectrum of pioneering efforts in both uni-directional and
bi-directional prediction based compression model designation. We further delve
into the optimization techniques employed in learned video compression (LVC),
emphasizing their technical innovations, advantages. Some standardization
progress is also reported. Furthermore, we investigate the system design and
hardware implementation challenges of the LVC inclusively. Finally, we present
the extensive simulation results to demonstrate the superior compression
performance of LVC models, addressing the question that why learned codecs and
AI-based video technology would have with broad impact on future visual
intelligence research.

</details>


### [335] [Make Both Ends Meet: A Synergistic Optimization Infrared Small Target Detection with Streamlined Computational Overhead](https://arxiv.org/abs/2504.21581)
*Yuxin Jing, Yuchen Zheng, Jufeng Zhao, Guangmang Cui, Yiming Zhu, Tianpei Zhang*

Main category: eess.IV

TL;DR: LE-IRSTD is a lightweight and efficient framework for infrared small target detection, improving accuracy and reducing computational overhead by optimizing YOLOv8n with MBConvblock, BSblock, AVCStem, and GSConv.


<details>
  <summary>Details</summary>
Motivation: Existing IRSTD methods face blurred target boundaries and high computational costs, prompting the need for a more efficient solution.

Method: The framework replaces YOLOv8n's C2f bottlenecks with MBConvblock and BSblock, introduces AVCStem with VKConv for adaptive kernels, and uses GSConv for feature shuffling.

Result: LE-IRSTD outperforms state-of-the-art methods in accuracy and lightweight performance.

Conclusion: The proposed LE-IRSTD effectively addresses computational inefficiency and boundary blurring in IRSTD, offering a robust and efficient solution.

Abstract: Infrared small target detection(IRSTD) is widely recognized as a challenging
task due to the inherent limitations of infrared imaging, including low
signal-to-noise ratios, lack of texture details, and complex background
interference. While most existing methods model IRSTD as a semantic
segmentation task, but they suffer from two critical drawbacks: (1)blurred
target boundaries caused by long-distance imaging dispersion; and (2) excessive
computational overhead due to indiscriminate feature stackin. To address these
issues, we propose the Lightweight Efficiency Infrared Small Target Detection
(LE-IRSTD), a lightweight and efficient framework based on YOLOv8n, with
following key innovations. Firstly, we identify that the multiple bottleneck
structures within the C2f component of the YOLOv8-n backbone contribute to an
increased computational burden. Therefore, we implement the Mobile Inverted
Bottleneck Convolution block (MBConvblock) and Bottleneck Structure block
(BSblock) in the backbone, effectively balancing the trade-off between
computational efficiency and the extraction of deep semantic information.
Secondly, we introduce the Attention-based Variable Convolution Stem (AVCStem)
structure, substituting the final convolution with Variable Kernel Convolution
(VKConv), which allows for adaptive convolutional kernels that can transform
into various shapes, facilitating the receptive field for the extraction of
targets. Finally, we employ Global Shuffle Convolution (GSConv) to shuffle the
channel dimension features obtained from different convolutional approaches,
thereby enhancing the robustness and generalization capabilities of our method.
Experimental results demonstrate that our LE-IRSTD method achieves compelling
results in both accuracy and lightweight performance, outperforming several
state-of-the-art deep learning methods.

</details>


### [336] [Selective Variable Convolution Meets Dynamic Content Guided Attention for Infrared Small Target Detection](https://arxiv.org/abs/2504.21612)
*Yirui Chen, Yiming Zhu, Yuxin Jing, Tianpei Zhang, Yuchen Zheng*

Main category: eess.IV

TL;DR: The paper proposes DCGANet, a dynamic content guided attention multiscale feature aggregation network, to improve infrared small target detection by addressing feature loss and enhancing discrimination between targets and backgrounds.


<details>
  <summary>Details</summary>
Motivation: Traditional CNNs struggle with feature extraction for small targets in infrared images, leading to critical feature loss. The paper aims to enhance detection accuracy by improving feature extraction and attention mechanisms.

Method: DCGANet includes a selective variable convolution (SVC) module for better receptive fields, a two-stage content guided attention module for focus refinement, and adaptive dynamic feature fusion (ADFF) for contextual integration.

Result: DCGANet achieves new benchmarks on multiple datasets, demonstrating superior detection accuracy and reduced false alarms.

Conclusion: The proposed DCGANet effectively addresses the limitations of CNNs in IRSTD, offering a robust solution for small target detection in complex backgrounds.

Abstract: Infrared Small Target Detection (IRSTD) system aims to identify small targets
in complex backgrounds. Due to the convolution operation in Convolutional
Neural Networks (CNNs), applying traditional CNNs to IRSTD presents challenges,
since the feature extraction of small targets is often insufficient, resulting
in the loss of critical features. To address these issues, we propose a dynamic
content guided attention multiscale feature aggregation network (DCGANet),
which adheres to the attention principle of 'coarse-to-fine' and achieves high
detection accuracy. First, we propose a selective variable convolution (SVC)
module that integrates the benefits of standard convolution, irregular
deformable convolution, and multi-rate dilated convolution. This module is
designed to expand the receptive field and enhance non-local features, thereby
effectively improving the discrimination of targets from backgrounds. Second,
the core component of DCGANet is a two-stage content guided attention module.
This module employs two-stage attention mechanism to initially direct the
network's focus to salient regions within the feature maps and subsequently
determine whether these regions correspond to targets or background
interference. By retaining the most significant responses, this mechanism
effectively suppresses false alarms. Additionally, we propose adaptive dynamic
feature fusion (ADFF) module to substitute for static feature cascading. This
dynamic feature fusion strategy enables DCGANet to adaptively integrate
contextual features, thereby enhancing its ability to discriminate true targets
from false alarms. DCGANet has achieved new benchmarks across multiple
datasets.

</details>


### [337] [Assimilation of SWOT Altimetry Data for Riverine Flood Reanalysis: From Synthetic to Real Data](https://arxiv.org/abs/2504.21670)
*Quentin Bonassies, Thanh Huy Nguyen, Ludovic Cassan, Andrea Piacentini, Sophie Ricci, Charlotte Emery, Christophe Fatras, Santiago Peña Luque, Raquel Rodriguez Suquet*

Main category: eess.IV

TL;DR: The study explores using SWOT mission data and in-situ measurements to improve flood reanalysis accuracy on the Garonne River, showing best results when combined.


<details>
  <summary>Details</summary>
Motivation: Floods are devastating, and improving flood modeling with remote sensing can reduce impacts and uncertainties.

Method: Used SWOT river node products and EnKF data assimilation, testing various strategies and revisit frequencies.

Result: Combining SWOT with in-situ data provided the most accurate flood dynamics representation.

Conclusion: Earth Observation data complements flood modeling, enhancing accuracy for future monitoring systems.

Abstract: Floods are one of the most common and devastating natural disasters
worldwide. The contribution of remote sensing is important for reducing the
impact of flooding both during the event itself and for improving hydrodynamic
models by reducing their associated uncertainties. This article presents the
innovative capabilities of the Surface Water and Ocean Topography (SWOT)
mission, especially its river node products, to enhance the accuracy of
riverine flood reanalysis, performed on a 50-km stretch of the Garonne River.
The experiments incorporate various data assimilation strategies, based on the
ensemble Kalman filter (EnKF), which allows for sequential updates of model
parameters based on available observations. The experimental results show that
while SWOT data alone offers some improvements, combining it with in-situ water
level measurements provides the most accurate representation of flood dynamics,
both at gauge stations and along the river. The study also investigates the
impact of different SWOT revisit frequencies on the models performance,
revealing that assimilating more frequent SWOT observations leads to more
reliable flood reanalyses. In the real event, it was demonstrated that the
assimilation of SWOT and in-situ data accurately reproduces the water level
dynamics, offering promising prospects for future flood monitoring systems.
Overall, this study emphasizes the complementary strengths of Earth Observation
data in improving the representation of the flood dynamics in the riverbed and
the floodplains.

</details>


### [338] [LoC-LIC: Low Complexity Learned Image Coding Using Hierarchical Feature Transforms](https://arxiv.org/abs/2504.21778)
*Ayman A. Ameen, Thomas Richter, André Kaup*

Main category: eess.IV

TL;DR: Proposed a low-complexity learned image compression model using hierarchical feature extraction, reducing computational load while maintaining efficiency.


<details>
  <summary>Details</summary>
Motivation: High complexity in current learned image compression models demands excessive computational resources.

Method: Hierarchical feature extraction transforms with fewer channels for high-resolution inputs and reduced spatial dimensions for feature maps with many channels.

Result: Reduced forward pass complexity from 1256 kMAC/Pixel to 270 kMAC/Pixel without performance loss.

Conclusion: The model enables efficient operation on various devices and inspires new architectures in image compression.

Abstract: Current learned image compression models typically exhibit high complexity,
which demands significant computational resources. To overcome these
challenges, we propose an innovative approach that employs hierarchical feature
extraction transforms to significantly reduce complexity while preserving bit
rate reduction efficiency. Our novel architecture achieves this by using fewer
channels for high spatial resolution inputs/feature maps. On the other hand,
feature maps with a large number of channels have reduced spatial dimensions,
thereby cutting down on computational load without sacrificing performance.
This strategy effectively reduces the forward pass complexity from \(1256 \,
\text{kMAC/Pixel}\) to just \(270 \, \text{kMAC/Pixel}\). As a result, the
reduced complexity model can open the way for learned image compression models
to operate efficiently across various devices and pave the way for the
development of new architectures in image compression technology.

</details>


### [339] [CAD-Unet: A Capsule Network-Enhanced Unet Architecture for Accurate Segmentation of COVID-19 Lung Infections from CT Images](https://arxiv.org/abs/2412.06314)
*Yijie Dang, Weijun Ma, Xiaohu Luo, Huaizhu Wang*

Main category: eess.IV

TL;DR: The paper introduces CAD-Unet, a deep learning model combining capsule networks with Unet for segmenting COVID-19 lung infections in CT images, achieving superior performance.


<details>
  <summary>Details</summary>
Motivation: The challenge of segmenting COVID-19 lung infections due to indistinct boundaries and low contrast in CT images motivates the development of a more effective model.

Method: CAD-Unet integrates capsule networks into Unet, using vectors for spatial information transfer and designing a capsule encoder path for efficient fusion.

Result: Extensive experiments on four datasets show CAD-Unet outperforms existing methods in segmentation tasks.

Conclusion: CAD-Unet effectively addresses segmentation challenges in COVID-19 lung infections, demonstrating its potential for clinical use.

Abstract: Since the outbreak of the COVID-19 pandemic in 2019, medical imaging has
emerged as a primary modality for diagnosing COVID-19 pneumonia. In clinical
settings, the segmentation of lung infections from computed tomography images
enables rapid and accurate quantification and diagnosis of COVID-19.
Segmentation of COVID-19 infections in the lungs poses a formidable challenge,
primarily due to the indistinct boundaries and limited contrast presented by
ground glass opacity manifestations. Moreover, the confounding similarity
between infiltrates, lung tissues, and lung walls further complicates this
segmentation task. To address these challenges, this paper introduces a novel
deep network architecture, called CAD-Unet, for segmenting COVID-19 lung
infections. In this architecture, capsule networks are incorporated into the
existing Unet framework. Capsule networks represent a novel network
architecture that differs from traditional convolutional neural networks. They
utilize vectors for information transfer among capsules, facilitating the
extraction of intricate lesion spatial information. Additionally, we design a
capsule encoder path and establish a coupling path between the unet encoder and
the capsule encoder. This design maximizes the complementary advantages of both
network structures while achieving efficient information fusion. \noindent
Finally, extensive experiments are conducted on four publicly available
datasets, encompassing binary segmentation tasks and multi-class segmentation
tasks. The experimental results demonstrate the superior segmentation
performance of the proposed model. The code has been released at:
https://github.com/AmanoTooko-jie/CAD-Unet.

</details>


### [340] [ScaleFusionNet: Transformer-Guided Multi-Scale Feature Fusion for Skin Lesion Segmentation](https://arxiv.org/abs/2503.03327)
*Saqib Qamar, Syed Furqan Qadri, Roobaea Alroobaea, Goram Mufarah M Alshmrani, Richard Jiang*

Main category: eess.IV

TL;DR: ScaleFusionNet, a segmentation model with Cross-Attention Transformer Module and AdaptiveFusionBlock, achieves high Dice scores for melanoma segmentation.


<details>
  <summary>Details</summary>
Motivation: Accurate and efficient segmentation of skin lesions is challenging but essential for medical analysis.

Method: Proposes ScaleFusionNet with CATM and AdaptiveFusionBlock, using Swin Transformer Blocks and adaptive multi-scale fusion.

Result: Achieves Dice scores of 92.94% (ISIC-2016) and 91.65% (ISIC-2018).

Conclusion: ScaleFusionNet effectively improves segmentation accuracy and feature fusion for skin lesion analysis.

Abstract: Melanoma is a malignant tumor originating from skin cell lesions. Accurate
and efficient segmentation of skin lesions is essential for quantitative
medical analysis but remains challenging. To address this, we propose
ScaleFusionNet, a segmentation model that integrates Cross-Attention
Transformer Module (CATM) and AdaptiveFusionBlock to enhance feature extraction
and fusion. The model employs a hybrid architecture encoder that effectively
captures both local and global features. We introduce CATM, which utilizes Swin
Transformer Blocks and Cross Attention Fusion (CAF) to adaptively refine
encoder-decoder feature fusion, reducing semantic gaps and improving
segmentation accuracy. Additionally, the AdaptiveFusionBlock is improved by
integrating adaptive multi-scale fusion, where Swin Transformer-based attention
complements deformable convolution-based multi-scale feature extraction. This
enhancement refines lesion boundaries and preserves fine-grained details.
ScaleFusionNet achieves Dice scores of 92.94% and 91.65% on ISIC-2016 and
ISIC-2018 datasets, respectively, demonstrating its effectiveness in skin
lesion analysis. Our code implementation is publicly available at GitHub.

</details>
