{"id": "2504.16188", "pdf": "https://arxiv.org/pdf/2504.16188", "abs": "https://arxiv.org/abs/2504.16188", "authors": ["Jabez Magomere", "Elena Kochkina", "Samuel Mensah", "Simerjot Kaur", "Charese H. Smiley"], "title": "FinNLI: Novel Dataset for Multi-Genre Financial Natural Language Inference Benchmarking", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": null, "summary": "We introduce FinNLI, a benchmark dataset for Financial Natural Language\nInference (FinNLI) across diverse financial texts like SEC Filings, Annual\nReports, and Earnings Call transcripts. Our dataset framework ensures diverse\npremise-hypothesis pairs while minimizing spurious correlations. FinNLI\ncomprises 21,304 pairs, including a high-quality test set of 3,304 instances\nannotated by finance experts. Evaluations show that domain shift significantly\ndegrades general-domain NLI performance. The highest Macro F1 scores for\npre-trained (PLMs) and large language models (LLMs) baselines are 74.57% and\n78.62%, respectively, highlighting the dataset's difficulty. Surprisingly,\ninstruction-tuned financial LLMs perform poorly, suggesting limited\ngeneralizability. FinNLI exposes weaknesses in current LLMs for financial\nreasoning, indicating room for improvement."}
{"id": "2504.16271", "pdf": "https://arxiv.org/pdf/2504.16271", "abs": "https://arxiv.org/abs/2504.16271", "authors": ["Frederik Bredgaard", "Martin Lund Trinhammer", "Elisa Bassignana"], "title": "The Language of Attachment: Modeling Attachment Dynamics in Psychotherapy", "categories": ["cs.CL"], "comment": null, "summary": "The delivery of mental healthcare through psychotherapy stands to benefit\nimmensely from developments within Natural Language Processing (NLP), in\nparticular through the automatic identification of patient specific qualities,\nsuch as attachment style. Currently, the assessment of attachment style is\nperformed manually using the Patient Attachment Coding System (PACS; Talia et\nal., 2017), which is complex, resource-consuming and requires extensive\ntraining. To enable wide and scalable adoption of attachment informed treatment\nand research, we propose the first exploratory analysis into automatically\nassessing patient attachment style from psychotherapy transcripts using NLP\nclassification models. We further analyze the results and discuss the\nimplications of using automated tools for this purpose -- e.g., confusing\n`preoccupied' patients with `avoidant' likely has a more negative impact on\ntherapy outcomes with respect to other mislabeling. Our work opens an avenue of\nresearch enabling more personalized psychotherapy and more targeted research\ninto the mechanisms of psychotherapy through advancements in NLP."}
{"id": "2504.16286", "pdf": "https://arxiv.org/pdf/2504.16286", "abs": "https://arxiv.org/abs/2504.16286", "authors": ["Li Weigang", "Pedro Carvalho Brom"], "title": "The Paradox of Poetic Intent in Back-Translation: Evaluating the Quality of Large Language Models in Chinese Translation", "categories": ["cs.CL", "68T50", "I.2.7"], "comment": "24 pages, 3 figures", "summary": "The rapid advancement of large language models (LLMs) has reshaped the\nlandscape of machine translation, yet challenges persist in preserving poetic\nintent, cultural heritage, and handling specialized terminology in\nChinese-English translation. This study constructs a diverse corpus\nencompassing Chinese scientific terminology, historical translation paradoxes,\nand literary metaphors. Utilizing a back-translation and Friedman test-based\nevaluation system (BT-Fried), we evaluate BLEU, CHRF, TER, and semantic\nsimilarity metrics across six major LLMs (e.g., GPT-4.5, DeepSeek V3) and three\ntraditional translation tools. Key findings include: (1) Scientific abstracts\noften benefit from back-translation, while traditional tools outperform LLMs in\nlinguistically distinct texts; (2) LLMs struggle with cultural and literary\nretention, exemplifying the \"paradox of poetic intent\"; (3) Some models exhibit\n\"verbatim back-translation\", reflecting emergent memory behavior; (4) A novel\nBLEU variant using Jieba segmentation and n-gram weighting is proposed. The\nstudy contributes to the empirical evaluation of Chinese NLP performance and\nadvances understanding of cultural fidelity in AI-mediated translation."}
{"id": "2504.16312", "pdf": "https://arxiv.org/pdf/2504.16312", "abs": "https://arxiv.org/abs/2504.16312", "authors": ["Zhangdie Yuan", "Andreas Vlachos"], "title": "Capturing Symmetry and Antisymmetry in Language Models through Symmetry-Aware Training Objectives", "categories": ["cs.CL"], "comment": null, "summary": "Capturing symmetric (e.g., country borders another country) and antisymmetric\n(e.g., parent_of) relations is crucial for a variety of applications. This\npaper tackles this challenge by introducing a novel Wikidata-derived natural\nlanguage inference dataset designed to evaluate large language models (LLMs).\nOur findings reveal that LLMs perform comparably to random chance on this\nbenchmark, highlighting a gap in relational understanding. To address this, we\nexplore encoder retraining via contrastive learning with k-nearest neighbors.\nThe retrained encoder matches the performance of fine-tuned classification\nheads while offering additional benefits, including greater efficiency in\nfew-shot learning and improved mitigation of catastrophic forgetting."}
{"id": "2504.16223", "pdf": "https://arxiv.org/pdf/2504.16223", "abs": "https://arxiv.org/abs/2504.16223", "authors": ["Jürgen Herre", "Schuyler Quackenbush", "Minje Kim", "Jan Skoglund"], "title": "Perceptual Audio Coding: A 40-Year Historical Perspective", "categories": ["eess.AS"], "comment": null, "summary": "In the history of audio and acoustic signal processing, perceptual audio\ncoding has certainly excelled as a bright success story by its ubiquitous\ndeployment in virtually all digital media devices, such as computers, tablets,\nmobile phones, set-top-boxes, and digital radios. From a technology\nperspective, perceptual audio coding has undergone tremendous development from\nthe first very basic perceptually driven coders (including the popular mp3\nformat) to today's full-blown integrated coding/rendering systems. This paper\nprovides a historical overview of this research journey by pinpointing the\npivotal development steps in the evolution of perceptual audio coding. Finally,\nit provides thoughts about future directions in this area."}
{"id": "2504.16129", "pdf": "https://arxiv.org/pdf/2504.16129", "abs": "https://arxiv.org/abs/2504.16129", "authors": ["Junwei Liao", "Muning Wen", "Jun Wang", "Weinan Zhang"], "title": "MARFT: Multi-Agent Reinforcement Fine-Tuning", "categories": ["cs.MA", "cs.AI", "cs.LG", "cs.RO"], "comment": "36 pages", "summary": "LLM-based Multi-Agent Systems have demonstrated remarkable capabilities in\naddressing complex, agentic tasks requiring multifaceted reasoning and\ncollaboration, from generating high-quality presentation slides to conducting\nsophisticated scientific research. Meanwhile, RL has been widely recognized for\nits effectiveness in enhancing agent intelligence, but limited research has\ninvestigated the fine-tuning of LaMAS using foundational RL techniques.\nMoreover, the direct application of MARL methodologies to LaMAS introduces\nsignificant challenges, stemming from the unique characteristics and mechanisms\ninherent to LaMAS. To address these challenges, this article presents a\ncomprehensive study of LLM-based MARL and proposes a novel paradigm termed\nMulti-Agent Reinforcement Fine-Tuning (MARFT). We introduce a universal\nalgorithmic framework tailored for LaMAS, outlining the conceptual foundations,\nkey distinctions, and practical implementation strategies. We begin by\nreviewing the evolution from RL to Reinforcement Fine-Tuning, setting the stage\nfor a parallel analysis in the multi-agent domain. In the context of LaMAS, we\nelucidate critical differences between MARL and MARFT. These differences\nmotivate a transition toward a novel, LaMAS-oriented formulation of RFT.\nCentral to this work is the presentation of a robust and scalable MARFT\nframework. We detail the core algorithm and provide a complete, open-source\nimplementation to facilitate adoption and further research. The latter sections\nof the paper explore real-world application perspectives and opening challenges\nin MARFT. By bridging theoretical underpinnings with practical methodologies,\nthis work aims to serve as a roadmap for researchers seeking to advance MARFT\ntoward resilient and adaptive solutions in agentic systems. Our implementation\nof the proposed framework is publicly available at:\nhttps://github.com/jwliao-ai/MARFT."}
{"id": "2504.16213", "pdf": "https://arxiv.org/pdf/2504.16213", "abs": "https://arxiv.org/abs/2504.16213", "authors": ["Andrew Barovic", "Armin Moin"], "title": "TinyML for Speech Recognition", "categories": ["cs.SD", "cs.AI", "eess.AS"], "comment": null, "summary": "We train and deploy a quantized 1D convolutional neural network model to\nconduct speech recognition on a highly resource-constrained IoT edge device.\nThis can be useful in various Internet of Things (IoT) applications, such as\nsmart homes and ambient assisted living for the elderly and people with\ndisabilities, just to name a few examples. In this paper, we first create a new\ndataset with over one hour of audio data that enables our research and will be\nuseful to future studies in this field. Second, we utilize the technologies\nprovided by Edge Impulse to enhance our model's performance and achieve a high\nAccuracy of up to 97% on our dataset. For the validation, we implement our\nprototype using the Arduino Nano 33 BLE Sense microcontroller board. This\nmicrocontroller board is specifically designed for IoT and AI applications,\nmaking it an ideal choice for our target use case scenarios. While most\nexisting research focuses on a limited set of keywords, our model can process\n23 different keywords, enabling complex commands."}
{"id": "2504.16405", "pdf": "https://arxiv.org/pdf/2504.16405", "abs": "https://arxiv.org/abs/2504.16405", "authors": ["Lancheng Gao", "Ziheng Jia", "Yunhao Zeng", "Wei Sun", "Yiming Zhang", "Wei Zhou", "Guangtao Zhai", "Xiongkuo Min"], "title": "EEmo-Bench: A Benchmark for Multi-modal Large Language Models on Image Evoked Emotion Assessment", "categories": ["cs.MM"], "comment": null, "summary": "The furnishing of multi-modal large language models (MLLMs) has led to the\nemergence of numerous benchmark studies, particularly those evaluating their\nperception and understanding capabilities.\n  Among these, understanding image-evoked emotions aims to enhance MLLMs'\nempathy, with significant applications such as human-machine interaction and\nadvertising recommendations. However, current evaluations of this MLLM\ncapability remain coarse-grained, and a systematic and comprehensive assessment\nis still lacking.\n  To this end, we introduce EEmo-Bench, a novel benchmark dedicated to the\nanalysis of the evoked emotions in images across diverse content categories.\n  Our core contributions include:\n  1) Regarding the diversity of the evoked emotions, we adopt an emotion\nranking strategy and employ the Valence-Arousal-Dominance (VAD) as emotional\nattributes for emotional assessment. In line with this methodology, 1,960\nimages are collected and manually annotated.\n  2) We design four tasks to evaluate MLLMs' ability to capture the evoked\nemotions by single images and their associated attributes: Perception, Ranking,\nDescription, and Assessment. Additionally, image-pairwise analysis is\nintroduced to investigate the model's proficiency in performing joint and\ncomparative analysis.\n  In total, we collect 6,773 question-answer pairs and perform a thorough\nassessment on 19 commonly-used MLLMs.\n  The results indicate that while some proprietary and large-scale open-source\nMLLMs achieve promising overall performance, the analytical capabilities in\ncertain evaluation dimensions remain suboptimal.\n  Our EEmo-Bench paves the path for further research aimed at enhancing the\ncomprehensive perceiving and understanding capabilities of MLLMs concerning\nimage-evoked emotions, which is crucial for machine-centric emotion perception\nand understanding."}
{"id": "2504.16237", "pdf": "https://arxiv.org/pdf/2504.16237", "abs": "https://arxiv.org/abs/2504.16237", "authors": ["Obed Korshie Dzikunu", "Amirhossein Toosi", "Shadab Ahamed", "Sara Harsini", "Francois Benard", "Xiaoxiao Li", "Arman Rahmim"], "title": "Comprehensive Evaluation of Quantitative Measurements from Automated Deep Segmentations of PSMA PET/CT Images", "categories": ["eess.IV", "cs.CV", "cs.LG"], "comment": "12 pages, 8 figures", "summary": "This study performs a comprehensive evaluation of quantitative measurements\nas extracted from automated deep-learning-based segmentation methods, beyond\ntraditional Dice Similarity Coefficient assessments, focusing on six\nquantitative metrics, namely SUVmax, SUVmean, total lesion activity (TLA),\ntumor volume (TMTV), lesion count, and lesion spread. We analyzed 380\nprostate-specific membrane antigen (PSMA) targeted [18F]DCFPyL PET/CT scans of\npatients with biochemical recurrence of prostate cancer, training deep neural\nnetworks, U-Net, Attention U-Net and SegResNet with four loss functions: Dice\nLoss, Dice Cross Entropy, Dice Focal Loss, and our proposed L1 weighted Dice\nFocal Loss (L1DFL). Evaluations indicated that Attention U-Net paired with\nL1DFL achieved the strongest correlation with the ground truth (concordance\ncorrelation = 0.90-0.99 for SUVmax and TLA), whereas models employing the Dice\nLoss and the other two compound losses, particularly with SegResNet,\nunderperformed. Equivalence testing (TOST, alpha = 0.05, Delta = 20%) confirmed\nhigh performance for SUV metrics, lesion count and TLA, with L1DFL yielding the\nbest performance. By contrast, tumor volume and lesion spread exhibited greater\nvariability. Bland-Altman, Coverage Probability, and Total Deviation Index\nanalyses further highlighted that our proposed L1DFL minimizes variability in\nquantification of the ground truth clinical measures. The code is publicly\navailable at: https://github.com/ObedDzik/pca\\_segment.git."}
{"id": "2504.16102", "pdf": "https://arxiv.org/pdf/2504.16102", "abs": "https://arxiv.org/abs/2504.16102", "authors": ["Xiwen Li", "Ross Whitaker", "Tolga Tasdizen"], "title": "Audio and Multiscale Visual Cues Driven Cross-modal Transformer for Idling Vehicle Detection", "categories": ["cs.CV", "cs.RO"], "comment": null, "summary": "Idling vehicle detection (IVD) supports real-time systems that reduce\npollution and emissions by dynamically messaging drivers to curb excess idling\nbehavior. In computer vision, IVD has become an emerging task that leverages\nvideo from surveillance cameras and audio from remote microphones to localize\nand classify vehicles in each frame as moving, idling, or engine-off. As with\nother cross-modal tasks, the key challenge lies in modeling the correspondence\nbetween audio and visual modalities, which differ in representation but provide\ncomplementary cues -- video offers spatial and motion context, while audio\nconveys engine activity beyond the visual field. The previous end-to-end model,\nwhich uses a basic attention mechanism, struggles to align these modalities\neffectively, often missing vehicle detections. To address this issue, we\npropose AVIVDNetv2, a transformer-based end-to-end detection network. It\nincorporates a cross-modal transformer with global patch-level learning, a\nmultiscale visual feature fusion module, and decoupled detection heads.\nExtensive experiments show that AVIVDNetv2 improves mAP by 7.66 over the\ndisjoint baseline and 9.42 over the E2E baseline, with consistent AP gains\nacross all vehicle categories. Furthermore, AVIVDNetv2 outperforms the\nstate-of-the-art method for sounding object localization, establishing a new\nperformance benchmark on the AVIVD dataset."}
{"id": "2504.16109", "pdf": "https://arxiv.org/pdf/2504.16109", "abs": "https://arxiv.org/abs/2504.16109", "authors": ["Jun-Peng Jiang", "Si-Yang Liu", "Hao-Run Cai", "Qile Zhou", "Han-Jia Ye"], "title": "Representation Learning for Tabular Data: A Comprehensive Survey", "categories": ["cs.LG"], "comment": null, "summary": "Tabular data, structured as rows and columns, is among the most prevalent\ndata types in machine learning classification and regression applications.\nModels for learning from tabular data have continuously evolved, with Deep\nNeural Networks (DNNs) recently demonstrating promising results through their\ncapability of representation learning. In this survey, we systematically\nintroduce the field of tabular representation learning, covering the\nbackground, challenges, and benchmarks, along with the pros and cons of using\nDNNs. We organize existing methods into three main categories according to\ntheir generalization capabilities: specialized, transferable, and general\nmodels. Specialized models focus on tasks where training and evaluation occur\nwithin the same data distribution. We introduce a hierarchical taxonomy for\nspecialized models based on the key aspects of tabular data -- features,\nsamples, and objectives -- and delve into detailed strategies for obtaining\nhigh-quality feature- and sample-level representations. Transferable models are\npre-trained on one or more datasets and subsequently fine-tuned on downstream\ntasks, leveraging knowledge acquired from homogeneous or heterogeneous sources,\nor even cross-modalities such as vision and language. General models, also\nknown as tabular foundation models, extend this concept further, allowing\ndirect application to downstream tasks without fine-tuning. We group these\ngeneral models based on the strategies used to adapt across heterogeneous\ndatasets. Additionally, we explore ensemble methods, which integrate the\nstrengths of multiple tabular models. Finally, we discuss representative\nextensions of tabular learning, including open-environment tabular machine\nlearning, multimodal learning with tabular data, and tabular understanding.\nMore information can be found in the following repository:\nhttps://github.com/LAMDA-Tabular/Tabular-Survey."}
{"id": "2504.16115", "pdf": "https://arxiv.org/pdf/2504.16115", "abs": "https://arxiv.org/abs/2504.16115", "authors": ["Yibo Jacky Zhang", "Sanmi Koyejo"], "title": "A Framework for Objective-Driven Dynamical Stochastic Fields", "categories": ["cs.AI", "cs.LG", "cs.MA", "nlin.AO"], "comment": null, "summary": "Fields offer a versatile approach for describing complex systems composed of\ninteracting and dynamic components. In particular, some of these dynamical and\nstochastic systems may exhibit goal-directed behaviors aimed at achieving\nspecific objectives, which we refer to as $\\textit{intelligent fields}$.\nHowever, due to their inherent complexity, it remains challenging to develop a\nformal theoretical description of such systems and to effectively translate\nthese descriptions into practical applications. In this paper, we propose three\nfundamental principles -- complete configuration, locality, and purposefulness\n-- to establish a theoretical framework for understanding intelligent fields.\nMoreover, we explore methodologies for designing such fields from the\nperspective of artificial intelligence applications. This initial investigation\naims to lay the groundwork for future theoretical developments and practical\nadvances in understanding and harnessing the potential of such objective-driven\ndynamical stochastic fields."}
{"id": "2504.16353", "pdf": "https://arxiv.org/pdf/2504.16353", "abs": "https://arxiv.org/abs/2504.16353", "authors": ["Arpana Hosabettu", "Harsh Shah"], "title": "Transformer-Based Extraction of Statutory Definitions from the U.S. Code", "categories": ["cs.CL", "cs.AI"], "comment": "7 pages, to be published in IEEE AIIoT 2025", "summary": "Automatic extraction of definitions from legal texts is critical for\nenhancing the comprehension and clarity of complex legal corpora such as the\nUnited States Code (U.S.C.). We present an advanced NLP system leveraging\ntransformer-based architectures to automatically extract defined terms, their\ndefinitions, and their scope from the U.S.C. We address the challenges of\nautomatically identifying legal definitions, extracting defined terms, and\ndetermining their scope within this complex corpus of over 200,000 pages of\nfederal statutory law. Building upon previous feature-based machine learning\nmethods, our updated model employs domain-specific transformers (Legal-BERT)\nfine-tuned specifically for statutory texts, significantly improving extraction\naccuracy. Our work implements a multi-stage pipeline that combines document\nstructure analysis with state-of-the-art language models to process legal text\nfrom the XML version of the U.S. Code. Each paragraph is first classified using\na fine-tuned legal domain BERT model to determine if it contains a definition.\nOur system then aggregates related paragraphs into coherent definitional units\nand applies a combination of attention mechanisms and rule-based patterns to\nextract defined terms and their jurisdictional scope. The definition extraction\nsystem is evaluated on multiple titles of the U.S. Code containing thousands of\ndefinitions, demonstrating significant improvements over previous approaches.\nOur best model achieves 96.8% precision and 98.9% recall (98.2% F1-score),\nsubstantially outperforming traditional machine learning classifiers. This work\ncontributes to improving accessibility and understanding of legal information\nwhile establishing a foundation for downstream legal reasoning tasks."}
{"id": "2504.16289", "pdf": "https://arxiv.org/pdf/2504.16289", "abs": "https://arxiv.org/abs/2504.16289", "authors": ["Toon van Waterschoot"], "title": "Deep, data-driven modeling of room acoustics: literature review and research perspectives", "categories": ["eess.AS", "cs.SD"], "comment": null, "summary": "Our everyday auditory experience is shaped by the acoustics of the indoor\nenvironments in which we live. Room acoustics modeling is aimed at establishing\nmathematical representations of acoustic wave propagation in such environments.\nThese representations are relevant to a variety of problems ranging from\necho-aided auditory indoor navigation to restoring speech understanding in\ncocktail party scenarios. Many disciplines in science and engineering have\nrecently witnessed a paradigm shift powered by deep learning (DL), and room\nacoustics research is no exception. The majority of deep, data-driven room\nacoustics models are inspired by DL-based speech and image processing, and\nhence lack the intrinsic space-time structure of acoustic wave propagation.\nMore recently, DL-based models for room acoustics that include either geometric\nor wave-based information have delivered promising results, primarily for the\nproblem of sound field reconstruction. In this review paper, we will provide an\nextensive and structured literature review on deep, data-driven modeling in\nroom acoustics. Moreover, we position these models in a framework that allows\nfor a conceptual comparison with traditional physical and data-driven models.\nFinally, we identify strengths and shortcomings of deep, data-driven room\nacoustics models and outline the main challenges for further research."}
{"id": "2409.12601", "pdf": "https://arxiv.org/pdf/2409.12601", "abs": "https://arxiv.org/abs/2409.12601", "authors": ["Luca Ballotta", "Áron Vékássy", "Stephanie Gil", "Michal Yemini"], "title": "Friedkin-Johnsen Model with Diminishing Competition", "categories": ["eess.SY", "cs.MA", "cs.SY"], "comment": "Copyright (c) 2024 IEEE. Personal use of this material is permitted.\n  However, permission to use this material for any other purposes must be\n  obtained from the IEEE by sending a request to pubs-permissions@ieee.org.\n  Fixed Assumption 1", "summary": "This letter studies the Friedkin-Johnsen (FJ) model with diminishing\ncompetition, or stubbornness. The original FJ model assumes that each agent\nassigns a constant competition weight to its initial opinion. In contrast, we\ninvestigate the effect of diminishing competition on the convergence point and\nspeed of the FJ dynamics. We prove that, if the competition is uniform across\nagents and vanishes asymptotically, the convergence point coincides with the\nnominal consensus reached with no competition. However, the diminishing\ncompetition slows down convergence according to its own rate of decay. We study\nthis phenomenon analytically and provide upper and lower bounds on the\nconvergence rate. Further, if competition is not uniform across agents, we show\nthat the convergence point may not coincide with the nominal consensus point.\nFinally, we evaluate our analytical insights numerically."}
{"id": "2504.16839", "pdf": "https://arxiv.org/pdf/2504.16839", "abs": "https://arxiv.org/abs/2504.16839", "authors": ["Nicolas Jonason", "Luca Casini", "Bob L. T. Sturm"], "title": "SMART: Tuning a symbolic music generation system with an audio domain aesthetic reward", "categories": ["cs.SD"], "comment": null, "summary": "Recent work has proposed training machine learning models to predict\naesthetic ratings for music audio. Our work explores whether such models can be\nused to finetune a symbolic music generation system with reinforcement\nlearning, and what effect this has on the system outputs. To test this, we use\ngroup relative policy optimization to finetune a piano MIDI model with Meta\nAudiobox Aesthetics ratings of audio-rendered outputs as the reward. We find\nthat this optimization has effects on multiple low-level features of the\ngenerated outputs, and improves the average subjective ratings in a preliminary\nlistening study with $14$ participants. We also find that over-optimization\ndramatically reduces diversity of model outputs."}
{"id": "2504.16586", "pdf": "https://arxiv.org/pdf/2504.16586", "abs": "https://arxiv.org/abs/2504.16586", "authors": ["Haotian Zhang", "Yuqi Li", "Li Li", "Dong Liu"], "title": "Learning Switchable Priors for Neural Image Compression", "categories": ["cs.MM"], "comment": "18 pages, 15 figures", "summary": "Neural image compression (NIC) usually adopts a predefined family of\nprobabilistic distributions as the prior of the latent variables, and meanwhile\nrelies on entropy models to estimate the parameters for the probabilistic\nfamily. More complex probabilistic distributions may fit the latent variables\nmore accurately, but also incur higher complexity of the entropy models,\nlimiting their practical value. To address this dilemma, we propose a solution\nto decouple the entropy model complexity from the prior distributions. We use a\nfinite set of trainable priors that correspond to samples of the parametric\nprobabilistic distributions. We train the entropy model to predict the index of\nthe appropriate prior within the set, rather than the specific parameters.\nSwitching between the trained priors further enables us to embrace a skip mode\ninto the prior set, which simply omits a latent variable during the entropy\ncoding. To demonstrate the practical value of our solution, we present a\nlightweight NIC model, namely FastNIC, together with the learning of switchable\npriors. FastNIC obtains a better trade-off between compression efficiency and\ncomputational complexity for neural image compression. We also implanted the\nswitchable priors into state-of-the-art NIC models and observed improved\ncompression efficiency with a significant reduction of entropy coding\ncomplexity."}
{"id": "2504.16745", "pdf": "https://arxiv.org/pdf/2504.16745", "abs": "https://arxiv.org/abs/2504.16745", "authors": ["Jialiang Zhang", "Feng Gao", "Yanhai Gan", "Junyu Dong", "Qian Du"], "title": "Frequency-Compensated Network for Daily Arctic Sea Ice Concentration Prediction", "categories": ["eess.IV", "cs.CV"], "comment": "Accepted by IEEE TGRS 2025", "summary": "Accurately forecasting sea ice concentration (SIC) in the Arctic is critical\nto global ecosystem health and navigation safety. However, current methods\nstill is confronted with two challenges: 1) these methods rarely explore the\nlong-term feature dependencies in the frequency domain. 2) they can hardly\npreserve the high-frequency details, and the changes in the marginal area of\nthe sea ice cannot be accurately captured. To this end, we present a\nFrequency-Compensated Network (FCNet) for Arctic SIC prediction on a daily\nbasis. In particular, we design a dual-branch network, including branches for\nfrequency feature extraction and convolutional feature extraction. For\nfrequency feature extraction, we design an adaptive frequency filter block,\nwhich integrates trainable layers with Fourier-based filters. By adding\nfrequency features, the FCNet can achieve refined prediction of edges and\ndetails. For convolutional feature extraction, we propose a high-frequency\nenhancement block to separate high and low-frequency information. Moreover,\nhigh-frequency features are enhanced via channel-wise attention, and temporal\nattention unit is employed for low-frequency feature extraction to capture\nlong-range sea ice changes. Extensive experiments are conducted on a\nsatellite-derived daily SIC dataset, and the results verify the effectiveness\nof the proposed FCNet. Our codes and data will be made public available at:\nhttps://github.com/oucailab/FCNet ."}
{"id": "2504.16103", "pdf": "https://arxiv.org/pdf/2504.16103", "abs": "https://arxiv.org/abs/2504.16103", "authors": ["Oussema Dhaouadi", "Johannes Meier", "Jacques Kaiser", "Daniel Cremers"], "title": "Shape Your Ground: Refining Road Surfaces Beyond Planar Representations", "categories": ["cs.CV", "cs.RO"], "comment": null, "summary": "Road surface reconstruction from aerial images is fundamental for autonomous\ndriving, urban planning, and virtual simulation, where smoothness, compactness,\nand accuracy are critical quality factors. Existing reconstruction methods\noften produce artifacts and inconsistencies that limit usability, while\ndownstream tasks have a tendency to represent roads as planes for simplicity\nbut at the cost of accuracy. We introduce FlexRoad, the first framework to\ndirectly address road surface smoothing by fitting Non-Uniform Rational\nB-Splines (NURBS) surfaces to 3D road points obtained from photogrammetric\nreconstructions or geodata providers. Our method at its core utilizes the\nElevation-Constrained Spatial Road Clustering (ECSRC) algorithm for robust\nanomaly correction, significantly reducing surface roughness and fitting\nerrors. To facilitate quantitative comparison between road surface\nreconstruction methods, we present GeoRoad Dataset (GeRoD), a diverse\ncollection of road surface and terrain profiles derived from openly accessible\ngeodata. Experiments on GeRoD and the photogrammetry-based DeepScenario Open 3D\nDataset (DSC3D) demonstrate that FlexRoad considerably surpasses commonly used\nroad surface representations across various metrics while being insensitive to\nvarious input sources, terrains, and noise types. By performing ablation\nstudies, we identify the key role of each component towards high-quality\nreconstruction performance, making FlexRoad a generic method for realistic road\nsurface modeling."}
{"id": "2504.16136", "pdf": "https://arxiv.org/pdf/2504.16136", "abs": "https://arxiv.org/abs/2504.16136", "authors": ["Chiung-Yi Tseng", "Junhao Song", "Ziqian Bi", "Tianyang Wang", "Chia Xin Liang", "Ming Liu"], "title": "Active Learning Methods for Efficient Data Utilization and Model Performance Enhancement", "categories": ["cs.LG"], "comment": null, "summary": "In the era of data-driven intelligence, the paradox of data abundance and\nannotation scarcity has emerged as a critical bottleneck in the advancement of\nmachine learning. This paper gives a detailed overview of Active Learning (AL),\nwhich is a strategy in machine learning that helps models achieve better\nperformance using fewer labeled examples. It introduces the basic concepts of\nAL and discusses how it is used in various fields such as computer vision,\nnatural language processing, transfer learning, and real-world applications.\nThe paper focuses on important research topics such as uncertainty estimation,\nhandling of class imbalance, domain adaptation, fairness, and the creation of\nstrong evaluation metrics and benchmarks. It also shows that learning methods\ninspired by humans and guided by questions can improve data efficiency and help\nmodels learn more effectively. In addition, this paper talks about current\nchallenges in the field, including the need to rebuild trust, ensure\nreproducibility, and deal with inconsistent methodologies. It points out that\nAL often gives better results than passive learning, especially when good\nevaluation measures are used. This work aims to be useful for both researchers\nand practitioners by providing key insights and proposing directions for future\nprogress in active learning."}
{"id": "2504.16209", "pdf": "https://arxiv.org/pdf/2504.16209", "abs": "https://arxiv.org/abs/2504.16209", "authors": ["Paul Zaidins", "Robert P. Goldman", "Ugur Kuter", "Dana Nau", "Mark Roberts"], "title": "HTN Plan Repair Algorithms Compared: Strengths and Weaknesses of Different Methods", "categories": ["cs.AI", "I.2.8"], "comment": "20 pages; 19 figures; To appear in the Proceedings for ICAPS 2025,\n  the 35th International Conference on Automated Planning and Schedulings", "summary": "This paper provides theoretical and empirical comparisons of three recent\nhierarchical plan repair algorithms: SHOPFixer, IPyHOPPER, and Rewrite. Our\ntheoretical results show that the three algorithms correspond to three\ndifferent definitions of the plan repair problem, leading to differences in the\nalgorithms' search spaces, the repair problems they can solve, and the kinds of\nrepairs they can make. Understanding these distinctions is important when\nchoosing a repair method for any given application.\n  Building on the theoretical results, we evaluate the algorithms empirically\nin a series of benchmark planning problems. Our empirical results provide more\ndetailed insight into the runtime repair performance of these systems and the\ncoverage of the repair problems solved, based on algorithmic properties such as\nreplanning, chronological backtracking, and backjumping over plan trees."}
{"id": "2504.16358", "pdf": "https://arxiv.org/pdf/2504.16358", "abs": "https://arxiv.org/abs/2504.16358", "authors": ["Tian Bai", "Huiyan Ying", "Kailong Suo", "Junqiu Wei", "Tao Fan", "Yuanfeng Song"], "title": "Text-to-TrajVis: Enabling Trajectory Data Visualizations from Natural Language Questions", "categories": ["cs.CL"], "comment": null, "summary": "This paper introduces the Text-to-TrajVis task, which aims to transform\nnatural language questions into trajectory data visualizations, facilitating\nthe development of natural language interfaces for trajectory visualization\nsystems. As this is a novel task, there is currently no relevant dataset\navailable in the community. To address this gap, we first devised a new\nvisualization language called Trajectory Visualization Language (TVL) to\nfacilitate querying trajectory data and generating visualizations. Building on\nthis foundation, we further proposed a dataset construction method that\nintegrates Large Language Models (LLMs) with human efforts to create\nhigh-quality data. Specifically, we first generate TVLs using a comprehensive\nand systematic process, and then label each TVL with corresponding natural\nlanguage questions using LLMs. This process results in the creation of the\nfirst large-scale Text-to-TrajVis dataset, named TrajVL, which contains 18,140\n(question, TVL) pairs. Based on this dataset, we systematically evaluated the\nperformance of multiple LLMs (GPT, Qwen, Llama, etc.) on this task. The\nexperimental results demonstrate that this task is both feasible and highly\nchallenging and merits further exploration within the research community."}
{"id": "2504.16441", "pdf": "https://arxiv.org/pdf/2504.16441", "abs": "https://arxiv.org/abs/2504.16441", "authors": ["Rongjin Li", "Weibin Zhang", "Dongpeng Chen", "Jintao Kang", "Xiaofen Xing"], "title": "SoCov: Semi-Orthogonal Parametric Pooling of Covariance Matrix for Speaker Recognition", "categories": ["eess.AS", "cs.SD"], "comment": "This paper has been accepted by IEEE ICASSP2025", "summary": "In conventional deep speaker embedding frameworks, the pooling layer\naggregates all frame-level features over time and computes their mean and\nstandard deviation statistics as inputs to subsequent segment-level layers.\nSuch statistics pooling strategy produces fixed-length representations from\nvariable-length speech segments. However, this method treats different\nframe-level features equally and discards covariance information. In this\npaper, we propose the Semi-orthogonal parameter pooling of Covariance matrix\n(SoCov) method. The SoCov pooling computes the covariance matrix from the\nself-attentive frame-level features and compresses it into a vector using the\nsemi-orthogonal parametric vectorization, which is then concatenated with the\nweighted standard deviation vector to form inputs to the segment-level layers.\nDeep embedding based on SoCov is called ``sc-vector''. The proposed sc-vector\nis compared to several different baselines on the SRE21 development and\nevaluation sets. The sc-vector system significantly outperforms the\nconventional x-vector system, with a relative reduction in EER of 15.5% on\nSRE21Eval. When using self-attentive deep feature, SoCov helps to reduce EER on\nSRE21Eval by about 30.9% relatively to the conventional ``mean + standard\ndeviation'' statistics."}
{"id": "2504.16234", "pdf": "https://arxiv.org/pdf/2504.16234", "abs": "https://arxiv.org/abs/2504.16234", "authors": ["Rene Pilz", "Johannes Schneider"], "title": "Using Phonemes in cascaded S2S translation pipeline", "categories": ["cs.LG", "cs.CL", "cs.SD", "eess.AS"], "comment": "Accepted at Swiss NLP Conference 2025", "summary": "This paper explores the idea of using phonemes as a textual representation\nwithin a conventional multilingual simultaneous speech-to-speech translation\npipeline, as opposed to the traditional reliance on text-based language\nrepresentations. To investigate this, we trained an open-source\nsequence-to-sequence model on the WMT17 dataset in two formats: one using\nstandard textual representation and the other employing phonemic\nrepresentation. The performance of both approaches was assessed using the BLEU\nmetric. Our findings shows that the phonemic approach provides comparable\nquality but offers several advantages, including lower resource requirements or\nbetter suitability for low-resource languages."}
{"id": "2504.16691", "pdf": "https://arxiv.org/pdf/2504.16691", "abs": "https://arxiv.org/abs/2504.16691", "authors": ["Xin Jiang", "Hao Tang", "Yonghua Pan", "Zechao Li"], "title": "Rethinking Vision Transformer for Large-Scale Fine-Grained Image Retrieval", "categories": ["cs.MM"], "comment": "Accepted by IEEE TMM", "summary": "Large-scale fine-grained image retrieval (FGIR) aims to retrieve images\nbelonging to the same subcategory as a given query by capturing subtle\ndifferences in a large-scale setting. Recently, Vision Transformers (ViT) have\nbeen employed in FGIR due to their powerful self-attention mechanism for\nmodeling long-range dependencies. However, most Transformer-based methods focus\nprimarily on leveraging self-attention to distinguish fine-grained details,\nwhile overlooking the high computational complexity and redundant dependencies\ninherent to these models, limiting their scalability and effectiveness in\nlarge-scale FGIR. In this paper, we propose an Efficient and Effective\nViT-based framework, termed \\textbf{EET}, which integrates token pruning module\nwith a discriminative transfer strategy to address these limitations.\nSpecifically, we introduce a content-based token pruning scheme to enhance the\nefficiency of the vanilla ViT, progressively removing background or\nlow-discriminative tokens at different stages by exploiting feature responses\nand self-attention mechanism. To ensure the resulting efficient ViT retains\nstrong discriminative power, we further present a discriminative transfer\nstrategy comprising both \\textit{discriminative knowledge transfer} and\n\\textit{discriminative region guidance}. Using a distillation paradigm, these\ncomponents transfer knowledge from a larger ``teacher'' ViT to a more efficient\n``student'' model, guiding the latter to focus on subtle yet crucial regions in\na cost-free manner. Extensive experiments on two widely-used fine-grained\ndatasets and four large-scale fine-grained datasets demonstrate the\neffectiveness of our method. Specifically, EET reduces the inference latency of\nViT-Small by 42.7\\% and boosts the retrieval performance of 16-bit hash codes\nby 5.15\\% on the challenging NABirds dataset."}
{"id": "2504.16774", "pdf": "https://arxiv.org/pdf/2504.16774", "abs": "https://arxiv.org/abs/2504.16774", "authors": ["Lakshita Agarwal", "Bindu Verma"], "title": "Advanced Chest X-Ray Analysis via Transformer-Based Image Descriptors and Cross-Model Attention Mechanism", "categories": ["eess.IV", "cs.CV"], "comment": null, "summary": "The examination of chest X-ray images is a crucial component in detecting\nvarious thoracic illnesses. This study introduces a new image description\ngeneration model that integrates a Vision Transformer (ViT) encoder with\ncross-modal attention and a GPT-4-based transformer decoder. The ViT captures\nhigh-quality visual features from chest X-rays, which are fused with text data\nthrough cross-modal attention to improve the accuracy, context, and richness of\nimage descriptions. The GPT-4 decoder transforms these fused features into\naccurate and relevant captions. The model was tested on the National Institutes\nof Health (NIH) and Indiana University (IU) Chest X-ray datasets. On the IU\ndataset, it achieved scores of 0.854 (B-1), 0.883 (CIDEr), 0.759 (METEOR), and\n0.712 (ROUGE-L). On the NIH dataset, it achieved the best performance on all\nmetrics: BLEU 1--4 (0.825, 0.788, 0.765, 0.752), CIDEr (0.857), METEOR (0.726),\nand ROUGE-L (0.705). This framework has the potential to enhance chest X-ray\nevaluation, assisting radiologists in more precise and efficient diagnosis."}
{"id": "2504.16114", "pdf": "https://arxiv.org/pdf/2504.16114", "abs": "https://arxiv.org/abs/2504.16114", "authors": ["Johannes Ferner", "Stefan Huber", "Saverio Messineo", "Angel Pop", "Martin Uray"], "title": "Persistence-based Hough Transform for Line Detection", "categories": ["cs.CV"], "comment": "Accepted at iDSC'25, Salzburg, Austria", "summary": "The Hough transform is a popular and classical technique in computer vision\nfor the detection of lines (or more general objects). It maps a pixel into a\ndual space -- the Hough space: each pixel is mapped to the set of lines through\nthis pixel, which forms a curve in Hough space. The detection of lines then\nbecomes a voting process to find those lines that received many votes by\npixels. However, this voting is done by thresholding, which is susceptible to\nnoise and other artifacts.\n  In this work, we present an alternative voting technique to detect peaks in\nthe Hough space based on persistent homology, which very naturally addresses\nlimitations of simple thresholding. Experiments on synthetic data show that our\nmethod significantly outperforms the original method, while also demonstrating\nenhanced robustness.\n  This work seeks to inspire future research in two key directions. First, we\nhighlight the untapped potential of Topological Data Analysis techniques and\nadvocate for their broader integration into existing methods, including\nwell-established ones. Secondly, we initiate a discussion on the mathematical\nstability of the Hough transform, encouraging exploration of mathematically\ngrounded improvements to enhance its robustness."}
{"id": "2504.16140", "pdf": "https://arxiv.org/pdf/2504.16140", "abs": "https://arxiv.org/abs/2504.16140", "authors": ["Max Hartman", "Lav Varshney"], "title": "SparseJEPA: Sparse Representation Learning of Joint Embedding Predictive Architectures", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Joint Embedding Predictive Architectures (JEPA) have emerged as a powerful\nframework for learning general-purpose representations. However, these models\noften lack interpretability and suffer from inefficiencies due to dense\nembedding representations. We propose SparseJEPA, an extension that integrates\nsparse representation learning into the JEPA framework to enhance the quality\nof learned representations. SparseJEPA employs a penalty method that encourages\nlatent space variables to be shared among data features with strong semantic\nrelationships, while maintaining predictive performance. We demonstrate the\neffectiveness of SparseJEPA by training on the CIFAR-100 dataset and\npre-training a lightweight Vision Transformer. The improved embeddings are\nutilized in linear-probe transfer learning for both image classification and\nlow-level tasks, showcasing the architecture's versatility across different\ntransfer tasks. Furthermore, we provide a theoretical proof that demonstrates\nthat the grouping mechanism enhances representation quality. This was done by\ndisplaying that grouping reduces Multiinformation among latent-variables,\nincluding proofing the Data Processing Inequality for Multiinformation. Our\nresults indicate that incorporating sparsity not only refines the latent space\nbut also facilitates the learning of more meaningful and interpretable\nrepresentations. In further work, hope to further extend this method by finding\nnew ways to leverage the grouping mechanism through object-centric\nrepresentation learning."}
{"id": "2504.16273", "pdf": "https://arxiv.org/pdf/2504.16273", "abs": "https://arxiv.org/abs/2504.16273", "authors": ["Joseph Lee", "Tianqi Shang", "Jae Young Baik", "Duy Duong-Tran", "Shu Yang", "Lingyao Li", "Li Shen"], "title": "Investigating LLMs in Clinical Triage: Promising Capabilities, Persistent Intersectional Biases", "categories": ["cs.AI", "cs.HC"], "comment": "Accepted to GenAI4Health Workshop @ AAAI 2025", "summary": "Large Language Models (LLMs) have shown promise in clinical decision support,\nyet their application to triage remains underexplored. We systematically\ninvestigate the capabilities of LLMs in emergency department triage through two\nkey dimensions: (1) robustness to distribution shifts and missing data, and (2)\ncounterfactual analysis of intersectional biases across sex and race. We assess\nmultiple LLM-based approaches, ranging from continued pre-training to\nin-context learning, as well as machine learning approaches. Our results\nindicate that LLMs exhibit superior robustness, and we investigate the key\nfactors contributing to the promising LLM-based approaches. Furthermore, in\nthis setting, we identify gaps in LLM preferences that emerge in particular\nintersections of sex and race. LLMs generally exhibit sex-based differences,\nbut they are most pronounced in certain racial groups. These findings suggest\nthat LLMs encode demographic preferences that may emerge in specific clinical\ncontexts or particular combinations of characteristics."}
{"id": "2504.16379", "pdf": "https://arxiv.org/pdf/2504.16379", "abs": "https://arxiv.org/abs/2504.16379", "authors": ["Yash Akhauri", "Anthony Fei", "Chi-Chih Chang", "Ahmed F. AbouElhamayed", "Yueying Li", "Mohamed S. Abdelfattah"], "title": "SplitReason: Learning To Offload Reasoning", "categories": ["cs.CL"], "comment": null, "summary": "Reasoning in large language models (LLMs) tends to produce substantially\nlonger token generation sequences than simpler language modeling tasks. This\nextended generation length reflects the multi-step, compositional nature of\nreasoning and is often correlated with higher solution accuracy. From an\nefficiency perspective, longer token generation exacerbates the inherently\nsequential and memory-bound decoding phase of LLMs. However, not all parts of\nthis expensive reasoning process are equally difficult to generate. We leverage\nthis observation by offloading only the most challenging parts of the reasoning\nprocess to a larger, more capable model, while performing most of the\ngeneration with a smaller, more efficient model; furthermore, we teach the\nsmaller model to identify these difficult segments and independently trigger\noffloading when needed. To enable this behavior, we annotate difficult segments\nacross 18k reasoning traces from the OpenR1-Math-220k chain-of-thought (CoT)\ndataset. We then apply supervised fine-tuning (SFT) and reinforcement learning\nfine-tuning (RLFT) to a 1.5B-parameter reasoning model, training it to learn to\noffload the most challenging parts of its own reasoning process to a larger\nmodel. This approach improves AIME24 reasoning accuracy by 24% and 28.3% while\noffloading 1.35% and 5% of the generated tokens respectively. We open-source\nour SplitReason model, data, code and logs."}
{"id": "2504.00621", "pdf": "https://arxiv.org/pdf/2504.00621", "abs": "https://arxiv.org/abs/2504.00621", "authors": ["Zexin Fang", "Bin Han", "Henrik H. Sveen", "C. Clark Cao", "Hans D. Schotten"], "title": "How Cyclic Acoustic Patterns Influence ASMR Perception: A Signal Processing Perspective", "categories": ["eess.AS"], "comment": "Submitted to IEEE Transactions on Cognitive and Developmental Systems", "summary": "Autonomous Sensory Meridian Response (ASMR) has been remarkably popular in\nthe recent decade. While its effect has been validated through behavioral\nstudies and neuro-physiological measurements such as electroencephalography\n(EEG) and related bio-signal analyses, its development and triggers remain a\nsubject of debate. Previous studies suggest that its triggers are highly linked\nwith cyclic patterns: predictable patterns introduce relaxation while\nvariations maintain intrigue. To validate this and further understand the\nimpact of acoustic features on ASMR effects, we designed three distinct cyclic\npatterns with monophonic and stereophonic variations, while controlling their\npredictability and randomness, and collected ASMR triggering scores through\nonline surveys. Then, we extracted cyclic features and carried out regression\nanalysis, seeking an explainable mapping of cyclic features and ASMR triggers.\nWe found that relaxing effects accumulate progressively and are independent of\nspatial orientation. Cyclic patterns significantly influence psychological and\nphysical effects, which remain invariant with time. Regression analysis\nrevealed that smoothly spread and energy-dense cyclic patterns most effectively\ntrigger ASMR responses."}
{"id": "2504.16276", "pdf": "https://arxiv.org/pdf/2504.16276", "abs": "https://arxiv.org/abs/2504.16276", "authors": ["Abhishek Jana", "Moeumu Uili", "James Atherton", "Mark O'Brien", "Joe Wood", "Leandra Brickson"], "title": "An Automated Pipeline for Few-Shot Bird Call Classification: A Case Study with the Tooth-Billed Pigeon", "categories": ["cs.LG", "cs.AI", "cs.CV", "cs.SD"], "comment": "16 pages, 5 figures, 4 tables", "summary": "This paper presents an automated one-shot bird call classification pipeline\ndesigned for rare species absent from large publicly available classifiers like\nBirdNET and Perch. While these models excel at detecting common birds with\nabundant training data, they lack options for species with only 1-3 known\nrecordings-a critical limitation for conservationists monitoring the last\nremaining individuals of endangered birds. To address this, we leverage the\nembedding space of large bird classification networks and develop a classifier\nusing cosine similarity, combined with filtering and denoising preprocessing\ntechniques, to optimize detection with minimal training data. We evaluate\nvarious embedding spaces using clustering metrics and validate our approach in\nboth a simulated scenario with Xeno-Canto recordings and a real-world test on\nthe critically endangered tooth-billed pigeon (Didunculus strigirostris), which\nhas no existing classifiers and only three confirmed recordings. The final\nmodel achieved 1.0 recall and 0.95 accuracy in detecting tooth-billed pigeon\ncalls, making it practical for use in the field. This open-source system\nprovides a practical tool for conservationists seeking to detect and monitor\nrare species on the brink of extinction."}
{"id": "2504.16798", "pdf": "https://arxiv.org/pdf/2504.16798", "abs": "https://arxiv.org/abs/2504.16798", "authors": ["Yuxiang Wei", "Yanteng Zhang", "Xi Xiao", "Tianyang Wang", "Xiao Wang", "Vince D. Calhoun"], "title": "4D Multimodal Co-attention Fusion Network with Latent Contrastive Alignment for Alzheimer's Diagnosis", "categories": ["cs.MM", "cs.CV", "cs.LG"], "comment": null, "summary": "Multimodal neuroimaging provides complementary structural and functional\ninsights into both human brain organization and disease-related dynamics.\nRecent studies demonstrate enhanced diagnostic sensitivity for Alzheimer's\ndisease (AD) through synergistic integration of neuroimaging data (e.g., sMRI,\nfMRI) with behavioral cognitive scores tabular data biomarkers. However, the\nintrinsic heterogeneity across modalities (e.g., 4D spatiotemporal fMRI\ndynamics vs. 3D anatomical sMRI structure) presents critical challenges for\ndiscriminative feature fusion. To bridge this gap, we propose M2M-AlignNet: a\ngeometry-aware multimodal co-attention network with latent alignment for early\nAD diagnosis using sMRI and fMRI. At the core of our approach is a\nmulti-patch-to-multi-patch (M2M) contrastive loss function that quantifies and\nreduces representational discrepancies via geometry-weighted patch\ncorrespondence, explicitly aligning fMRI components across brain regions with\ntheir sMRI structural substrates without one-to-one constraints. Additionally,\nwe propose a latent-as-query co-attention module to autonomously discover\nfusion patterns, circumventing modality prioritization biases while minimizing\nfeature redundancy. We conduct extensive experiments to confirm the\neffectiveness of our method and highlight the correspondance between fMRI and\nsMRI as AD biomarkers."}
{"id": "2504.16319", "pdf": "https://arxiv.org/pdf/2504.16319", "abs": "https://arxiv.org/abs/2504.16319", "authors": ["Connor Blais", "Md Abdul Baset Sarker", "Masudul H. Imtiaz"], "title": "Vision Controlled Orthotic Hand Exoskeleton", "categories": ["cs.RO", "cs.SY", "eess.IV", "eess.SY"], "comment": null, "summary": "This paper presents the design and implementation of an AI vision-controlled\northotic hand exoskeleton to enhance rehabilitation and assistive functionality\nfor individuals with hand mobility impairments. The system leverages a Google\nCoral Dev Board Micro with an Edge TPU to enable real-time object detection\nusing a customized MobileNet\\_V2 model trained on a six-class dataset. The\nexoskeleton autonomously detects objects, estimates proximity, and triggers\npneumatic actuation for grasp-and-release tasks, eliminating the need for\nuser-specific calibration needed in traditional EMG-based systems. The design\nprioritizes compactness, featuring an internal battery. It achieves an 8-hour\nruntime with a 1300 mAh battery. Experimental results demonstrate a 51ms\ninference speed, a significant improvement over prior iterations, though\nchallenges persist in model robustness under varying lighting conditions and\nobject orientations. While the most recent YOLO model (YOLOv11) showed\npotential with 15.4 FPS performance, quantization issues hindered deployment.\nThe prototype underscores the viability of vision-controlled exoskeletons for\nreal-world assistive applications, balancing portability, efficiency, and\nreal-time responsiveness, while highlighting future directions for model\noptimization and hardware miniaturization."}
{"id": "2504.16117", "pdf": "https://arxiv.org/pdf/2504.16117", "abs": "https://arxiv.org/abs/2504.16117", "authors": ["Sridevi Polavaram", "Xin Zhou", "Meenu Ravi", "Mohammad Zarei", "Anmol Srivastava"], "title": "Context-Awareness and Interpretability of Rare Occurrences for Discovery and Formalization of Critical Failure Modes", "categories": ["cs.CV", "cs.AI", "cs.HC"], "comment": "Accepted to IEEE Conference for Artificial Intelligence, 2025", "summary": "Vision systems are increasingly deployed in critical domains such as\nsurveillance, law enforcement, and transportation. However, their\nvulnerabilities to rare or unforeseen scenarios pose significant safety risks.\nTo address these challenges, we introduce Context-Awareness and\nInterpretability of Rare Occurrences (CAIRO), an ontology-based human-assistive\ndiscovery framework for failure cases (or CP - Critical Phenomena) detection\nand formalization. CAIRO by design incentivizes human-in-the-loop for testing\nand evaluation of criticality that arises from misdetections, adversarial\nattacks, and hallucinations in AI black-box models. Our robust analysis of\nobject detection model(s) failures in automated driving systems (ADS) showcases\nscalable and interpretable ways of formalizing the observed gaps between camera\nperception and real-world contexts, resulting in test cases stored as explicit\nknowledge graphs (in OWL/XML format) amenable for sharing, downstream analysis,\nlogical reasoning, and accountability."}
{"id": "2504.16141", "pdf": "https://arxiv.org/pdf/2504.16141", "abs": "https://arxiv.org/abs/2504.16141", "authors": ["Yue Shi", "Liangxiu Han", "Xin Zhang", "Tam Sobeih", "Thomas Gaiser", "Nguyen Huu Thuy", "Dominik Behrend", "Amit Kumar Srivastava", "Krishnagopal Halder", "Frank Ewert"], "title": "Deep Learning Meets Process-Based Models: A Hybrid Approach to Agricultural Challenges", "categories": ["cs.LG"], "comment": null, "summary": "Process-based models (PBMs) and deep learning (DL) are two key approaches in\nagricultural modelling, each offering distinct advantages and limitations. PBMs\nprovide mechanistic insights based on physical and biological principles,\nensuring interpretability and scientific rigour. However, they often struggle\nwith scalability, parameterisation, and adaptation to heterogeneous\nenvironments. In contrast, DL models excel at capturing complex, nonlinear\npatterns from large datasets but may suffer from limited interpretability, high\ncomputational demands, and overfitting in data-scarce scenarios.\n  This study presents a systematic review of PBMs, DL models, and hybrid PBM-DL\nframeworks, highlighting their applications in agricultural and environmental\nmodelling. We classify hybrid PBM-DL approaches into DL-informed PBMs, where\nneural networks refine process-based models, and PBM-informed DL, where\nphysical constraints guide deep learning predictions. Additionally, we conduct\na case study on crop dry biomass prediction, comparing hybrid models against\nstandalone PBMs and DL models under varying data quality, sample sizes, and\nspatial conditions. The results demonstrate that hybrid models consistently\noutperform traditional PBMs and DL models, offering greater robustness to noisy\ndata and improved generalisation across unseen locations.\n  Finally, we discuss key challenges, including model interpretability,\nscalability, and data requirements, alongside actionable recommendations for\nadvancing hybrid modelling in agriculture. By integrating domain knowledge with\nAI-driven approaches, this study contributes to the development of scalable,\ninterpretable, and reproducible agricultural models that support data-driven\ndecision-making for sustainable agriculture."}
{"id": "2504.16622", "pdf": "https://arxiv.org/pdf/2504.16622", "abs": "https://arxiv.org/abs/2504.16622", "authors": ["Christoforus Yoga Haryanto", "Emily Lomempow"], "title": "Cognitive Silicon: An Architectural Blueprint for Post-Industrial Computing Systems", "categories": ["cs.AI", "cs.CY"], "comment": "Working Paper, 37 pages, 1 figure, 5 tables", "summary": "Autonomous AI systems reveal foundational limitations in deterministic,\nhuman-authored computing architectures. This paper presents Cognitive Silicon:\na hypothetical full-stack architectural framework projected toward 2035,\nexploring a possible trajectory for cognitive computing system design. The\nproposed architecture would integrate symbolic scaffolding, governed memory,\nruntime moral coherence, and alignment-aware execution across\nsilicon-to-semantics layers. Our design grammar has emerged from dialectical\nco-design with LLMs under asymmetric epistemic conditions--creating structured\nfriction to expose blind spots and trade-offs. The envisioned framework would\nestablish mortality as a natural consequence of physical constraints,\nnon-copyable tacit knowledge, and non-cloneable identity keys as\ncognitive-embodiment primitives. Core tensions (trust/agency,\nscaffolding/emergence, execution/governance) would function as central\narchitectural pressures rather than edge cases. The architecture theoretically\nconverges with the Free Energy Principle, potentially offering a formal account\nof how cognitive systems could maintain identity through prediction error\nminimization across physical and computational boundaries. The resulting\nframework aims to deliver a morally tractable cognitive infrastructure that\ncould maintain human-alignment through irreversible hardware constraints and\nidentity-bound epistemic mechanisms resistant to replication or subversion."}
{"id": "2504.16394", "pdf": "https://arxiv.org/pdf/2504.16394", "abs": "https://arxiv.org/abs/2504.16394", "authors": ["Fahmida Liza Piya", "Rahmatollah Beheshti"], "title": "ConTextual: Improving Clinical Text Summarization in LLMs with Context-preserving Token Filtering and Knowledge Graphs", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Unstructured clinical data can serve as a unique and rich source of\ninformation that can meaningfully inform clinical practice. Extracting the most\npertinent context from such data is critical for exploiting its true potential\ntoward optimal and timely decision-making in patient care. While prior research\nhas explored various methods for clinical text summarization, most prior\nstudies either process all input tokens uniformly or rely on heuristic-based\nfilters, which can overlook nuanced clinical cues and fail to prioritize\ninformation critical for decision-making. In this study, we propose Contextual,\na novel framework that integrates a Context-Preserving Token Filtering method\nwith a Domain-Specific Knowledge Graph (KG) for contextual augmentation. By\npreserving context-specific important tokens and enriching them with structured\nknowledge, ConTextual improves both linguistic coherence and clinical fidelity.\nOur extensive empirical evaluations on two public benchmark datasets\ndemonstrate that ConTextual consistently outperforms other baselines. Our\nproposed approach highlights the complementary role of token-level filtering\nand structured retrieval in enhancing both linguistic and clinical integrity,\nas well as offering a scalable solution for improving precision in clinical\ntext generation."}
{"id": "2408.05928", "pdf": "https://arxiv.org/pdf/2408.05928", "abs": "https://arxiv.org/abs/2408.05928", "authors": ["Xiaoxiao Miao", "Yuxiang Zhang", "Xin Wang", "Natalia Tomashenko", "Donny Cheng Lock Soh", "Ian Mcloughlin"], "title": "Adapting General Disentanglement-Based Speaker Anonymization for Enhanced Emotion Preservation", "categories": ["cs.SD", "eess.AS"], "comment": "Accepted by computer speech and language", "summary": "A general disentanglement-based speaker anonymization system typically\nseparates speech into content, speaker, and prosody features using individual\nencoders. This paper explores how to adapt such a system when a new speech\nattribute, for example, emotion, needs to be preserved to a greater extent.\nWhile existing systems are good at anonymizing speaker embeddings, they are not\ndesigned to preserve emotion. Two strategies for this are examined. First, we\nshow that integrating emotion embeddings from a pre-trained emotion encoder can\nhelp preserve emotional cues, even though this approach slightly compromises\nprivacy protection. Alternatively, we propose an emotion compensation strategy\nas a post-processing step applied to anonymized speaker embeddings. This\nconceals the original speaker's identity and reintroduces the emotional traits\nlost during speaker embedding anonymization. Specifically, we model the emotion\nattribute using support vector machines to learn separate boundaries for each\nemotion. During inference, the original speaker embedding is processed in two\nways: one, by an emotion indicator to predict emotion and select the\nemotion-matched SVM accurately; and two, by a speaker anonymizer to conceal\nspeaker characteristics. The anonymized speaker embedding is then modified\nalong the corresponding SVM boundary towards an enhanced emotional direction to\nsave the emotional cues. The proposed strategies are also expected to be useful\nfor adapting a general disentanglement-based speaker anonymization system to\npreserve other target paralinguistic attributes, with potential for a range of\ndownstream tasks."}
{"id": "2504.16459", "pdf": "https://arxiv.org/pdf/2504.16459", "abs": "https://arxiv.org/abs/2504.16459", "authors": ["Yuga Tsukuda", "Naoto Nishida", "Jun Lu", "Yoichi Ochiai"], "title": "Insect-Computer Hybrid Speaker: Speaker using Chirp of the Cicada Controlled by Electrical Muscle Stimulation", "categories": ["cs.HC", "cs.AR", "cs.ET", "cs.RO", "cs.SD"], "comment": "6 pages, 3 figures", "summary": "We propose \"Insect-Computer Hybrid Speaker\", which enables us to make musics\nmade from combinations of computer and insects. Lots of studies have proposed\nmethods and interfaces for controlling insects and obtaining feedback. However,\nthere have been less research on the use of insects for interaction with third\nparties. In this paper, we propose a method in which cicadas are used as\nspeakers triggered by using Electrical Muscle Stimulation (EMS). We explored\nand investigated the suitable waveform of chirp to be controlled, the\nappropriate voltage range, and the maximum pitch at which cicadas can chirp."}
{"id": "2504.16322", "pdf": "https://arxiv.org/pdf/2504.16322", "abs": "https://arxiv.org/abs/2504.16322", "authors": ["Haoyuan Zhao", "Jianxin Shi", "Guanzhen Wu", "Hao Fang", "Yi Ching Chou", "Long Chen", "Feng Wang", "Jiangchuan Liu"], "title": "BAROC: Concealing Packet Losses in LSNs with Bimodal Behavior Awareness for Livecast Ingestion", "categories": ["cs.NI", "cs.MM"], "comment": "This is the preprint version of the paper accepted to IEEE INFOCOM\n  2025", "summary": "The advent of Low-Earth Orbit satellite networks (LSNs), exemplified by\ninitiatives like \\emph{Starlink}, \\emph{OneWeb} and \\emph{Kuiper}, has ushered\nin a new era of ``Internet from Space\" global connectivity. Recent studies have\nshown that LSNs are capable of providing unprecedented download capacity and\nlow latency to support Livecast viewing. However, Livecast ingestion still\nfaces significant challenges, such as limited uplink capacity, bandwidth\ndegradation, and the burst of packet loss due to frequent satellite\nreallocations, which cause previous recovery and adaptive solutions to be\ninferior under this new scenario. In this paper, we conduct an in-depth\nmeasurement study dedicated to understanding the implications of satellite\nreallocations, which reveals that the network status during reallocations with\nnetwork anomalies exhibits a different distribution, leading to bimodal\nbehaviors on the overall network performance. Motivated by this finding, we\npropose BAROC, a framework that can effectively conceal burst packet losses by\ncombining a novel proposed MTP-Informer with bimodal behavior awareness during\nsatellite reallocation. BAROC enhances video QoE on the server side by\naddressing the above challenges and jointly determining the optimal video\nencoding and recovery parameters. Our extensive evaluation shows that BAROC\noutperforms other video delivery recovery approaches, achieving an average PSNR\nimprovement of $1.95$ dB and a maximum of $3.44$ dB, along with enhancements in\nframe rate and parity packet utilization. Additionally, a comprehensive\nablation study is conducted to assess the effectiveness of MTP-Informer and\ncomponents in BAROC."}
{"id": "2504.16404", "pdf": "https://arxiv.org/pdf/2504.16404", "abs": "https://arxiv.org/abs/2504.16404", "authors": ["Md Fahimuzzman Sohan"], "title": "Assessing the Feasibility of Internet-Sourced Video for Automatic Cattle Lameness Detection", "categories": ["cs.CV", "cs.AI", "cs.LG", "eess.IV"], "comment": null, "summary": "Cattle lameness is often caused by hoof injuries or interdigital dermatitis,\nleads to pain and significantly impacts essential physiological activities such\nas walking, feeding, and drinking. This study presents a deep learning-based\nmodel for detecting cattle lameness, sickness, or gait abnormalities using\npublicly available video data. The dataset consists of 50 unique videos from 40\nindividual cattle, recorded from various angles in both indoor and outdoor\nenvironments. Half of the dataset represents naturally walking\n(normal/non-lame) cattle, while the other half consists of cattle exhibiting\ngait abnormalities (lame). To enhance model robustness and generalizability,\ndata augmentation was applied to the training data. The pre-processed videos\nwere then classified using two deep learning models: ConvLSTM2D and 3D CNN. A\ncomparative analysis of the results demonstrates strong classification\nperformance. Specifically, the 3D CNN model achieved a video-level\nclassification accuracy of 90%, with precision, recall, and f1-score of 90.9%,\n90.9%, and 90.91% respectively. The ConvLSTM2D model exhibited a slightly lower\naccuracy of 85%. This study highlights the effectiveness of directly applying\nclassification models to learn spatiotemporal features from video data,\noffering an alternative to traditional multi-stage approaches that typically\ninvolve object detection, pose estimation, and feature extraction. Besides, the\nfindings demonstrate that the proposed deep learning models, particularly the\n3D CNN, effectively classify and detect lameness in cattle while simplifying\nthe processing pipeline."}
{"id": "2504.16127", "pdf": "https://arxiv.org/pdf/2504.16127", "abs": "https://arxiv.org/abs/2504.16127", "authors": ["Xingxing Zuo", "Nikhil Ranganathan", "Connor Lee", "Georgia Gkioxari", "Soon-Jo Chung"], "title": "MonoTher-Depth: Enhancing Thermal Depth Estimation via Confidence-Aware Distillation", "categories": ["cs.CV", "cs.RO"], "comment": "8 Pages; The code will be available at\n  https://github.com/ZuoJiaxing/monother_depth", "summary": "Monocular depth estimation (MDE) from thermal images is a crucial technology\nfor robotic systems operating in challenging conditions such as fog, smoke, and\nlow light. The limited availability of labeled thermal data constrains the\ngeneralization capabilities of thermal MDE models compared to foundational RGB\nMDE models, which benefit from datasets of millions of images across diverse\nscenarios. To address this challenge, we introduce a novel pipeline that\nenhances thermal MDE through knowledge distillation from a versatile RGB MDE\nmodel. Our approach features a confidence-aware distillation method that\nutilizes the predicted confidence of the RGB MDE to selectively strengthen the\nthermal MDE model, capitalizing on the strengths of the RGB model while\nmitigating its weaknesses. Our method significantly improves the accuracy of\nthe thermal MDE, independent of the availability of labeled depth supervision,\nand greatly expands its applicability to new scenarios. In our experiments on\nnew scenarios without labeled depth, the proposed confidence-aware distillation\nmethod reduces the absolute relative error of thermal MDE by 22.88\\% compared\nto the baseline without distillation."}
{"id": "2504.16214", "pdf": "https://arxiv.org/pdf/2504.16214", "abs": "https://arxiv.org/abs/2504.16214", "authors": ["Xiao Zhang", "Yaoyao Ding", "Yang Hu", "Gennady Pekhimenko"], "title": "Hexcute: A Tile-based Programming Language with Automatic Layout and Task-Mapping Synthesis", "categories": ["cs.LG", "cs.AI", "cs.PL"], "comment": "17 pages, 24 figures", "summary": "Deep learning (DL) workloads mainly run on accelerators like GPUs. Recent DL\nquantization techniques demand a new matrix multiplication operator with mixed\ninput data types, further complicating GPU optimization. Prior high-level\ncompilers like Triton lack the expressiveness to implement key optimizations\nlike fine-grained data pipelines and hardware-friendly memory layouts for these\noperators, while low-level programming models, such as Hidet, Graphene, and\nCUTLASS, require significant programming efforts. To balance expressiveness\nwith engineering effort, we propose Hexcute, a tile-based programming language\nthat exposes shared memory and register abstractions to enable fine-grained\noptimization for these operators. Additionally, Hexcute leverages task mapping\nto schedule the GPU program, and to reduce programming efforts, it automates\nlayout and task mapping synthesis with a novel type-inference-based algorithm.\nOur evaluation shows that Hexcute generalizes to a wide range of DL operators,\nachieves 1.7-11.28$\\times$ speedup over existing DL compilers for mixed-type\noperators, and brings up to 2.91$\\times$ speedup in the end-to-end evaluation."}
{"id": "2504.16635", "pdf": "https://arxiv.org/pdf/2504.16635", "abs": "https://arxiv.org/abs/2504.16635", "authors": ["Fredy Pokou", "Jules Sadefo Kamdem", "François Benhmad"], "title": "Bridging Econometrics and AI: VaR Estimation via Reinforcement Learning and GARCH Models", "categories": ["cs.AI", "q-fin.CP", "q-fin.RM", "q-fin.ST"], "comment": null, "summary": "In an environment of increasingly volatile financial markets, the accurate\nestimation of risk remains a major challenge. Traditional econometric models,\nsuch as GARCH and its variants, are based on assumptions that are often too\nrigid to adapt to the complexity of the current market dynamics. To overcome\nthese limitations, we propose a hybrid framework for Value-at-Risk (VaR)\nestimation, combining GARCH volatility models with deep reinforcement learning.\nOur approach incorporates directional market forecasting using the Double Deep\nQ-Network (DDQN) model, treating the task as an imbalanced classification\nproblem. This architecture enables the dynamic adjustment of risk-level\nforecasts according to market conditions. Empirical validation on daily\nEurostoxx 50 data covering periods of crisis and high volatility shows a\nsignificant improvement in the accuracy of VaR estimates, as well as a\nreduction in the number of breaches and also in capital requirements, while\nrespecting regulatory risk thresholds. The ability of the model to adjust risk\nlevels in real time reinforces its relevance to modern and proactive risk\nmanagement."}
{"id": "2504.16408", "pdf": "https://arxiv.org/pdf/2504.16408", "abs": "https://arxiv.org/abs/2504.16408", "authors": ["Jiahao Yuan", "Xingzhe Sun", "Xing Yu", "Jingwen Wang", "Dehui Du", "Zhiqing Cui", "Zixiang Di"], "title": "Less is More: Enhancing Structured Multi-Agent Reasoning via Quality-Guided Distillation", "categories": ["cs.CL"], "comment": null, "summary": "The XLLM@ACL2025 Shared Task-III formulates a low-resource structural\nreasoning task that challenges LLMs to generate interpretable, step-by-step\nrationales with minimal labeled data. We present Less is More, the third-place\nwinning approach in the XLLM@ACL2025 Shared Task-III, which focuses on\nstructured reasoning from only 24 labeled examples. Our approach leverages a\nmulti-agent framework with reverse-prompt induction, retrieval-augmented\nreasoning synthesis via GPT-4o, and dual-stage reward-guided filtering to\ndistill high-quality supervision across three subtasks: question parsing, CoT\nparsing, and step-level verification. All modules are fine-tuned from\nMeta-Llama-3-8B-Instruct under a unified LoRA+ setup. By combining structure\nvalidation with reward filtering across few-shot and zero-shot prompts, our\npipeline consistently improves structure reasoning quality. These results\nunderscore the value of controllable data distillation in enhancing structured\ninference under low-resource constraints. Our code is available at\nhttps://github.com/Jiahao-Yuan/Less-is-More."}
{"id": "2503.10522", "pdf": "https://arxiv.org/pdf/2503.10522", "abs": "https://arxiv.org/abs/2503.10522", "authors": ["Zeyue Tian", "Yizhu Jin", "Zhaoyang Liu", "Ruibin Yuan", "Xu Tan", "Qifeng Chen", "Wei Xue", "Yike Guo"], "title": "AudioX: Diffusion Transformer for Anything-to-Audio Generation", "categories": ["cs.MM", "cs.CV", "cs.LG", "cs.SD", "eess.AS"], "comment": "The code and datasets will be available at\n  https://zeyuet.github.io/AudioX/", "summary": "Audio and music generation have emerged as crucial tasks in many\napplications, yet existing approaches face significant limitations: they\noperate in isolation without unified capabilities across modalities, suffer\nfrom scarce high-quality, multi-modal training data, and struggle to\neffectively integrate diverse inputs. In this work, we propose AudioX, a\nunified Diffusion Transformer model for Anything-to-Audio and Music Generation.\nUnlike previous domain-specific models, AudioX can generate both general audio\nand music with high quality, while offering flexible natural language control\nand seamless processing of various modalities including text, video, image,\nmusic, and audio. Its key innovation is a multi-modal masked training strategy\nthat masks inputs across modalities and forces the model to learn from masked\ninputs, yielding robust and unified cross-modal representations. To address\ndata scarcity, we curate two comprehensive datasets: vggsound-caps with 190K\naudio captions based on the VGGSound dataset, and V2M-caps with 6 million music\ncaptions derived from the V2M dataset. Extensive experiments demonstrate that\nAudioX not only matches or outperforms state-of-the-art specialized models, but\nalso offers remarkable versatility in handling diverse input modalities and\ngeneration tasks within a unified architecture. The code and datasets will be\navailable at https://zeyuet.github.io/AudioX/"}
{"id": "2504.16416", "pdf": "https://arxiv.org/pdf/2504.16416", "abs": "https://arxiv.org/abs/2504.16416", "authors": ["Tao Long", "Kendra Wannamaker", "Jo Vermeulen", "George Fitzmaurice", "Justin Matejka"], "title": "FeedQUAC: Quick Unobtrusive AI-Generated Commentary", "categories": ["cs.HC", "cs.AI", "cs.CY", "cs.MM"], "comment": "20 pages, 12 figures", "summary": "Design thrives on feedback. However, gathering constant feedback throughout\nthe design process can be labor-intensive and disruptive. We explore how AI can\nbridge this gap by providing effortless, ambient feedback. We introduce\nFeedQUAC, a design companion that delivers real-time AI-generated commentary\nfrom a variety of perspectives through different personas. A design probe study\nwith eight participants highlights how designers can leverage quick yet ambient\nAI feedback to enhance their creative workflows. Participants highlight\nbenefits such as convenience, playfulness, confidence boost, and inspiration\nfrom this lightweight feedback agent, while suggesting additional features,\nlike chat interaction and context curation. We discuss the role of AI feedback,\nits strengths and limitations, and how to integrate it into existing design\nworkflows while balancing user involvement. Our findings also suggest that\nambient interaction is a valuable consideration for both the design and\nevaluation of future creativity support systems."}
{"id": "2302.06352", "pdf": "https://arxiv.org/pdf/2302.06352", "abs": "https://arxiv.org/abs/2302.06352", "authors": ["Francesco Santini", "Jakob Wasserthal", "Abramo Agosti", "Xeni Deligianni", "Kevin R. Keene", "Hermien E. Kan", "Stefan Sommer", "Fengdan Wang", "Claudia Weidensteiner", "Giulia Manco", "Matteo Paoletti", "Valentina Mazzoli", "Arjun Desai", "Anna Pichiecchio"], "title": "Deep Anatomical Federated Network (Dafne): An open client-server framework for the continuous, collaborative improvement of deep learning-based medical image segmentation", "categories": ["eess.IV", "cs.CV", "cs.LG"], "comment": "In this new version: change affiliation of A. Pichiecchio. Note\n  regarding the license/copyright: This submission is conforming with the RSNA\n  Preprint policy available here: this https URL, which REQUIRES authors to\n  update the version on preprint servers with the accepted version and the\n  copyright notice as indicated in the PDF", "summary": "Purpose: To present and evaluate Dafne (deep anatomical federated network), a\nfreely available decentralized, collaborative deep learning system for the\nsemantic segmentation of radiological images through federated incremental\nlearning. Materials and Methods: Dafne is free software with a client-server\narchitecture. The client side is an advanced user interface that applies the\ndeep learning models stored on the server to the user's data and allows the\nuser to check and refine the prediction. Incremental learning is then performed\nat the client's side and sent back to the server, where it is integrated into\nthe root model. Dafne was evaluated locally, by assessing the performance gain\nacross model generations on 38 MRI datasets of the lower legs, and through the\nanalysis of real-world usage statistics (n = 639 use-cases). Results: Dafne\ndemonstrated a statistically improvement in the accuracy of semantic\nsegmentation over time (average increase of the Dice Similarity Coefficient by\n0.007 points/generation on the local validation set, p < 0.001). Qualitatively,\nthe models showed enhanced performance on various radiologic image types,\nincluding those not present in the initial training sets, indicating good model\ngeneralizability. Conclusion: Dafne showed improvement in segmentation quality\nover time, demonstrating potential for learning and generalization."}
{"id": "2504.16128", "pdf": "https://arxiv.org/pdf/2504.16128", "abs": "https://arxiv.org/abs/2504.16128", "authors": ["Stanley Mugisha", "Rashid Kisitu", "Florence Tushabe"], "title": "Hybrid Knowledge Transfer through Attention and Logit Distillation for On-Device Vision Systems in Agricultural IoT", "categories": ["cs.CV", "cs.AI", "cs.LG", "I.2.10; I.4.9"], "comment": "12 pages and 4 figures", "summary": "Integrating deep learning applications into agricultural IoT systems faces a\nserious challenge of balancing the high accuracy of Vision Transformers (ViTs)\nwith the efficiency demands of resource-constrained edge devices. Large\ntransformer models like the Swin Transformers excel in plant disease\nclassification by capturing global-local dependencies. However, their\ncomputational complexity (34.1 GFLOPs) limits applications and renders them\nimpractical for real-time on-device inference. Lightweight models such as\nMobileNetV3 and TinyML would be suitable for on-device inference but lack the\nrequired spatial reasoning for fine-grained disease detection. To bridge this\ngap, we propose a hybrid knowledge distillation framework that synergistically\ntransfers logit and attention knowledge from a Swin Transformer teacher to a\nMobileNetV3 student model. Our method includes the introduction of adaptive\nattention alignment to resolve cross-architecture mismatch (resolution,\nchannels) and a dual-loss function optimizing both class probabilities and\nspatial focus. On the lantVillage-Tomato dataset (18,160 images), the distilled\nMobileNetV3 attains 92.4% accuracy relative to 95.9% for Swin-L but at an 95%\nreduction on PC and < 82% in inference latency on IoT devices. (23ms on PC CPU\nand 86ms/image on smartphone CPUs). Key innovations include IoT-centric\nvalidation metrics (13 MB memory, 0.22 GFLOPs) and dynamic resolution-matching\nattention maps. Comparative experiments show significant improvements over\nstandalone CNNs and prior distillation methods, with a 3.5% accuracy gain over\nMobileNetV3 baselines. Significantly, this work advances real-time,\nenergy-efficient crop monitoring in precision agriculture and demonstrates how\nwe can attain ViT-level diagnostic precision on edge devices. Code and models\nwill be made available for replication after acceptance."}
{"id": "2504.16238", "pdf": "https://arxiv.org/pdf/2504.16238", "abs": "https://arxiv.org/abs/2504.16238", "authors": ["Léandre Eberhard", "Nirek Sharma", "Filipp Shelobolin", "Aalok Ganesh Shanbhag"], "title": "General Post-Processing Framework for Fairness Adjustment of Machine Learning Models", "categories": ["cs.LG"], "comment": "Submitted to FAccT 2025. Does not include reviewer feedback yet", "summary": "As machine learning increasingly influences critical domains such as credit\nunderwriting, public policy, and talent acquisition, ensuring compliance with\nfairness constraints is both a legal and ethical imperative. This paper\nintroduces a novel framework for fairness adjustments that applies to diverse\nmachine learning tasks, including regression and classification, and\naccommodates a wide range of fairness metrics. Unlike traditional approaches\ncategorized as pre-processing, in-processing, or post-processing, our method\nadapts in-processing techniques for use as a post-processing step. By\ndecoupling fairness adjustments from the model training process, our framework\npreserves model performance on average while enabling greater flexibility in\nmodel development. Key advantages include eliminating the need for custom loss\nfunctions, enabling fairness tuning using different datasets, accommodating\nproprietary models as black-box systems, and providing interpretable insights\ninto the fairness adjustments. We demonstrate the effectiveness of this\napproach by comparing it to Adversarial Debiasing, showing that our framework\nachieves a comparable fairness/accuracy tradeoff on real-world datasets."}
{"id": "2504.16728", "pdf": "https://arxiv.org/pdf/2504.16728", "abs": "https://arxiv.org/abs/2504.16728", "authors": ["Aniketh Garikaparthi", "Manasi Patwardhan", "Lovekesh Vig", "Arman Cohan"], "title": "IRIS: Interactive Research Ideation System for Accelerating Scientific Discovery", "categories": ["cs.AI", "cs.CL"], "comment": "6 pages main-text, 2 pages appendix", "summary": "The rapid advancement in capabilities of large language models (LLMs) raises\na pivotal question: How can LLMs accelerate scientific discovery? This work\ntackles the crucial first stage of research, generating novel hypotheses. While\nrecent work on automated hypothesis generation focuses on multi-agent\nframeworks and extending test-time compute, none of the approaches effectively\nincorporate transparency and steerability through a synergistic\nHuman-in-the-loop (HITL) approach. To address this gap, we introduce IRIS:\nInteractive Research Ideation System, an open-source platform designed for\nresearchers to leverage LLM-assisted scientific ideation. IRIS incorporates\ninnovative features to enhance ideation, including adaptive test-time compute\nexpansion via Monte Carlo Tree Search (MCTS), fine-grained feedback mechanism,\nand query-based literature synthesis. Designed to empower researchers with\ngreater control and insight throughout the ideation process. We additionally\nconduct a user study with researchers across diverse disciplines, validating\nthe effectiveness of our system in enhancing ideation. We open-source our code\nat https://github.com/Anikethh/IRIS-Interactive-Research-Ideation-System"}
{"id": "2504.16411", "pdf": "https://arxiv.org/pdf/2504.16411", "abs": "https://arxiv.org/abs/2504.16411", "authors": ["Kosuke Yamada", "Peinan Zhang"], "title": "Out-of-the-Box Conditional Text Embeddings from Large Language Models", "categories": ["cs.CL"], "comment": "work in progress", "summary": "Conditional text embedding is a proposed representation that captures the\nshift in perspective on texts when conditioned on a specific aspect. Previous\nmethods have relied on extensive training data for fine-tuning models, leading\nto challenges in terms of labor and resource costs. We propose PonTE, a novel\nunsupervised conditional text embedding method that leverages a causal large\nlanguage model and a conditional prompt. Through experiments on conditional\nsemantic text similarity and text clustering, we demonstrate that PonTE can\ngenerate useful conditional text embeddings and achieve performance comparable\nto supervised methods without fine-tuning. We also show the interpretability of\ntext embeddings with PonTE by analyzing word generation following prompts and\nembedding visualization."}
{"id": "2504.16427", "pdf": "https://arxiv.org/pdf/2504.16427", "abs": "https://arxiv.org/abs/2504.16427", "authors": ["Hanlei Zhang", "Zhuohang Li", "Yeshuang Zhu", "Hua Xu", "Peiwu Wang", "Jinchao Zhang", "Jie Zhou", "Haige Zhu"], "title": "Can Large Language Models Help Multimodal Language Analysis? MMLA: A Comprehensive Benchmark", "categories": ["cs.CL", "cs.AI", "cs.MM"], "comment": "23 pages, 5 figures", "summary": "Multimodal language analysis is a rapidly evolving field that leverages\nmultiple modalities to enhance the understanding of high-level semantics\nunderlying human conversational utterances. Despite its significance, little\nresearch has investigated the capability of multimodal large language models\n(MLLMs) to comprehend cognitive-level semantics. In this paper, we introduce\nMMLA, a comprehensive benchmark specifically designed to address this gap. MMLA\ncomprises over 61K multimodal utterances drawn from both staged and real-world\nscenarios, covering six core dimensions of multimodal semantics: intent,\nemotion, dialogue act, sentiment, speaking style, and communication behavior.\nWe evaluate eight mainstream branches of LLMs and MLLMs using three methods:\nzero-shot inference, supervised fine-tuning, and instruction tuning. Extensive\nexperiments reveal that even fine-tuned models achieve only about 60%~70%\naccuracy, underscoring the limitations of current MLLMs in understanding\ncomplex human language. We believe that MMLA will serve as a solid foundation\nfor exploring the potential of large language models in multimodal language\nanalysis and provide valuable resources to advance this field. The datasets and\ncode are open-sourced at https://github.com/thuiar/MMLA."}
{"id": "2406.15222", "pdf": "https://arxiv.org/pdf/2406.15222", "abs": "https://arxiv.org/abs/2406.15222", "authors": ["Yujian Hu", "Yilang Xiang", "Yan-Jie Zhou", "Yangyan He", "Dehai Lang", "Shifeng Yang", "Xiaolong Du", "Chunlan Den", "Youyao Xu", "Gaofeng Wang", "Zhengyao Ding", "Jingyong Huang", "Wenjun Zhao", "Xuejun Wu", "Donglin Li", "Qianqian Zhu", "Zhenjiang Li", "Chenyang Qiu", "Ziheng Wu", "Yunjun He", "Chen Tian", "Yihui Qiu", "Zuodong Lin", "Xiaolong Zhang", "Yuan He", "Zhenpeng Yuan", "Xiaoxiang Zhou", "Rong Fan", "Ruihan Chen", "Wenchao Guo", "Jianpeng Zhang", "Tony C. W. Mok", "Zi Li", "Mannudeep K. Kalra", "Le Lu", "Wenbo Xiao", "Xiaoqiang Li", "Yun Bian", "Chengwei Shao", "Guofu Wang", "Wei Lu", "Zhengxing Huang", "Minfeng Xu", "Hongkun Zhang"], "title": "A Deep Learning System for Rapid and Accurate Warning of Acute Aortic Syndrome on Non-contrast CT in China", "categories": ["eess.IV", "cs.AI", "cs.CV"], "comment": null, "summary": "The accurate and timely diagnosis of acute aortic syndromes (AAS) in patients\npresenting with acute chest pain remains a clinical challenge. Aortic CT\nangiography (CTA) is the imaging protocol of choice in patients with suspected\nAAS. However, due to economic and workflow constraints in China, the majority\nof suspected patients initially undergo non-contrast CT as the initial imaging\ntesting, and CTA is reserved for those at higher risk. In this work, we present\nan artificial intelligence-based warning system, iAorta, using non-contrast CT\nfor AAS identification in China, which demonstrates remarkably high accuracy\nand provides clinicians with interpretable warnings. iAorta was evaluated\nthrough a comprehensive step-wise study. In the multi-center retrospective\nstudy (n = 20,750), iAorta achieved a mean area under the receiver operating\ncurve (AUC) of 0.958 (95% CI 0.950-0.967). In the large-scale real-world study\n(n = 137,525), iAorta demonstrated consistently high performance across various\nnon-contrast CT protocols, achieving a sensitivity of 0.913-0.942 and a\nspecificity of 0.991-0.993. In the prospective comparative study (n = 13,846),\niAorta demonstrated the capability to significantly shorten the time to correct\ndiagnostic pathway. For the prospective pilot deployment that we conducted,\niAorta correctly identified 21 out of 22 patients with AAS among 15,584\nconsecutive patients presenting with acute chest pain and under non-contrast CT\nprotocol in the emergency department (ED) and enabled the average diagnostic\ntime of these 21 AAS positive patients to be 102.1 (75-133) mins. Last, the\niAorta can help avoid delayed or missed diagnosis of AAS in settings where\nnon-contrast CT remains the unavoidable the initial or only imaging test in\nresource-constrained regions and in patients who cannot or did not receive\nintravenous contrast."}
{"id": "2504.16134", "pdf": "https://arxiv.org/pdf/2504.16134", "abs": "https://arxiv.org/abs/2504.16134", "authors": ["Mohammad Abu Tami", "Mohammed Elhenawy", "Huthaifa I. Ashqar"], "title": "Multimodal Large Language Models for Enhanced Traffic Safety: A Comprehensive Review and Future Trends", "categories": ["cs.CV", "cs.CL"], "comment": null, "summary": "Traffic safety remains a critical global challenge, with traditional Advanced\nDriver-Assistance Systems (ADAS) often struggling in dynamic real-world\nscenarios due to fragmented sensor processing and susceptibility to adversarial\nconditions. This paper reviews the transformative potential of Multimodal Large\nLanguage Models (MLLMs) in addressing these limitations by integrating\ncross-modal data such as visual, spatial, and environmental inputs to enable\nholistic scene understanding. Through a comprehensive analysis of MLLM-based\napproaches, we highlight their capabilities in enhancing perception,\ndecision-making, and adversarial robustness, while also examining the role of\nkey datasets (e.g., KITTI, DRAMA, ML4RoadSafety) in advancing research.\nFurthermore, we outline future directions, including real-time edge deployment,\ncausality-driven reasoning, and human-AI collaboration. By positioning MLLMs as\na cornerstone for next-generation traffic safety systems, this review\nunderscores their potential to revolutionize the field, offering scalable,\ncontext-aware solutions that proactively mitigate risks and improve overall\nroad safety."}
{"id": "2504.16255", "pdf": "https://arxiv.org/pdf/2504.16255", "abs": "https://arxiv.org/abs/2504.16255", "authors": ["Tina Behzad", "Mithilesh Kumar Singh", "Anthony J. Ripa", "Klaus Mueller"], "title": "FairPlay: A Collaborative Approach to Mitigate Bias in Datasets for Improved AI Fairness", "categories": ["cs.LG", "cs.CY", "cs.HC", "H.5.2; H.5.3; I.2.6"], "comment": "Accepted at ACM CSCW 2025. 30 pages total (including references and\n  supplementary material). Contains 10 figures", "summary": "The issue of fairness in decision-making is a critical one, especially given\nthe variety of stakeholder demands for differing and mutually incompatible\nversions of fairness. Adopting a strategic interaction of perspectives provides\nan alternative to enforcing a singular standard of fairness. We present a\nweb-based software application, FairPlay, that enables multiple stakeholders to\ndebias datasets collaboratively. With FairPlay, users can negotiate and arrive\nat a mutually acceptable outcome without a universally agreed-upon theory of\nfairness. In the absence of such a tool, reaching a consensus would be highly\nchallenging due to the lack of a systematic negotiation process and the\ninability to modify and observe changes. We have conducted user studies that\ndemonstrate the success of FairPlay, as users could reach a consensus within\nabout five rounds of gameplay, illustrating the application's potential for\nenhancing fairness in AI systems."}
{"id": "2504.16736", "pdf": "https://arxiv.org/pdf/2504.16736", "abs": "https://arxiv.org/abs/2504.16736", "authors": ["Yingxuan Yang", "Huacan Chai", "Yuanyi Song", "Siyuan Qi", "Muning Wen", "Ning Li", "Junwei Liao", "Haoyi Hu", "Jianghao Lin", "Gaowei Chang", "Weiwen Liu", "Ying Wen", "Yong Yu", "Weinan Zhang"], "title": "A Survey of AI Agent Protocols", "categories": ["cs.AI"], "comment": null, "summary": "The rapid development of large language models (LLMs) has led to the\nwidespread deployment of LLM agents across diverse industries, including\ncustomer service, content generation, data analysis, and even healthcare.\nHowever, as more LLM agents are deployed, a major issue has emerged: there is\nno standard way for these agents to communicate with external tools or data\nsources. This lack of standardized protocols makes it difficult for agents to\nwork together or scale effectively, and it limits their ability to tackle\ncomplex, real-world tasks. A unified communication protocol for LLM agents\ncould change this. It would allow agents and tools to interact more smoothly,\nencourage collaboration, and triggering the formation of collective\nintelligence. In this paper, we provide a systematic overview of existing\ncommunication protocols for LLM agents. We classify them into four main\ncategories and make an analysis to help users and developers select the most\nsuitable protocols for specific applications. Additionally, we conduct a\ncomparative performance analysis of these protocols across key dimensions such\nas security, scalability, and latency. Finally, we explore future challenges,\nsuch as how protocols can adapt and survive in fast-evolving environments, and\nwhat qualities future protocols might need to support the next generation of\nLLM agent ecosystems. We expect this work to serve as a practical reference for\nboth researchers and engineers seeking to design, evaluate, or integrate robust\ncommunication infrastructures for intelligent agents."}
{"id": "2504.16414", "pdf": "https://arxiv.org/pdf/2504.16414", "abs": "https://arxiv.org/abs/2504.16414", "authors": ["Mohammad Khodadad", "Ali Shiraee Kasmaee", "Mahdi Astaraki", "Nicholas Sherck", "Hamidreza Mahyar", "Soheila Samiee"], "title": "Evaluating Multi-Hop Reasoning in Large Language Models: A Chemistry-Centric Case Study", "categories": ["cs.CL"], "comment": null, "summary": "In this study, we introduced a new benchmark consisting of a curated dataset\nand a defined evaluation process to assess the compositional reasoning\ncapabilities of large language models within the chemistry domain. We designed\nand validated a fully automated pipeline, verified by subject matter experts,\nto facilitate this task. Our approach integrates OpenAI reasoning models with\nnamed entity recognition (NER) systems to extract chemical entities from recent\nliterature, which are then augmented with external knowledge bases to form a\ncomprehensive knowledge graph. By generating multi-hop questions across these\ngraphs, we assess LLM performance in both context-augmented and non-context\naugmented settings. Our experiments reveal that even state-of-the-art models\nface significant challenges in multi-hop compositional reasoning. The results\nreflect the importance of augmenting LLMs with document retrieval, which can\nhave a substantial impact on improving their performance. However, even perfect\nretrieval accuracy with full context does not eliminate reasoning errors,\nunderscoring the complexity of compositional reasoning. This work not only\nbenchmarks and highlights the limitations of current LLMs but also presents a\nnovel data generation pipeline capable of producing challenging reasoning\ndatasets across various domains. Overall, this research advances our\nunderstanding of reasoning in computational linguistics."}
{"id": "2504.16505", "pdf": "https://arxiv.org/pdf/2504.16505", "abs": "https://arxiv.org/abs/2504.16505", "authors": ["Meng Chu", "Yukang Chen", "Haokun Gui", "Shaozuo Yu", "Yi Wang", "Jiaya Jia"], "title": "TraveLLaMA: Facilitating Multi-modal Large Language Models to Understand Urban Scenes and Provide Travel Assistance", "categories": ["cs.CV", "cs.MM"], "comment": null, "summary": "Tourism and travel planning increasingly rely on digital assistance, yet\nexisting multimodal AI systems often lack specialized knowledge and contextual\nunderstanding of urban environments. We present TraveLLaMA, a specialized\nmultimodal language model designed for urban scene understanding and travel\nassistance. Our work addresses the fundamental challenge of developing\npractical AI travel assistants through a novel large-scale dataset of 220k\nquestion-answer pairs. This comprehensive dataset uniquely combines 130k text\nQA pairs meticulously curated from authentic travel forums with GPT-enhanced\nresponses, alongside 90k vision-language QA pairs specifically focused on map\nunderstanding and scene comprehension. Through extensive fine-tuning\nexperiments on state-of-the-art vision-language models (LLaVA, Qwen-VL,\nShikra), we demonstrate significant performance improvements ranging from\n6.5\\%-9.4\\% in both pure text travel understanding and visual question\nanswering tasks. Our model exhibits exceptional capabilities in providing\ncontextual travel recommendations, interpreting map locations, and\nunderstanding place-specific imagery while offering practical information such\nas operating hours and visitor reviews. Comparative evaluations show TraveLLaMA\nsignificantly outperforms general-purpose models in travel-specific tasks,\nestablishing a new benchmark for multi-modal travel assistance systems."}
{"id": "2411.10843", "pdf": "https://arxiv.org/pdf/2411.10843", "abs": "https://arxiv.org/abs/2411.10843", "authors": ["Santhosh Malarvannan", "Pandiyaraju V", "Shravan Venkatraman", "Abeshek A", "Priyadarshini B", "Kannan A"], "title": "A Novel Adaptive Hybrid Focal-Entropy Loss for Enhancing Diabetic Retinopathy Detection Using Convolutional Neural Networks", "categories": ["eess.IV", "cs.AI", "cs.CV", "cs.LG", "68T07, 92C55, 68U10", "I.2.10; I.5.1; J.3"], "comment": "7 pages,7 figures", "summary": "Diabetic retinopathy is a leading cause of blindness around the world and\ndemands precise AI-based diagnostic tools. Traditional loss functions in\nmulti-class classification, such as Categorical Cross-Entropy (CCE), are very\ncommon but break down with class imbalance, especially in cases with inherently\nchallenging or overlapping classes, which leads to biased and less sensitive\nmodels. Since a heavy imbalance exists in the number of examples for higher\nseverity stage 4 diabetic retinopathy, etc., classes compared to those very\nearly stages like class 0, achieving class balance is key. For this purpose, we\npropose the Adaptive Hybrid Focal-Entropy Loss which combines the ideas of\nfocal loss and entropy loss with adaptive weighting in order to focus on\nminority classes and highlight the challenging samples. The state-of-the art\nmodels applied for diabetic retinopathy detection with AHFE revealed good\nperformance improvements, indicating the top performances of ResNet50 at\n99.79%, DenseNet121 at 98.86%, Xception at 98.92%, MobileNetV2 at 97.84%, and\nInceptionV3 at 93.62% accuracy. This sheds light into how AHFE promotes\nenhancement in AI-driven diagnostics for complex and imbalanced medical\ndatasets."}
{"id": "2504.16145", "pdf": "https://arxiv.org/pdf/2504.16145", "abs": "https://arxiv.org/abs/2504.16145", "authors": ["Jingchao Wang", "Hong Wang", "Wenlong Zhang", "Kunhua Ji", "Dingjiang Huang", "Yefeng Zheng"], "title": "Progressive Language-guided Visual Learning for Multi-Task Visual Grounding", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Multi-task visual grounding (MTVG) includes two sub-tasks, i.e., Referring\nExpression Comprehension (REC) and Referring Expression Segmentation (RES). The\nexisting representative approaches generally follow the research pipeline which\nmainly consists of three core procedures, including independent feature\nextraction for visual and linguistic modalities, respectively, cross-modal\ninteraction module, and independent prediction heads for different sub-tasks.\nAlbeit achieving remarkable performance, this research line has two\nlimitations: 1) The linguistic content has not been fully injected into the\nentire visual backbone for boosting more effective visual feature extraction\nand it needs an extra cross-modal interaction module; 2) The relationship\nbetween REC and RES tasks is not effectively exploited to help the\ncollaborative prediction for more accurate output. To deal with these problems,\nin this paper, we propose a Progressive Language-guided Visual Learning\nframework for multi-task visual grounding, called PLVL, which not only finely\nmine the inherent feature expression of the visual modality itself but also\nprogressively inject the language information to help learn linguistic-related\nvisual features. In this manner, our PLVL does not need additional cross-modal\nfusion module while fully introducing the language guidance. Furthermore, we\nanalyze that the localization center for REC would help identify the\nto-be-segmented object region for RES to some extent. Inspired by this\ninvestigation, we design a multi-task head to accomplish collaborative\npredictions for these two sub-tasks. Extensive experiments conducted on several\nbenchmark datasets comprehensively substantiate that our PLVL obviously\noutperforms the representative methods in both REC and RES tasks.\nhttps://github.com/jcwang0602/PLVL"}
{"id": "2504.16262", "pdf": "https://arxiv.org/pdf/2504.16262", "abs": "https://arxiv.org/abs/2504.16262", "authors": ["Junn Yong Loo", "Michelle Adeline", "Julia Kaiwen Lau", "Fang Yu Leong", "Hwa Hui Tew", "Arghya Pal", "Vishnu Monn Baskaran", "Chee-Ming Ting", "Raphaël C. -W. Phan"], "title": "Learning Energy-Based Generative Models via Potential Flow: A Variational Principle Approach to Probability Density Homotopy Matching", "categories": ["cs.LG"], "comment": "Accepted by Transactions on Machine Learning Research (TMLR)", "summary": "Energy-based models (EBMs) are a powerful class of probabilistic generative\nmodels due to their flexibility and interpretability. However, relationships\nbetween potential flows and explicit EBMs remain underexplored, while\ncontrastive divergence training via implicit Markov chain Monte Carlo (MCMC)\nsampling is often unstable and expensive in high-dimensional settings. In this\npaper, we propose Variational Potential Flow Bayes (VPFB), a new energy-based\ngenerative framework that eliminates the need for implicit MCMC sampling and\ndoes not rely on auxiliary networks or cooperative training. VPFB learns an\nenergy-parameterized potential flow by constructing a flow-driven density\nhomotopy that is matched to the data distribution through a variational loss\nminimizing the Kullback-Leibler divergence between the flow-driven and marginal\nhomotopies. This principled formulation enables robust and efficient generative\nmodeling while preserving the interpretability of EBMs. Experimental results on\nimage generation, interpolation, out-of-distribution detection, and\ncompositional generation confirm the effectiveness of VPFB, showing that our\nmethod performs competitively with existing approaches in terms of sample\nquality and versatility across diverse generative modeling tasks."}
{"id": "2504.16760", "pdf": "https://arxiv.org/pdf/2504.16760", "abs": "https://arxiv.org/abs/2504.16760", "authors": ["Bartosz Piotrowski", "Witold Drzewakowski", "Konrad Staniszewski", "Piotr Miłoś"], "title": "Lightweight Latent Verifiers for Efficient Meta-Generation Strategies", "categories": ["cs.AI"], "comment": null, "summary": "Verifiers are auxiliary models that assess the correctness of outputs\ngenerated by base large language models (LLMs). They play a crucial role in\nmany strategies for solving reasoning-intensive problems with LLMs. Typically,\nverifiers are LLMs themselves, often as large (or larger) than the base model\nthey support, making them computationally expensive. In this work, we introduce\na novel lightweight verification approach, LiLaVe, which reliably extracts\ncorrectness signals from the hidden states of the base LLM. A key advantage of\nLiLaVe is its ability to operate with only a small fraction of the\ncomputational budget required by traditional LLM-based verifiers. To\ndemonstrate its practicality, we couple LiLaVe with popular meta-generation\nstrategies, like best-of-n or self-consistency. Moreover, we design novel\nLiLaVe-based approaches, like conditional self-correction or conditional\nmajority voting, that significantly improve both accuracy and efficiency in\ngeneration tasks with smaller LLMs. Our work demonstrates the fruitfulness of\nextracting latent information from the hidden states of LLMs, and opens the\ndoor to scalable and resource-efficient solutions for reasoning-intensive\napplications."}
{"id": "2504.16448", "pdf": "https://arxiv.org/pdf/2504.16448", "abs": "https://arxiv.org/abs/2504.16448", "authors": ["Shuguang Zhao", "Qiangzhong Feng", "Zhiyang He", "Peipei Sun", "Yingying Wang", "Xiaodong Tao", "Xiaoliang Lu", "Mei Cheng", "Xinyue Wu", "Yanyan Wang", "Wei Liang"], "title": "EMRModel: A Large Language Model for Extracting Medical Consultation Dialogues into Structured Medical Records", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Medical consultation dialogues contain critical clinical information, yet\ntheir unstructured nature hinders effective utilization in diagnosis and\ntreatment. Traditional methods, relying on rule-based or shallow machine\nlearning techniques, struggle to capture deep and implicit semantics. Recently,\nlarge pre-trained language models and Low-Rank Adaptation (LoRA), a lightweight\nfine-tuning method, have shown promise for structured information extraction.\nWe propose EMRModel, a novel approach that integrates LoRA-based fine-tuning\nwith code-style prompt design, aiming to efficiently convert medical\nconsultation dialogues into structured electronic medical records (EMRs).\nAdditionally, we construct a high-quality, realistically grounded dataset of\nmedical consultation dialogues with detailed annotations. Furthermore, we\nintroduce a fine-grained evaluation benchmark for medical consultation\ninformation extraction and provide a systematic evaluation methodology,\nadvancing the optimization of medical natural language processing (NLP) models.\nExperimental results show EMRModel achieves an F1 score of 88.1%, improving\nby49.5% over standard pre-trained models. Compared to traditional LoRA\nfine-tuning methods, our model shows superior performance, highlighting its\neffectiveness in structured medical record extraction tasks."}
{"id": "2404.04545", "pdf": "https://arxiv.org/pdf/2404.04545", "abs": "https://arxiv.org/abs/2404.04545", "authors": ["Weize Quan", "Yunfei Feng", "Ming Zhou", "Yunzhen Zhao", "Tong Wang", "Dong-Ming Yan"], "title": "TCAN: Text-oriented Cross Attention Network for Multimodal Sentiment Analysis", "categories": ["cs.MM", "cs.CL"], "comment": null, "summary": "Multimodal Sentiment Analysis (MSA) endeavors to understand human sentiment\nby leveraging language, visual, and acoustic modalities. Despite the remarkable\nperformance exhibited by previous MSA approaches, the presence of inherent\nmultimodal heterogeneities poses a challenge, with the contribution of\ndifferent modalities varying considerably. Past research predominantly focused\non improving representation learning techniques and feature fusion strategies.\nHowever, many of these efforts overlooked the variation in semantic richness\namong different modalities, treating each modality uniformly. This approach may\nlead to underestimating the significance of strong modalities while\noveremphasizing the importance of weak ones. Motivated by these insights, we\nintroduce a Text-oriented Cross-Attention Network (TCAN), emphasizing the\npredominant role of the text modality in MSA. Specifically, for each multimodal\nsample, by taking unaligned sequences of the three modalities as inputs, we\ninitially allocate the extracted unimodal features into a visual-text and an\nacoustic-text pair. Subsequently, we implement self-attention on the text\nmodality and apply text-queried cross-attention to the visual and acoustic\nmodalities. To mitigate the influence of noise signals and redundant features,\nwe incorporate a gated control mechanism into the framework. Additionally, we\nintroduce unimodal joint learning to gain a deeper understanding of homogeneous\nemotional tendencies across diverse modalities through backpropagation.\nExperimental results demonstrate that TCAN consistently outperforms\nstate-of-the-art MSA methods on two datasets (CMU-MOSI and CMU-MOSEI)."}
{"id": "2411.12919", "pdf": "https://arxiv.org/pdf/2411.12919", "abs": "https://arxiv.org/abs/2411.12919", "authors": ["Asad Aali", "Marius Arvinte", "Sidharth Kumar", "Yamin I. Arefeen", "Jonathan I. Tamir"], "title": "Robust multi-coil MRI reconstruction via self-supervised denoising", "categories": ["eess.IV", "cs.AI", "cs.CV", "cs.LG"], "comment": null, "summary": "We study the effect of incorporating self-supervised denoising as a\npre-processing step for training deep learning (DL) based reconstruction\nmethods on data corrupted by Gaussian noise. K-space data employed for training\nare typically multi-coil and inherently noisy. Although DL-based reconstruction\nmethods trained on fully sampled data can enable high reconstruction quality,\nobtaining large, noise-free datasets is impractical. We leverage Generalized\nStein's Unbiased Risk Estimate (GSURE) for denoising. We evaluate two DL-based\nreconstruction methods: Diffusion Probabilistic Models (DPMs) and Model-Based\nDeep Learning (MoDL). We evaluate the impact of denoising on the performance of\nthese DL-based methods in solving accelerated multi-coil magnetic resonance\nimaging (MRI) reconstruction. The experiments were carried out on T2-weighted\nbrain and fat-suppressed proton-density knee scans. We observed that\nself-supervised denoising enhances the quality and efficiency of MRI\nreconstructions across various scenarios. Specifically, employing denoised\nimages rather than noisy counterparts when training DL networks results in\nlower normalized root mean squared error (NRMSE), higher structural similarity\nindex measure (SSIM) and peak signal-to-noise ratio (PSNR) across different SNR\nlevels, including 32dB, 22dB, and 12dB for T2-weighted brain data, and 24dB,\n14dB, and 4dB for fat-suppressed knee data. Overall, we showed that denoising\nis an essential pre-processing technique capable of improving the efficacy of\nDL-based MRI reconstruction methods under diverse conditions. By refining the\nquality of input data, denoising enables training more effective DL networks,\npotentially bypassing the need for noise-free reference MRI scans."}
{"id": "2504.16150", "pdf": "https://arxiv.org/pdf/2504.16150", "abs": "https://arxiv.org/abs/2504.16150", "authors": ["Sarah Day", "Jesse Dimino", "Matt Jester", "Kaitlin Keegan", "Thomas Weighill"], "title": "Classification of Firn Data via Topological Features", "categories": ["cs.CV", "math.AT", "55N31, 68T45"], "comment": null, "summary": "In this paper we evaluate the performance of topological features for\ngeneralizable and robust classification of firn image data, with the broader\ngoal of understanding the advantages, pitfalls, and trade-offs in topological\nfeaturization. Firn refers to layers of granular snow within glaciers that\nhaven't been compressed into ice. This compactification process imposes\ndistinct topological and geometric structure on firn that varies with depth\nwithin the firn column, making topological data analysis (TDA) a natural choice\nfor understanding the connection between depth and structure. We use two\nclasses of topological features, sublevel set features and distance transform\nfeatures, together with persistence curves, to predict sample depth from\nmicroCT images. A range of challenging training-test scenarios reveals that no\none choice of method dominates in all categories, and uncoveres a web of\ntrade-offs between accuracy, interpretability, and generalizability."}
{"id": "2504.16263", "pdf": "https://arxiv.org/pdf/2504.16263", "abs": "https://arxiv.org/abs/2504.16263", "authors": ["Magnus Sieverding", "Nathan Steffen", "Kelly Cohen"], "title": "Gradient-Optimized Fuzzy Classifier: A Benchmark Study Against State-of-the-Art Models", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "This paper presents a performance benchmarking study of a Gradient-Optimized\nFuzzy Inference System (GF) classifier against several state-of-the-art machine\nlearning models, including Random Forest, XGBoost, Logistic Regression, Support\nVector Machines, and Neural Networks. The evaluation was conducted across five\ndatasets from the UCI Machine Learning Repository, each chosen for their\ndiversity in input types, class distributions, and classification complexity.\nUnlike traditional Fuzzy Inference Systems that rely on derivative-free\noptimization methods, the GF leverages gradient descent to significantly\nimproving training efficiency and predictive performance. Results demonstrate\nthat the GF model achieved competitive, and in several cases superior,\nclassification accuracy while maintaining high precision and exceptionally low\ntraining times. In particular, the GF exhibited strong consistency across folds\nand datasets, underscoring its robustness in handling noisy data and variable\nfeature sets. These findings support the potential of gradient optimized fuzzy\nsystems as interpretable, efficient, and adaptable alternatives to more complex\ndeep learning models in supervised learning tasks."}
{"id": "2504.16891", "pdf": "https://arxiv.org/pdf/2504.16891", "abs": "https://arxiv.org/abs/2504.16891", "authors": ["Ivan Moshkov", "Darragh Hanley", "Ivan Sorokin", "Shubham Toshniwal", "Christof Henkel", "Benedikt Schifferer", "Wei Du", "Igor Gitman"], "title": "AIMO-2 Winning Solution: Building State-of-the-Art Mathematical Reasoning Models with OpenMathReasoning dataset", "categories": ["cs.AI", "cs.CL", "cs.LG"], "comment": "Report of AIMO-2 winning submission", "summary": "This paper presents our winning submission to the AI Mathematical Olympiad -\nProgress Prize 2 (AIMO-2) competition. Our recipe for building state-of-the-art\nmathematical reasoning models relies on three key pillars. First, we create a\nlarge-scale dataset comprising 540K unique high-quality math problems,\nincluding olympiad-level problems, and their 3.2M long-reasoning solutions.\nSecond, we develop a novel method to integrate code execution with long\nreasoning models through iterative training, generation, and quality filtering,\nresulting in 1.7M high-quality Tool-Integrated Reasoning solutions. Third, we\ncreate a pipeline to train models to select the most promising solution from\nmany candidates. We show that such generative solution selection (GenSelect)\ncan significantly improve upon majority voting baseline. Combining these ideas,\nwe train a series of models that achieve state-of-the-art results on\nmathematical reasoning benchmarks. To facilitate further research, we release\nour code, models, and the complete OpenMathReasoning dataset under a\ncommercially permissive license."}
{"id": "2504.16460", "pdf": "https://arxiv.org/pdf/2504.16460", "abs": "https://arxiv.org/abs/2504.16460", "authors": ["Vignesh Ethiraj", "Sidhanth Menon", "Divya Vijay"], "title": "T-VEC: A Telecom-Specific Vectorization Model with Enhanced Semantic Understanding via Deep Triplet Loss Fine-Tuning", "categories": ["cs.CL", "cs.AI", "68T50"], "comment": "Introduces T-VEC, a telecom-specific text embedding model. Fine-tuned\n  gte-Qwen2-1.5B-instruct on curated telecom data points. Includes the first\n  open-source telecom tokenizer. Model available at\n  https://huggingface.co/NetoAISolutions/T-VEC", "summary": "The specialized vocabulary and complex concepts of the telecommunications\nindustry present significant challenges for standard Natural Language\nProcessing models. Generic text embeddings often fail to capture\ntelecom-specific semantics, hindering downstream task performance. We introduce\nT-VEC (Telecom Vectorization Model), a novel embedding model tailored for the\ntelecom domain through deep fine-tuning. Developed by NetoAI, T-VEC is created\nby adapting the state-of-the-art gte-Qwen2-1.5B-instruct model using a triplet\nloss objective on a meticulously curated, large-scale dataset of\ntelecom-specific data. Crucially, this process involved substantial\nmodification of weights across 338 layers of the base model, ensuring deep\nintegration of domain knowledge, far exceeding superficial adaptation\ntechniques. We quantify this deep change via weight difference analysis. A key\ncontribution is the development and open-sourcing (MIT License) of the first\ndedicated telecom-specific tokenizer, enhancing the handling of industry\njargon. T-VEC achieves a leading average MTEB score (0.825) compared to\nestablished models and demonstrates vastly superior performance (0.9380 vs.\nless than 0.07) on our internal telecom-specific triplet evaluation benchmark,\nindicating an exceptional grasp of domain-specific nuances, visually confirmed\nby improved embedding separation. This work positions NetoAI at the forefront\nof telecom AI innovation, providing the community with a powerful, deeply\nadapted, open-source tool."}
{"id": "2405.07229", "pdf": "https://arxiv.org/pdf/2405.07229", "abs": "https://arxiv.org/abs/2405.07229", "authors": ["Xiaocui Yang", "Wenfang Wu", "Shi Feng", "Ming Wang", "Daling Wang", "Yang Li", "Qi Sun", "Yifei Zhang", "Xiaoming Fu", "Soujanya Poria"], "title": "MM-InstructEval: Zero-Shot Evaluation of (Multimodal) Large Language Models on Multimodal Reasoning Tasks", "categories": ["cs.MM"], "comment": "Accepted by the Information Fusion Journal", "summary": "The emergence of multimodal large language models (MLLMs) has triggered\nextensive research in model evaluation. While existing evaluation studies\nprimarily focus on unimodal (vision-only) comprehension and reasoning\ncapabilities, they overlook critical assessments of complex multimodal\nreasoning tasks that require integrated understanding of both visual and\ntextual contexts. Such multimodal tasks present unique challenges, demanding\nsophisticated reasoning across multiple modalities and deep comprehension of\nmultimodal contexts. In this paper, we present MM-InstructEval, a comprehensive\nevaluation framework that incorporates diverse metrics to assess model\nperformance across various multimodal reasoning tasks with vision-text\ncontexts. We conduct extensive zero-shot evaluations on 45 models (including 36\nMLLMs) across 16 multimodal datasets, encompassing 6 distinct tasks using 10\ndifferent instructions. Our framework introduces multiple innovative metrics,\nincluding the 'Best Performance' metric to benchmark peak model capabilities,\nthe 'Mean Relative Gain' metric to assess overall efficacy across models and\ninstructions, the 'Stability' metric to measure robustness, and the\n'Adaptability' metric to quantify the compatibility between models and\ninstructions. Through comprehensive evaluation and analysis, we uncover several\nsignificant insights about model architectures, instruction formats, and their\ninteractions in multimodal reasoning tasks. Our findings establish new\nbenchmarks for assessing the reasoning capabilities of MLLMs and provide\nstrategic guidance for future developments. To facilitate continued research\nand evaluation in this field, we release our framework and resources at\nhttps://github.com/declare-lab/MM-InstructEval, with an interactive leaderboard\navailable at MM-InstructEval Leaderboard\n(https://declare-lab.github.io/MM-InstructEval/)."}
{"id": "2501.15414", "pdf": "https://arxiv.org/pdf/2501.15414", "abs": "https://arxiv.org/abs/2501.15414", "authors": ["Weixuan Chen", "Qianqian Yang", "Yuhao Chen", "Chongwen Huang", "Qian Wang", "Zehui Xiong", "Zhaoyang Zhang"], "title": "Semantic Communication with Entropy-and-Channel-Adaptive Rate Control over Multi-User MIMO Fading Channels", "categories": ["eess.IV"], "comment": null, "summary": "Although significant improvements in transmission efficiency have been\nachieved, existing semantic communication (SemCom) methods typically use a\nfixed transmission rate for varying channel conditions and transmission\ncontents, leading to performance degradation under harsh channel conditions. To\naddress these challenges, we propose a novel SemCom method for wireless image\ntransmission that integrates entropy-andchannel-adaptive rate control\nmechanism, specifically designed for multi-user multiple-input multiple-output\n(MU-MIMO) fading channels. Unlike existing methods, our system dynamically\nadjusts transmission rates by leveraging the entropy of feature maps, channel\nstate information (CSI), and signal-to-noise ratio (SNR), ensuring optimal\ncommunication resource usage. It incorporates feature map pruning, channel\nattention, spatial attention, and multi-head self-attention (MHSA) to\neffectively prioritize critical semantic features while minimizing unnecessary\ntransmission overhead. Experimental results demonstrate that the proposed\nsystem outperforms separated source and channel coding and deep joint source\nand channel coding (Deep JSCC), in terms of rate-distortion performance,\nflexibility, and robustness, particularly in challenging scenarios such as low\nSNR, imperfect CSI, and inter-user interference."}
{"id": "2504.16171", "pdf": "https://arxiv.org/pdf/2504.16171", "abs": "https://arxiv.org/abs/2504.16171", "authors": ["Zezhang Yang", "Zitong Yu", "Nuri Choi", "Abhinav K. Jha"], "title": "A detection-task-specific deep-learning method to improve the quality of sparse-view myocardial perfusion SPECT images", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Myocardial perfusion imaging (MPI) with single-photon emission computed\ntomography (SPECT) is a widely used and cost-effective diagnostic tool for\ncoronary artery disease. However, the lengthy scanning time in this imaging\nprocedure can cause patient discomfort, motion artifacts, and potentially\ninaccurate diagnoses due to misalignment between the SPECT scans and the\nCT-scans which are acquired for attenuation compensation. Reducing projection\nangles is a potential way to shorten scanning time, but this can adversely\nimpact the quality of the reconstructed images. To address this issue, we\npropose a detection-task-specific deep-learning method for sparse-view MPI\nSPECT images. This method integrates an observer loss term that penalizes the\nloss of anthropomorphic channel features with the goal of improving performance\nin perfusion defect-detection task. We observed that, on the task of detecting\nmyocardial perfusion defects, the proposed method yielded an area under the\nreceiver operating characteristic (ROC) curve (AUC) significantly larger than\nthe sparse-view protocol. Further, the proposed method was observed to be able\nto restore the structure of the left ventricle wall, demonstrating ability to\novercome sparse-sampling artifacts. Our preliminary results motivate further\nevaluations of the method."}
{"id": "2504.16268", "pdf": "https://arxiv.org/pdf/2504.16268", "abs": "https://arxiv.org/abs/2504.16268", "authors": ["Abdesslem Layeb"], "title": "Boosting Classifier Performance with Opposition-Based Data Transformation", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "In this paper, we introduce a novel data transformation framework based on\nOpposition-Based Learning (OBL) to boost the performance of traditional\nclassification algorithms. Originally developed to accelerate convergence in\noptimization tasks, OBL is leveraged here to generate synthetic opposite\nsamples that replace the acutely training data and improve decision boundary\nformation. We explore three OBL variants; Global OBL, Class-Wise OBL, and\nLocalized Class-Wise OBL; and integrate them with several widely used\nclassifiers, including K-Nearest Neighbors (KNN), Support Vector Machines\n(SVM), Logistic Regression (LR), and Decision Tree (DT). Extensive experiments\nconducted on 26 heterogeneous and high-dimensional datasets demonstrate that\nOBL-enhanced classifiers consistently outperform their standard counterparts in\nterms of accuracy and F1-score, frequently achieving near-perfect or perfect\nclassification. Furthermore, OBL contributes to improved computational\nefficiency, particularly in SVM and LR. These findings underscore the potential\nof OBL as a lightweight yet powerful data transformation strategy for enhancing\nclassification performance, especially in complex or sparse learning\nenvironments."}
{"id": "2504.16093", "pdf": "https://arxiv.org/pdf/2504.16093", "abs": "https://arxiv.org/abs/2504.16093", "authors": ["Yurun Ge", "Lucas Böttcher", "Tom Chou", "Maria R. D'Orsogna"], "title": "Efficient Portfolio Selection through Preference Aggregation with Quicksort and the Bradley--Terry Model", "categories": ["q-fin.PM", "cs.AI", "math.PR", "60-08, 90-08", "G.3; I.6.1; J.4"], "comment": "15pp, 4 figs", "summary": "How to allocate limited resources to projects that will yield the greatest\nlong-term benefits is a problem that often arises in decision-making under\nuncertainty. For example, organizations may need to evaluate and select\ninnovation projects with risky returns. Similarly, when allocating resources to\nresearch projects, funding agencies are tasked with identifying the most\npromising proposals based on idiosyncratic criteria. Finally, in participatory\nbudgeting, a local community may need to select a subset of public projects to\nfund. Regardless of context, agents must estimate the uncertain values of a\npotentially large number of projects. Developing parsimonious methods to\ncompare these projects, and aggregating agent evaluations so that the overall\nbenefit is maximized, are critical in assembling the best project portfolio.\nUnlike in standard sorting algorithms, evaluating projects on the basis of\nuncertain long-term benefits introduces additional complexities. We propose\ncomparison rules based on Quicksort and the Bradley--Terry model, which\nconnects rankings to pairwise \"win\" probabilities. In our model, each agent\ndetermines win probabilities of a pair of projects based on his or her specific\nevaluation of the projects' long-term benefit. The win probabilities are then\nappropriately aggregated and used to rank projects. Several of the methods we\npropose perform better than the two most effective aggregation methods\ncurrently available. Additionally, our methods can be combined with sampling\ntechniques to significantly reduce the number of pairwise comparisons. We also\ndiscuss how the Bradley--Terry portfolio selection approach can be implemented\nin practice."}
{"id": "2504.16511", "pdf": "https://arxiv.org/pdf/2504.16511", "abs": "https://arxiv.org/abs/2504.16511", "authors": ["Fengze Liu", "Weidong Zhou", "Binbin Liu", "Zhimiao Yu", "Yifan Zhang", "Haobin Lin", "Yifeng Yu", "Xiaohuan Zhou", "Taifeng Wang", "Yong Cao"], "title": "QuaDMix: Quality-Diversity Balanced Data Selection for Efficient LLM Pretraining", "categories": ["cs.CL"], "comment": null, "summary": "Quality and diversity are two critical metrics for the training data of large\nlanguage models (LLMs), positively impacting performance. Existing studies\noften optimize these metrics separately, typically by first applying quality\nfiltering and then adjusting data proportions. However, these approaches\noverlook the inherent trade-off between quality and diversity, necessitating\ntheir joint consideration. Given a fixed training quota, it is essential to\nevaluate both the quality of each data point and its complementary effect on\nthe overall dataset. In this paper, we introduce a unified data selection\nframework called QuaDMix, which automatically optimizes the data distribution\nfor LLM pretraining while balancing both quality and diversity. Specifically,\nwe first propose multiple criteria to measure data quality and employ domain\nclassification to distinguish data points, thereby measuring overall diversity.\nQuaDMix then employs a unified parameterized data sampling function that\ndetermines the sampling probability of each data point based on these quality\nand diversity related labels. To accelerate the search for the optimal\nparameters involved in the QuaDMix framework, we conduct simulated experiments\non smaller models and use LightGBM for parameters searching, inspired by the\nRegMix method. Our experiments across diverse models and datasets demonstrate\nthat QuaDMix achieves an average performance improvement of 7.2% across\nmultiple benchmarks. These results outperform the independent strategies for\nquality and diversity, highlighting the necessity and ability to balance data\nquality and diversity."}
{"id": "2504.11512", "pdf": "https://arxiv.org/pdf/2504.11512", "abs": "https://arxiv.org/abs/2504.11512", "authors": ["Sara Sippola", "Siiri Rautio", "Andreas Hauptmann", "Takanori Ide", "Samuli Siltanen"], "title": "Learned enclosure method for experimental EIT data", "categories": ["eess.IV", "cs.LG", "math.AP"], "comment": null, "summary": "Electrical impedance tomography (EIT) is a non-invasive imaging method with\ndiverse applications, including medical imaging and non-destructive testing.\nThe inverse problem of reconstructing internal electrical conductivity from\nboundary measurements is nonlinear and highly ill-posed, making it difficult to\nsolve accurately. In recent years, there has been growing interest in combining\nanalytical methods with machine learning to solve inverse problems. In this\npaper, we propose a method for estimating the convex hull of inclusions from\nboundary measurements by combining the enclosure method proposed by Ikehata\nwith neural networks. We demonstrate its performance using experimental data.\nCompared to the classical enclosure method with least squares fitting, the\nlearned convex hull achieves superior performance on both simulated and\nexperimental data."}
{"id": "2504.16181", "pdf": "https://arxiv.org/pdf/2504.16181", "abs": "https://arxiv.org/abs/2504.16181", "authors": ["Banafsheh Karimian", "Giulia Avanzato", "Soufian Belharbi", "Luke McCaffrey", "Mohammadhadi Shateri", "Eric Granger"], "title": "CLIP-IT: CLIP-based Pairing for Histology Images Classification", "categories": ["cs.CV"], "comment": null, "summary": "Multimodal learning has shown significant promise for improving medical image\nanalysis by integrating information from complementary data sources. This is\nwidely employed for training vision-language models (VLMs) for cancer detection\nbased on histology images and text reports. However, one of the main\nlimitations in training these VLMs is the requirement for large paired\ndatasets, raising concerns over privacy, and data collection, annotation, and\nmaintenance costs. To address this challenge, we introduce CLIP-IT method to\ntrain a vision backbone model to classify histology images by pairing them with\nprivileged textual information from an external source. At first, the modality\npairing step relies on a CLIP-based model to match histology images with\nsemantically relevant textual report data from external sources, creating an\naugmented multimodal dataset without the need for manually paired samples.\nThen, we propose a multimodal training procedure that distills the knowledge\nfrom the paired text modality to the unimodal image classifier for enhanced\nperformance without the need for the textual data during inference. A\nparameter-efficient fine-tuning method is used to efficiently address the\nmisalignment between the main (image) and paired (text) modalities. During\ninference, the improved unimodal histology classifier is used, with only\nminimal additional computational complexity. Our experiments on challenging\nPCAM, CRC, and BACH histology image datasets show that CLIP-IT can provide a\ncost-effective approach to leverage privileged textual information and\noutperform unimodal classifiers for histology."}
{"id": "2504.16272", "pdf": "https://arxiv.org/pdf/2504.16272", "abs": "https://arxiv.org/abs/2504.16272", "authors": ["Ryan Koo", "Ian Yang", "Vipul Raheja", "Mingyi Hong", "Kwang-Sung Jun", "Dongyeop Kang"], "title": "Learning Explainable Dense Reward Shapes via Bayesian Optimization", "categories": ["cs.LG"], "comment": null, "summary": "Current reinforcement learning from human feedback (RLHF) pipelines for large\nlanguage model (LLM) alignment typically assign scalar rewards to sequences,\nusing the final token as a surrogate indicator for the quality of the entire\nsequence. However, this leads to sparse feedback and suboptimal token-level\ncredit assignment. In this work, we frame reward shaping as an optimization\nproblem focused on token-level credit assignment. We propose a reward-shaping\nfunction leveraging explainability methods such as SHAP and LIME to estimate\nper-token rewards from the reward model. To learn parameters of this shaping\nfunction, we employ a bilevel optimization framework that integrates Bayesian\nOptimization and policy training to handle noise from the token reward\nestimates. Our experiments show that achieving a better balance of token-level\nreward attribution leads to performance improvements over baselines on\ndownstream tasks and finds an optimal policy faster during training.\nFurthermore, we show theoretically that explainability methods that are feature\nadditive attribution functions maintain the optimal policy as the original\nreward."}
{"id": "2504.16096", "pdf": "https://arxiv.org/pdf/2504.16096", "abs": "https://arxiv.org/abs/2504.16096", "authors": ["Jiaxing Xu", "Kai He", "Yue Tang", "Wei Li", "Mengcheng Lan", "Xia Dong", "Yiping Ke", "Mengling Feng"], "title": "BrainPrompt: Multi-Level Brain Prompt Enhancement for Neurological Condition Identification", "categories": ["q-bio.NC", "cs.AI", "cs.CV"], "comment": null, "summary": "Neurological conditions, such as Alzheimer's Disease, are challenging to\ndiagnose, particularly in the early stages where symptoms closely resemble\nhealthy controls. Existing brain network analysis methods primarily focus on\ngraph-based models that rely solely on imaging data, which may overlook\nimportant non-imaging factors and limit the model's predictive power and\ninterpretability. In this paper, we present BrainPrompt, an innovative\nframework that enhances Graph Neural Networks (GNNs) by integrating Large\nLanguage Models (LLMs) with knowledge-driven prompts, enabling more effective\ncapture of complex, non-imaging information and external knowledge for\nneurological disease identification. BrainPrompt integrates three types of\nknowledge-driven prompts: (1) ROI-level prompts to encode the identity and\nfunction of each brain region, (2) subject-level prompts that incorporate\ndemographic information, and (3) disease-level prompts to capture the temporal\nprogression of disease. By leveraging these multi-level prompts, BrainPrompt\neffectively harnesses knowledge-enhanced multi-modal information from LLMs,\nenhancing the model's capability to predict neurological disease stages and\nmeanwhile offers more interpretable results. We evaluate BrainPrompt on two\nresting-state functional Magnetic Resonance Imaging (fMRI) datasets from\nneurological disorders, showing its superiority over state-of-the-art methods.\nAdditionally, a biomarker study demonstrates the framework's ability to extract\nvaluable and interpretable information aligned with domain knowledge in\nneuroscience."}
{"id": "2504.16537", "pdf": "https://arxiv.org/pdf/2504.16537", "abs": "https://arxiv.org/abs/2504.16537", "authors": ["Hong Ting Tsang", "Zihao Wang", "Yangqiu Song"], "title": "Transformers for Complex Query Answering over Knowledge Hypergraphs", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Complex Query Answering (CQA) has been extensively studied in recent years.\nIn order to model data that is closer to real-world distribution, knowledge\ngraphs with different modalities have been introduced. Triple KGs, as the\nclassic KGs composed of entities and relations of arity 2, have limited\nrepresentation of real-world facts. Real-world data is more sophisticated.\nWhile hyper-relational graphs have been introduced, there are limitations in\nrepresenting relationships of varying arity that contain entities with equal\ncontributions. To address this gap, we sampled new CQA datasets: JF17k-HCQA and\nM-FB15k-HCQA. Each dataset contains various query types that include logical\noperations such as projection, negation, conjunction, and disjunction. In order\nto answer knowledge hypergraph (KHG) existential first-order queries, we\npropose a two-stage transformer model, the Logical Knowledge Hypergraph\nTransformer (LKHGT), which consists of a Projection Encoder for atomic\nprojection and a Logical Encoder for complex logical operations. Both encoders\nare equipped with Type Aware Bias (TAB) for capturing token interactions.\nExperimental results on CQA datasets show that LKHGT is a state-of-the-art CQA\nmethod over KHG and is able to generalize to out-of-distribution query types."}
{"id": "2504.13340", "pdf": "https://arxiv.org/pdf/2504.13340", "abs": "https://arxiv.org/abs/2504.13340", "authors": ["Oliver Mills", "Philip Conaghan", "Nishant Ravikumar", "Samuel Relton"], "title": "Putting the Segment Anything Model to the Test with 3D Knee MRI -- A Comparison with State-of-the-Art Performance", "categories": ["eess.IV", "cs.AI", "cs.CV"], "comment": "Work accepted at BMVC 2024. Minor changes to the camera-ready version\n  since acceptance include a corrected running header and the addition of an\n  Acknowledgments section (including code availability)", "summary": "Menisci are cartilaginous tissue found within the knee that contribute to\njoint lubrication and weight dispersal. Damage to menisci can lead to onset and\nprogression of knee osteoarthritis (OA), a condition that is a leading cause of\ndisability, and for which there are few effective therapies. Accurate automated\nsegmentation of menisci would allow for earlier detection and treatment of\nmeniscal abnormalities, as well as shedding more light on the role the menisci\nplay in OA pathogenesis. Focus in this area has mainly used variants of\nconvolutional networks, but there has been no attempt to utilise recent large\nvision transformer segmentation models. The Segment Anything Model (SAM) is a\nso-called foundation segmentation model, which has been found useful across a\nrange of different tasks due to the large volume of data used for training the\nmodel. In this study, SAM was adapted to perform fully-automated segmentation\nof menisci from 3D knee magnetic resonance images. A 3D U-Net was also trained\nas a baseline. It was found that, when fine-tuning only the decoder, SAM was\nunable to compete with 3D U-Net, achieving a Dice score of $0.81\\pm0.03$,\ncompared to $0.87\\pm0.03$, on a held-out test set. When fine-tuning SAM\nend-to-end, a Dice score of $0.87\\pm0.03$ was achieved. The performance of both\nthe end-to-end trained SAM configuration and the 3D U-Net were comparable to\nthe winning Dice score ($0.88\\pm0.03$) in the IWOAI Knee MRI Segmentation\nChallenge 2019. Performance in terms of the Hausdorff Distance showed that both\nconfigurations of SAM were inferior to 3D U-Net in matching the meniscus\nmorphology. Results demonstrated that, despite its generalisability, SAM was\nunable to outperform a basic 3D U-Net in meniscus segmentation, and may not be\nsuitable for similar 3D medical image segmentation tasks also involving fine\nanatomical structures with low contrast and poorly-defined boundaries."}
{"id": "2504.16242", "pdf": "https://arxiv.org/pdf/2504.16242", "abs": "https://arxiv.org/abs/2504.16242", "authors": ["Henry Marichal", "Verónica Casaravilla", "Candice Power", "Karolain Mello", "Joaquín Mazarino", "Christine Lucas", "Ludmila Profumo", "Diego Passarella", "Gregory Randall"], "title": "DeepCS-TRD, a Deep Learning-based Cross-Section Tree Ring Detector", "categories": ["cs.CV"], "comment": "12 pages, 6 figures. Accepted in ICIAP 2025", "summary": "Here, we propose Deep CS-TRD, a new automatic algorithm for detecting tree\nrings in whole cross-sections. It substitutes the edge detection step of CS-TRD\nby a deep-learning-based approach (U-Net), which allows the application of the\nmethod to different image domains: microscopy, scanner or smartphone acquired,\nand species (Pinus taeda, Gleditsia triachantos and Salix glauca).\nAdditionally, we introduce two publicly available datasets of annotated images\nto the community. The proposed method outperforms state-of-the-art approaches\nin macro images (Pinus taeda and Gleditsia triacanthos) while showing slightly\nlower performance in microscopy images of Salix glauca. To our knowledge, this\nis the first paper that studies automatic tree ring detection for such\ndifferent species and acquisition conditions. The dataset and source code are\navailable in https://github.com/hmarichal93/deepcstrd"}
{"id": "2504.16275", "pdf": "https://arxiv.org/pdf/2504.16275", "abs": "https://arxiv.org/abs/2504.16275", "authors": ["Jannis Born", "Filip Skogh", "Kahn Rhrissorrakrai", "Filippo Utro", "Nico Wagner", "Aleksandros Sobczyk"], "title": "Quantum Doubly Stochastic Transformers", "categories": ["cs.LG", "cs.AI", "cs.CE", "cs.CV"], "comment": "Under Review", "summary": "At the core of the Transformer, the Softmax normalizes the attention matrix\nto be right stochastic. Previous research has shown that this often\ndestabilizes training and that enforcing the attention matrix to be doubly\nstochastic (through Sinkhorn's algorithm) consistently improves performance\nacross different tasks, domains and Transformer flavors. However, Sinkhorn's\nalgorithm is iterative, approximative, non-parametric and thus inflexible\nw.r.t. the obtained doubly stochastic matrix (DSM). Recently, it has been\nproven that DSMs can be obtained with a parametric quantum circuit, yielding a\nnovel quantum inductive bias for DSMs with no known classical analogue.\nMotivated by this, we demonstrate the feasibility of a hybrid classical-quantum\ndoubly stochastic Transformer (QDSFormer) that replaces the Softmax in the\nself-attention layer with a variational quantum circuit. We study the\nexpressive power of the circuit and find that it yields more diverse DSMs that\nbetter preserve information than classical operators. Across multiple\nsmall-scale object recognition tasks, we find that our QDSFormer consistently\nsurpasses both a standard Vision Transformer and other doubly stochastic\nTransformers. Beyond the established Sinkformer, this comparison includes a\nnovel quantum-inspired doubly stochastic Transformer (based on QR\ndecomposition) that can be of independent interest. The QDSFormer also shows\nimproved training stability and lower performance variation suggesting that it\nmay mitigate the notoriously unstable training of ViTs on small-scale data."}
{"id": "2504.16097", "pdf": "https://arxiv.org/pdf/2504.16097", "abs": "https://arxiv.org/abs/2504.16097", "authors": ["Arthur Buzelin", "Pedro Robles Dutenhefner", "Turi Rezende", "Luisa G. Porfirio", "Pedro Bento", "Yan Aquino", "Jose Fernandes", "Caio Santana", "Gabriela Miana", "Gisele L. Pappa", "Antonio Ribeiro", "Wagner Meira Jr"], "title": "A CNN-based Local-Global Self-Attention via Averaged Window Embeddings for Hierarchical ECG Analysis", "categories": ["eess.SP", "cs.AI", "cs.LG"], "comment": null, "summary": "Cardiovascular diseases remain the leading cause of global mortality,\nemphasizing the critical need for efficient diagnostic tools such as\nelectrocardiograms (ECGs). Recent advancements in deep learning, particularly\ntransformers, have revolutionized ECG analysis by capturing detailed waveform\nfeatures as well as global rhythm patterns. However, traditional transformers\nstruggle to effectively capture local morphological features that are critical\nfor accurate ECG interpretation. We propose a novel Local-Global Attention ECG\nmodel (LGA-ECG) to address this limitation, integrating convolutional inductive\nbiases with global self-attention mechanisms. Our approach extracts queries by\naveraging embeddings obtained from overlapping convolutional windows, enabling\nfine-grained morphological analysis, while simultaneously modeling global\ncontext through attention to keys and values derived from the entire sequence.\nExperiments conducted on the CODE-15 dataset demonstrate that LGA-ECG\noutperforms state-of-the-art models and ablation studies validate the\neffectiveness of the local-global attention strategy. By capturing the\nhierarchical temporal dependencies and morphological patterns in ECG signals,\nthis new design showcases its potential for clinical deployment with robust\nautomated ECG classification."}
{"id": "2504.16574", "pdf": "https://arxiv.org/pdf/2504.16574", "abs": "https://arxiv.org/abs/2504.16574", "authors": ["Lizhe Chen", "Binjia Zhou", "Yuyao Ge", "Jiayi Chen", "Shiguang NI"], "title": "PIS: Linking Importance Sampling and Attention Mechanisms for Efficient Prompt Compression", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Large language models (LLMs) have achieved remarkable progress, demonstrating\nunprecedented capabilities across various natural language processing tasks.\nHowever, the high costs associated with such exceptional performance limit the\nwidespread adoption of LLMs, highlighting the need for prompt compression.\nExisting prompt compression methods primarily rely on heuristic truncation or\nabstractive summarization techniques, which fundamentally overlook the\nintrinsic mechanisms of LLMs and lack a systematic evaluation of token\nimportance for generation. In this work, we introduce Prompt Importance\nSampling (PIS), a novel compression framework that dynamically compresses\nprompts by sampling important tokens based on the analysis of attention scores\nof hidden states. PIS employs a dual-level compression mechanism: 1) at the\ntoken level, we quantify saliency using LLM-native attention scores and\nimplement adaptive compression through a lightweight 9-layer reinforcement\nlearning (RL) network; 2) at the semantic level, we propose a Russian roulette\nsampling strategy for sentence-level importance sampling. Comprehensive\nevaluations across multiple domain benchmarks demonstrate that our method\nachieves state-of-the-art compression performance. Notably, our framework\nserendipitously enhances reasoning efficiency through optimized context\nstructuring. This work advances prompt engineering by offering both theoretical\ngrounding and practical efficiency in context management for LLMs."}
{"id": "2502.16746", "pdf": "https://arxiv.org/pdf/2502.16746", "abs": "https://arxiv.org/abs/2502.16746", "authors": ["Giulio V. Minore", "Louis Dwyer-Hemmings", "Timothy J. P. Bray", "Hui Zhang"], "title": "Resolving quantitative MRI model degeneracy in self-supervised machine learning", "categories": ["physics.med-ph", "eess.IV"], "comment": "Accepted at IPMI 2025", "summary": "Quantitative MRI (qMRI) estimates tissue properties of interest from measured\nMRI signals. This process is conventionally achieved by model fitting, whose\ncomputational expense limits qMRI's clinical use, motivating recent development\nof machine learning-based methods. Self-supervised approaches are particularly\npopular as they avoid the pitfall of distributional shift that affects\nsupervised methods. However, it is unknown how such methods behave if similar\nsignals can result from multiple tissue properties, a common challenge known as\nmodel degeneracy. Understanding this is crucial for ascertaining the scope\nwithin which self-supervised approaches may be applied. To this end, this work\nmakes two contributions. First, we demonstrate that model degeneracy\ncompromises self-supervised approaches, motivating the development of\nmitigation strategies. Second, we propose a mitigation strategy based on\napplying appropriate constraining transforms on the output of the bottleneck\nlayer of the autoencoder network typically employed in self-supervised\napproaches. We illustrate both contributions using the estimation of proton\ndensity fat fraction and $R_2^*$ from chemical shift-encoded MRI, an ideal\nexemplar due to its exhibition of degeneracy across the full parameter space.\nThe results from both simulation and $\\textit{in vivo}$ experiments demonstrate\nthat the proposed strategy helps resolve model degeneracy."}
{"id": "2504.16290", "pdf": "https://arxiv.org/pdf/2504.16290", "abs": "https://arxiv.org/abs/2504.16290", "authors": ["André Longon"], "title": "Naturally Computed Scale Invariance in the Residual Stream of ResNet18", "categories": ["cs.CV", "cs.LG"], "comment": null, "summary": "An important capacity in visual object recognition is invariance to\nimage-altering variables which leave the identity of objects unchanged, such as\nlighting, rotation, and scale. How do neural networks achieve this? Prior\nmechanistic interpretability research has illuminated some invariance-building\ncircuitry in InceptionV1, but the results are limited and networks with\ndifferent architectures have remained largely unexplored. This work\ninvestigates ResNet18 with a particular focus on its residual stream, an\narchitectural component which InceptionV1 lacks. We observe that many\nconvolutional channels in intermediate blocks exhibit scale invariant\nproperties, computed by the element-wise residual summation of scale\nequivariant representations: the block input's smaller-scale copy with the\nblock pre-sum output's larger-scale copy. Through subsequent ablation\nexperiments, we attempt to causally link these neural properties with\nscale-robust object recognition behavior. Our tentative findings suggest how\nthe residual stream computes scale invariance and its possible role in\nbehavior. Code is available at:\nhttps://github.com/cest-andre/residual-stream-interp"}
{"id": "2504.16277", "pdf": "https://arxiv.org/pdf/2504.16277", "abs": "https://arxiv.org/abs/2504.16277", "authors": ["Neha Hulkund", "Alaa Maalouf", "Levi Cai", "Daniel Yang", "Tsun-Hsuan Wang", "Abigail O'Neil", "Timm Haucke", "Sandeep Mukherjee", "Vikram Ramaswamy", "Judy Hansen Shen", "Gabriel Tseng", "Mike Walmsley", "Daniela Rus", "Ken Goldberg", "Hannah Kerner", "Irene Chen", "Yogesh Girdhar", "Sara Beery"], "title": "DataS^3: Dataset Subset Selection for Specialization", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "In many real-world machine learning (ML) applications (e.g. detecting broken\nbones in x-ray images, detecting species in camera traps), in practice models\nneed to perform well on specific deployments (e.g. a specific hospital, a\nspecific national park) rather than the domain broadly. However, deployments\noften have imbalanced, unique data distributions. Discrepancy between the\ntraining distribution and the deployment distribution can lead to suboptimal\nperformance, highlighting the need to select deployment-specialized subsets\nfrom the available training data. We formalize dataset subset selection for\nspecialization (DS3): given a training set drawn from a general distribution\nand a (potentially unlabeled) query set drawn from the desired\ndeployment-specific distribution, the goal is to select a subset of the\ntraining data that optimizes deployment performance.\n  We introduce DataS^3; the first dataset and benchmark designed specifically\nfor the DS3 problem. DataS^3 encompasses diverse real-world application\ndomains, each with a set of distinct deployments to specialize in. We conduct a\ncomprehensive study evaluating algorithms from various families--including\ncoresets, data filtering, and data curation--on DataS^3, and find that\ngeneral-distribution methods consistently fail on deployment-specific tasks.\nAdditionally, we demonstrate the existence of manually curated\n(deployment-specific) expert subsets that outperform training on all available\ndata with accuracy gains up to 51.3 percent. Our benchmark highlights the\ncritical role of tailored dataset curation in enhancing performance and\ntraining efficiency on deployment-specific distributions, which we posit will\nonly become more important as global, public datasets become available across\ndomains and ML models are deployed in the real world."}
{"id": "2504.16099", "pdf": "https://arxiv.org/pdf/2504.16099", "abs": "https://arxiv.org/abs/2504.16099", "authors": ["Luyuan Zhang", "Xidong Mu", "An Liu", "Yuanwei Liu"], "title": "Two-Timescale Joint Transmit and Pinching Beamforming for Pinching-Antenna Systems", "categories": ["eess.SP", "cs.AI", "cs.IT", "math.IT"], "comment": "5 pages, 4 figures, letter", "summary": "Pinching antenna systems (PASS) have been proposed as a revolutionary\nflexible antenna technology which facilitates line-of-sight links via numerous\nlow-cost pinching antennas with adjustable activation positions over\nwaveguides. This letter proposes a two-timescale joint transmit and pinching\nbeamforming design for the maximization of sum rate of a PASS-based downlink\nmulti-user multiple input single output system. A primal dual decomposition\nmethod is developed to decouple the two-timescale problem into two\nsub-problems: 1) A Karush-Kuhn-Tucker-guided dual learning-based approach is\nproposed to solve the short-term transmit beamforming design sub-problem; 2)\nThe long-term pinching beamforming design sub-problem is tackled by adopting a\nstochastic successive convex approximation method. Simulation results\ndemonstrate that the proposed two-timescale algorithm achieves a significant\nperformance gain compared to other baselines."}
{"id": "2504.16601", "pdf": "https://arxiv.org/pdf/2504.16601", "abs": "https://arxiv.org/abs/2504.16601", "authors": ["Andy Li", "Wei Zhou", "Rashina Hoda", "Chris Bain", "Peter Poon"], "title": "Comparing Large Language Models and Traditional Machine Translation Tools for Translating Medical Consultation Summaries: A Pilot Study", "categories": ["cs.CL", "cs.AI"], "comment": "8 pages, 2 tables and 1 Figure", "summary": "This study evaluates how well large language models (LLMs) and traditional\nmachine translation (MT) tools translate medical consultation summaries from\nEnglish into Arabic, Chinese, and Vietnamese. It assesses both patient,\nfriendly and clinician, focused texts using standard automated metrics. Results\nshowed that traditional MT tools generally performed better, especially for\ncomplex texts, while LLMs showed promise, particularly in Vietnamese and\nChinese, when translating simpler summaries. Arabic translations improved with\ncomplexity due to the language's morphology. Overall, while LLMs offer\ncontextual flexibility, they remain inconsistent, and current evaluation\nmetrics fail to capture clinical relevance. The study highlights the need for\ndomain-specific training, improved evaluation methods, and human oversight in\nmedical translation."}
{"id": "2504.16304", "pdf": "https://arxiv.org/pdf/2504.16304", "abs": "https://arxiv.org/abs/2504.16304", "authors": ["Wonjeong Jo", "Magdalena Wojcieszak"], "title": "MetaHarm: Harmful YouTube Video Dataset Annotated by Domain Experts, GPT-4-Turbo, and Crowdworkers", "categories": ["cs.CV"], "comment": null, "summary": "Short video platforms, such as YouTube, Instagram, or TikTok, are used by\nbillions of users. These platforms expose users to harmful content, ranging\nfrom clickbait or physical harms to hate or misinformation. Yet, we lack a\ncomprehensive understanding and measurement of online harm on short video\nplatforms. Toward this end, we present two large-scale datasets of multi-modal\nand multi-categorical online harm: (1) 60,906 systematically selected\npotentially harmful YouTube videos and (2) 19,422 videos annotated by three\nlabeling actors: trained domain experts, GPT-4-Turbo (using 14 image frames, 1\nthumbnail, and text metadata), and crowdworkers (Amazon Mechanical Turk master\nworkers). The annotated dataset includes both (a) binary classification\n(harmful vs. harmless) and (b) multi-label categorizations of six harm\ncategories: Information, Hate and harassment, Addictive, Clickbait, Sexual, and\nPhysical harms. Furthermore, the annotated dataset provides (1) ground truth\ndata with videos annotated consistently across (a) all three actors and (b) the\nmajority of the labeling actors, and (2) three data subsets labeled by\nindividual actors. These datasets are expected to facilitate future work on\nonline harm, aid in (multi-modal) classification efforts, and advance the\nidentification and potential mitigation of harmful content on video platforms."}
{"id": "2504.16283", "pdf": "https://arxiv.org/pdf/2504.16283", "abs": "https://arxiv.org/abs/2504.16283", "authors": ["Jaya Narain", "Amrit Romana", "Vikramjit Mitra", "Colin Lea", "Shirley Ren"], "title": "Affect Models Have Weak Generalizability to Atypical Speech", "categories": ["cs.LG"], "comment": "Preprint", "summary": "Speech and voice conditions can alter the acoustic properties of speech,\nwhich could impact the performance of paralinguistic models for affect for\npeople with atypical speech. We evaluate publicly available models for\nrecognizing categorical and dimensional affect from speech on a dataset of\natypical speech, comparing results to datasets of typical speech. We\ninvestigate three dimensions of speech atypicality: intelligibility, which is\nrelated to pronounciation; monopitch, which is related to prosody, and\nharshness, which is related to voice quality. We look at (1) distributional\ntrends of categorical affect predictions within the dataset, (2) distributional\ncomparisons of categorical affect predictions to similar datasets of typical\nspeech, and (3) correlation strengths between text and speech predictions for\nspontaneous speech for valence and arousal. We find that the output of affect\nmodels is significantly impacted by the presence and degree of speech\natypicalities. For instance, the percentage of speech predicted as sad is\nsignificantly higher for all types and grades of atypical speech when compared\nto similar typical speech datasets. In a preliminary investigation on improving\nrobustness for atypical speech, we find that fine-tuning models on\npseudo-labeled atypical speech data improves performance on atypical speech\nwithout impacting performance on typical speech. Our results emphasize the need\nfor broader training and evaluation datasets for speech emotion models, and for\nmodeling approaches that are robust to voice and speech differences."}
{"id": "2504.16100", "pdf": "https://arxiv.org/pdf/2504.16100", "abs": "https://arxiv.org/abs/2504.16100", "authors": ["Eloi Lindas", "Yannig Goude", "Philippe Ciais"], "title": "Towards Accurate Forecasting of Renewable Energy : Building Datasets and Benchmarking Machine Learning Models for Solar and Wind Power in France", "categories": ["eess.SP", "cs.AI", "cs.LG", "stat.ML"], "comment": "24 pages, 4 tables, 18 figures", "summary": "Accurate prediction of non-dispatchable renewable energy sources is essential\nfor grid stability and price prediction. Regional power supply forecasts are\nusually indirect through a bottom-up approach of plant-level forecasts,\nincorporate lagged power values, and do not use the potential of spatially\nresolved data. This study presents a comprehensive methodology for predicting\nsolar and wind power production at country scale in France using machine\nlearning models trained with spatially explicit weather data combined with\nspatial information about production sites capacity. A dataset is built\nspanning from 2012 to 2023, using daily power production data from RTE (the\nnational grid operator) as the target variable, with daily weather data from\nERA5, production sites capacity and location, and electricity prices as input\nfeatures. Three modeling approaches are explored to handle spatially resolved\nweather data: spatial averaging over the country, dimension reduction through\nprincipal component analysis, and a computer vision architecture to exploit\ncomplex spatial relationships. The study benchmarks state-of-the-art machine\nlearning models as well as hyperparameter tuning approaches based on\ncross-validation methods on daily power production data. Results indicate that\ncross-validation tailored to time series is best suited to reach low error. We\nfound that neural networks tend to outperform traditional tree-based models,\nwhich face challenges in extrapolation due to the increasing renewable capacity\nover time. Model performance ranges from 4% to 10% in nRMSE for midterm\nhorizon, achieving similar error metrics to local models established at a\nsingle-plant level, highlighting the potential of these methods for regional\npower supply forecasting."}
{"id": "2504.16604", "pdf": "https://arxiv.org/pdf/2504.16604", "abs": "https://arxiv.org/abs/2504.16604", "authors": ["Mareike Lisker", "Christina Gottschalk", "Helena Mihaljević"], "title": "Debunking with Dialogue? Exploring AI-Generated Counterspeech to Challenge Conspiracy Theories", "categories": ["cs.CL", "cs.AI", "cs.SI", "I.2.7"], "comment": "15 pages", "summary": "Counterspeech is a key strategy against harmful online content, but scaling\nexpert-driven efforts is challenging. Large Language Models (LLMs) present a\npotential solution, though their use in countering conspiracy theories is\nunder-researched. Unlike for hate speech, no datasets exist that pair\nconspiracy theory comments with expert-crafted counterspeech. We address this\ngap by evaluating the ability of GPT-4o, Llama 3, and Mistral to effectively\napply counterspeech strategies derived from psychological research provided\nthrough structured prompts. Our results show that the models often generate\ngeneric, repetitive, or superficial results. Additionally, they\nover-acknowledge fear and frequently hallucinate facts, sources, or figures,\nmaking their prompt-based use in practical applications problematic."}
{"id": "2504.16315", "pdf": "https://arxiv.org/pdf/2504.16315", "abs": "https://arxiv.org/abs/2504.16315", "authors": ["Sen Fang", "Chunyu Sui", "Hongwei Yi", "Carol Neidle", "Dimitris N. Metaxas"], "title": "SignX: The Foundation Model for Sign Recognition", "categories": ["cs.CV", "cs.CL"], "comment": null, "summary": "The complexity of sign language data processing brings many challenges. The\ncurrent approach to recognition of ASL signs aims to translate RGB sign\nlanguage videos through pose information into English-based ID glosses, which\nserve to uniquely identify ASL signs. Note that there is no shared convention\nfor assigning such glosses to ASL signs, so it is essential that the same\nglossing conventions are used for all of the data in the datasets that are\nemployed. This paper proposes SignX, a foundation model framework for sign\nrecognition. It is a concise yet powerful framework applicable to multiple\nhuman activity recognition scenarios. First, we developed a Pose2Gloss\ncomponent based on an inverse diffusion model, which contains a multi-track\npose fusion layer that unifies five of the most powerful pose information\nsources--SMPLer-X, DWPose, Mediapipe, PrimeDepth, and Sapiens\nSegmentation--into a single latent pose representation. Second, we trained a\nVideo2Pose module based on ViT that can directly convert raw video into signer\npose representation. Through this 2-stage training framework, we enable sign\nlanguage recognition models to be compatible with existing pose formats, laying\nthe foundation for the common pose estimation necessary for sign recognition.\nExperimental results show that SignX can recognize signs from sign language\nvideo, producing predicted gloss representations with greater accuracy than has\nbeen reported in prior work."}
{"id": "2504.16318", "pdf": "https://arxiv.org/pdf/2504.16318", "abs": "https://arxiv.org/abs/2504.16318", "authors": ["Kisung You"], "title": "Semantics at an Angle: When Cosine Similarity Works Until It Doesn't", "categories": ["cs.LG"], "comment": null, "summary": "Cosine similarity has become a standard metric for comparing embeddings in\nmodern machine learning. Its scale-invariance and alignment with model training\nobjectives have contributed to its widespread adoption. However, recent studies\nhave revealed important limitations, particularly when embedding norms carry\nmeaningful semantic information. This informal article offers a reflective and\nselective examination of the evolution, strengths, and limitations of cosine\nsimilarity. We highlight why it performs well in many settings, where it tends\nto break down, and how emerging alternatives are beginning to address its blind\nspots. We hope to offer a mix of conceptual clarity and practical perspective,\nespecially for quantitative scientists who think about embeddings not just as\nvectors, but as geometric and philosophical objects."}
{"id": "2504.16101", "pdf": "https://arxiv.org/pdf/2504.16101", "abs": "https://arxiv.org/abs/2504.16101", "authors": ["Lei Kang", "Xuanshuo Fu", "Javier Vazquez-Corral", "Ernest Valveny", "Dimosthenis Karatzas"], "title": "xLSTM-ECG: Multi-label ECG Classification via Feature Fusion with xLSTM", "categories": ["eess.SP", "cs.AI", "cs.LG"], "comment": null, "summary": "Cardiovascular diseases (CVDs) remain the leading cause of mortality\nworldwide, highlighting the critical need for efficient and accurate diagnostic\ntools. Electrocardiograms (ECGs) are indispensable in diagnosing various heart\nconditions; however, their manual interpretation is time-consuming and\nerror-prone. In this paper, we propose xLSTM-ECG, a novel approach that\nleverages an extended Long Short-Term Memory (xLSTM) network for multi-label\nclassification of ECG signals, using the PTB-XL dataset. To the best of our\nknowledge, this work represents the first design and application of xLSTM\nmodules specifically adapted for multi-label ECG classification. Our method\nemploys a Short-Time Fourier Transform (STFT) to convert time-series ECG\nwaveforms into the frequency domain, thereby enhancing feature extraction. The\nxLSTM architecture is specifically tailored to address the complexities of\n12-lead ECG recordings by capturing both local and global signal features.\nComprehensive experiments on the PTB-XL dataset reveal that our model achieves\nstrong multi-label classification performance, while additional tests on the\nGeorgia 12-Lead dataset underscore its robustness and efficiency. This approach\nsignificantly improves ECG classification accuracy, thereby advancing clinical\ndiagnostics and patient care. The code will be publicly available upon\nacceptance."}
{"id": "2504.16627", "pdf": "https://arxiv.org/pdf/2504.16627", "abs": "https://arxiv.org/abs/2504.16627", "authors": ["Prasanna Devadiga", "Arya Suneesh", "Pawan Kumar Rajpoot", "Bharatdeep Hazarika", "Aditya U Baliga"], "title": "TIFIN India at SemEval-2025: Harnessing Translation to Overcome Multilingual IR Challenges in Fact-Checked Claim Retrieval", "categories": ["cs.CL"], "comment": null, "summary": "We address the challenge of retrieving previously fact-checked claims in\nmonolingual and crosslingual settings - a critical task given the global\nprevalence of disinformation. Our approach follows a two-stage strategy: a\nreliable baseline retrieval system using a fine-tuned embedding model and an\nLLM-based reranker. Our key contribution is demonstrating how LLM-based\ntranslation can overcome the hurdles of multilingual information retrieval.\nAdditionally, we focus on ensuring that the bulk of the pipeline can be\nreplicated on a consumer GPU. Our final integrated system achieved a success@10\nscore of 0.938 and 0.81025 on the monolingual and crosslingual test sets,\nrespectively."}
{"id": "2504.16362", "pdf": "https://arxiv.org/pdf/2504.16362", "abs": "https://arxiv.org/abs/2504.16362", "authors": ["Colton R. Crum", "Adam Czajka"], "title": "Almost Right: Making First-layer Kernels Nearly Orthogonal Improves Model Generalization", "categories": ["cs.CV"], "comment": "8 pages, 1 figure, 3 tables", "summary": "An ongoing research challenge within several domains in computer vision is\nhow to increase model generalization capabilities. Several attempts to improve\nmodel generalization performance are heavily inspired by human perceptual\nintelligence, which is remarkable in both its performance and efficiency to\ngeneralize to unknown samples. Many of these methods attempt to force portions\nof the network to be orthogonal, following some observation within neuroscience\nrelated to early vision processes. In this paper, we propose a loss component\nthat regularizes the filtering kernels in the first convolutional layer of a\nnetwork to make them nearly orthogonal. Deviating from previous works, we give\nthe network flexibility in which pairs of kernels it makes orthogonal, allowing\nthe network to navigate to a better solution space, imposing harsh penalties.\nWithout architectural modifications, we report substantial gains in\ngeneralization performance using the proposed loss against previous works\n(including orthogonalization- and saliency-based regularization methods) across\nthree different architectures (ResNet-50, DenseNet-121, ViT-b-16) and two\ndifficult open-set recognition tasks: presentation attack detection in iris\nbiometrics, and anomaly detection in chest X-ray images."}
{"id": "2504.16360", "pdf": "https://arxiv.org/pdf/2504.16360", "abs": "https://arxiv.org/abs/2504.16360", "authors": ["Mao Wang", "Tao Wu", "Xingping Xian", "Shaojie Qiao", "Weina Niu", "Canyixing Cui"], "title": "Disentangled Graph Representation Based on Substructure-Aware Graph Optimal Matching Kernel Convolutional Networks", "categories": ["cs.LG"], "comment": null, "summary": "Graphs effectively characterize relational data, driving graph representation\nlearning methods that uncover underlying predictive information. As\nstate-of-the-art approaches, Graph Neural Networks (GNNs) enable end-to-end\nlearning for diverse tasks. Recent disentangled graph representation learning\nenhances interpretability by decoupling independent factors in graph data.\nHowever, existing methods often implicitly and coarsely characterize graph\nstructures, limiting structural pattern analysis within the graph. This paper\nproposes the Graph Optimal Matching Kernel Convolutional Network (GOMKCN) to\naddress this limitation. We view graphs as node-centric subgraphs, where each\nsubgraph acts as a structural factor encoding position-specific information.\nThis transforms graph prediction into structural pattern recognition. Inspired\nby CNNs, GOMKCN introduces the Graph Optimal Matching Kernel (GOMK) as a\nconvolutional operator, computing similarities between subgraphs and learnable\ngraph filters. Mathematically, GOMK maps subgraphs and filters into a Hilbert\nspace, representing graphs as point sets. Disentangled representations emerge\nfrom projecting subgraphs onto task-optimized filters, which adaptively capture\nrelevant structural patterns via gradient descent. Crucially, GOMK incorporates\nlocal correspondences in similarity measurement, resolving the trade-off\nbetween differentiability and accuracy in graph kernels. Experiments validate\nthat GOMKCN achieves superior accuracy and interpretability in graph pattern\nmining and prediction. The framework advances the theoretical foundation for\ndisentangled graph representation learning."}
{"id": "2504.16110", "pdf": "https://arxiv.org/pdf/2504.16110", "abs": "https://arxiv.org/abs/2504.16110", "authors": ["Krti Tallam"], "title": "Security-First AI: Foundations for Robust and Trustworthy Systems", "categories": ["cs.CR", "cs.AI"], "comment": null, "summary": "The conversation around artificial intelligence (AI) often focuses on safety,\ntransparency, accountability, alignment, and responsibility. However, AI\nsecurity (i.e., the safeguarding of data, models, and pipelines from\nadversarial manipulation) underpins all of these efforts. This manuscript\nposits that AI security must be prioritized as a foundational layer. We present\na hierarchical view of AI challenges, distinguishing security from safety, and\nargue for a security-first approach to enable trustworthy and resilient AI\nsystems. We discuss core threat models, key attack vectors, and emerging\ndefense mechanisms, concluding that a metric-driven approach to AI security is\nessential for robust AI safety, transparency, and accountability."}
{"id": "2504.16677", "pdf": "https://arxiv.org/pdf/2504.16677", "abs": "https://arxiv.org/abs/2504.16677", "authors": ["Luisa Shimabucoro", "Ahmet Ustun", "Marzieh Fadaee", "Sebastian Ruder"], "title": "A Post-trainer's Guide to Multilingual Training Data: Uncovering Cross-lingual Transfer Dynamics", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "In order for large language models to be useful across the globe, they are\nfine-tuned to follow instructions on multilingual data. Despite the ubiquity of\nsuch post-training, a clear understanding of the dynamics that enable\ncross-lingual transfer remains elusive. This study examines cross-lingual\ntransfer (CLT) dynamics in realistic post-training settings. We study two model\nfamilies of up to 35B parameters in size trained on carefully controlled\nmixtures of multilingual data on three generative tasks with varying levels of\ncomplexity (summarization, instruction following, and mathematical reasoning)\nin both single-task and multi-task instruction tuning settings. Overall, we\nfind that the dynamics of cross-lingual transfer and multilingual performance\ncannot be explained by isolated variables, varying depending on the combination\nof post-training settings. Finally, we identify the conditions that lead to\neffective cross-lingual transfer in practice."}
{"id": "2504.16364", "pdf": "https://arxiv.org/pdf/2504.16364", "abs": "https://arxiv.org/abs/2504.16364", "authors": ["Fengchun Liu", "Tong Zhang", "Chunying Zhang"], "title": "CLPSTNet: A Progressive Multi-Scale Convolutional Steganography Model Integrating Curriculum Learning", "categories": ["cs.CV", "cs.AI", "cs.CR"], "comment": null, "summary": "In recent years, a large number of works have introduced Convolutional Neural\nNetworks (CNNs) into image steganography, which transform traditional\nsteganography methods such as hand-crafted features and prior knowledge design\ninto steganography methods that neural networks autonomically learn information\nembedding. However, due to the inherent complexity of digital images, issues of\ninvisibility and security persist when using CNN models for information\nembedding. In this paper, we propose Curriculum Learning Progressive Steganophy\nNetwork (CLPSTNet). The network consists of multiple progressive multi-scale\nconvolutional modules that integrate Inception structures and dilated\nconvolutions. The module contains multiple branching pathways, starting from a\nsmaller convolutional kernel and dilatation rate, extracting the basic, local\nfeature information from the feature map, and gradually expanding to the\nconvolution with a larger convolutional kernel and dilatation rate for\nperceiving the feature information of a larger receptive field, so as to\nrealize the multi-scale feature extraction from shallow to deep, and from fine\nto coarse, allowing the shallow secret information features to be refined in\ndifferent fusion stages. The experimental results show that the proposed\nCLPSTNet not only has high PSNR , SSIM metrics and decoding accuracy on three\nlarge public datasets, ALASKA2, VOC2012 and ImageNet, but also the\nsteganographic images generated by CLPSTNet have low steganalysis scores.You\ncan find our code at\n\\href{https://github.com/chaos-boops/CLPSTNet}{https://github.com/chaos-boops/CLPSTNet}."}
{"id": "2504.16415", "pdf": "https://arxiv.org/pdf/2504.16415", "abs": "https://arxiv.org/abs/2504.16415", "authors": ["Neharika Jali", "Eshika Pathak", "Pranay Sharma", "Guannan Qu", "Gauri Joshi"], "title": "Natural Policy Gradient for Average Reward Non-Stationary RL", "categories": ["cs.LG", "stat.ML"], "comment": null, "summary": "We consider the problem of non-stationary reinforcement learning (RL) in the\ninfinite-horizon average-reward setting. We model it by a Markov Decision\nProcess with time-varying rewards and transition probabilities, with a\nvariation budget of $\\Delta_T$. Existing non-stationary RL algorithms focus on\nmodel-based and model-free value-based methods. Policy-based methods despite\ntheir flexibility in practice are not theoretically well understood in\nnon-stationary RL. We propose and analyze the first model-free policy-based\nalgorithm, Non-Stationary Natural Actor-Critic (NS-NAC), a policy gradient\nmethod with a restart based exploration for change and a novel interpretation\nof learning rates as adapting factors. Further, we present a bandit-over-RL\nbased parameter-free algorithm BORL-NS-NAC that does not require prior\nknowledge of the variation budget $\\Delta_T$. We present a dynamic regret of\n$\\tilde{\\mathscr O}(|S|^{1/2}|A|^{1/2}\\Delta_T^{1/6}T^{5/6})$ for both\nalgorithms, where $T$ is the time horizon, and $|S|$, $|A|$ are the sizes of\nthe state and action spaces. The regret analysis leverages a novel adaptation\nof the Lyapunov function analysis of NAC to dynamic environments and\ncharacterizes the effects of simultaneous updates in policy, value function\nestimate and changes in the environment."}
{"id": "2504.16112", "pdf": "https://arxiv.org/pdf/2504.16112", "abs": "https://arxiv.org/abs/2504.16112", "authors": ["Myunghyun Rhee", "Joonseop Sim", "Taeyoung Ahn", "Seungyong Lee", "Daegun Yoon", "Euiseok Kim", "Kyoung Park", "Youngpyo Joo", "Hosik Kim"], "title": "HPU: High-Bandwidth Processing Unit for Scalable, Cost-effective LLM Inference via GPU Co-processing", "categories": ["cs.AR", "cs.AI", "cs.CL", "cs.DC"], "comment": "6 pages", "summary": "The attention layer, a core component of Transformer-based LLMs, brings out\ninefficiencies in current GPU systems due to its low operational intensity and\nthe substantial memory requirements of KV caches. We propose a High-bandwidth\nProcessing Unit (HPU), a memoryintensive co-processor that enhances GPU\nresource utilization during large-batched LLM inference. By offloading\nmemory-bound operations, the HPU allows the GPU to focus on compute-intensive\ntasks, increasing overall efficiency. Also, the HPU, as an add-on card, scales\nout to accommodate surging memory demands driven by large batch sizes and\nextended sequence lengths. In this paper, we show the HPU prototype implemented\nwith PCIe-based FPGA cards mounted on a GPU system. Our novel GPU-HPU\nheterogeneous system demonstrates up to 4.1x performance gains and 4.6x energy\nefficiency improvements over a GPUonly system, providing scalability without\nincreasing the number of GPUs."}
{"id": "2504.16754", "pdf": "https://arxiv.org/pdf/2504.16754", "abs": "https://arxiv.org/abs/2504.16754", "authors": ["Kwangseob Ahn"], "title": "HEMA : A Hippocampus-Inspired Extended Memory Architecture for Long-Context AI Conversations", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Large language models (LLMs) struggle with maintaining coherence in extended\nconversations spanning hundreds of turns, despite performing well within their\ncontext windows. This paper introduces HEMA (Hippocampus-Inspired Extended\nMemory Architecture), a dual-memory system inspired by human cognitive\nprocesses. HEMA combines Compact Memory - a continuously updated one-sentence\nsummary preserving global narrative coherence, and Vector Memory - an episodic\nstore of chunk embeddings queried via cosine similarity. When integrated with a\n6B-parameter transformer, HEMA maintains coherent dialogues beyond 300 turns\nwhile keeping prompt length under 3,500 tokens. Experimental results show\nsubstantial improvements: factual recall accuracy increases from 41% to 87%,\nand human-rated coherence improves from 2.7 to 4.3 on a 5-point scale. With 10K\nindexed chunks, Vector Memory achieves P@5 >= 0.80 and R@50 >= 0.74, doubling\nthe area under the precision-recall curve compared to summarization-only\napproaches. Ablation studies reveal two key insights: semantic forgetting\nthrough age-weighted pruning reduces retrieval latency by 34% with minimal\nrecall loss, and a two-level summary hierarchy prevents cascade errors in\nultra-long conversations exceeding 1,000 turns. HEMA demonstrates that\ncombining verbatim recall with semantic continuity provides a practical\nsolution for privacy-aware conversational AI capable of month-long dialogues\nwithout model retraining."}
{"id": "2504.16368", "pdf": "https://arxiv.org/pdf/2504.16368", "abs": "https://arxiv.org/abs/2504.16368", "authors": ["Linhua Kong", "Dongxia Chang", "Lian Liu", "Zisen Kong", "Pengyuan Li", "Yao Zhao"], "title": "Revisiting Radar Camera Alignment by Contrastive Learning for 3D Object Detection", "categories": ["cs.CV"], "comment": null, "summary": "Recently, 3D object detection algorithms based on radar and camera fusion\nhave shown excellent performance, setting the stage for their application in\nautonomous driving perception tasks. Existing methods have focused on dealing\nwith feature misalignment caused by the domain gap between radar and camera.\nHowever, existing methods either neglect inter-modal features interaction\nduring alignment or fail to effectively align features at the same spatial\nlocation across modalities. To alleviate the above problems, we propose a new\nalignment model called Radar Camera Alignment (RCAlign). Specifically, we\ndesign a Dual-Route Alignment (DRA) module based on contrastive learning to\nalign and fuse the features between radar and camera. Moreover, considering the\nsparsity of radar BEV features, a Radar Feature Enhancement (RFE) module is\nproposed to improve the densification of radar BEV features with the knowledge\ndistillation loss. Experiments show RCAlign achieves a new state-of-the-art on\nthe public nuScenes benchmark in radar camera fusion for 3D Object Detection.\nFurthermore, the RCAlign achieves a significant performance gain (4.3\\% NDS and\n8.4\\% mAP) in real-time 3D detection compared to the latest state-of-the-art\nmethod (RCBEVDet)."}
{"id": "2504.16430", "pdf": "https://arxiv.org/pdf/2504.16430", "abs": "https://arxiv.org/abs/2504.16430", "authors": ["Andrew Ilyas", "Logan Engstrom"], "title": "MAGIC: Near-Optimal Data Attribution for Deep Learning", "categories": ["cs.LG", "cs.CL", "cs.CV", "stat.ML"], "comment": null, "summary": "The goal of predictive data attribution is to estimate how adding or removing\na given set of training datapoints will affect model predictions. In convex\nsettings, this goal is straightforward (i.e., via the infinitesimal jackknife).\nIn large-scale (non-convex) settings, however, existing methods are far less\nsuccessful -- current methods' estimates often only weakly correlate with\nground truth. In this work, we present a new data attribution method (MAGIC)\nthat combines classical methods and recent advances in metadifferentiation to\n(nearly) optimally estimate the effect of adding or removing training data on\nmodel predictions."}
{"id": "2504.16113", "pdf": "https://arxiv.org/pdf/2504.16113", "abs": "https://arxiv.org/abs/2504.16113", "authors": ["Xin Wang", "Xiaoqi Li"], "title": "AI-Based Vulnerability Analysis of NFT Smart Contracts", "categories": ["cs.CR", "cs.AI"], "comment": null, "summary": "In the research experiment of this article, our research work is divided into\nseveral stages. Firstly, we collected a large number of smart contract codes\nand classified them, identifying several common defects, including Risky\nMutably Porxy, ERC-721 Recentrancy, Unlimited Mining, Missing Requirements, and\nPublic Burns. Secondly, we used Python to process the smart contracts. On the\none hand, we modified the file names, and on the other hand, we batched the\nprocess of the content for analysis and application. Next, we built a model of\nthe decision tree. Firstly, we carried out the feature extraction. We selected\nthe algorithm and divided the data. After comparing and processing, we chose\nthe CART classification tree to process. By gene coefficient, we analyzed and\nsorted the data, and got the initial model of the decision tree. Then, we\nintroduced the random forest model on the basis of the decision tree. From\nabstracting the same amount of samples to selecting features randomly.From\nadjusting and optimizing parameters to completing the construction of the\nforest model. Finally, we compared and analyzed the decision tree, random\nforest, and self-built model in the paper and drew general conclusions."}
{"id": "2504.16768", "pdf": "https://arxiv.org/pdf/2504.16768", "abs": "https://arxiv.org/abs/2504.16768", "authors": ["Waad Alhoshan", "Alessio Ferrari", "Liping Zhao"], "title": "How Effective are Generative Large Language Models in Performing Requirements Classification?", "categories": ["cs.CL", "cs.AI", "cs.SE"], "comment": null, "summary": "In recent years, transformer-based large language models (LLMs) have\nrevolutionised natural language processing (NLP), with generative models\nopening new possibilities for tasks that require context-aware text generation.\nRequirements engineering (RE) has also seen a surge in the experimentation of\nLLMs for different tasks, including trace-link detection, regulatory\ncompliance, and others. Requirements classification is a common task in RE.\nWhile non-generative LLMs like BERT have been successfully applied to this\ntask, there has been limited exploration of generative LLMs. This gap raises an\nimportant question: how well can generative LLMs, which produce context-aware\noutputs, perform in requirements classification? In this study, we explore the\neffectiveness of three generative LLMs-Bloom, Gemma, and Llama-in performing\nboth binary and multi-class requirements classification. We design an extensive\nexperimental study involving over 400 experiments across three widely used\ndatasets (PROMISE NFR, Functional-Quality, and SecReq). Our study concludes\nthat while factors like prompt design and LLM architecture are universally\nimportant, others-such as dataset variations-have a more situational impact,\ndepending on the complexity of the classification task. This insight can guide\nfuture model development and deployment strategies, focusing on optimising\nprompt structures and aligning model architectures with task-specific needs for\nimproved performance."}
{"id": "2504.16389", "pdf": "https://arxiv.org/pdf/2504.16389", "abs": "https://arxiv.org/abs/2504.16389", "authors": ["Yuanjian Wang", "Yufei Deng", "Rong Xiao", "Jiahao Fan", "Chenwei Tang", "Deng Xiong", "Jiancheng Lv"], "title": "SaENeRF: Suppressing Artifacts in Event-based Neural Radiance Fields", "categories": ["cs.CV"], "comment": "Accepted by IJCNN 2025", "summary": "Event cameras are neuromorphic vision sensors that asynchronously capture\nchanges in logarithmic brightness changes, offering significant advantages such\nas low latency, low power consumption, low bandwidth, and high dynamic range.\nWhile these characteristics make them ideal for high-speed scenarios,\nreconstructing geometrically consistent and photometrically accurate 3D\nrepresentations from event data remains fundamentally challenging. Current\nevent-based Neural Radiance Fields (NeRF) methods partially address these\nchallenges but suffer from persistent artifacts caused by aggressive network\nlearning in early stages and the inherent noise of event cameras. To overcome\nthese limitations, we present SaENeRF, a novel self-supervised framework that\neffectively suppresses artifacts and enables 3D-consistent, dense, and\nphotorealistic NeRF reconstruction of static scenes solely from event streams.\nOur approach normalizes predicted radiance variations based on accumulated\nevent polarities, facilitating progressive and rapid learning for scene\nrepresentation construction. Additionally, we introduce regularization losses\nspecifically designed to suppress artifacts in regions where photometric\nchanges fall below the event threshold and simultaneously enhance the light\nintensity difference of non-zero events, thereby improving the visual fidelity\nof the reconstructed scene. Extensive qualitative and quantitative experiments\ndemonstrate that our method significantly reduces artifacts and achieves\nsuperior reconstruction quality compared to existing methods. The code is\navailable at https://github.com/Mr-firework/SaENeRF."}
{"id": "2504.16431", "pdf": "https://arxiv.org/pdf/2504.16431", "abs": "https://arxiv.org/abs/2504.16431", "authors": ["Ruixiang Zhang", "Shuangfei Zhai", "Yizhe Zhang", "James Thornton", "Zijing Ou", "Joshua Susskind", "Navdeep Jaitly"], "title": "Target Concrete Score Matching: A Holistic Framework for Discrete Diffusion", "categories": ["cs.LG"], "comment": null, "summary": "Discrete diffusion is a promising framework for modeling and generating\ndiscrete data. In this work, we present Target Concrete Score Matching (TCSM),\na novel and versatile objective for training and fine-tuning discrete diffusion\nmodels. TCSM provides a general framework with broad applicability. It supports\npre-training discrete diffusion models directly from data samples, and many\nexisting discrete diffusion approaches naturally emerge as special cases of our\nmore general TCSM framework. Furthermore, the same TCSM objective extends to\npost-training of discrete diffusion models, including fine-tuning using reward\nfunctions or preference data, and distillation of knowledge from pre-trained\nautoregressive models. These new capabilities stem from the core idea of TCSM,\nestimating the concrete score of the target distribution, which resides in the\noriginal (clean) data space. This allows seamless integration with reward\nfunctions and pre-trained models, which inherently only operate in the clean\ndata space rather than the noisy intermediate spaces of diffusion processes.\nOur experiments on language modeling tasks demonstrate that TCSM matches or\nsurpasses current methods. Additionally, TCSM is versatile, applicable to both\npre-training and post-training scenarios, offering greater flexibility and\nsample efficiency."}
{"id": "2504.16116", "pdf": "https://arxiv.org/pdf/2504.16116", "abs": "https://arxiv.org/abs/2504.16116", "authors": ["Miracle Master", "Rainy Sun", "Anya Reese", "Joey Ouyang", "Alex Chen", "Winter Dong", "Frank Li", "James Yi", "Garry Zhao", "Tony Ling", "Hobert Wong", "Lowes Yang"], "title": "DMind Benchmark: The First Comprehensive Benchmark for LLM Evaluation in the Web3 Domain", "categories": ["cs.CR", "cs.AI"], "comment": null, "summary": "Recent advances in Large Language Models (LLMs) have led to significant\nprogress on a wide range of natural language processing tasks. However, their\neffectiveness in specialized and rapidly evolving domains such as Web3 remains\nunderexplored. In this paper, we introduce DMind Benchmark, a novel framework\nthat systematically tests LLMs across nine key categories encompassing\nblockchain fundamentals, infrastructure, smart contract analysis, decentralized\nfinance (DeFi), decentralized autonomous organizations (DAOs), non-fungible\ntokens (NFTs), token economics, meme concepts, and security vulnerabilities.\n  DMind Benchmark goes beyond conventional multiple-choice questions by\nincorporating domain-specific subjective tasks (e.g., smart contract code\nauditing and repair, numeric reasoning on on-chain data, and fill-in\nassessments), thereby capturing real-world complexities and stress-testing\nmodel adaptability. We evaluate fifteen popular LLMs (from ChatGPT, DeepSeek,\nClaude, and Gemini series) on DMind Benchmark, uncovering performance gaps in\nWeb3-specific reasoning and application, particularly in emerging areas like\ntoken economics and meme concepts. Even the strongest models face significant\nchallenges in identifying subtle security vulnerabilities and analyzing complex\nDeFi mechanisms. To foster progress in this area, we publicly release our\nbenchmark dataset, evaluation pipeline, and annotated results at\nhttp://www.dmind.ai, offering a valuable resource for advancing specialized\ndomain adaptation and the development of more robust Web3-enabled LLMs."}
{"id": "2504.16778", "pdf": "https://arxiv.org/pdf/2504.16778", "abs": "https://arxiv.org/abs/2504.16778", "authors": ["Sarah Jabbour", "Trenton Chang", "Anindya Das Antar", "Joseph Peper", "Insu Jang", "Jiachen Liu", "Jae-Won Chung", "Shiqi He", "Michael Wellman", "Bryan Goodman", "Elizabeth Bondi-Kelly", "Kevin Samy", "Rada Mihalcea", "Mosharaf Chowhury", "David Jurgens", "Lu Wang"], "title": "Evaluation Framework for AI Systems in \"the Wild\"", "categories": ["cs.CL", "cs.AI", "cs.CY"], "comment": "35 pages", "summary": "Generative AI (GenAI) models have become vital across industries, yet current\nevaluation methods have not adapted to their widespread use. Traditional\nevaluations often rely on benchmarks and fixed datasets, frequently failing to\nreflect real-world performance, which creates a gap between lab-tested outcomes\nand practical applications. This white paper proposes a comprehensive framework\nfor how we should evaluate real-world GenAI systems, emphasizing diverse,\nevolving inputs and holistic, dynamic, and ongoing assessment approaches. The\npaper offers guidance for practitioners on how to design evaluation methods\nthat accurately reflect real-time capabilities, and provides policymakers with\nrecommendations for crafting GenAI policies focused on societal impacts, rather\nthan fixed performance numbers or parameter sizes. We advocate for holistic\nframeworks that integrate performance, fairness, and ethics and the use of\ncontinuous, outcome-oriented methods that combine human and automated\nassessments while also being transparent to foster trust among stakeholders.\nImplementing these strategies ensures GenAI models are not only technically\nproficient but also ethically responsible and impactful."}
{"id": "2504.16419", "pdf": "https://arxiv.org/pdf/2504.16419", "abs": "https://arxiv.org/abs/2504.16419", "authors": ["Qi Yang", "Weichen Bi", "Haiyang Shen", "Yaoqi Guo", "Yun Ma"], "title": "PixelWeb: The First Web GUI Dataset with Pixel-Wise Labels", "categories": ["cs.CV", "cs.AI", "cs.HC"], "comment": null, "summary": "Graphical User Interface (GUI) datasets are crucial for various downstream\ntasks. However, GUI datasets often generate annotation information through\nautomatic labeling, which commonly results in inaccurate GUI element BBox\nannotations, including missing, duplicate, or meaningless BBoxes. These issues\ncan degrade the performance of models trained on these datasets, limiting their\neffectiveness in real-world applications. Additionally, existing GUI datasets\nonly provide BBox annotations visually, which restricts the development of\nvisually related GUI downstream tasks. To address these issues, we introduce\nPixelWeb, a large-scale GUI dataset containing over 100,000 annotated web\npages. PixelWeb is constructed using a novel automatic annotation approach that\nintegrates visual feature extraction and Document Object Model (DOM) structure\nanalysis through two core modules: channel derivation and layer analysis.\nChannel derivation ensures accurate localization of GUI elements in cases of\nocclusion and overlapping elements by extracting BGRA four-channel bitmap\nannotations. Layer analysis uses the DOM to determine the visibility and\nstacking order of elements, providing precise BBox annotations. Additionally,\nPixelWeb includes comprehensive metadata such as element images, contours, and\nmask annotations. Manual verification by three independent annotators confirms\nthe high quality and accuracy of PixelWeb annotations. Experimental results on\nGUI element detection tasks show that PixelWeb achieves performance on the\nmAP95 metric that is 3-7 times better than existing datasets. We believe that\nPixelWeb has great potential for performance improvement in downstream tasks\nsuch as GUI generation and automated user interaction."}
{"id": "2504.16432", "pdf": "https://arxiv.org/pdf/2504.16432", "abs": "https://arxiv.org/abs/2504.16432", "authors": ["Ziran Liang", "Rui An", "Wenqi Fan", "Yanghui Rao", "Yuxuan Liang"], "title": "iTFKAN: Interpretable Time Series Forecasting with Kolmogorov-Arnold Network", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "As time evolves, data within specific domains exhibit predictability that\nmotivates time series forecasting to predict future trends from historical\ndata. However, current deep forecasting methods can achieve promising\nperformance but generally lack interpretability, hindering trustworthiness and\npractical deployment in safety-critical applications such as auto-driving and\nhealthcare. In this paper, we propose a novel interpretable model, iTFKAN, for\ncredible time series forecasting. iTFKAN enables further exploration of model\ndecision rationales and underlying data patterns due to its interpretability\nachieved through model symbolization. Besides, iTFKAN develops two strategies,\nprior knowledge injection, and time-frequency synergy learning, to effectively\nguide model learning under complex intertwined time series data. Extensive\nexperimental results demonstrated that iTFKAN can achieve promising forecasting\nperformance while simultaneously possessing high interpretive capabilities."}
{"id": "2504.16118", "pdf": "https://arxiv.org/pdf/2504.16118", "abs": "https://arxiv.org/abs/2504.16118", "authors": ["Milad Rahmati"], "title": "Towards Explainable and Lightweight AI for Real-Time Cyber Threat Hunting in Edge Networks", "categories": ["cs.CR", "cs.AI"], "comment": null, "summary": "As cyber threats continue to evolve, securing edge networks has become\nincreasingly challenging due to their distributed nature and resource\nlimitations. Many AI-driven threat detection systems rely on complex deep\nlearning models, which, despite their high accuracy, suffer from two major\ndrawbacks: lack of interpretability and high computational cost. Black-box AI\nmodels make it difficult for security analysts to understand the reasoning\nbehind their predictions, limiting their practical deployment. Moreover,\nconventional deep learning techniques demand significant computational\nresources, rendering them unsuitable for edge devices with limited processing\npower. To address these issues, this study introduces an Explainable and\nLightweight AI (ELAI) framework designed for real-time cyber threat detection\nin edge networks. Our approach integrates interpretable machine learning\nalgorithms with optimized lightweight deep learning techniques, ensuring both\ntransparency and computational efficiency. The proposed system leverages\ndecision trees, attention-based deep learning, and federated learning to\nenhance detection accuracy while maintaining explainability. We evaluate ELAI\nusing benchmark cybersecurity datasets, such as CICIDS and UNSW-NB15, assessing\nits performance across diverse cyberattack scenarios. Experimental results\ndemonstrate that the proposed framework achieves high detection rates with\nminimal false positives, all while significantly reducing computational demands\ncompared to traditional deep learning methods. The key contributions of this\nwork include: (1) a novel interpretable AI-based cybersecurity model tailored\nfor edge computing environments, (2) an optimized lightweight deep learning\napproach for real-time cyber threat detection, and (3) a comprehensive analysis\nof explainability techniques in AI-driven cybersecurity applications."}
{"id": "2504.16786", "pdf": "https://arxiv.org/pdf/2504.16786", "abs": "https://arxiv.org/abs/2504.16786", "authors": ["Fengwei Zhou", "Jiafei Song", "Wenjin Jason Li", "Gengjian Xue", "Zhikang Zhao", "Yichao Lu", "Bailin Na"], "title": "MOOSComp: Improving Lightweight Long-Context Compressor via Mitigating Over-Smoothing and Incorporating Outlier Scores", "categories": ["cs.CL", "cs.LG"], "comment": null, "summary": "Recent advances in large language models have significantly improved their\nability to process long-context input, but practical applications are\nchallenged by increased inference time and resource consumption, particularly\nin resource-constrained environments. To address these challenges, we propose\nMOOSComp, a token-classification-based long-context compression method that\nenhances the performance of a BERT-based compressor by mitigating the\nover-smoothing problem and incorporating outlier scores. In the training phase,\nwe add an inter-class cosine similarity loss term to penalize excessively\nsimilar token representations, thereby improving the token classification\naccuracy. During the compression phase, we introduce outlier scores to preserve\nrare but critical tokens that are prone to be discarded in task-agnostic\ncompression. These scores are integrated with the classifier's output, making\nthe compressor more generalizable to various tasks. Superior performance is\nachieved at various compression ratios on long-context understanding and\nreasoning benchmarks. Moreover, our method obtains a speedup of 3.3x at a 4x\ncompression ratio on a resource-constrained mobile device."}
{"id": "2504.16433", "pdf": "https://arxiv.org/pdf/2504.16433", "abs": "https://arxiv.org/abs/2504.16433", "authors": ["Hariseetharam Gunduboina", "Muhammad Haris Khan", "Biplab Banerjee"], "title": "FrogDogNet: Fourier frequency Retained visual prompt Output Guidance for Domain Generalization of CLIP in Remote Sensing", "categories": ["cs.CV"], "comment": null, "summary": "In recent years, large-scale vision-language models (VLMs) like CLIP have\ngained attention for their zero-shot inference using instructional text\nprompts. While these models excel in general computer vision, their potential\nfor domain generalization in remote sensing (RS) remains underexplored.\nExisting approaches enhance prompt learning by generating visual prompt tokens\nbut rely on full-image features, introducing noise and background artifacts\nthat vary within a class, causing misclassification. To address this, we\npropose FrogDogNet, a novel prompt learning framework integrating Fourier\nfrequency filtering and self-attention to improve RS scene classification and\ndomain generalization. FrogDogNet selectively retains invariant low-frequency\ncomponents while eliminating noise and irrelevant backgrounds, ensuring robust\nfeature representation across domains. The model first extracts significant\nfeatures via projection and self-attention, then applies frequency-based\nfiltering to preserve essential structural information for prompt learning.\nExtensive experiments on four RS datasets and three domain generalization tasks\nshow that FrogDogNet consistently outperforms state-of-the-art prompt learning\nmethods, demonstrating superior adaptability across domain shifts. Our findings\nhighlight the effectiveness of frequency-based invariant feature retention in\ngeneralization, paving the way for broader applications. Our code is available\nat https://github.com/HariseetharamG/FrogDogNet"}
{"id": "2504.16438", "pdf": "https://arxiv.org/pdf/2504.16438", "abs": "https://arxiv.org/abs/2504.16438", "authors": ["Charlie Hou", "Mei-Yu Wang", "Yige Zhu", "Daniel Lazar", "Giulia Fanti"], "title": "Private Federated Learning using Preference-Optimized Synthetic Data", "categories": ["cs.LG", "cs.AI", "cs.CR", "cs.DC"], "comment": "Spotlight presentation at SynthData Workshop ICLR25", "summary": "In practical settings, differentially private Federated learning (DP-FL) is\nthe dominant method for training models from private, on-device client data.\nRecent work has suggested that DP-FL may be enhanced or outperformed by methods\nthat use DP synthetic data (Wu et al., 2024; Hou et al., 2024). The primary\nalgorithms for generating DP synthetic data for FL applications require careful\nprompt engineering based on public information and/or iterative private client\nfeedback. Our key insight is that the private client feedback collected by\nprior DP synthetic data methods (Hou et al., 2024; Xie et al., 2024) can be\nviewed as a preference ranking. Our algorithm, Preference Optimization for\nPrivate Client Data (POPri) harnesses client feedback using preference\noptimization algorithms such as Direct Preference Optimization (DPO) to\nfine-tune LLMs to generate high-quality DP synthetic data. To evaluate POPri,\nwe release LargeFedBench, a new federated text benchmark for uncontaminated LLM\nevaluations on federated client data. POPri substantially improves the utility\nof DP synthetic data relative to prior work on LargeFedBench datasets and an\nexisting benchmark from Xie et al. (2024). POPri closes the gap between\nnext-token prediction accuracy in the fully-private and non-private settings by\nup to 68%, compared to 52% for prior synthetic data methods, and 10% for\nstate-of-the-art DP federated learning methods. The code and data are available\nat https://github.com/meiyuw/POPri."}
{"id": "2504.16120", "pdf": "https://arxiv.org/pdf/2504.16120", "abs": "https://arxiv.org/abs/2504.16120", "authors": ["Chaima Njeh", "Haïfa Nakouri", "Fehmi Jaafar"], "title": "A Data-Centric Approach for Safe and Secure Large Language Models against Threatening and Toxic Content", "categories": ["cs.CR", "cs.AI"], "comment": "This paper is under revision in the International Journal of\n  Information Security", "summary": "Large Language Models (LLM) have made remarkable progress, but concerns about\npotential biases and harmful content persist. To address these apprehensions,\nwe introduce a practical solution for ensuring LLM's safe and ethical use. Our\nnovel approach focuses on a post-generation correction mechanism, the\nBART-Corrective Model, which adjusts generated content to ensure safety and\nsecurity. Unlike relying solely on model fine-tuning or prompt engineering, our\nmethod provides a robust data-centric alternative for mitigating harmful\ncontent. We demonstrate the effectiveness of our approach through experiments\non multiple toxic datasets, which show a significant reduction in mean toxicity\nand jail-breaking scores after integration. Specifically, our results show a\nreduction of 15% and 21% in mean toxicity and jail-breaking scores with GPT-4,\na substantial reduction of 28% and 5% with PaLM2, a reduction of approximately\n26% and 23% with Mistral-7B, and a reduction of 11.1% and 19% with Gemma-2b-it.\nThese results demonstrate the potential of our approach to improve the safety\nand security of LLM, making them more suitable for real-world applications."}
{"id": "2504.16787", "pdf": "https://arxiv.org/pdf/2504.16787", "abs": "https://arxiv.org/abs/2504.16787", "authors": ["Ningning Zhang", "Chi Zhang", "Zhizhong Tan", "Xingxing Yang", "Weiping Deng", "Wenyong Wang"], "title": "Credible plan-driven RAG method for Multi-hop Question Answering", "categories": ["cs.CL", "cs.AI", "I.2.0"], "comment": "18 pages, 3 figures", "summary": "Multi-hop question answering (QA) presents a considerable challenge for\nRetrieval-Augmented Generation (RAG), requiring the structured decomposition of\ncomplex queries into logical reasoning paths and the generation of dependable\nintermediate results. However, deviations in reasoning paths or errors in\nintermediate results, which are common in current RAG methods, may propagate\nand accumulate throughout the reasoning process, diminishing the accuracy of\nthe answer to complex queries. To address this challenge, we propose the\nPlan-then-Act-and-Review (PAR RAG) framework, which is organized into three key\nstages: planning, act, and review, and aims to offer an interpretable and\nincremental reasoning paradigm for accurate and reliable multi-hop question\nanswering by mitigating error propagation.PAR RAG initially applies a top-down\nproblem decomposition strategy, formulating a comprehensive plan that\nintegrates multiple executable steps from a holistic viewpoint. This approach\navoids the pitfalls of local optima common in traditional RAG methods, ensuring\nthe accuracy of the entire reasoning path. Subsequently, PAR RAG incorporates a\nplan execution mechanism based on multi-granularity verification. By utilizing\nboth coarse-grained similarity information and fine-grained relevant data, the\nframework thoroughly checks and adjusts intermediate results, ensuring process\naccuracy while effectively managing error propagation and amplification.\nExperimental results on multi-hop QA datasets demonstrate that the PAR RAG\nframework substantially outperforms existing state-of-the-art methods in key\nmetrics, including EM and F1 scores."}
{"id": "2504.16443", "pdf": "https://arxiv.org/pdf/2504.16443", "abs": "https://arxiv.org/abs/2504.16443", "authors": ["Duy-Tho Le", "Trung Pham", "Jianfei Cai", "Hamid Rezatofighi"], "title": "Marginalized Generalized IoU (MGIoU): A Unified Objective Function for Optimizing Any Convex Parametric Shapes", "categories": ["cs.CV"], "comment": "8 pages", "summary": "Optimizing the similarity between parametric shapes is crucial for numerous\ncomputer vision tasks, where Intersection over Union (IoU) stands as the\ncanonical measure. However, existing optimization methods exhibit significant\nshortcomings: regression-based losses like L1/L2 lack correlation with IoU,\nIoU-based losses are unstable and limited to simple shapes, and task-specific\nmethods are computationally intensive and not generalizable accross domains. As\na result, the current landscape of parametric shape objective functions has\nbecome scattered, with each domain proposing distinct IoU approximations. To\naddress this, we unify the parametric shape optimization objective functions by\nintroducing Marginalized Generalized IoU (MGIoU), a novel loss function that\novercomes these challenges by projecting structured convex shapes onto their\nunique shape Normals to compute one-dimensional normalized GIoU. MGIoU offers a\nsimple, efficient, fully differentiable approximation strongly correlated with\nIoU. We then extend MGIoU to MGIoU+ that supports optimizing unstructured\nconvex shapes. Together, MGIoU and MGIoU+ unify parametric shape optimization\nacross diverse applications. Experiments on standard benchmarks demonstrate\nthat MGIoU and MGIoU+ consistently outperform existing losses while reducing\nloss computation latency by 10-40x. Additionally, MGIoU and MGIoU+ satisfy\nmetric properties and scale-invariance, ensuring robustness as an objective\nfunction. We further propose MGIoU- for minimizing overlaps in tasks like\ncollision-free trajectory prediction. Code is available at\nhttps://ldtho.github.io/MGIoU"}
{"id": "2504.16447", "pdf": "https://arxiv.org/pdf/2504.16447", "abs": "https://arxiv.org/abs/2504.16447", "authors": ["Jeesuk Shin", "Cheolwoong Kim", "Sunwoong Yang", "Minseo Lee", "Sung Joong Kim", "Joongoo Jeon"], "title": "Node Assigned physics-informed neural networks for thermal-hydraulic system simulation: CVH/FL module", "categories": ["cs.LG"], "comment": "40 pages, 12 figures. Jeesuk Shin and Cheolwoong Kim contributed\n  equally to this work. Sung Joong Kim and Joongoo Jeon are co-corresponding\n  authors", "summary": "Severe accidents (SAs) in nuclear power plants have been analyzed using\nthermal-hydraulic (TH) system codes such as MELCOR and MAAP. These codes\nefficiently simulate the progression of SAs, while they still have inherent\nlimitations due to their inconsistent finite difference schemes. The use of\nempirical schemes incorporating both implicit and explicit formulations\ninherently induces unidirectional coupling in multi-physics analyses. The\nobjective of this study is to develop a novel numerical method for TH system\ncodes using physics-informed neural network (PINN). They have shown strength in\nsolving multi-physics due to the innate feature of neural networks-automatic\ndifferentiation. We propose a node-assigned PINN (NA-PINN) that is suitable for\nthe control volume approach-based system codes. NA-PINN addresses the issue of\nspatial governing equation variation by assigning an individual network to each\nnodalization of the system code, such that spatial information is excluded from\nboth the input and output domains, and each subnetwork learns to approximate a\npurely temporal solution. In this phase, we evaluated the accuracy of the PINN\nmethods for the hydrodynamic module. In the 6 water tank simulation, PINN and\nNA-PINN showed maximum absolute errors of 1.678 and 0.007, respectively. It\nshould be noted that only NA-PINN demonstrated acceptable accuracy. To the best\nof the authors' knowledge, this is the first study to successfully implement a\nsystem code using PINN. Our future work involves extending NA-PINN to a\nmulti-physics solver and developing it in a surrogate manner."}
{"id": "2504.16122", "pdf": "https://arxiv.org/pdf/2504.16122", "abs": "https://arxiv.org/abs/2504.16122", "authors": ["Xuhui Zhou", "Zhe Su", "Sophie Feng", "Jiaxu Zhou", "Jen-tse Huang", "Hsien-Te Kao", "Spencer Lynch", "Svitlana Volkova", "Tongshuang Sherry Wu", "Anita Woolley", "Hao Zhu", "Maarten Sap"], "title": "SOTOPIA-S4: a user-friendly system for flexible, customizable, and large-scale social simulation", "categories": ["cs.CY", "cs.AI"], "comment": "The first author and the second author contributed equally", "summary": "Social simulation through large language model (LLM) agents is a promising\napproach to explore and validate hypotheses related to social science questions\nand LLM agents behavior. We present SOTOPIA-S4, a fast, flexible, and scalable\nsocial simulation system that addresses the technical barriers of current\nframeworks while enabling practitioners to generate multi-turn and multi-party\nLLM-based interactions with customizable evaluation metrics for hypothesis\ntesting. SOTOPIA-S4 comes as a pip package that contains a simulation engine,\nan API server with flexible RESTful APIs for simulation management, and a web\ninterface that enables both technical and non-technical users to design, run,\nand analyze simulations without programming. We demonstrate the usefulness of\nSOTOPIA-S4 with two use cases involving dyadic hiring negotiation and\nmulti-party planning scenarios."}
{"id": "2504.16795", "pdf": "https://arxiv.org/pdf/2504.16795", "abs": "https://arxiv.org/abs/2504.16795", "authors": ["Xiang Hu", "Jiaqi Leng", "Jun Zhao", "Kewei Tu", "Wei Wu"], "title": "Random Long-Context Access for Mamba via Hardware-aligned Hierarchical Sparse Attention", "categories": ["cs.CL", "cs.AI"], "comment": "preprint", "summary": "A key advantage of Recurrent Neural Networks (RNNs) over Transformers is\ntheir linear computational and space complexity enables faster training and\ninference for long sequences. However, RNNs are fundamentally unable to\nrandomly access historical context, and simply integrating attention mechanisms\nmay undermine their efficiency advantages. To overcome this limitation, we\npropose \\textbf{H}ierarchical \\textbf{S}parse \\textbf{A}ttention (HSA), a novel\nattention mechanism that enhances RNNs with long-range random access\nflexibility while preserving their merits in efficiency and length\ngeneralization. HSA divides inputs into chunks, selecting the top-$k$ chunks\nand hierarchically aggregates information. The core innovation lies in learning\ntoken-to-chunk relevance based on fine-grained token-level information inside\neach chunk. This approach enhances the precision of chunk selection across both\nin-domain and out-of-domain context lengths. To make HSA efficient, we further\nintroduce a hardware-aligned kernel design. By combining HSA with Mamba, we\nintroduce RAMba, which achieves perfect accuracy in passkey retrieval across 64\nmillion contexts despite pre-training on only 4K-length contexts, and\nsignificant improvements on various downstream tasks, with nearly constant\nmemory footprint. These results show RAMba's huge potential in long-context\nmodeling."}
{"id": "2504.16455", "pdf": "https://arxiv.org/pdf/2504.16455", "abs": "https://arxiv.org/abs/2504.16455", "authors": ["Shun Zou", "Yi Zou", "Juncheng Li", "Guangwei Gao", "Guojun Qi"], "title": "Cross Paradigm Representation and Alignment Transformer for Image Deraining", "categories": ["cs.CV"], "comment": "code: https://github.com/zs1314/CPRAformer", "summary": "Transformer-based networks have achieved strong performance in low-level\nvision tasks like image deraining by utilizing spatial or channel-wise\nself-attention. However, irregular rain patterns and complex geometric overlaps\nchallenge single-paradigm architectures, necessitating a unified framework to\nintegrate complementary global-local and spatial-channel representations. To\naddress this, we propose a novel Cross Paradigm Representation and Alignment\nTransformer (CPRAformer). Its core idea is the hierarchical representation and\nalignment, leveraging the strengths of both paradigms (spatial-channel and\nglobal-local) to aid image reconstruction. It bridges the gap within and\nbetween paradigms, aligning and coordinating them to enable deep interaction\nand fusion of features. Specifically, we use two types of self-attention in the\nTransformer blocks: sparse prompt channel self-attention (SPC-SA) and spatial\npixel refinement self-attention (SPR-SA). SPC-SA enhances global channel\ndependencies through dynamic sparsity, while SPR-SA focuses on spatial rain\ndistribution and fine-grained texture recovery. To address the feature\nmisalignment and knowledge differences between them, we introduce the Adaptive\nAlignment Frequency Module (AAFM), which aligns and interacts with features in\na two-stage progressive manner, enabling adaptive guidance and complementarity.\nThis reduces the information gap within and between paradigms. Through this\nunified cross-paradigm dynamic interaction framework, we achieve the extraction\nof the most valuable interactive fusion information from the two paradigms.\nExtensive experiments demonstrate that our model achieves state-of-the-art\nperformance on eight benchmark datasets and further validates CPRAformer's\nrobustness in other image restoration tasks and downstream applications."}
{"id": "2504.16450", "pdf": "https://arxiv.org/pdf/2504.16450", "abs": "https://arxiv.org/abs/2504.16450", "authors": ["Rubing Yang", "Pratik Chaudhari"], "title": "An Effective Gram Matrix Characterizes Generalization in Deep Networks", "categories": ["cs.LG", "stat.ML"], "comment": null, "summary": "We derive a differential equation that governs the evolution of the\ngeneralization gap when a deep network is trained by gradient descent. This\ndifferential equation is controlled by two quantities, a contraction factor\nthat brings together trajectories corresponding to slightly different datasets,\nand a perturbation factor that accounts for them training on different\ndatasets. We analyze this differential equation to compute an ``effective Gram\nmatrix'' that characterizes the generalization gap after training in terms of\nthe alignment between this Gram matrix and a certain initial ``residual''.\nEmpirical evaluations on image classification datasets indicate that this\nanalysis can predict the test loss accurately. Further, at any point during\ntraining, the residual predominantly lies in the subspace of the effective Gram\nmatrix with the smallest eigenvalues. This indicates that the training process\nis benign, i.e., it does not lead to significant deterioration of the\ngeneralization gap (which is zero at initialization). The alignment between the\neffective Gram matrix and the residual is different for different datasets and\narchitectures. The match/mismatch of the data and the architecture is primarily\nresponsible for good/bad generalization."}
{"id": "2504.16130", "pdf": "https://arxiv.org/pdf/2504.16130", "abs": "https://arxiv.org/abs/2504.16130", "authors": ["Pengju Ren", "Ri-gui Zhou", "Yaochong Li"], "title": "A Self-supervised Learning Method for Raman Spectroscopy based on Masked Autoencoders", "categories": ["eess.SP", "cs.AI", "cs.LG"], "comment": "15 pages, 10 figures", "summary": "Raman spectroscopy serves as a powerful and reliable tool for analyzing the\nchemical information of substances. The integration of Raman spectroscopy with\ndeep learning methods enables rapid qualitative and quantitative analysis of\nmaterials. Most existing approaches adopt supervised learning methods. Although\nsupervised learning has achieved satisfactory accuracy in spectral analysis, it\nis still constrained by costly and limited well-annotated spectral datasets for\ntraining. When spectral annotation is challenging or the amount of annotated\ndata is insufficient, the performance of supervised learning in spectral\nmaterial identification declines. In order to address the challenge of feature\nextraction from unannotated spectra, we propose a self-supervised learning\nparadigm for Raman Spectroscopy based on a Masked AutoEncoder, termed SMAE.\nSMAE does not require any spectral annotations during pre-training. By randomly\nmasking and then reconstructing the spectral information, the model learns\nessential spectral features. The reconstructed spectra exhibit certain\ndenoising properties, improving the signal-to-noise ratio (SNR) by more than\ntwofold. Utilizing the network weights obtained from masked pre-training, SMAE\nachieves clustering accuracy of over 80% for 30 classes of isolated bacteria in\na pathogenic bacterial dataset, demonstrating significant improvements compared\nto classical unsupervised methods and other state-of-the-art deep clustering\nmethods. After fine-tuning the network with a limited amount of annotated data,\nSMAE achieves an identification accuracy of 83.90% on the test set, presenting\ncompetitive performance against the supervised ResNet (83.40%)."}
{"id": "2504.16813", "pdf": "https://arxiv.org/pdf/2504.16813", "abs": "https://arxiv.org/abs/2504.16813", "authors": ["Sima Iranmanesh", "Hadeel Saadany", "Edlira Vakaj"], "title": "LLM-assisted Graph-RAG Information Extraction from IFC Data", "categories": ["cs.CL"], "comment": "2025 European Conference on Computing in Construction", "summary": "IFC data has become the general building information standard for\ncollaborative work in the construction industry. However, IFC data can be very\ncomplicated because it allows for multiple ways to represent the same product\ninformation. In this research, we utilise the capabilities of LLMs to parse the\nIFC data with Graph Retrieval-Augmented Generation (Graph-RAG) technique to\nretrieve building object properties and their relations. We will show that,\ndespite limitations due to the complex hierarchy of the IFC data, the Graph-RAG\nparsing enhances generative LLMs like GPT-4o with graph-based knowledge,\nenabling natural language query-response retrieval without the need for a\ncomplex pipeline."}
{"id": "2504.16467", "pdf": "https://arxiv.org/pdf/2504.16467", "abs": "https://arxiv.org/abs/2504.16467", "authors": ["Qishan He", "Lingjun Zhao", "Ru Luo", "Siqian Zhang", "Lin Lei", "Kefeng Ji", "Gangyao Kuang"], "title": "MTSGL: Multi-Task Structure Guided Learning for Robust and Interpretable SAR Aircraft Recognition", "categories": ["cs.CV"], "comment": null, "summary": "Aircraft recognition in synthetic aperture radar (SAR) imagery is a\nfundamental mission in both military and civilian applications. Recently deep\nlearning (DL) has emerged a dominant paradigm for its explosive performance on\nextracting discriminative features. However, current classification algorithms\nfocus primarily on learning decision hyperplane without enough comprehension on\naircraft structural knowledge. Inspired by the fined aircraft annotation\nmethods for optical remote sensing images (RSI), we first introduce a\nstructure-based SAR aircraft annotations approach to provide structural and\ncompositional supplement information. On this basis, we propose a multi-task\nstructure guided learning (MTSGL) network for robust and interpretable SAR\naircraft recognition. Besides the classification task, MTSGL includes a\nstructural semantic awareness (SSA) module and a structural consistency\nregularization (SCR) module. The SSA is designed to capture structure semantic\ninformation, which is conducive to gain human-like comprehension of aircraft\nknowledge. The SCR helps maintain the geometric consistency between the\naircraft structure in SAR imagery and the proposed annotation. In this process,\nthe structural attribute can be disentangled in a geometrically meaningful\nmanner. In conclusion, the MTSGL is presented with the expert-level aircraft\nprior knowledge and structure guided learning paradigm, aiming to comprehend\nthe aircraft concept in a way analogous to the human cognitive process.\nExtensive experiments are conducted on a self-constructed multi-task SAR\naircraft recognition dataset (MT-SARD) and the effective results illustrate the\nsuperiority of robustness and interpretation ability of the proposed MTSGL."}
{"id": "2504.16501", "pdf": "https://arxiv.org/pdf/2504.16501", "abs": "https://arxiv.org/abs/2504.16501", "authors": ["Seungyoon Choi", "Sein Kim", "Hongseok Kang", "Wonjoong Kim", "Chanyoung Park"], "title": "Dynamic Time-aware Continual User Representation Learning", "categories": ["cs.LG"], "comment": null, "summary": "Traditional user modeling (UM) approaches have primarily focused on designing\nmodels for a single specific task, but they face limitations in generalization\nand adaptability across various tasks. Recognizing these challenges, recent\nstudies have shifted towards continual learning (CL)-based universal user\nrepresentation learning aiming to develop a single model capable of handling\nmultiple tasks. Despite advancements, existing methods are in fact evaluated\nunder an unrealistic scenario that does not consider the passage of time as\ntasks progress, which overlooks newly emerged items that may change the item\ndistribution of previous tasks. In this paper, we introduce a practical\nevaluation scenario on which CL-based universal user representation learning\napproaches should be evaluated, which takes into account the passage of time as\ntasks progress. Then, we propose a novel framework Dynamic Time-aware continual\nuser representation learner, named DITTO, designed to alleviate catastrophic\nforgetting despite continuous shifts in item distribution, while also allowing\nthe knowledge acquired from previous tasks to adapt to the current shifted item\ndistribution. Through our extensive experiments, we demonstrate the superiority\nof DITTO over state-of-the-art methods under a practical evaluation scenario.\nOur source code is available at\nhttps://github.com/seungyoon-Choi/DITTO_official."}
{"id": "2504.16131", "pdf": "https://arxiv.org/pdf/2504.16131", "abs": "https://arxiv.org/abs/2504.16131", "authors": ["Samuel Yen-Chi Chen", "Zhiding Liang"], "title": "Introduction to Quantum Machine Learning and Quantum Architecture Search", "categories": ["quant-ph", "cs.AI", "cs.ET", "cs.LG", "cs.NE"], "comment": "ISCAS 2025 Tutorial", "summary": "Recent advancements in quantum computing (QC) and machine learning (ML) have\nfueled significant research efforts aimed at integrating these two\ntransformative technologies. Quantum machine learning (QML), an emerging\ninterdisciplinary field, leverages quantum principles to enhance the\nperformance of ML algorithms. Concurrently, the exploration of systematic and\nautomated approaches for designing high-performance quantum circuit\narchitectures for QML tasks has gained prominence, as these methods empower\nresearchers outside the quantum computing domain to effectively utilize\nquantum-enhanced tools. This tutorial will provide an in-depth overview of\nrecent breakthroughs in both areas, highlighting their potential to expand the\napplication landscape of QML across diverse fields."}
{"id": "2504.16832", "pdf": "https://arxiv.org/pdf/2504.16832", "abs": "https://arxiv.org/abs/2504.16832", "authors": ["Luu Quy Tung", "Hoang Quoc Viet", "Vo Trong Thu"], "title": "GreenMind: A Next-Generation Vietnamese Large Language Model for Structured and Logical Reasoning", "categories": ["cs.CL"], "comment": null, "summary": "Chain-of-Thought (CoT) is a robust approach for tackling LLM tasks that\nrequire intermediate reasoning steps prior to generating a final answer. In\nthis paper, we present GreenMind-Medium-14B-R1, the Vietnamese reasoning model\ninspired by the finetuning strategy based on Group Relative Policy\nOptimization. We also leverage a high-quality Vietnamese synthesized reasoning\ndataset and design two reward functions to tackle the main limitations of this\ntechnique: (i) language mixing, where we explicitly detect the presence of\nbiased language characters during the process of sampling tokens, and (ii) we\nleverage Sentence Transformer-based models to ensure that the generated\nreasoning content maintains factual correctness and does not distort the final\noutput. Experimental results on the Vietnamese dataset from the VLSP 2023\nChallenge demonstrate that our model outperforms prior works and enhances\nlinguistic consistency in its responses. Furthermore, we extend our evaluation\nto SeaExam-a multilingual multiple-choice dataset, showing the effectiveness of\nour reasoning method compared to few-shot prompting techniques."}
{"id": "2504.16471", "pdf": "https://arxiv.org/pdf/2504.16471", "abs": "https://arxiv.org/abs/2504.16471", "authors": ["Boyue Xu", "Ruichao Hou", "Tongwei Ren", "Gangshan Wu"], "title": "RGB-D Video Object Segmentation via Enhanced Multi-store Feature Memory", "categories": ["cs.CV"], "comment": null, "summary": "The RGB-Depth (RGB-D) Video Object Segmentation (VOS) aims to integrate the\nfine-grained texture information of RGB with the spatial geometric clues of\ndepth modality, boosting the performance of segmentation. However,\noff-the-shelf RGB-D segmentation methods fail to fully explore cross-modal\ninformation and suffer from object drift during long-term prediction. In this\npaper, we propose a novel RGB-D VOS method via multi-store feature memory for\nrobust segmentation. Specifically, we design the hierarchical modality\nselection and fusion, which adaptively combines features from both modalities.\nAdditionally, we develop a segmentation refinement module that effectively\nutilizes the Segmentation Anything Model (SAM) to refine the segmentation mask,\nensuring more reliable results as memory to guide subsequent segmentation\ntasks. By leveraging spatio-temporal embedding and modality embedding, mixed\nprompts and fused images are fed into SAM to unleash its potential in RGB-D\nVOS. Experimental results show that the proposed method achieves\nstate-of-the-art performance on the latest RGB-D VOS benchmark."}
{"id": "2504.16506", "pdf": "https://arxiv.org/pdf/2504.16506", "abs": "https://arxiv.org/abs/2504.16506", "authors": ["Ruxue Shi", "Yili Wang", "Mengnan Du", "Xu Shen", "Xin Wang"], "title": "A Comprehensive Survey of Synthetic Tabular Data Generation", "categories": ["cs.LG"], "comment": null, "summary": "Tabular data remains one of the most prevalent and critical data formats\nacross diverse real-world applications. However, its effective use in machine\nlearning (ML) is often constrained by challenges such as data scarcity, privacy\nconcerns, and class imbalance. Synthetic data generation has emerged as a\npromising solution, leveraging generative models to learn the distribution of\nreal datasets and produce high-fidelity, privacy-preserving samples. Various\ngenerative paradigms have been explored, including energy-based models (EBMs),\nvariational autoencoders (VAEs), generative adversarial networks (GANs), large\nlanguage models (LLMs), and diffusion models. While several surveys have\ninvestigated synthetic tabular data generation, most focus on narrow subdomains\nor specific generative methods, such as GANs, diffusion models, or\nprivacy-preserving techniques. This limited scope often results in fragmented\ninsights, lacking a comprehensive synthesis that bridges diverse approaches. In\nparticular, recent advances driven by LLMs and diffusion-based models remain\nunderexplored. This gap hinders a holistic understanding of the field`s\nevolution, methodological interplay, and open challenges. To address this, our\nsurvey provides a unified and systematic review of synthetic tabular data\ngeneration. Our contributions are threefold: (1) we propose a comprehensive\ntaxonomy that organizes existing methods into traditional approaches,\ndiffusion-based methods, and LLM-based models, and provide an in-depth\ncomparative analysis; (2) we detail the complete pipeline for synthetic tabular\ndata generation, including data synthesis, post-processing, and evaluation; (3)\nwe identify major challenges, explore real-world applications, and outline open\nresearch questions and future directions to guide future work in this rapidly\nevolving area."}
{"id": "2504.16132", "pdf": "https://arxiv.org/pdf/2504.16132", "abs": "https://arxiv.org/abs/2504.16132", "authors": ["Andrew M. Olney", "Sidney K. D'Mello", "Natalie Person", "Whitney Cade", "Patrick Hays", "Claire W. Dempsey", "Blair Lehman", "Betsy Williams", "Art Graesser"], "title": "Efficacy of a Computer Tutor that Models Expert Human Tutors", "categories": ["cs.CY", "cs.AI", "I.2.4; I.2.7; K.3.1"], "comment": "Shortened version of this paper has been accepted to AIED 2025", "summary": "Tutoring is highly effective for promoting learning. However, the\ncontribution of expertise to tutoring effectiveness is unclear and continues to\nbe debated. We conducted a 9-week learning efficacy study of an intelligent\ntutoring system (ITS) for biology modeled on expert human tutors with two\ncontrol conditions: human tutors who were experts in the domain but not in\ntutoring and a no-tutoring condition. All conditions were supplemental to\nclassroom instruction, and students took learning tests immediately before and\nafter tutoring sessions as well as delayed tests 1-2 weeks later. Analysis\nusing logistic mixed-effects modeling indicates significant positive effects on\nthe immediate post-test for the ITS (d =.71) and human tutors (d =.66) which\nare in the 99th percentile of meta-analytic effects, as well as significant\npositive effects on the delayed post-test for the ITS (d =.36) and human tutors\n(d =.39). We discuss implications for the role of expertise in tutoring and the\ndesign of future studies."}
{"id": "2504.16855", "pdf": "https://arxiv.org/pdf/2504.16855", "abs": "https://arxiv.org/abs/2504.16855", "authors": ["Zijing Shi", "Meng Fang", "Ling Chen"], "title": "Monte Carlo Planning with Large Language Model for Text-Based Game Agents", "categories": ["cs.CL"], "comment": null, "summary": "Text-based games provide valuable environments for language-based autonomous\nagents. However, planning-then-learning paradigms, such as those combining\nMonte Carlo Tree Search (MCTS) and reinforcement learning (RL), are notably\ntime-consuming due to extensive iterations. Additionally, these algorithms\nperform uncertainty-driven exploration but lack language understanding and\nreasoning abilities. In this paper, we introduce the Monte Carlo planning with\nDynamic Memory-guided Large language model (MC-DML) algorithm. MC-DML leverages\nthe language understanding and reasoning capabilities of Large Language Models\n(LLMs) alongside the exploratory advantages of tree search algorithms.\nSpecifically, we enhance LLMs with in-trial and cross-trial memory mechanisms,\nenabling them to learn from past experiences and dynamically adjust action\nevaluations during planning. We conduct experiments on a series of text-based\ngames from the Jericho benchmark. Our results demonstrate that the MC-DML\nalgorithm significantly enhances performance across various games at the\ninitial planning phase, outperforming strong contemporary methods that require\nmultiple iterations. This demonstrates the effectiveness of our algorithm,\npaving the way for more efficient language-grounded planning in complex\nenvironments."}
{"id": "2504.16487", "pdf": "https://arxiv.org/pdf/2504.16487", "abs": "https://arxiv.org/abs/2504.16487", "authors": ["Yahao Lu", "Yuehui Li", "Xingyuan Guo", "Shuai Yuan", "Yukai Shi", "Liang Lin"], "title": "Rethinking Generalizable Infrared Small Target Detection: A Real-scene Benchmark and Cross-view Representation Learning", "categories": ["cs.CV"], "comment": "A benchmark associated with real-world scenes for the Infrared Small\n  Target Detection (ISTD) is presented", "summary": "Infrared small target detection (ISTD) is highly sensitive to sensor type,\nobservation conditions, and the intrinsic properties of the target. These\nfactors can introduce substantial variations in the distribution of acquired\ninfrared image data, a phenomenon known as domain shift. Such distribution\ndiscrepancies significantly hinder the generalization capability of ISTD models\nacross diverse scenarios. To tackle this challenge, this paper introduces an\nISTD framework enhanced by domain adaptation. To alleviate distribution shift\nbetween datasets and achieve cross-sample alignment, we introduce Cross-view\nChannel Alignment (CCA). Additionally, we propose the Cross-view Top-K Fusion\nstrategy, which integrates target information with diverse background features,\nenhancing the model' s ability to extract critical data characteristics. To\nfurther mitigate the impact of noise on ISTD, we develop a Noise-guided\nRepresentation learning strategy. This approach enables the model to learn more\nnoise-resistant feature representations, to improve its generalization\ncapability across diverse noisy domains. Finally, we develop a dedicated\ninfrared small target dataset, RealScene-ISTD. Compared to state-of-the-art\nmethods, our approach demonstrates superior performance in terms of detection\nprobability (Pd), false alarm rate (Fa), and intersection over union (IoU). The\ncode is available at: https://github.com/luy0222/RealScene-ISTD."}
{"id": "2504.16553", "pdf": "https://arxiv.org/pdf/2504.16553", "abs": "https://arxiv.org/abs/2504.16553", "authors": ["Mohammad Mahdi Abedi", "David Pardo", "Tariq Alkhalifah"], "title": "Least-Squares-Embedded Optimization for Accelerated Convergence of PINNs in Acoustic Wavefield Simulations", "categories": ["cs.LG", "physics.comp-ph", "physics.geo-ph"], "comment": null, "summary": "Physics-Informed Neural Networks (PINNs) have shown promise in solving\npartial differential equations (PDEs), including the frequency-domain Helmholtz\nequation. However, standard training of PINNs using gradient descent (GD)\nsuffers from slow convergence and instability, particularly for high-frequency\nwavefields. For scattered acoustic wavefield simulation based on Helmholtz\nequation, we derive a hybrid optimization framework that accelerates training\nconvergence by embedding a least-squares (LS) solver directly into the GD loss\nfunction. This formulation enables optimal updates for the linear output layer.\nOur method is applicable with or without perfectly matched layers (PML), and we\nprovide practical tensor-based implementations for both scenarios. Numerical\nexperiments on benchmark velocity models demonstrate that our approach achieves\nfaster convergence, higher accuracy, and improved stability compared to\nconventional PINN training. In particular, our results show that the\nLS-enhanced method converges rapidly even in cases where standard GD-based\ntraining fails. The LS solver operates on a small normal matrix, ensuring\nminimal computational overhead and making the method scalable for large-scale\nwavefield simulations."}
{"id": "2504.16133", "pdf": "https://arxiv.org/pdf/2504.16133", "abs": "https://arxiv.org/abs/2504.16133", "authors": ["Milad Leyli-abadi", "Ricardo J. Bessa", "Jan Viebahn", "Daniel Boos", "Clark Borst", "Alberto Castagna", "Ricardo Chavarriaga", "Mohamed Hassouna", "Bruno Lemetayer", "Giulia Leto", "Antoine Marot", "Maroua Meddeb", "Manuel Meyer", "Viola Schiaffonati", "Manuel Schneider", "Toni Waefler"], "title": "A Conceptual Framework for AI-based Decision Systems in Critical Infrastructures", "categories": ["cs.CY", "cs.AI"], "comment": null, "summary": "The interaction between humans and AI in safety-critical systems presents a\nunique set of challenges that remain partially addressed by existing\nframeworks. These challenges stem from the complex interplay of requirements\nfor transparency, trust, and explainability, coupled with the necessity for\nrobust and safe decision-making. A framework that holistically integrates human\nand AI capabilities while addressing these concerns is notably required,\nbridging the critical gaps in designing, deploying, and maintaining safe and\neffective systems. This paper proposes a holistic conceptual framework for\ncritical infrastructures by adopting an interdisciplinary approach. It\nintegrates traditionally distinct fields such as mathematics, decision theory,\ncomputer science, philosophy, psychology, and cognitive engineering and draws\non specialized engineering domains, particularly energy, mobility, and\naeronautics. The flexibility in its adoption is also demonstrated through its\ninstantiation on an already existing framework."}
{"id": "2504.16856", "pdf": "https://arxiv.org/pdf/2504.16856", "abs": "https://arxiv.org/abs/2504.16856", "authors": ["Alexander Shvets"], "title": "Emo Pillars: Knowledge Distillation to Support Fine-Grained Context-Aware and Context-Less Emotion Classification", "categories": ["cs.CL"], "comment": null, "summary": "Most datasets for sentiment analysis lack context in which an opinion was\nexpressed, often crucial for emotion understanding, and are mainly limited by a\nfew emotion categories. Foundation large language models (LLMs) like GPT-4\nsuffer from over-predicting emotions and are too resource-intensive. We design\nan LLM-based data synthesis pipeline and leverage a large model, Mistral-7b,\nfor the generation of training examples for more accessible, lightweight\nBERT-type encoder models. We focus on enlarging the semantic diversity of\nexamples and propose grounding the generation into a corpus of narratives to\nproduce non-repetitive story-character-centered utterances with unique contexts\nover 28 emotion classes. By running 700K inferences in 450 GPU hours, we\ncontribute with the dataset of 100K contextual and also 300K context-less\nexamples to cover both scenarios. We use it for fine-tuning pre-trained\nencoders, which results in several Emo Pillars models. We show that Emo Pillars\nmodels are highly adaptive to new domains when tuned to specific tasks such as\nGoEmotions, ISEAR, IEMOCAP, and EmoContext, reaching the SOTA performance on\nthe first three. We also validate our dataset, conducting statistical analysis\nand human evaluation, and confirm the success of our measures in utterance\ndiversification (although less for the neutral class) and context\npersonalization, while pointing out the need for improved handling of\nout-of-taxonomy labels within the pipeline."}
{"id": "2504.16499", "pdf": "https://arxiv.org/pdf/2504.16499", "abs": "https://arxiv.org/abs/2504.16499", "authors": ["Daniil Sinitsyn", "Linus Härenstam-Nielsen", "Daniel Cremers"], "title": "PRaDA: Projective Radial Distortion Averaging", "categories": ["cs.CV"], "comment": "Accepted at CVPR 2025. 8 pages + references", "summary": "We tackle the problem of automatic calibration of radially distorted cameras\nin challenging conditions. Accurately determining distortion parameters\ntypically requires either 1) solving the full Structure from Motion (SfM)\nproblem involving camera poses, 3D points, and the distortion parameters, which\nis only possible if many images with sufficient overlap are provided, or 2)\nrelying heavily on learning-based methods that are comparatively less accurate.\nIn this work, we demonstrate that distortion calibration can be decoupled from\n3D reconstruction, maintaining the accuracy of SfM-based methods while avoiding\nmany of the associated complexities. This is achieved by working in Projective\nSpace, where the geometry is unique up to a homography, which encapsulates all\ncamera parameters except for distortion. Our proposed method, Projective Radial\nDistortion Averaging, averages multiple distortion estimates in a fully\nprojective framework without creating 3d points and full bundle adjustment. By\nrelying on pairwise projective relations, our methods support any\nfeature-matching approaches without constructing point tracks across multiple\nimages."}
{"id": "2504.16559", "pdf": "https://arxiv.org/pdf/2504.16559", "abs": "https://arxiv.org/abs/2504.16559", "authors": ["Adam Izdebski", "Jan Olszewski", "Pankhil Gawade", "Krzysztof Koras", "Serra Korkmaz", "Valentin Rauscher", "Jakub M. Tomczak", "Ewa Szczurek"], "title": "Unified Molecule Generation and Property Prediction", "categories": ["cs.LG", "q-bio.QM", "68T01", "I.2.1"], "comment": "17 pages, 4 figures", "summary": "Modeling the joint distribution of the data samples and their properties\nallows to construct a single model for both data generation and property\nprediction, with synergistic capabilities reaching beyond purely generative or\npredictive models. However, training joint models presents daunting\narchitectural and optimization challenges. Here, we propose Hyformer, a\ntransformer-based joint model that successfully blends the generative and\npredictive functionalities, using an alternating attention mask together with a\nunified pre-training scheme. We show that Hyformer rivals other joint models,\nas well as state-of-the-art molecule generation and property prediction models.\nAdditionally, we show the benefits of joint modeling in downstream tasks of\nmolecular representation learning, hit identification and antimicrobial peptide\ndesign."}
{"id": "2504.16138", "pdf": "https://arxiv.org/pdf/2504.16138", "abs": "https://arxiv.org/abs/2504.16138", "authors": ["Iyngkarran Kumar", "Sam Manning"], "title": "Trends in Frontier AI Model Count: A Forecast to 2028", "categories": ["cs.CY", "cs.AI"], "comment": null, "summary": "Governments are starting to impose requirements on AI models based on how\nmuch compute was used to train them. For example, the EU AI Act imposes\nrequirements on providers of general-purpose AI with systemic risk, which\nincludes systems trained using greater than $10^{25}$ floating point operations\n(FLOP). In the United States' AI Diffusion Framework, a training compute\nthreshold of $10^{26}$ FLOP is used to identify \"controlled models\" which face\na number of requirements. We explore how many models such training compute\nthresholds will capture over time. We estimate that by the end of 2028, there\nwill be between 103-306 foundation models exceeding the $10^{25}$ FLOP\nthreshold put forward in the EU AI Act (90% CI), and 45-148 models exceeding\nthe $10^{26}$ FLOP threshold that defines controlled models in the AI Diffusion\nFramework (90% CI). We also find that the number of models exceeding these\nabsolute compute thresholds each year will increase superlinearly -- that is,\neach successive year will see more new models captured within the threshold\nthan the year before. Thresholds that are defined with respect to the largest\ntraining run to date (for example, such that all models within one order of\nmagnitude of the largest training run to date are captured by the threshold)\nsee a more stable trend, with a median forecast of 14-16 models being captured\nby this definition annually from 2025-2028."}
{"id": "2504.16858", "pdf": "https://arxiv.org/pdf/2504.16858", "abs": "https://arxiv.org/abs/2504.16858", "authors": ["Hanwen Du", "Bo Peng", "Xia Ning"], "title": "Planning with Diffusion Models for Target-Oriented Dialogue Systems", "categories": ["cs.CL"], "comment": null, "summary": "Target-Oriented Dialogue (TOD) remains a significant challenge in the LLM\nera, where strategic dialogue planning is crucial for directing conversations\ntoward specific targets. However, existing dialogue planning methods generate\ndialogue plans in a step-by-step sequential manner, and may suffer from\ncompounding errors and myopic actions. To address these limitations, we\nintroduce a novel dialogue planning framework, DiffTOD, which leverages\ndiffusion models to enable non-sequential dialogue planning. DiffTOD formulates\ndialogue planning as a trajectory generation problem with conditional guidance,\nand leverages a diffusion language model to estimate the likelihood of the\ndialogue trajectory. To optimize the dialogue action strategies, DiffTOD\nintroduces three tailored guidance mechanisms for different target types,\noffering flexible guidance towards diverse TOD targets at test time. Extensive\nexperiments across three diverse TOD settings show that DiffTOD can effectively\nperform non-myopic lookahead exploration and optimize action strategies over a\nlong horizon through non-sequential dialogue planning, and demonstrates strong\nflexibility across complex and diverse dialogue scenarios. Our code and data\nare accessible through https://anonymous.4open.science/r/DiffTOD."}
{"id": "2504.16515", "pdf": "https://arxiv.org/pdf/2504.16515", "abs": "https://arxiv.org/abs/2504.16515", "authors": ["Abdul Hannaan", "Zubair Shah", "Aiman Erbad", "Amr Mohamed", "Ali Safa"], "title": "Federated Learning of Low-Rank One-Shot Image Detection Models in Edge Devices with Scalable Accuracy and Compute Complexity", "categories": ["cs.CV", "cs.AI"], "comment": "accepted for publication at IEEE IWCMC 2025", "summary": "This paper introduces a novel federated learning framework termed LoRa-FL\ndesigned for training low-rank one-shot image detection models deployed on edge\ndevices. By incorporating low-rank adaptation techniques into one-shot\ndetection architectures, our method significantly reduces both computational\nand communication overhead while maintaining scalable accuracy. The proposed\nframework leverages federated learning to collaboratively train lightweight\nimage recognition models, enabling rapid adaptation and efficient deployment\nacross heterogeneous, resource-constrained devices. Experimental evaluations on\nthe MNIST and CIFAR10 benchmark datasets, both in an\nindependent-and-identically-distributed (IID) and non-IID setting, demonstrate\nthat our approach achieves competitive detection performance while\nsignificantly reducing communication bandwidth and compute complexity. This\nmakes it a promising solution for adaptively reducing the communication and\ncompute power overheads, while not sacrificing model accuracy."}
{"id": "2504.16580", "pdf": "https://arxiv.org/pdf/2504.16580", "abs": "https://arxiv.org/abs/2504.16580", "authors": ["Ignacio Peis", "Batuhan Koyuncu", "Isabel Valera", "Jes Frellsen"], "title": "Hyper-Transforming Latent Diffusion Models", "categories": ["cs.LG", "stat.ML"], "comment": null, "summary": "We introduce a novel generative framework for functions by integrating\nImplicit Neural Representations (INRs) and Transformer-based hypernetworks into\nlatent variable models. Unlike prior approaches that rely on MLP-based\nhypernetworks with scalability limitations, our method employs a\nTransformer-based decoder to generate INR parameters from latent variables,\naddressing both representation capacity and computational efficiency. Our\nframework extends latent diffusion models (LDMs) to INR generation by replacing\nstandard decoders with a Transformer-based hypernetwork, which can be trained\neither from scratch or via hyper-transforming-a strategy that fine-tunes only\nthe decoder while freezing the pre-trained latent space. This enables efficient\nadaptation of existing generative models to INR-based representations without\nrequiring full retraining."}
{"id": "2504.16139", "pdf": "https://arxiv.org/pdf/2504.16139", "abs": "https://arxiv.org/abs/2504.16139", "authors": ["Sridharan Sankaran"], "title": "Enhancing Trust Through Standards: A Comparative Risk-Impact Framework for Aligning ISO AI Standards with Global Ethical and Regulatory Contexts", "categories": ["cs.CY", "cs.AI"], "comment": null, "summary": "As artificial intelligence (AI) reshapes industries and societies, ensuring\nits trustworthiness-through mitigating ethical risks like bias, opacity, and\naccountability deficits-remains a global challenge. International Organization\nfor Standardization (ISO) AI standards, such as ISO/IEC 24027 and 24368, aim to\nfoster responsible development by embedding fairness, transparency, and risk\nmanagement into AI systems. However, their effectiveness varies across diverse\nregulatory landscapes, from the EU's risk-based AI Act to China's\nstability-focused measures and the U.S.'s fragmented state-led initiatives.\nThis paper introduces a novel Comparative Risk-Impact Assessment Framework to\nevaluate how well ISO standards address ethical risks within these contexts,\nproposing enhancements to strengthen their global applicability. By mapping ISO\nstandards to the EU AI Act and surveying regulatory frameworks in ten\nregions-including the UK, Canada, India, Japan, Singapore, South Korea, and\nBrazil-we establish a baseline for ethical alignment. The framework, applied to\ncase studies in the EU, US-Colorado, and China, reveals gaps: voluntary ISO\nstandards falter in enforcement (e.g., Colorado) and undervalue region-specific\nrisks like privacy (China). We recommend mandatory risk audits, region-specific\nannexes, and a privacy-focused module to enhance ISO's adaptability. This\napproach not only synthesizes global trends but also offers a replicable tool\nfor aligning standardization with ethical imperatives, fostering\ninteroperability and trust in AI worldwide. Policymakers and standards bodies\ncan leverage these insights to evolve AI governance, ensuring it meets diverse\nsocietal needs as the technology advances."}
{"id": "2504.16884", "pdf": "https://arxiv.org/pdf/2504.16884", "abs": "https://arxiv.org/abs/2504.16884", "authors": ["Joseph M. Denning", "Xiaohan", "Guo", "Bryor Snefjella", "Idan A. Blank"], "title": "Do Large Language Models know who did what to whom?", "categories": ["cs.CL"], "comment": null, "summary": "Large Language Models (LLMs) are commonly criticized for not understanding\nlanguage. However, many critiques focus on cognitive abilities that, in humans,\nare distinct from language processing. Here, we instead study a kind of\nunderstanding tightly linked to language: inferring who did what to whom\n(thematic roles) in a sentence. Does the central training objective of\nLLMs-word prediction-result in sentence representations that capture thematic\nroles? In two experiments, we characterized sentence representations in four\nLLMs. In contrast to human similarity judgments, in LLMs the overall\nrepresentational similarity of sentence pairs reflected syntactic similarity\nbut not whether their agent and patient assignments were identical vs.\nreversed. Furthermore, we found little evidence that thematic role information\nwas available in any subset of hidden units. However, some attention heads\nrobustly captured thematic roles, independently of syntax. Therefore, LLMs can\nextract thematic roles but, relative to humans, this information influences\ntheir representations more weakly."}
{"id": "2504.16516", "pdf": "https://arxiv.org/pdf/2504.16516", "abs": "https://arxiv.org/abs/2504.16516", "authors": ["Junrong Yue", "Yifan Zhang", "Chuan Qin", "Bo Li", "Xiaomin Lie", "Xinlei Yu", "Wenxin Zhang", "Zhendong Zhao"], "title": "Think Hierarchically, Act Dynamically: Hierarchical Multi-modal Fusion and Reasoning for Vision-and-Language Navigation", "categories": ["cs.CV", "cs.AI"], "comment": "11 pages, 4 figures, Submitted to ACM MM 2025", "summary": "Vision-and-Language Navigation (VLN) aims to enable embodied agents to follow\nnatural language instructions and reach target locations in real-world\nenvironments. While prior methods often rely on either global scene\nrepresentations or object-level features, these approaches are insufficient for\ncapturing the complex interactions across modalities required for accurate\nnavigation. In this paper, we propose a Multi-level Fusion and Reasoning\nArchitecture (MFRA) to enhance the agent's ability to reason over visual\nobservations, language instructions and navigation history. Specifically, MFRA\nintroduces a hierarchical fusion mechanism that aggregates multi-level\nfeatures-ranging from low-level visual cues to high-level semantic\nconcepts-across multiple modalities. We further design a reasoning module that\nleverages fused representations to infer navigation actions through\ninstruction-guided attention and dynamic context integration. By selectively\ncapturing and combining relevant visual, linguistic, and temporal signals, MFRA\nimproves decision-making accuracy in complex navigation scenarios. Extensive\nexperiments on benchmark VLN datasets including REVERIE, R2R, and SOON\ndemonstrate that MFRA achieves superior performance compared to\nstate-of-the-art methods, validating the effectiveness of multi-level modal\nfusion for embodied navigation."}
{"id": "2504.16585", "pdf": "https://arxiv.org/pdf/2504.16585", "abs": "https://arxiv.org/abs/2504.16585", "authors": ["Xiaofei Wu", "Rongmei Liang"], "title": "Enhancing Variable Selection in Large-scale Logistic Regression: Leveraging Manual Labeling with Beneficial Noise", "categories": ["cs.LG", "stat.CO", "stat.ML"], "comment": null, "summary": "In large-scale supervised learning, penalized logistic regression (PLR)\neffectively addresses the overfitting problem by introducing regularization\nterms yet its performance still depends on efficient variable selection\nstrategies. This paper theoretically demonstrates that label noise stemming\nfrom manual labeling, which is solely related to classification difficulty,\nrepresents a type of beneficial noise for variable selection in PLR. This\nbenefit is reflected in a more accurate estimation of the selected non-zero\ncoefficients when compared with the case where only truth labels are used.\nUnder large-scale settings, the sample size for PLR can become very large,\nmaking it infeasible to store on a single machine. In such cases, distributed\ncomputing methods are required to handle PLR model with manual labeling. This\npaper presents a partition-insensitive parallel algorithm founded on the ADMM\n(alternating direction method of multipliers) algorithm to address PLR by\nincorporating manual labeling. The partition insensitivity of the proposed\nalgorithm refers to the fact that the solutions obtained by the algorithm will\nnot change with the distributed storage of data. In addition, the algorithm has\nglobal convergence and a sublinear convergence rate. Experimental results\nindicate that, as compared with traditional variable selection classification\ntechniques, the PLR with manually-labeled noisy data achieves higher estimation\nand classification accuracy across multiple large-scale datasets."}
{"id": "2504.16142", "pdf": "https://arxiv.org/pdf/2504.16142", "abs": "https://arxiv.org/abs/2504.16142", "authors": ["Hangxu Liu", "Yaojie Sun", "Yu Wang"], "title": "A Non-Invasive Load Monitoring Method for Edge Computing Based on MobileNetV3 and Dynamic Time Regulation", "categories": ["eess.SP", "cs.AI", "cs.LG"], "comment": null, "summary": "In recent years, non-intrusive load monitoring (NILM) technology has\nattracted much attention in the related research field by virtue of its unique\nadvantage of utilizing single meter data to achieve accurate decomposition of\ndevice-level energy consumption. Cutting-edge methods based on machine learning\nand deep learning have achieved remarkable results in load decomposition\naccuracy by fusing time-frequency domain features. However, these methods\ngenerally suffer from high computational costs and huge memory requirements,\nwhich become the main obstacles for their deployment on resource-constrained\nmicrocontroller units (MCUs). To address these challenges, this study proposes\nan innovative Dynamic Time Warping (DTW) algorithm in the time-frequency domain\nand systematically compares and analyzes the performance of six machine\nlearning techniques in home electricity scenarios. Through complete\nexperimental validation on edge MCUs, this scheme successfully achieves a\nrecognition accuracy of 95%. Meanwhile, this study deeply optimizes the\nfrequency domain feature extraction process, which effectively reduces the\nrunning time by 55.55% and the storage overhead by about 34.6%. The algorithm\nperformance will be further optimized in future research work. Considering that\nthe elimination of voltage transformer design can significantly reduce the\ncost, the subsequent research will focus on this direction, and is committed to\nproviding more cost-effective solutions for the practical application of NILM,\nand providing a solid theoretical foundation and feasible technical paths for\nthe design of efficient NILM systems in edge computing environments."}
{"id": "2504.16913", "pdf": "https://arxiv.org/pdf/2504.16913", "abs": "https://arxiv.org/abs/2504.16913", "authors": ["Shifali Agrahari", "Sanasam Ranbir Singh"], "title": "Tracing Thought: Using Chain-of-Thought Reasoning to Identify the LLM Behind AI-Generated Text", "categories": ["cs.CL", "cs.AI"], "comment": "De-Factify 4: 4th Workshop on Multimodal Fact Checking and Hate\n  Speech Detection, co-located with AAAI 2025. Pennsylvania", "summary": "In recent years, the detection of AI-generated text has become a critical\narea of research due to concerns about academic integrity, misinformation, and\nethical AI deployment. This paper presents COT Fine-tuned, a novel framework\nfor detecting AI-generated text and identifying the specific language model.\nresponsible for generating the text. We propose a dual-task approach, where\nTask A involves classifying text as AI-generated or human-written, and Task B\nidentifies the specific LLM behind the text. The key innovation of our method\nlies in the use of Chain-of-Thought reasoning, which enables the model to\ngenerate explanations for its predictions, enhancing transparency and\ninterpretability. Our experiments demonstrate that COT Fine-tuned achieves high\naccuracy in both tasks, with strong performance in LLM identification and\nhuman-AI classification. We also show that the CoT reasoning process\ncontributes significantly to the models effectiveness and interpretability."}
{"id": "2504.16520", "pdf": "https://arxiv.org/pdf/2504.16520", "abs": "https://arxiv.org/abs/2504.16520", "authors": ["Wenwei Li", "Liyi Cai", "Wu Chen", "Anan Li"], "title": "A Few-Shot Metric Learning Method with Dual-Channel Attention for Cross-Modal Same-Neuron Identification", "categories": ["cs.CV", "q-bio.NC"], "comment": "23 pages, 9 figures, submitted to arXiv for public access", "summary": "In neuroscience research, achieving single-neuron matching across different\nimaging modalities is critical for understanding the relationship between\nneuronal structure and function. However, modality gaps and limited annotations\npresent significant challenges. We propose a few-shot metric learning method\nwith a dual-channel attention mechanism and a pretrained vision transformer to\nenable robust cross-modal neuron identification. The local and global channels\nextract soma morphology and fiber context, respectively, and a gating mechanism\nfuses their outputs. To enhance the model's fine-grained discrimination\ncapability, we introduce a hard sample mining strategy based on the\nMultiSimilarityMiner algorithm, along with the Circle Loss function.\nExperiments on two-photon and fMOST datasets demonstrate superior Top-K\naccuracy and recall compared to existing methods. Ablation studies and t-SNE\nvisualizations validate the effectiveness of each module. The method also\nachieves a favorable trade-off between accuracy and training efficiency under\ndifferent fine-tuning strategies. These results suggest that the proposed\napproach offers a promising technical solution for accurate single-cell level\nmatching and multimodal neuroimaging integration."}
{"id": "2504.16624", "pdf": "https://arxiv.org/pdf/2504.16624", "abs": "https://arxiv.org/abs/2504.16624", "authors": ["Leo Henry", "Thomas Neele", "Mohammad Mousavi", "Matteo Sammartino"], "title": "Compositional Active Learning of Synchronous Systems through Automated Alphabet Refinement", "categories": ["cs.LG", "cs.FL"], "comment": null, "summary": "Active automata learning infers automaton models of systems from behavioral\nobservations, a technique successfully applied to a wide range of domains.\nCompositional approaches for concurrent systems have recently emerged. We take\na significant step beyond available results, including those by the authors,\nand develop a general technique for compositional learning of a synchronizing\nparallel system with an unknown decomposition. Our approach automatically\nrefines the global alphabet into component alphabets while learning the\ncomponent models. We develop a theoretical treatment of distributions of\nalphabets, i.e., sets of possibly overlapping component alphabets. We\ncharacterize counter-examples that reveal inconsistencies with global\nobservations, and show how to systematically update the distribution to restore\nconsistency. We present a compositional learning algorithm implementing these\nideas, where learning counterexamples precisely correspond to distribution\ncounterexamples under well-defined conditions. We provide an implementation,\ncalled CoalA, using the state-of-the-art active learning library LearnLib. Our\nexperiments show that in more than 630 subject systems, CoalA delivers orders\nof magnitude improvements (up to five orders) in membership queries and in\nsystems with significant concurrency, it also achieves better scalability in\nthe number of equivalence queries."}
{"id": "2504.16144", "pdf": "https://arxiv.org/pdf/2504.16144", "abs": "https://arxiv.org/abs/2504.16144", "authors": ["Ahmed El Fekih Zguir", "Ferda Ofli", "Muhammad Imran"], "title": "Detecting Actionable Requests and Offers on Social Media During Crises Using LLMs", "categories": ["cs.IR", "cs.AI"], "comment": null, "summary": "Natural disasters often result in a surge of social media activity, including\nrequests for assistance, offers of help, sentiments, and general updates. To\nenable humanitarian organizations to respond more efficiently, we propose a\nfine-grained hierarchical taxonomy to systematically organize crisis-related\ninformation about requests and offers into three critical dimensions: supplies,\nemergency personnel, and actions. Leveraging the capabilities of Large Language\nModels (LLMs), we introduce Query-Specific Few-shot Learning (QSF Learning)\nthat retrieves class-specific labeled examples from an embedding database to\nenhance the model's performance in detecting and classifying posts. Beyond\nclassification, we assess the actionability of messages to prioritize posts\nrequiring immediate attention. Extensive experiments demonstrate that our\napproach outperforms baseline prompting strategies, effectively identifying and\nprioritizing actionable requests and offers."}
{"id": "2504.16918", "pdf": "https://arxiv.org/pdf/2504.16918", "abs": "https://arxiv.org/abs/2504.16918", "authors": ["Raghav Thind", "Youran Sun", "Ling Liang", "Haizhao Yang"], "title": "OptimAI: Optimization from Natural Language Using LLM-Powered AI Agents", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Optimization plays a vital role in scientific research and practical\napplications, but formulating a concrete optimization problem described in\nnatural language into a mathematical form and selecting a suitable solver to\nsolve the problem requires substantial domain expertise. We introduce\n\\textbf{OptimAI}, a framework for solving \\underline{Optim}ization problems\ndescribed in natural language by leveraging LLM-powered \\underline{AI} agents,\nachieving superior performance over current state-of-the-art methods. Our\nframework is built upon four key roles: (1) a \\emph{formulator} that translates\nnatural language problem descriptions into precise mathematical formulations;\n(2) a \\emph{planner} that constructs a high-level solution strategy prior to\nexecution; and (3) a \\emph{coder} and a \\emph{code critic} capable of\ninteracting with the environment and reflecting on outcomes to refine future\nactions. Ablation studies confirm that all roles are essential; removing the\nplanner or code critic results in $5.8\\times$ and $3.1\\times$ drops in\nproductivity, respectively. Furthermore, we introduce UCB-based debug\nscheduling to dynamically switch between alternative plans, yielding an\nadditional $3.3\\times$ productivity gain. Our design emphasizes multi-agent\ncollaboration, allowing us to conveniently explore the synergistic effect of\ncombining diverse models within a unified system. Our approach attains 88.1\\%\naccuracy on the NLP4LP dataset and 71.2\\% on the Optibench (non-linear w/o\ntable) subset, reducing error rates by 58\\% and 50\\% respectively over prior\nbest results."}
{"id": "2504.16538", "pdf": "https://arxiv.org/pdf/2504.16538", "abs": "https://arxiv.org/abs/2504.16538", "authors": ["Joan Perez", "Giovanni Fusco"], "title": "Streetscape Analysis with Generative AI (SAGAI): Vision-Language Assessment and Mapping of Urban Scenes", "categories": ["cs.CV", "cs.LG", "I.2; I.4; J.4"], "comment": "25 pages, 6 figures in main paper, 6 figures in appendices", "summary": "Streetscapes are an essential component of urban space. Their assessment is\npresently either limited to morphometric properties of their mass skeleton or\nrequires labor-intensive qualitative evaluations of visually perceived\nqualities. This paper introduces SAGAI: Streetscape Analysis with Generative\nArtificial Intelligence, a modular workflow for scoring street-level urban\nscenes using open-access data and vision-language models. SAGAI integrates\nOpenStreetMap geometries, Google Street View imagery, and a lightweight version\nof the LLaVA model to generate structured spatial indicators from images via\ncustomizable natural language prompts. The pipeline includes an automated\nmapping module that aggregates visual scores at both the point and street\nlevels, enabling direct cartographic interpretation. It operates without\ntask-specific training or proprietary software dependencies, supporting\nscalable and interpretable analysis of urban environments. Two exploratory case\nstudies in Nice and Vienna illustrate SAGAI's capacity to produce geospatial\noutputs from vision-language inference. The initial results show strong\nperformance for binary urban-rural scene classification, moderate precision in\ncommercial feature detection, and lower estimates, but still informative, of\nsidewalk width. Fully deployable by any user, SAGAI can be easily adapted to a\nwide range of urban research themes, such as walkability, safety, or urban\ndesign, through prompt modification alone."}
{"id": "2504.16628", "pdf": "https://arxiv.org/pdf/2504.16628", "abs": "https://arxiv.org/abs/2504.16628", "authors": ["Haoran Gu", "Handing Wang", "Yi Mei", "Mengjie Zhang", "Yaochu Jin"], "title": "ParetoHqD: Fast Offline Multiobjective Alignment of Large Language Models using Pareto High-quality Data", "categories": ["cs.LG", "cs.CL"], "comment": "19 pages, 6 figure, Multiobjective Alignment of LLMs", "summary": "Aligning large language models with multiple human expectations and values is\ncrucial for ensuring that they adequately serve a variety of user needs. To\nthis end, offline multiobjective alignment algorithms such as the\nRewards-in-Context algorithm have shown strong performance and efficiency.\nHowever, inappropriate preference representations and training with imbalanced\nreward scores limit the performance of such algorithms. In this work, we\nintroduce ParetoHqD that addresses the above issues by representing human\npreferences as preference directions in the objective space and regarding data\nnear the Pareto front as ''high-quality'' data. For each preference, ParetoHqD\nfollows a two-stage supervised fine-tuning process, where each stage uses an\nindividual Pareto high-quality training set that best matches its preference\ndirection. The experimental results have demonstrated the superiority of\nParetoHqD over five baselines on two multiobjective alignment tasks."}
{"id": "2504.16148", "pdf": "https://arxiv.org/pdf/2504.16148", "abs": "https://arxiv.org/abs/2504.16148", "authors": ["Danial Hooshyar", "Gustav Šír", "Yeongwook Yang", "Eve Kikas", "Raija Hämäläinen", "Tommi Kärkkäinen", "Dragan Gašević", "Roger Azevedo"], "title": "Towards responsible AI for education: Hybrid human-AI to confront the Elephant in the room", "categories": ["cs.CY", "cs.AI"], "comment": null, "summary": "Despite significant advancements in AI-driven educational systems and ongoing\ncalls for responsible AI for education, several critical issues remain\nunresolved -- acting as the elephant in the room within AI in education,\nlearning analytics, educational data mining, learning sciences, and educational\npsychology communities. This critical analysis identifies and examines nine\npersistent challenges that continue to undermine the fairness, transparency,\nand effectiveness of current AI methods and applications in education. These\ninclude: (1) the lack of clarity around what AI for education truly means --\noften ignoring the distinct purposes, strengths, and limitations of different\nAI families -- and the trend of equating it with domain-agnostic,\ncompany-driven large language models; (2) the widespread neglect of essential\nlearning processes such as motivation, emotion, and (meta)cognition in\nAI-driven learner modelling and their contextual nature; (3) limited\nintegration of domain knowledge and lack of stakeholder involvement in AI\ndesign and development; (4) continued use of non-sequential machine learning\nmodels on temporal educational data; (5) misuse of non-sequential metrics to\nevaluate sequential models; (6) use of unreliable explainable AI methods to\nprovide explanations for black-box models; (7) ignoring ethical guidelines in\naddressing data inconsistencies during model training; (8) use of mainstream AI\nmethods for pattern discovery and learning analytics without systematic\nbenchmarking; and (9) overemphasis on global prescriptions while overlooking\nlocalised, student-specific recommendations. Supported by theoretical and\nempirical research, we demonstrate how hybrid AI methods -- specifically\nneural-symbolic AI -- can address the elephant in the room and serve as the\nfoundation for responsible, trustworthy AI systems in education."}
{"id": "2504.16921", "pdf": "https://arxiv.org/pdf/2504.16921", "abs": "https://arxiv.org/abs/2504.16921", "authors": ["José Ángel González", "Ian Borrego Obrador", "Álvaro Romo Herrero", "Areg Mikael Sarvazyan", "Mara Chinea-Ríos", "Angelo Basile", "Marc Franco-Salvador"], "title": "IberBench: LLM Evaluation on Iberian Languages", "categories": ["cs.CL"], "comment": null, "summary": "Large Language Models (LLMs) remain difficult to evaluate comprehensively,\nparticularly for languages other than English, where high-quality data is often\nlimited. Existing benchmarks and leaderboards are predominantly\nEnglish-centric, with only a few addressing other languages. These benchmarks\nfall short in several key areas: they overlook the diversity of language\nvarieties, prioritize fundamental Natural Language Processing (NLP)\ncapabilities over tasks of industrial relevance, and are static. With these\naspects in mind, we present IberBench, a comprehensive and extensible benchmark\ndesigned to assess LLM performance on both fundamental and industry-relevant\nNLP tasks, in languages spoken across the Iberian Peninsula and Ibero-America.\nIberBench integrates 101 datasets from evaluation campaigns and recent\nbenchmarks, covering 22 task categories such as sentiment and emotion analysis,\ntoxicity detection, and summarization. The benchmark addresses key limitations\nin current evaluation practices, such as the lack of linguistic diversity and\nstatic evaluation setups by enabling continual updates and community-driven\nmodel and dataset submissions moderated by a committee of experts. We evaluate\n23 LLMs ranging from 100 million to 14 billion parameters and provide empirical\ninsights into their strengths and limitations. Our findings indicate that (i)\nLLMs perform worse on industry-relevant tasks than in fundamental ones, (ii)\nperformance is on average lower for Galician and Basque, (iii) some tasks show\nresults close to random, and (iv) in other tasks LLMs perform above random but\nbelow shared task systems. IberBench offers open-source implementations for the\nentire evaluation pipeline, including dataset normalization and hosting,\nincremental evaluation of LLMs, and a publicly accessible leaderboard."}
{"id": "2504.16545", "pdf": "https://arxiv.org/pdf/2504.16545", "abs": "https://arxiv.org/abs/2504.16545", "authors": ["Andrea Conti", "Matteo Poggi", "Valerio Cambareri", "Martin R. Oswald", "Stefano Mattoccia"], "title": "ToF-Splatting: Dense SLAM using Sparse Time-of-Flight Depth and Multi-Frame Integration", "categories": ["cs.CV"], "comment": null, "summary": "Time-of-Flight (ToF) sensors provide efficient active depth sensing at\nrelatively low power budgets; among such designs, only very sparse measurements\nfrom low-resolution sensors are considered to meet the increasingly limited\npower constraints of mobile and AR/VR devices. However, such extreme sparsity\nlevels limit the seamless usage of ToF depth in SLAM. In this work, we propose\nToF-Splatting, the first 3D Gaussian Splatting-based SLAM pipeline tailored for\nusing effectively very sparse ToF input data. Our approach improves upon the\nstate of the art by introducing a multi-frame integration module, which\nproduces dense depth maps by merging cues from extremely sparse ToF depth,\nmonocular color, and multi-view geometry. Extensive experiments on both\nsynthetic and real sparse ToF datasets demonstrate the viability of our\napproach, as it achieves state-of-the-art tracking and mapping performances on\nreference datasets."}
{"id": "2504.16639", "pdf": "https://arxiv.org/pdf/2504.16639", "abs": "https://arxiv.org/abs/2504.16639", "authors": ["Haoran Chen", "Jiapeng Liu", "Jiafan Wang", "Wenjun Shi"], "title": "DAPLSR: Data Augmentation Partial Least Squares Regression Model via Manifold Optimization", "categories": ["cs.LG", "cs.IR"], "comment": null, "summary": "Traditional Partial Least Squares Regression (PLSR) models frequently\nunderperform when handling data characterized by uneven categories. To address\nthe issue, this paper proposes a Data Augmentation Partial Least Squares\nRegression (DAPLSR) model via manifold optimization. The DAPLSR model\nintroduces the Synthetic Minority Over-sampling Technique (SMOTE) to increase\nthe number of samples and utilizes the Value Difference Metric (VDM) to select\nthe nearest neighbor samples that closely resemble the original samples for\ngenerating synthetic samples. In solving the model, in order to obtain a more\naccurate numerical solution for PLSR, this paper proposes a manifold\noptimization method that uses the geometric properties of the constraint space\nto improve model degradation and optimization. Comprehensive experiments show\nthat the proposed DAPLSR model achieves superior classification performance and\noutstanding evaluation metrics on various datasets, significantly outperforming\nexisting methods."}
{"id": "2504.16152", "pdf": "https://arxiv.org/pdf/2504.16152", "abs": "https://arxiv.org/abs/2504.16152", "authors": ["Mohammad Molaee", "Nasrollah Moghadam Charkari"], "title": "Heterogeneous networks in drug-target interaction prediction", "categories": ["q-bio.BM", "cs.AI", "cs.LG"], "comment": "18 pages, 5 figures, 10 tables", "summary": "Drug discovery requires a tremendous amount of time and cost. Computational\ndrug-target interaction prediction, a significant part of this process, can\nreduce these requirements by narrowing the search space for wet lab\nexperiments. In this survey, we provide comprehensive details of graph machine\nlearning-based methods in predicting drug-target interaction, as they have\nshown promising results in this field. These details include the overall\nframework, main contribution, datasets, and their source codes. The selected\npapers were mainly published from 2020 to 2024. Prior to discussing papers, we\nbriefly introduce the datasets commonly used with these methods and\nmeasurements to assess their performance. Finally, future challenges and some\ncrucial areas that need to be explored are discussed."}
{"id": "2504.16092", "pdf": "https://arxiv.org/pdf/2504.16092", "abs": "https://arxiv.org/abs/2504.16092", "authors": ["Mahrad Almotahari"], "title": "Cooperative Speech, Semantic Competence, and AI", "categories": ["cs.CY", "cs.CL", "cs.HC"], "comment": "25 pages", "summary": "Cooperative speech is purposive. From the speaker's perspective, one crucial\npurpose is the transmission of knowledge. Cooperative speakers care about\ngetting things right for their conversational partners. This attitude is a kind\nof respect. Cooperative speech is an ideal form of communication because\nparticipants have respect for each other. And having respect within a\ncooperative enterprise is sufficient for a particular kind of moral standing:\nwe ought to respect those who have respect for us. Respect demands reciprocity.\nI maintain that large language models aren't owed the kind of respect that\npartly constitutes a cooperative conversation. This implies that they aren't\ncooperative interlocutors, otherwise we would be obliged to reciprocate the\nattitude. Leveraging this conclusion, I argue that present-day LLMs are\nincapable of assertion and that this raises an overlooked doubt about their\nsemantic competence. One upshot of this argument is that knowledge of meaning\nisn't just a subject for the cognitive psychologist. It's also a subject for\nthe moral psychologist."}
{"id": "2504.16557", "pdf": "https://arxiv.org/pdf/2504.16557", "abs": "https://arxiv.org/abs/2504.16557", "authors": ["Murat Bilgehan Ertan", "Ronak Sahu", "Phuong Ha Nguyen", "Kaleel Mahmood", "Marten van Dijk"], "title": "Beyond Anonymization: Object Scrubbing for Privacy-Preserving 2D and 3D Vision Tasks", "categories": ["cs.CV"], "comment": "Submitted to ICCV 2025", "summary": "We introduce ROAR (Robust Object Removal and Re-annotation), a scalable\nframework for privacy-preserving dataset obfuscation that eliminates sensitive\nobjects instead of modifying them. Our method integrates instance segmentation\nwith generative inpainting to remove identifiable entities while preserving\nscene integrity. Extensive evaluations on 2D COCO-based object detection show\nthat ROAR achieves 87.5% of the baseline detection average precision (AP),\nwhereas image dropping achieves only 74.2% of the baseline AP, highlighting the\nadvantage of scrubbing in preserving dataset utility. The degradation is even\nmore severe for small objects due to occlusion and loss of fine-grained\ndetails. Furthermore, in NeRF-based 3D reconstruction, our method incurs a PSNR\nloss of at most 1.66 dB while maintaining SSIM and improving LPIPS,\ndemonstrating superior perceptual quality. Our findings establish object\nremoval as an effective privacy framework, achieving strong privacy guarantees\nwith minimal performance trade-offs. The results highlight key challenges in\ngenerative inpainting, occlusion-robust segmentation, and task-specific\nscrubbing, setting the foundation for future advancements in privacy-preserving\nvision systems."}
{"id": "2504.16667", "pdf": "https://arxiv.org/pdf/2504.16667", "abs": "https://arxiv.org/abs/2504.16667", "authors": ["Zhaohan Daniel Guo", "Bernardo Avila Pires", "Khimya Khetarpal", "Dale Schuurmans", "Bo Dai"], "title": "Representation Learning via Non-Contrastive Mutual Information", "categories": ["cs.LG", "cs.AI", "cs.CV", "stat.ML", "I.2.6; I.2.10"], "comment": null, "summary": "Labeling data is often very time consuming and expensive, leaving us with a\nmajority of unlabeled data. Self-supervised representation learning methods\nsuch as SimCLR (Chen et al., 2020) or BYOL (Grill et al., 2020) have been very\nsuccessful at learning meaningful latent representations from unlabeled image\ndata, resulting in much more general and transferable representations for\ndownstream tasks. Broadly, self-supervised methods fall into two types: 1)\nContrastive methods, such as SimCLR; and 2) Non-Contrastive methods, such as\nBYOL. Contrastive methods are generally trying to maximize mutual information\nbetween related data points, so they need to compare every data point to every\nother data point, resulting in high variance, and thus requiring large batch\nsizes to work well. Non-contrastive methods like BYOL have much lower variance\nas they do not need to make pairwise comparisons, but are much trickier to\nimplement as they have the possibility of collapsing to a constant vector. In\nthis paper, we aim to develop a self-supervised objective that combines the\nstrength of both types. We start with a particular contrastive method called\nthe Spectral Contrastive Loss (HaoChen et al., 2021; Lu et al., 2024), and we\nconvert it into a more general non-contrastive form; this removes the pairwise\ncomparisons resulting in lower variance, but keeps the mutual information\nformulation of the contrastive method preventing collapse. We call our new\nobjective the Mutual Information Non-Contrastive (MINC) loss. We test MINC by\nlearning image representations on ImageNet (similar to SimCLR and BYOL) and\nshow that it consistently improves upon the Spectral Contrastive loss baseline."}
{"id": "2504.16153", "pdf": "https://arxiv.org/pdf/2504.16153", "abs": "https://arxiv.org/abs/2504.16153", "authors": ["Kanwal Aalijah"], "title": "Leveraging Social Media Analytics for Sustainability Trend Detection in Saudi Arabias Evolving Market", "categories": ["cs.CY", "cs.AI"], "comment": "9", "summary": "Saudi Arabias rapid economic growth and social evolution under Vision 2030\npresent a unique opportunity to track emerging trends in real time. Uncovering\ntrends in real time can open up new avenues for business and investment\nopportunities. This paper explores how AI and social media analytics can\nuncover and monitor these trends across sectors like sustainability,\nconstruction, food beverages industry, tourism, technology, and entertainment.\nThis paper focus on use of AI-driven methodology to identify sustainability\ntrends across Saudi Arabia. We processed millions of social media posts, news,\nblogs in order to understand sustainability trends in the region. The paper\npresents an AI approach that can help economists, businesses, government to\nunderstand sustainability trends and make better decisions around them. This\napproach offers both sector-specific and cross-sector insights, giving\ndecision-makers a reliable, up to date snapshot of Saudi Arabias market shifts.\nBeyond Saudi Arabia, this framework also shows potential for adapting to other\nregions. Overall, our findings highlight how by using AI-methodologies, give\ndecision makers a reliable method to understand how initiatives are perceived\nand adopted by the public and understand growth of trends."}
{"id": "2504.16121", "pdf": "https://arxiv.org/pdf/2504.16121", "abs": "https://arxiv.org/abs/2504.16121", "authors": ["Muhammad Rafsan Kabir", "Rafeed Mohammad Sultan", "Fuad Rahman", "Mohammad Ruhul Amin", "Sifat Momen", "Nabeel Mohammed", "Shafin Rahman"], "title": "LegalRAG: A Hybrid RAG System for Multilingual Legal Information Retrieval", "categories": ["cs.IR", "cs.CL", "cs.LG"], "comment": "Accepted at IJCNN 2025", "summary": "Natural Language Processing (NLP) and computational linguistic techniques are\nincreasingly being applied across various domains, yet their use in legal and\nregulatory tasks remains limited. To address this gap, we develop an efficient\nbilingual question-answering framework for regulatory documents, specifically\nthe Bangladesh Police Gazettes, which contain both English and Bangla text. Our\napproach employs modern Retrieval Augmented Generation (RAG) pipelines to\nenhance information retrieval and response generation. In addition to\nconventional RAG pipelines, we propose an advanced RAG-based approach that\nimproves retrieval performance, leading to more precise answers. This system\nenables efficient searching for specific government legal notices, making legal\ninformation more accessible. We evaluate both our proposed and conventional RAG\nsystems on a diverse test set on Bangladesh Police Gazettes, demonstrating that\nour approach consistently outperforms existing methods across all evaluation\nmetrics."}
{"id": "2504.16564", "pdf": "https://arxiv.org/pdf/2504.16564", "abs": "https://arxiv.org/abs/2504.16564", "authors": ["Zhongtao Wang", "Xizhe Cao", "Yisong Chen", "Guoping Wang"], "title": "SAIP-Net: Enhancing Remote Sensing Image Segmentation via Spectral Adaptive Information Propagation", "categories": ["cs.CV", "cs.GR"], "comment": null, "summary": "Semantic segmentation of remote sensing imagery demands precise spatial\nboundaries and robust intra-class consistency, challenging conventional\nhierarchical models. To address limitations arising from spatial domain feature\nfusion and insufficient receptive fields, this paper introduces SAIP-Net, a\nnovel frequency-aware segmentation framework that leverages Spectral Adaptive\nInformation Propagation. SAIP-Net employs adaptive frequency filtering and\nmulti-scale receptive field enhancement to effectively suppress intra-class\nfeature inconsistencies and sharpen boundary lines. Comprehensive experiments\ndemonstrate significant performance improvements over state-of-the-art methods,\nhighlighting the effectiveness of spectral-adaptive strategies combined with\nexpanded receptive fields for remote sensing image segmentation."}
{"id": "2504.16668", "pdf": "https://arxiv.org/pdf/2504.16668", "abs": "https://arxiv.org/abs/2504.16668", "authors": ["Shuyue Wei", "Yongxin Tong", "Zimu Zhou", "Tianran He", "Yi Xu"], "title": "Efficient Data Valuation Approximation in Federated Learning: A Sampling-based Approach", "categories": ["cs.LG", "cs.DB"], "comment": null, "summary": "Federated learning paradigm to utilize datasets across multiple data\nproviders. In FL, cross-silo data providers often hesitate to share their\nhigh-quality dataset unless their data value can be fairly assessed. Shapley\nvalue (SV) has been advocated as the standard metric for data valuation in FL\ndue to its desirable properties. However, the computational overhead of SV is\nprohibitive in practice, as it inherently requires training and evaluating an\nFL model across an exponential number of dataset combinations. Furthermore,\nexisting solutions fail to achieve high accuracy and efficiency, making\npractical use of SV still out of reach, because they ignore choosing suitable\ncomputation scheme for approximation framework and overlook the property of\nutility function in FL. We first propose a unified stratified-sampling\nframework for two widely-used schemes. Then, we analyze and choose the more\npromising scheme under the FL linear regression assumption. After that, we\nidentify a phenomenon termed key combinations, where only limited dataset\ncombinations have a high-impact on final data value. Building on these\ninsights, we propose a practical approximation algorithm, IPSS, which\nstrategically selects high-impact dataset combinations rather than evaluating\nall possible combinations, thus substantially reducing time cost with minor\napproximation error. Furthermore, we conduct extensive evaluations on the FL\nbenchmark datasets to demonstrate that our proposed algorithm outperforms a\nseries of representative baselines in terms of efficiency and effectiveness."}
{"id": "2504.16172", "pdf": "https://arxiv.org/pdf/2504.16172", "abs": "https://arxiv.org/abs/2504.16172", "authors": ["Zexi Fan", "Yan Sun", "Shihao Yang", "Yiping Lu"], "title": "Physics-Informed Inference Time Scaling via Simulation-Calibrated Scientific Machine Learning", "categories": ["math.NA", "cs.AI", "cs.LG", "cs.NA", "math.PR", "stat.ML"], "comment": null, "summary": "High-dimensional partial differential equations (PDEs) pose significant\ncomputational challenges across fields ranging from quantum chemistry to\neconomics and finance. Although scientific machine learning (SciML) techniques\noffer approximate solutions, they often suffer from bias and neglect crucial\nphysical insights. Inspired by inference-time scaling strategies in language\nmodels, we propose Simulation-Calibrated Scientific Machine Learning (SCaSML),\na physics-informed framework that dynamically refines and debiases the SCiML\npredictions during inference by enforcing the physical laws. SCaSML leverages\nderived new physical laws that quantifies systematic errors and employs Monte\nCarlo solvers based on the Feynman-Kac and Elworthy-Bismut-Li formulas to\ndynamically correct the prediction. Both numerical and theoretical analysis\nconfirms enhanced convergence rates via compute-optimal inference methods. Our\nnumerical experiments demonstrate that SCaSML reduces errors by 20-50% compared\nto the base surrogate model, establishing it as the first algorithm to refine\napproximated solutions to high-dimensional PDE during inference. Code of SCaSML\nis available at https://github.com/Francis-Fan-create/SCaSML."}
{"id": "2504.16204", "pdf": "https://arxiv.org/pdf/2504.16204", "abs": "https://arxiv.org/abs/2504.16204", "authors": ["Christian Djeffal"], "title": "Reflexive Prompt Engineering: A Framework for Responsible Prompt Engineering and Interaction Design", "categories": ["cs.CY", "cs.AI", "cs.CL", "cs.ET"], "comment": "20 pages one figure", "summary": "Responsible prompt engineering has emerged as a critical framework for\nensuring that generative artificial intelligence (AI) systems serve society's\nneeds while minimizing potential harms. As generative AI applications become\nincreasingly powerful and ubiquitous, the way we instruct and interact with\nthem through prompts has profound implications for fairness, accountability,\nand transparency. This article examines how strategic prompt engineering can\nembed ethical and legal considerations and societal values directly into AI\ninteractions, moving beyond mere technical optimization for functionality. This\narticle proposes a comprehensive framework for responsible prompt engineering\nthat encompasses five interconnected components: prompt design, system\nselection, system configuration, performance evaluation, and prompt management.\nDrawing from empirical evidence, the paper demonstrates how each component can\nbe leveraged to promote improved societal outcomes while mitigating potential\nrisks. The analysis reveals that effective prompt engineering requires a\ndelicate balance between technical precision and ethical consciousness,\ncombining the systematic rigor and focus on functionality with the nuanced\nunderstanding of social impact. Through examination of real-world and emerging\npractices, the article illustrates how responsible prompt engineering serves as\na crucial bridge between AI development and deployment, enabling organizations\nto fine-tune AI outputs without modifying underlying model architectures. This\napproach aligns with broader \"Responsibility by Design\" principles, embedding\nethical considerations directly into the implementation process rather than\ntreating them as post-hoc additions. The article concludes by identifying key\nresearch directions and practical guidelines for advancing the field of\nresponsible prompt engineering."}
{"id": "2504.16570", "pdf": "https://arxiv.org/pdf/2504.16570", "abs": "https://arxiv.org/abs/2504.16570", "authors": ["Giacomo Pacini", "Lorenzo Bianchi", "Luca Ciampi", "Nicola Messina", "Giuseppe Amato", "Fabrizio Falchi"], "title": "CountingDINO: A Training-free Pipeline for Class-Agnostic Counting using Unsupervised Backbones", "categories": ["cs.CV"], "comment": "13 pages, 2 figures, 2 tables. Project website:\n  https://lorebianchi98.github.io/CountingDINO/", "summary": "Class-agnostic counting (CAC) aims to estimate the number of objects in\nimages without being restricted to predefined categories. However, while\ncurrent exemplar-based CAC methods offer flexibility at inference time, they\nstill rely heavily on labeled data for training, which limits scalability and\ngeneralization to many downstream use cases. In this paper, we introduce\nCountingDINO, the first training-free exemplar-based CAC framework that\nexploits a fully unsupervised feature extractor. Specifically, our approach\nemploys self-supervised vision-only backbones to extract object-aware features,\nand it eliminates the need for annotated data throughout the entire proposed\npipeline. At inference time, we extract latent object prototypes via ROI-Align\nfrom DINO features and use them as convolutional kernels to generate similarity\nmaps. These are then transformed into density maps through a simple yet\neffective normalization scheme. We evaluate our approach on the FSC-147\nbenchmark, where we outperform a baseline under the same label-free setting.\nOur method also achieves competitive -- and in some cases superior -- results\ncompared to training-free approaches relying on supervised backbones, as well\nas several fully supervised state-of-the-art methods. This demonstrates that\ntraining-free CAC can be both scalable and competitive. Website:\nhttps://lorebianchi98.github.io/CountingDINO/"}
{"id": "2504.16682", "pdf": "https://arxiv.org/pdf/2504.16682", "abs": "https://arxiv.org/abs/2504.16682", "authors": ["Youngmi Hur", "Hyojae Lim", "Mikyoung Lim"], "title": "Provable wavelet-based neural approximation", "categories": ["cs.LG", "math.CA", "stat.ML"], "comment": null, "summary": "In this paper, we develop a wavelet-based theoretical framework for analyzing\nthe universal approximation capabilities of neural networks over a wide range\nof activation functions. Leveraging wavelet frame theory on the spaces of\nhomogeneous type, we derive sufficient conditions on activation functions to\nensure that the associated neural network approximates any functions in the\ngiven space, along with an error estimate. These sufficient conditions\naccommodate a variety of smooth activation functions, including those that\nexhibit oscillatory behavior. Furthermore, by considering the $L^2$-distance\nbetween smooth and non-smooth activation functions, we establish a generalized\napproximation result that is applicable to non-smooth activations, with the\nerror explicitly controlled by this distance. This provides increased\nflexibility in the design of network architectures."}
{"id": "2504.16173", "pdf": "https://arxiv.org/pdf/2504.16173", "abs": "https://arxiv.org/abs/2504.16173", "authors": ["Pedro Antunes", "Artur Podobas"], "title": "FPGA-Based Neural Network Accelerators for Space Applications: A Survey", "categories": ["cs.AR", "cs.AI"], "comment": null, "summary": "Space missions are becoming increasingly ambitious, necessitating\nhigh-performance onboard spacecraft computing systems. In response,\nfield-programmable gate arrays (FPGAs) have garnered significant interest due\nto their flexibility, cost-effectiveness, and radiation tolerance potential.\nConcurrently, neural networks (NNs) are being recognized for their capability\nto execute space mission tasks such as autonomous operations, sensor data\nanalysis, and data compression. This survey serves as a valuable resource for\nresearchers aiming to implement FPGA-based NN accelerators in space\napplications. By analyzing existing literature, identifying trends and gaps,\nand proposing future research directions, this work highlights the potential of\nthese accelerators to enhance onboard computing systems."}
{"id": "2504.16264", "pdf": "https://arxiv.org/pdf/2504.16264", "abs": "https://arxiv.org/abs/2504.16264", "authors": ["Francisco Valentini", "Diego Kozlowski", "Vincent Larivière"], "title": "CLIRudit: Cross-Lingual Information Retrieval of Scientific Documents", "categories": ["cs.IR", "cs.CL"], "comment": null, "summary": "Cross-lingual information retrieval (CLIR) consists in finding relevant\ndocuments in a language that differs from the language of the queries. This\npaper presents CLIRudit, a new dataset created to evaluate cross-lingual\nacademic search, focusing on English queries and French documents. The dataset\nis built using bilingual article metadata from \\'Erudit, a Canadian publishing\nplatform, and is designed to represent scenarios in which researchers search\nfor scholarly content in languages other than English. We perform a\ncomprehensive benchmarking of different zero-shot first-stage retrieval methods\non the dataset, including dense and sparse retrievers, query and document\nmachine translation, and state-of-the-art multilingual retrievers. Our results\nshow that large dense retrievers, not necessarily trained for the cross-lingual\nretrieval task, can achieve zero-shot performance comparable to using ground\ntruth human translations, without the need for machine translation. Sparse\nretrievers, such as BM25 or SPLADE, combined with document translation, show\ncompetitive results, providing an efficient alternative to large dense models.\nThis research advances the understanding of cross-lingual academic information\nretrieval and provides a framework that others can use to build comparable\ndatasets across different languages and disciplines. By making the dataset and\ncode publicly available, we aim to facilitate further research that will help\nmake scientific knowledge more accessible across language barriers."}
{"id": "2504.16591", "pdf": "https://arxiv.org/pdf/2504.16591", "abs": "https://arxiv.org/abs/2504.16591", "authors": ["Tristan Kenneweg", "Philip Kenneweg", "Barbara Hammer"], "title": "JEPA for RL: Investigating Joint-Embedding Predictive Architectures for Reinforcement Learning", "categories": ["cs.CV"], "comment": "Published at ESANN 2025", "summary": "Joint-Embedding Predictive Architectures (JEPA) have recently become popular\nas promising architectures for self-supervised learning. Vision transformers\nhave been trained using JEPA to produce embeddings from images and videos,\nwhich have been shown to be highly suitable for downstream tasks like\nclassification and segmentation. In this paper, we show how to adapt the JEPA\narchitecture to reinforcement learning from images. We discuss model collapse,\nshow how to prevent it, and provide exemplary data on the classical Cart Pole\ntask."}
{"id": "2504.16683", "pdf": "https://arxiv.org/pdf/2504.16683", "abs": "https://arxiv.org/abs/2504.16683", "authors": ["Ceren Yildirim", "Kamer Kaya", "Sinan Yildirim", "Erkay Savas"], "title": "MCMC for Bayesian estimation of Differential Privacy from Membership Inference Attacks", "categories": ["cs.LG", "stat.ML"], "comment": "Code available:\n  https://github.com/cerenyildirim/MCMC_for_Bayesian_estimation", "summary": "We propose a new framework for Bayesian estimation of differential privacy,\nincorporating evidence from multiple membership inference attacks (MIA).\nBayesian estimation is carried out via a Markov chain Monte Carlo (MCMC)\nalgorithm, named MCMC-DP-Est, which provides an estimate of the full posterior\ndistribution of the privacy parameter (e.g., instead of just credible\nintervals). Critically, the proposed method does not assume that privacy\nauditing is performed with the most powerful attack on the worst-case (dataset,\nchallenge point) pair, which is typically unrealistic. Instead, MCMC-DP-Est\njointly estimates the strengths of MIAs used and the privacy of the training\nalgorithm, yielding a more cautious privacy analysis. We also present an\neconomical way to generate measurements for the performance of an MIA that is\nto be used by the MCMC method to estimate privacy. We present the use of the\nmethods with numerical examples with both artificial and real data."}
{"id": "2504.16193", "pdf": "https://arxiv.org/pdf/2504.16193", "abs": "https://arxiv.org/abs/2504.16193", "authors": ["Carmine Attanasio", "Alireza Mortezapour"], "title": "Quality of explanation of xAI from the prespective of Italian end-users: Italian version of System Causability Scale (SCS)", "categories": ["cs.HC", "cs.AI"], "comment": "This work will be presented in Coperman 2025 Conference", "summary": "Background and aim: Considering the scope of the application of artificial\nintelligence beyond the field of computer science, one of the concerns of\nresearchers is to provide quality explanations about the functioning of\nalgorithms based on artificial intelligence and the data extracted from it. The\npurpose of the present study is to validate the Italian version of system\ncausability scale (I-SCS) to measure the quality of explanations provided in a\nxAI.\n  Method: For this purpose, the English version, initially provided in 2020 in\ncoordination with the main developer, was utilized. The forward-backward\ntranslation method was applied to ensure accuracy. Finally, these nine steps\nwere completed by calculating the content validity index/ratio and conducting\ncognitive interviews with representative end users.\n  Results: The original version of the questionnaire consisted of 10 questions.\nHowever, based on the obtained indexes (CVR below 0.49), one question (Question\n8) was entirely removed. After completing the aforementioned steps, the Italian\nversion contained 9 questions. The representative sample of Italian end users\nfully comprehended the meaning and content of the questions in the Italian\nversion.\n  Conclusion: The Italian version obtained in this study can be used in future\nresearch studies as well as in the field by xAI developers. This tool can be\nused to measure the quality of explanations provided for an xAI system in\nItalian culture."}
{"id": "2504.16828", "pdf": "https://arxiv.org/pdf/2504.16828", "abs": "https://arxiv.org/abs/2504.16828", "authors": ["Muhammad Khalifa", "Rishabh Agarwal", "Lajanugen Logeswaran", "Jaekyeom Kim", "Hao Peng", "Moontae Lee", "Honglak Lee", "Lu Wang"], "title": "Process Reward Models That Think", "categories": ["cs.LG", "cs.AI", "cs.CL"], "comment": null, "summary": "Step-by-step verifiers -- also known as process reward models (PRMs) -- are a\nkey ingredient for test-time scaling. PRMs require step-level supervision,\nmaking them expensive to train. This work aims to build data-efficient PRMs as\nverbalized step-wise reward models that verify every step in the solution by\ngenerating a verification chain-of-thought (CoT). We propose ThinkPRM, a long\nCoT verifier fine-tuned on orders of magnitude fewer process labels than those\nrequired by discriminative PRMs. Our approach capitalizes on the inherent\nreasoning abilities of long CoT models, and outperforms LLM-as-a-Judge and\ndiscriminative verifiers -- using only 1% of the process labels in PRM800K --\nacross several challenging benchmarks. Specifically, ThinkPRM beats the\nbaselines on ProcessBench, MATH-500, and AIME '24 under best-of-N selection and\nreward-guided search. In an out-of-domain evaluation on a subset of\nGPQA-Diamond and LiveCodeBench, our PRM surpasses discriminative verifiers\ntrained on the full PRM800K by 8% and 4.5%, respectively. Lastly, under the\nsame token budget, ThinkPRM scales up verification compute more effectively\ncompared to LLM-as-a-Judge, outperforming it by 7.2% on a subset of\nProcessBench. Our work highlights the value of generative, long CoT PRMs that\ncan scale test-time compute for verification while requiring minimal\nsupervision for training. Our code, data, and models will be released at\nhttps://github.com/mukhal/thinkprm."}
{"id": "2504.16612", "pdf": "https://arxiv.org/pdf/2504.16612", "abs": "https://arxiv.org/abs/2504.16612", "authors": ["Max Kirchner", "Alexander C. Jenke", "Sebastian Bodenstedt", "Fiona R. Kolbinger", "Oliver Saldanha", "Jakob N. Kather", "Martin Wagner", "Stefanie Speidel"], "title": "Federated EndoViT: Pretraining Vision Transformers via Federated Learning on Endoscopic Image Collections", "categories": ["cs.CV", "cs.LG"], "comment": "Preprint submitted to MEDIA", "summary": "Purpose: In this study, we investigate the training of foundation models\nusing federated learning to address data-sharing limitations and enable\ncollaborative model training without data transfer for minimally invasive\nsurgery. Methods: Inspired by the EndoViT study, we adapt the Masked\nAutoencoder for federated learning, enhancing it with adaptive Sharpness-Aware\nMinimization (FedSAM) and Stochastic Weight Averaging (SWA). Our model is\npretrained on the Endo700k dataset collection and later fine-tuned and\nevaluated for tasks such as Semantic Segmentation, Action Triplet Recognition,\nand Surgical Phase Recognition. Results: Our findings demonstrate that\nintegrating adaptive FedSAM into the federated MAE approach improves\npretraining, leading to a reduction in reconstruction loss per patch. The\napplication of FL-EndoViT in surgical downstream tasks results in performance\ncomparable to CEN-EndoViT. Furthermore, FL-EndoViT exhibits advantages over\nCEN-EndoViT in surgical scene segmentation when data is limited and in action\ntriplet recognition when large datasets are used. Conclusion: These findings\nhighlight the potential of federated learning for privacy-preserving training\nof surgical foundation models, offering a robust and generalizable solution for\nsurgical data science. Effective collaboration requires adapting federated\nlearning methods, such as the integration of FedSAM, which can accommodate the\ninherent data heterogeneity across institutions. In future, exploring FL in\nvideo-based models may enhance these capabilities by incorporating\nspatiotemporal dynamics crucial for real-world surgical environments."}
{"id": "2504.16693", "pdf": "https://arxiv.org/pdf/2504.16693", "abs": "https://arxiv.org/abs/2504.16693", "authors": ["Wenxuan Li", "Hang Zhao", "Zhiyuan Yu", "Yu Du", "Qin Zou", "Ruizhen Hu", "Kai Xu"], "title": "PIN-WM: Learning Physics-INformed World Models for Non-Prehensile Manipulation", "categories": ["cs.LG", "cs.RO"], "comment": null, "summary": "While non-prehensile manipulation (e.g., controlled pushing/poking)\nconstitutes a foundational robotic skill, its learning remains challenging due\nto the high sensitivity to complex physical interactions involving friction and\nrestitution. To achieve robust policy learning and generalization, we opt to\nlearn a world model of the 3D rigid body dynamics involved in non-prehensile\nmanipulations and use it for model-based reinforcement learning. We propose\nPIN-WM, a Physics-INformed World Model that enables efficient end-to-end\nidentification of a 3D rigid body dynamical system from visual observations.\nAdopting differentiable physics simulation, PIN-WM can be learned with only\nfew-shot and task-agnostic physical interaction trajectories. Further, PIN-WM\nis learned with observational loss induced by Gaussian Splatting without\nneeding state estimation. To bridge Sim2Real gaps, we turn the learned PIN-WM\ninto a group of Digital Cousins via physics-aware randomizations which perturb\nphysics and rendering parameters to generate diverse and meaningful variations\nof the PIN-WM. Extensive evaluations on both simulation and real-world tests\ndemonstrate that PIN-WM, enhanced with physics-aware digital cousins,\nfacilitates learning robust non-prehensile manipulation skills with Sim2Real\ntransfer, surpassing the Real2Sim2Real state-of-the-arts."}
{"id": "2504.16226", "pdf": "https://arxiv.org/pdf/2504.16226", "abs": "https://arxiv.org/abs/2504.16226", "authors": ["Yazan Otoum", "Arghavan Asad", "Amiya Nayak"], "title": "Blockchain Meets Adaptive Honeypots: A Trust-Aware Approach to Next-Gen IoT Security", "categories": ["cs.CR", "cs.AI", "cs.ET", "cs.LG"], "comment": "This paper has been submitted to the IEEE Transactions on Network\n  Science and Engineering (TNSE) for possible publication", "summary": "Edge computing-based Next-Generation Wireless Networks (NGWN)-IoT offer\nenhanced bandwidth capacity for large-scale service provisioning but remain\nvulnerable to evolving cyber threats. Existing intrusion detection and\nprevention methods provide limited security as adversaries continually adapt\ntheir attack strategies. We propose a dynamic attack detection and prevention\napproach to address this challenge. First, blockchain-based authentication uses\nthe Deoxys Authentication Algorithm (DAA) to verify IoT device legitimacy\nbefore data transmission. Next, a bi-stage intrusion detection system is\nintroduced: the first stage uses signature-based detection via an Improved\nRandom Forest (IRF) algorithm. In contrast, the second stage applies\nfeature-based anomaly detection using a Diffusion Convolution Recurrent Neural\nNetwork (DCRNN). To ensure Quality of Service (QoS) and maintain Service Level\nAgreements (SLA), trust-aware service migration is performed using Heap-Based\nOptimization (HBO). Additionally, on-demand virtual High-Interaction honeypots\ndeceive attackers and extract attack patterns, which are securely stored using\nthe Bimodal Lattice Signature Scheme (BLISS) to enhance signature-based\nIntrusion Detection Systems (IDS). The proposed framework is implemented in the\nNS3 simulation environment and evaluated against existing methods across\nmultiple performance metrics, including accuracy, attack detection rate, false\nnegative rate, precision, recall, ROC curve, memory usage, CPU usage, and\nexecution time. Experimental results demonstrate that the framework\nsignificantly outperforms existing approaches, reinforcing the security of\nNGWN-enabled IoT ecosystems"}
{"id": "2111.15473", "pdf": "https://arxiv.org/pdf/2111.15473", "abs": "https://arxiv.org/abs/2111.15473", "authors": ["Andrew Kiruluta", "Andreas Lemos", "Eric Lundy"], "title": "Beyond Self Attention: A Subquadratic Fourier Wavelet Transformer with Multi Modal Fusion", "categories": ["cs.CL", "cs.LG"], "comment": "7 pages, 4 figures, 5 tables", "summary": "We revisit the use of spectral techniques to replaces the attention mechanism\nin Transformers through Fourier Transform based token mixing, and present a\ncomprehensive and novel reformulation of this technique in next generation\ntransformer models. We provide expanded literature context, detailed\nmathematical formulations of Fourier mixing and causal masking, and introduce a\nnovel MultiDomain Fourier Wavelet Attention(MDFWA) that integrates frequency\nand time localized transforms to capture both global and local dependencies\nefficiently. We derive the complexity bounds, gradient formulas, and show that\nMDFWA achieves sub quadratic time and memory cost while improving expressive\npower. We validate our design on an abstractive summarization task using PubMed\ndataset, by enhancing the proposed approach with learned frequency bases,\nadaptive scale selection, and multi-modal extensions."}
{"id": "2504.16616", "pdf": "https://arxiv.org/pdf/2504.16616", "abs": "https://arxiv.org/abs/2504.16616", "authors": ["Haosheng Chen", "Lian Luo", "Mengjingcheng Mo", "Zhanjie Wu", "Guobao Xiao", "Ji Gan", "Jiaxu Leng", "Xinbo Gao"], "title": "EHGCN: Hierarchical Euclidean-Hyperbolic Fusion via Motion-Aware GCN for Hybrid Event Stream Perception", "categories": ["cs.CV"], "comment": null, "summary": "Event cameras, with microsecond temporal resolution and high dynamic range\n(HDR) characteristics, emit high-speed event stream for perception tasks.\nDespite the recent advancement in GNN-based perception methods, they are prone\nto use straightforward pairwise connectivity mechanisms in the pure Euclidean\nspace where they struggle to capture long-range dependencies and fail to\neffectively characterize the inherent hierarchical structures of non-uniformly\ndistributed event stream. To this end, in this paper we propose a novel\napproach named EHGCN, which is a pioneer to perceive event stream in both\nEuclidean and hyperbolic spaces for event vision. In EHGCN, we introduce an\nadaptive sampling strategy to dynamically regulate sampling rates, retaining\ndiscriminative events while attenuating chaotic noise. Then we present a Markov\nVector Field (MVF)-driven motion-aware hyperedge generation method based on\nmotion state transition probabilities, thereby eliminating cross-target\nspurious associations and providing critically topological priors while\ncapturing long-range dependencies between events. Finally, we propose a\nEuclidean-Hyperbolic GCN to fuse the information locally aggregated and\nglobally hierarchically modeled in Euclidean and hyperbolic spaces,\nrespectively, to achieve hybrid event perception. Experimental results on event\nperception tasks such as object detection and recognition validate the\neffectiveness of our approach."}
{"id": "2504.16711", "pdf": "https://arxiv.org/pdf/2504.16711", "abs": "https://arxiv.org/abs/2504.16711", "authors": ["Shiyin Tan", "Jaeeon Park", "Dongyuan Li", "Renhe Jiang", "Manabu Okumura"], "title": "A Unified Retrieval Framework with Document Ranking and EDU Filtering for Multi-document Summarization", "categories": ["cs.LG", "cs.IR"], "comment": null, "summary": "In the field of multi-document summarization (MDS), transformer-based models\nhave demonstrated remarkable success, yet they suffer an input length\nlimitation. Current methods apply truncation after the retrieval process to fit\nthe context length; however, they heavily depend on manually well-crafted\nqueries, which are impractical to create for each document set for MDS.\nAdditionally, these methods retrieve information at a coarse granularity,\nleading to the inclusion of irrelevant content. To address these issues, we\npropose a novel retrieval-based framework that integrates query selection and\ndocument ranking and shortening into a unified process. Our approach identifies\nthe most salient elementary discourse units (EDUs) from input documents and\nutilizes them as latent queries. These queries guide the document ranking by\ncalculating relevance scores. Instead of traditional truncation, our approach\nfilters out irrelevant EDUs to fit the context length, ensuring that only\ncritical information is preserved for summarization. We evaluate our framework\non multiple MDS datasets, demonstrating consistent improvements in ROUGE\nmetrics while confirming its scalability and flexibility across diverse model\narchitectures. Additionally, we validate its effectiveness through an in-depth\nanalysis, emphasizing its ability to dynamically select appropriate queries and\naccurately rank documents based on their relevance scores. These results\ndemonstrate that our framework effectively addresses context-length\nconstraints, establishing it as a robust and reliable solution for MDS."}
{"id": "2504.16316", "pdf": "https://arxiv.org/pdf/2504.16316", "abs": "https://arxiv.org/abs/2504.16316", "authors": ["Hossein Shokouhinejad", "Griffin Higgins", "Roozbeh Razavi-Far", "Hesamodin Mohammadian", "Ali A. Ghorbani"], "title": "On the Consistency of GNN Explanations for Malware Detection", "categories": ["cs.CR", "cs.AI", "cs.LG"], "comment": null, "summary": "Control Flow Graphs (CFGs) are critical for analyzing program execution and\ncharacterizing malware behavior. With the growing adoption of Graph Neural\nNetworks (GNNs), CFG-based representations have proven highly effective for\nmalware detection. This study proposes a novel framework that dynamically\nconstructs CFGs and embeds node features using a hybrid approach combining\nrule-based encoding and autoencoder-based embedding. A GNN-based classifier is\nthen constructed to detect malicious behavior from the resulting graph\nrepresentations. To improve model interpretability, we apply state-of-the-art\nexplainability techniques, including GNNExplainer, PGExplainer, and\nCaptumExplainer, the latter is utilized three attribution methods: Integrated\nGradients, Guided Backpropagation, and Saliency. In addition, we introduce a\nnovel aggregation method, called RankFusion, that integrates the outputs of the\ntop-performing explainers to enhance the explanation quality. We also evaluate\nexplanations using two subgraph extraction strategies, including the proposed\nGreedy Edge-wise Composition (GEC) method for improved structural coherence. A\ncomprehensive evaluation using accuracy, fidelity, and consistency metrics\ndemonstrates the effectiveness of the proposed framework in terms of accurate\nidentification of malware samples and generating reliable and interpretable\nexplanations."}
{"id": "2307.08813", "pdf": "https://arxiv.org/pdf/2307.08813", "abs": "https://arxiv.org/abs/2307.08813", "authors": ["Gilchan Park", "Byung-Jun Yoon", "Xihaier Luo", "Vanessa López-Marrero", "Shinjae Yoo", "Shantenu Jha"], "title": "Comparative Performance Evaluation of Large Language Models for Extracting Molecular Interactions and Pathway Knowledge", "categories": ["cs.CL", "cs.LG"], "comment": null, "summary": "Background: Identification of the interactions and regulatory relations\nbetween biomolecules play pivotal roles in understanding complex biological\nsystems and the mechanisms underlying diverse biological functions. However,\nthe collection of such molecular interactions has heavily relied on expert\ncuration in the past, making it labor-intensive and time-consuming. To mitigate\nthese challenges, we propose leveraging the capabilities of large language\nmodels (LLMs) to automate genome-scale extraction of this crucial knowledge.\n  Results: In this study, we investigate the efficacy of various LLMs in\naddressing biological tasks, such as the recognition of protein interactions,\nidentification of genes linked to pathways affected by low-dose radiation, and\nthe delineation of gene regulatory relationships. Overall, the larger models\nexhibited superior performance, indicating their potential for specific tasks\nthat involve the extraction of complex interactions among genes and proteins.\nAlthough these models possessed detailed information for distinct gene and\nprotein groups, they faced challenges in identifying groups with diverse\nfunctions and in recognizing highly correlated gene regulatory relationships.\n  Conclusions: By conducting a comprehensive assessment of the state-of-the-art\nmodels using well-established molecular interaction and pathway databases, our\nstudy reveals that LLMs can identify genes/proteins associated with pathways of\ninterest and predict their interactions to a certain extent. Furthermore, these\nmodels can provide important insights, marking a noteworthy stride toward\nadvancing our understanding of biological systems through AI-assisted knowledge\ndiscovery."}
{"id": "2504.16636", "pdf": "https://arxiv.org/pdf/2504.16636", "abs": "https://arxiv.org/abs/2504.16636", "authors": ["Xianrui Luo", "Zijin Wu", "Juewen Peng", "Huiqiang Sun", "Zhiguo Cao", "Guosheng Lin"], "title": "Dual-Camera All-in-Focus Neural Radiance Fields", "categories": ["cs.CV"], "comment": "Published by IEEE TPAMI 2025", "summary": "We present the first framework capable of synthesizing the all-in-focus\nneural radiance field (NeRF) from inputs without manual refocusing. Without\nrefocusing, the camera will automatically focus on the fixed object for all\nviews, and current NeRF methods typically using one camera fail due to the\nconsistent defocus blur and a lack of sharp reference. To restore the\nall-in-focus NeRF, we introduce the dual-camera from smartphones, where the\nultra-wide camera has a wider depth-of-field (DoF) and the main camera\npossesses a higher resolution. The dual camera pair saves the high-fidelity\ndetails from the main camera and uses the ultra-wide camera's deep DoF as\nreference for all-in-focus restoration. To this end, we first implement spatial\nwarping and color matching to align the dual camera, followed by a\ndefocus-aware fusion module with learnable defocus parameters to predict a\ndefocus map and fuse the aligned camera pair. We also build a multi-view\ndataset that includes image pairs of the main and ultra-wide cameras in a\nsmartphone. Extensive experiments on this dataset verify that our solution,\ntermed DC-NeRF, can produce high-quality all-in-focus novel views and compares\nfavorably against strong baselines quantitatively and qualitatively. We further\nshow DoF applications of DC-NeRF with adjustable blur intensity and focal\nplane, including refocusing and split diopter."}
{"id": "2504.16748", "pdf": "https://arxiv.org/pdf/2504.16748", "abs": "https://arxiv.org/abs/2504.16748", "authors": ["Yanan Zhao", "Feng Ji", "Kai Zhao", "Xuhao Li", "Qiyu Kang", "Wenfei Liang", "Yahya Alkhatib", "Xingchao Jian", "Wee Peng Tay"], "title": "Simple Graph Contrastive Learning via Fractional-order Neural Diffusion Networks", "categories": ["cs.LG"], "comment": "Submitted to ICML", "summary": "Graph Contrastive Learning (GCL) has recently made progress as an\nunsupervised graph representation learning paradigm. GCL approaches can be\ncategorized into augmentation-based and augmentation-free methods. The former\nrelies on complex data augmentations, while the latter depends on encoders that\ncan generate distinct views of the same input. Both approaches may require\nnegative samples for training. In this paper, we introduce a novel\naugmentation-free GCL framework based on graph neural diffusion models.\nSpecifically, we utilize learnable encoders governed by Fractional Differential\nEquations (FDE). Each FDE is characterized by an order parameter of the\ndifferential operator. We demonstrate that varying these parameters allows us\nto produce learnable encoders that generate diverse views, capturing either\nlocal or global information, for contrastive learning. Our model does not\nrequire negative samples for training and is applicable to both homophilic and\nheterophilic datasets. We demonstrate its effectiveness across various\ndatasets, achieving state-of-the-art performance."}
{"id": "2504.16343", "pdf": "https://arxiv.org/pdf/2504.16343", "abs": "https://arxiv.org/abs/2504.16343", "authors": ["Chad Marshall", "Andrew Barovic", "Armin Moin"], "title": "Mining Software Repositories for Expert Recommendation", "categories": ["cs.SE", "cs.AI"], "comment": null, "summary": "We propose an automated approach to bug assignment to developers in large\nopen-source software projects. This way, we assist human bug triagers who are\nin charge of finding the best developer with the right level of expertise in a\nparticular area to be assigned to a newly reported issue. Our approach is based\non the history of software development as documented in the issue tracking\nsystems. We deploy BERTopic and techniques from TopicMiner. Our approach works\nbased on the bug reports' features, such as the corresponding products and\ncomponents, as well as their priority and severity levels. We sort developers\nbased on their experience with specific combinations of new reports. The\nevaluation is performed using Top-k accuracy, and the results are compared with\nthe reported results in prior work, namely TopicMiner MTM, BUGZIE, Bug triaging\nvia deep Reinforcement Learning BT-RL, and LDA-SVM. The evaluation data come\nfrom various Eclipse and Mozilla projects, such as JDT, Firefox, and\nThunderbird."}
{"id": "2403.05720", "pdf": "https://arxiv.org/pdf/2403.05720", "abs": "https://arxiv.org/abs/2403.05720", "authors": ["Asad Aali", "Dave Van Veen", "Yamin Ishraq Arefeen", "Jason Hom", "Christian Bluethgen", "Eduardo Pontes Reis", "Sergios Gatidis", "Namuun Clifford", "Joseph Daws", "Arash S. Tehrani", "Jangwon Kim", "Akshay S. Chaudhari"], "title": "A dataset and benchmark for hospital course summarization with adapted large language models", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": null, "summary": "Brief hospital course (BHC) summaries are clinical documents that summarize a\npatient's hospital stay. While large language models (LLMs) depict remarkable\ncapabilities in automating real-world tasks, their capabilities for healthcare\napplications such as synthesizing BHCs from clinical notes have not been shown.\nWe introduce a novel pre-processed dataset, the MIMIC-IV-BHC, encapsulating\nclinical note and brief hospital course (BHC) pairs to adapt LLMs for BHC\nsynthesis. Furthermore, we introduce a benchmark of the summarization\nperformance of two general-purpose LLMs and three healthcare-adapted LLMs.\nUsing clinical notes as input, we apply prompting-based (using in-context\nlearning) and fine-tuning-based adaptation strategies to three open-source LLMs\n(Clinical-T5-Large, Llama2-13B, FLAN-UL2) and two proprietary LLMs (GPT-3.5,\nGPT-4). We evaluate these LLMs across multiple context-length inputs using\nnatural language similarity metrics. We further conduct a clinical study with\nfive clinicians, comparing clinician-written and LLM-generated BHCs across 30\nsamples, focusing on their potential to enhance clinical decision-making\nthrough improved summary quality. We observe that the Llama2-13B fine-tuned LLM\noutperforms other domain-adapted models given quantitative evaluation metrics\nof BLEU and BERT-Score. GPT-4 with in-context learning shows more robustness to\nincreasing context lengths of clinical note inputs than fine-tuned Llama2-13B.\nDespite comparable quantitative metrics, the reader study depicts a significant\npreference for summaries generated by GPT-4 with in-context learning compared\nto both Llama2-13B fine-tuned summaries and the original summaries,\nhighlighting the need for qualitative clinical evaluation."}
{"id": "2504.16637", "pdf": "https://arxiv.org/pdf/2504.16637", "abs": "https://arxiv.org/abs/2504.16637", "authors": ["Qifan Li", "Tianyi Liang", "Xingtao Wang", "Xiaopeng Fan"], "title": "RouteWinFormer: A Route-Window Transformer for Middle-range Attention in Image Restoration", "categories": ["cs.CV"], "comment": null, "summary": "Transformer models have recently garnered significant attention in image\nrestoration due to their ability to capture long-range pixel dependencies.\nHowever, long-range attention often results in computational overhead without\npractical necessity, as degradation and context are typically localized.\nNormalized average attention distance across various degradation datasets shows\nthat middle-range attention is enough for image restoration. Building on this\ninsight, we propose RouteWinFormer, a novel window-based Transformer that\nmodels middle-range context for image restoration. RouteWinFormer incorporates\nRoute-Windows Attnetion Module, which dynamically selects relevant nearby\nwindows based on regional similarity for attention aggregation, extending the\nreceptive field to a mid-range size efficiently. In addition, we introduce\nMulti-Scale Structure Regularization during training, enabling the sub-scale of\nthe U-shaped network to focus on structural information, while the\noriginal-scale learns degradation patterns based on generalized image structure\npriors. Extensive experiments demonstrate that RouteWinFormer outperforms\nstate-of-the-art methods across 9 datasets in various image restoration tasks."}
{"id": "2504.16755", "pdf": "https://arxiv.org/pdf/2504.16755", "abs": "https://arxiv.org/abs/2504.16755", "authors": ["Owain Parry", "Phil McMinn"], "title": "QAOA-PCA: Enhancing Efficiency in the Quantum Approximate Optimization Algorithm via Principal Component Analysis", "categories": ["cs.LG", "cs.ET"], "comment": null, "summary": "The Quantum Approximate Optimization Algorithm (QAOA) is a promising\nvariational algorithm for solving combinatorial optimization problems on\nnear-term devices. However, as the number of layers in a QAOA circuit\nincreases, which is correlated with the quality of the solution, the number of\nparameters to optimize grows linearly. This results in more iterations required\nby the classical optimizer, which results in an increasing computational burden\nas more circuit executions are needed. To mitigate this issue, we introduce\nQAOA-PCA, a novel reparameterization technique that employs Principal Component\nAnalysis (PCA) to reduce the dimensionality of the QAOA parameter space. By\nextracting principal components from optimized parameters of smaller problem\ninstances, QAOA-PCA facilitates efficient optimization with fewer parameters on\nlarger instances. Our empirical evaluation on the prominent MaxCut problem\ndemonstrates that QAOA-PCA consistently requires fewer iterations than standard\nQAOA, achieving substantial efficiency gains. While this comes at the cost of a\nslight reduction in approximation ratio compared to QAOA with the same number\nof layers, QAOA-PCA almost always outperforms standard QAOA when matched by\nparameter count. QAOA-PCA strikes a favorable balance between efficiency and\nperformance, reducing optimization overhead without significantly compromising\nsolution quality."}
{"id": "2504.16350", "pdf": "https://arxiv.org/pdf/2504.16350", "abs": "https://arxiv.org/abs/2504.16350", "authors": ["Ilya Tyagin", "Marwa H. Farag", "Kyle Sherbert", "Karunya Shirali", "Yuri Alexeev", "Ilya Safro"], "title": "QAOA-GPT: Efficient Generation of Adaptive and Regular Quantum Approximate Optimization Algorithm Circuits", "categories": ["quant-ph", "cs.AI"], "comment": null, "summary": "Quantum computing has the potential to improve our ability to solve certain\noptimization problems that are computationally difficult for classical\ncomputers, by offering new algorithmic approaches that may provide speedups\nunder specific conditions. In this work, we introduce QAOA-GPT, a generative\nframework that leverages Generative Pretrained Transformers (GPT) to directly\nsynthesize quantum circuits for solving quadratic unconstrained binary\noptimization problems, and demonstrate it on the MaxCut problem on graphs. To\ndiversify the training circuits and ensure their quality, we have generated a\nsynthetic dataset using the adaptive QAOA approach, a method that incrementally\nbuilds and optimizes problem-specific circuits. The experiments conducted on a\ncurated set of graph instances demonstrate that QAOA-GPT, generates high\nquality quantum circuits for new problem instances unseen in the training as\nwell as successfully parametrizes QAOA. Our results show that using QAOA-GPT to\ngenerate quantum circuits will significantly decrease both the computational\noverhead of classical QAOA and adaptive approaches that often use gradient\nevaluation to generate the circuit and the classical optimization of the\ncircuit parameters. Our work shows that generative AI could be a promising\navenue to generate compact quantum circuits in a scalable way."}
{"id": "2403.12766", "pdf": "https://arxiv.org/pdf/2403.12766", "abs": "https://arxiv.org/abs/2403.12766", "authors": ["Cunxiang Wang", "Ruoxi Ning", "Boqi Pan", "Tonghui Wu", "Qipeng Guo", "Cheng Deng", "Guangsheng Bao", "Xiangkun Hu", "Zheng Zhang", "Qian Wang", "Yue Zhang"], "title": "NovelQA: Benchmarking Question Answering on Documents Exceeding 200K Tokens", "categories": ["cs.CL"], "comment": "Accepted by ICLR-2025", "summary": "Recent advancements in Large Language Models (LLMs) have pushed the\nboundaries of natural language processing, especially in long-context\nunderstanding. However, the evaluation of these models' long-context abilities\nremains a challenge due to the limitations of current benchmarks. To address\nthis gap, we introduce NovelQA, a benchmark tailored for evaluating LLMs with\ncomplex, extended narratives. Constructed from English novels, NovelQA offers a\nunique blend of complexity, length, and narrative coherence, making it an ideal\ntool for assessing deep textual understanding in LLMs. This paper details the\ndesign and construction of NovelQA, focusing on its comprehensive manual\nannotation process and the variety of question types aimed at evaluating\nnuanced comprehension. Our evaluation of long-context LLMs on NovelQA reveals\nsignificant insights into their strengths and weaknesses. Notably, the models\nstruggle with multi-hop reasoning, detail-oriented questions, and handling\nextremely long inputs, with average lengths exceeding 200,000 tokens. Results\nhighlight the need for substantial advancements in LLMs to enhance their\nlong-context comprehension and contribute effectively to computational literary\nanalysis."}
{"id": "2504.16640", "pdf": "https://arxiv.org/pdf/2504.16640", "abs": "https://arxiv.org/abs/2504.16640", "authors": ["Hasan Algafri", "Hamzah Luqman", "Sarah Alyami", "Issam Laradji"], "title": "SSLR: A Semi-Supervised Learning Method for Isolated Sign Language Recognition", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Sign language is the primary communication language for people with disabling\nhearing loss. Sign language recognition (SLR) systems aim to recognize sign\ngestures and translate them into spoken language. One of the main challenges in\nSLR is the scarcity of annotated datasets. To address this issue, we propose a\nsemi-supervised learning (SSL) approach for SLR (SSLR), employing a\npseudo-label method to annotate unlabeled samples. The sign gestures are\nrepresented using pose information that encodes the signer's skeletal joint\npoints. This information is used as input for the Transformer backbone model\nutilized in the proposed approach. To demonstrate the learning capabilities of\nSSL across various labeled data sizes, several experiments were conducted using\ndifferent percentages of labeled data with varying numbers of classes. The\nperformance of the SSL approach was compared with a fully supervised\nlearning-based model on the WLASL-100 dataset. The obtained results of the SSL\nmodel outperformed the supervised learning-based model with less labeled data\nin many cases."}
{"id": "2504.16763", "pdf": "https://arxiv.org/pdf/2504.16763", "abs": "https://arxiv.org/abs/2504.16763", "authors": ["Edison Mucllari", "Aswin Raghavan", "Zachary Alan Daniels"], "title": "Noise-Tolerant Coreset-Based Class Incremental Continual Learning", "categories": ["cs.LG", "cs.AI", "cs.CV", "cs.NE"], "comment": "Work-in-Progress", "summary": "Many applications of computer vision require the ability to adapt to novel\ndata distributions after deployment. Adaptation requires algorithms capable of\ncontinual learning (CL). Continual learners must be plastic to adapt to novel\ntasks while minimizing forgetting of previous tasks.However, CL opens up\navenues for noise to enter the training pipeline and disrupt the CL. This work\nfocuses on label noise and instance noise in the context of class-incremental\nlearning (CIL), where new classes are added to a classifier over time, and\nthere is no access to external data from past classes. We aim to understand the\nsensitivity of CL methods that work by replaying items from a memory\nconstructed using the idea of Coresets. We derive a new bound for the\nrobustness of such a method to uncorrelated instance noise under a general\nadditive noise threat model, revealing several insights. Putting the theory\ninto practice, we create two continual learning algorithms to construct\nnoise-tolerant replay buffers. We empirically compare the effectiveness of\nprior memory-based continual learners and the proposed algorithms under label\nand uncorrelated instance noise on five diverse datasets. We show that existing\nmemory-based CL are not robust whereas the proposed methods exhibit significant\nimprovements in maximizing classification accuracy and minimizing forgetting in\nthe noisy CIL setting."}
{"id": "2504.16352", "pdf": "https://arxiv.org/pdf/2504.16352", "abs": "https://arxiv.org/abs/2504.16352", "authors": ["Jiwan Kim", "Hongseok Kang", "Sein Kim", "Kibum Kim", "Chanyoung Park"], "title": "Disentangling and Generating Modalities for Recommendation in Missing Modality Scenarios", "categories": ["cs.IR", "cs.AI"], "comment": "SIGIR 2025", "summary": "Multi-modal recommender systems (MRSs) have achieved notable success in\nimproving personalization by leveraging diverse modalities such as images,\ntext, and audio. However, two key challenges remain insufficiently addressed:\n(1) Insufficient consideration of missing modality scenarios and (2) the\noverlooking of unique characteristics of modality features. These challenges\nresult in significant performance degradation in realistic situations where\nmodalities are missing. To address these issues, we propose Disentangling and\nGenerating Modality Recommender (DGMRec), a novel framework tailored for\nmissing modality scenarios. DGMRec disentangles modality features into general\nand specific modality features from an information-based perspective, enabling\nricher representations for recommendation. Building on this, it generates\nmissing modality features by integrating aligned features from other modalities\nand leveraging user modality preferences. Extensive experiments show that\nDGMRec consistently outperforms state-of-the-art MRSs in challenging scenarios,\nincluding missing modalities and new item settings as well as diverse missing\nratios and varying levels of missing modalities. Moreover, DGMRec's\ngeneration-based approach enables cross-modal retrieval, a task inapplicable\nfor existing MRSs, highlighting its adaptability and potential for real-world\napplications. Our code is available at https://github.com/ptkjw1997/DGMRec."}
{"id": "2405.20770", "pdf": "https://arxiv.org/pdf/2405.20770", "abs": "https://arxiv.org/abs/2405.20770", "authors": ["Guang Lin", "Toshihisa Tanaka", "Qibin Zhao"], "title": "Large Language Model Sentinel: LLM Agent for Adversarial Purification", "categories": ["cs.CL", "cs.AI", "cs.CR"], "comment": null, "summary": "Over the past two years, the use of large language models (LLMs) has advanced\nrapidly. While these LLMs offer considerable convenience, they also raise\nsecurity concerns, as LLMs are vulnerable to adversarial attacks by some\nwell-designed textual perturbations. In this paper, we introduce a novel\ndefense technique named Large LAnguage MOdel Sentinel (LLAMOS), which is\ndesigned to enhance the adversarial robustness of LLMs by purifying the\nadversarial textual examples before feeding them into the target LLM. Our\nmethod comprises two main components: a) Agent instruction, which can simulate\na new agent for adversarial defense, altering minimal characters to maintain\nthe original meaning of the sentence while defending against attacks; b)\nDefense guidance, which provides strategies for modifying clean or adversarial\nexamples to ensure effective defense and accurate outputs from the target LLMs.\nRemarkably, the defense agent demonstrates robust defensive capabilities even\nwithout learning from adversarial examples. Additionally, we conduct an\nintriguing adversarial experiment where we develop two agents, one for defense\nand one for attack, and engage them in mutual confrontation. During the\nadversarial interactions, neither agent completely beat the other. Extensive\nexperiments on both open-source and closed-source LLMs demonstrate that our\nmethod effectively defends against adversarial attacks, thereby enhancing\nadversarial robustness."}
{"id": "2504.16655", "pdf": "https://arxiv.org/pdf/2504.16655", "abs": "https://arxiv.org/abs/2504.16655", "authors": ["Younggeol Cho", "Elisa Motta", "Olivia Nocentini", "Marta Lagomarsino", "Andrea Merello", "Marco Crepaldi", "Arash Ajoudani"], "title": "WiFi based Human Fall and Activity Recognition using Transformer based Encoder Decoder and Graph Neural Networks", "categories": ["cs.CV"], "comment": "8 pages, 4 figures", "summary": "Human pose estimation and action recognition have received attention due to\ntheir critical roles in healthcare monitoring, rehabilitation, and assistive\ntechnologies. In this study, we proposed a novel architecture named Transformer\nbased Encoder Decoder Network (TED Net) designed for estimating human skeleton\nposes from WiFi Channel State Information (CSI). TED Net integrates\nconvolutional encoders with transformer based attention mechanisms to capture\nspatiotemporal features from CSI signals. The estimated skeleton poses were\nused as input to a customized Directed Graph Neural Network (DGNN) for action\nrecognition. We validated our model on two datasets: a publicly available multi\nmodal dataset for assessing general pose estimation, and a newly collected\ndataset focused on fall related scenarios involving 20 participants.\nExperimental results demonstrated that TED Net outperformed existing approaches\nin pose estimation, and that the DGNN achieves reliable action classification\nusing CSI based skeletons, with performance comparable to RGB based systems.\nNotably, TED Net maintains robust performance across both fall and non fall\ncases. These findings highlight the potential of CSI driven human skeleton\nestimation for effective action recognition, particularly in home environments\nsuch as elderly fall detection. In such settings, WiFi signals are often\nreadily available, offering a privacy preserving alternative to vision based\nmethods, which may raise concerns about continuous camera monitoring."}
{"id": "2504.16767", "pdf": "https://arxiv.org/pdf/2504.16767", "abs": "https://arxiv.org/abs/2504.16767", "authors": ["Andrea Nóvoa", "Luca Magri"], "title": "Online model learning with data-assimilated reservoir computers", "categories": ["cs.LG", "physics.flu-dyn", "stat.AP"], "comment": "8 pages, 5 figures", "summary": "We propose an online learning framework for forecasting nonlinear\nspatio-temporal signals (fields). The method integrates (i) dimensionality\nreduction, here, a simple proper orthogonal decomposition (POD) projection;\n(ii) a generalized autoregressive model to forecast reduced dynamics, here, a\nreservoir computer; (iii) online adaptation to update the reservoir computer\n(the model), here, ensemble sequential data assimilation.We demonstrate the\nframework on a wake past a cylinder governed by the Navier-Stokes equations,\nexploring the assimilation of full flow fields (projected onto POD modes) and\nsparse sensors. Three scenarios are examined: a na\\\"ive physical state\nestimation; a two-fold estimation of physical and reservoir states; and a\nthree-fold estimation that also adjusts the model parameters. The two-fold\nstrategy significantly improves ensemble convergence and reduces reconstruction\nerror compared to the na\\\"ive approach. The three-fold approach enables robust\nonline training of partially-trained reservoir computers, overcoming\nlimitations of a priori training. By unifying data-driven reduced order\nmodelling with Bayesian data assimilation, this work opens new opportunities\nfor scalable online model learning for nonlinear time series forecasting."}
{"id": "2504.16357", "pdf": "https://arxiv.org/pdf/2504.16357", "abs": "https://arxiv.org/abs/2504.16357", "authors": ["Ying Chang", "Xiaohu Shi", "Xiaohui Zhao", "Zhaohuang Chen", "Deyin Ma"], "title": "DP2FL: Dual Prompt Personalized Federated Learning in Foundation Models", "categories": ["cs.DC", "cs.AI", "cs.LG"], "comment": null, "summary": "Personalized federated learning (PFL) has garnered significant attention for\nits ability to address heterogeneous client data distributions while preserving\ndata privacy. However, when local client data is limited, deep learning models\noften suffer from insufficient training, leading to suboptimal performance.\nFoundation models, such as CLIP (Contrastive Language-Image Pretraining),\nexhibit strong feature extraction capabilities and can alleviate this issue by\nfine-tuning on limited local data. Despite their potential, foundation models\nare rarely utilized in federated learning scenarios, and challenges related to\nintegrating new clients remain largely unresolved. To address these challenges,\nwe propose the Dual Prompt Personalized Federated Learning (DP2FL) framework,\nwhich introduces dual prompts and an adaptive aggregation strategy. DP2FL\ncombines global task awareness with local data-driven insights, enabling local\nmodels to achieve effective generalization while remaining adaptable to\nspecific data distributions. Moreover, DP2FL introduces a global model that\nenables prediction on new data sources and seamlessly integrates newly added\nclients without requiring retraining. Experimental results in highly\nheterogeneous environments validate the effectiveness of DP2FL's prompt design\nand aggregation strategy, underscoring the advantages of prediction on novel\ndata sources and demonstrating the seamless integration of new clients into the\nfederated learning framework."}
{"id": "2406.15231", "pdf": "https://arxiv.org/pdf/2406.15231", "abs": "https://arxiv.org/abs/2406.15231", "authors": ["Yanis Labrak", "Markus Frohmann", "Gabriel Meseguer-Brocal", "Elena V. Epure"], "title": "Synthetic Lyrics Detection Across Languages and Genres", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "Published in the workshop TrustNLP @ NAACL", "summary": "In recent years, the use of large language models (LLMs) to generate music\ncontent, particularly lyrics, has gained in popularity. These advances provide\nvaluable tools for artists and enhance their creative processes, but they also\nraise concerns about copyright violations, consumer satisfaction, and content\nspamming. Previous research has explored content detection in various domains.\nHowever, no work has focused on the text modality, lyrics, in music. To address\nthis gap, we curated a diverse dataset of real and synthetic lyrics from\nmultiple languages, music genres, and artists. The generation pipeline was\nvalidated using both humans and automated methods. We performed a thorough\nevaluation of existing synthetic text detection approaches on lyrics, a\npreviously unexplored data type. We also investigated methods to adapt the\nbest-performing features to lyrics through unsupervised domain adaptation.\nFollowing both music and industrial constraints, we examined how well these\napproaches generalize across languages, scale with data availability, handle\nmultilingual language content, and perform on novel genres in few-shot\nsettings. Our findings show promising results that could inform policy\ndecisions around AI-generated music and enhance transparency for users."}
{"id": "2504.16656", "pdf": "https://arxiv.org/pdf/2504.16656", "abs": "https://arxiv.org/abs/2504.16656", "authors": ["Chris", "Yichen Wei", "Yi Peng", "Xiaokun Wang", "Weijie Qiu", "Wei Shen", "Tianyidan Xie", "Jiangbo Pei", "Jianhao Zhang", "Yunzhuo Hao", "Xuchen Song", "Yang Liu", "Yahui Zhou"], "title": "Skywork R1V2: Multimodal Hybrid Reinforcement Learning for Reasoning", "categories": ["cs.CV"], "comment": null, "summary": "We present Skywork R1V2, a next-generation multimodal reasoning model and a\nmajor leap forward from its predecessor, Skywork R1V. At its core, R1V2\nintroduces a hybrid reinforcement learning paradigm that harmonizes\nreward-model guidance with rule-based strategies, thereby addressing the\nlong-standing challenge of balancing sophisticated reasoning capabilities with\nbroad generalization. To further enhance training efficiency, we propose the\nSelective Sample Buffer (SSB) mechanism, which effectively counters the\n``Vanishing Advantages'' dilemma inherent in Group Relative Policy Optimization\n(GRPO) by prioritizing high-value samples throughout the optimization process.\nNotably, we observe that excessive reinforcement signals can induce visual\nhallucinations--a phenomenon we systematically monitor and mitigate through\ncalibrated reward thresholds throughout the training process. Empirical results\naffirm the exceptional capability of R1V2, with benchmark-leading performances\nsuch as 62.6 on OlympiadBench, 79.0 on AIME2024, 63.6 on LiveCodeBench, and\n74.0 on MMMU. These results underscore R1V2's superiority over existing\nopen-source models and demonstrate significant progress in closing the\nperformance gap with premier proprietary systems, including Gemini 2.5 and\nOpenAI o4-mini. The Skywork R1V2 model weights have been publicly released to\npromote openness and reproducibility\nhttps://huggingface.co/Skywork/Skywork-R1V2-38B."}
{"id": "2504.16831", "pdf": "https://arxiv.org/pdf/2504.16831", "abs": "https://arxiv.org/abs/2504.16831", "authors": ["Frederik L. Dennig", "Nina Geyer", "Daniela Blumberg", "Yannick Metz", "Daniel A. Keim"], "title": "Evaluating Autoencoders for Parametric and Invertible Multidimensional Projections", "categories": ["cs.LG"], "comment": "12 pages, 7 figures, 2 tables, LaTeX; to appear at the 16th\n  International EuroVis Workshop on Visual Analytics (EuroVA'25)", "summary": "Recently, neural networks have gained attention for creating parametric and\ninvertible multidimensional data projections. Parametric projections allow for\nembedding previously unseen data without recomputing the projection as a whole,\nwhile invertible projections enable the generation of new data points. However,\nthese properties have never been explored simultaneously for arbitrary\nprojection methods. We evaluate three autoencoder (AE) architectures for\ncreating parametric and invertible projections. Based on a given projection, we\ntrain AEs to learn a mapping into 2D space and an inverse mapping into the\noriginal space. We perform a quantitative and qualitative comparison on four\ndatasets of varying dimensionality and pattern complexity using t-SNE. Our\nresults indicate that AEs with a customized loss function can create smoother\nparametric and inverse projections than feed-forward neural networks while\ngiving users control over the strength of the smoothing effect."}
{"id": "2504.16378", "pdf": "https://arxiv.org/pdf/2504.16378", "abs": "https://arxiv.org/abs/2504.16378", "authors": ["Tadashi Okoshi", "Zexiong Gao", "Tan Yi Zhen", "Takumi Karasawa", "Takeshi Miki", "Wataru Sasaki", "Rajesh K. Balan"], "title": "Cyberoception: Finding a Painlessly-Measurable New Sense in the Cyberworld Towards Emotion-Awareness in Computing", "categories": ["cs.HC", "cs.AI"], "comment": "Accepted by ACM CHI2025", "summary": "In Affective computing, recognizing users' emotions accurately is the basis\nof affective human-computer interaction. Understanding users' interoception\ncontributes to a better understanding of individually different emotional\nabilities, which is essential for achieving inter-individually accurate emotion\nestimation. However, existing interoception measurement methods, such as the\nheart rate discrimination task, have several limitations, including their\ndependence on a well-controlled laboratory environment and precision apparatus,\nmaking monitoring users' interoception challenging. This study aims to\ndetermine other forms of data that can explain users' interoceptive or similar\nstates in their real-world lives and propose a novel hypothetical concept\n\"cyberoception,\" a new sense (1) which has properties similar to interoception\nin terms of the correlation with other emotion-related abilities, and (2) which\ncan be measured only by the sensors embedded inside commodity smartphone\ndevices in users' daily lives. Results from a 10-day-long in-lab/in-the-wild\nhybrid experiment reveal a specific cyberoception type \"Turn On\" (users'\nsubjective sensory perception about the frequency of turning-on behavior on\ntheir smartphones), significantly related to participants' emotional valence.\nWe anticipate that cyberoception to serve as a fundamental building block for\ndeveloping more \"emotion-aware\", user-friendly applications and services."}
{"id": "2407.03004", "pdf": "https://arxiv.org/pdf/2407.03004", "abs": "https://arxiv.org/abs/2407.03004", "authors": ["Meghal Dani", "Muthu Jeyanthi Prakash", "Zeynep Akata", "Stefanie Liebe"], "title": "SemioLLM: Evaluating Large Language Models for Diagnostic Reasoning from Unstructured Clinical Narratives in Epilepsy", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Large Language Models (LLMs) have been shown to encode clinical knowledge.\nMany evaluations, however, rely on structured question-answer benchmarks,\noverlooking critical challenges of interpreting and reasoning about\nunstructured clinical narratives in real-world settings. Using free-text\nclinical descriptions, we present SemioLLM, an evaluation framework that\nbenchmarks 6 state-of-the-art models (GPT-3.5, GPT-4, Mixtral-8x7B, Qwen-72B,\nLlaMa2, LlaMa3) on a core diagnostic task in epilepsy. Leveraging a database of\n1,269 seizure descriptions, we show that most LLMs are able to accurately and\nconfidently generate probabilistic predictions of seizure onset zones in the\nbrain. Most models approach clinician-level performance after prompt\nengineering, with expert-guided chain-of-thought reasoning leading to the most\nconsistent improvements. Performance was further strongly modulated by clinical\nin-context impersonation, narrative length and language context (13.7%, 32.7%\nand 14.2% performance variation, respectively). However, expert analysis of\nreasoning outputs revealed that correct prediction can be based on hallucinated\nknowledge and deficient source citation accuracy, underscoring the need to\nimprove interpretability of LLMs in clinical use. Overall, SemioLLM provides a\nscalable, domain-adaptable framework for evaluating LLMs in clinical\ndisciplines where unstructured verbal descriptions encode diagnostic\ninformation. By identifying both the strengths and limitations of\nstate-of-the-art models, our work supports the development of clinically robust\nand globally applicable AI systems for healthcare."}
{"id": "2504.16658", "pdf": "https://arxiv.org/pdf/2504.16658", "abs": "https://arxiv.org/abs/2504.16658", "authors": ["Ole-Christian Galbo Engstrøm", "Erik Schou Dreier", "Birthe Møller Jespersen", "Kim Steenstrup Pedersen"], "title": "A Time Series Dataset of NIR Spectra and RGB and NIR-HSI Images of the Barley Germination Process", "categories": ["cs.CV"], "comment": null, "summary": "We provide an open-source dataset of RGB and NIR-HSI (near-infrared\nhyperspectral imaging) images with associated segmentation masks and NIR\nspectra of 2242 individual malting barley kernels. We imaged every kernel\npre-exposure to moisture and every 24 hours after exposure to moisture for five\nconsecutive days. Every barley kernel was labeled as germinated or not\ngerminated during each image acquisition. The barley kernels were imaged with\nblack filter paper as the background, facilitating straight-forward intensity\nthreshold-based segmentation, e.g., by Otsu's method. This dataset facilitates\ntime series analysis of germination time for barley kernels using either RGB\nimage analysis, NIR spectral analysis, NIR-HSI analysis, or a combination\nhereof."}
{"id": "2504.16834", "pdf": "https://arxiv.org/pdf/2504.16834", "abs": "https://arxiv.org/abs/2504.16834", "authors": ["Yilin Zhai", "Hongyuan Shi", "Chao Zhan", "Qing Wang", "Zaijin You", "Nan Wang"], "title": "Improving Significant Wave Height Prediction Using Chronos Models", "categories": ["cs.LG", "cs.AI", "physics.ao-ph"], "comment": null, "summary": "Accurate wave height prediction is critical for maritime safety and coastal\nresilience, yet conventional physics-based models and traditional machine\nlearning methods face challenges in computational efficiency and nonlinear\ndynamics modeling. This study introduces Chronos, the first implementation of a\nlarge language model (LLM)-powered temporal architecture (Chronos) optimized\nfor wave forecasting. Through advanced temporal pattern recognition applied to\nhistorical wave data from three strategically chosen marine zones in the\nNorthwest Pacific basin, our framework achieves multimodal improvements: (1)\n14.3% reduction in training time with 2.5x faster inference speed compared to\nPatchTST baselines, achieving 0.575 mean absolute scaled error (MASE) units;\n(2) superior short-term forecasting (1-24h) across comprehensive metrics; (3)\nsustained predictive leadership in extended-range forecasts (1-120h); and (4)\ndemonstrated zero-shot capability maintaining median performance (rank 4/12)\nagainst specialized operational models. This LLM-enhanced temporal modeling\nparadigm establishes a new standard in wave prediction, offering both\ncomputationally efficient solutions and a transferable framework for complex\ngeophysical systems modeling."}
{"id": "2504.16381", "pdf": "https://arxiv.org/pdf/2504.16381", "abs": "https://arxiv.org/abs/2504.16381", "authors": ["Magnus Petersen", "Roberto Covino"], "title": "PINN-MEP: Continuous Neural Representations for Minimum-Energy Path Discovery in Molecular Systems", "categories": ["physics.chem-ph", "cs.AI", "physics.comp-ph"], "comment": null, "summary": "Characterizing conformational transitions in physical systems remains a\nfundamental challenge in the computational sciences. Traditional sampling\nmethods like molecular dynamics (MD) or MCMC often struggle with the\nhigh-dimensional nature of molecular systems and the high energy barriers of\ntransitions between stable states. While these transitions are rare events in\nsimulation timescales, they often represent the most biologically significant\nprocesses - for example, the conformational change of an ion channel protein\nfrom its closed to open state, which controls cellular ion flow and is crucial\nfor neural signaling. Such transitions in real systems may take milliseconds to\nseconds but could require months or years of continuous simulation to observe\neven once. We present a method that reformulates transition path generation as\na continuous optimization problem solved through physics-informed neural\nnetworks (PINNs) inspired by string methods for minimum-energy path (MEP)\ngeneration. By representing transition paths as implicit neural functions and\nleveraging automatic differentiation with differentiable molecular dynamics\nforce fields, our method enables the efficient discovery of physically\nrealistic transition pathways without requiring expensive path sampling. We\ndemonstrate our method's effectiveness on two proteins, including an explicitly\nhydrated bovine pancreatic trypsin inhibitor (BPTI) system with over 8,300\natoms."}
{"id": "2407.12022", "pdf": "https://arxiv.org/pdf/2407.12022", "abs": "https://arxiv.org/abs/2407.12022", "authors": ["Peiyang Wu", "Nan Guo", "Xiao Xiao", "Wenming Li", "Xiaochun Ye", "Dongrui Fan"], "title": "ITERTL: An Iterative Framework for Fine-tuning LLMs for RTL Code Generation", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Recently, large language models (LLMs) have demonstrated excellent\nperformance, inspiring researchers to explore their use in automating register\ntransfer level (RTL) code generation and improving hardware design efficiency.\nHowever, the existing approaches to fine-tune LLMs for RTL generation typically\nare conducted on fixed datasets, which do not fully stimulate the capability of\nLLMs and require large amounts of reference data, which are costly to acquire.\nTo mitigate these issues, we innovatively introduce an iterative training\nparadigm named ITERTL. During each iteration, samples are drawn from the model\ntrained in the previous cycle. Then these new samples are employed for training\nin current loop. Furthermore, we introduce a plug-and-play data filtering\nstrategy, thereby encouraging the model to generate high-quality,\nself-contained code. Our model outperforms GPT4 and state-of-the-art (SOTA)\nopen-source models, achieving remarkable 53.8% pass@1 rate on VerilogEval-human\nbenchmark. Under similar conditions of data quantity and quality, our approach\nsignificantly outperforms the baseline. Extensive experiments validate the\neffectiveness of the proposed method."}
{"id": "2504.16665", "pdf": "https://arxiv.org/pdf/2504.16665", "abs": "https://arxiv.org/abs/2504.16665", "authors": ["Wenping Ma", "Boyou Xue", "Mengru Ma", "Chuang Chen", "Hekai Zhang", "Hao Zhu"], "title": "A Diff-Attention Aware State Space Fusion Model for Remote Sensing Classification", "categories": ["cs.CV"], "comment": "12 pages,9 figures", "summary": "Multispectral (MS) and panchromatic (PAN) images describe the same land\nsurface, so these images not only have their own advantages, but also have a\nlot of similar information. In order to separate these similar information and\ntheir respective advantages, reduce the feature redundancy in the fusion stage.\nThis paper introduces a diff-attention aware state space fusion model\n(DAS2F-Model) for multimodal remote sensing image classification. Based on the\nselective state space model, a cross-modal diff-attention module (CMDA-Module)\nis designed to extract and separate the common features and their respective\ndominant features of MS and PAN images. Among this, space preserving visual\nmamba (SPVM) retains image spatial features and captures local features by\noptimizing visual mamba's input reasonably. Considering that features in the\nfusion stage will have large semantic differences after feature separation and\nsimple fusion operations struggle to effectively integrate these significantly\ndifferent features, an attention-aware linear fusion module (AALF-Module) is\nproposed. It performs pixel-wise linear fusion by calculating influence\ncoefficients. This mechanism can fuse features with large semantic differences\nwhile keeping the feature size unchanged. Empirical evaluations indicate that\nthe presented method achieves better results than alternative approaches. The\nrelevant code can be found at:https://github.com/AVKSKVL/DAS-F-Model"}
{"id": "2504.16866", "pdf": "https://arxiv.org/pdf/2504.16866", "abs": "https://arxiv.org/abs/2504.16866", "authors": ["Panagiotis Kakosimos", "Alireza Nemat Saberi", "Luca Peretti"], "title": "An Adaptive ML Framework for Power Converter Monitoring via Federated Transfer Learning", "categories": ["cs.LG"], "comment": "2025 IEEE. Personal use of this material is permitted. Permission\n  from IEEE must be obtained for all other uses, in any current or future\n  media, including reprinting/republishing this material for advertising or\n  promotional purposes, creating new collective works, for resale or\n  redistribution to servers or lists, or reuse of any copyrighted component of\n  this work in other works", "summary": "This study explores alternative framework configurations for adapting thermal\nmachine learning (ML) models for power converters by combining transfer\nlearning (TL) and federated learning (FL) in a piecewise manner. This approach\ninherently addresses challenges such as varying operating conditions, data\nsharing limitations, and security implications. The framework starts with a\nbase model that is incrementally adapted by multiple clients via adapting three\nstate-of-the-art domain adaptation techniques: Fine-tuning, Transfer Component\nAnalysis (TCA), and Deep Domain Adaptation (DDA). The Flower framework is\nemployed for FL, using Federated Averaging for aggregation. Validation with\nfield data demonstrates that fine-tuning offers a straightforward TL approach\nwith high accuracy, making it suitable for practical applications. Benchmarking\nresults reveal a comprehensive comparison of these methods, showcasing their\nrespective strengths and weaknesses when applied in different scenarios.\nLocally hosted FL enhances performance when data aggregation is not feasible,\nwhile cloud-based FL becomes more practical with a significant increase in the\nnumber of clients, addressing scalability and connectivity challenges."}
{"id": "2504.16420", "pdf": "https://arxiv.org/pdf/2504.16420", "abs": "https://arxiv.org/abs/2504.16420", "authors": ["Chengkai Huang", "Hongtao Huang", "Tong Yu", "Kaige Xie", "Junda Wu", "Shuai Zhang", "Julian Mcauley", "Dietmar Jannach", "Lina Yao"], "title": "A Survey of Foundation Model-Powered Recommender Systems: From Feature-Based, Generative to Agentic Paradigms", "categories": ["cs.IR", "cs.AI"], "comment": null, "summary": "Recommender systems (RS) have become essential in filtering information and\npersonalizing content for users. RS techniques have traditionally relied on\nmodeling interactions between users and items as well as the features of\ncontent using models specific to each task. The emergence of foundation models\n(FMs), large scale models trained on vast amounts of data such as GPT, LLaMA\nand CLIP, is reshaping the recommendation paradigm. This survey provides a\ncomprehensive overview of the Foundation Models for Recommender Systems\n(FM4RecSys), covering their integration in three paradigms: (1) Feature-Based\naugmentation of representations, (2) Generative recommendation approaches, and\n(3) Agentic interactive systems. We first review the data foundations of RS,\nfrom traditional explicit or implicit feedback to multimodal content sources.\nWe then introduce FMs and their capabilities for representation learning,\nnatural language understanding, and multi-modal reasoning in RS contexts. The\ncore of the survey discusses how FMs enhance RS under different paradigms.\nAfterward, we examine FM applications in various recommendation tasks. Through\nan analysis of recent research, we highlight key opportunities that have been\nrealized as well as challenges encountered. Finally, we outline open research\ndirections and technical challenges for next-generation FM4RecSys. This survey\nnot only reviews the state-of-the-art methods but also provides a critical\nanalysis of the trade-offs among the feature-based, the generative, and the\nagentic paradigms, outlining key open issues and future research directions."}
{"id": "2407.16615", "pdf": "https://arxiv.org/pdf/2407.16615", "abs": "https://arxiv.org/abs/2407.16615", "authors": ["Ricardo Dominguez-Olmedo", "Vedant Nanda", "Rediet Abebe", "Stefan Bechtold", "Christoph Engel", "Jens Frankenreiter", "Krishna Gummadi", "Moritz Hardt", "Michael Livermore"], "title": "Lawma: The Power of Specialization for Legal Annotation", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "ICLR 2025", "summary": "Annotation and classification of legal text are central components of\nempirical legal research. Traditionally, these tasks are often delegated to\ntrained research assistants. Motivated by the advances in language modeling,\nempirical legal scholars are increasingly turning to prompting commercial\nmodels, hoping that it will alleviate the significant cost of human annotation.\nDespite growing use, our understanding of how to best utilize large language\nmodels for legal annotation remains limited. To bridge this gap, we introduce\nCaselawQA, a benchmark comprising 260 legal annotation tasks, nearly all new to\nthe machine learning community. We demonstrate that commercial models, such as\nGPT-4.5 and Claude 3.7 Sonnet, achieve non-trivial yet highly variable\naccuracy, generally falling short of the performance required for legal work.\nWe then demonstrate that small, lightly fine-tuned models outperform commercial\nmodels. A few hundred to a thousand labeled examples are usually enough to\nachieve higher accuracy. Our work points to a viable alternative to the\npredominant practice of prompting commercial models. For concrete legal\nannotation tasks with some available labeled data, researchers are likely\nbetter off using a fine-tuned open-source model."}
{"id": "2504.16684", "pdf": "https://arxiv.org/pdf/2504.16684", "abs": "https://arxiv.org/abs/2504.16684", "authors": ["Gerardus Croonen", "Andreas Trondl", "Julia Simon", "Daniel Steininger"], "title": "SemanticSugarBeets: A Multi-Task Framework and Dataset for Inspecting Harvest and Storage Characteristics of Sugar Beets", "categories": ["cs.CV", "cs.LG"], "comment": "Accepted at Conference on Computer Vision and Pattern Recognition\n  Workshops (CVPRW). Code and dataset available at\n  https://github.com/semanticsugarbeets/semanticsugarbeets", "summary": "While sugar beets are stored prior to processing, they lose sugar due to\nfactors such as microorganisms present in adherent soil and excess vegetation.\nTheir automated visual inspection promises to aide in quality assurance and\nthereby increase efficiency throughout the processing chain of sugar\nproduction. In this work, we present a novel high-quality annotated dataset and\ntwo-stage method for the detection, semantic segmentation and mass estimation\nof post-harvest and post-storage sugar beets in monocular RGB images. We\nconduct extensive ablation experiments for the detection of sugar beets and\ntheir fine-grained semantic segmentation regarding damages, rot, soil adhesion\nand excess vegetation. For these tasks, we evaluate multiple image sizes, model\narchitectures and encoders, as well as the influence of environmental\nconditions. Our experiments show an mAP50-95 of 98.8 for sugar-beet detection\nand an mIoU of 64.0 for the best-performing segmentation model."}
{"id": "2504.16871", "pdf": "https://arxiv.org/pdf/2504.16871", "abs": "https://arxiv.org/abs/2504.16871", "authors": ["Mirian Hipolito Garcia", "Camille Couturier", "Daniel Madrigal Diaz", "Ankur Mallick", "Anastasios Kyrillidis", "Robert Sim", "Victor Ruhle", "Saravan Rajmohan"], "title": "Exploring How LLMs Capture and Represent Domain-Specific Knowledge", "categories": ["cs.LG"], "comment": null, "summary": "We study whether Large Language Models (LLMs) inherently capture\ndomain-specific nuances in natural language. Our experiments probe the domain\nsensitivity of LLMs by examining their ability to distinguish queries from\ndifferent domains using hidden states generated during the prefill phase. We\nreveal latent domain-related trajectories that indicate the model's internal\nrecognition of query domains. We also study the robustness of these domain\nrepresentations to variations in prompt styles and sources. Our approach\nleverages these representations for model selection, mapping the LLM that best\nmatches the domain trace of the input query (i.e., the model with the highest\nperformance on similar traces). Our findings show that LLMs can differentiate\nqueries for related domains, and that the fine-tuned model is not always the\nmost accurate. Unlike previous work, our interpretations apply to both closed\nand open-ended generative tasks"}
{"id": "2504.16464", "pdf": "https://arxiv.org/pdf/2504.16464", "abs": "https://arxiv.org/abs/2504.16464", "authors": ["Ying Li", "Xiaobao Wei", "Xiaowei Chi", "Yuming Li", "Zhongyu Zhao", "Hao Wang", "Ningning Ma", "Ming Lu", "Shanghang Zhang"], "title": "ManipDreamer: Boosting Robotic Manipulation World Model with Action Tree and Visual Guidance", "categories": ["cs.RO", "cs.AI"], "comment": "9 pages, 3 figures", "summary": "While recent advancements in robotic manipulation video synthesis have shown\npromise, significant challenges persist in ensuring effective\ninstruction-following and achieving high visual quality. Recent methods, like\nRoboDreamer, utilize linguistic decomposition to divide instructions into\nseparate lower-level primitives, conditioning the world model on these\nprimitives to achieve compositional instruction-following. However, these\nseparate primitives do not consider the relationships that exist between them.\nFurthermore, recent methods neglect valuable visual guidance, including depth\nand semantic guidance, both crucial for enhancing visual quality. This paper\nintroduces ManipDreamer, an advanced world model based on the action tree and\nvisual guidance. To better learn the relationships between instruction\nprimitives, we represent the instruction as the action tree and assign\nembeddings to tree nodes, each instruction can acquire its embeddings by\nnavigating through the action tree. The instruction embeddings can be used to\nguide the world model. To enhance visual quality, we combine depth and semantic\nguidance by introducing a visual guidance adapter compatible with the world\nmodel. This visual adapter enhances both the temporal and physical consistency\nof video generation. Based on the action tree and visual guidance, ManipDreamer\nsignificantly boosts the instruction-following ability and visual quality.\nComprehensive evaluations on robotic manipulation benchmarks reveal that\nManipDreamer achieves large improvements in video quality metrics in both seen\nand unseen tasks, with PSNR improved from 19.55 to 21.05, SSIM improved from\n0.7474 to 0.7982 and reduced Flow Error from 3.506 to 3.201 in unseen tasks,\ncompared to the recent RoboDreamer model. Additionally, our method increases\nthe success rate of robotic manipulation tasks by 2.5% in 6 RLbench tasks on\naverage."}
{"id": "2407.17914", "pdf": "https://arxiv.org/pdf/2407.17914", "abs": "https://arxiv.org/abs/2407.17914", "authors": ["Anna Bavaresco", "Marianne de Heer Kloots", "Sandro Pezzelle", "Raquel Fernández"], "title": "Modelling Multimodal Integration in Human Concept Processing with Vision-Language Models", "categories": ["cs.CL"], "comment": null, "summary": "Text representations from language models have proven remarkably predictive\nof human neural activity involved in language processing, with the recent\ntransformer-based models outperforming previous architectures in downstream\ntasks and prediction of brain responses. However, the word representations\nlearnt by language-only models may be limited in that they lack sensory\ninformation from other modalities, which several cognitive and neuroscience\nstudies showed to be reflected in human meaning representations. Here, we\nleverage current pre-trained vision-language models (VLMs) to investigate\nwhether the integration of visuo-linguistic information they operate leads to\nrepresentations that are more aligned with human brain activity than those\nobtained by models trained with language-only input. We focus on fMRI responses\nrecorded while participants read concept words in the context of either a full\nsentence or a picture. Our results reveal that VLM representations correlate\nmore strongly than those by language-only models with activations in brain\nareas functionally related to language processing. Additionally, we find that\ntransformer-based vision-language encoders -- e.g., LXMERT and VisualBERT --\nyield more brain-aligned representations than generative VLMs, whose\nautoregressive abilities do not seem to provide an advantage when modelling\nsingle words. Finally, our ablation analyses suggest that the high brain\nalignment achieved by some of the VLMs we evaluate results from semantic\ninformation acquired specifically during multimodal pretraining as opposed to\nbeing already encoded in their unimodal modules. Altogether, our findings\nindicate an advantage of multimodal models in predicting human brain\nactivations, which reveals that modelling language and vision integration has\nthe potential to capture the multimodal nature of human concept\nrepresentations."}
{"id": "2504.16692", "pdf": "https://arxiv.org/pdf/2504.16692", "abs": "https://arxiv.org/abs/2504.16692", "authors": ["Xinru Meng", "Han Sun", "Jiamei Liu", "Ningzhong Liu", "Huiyu Zhou"], "title": "Energy-Based Pseudo-Label Refining for Source-free Domain Adaptation", "categories": ["cs.CV"], "comment": "8 pages, 3 figures, accepted by PRL. code at\n  https://github.com/Sthen111/EBPR", "summary": "Source-free domain adaptation (SFDA), which involves adapting models without\naccess to source data, is both demanding and challenging. Existing SFDA\ntechniques typically rely on pseudo-labels generated from confidence levels,\nleading to negative transfer due to significant noise. To tackle this problem,\nEnergy-Based Pseudo-Label Refining (EBPR) is proposed for SFDA. Pseudo-labels\nare created for all sample clusters according to their energy scores. Global\nand class energy thresholds are computed to selectively filter pseudo-labels.\nFurthermore, a contrastive learning strategy is introduced to filter difficult\nsamples, aligning them with their augmented versions to learn more\ndiscriminative features. Our method is validated on the Office-31, Office-Home,\nand VisDA-C datasets, consistently finding that our model outperformed\nstate-of-the-art methods."}
{"id": "2504.16875", "pdf": "https://arxiv.org/pdf/2504.16875", "abs": "https://arxiv.org/abs/2504.16875", "authors": ["Julian Bedei", "Murray McBain", "Charles Robert Koch", "Jakob Andert", "David Gordon"], "title": "Hybrid Reinforcement Learning and Model Predictive Control for Adaptive Control of Hydrogen-Diesel Dual-Fuel Combustion", "categories": ["cs.LG"], "comment": null, "summary": "Reinforcement Learning (RL) and Machine Learning Integrated Model Predictive\nControl (ML-MPC) are promising approaches for optimizing hydrogen-diesel\ndual-fuel engine control, as they can effectively control multiple-input\nmultiple-output systems and nonlinear processes. ML-MPC is advantageous for\nproviding safe and optimal controls, ensuring the engine operates within\npredefined safety limits. In contrast, RL is distinguished by its adaptability\nto changing conditions through its learning-based approach. However, the\npractical implementation of either method alone poses challenges. RL requires\nhigh variance in control inputs during early learning phases, which can pose\nrisks to the system by potentially executing unsafe actions, leading to\nmechanical damage. Conversely, ML-MPC relies on an accurate system model to\ngenerate optimal control inputs and has limited adaptability to system drifts,\nsuch as injector aging, which naturally occur in engine applications. To\naddress these limitations, this study proposes a hybrid RL and ML-MPC approach\nthat uses an ML-MPC framework while incorporating an RL agent to dynamically\nadjust the ML-MPC load tracking reference in response to changes in the\nenvironment. At the same time, the ML-MPC ensures that actions stay safe\nthroughout the RL agent's exploration. To evaluate the effectiveness of this\napproach, fuel pressure is deliberately varied to introduce a model-plant\nmismatch between the ML-MPC and the engine test bench. The result of this\nmismatch is a root mean square error (RMSE) in indicated mean effective\npressure of 0.57 bar when running the ML-MPC. The experimental results\ndemonstrate that RL successfully adapts to changing boundary conditions by\naltering the tracking reference while ML-MPC ensures safe control inputs. The\nquantitative improvement in load tracking by implementing RL is an RSME of 0.44\nbar."}
{"id": "2504.16472", "pdf": "https://arxiv.org/pdf/2504.16472", "abs": "https://arxiv.org/abs/2504.16472", "authors": ["Mark Harman", "Peter O'Hearn", "Shubho Sengupta"], "title": "Harden and Catch for Just-in-Time Assured LLM-Based Software Testing: Open Research Challenges", "categories": ["cs.SE", "cs.AI"], "comment": "To Appear as keynote paper at FSE 2025", "summary": "Despite decades of research and practice in automated software testing,\nseveral fundamental concepts remain ill-defined and under-explored, yet offer\nenormous potential real-world impact. We show that these concepts raise\nexciting new challenges in the context of Large Language Models for software\ntest generation. More specifically, we formally define and investigate the\nproperties of hardening and catching tests. A hardening test is one that seeks\nto protect against future regressions, while a catching test is one that\ncatches such a regression or a fault in new functionality introduced by a code\nchange. Hardening tests can be generated at any time and may become catching\ntests when a future regression is caught. We also define and motivate the\nCatching `Just-in-Time' (JiTTest) Challenge, in which tests are generated\n`just-in-time' to catch new faults before they land into production. We show\nthat any solution to Catching JiTTest generation can also be repurposed to\ncatch latent faults in legacy code. We enumerate possible outcomes for\nhardening and catching tests and JiTTests, and discuss open research problems,\ndeployment options, and initial results from our work on automated LLM-based\nhardening at Meta. This paper\\footnote{Author order is alphabetical. The\ncorresponding author is Mark Harman.} was written to accompany the keynote by\nthe authors at the ACM International Conference on the Foundations of Software\nEngineering (FSE) 2025."}
{"id": "2408.06931", "pdf": "https://arxiv.org/pdf/2408.06931", "abs": "https://arxiv.org/abs/2408.06931", "authors": ["João Gonçalves", "Nick Jelicic", "Michele Murgia", "Evert Stamhuis"], "title": "The advantages of context specific language models: the case of the Erasmian Language Model", "categories": ["cs.CL", "cs.AI"], "comment": "12 pages, 3 figures, 1 table", "summary": "The current trend to improve language model performance seems to be based on\nscaling up with the number of parameters (e.g. the state of the art GPT4 model\nhas approximately 1.7 trillion parameters) or the amount of training data fed\ninto the model. However this comes at significant costs in terms of\ncomputational resources and energy costs that compromise the sustainability of\nAI solutions, as well as risk relating to privacy and misuse. In this paper we\npresent the Erasmian Language Model (ELM) a small context specific, 900 million\nparameter model, pre-trained and fine-tuned by and for Erasmus University\nRotterdam. We show how the model performs adequately in a classroom context for\nessay writing, and how it achieves superior performance in subjects that are\npart of its context. This has implications for a wide range of institutions and\norganizations, showing that context specific language models may be a viable\nalternative for resource constrained, privacy sensitive use cases."}
{"id": "2504.16722", "pdf": "https://arxiv.org/pdf/2504.16722", "abs": "https://arxiv.org/abs/2504.16722", "authors": ["Yingjie Xi", "Jian Jun Zhang", "Xiaosong Yang"], "title": "PMG: Progressive Motion Generation via Sparse Anchor Postures Curriculum Learning", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "In computer animation, game design, and human-computer interaction,\nsynthesizing human motion that aligns with user intent remains a significant\nchallenge. Existing methods have notable limitations: textual approaches offer\nhigh-level semantic guidance but struggle to describe complex actions\naccurately; trajectory-based techniques provide intuitive global motion\ndirection yet often fall short in generating precise or customized character\nmovements; and anchor poses-guided methods are typically confined to synthesize\nonly simple motion patterns. To generate more controllable and precise human\nmotions, we propose \\textbf{ProMoGen (Progressive Motion Generation)}, a novel\nframework that integrates trajectory guidance with sparse anchor motion\ncontrol. Global trajectories ensure consistency in spatial direction and\ndisplacement, while sparse anchor motions only deliver precise action guidance\nwithout displacement. This decoupling enables independent refinement of both\naspects, resulting in a more controllable, high-fidelity, and sophisticated\nmotion synthesis. ProMoGen supports both dual and single control paradigms\nwithin a unified training process. Moreover, we recognize that direct learning\nfrom sparse motions is inherently unstable, we introduce \\textbf{SAP-CL (Sparse\nAnchor Posture Curriculum Learning)}, a curriculum learning strategy that\nprogressively adjusts the number of anchors used for guidance, thereby enabling\nmore precise and stable convergence. Extensive experiments demonstrate that\nProMoGen excels in synthesizing vivid and diverse motions guided by predefined\ntrajectory and arbitrary anchor frames. Our approach seamlessly integrates\npersonalized motion with structured guidance, significantly outperforming\nstate-of-the-art methods across multiple control scenarios."}
{"id": "2504.16929", "pdf": "https://arxiv.org/pdf/2504.16929", "abs": "https://arxiv.org/abs/2504.16929", "authors": ["Shaden Alshammari", "John Hershey", "Axel Feldmann", "William T. Freeman", "Mark Hamilton"], "title": "I-Con: A Unifying Framework for Representation Learning", "categories": ["cs.LG", "cs.AI", "cs.CV", "cs.IT", "math.IT"], "comment": "ICLR 2025; website: https://aka.ms/i-con . Proceedings of the\n  Thirteenth International Conference on Learning Representations (ICLR 2025)", "summary": "As the field of representation learning grows, there has been a proliferation\nof different loss functions to solve different classes of problems. We\nintroduce a single information-theoretic equation that generalizes a large\ncollection of modern loss functions in machine learning. In particular, we\nintroduce a framework that shows that several broad classes of machine learning\nmethods are precisely minimizing an integrated KL divergence between two\nconditional distributions: the supervisory and learned representations. This\nviewpoint exposes a hidden information geometry underlying clustering, spectral\nmethods, dimensionality reduction, contrastive learning, and supervised\nlearning. This framework enables the development of new loss functions by\ncombining successful techniques from across the literature. We not only present\na wide array of proofs, connecting over 23 different approaches, but we also\nleverage these theoretical results to create state-of-the-art unsupervised\nimage classifiers that achieve a +8% improvement over the prior\nstate-of-the-art on unsupervised classification on ImageNet-1K. We also\ndemonstrate that I-Con can be used to derive principled debiasing methods which\nimprove contrastive representation learners."}
{"id": "2504.16479", "pdf": "https://arxiv.org/pdf/2504.16479", "abs": "https://arxiv.org/abs/2504.16479", "authors": ["Yujie Qin", "Ming He", "Changyong Yu", "Ming Ni", "Xian Liu", "Xiaochen Bo"], "title": "The Dance of Atoms-De Novo Protein Design with Diffusion Model", "categories": ["q-bio.BM", "cs.AI"], "comment": null, "summary": "The de novo design of proteins refers to creating proteins with specific\nstructures and functions that do not naturally exist. In recent years, the\naccumulation of high-quality protein structure and sequence data and\ntechnological advancements have paved the way for the successful application of\ngenerative artificial intelligence (AI) models in protein design. These models\nhave surpassed traditional approaches that rely on fragments and\nbioinformatics. They have significantly enhanced the success rate of de novo\nprotein design, and reduced experimental costs, leading to breakthroughs in the\nfield. Among various generative AI models, diffusion models have yielded the\nmost promising results in protein design. In the past two to three years, more\nthan ten protein design models based on diffusion models have emerged. Among\nthem, the representative model, RFDiffusion, has demonstrated success rates in\n25 protein design tasks that far exceed those of traditional methods, and other\nAI-based approaches like RFjoint and hallucination. This review will\nsystematically examine the application of diffusion models in generating\nprotein backbones and sequences. We will explore the strengths and limitations\nof different models, summarize successful cases of protein design using\ndiffusion models, and discuss future development directions."}
{"id": "2409.06601", "pdf": "https://arxiv.org/pdf/2409.06601", "abs": "https://arxiv.org/abs/2409.06601", "authors": ["Yetao Wu", "Yihong Wang", "Teng Chen", "Ningyuan Xi", "Qingqing Gu", "Hongyang Lei", "Luo Ji"], "title": "lamss: when large language models meet self-skepticism", "categories": ["cs.CL", "cs.LG"], "comment": "11 pages, 6 figures", "summary": "Hallucination is a major challenge for large language models (LLMs), prevent\ning their further application in some fields. The skeptical thinking of\nhumankind\n  could be useful for LLMs to self-cognition, self-reflection and alleviate\ntheir hal lucinations. Inspired by this consideration, we propose a novel\napproach called\n  LaMsS, which combines the semantic understanding capability of LLMs with\n  self-skepticism. By introducing a series of skepticism tokens and augmenting\n  them into the vocabulary, we conduct both pertaining and finetuning, which\nallow\n  the LLM to decode each normal token followed by a skeptical token, represent\ning different skepticism levels. By calculating the response skepticism given a\n  query, one can define a new self-aware LLM which is only willing to answer\n  with relative lower skepticism level than the threshold. By examining the\naccu racy, AUC and AP of willingly answering questions, we demonstrate that\nLaMsS\n  achieves better performance than baselines on both multi-choice questions and\n  open-domain question-answering benchmarks, and can generalize to multi-task\n  and out-of-domain settings. Our study sheds some lights on the\nself-skepticism\n  modeling on further artificial intelligence. Project code and model\ncheckpoints\n  can be found in https://anonymous.4open.science/r/SM-1E76."}
{"id": "2504.16723", "pdf": "https://arxiv.org/pdf/2504.16723", "abs": "https://arxiv.org/abs/2504.16723", "authors": ["Ali Anaissi", "Junaid Akram", "Kunal Chaturvedi", "Ali Braytee"], "title": "Detecting and Understanding Hateful Contents in Memes Through Captioning and Visual Question-Answering", "categories": ["cs.CV", "cs.AI"], "comment": "13 pages, 2 figures, 2025 International Conference on Computational\n  Science", "summary": "Memes are widely used for humor and cultural commentary, but they are\nincreasingly exploited to spread hateful content. Due to their multimodal\nnature, hateful memes often evade traditional text-only or image-only detection\nsystems, particularly when they employ subtle or coded references. To address\nthese challenges, we propose a multimodal hate detection framework that\nintegrates key components: OCR to extract embedded text, captioning to describe\nvisual content neutrally, sub-label classification for granular categorization\nof hateful content, RAG for contextually relevant retrieval, and VQA for\niterative analysis of symbolic and contextual cues. This enables the framework\nto uncover latent signals that simpler pipelines fail to detect. Experimental\nresults on the Facebook Hateful Memes dataset reveal that the proposed\nframework exceeds the performance of unimodal and conventional multimodal\nmodels in both accuracy and AUC-ROC."}
{"id": "2504.16098", "pdf": "https://arxiv.org/pdf/2504.16098", "abs": "https://arxiv.org/abs/2504.16098", "authors": ["Tianning Feng", "Junting Ni", "Ezequiel Gleichgerrcht", "Wei Jin"], "title": "SeizureFormer: A Transformer Model for IEA-Based Seizure Risk Forecasting", "categories": ["eess.SP", "cs.LG", "I.5.1; I.2.6"], "comment": "9 pages, 2 figures. Submitted to AMIA 2025. Also submitted as an\n  undergraduate honors thesis at Emory University", "summary": "We present SeizureFormer, a Transformer-based model for long-term seizure\nrisk forecasting using interictal epileptiform activity (IEA) surrogate\nbiomarkers and long episode (LE) biomarkers from responsive neurostimulation\n(RNS) systems. Unlike raw scalp EEG-based models, SeizureFormer leverages\nstructured, clinically relevant features and integrates CNN-based patch\nembedding, multi-head self-attention, and squeeze-and-excitation blocks to\nmodel both short-term dynamics and long-term seizure cycles. Tested across five\npatients and multiple prediction windows (1 to 14 days), SeizureFormer achieved\nstate-of-the-art performance with mean ROC AUC of 79.44 percent and mean PR AUC\nof 76.29 percent. Compared to statistical, machine learning, and deep learning\nbaselines, it demonstrates enhanced generalizability and seizure risk\nforecasting performance under class imbalance. This work supports future\nclinical integration of interpretable and robust seizure forecasting tools for\npersonalized epilepsy management."}
{"id": "2504.16485", "pdf": "https://arxiv.org/pdf/2504.16485", "abs": "https://arxiv.org/abs/2504.16485", "authors": ["Syed Mohammad Kashif", "Peng Liang", "Amjed Tahir"], "title": "On Developers' Self-Declaration of AI-Generated Code: An Analysis of Practices", "categories": ["cs.SE", "cs.AI"], "comment": "35 pages, 17 images, 8 tables, Manuscript submitted to a journal\n  (2025)", "summary": "AI code generation tools have gained significant popularity among developers,\nwho use them to assist in software development due to their capability to\ngenerate code. Existing studies mainly explored the quality, e.g., correctness\nand security, of AI-generated code, while in real-world software development,\nthe prerequisite is to distinguish AI-generated code from human-written code,\nwhich emphasizes the need to explicitly declare AI-generated code by\ndevelopers. To this end, this study intends to understand the ways developers\nuse to self-declare AI-generated code and explore the reasons why developers\nchoose to self-declare or not. We conducted a mixed-methods study consisting of\ntwo phases. In the first phase, we mined GitHub repositories and collected 613\ninstances of AI-generated code snippets. In the second phase, we conducted a\nfollow-up industrial survey, which received 111 valid responses. Our research\nrevealed the practices followed by developers to self-declare AI-generated\ncode. Most practitioners (76.6%) always or sometimes self-declare AI-generated\ncode. In contrast, other practitioners (23.4%) noted that they never\nself-declare AI-generated code. The reasons for self-declaring AI-generated\ncode include the need to track and monitor the code for future review and\ndebugging, and ethical considerations. The reasons for not self-declaring\nAI-generated code include extensive modifications to AI-generated code and the\ndevelopers' perception that self-declaration is an unnecessary activity. We\nfinally provided guidelines for practitioners to self-declare AI-generated\ncode, addressing ethical and code quality concerns."}
{"id": "2410.19572", "pdf": "https://arxiv.org/pdf/2410.19572", "abs": "https://arxiv.org/abs/2410.19572", "authors": ["Ishneet Sukhvinder Singh", "Ritvik Aggarwal", "Ibrahim Allahverdiyev", "Muhammad Taha", "Aslihan Akalin", "Kevin Zhu", "Sean O'Brien"], "title": "ChunkRAG: Novel LLM-Chunk Filtering Method for RAG Systems", "categories": ["cs.CL"], "comment": "Accepted at Conference of the North American Chapter of the\n  Association for Computational Linguistics, Student Research Workshop 2025\n  (NAACL SRW 2025)", "summary": "Retrieval-Augmented Generation (RAG) systems using large language models\n(LLMs) often generate inaccurate responses due to the retrieval of irrelevant\nor loosely related information. Existing methods, which operate at the document\nlevel, fail to effectively filter out such content. We propose LLM-driven chunk\nfiltering, ChunkRAG, a framework that enhances RAG systems by evaluating and\nfiltering retrieved information at the chunk level. Our approach employs\nsemantic chunking to divide documents into coherent sections and utilizes\nLLM-based relevance scoring to assess each chunk's alignment with the user's\nquery. By filtering out less pertinent chunks before the generation phase, we\nsignificantly reduce hallucinations and improve factual accuracy. Experiments\nshow that our method outperforms existing RAG models, achieving higher accuracy\non tasks requiring precise information retrieval. This advancement enhances the\nreliability of RAG systems, making them particularly beneficial for\napplications like fact-checking and multi-hop reasoning."}
{"id": "2504.16727", "pdf": "https://arxiv.org/pdf/2504.16727", "abs": "https://arxiv.org/abs/2504.16727", "authors": ["Zhiyuan Fan", "Yumeng Wang", "Sandeep Polisetty", "Yi R.", "Fung"], "title": "V$^2$R-Bench: Holistically Evaluating LVLM Robustness to Fundamental Visual Variations", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Large Vision Language Models (LVLMs) excel in various vision-language tasks.\nYet, their robustness to visual variations in position, scale, orientation, and\ncontext that objects in natural scenes inevitably exhibit due to changes in\nviewpoint and environment remains largely underexplored. To bridge this gap, we\nintroduce V$^2$R-Bench, a comprehensive benchmark framework for evaluating\nVisual Variation Robustness of LVLMs, which encompasses automated evaluation\ndataset generation and principled metrics for thorough robustness assessment.\nThrough extensive evaluation on 21 LVLMs, we reveal a surprising vulnerability\nto visual variations, in which even advanced models that excel at complex\nvision-language tasks significantly underperform on simple tasks such as object\nrecognition. Interestingly, these models exhibit a distinct visual position\nbias that contradicts theories of effective receptive fields, and demonstrate a\nhuman-like visual acuity threshold. To identify the source of these\nvulnerabilities, we present a systematic framework for component-level\nanalysis, featuring a novel visualization approach for aligned visual features.\nResults show that these vulnerabilities stem from error accumulation in the\npipeline architecture and inadequate multimodal alignment. Complementary\nexperiments with synthetic data further demonstrate that these limitations are\nfundamentally architectural deficiencies, scoring the need for architectural\ninnovations in future LVLM designs."}
{"id": "2504.16489", "pdf": "https://arxiv.org/pdf/2504.16489", "abs": "https://arxiv.org/abs/2504.16489", "authors": ["Senmao Qi", "Yifei Zou", "Peng Li", "Ziyi Lin", "Xiuzhen Cheng", "Dongxiao Yu"], "title": "Amplified Vulnerabilities: Structured Jailbreak Attacks on LLM-based Multi-Agent Debate", "categories": ["cs.CR", "cs.AI"], "comment": "33 pages, 5 figures", "summary": "Multi-Agent Debate (MAD), leveraging collaborative interactions among Large\nLanguage Models (LLMs), aim to enhance reasoning capabilities in complex tasks.\nHowever, the security implications of their iterative dialogues and\nrole-playing characteristics, particularly susceptibility to jailbreak attacks\neliciting harmful content, remain critically underexplored. This paper\nsystematically investigates the jailbreak vulnerabilities of four prominent MAD\nframeworks built upon leading commercial LLMs (GPT-4o, GPT-4, GPT-3.5-turbo,\nand DeepSeek) without compromising internal agents. We introduce a novel\nstructured prompt-rewriting framework specifically designed to exploit MAD\ndynamics via narrative encapsulation, role-driven escalation, iterative\nrefinement, and rhetorical obfuscation. Our extensive experiments demonstrate\nthat MAD systems are inherently more vulnerable than single-agent setups.\nCrucially, our proposed attack methodology significantly amplifies this\nfragility, increasing average harmfulness from 28.14% to 80.34% and achieving\nattack success rates as high as 80% in certain scenarios. These findings reveal\nintrinsic vulnerabilities in MAD architectures and underscore the urgent need\nfor robust, specialized defenses prior to real-world deployment."}
{"id": "2411.03883", "pdf": "https://arxiv.org/pdf/2411.03883", "abs": "https://arxiv.org/abs/2411.03883", "authors": ["Laura Cabello", "Carmen Martin-Turrero", "Uchenna Akujuobi", "Anders Søgaard", "Carlos Bobed"], "title": "MEG: Medical Knowledge-Augmented Large Language Models for Question Answering", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": null, "summary": "Question answering is a natural language understanding task that involves\nreasoning over both explicit context, and unstated relevant domain knowledge.\nDespite the high cost of training, large language models (LLMs) -- the backbone\nof most modern question-answering systems -- still struggle to reliably capture\nthe nuanced relationships between concepts that are crucial for reasoning in\nspecialized fields like medicine. In this work, we present MEG, a\nparameter-efficient approach for medical knowledge-augmented LLMs. MEG uses a\nlightweight mapping network to incorporate knowledge graph embeddings into the\nLLM, enabling it to leverage external knowledge in a cost-effective way. We\nevaluate our method on four popular medical multiple-choice datasets and show\nthat LLMs i) can effectively interpret knowledge graph embeddings and ii) gain\nsignificant advantages from the factual grounding these embeddings provide. MEG\nattains an average of +6.7% and +9.9% accuracy over specialized models like\nBioMistral-7B and MediTron-7B, respectively. Finally, we show that MEG's\nperformance remains robust to the choice of graph encoder."}
{"id": "2504.16739", "pdf": "https://arxiv.org/pdf/2504.16739", "abs": "https://arxiv.org/abs/2504.16739", "authors": ["Tristan Piater", "Björn Barz", "Alexander Freytag"], "title": "Prompt-Tuning SAM: From Generalist to Specialist with only 2048 Parameters and 16 Training Images", "categories": ["cs.CV"], "comment": null, "summary": "The Segment Anything Model (SAM) is widely used for segmenting a diverse\nrange of objects in natural images from simple user prompts like points or\nbounding boxes. However, SAM's performance decreases substantially when applied\nto non-natural domains like microscopic imaging. Furthermore, due to SAM's\ninteractive design, it requires a precise prompt for each image and object,\nwhich is unfeasible in many automated biomedical applications. Previous\nsolutions adapt SAM by training millions of parameters via fine-tuning large\nparts of the model or of adapter layers. In contrast, we show that as little as\n2,048 additional parameters are sufficient for turning SAM into a use-case\nspecialist for a certain downstream task. Our novel PTSAM (prompt-tuned SAM)\nmethod uses prompt-tuning, a parameter-efficient fine-tuning technique, to\nadapt SAM for a specific task. We validate the performance of our approach on\nmultiple microscopic and one medical dataset. Our results show that\nprompt-tuning only SAM's mask decoder already leads to a performance on-par\nwith state-of-the-art techniques while requiring roughly 2,000x less trainable\nparameters. For addressing domain gaps, we find that additionally prompt-tuning\nSAM's image encoder is beneficial, further improving segmentation accuracy by\nup to 18% over state-of-the-art results. Since PTSAM can be reliably trained\nwith as little as 16 annotated images, we find it particularly helpful for\napplications with limited training data and domain shifts."}
{"id": "2504.16137", "pdf": "https://arxiv.org/pdf/2504.16137", "abs": "https://arxiv.org/abs/2504.16137", "authors": ["Jasper Götting", "Pedro Medeiros", "Jon G Sanders", "Nathaniel Li", "Long Phan", "Karam Elabd", "Lennart Justen", "Dan Hendrycks", "Seth Donoughe"], "title": "Virology Capabilities Test (VCT): A Multimodal Virology Q&A Benchmark", "categories": ["cs.CY", "cs.LG"], "comment": "31 pages", "summary": "We present the Virology Capabilities Test (VCT), a large language model (LLM)\nbenchmark that measures the capability to troubleshoot complex virology\nlaboratory protocols. Constructed from the inputs of dozens of PhD-level expert\nvirologists, VCT consists of $322$ multimodal questions covering fundamental,\ntacit, and visual knowledge that is essential for practical work in virology\nlaboratories. VCT is difficult: expert virologists with access to the internet\nscore an average of $22.1\\%$ on questions specifically in their sub-areas of\nexpertise. However, the most performant LLM, OpenAI's o3, reaches $43.8\\%$\naccuracy, outperforming $94\\%$ of expert virologists even within their\nsub-areas of specialization. The ability to provide expert-level virology\ntroubleshooting is inherently dual-use: it is useful for beneficial research,\nbut it can also be misused. Therefore, the fact that publicly available models\noutperform virologists on VCT raises pressing governance considerations. We\npropose that the capability of LLMs to provide expert-level troubleshooting of\ndual-use virology work should be integrated into existing frameworks for\nhandling dual-use technologies in the life sciences."}
{"id": "2504.16548", "pdf": "https://arxiv.org/pdf/2504.16548", "abs": "https://arxiv.org/abs/2504.16548", "authors": ["Lirui Guo", "Michael G. Burke", "Wynita M. Griggs"], "title": "Exploring human-SAV interaction using large language models: The impact of psychological ownership and anthropomorphism on user experience", "categories": ["cs.HC", "cs.AI", "cs.ET"], "comment": null, "summary": "There has been extensive prior work exploring how psychological factors such\nas anthropomorphism affect the adoption of shared autonomous vehicles (SAVs).\nHowever, limited research has been conducted on how prompt strategies in large\nlanguage model (LLM)-powered SAV User Interfaces (UIs) affect users'\nperceptions, experiences, and intentions to adopt such technology. In this\nwork, we investigate how conversational UIs powered by LLMs drive these\npsychological factors and psychological ownership, the sense of possession a\nuser may come to feel towards an entity or object they may not legally own. We\ndesigned four SAV UIs with varying levels of anthropomorphic characteristics\nand psychological ownership triggers. Quantitative measures of psychological\nownership, anthropomorphism, quality of service, disclosure tendency, sentiment\nof SAV responses, and overall acceptance were collected after participants\ninteracted with each SAV. Qualitative feedback was also gathered regarding the\nexperience of psychological ownership during the interactions. The results\nindicate that an SAV conversational UI designed to be more anthropomorphic and\nto induce psychological ownership improved users' perceptions of the SAV's\nhuman-like qualities and improved the sentiment of responses compared to a\ncontrol condition. These findings provide practical guidance for designing\nLLM-based conversational UIs that enhance user experience and adoption of SAVs."}
{"id": "2411.05000", "pdf": "https://arxiv.org/pdf/2411.05000", "abs": "https://arxiv.org/abs/2411.05000", "authors": ["Jonathan Roberts", "Kai Han", "Samuel Albanie"], "title": "Needle Threading: Can LLMs Follow Threads through Near-Million-Scale Haystacks?", "categories": ["cs.CL"], "comment": "Accepted at ICLR 2025", "summary": "As the context limits of Large Language Models (LLMs) increase, the range of\npossible applications and downstream functions broadens. In many real-world\ntasks, decisions depend on details scattered across collections of often\ndisparate documents containing mostly irrelevant information. Long-context LLMs\nappear well-suited to this form of complex information retrieval and reasoning,\nwhich has traditionally proven costly and time-consuming. However, although the\ndevelopment of longer context models has seen rapid gains in recent years, our\nunderstanding of how effectively LLMs use their context has not kept pace. To\naddress this, we conduct a set of retrieval experiments designed to evaluate\nthe capabilities of 17 leading LLMs, such as their ability to follow threads of\ninformation through the context window. Strikingly, we find that many models\nare remarkably threadsafe: capable of simultaneously following multiple threads\nwithout significant loss in performance. Still, for many models, we find the\neffective context limit is significantly shorter than the supported context\nlength, with accuracy decreasing as the context window grows. Our study also\nhighlights the important point that token counts from different tokenizers\nshould not be directly compared -- they often correspond to substantially\ndifferent numbers of written characters. We release our code and long-context\nexperimental data."}
{"id": "2504.16740", "pdf": "https://arxiv.org/pdf/2504.16740", "abs": "https://arxiv.org/abs/2504.16740", "authors": ["Farhad G. Zanjani", "Davide Abati", "Auke Wiggers", "Dimitris Kalatzis", "Jens Petersen", "Hong Cai", "Amirhossein Habibian"], "title": "Gaussian Splatting is an Effective Data Generator for 3D Object Detection", "categories": ["cs.CV"], "comment": null, "summary": "We investigate data augmentation for 3D object detection in autonomous\ndriving. We utilize recent advancements in 3D reconstruction based on Gaussian\nSplatting for 3D object placement in driving scenes. Unlike existing\ndiffusion-based methods that synthesize images conditioned on BEV layouts, our\napproach places 3D objects directly in the reconstructed 3D space with\nexplicitly imposed geometric transformations. This ensures both the physical\nplausibility of object placement and highly accurate 3D pose and position\nannotations.\n  Our experiments demonstrate that even by integrating a limited number of\nexternal 3D objects into real scenes, the augmented data significantly enhances\n3D object detection performance and outperforms existing diffusion-based 3D\naugmentation for object detection. Extensive testing on the nuScenes dataset\nreveals that imposing high geometric diversity in object placement has a\ngreater impact compared to the appearance diversity of objects. Additionally,\nwe show that generating hard examples, either by maximizing detection loss or\nimposing high visual occlusion in camera images, does not lead to more\nefficient 3D data augmentation for camera-based 3D object detection in\nautonomous driving."}
{"id": "2504.16143", "pdf": "https://arxiv.org/pdf/2504.16143", "abs": "https://arxiv.org/abs/2504.16143", "authors": ["Gideon Vos", "Maryam Ebrahimpour", "Liza van Eijk", "Zoltan Sarnyai", "Mostafa Rahimi Azghadi"], "title": "A Statistical Approach for Synthetic EEG Data Generation", "categories": ["eess.SP", "cs.LG", "68T01, 92-08"], "comment": "24 pages, 10 figures", "summary": "Electroencephalogram (EEG) data is crucial for diagnosing mental health\nconditions but is costly and time-consuming to collect at scale. Synthetic data\ngeneration offers a promising solution to augment datasets for machine learning\napplications. However, generating high-quality synthetic EEG that preserves\nemotional and mental health signals remains challenging. This study proposes a\nmethod combining correlation analysis and random sampling to generate realistic\nsynthetic EEG data.\n  We first analyze interdependencies between EEG frequency bands using\ncorrelation analysis. Guided by this structure, we generate synthetic samples\nvia random sampling. Samples with high correlation to real data are retained\nand evaluated through distribution analysis and classification tasks. A Random\nForest model trained to distinguish synthetic from real EEG performs at chance\nlevel, indicating high fidelity.\n  The generated synthetic data closely match the statistical and structural\nproperties of the original EEG, with similar correlation coefficients and no\nsignificant differences in PERMANOVA tests. This method provides a scalable,\nprivacy-preserving approach for augmenting EEG datasets, enabling more\nefficient model training in mental health research."}
{"id": "2504.16562", "pdf": "https://arxiv.org/pdf/2504.16562", "abs": "https://arxiv.org/abs/2504.16562", "authors": ["Julian Rasch", "Florian Müller", "Francesco Chiossi"], "title": "A Vision for AI-Driven Adaptation of Dynamic AR Content to Users and Environments", "categories": ["cs.HC", "cs.AI"], "comment": null, "summary": "Augmented Reality (AR) is transforming the way we interact with virtual\ninformation in the physical world. By overlaying digital content in real-world\nenvironments, AR enables new forms of immersive and engaging experiences.\nHowever, existing AR systems often struggle to effectively manage the many\ninteractive possibilities that AR presents. This vision paper speculates on\nAI-driven approaches for adaptive AR content placement, dynamically adjusting\nto user movement and environmental changes. By leveraging machine learning\nmethods, such a system would intelligently manage content distribution between\nAR projections integrated into the external environment and fixed static\ncontent, enabling seamless UI layout and potentially reducing users' cognitive\nload. By exploring the possibilities of AI-driven dynamic AR content placement,\nwe aim to envision new opportunities for innovation and improvement in various\nindustries, from urban navigation and workplace productivity to immersive\nlearning and beyond. This paper outlines a vision for the development of more\nintuitive, engaging, and effective AI-powered AR experiences."}
{"id": "2411.06037", "pdf": "https://arxiv.org/pdf/2411.06037", "abs": "https://arxiv.org/abs/2411.06037", "authors": ["Hailey Joren", "Jianyi Zhang", "Chun-Sung Ferng", "Da-Cheng Juan", "Ankur Taly", "Cyrus Rashtchian"], "title": "Sufficient Context: A New Lens on Retrieval Augmented Generation Systems", "categories": ["cs.CL"], "comment": null, "summary": "Augmenting LLMs with context leads to improved performance across many\napplications. Despite much research on Retrieval Augmented Generation (RAG)\nsystems, an open question is whether errors arise because LLMs fail to utilize\nthe context from retrieval or the context itself is insufficient to answer the\nquery. To shed light on this, we develop a new notion of sufficient context,\nalong with a method to classify instances that have enough information to\nanswer the query. We then use sufficient context to analyze several models and\ndatasets. By stratifying errors based on context sufficiency, we find that\nlarger models with higher baseline performance (Gemini 1.5 Pro, GPT 4o, Claude\n3.5) excel at answering queries when the context is sufficient, but often\noutput incorrect answers instead of abstaining when the context is not. On the\nother hand, smaller models with lower baseline performance (Mistral 3, Gemma 2)\nhallucinate or abstain often, even with sufficient context. We further\ncategorize cases when the context is useful, and improves accuracy, even though\nit does not fully answer the query and the model errs without the context.\nBuilding on our findings, we explore ways to reduce hallucinations in RAG\nsystems, including a new selective generation method that leverages sufficient\ncontext information for guided abstention. Our method improves the fraction of\ncorrect answers among times where the model responds by 2--10\\% for Gemini,\nGPT, and Gemma. Key findings and the prompts used in our autorater analysis are\navailable on our github."}
{"id": "2504.16749", "pdf": "https://arxiv.org/pdf/2504.16749", "abs": "https://arxiv.org/abs/2504.16749", "authors": ["Rupak Bose", "Chinedu Innocent Nwoye", "Jorge Lazo", "Joël Lukas Lavanchy", "Nicolas Padoy"], "title": "Feature Mixing Approach for Detecting Intraoperative Adverse Events in Laparoscopic Roux-en-Y Gastric Bypass Surgery", "categories": ["cs.CV"], "comment": "9 pages, 7 figures, 8 tables, Release new dataset annotations", "summary": "Intraoperative adverse events (IAEs), such as bleeding or thermal injury, can\nlead to severe postoperative complications if undetected. However, their rarity\nresults in highly imbalanced datasets, posing challenges for AI-based detection\nand severity quantification. We propose BetaMixer, a novel deep learning model\nthat addresses these challenges through a Beta distribution-based mixing\napproach, converting discrete IAE severity scores into continuous values for\nprecise severity regression (0-5 scale). BetaMixer employs Beta\ndistribution-based sampling to enhance underrepresented classes and regularizes\nintermediate embeddings to maintain a structured feature space. A generative\napproach aligns the feature space with sampled IAE severity, enabling robust\nclassification and severity regression via a transformer. Evaluated on the\nMultiBypass140 dataset, which we extended with IAE labels, BetaMixer achieves a\nweighted F1 score of 0.76, recall of 0.81, PPV of 0.73, and NPV of 0.84,\ndemonstrating strong performance on imbalanced data. By integrating Beta\ndistribution-based sampling, feature mixing, and generative modeling, BetaMixer\noffers a robust solution for IAE detection and quantification in clinical\nsettings."}
{"id": "2504.16185", "pdf": "https://arxiv.org/pdf/2504.16185", "abs": "https://arxiv.org/abs/2504.16185", "authors": ["Emily Minus", "R. Yates Coley", "Susan M. Shortreed", "Brian D. Williamson"], "title": "Behavior of prediction performance metrics with rare events", "categories": ["stat.ML", "cs.LG"], "comment": "55 pages (21 main, 34 supplementary), 26 tables (3 main, 23\n  supplementary), 5 figures (3 main, 2 supplementary)", "summary": "Area under the receiving operator characteristic curve (AUC) is commonly\nreported alongside binary prediction models. However, there are concerns that\nAUC might be a misleading measure of prediction performance in the rare event\nsetting. This setting is common since many events of clinical importance are\nrare events. We conducted a simulation study to determine when or whether AUC\nis unstable in the rare event setting. Specifically, we aimed to determine\nwhether the bias and variance of AUC are driven by the number of events or the\nevent rate. We also investigated the behavior of other commonly used measures\nof prediction performance, including positive predictive value, accuracy,\nsensitivity, and specificity. Our results indicate that poor AUC behavior -- as\nmeasured by empirical bias, variability of cross-validated AUC estimates, and\nempirical coverage of confidence intervals -- is driven by the minimum class\nsize, not event rate. Performance of sensitivity is driven by the number of\nevents, while that of specificity is driven by the number of non-events. Other\nmeasures, including positive predictive value and accuracy, depend on the event\nrate even in large samples. AUC is reliable in the rare event setting provided\nthat the total number of events is moderately large."}
{"id": "2504.16573", "pdf": "https://arxiv.org/pdf/2504.16573", "abs": "https://arxiv.org/abs/2504.16573", "authors": ["Xianghe Liu", "Jiaqi Xu", "Tao Sun"], "title": "PsyCounAssist: A Full-Cycle AI-Powered Psychological Counseling Assistant System", "categories": ["cs.HC", "cs.AI"], "comment": null, "summary": "Psychological counseling is a highly personalized and dynamic process that\nrequires therapists to continuously monitor emotional changes, document session\ninsights, and maintain therapeutic continuity. In this paper, we introduce\nPsyCounAssist, a comprehensive AI-powered counseling assistant system\nspecifically designed to augment psychological counseling practices.\nPsyCounAssist integrates multimodal emotion recognition combining speech and\nphotoplethysmography (PPG) signals for accurate real-time affective analysis,\nautomated structured session reporting using large language models (LLMs), and\npersonalized AI-generated follow-up support. Deployed on Android-based tablet\ndevices, the system demonstrates practical applicability and flexibility in\nreal-world counseling scenarios. Experimental evaluation confirms the\nreliability of PPG-based emotional classification and highlights the system's\npotential for non-intrusive, privacy-aware emotional support. PsyCounAssist\nrepresents a novel approach to ethically and effectively integrating AI into\npsychological counseling workflows."}
{"id": "2412.06845", "pdf": "https://arxiv.org/pdf/2412.06845", "abs": "https://arxiv.org/abs/2412.06845", "authors": ["Pu Zhao", "Xuan Shen", "Zhenglun Kong", "Yixin Shen", "Sung-En Chang", "Timothy Rupprecht", "Lei Lu", "Enfu Nan", "Changdi Yang", "Yumei He", "Weiyan Shi", "Xingchen Xu", "Yu Huang", "Wei Jiang", "Wei Wang", "Yue Chen", "Yong He", "Yanzhi Wang"], "title": "7B Fully Open Source Moxin-LLM -- From Pretraining to GRPO-based Reinforcement Learning Enhancement", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": null, "summary": "Recently, Large Language Models (LLMs) have undergone a significant\ntransformation, marked by a rapid rise in both their popularity and\ncapabilities. Leading this evolution are proprietary LLMs like GPT-4 and\nGPT-o1, which have captured widespread attention in the AI community due to\ntheir remarkable performance and versatility. Simultaneously, open-source LLMs,\nsuch as LLaMA, have made great contributions to the ever-increasing popularity\nof LLMs due to the ease to customize and deploy the models across diverse\napplications. Although open-source LLMs present unprecedented opportunities for\ninnovation and research, the commercialization of LLMs has raised concerns\nabout transparency, reproducibility, and safety. Many open-source LLMs fail to\nmeet fundamental transparency requirements by withholding essential components\nlike training code and data, which may hinder further innovations on LLMs. To\nmitigate this issue, we introduce Moxin 7B, a fully open-source LLM developed,\nadhering to principles of open science, open source, open data, and open\naccess. We release the pre-training code and configurations, training and\nfine-tuning datasets, and intermediate and final checkpoints, aiming to make\ncontinuous commitments to fully open-source LLMs. After pre-training and\nobtaining the base model, we finetune the Moxin Base model with SOTA\npost-training framework and instruction data to obtain Moxin Instruct model. To\nimprove the reasoning capability, we further finetune our Instruct model with\nchain-of-thought data distilled from DeepSeek R1, and then use Group Relative\nPolicy Optimization (GRPO), an efficient and effective reinforcement learning\nalgorithm following DeepSeek R1, to finetune our model, leading to the Moxin\nReasoning model. Experiments show that our models achieve superior performance\nin various evaluations such as zero-shot evaluation, few-shot evaluation, and\nCoT evaluation."}
{"id": "2504.16761", "pdf": "https://arxiv.org/pdf/2504.16761", "abs": "https://arxiv.org/abs/2504.16761", "authors": ["Lakshita Agarwal", "Bindu Verma"], "title": "Tri-FusionNet: Enhancing Image Description Generation with Transformer-based Fusion Network and Dual Attention Mechanism", "categories": ["cs.CV"], "comment": null, "summary": "Image description generation is essential for accessibility and AI\nunderstanding of visual content. Recent advancements in deep learning have\nsignificantly improved natural language processing and computer vision. In this\nwork, we propose Tri-FusionNet, a novel image description generation model that\nintegrates transformer modules: a Vision Transformer (ViT) encoder module with\ndual-attention mechanism, a Robustly Optimized BERT Approach (RoBERTa) decoder\nmodule, and a Contrastive Language-Image Pre-Training (CLIP) integrating\nmodule. The ViT encoder, enhanced with dual attention, focuses on relevant\nspatial regions and linguistic context, improving image feature extraction. The\nRoBERTa decoder is employed to generate precise textual descriptions. CLIP's\nintegrating module aligns visual and textual data through contrastive learning,\nensuring effective combination of both modalities. This fusion of ViT, RoBERTa,\nand CLIP, along with dual attention, enables the model to produce more\naccurate, contextually rich, and flexible descriptions. The proposed framework\ndemonstrated competitive performance on the Flickr30k and Flickr8k datasets,\nwith BLEU scores ranging from 0.767 to 0.456 and 0.784 to 0.479, CIDEr scores\nof 1.679 and 1.483, METEOR scores of 0.478 and 0.358, and ROUGE-L scores of\n0.567 and 0.789, respectively. On MS-COCO, the framework obtained BLEU scores\nof 0.893 (B-1), 0.821 (B-2), 0.794 (B-3), and 0.725 (B-4). The results\ndemonstrate the effectiveness of Tri-FusionNet in generating high-quality image\ndescriptions."}
{"id": "2504.16192", "pdf": "https://arxiv.org/pdf/2504.16192", "abs": "https://arxiv.org/abs/2504.16192", "authors": ["Lucas Howard", "Aneesh C. Subramanian", "Gregory Thompson", "Benjamin Johnson", "Thomas Auligne"], "title": "Probabilistic Emulation of the Community Radiative Transfer Model Using Machine Learning", "categories": ["physics.ao-ph", "cs.LG", "stat.ML"], "comment": "26 pages, 9 figures, 1 table", "summary": "The continuous improvement in weather forecast skill over the past several\ndecades is largely due to the increasing quantity of available satellite\nobservations and their assimilation into operational forecast systems.\nAssimilating these observations requires observation operators in the form of\nradiative transfer models. Significant efforts have been dedicated to enhancing\nthe computational efficiency of these models. Computational cost remains a\nbottleneck, and a large fraction of available data goes unused for\nassimilation. To address this, we used machine learning to build an efficient\nneural network based probabilistic emulator of the Community Radiative Transfer\nModel (CRTM), applied to the GOES Advanced Baseline Imager. The trained NN\nemulator predicts brightness temperatures output by CRTM and the corresponding\nerror with respect to CRTM. RMSE of the predicted brightness temperature is 0.3\nK averaged across all channels. For clear sky conditions, the RMSE is less than\n0.1 K for 9 out of 10 infrared channels. The error predictions are generally\nreliable across a wide range of conditions. Explainable AI methods demonstrate\nthat the trained emulator reproduces the relevant physics, increasing\nconfidence that the model will perform well when presented with new data."}
{"id": "2504.16576", "pdf": "https://arxiv.org/pdf/2504.16576", "abs": "https://arxiv.org/abs/2504.16576", "authors": ["Xu Guo", "Tong Zhang", "Fuyun Wang", "Xudong Wang", "Xiaoya Zhang", "Xin Liu", "Zhen Cui"], "title": "MMHCL: Multi-Modal Hypergraph Contrastive Learning for Recommendation", "categories": ["cs.IR", "cs.AI"], "comment": "23 pages, 8 figures. This manuscript is currently under major\n  revision for ACM Transactions on Multimedia Computing, Communications, and\n  Applications (ACM TOMM)", "summary": "The burgeoning presence of multimodal content-sharing platforms propels the\ndevelopment of personalized recommender systems. Previous works usually suffer\nfrom data sparsity and cold-start problems, and may fail to adequately explore\nsemantic user-product associations from multimodal data. To address these\nissues, we propose a novel Multi-Modal Hypergraph Contrastive Learning (MMHCL)\nframework for user recommendation. For a comprehensive information exploration\nfrom user-product relations, we construct two hypergraphs, i.e. a user-to-user\n(u2u) hypergraph and an item-to-item (i2i) hypergraph, to mine shared\npreferences among users and intricate multimodal semantic resemblance among\nitems, respectively. This process yields denser second-order semantics that are\nfused with first-order user-item interaction as complementary to alleviate the\ndata sparsity issue. Then, we design a contrastive feature enhancement paradigm\nby applying synergistic contrastive learning. By maximizing/minimizing the\nmutual information between second-order (e.g. shared preference pattern for\nusers) and first-order (information of selected items for users) embeddings of\nthe same/different users and items, the feature distinguishability can be\neffectively enhanced. Compared with using sparse primary user-item interaction\nonly, our MMHCL obtains denser second-order hypergraphs and excavates more\nabundant shared attributes to explore the user-product associations, which to a\ncertain extent alleviates the problems of data sparsity and cold-start.\nExtensive experiments have comprehensively demonstrated the effectiveness of\nour method. Our code is publicly available at: https://github.com/Xu107/MMHCL."}
{"id": "2412.13612", "pdf": "https://arxiv.org/pdf/2412.13612", "abs": "https://arxiv.org/abs/2412.13612", "authors": ["Xuemei Tang", "Xufeng Duan", "Zhenguang G. Cai"], "title": "Large Language Models for Automated Literature Review: An Evaluation of Reference Generation, Abstract Writing, and Review Composition", "categories": ["cs.CL", "cs.AI"], "comment": "12 pages, 5 figures, 5 tables", "summary": "Large language models (LLMs) have emerged as a potential solution to automate\nthe complex processes involved in writing literature reviews, such as\nliterature collection, organization, and summarization. However, it is yet\nunclear how good LLMs are at automating comprehensive and reliable literature\nreviews. This study introduces a framework to automatically evaluate the\nperformance of LLMs in three key tasks of literature writing: reference\ngeneration, literature summary, and literature review composition. We introduce\nmultidimensional evaluation metrics that assess the hallucination rates in\ngenerated references and measure the semantic coverage and factual consistency\nof the literature summaries and compositions against human-written\ncounterparts. The experimental results reveal that even the most advanced\nmodels still generate hallucinated references, despite recent progress.\nMoreover, we observe that the performance of different models varies across\ndisciplines when it comes to writing literature reviews. These findings\nhighlight the need for further research and development to improve the\nreliability of LLMs in automating academic literature reviews."}
{"id": "2504.16788", "pdf": "https://arxiv.org/pdf/2504.16788", "abs": "https://arxiv.org/abs/2504.16788", "authors": ["Lakshita Agarwal", "Bindu Verma"], "title": "Towards Explainable AI: Multi-Modal Transformer for Video-based Image Description Generation", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Understanding and analyzing video actions are essential for producing\ninsightful and contextualized descriptions, especially for video-based\napplications like intelligent monitoring and autonomous systems. The proposed\nwork introduces a novel framework for generating natural language descriptions\nfrom video datasets by combining textual and visual modalities. The suggested\narchitecture makes use of ResNet50 to extract visual features from video frames\nthat are taken from the Microsoft Research Video Description Corpus (MSVD), and\nBerkeley DeepDrive eXplanation (BDD-X) datasets. The extracted visual\ncharacteristics are converted into patch embeddings and then run through an\nencoder-decoder model based on Generative Pre-trained Transformer-2 (GPT-2). In\norder to align textual and visual representations and guarantee high-quality\ndescription production, the system uses multi-head self-attention and\ncross-attention techniques. The model's efficacy is demonstrated by performance\nevaluation using BLEU (1-4), CIDEr, METEOR, and ROUGE-L. The suggested\nframework outperforms traditional methods with BLEU-4 scores of 0.755 (BDD-X)\nand 0.778 (MSVD), CIDEr scores of 1.235 (BDD-X) and 1.315 (MSVD), METEOR scores\nof 0.312 (BDD-X) and 0.329 (MSVD), and ROUGE-L scores of 0.782 (BDD-X) and\n0.795 (MSVD). By producing human-like, contextually relevant descriptions,\nstrengthening interpretability, and improving real-world applications, this\nresearch advances explainable AI."}
{"id": "2504.16227", "pdf": "https://arxiv.org/pdf/2504.16227", "abs": "https://arxiv.org/abs/2504.16227", "authors": ["Amir Ali-Pour", "Sadra Bekrani", "Laya Samizadeh", "Julien Gascon-Samson"], "title": "Towards a Distributed Federated Learning Aggregation Placement using Particle Swarm Intelligence", "categories": ["cs.DC", "cs.LG", "cs.NE", "cs.NI"], "comment": null, "summary": "Federated learning has become a promising distributed learning concept with\nextra insurance on data privacy. Extensive studies on various models of\nFederated learning have been done since the coinage of its term. One of the\nimportant derivatives of federated learning is hierarchical semi-decentralized\nfederated learning, which distributes the load of the aggregation task over\nmultiple nodes and parallelizes the aggregation workload at the breadth of each\nlevel of the hierarchy. Various methods have also been proposed to perform\ninter-cluster and intra-cluster aggregation optimally. Most of the solutions,\nnonetheless, require monitoring the nodes' performance and resource consumption\nat each round, which necessitates frequently exchanging systematic data. To\noptimally perform distributed aggregation in SDFL with minimal reliance on\nsystematic data, we propose Flag-Swap, a Particle Swarm Optimization (PSO)\nmethod that optimizes the aggregation placement according only to the\nprocessing delay. Our simulation results show that PSO-based placement can find\nthe optimal placement relatively fast, even in scenarios with many clients as\ncandidates for aggregation. Our real-world docker-based implementation of\nFlag-Swap over the recently emerged FL framework shows superior performance\ncompared to black-box-based deterministic placement strategies, with about 43%\nminutes faster than random placement, and 32% minutes faster than uniform\nplacement, in terms of total processing time."}
{"id": "2504.16584", "pdf": "https://arxiv.org/pdf/2504.16584", "abs": "https://arxiv.org/abs/2504.16584", "authors": ["Md. Azizul Hakim Bappy", "Hossen A Mustafa", "Prottoy Saha", "Rajinus Salehat"], "title": "Case Study: Fine-tuning Small Language Models for Accurate and Private CWE Detection in Python Code", "categories": ["cs.CR", "cs.AI"], "comment": "11 pages, 2 figures, 3 tables. Dataset available at\n  https://huggingface.co/datasets/floxihunter/synthetic_python_cwe. Model\n  available at https://huggingface.co/floxihunter/codegen-mono-CWEdetect.\n  Keywords: Small Language Models (SLMs), Vulnerability Detection, CWE,\n  Fine-tuning, Python Security, Privacy-Preserving Code Analysis", "summary": "Large Language Models (LLMs) have demonstrated significant capabilities in\nunderstanding and analyzing code for security vulnerabilities, such as Common\nWeakness Enumerations (CWEs). However, their reliance on cloud infrastructure\nand substantial computational requirements pose challenges for analyzing\nsensitive or proprietary codebases due to privacy concerns and inference costs.\nThis work explores the potential of Small Language Models (SLMs) as a viable\nalternative for accurate, on-premise vulnerability detection. We investigated\nwhether a 350-million parameter pre-trained code model (codegen-mono) could be\neffectively fine-tuned to detect the MITRE Top 25 CWEs specifically within\nPython code. To facilitate this, we developed a targeted dataset of 500\nexamples using a semi-supervised approach involving LLM-driven synthetic data\ngeneration coupled with meticulous human review. Initial tests confirmed that\nthe base codegen-mono model completely failed to identify CWEs in our samples.\nHowever, after applying instruction-following fine-tuning, the specialized SLM\nachieved remarkable performance on our test set, yielding approximately 99%\naccuracy, 98.08% precision, 100% recall, and a 99.04% F1-score. These results\nstrongly suggest that fine-tuned SLMs can serve as highly accurate and\nefficient tools for CWE detection, offering a practical and privacy-preserving\nsolution for integrating advanced security analysis directly into development\nworkflows."}
{"id": "2502.04718", "pdf": "https://arxiv.org/pdf/2502.04718", "abs": "https://arxiv.org/abs/2502.04718", "authors": ["Sourabrata Mukherjee", "Atul Kr. Ojha", "John P. McCrae", "Ondrej Dusek"], "title": "Evaluating Text Style Transfer Evaluation: Are There Any Reliable Metrics?", "categories": ["cs.CL"], "comment": "Accepted at NAACL SRW 2025", "summary": "Text style transfer (TST) is the task of transforming a text to reflect a\nparticular style while preserving its original content. Evaluating TST outputs\nis a multidimensional challenge, requiring the assessment of style transfer\naccuracy, content preservation, and naturalness. Using human evaluation is\nideal but costly, as is common in other natural language processing (NLP)\ntasks, however, automatic metrics for TST have not received as much attention\nas metrics for, e.g., machine translation or summarization. In this paper, we\nexamine both set of existing and novel metrics from broader NLP tasks for TST\nevaluation, focusing on two popular subtasks, sentiment transfer and\ndetoxification, in a multilingual context comprising English, Hindi, and\nBengali. By conducting meta-evaluation through correlation with human\njudgments, we demonstrate the effectiveness of these metrics when used\nindividually and in ensembles. Additionally, we investigate the potential of\nlarge language models (LLMs) as tools for TST evaluation. Our findings\nhighlight newly applied advanced NLP metrics and LLM-based evaluations provide\nbetter insights than existing TST metrics. Our oracle ensemble approaches show\neven more potential."}
{"id": "2504.16801", "pdf": "https://arxiv.org/pdf/2504.16801", "abs": "https://arxiv.org/abs/2504.16801", "authors": ["Xiaoxing Hu", "Kaicheng Yang", "Jun Wang", "Haoran Xu", "Ziyong Feng", "Yupei Wang"], "title": "Decoupled Global-Local Alignment for Improving Compositional Understanding", "categories": ["cs.CV"], "comment": null, "summary": "Contrastive Language-Image Pre-training (CLIP) has achieved success on\nmultiple downstream tasks by aligning image and text modalities. However, the\nnature of global contrastive learning limits CLIP's ability to comprehend\ncompositional concepts, such as relations and attributes. Although recent\nstudies employ global hard negative samples to improve compositional\nunderstanding, these methods significantly compromise the model's inherent\ngeneral capabilities by forcibly distancing textual negative samples from\nimages in the embedding space. To overcome this limitation, we introduce a\nDecoupled Global-Local Alignment (DeGLA) framework that improves compositional\nunderstanding while substantially mitigating losses in general capabilities. To\noptimize the retention of the model's inherent capabilities, we incorporate a\nself-distillation mechanism within the global alignment process, aligning the\nlearnable image-text encoder with a frozen teacher model derived from an\nexponential moving average. Under the constraint of self-distillation, it\neffectively mitigates the catastrophic forgetting of pretrained knowledge\nduring fine-tuning. To improve compositional understanding, we first leverage\nthe in-context learning capability of Large Language Models (LLMs) to construct\nabout 2M high-quality negative captions across five types. Subsequently, we\npropose the Image-Grounded Contrast (IGC) loss and Text-Grounded Contrast (TGC)\nloss to enhance vision-language compositionally. Extensive experimental results\ndemonstrate the effectiveness of the DeGLA framework. Compared to previous\nstate-of-the-art methods, DeGLA achieves an average enhancement of 3.5% across\nthe VALSE, SugarCrepe, and ARO benchmarks. Concurrently, it obtains an average\nperformance improvement of 13.0% on zero-shot classification tasks across\neleven datasets. Our code will be released at\nhttps://github.com/xiaoxing2001/DeGLA"}
{"id": "2504.16266", "pdf": "https://arxiv.org/pdf/2504.16266", "abs": "https://arxiv.org/abs/2504.16266", "authors": ["Ye Qiao", "Zhiheng Cheng", "Yifan Zhang", "Yian Wang", "Sitao Huang"], "title": "TeLLMe: An Energy-Efficient Ternary LLM Accelerator for Prefilling and Decoding on Edge FPGAs", "categories": ["cs.AR", "cs.LG"], "comment": null, "summary": "Deploying large language models (LLMs) on edge platforms is challenged by\ntheir high computational and memory demands. Although recent low-bit\nquantization methods (e.g., BitNet, DeepSeek) compress weights to as little as\n1.58 bits with minimal accuracy loss, edge deployment is still constrained by\nlimited on-chip resources, power budgets, and the often-neglected latency of\nthe prefill phase. We present TeLLMe, the first ternary LLM accelerator for\nlow-power FPGAs (e.g., AMD KV260) that fully supports both prefill and\nautoregressive decoding using 1.58-bit weights and 8-bit activations. Our\ncontributions include: (1) a table-lookup matrix engine for ternary matmul that\nmerges grouped activations with online precomputation to minimize resource use;\n(2) a fused, bandwidth-efficient attention module featuring a reversed\nreordering scheme to accelerate prefill; and (3) a tightly integrated\nnormalization and quantization--dequantization unit optimized for ultra-low-bit\ninference. Under a 7W power budget, TeLLMe delivers up to 9 tokens/s throughput\nover 1,024-token contexts and prefill latencies of 0.55--1.15 s for 64--128\ntoken prompts, marking a significant energy-efficiency advance and establishing\na new edge FPGA benchmark for generative AI."}
{"id": "2504.16651", "pdf": "https://arxiv.org/pdf/2504.16651", "abs": "https://arxiv.org/abs/2504.16651", "authors": ["William Corrias", "Fabio De Gaspari", "Dorjan Hitaj", "Luigi V. Mancini"], "title": "MAYA: Addressing Inconsistencies in Generative Password Guessing through a Unified Benchmark", "categories": ["cs.CR", "cs.AI", "cs.LG"], "comment": null, "summary": "The rapid evolution of generative models has led to their integration across\nvarious fields, including password guessing, aiming to generate passwords that\nresemble human-created ones in complexity, structure, and patterns. Despite\ngenerative model's promise, inconsistencies in prior research and a lack of\nrigorous evaluation have hindered a comprehensive understanding of their true\npotential. In this paper, we introduce MAYA, a unified, customizable,\nplug-and-play password benchmarking framework. MAYA provides a standardized\napproach for evaluating generative password-guessing models through a rigorous\nset of advanced testing scenarios and a collection of eight real-life password\ndatasets. Using MAYA, we comprehensively evaluate six state-of-the-art\napproaches, which have been re-implemented and adapted to ensure\nstandardization, for a total of over 15,000 hours of computation. Our findings\nindicate that these models effectively capture different aspects of human\npassword distribution and exhibit strong generalization capabilities. However,\ntheir effectiveness varies significantly with long and complex passwords.\nThrough our evaluation, sequential models consistently outperform other\ngenerative architectures and traditional password-guessing tools, demonstrating\nunique capabilities in generating accurate and complex guesses. Moreover,\nmodels learn and generate different password distributions, enabling a\nmulti-model attack that outperforms the best individual model. By releasing\nMAYA, we aim to foster further research, providing the community with a new\ntool to consistently and reliably benchmark password-generation techniques. Our\nframework is publicly available at\nhttps://github.com/williamcorrias/MAYA-Password-Benchmarking"}
{"id": "2502.13108", "pdf": "https://arxiv.org/pdf/2502.13108", "abs": "https://arxiv.org/abs/2502.13108", "authors": ["Priyaranjan Pattnayak", "Hitesh Laxmichand Patel", "Amit Agarwal", "Bhargava Kumar", "Srikant Panda", "Tejaswini Kumar"], "title": "Clinical QA 2.0: Multi-Task Learning for Answer Extraction and Categorization", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": null, "summary": "Clinical Question Answering (CQA) plays a crucial role in medical\ndecision-making, enabling physicians to extract relevant information from\nElectronic Medical Records (EMRs). While transformer-based models such as BERT,\nBioBERT, and ClinicalBERT have demonstrated state-of-the-art performance in\nCQA, existing models lack the ability to categorize extracted answers, which is\ncritical for structured retrieval, content filtering, and medical decision\nsupport.\n  To address this limitation, we introduce a Multi-Task Learning (MTL)\nframework that jointly trains CQA models for both answer extraction and medical\ncategorization. In addition to predicting answer spans, our model classifies\nresponses into five standardized medical categories: Diagnosis, Medication,\nSymptoms, Procedure, and Lab Reports. This categorization enables more\nstructured and interpretable outputs, making clinical QA models more useful in\nreal-world healthcare settings.\n  We evaluate our approach on emrQA, a large-scale dataset for medical question\nanswering. Results show that MTL improves F1-score by 2.2% compared to standard\nfine-tuning, while achieving 90.7% accuracy in answer categorization. These\nfindings suggest that MTL not only enhances CQA performance but also introduces\nan effective mechanism for categorization and structured medical information\nretrieval."}
{"id": "2504.16840", "pdf": "https://arxiv.org/pdf/2504.16840", "abs": "https://arxiv.org/abs/2504.16840", "authors": ["Joe Hrzich", "Michael A. Beck", "Christopher P. Bidinosti", "Christopher J. Henry", "Kalhari Manawasinghe", "Karen Tanino"], "title": "A Low-Cost Photogrammetry System for 3D Plant Modeling and Phenotyping", "categories": ["cs.CV"], "comment": null, "summary": "We present an open-source, low-cost photogrammetry system for 3D plant\nmodeling and phenotyping. The system uses a structure-from-motion approach to\nreconstruct 3D representations of the plants via point clouds. Using wheat as\nan example, we demonstrate how various phenotypic traits can be computed easily\nfrom the point clouds. These include standard measurements such as plant height\nand radius, as well as features that would be more cumbersome to measure by\nhand, such as leaf angles and convex hull. We further demonstrate the utility\nof the system through the investigation of specific metrics that may yield\nobjective classifications of erectophile versus planophile wheat canopy\narchitectures."}
{"id": "2504.16269", "pdf": "https://arxiv.org/pdf/2504.16269", "abs": "https://arxiv.org/abs/2504.16269", "authors": ["Ye Qiao", "Zhiheng Cheng", "Yian Wang", "Yifan Zhang", "Yunzhe Deng", "Sitao Huang"], "title": "COBRA: Algorithm-Architecture Co-optimized Binary Transformer Accelerator for Edge Inference", "categories": ["cs.AR", "cs.LG"], "comment": null, "summary": "Transformer-based models have demonstrated superior performance in various\nfields, including natural language processing and computer vision. However,\ntheir enormous model size and high demands in computation, memory, and\ncommunication limit their deployment to edge platforms for local, secure\ninference. Binary transformers offer a compact, low-complexity solution for\nedge deployment with reduced bandwidth needs and acceptable accuracy. However,\nexisting binary transformers perform inefficiently on current hardware due to\nthe lack of binary specific optimizations. To address this, we introduce COBRA,\nan algorithm-architecture co-optimized binary Transformer accelerator for edge\ncomputing. COBRA features a real 1-bit binary multiplication unit, enabling\nmatrix operations with -1, 0, and +1 values, surpassing ternary methods. With\nfurther hardware-friendly optimizations in the attention block, COBRA achieves\nup to 3,894.7 GOPS throughput and 448.7 GOPS/Watt energy efficiency on edge\nFPGAs, delivering a 311x energy efficiency improvement over GPUs and a 3.5x\nthroughput improvement over the state-of-the-art binary accelerator, with only\nnegligible inference accuracy degradation."}
{"id": "2504.16680", "pdf": "https://arxiv.org/pdf/2504.16680", "abs": "https://arxiv.org/abs/2504.16680", "authors": ["Chenhao Li", "Andreas Krause", "Marco Hutter"], "title": "Offline Robotic World Model: Learning Robotic Policies without a Physics Simulator", "categories": ["cs.RO", "cs.AI", "cs.LG"], "comment": null, "summary": "Reinforcement Learning (RL) has demonstrated impressive capabilities in\nrobotic control but remains challenging due to high sample complexity, safety\nconcerns, and the sim-to-real gap. While offline RL eliminates the need for\nrisky real-world exploration by learning from pre-collected data, it suffers\nfrom distributional shift, limiting policy generalization. Model-Based RL\n(MBRL) addresses this by leveraging predictive models for synthetic rollouts,\nyet existing approaches often lack robust uncertainty estimation, leading to\ncompounding errors in offline settings. We introduce Offline Robotic World\nModel (RWM-O), a model-based approach that explicitly estimates epistemic\nuncertainty to improve policy learning without reliance on a physics simulator.\nBy integrating these uncertainty estimates into policy optimization, our\napproach penalizes unreliable transitions, reducing overfitting to model errors\nand enhancing stability. Experimental results show that RWM-O improves\ngeneralization and safety, enabling policy learning purely from real-world data\nand advancing scalable, data-efficient RL for robotics."}
{"id": "2503.01840", "pdf": "https://arxiv.org/pdf/2503.01840", "abs": "https://arxiv.org/abs/2503.01840", "authors": ["Yuhui Li", "Fangyun Wei", "Chao Zhang", "Hongyang Zhang"], "title": "EAGLE-3: Scaling up Inference Acceleration of Large Language Models via Training-Time Test", "categories": ["cs.CL"], "comment": null, "summary": "The sequential nature of modern LLMs makes them expensive and slow, and\nspeculative sampling has proven to be an effective solution to this problem.\nMethods like EAGLE perform autoregression at the feature level, reusing\ntop-layer features from the target model to achieve better results than vanilla\nspeculative sampling. A growing trend in the LLM community is scaling up\ntraining data to improve model intelligence without increasing inference costs.\nHowever, we observe that scaling up data provides limited improvements for\nEAGLE. We identify that this limitation arises from EAGLE's feature prediction\nconstraints. In this paper, we introduce EAGLE-3, which abandons feature\nprediction in favor of direct token prediction and replaces reliance on\ntop-layer features with multi-layer feature fusion via a technique named\ntraining-time test. These improvements significantly enhance performance and\nenable the draft model to fully benefit from scaling up training data. Our\nexperiments include both chat models and reasoning models, evaluated on five\ntasks. The results show that EAGLE-3 achieves a speedup ratio up to 6.5x, with\nabout 1.4x improvement over EAGLE-2. In the SGLang framework, EAGLE-3 achieves\na 1.38x throughput improvement at a batch size of 64. The code is available at\nhttps://github.com/SafeAILab/EAGLE."}
{"id": "2504.16851", "pdf": "https://arxiv.org/pdf/2504.16851", "abs": "https://arxiv.org/abs/2504.16851", "authors": ["Ruben Gonzalez Avilés", "Linus Scheibenreif", "Nassim Ait Ali Braham", "Benedikt Blumenstiel", "Thomas Brunschwiler", "Ranjini Guruprasad", "Damian Borth", "Conrad Albrecht", "Paolo Fraccaro", "Devyani Lambhate", "Johannes Jakubik"], "title": "Hyperspectral Vision Transformers for Greenhouse Gas Estimations from Space", "categories": ["cs.CV"], "comment": null, "summary": "Hyperspectral imaging provides detailed spectral information and holds\nsignificant potential for monitoring of greenhouse gases (GHGs). However, its\napplication is constrained by limited spatial coverage and infrequent revisit\ntimes. In contrast, multispectral imaging offers broader spatial and temporal\ncoverage but often lacks the spectral detail that can enhance GHG detection. To\naddress these challenges, this study proposes a spectral transformer model that\nsynthesizes hyperspectral data from multispectral inputs. The model is\npre-trained via a band-wise masked autoencoder and subsequently fine-tuned on\nspatio-temporally aligned multispectral-hyperspectral image pairs. The\nresulting synthetic hyperspectral data retain the spatial and temporal benefits\nof multispectral imagery and improve GHG prediction accuracy relative to using\nmultispectral data alone. This approach effectively bridges the trade-off\nbetween spectral resolution and coverage, highlighting its potential to advance\natmospheric monitoring by combining the strengths of hyperspectral and\nmultispectral systems with self-supervised deep learning."}
{"id": "2504.16270", "pdf": "https://arxiv.org/pdf/2504.16270", "abs": "https://arxiv.org/abs/2504.16270", "authors": ["Naren Sarayu Manoj"], "title": "A Geometric Approach to Problems in Optimization and Data Science", "categories": ["math.OC", "cs.LG", "stat.ML"], "comment": "PhD dissertation", "summary": "We give new results for problems in computational and statistical machine\nlearning using tools from high-dimensional geometry and probability.\n  We break up our treatment into two parts. In Part I, we focus on\ncomputational considerations in optimization. Specifically, we give new\nalgorithms for approximating convex polytopes in a stream, sparsification and\nrobust least squares regression, and dueling optimization.\n  In Part II, we give new statistical guarantees for data science problems. In\nparticular, we formulate a new model in which we analyze statistical properties\nof backdoor data poisoning attacks, and we study the robustness of graph\nclustering algorithms to ``helpful'' misspecification."}
{"id": "2504.16738", "pdf": "https://arxiv.org/pdf/2504.16738", "abs": "https://arxiv.org/abs/2504.16738", "authors": ["Itamar Mishani", "Yorai Shaoul", "Maxim Likhachev"], "title": "MOSAIC: A Skill-Centric Algorithmic Framework for Long-Horizon Manipulation Planning", "categories": ["cs.RO", "cs.AI"], "comment": "Under review. Project page: https://skill-mosaic.github.io", "summary": "Planning long-horizon motions using a set of predefined skills is a key\nchallenge in robotics and AI. Addressing this challenge requires methods that\nsystematically explore skill combinations to uncover task-solving sequences,\nharness generic, easy-to-learn skills (e.g., pushing, grasping) to generalize\nacross unseen tasks, and bypass reliance on symbolic world representations that\ndemand extensive domain and task-specific knowledge. Despite significant\nprogress, these elements remain largely disjoint in existing approaches,\nleaving a critical gap in achieving robust, scalable solutions for complex,\nlong-horizon problems. In this work, we present MOSAIC, a skill-centric\nframework that unifies these elements by using the skills themselves to guide\nthe planning process. MOSAIC uses two families of skills: Generators compute\nexecutable trajectories and world configurations, and Connectors link these\nindependently generated skill trajectories by solving boundary value problems,\nenabling progress toward completing the overall task. By breaking away from the\nconventional paradigm of incrementally discovering skills from predefined start\nor goal states--a limitation that significantly restricts exploration--MOSAIC\nfocuses planning efforts on regions where skills are inherently effective. We\ndemonstrate the efficacy of MOSAIC in both simulated and real-world robotic\nmanipulation tasks, showcasing its ability to solve complex long-horizon\nplanning problems using a diverse set of skills incorporating generative\ndiffusion models, motion planning algorithms, and manipulation-specific models.\nVisit https://skill-mosaic.github.io for demonstrations and examples."}
{"id": "2503.14477", "pdf": "https://arxiv.org/pdf/2503.14477", "abs": "https://arxiv.org/abs/2503.14477", "authors": ["Ziwei Ji", "Lei Yu", "Yeskendir Koishekenov", "Yejin Bang", "Anthony Hartshorn", "Alan Schelten", "Cheng Zhang", "Pascale Fung", "Nicola Cancedda"], "title": "Calibrating Verbal Uncertainty as a Linear Feature to Reduce Hallucinations", "categories": ["cs.CL"], "comment": null, "summary": "LLMs often adopt an assertive language style also when making false claims.\nSuch ``overconfident hallucinations'' mislead users and erode trust. Achieving\nthe ability to express in language the actual degree of uncertainty around a\nclaim is therefore of great importance. We find that ``verbal uncertainty'' is\ngoverned by a single linear feature in the representation space of LLMs, and\nshow that this has only moderate correlation with the actual ``semantic\nuncertainty'' of the model. We apply this insight and show that (1) the\nmismatch between semantic and verbal uncertainty is a better predictor of\nhallucinations than semantic uncertainty alone and (2) we can intervene on\nverbal uncertainty at inference time and reduce confident hallucinations on\nshort-form answers, achieving an average relative reduction of ~30%."}
{"id": "2504.16870", "pdf": "https://arxiv.org/pdf/2504.16870", "abs": "https://arxiv.org/abs/2504.16870", "authors": ["Chenxi Duan"], "title": "High-Quality Cloud-Free Optical Image Synthesis Using Multi-Temporal SAR and Contaminated Optical Data", "categories": ["cs.CV"], "comment": null, "summary": "Addressing gaps caused by cloud cover and the long revisit cycle of\nsatellites is vital for providing essential data to support remote sensing\napplications. This paper tackles the challenges of missing optical data\nsynthesis, particularly in complex scenarios with cloud cover. We propose\nCRSynthNet, a novel image synthesis network that incorporates innovative\ndesigned modules such as the DownUp Block and Fusion Attention to enhance\naccuracy. Experimental results validate the effectiveness of CRSynthNet,\ndemonstrating substantial improvements in restoring structural details,\npreserving spectral consist, and achieving superior visual effects that far\nexceed those produced by comparison methods. It achieves quantitative\nimprovements across multiple metrics: a peak signal-to-noise ratio (PSNR) of\n26.978, a structural similarity index measure (SSIM) of 0.648, and a root mean\nsquare error (RMSE) of 0.050. Furthermore, this study creates the TCSEN12\ndataset, a valuable resource specifically designed to address cloud cover\nchallenges in missing optical data synthesis study. The dataset uniquely\nincludes cloud-covered images and leverages earlier image to predict later\nimage, offering a realistic representation of real-world scenarios. This study\noffer practical method and valuable resources for optical satellite image\nsynthesis task."}
{"id": "2504.16320", "pdf": "https://arxiv.org/pdf/2504.16320", "abs": "https://arxiv.org/abs/2504.16320", "authors": ["Yaofeng Cheng", "Fusheng Zha", "Wei Guo", "Pengfei Wang", "Chao Zeng", "Lining Sun", "Chenguang Yang"], "title": "PCF-Grasp: Converting Point Completion to Geometry Feature to Enhance 6-DoF Grasp", "categories": ["cs.RO", "cs.LG"], "comment": null, "summary": "The 6-Degree of Freedom (DoF) grasp method based on point clouds has shown\nsignificant potential in enabling robots to grasp target objects. However, most\nexisting methods are based on the point clouds (2.5D points) generated from\nsingle-view depth images. These point clouds only have one surface side of the\nobject providing incomplete geometry information, which mislead the grasping\nalgorithm to judge the shape of the target object, resulting in low grasping\naccuracy. Humans can accurately grasp objects from a single view by leveraging\ntheir geometry experience to estimate object shapes. Inspired by humans, we\npropose a novel 6-DoF grasping framework that converts the point completion\nresults as object shape features to train the 6-DoF grasp network. Here, point\ncompletion can generate approximate complete points from the 2.5D points\nsimilar to the human geometry experience, and converting it as shape features\nis the way to utilize it to improve grasp efficiency. Furthermore, due to the\ngap between the network generation and actual execution, we integrate a score\nfilter into our framework to select more executable grasp proposals for the\nreal robot. This enables our method to maintain a high grasp quality in any\ncamera viewpoint. Extensive experiments demonstrate that utilizing complete\npoint features enables the generation of significantly more accurate grasp\nproposals and the inclusion of a score filter greatly enhances the credibility\nof real-world robot grasping. Our method achieves a 17.8\\% success rate higher\nthan the state-of-the-art method in real-world experiments."}
{"id": "2503.16419", "pdf": "https://arxiv.org/pdf/2503.16419", "abs": "https://arxiv.org/abs/2503.16419", "authors": ["Yang Sui", "Yu-Neng Chuang", "Guanchu Wang", "Jiamu Zhang", "Tianyi Zhang", "Jiayi Yuan", "Hongyi Liu", "Andrew Wen", "Shaochen Zhong", "Hanjie Chen", "Xia Hu"], "title": "Stop Overthinking: A Survey on Efficient Reasoning for Large Language Models", "categories": ["cs.CL"], "comment": "Project Website:\n  https://github.com/Eclipsess/Awesome-Efficient-Reasoning-LLMs", "summary": "Large Language Models (LLMs) have demonstrated remarkable capabilities in\ncomplex tasks. Recent advancements in Large Reasoning Models (LRMs), such as\nOpenAI o1 and DeepSeek-R1, have further improved performance in System-2\nreasoning domains like mathematics and programming by harnessing supervised\nfine-tuning (SFT) and reinforcement learning (RL) techniques to enhance the\nChain-of-Thought (CoT) reasoning. However, while longer CoT reasoning sequences\nimprove performance, they also introduce significant computational overhead due\nto verbose and redundant outputs, known as the \"overthinking phenomenon\". In\nthis paper, we provide the first structured survey to systematically\ninvestigate and explore the current progress toward achieving efficient\nreasoning in LLMs. Overall, relying on the inherent mechanism of LLMs, we\ncategorize existing works into several key directions: (1) model-based\nefficient reasoning, which considers optimizing full-length reasoning models\ninto more concise reasoning models or directly training efficient reasoning\nmodels; (2) reasoning output-based efficient reasoning, which aims to\ndynamically reduce reasoning steps and length during inference; (3) input\nprompts-based efficient reasoning, which seeks to enhance reasoning efficiency\nbased on input prompt properties such as difficulty or length control.\nAdditionally, we introduce the use of efficient data for training reasoning\nmodels, explore the reasoning capabilities of small language models, and\ndiscuss evaluation methods and benchmarking."}
{"id": "2504.16907", "pdf": "https://arxiv.org/pdf/2504.16907", "abs": "https://arxiv.org/abs/2504.16907", "authors": ["Ruotong Wang", "Mingli Zhu", "Jiarong Ou", "Rui Chen", "Xin Tao", "Pengfei Wan", "Baoyuan Wu"], "title": "BadVideo: Stealthy Backdoor Attack against Text-to-Video Generation", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Text-to-video (T2V) generative models have rapidly advanced and found\nwidespread applications across fields like entertainment, education, and\nmarketing. However, the adversarial vulnerabilities of these models remain\nrarely explored. We observe that in T2V generation tasks, the generated videos\noften contain substantial redundant information not explicitly specified in the\ntext prompts, such as environmental elements, secondary objects, and additional\ndetails, providing opportunities for malicious attackers to embed hidden\nharmful content. Exploiting this inherent redundancy, we introduce BadVideo,\nthe first backdoor attack framework tailored for T2V generation. Our attack\nfocuses on designing target adversarial outputs through two key strategies: (1)\nSpatio-Temporal Composition, which combines different spatiotemporal features\nto encode malicious information; (2) Dynamic Element Transformation, which\nintroduces transformations in redundant elements over time to convey malicious\ninformation. Based on these strategies, the attacker's malicious target\nseamlessly integrates with the user's textual instructions, providing high\nstealthiness. Moreover, by exploiting the temporal dimension of videos, our\nattack successfully evades traditional content moderation systems that\nprimarily analyze spatial information within individual frames. Extensive\nexperiments demonstrate that BadVideo achieves high attack success rates while\npreserving original semantics and maintaining excellent performance on clean\ninputs. Overall, our work reveals the adversarial vulnerability of T2V models,\ncalling attention to potential risks and misuse. Our project page is at\nhttps://wrt2000.github.io/BadVideo2025/."}
{"id": "2504.16331", "pdf": "https://arxiv.org/pdf/2504.16331", "abs": "https://arxiv.org/abs/2504.16331", "authors": ["Jie JW Wu", "Manav Chaudhary", "Davit Abrahamyan", "Arhaan Khaku", "Anjiang Wei", "Fatemeh H. Fard"], "title": "ClarifyCoder: Clarification-Aware Fine-Tuning for Programmatic Problem Solving", "categories": ["cs.SE", "cs.LG"], "comment": "12 pages, 5 figures, 6 tables", "summary": "Large language models (LLMs) have demonstrated remarkable capabilities in\ncode generation tasks. However, a significant gap remains between their current\nperformance and that of expert software engineers. A key differentiator is that\nhuman engineers actively seek clarification when faced with ambiguous\nrequirements, while LLMs typically generate code regardless of uncertainties in\nthe problem description. We present ClarifyCoder, a novel framework with\nsynthetic data generation and instruction-tuning that enables LLMs to identify\nambiguities and request clarification before proceeding with code generation.\nWhile recent work has focused on LLM-based agents for iterative code\ngeneration, we argue that the fundamental ability to recognize and query\nambiguous requirements should be intrinsic to the models themselves. Our\napproach consists of two main components: (1) a data synthesis technique that\naugments existing programming datasets with scenarios requiring clarification\nto generate clarification-aware training data, and (2) a fine-tuning strategy\nthat teaches models to prioritize seeking clarification over immediate code\ngeneration when faced with incomplete or ambiguous requirements. We further\nprovide an empirical analysis of integrating ClarifyCoder with standard\nfine-tuning for a joint optimization of both clarify-awareness and coding\nability. Experimental results demonstrate that ClarifyCoder significantly\nimproves the communication capabilities of Code LLMs through meaningful\nclarification dialogues while maintaining code generation capabilities."}
{"id": "2504.16791", "pdf": "https://arxiv.org/pdf/2504.16791", "abs": "https://arxiv.org/abs/2504.16791", "authors": ["S. A. K. Leeney", "H. T. J. Bevins", "E. de Lera Acedo", "W. J. Handley", "C. Kirkham", "R. S. Patel", "J. Zhu", "D. Molnar", "J. Cumner", "D. Anstey", "K. Artuc", "G. Bernardi", "M. Bucher", "S. Carey", "J. Cavillot", "R. Chiello", "W. Croukamp", "D. I. L. de Villiers", "J. A. Ely", "A. Fialkov", "T. Gessey-Jones", "G. Kulkarni", "A. Magro", "P. D. Meerburg", "S. Mittal", "J. H. N. Pattison", "S. Pegwal", "C. M. Pieterse", "J. R. Pritchard", "E. Puchwein", "N. Razavi-Ghods", "I. L. V. Roque", "A. Saxena", "K. H. Scheutwinkel", "P. Scott", "E. Shen", "P. H. Sims", "M. Spinelli"], "title": "Radiometer Calibration using Machine Learning", "categories": ["astro-ph.IM", "astro-ph.CO", "cs.AI"], "comment": "Under peer review for publication in Nature Scientific Reports as\n  part of the Radio Astronomy collection", "summary": "Radiometers are crucial instruments in radio astronomy, forming the primary\ncomponent of nearly all radio telescopes. They measure the intensity of\nelectromagnetic radiation, converting this radiation into electrical signals. A\nradiometer's primary components are an antenna and a Low Noise Amplifier (LNA),\nwhich is the core of the ``receiver'' chain. Instrumental effects introduced by\nthe receiver are typically corrected or removed during calibration. However,\nimpedance mismatches between the antenna and receiver can introduce unwanted\nsignal reflections and distortions. Traditional calibration methods, such as\nDicke switching, alternate the receiver input between the antenna and a\nwell-characterised reference source to mitigate errors by comparison. Recent\nadvances in Machine Learning (ML) offer promising alternatives. Neural\nnetworks, which are trained using known signal sources, provide a powerful\nmeans to model and calibrate complex systems where traditional analytical\napproaches struggle. These methods are especially relevant for detecting the\nfaint sky-averaged 21-cm signal from atomic hydrogen at high redshifts. This is\none of the main challenges in observational Cosmology today. Here, for the\nfirst time, we introduce and test a machine learning-based calibration\nframework capable of achieving the precision required for radiometric\nexperiments aiming to detect the 21-cm line."}
{"id": "2503.20533", "pdf": "https://arxiv.org/pdf/2503.20533", "abs": "https://arxiv.org/abs/2503.20533", "authors": ["Yijiong Yu"], "title": "Accelerate Parallelizable Reasoning via Parallel Decoding within One Sequence", "categories": ["cs.CL"], "comment": "Our code is available in\n  https://github.com/yuyijiong/parallel-decoding-in-one-sequence", "summary": "Recent advances in reasoning models have demonstrated significant\nimprovements in accuracy, particularly for complex tasks such as mathematical\nreasoning, by employing detailed and comprehensive reasoning processes.\nHowever, generating these lengthy reasoning sequences is computationally\nexpensive and time-consuming. To address this inefficiency, we leverage the\ninherent parallelizability of certain tasks to accelerate the reasoning\nprocess. Specifically, when multiple parallel reasoning branches exist, we\ndecode multiple tokens per step using a specialized attention mask, processing\nthem within a single sequence, avoiding additional memory usage. Experimental\nresults show that our method achieves over 100% speedup in decoding time while\nmaintaining the answer quality."}
{"id": "2504.16915", "pdf": "https://arxiv.org/pdf/2504.16915", "abs": "https://arxiv.org/abs/2504.16915", "authors": ["Chong Mou", "Yanze Wu", "Wenxu Wu", "Zinan Guo", "Pengze Zhang", "Yufeng Cheng", "Yiming Luo", "Fei Ding", "Shiwen Zhang", "Xinghui Li", "Mengtian Li", "Songtao Zhao", "Jian Zhang", "Qian He", "Xinglong Wu"], "title": "DreamO: A Unified Framework for Image Customization", "categories": ["cs.CV"], "comment": null, "summary": "Recently, extensive research on image customization (e.g., identity, subject,\nstyle, background, etc.) demonstrates strong customization capabilities in\nlarge-scale generative models. However, most approaches are designed for\nspecific tasks, restricting their generalizability to combine different types\nof condition. Developing a unified framework for image customization remains an\nopen challenge. In this paper, we present DreamO, an image customization\nframework designed to support a wide range of tasks while facilitating seamless\nintegration of multiple conditions. Specifically, DreamO utilizes a diffusion\ntransformer (DiT) framework to uniformly process input of different types.\nDuring training, we construct a large-scale training dataset that includes\nvarious customization tasks, and we introduce a feature routing constraint to\nfacilitate the precise querying of relevant information from reference images.\nAdditionally, we design a placeholder strategy that associates specific\nplaceholders with conditions at particular positions, enabling control over the\nplacement of conditions in the generated results. Moreover, we employ a\nprogressive training strategy consisting of three stages: an initial stage\nfocused on simple tasks with limited data to establish baseline consistency, a\nfull-scale training stage to comprehensively enhance the customization\ncapabilities, and a final quality alignment stage to correct quality biases\nintroduced by low-quality data. Extensive experiments demonstrate that the\nproposed DreamO can effectively perform various image customization tasks with\nhigh quality and flexibly integrate different types of control conditions."}
{"id": "2504.16334", "pdf": "https://arxiv.org/pdf/2504.16334", "abs": "https://arxiv.org/abs/2504.16334", "authors": ["Kamran Majid"], "title": "Deep Neural Network Emulation of the Quantum-Classical Transition via Learned Wigner Function Dynamics", "categories": ["quant-ph", "cs.LG"], "comment": null, "summary": "The emergence of classical behavior from quantum mechanics as Planck's\nconstant $\\hbar$ approaches zero remains a fundamental challenge in physics\n[1-3]. This paper introduces a novel approach employing deep neural networks to\ndirectly learn the dynamical mapping from initial quantum state parameters (for\nGaussian wave packets of the one-dimensional harmonic oscillator) and $\\hbar$\nto the parameters of the time-evolved Wigner function in phase space [4-6]. A\ncomprehensive dataset of analytically derived time-evolved Wigner functions was\ngenerated, and a deep feedforward neural network with an enhanced architecture\nwas successfully trained for this prediction task, achieving a final training\nloss of ~ 0.0390. The network demonstrates a significant and previously\nunrealized ability to accurately capture the underlying mapping of the Wigner\nfunction dynamics. This allows for a direct emulation of the quantum-classical\ntransition by predicting the evolution of phase-space distributions as $\\hbar$\nis systematically varied. The implications of these findings for providing a\nnew computational lens on the emergence of classicality are discussed,\nhighlighting the potential of this direct phase-space learning approach for\nstudying fundamental aspects of quantum mechanics. This work presents a\nsignificant advancement beyond previous efforts that focused on learning\nobservable mappings [7], offering a direct route via the phase-space\nrepresentation."}
{"id": "2504.16837", "pdf": "https://arxiv.org/pdf/2504.16837", "abs": "https://arxiv.org/abs/2504.16837", "authors": ["Daniele Carnevale", "Gianlorenzo D'Angelo", "Martin Olsen"], "title": "Approximating Optimal Labelings for Temporal Connectivity", "categories": ["cs.DS", "cs.AI"], "comment": null, "summary": "In a temporal graph the edge set dynamically changes over time according to a\nset of time-labels associated with each edge that indicates at which time-steps\nthe edge is available. Two vertices are connected if there is a path connecting\nthem in which the edges are traversed in increasing order of their labels. We\nstudy the problem of scheduling the availability time of the edges of a\ntemporal graph in such a way that all pairs of vertices are connected within a\ngiven maximum allowed time $a$ and the overall number of labels is minimized.\n  The problem, known as \\emph{Minimum Aged Labeling} (MAL), has several\napplications in logistics, distribution scheduling, and information spreading\nin social networks, where carefully choosing the time-labels can significantly\nreduce infrastructure costs, fuel consumption, or greenhouse gases.\n  The problem MAL has previously been proved to be NP-complete on undirected\ngraphs and \\APX-hard on directed graphs. In this paper, we extend our knowledge\non the complexity and approximability of MAL in several directions. We first\nshow that the problem cannot be approximated within a factor better than\n$O(\\log n)$ when $a\\geq 2$, unless $\\text{P} = \\text{NP}$, and a factor better\nthan $2^{\\log ^{1-\\epsilon} n}$ when $a\\geq 3$, unless $\\text{NP}\\subseteq\n\\text{DTIME}(2^{\\text{polylog}(n)})$, where $n$ is the number of vertices in\nthe graph. Then we give a set of approximation algorithms that, under some\nconditions, almost match these lower bounds. In particular, we show that the\napproximation depends on a relation between $a$ and the diameter of the input\ngraph.\n  We further establish a connection with a foundational optimization problem on\nstatic graphs called \\emph{Diameter Constrained Spanning Subgraph} (DCSS) and\nshow that our hardness results also apply to DCSS."}
{"id": "2504.02894", "pdf": "https://arxiv.org/pdf/2504.02894", "abs": "https://arxiv.org/abs/2504.02894", "authors": ["Ahsan Bilal", "Beiyu Lin"], "title": "OnRL-RAG: Real-Time Personalized Mental Health Dialogue System", "categories": ["cs.CL", "cs.AI"], "comment": "It needs more revisions. I am currently working on it with my\n  co-author", "summary": "Large language models (LLMs) have been widely used for various tasks and\napplications. However, LLMs and fine-tuning are limited to the pre-trained\ndata. For example, ChatGPT's world knowledge until 2021 can be outdated or\ninaccurate. To enhance the capabilities of LLMs, Retrieval-Augmented Generation\n(RAG), is proposed to augment LLMs with additional, new, latest details and\ninformation to LLMs. While RAG offers the correct information, it may not best\npresent it, especially to different population groups with personalizations.\nReinforcement Learning from Human Feedback (RLHF) adapts to user needs by\naligning model responses with human preference through feedback loops. In\nreal-life applications, such as mental health problems, a dynamic and\nfeedback-based model would continuously adapt to new information and offer\npersonalized assistance due to complex factors fluctuating in a daily\nenvironment. Thus, we propose an Online Reinforcement Learning-based\nRetrieval-Augmented Generation (OnRL-RAG) system to detect and personalize the\nresponding systems to mental health problems, such as stress, anxiety, and\ndepression. We use an open-source dataset collected from 2028 College Students\nwith 28 survey questions for each student to demonstrate the performance of our\nproposed system with the existing systems. Our system achieves superior\nperformance compared to standard RAG and simple LLM via GPT-4o, GPT-4o-mini,\nGemini-1.5, and GPT-3.5. This work would open up the possibilities of real-life\napplications of LLMs for personalized services in the everyday environment. The\nresults will also help researchers in the fields of sociology, psychology, and\nneuroscience to align their theories more closely with the actual human daily\nenvironment."}
{"id": "2504.16922", "pdf": "https://arxiv.org/pdf/2504.16922", "abs": "https://arxiv.org/abs/2504.16922", "authors": ["Ali Hassani", "Fengzhe Zhou", "Aditya Kane", "Jiannan Huang", "Chieh-Yun Chen", "Min Shi", "Steven Walton", "Markus Hoehnerbach", "Vijay Thakkar", "Michael Isaev", "Qinsheng Zhang", "Bing Xu", "Haicheng Wu", "Wen-mei Hwu", "Ming-Yu Liu", "Humphrey Shi"], "title": "Generalized Neighborhood Attention: Multi-dimensional Sparse Attention at the Speed of Light", "categories": ["cs.CV", "cs.AI", "cs.LG"], "comment": "https://github.com/SHI-Labs/NATTEN/", "summary": "Many sparse attention mechanisms such as Neighborhood Attention have\ntypically failed to consistently deliver speedup over the self attention\nbaseline. This is largely due to the level of complexity in attention\ninfrastructure, and the rapid evolution of AI hardware architecture. At the\nsame time, many state-of-the-art foundational models, particularly in computer\nvision, are heavily bound by attention, and need reliable sparsity to escape\nthe O(n^2) complexity. In this paper, we study a class of promising sparse\nattention mechanisms that focus on locality, and aim to develop a better\nanalytical model of their performance improvements. We first introduce\nGeneralized Neighborhood Attention (GNA), which can describe sliding window,\nstrided sliding window, and blocked attention. We then consider possible design\nchoices in implementing these approaches, and create a simulator that can\nprovide much more realistic speedup upper bounds for any given setting.\nFinally, we implement GNA on top of a state-of-the-art fused multi-headed\nattention (FMHA) kernel designed for the NVIDIA Blackwell architecture in\nCUTLASS. Our implementation can fully realize the maximum speedup theoretically\npossible in many perfectly block-sparse cases, and achieves an effective\nutilization of 1.3 petaFLOPs/second in FP16. In addition, we plug various GNA\nconfigurations into off-the-shelf generative models, such as Cosmos-7B,\nHunyuanVideo, and FLUX, and show that it can deliver 28% to 46% end-to-end\nspeedup on B200 without any fine-tuning. We will open source our simulator and\nBlackwell kernels directly through the NATTEN project."}
{"id": "2504.16355", "pdf": "https://arxiv.org/pdf/2504.16355", "abs": "https://arxiv.org/abs/2504.16355", "authors": ["Hassan Asghar", "Chenhan Zhang", "Dali Kaafar"], "title": "Property-Preserving Hashing for $\\ell_1$-Distance Predicates: Applications to Countering Adversarial Input Attacks", "categories": ["cs.CR", "cs.IT", "cs.LG", "math.IT"], "comment": null, "summary": "Perceptual hashing is used to detect whether an input image is similar to a\nreference image with a variety of security applications. Recently, they have\nbeen shown to succumb to adversarial input attacks which make small\nimperceptible changes to the input image yet the hashing algorithm does not\ndetect its similarity to the original image. Property-preserving hashing (PPH)\nis a recent construct in cryptography, which preserves some property\n(predicate) of its inputs in the hash domain. Researchers have so far shown\nconstructions of PPH for Hamming distance predicates, which, for instance,\noutputs 1 if two inputs are within Hamming distance $t$. A key feature of PPH\nis its strong correctness guarantee, i.e., the probability that the predicate\nwill not be correctly evaluated in the hash domain is negligible. Motivated by\nthe use case of detecting similar images under adversarial setting, we propose\nthe first PPH construction for an $\\ell_1$-distance predicate. Roughly, this\npredicate checks if the two one-sided $\\ell_1$-distances between two images are\nwithin a threshold $t$. Since many adversarial attacks use $\\ell_2$-distance\n(related to $\\ell_1$-distance) as the objective function to perturb the input\nimage, by appropriately choosing the threshold $t$, we can force the attacker\nto add considerable noise to evade detection, and hence significantly\ndeteriorate the image quality. Our proposed scheme is highly efficient, and\nruns in time $O(t^2)$. For grayscale images of size $28 \\times 28$, we can\nevaluate the predicate in $0.0784$ seconds when pixel values are perturbed by\nup to $1 \\%$. For larger RGB images of size $224 \\times 224$, by dividing the\nimage into 1,000 blocks, we achieve times of $0.0128$ seconds per block for $1\n\\%$ change, and up to $0.2641$ seconds per block for $14\\%$ change."}
{"id": "2504.16902", "pdf": "https://arxiv.org/pdf/2504.16902", "abs": "https://arxiv.org/abs/2504.16902", "authors": ["Idan Habler", "Ken Huang", "Vineeth Sai Narajala", "Prashant Kulkarni"], "title": "Building A Secure Agentic AI Application Leveraging A2A Protocol", "categories": ["cs.CR", "cs.AI"], "comment": "13 pages, 4 figures, 1 table, Authors contributed equally to this\n  work", "summary": "As Agentic AI systems evolve from basic workflows to complex multi agent\ncollaboration, robust protocols such as Google's Agent2Agent (A2A) become\nessential enablers. To foster secure adoption and ensure the reliability of\nthese complex interactions, understanding the secure implementation of A2A is\nessential. This paper addresses this goal by providing a comprehensive security\nanalysis centered on the A2A protocol. We examine its fundamental elements and\noperational dynamics, situating it within the framework of agent communication\ndevelopment. Utilizing the MAESTRO framework, specifically designed for AI\nrisks, we apply proactive threat modeling to assess potential security issues\nin A2A deployments, focusing on aspects such as Agent Card management, task\nexecution integrity, and authentication methodologies.\n  Based on these insights, we recommend practical secure development\nmethodologies and architectural best practices designed to build resilient and\neffective A2A systems. Our analysis also explores how the synergy between A2A\nand the Model Context Protocol (MCP) can further enhance secure\ninteroperability. This paper equips developers and architects with the\nknowledge and practical guidance needed to confidently leverage the A2A\nprotocol for building robust and secure next generation agentic applications."}
{"id": "2504.10157", "pdf": "https://arxiv.org/pdf/2504.10157", "abs": "https://arxiv.org/abs/2504.10157", "authors": ["Xinnong Zhang", "Jiayu Lin", "Xinyi Mou", "Shiyue Yang", "Xiawei Liu", "Libo Sun", "Hanjia Lyu", "Yihang Yang", "Weihong Qi", "Yue Chen", "Guanying Li", "Ling Yan", "Yao Hu", "Siming Chen", "Yu Wang", "Xuanjing Huang", "Jiebo Luo", "Shiping Tang", "Libo Wu", "Baohua Zhou", "Zhongyu Wei"], "title": "SocioVerse: A World Model for Social Simulation Powered by LLM Agents and A Pool of 10 Million Real-World Users", "categories": ["cs.CL", "cs.CY"], "comment": "work in progress", "summary": "Social simulation is transforming traditional social science research by\nmodeling human behavior through interactions between virtual individuals and\ntheir environments. With recent advances in large language models (LLMs), this\napproach has shown growing potential in capturing individual differences and\npredicting group behaviors. However, existing methods face alignment challenges\nrelated to the environment, target users, interaction mechanisms, and\nbehavioral patterns. To this end, we introduce SocioVerse, an LLM-agent-driven\nworld model for social simulation. Our framework features four powerful\nalignment components and a user pool of 10 million real individuals. To\nvalidate its effectiveness, we conducted large-scale simulation experiments\nacross three distinct domains: politics, news, and economics. Results\ndemonstrate that SocioVerse can reflect large-scale population dynamics while\nensuring diversity, credibility, and representativeness through standardized\nprocedures and minimal manual adjustments."}
{"id": "2504.16930", "pdf": "https://arxiv.org/pdf/2504.16930", "abs": "https://arxiv.org/abs/2504.16930", "authors": ["David Yan", "Alexander Raistrick", "Jia Deng"], "title": "Procedural Dataset Generation for Zero-Shot Stereo Matching", "categories": ["cs.CV"], "comment": null, "summary": "Synthetic datasets are a crucial ingredient for training stereo matching\nnetworks, but the question of what makes a stereo dataset effective remains\nlargely unexplored. We investigate the design space of synthetic datasets by\nvarying the parameters of a procedural dataset generator, and report the\neffects on zero-shot stereo matching performance using standard benchmarks. We\ncollect the best settings to produce Infinigen-Stereo, a procedural generator\nspecifically optimized for zero-shot stereo datasets. Models trained only on\ndata from our system outperform robust baselines trained on a combination of\nexisting synthetic datasets and have stronger zero-shot stereo matching\nperformance than public checkpoints from prior works. We open source our system\nat https://github.com/princeton-vl/InfinigenStereo to enable further research\non procedural stereo datasets."}
{"id": "2504.16356", "pdf": "https://arxiv.org/pdf/2504.16356", "abs": "https://arxiv.org/abs/2504.16356", "authors": ["Jiahe Lin", "Yikai Zhang", "George Michailidis"], "title": "Covariate-dependent Graphical Model Estimation via Neural Networks with Statistical Guarantees", "categories": ["stat.ML", "cs.LG", "stat.ME"], "comment": "Accepted by Transactions on Machine Learning Research (TMLR)", "summary": "Graphical models are widely used in diverse application domains to model the\nconditional dependencies amongst a collection of random variables. In this\npaper, we consider settings where the graph structure is covariate-dependent,\nand investigate a deep neural network-based approach to estimate it. The method\nallows for flexible functional dependency on the covariate, and fits the data\nreasonably well in the absence of a Gaussianity assumption. Theoretical results\nwith PAC guarantees are established for the method, under assumptions commonly\nused in an Empirical Risk Minimization framework. The performance of the\nproposed method is evaluated on several synthetic data settings and benchmarked\nagainst existing approaches. The method is further illustrated on real datasets\ninvolving data from neuroscience and finance, respectively, and produces\ninterpretable results."}
{"id": "2504.16925", "pdf": "https://arxiv.org/pdf/2504.16925", "abs": "https://arxiv.org/abs/2504.16925", "authors": ["Amber Xie", "Oleh Rybkin", "Dorsa Sadigh", "Chelsea Finn"], "title": "Latent Diffusion Planning for Imitation Learning", "categories": ["cs.RO", "cs.AI"], "comment": null, "summary": "Recent progress in imitation learning has been enabled by policy\narchitectures that scale to complex visuomotor tasks, multimodal distributions,\nand large datasets. However, these methods often rely on learning from large\namount of expert demonstrations. To address these shortcomings, we propose\nLatent Diffusion Planning (LDP), a modular approach consisting of a planner\nwhich can leverage action-free demonstrations, and an inverse dynamics model\nwhich can leverage suboptimal data, that both operate over a learned latent\nspace. First, we learn a compact latent space through a variational\nautoencoder, enabling effective forecasting of future states in image-based\ndomains. Then, we train a planner and an inverse dynamics model with diffusion\nobjectives. By separating planning from action prediction, LDP can benefit from\nthe denser supervision signals of suboptimal and action-free data. On simulated\nvisual robotic manipulation tasks, LDP outperforms state-of-the-art imitation\nlearning approaches, as they cannot leverage such additional data."}
{"id": "2504.10982", "pdf": "https://arxiv.org/pdf/2504.10982", "abs": "https://arxiv.org/abs/2504.10982", "authors": ["Yingjian Chen", "Feiyang Li", "Xingyu Song", "Tianxiao Li", "Zixin Xu", "Xiujie Chen", "Issey Sukeda", "Irene Li"], "title": "Exploring the Role of Knowledge Graph-Based RAG in Japanese Medical Question Answering with Small-Scale LLMs", "categories": ["cs.CL", "cs.AI"], "comment": "10 pages", "summary": "Large language models (LLMs) perform well in medical QA, but their\neffectiveness in Japanese contexts is limited due to privacy constraints that\nprevent the use of commercial models like GPT-4 in clinical settings. As a\nresult, recent efforts focus on instruction-tuning open-source LLMs, though the\npotential of combining them with retrieval-augmented generation (RAG) remains\nunderexplored. To bridge this gap, we are the first to explore a knowledge\ngraph-based (KG) RAG framework for Japanese medical QA small-scale open-source\nLLMs. Experimental results show that KG-based RAG has only a limited impact on\nJapanese medical QA using small-scale open-source LLMs. Further case studies\nreveal that the effectiveness of the RAG is sensitive to the quality and\nrelevance of the external retrieved content. These findings offer valuable\ninsights into the challenges and potential of applying RAG in Japanese medical\nQA, while also serving as a reference for other low-resource languages."}
{"id": "2504.16306", "pdf": "https://arxiv.org/pdf/2504.16306", "abs": "https://arxiv.org/abs/2504.16306", "authors": ["Yanlin Zhou", "Mostafa El-Khamy", "Kee-Bong Song"], "title": "Regularizing Differentiable Architecture Search with Smooth Activation", "categories": ["cs.NE", "cs.CV"], "comment": null, "summary": "Differentiable Architecture Search (DARTS) is an efficient Neural\nArchitecture Search (NAS) method but suffers from robustness, generalization,\nand discrepancy issues. Many efforts have been made towards the performance\ncollapse issue caused by skip dominance with various regularization techniques\ntowards operation weights, path weights, noise injection, and super-network\nredesign. It had become questionable at a certain point if there could exist a\nbetter and more elegant way to retract the search to its intended goal -- NAS\nis a selection problem. In this paper, we undertake a simple but effective\napproach, named Smooth Activation DARTS (SA-DARTS), to overcome skip dominance\nand discretization discrepancy challenges. By leveraging a smooth activation\nfunction on architecture weights as an auxiliary loss, our SA-DARTS mitigates\nthe unfair advantage of weight-free operations, converging to fanned-out\narchitecture weight values, and can recover the search process from\nskip-dominance initialization. Through theoretical and empirical analysis, we\ndemonstrate that the SA-DARTS can yield new state-of-the-art (SOTA) results on\nNAS-Bench-201, classification, and super-resolution. Further, we show that\nSA-DARTS can help improve the performance of SOTA models with fewer parameters,\nsuch as Information Multi-distillation Network on the super-resolution task."}
{"id": "2504.16371", "pdf": "https://arxiv.org/pdf/2504.16371", "abs": "https://arxiv.org/abs/2504.16371", "authors": ["Arghavan Zibaie", "Spencer Hutchinson", "Ramtin Pedarsani", "Mahnoosh Alizadeh"], "title": "The Safety-Privacy Tradeoff in Linear Bandits", "categories": ["math.OC", "cs.LG"], "comment": "16 pages, 3 figures, accepted to 2025 IEEE International Symposium on\n  Information Theory (ISIT)", "summary": "We consider a collection of linear stochastic bandit problems, each modeling\nthe random response of different agents to proposed interventions, coupled\ntogether by a global safety constraint. We assume a central coordinator must\nchoose actions to play on each bandit with the objective of regret\nminimization, while also ensuring that the expected response of all agents\nsatisfies the global safety constraints at each round, in spite of uncertainty\nabout the bandits' parameters. The agents consider their observed responses to\nbe private and in order to protect their sensitive information, the data\nsharing with the central coordinator is performed under local differential\nprivacy (LDP). However, providing higher level of privacy to different agents\nwould have consequences in terms of safety and regret. We formalize these\ntradeoffs by building on the notion of the sharpness of the safety set - a\nmeasure of how the geometric properties of the safe set affects the growth of\nregret - and propose a unilaterally unimprovable vector of privacy levels for\ndifferent agents given a maximum regret budget."}
{"id": "2311.00530", "pdf": "https://arxiv.org/pdf/2311.00530", "abs": "https://arxiv.org/abs/2311.00530", "authors": ["Jinzhou Lin", "Han Gao", "Xuxiang Feng", "Rongtao Xu", "Changwei Wang", "Man Zhang", "Li Guo", "Shibiao Xu"], "title": "Advances in Embodied Navigation Using Large Language Models: A Survey", "categories": ["cs.AI"], "comment": "Submited to IEEE TRANSACTIONS ON SYSTEMS, MAN, AND CYBERNETICS:\n  SYSTEMS", "summary": "In recent years, the rapid advancement of Large Language Models (LLMs) such\nas the Generative Pre-trained Transformer (GPT) has attracted increasing\nattention due to their potential in a variety of practical applications. The\napplication of LLMs with Embodied Intelligence has emerged as a significant\narea of focus. Among the myriad applications of LLMs, navigation tasks are\nparticularly noteworthy because they demand a deep understanding of the\nenvironment and quick, accurate decision-making. LLMs can augment embodied\nintelligence systems with sophisticated environmental perception and\ndecision-making support, leveraging their robust language and image-processing\ncapabilities. This article offers an exhaustive summary of the symbiosis\nbetween LLMs and embodied intelligence with a focus on navigation. It reviews\nstate-of-the-art models, research methodologies, and assesses the advantages\nand disadvantages of existing embodied navigation models and datasets. Finally,\nthe article elucidates the role of LLMs in embodied intelligence, based on\ncurrent research, and forecasts future directions in the field. A comprehensive\nlist of studies in this survey is available at\nhttps://github.com/Rongtao-Xu/Awesome-LLM-EN."}
{"id": "2504.13217", "pdf": "https://arxiv.org/pdf/2504.13217", "abs": "https://arxiv.org/abs/2504.13217", "authors": ["Jennifer Haase", "Finn Klessascheck", "Jan Mendling", "Sebastian Pokutta"], "title": "Sustainability via LLM Right-sizing", "categories": ["cs.CL", "cs.AI"], "comment": "17 pages, 2 Figures, 6 Tables", "summary": "Large language models (LLMs) have become increasingly embedded in\norganizational workflows. This has raised concerns over their energy\nconsumption, financial costs, and data sovereignty. While performance\nbenchmarks often celebrate cutting-edge models, real-world deployment decisions\nrequire a broader perspective: when is a smaller, locally deployable model\n\"good enough\"? This study offers an empirical answer by evaluating eleven\nproprietary and open-weight LLMs across ten everyday occupational tasks,\nincluding summarizing texts, generating schedules, and drafting emails and\nproposals. Using a dual-LLM-based evaluation framework, we automated task\nexecution and standardized evaluation across ten criteria related to output\nquality, factual accuracy, and ethical responsibility. Results show that GPT-4o\ndelivers consistently superior performance but at a significantly higher cost\nand environmental footprint. Notably, smaller models like Gemma-3 and Phi-4\nachieved strong and reliable results on most tasks, suggesting their viability\nin contexts requiring cost-efficiency, local deployment, or privacy. A cluster\nanalysis revealed three model groups -- premium all-rounders, competent\ngeneralists, and limited but safe performers -- highlighting trade-offs between\nquality, control, and sustainability. Significantly, task type influenced model\neffectiveness: conceptual tasks challenged most models, while aggregation and\ntransformation tasks yielded better performances. We argue for a shift from\nperformance-maximizing benchmarks to task- and context-aware sufficiency\nassessments that better reflect organizational priorities. Our approach\ncontributes a scalable method to evaluate AI models through a sustainability\nlens and offers actionable guidance for responsible LLM deployment in practice."}
{"id": "2504.16606", "pdf": "https://arxiv.org/pdf/2504.16606", "abs": "https://arxiv.org/abs/2504.16606", "authors": ["Zhongtao Wang", "Mai Su", "Huishan Au", "Yilong Li", "Xizhe Cao", "Chengwei Pan", "Yisong Chen", "Guoping Wang"], "title": "HUG: Hierarchical Urban Gaussian Splatting with Block-Based Reconstruction", "categories": ["cs.GR", "cs.CV"], "comment": null, "summary": "As urban 3D scenes become increasingly complex and the demand for\nhigh-quality rendering grows, efficient scene reconstruction and rendering\ntechniques become crucial. We present HUG, a novel approach to address\ninefficiencies in handling large-scale urban environments and intricate details\nbased on 3D Gaussian splatting. Our method optimizes data partitioning and the\nreconstruction pipeline by incorporating a hierarchical neural Gaussian\nrepresentation. We employ an enhanced block-based reconstruction pipeline\nfocusing on improving reconstruction quality within each block and reducing the\nneed for redundant training regions around block boundaries. By integrating\nneural Gaussian representation with a hierarchical architecture, we achieve\nhigh-quality scene rendering at a low computational cost. This is demonstrated\nby our state-of-the-art results on public benchmarks, which prove the\neffectiveness and advantages in large-scale urban scene representation."}
{"id": "2504.16397", "pdf": "https://arxiv.org/pdf/2504.16397", "abs": "https://arxiv.org/abs/2504.16397", "authors": ["Banruo Liu", "Wei-Yu Lin", "Minghao Fang", "Yihan Jiang", "Fan Lai"], "title": "Circinus: Efficient Query Planner for Compound ML Serving", "categories": ["cs.DB", "cs.LG"], "comment": null, "summary": "The rise of compound AI serving -- integrating multiple operators in a\npipeline that may span edge and cloud tiers -- enables end-user applications\nsuch as autonomous driving, generative AI-powered meeting companions, and\nimmersive gaming. Achieving high service goodput -- i.e., meeting service level\nobjectives (SLOs) for pipeline latency, accuracy, and costs -- requires\neffective planning of operator placement, configuration, and resource\nallocation across infrastructure tiers. However, the diverse SLO requirements,\nvarying edge capabilities, and high query volumes create an enormous planning\nsearch space, rendering current solutions fundamentally limited for real-time\nserving and cost-efficient deployments.\n  This paper presents Circinus, an SLO-aware query planner for large-scale\ncompound AI workloads. Circinus novelly decomposes multi-query planning and\nmulti-dimensional SLO objectives while preserving global decision quality. By\nexploiting plan similarities within and across queries, it significantly\nreduces search steps. It further improves per-step efficiency with a\nprecision-aware plan profiler that incrementally profiles and strategically\napplies early stopping based on imprecise estimates of plan performance. At\nscale, Circinus selects query-plan combinations to maximize global SLO goodput.\nEvaluations in real-world settings show that Circinus improves service goodput\nby 3.2-5.0$\\times$, accelerates query planning by 4.2-5.8$\\times$, achieving\nquery response in seconds, while reducing deployment costs by 3.2-4.0$\\times$\nover state of the arts even in their intended single-tier deployments."}
{"id": "2402.07877", "pdf": "https://arxiv.org/pdf/2402.07877", "abs": "https://arxiv.org/abs/2402.07877", "authors": ["Yangxinyu Xie", "Bowen Jiang", "Tanwi Mallick", "Joshua David Bergerson", "John K. Hutchison", "Duane R. Verner", "Jordan Branham", "M. Ross Alexander", "Robert B. Ross", "Yan Feng", "Leslie-Anne Levy", "Weijie Su", "Camillo J. Taylor"], "title": "WildfireGPT: Tailored Large Language Model for Wildfire Analysis", "categories": ["cs.AI"], "comment": "restoring content for arXiv:2402.07877v2 which was replaced in error", "summary": "Recent advancement of large language models (LLMs) represents a\ntransformational capability at the frontier of artificial intelligence.\nHowever, LLMs are generalized models, trained on extensive text corpus, and\noften struggle to provide context-specific information, particularly in areas\nrequiring specialized knowledge, such as wildfire details within the broader\ncontext of climate change. For decision-makers focused on wildfire resilience\nand adaptation, it is crucial to obtain responses that are not only precise but\nalso domain-specific. To that end, we developed WildfireGPT, a prototype LLM\nagent designed to transform user queries into actionable insights on wildfire\nrisks. We enrich WildfireGPT by providing additional context, such as climate\nprojections and scientific literature, to ensure its information is current,\nrelevant, and scientifically accurate. This enables WildfireGPT to be an\neffective tool for delivering detailed, user-specific insights on wildfire\nrisks to support a diverse set of end users, including but not limited to\nresearchers and engineers, for making positive impact and decision making."}
{"id": "2504.16005", "pdf": "https://arxiv.org/pdf/2504.16005", "abs": "https://arxiv.org/abs/2504.16005", "authors": ["Tom Zehle", "Moritz Schlager", "Timo Heiß", "Matthias Feurer"], "title": "CAPO: Cost-Aware Prompt Optimization", "categories": ["cs.CL", "cs.AI", "cs.NE", "stat.ML"], "comment": "Submitted to AutoML 2025", "summary": "Large language models (LLMs) have revolutionized natural language processing\nby solving a wide range of tasks simply guided by a prompt. Yet their\nperformance is highly sensitive to prompt formulation. While automated prompt\noptimization addresses this challenge by finding optimal prompts, current\nmethods require a substantial number of LLM calls and input tokens, making\nprompt optimization expensive. We introduce CAPO (Cost-Aware Prompt\nOptimization), an algorithm that enhances prompt optimization efficiency by\nintegrating AutoML techniques. CAPO is an evolutionary approach with LLMs as\noperators, incorporating racing to save evaluations and multi-objective\noptimization to balance performance with prompt length. It jointly optimizes\ninstructions and few-shot examples while leveraging task descriptions for\nimproved robustness. Our extensive experiments across diverse datasets and LLMs\ndemonstrate that CAPO outperforms state-of-the-art discrete prompt optimization\nmethods in 11/15 cases with improvements up to 21%p. Our algorithm achieves\nbetter performances already with smaller budgets, saves evaluations through\nracing, and decreases average prompt length via a length penalty, making it\nboth cost-efficient and cost-aware. Even without few-shot examples, CAPO\noutperforms its competitors and generally remains robust to initial prompts.\nCAPO represents an important step toward making prompt optimization more\npowerful and accessible by improving cost-efficiency."}
{"id": "2112.01525", "pdf": "https://arxiv.org/pdf/2112.01525", "abs": "https://arxiv.org/abs/2112.01525", "authors": ["Utkarsh Singhal", "Yifei Xing", "Stella X. Yu"], "title": "Co-domain Symmetry for Complex-Valued Deep Learning", "categories": ["cs.CV", "cs.AI", "cs.LG"], "comment": null, "summary": "We study complex-valued scaling as a type of symmetry natural and unique to\ncomplex-valued measurements and representations. Deep Complex Networks (DCN)\nextends real-valued algebra to the complex domain without addressing\ncomplex-valued scaling. SurReal takes a restrictive manifold view of complex\nnumbers, adopting a distance metric to achieve complex-scaling invariance while\nlosing rich complex-valued information. We analyze complex-valued scaling as a\nco-domain transformation and design novel equivariant and invariant neural\nnetwork layer functions for this special transformation. We also propose novel\ncomplex-valued representations of RGB images, where complex-valued scaling\nindicates hue shift or correlated changes across color channels. Benchmarked on\nMSTAR, CIFAR10, CIFAR100, and SVHN, our co-domain symmetric (CDS) classifiers\ndeliver higher accuracy, better generalization, robustness to co-domain\ntransformations, and lower model bias and variance than DCN and SurReal with\nfar fewer parameters."}
{"id": "2504.16449", "pdf": "https://arxiv.org/pdf/2504.16449", "abs": "https://arxiv.org/abs/2504.16449", "authors": ["Ye Tian", "Yanqiu Yu", "Jianguo Sun", "Yanbin Wang"], "title": "From Past to Present: A Survey of Malicious URL Detection Techniques, Datasets and Code Repositories", "categories": ["cs.CR", "cs.LG"], "comment": null, "summary": "Malicious URLs persistently threaten the cybersecurity ecosystem, by either\ndeceiving users into divulging private data or distributing harmful payloads to\ninfiltrate host systems. Gaining timely insights into the current state of this\nongoing battle holds significant importance. However, existing reviews exhibit\n4 critical gaps: 1) Their reliance on algorithm-centric taxonomies obscures\nunderstanding of how detection approaches exploit specific modal information\nchannels; 2) They fail to incorporate pivotal LLM/Transformer-based defenses;\n3) No open-source implementations are collected to facilitate benchmarking; 4)\nInsufficient dataset coverage.This paper presents a comprehensive review of\nmalicious URL detection technologies, systematically analyzing methods from\ntraditional blacklisting to advanced deep learning approaches (e.g.\nTransformer, GNNs, and LLMs). Unlike prior surveys, we propose a novel\nmodality-based taxonomy that categorizes existing works according to their\nprimary data modalities (URL, HTML, Visual, etc.). This hierarchical\nclassification enables both rigorous technical analysis and clear understanding\nof multimodal information utilization. Furthermore, to establish a profile of\naccessible datasets and address the lack of standardized benchmarking (where\ncurrent studies often lack proper baseline comparisons), we curate and analyze:\n1) publicly available datasets (2016-2024), and 2) open-source implementations\nfrom published works(2013-2025). Then, we outline essential design principles\nand architectural frameworks for product-level implementations. The review\nconcludes by examining emerging challenges and proposing actionable directions\nfor future research. We maintain a GitHub repository for ongoing curating\ndatasets and open-source implementations:\nhttps://github.com/sevenolu7/Malicious-URL-Detection-Open-Source/tree/master."}
{"id": "2403.04105", "pdf": "https://arxiv.org/pdf/2403.04105", "abs": "https://arxiv.org/abs/2403.04105", "authors": ["Lekang Jiang", "Stephan Goetz"], "title": "Natural Language Processing in the Patent Domain: A Survey", "categories": ["cs.AI"], "comment": "Published in Artificial Intelligence Review", "summary": "Patents, which encapsulate crucial technical and legal information in text\nform and referenced drawings, present a rich domain for natural language\nprocessing (NLP) applications. As NLP technologies evolve, large language\nmodels (LLMs) have demonstrated outstanding capabilities in general text\nprocessing and generation tasks. However, the application of LLMs in the patent\ndomain remains under-explored and under-developed due to the complexity of\npatents, particularly their language and legal framework. Understanding the\nunique characteristics of patent documents and related research in the patent\ndomain becomes essential for researchers to apply these tools effectively.\nTherefore, this paper aims to equip NLP researchers with the essential\nknowledge to navigate this complex domain efficiently. We introduce the\nrelevant fundamental aspects of patents to provide solid background\ninformation. In addition, we systematically break down the structural and\nlinguistic characteristics unique to patents and map out how NLP can be\nleveraged for patent analysis and generation. Moreover, we demonstrate the\nspectrum of text-based and multimodal patent-related tasks, including nine\npatent analysis and four patent generation tasks."}
{"id": "2504.16046", "pdf": "https://arxiv.org/pdf/2504.16046", "abs": "https://arxiv.org/abs/2504.16046", "authors": ["Jingyu Zhang", "Jiacan Yu", "Marc Marone", "Benjamin Van Durme", "Daniel Khashabi"], "title": "Certified Mitigation of Worst-Case LLM Copyright Infringement", "categories": ["cs.CL"], "comment": null, "summary": "The exposure of large language models (LLMs) to copyrighted material during\npre-training raises concerns about unintentional copyright infringement post\ndeployment. This has driven the development of \"copyright takedown\" methods,\npost-training approaches aimed at preventing models from generating content\nsubstantially similar to copyrighted ones. While current mitigation approaches\nare somewhat effective for average-case risks, we demonstrate that they\noverlook worst-case copyright risks exhibits by the existence of long, verbatim\nquotes from copyrighted sources. We propose BloomScrub, a remarkably simple yet\nhighly effective inference-time approach that provides certified copyright\ntakedown. Our method repeatedly interleaves quote detection with rewriting\ntechniques to transform potentially infringing segments. By leveraging\nefficient data sketches (Bloom filters), our approach enables scalable\ncopyright screening even for large-scale real-world corpora. When quotes beyond\na length threshold cannot be removed, the system can abstain from responding,\noffering certified risk reduction. Experimental results show that BloomScrub\nreduces infringement risk, preserves utility, and accommodates different levels\nof enforcement stringency with adaptive abstention. Our results suggest that\nlightweight, inference-time methods can be surprisingly effective for copyright\nprevention."}
{"id": "2312.04867", "pdf": "https://arxiv.org/pdf/2312.04867", "abs": "https://arxiv.org/abs/2312.04867", "authors": ["Pei Lin", "Sihang Xu", "Hongdi Yang", "Yiran Liu", "Xin Chen", "Jingya Wang", "Jingyi Yu", "Lan Xu"], "title": "HandDiffuse: Generative Controllers for Two-Hand Interactions via Diffusion Models", "categories": ["cs.CV"], "comment": null, "summary": "Existing hands datasets are largely short-range and the interaction is weak\ndue to the self-occlusion and self-similarity of hands, which can not yet fit\nthe need for interacting hands motion generation. To rescue the data scarcity,\nwe propose HandDiffuse12.5M, a novel dataset that consists of temporal\nsequences with strong two-hand interactions. HandDiffuse12.5M has the largest\nscale and richest interactions among the existing two-hand datasets. We further\npresent a strong baseline method HandDiffuse for the controllable motion\ngeneration of interacting hands using various controllers. Specifically, we\napply the diffusion model as the backbone and design two motion representations\nfor different controllers. To reduce artifacts, we also propose Interaction\nLoss which explicitly quantifies the dynamic interaction process. Our\nHandDiffuse enables various applications with vivid two-hand interactions,\ni.e., motion in-betweening and trajectory control. Experiments show that our\nmethod outperforms the state-of-the-art techniques in motion generation and can\nalso contribute to data augmentation for other datasets. Our dataset,\ncorresponding codes, and pre-trained models will be disseminated to the\ncommunity for future research towards two-hand interaction modeling."}
{"id": "2504.16474", "pdf": "https://arxiv.org/pdf/2504.16474", "abs": "https://arxiv.org/abs/2504.16474", "authors": ["Meixi Zheng", "Kehan Wu", "Yanbo Fan", "Rui Huang", "Baoyuan Wu"], "title": "Seeking Flat Minima over Diverse Surrogates for Improved Adversarial Transferability: A Theoretical Framework and Algorithmic Instantiation", "categories": ["cs.CR", "cs.LG"], "comment": "26 pages, 6 figures", "summary": "The transfer-based black-box adversarial attack setting poses the challenge\nof crafting an adversarial example (AE) on known surrogate models that remain\neffective against unseen target models. Due to the practical importance of this\ntask, numerous methods have been proposed to address this challenge. However,\nmost previous methods are heuristically designed and intuitively justified,\nlacking a theoretical foundation. To bridge this gap, we derive a novel\ntransferability bound that offers provable guarantees for adversarial\ntransferability. Our theoretical analysis has the advantages of \\textit{(i)}\ndeepening our understanding of previous methods by building a general attack\nframework and \\textit{(ii)} providing guidance for designing an effective\nattack algorithm. Our theoretical results demonstrate that optimizing AEs\ntoward flat minima over the surrogate model set, while controlling the\nsurrogate-target model shift measured by the adversarial model discrepancy,\nyields a comprehensive guarantee for AE transferability. The results further\nlead to a general transfer-based attack framework, within which we observe that\nprevious methods consider only partial factors contributing to the\ntransferability. Algorithmically, inspired by our theoretical results, we first\nelaborately construct the surrogate model set in which models exhibit diverse\nadversarial vulnerabilities with respect to AEs to narrow an instantiated\nadversarial model discrepancy. Then, a \\textit{model-Diversity-compatible\nReverse Adversarial Perturbation} (DRAP) is generated to effectively promote\nthe flatness of AEs over diverse surrogate models to improve transferability.\nExtensive experiments on NIPS2017 and CIFAR-10 datasets against various target\nmodels demonstrate the effectiveness of our proposed attack."}
{"id": "2408.00041", "pdf": "https://arxiv.org/pdf/2408.00041", "abs": "https://arxiv.org/abs/2408.00041", "authors": ["Junru Chen", "Tianyu Cao", "Jing Xu", "Jiahe Li", "Zhilong Chen", "Tao Xiao", "Yang Yang"], "title": "Con4m: Context-aware Consistency Learning Framework for Segmented Time Series Classification", "categories": ["cs.AI"], "comment": null, "summary": "Time Series Classification (TSC) encompasses two settings: classifying entire\nsequences or classifying segmented subsequences. The raw time series for\nsegmented TSC usually contain Multiple classes with Varying Duration of each\nclass (MVD). Therefore, the characteristics of MVD pose unique challenges for\nsegmented TSC, yet have been largely overlooked by existing works.\nSpecifically, there exists a natural temporal dependency between consecutive\ninstances (segments) to be classified within MVD. However, mainstream TSC\nmodels rely on the assumption of independent and identically distributed\n(i.i.d.), focusing on independently modeling each segment. Additionally,\nannotators with varying expertise may provide inconsistent boundary labels,\nleading to unstable performance of noise-free TSC models. To address these\nchallenges, we first formally demonstrate that valuable contextual information\nenhances the discriminative power of classification instances. Leveraging the\ncontextual priors of MVD at both the data and label levels, we propose a novel\nconsistency learning framework Con4m, which effectively utilizes contextual\ninformation more conducive to discriminating consecutive segments in segmented\nTSC tasks, while harmonizing inconsistent boundary labels for training.\nExtensive experiments across multiple datasets validate the effectiveness of\nCon4m in handling segmented TSC tasks on MVD. The source code is available at\nhttps://github.com/MrNobodyCali/Con4m."}
{"id": "2410.06172", "pdf": "https://arxiv.org/pdf/2410.06172", "abs": "https://arxiv.org/abs/2410.06172", "authors": ["Kaiwen Zhou", "Chengzhi Liu", "Xuandong Zhao", "Anderson Compalas", "Dawn Song", "Xin Eric Wang"], "title": "Multimodal Situational Safety", "categories": ["cs.AI", "cs.CL"], "comment": "ICLR 2025 Camera Ready", "summary": "Multimodal Large Language Models (MLLMs) are rapidly evolving, demonstrating\nimpressive capabilities as multimodal assistants that interact with both humans\nand their environments. However, this increased sophistication introduces\nsignificant safety concerns. In this paper, we present the first evaluation and\nanalysis of a novel safety challenge termed Multimodal Situational Safety,\nwhich explores how safety considerations vary based on the specific situation\nin which the user or agent is engaged. We argue that for an MLLM to respond\nsafely, whether through language or action, it often needs to assess the safety\nimplications of a language query within its corresponding visual context. To\nevaluate this capability, we develop the Multimodal Situational Safety\nbenchmark (MSSBench) to assess the situational safety performance of current\nMLLMs. The dataset comprises 1,820 language query-image pairs, half of which\nthe image context is safe, and the other half is unsafe. We also develop an\nevaluation framework that analyzes key safety aspects, including explicit\nsafety reasoning, visual understanding, and, crucially, situational safety\nreasoning. Our findings reveal that current MLLMs struggle with this nuanced\nsafety problem in the instruction-following setting and struggle to tackle\nthese situational safety challenges all at once, highlighting a key area for\nfuture research. Furthermore, we develop multi-agent pipelines to coordinately\nsolve safety challenges, which shows consistent improvement in safety over the\noriginal MLLM response. Code and data: mssbench.github.io."}
{"id": "2404.03819", "pdf": "https://arxiv.org/pdf/2404.03819", "abs": "https://arxiv.org/abs/2404.03819", "authors": ["Yirui Wang", "Qinji Yu", "Ke Yan", "Haoshen Li", "Dazhou Guo", "Li Zhang", "Le Lu", "Na Shen", "Qifeng Wang", "Xiaowei Ding", "Xianghua Ye", "Dakai Jin"], "title": "Effective Lymph Nodes Detection in CT Scans Using Location Debiased Query Selection and Contrastive Query Representation in Transformer", "categories": ["cs.CV"], "comment": "Accepted by ECCV24", "summary": "Lymph node (LN) assessment is a critical, indispensable yet very challenging\ntask in the routine clinical workflow of radiology and oncology. Accurate LN\nanalysis is essential for cancer diagnosis, staging, and treatment planning.\nFinding scatteredly distributed, low-contrast clinically relevant LNs in 3D CT\nis difficult even for experienced physicians under high inter-observer\nvariations. Previous automatic LN detection works typically yield limited\nrecall and high false positives (FPs) due to adjacent anatomies with similar\nimage intensities, shapes, or textures (vessels, muscles, esophagus, etc). In\nthis work, we propose a new LN DEtection TRansformer, named LN-DETR, to achieve\nmore accurate performance. By enhancing the 2D backbone with a multi-scale 2.5D\nfeature fusion to incorporate 3D context explicitly, more importantly, we make\ntwo main contributions to improve the representation quality of LN queries. 1)\nConsidering that LN boundaries are often unclear, an IoU prediction head and a\nlocation debiased query selection are proposed to select LN queries of higher\nlocalization accuracy as the decoder query's initialization. 2) To reduce FPs,\nquery contrastive learning is employed to explicitly reinforce LN queries\ntowards their best-matched ground-truth queries over unmatched query\npredictions. Trained and tested on 3D CT scans of 1067 patients (with 10,000+\nlabeled LNs) via combining seven LN datasets from different body parts (neck,\nchest, and abdomen) and pathologies/cancers, our method significantly improves\nthe performance of previous leading methods by > 4-5% average recall at the\nsame FP rates in both internal and external testing. We further evaluate on the\nuniversal lesion detection task using NIH DeepLesion benchmark, and our method\nachieves the top performance of 88.46% averaged recall across 0.5 to 4 FPs per\nimage, compared with other leading reported results."}
{"id": "2504.16493", "pdf": "https://arxiv.org/pdf/2504.16493", "abs": "https://arxiv.org/abs/2504.16493", "authors": ["Luuk H. E. Kempen", "Marius Juul Nielsen", "Mie Andersen"], "title": "Breaking scaling relations with inverse catalysts: a machine learning exploration of trends in $\\mathrm{CO_2}$ hydrogenation energy barriers", "categories": ["cond-mat.mtrl-sci", "cs.LG", "physics.chem-ph"], "comment": "10 pages, 6 figures + supporting information (5 pages, 7 figures, 2\n  tables)", "summary": "The conversion of $\\mathrm{CO_2}$ into useful products such as methanol is a\nkey strategy for abating climate change and our dependence on fossil fuels.\nDeveloping new catalysts for this process is costly and time-consuming and can\nthus benefit from computational exploration of possible active sites. However,\nthis is complicated by the complexity of the materials and reaction networks.\nHere, we present a workflow for exploring transition states of elementary\nreaction steps at inverse catalysts, which is based on the training of a neural\nnetwork-based machine learning interatomic potential. We focus on the crucial\nformate intermediate and its formation over nanoclusters of indium oxide\nsupported on Cu(111). The speedup compared to an approach purely based on\ndensity functional theory allows us to probe a wide variety of active sites\nfound at nanoclusters of different sizes and stoichiometries. Analysis of the\nobtained set of transition state geometries reveals different\nstructure--activity trends at the edge or interior of the nanoclusters.\nFurthermore, the identified geometries allow for the breaking of linear scaling\nrelations, which could be a key underlying reason for the excellent catalytic\nperformance of inverse catalysts observed in experiments."}
{"id": "2409.15114", "pdf": "https://arxiv.org/pdf/2409.15114", "abs": "https://arxiv.org/abs/2409.15114", "authors": ["Lucas Heublein", "Tobias Feigl", "Thorsten Nowak", "Alexander Rügamer", "Christopher Mutschler", "Felix Ott"], "title": "Evaluating ML Robustness in GNSS Interference Classification, Characterization & Localization", "categories": ["cs.AI", "94-05, 82-11", "E.0; I.2.0; I.5.4; I.5.1"], "comment": null, "summary": "Jamming devices disrupt signals from the global navigation satellite system\n(GNSS) and pose a significant threat, as they compromise the robustness of\naccurate positioning. The detection of anomalies within frequency snapshots is\ncrucial to counteract these interferences effectively. A critical preliminary\ncountermeasure involves the reliable classification of interferences and the\ncharacterization and localization of jamming devices. This paper introduces an\nextensive dataset comprising snapshots obtained from a low-frequency antenna\nthat capture various generated interferences within a large-scale environment,\nincluding controlled multipath effects. Our objective is to assess the\nresilience of machine learning (ML) models against environmental changes, such\nas multipath effects, variations in interference attributes, such as\ninterference class, bandwidth, and signal power, the accuracy of jamming device\nlocalization, and the constraints imposed by snapshot input lengths.\nFurthermore, we evaluate the performance of a diverse set of 129 distinct\nvision encoder models across all tasks. By analyzing the aleatoric and\nepistemic uncertainties, we demonstrate the adaptability of our model in\ngeneralizing across diverse facets, thus establishing its suitability for\nreal-world applications. Dataset:\nhttps://gitlab.cc-asp.fraunhofer.de/darcy_gnss/controlled_low_frequency"}
{"id": "2410.22663", "pdf": "https://arxiv.org/pdf/2410.22663", "abs": "https://arxiv.org/abs/2410.22663", "authors": ["Lam Nguyen Tung", "Steven Cho", "Xiaoning Du", "Neelofar Neelofar", "Valerio Terragni", "Stefano Ruberto", "Aldeida Aleti"], "title": "Automated Trustworthiness Oracle Generation for Machine Learning Text Classifiers", "categories": ["cs.SE", "cs.CL", "cs.CR"], "comment": "24 pages, 5 tables, 9 figures, Camera-ready version accepted to FSE\n  2025", "summary": "Machine learning (ML) for text classification has been widely used in various\ndomains. These applications can significantly impact ethics, economics, and\nhuman behavior, raising serious concerns about trusting ML decisions. Studies\nindicate that conventional metrics are insufficient to build human trust in ML\nmodels. These models often learn spurious correlations and predict based on\nthem. In the real world, their performance can deteriorate significantly. To\navoid this, a common practice is to test whether predictions are reasonable\nbased on valid patterns in the data. Along with this, a challenge known as the\ntrustworthiness oracle problem has been introduced. Due to the lack of\nautomated trustworthiness oracles, the assessment requires manual validation of\nthe decision process disclosed by explanation methods. However, this is\ntime-consuming, error-prone, and unscalable.\n  We propose TOKI, the first automated trustworthiness oracle generation method\nfor text classifiers. TOKI automatically checks whether the words contributing\nthe most to a prediction are semantically related to the predicted class.\nSpecifically, we leverage ML explanations to extract the decision-contributing\nwords and measure their semantic relatedness with the class based on word\nembeddings. We also introduce a novel adversarial attack method that targets\ntrustworthiness vulnerabilities identified by TOKI. To evaluate their alignment\nwith human judgement, experiments are conducted. We compare TOKI with a naive\nbaseline based solely on model confidence and TOKI-guided adversarial attack\nmethod with A2T, a SOTA adversarial attack method. Results show that relying on\nprediction uncertainty cannot effectively distinguish between trustworthy and\nuntrustworthy predictions, TOKI achieves 142% higher accuracy than the naive\nbaseline, and TOKI-guided attack method is more effective with fewer\nperturbations than A2T."}
{"id": "2406.00622", "pdf": "https://arxiv.org/pdf/2406.00622", "abs": "https://arxiv.org/abs/2406.00622", "authors": ["Xingrui Wang", "Wufei Ma", "Angtian Wang", "Shuo Chen", "Adam Kortylewski", "Alan Yuille"], "title": "Compositional 4D Dynamic Scenes Understanding with Physics Priors for Video Question Answering", "categories": ["cs.CV", "cs.AI"], "comment": "ICLR 2025 accepted paper. Project url:\n  https://xingruiwang.github.io/projects/DynSuperCLEVR/", "summary": "For vision-language models (VLMs), understanding the dynamic properties of\nobjects and their interactions in 3D scenes from videos is crucial for\neffective reasoning about high-level temporal and action semantics. Although\nhumans are adept at understanding these properties by constructing 3D and\ntemporal (4D) representations of the world, current video understanding models\nstruggle to extract these dynamic semantics, arguably because these models use\ncross-frame reasoning without underlying knowledge of the 3D/4D scenes. In this\nwork, we introduce DynSuperCLEVR, the first video question answering dataset\nthat focuses on language understanding of the dynamic properties of 3D objects.\nWe concentrate on three physical concepts -- velocity, acceleration, and\ncollisions within 4D scenes. We further generate three types of questions,\nincluding factual queries, future predictions, and counterfactual reasoning\nthat involve different aspects of reasoning about these 4D dynamic properties.\nTo further demonstrate the importance of explicit scene representations in\nanswering these 4D dynamics questions, we propose NS-4DPhysics, a\nNeural-Symbolic VideoQA model integrating Physics prior for 4D dynamic\nproperties with explicit scene representation of videos. Instead of answering\nthe questions directly from the video text input, our method first estimates\nthe 4D world states with a 3D generative model powered by physical priors, and\nthen uses neural symbolic reasoning to answer the questions based on the 4D\nworld states. Our evaluation on all three types of questions in DynSuperCLEVR\nshows that previous video question answering models and large multimodal models\nstruggle with questions about 4D dynamics, while our NS-4DPhysics significantly\noutperforms previous state-of-the-art models. Our code and data are released in\nhttps://xingruiwang.github.io/projects/DynSuperCLEVR/."}
{"id": "2504.16503", "pdf": "https://arxiv.org/pdf/2504.16503", "abs": "https://arxiv.org/abs/2504.16503", "authors": ["Jiří Kubalík", "Robert Babuška"], "title": "Neuro-Evolutionary Approach to Physics-Aware Symbolic Regression", "categories": ["cs.NE", "cs.LG"], "comment": null, "summary": "Symbolic regression is a technique that can automatically derive analytic\nmodels from data. Traditionally, symbolic regression has been implemented\nprimarily through genetic programming that evolves populations of candidate\nsolutions sampled by genetic operators, crossover and mutation. More recently,\nneural networks have been employed to learn the entire analytical model, i.e.,\nits structure and coefficients, using regularized gradient-based optimization.\nAlthough this approach tunes the model's coefficients better, it is prone to\npremature convergence to suboptimal model structures. Here, we propose a\nneuro-evolutionary symbolic regression method that combines the strengths of\nevolutionary-based search for optimal neural network (NN) topologies with\ngradient-based tuning of the network's parameters. Due to the inherent high\ncomputational demand of evolutionary algorithms, it is not feasible to learn\nthe parameters of every candidate NN topology to full convergence. Thus, our\nmethod employs a memory-based strategy and population perturbations to enhance\nexploitation and reduce the risk of being trapped in suboptimal NNs. In this\nway, each NN topology can be trained using only a short sequence of\nbackpropagation iterations. The proposed method was experimentally evaluated on\nthree real-world test problems and has been shown to outperform other NN-based\napproaches regarding the quality of the models obtained."}
{"id": "2411.12308", "pdf": "https://arxiv.org/pdf/2411.12308", "abs": "https://arxiv.org/abs/2411.12308", "authors": ["Christel Grimaud", "Dominique Longin", "Andreas Herzig"], "title": "SNN-Based Online Learning of Concepts and Action Laws in an Open World", "categories": ["cs.AI", "cs.LG", "cs.NE", "cs.RO"], "comment": null, "summary": "We present the architecture of a fully autonomous, bio-inspired cognitive\nagent built around a spiking neural network (SNN) implementing the agent's\nsemantic memory. This agent explores its universe and learns concepts of\nobjects/situations and of its own actions in a one-shot manner. While\nobject/situation concepts are unary, action concepts are triples made up of an\ninitial situation, a motor activity, and an outcome. They embody the agent's\nknowledge of its universe's action laws. Both kinds of concepts have different\ndegrees of generality. To make decisions the agent queries its semantic memory\nfor the expected outcomes of envisaged actions and chooses the action to take\non the basis of these predictions. Our experiments show that the agent handles\nnew situations by appealing to previously learned general concepts and rapidly\nmodifies its concepts to adapt to environment changes."}
{"id": "2411.18203", "pdf": "https://arxiv.org/pdf/2411.18203", "abs": "https://arxiv.org/abs/2411.18203", "authors": ["Di Zhang", "Junxian Li", "Jingdi Lei", "Xunzhi Wang", "Yujie Liu", "Zonglin Yang", "Jiatong Li", "Weida Wang", "Suorong Yang", "Jianbo Wu", "Peng Ye", "Wanli Ouyang", "Dongzhan Zhou"], "title": "Critic-V: VLM Critics Help Catch VLM Errors in Multimodal Reasoning", "categories": ["cs.CV", "cs.CL"], "comment": "16 pages, 11 figures", "summary": "Vision-language models (VLMs) have shown remarkable advancements in\nmultimodal reasoning tasks. However, they still often generate inaccurate or\nirrelevant responses due to issues like hallucinated image understandings or\nunrefined reasoning paths. To address these challenges, we introduce Critic-V,\na novel framework inspired by the Actor-Critic paradigm to boost the reasoning\ncapability of VLMs. This framework decouples the reasoning process and critic\nprocess by integrating two independent components: the Reasoner, which\ngenerates reasoning paths based on visual and textual inputs, and the Critic,\nwhich provides constructive critique to refine these paths. In this approach,\nthe Reasoner generates reasoning responses according to text prompts, which can\nevolve iteratively as a policy based on feedback from the Critic. This\ninteraction process was theoretically driven by a reinforcement learning\nframework where the Critic offers natural language critiques instead of scalar\nrewards, enabling more nuanced feedback to boost the Reasoner's capability on\ncomplex reasoning tasks. The Critic model is trained using Direct Preference\nOptimization (DPO), leveraging a preference dataset of critiques ranked by\nRule-based Reward~(RBR) to enhance its critic capabilities. Evaluation results\nshow that the Critic-V framework significantly outperforms existing methods,\nincluding GPT-4V, on 5 out of 8 benchmarks, especially regarding reasoning\naccuracy and efficiency. Combining a dynamic text-based policy for the Reasoner\nand constructive feedback from the preference-optimized Critic enables a more\nreliable and context-sensitive multimodal reasoning process. Our approach\nprovides a promising solution to enhance the reliability of VLMs, improving\ntheir performance in real-world reasoning-heavy multimodal applications such as\nautonomous driving and embodied intelligence."}
{"id": "2407.05238", "pdf": "https://arxiv.org/pdf/2407.05238", "abs": "https://arxiv.org/abs/2407.05238", "authors": ["Jiahao Nie", "Fei Xie", "Sifan Zhou", "Xueyi Zhou", "Dong-Kyu Chae", "Zhiwei He"], "title": "P2P: Part-to-Part Motion Cues Guide a Strong Tracking Framework for LiDAR Point Clouds", "categories": ["cs.CV"], "comment": "Accept by IJCV", "summary": "3D single object tracking (SOT) methods based on appearance matching has long\nsuffered from insufficient appearance information incurred by incomplete,\ntextureless and semantically deficient LiDAR point clouds. While motion\nparadigm exploits motion cues instead of appearance matching for tracking, it\nincurs complex multi-stage processing and segmentation module. In this paper,\nwe first provide in-depth explorations on motion paradigm, which proves that\n(\\textbf{i}) it is feasible to directly infer target relative motion from point\nclouds across consecutive frames; (\\textbf{ii}) fine-grained information\ncomparison between consecutive point clouds facilitates target motion modeling.\nWe thereby propose to perform part-to-part motion modeling for consecutive\npoint clouds and introduce a novel tracking framework, termed \\textbf{P2P}. The\nnovel framework fuses each corresponding part information between consecutive\npoint clouds, effectively exploring detailed information changes and thus\nmodeling accurate target-related motion cues. Following this framework, we\npresent P2P-point and P2P-voxel models, incorporating implicit and explicit\npart-to-part motion modeling by point- and voxel-based representation,\nrespectively. Without bells and whistles, P2P-voxel sets a new state-of-the-art\nperformance ($\\sim$\\textbf{89\\%}, \\textbf{72\\%} and \\textbf{63\\%} precision on\nKITTI, NuScenes and Waymo Open Dataset, respectively). Moreover, under the same\npoint-based representation, P2P-point outperforms the previous motion tracker\nM$^2$Track by \\textbf{3.3\\%} and \\textbf{6.7\\%} on the KITTI and NuScenes,\nwhile running at a considerably high speed of \\textbf{107 Fps} on a single\nRTX3090 GPU. The source code and pre-trained models are available at\nhttps://github.com/haooozi/P2P."}
{"id": "2504.16555", "pdf": "https://arxiv.org/pdf/2504.16555", "abs": "https://arxiv.org/abs/2504.16555", "authors": ["Eugenio Clerico", "Hamish Flynn", "Wojciech Kotłowski", "Gergely Neu"], "title": "Confidence Sequences for Generalized Linear Models via Regret Analysis", "categories": ["math.ST", "cs.LG", "stat.ML", "stat.TH"], "comment": null, "summary": "We develop a methodology for constructing confidence sets for parameters of\nstatistical models via a reduction to sequential prediction. Our key\nobservation is that for any generalized linear model (GLM), one can construct\nan associated game of sequential probability assignment such that achieving low\nregret in the game implies a high-probability upper bound on the excess\nlikelihood of the true parameter of the GLM. This allows us to develop a scheme\nthat we call online-to-confidence-set conversions, which effectively reduces\nthe problem of proving the desired statistical claim to an algorithmic\nquestion. We study two varieties of this conversion scheme: 1) analytical\nconversions that only require proving the existence of algorithms with low\nregret and provide confidence sets centered at the maximum-likelihood estimator\n2) algorithmic conversions that actively leverage the output of the online\nalgorithm to construct confidence sets (and may be centered at other,\nadaptively constructed point estimators). The resulting methodology recovers\nall state-of-the-art confidence set constructions within a single framework,\nand also provides several new types of confidence sets that were previously\nunknown in the literature."}
{"id": "2412.12661", "pdf": "https://arxiv.org/pdf/2412.12661", "abs": "https://arxiv.org/abs/2412.12661", "authors": ["Hritik Bansal", "Daniel Israel", "Siyan Zhao", "Shufan Li", "Tung Nguyen", "Aditya Grover"], "title": "MedMax: Mixed-Modal Instruction Tuning for Training Biomedical Assistants", "categories": ["cs.AI", "cs.CL", "cs.CV"], "comment": "29 pages", "summary": "Recent advancements in mixed-modal generative have opened new avenues for\ndeveloping unified biomedical assistants capable of analyzing biomedical\nimages, answering complex questions about them, and generating multimodal\npatient reports. However, existing datasets face challenges such as small\nsizes, limited coverage of biomedical tasks and domains, and a reliance on\nnarrow sources. To address these gaps, we present MedMax, a large-scale\nmultimodal biomedical instruction-tuning dataset for mixed-modal foundation\nmodels. With 1.47 million instances, MedMax encompasses a diverse range of\ntasks, including interleaved image-text generation, biomedical image captioning\nand generation, visual chat, and report understanding. These tasks span\nknowledge across diverse biomedical domains, including radiology and\nhistopathology, grounded in medical papers and YouTube videos. Subsequently, we\nfine-tune a mixed-modal foundation model on the MedMax dataset, achieving\nsignificant performance improvements: a 26% gain over the Chameleon model and\nan 18.3% improvement over GPT-4o across 12 downstream biomedical visual\nquestion-answering tasks. Finally, we introduce a unified evaluation suite for\nbiomedical tasks to guide the development of mixed-modal biomedical AI\nassistants. The data, model, and code is available at\nhttps://mint-medmax.github.io/."}
{"id": "2412.06412", "pdf": "https://arxiv.org/pdf/2412.06412", "abs": "https://arxiv.org/abs/2412.06412", "authors": ["Cunshi Wang", "Xinjie Hu", "Yu Zhang", "Xunhao Chen", "Pengliang Du", "Yiming Mao", "Rui Wang", "Yuyang Li", "Ying Wu", "Hang Yang", "Yansong Li", "Beichuan Wang", "Haiyang Mu", "Zheng Wang", "Jianfeng Tian", "Liang Ge", "Yongna Mao", "Shengming Li", "Xiaomeng Lu", "Jinhang Zou", "Yang Huang", "Ningchen Sun", "Jie Zheng", "Min He", "Yu Bai", "Junjie Jin", "Hong Wu", "Jifeng Liu"], "title": "StarWhisper Telescope: Agent-Based Observation Assistant System to Approach AI Astrophysicist", "categories": ["astro-ph.IM", "cs.AI", "cs.CL"], "comment": "36 pages", "summary": "With the rapid advancements in Large Language Models (LLMs), LLM-based agents\nhave introduced convenient and user-friendly methods for leveraging tools\nacross various domains. In the field of astronomical observation, the\nconstruction of new telescopes has significantly increased astronomers'\nworkload. Deploying LLM-powered agents can effectively alleviate this burden\nand reduce the costs associated with training personnel. Within the Nearby\nGalaxy Supernovae Survey (NGSS) project, which encompasses eight telescopes\nacross three observation sites, aiming to find the transients from the galaxies\nin 50 mpc, we have developed the \\textbf{StarWhisper Telescope System} to\nmanage the entire observation process. This system automates tasks such as\ngenerating observation lists, conducting observations, analyzing data, and\nproviding feedback to the observer. Observation lists are customized for\ndifferent sites and strategies to ensure comprehensive coverage of celestial\nobjects. After manual verification, these lists are uploaded to the telescopes\nvia the agents in the system, which initiates observations upon neutral\nlanguage. The observed images are analyzed in real-time, and the transients are\npromptly communicated to the observer. The agent modifies them into a real-time\nfollow-up observation proposal and send to the Xinglong observatory group chat,\nthen add them to the next-day observation lists. Additionally, the integration\nof AI agents within the system provides online accessibility, saving\nastronomers' time and encouraging greater participation from amateur\nastronomers in the NGSS project."}
{"id": "2407.15842", "pdf": "https://arxiv.org/pdf/2407.15842", "abs": "https://arxiv.org/abs/2407.15842", "authors": ["Ruixiang Jiang", "Changwen Chen"], "title": "DiffArtist: Towards Structure and Appearance Controllable Image Stylization", "categories": ["cs.CV", "cs.GR"], "comment": "Homepage: https://DiffusionArtist.github.io", "summary": "Artistic style includes both structural and appearance elements. Existing\nneural stylization techniques primarily focus on transferring appearance\nfeatures such as color and texture, often neglecting the equally crucial aspect\nof structural stylization. In this paper, we present a comprehensive study on\nthe simultaneous stylization of structure and appearance of 2D images.\nSpecifically, we introduce DiffArtist, which, to the best of our knowledge, is\nthe first stylization method to allow for dual controllability over structure\nand appearance. Our key insight is to represent structure and appearance as\nseparate diffusion processes to achieve complete disentanglement without\nrequiring any training, thereby endowing users with unprecedented\ncontrollability for both components. The evaluation of stylization of both\nappearance and structure, however, remains challenging as it necessitates\nsemantic understanding. To this end, we further propose a Multimodal LLM-based\nstyle evaluator, which better aligns with human preferences than metrics\nlacking semantic understanding. With this powerful evaluator, we conduct\nextensive analysis, demonstrating that DiffArtist achieves superior style\nfidelity, editability, and structure-appearance disentanglement. These merits\nmake DiffArtist a highly versatile solution for creative applications. Project\nhomepage: https://github.com/songrise/Artist."}
{"id": "2504.16588", "pdf": "https://arxiv.org/pdf/2504.16588", "abs": "https://arxiv.org/abs/2504.16588", "authors": ["Defne E. Ozan", "Andrea Nóvoa", "Luca Magri"], "title": "Data-Assimilated Model-Based Reinforcement Learning for Partially Observed Chaotic Flows", "categories": ["eess.SY", "cs.LG", "cs.SY", "physics.flu-dyn"], "comment": null, "summary": "The goal of many applications in energy and transport sectors is to control\nturbulent flows. However, because of chaotic dynamics and high dimensionality,\nthe control of turbulent flows is exceedingly difficult. Model-free\nreinforcement learning (RL) methods can discover optimal control policies by\ninteracting with the environment, but they require full state information,\nwhich is often unavailable in experimental settings. We propose a\ndata-assimilated model-based RL (DA-MBRL) framework for systems with partial\nobservability and noisy measurements. Our framework employs a control-aware\nEcho State Network for data-driven prediction of the dynamics, and integrates\ndata assimilation with an Ensemble Kalman Filter for real-time state\nestimation. An off-policy actor-critic algorithm is employed to learn optimal\ncontrol strategies from state estimates. The framework is tested on the\nKuramoto-Sivashinsky equation, demonstrating its effectiveness in stabilizing a\nspatiotemporally chaotic flow from noisy and partial measurements."}
{"id": "2503.13657", "pdf": "https://arxiv.org/pdf/2503.13657", "abs": "https://arxiv.org/abs/2503.13657", "authors": ["Mert Cemri", "Melissa Z. Pan", "Shuyi Yang", "Lakshya A. Agrawal", "Bhavya Chopra", "Rishabh Tiwari", "Kurt Keutzer", "Aditya Parameswaran", "Dan Klein", "Kannan Ramchandran", "Matei Zaharia", "Joseph E. Gonzalez", "Ion Stoica"], "title": "Why Do Multi-Agent LLM Systems Fail?", "categories": ["cs.AI"], "comment": "ArXiv v2", "summary": "Despite growing enthusiasm for Multi-Agent LLM Systems (MAS), their\nperformance gains on popular benchmarks often remain minimal compared with\nsingle-agent frameworks. This gap highlights the need to systematically analyze\nthe challenges hindering MAS effectiveness.\n  We present MAST (Multi-Agent System Failure Taxonomy), the first empirically\ngrounded taxonomy designed to understand MAS failures. We analyze seven popular\nMAS frameworks across over 200 tasks, involving six expert human annotators.\nThrough this process, we identify 14 unique failure modes, organized into 3\noverarching categories, (i) specification issues, (ii) inter-agent\nmisalignment, and (iii) task verification. MAST emerges iteratively from\nrigorous inter-annotator agreement studies, achieving a Cohen's Kappa score of\n0.88. To support scalable evaluation, we develop a validated LLM-as-a-Judge\npipeline integrated with MAST. We leverage two case studies to demonstrate\nMAST's practical utility in analyzing failures and guiding MAS development. Our\nfindings reveal that identified failures require more complex solutions,\nhighlighting a clear roadmap for future research. We open source our\ncomprehensive dataset and LLM annotator to facilitate further development of\nMAS."}
{"id": "2503.11662", "pdf": "https://arxiv.org/pdf/2503.11662", "abs": "https://arxiv.org/abs/2503.11662", "authors": ["Runzhi Wang", "Prianka Sengupta", "Cristhian Roman-Vicharra", "Yiran Chen", "Jiang Hu"], "title": "Lorecast: Layout-Aware Performance and Power Forecasting from Natural Language", "categories": ["cs.AR", "cs.AI", "cs.CL", "cs.LG"], "comment": null, "summary": "In chip design planning, obtaining reliable performance and power forecasts\nfor various design options is of critical importance. Traditionally, this\ninvolves using system-level models, which often lack accuracy, or trial\nsynthesis, which is both labor-intensive and time-consuming. We introduce a new\nmethodology, called Lorecast, which accepts English prompts as input to rapidly\ngenerate layout-aware performance and power estimates. This approach bypasses\nthe need for HDL code development and synthesis, making it both fast and\nuser-friendly. Experimental results demonstrate that Lorecast achieves accuracy\nwithin a few percent of error compared to post-layout analysis, while\nsignificantly reducing turnaround time."}
{"id": "2408.11208", "pdf": "https://arxiv.org/pdf/2408.11208", "abs": "https://arxiv.org/abs/2408.11208", "authors": ["Alex N. Wang", "Christopher Hoang", "Yuwen Xiong", "Yann LeCun", "Mengye Ren"], "title": "PooDLe: Pooled and dense self-supervised learning from naturalistic videos", "categories": ["cs.CV", "cs.LG"], "comment": "Project page: https://agenticlearning.ai/poodle/", "summary": "Self-supervised learning has driven significant progress in learning from\nsingle-subject, iconic images. However, there are still unanswered questions\nabout the use of minimally-curated, naturalistic video data, which contain\ndense scenes with many independent objects, imbalanced class distributions, and\nvarying object sizes. In this paper, we propose PooDLe, a self-supervised\nlearning method that combines an invariance-based objective on pooled\nrepresentations with a dense SSL objective that enforces equivariance to\noptical flow warping. Our results show that a unified objective applied at\nmultiple feature scales is essential for learning effective image\nrepresentations from naturalistic videos. We validate our method with\nexperiments on the BDD100K driving video dataset and the Walking Tours\nfirst-person video dataset, demonstrating its ability to capture spatial\nunderstanding from a dense objective and semantic understanding via a pooled\nrepresentation objective."}
{"id": "2504.16595", "pdf": "https://arxiv.org/pdf/2504.16595", "abs": "https://arxiv.org/abs/2504.16595", "authors": ["Gojko Perovic", "Nuno Ferreira Duarte", "Atabak Dehban", "Gonçalo Teixeira", "Egidio Falotico", "José Santos-Victor"], "title": "HERB: Human-augmented Efficient Reinforcement learning for Bin-packing", "categories": ["cs.RO", "cs.LG"], "comment": "7 pages, 5 Figures", "summary": "Packing objects efficiently is a fundamental problem in logistics, warehouse\nautomation, and robotics. While traditional packing solutions focus on\ngeometric optimization, packing irregular, 3D objects presents significant\nchallenges due to variations in shape and stability. Reinforcement\nLearning~(RL) has gained popularity in robotic packing tasks, but training\npurely from simulation can be inefficient and computationally expensive. In\nthis work, we propose HERB, a human-augmented RL framework for packing\nirregular objects. We first leverage human demonstrations to learn the best\nsequence of objects to pack, incorporating latent factors such as space\noptimization, stability, and object relationships that are difficult to model\nexplicitly. Next, we train a placement algorithm that uses visual information\nto determine the optimal object positioning inside a packing container. Our\napproach is validated through extensive performance evaluations, analyzing both\npacking efficiency and latency. Finally, we demonstrate the real-world\nfeasibility of our method on a robotic system. Experimental results show that\nour method outperforms geometric and purely RL-based approaches by leveraging\nhuman intuition, improving both packing robustness and adaptability. This work\nhighlights the potential of combining human expertise-driven RL to tackle\ncomplex real-world packing challenges in robotic systems."}
{"id": "2503.16743", "pdf": "https://arxiv.org/pdf/2503.16743", "abs": "https://arxiv.org/abs/2503.16743", "authors": ["Alberto Hernández-Espinosa", "Luan Ozelim", "Felipe S. Abrahão", "Hector Zenil"], "title": "SuperARC: An Agnostic Test for Narrow, General, and Super Intelligence Based On the Principles of Recursive Compression and Algorithmic Probability", "categories": ["cs.AI", "cs.IT", "math.IT"], "comment": "51 pages + Technical Supplementary Information, 79 pages total", "summary": "We introduce an open-ended test grounded in algorithmic probability that can\navoid benchmark contamination in the quantitative evaluation of frontier models\nin the context of their Artificial General Intelligence (AGI) and\nSuperintelligence (ASI) claims. Unlike other tests, this test does not rely on\nstatistical compression methods (such as GZIP or LZW), which are more closely\nrelated to Shannon entropy than to Kolmogorov complexity and are not able to\ntest beyond simple pattern matching. The test challenges aspects of AI, in\nparticular LLMs, related to features of intelligence of fundamental nature such\nas synthesis and model creation in the context of inverse problems (generating\nnew knowledge from observation). We argue that metrics based on model\nabstraction and abduction (optimal Bayesian `inference') for predictive\n`planning' can provide a robust framework for testing intelligence, including\nnatural intelligence (human and animal), narrow AI, AGI, and ASI. We found that\nLLM model versions tend to be fragile and incremental as a result of\nmemorisation only with progress likely driven by the size of training data. The\nresults were compared with a hybrid neurosymbolic approach that theoretically\nguarantees universal intelligence based on the principles of algorithmic\nprobability and Kolmogorov complexity. The method outperforms LLMs in a\nproof-of-concept on short binary sequences. We prove that compression is\nequivalent and directly proportional to a system's predictive power and vice\nversa. That is, if a system can better predict it can better compress, and if\nit can better compress, then it can better predict. Our findings strengthen the\nsuspicion regarding the fundamental limitations of LLMs, exposing them as\nsystems optimised for the perception of mastery over human language."}
{"id": "2504.00044", "pdf": "https://arxiv.org/pdf/2504.00044", "abs": "https://arxiv.org/abs/2504.00044", "authors": ["Riccardo Cantini", "Fabrizio Marozzo", "Alessio Orsino", "Domenico Talia", "Paolo Trunfio"], "title": "Dynamic hashtag recommendation in social media with trend shift detection and adaptation", "categories": ["cs.SI", "cs.CL", "cs.DC", "cs.NE"], "comment": null, "summary": "Hashtag recommendation systems have emerged as a key tool for automatically\nsuggesting relevant hashtags and enhancing content categorization and search.\nHowever, existing static models struggle to adapt to the highly dynamic nature\nof social media conversations, where new hashtags constantly emerge and\nexisting ones undergo semantic shifts. To address these challenges, this paper\nintroduces H-ADAPTS (Hashtag recommendAtion by Detecting and adAPting to Trend\nShifts), a dynamic hashtag recommendation methodology that employs a\ntrend-aware mechanism to detect shifts in hashtag usage-reflecting evolving\ntrends and topics within social media conversations-and triggers efficient\nmodel adaptation based on a (small) set of recent posts. Additionally, the\nApache Storm framework is leveraged to support scalable and fault-tolerant\nanalysis of high-velocity social data, enabling the timely detection of trend\nshifts. Experimental results from two real-world case studies, including the\nCOVID-19 pandemic and the 2020 US presidential election, demonstrate the\neffectiveness of H-ADAPTS in providing timely and relevant hashtag\nrecommendations by adapting to emerging trends, significantly outperforming\nexisting solutions."}
{"id": "2409.16706", "pdf": "https://arxiv.org/pdf/2409.16706", "abs": "https://arxiv.org/abs/2409.16706", "authors": ["Youngwan Jin", "Incheol Park", "Hanbin Song", "Hyeongjin Ju", "Yagiz Nalcakan", "Shiho Kim"], "title": "Pix2Next: Leveraging Vision Foundation Models for RGB to NIR Image Translation", "categories": ["cs.CV", "cs.AI"], "comment": "19 pages,12 figures", "summary": "This paper proposes Pix2Next, a novel image-to-image translation framework\ndesigned to address the challenge of generating high-quality Near-Infrared\n(NIR) images from RGB inputs. Our approach leverages a state-of-the-art Vision\nFoundation Model (VFM) within an encoder-decoder architecture, incorporating\ncross-attention mechanisms to enhance feature integration. This design captures\ndetailed global representations and preserves essential spectral\ncharacteristics, treating RGB-to-NIR translation as more than a simple domain\ntransfer problem. A multi-scale PatchGAN discriminator ensures realistic image\ngeneration at various detail levels, while carefully designed loss functions\ncouple global context understanding with local feature preservation. We\nperformed experiments on the RANUS dataset to demonstrate Pix2Next's advantages\nin quantitative metrics and visual quality, improving the FID score by 34.81%\ncompared to existing methods. Furthermore, we demonstrate the practical utility\nof Pix2Next by showing improved performance on a downstream object detection\ntask using generated NIR data to augment limited real NIR datasets. The\nproposed approach enables the scaling up of NIR datasets without additional\ndata acquisition or annotation efforts, potentially accelerating advancements\nin NIR-based computer vision applications."}
{"id": "2504.16688", "pdf": "https://arxiv.org/pdf/2504.16688", "abs": "https://arxiv.org/abs/2504.16688", "authors": ["Nahshon Mokua", "Obiri", "Kristof", "Van Laerhoven"], "title": "A Statistical Evaluation of Indoor LoRaWAN Environment-Aware Propagation for 6G: MLR, ANOVA, and Residual Distribution Analysis", "categories": ["cs.NI", "cs.LG", "eess.SP"], "comment": "\\c{opyright} 2025 IEEE. Personal use of this material is permitted.\n  This is the accepted version of the article: To appear in the 2025 Joint\n  European Conference on Networks and Communications & 6G Summit (EuCNC/6G\n  Summit)", "summary": "Modeling path loss in indoor LoRaWAN technology deployments is inherently\nchallenging due to structural obstructions, occupant density and activities,\nand fluctuating environmental conditions. This study proposes a two-stage\napproach to capture and analyze these complexities using an extensive dataset\nof 1,328,334 field measurements collected over six months in a single-floor\noffice at the University of Siegen's Hoelderlinstrasse Campus, Germany. First,\nwe implement a multiple linear regression model that includes traditional\npropagation metrics (distance, structural walls) and an extension with proposed\nenvironmental variables (relative humidity, temperature, carbon dioxide,\nparticulate matter, and barometric pressure). Using analysis of variance, we\ndemonstrate that adding these environmental factors can reduce unexplained\nvariance by 42.32 percent. Secondly, we examine residual distributions by\nfitting five candidate probability distributions: Normal, Skew-Normal, Cauchy,\nStudent's t, and Gaussian Mixture Models with one to five components. Our\nresults show that a four-component Gaussian Mixture Model captures the residual\nheterogeneity of indoor signal propagation most accurately, significantly\noutperforming single-distribution approaches. Given the push toward\nultra-reliable, context-aware communications in 6G networks, our analysis shows\nthat environment-aware modeling can substantially improve LoRaWAN network\ndesign in dynamic indoor IoT deployments."}
{"id": "2503.17604", "pdf": "https://arxiv.org/pdf/2503.17604", "abs": "https://arxiv.org/abs/2503.17604", "authors": ["Vignesh Prabhakar", "Md Amirul Islam", "Adam Atanas", "Yao-Ting Wang", "Joah Han", "Aastha Jhunjhunwala", "Rucha Apte", "Robert Clark", "Kang Xu", "Zihan Wang", "Kai Liu"], "title": "OmniScience: A Domain-Specialized LLM for Scientific Reasoning and Discovery", "categories": ["cs.AI"], "comment": null, "summary": "Large Language Models (LLMs) have demonstrated remarkable potential in\nadvancing scientific knowledge and addressing complex challenges. In this work,\nwe introduce OmniScience, a specialized large reasoning model for general\nscience, developed through three key components: (1) domain adaptive\npretraining on a carefully curated corpus of scientific literature, (2)\ninstruction tuning on a specialized dataset to guide the model in following\ndomain-specific tasks, and (3) reasoning-based knowledge distillation through\nfine-tuning to significantly enhance its ability to generate contextually\nrelevant and logically sound responses. We demonstrate the versatility of\nOmniScience by developing a battery agent that efficiently ranks molecules as\npotential electrolyte solvents or additives. Comprehensive evaluations reveal\nthat OmniScience is competitive with state-of-the-art large reasoning models on\nthe GPQA Diamond and domain-specific battery benchmarks, while outperforming\nall public reasoning and non-reasoning models with similar parameter counts. We\nfurther demonstrate via ablation experiments that domain adaptive pretraining\nand reasoning-based knowledge distillation are critical to attain our\nperformance levels, across benchmarks."}
{"id": "2504.08619", "pdf": "https://arxiv.org/pdf/2504.08619", "abs": "https://arxiv.org/abs/2504.08619", "authors": ["Zhiqiu Xia", "Lang Zhu", "Bingzhe Li", "Feng Chen", "Qiannan Li", "Chunhua Liao", "Feiyi Wang", "Hang Liu"], "title": "Analyzing 16,193 LLM Papers for Fun and Profits", "categories": ["cs.DL", "cs.CL"], "comment": null, "summary": "Large Language Models (LLMs) are reshaping the landscape of computer science\nresearch, driving significant shifts in research priorities across diverse\nconferences and fields. This study provides a comprehensive analysis of the\npublication trend of LLM-related papers in 77 top-tier computer science\nconferences over the past six years (2019-2024). We approach this analysis from\nfour distinct perspectives: (1) We investigate how LLM research is driving\ntopic shifts within major conferences. (2) We adopt a topic modeling approach\nto identify various areas of LLM-related topic growth and reveal the topics of\nconcern at different conferences. (3) We explore distinct contribution patterns\nof academic and industrial institutions. (4) We study the influence of national\norigins on LLM development trajectories. Synthesizing the findings from these\ndiverse analytical angles, we derive ten key insights that illuminate the\ndynamics and evolution of the LLM research ecosystem."}
{"id": "2410.17988", "pdf": "https://arxiv.org/pdf/2410.17988", "abs": "https://arxiv.org/abs/2410.17988", "authors": ["Zhiwu Zheng", "Lauren Mentzer", "Berk Iskender", "Michael Price", "Colm Prendergast", "Audren Cloitre"], "title": "Semantic Segmentation and Scene Reconstruction of RGB-D Image Frames: An End-to-End Modular Pipeline for Robotic Applications", "categories": ["cs.CV", "cs.RO"], "comment": null, "summary": "Robots operating in unstructured environments require a comprehensive\nunderstanding of their surroundings, necessitating geometric and semantic\ninformation from sensor data. Traditional RGB-D processing pipelines focus\nprimarily on geometric reconstruction, limiting their ability to support\nadvanced robotic perception, planning, and interaction. A key challenge is the\nlack of generalized methods for segmenting RGB-D data into semantically\nmeaningful components while maintaining accurate geometric representations. We\nintroduce a novel end-to-end modular pipeline that integrates state-of-the-art\nsemantic segmentation, human tracking, point-cloud fusion, and scene\nreconstruction. Our approach improves semantic segmentation accuracy by\nleveraging the foundational segmentation model SAM2 with a hybrid method that\ncombines its mask generation with a semantic classification model, resulting in\nsharper masks and high classification accuracy. Compared to SegFormer and\nOneFormer, our method achieves a similar semantic segmentation accuracy (mIoU\nof 47.0% vs 45.9% in the ADE20K dataset) but provides much more precise object\nboundaries. Additionally, our human tracking algorithm interacts with the\nsegmentation enabling continuous tracking even when objects leave and re-enter\nthe frame by object re-identification. Our point cloud fusion approach reduces\ncomputation time by 1.81x while maintaining a small mean reconstruction error\nof 25.3 mm by leveraging the semantic information. We validate our approach on\nbenchmark datasets and real-world Kinect RGB-D data, demonstrating improved\nefficiency, accuracy, and usability. Our structured representation, stored in\nthe Universal Scene Description (USD) format, supports efficient querying,\nvisualization, and robotic simulation, making it practical for real-world\ndeployment."}
{"id": "2504.16732", "pdf": "https://arxiv.org/pdf/2504.16732", "abs": "https://arxiv.org/abs/2504.16732", "authors": ["Yanjie Wu", "Yuhao Ji", "Saiho Lee", "Juniad Akram", "Ali Braytee", "Ali Anaissi"], "title": "Simplified Swarm Learning Framework for Robust and Scalable Diagnostic Services in Cancer Histopathology", "categories": ["cs.DC", "cs.LG"], "comment": "8 pages, 4 figures, 2025 International Conference on Computational\n  Science", "summary": "The complexities of healthcare data, including privacy concerns, imbalanced\ndatasets, and interoperability issues, necessitate innovative machine learning\nsolutions. Swarm Learning (SL), a decentralized alternative to Federated\nLearning, offers privacy-preserving distributed training, but its reliance on\nblockchain technology hinders accessibility and scalability. This paper\nintroduces a \\textit{Simplified Peer-to-Peer Swarm Learning (P2P-SL) Framework}\ntailored for resource-constrained environments. By eliminating blockchain\ndependencies and adopting lightweight peer-to-peer communication, the proposed\nframework ensures robust model synchronization while maintaining data privacy.\nApplied to cancer histopathology, the framework integrates optimized\npre-trained models, such as TorchXRayVision, enhanced with DenseNet decoders,\nto improve diagnostic accuracy. Extensive experiments demonstrate the\nframework's efficacy in handling imbalanced and biased datasets, achieving\ncomparable performance to centralized models while preserving privacy. This\nstudy paves the way for democratizing advanced machine learning in healthcare,\noffering a scalable, accessible, and efficient solution for privacy-sensitive\ndiagnostic applications."}
{"id": "2504.14128", "pdf": "https://arxiv.org/pdf/2504.14128", "abs": "https://arxiv.org/abs/2504.14128", "authors": ["Christopher Zhang Cui", "Xingdi Yuan", "Ziang Xiao", "Prithviraj Ammanabrolu", "Marc-Alexandre Côté"], "title": "TALES: Text Adventure Learning Environment Suite", "categories": ["cs.AI", "cs.CL"], "comment": null, "summary": "Reasoning is an essential skill to enable Large Language Models (LLMs) to\ninteract with the world. As tasks become more complex, they demand increasingly\nsophisticated and diverse reasoning capabilities for sequential\ndecision-making, requiring structured reasoning over the context history to\ndetermine the next best action. We introduce TALES, a diverse collection of\nsynthetic and human-written text-adventure games designed to challenge and\nevaluate diverse reasoning capabilities. We present results over a range of\nLLMs, open- and closed-weights, performing a qualitative analysis on the top\nperforming models. Despite an impressive showing on synthetic games, even the\ntop LLM-driven agents fail to achieve 15% on games designed for human\nenjoyment. Code and visualization of the experiments can be found at\nhttps://microsoft.github.io/tales."}
{"id": "2504.13955", "pdf": "https://arxiv.org/pdf/2504.13955", "abs": "https://arxiv.org/abs/2504.13955", "authors": ["Suhas BN", "Dominik Mattioli", "Saeed Abdullah", "Rosa I. Arriaga", "Chris W. Wiese", "Andrew M. Sherrill"], "title": "Thousand Voices of Trauma: A Large-Scale Synthetic Dataset for Modeling Prolonged Exposure Therapy Conversations", "categories": ["cs.CY", "cs.AI", "cs.CL", "cs.HC", "cs.LG", "68T50", "I.2.7; H.5.2"], "comment": "14 pages, 6 figures", "summary": "The advancement of AI systems for mental health support is hindered by\nlimited access to therapeutic conversation data, particularly for trauma\ntreatment. We present Thousand Voices of Trauma, a synthetic benchmark dataset\nof 3,000 therapy conversations based on Prolonged Exposure therapy protocols\nfor Post-traumatic Stress Disorder (PTSD). The dataset comprises 500 unique\ncases, each explored through six conversational perspectives that mirror the\nprogression of therapy from initial anxiety to peak distress to emotional\nprocessing. We incorporated diverse demographic profiles (ages 18-80, M=49.3,\n49.4% male, 44.4% female, 6.2% non-binary), 20 trauma types, and 10\ntrauma-related behaviors using deterministic and probabilistic generation\nmethods. Analysis reveals realistic distributions of trauma types (witnessing\nviolence 10.6%, bullying 10.2%) and symptoms (nightmares 23.4%, substance abuse\n20.8%). Clinical experts validated the dataset's therapeutic fidelity,\nhighlighting its emotional depth while suggesting refinements for greater\nauthenticity. We also developed an emotional trajectory benchmark with\nstandardized metrics for evaluating model responses. This privacy-preserving\ndataset addresses critical gaps in trauma-focused mental health data, offering\na valuable resource for advancing both patient-facing applications and\nclinician training tools."}
{"id": "2411.17163", "pdf": "https://arxiv.org/pdf/2411.17163", "abs": "https://arxiv.org/abs/2411.17163", "authors": ["Jingkai Wang", "Jue Gong", "Lin Zhang", "Zheng Chen", "Xing Liu", "Hong Gu", "Yutong Liu", "Yulun Zhang", "Xiaokang Yang"], "title": "OSDFace: One-Step Diffusion Model for Face Restoration", "categories": ["cs.CV"], "comment": "Accepted to CVPR 2025. The code and model will be available at\n  https://github.com/jkwang28/OSDFace", "summary": "Diffusion models have demonstrated impressive performance in face\nrestoration. Yet, their multi-step inference process remains computationally\nintensive, limiting their applicability in real-world scenarios. Moreover,\nexisting methods often struggle to generate face images that are harmonious,\nrealistic, and consistent with the subject's identity. In this work, we propose\nOSDFace, a novel one-step diffusion model for face restoration. Specifically,\nwe propose a visual representation embedder (VRE) to better capture prior\ninformation and understand the input face. In VRE, low-quality faces are\nprocessed by a visual tokenizer and subsequently embedded with a\nvector-quantized dictionary to generate visual prompts. Additionally, we\nincorporate a facial identity loss derived from face recognition to further\nensure identity consistency. We further employ a generative adversarial network\n(GAN) as a guidance model to encourage distribution alignment between the\nrestored face and the ground truth. Experimental results demonstrate that\nOSDFace surpasses current state-of-the-art (SOTA) methods in both visual\nquality and quantitative metrics, generating high-fidelity, natural face images\nwith high identity consistency. The code and model will be released at\nhttps://github.com/jkwang28/OSDFace."}
{"id": "2504.16864", "pdf": "https://arxiv.org/pdf/2504.16864", "abs": "https://arxiv.org/abs/2504.16864", "authors": ["Manuel Quintero", "William T. Stephenson", "Advik Shreekumar", "Tamara Broderick"], "title": "Common Functional Decompositions Can Mis-attribute Differences in Outcomes Between Populations", "categories": ["stat.ME", "cs.LG", "econ.EM", "stat.ML"], "comment": "30 pages, appearing in 2nd Workshop on Navigating and Addressing Data\n  Problems for Foundation Models (DATA-FM @ ICLR 2025)", "summary": "In science and social science, we often wish to explain why an outcome is\ndifferent in two populations. For instance, if a jobs program benefits members\nof one city more than another, is that due to differences in program\nparticipants (particular covariates) or the local labor markets (outcomes given\ncovariates)? The Kitagawa-Oaxaca-Blinder (KOB) decomposition is a standard tool\nin econometrics that explains the difference in the mean outcome across two\npopulations. However, the KOB decomposition assumes a linear relationship\nbetween covariates and outcomes, while the true relationship may be\nmeaningfully nonlinear. Modern machine learning boasts a variety of nonlinear\nfunctional decompositions for the relationship between outcomes and covariates\nin one population. It seems natural to extend the KOB decomposition using these\nfunctional decompositions. We observe that a successful extension should not\nattribute the differences to covariates -- or, respectively, to outcomes given\ncovariates -- if those are the same in the two populations. Unfortunately, we\ndemonstrate that, even in simple examples, two common decompositions --\nfunctional ANOVA and Accumulated Local Effects -- can attribute differences to\noutcomes given covariates, even when they are identical in two populations. We\nprovide a characterization of when functional ANOVA misattributes, as well as a\ngeneral property that any discrete decomposition must satisfy to avoid\nmisattribution. We show that if the decomposition is independent of its input\ndistribution, it does not misattribute. We further conjecture that\nmisattribution arises in any reasonable additive decomposition that depends on\nthe distribution of the covariates."}
{"id": "2504.15610", "pdf": "https://arxiv.org/pdf/2504.15610", "abs": "https://arxiv.org/abs/2504.15610", "authors": ["Md Millat Hosen"], "title": "A LoRA-Based Approach to Fine-Tuning LLMs for Educational Guidance in Resource-Constrained Settings", "categories": ["cs.AI", "68T05 (Learning and adaptive systems), 68T07 (Artificial\n  intelligence and education)"], "comment": "18 pages, 6 figures (3 graphs + 3 flowchart/architecture diagrams),\n  submitted as a preprint for review consideration in AI for Education or\n  Machine Learning applications in low-resource settings. Includes detailed\n  experiments with LoRA and quantization methods for efficient LLM fine-tuning", "summary": "The current study describes a cost-effective method for adapting large\nlanguage models (LLMs) for academic advising with study-abroad contexts in mind\nand for application in low-resource methods for acculturation. With the\nMistral-7B-Instruct model applied with a Low-Rank Adaptation (LoRA) method and\na 4-bit quantization method, the model underwent training in two distinct\nstages related to this study's purpose to enhance domain specificity while\nmaintaining computational efficiency. In Phase 1, the model was conditioned\nwith a synthetic dataset via the Gemini Pro API, and in Phase 2, it was trained\nwith manually curated datasets from the StudyAbroadGPT project to achieve\nenhanced, contextualized responses. Technical innovations entailed\nmemory-efficient quantization, parameter-efficient adaptation, and continuous\ntraining analytics via Weights & Biases. After training, this study\ndemonstrated a reduction in training loss by 52.7%, 92% accuracy in\ndomain-specific recommendations, achieved 95% markdown-based formatting\nsupport, and a median run-rate of 100 samples per second on off-the-shelf GPU\nequipment. These findings support the effective application of\ninstruction-tuned LLMs within educational advisers, especially in low-resource\ninstitutional scenarios. Limitations included decreased generalizability and\nthe application of a synthetically generated dataset, but this framework is\nscalable for adding new multilingual-augmented and real-time academic advising\nprocesses. Future directions may include plans for the integration of\nretrieval-augmented generation, applying dynamic quantization routines, and\nconnecting to real-time academic databases to increase adaptability and\naccuracy."}
{"id": "2412.01552", "pdf": "https://arxiv.org/pdf/2412.01552", "abs": "https://arxiv.org/abs/2412.01552", "authors": ["Xingyu Liu", "Gu Wang", "Chengxi Li", "Yingyue Li", "Chenyangguang Zhang", "Ziqin Huang", "Xiangyang Ji"], "title": "GFreeDet: Exploiting Gaussian Splatting and Foundation Models for Model-free Unseen Object Detection in the BOP Challenge 2024", "categories": ["cs.CV", "cs.RO"], "comment": "CVPR 2025 CV4MR Workshop (citation style changed)", "summary": "We present GFreeDet, an unseen object detection approach that leverages\nGaussian splatting and vision Foundation models under model-free setting.\nUnlike existing methods that rely on predefined CAD templates, GFreeDet\nreconstructs objects directly from reference videos using Gaussian splatting,\nenabling robust detection of novel objects without prior 3D models. Evaluated\non the BOP-H3 benchmark, GFreeDet achieves comparable performance to CAD-based\nmethods, demonstrating the viability of model-free detection for mixed reality\n(MR) applications. Notably, GFreeDet won the best overall method and the best\nfast method awards in the model-free 2D detection track at BOP Challenge 2024."}
{"id": "2504.16879", "pdf": "https://arxiv.org/pdf/2504.16879", "abs": "https://arxiv.org/abs/2504.16879", "authors": ["Puja Chaudhury", "Alexander Estornell", "Michael Everett"], "title": "Learning Verifiable Control Policies Using Relaxed Verification", "categories": ["eess.SY", "cs.LG", "cs.SY"], "comment": null, "summary": "To provide safety guarantees for learning-based control systems, recent work\nhas developed formal verification methods to apply after training ends.\nHowever, if the trained policy does not meet the specifications, or there is\nconservatism in the verification algorithm, establishing these guarantees may\nnot be possible. Instead, this work proposes to perform verification throughout\ntraining to ultimately aim for policies whose properties can be evaluated\nthroughout runtime with lightweight, relaxed verification algorithms. The\napproach is to use differentiable reachability analysis and incorporate new\ncomponents into the loss function. Numerical experiments on a quadrotor model\nand unicycle model highlight the ability of this approach to lead to learned\ncontrol policies that satisfy desired reach-avoid and invariance\nspecifications."}
{"id": "2209.15157", "pdf": "https://arxiv.org/pdf/2209.15157", "abs": "https://arxiv.org/abs/2209.15157", "authors": ["Burcu Sayin", "Jie Yang", "Xinyue Chen", "Andrea Passerini", "Fabio Casati"], "title": "Rethinking and Recomputing the Value of Machine Learning Models", "categories": ["cs.LG", "cs.AI"], "comment": "Accepted at the Journal of Artificial Intelligence Review", "summary": "In this paper, we argue that the prevailing approach to training and\nevaluating machine learning models often fails to consider their real-world\napplication within organizational or societal contexts, where they are intended\nto create beneficial value for people. We propose a shift in perspective,\nredefining model assessment and selection to emphasize integration into\nworkflows that combine machine predictions with human expertise, particularly\nin scenarios requiring human intervention for low-confidence predictions.\nTraditional metrics like accuracy and f-score fail to capture the beneficial\nvalue of models in such hybrid settings. To address this, we introduce a simple\nyet theoretically sound \"value\" metric that incorporates task-specific costs\nfor correct predictions, errors, and rejections, offering a practical framework\nfor real-world evaluation. Through extensive experiments, we show that existing\nmetrics fail to capture real-world needs, often leading to suboptimal choices\nin terms of value when used to rank classifiers. Furthermore, we emphasize the\ncritical role of calibration in determining model value, showing that simple,\nwell-calibrated models can often outperform more complex models that are\nchallenging to calibrate."}
{"id": "2412.06770", "pdf": "https://arxiv.org/pdf/2412.06770", "abs": "https://arxiv.org/abs/2412.06770", "authors": ["Viktor Rudnev", "Gereon Fox", "Mohamed Elgharib", "Christian Theobalt", "Vladislav Golyanik"], "title": "Dynamic EventNeRF: Reconstructing General Dynamic Scenes from Multi-view RGB and Event Streams", "categories": ["cs.CV"], "comment": "17 pages, 13 figures, 7 tables; CVPRW 2025", "summary": "Volumetric reconstruction of dynamic scenes is an important problem in\ncomputer vision. It is especially challenging in poor lighting and with fast\nmotion. This is partly due to limitations of RGB cameras: To capture frames\nunder low lighting, the exposure time needs to be increased, which leads to\nmore motion blur. In contrast, event cameras, which record changes in pixel\nbrightness asynchronously, are much less dependent on lighting, making them\nmore suitable for recording fast motion. We hence propose the first method to\nspatiotemporally reconstruct a scene from sparse multi-view event streams and\nsparse RGB frames. We train a sequence of cross-faded time-conditioned NeRF\nmodels, one per short recording segment. The individual segments are supervised\nwith a set of event- and RGB-based losses and sparse-view regularisation. We\nassemble a real-world multi-view camera rig with six static event cameras\naround the object and record a benchmark multi-view event stream dataset of\nchallenging motions. Our work outperforms RGB-based baselines, producing\nstate-of-the-art results, and opens up the topic of multi-view event-based\nreconstruction as a new path for fast scene capture beyond RGB cameras. The\ncode and the data will be released soon at\nhttps://4dqv.mpi-inf.mpg.de/DynEventNeRF/"}
{"id": "2504.16886", "pdf": "https://arxiv.org/pdf/2504.16886", "abs": "https://arxiv.org/abs/2504.16886", "authors": ["Arnav Sharma", "Anthony Gitter"], "title": "Exploring zero-shot structure-based protein fitness prediction", "categories": ["q-bio.QM", "cs.LG", "q-bio.BM"], "comment": "26 pages, 7 figures", "summary": "The ability to make zero-shot predictions about the fitness consequences of\nprotein sequence changes with pre-trained machine learning models enables many\npractical applications. Such models can be applied for downstream tasks like\ngenetic variant interpretation and protein engineering without additional\nlabeled data. The advent of capable protein structure prediction tools has led\nto the availability of orders of magnitude more precomputed predicted\nstructures, giving rise to powerful structure-based fitness prediction models.\nThrough our experiments, we assess several modeling choices for structure-based\nmodels and their effects on downstream fitness prediction. Zero-shot fitness\nprediction models can struggle to assess the fitness landscape within\ndisordered regions of proteins, those that lack a fixed 3D structure. We\nconfirm the importance of matching protein structures to fitness assays and\nfind that predicted structures for disordered regions can be misleading and\naffect predictive performance. Lastly, we evaluate an additional\nstructure-based model on the ProteinGym substitution benchmark and show that\nsimple multi-modal ensembles are strong baselines."}
{"id": "2401.00477", "pdf": "https://arxiv.org/pdf/2401.00477", "abs": "https://arxiv.org/abs/2401.00477", "authors": ["Junghoon Kim", "Taejoon Kim", "Anindya Bijoy Das", "Seyyedali Hosseinalipour", "David J. Love", "Christopher G. Brinton"], "title": "Coding for Gaussian Two-Way Channels: Linear and Learning-Based Approaches", "categories": ["cs.IT", "cs.AI", "cs.SY", "eess.SY", "math.IT"], "comment": "This work has been accepted for publication in the IEEE Transactions\n  on Information Theory", "summary": "Although user cooperation cannot improve the capacity of Gaussian two-way\nchannels (GTWCs) with independent noises, it can improve communication\nreliability. In this work, we aim to enhance and balance the communication\nreliability in GTWCs by minimizing the sum of error probabilities via joint\ndesign of encoders and decoders at the users. We first formulate general\nencoding/decoding functions, where the user cooperation is captured by the\ncoupling of user encoding processes. The coupling effect renders the\nencoder/decoder design non-trivial, requiring effective decoding to capture\nthis effect, as well as efficient power management at the encoders within power\nconstraints. To address these challenges, we propose two different two-way\ncoding strategies: linear coding and learning-based coding. For linear coding,\nwe propose optimal linear decoding and discuss new insights on encoding\nregarding user cooperation to balance reliability. We then propose an efficient\nalgorithm for joint encoder/decoder design. For learning-based coding, we\nintroduce a novel recurrent neural network (RNN)-based coding architecture,\nwhere we propose interactive RNNs and a power control layer for encoding, and\nwe incorporate bi-directional RNNs with an attention mechanism for decoding.\nThrough simulations, we show that our two-way coding methodologies outperform\nconventional channel coding schemes (that do not utilize user cooperation)\nsignificantly in sum-error performance. We also demonstrate that our linear\ncoding excels at high signal-to-noise ratios (SNRs), while our RNN-based coding\nperforms best at low SNRs. We further investigate our two-way coding strategies\nin terms of power distribution, two-way coding benefit, different coding rates,\nand block-length gain."}
{"id": "2412.11074", "pdf": "https://arxiv.org/pdf/2412.11074", "abs": "https://arxiv.org/abs/2412.11074", "authors": ["Baocai Yin", "Ji Zhao", "Huajie Jiang", "Ningning Hou", "Yongli Hu", "Amin Beheshti", "Ming-Hsuan Yang", "Yuankai Qi"], "title": "Adapter-Enhanced Semantic Prompting for Continual Learning", "categories": ["cs.CV", "cs.LG"], "comment": "This work has been submitted to the IEEE for possible publication", "summary": "Continual learning (CL) enables models to adapt to evolving data streams. A\nmajor challenge of CL is catastrophic forgetting, where new knowledge will\noverwrite previously acquired knowledge. Traditional methods usually retain the\npast data for replay or add additional branches in the model to learn new\nknowledge, which has high memory requirements. In this paper, we propose a\nnovel lightweight CL framework, Adapter-Enhanced Semantic Prompting (AESP),\nwhich integrates prompt tuning and adapter techniques. Specifically, we design\nsemantic-guided prompts to enhance the generalization ability of visual\nfeatures and utilize adapters to efficiently fuse the semantic information,\naiming to learn more adaptive features for the continual learning task.\nFurthermore, to choose the right task prompt for feature adaptation, we have\ndeveloped a novel matching mechanism for prompt selection. Extensive\nexperiments on three CL datasets demonstrate that our approach achieves\nfavorable performance across multiple metrics, showing its potential for\nadvancing CL."}
{"id": "2504.16917", "pdf": "https://arxiv.org/pdf/2504.16917", "abs": "https://arxiv.org/abs/2504.16917", "authors": ["Ghazal Mirzaee", "Jonathan Chang", "Shahrzad Latifi"], "title": "Application of an attention-based CNN-BiLSTM framework for in vivo two-photon calcium imaging of neuronal ensembles: decoding complex bilateral forelimb movements from unilateral M1", "categories": ["q-bio.NC", "cs.LG"], "comment": null, "summary": "Decoding behavior, such as movement, from multiscale brain networks remains a\ncentral objective in neuroscience. Over the past decades, artificial\nintelligence and machine learning have played an increasingly significant role\nin elucidating the neural mechanisms underlying motor function. The advancement\nof brain-monitoring technologies, capable of capturing complex neuronal signals\nwith high spatial and temporal resolution, necessitates the development and\napplication of more sophisticated machine learning models for behavioral\ndecoding. In this study, we employ a hybrid deep learning framework, an\nattention-based CNN-BiLSTM model, to decode skilled and complex forelimb\nmovements using signals obtained from in vivo two-photon calcium imaging. Our\nfindings demonstrate that the intricate movements of both ipsilateral and\ncontralateral forelimbs can be accurately decoded from unilateral M1 neuronal\nensembles. These results highlight the efficacy of advanced hybrid deep\nlearning models in capturing the spatiotemporal dependencies of neuronal\nnetworks activity linked to complex movement execution."}
{"id": "2403.16354", "pdf": "https://arxiv.org/pdf/2403.16354", "abs": "https://arxiv.org/abs/2403.16354", "authors": ["Kyla H. Levin", "Nicolas van Kempen", "Emery D. Berger", "Stephen N. Freund"], "title": "ChatDBG: Augmenting Debugging with Large Language Models", "categories": ["cs.SE", "cs.AI", "cs.LG", "cs.PL"], "comment": "22 pages, to appear at FSE 2025", "summary": "Debugging is a critical but challenging task for programmers. This paper\nproposes ChatDBG, an AI-powered debugging assistant. ChatDBG integrates large\nlanguage models (LLMs) to significantly enhance the capabilities and\nuser-friendliness of conventional debuggers. ChatDBG lets programmers engage in\na collaborative dialogue with the debugger, allowing them to pose complex\nquestions about program state, perform root cause analysis for crashes or\nassertion failures, and explore open-ended queries like \"why is x null?\". To\nhandle these queries, ChatDBG grants the LLM autonomy to \"take the wheel\": it\ncan act as an independent agent capable of querying and controlling the\ndebugger to navigate through stacks and inspect program state. It then reports\nits findings and yields back control to the programmer. By leveraging the\nreal-world knowledge embedded in LLMs, ChatDBG can diagnose issues identifiable\nonly through the use of domain-specific reasoning. Our ChatDBG prototype\nintegrates with standard debuggers including LLDB and GDB for native code and\nPdb for Python. Our evaluation across a diverse set of code, including C/C++\ncode with known bugs and a suite of Python code including standalone scripts\nand Jupyter notebooks, demonstrates that ChatDBG can successfully analyze root\ncauses, explain bugs, and generate accurate fixes for a wide range of\nreal-world errors. For the Python programs, a single query led to an actionable\nbug fix 67% of the time; one additional follow-up query increased the success\nrate to 85%. ChatDBG has seen rapid uptake; it has already been downloaded more\nthan 75,000 times."}
{"id": "2501.04606", "pdf": "https://arxiv.org/pdf/2501.04606", "abs": "https://arxiv.org/abs/2501.04606", "authors": ["Yangfan He", "Sida Li", "Jianhui Wang", "Kun Li", "Xinyuan Song", "Xinhang Yuan", "Keqin Li", "Kuan Lu", "Menghao Huo", "Jiaqi Chen", "Miao Zhang", "Xueqian Wang"], "title": "Enhancing Low-Cost Video Editing with Lightweight Adaptors and Temporal-Aware Inversion", "categories": ["cs.CV"], "comment": null, "summary": "Recent advancements in text-to-image (T2I) generation using diffusion models\nhave enabled cost-effective video-editing applications by leveraging\npre-trained models, eliminating the need for resource-intensive training.\nHowever, the frame-independence of T2I generation often results in poor\ntemporal consistency. Existing methods address this issue through temporal\nlayer fine-tuning or inference-based temporal propagation, but these approaches\nsuffer from high training costs or limited temporal coherence. To address these\nchallenges, we propose a General and Efficient Adapter (GE-Adapter) that\nintegrates temporal-spatial and semantic consistency with Baliteral DDIM\ninversion. This framework introduces three key components: (1) Frame-based\nTemporal Consistency Blocks (FTC Blocks) to capture frame-specific features and\nenforce smooth inter-frame transitions via temporally-aware loss functions; (2)\nChannel-dependent Spatial Consistency Blocks (SCD Blocks) employing bilateral\nfilters to enhance spatial coherence by reducing noise and artifacts; and (3)\nToken-based Semantic Consistency Module (TSC Module) to maintain semantic\nalignment using shared prompt tokens and frame-specific tokens. Our method\nsignificantly improves perceptual quality, text-image alignment, and temporal\ncoherence, as demonstrated on the MSR-VTT dataset. Additionally, it achieves\nenhanced fidelity and frame-to-frame coherence, offering a practical solution\nfor T2V editing."}
{"id": "2504.16923", "pdf": "https://arxiv.org/pdf/2504.16923", "abs": "https://arxiv.org/abs/2504.16923", "authors": ["Jacob Levy", "Jason Gibson", "Bogdan Vlahov", "Erica Tevere", "Evangelos Theodorou", "David Fridovich-Keil", "Patrick Spieler"], "title": "Meta-Learning Online Dynamics Model Adaptation in Off-Road Autonomous Driving", "categories": ["cs.RO", "cs.LG", "cs.SY", "eess.SY"], "comment": null, "summary": "High-speed off-road autonomous driving presents unique challenges due to\ncomplex, evolving terrain characteristics and the difficulty of accurately\nmodeling terrain-vehicle interactions. While dynamics models used in\nmodel-based control can be learned from real-world data, they often struggle to\ngeneralize to unseen terrain, making real-time adaptation essential. We propose\na novel framework that combines a Kalman filter-based online adaptation scheme\nwith meta-learned parameters to address these challenges. Offline meta-learning\noptimizes the basis functions along which adaptation occurs, as well as the\nadaptation parameters, while online adaptation dynamically adjusts the onboard\ndynamics model in real time for model-based control. We validate our approach\nthrough extensive experiments, including real-world testing on a full-scale\nautonomous off-road vehicle, demonstrating that our method outperforms baseline\napproaches in prediction accuracy, performance, and safety metrics,\nparticularly in safety-critical scenarios. Our results underscore the\neffectiveness of meta-learned dynamics model adaptation, advancing the\ndevelopment of reliable autonomous systems capable of navigating diverse and\nunseen environments. Video is available at: https://youtu.be/cCKHHrDRQEA"}
{"id": "2404.17962", "pdf": "https://arxiv.org/pdf/2404.17962", "abs": "https://arxiv.org/abs/2404.17962", "authors": ["Pranav Gokhale", "Caitlin Carnahan", "William Clark", "Teague Tomesh", "Frederic T. Chong"], "title": "Deep Learning for Low-Latency, Quantum-Ready RF Sensing", "categories": ["quant-ph", "cs.AI", "cs.LG", "cs.PF", "cs.SY", "eess.SY"], "comment": null, "summary": "Recent work has shown the promise of applying deep learning to enhance\nsoftware processing of radio frequency (RF) signals. In parallel, hardware\ndevelopments with quantum RF sensors based on Rydberg atoms are breaking\nlongstanding barriers in frequency range, resolution, and sensitivity. In this\npaper, we describe our implementations of quantum-ready machine learning\napproaches for RF signal classification. Our primary objective is latency:\nwhile deep learning offers a more powerful computational paradigm, it also\ntraditionally incurs latency overheads that hinder wider scale deployment. Our\nwork spans three axes. (1) A novel continuous wavelet transform (CWT) based\nrecurrent neural network (RNN) architecture that enables flexible online\nclassification of RF signals on-the-fly with reduced sampling time. (2)\nLow-latency inference techniques for both GPU and CPU that span over 100x\nreductions in inference time, enabling real-time operation with sub-millisecond\ninference. (3) Quantum-readiness validated through application of our models to\nphysics-based simulation of Rydberg atom QRF sensors. Altogether, our work\nbridges towards next-generation RF sensors that use quantum technology to\nsurpass previous physical limits, paired with latency-optimized AI/ML software\nthat is suitable for real-time deployment."}
{"id": "2501.09466", "pdf": "https://arxiv.org/pdf/2501.09466", "abs": "https://arxiv.org/abs/2501.09466", "authors": ["Hualie Jiang", "Zhiqiang Lou", "Laiyan Ding", "Rui Xu", "Minglang Tan", "Wenjie Jiang", "Rui Huang"], "title": "DEFOM-Stereo: Depth Foundation Model Based Stereo Matching", "categories": ["cs.CV"], "comment": "https://insta360-research-team.github.io/DEFOM-Stereo/", "summary": "Stereo matching is a key technique for metric depth estimation in computer\nvision and robotics. Real-world challenges like occlusion and non-texture\nhinder accurate disparity estimation from binocular matching cues. Recently,\nmonocular relative depth estimation has shown remarkable generalization using\nvision foundation models. Thus, to facilitate robust stereo matching with\nmonocular depth cues, we incorporate a robust monocular relative depth model\ninto the recurrent stereo-matching framework, building a new framework for\ndepth foundation model-based stereo-matching, DEFOM-Stereo. In the feature\nextraction stage, we construct the combined context and matching feature\nencoder by integrating features from conventional CNNs and DEFOM. In the update\nstage, we use the depth predicted by DEFOM to initialize the recurrent\ndisparity and introduce a scale update module to refine the disparity at the\ncorrect scale. DEFOM-Stereo is verified to have much stronger zero-shot\ngeneralization compared with SOTA methods. Moreover, DEFOM-Stereo achieves top\nperformance on the KITTI 2012, KITTI 2015, Middlebury, and ETH3D benchmarks,\nranking $1^{st}$ on many metrics. In the joint evaluation under the robust\nvision challenge, our model simultaneously outperforms previous models on the\nindividual benchmarks, further demonstrating its outstanding capabilities."}
{"id": "2310.16705", "pdf": "https://arxiv.org/pdf/2310.16705", "abs": "https://arxiv.org/abs/2310.16705", "authors": ["Dai Hai Nguyen", "Tetsuya Sakurai", "Hiroshi Mamitsuka"], "title": "Wasserstein Gradient Flow over Variational Parameter Space for Variational Inference", "categories": ["cs.LG", "stat.ML"], "comment": "Accepted to AISTATS 2025", "summary": "Variational inference (VI) can be cast as an optimization problem in which\nthe variational parameters are tuned to closely align a variational\ndistribution with the true posterior. The optimization task can be approached\nthrough vanilla gradient descent in black-box VI or natural-gradient descent in\nnatural-gradient VI. In this work, we reframe VI as the optimization of an\nobjective that concerns probability distributions defined over a\n\\textit{variational parameter space}. Subsequently, we propose Wasserstein\ngradient descent for tackling this optimization problem. Notably, the\noptimization techniques, namely black-box VI and natural-gradient VI, can be\nreinterpreted as specific instances of the proposed Wasserstein gradient\ndescent. To enhance the efficiency of optimization, we develop practical\nmethods for numerically solving the discrete gradient flows. We validate the\neffectiveness of the proposed methods through empirical experiments on a\nsynthetic dataset, supplemented by theoretical analyses."}
{"id": "2406.12413", "pdf": "https://arxiv.org/pdf/2406.12413", "abs": "https://arxiv.org/abs/2406.12413", "authors": ["Georgios Amanatidis", "Aris Filos-Ratsikas", "Alkmini Sgouritsa"], "title": "Pushing the Frontier on Approximate EFX Allocations", "categories": ["cs.GT", "cs.AI", "cs.DM"], "comment": "The conference version of this work has been accepted to the\n  Twenty-Fifth ACM Conference on Economics and Computation (EC 2024)", "summary": "We study the problem of allocating a set of indivisible goods to a set of\nagents with additive valuation functions, aiming to achieve approximate\nenvy-freeness up to any good ($\\alpha$-EFX). The state-of-the-art results on\nthe problem include that (exact) EFX allocations exist when (a) there are at\nmost three agents, or (b) the agents' valuation functions can take at most two\nvalues, or (c) the agents' valuation functions can be represented via a graph.\nFor $\\alpha$-EFX, it is known that a $0.618$-EFX allocation exists for any\nnumber of agents with additive valuation functions. In this paper, we show that\n$2/3$-EFX allocations exist when (a) there are at most \\emph{seven agents}, (b)\nthe agents' valuation functions can take at most \\emph{three values}, or (c)\nthe agents' valuation functions can be represented via a \\emph{multigraph}. Our\nresults can be interpreted in two ways. First, by relaxing the notion of EFX to\n$2/3$-EFX, we obtain existence results for strict generalizations of the\nsettings for which exact EFX allocations are known to exist. Secondly, by\nimposing restrictions on the setting, we manage to beat the barrier of $0.618$\nand achieve an approximation guarantee of $2/3$. Therefore, our results push\nthe \\emph{frontier} of existence and computation of approximate EFX\nallocations, and provide insights into the challenges of settling the existence\nof exact EFX allocations."}
{"id": "2501.11515", "pdf": "https://arxiv.org/pdf/2501.11515", "abs": "https://arxiv.org/abs/2501.11515", "authors": ["Zixuan Chen", "Yujin Wang", "Xin Cai", "Zhiyuan You", "Zheming Lu", "Fan Zhang", "Shi Guo", "Tianfan Xue"], "title": "UltraFusion: Ultra High Dynamic Imaging using Exposure Fusion", "categories": ["cs.CV"], "comment": "Accepted by CVPR 2025. Project Page:\n  https://openimaginglab.github.io/UltraFusion", "summary": "Capturing high dynamic range (HDR) scenes is one of the most important issues\nin camera design. Majority of cameras use exposure fusion, which fuses images\ncaptured by different exposure levels, to increase dynamic range. However, this\napproach can only handle images with limited exposure difference, normally 3-4\nstops. When applying to very high dynamic range scenes where a large exposure\ndifference is required, this approach often fails due to incorrect alignment or\ninconsistent lighting between inputs, or tone mapping artifacts. In this work,\nwe propose \\model, the first exposure fusion technique that can merge inputs\nwith 9 stops differences. The key idea is that we model exposure fusion as a\nguided inpainting problem, where the under-exposed image is used as a guidance\nto fill the missing information of over-exposed highlights in the over-exposed\nregion. Using an under-exposed image as a soft guidance, instead of a hard\nconstraint, our model is robust to potential alignment issue or lighting\nvariations. Moreover, by utilizing the image prior of the generative model, our\nmodel also generates natural tone mapping, even for very high-dynamic range\nscenes. Our approach outperforms HDR-Transformer on latest HDR benchmarks.\nMoreover, to test its performance in ultra high dynamic range scenes, we\ncapture a new real-world exposure fusion benchmark, UltraFusion dataset, with\nexposure differences up to 9 stops, and experiments show that UltraFusion can\ngenerate beautiful and high-quality fusion results under various scenarios.\nCode and data will be available at\nhttps://openimaginglab.github.io/UltraFusion."}
{"id": "2403.02573", "pdf": "https://arxiv.org/pdf/2403.02573", "abs": "https://arxiv.org/abs/2403.02573", "authors": ["Zhongdong Liu", "Keyuan Zhang", "Bin Li", "Yin Sun", "Y. Thomas Hou", "Bo Ji"], "title": "Learning-augmented Online Minimization of Age of Information and Transmission Costs", "categories": ["cs.LG"], "comment": "This paper has been accepted for publication in the IEEE Transactions\n  on Network Science and Engineering (TNSE), April 2025. A preliminary version\n  of this work is to be presented at IEEE INFOCOM 2024 Age and Semantics of\n  Information Workshop", "summary": "We consider a discrete-time system where a resource-constrained source (e.g.,\na small sensor) transmits its time-sensitive data to a destination over a\ntime-varying wireless channel. Each transmission incurs a fixed transmission\ncost (e.g., energy cost), and no transmission results in a staleness cost\nrepresented by the Age-of-Information. The source must balance the tradeoff\nbetween transmission and staleness costs. To address this challenge, we develop\na robust online algorithm to minimize the sum of transmission and staleness\ncosts, ensuring a worst-case performance guarantee. While online algorithms are\nrobust, they are usually overly conservative and may have a poor average\nperformance in typical scenarios. In contrast, by leveraging historical data\nand prediction models, machine learning (ML) algorithms perform well in average\ncases. However, they typically lack worst-case performance guarantees. To\nachieve the best of both worlds, we design a learning-augmented online\nalgorithm that exhibits two desired properties: (i) consistency: closely\napproximating the optimal offline algorithm when the ML prediction is accurate\nand trusted; (ii) robustness: ensuring worst-case performance guarantee even ML\npredictions are inaccurate. Finally, we perform extensive simulations to show\nthat our online algorithm performs well empirically and that our\nlearning-augmented algorithm achieves both consistency and robustness."}
{"id": "2407.15192", "pdf": "https://arxiv.org/pdf/2407.15192", "abs": "https://arxiv.org/abs/2407.15192", "authors": ["Joshua Shay Kricheli", "Khoa Vo", "Aniruddha Datta", "Spencer Ozgur", "Paulo Shakarian"], "title": "Error Detection and Constraint Recovery in Hierarchical Multi-Label Classification without Prior Knowledge", "categories": ["cs.LG", "cs.AI", "cs.LO", "cs.SC"], "comment": null, "summary": "Recent advances in Hierarchical Multi-label Classification (HMC),\nparticularly neurosymbolic-based approaches, have demonstrated improved\nconsistency and accuracy by enforcing constraints on a neural model during\ntraining. However, such work assumes the existence of such constraints\na-priori. In this paper, we relax this strong assumption and present an\napproach based on Error Detection Rules (EDR) that allow for learning\nexplainable rules about the failure modes of machine learning models. We show\nthat these rules are not only effective in detecting when a machine learning\nclassifier has made an error but also can be leveraged as constraints for HMC,\nthereby allowing the recovery of explainable constraints even if they are not\nprovided. We show that our approach is effective in detecting machine learning\nerrors and recovering constraints, is noise tolerant, and can function as a\nsource of knowledge for neurosymbolic models on multiple datasets, including a\nnewly introduced military vehicle recognition dataset."}
{"id": "2502.07758", "pdf": "https://arxiv.org/pdf/2502.07758", "abs": "https://arxiv.org/abs/2502.07758", "authors": ["Nektarios A. Valous", "Eckhard Hitzer", "Dragoş Duşe", "Rodrigo Rojas Moraleda", "Ferdinand Popp", "Meggy Suarez-Carmona", "Anna Berthel", "Ismini Papageorgiou", "Carlo Fremd", "Alexander Rölle", "Christina C. Westhoff", "Bénédicte Lenoir", "Niels Halama", "Inka Zörnig", "Dirk Jäger"], "title": "Novel computational workflows for natural and biomedical image processing based on hypercomplex algebras", "categories": ["cs.CV", "cs.LG"], "comment": "24 pages, 18 figures, 14 tables", "summary": "Hypercomplex image processing extends conventional techniques in a unified\nparadigm encompassing algebraic and geometric principles. This work leverages\nquaternions and the two-dimensional orthogonal planes split framework\n(splitting of a quaternion - representing a pixel - into pairs of orthogonal 2D\nplanes) for natural/biomedical image analysis through the following\ncomputational workflows and outcomes: natural/biomedical image re-colorization,\nnatural image de-colorization, natural/biomedical image contrast enhancement,\ncomputational re-staining and stain separation in histological images, and\nperformance gains in machine/deep learning pipelines for histological images.\nThe workflows are analyzed separately for natural and biomedical images to\nshowcase the effectiveness of the proposed approaches. The proposed workflows\ncan regulate color appearance (e.g. with alternative renditions and grayscale\nconversion) and image contrast, be part of automated image processing pipelines\n(e.g. isolating stain components, boosting learning models), and assist in\ndigital pathology applications (e.g. enhancing biomarker visibility, enabling\ncolorblind-friendly renditions). Employing only basic arithmetic and matrix\noperations, this work offers a computationally accessible methodology - in the\nhypercomplex domain - that showcases versatility and consistency across image\nprocessing tasks and a range of computer vision and biomedical applications.\nThe proposed non-data-driven methods achieve comparable or better results\n(particularly in cases involving well-known methods) to those reported in the\nliterature, showcasing the potential of robust theoretical frameworks with\npractical effectiveness. Results, methods, and limitations are detailed\nalongside discussion of promising extensions, emphasizing the potential of\nfeature-rich mathematical/computational frameworks for natural and biomedical\nimages."}
{"id": "2405.01142", "pdf": "https://arxiv.org/pdf/2405.01142", "abs": "https://arxiv.org/abs/2405.01142", "authors": ["Yipeng Li", "Xinchen Lyu"], "title": "Sharp Bounds for Sequential Federated Learning on Heterogeneous Data", "categories": ["cs.LG"], "comment": "arXiv admin note: text overlap with arXiv:2311.03154", "summary": "There are two paradigms in Federated Learning (FL): parallel FL (PFL), where\nmodels are trained in a parallel manner across clients, and sequential FL\n(SFL), where models are trained in a sequential manner across clients.\nSpecifically, in PFL, clients perform local updates independently and send the\nupdated model parameters to a global server for aggregation; in SFL, one client\nstarts its local updates only after receiving the model parameters from the\nprevious client in the sequence. In contrast to that of PFL, the convergence\ntheory of SFL on heterogeneous data is still lacking. To resolve the\ntheoretical dilemma of SFL, we establish sharp convergence guarantees for SFL\non heterogeneous data with both upper and lower bounds. Specifically, we derive\nthe upper bounds for the strongly convex, general convex and non-convex\nobjective functions, and construct the matching lower bounds for the strongly\nconvex and general convex objective functions. Then, we compare the upper\nbounds of SFL with those of PFL, showing that SFL outperforms PFL on\nheterogeneous data (at least, when the level of heterogeneity is relatively\nhigh). Experimental results validate the counterintuitive theoretical finding."}
{"id": "2409.05202", "pdf": "https://arxiv.org/pdf/2409.05202", "abs": "https://arxiv.org/abs/2409.05202", "authors": ["Xin Jin", "Hongyu Zhu", "Siyuan Li", "Zedong Wang", "Zicheng Liu", "Juanxi Tian", "Chang Yu", "Huafeng Qin", "Stan Z. Li"], "title": "A Survey on Mixup Augmentations and Beyond", "categories": ["cs.LG", "cs.AI", "cs.CV"], "comment": "Preprint V2 with 30 pages main text. Online project at\n  https://github.com/Westlake-AI/Awesome-Mixup", "summary": "As Deep Neural Networks have achieved thrilling breakthroughs in the past\ndecade, data augmentations have garnered increasing attention as regularization\ntechniques when massive labeled data are unavailable. Among existing\naugmentations, Mixup and relevant data-mixing methods that convexly combine\nselected samples and the corresponding labels are widely adopted because they\nyield high performances by generating data-dependent virtual data while easily\nmigrating to various domains. This survey presents a comprehensive review of\nfoundational mixup methods and their applications. We first elaborate on the\ntraining pipeline with mixup augmentations as a unified framework containing\nmodules. A reformulated framework could contain various mixup methods and give\nintuitive operational procedures. Then, we systematically investigate the\napplications of mixup augmentations on vision downstream tasks, various data\nmodalities, and some analysis \\& theorems of mixup. Meanwhile, we conclude the\ncurrent status and limitations of mixup research and point out further work for\neffective and efficient mixup augmentations. This survey can provide\nresearchers with the current state of the art in mixup methods and provide some\ninsights and guidance roles in the mixup arena. An online project with this\nsurvey is available at https://github.com/Westlake-AI/Awesome-Mixup."}
{"id": "2503.04067", "pdf": "https://arxiv.org/pdf/2503.04067", "abs": "https://arxiv.org/abs/2503.04067", "authors": ["Ziqi Ni", "Ao Fu", "Yi Zhou"], "title": "FREAK: Frequency-modulated High-fidelity and Real-time Audio-driven Talking Portrait Synthesis", "categories": ["cs.CV"], "comment": "Accepted by ICMR 2025", "summary": "Achieving high-fidelity lip-speech synchronization in audio-driven talking\nportrait synthesis remains challenging. While multi-stage pipelines or\ndiffusion models yield high-quality results, they suffer from high\ncomputational costs. Some approaches perform well on specific individuals with\nlow resources, yet still exhibit mismatched lip movements. The aforementioned\nmethods are modeled in the pixel domain. We observed that there are noticeable\ndiscrepancies in the frequency domain between the synthesized talking videos\nand natural videos. Currently, no research on talking portrait synthesis has\nconsidered this aspect. To address this, we propose a FREquency-modulated,\nhigh-fidelity, and real-time Audio-driven talKing portrait synthesis framework,\nnamed FREAK, which models talking portraits from the frequency domain\nperspective, enhancing the fidelity and naturalness of the synthesized\nportraits. FREAK introduces two novel frequency-based modules: 1) the Visual\nEncoding Frequency Modulator (VEFM) to couple multi-scale visual features in\nthe frequency domain, better preserving visual frequency information and\nreducing the gap in the frequency spectrum between synthesized and natural\nframes. and 2) the Audio Visual Frequency Modulator (AVFM) to help the model\nlearn the talking pattern in the frequency domain and improve audio-visual\nsynchronization. Additionally, we optimize the model in both pixel domain and\nfrequency domain jointly. Furthermore, FREAK supports seamless switching\nbetween one-shot and video dubbing settings, offering enhanced flexibility. Due\nto its superior performance, it can simultaneously support high-resolution\nvideo results and real-time inference. Extensive experiments demonstrate that\nour method synthesizes high-fidelity talking portraits with detailed facial\ntextures and precise lip synchronization in real-time, outperforming\nstate-of-the-art methods."}
{"id": "2405.16168", "pdf": "https://arxiv.org/pdf/2405.16168", "abs": "https://arxiv.org/abs/2405.16168", "authors": ["Or Raveh", "Junya Honda", "Masashi Sugiyama"], "title": "Multi-Player Approaches for Dueling Bandits", "categories": ["cs.LG", "stat.ML"], "comment": null, "summary": "Various approaches have emerged for multi-armed bandits in distributed\nsystems. The multiplayer dueling bandit problem, common in scenarios with only\npreference-based information like human feedback, introduces challenges related\nto controlling collaborative exploration of non-informative arm pairs, but has\nreceived little attention. To fill this gap, we demonstrate that the direct use\nof a Follow Your Leader black-box approach matches the lower bound for this\nsetting when utilizing known dueling bandit algorithms as a foundation.\nAdditionally, we analyze a message-passing fully distributed approach with a\nnovel Condorcet-winner recommendation protocol, resulting in expedited\nexploration in many cases. Our experimental comparisons reveal that our\nmultiplayer algorithms surpass single-player benchmark algorithms, underscoring\ntheir efficacy in addressing the nuanced challenges of the multiplayer dueling\nbandit setting."}
{"id": "2411.00348", "pdf": "https://arxiv.org/pdf/2411.00348", "abs": "https://arxiv.org/abs/2411.00348", "authors": ["Kuo-Han Hung", "Ching-Yun Ko", "Ambrish Rawat", "I-Hsin Chung", "Winston H. Hsu", "Pin-Yu Chen"], "title": "Attention Tracker: Detecting Prompt Injection Attacks in LLMs", "categories": ["cs.CR", "cs.AI", "cs.LG"], "comment": "Project page:\n  https://huggingface.co/spaces/TrustSafeAI/Attention-Tracker", "summary": "Large Language Models (LLMs) have revolutionized various domains but remain\nvulnerable to prompt injection attacks, where malicious inputs manipulate the\nmodel into ignoring original instructions and executing designated action. In\nthis paper, we investigate the underlying mechanisms of these attacks by\nanalyzing the attention patterns within LLMs. We introduce the concept of the\ndistraction effect, where specific attention heads, termed important heads,\nshift focus from the original instruction to the injected instruction. Building\non this discovery, we propose Attention Tracker, a training-free detection\nmethod that tracks attention patterns on instruction to detect prompt injection\nattacks without the need for additional LLM inference. Our method generalizes\neffectively across diverse models, datasets, and attack types, showing an AUROC\nimprovement of up to 10.0% over existing methods, and performs well even on\nsmall LLMs. We demonstrate the robustness of our approach through extensive\nevaluations and provide insights into safeguarding LLM-integrated systems from\nprompt injection vulnerabilities."}
{"id": "2503.06276", "pdf": "https://arxiv.org/pdf/2503.06276", "abs": "https://arxiv.org/abs/2503.06276", "authors": ["Songping Wang", "Xinquan Yue", "Yueming Lyu", "Caifeng Shan"], "title": "Exploring Adversarial Transferability between Kolmogorov-arnold Networks", "categories": ["cs.CV"], "comment": "After the submission of the paper, we realized that the study still\n  has room for expansion. In order to make the research findings more profound\n  and comprehensive, we have decided to withdraw the paper so that we can\n  conduct further research and expansion", "summary": "Kolmogorov-Arnold Networks (KANs) have emerged as a transformative model\nparadigm, significantly impacting various fields. However, their adversarial\nrobustness remains less underexplored, especially across different KAN\narchitectures. To explore this critical safety issue, we conduct an analysis\nand find that due to overfitting to the specific basis functions of KANs, they\npossess poor adversarial transferability among different KANs. To tackle this\nchallenge, we propose AdvKAN, the first transfer attack method for KANs. AdvKAN\nintegrates two key components: 1) a Breakthrough-Defense Surrogate Model\n(BDSM), which employs a breakthrough-defense training strategy to mitigate\noverfitting to the specific structures of KANs. 2) a Global-Local Interaction\n(GLI) technique, which promotes sufficient interaction between adversarial\ngradients of hierarchical levels, further smoothing out loss surfaces of KANs.\nBoth of them work together to enhance the strength of transfer attack among\ndifferent KANs. Extensive experimental results on various KANs and datasets\ndemonstrate the effectiveness of AdvKAN, which possesses notably superior\nattack capabilities and deeply reveals the vulnerabilities of KANs. Code will\nbe released upon acceptance."}
{"id": "2406.01386", "pdf": "https://arxiv.org/pdf/2406.01386", "abs": "https://arxiv.org/abs/2406.01386", "authors": ["Xutong Liu", "Siwei Wang", "Jinhang Zuo", "Han Zhong", "Xuchuang Wang", "Zhiyong Wang", "Shuai Li", "Mohammad Hajiesmaili", "John C. S. Lui", "Wei Chen"], "title": "Combinatorial Multivariant Multi-Armed Bandits with Applications to Episodic Reinforcement Learning and Beyond", "categories": ["cs.LG"], "comment": null, "summary": "We introduce a novel framework of combinatorial multi-armed bandits (CMAB)\nwith multivariant and probabilistically triggering arms (CMAB-MT), where the\noutcome of each arm is a $d$-dimensional multivariant random variable and the\nfeedback follows a general arm triggering process. Compared with existing CMAB\nworks, CMAB-MT not only enhances the modeling power but also allows improved\nresults by leveraging distinct statistical properties for multivariant random\nvariables. For CMAB-MT, we propose a general 1-norm multivariant and triggering\nprobability-modulated smoothness condition, and an optimistic CUCB-MT algorithm\nbuilt upon this condition. Our framework can include many important problems as\napplications, such as episodic reinforcement learning (RL) and probabilistic\nmaximum coverage for goods distribution, all of which meet the above smoothness\ncondition and achieve matching or improved regret bounds compared to existing\nworks. Through our new framework, we build the first connection between the\nepisodic RL and CMAB literature, by offering a new angle to solve the episodic\nRL through the lens of CMAB, which may encourage more interactions between\nthese two important directions."}
{"id": "2411.01055", "pdf": "https://arxiv.org/pdf/2411.01055", "abs": "https://arxiv.org/abs/2411.01055", "authors": ["Leandro Von Krannichfeldt", "Kristina Orehounig", "Olga Fink"], "title": "Combining Physics-based and Data-driven Modeling for Building Energy Systems", "categories": ["eess.SY", "cs.AI", "cs.LG", "cs.SY"], "comment": null, "summary": "Building energy modeling plays a vital role in optimizing the operation of\nbuilding energy systems by providing accurate predictions of the building's\nreal-world conditions. In this context, various techniques have been explored,\nranging from traditional physics-based models to data-driven models. Recently,\nresearchers are combining physics-based and data-driven models into hybrid\napproaches. This includes using the physics-based model output as additional\ndata-driven input, learning the residual between physics-based model and real\ndata, learning a surrogate of the physics-based model, or fine-tuning a\nsurrogate model with real data. However, a comprehensive comparison of the\ninherent advantages of these hybrid approaches is still missing. The primary\nobjective of this work is to evaluate four predominant hybrid approaches in\nbuilding energy modeling through a real-world case study, with focus on indoor\nthermodynamics. To achieve this, we devise three scenarios reflecting common\nlevels of building documentation and sensor availability, assess their\nperformance, and analyze their explainability using hierarchical Shapley\nvalues. The real-world study reveals three notable findings. First, greater\nbuilding documentation and sensor availability lead to higher prediction\naccuracy for hybrid approaches. Second, the performance of hybrid approaches\ndepends on the type of building room, but the residual approach using a\nFeedforward Neural Network as data-driven sub-model performs best on average\nacross all rooms. This hybrid approach also demonstrates a superior ability to\nleverage the simulation from the physics-based sub-model. Third, hierarchical\nShapley values prove to be an effective tool for explaining and improving\nhybrid models while accounting for input correlations."}
{"id": "2503.12542", "pdf": "https://arxiv.org/pdf/2503.12542", "abs": "https://arxiv.org/abs/2503.12542", "authors": ["Peiran Wu", "Yunze Liu", "Miao Liu", "Junxiao Shen"], "title": "ST-Think: How Multimodal Large Language Models Reason About 4D Worlds from Ego-Centric Videos", "categories": ["cs.CV"], "comment": null, "summary": "Humans excel at spatial-temporal reasoning, effortlessly interpreting dynamic\nvisual events from an egocentric viewpoint. However, whether multimodal large\nlanguage models (MLLMs) can similarly understand the 4D world remains\nuncertain. This paper explores multimodal spatial-temporal reasoning from an\negocentric perspective, aiming to equip MLLMs with human-like reasoning\ncapabilities. To support this objective, we introduce \\textbf{Ego-ST Bench}, a\nnovel benchmark containing over 5,000 question-answer pairs across four\ncategories, systematically evaluating spatial, temporal, and integrated\nspatial-temporal reasoning. Additionally, we propose \\textbf{ST-R1} training\nparadigm, a video-based reasoning model that incorporates reverse thinking into\nits reinforcement learning process, significantly enhancing performance. We\ncombine long-chain-of-thought (long-CoT) supervised fine-tuning with Group\nRelative Policy Optimization (GRPO) reinforcement learning, achieving notable\nimprovements with limited high-quality data. Ego-ST Bench and ST-R1 provide\nvaluable insights and resources for advancing video-based spatial-temporal\nreasoning research."}
{"id": "2406.04239", "pdf": "https://arxiv.org/pdf/2406.04239", "abs": "https://arxiv.org/abs/2406.04239", "authors": ["Axel Levy", "Eric R. Chan", "Sara Fridovich-Keil", "Frédéric Poitevin", "Ellen D. Zhong", "Gordon Wetzstein"], "title": "Solving Inverse Problems in Protein Space Using Diffusion-Based Priors", "categories": ["cs.LG"], "comment": null, "summary": "The interaction of a protein with its environment can be understood and\ncontrolled via its 3D structure. Experimental methods for protein structure\ndetermination, such as X-ray crystallography or cryogenic electron microscopy,\nshed light on biological processes but introduce challenging inverse problems.\nLearning-based approaches have emerged as accurate and efficient methods to\nsolve these inverse problems for 3D structure determination, but are\nspecialized for a predefined type of measurement. Here, we introduce a\nversatile framework to turn biophysical measurements, such as cryo-EM density\nmaps, into 3D atomic models. Our method combines a physics-based forward model\nof the measurement process with a pretrained generative model providing a\ntask-agnostic, data-driven prior. Our method outperforms posterior sampling\nbaselines on linear and non-linear inverse problems. In particular, it is the\nfirst diffusion-based method for refining atomic models from cryo-EM maps and\nbuilding atomic models from sparse distance matrices."}
{"id": "2411.14922", "pdf": "https://arxiv.org/pdf/2411.14922", "abs": "https://arxiv.org/abs/2411.14922", "authors": ["Zewen Long", "Liang Wang", "Shu Wu", "Qiang Liu", "Liang Wang"], "title": "GOT4Rec: Graph of Thoughts for Sequential Recommendation", "categories": ["cs.IR", "cs.AI"], "comment": null, "summary": "With their vast open-world knowledge and reasoning abilities, large language\nmodels (LLMs) have become a promising tool for sequential recommendation.\nResearchers have explored various methods to harness these capabilities, but\nmost existing approaches rely on simple input-output prompting, failing to\neffectively bridge the gap between LLMs' general knowledge and the specific\nneeds of recommendation tasks. While reasoning strategies like chain-of-thought\n(CoT) have been introduced to enhance performance, they often produce\ninaccurate recommendations due to underutilized user preference information and\ninsufficient reasoning depth. To address these challenges, we propose GOT4Rec,\na novel sequential recommendation method leveraging the graph of thoughts (GoT)\nreasoning strategy. Our method focuses on three key types of information in\nuser histories: short-term interests, long-term interests and collaborative\ninformation from other users. It enables LLMs to reason independently and\ngenerate recommendations, subsequently aggregating results to derive final\nitems. This method allows LLMs, with enhanced reasoning capabilities, to better\nutilize the user sequence information, producing more accurate recommendations\nand comprehensive explanations. Extensive experiments on real-world datasets\ndemonstrate the effectiveness of GOT4Rec, outperforming existing\nstate-of-the-art baselines with an average improvement of 37.11%. Our code is\navailable at https://anonymous.4open.science/r/GOT4Rec."}
{"id": "2503.12652", "pdf": "https://arxiv.org/pdf/2503.12652", "abs": "https://arxiv.org/abs/2503.12652", "authors": ["Tsu-Jui Fu", "Yusu Qian", "Chen Chen", "Wenze Hu", "Zhe Gan", "Yinfei Yang"], "title": "UniVG: A Generalist Diffusion Model for Unified Image Generation and Editing", "categories": ["cs.CV"], "comment": null, "summary": "Text-to-Image (T2I) diffusion models have shown impressive results in\ngenerating visually compelling images following user prompts. Building on this,\nvarious methods further fine-tune the pre-trained T2I model for specific tasks.\nHowever, this requires separate model architectures, training designs, and\nmultiple parameter sets to handle different tasks. In this paper, we introduce\nUniVG, a generalist diffusion model capable of supporting a diverse range of\nimage generation tasks with a single set of weights. UniVG treats multi-modal\ninputs as unified conditions to enable various downstream applications, ranging\nfrom T2I generation, inpainting, instruction-based editing, identity-preserving\ngeneration, and layout-guided generation, to depth estimation and referring\nsegmentation. Through comprehensive empirical studies on data mixing and\nmulti-task training, we provide detailed insights into the training processes\nand decisions that inform our final designs. For example, we show that T2I\ngeneration and other tasks, such as instruction-based editing, can coexist\nwithout performance trade-offs, while auxiliary tasks like depth estimation and\nreferring segmentation enhance image editing. Notably, our model can even\noutperform some task-specific models on their respective benchmarks, marking a\nsignificant step towards a unified image generation model."}
{"id": "2407.14058", "pdf": "https://arxiv.org/pdf/2407.14058", "abs": "https://arxiv.org/abs/2407.14058", "authors": ["Jingyao Wang", "Siyu Zhao", "Wenwen Qiang", "Jiangmeng Li", "Fuchun Sun", "Hui Xiong"], "title": "Towards the Causal Complete Cause of Multi-Modal Representation Learning", "categories": ["cs.LG"], "comment": null, "summary": "Multi-Modal Learning (MML) aims to learn effective representations across\nmodalities for accurate predictions. Existing methods typically focus on\nmodality consistency and specificity to learn effective representations.\nHowever, from a causal perspective, they may lead to representations that\ncontain insufficient and unnecessary information. To address this, we propose\nthat effective MML representations should be causally sufficient and necessary.\nConsidering practical issues like spurious correlations and modality conflicts,\nwe relax the exogeneity and monotonicity assumptions prevalent in prior works\nand explore the concepts specific to MML, i.e., Causal Complete Cause\n(\\(C^3\\)). We begin by defining \\(C^3\\), which quantifies the probability of\nrepresentations being causally sufficient and necessary. We then discuss the\nidentifiability of \\(C^3\\) and introduce an instrumental variable to support\nidentifying \\(C^3\\) with non-exogeneity and non-monotonicity. Building on this,\nwe conduct the $C^3$ measurement, i.e., \\(C^3\\) risk. We propose a twin network\nto estimate it through (i) the real-world branch: utilizing the instrumental\nvariable for sufficiency, and (ii) the hypothetical-world branch: applying\ngradient-based counterfactual modeling for necessity. Theoretical analyses\nconfirm its reliability. Based on these results, we propose $C^3$\nRegularization, a plug-and-play method that enforces the causal completeness of\nthe learned representations by minimizing \\(C^3\\) risk. Extensive experiments\ndemonstrate its effectiveness."}
{"id": "2412.12161", "pdf": "https://arxiv.org/pdf/2412.12161", "abs": "https://arxiv.org/abs/2412.12161", "authors": ["Bao-Bing Li", "Yi Gu", "Shao-Feng Wu"], "title": "Discover physical concepts and equations with machine learning", "categories": ["cs.LG", "cond-mat.dis-nn", "cs.AI", "physics.comp-ph"], "comment": null, "summary": "Machine learning can uncover physical concepts or physical equations when\nprior knowledge from the other is available. However, these two aspects are\noften intertwined and cannot be discovered independently. We extend SciNet,\nwhich is a neural network architecture that simulates the human physical\nreasoning process for physics discovery, by proposing a model that combines\nVariational Autoencoders (VAE) with Neural Ordinary Differential Equations\n(Neural ODEs). This allows us to simultaneously discover physical concepts and\ngoverning equations from simulated experimental data across various physical\nsystems. We apply the model to several examples inspired by the history of\nphysics, including Copernicus' heliocentrism, Newton's law of gravity,\nSchr\\\"odinger's wave mechanics, and Pauli's spin-magnetic formulation. The\nresults demonstrate that the correct physical theories can emerge in the neural\nnetwork."}
{"id": "2503.19215", "pdf": "https://arxiv.org/pdf/2503.19215", "abs": "https://arxiv.org/abs/2503.19215", "authors": ["Bilal Alsallakh", "Timothy Wroge", "Vivek Miglani", "Narine Kokhlikyan"], "title": "On Symmetries in Convolutional Weights", "categories": ["cs.CV"], "comment": "Accepted to the ICLR 2025 Workshop on Weight Space Learning (WSL)", "summary": "We explore the symmetry of the mean k x k weight kernel in each layer of\nvarious convolutional neural networks. Unlike individual neurons, the mean\nkernels in internal layers tend to be symmetric about their centers instead of\nfavoring specific directions. We investigate why this symmetry emerges in\nvarious datasets and models, and how it is impacted by certain architectural\nchoices. We show how symmetry correlates with desirable properties such as\nshift and flip consistency, and might constitute an inherent inductive bias in\nconvolutional neural networks."}
{"id": "2408.11266", "pdf": "https://arxiv.org/pdf/2408.11266", "abs": "https://arxiv.org/abs/2408.11266", "authors": ["Georgios Is. Detorakis"], "title": "Practical Aspects on Solving Differential Equations Using Deep Learning: A Primer", "categories": ["cs.LG", "cs.NA", "math.NA"], "comment": "32 pages, 12 figures, primer (tutorial)", "summary": "Deep learning has become a popular tool across many scientific fields,\nincluding the study of differential equations, particularly partial\ndifferential equations. This work introduces the basic principles of deep\nlearning and the Deep Galerkin method, which uses deep neural networks to solve\ndifferential equations. This primer aims to provide technical and practical\ninsights into the Deep Galerkin method and its implementation. We demonstrate\nhow to solve the one-dimensional heat equation step-by-step. We also show how\nto apply the Deep Galerkin method to solve systems of ordinary differential\nequations and integral equations, such as the Fredholm of the second kind.\nAdditionally, we provide code snippets within the text and the complete source\ncode on Github. The examples are designed so that one can run them on a simple\ncomputer without needing a GPU."}
{"id": "2501.00798", "pdf": "https://arxiv.org/pdf/2501.00798", "abs": "https://arxiv.org/abs/2501.00798", "authors": ["Leonard Puškáč", "Marek Benovič", "Jakub Breier", "Xiaolu Hou"], "title": "Make Shuffling Great Again: A Side-Channel Resistant Fisher-Yates Algorithm for Protecting Neural Networks", "categories": ["cs.CR", "cs.AI"], "comment": null, "summary": "Neural network models implemented in embedded devices have been shown to be\nsusceptible to side-channel attacks (SCAs), allowing recovery of proprietary\nmodel parameters, such as weights and biases. There are already available\ncountermeasure methods currently used for protecting cryptographic\nimplementations that can be tailored to protect embedded neural network models.\nShuffling, a hiding-based countermeasure that randomly shuffles the order of\ncomputations, was shown to be vulnerable to SCA when the Fisher-Yates algorithm\nis used. In this paper, we propose a design of an SCA-secure version of the\nFisher-Yates algorithm. By integrating the masking technique for modular\nreduction and Blakely's method for modular multiplication, we effectively\nremove the vulnerability in the division operation that led to side-channel\nleakage in the original version of the algorithm. We experimentally evaluate\nthat the countermeasure is effective against SCA by implementing a correlation\npower analysis attack on an embedded neural network model implemented on ARM\nCortex-M4. Compared to the original proposal, the memory overhead is $2\\times$\nthe biggest layer of the network, while the time overhead varies from $4\\%$ to\n$0.49\\%$ for a layer with $100$ and $1000$ neurons, respectively."}
{"id": "2503.20698", "pdf": "https://arxiv.org/pdf/2503.20698", "abs": "https://arxiv.org/abs/2503.20698", "authors": ["Saron Samuel", "Dan DeGenaro", "Jimena Guallar-Blasco", "Kate Sanders", "Oluwaseun Eisape", "Arun Reddy", "Alexander Martin", "Andrew Yates", "Eugene Yang", "Cameron Carpenter", "David Etter", "Efsun Kayi", "Matthew Wiesner", "Kenton Murray", "Reno Kriz"], "title": "MMMORRF: Multimodal Multilingual Modularized Reciprocal Rank Fusion", "categories": ["cs.CV", "cs.IR"], "comment": null, "summary": "Videos inherently contain multiple modalities, including visual events, text\noverlays, sounds, and speech, all of which are important for retrieval.\nHowever, state-of-the-art multimodal language models like VAST and LanguageBind\nare built on vision-language models (VLMs), and thus overly prioritize visual\nsignals. Retrieval benchmarks further reinforce this bias by focusing on visual\nqueries and neglecting other modalities. We create a search system MMMORRF that\nextracts text and features from both visual and audio modalities and integrates\nthem with a novel modality-aware weighted reciprocal rank fusion. MMMORRF is\nboth effective and efficient, demonstrating practicality in searching videos\nbased on users' information needs instead of visual descriptive queries. We\nevaluate MMMORRF on MultiVENT 2.0 and TVR, two multimodal benchmarks designed\nfor more targeted information needs, and find that it improves nDCG@20 by 81%\nover leading multimodal encoders and 37% over single-modality retrieval,\ndemonstrating the value of integrating diverse modalities."}
{"id": "2410.21518", "pdf": "https://arxiv.org/pdf/2410.21518", "abs": "https://arxiv.org/abs/2410.21518", "authors": ["Wenxian Shi", "Menghua Wu", "Regina Barzilay"], "title": "Predicting sub-population specific viral evolution", "categories": ["cs.LG"], "comment": null, "summary": "Forecasting the change in the distribution of viral variants is crucial for\ntherapeutic design and disease surveillance. This task poses significant\nmodeling challenges due to the sharp differences in virus distributions across\nsub-populations (e.g., countries) and their dynamic interactions. Existing\nmachine learning approaches that model the variant distribution as a whole are\nincapable of making location-specific predictions and ignore transmissions that\nshape the viral landscape. In this paper, we propose a sub-population specific\nprotein evolution model, which predicts the time-resolved distributions of\nviral proteins in different locations. The algorithm explicitly models the\ntransmission rates between sub-populations and learns their interdependence\nfrom data. The change in protein distributions across all sub-populations is\ndefined through a linear ordinary differential equation (ODE) parametrized by\ntransmission rates. Solving this ODE yields the likelihood of a given protein\noccurring in particular sub-populations. Multi-year evaluation on both\nSARS-CoV-2 and influenza A/H3N2 demonstrates that our model outperforms\nbaselines in accurately predicting distributions of viral proteins across\ncontinents and countries. We also find that the transmission rates learned from\ndata are consistent with the transmission pathways discovered by retrospective\nphylogenetic analysis."}
{"id": "2501.10100", "pdf": "https://arxiv.org/pdf/2501.10100", "abs": "https://arxiv.org/abs/2501.10100", "authors": ["Chenhao Li", "Andreas Krause", "Marco Hutter"], "title": "Robotic World Model: A Neural Network Simulator for Robust Policy Optimization in Robotics", "categories": ["cs.RO", "cs.AI", "cs.LG"], "comment": null, "summary": "Learning robust and generalizable world models is crucial for enabling\nefficient and scalable robotic control in real-world environments. In this\nwork, we introduce a novel framework for learning world models that accurately\ncapture complex, partially observable, and stochastic dynamics. The proposed\nmethod employs a dual-autoregressive mechanism and self-supervised training to\nachieve reliable long-horizon predictions without relying on domain-specific\ninductive biases, ensuring adaptability across diverse robotic tasks. We\nfurther propose a policy optimization framework that leverages world models for\nefficient training in imagined environments and seamless deployment in\nreal-world systems. This work advances model-based reinforcement learning by\naddressing the challenges of long-horizon prediction, error accumulation, and\nsim-to-real transfer. By providing a scalable and robust framework, the\nintroduced methods pave the way for adaptive and efficient robotic systems in\nreal-world applications."}
{"id": "2504.01503", "pdf": "https://arxiv.org/pdf/2504.01503", "abs": "https://arxiv.org/abs/2504.01503", "authors": ["Ziteng Cui", "Xuangeng Chu", "Tatsuya Harada"], "title": "Luminance-GS: Adapting 3D Gaussian Splatting to Challenging Lighting Conditions with View-Adaptive Curve Adjustment", "categories": ["cs.CV"], "comment": "CVPR 2025, project page:\n  https://cuiziteng.github.io/Luminance_GS_web/", "summary": "Capturing high-quality photographs under diverse real-world lighting\nconditions is challenging, as both natural lighting (e.g., low-light) and\ncamera exposure settings (e.g., exposure time) significantly impact image\nquality. This challenge becomes more pronounced in multi-view scenarios, where\nvariations in lighting and image signal processor (ISP) settings across\nviewpoints introduce photometric inconsistencies. Such lighting degradations\nand view-dependent variations pose substantial challenges to novel view\nsynthesis (NVS) frameworks based on Neural Radiance Fields (NeRF) and 3D\nGaussian Splatting (3DGS). To address this, we introduce Luminance-GS, a novel\napproach to achieving high-quality novel view synthesis results under diverse\nchallenging lighting conditions using 3DGS. By adopting per-view color matrix\nmapping and view-adaptive curve adjustments, Luminance-GS achieves\nstate-of-the-art (SOTA) results across various lighting conditions -- including\nlow-light, overexposure, and varying exposure -- while not altering the\noriginal 3DGS explicit representation. Compared to previous NeRF- and\n3DGS-based baselines, Luminance-GS provides real-time rendering speed with\nimproved reconstruction quality."}
{"id": "2411.04225", "pdf": "https://arxiv.org/pdf/2411.04225", "abs": "https://arxiv.org/abs/2411.04225", "authors": ["Jung Yeon Park", "Sujay Bhatt", "Sihan Zeng", "Lawson L. S. Wong", "Alec Koppel", "Sumitra Ganesh", "Robin Walters"], "title": "Approximate Equivariance in Reinforcement Learning", "categories": ["cs.LG"], "comment": "AISTATS 2025", "summary": "Equivariant neural networks have shown great success in reinforcement\nlearning, improving sample efficiency and generalization when there is symmetry\nin the task. However, in many problems, only approximate symmetry is present,\nwhich makes imposing exact symmetry inappropriate. Recently, approximately\nequivariant networks have been proposed for supervised classification and\nmodeling physical systems. In this work, we develop approximately equivariant\nalgorithms in reinforcement learning (RL). We define approximately equivariant\nMDPs and theoretically characterize the effect of approximate equivariance on\nthe optimal $Q$ function. We propose novel RL architectures using relaxed group\nand steerable convolutions and experiment on several continuous control domains\nand stock trading with real financial data. Our results demonstrate that the\napproximately equivariant network performs on par with exactly equivariant\nnetworks when exact symmetries are present, and outperforms them when the\ndomains exhibit approximate symmetry. As an added byproduct of these\ntechniques, we observe increased robustness to noise at test time. Our code is\navailable at https://github.com/jypark0/approx_equiv_rl."}
{"id": "2502.10475", "pdf": "https://arxiv.org/pdf/2502.10475", "abs": "https://arxiv.org/abs/2502.10475", "authors": ["Zihang Cheng", "Huiping Zhuang", "Chun Li", "Xin Meng", "Ming Li", "Fei Richard Yu", "Liqiang Nie"], "title": "X-SG$^2$S: Safe and Generalizable Gaussian Splatting with X-dimensional Watermarks", "categories": ["cs.CR", "cs.AI", "cs.CV"], "comment": null, "summary": "3D Gaussian Splatting (3DGS) has been widely used in 3D reconstruction and 3D\ngeneration. Training to get a 3DGS scene often takes a lot of time and\nresources and even valuable inspiration. The increasing amount of 3DGS digital\nasset have brought great challenges to the copyright protection. However, it\nstill lacks profound exploration targeted at 3DGS. In this paper, we propose a\nnew framework X-SG$^2$S which can simultaneously watermark 1 to 3D messages\nwhile keeping the original 3DGS scene almost unchanged. Generally, we have a\nX-SG$^2$S injector for adding multi-modal messages simultaneously and an\nextractor for extract them. Specifically, we first split the watermarks into\nmessage patches in a fixed manner and sort the 3DGS points. A self-adaption\ngate is used to pick out suitable location for watermarking. Then use a\nXD(multi-dimension)-injection heads to add multi-modal messages into sorted\n3DGS points. A learnable gate can recognize the location with extra messages\nand XD-extraction heads can restore hidden messages from the location\nrecommended by the learnable gate. Extensive experiments demonstrated that the\nproposed X-SG$^2$S can effectively conceal multi modal messages without\nchanging pretrained 3DGS pipeline or the original form of 3DGS parameters.\nMeanwhile, with simple and efficient model structure and high practicality,\nX-SG$^2$S still shows good performance in hiding and extracting multi-modal\ninner structured or unstructured messages. X-SG$^2$S is the first to unify 1 to\n3D watermarking model for 3DGS and the first framework to add multi-modal\nwatermarks simultaneous in one 3DGS which pave the wave for later researches."}
{"id": "2504.02812", "pdf": "https://arxiv.org/pdf/2504.02812", "abs": "https://arxiv.org/abs/2504.02812", "authors": ["Van Nguyen Nguyen", "Stephen Tyree", "Andrew Guo", "Mederic Fourmy", "Anas Gouda", "Taeyeop Lee", "Sungphill Moon", "Hyeontae Son", "Lukas Ranftl", "Jonathan Tremblay", "Eric Brachmann", "Bertram Drost", "Vincent Lepetit", "Carsten Rother", "Stan Birchfield", "Jiri Matas", "Yann Labbe", "Martin Sundermeyer", "Tomas Hodan"], "title": "BOP Challenge 2024 on Model-Based and Model-Free 6D Object Pose Estimation", "categories": ["cs.CV"], "comment": "arXiv admin note: text overlap with arXiv:2403.09799", "summary": "We present the evaluation methodology, datasets and results of the BOP\nChallenge 2024, the 6th in a series of public competitions organized to capture\nthe state of the art in 6D object pose estimation and related tasks. In 2024,\nour goal was to transition BOP from lab-like setups to real-world scenarios.\nFirst, we introduced new model-free tasks, where no 3D object models are\navailable and methods need to onboard objects just from provided reference\nvideos. Second, we defined a new, more practical 6D object detection task where\nidentities of objects visible in a test image are not provided as input. Third,\nwe introduced new BOP-H3 datasets recorded with high-resolution sensors and\nAR/VR headsets, closely resembling real-world scenarios. BOP-H3 include 3D\nmodels and onboarding videos to support both model-based and model-free tasks.\nParticipants competed on seven challenge tracks. Notably, the best 2024 method\nfor model-based 6D localization of unseen objects (FreeZeV2.1) achieves 22%\nhigher accuracy on BOP-Classic-Core than the best 2023 method (GenFlow), and is\nonly 4% behind the best 2023 method for seen objects (GPose2023) although being\nsignificantly slower (24.9 vs 2.7s per image). A more practical 2024 method for\nthis task is Co-op which takes only 0.8s per image and is 13% more accurate\nthan GenFlow. Methods have similar rankings on 6D detection as on 6D\nlocalization but higher run time. On model-based 2D detection of unseen\nobjects, the best 2024 method (MUSE) achieves 21--29% relative improvement\ncompared to the best 2023 method (CNOS). However, the 2D detection accuracy for\nunseen objects is still -35% behind the accuracy for seen objects (GDet2023),\nand the 2D detection stage is consequently the main bottleneck of existing\npipelines for 6D localization/detection of unseen objects. The online\nevaluation system stays open and is available at http://bop.felk.cvut.cz/"}
{"id": "2411.10471", "pdf": "https://arxiv.org/pdf/2411.10471", "abs": "https://arxiv.org/abs/2411.10471", "authors": ["Fanjin Wang", "Maryam Parhizkar", "Anthony Harker", "Mohan Edirisinghe"], "title": "Constrained composite Bayesian optimization for rational synthesis of polymeric particles", "categories": ["cs.LG", "cond-mat.soft", "physics.data-an"], "comment": "Revised version with additional experiments in result section Figure\n  2", "summary": "Polymeric nano- and micro-scale particles have critical roles in tackling\ncritical healthcare and energy challenges with their miniature characteristics.\nHowever, tailoring their synthesis process to meet specific design targets has\ntraditionally depended on domain expertise and costly trial-and-errors.\nRecently, modeling strategies, particularly Bayesian optimization (BO), have\nbeen proposed to aid materials discovery for maximized/minimized properties.\nComing from practical demands, this study for the first time integrates\nconstrained and composite Bayesian optimization (CCBO) to perform efficient\ntarget value optimization under black-box feasibility constraints and limited\ndata for laboratory experimentation. Using a synthetic problem that simulates\nelectrospraying, a model nanomanufacturing process, CCBO strategically avoided\ninfeasible conditions and efficiently optimized particle production towards\npredefined size targets, surpassing standard BO pipelines and providing\ndecisions comparable to human experts. Further laboratory experiments validated\nCCBO capability to guide the rational synthesis of poly(lactic-co-glycolic\nacid) (PLGA) particles with diameters of 300 nm and 3.0 $\\mu$m via\nelectrospraying. With minimal initial data and unknown experiment constraints,\nCCBO reached the design targets within 4 iterations. Overall, the CCBO approach\npresents a versatile and holistic optimization paradigm for next-generation\ntarget-driven particle synthesis empowered by artificial intelligence (AI)."}
{"id": "2502.15013", "pdf": "https://arxiv.org/pdf/2502.15013", "abs": "https://arxiv.org/abs/2502.15013", "authors": ["Majid Farhadloo", "Arun Sharma", "Mingzhou Yang", "Bharat Jayaprakash", "William Northrop", "Shashi Shekhar"], "title": "Towards Physics-Guided Foundation Models", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Traditional foundation models are pre-trained on broad datasets to reduce the\ntraining resources (e.g., time, energy, labeled samples) needed for fine-tuning\na wide range of downstream tasks. However, traditional foundation models\nstruggle with out-of-distribution prediction and can produce outputs that are\nunrealistic and physically infeasible. We propose the notation of\nphysics-guided foundation models (PGFM), that is, foundation models integrated\nwith broad or general domain (e.g., scientific) physical knowledge applicable\nto a wide range of downstream tasks."}
{"id": "2504.06121", "pdf": "https://arxiv.org/pdf/2504.06121", "abs": "https://arxiv.org/abs/2504.06121", "authors": ["Ronghui Zhang", "Yuhang Ma", "Tengfei Li", "Ziyu Lin", "Yueying Wu", "Junzhou Chen", "Lin Zhang", "Jia Hu", "Tony Z. Qiu", "Konghui Guo"], "title": "A Robust Real-Time Lane Detection Method with Fog-Enhanced Feature Fusion for Foggy Conditions", "categories": ["cs.CV"], "comment": null, "summary": "Lane detection is a critical component of Advanced Driver Assistance Systems\n(ADAS). Existing lane detection algorithms generally perform well under\nfavorable weather conditions. However, their performance degrades significantly\nin adverse conditions, such as fog, which increases the risk of traffic\naccidents. This challenge is compounded by the lack of specialized datasets and\nmethods designed for foggy environments. To address this, we introduce the\nFoggyLane dataset, captured in real-world foggy scenarios, and synthesize two\nadditional datasets, FoggyCULane and FoggyTusimple, from existing popular lane\ndetection datasets. Furthermore, we propose a robust Fog-Enhanced Network for\nlane detection, incorporating a Global Feature Fusion Module (GFFM) to capture\nglobal relationships in foggy images, a Kernel Feature Fusion Module (KFFM) to\nmodel the structural and positional relationships of lane instances, and a\nLow-level Edge Enhanced Module (LEEM) to address missing edge details in foggy\nconditions. Comprehensive experiments demonstrate that our method achieves\nstate-of-the-art performance, with F1-scores of 95.04 on FoggyLane, 79.85 on\nFoggyCULane, and 96.95 on FoggyTusimple. Additionally, with TensorRT\nacceleration, the method reaches a processing speed of 38.4 FPS on the NVIDIA\nJetson AGX Orin, confirming its real-time capabilities and robustness in foggy\nenvironments."}
{"id": "2411.19584", "pdf": "https://arxiv.org/pdf/2411.19584", "abs": "https://arxiv.org/abs/2411.19584", "authors": ["Hemal Mahmud", "Hasan Mahmud", "Mohammad Rifat Ahmmad Rashid"], "title": "Enhancing Sentiment Analysis in Bengali Texts: A Hybrid Approach Using Lexicon-Based Algorithm and Pretrained Language Model Bangla-BERT", "categories": ["cs.LG"], "comment": "13 pages, 12 figures", "summary": "Sentiment analysis (SA) is a process of identifying the emotional tone or\npolarity within a given text and aims to uncover the user's complex emotions\nand inner feelings. While sentiment analysis has been extensively studied for\nlanguages like English, research in Bengali, remains limited, particularly for\nfine-grained sentiment categorization. This work aims to connect this gap by\ndeveloping a novel approach that integrates rule-based algorithms with\npre-trained language models. We developed a dataset from scratch, comprising\nover 15,000 manually labeled reviews. Next, we constructed a Lexicon Data\nDictionary, assigning polarity scores to the reviews. We developed a novel rule\nbased algorithm Bangla Sentiment Polarity Score (BSPS), an approach capable of\ngenerating sentiment scores and classifying reviews into nine distinct\nsentiment categories. To assess the performance of this method, we evaluated\nthe classified sentiments using BanglaBERT, a pre-trained transformer-based\nlanguage model. We also performed sentiment classification directly with\nBanglaBERT on the original data and evaluated this model's results. Our\nanalysis revealed that the BSPS + BanglaBERT hybrid approach outperformed the\nstandalone BanglaBERT model, achieving higher accuracy, precision, and nuanced\nclassification across the nine sentiment categories. The results of our study\nemphasize the value and effectiveness of combining rule-based and pre-trained\nlanguage model approaches for enhanced sentiment analysis in Bengali and\nsuggest pathways for future research and application in languages with similar\nlinguistic complexities."}
{"id": "2503.02881", "pdf": "https://arxiv.org/pdf/2503.02881", "abs": "https://arxiv.org/abs/2503.02881", "authors": ["Han Xue", "Jieji Ren", "Wendi Chen", "Gu Zhang", "Yuan Fang", "Guoying Gu", "Huazhe Xu", "Cewu Lu"], "title": "Reactive Diffusion Policy: Slow-Fast Visual-Tactile Policy Learning for Contact-Rich Manipulation", "categories": ["cs.RO", "cs.AI", "cs.LG"], "comment": "Accepted to RSS 2025. Project page:\n  https://reactive-diffusion-policy.github.io", "summary": "Humans can accomplish complex contact-rich tasks using vision and touch, with\nhighly reactive capabilities such as fast response to external changes and\nadaptive control of contact forces; however, this remains challenging for\nrobots. Existing visual imitation learning (IL) approaches rely on action\nchunking to model complex behaviors, which lacks the ability to respond\ninstantly to real-time tactile feedback during the chunk execution.\nFurthermore, most teleoperation systems struggle to provide fine-grained\ntactile / force feedback, which limits the range of tasks that can be\nperformed. To address these challenges, we introduce TactAR, a low-cost\nteleoperation system that provides real-time tactile feedback through Augmented\nReality (AR), along with Reactive Diffusion Policy (RDP), a novel slow-fast\nvisual-tactile imitation learning algorithm for learning contact-rich\nmanipulation skills. RDP employs a two-level hierarchy: (1) a slow latent\ndiffusion policy for predicting high-level action chunks in latent space at low\nfrequency, (2) a fast asymmetric tokenizer for closed-loop tactile feedback\ncontrol at high frequency. This design enables both complex trajectory modeling\nand quick reactive behavior within a unified framework. Through extensive\nevaluation across three challenging contact-rich tasks, RDP significantly\nimproves performance compared to state-of-the-art visual IL baselines.\nFurthermore, experiments show that RDP is applicable across different tactile /\nforce sensors. Code and videos are available on\nhttps://reactive-diffusion-policy.github.io."}
{"id": "2504.11008", "pdf": "https://arxiv.org/pdf/2504.11008", "abs": "https://arxiv.org/abs/2504.11008", "authors": ["Qinyue Tong", "Ziqian Lu", "Jun Liu", "Yangming Zheng", "Zheming Lu"], "title": "MediSee: Reasoning-based Pixel-level Perception in Medical Images", "categories": ["cs.CV", "cs.AI"], "comment": "10 pages, 6 figures", "summary": "Despite remarkable advancements in pixel-level medical image perception,\nexisting methods are either limited to specific tasks or heavily rely on\naccurate bounding boxes or text labels as input prompts. However, the medical\nknowledge required for input is a huge obstacle for general public, which\ngreatly reduces the universality of these methods. Compared with these\ndomain-specialized auxiliary information, general users tend to rely on oral\nqueries that require logical reasoning. In this paper, we introduce a novel\nmedical vision task: Medical Reasoning Segmentation and Detection (MedSD),\nwhich aims to comprehend implicit queries about medical images and generate the\ncorresponding segmentation mask and bounding box for the target object. To\naccomplish this task, we first introduce a Multi-perspective, Logic-driven\nMedical Reasoning Segmentation and Detection (MLMR-SD) dataset, which\nencompasses a substantial collection of medical entity targets along with their\ncorresponding reasoning. Furthermore, we propose MediSee, an effective baseline\nmodel designed for medical reasoning segmentation and detection. The\nexperimental results indicate that the proposed method can effectively address\nMedSD with implicit colloquial queries and outperform traditional medical\nreferring segmentation methods."}
{"id": "2412.11631", "pdf": "https://arxiv.org/pdf/2412.11631", "abs": "https://arxiv.org/abs/2412.11631", "authors": ["Yuyang Tao", "Shufei Ge"], "title": "A Mapper Algorithm with implicit intervals and its optimization", "categories": ["cs.LG", "stat.ML"], "comment": null, "summary": "The Mapper algorithm is an essential tool for visualizing complex, high\ndimensional data in topology data analysis (TDA) and has been widely used in\nbiomedical research. It outputs a combinatorial graph whose structure implies\nthe shape of the data. However,the need for manual parameter tuning and fixed\nintervals, along with fixed overlapping ratios may impede the performance of\nthe standard Mapper algorithm. Variants of the standard Mapper algorithms have\nbeen developed to address these limitations, yet most of them still require\nmanual tuning of parameters. Additionally, many of these variants, including\nthe standard version found in the literature, were built within a deterministic\nframework and overlooked the uncertainty inherent in the data. To relax these\nlimitations, in this work, we introduce a novel framework that implicitly\nrepresents intervals through a hidden assignment matrix, enabling automatic\nparameter optimization via stochastic gradient descent. In this work, we\ndevelop a soft Mapper framework based on a Gaussian mixture model(GMM) for\nflexible and implicit interval construction. We further illustrate the\nrobustness of the soft Mapper algorithm by introducing the Mapper graph mode as\na point estimation for the output graph. Moreover, a stochastic gradient\ndescent algorithm with a specific topological loss function is proposed for\noptimizing parameters in the model. Both simulation and application studies\ndemonstrate its effectiveness in capturing the underlying topological\nstructures. In addition, the application to an RNA expression dataset obtained\nfrom the Mount Sinai/JJ Peters VA Medical Center Brain Bank (MSBB) successfully\nidentifies a distinct subgroup of Alzheimer's Disease."}
{"id": "2503.21615", "pdf": "https://arxiv.org/pdf/2503.21615", "abs": "https://arxiv.org/abs/2503.21615", "authors": ["Vikas Kushwaha", "Sruti Srinivasa Ragavan", "Subhajit Roy"], "title": "A Measure Based Generalizable Approach to Understandability", "categories": ["cs.HC", "cs.AI", "cs.SE"], "comment": "6 pages", "summary": "Successful agent-human partnerships require that any agent generated\ninformation is understandable to the human, and that the human can easily steer\nthe agent towards a goal. Such effective communication requires the agent to\ndevelop a finer-level notion of what is understandable to the human.\nState-of-the-art agents, including LLMs, lack this detailed notion of\nunderstandability because they only capture average human sensibilities from\nthe training data, and therefore afford limited steerability (e.g., requiring\nnon-trivial prompt engineering).\n  In this paper, instead of only relying on data, we argue for developing\ngeneralizable, domain-agnostic measures of understandability that can be used\nas directives for these agents. Existing research on understandability measures\nis fragmented, we survey various such efforts across domains, and lay a\ncognitive-science-rooted groundwork for more coherent and domain-agnostic\nresearch investigations in future."}
{"id": "2504.12129", "pdf": "https://arxiv.org/pdf/2504.12129", "abs": "https://arxiv.org/abs/2504.12129", "authors": ["Songping Wang", "Yueming Lyu", "Shiqi Liu", "Ning Li", "Tong Tong", "Hao Sun", "Caifeng Shan"], "title": "Anti-Aesthetics: Protecting Facial Privacy against Customized Text-to-Image Synthesis", "categories": ["cs.CV"], "comment": "After the submission of the paper, we realized that the study still\n  has room for expansion. In order to make the research findings more profound\n  and comprehensive, we have decided to withdraw the paper so that we can\n  conduct further research and expansion", "summary": "The rise of customized diffusion models has spurred a boom in personalized\nvisual content creation, but also poses risks of malicious misuse, severely\nthreatening personal privacy and copyright protection. Some studies show that\nthe aesthetic properties of images are highly positively correlated with human\nperception of image quality. Inspired by this, we approach the problem from a\nnovel and intriguing aesthetic perspective to degrade the generation quality of\nmaliciously customized models, thereby achieving better protection of facial\nidentity. Specifically, we propose a Hierarchical Anti-Aesthetic (HAA)\nframework to fully explore aesthetic cues, which consists of two key branches:\n1) Global Anti-Aesthetics: By establishing a global anti-aesthetic reward\nmechanism and a global anti-aesthetic loss, it can degrade the overall\naesthetics of the generated content; 2) Local Anti-Aesthetics: A local\nanti-aesthetic reward mechanism and a local anti-aesthetic loss are designed to\nguide adversarial perturbations to disrupt local facial identity. By seamlessly\nintegrating both branches, our HAA effectively achieves the goal of\nanti-aesthetics from a global to a local level during customized generation.\nExtensive experiments show that HAA outperforms existing SOTA methods largely\nin identity removal, providing a powerful tool for protecting facial privacy\nand copyright."}
{"id": "2412.12864", "pdf": "https://arxiv.org/pdf/2412.12864", "abs": "https://arxiv.org/abs/2412.12864", "authors": ["Yoontae Hwang", "Yongjae Lee"], "title": "Geodesic Flow Kernels for Semi-Supervised Learning on Mixed-Variable Tabular Dataset", "categories": ["cs.LG"], "comment": "AAAI-25", "summary": "Tabular data poses unique challenges due to its heterogeneous nature,\ncombining both continuous and categorical variables. Existing approaches often\nstruggle to effectively capture the underlying structure and relationships\nwithin such data. We propose GFTab (Geodesic Flow Kernels for Semi- Supervised\nLearning on Mixed-Variable Tabular Dataset), a semi-supervised framework\nspecifically designed for tabular datasets. GFTab incorporates three key\ninnovations: 1) Variable-specific corruption methods tailored to the distinct\nproperties of continuous and categorical variables, 2) A Geodesic flow kernel\nbased similarity measure to capture geometric changes between corrupted inputs,\nand 3) Tree-based embedding to leverage hierarchical relationships from\navailable labeled data. To rigorously evaluate GFTab, we curate a comprehensive\nset of 21 tabular datasets spanning various domains, sizes, and variable\ncompositions. Our experimental results show that GFTab outperforms existing\nML/DL models across many of these datasets, particularly in settings with\nlimited labeled data."}
{"id": "2504.00060", "pdf": "https://arxiv.org/pdf/2504.00060", "abs": "https://arxiv.org/abs/2504.00060", "authors": ["Hongjie He", "Xu Pan", "Yudong Yao"], "title": "CF-CAM: Cluster Filter Class Activation Mapping for Reliable Gradient-Based Interpretability", "categories": ["cs.LG", "cs.AI", "cs.CV"], "comment": null, "summary": "As deep learning continues to advance, the transparency of neural network\ndecision-making remains a critical challenge, limiting trust and applicability\nin high-stakes domains. Class Activation Mapping (CAM) techniques have emerged\nas a key approach toward visualizing model decisions, yet existing methods face\ninherent trade-offs. Gradient-based CAM variants suffer from sensitivity to\ngradient perturbations due to gradient noise, leading to unstable and\nunreliable explanations. Conversely, gradient-free approaches mitigate gradient\ninstability but incur significant computational overhead and inference latency.\nTo address these limitations, we propose a Cluster Filter Class Activation Map\n(CF-CAM) technique, a novel framework that reintroduces gradient-based\nweighting while enhancing robustness against gradient noise. CF-CAM utilizes\nhierarchical importance weighting strategy to balance discriminative feature\npreservation and noise elimination. A density-aware channel clustering method\nvia Density-Based Spatial Clustering of Applications with Noise (DBSCAN) groups\nsemantically relevant feature channels and discard noise-prone activations.\nAdditionally, cluster-conditioned gradient filtering leverages Gaussian filters\nto refine gradient signals, preserving edge-aware localization while\nsuppressing noise impact. Experiment results demonstrate that CF-CAM achieves\nsuperior interpretability performance while enhancing computational efficiency,\noutperforming state-of-the-art CAM methods in faithfulness and robustness. By\neffectively mitigating gradient instability without excessive computational\ncost, CF-CAM provides a competitive solution for enhancing the interpretability\nof deep neural networks in critical applications such as autonomous driving and\nmedical diagnosis."}
{"id": "2504.13024", "pdf": "https://arxiv.org/pdf/2504.13024", "abs": "https://arxiv.org/abs/2504.13024", "authors": ["Daniel Gonzalez-Alvarado", "Fabio Schlindwein", "Jonas Cassel", "Laura Steingruber", "Stefania Petra", "Christoph Schnörr"], "title": "Riemannian Patch Assignment Gradient Flows", "categories": ["cs.CV"], "comment": null, "summary": "This paper introduces patch assignment flows for metric data labeling on\ngraphs. Labelings are determined by regularizing initial local labelings\nthrough the dynamic interaction of both labels and label assignments across the\ngraph, entirely encoded by a dictionary of competing labeled patches and\nmediated by patch assignment variables. Maximal consistency of patch\nassignments is achieved by geometric numerical integration of a Riemannian\nascent flow, as critical point of a Lagrangian action functional. Experiments\nillustrate properties of the approach, including uncertainty quantification of\nlabel assignments."}
{"id": "2501.03865", "pdf": "https://arxiv.org/pdf/2501.03865", "abs": "https://arxiv.org/abs/2501.03865", "authors": ["Yiting Hu", "Lingjie Duan"], "title": "Truthful mechanisms for linear bandit games with private contexts", "categories": ["cs.LG", "cs.GT"], "comment": "Accepted by AAMAS 2025", "summary": "The contextual bandit problem, where agents arrive sequentially with personal\ncontexts and the system adapts its arm allocation decisions accordingly, has\nrecently garnered increasing attention for enabling more personalized outcomes.\nHowever, in many healthcare and recommendation applications, agents have\nprivate profiles and may misreport their contexts to gain from the system. For\nexample, in adaptive clinical trials, where hospitals sequentially recruit\nvolunteers to test multiple new treatments and adjust plans based on\nvolunteers' reported profiles such as symptoms and interim data, participants\nmay misreport severe side effects like allergy and nausea to avoid perceived\nsuboptimal treatments. We are the first to study this issue of private context\nmisreporting in a stochastic contextual bandit game between the system and\nnon-repeated agents. We show that traditional low-regret algorithms, such as\nUCB family algorithms and Thompson sampling, fail to ensure truthful reporting\nand can result in linear regret in the worst case, while traditional truthful\nalgorithms like explore-then-commit (ETC) and $\\epsilon$-greedy algorithm incur\nsublinear but high regret. We propose a mechanism that uses a linear program to\nensure truthfulness while minimizing deviation from Thompson sampling, yielding\nan $O(\\ln T)$ frequentist regret. Our numerical experiments further demonstrate\nstrong performance in multiple contexts and across other distribution families."}
{"id": "2504.00513", "pdf": "https://arxiv.org/pdf/2504.00513", "abs": "https://arxiv.org/abs/2504.00513", "authors": ["Asma Yamani", "Malak Baslyman", "Moataz Ahmed"], "title": "Leveraging LLMs for User Stories in AI Systems: UStAI Dataset", "categories": ["cs.SE", "cs.AI"], "comment": null, "summary": "AI systems are gaining widespread adoption across various sectors and\ndomains. Creating high-quality AI system requirements is crucial for aligning\nthe AI system with business goals and consumer values and for social\nresponsibility. However, with the uncertain nature of AI systems and the heavy\nreliance on sensitive data, more research is needed to address the elicitation\nand analysis of AI systems requirements. With the proprietary nature of many AI\nsystems, there is a lack of open-source requirements artifacts and technical\nrequirements documents for AI systems, limiting broader research and\ninvestigation. With Large Language Models (LLMs) emerging as a promising\nalternative to human-generated text, this paper investigates the potential use\nof LLMs to generate user stories for AI systems based on abstracts from\nscholarly papers. We conducted an empirical evaluation using three LLMs and\ngenerated $1260$ user stories from $42$ abstracts from $26$ domains. We assess\ntheir quality using the Quality User Story (QUS) framework. Moreover, we\nidentify relevant non-functional requirements (NFRs) and ethical principles.\nOur analysis demonstrates that the investigated LLMs can generate user stories\ninspired by the needs of various stakeholders, offering a promising approach\nfor generating user stories for research purposes and for aiding in the early\nrequirements elicitation phase of AI systems. We have compiled and curated a\ncollection of stories generated by various LLMs into a dataset (UStAI), which\nis now publicly available for use."}
{"id": "2504.13460", "pdf": "https://arxiv.org/pdf/2504.13460", "abs": "https://arxiv.org/abs/2504.13460", "authors": ["Hongwei Ji", "Wulian Yun", "Mengshi Qi", "Huadong Ma"], "title": "Chain-of-Thought Textual Reasoning for Few-shot Temporal Action Localization", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Traditional temporal action localization (TAL) methods rely on large amounts\nof detailed annotated data, whereas few-shot TAL reduces this dependence by\nusing only a few training samples to identify unseen action categories.\nHowever, existing few-shot TAL methods typically focus solely on video-level\ninformation, neglecting textual information, which can provide valuable\nsemantic support for the localization task. Therefore, we propose a new\nfew-shot temporal action localization method by Chain-of-Thought textual\nreasoning to improve localization performance. Specifically, we design a novel\nfew-shot learning framework that leverages textual semantic information to\nenhance the model's ability to capture action commonalities and variations,\nwhich includes a semantic-aware text-visual alignment module designed to align\nthe query and support videos at different levels. Meanwhile, to better express\nthe temporal dependencies and causal relationships between actions at the\ntextual level to assist action localization, we design a Chain of Thought\n(CoT)-like reasoning method that progressively guides the Vision Language Model\n(VLM) and Large Language Model (LLM) to generate CoT-like text descriptions for\nvideos. The generated texts can capture more variance of action than visual\nfeatures. We conduct extensive experiments on the publicly available\nActivityNet1.3 and THUMOS14 datasets. We introduce the first dataset named\nHuman-related Anomaly Localization and explore the application of the TAL task\nin human anomaly detection. The experimental results demonstrate that our\nproposed method significantly outperforms existing methods in single-instance\nand multi-instance scenarios. We will release our code, data and benchmark."}
{"id": "2502.14840", "pdf": "https://arxiv.org/pdf/2502.14840", "abs": "https://arxiv.org/abs/2502.14840", "authors": ["Arun Sharma", "Majid Farhadloo", "Mingzhou Yang", "Ruolei Zeng", "Subhankar Ghosh", "Shashi Shekhar"], "title": "Spatial Distribution-Shift Aware Knowledge-Guided Machine Learning", "categories": ["cs.LG"], "comment": null, "summary": "Given inputs of diverse soil characteristics and climate data gathered from\nvarious regions, we aimed to build a model to predict accurate land emissions.\nThe problem is important since accurate quantification of the carbon cycle in\nagroecosystems is crucial for mitigating climate change and ensuring\nsustainable food production. Predicting accurate land emissions is challenging\nsince calibrating the heterogeneous nature of soil properties, moisture, and\nenvironmental conditions is hard at decision-relevant scales. Traditional\napproaches do not adequately estimate land emissions due to\nlocation-independent parameters failing to leverage the spatial heterogeneity\nand also require large datasets. To overcome these limitations, we proposed\nSpatial Distribution-Shift Aware Knowledge-Guided Machine Learning (SDSA-KGML),\nwhich leverages location-dependent parameters that account for significant\nspatial heterogeneity in soil moisture from multiple sites within the same\nregion. Experimental results demonstrate that SDSA-KGML models achieve higher\nlocal accuracy for the specified states in the Midwest Region."}
{"id": "2504.07540", "pdf": "https://arxiv.org/pdf/2504.07540", "abs": "https://arxiv.org/abs/2504.07540", "authors": ["José I. Orlicki"], "title": "PoGO: A Scalable Proof of Useful Work via Quantized Gradient Descent and Merkle Proofs", "categories": ["cs.LG", "cs.AI"], "comment": "14 pages, 1 figure, 1 table", "summary": "We present a design called Proof of Gradient Optimization (PoGO) for\nblockchain consensus, where miners produce verifiable evidence of training\nlarge-scale machine-learning models. Building on previous work, we incorporate\nquantized gradients (4-bit precision) to reduce storage and computation\nrequirements, while still preserving the ability of verifiers to check that\nreal progress has been made on lowering the model's loss. Additionally, we\nemploy Merkle proofs over the full 32-bit model to handle large parameter sets\nand to enable random leaf checks with minimal on-chain data. We illustrate\nthese ideas using GPT-3 (175B parameters) as a reference example and also refer\nto smaller but high-performance models (e.g., Gemma~3 with 27B parameters). We\nprovide an empirical cost analysis showing that verification is significantly\ncheaper than training, thanks in part to quantization and sampling. We also\ndiscuss the necessity of longer block times (potentially hours) when\nincorporating meaningful training steps, the trade-offs when using specialized\nGPU hardware, and how binary diffs may incrementally optimize updates. Finally,\nwe note that fine-tuning can be handled in a similar manner, merely changing\nthe dataset and the manner of sampling but preserving the overall verification\nflow. Our protocol allows verifiers to issue either positive or negative\nattestations; these are aggregated at finalization to either confirm the update\nor slash the miner."}
{"id": "2504.13763", "pdf": "https://arxiv.org/pdf/2504.13763", "abs": "https://arxiv.org/abs/2504.13763", "authors": ["Ryota Takatsuki", "Sonia Joseph", "Ippei Fujisawa", "Ryota Kanai"], "title": "Decoding Vision Transformers: the Diffusion Steering Lens", "categories": ["cs.CV", "cs.AI"], "comment": "12 pages, 17 figures. Accepted to the CVPR 2025 Workshop on\n  Mechanistic Interpretability for Vision (MIV)", "summary": "Logit Lens is a widely adopted method for mechanistic interpretability of\ntransformer-based language models, enabling the analysis of how internal\nrepresentations evolve across layers by projecting them into the output\nvocabulary space. Although applying Logit Lens to Vision Transformers (ViTs) is\ntechnically straightforward, its direct use faces limitations in capturing the\nrichness of visual representations. Building on the work of Toker et al.\n(2024)~\\cite{Toker2024-ve}, who introduced Diffusion Lens to visualize\nintermediate representations in the text encoders of text-to-image diffusion\nmodels, we demonstrate that while Diffusion Lens can effectively visualize\nresidual stream representations in image encoders, it fails to capture the\ndirect contributions of individual submodules. To overcome this limitation, we\npropose \\textbf{Diffusion Steering Lens} (DSL), a novel, training-free approach\nthat steers submodule outputs and patches subsequent indirect contributions. We\nvalidate our method through interventional studies, showing that DSL provides\nan intuitive and reliable interpretation of the internal processing in ViTs."}
{"id": "2503.21048", "pdf": "https://arxiv.org/pdf/2503.21048", "abs": "https://arxiv.org/abs/2503.21048", "authors": ["Ichiro Ohta", "Shota Koyanagi", "Kayo Kinjo", "Jun Ohkubo"], "title": "Integrated utilization of equations and small dataset in the Koopman operator: applications to forward and inverse problems", "categories": ["cs.LG"], "comment": "10 pages, 8 figures", "summary": "In recent years, there has been a growing interest in data-driven approaches\nin physics, such as extended dynamic mode decomposition (EDMD). The EDMD\nalgorithm focuses on nonlinear time-evolution systems, and the constructed\nKoopman matrix yields the next-time prediction with only linear matrix-product\noperations. Note that data-driven approaches generally require a large dataset.\nHowever, assume that one has some prior knowledge, even if it may be ambiguous.\nThen, one could achieve sufficient learning from only a small dataset by taking\nadvantage of the prior knowledge. This paper yields methods for incorporating\nambiguous prior knowledge into the EDMD algorithm. The ambiguous prior\nknowledge in this paper corresponds to the underlying time-evolution equations\nwith unknown parameters. First, we apply the proposed method to forward\nproblems, i.e., prediction tasks. Second, we propose a scheme to apply the\nproposed method to inverse problems, i.e., parameter estimation tasks. We\ndemonstrate the learning with only a small dataset using guiding examples,\ni.e., the Duffing and the van der Pol systems."}
{"id": "2504.08169", "pdf": "https://arxiv.org/pdf/2504.08169", "abs": "https://arxiv.org/abs/2504.08169", "authors": ["Jinfeng Zhuang", "Yinrui Li", "Runze Su", "Ke Xu", "Zhixuan Shao", "Kungang Li", "Ling Leng", "Han Sun", "Meng Qi", "Yixiong Meng", "Yang Tang", "Zhifang Liu", "Qifei Shen", "Aayush Mudgal", "Caleb Lu", "Jie Liu", "Hongda Shen"], "title": "On the Practice of Deep Hierarchical Ensemble Network for Ad Conversion Rate Prediction", "categories": ["cs.LG", "cs.AI", "stat.AP", "stat.ML"], "comment": "Accepted by WWW 2025", "summary": "The predictions of click through rate (CTR) and conversion rate (CVR) play a\ncrucial role in the success of ad-recommendation systems. A Deep Hierarchical\nEnsemble Network (DHEN) has been proposed to integrate multiple feature\ncrossing modules and has achieved great success in CTR prediction. However, its\nperformance for CVR prediction is unclear in the conversion ads setting, where\nan ad bids for the probability of a user's off-site actions on a third party\nwebsite or app, including purchase, add to cart, sign up, etc. A few challenges\nin DHEN: 1) What feature-crossing modules (MLP, DCN, Transformer, to name a\nfew) should be included in DHEN? 2) How deep and wide should DHEN be to achieve\nthe best trade-off between efficiency and efficacy? 3) What hyper-parameters to\nchoose in each feature-crossing module? Orthogonal to the model architecture,\nthe input personalization features also significantly impact model performance\nwith a high degree of freedom. In this paper, we attack this problem and\npresent our contributions biased to the applied data science side, including:\n  First, we propose a multitask learning framework with DHEN as the single\nbackbone model architecture to predict all CVR tasks, with a detailed study on\nhow to make DHEN work effectively in practice; Second, we build both on-site\nreal-time user behavior sequences and off-site conversion event sequences for\nCVR prediction purposes, and conduct ablation study on its importance; Last but\nnot least, we propose a self-supervised auxiliary loss to predict future\nactions in the input sequence, to help resolve the label sparseness issue in\nCVR prediction.\n  Our method achieves state-of-the-art performance compared to previous single\nfeature crossing modules with pre-trained user personalization features."}
{"id": "2504.14509", "pdf": "https://arxiv.org/pdf/2504.14509", "abs": "https://arxiv.org/abs/2504.14509", "authors": ["Fulong Ye", "Miao Hua", "Pengze Zhang", "Xinghui Li", "Qichao Sun", "Songtao Zhao", "Qian He", "Xinglong Wu"], "title": "DreamID: High-Fidelity and Fast diffusion-based Face Swapping via Triplet ID Group Learning", "categories": ["cs.CV", "cs.AI"], "comment": "Project: https://superhero-7.github.io/DreamID/", "summary": "In this paper, we introduce DreamID, a diffusion-based face swapping model\nthat achieves high levels of ID similarity, attribute preservation, image\nfidelity, and fast inference speed. Unlike the typical face swapping training\nprocess, which often relies on implicit supervision and struggles to achieve\nsatisfactory results. DreamID establishes explicit supervision for face\nswapping by constructing Triplet ID Group data, significantly enhancing\nidentity similarity and attribute preservation. The iterative nature of\ndiffusion models poses challenges for utilizing efficient image-space loss\nfunctions, as performing time-consuming multi-step sampling to obtain the\ngenerated image during training is impractical. To address this issue, we\nleverage the accelerated diffusion model SD Turbo, reducing the inference steps\nto a single iteration, enabling efficient pixel-level end-to-end training with\nexplicit Triplet ID Group supervision. Additionally, we propose an improved\ndiffusion-based model architecture comprising SwapNet, FaceNet, and ID Adapter.\nThis robust architecture fully unlocks the power of the Triplet ID Group\nexplicit supervision. Finally, to further extend our method, we explicitly\nmodify the Triplet ID Group data during training to fine-tune and preserve\nspecific attributes, such as glasses and face shape. Extensive experiments\ndemonstrate that DreamID outperforms state-of-the-art methods in terms of\nidentity similarity, pose and expression preservation, and image fidelity.\nOverall, DreamID achieves high-quality face swapping results at 512*512\nresolution in just 0.6 seconds and performs exceptionally well in challenging\nscenarios such as complex lighting, large angles, and occlusions."}
{"id": "2504.05812", "pdf": "https://arxiv.org/pdf/2504.05812", "abs": "https://arxiv.org/abs/2504.05812", "authors": ["Qingyang Zhang", "Haitao Wu", "Changqing Zhang", "Peilin Zhao", "Yatao Bian"], "title": "Right Question is Already Half the Answer: Fully Unsupervised LLM Reasoning Incentivization", "categories": ["cs.LG"], "comment": "Ongoing work. First released on April 8, 2025. Updated the natural\n  reasoning results on April 23, 2025", "summary": "While large language models (LLMs) have demonstrated exceptional capabilities\nin challenging tasks such as mathematical reasoning, existing methods to\nenhance reasoning ability predominantly rely on supervised fine-tuning (SFT)\nfollowed by reinforcement learning (RL) on reasoning-specific data after\npre-training. However, these approaches critically depend on external\nsupervision--such as human-labelled reasoning traces, verified golden answers,\nor pre-trained reward models--which limits scalability and practical\napplicability. In this work, we propose Entropy Minimized Policy Optimization\n(EMPO), which makes an early attempt at fully unsupervised LLM reasoning\nincentivization. EMPO does not require any supervised information for\nincentivizing reasoning capabilities (i.e., neither verifiable reasoning\ntraces, problems with golden answers, nor additional pre-trained reward\nmodels). By continuously minimizing the predictive entropy of LLMs on unlabeled\nuser queries in a latent semantic space, EMPO enables purely self-supervised\nevolution of reasoning capabilities with strong flexibility and practicality.\nOur experiments demonstrate competitive performance of EMPO on both\nmathematical reasoning and free-form natural reasoning tasks. Specifically,\nwithout any supervised signals, \\ours boosts the accuracy of Qwen2.5-Math-7B\nBase from 30.7\\% to 48.1\\% on mathematical benchmarks and improves the accuracy\nof Qwen2.5-7B Base from 32.1\\% to 50.1\\% on MMLU-Pro."}
{"id": "2504.12557", "pdf": "https://arxiv.org/pdf/2504.12557", "abs": "https://arxiv.org/abs/2504.12557", "authors": ["Siow Meng Low", "Akshat Kumar"], "title": "TraCeS: Trajectory Based Credit Assignment From Sparse Safety Feedback", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "In safe reinforcement learning (RL), auxiliary safety costs are used to align\nthe agent to safe decision making. In practice, safety constraints, including\ncost functions and budgets, are unknown or hard to specify, as it requires\nanticipation of all possible unsafe behaviors. We therefore address a general\nsetting where the true safety definition is unknown, and has to be learned from\nsparsely labeled data. Our key contributions are: first, we design a safety\nmodel that performs credit assignment to estimate each decision step's impact\non the overall safety using a dataset of diverse trajectories and their\ncorresponding binary safety labels (i.e., whether the corresponding trajectory\nis safe/unsafe). Second, we illustrate the architecture of our safety model to\ndemonstrate its ability to learn a separate safety score for each timestep.\nThird, we reformulate the safe RL problem using the proposed safety model and\nderive an effective algorithm to optimize a safe yet rewarding policy. Finally,\nour empirical results corroborate our findings and show that this approach is\neffective in satisfying unknown safety definition, and scalable to various\ncontinuous control tasks."}
{"id": "2504.14921", "pdf": "https://arxiv.org/pdf/2504.14921", "abs": "https://arxiv.org/abs/2504.14921", "authors": ["Songping Wang", "Hanqing Liu", "Yueming Lyu", "Xiantao Hu", "Ziwen He", "Wei Wang", "Caifeng Shan", "Liang Wang"], "title": "Fast Adversarial Training with Weak-to-Strong Spatial-Temporal Consistency in the Frequency Domain on Videos", "categories": ["cs.CV", "cs.AI"], "comment": "After the submission of the paper, we realized that the study still\n  has room for expansion. In order to make the research findings more profound\n  and comprehensive, we have decided to withdraw the paper so that we can\n  conduct further research and expansion", "summary": "Adversarial Training (AT) has been shown to significantly enhance adversarial\nrobustness via a min-max optimization approach. However, its effectiveness in\nvideo recognition tasks is hampered by two main challenges. First, fast\nadversarial training for video models remains largely unexplored, which\nseverely impedes its practical applications. Specifically, most video\nadversarial training methods are computationally costly, with long training\ntimes and high expenses. Second, existing methods struggle with the trade-off\nbetween clean accuracy and adversarial robustness. To address these challenges,\nwe introduce Video Fast Adversarial Training with Weak-to-Strong consistency\n(VFAT-WS), the first fast adversarial training method for video data.\nSpecifically, VFAT-WS incorporates the following key designs: First, it\nintegrates a straightforward yet effective temporal frequency augmentation\n(TF-AUG), and its spatial-temporal enhanced form STF-AUG, along with a\nsingle-step PGD attack to boost training efficiency and robustness. Second, it\ndevises a weak-to-strong spatial-temporal consistency regularization, which\nseamlessly integrates the simpler TF-AUG and the more complex STF-AUG.\nLeveraging the consistency regularization, it steers the learning process from\nsimple to complex augmentations. Both of them work together to achieve a better\ntrade-off between clean accuracy and robustness. Extensive experiments on\nUCF-101 and HMDB-51 with both CNN and Transformer-based models demonstrate that\nVFAT-WS achieves great improvements in adversarial robustness and corruption\nrobustness, while accelerating training by nearly 490%."}
{"id": "2504.13228", "pdf": "https://arxiv.org/pdf/2504.13228", "abs": "https://arxiv.org/abs/2504.13228", "authors": ["Anna C. M. Thöni", "Yoram Bachrach", "Tal Kachman"], "title": "Modelling Mean-Field Games with Neural Ordinary Differential Equations", "categories": ["cs.LG", "cs.GT"], "comment": null, "summary": "Mean-field game theory relies on approximating games that would otherwise\nhave been intractable to model. While the games can be solved analytically via\nthe associated system of partial derivatives, this approach is not model-free,\ncan lead to the loss of the existence or uniqueness of solutions and may suffer\nfrom modelling bias. To reduce the dependency between the model and the game,\nwe combine mean-field game theory with deep learning in the form of neural\nordinary differential equations. The resulting model is data-driven,\nlightweight and can learn extensive strategic interactions that are hard to\ncapture using mean-field theory alone. In addition, the model is based on\nautomatic differentiation, making it more robust and objective than approaches\nbased on finite differences. We highlight the efficiency and flexibility of our\napproach by solving three mean-field games that vary in their complexity,\nobservability and the presence of noise. Using these results, we show that the\nmodel is flexible, lightweight and requires few observations to learn the\ndistribution underlying the data."}
{"id": "2504.12609", "pdf": "https://arxiv.org/pdf/2504.12609", "abs": "https://arxiv.org/abs/2504.12609", "authors": ["Tyler Ga Wei Lum", "Olivia Y. Lee", "C. Karen Liu", "Jeannette Bohg"], "title": "Crossing the Human-Robot Embodiment Gap with Sim-to-Real RL using One Human Demonstration", "categories": ["cs.RO", "cs.AI"], "comment": "15 pages, 13 figures", "summary": "Teaching robots dexterous manipulation skills often requires collecting\nhundreds of demonstrations using wearables or teleoperation, a process that is\nchallenging to scale. Videos of human-object interactions are easier to collect\nand scale, but leveraging them directly for robot learning is difficult due to\nthe lack of explicit action labels from videos and morphological differences\nbetween robot and human hands. We propose Human2Sim2Robot, a novel\nreal-to-sim-to-real framework for training dexterous manipulation policies\nusing only one RGB-D video of a human demonstrating a task. Our method utilizes\nreinforcement learning (RL) in simulation to cross the human-robot embodiment\ngap without relying on wearables, teleoperation, or large-scale data collection\ntypically necessary for imitation learning methods. From the demonstration, we\nextract two task-specific components: (1) the object pose trajectory to define\nan object-centric, embodiment-agnostic reward function, and (2) the\npre-manipulation hand pose to initialize and guide exploration during RL\ntraining. We found that these two components are highly effective for learning\nthe desired task, eliminating the need for task-specific reward shaping and\ntuning. We demonstrate that Human2Sim2Robot outperforms object-aware open-loop\ntrajectory replay by 55% and imitation learning with data augmentation by 68%\nacross grasping, non-prehensile manipulation, and multi-step tasks. Project\nSite: https://human2sim2robot.github.io"}
{"id": "2504.15865", "pdf": "https://arxiv.org/pdf/2504.15865", "abs": "https://arxiv.org/abs/2504.15865", "authors": ["Lotfi Abdelkrim Mecharbat", "Ibrahim Almakky", "Martin Takac", "Mohammad Yaqub"], "title": "MedNNS: Supernet-based Medical Task-Adaptive Neural Network Search", "categories": ["cs.CV", "cs.AI", "cs.LG"], "comment": null, "summary": "Deep learning (DL) has achieved remarkable progress in the field of medical\nimaging. However, adapting DL models to medical tasks remains a significant\nchallenge, primarily due to two key factors: (1) architecture selection, as\ndifferent tasks necessitate specialized model designs, and (2) weight\ninitialization, which directly impacts the convergence speed and final\nperformance of the models. Although transfer learning from ImageNet is a widely\nadopted strategy, its effectiveness is constrained by the substantial\ndifferences between natural and medical images. To address these challenges, we\nintroduce Medical Neural Network Search (MedNNS), the first Neural Network\nSearch framework for medical imaging applications. MedNNS jointly optimizes\narchitecture selection and weight initialization by constructing a meta-space\nthat encodes datasets and models based on how well they perform together. We\nbuild this space using a Supernetwork-based approach, expanding the model zoo\nsize by 51x times over previous state-of-the-art (SOTA) methods. Moreover, we\nintroduce rank loss and Fr\\'echet Inception Distance (FID) loss into the\nconstruction of the space to capture inter-model and inter-dataset\nrelationships, thereby achieving more accurate alignment in the meta-space.\nExperimental results across multiple datasets demonstrate that MedNNS\nsignificantly outperforms both ImageNet pre-trained DL models and SOTA Neural\nArchitecture Search (NAS) methods, achieving an average accuracy improvement of\n1.7% across datasets while converging substantially faster. The code and the\nprocessed meta-space is available at https://github.com/BioMedIA-MBZUAI/MedNNS."}
{"id": "2504.13945", "pdf": "https://arxiv.org/pdf/2504.13945", "abs": "https://arxiv.org/abs/2504.13945", "authors": ["Zhanglin Wu", "Tengfei Song", "Ning Xie", "Mengli Zhu", "Weidong Zhang", "Shuang Wu", "Pengfei Li", "Chong Li", "Junhao Zhu", "Hao Yang", "Shiliang Sun"], "title": "Evaluating Menu OCR and Translation: A Benchmark for Aligning Human and Automated Evaluations in Large Vision-Language Models", "categories": ["cs.LG", "cs.AI"], "comment": "12 pages, 5 figures, 5 Tables", "summary": "The rapid advancement of large vision-language models (LVLMs) has\nsignificantly propelled applications in document understanding, particularly in\noptical character recognition (OCR) and multilingual translation. However,\ncurrent evaluations of LVLMs, like the widely used OCRBench, mainly focus on\nverifying the correctness of their short-text responses and long-text responses\nwith simple layout, while the evaluation of their ability to understand long\ntexts with complex layout design is highly significant but largely overlooked.\nIn this paper, we propose Menu OCR and Translation Benchmark (MOTBench), a\nspecialized evaluation framework emphasizing the pivotal role of menu\ntranslation in cross-cultural communication. MOTBench requires LVLMs to\naccurately recognize and translate each dish, along with its price and unit\nitems on a menu, providing a comprehensive assessment of their visual\nunderstanding and language processing capabilities. Our benchmark is comprised\nof a collection of Chinese and English menus, characterized by intricate\nlayouts, a variety of fonts, and culturally specific elements across different\nlanguages, along with precise human annotations. Experiments show that our\nautomatic evaluation results are highly consistent with professional human\nevaluation. We evaluate a range of publicly available state-of-the-art LVLMs,\nand through analyzing their output to identify the strengths and weaknesses in\ntheir performance, offering valuable insights to guide future advancements in\nLVLM development. MOTBench is available at https://github.com/gitwzl/MOTBench."}
{"id": "2504.13969", "pdf": "https://arxiv.org/pdf/2504.13969", "abs": "https://arxiv.org/abs/2504.13969", "authors": ["Nayoung Choi", "Peace Cyebukayire", "Jinho D. Choi"], "title": "Tinker Tales: Interactive Storytelling Framework for Early Childhood Narrative Development and AI Literacy", "categories": ["cs.HC", "cs.AI", "cs.CY"], "comment": null, "summary": "This paper presents Tinker Tales, an interactive storytelling framework in\nthe format of a board game, designed to support both narrative development and\nAI literacy in early childhood. The framework integrates tangible and\nspeech-based interactions with AI through NFC chip-attached pawns and tokens,\nalong with a speaker and microphone. Children select and define key story\nelements-such as characters, places, items, and emotions-using the pawns and\ntokens, providing further details to the AI and receiving proper assistance,\nsimilar to how adults prompt AI for specific tasks (e.g., writing). For\nevaluation, several game sessions were simulated with a child AI agent, and the\nquality and safety of the generated stories were assessed from various\nperspectives. This work highlights the potential of combining physical and\ndigital elements in AI literacy, offering a safe and engaging way for children\nto learn how to effectively collaborate with AI."}
{"id": "2504.15918", "pdf": "https://arxiv.org/pdf/2504.15918", "abs": "https://arxiv.org/abs/2504.15918", "authors": ["Chang Zong", "Bin Li", "Shoujun Zhou", "Jian Wan", "Lei Zhang"], "title": "Ask2Loc: Learning to Locate Instructional Visual Answers by Asking Questions", "categories": ["cs.CV", "cs.AI", "cs.HC", "68T45, 68T20"], "comment": "16 pages, 8 figures", "summary": "Locating specific segments within an instructional video is an efficient way\nto acquire guiding knowledge. Generally, the task of obtaining video segments\nfor both verbal explanations and visual demonstrations is known as visual\nanswer localization (VAL). However, users often need multiple interactions to\nobtain answers that align with their expectations when using the system. During\nthese interactions, humans deepen their understanding of the video content by\nasking themselves questions, thereby accurately identifying the location.\nTherefore, we propose a new task, named In-VAL, to simulate the multiple\ninteractions between humans and videos in the procedure of obtaining visual\nanswers. The In-VAL task requires interactively addressing several semantic gap\nissues, including 1) the ambiguity of user intent in the input questions, 2)\nthe incompleteness of language in video subtitles, and 3) the fragmentation of\ncontent in video segments. To address these issues, we propose Ask2Loc, a\nframework for resolving In-VAL by asking questions. It includes three key\nmodules: 1) a chatting module to refine initial questions and uncover clear\nintentions, 2) a rewriting module to generate fluent language and create\ncomplete descriptions, and 3) a searching module to broaden local context and\nprovide integrated content. We conduct extensive experiments on three\nreconstructed In-VAL datasets. Compared to traditional end-to-end and two-stage\nmethods, our proposed Ask2Loc can improve performance by up to 14.91 (mIoU) on\nthe In-VAL task. Our code and datasets can be accessed at\nhttps://github.com/changzong/Ask2Loc."}
{"id": "2504.14051", "pdf": "https://arxiv.org/pdf/2504.14051", "abs": "https://arxiv.org/abs/2504.14051", "authors": ["Raghavv Goel", "Junyoung Park", "Mukul Gagrani", "Dalton Jones", "Matthew Morse", "Harper Langston", "Mingu Lee", "Chris Lott"], "title": "CAOTE: KV Caching through Attention Output Error based Token Eviction", "categories": ["cs.LG"], "comment": "14 pages, 2 figures", "summary": "While long context support of large language models has extended their\nabilities, it also incurs challenges in memory and compute which becomes\ncrucial bottlenecks in resource-restricted devices. Token eviction, a widely\nadopted post-training methodology designed to alleviate the bottlenecks by\nevicting less important tokens from the cache, typically uses attention scores\nas proxy metrics for token importance. However, one major limitation of\nattention score as a token-wise importance metrics is that it lacks the\ninformation about contribution of tokens to the attention output. In this\npaper, we propose a simple eviction criterion based on the contribution of\ncached tokens to attention outputs. Our method, CAOTE, optimizes for eviction\nerror due to token eviction, by seamlessly integrating attention scores and\nvalue vectors. This is the first method which uses value vector information on\ntop of attention-based eviction scores. Additionally, CAOTE can act as a\nmeta-heuristic method with flexible usage with any token eviction method. We\nshow that CAOTE, when combined with the state-of-the-art attention score-based\nmethods, always improves accuracies on the downstream task, indicating the\nimportance of leveraging information from values during token eviction process."}
{"id": "2504.14985", "pdf": "https://arxiv.org/pdf/2504.14985", "abs": "https://arxiv.org/abs/2504.14985", "authors": ["Fatih Deniz", "Dorde Popovic", "Yazan Boshmaf", "Euisuh Jeong", "Minhaj Ahmad", "Sanjay Chawla", "Issa Khalil"], "title": "aiXamine: Simplified LLM Safety and Security", "categories": ["cs.CR", "cs.AI"], "comment": null, "summary": "Evaluating Large Language Models (LLMs) for safety and security remains a\ncomplex task, often requiring users to navigate a fragmented landscape of ad\nhoc benchmarks, datasets, metrics, and reporting formats. To address this\nchallenge, we present aiXamine, a comprehensive black-box evaluation platform\nfor LLM safety and security. aiXamine integrates over 40 tests (i.e.,\nbenchmarks) organized into eight key services targeting specific dimensions of\nsafety and security: adversarial robustness, code security, fairness and bias,\nhallucination, model and data privacy, out-of-distribution (OOD) robustness,\nover-refusal, and safety alignment. The platform aggregates the evaluation\nresults into a single detailed report per model, providing a detailed breakdown\nof model performance, test examples, and rich visualizations. We used aiXamine\nto assess over 50 publicly available and proprietary LLMs, conducting over 2K\nexaminations. Our findings reveal notable vulnerabilities in leading models,\nincluding susceptibility to adversarial attacks in OpenAI's GPT-4o, biased\noutputs in xAI's Grok-3, and privacy weaknesses in Google's Gemini 2.0.\nAdditionally, we observe that open-source models can match or exceed\nproprietary models in specific services such as safety alignment, fairness and\nbias, and OOD robustness. Finally, we identify trade-offs between distillation\nstrategies, model size, training methods, and architectural choices."}
{"id": "2412.05053", "pdf": "https://arxiv.org/pdf/2412.05053", "abs": "https://arxiv.org/abs/2412.05053", "authors": ["Kaizhen Sun", "Jinghang Li", "Kuan Dai", "Bangyan Liao", "Wei Xiong", "Yi Zhou"], "title": "EvTTC: An Event Camera Dataset for Time-to-Collision Estimation", "categories": ["cs.RO", "cs.CV"], "comment": "10 pages, 7 figures, 5 tables", "summary": "Time-to-Collision (TTC) estimation lies in the core of the forward collision\nwarning (FCW) functionality, which is key to all Automatic Emergency Braking\n(AEB) systems. Although the success of solutions using frame-based cameras\n(e.g., Mobileye's solutions) has been witnessed in normal situations, some\nextreme cases, such as the sudden variation in the relative speed of leading\nvehicles and the sudden appearance of pedestrians, still pose significant risks\nthat cannot be handled. This is due to the inherent imaging principles of\nframe-based cameras, where the time interval between adjacent exposures\nintroduces considerable system latency to AEB. Event cameras, as a novel\nbio-inspired sensor, offer ultra-high temporal resolution and can\nasynchronously report brightness changes at the microsecond level. To explore\nthe potential of event cameras in the above-mentioned challenging cases, we\npropose EvTTC, which is, to the best of our knowledge, the first multi-sensor\ndataset focusing on TTC tasks under high-relative-speed scenarios. EvTTC\nconsists of data collected using standard cameras and event cameras, covering\nvarious potential collision scenarios in daily driving and involving multiple\ncollision objects. Additionally, LiDAR and GNSS/INS measurements are provided\nfor the calculation of ground-truth TTC. Considering the high cost of testing\nTTC algorithms on full-scale mobile platforms, we also provide a small-scale\nTTC testbed for experimental validation and data augmentation. All the data and\nthe design of the testbed are open sourced, and they can serve as a benchmark\nthat will facilitate the development of vision-based TTC techniques."}
{"id": "2504.14960", "pdf": "https://arxiv.org/pdf/2504.14960", "abs": "https://arxiv.org/abs/2504.14960", "authors": ["Dennis Liu", "Zijie Yan", "Xin Yao", "Tong Liu", "Vijay Korthikanti", "Evan Wu", "Shiqing Fan", "Gao Deng", "Hongxiao Bai", "Jianbin Chang", "Ashwath Aithal", "Michael Andersch", "Mohammad Shoeybi", "Jiajie Yao", "Chandler Zhou", "David Wu", "Xipeng Li", "June Yang"], "title": "MoE Parallel Folding: Heterogeneous Parallelism Mappings for Efficient Large-Scale MoE Model Training with Megatron Core", "categories": ["cs.LG", "cs.DC"], "comment": null, "summary": "Mixture of Experts (MoE) models enhance neural network scalability by\ndynamically selecting relevant experts per input token, enabling larger model\nsizes while maintaining manageable computation costs. However, efficient\ntraining of large-scale MoE models across thousands of GPUs presents\nsignificant challenges due to limitations in existing parallelism strategies.\nWe introduce an end-to-end training framework for large-scale MoE models that\nutilizes five-dimensional hybrid parallelism: Tensor Parallelism, Expert\nParallelism, Context Parallelism, Data Parallelism, and Pipeline Parallelism.\nCentral to our approach is MoE Parallel Folding, a novel strategy that\ndecouples the parallelization of attention and MoE layers in Transformer\nmodels, allowing each layer type to adopt optimal parallel configurations.\nAdditionally, we develop a flexible token-level dispatcher that supports both\ntoken-dropping and token-dropless MoE training across all five dimensions of\nparallelism. This dispatcher accommodates dynamic tensor shapes and coordinates\ndifferent parallelism schemes for Attention and MoE layers, facilitating\ncomplex parallelism implementations. Our experiments demonstrate significant\nimprovements in training efficiency and scalability. We achieve up to 49.3%\nModel Flops Utilization (MFU) for the Mixtral 8x22B model and 39.0% MFU for the\nQwen2-57B-A14B model on H100 GPUs, outperforming existing methods. The\nframework scales efficiently up to 1,024 GPUs and maintains high performance\nwith sequence lengths up to 128K tokens, validating its effectiveness for\nlarge-scale MoE model training. The code is available in Megatron-Core."}
{"id": "2504.15515", "pdf": "https://arxiv.org/pdf/2504.15515", "abs": "https://arxiv.org/abs/2504.15515", "authors": ["Wuchen Li"], "title": "Transport f divergences", "categories": ["math.ST", "cs.AI", "cs.IT", "math.IT", "stat.TH"], "comment": "Comments are welcome", "summary": "We define a class of divergences to measure differences between probability\ndensity functions in one-dimensional sample space. The construction is based on\nthe convex function with the Jacobi operator of mapping function that\npushforwards one density to the other. We call these information measures\ntransport f-divergences. We present several properties of transport\n$f$-divergences, including invariances, convexities, variational formulations,\nand Taylor expansions in terms of mapping functions. Examples of transport\nf-divergences in generative models are provided."}
{"id": "2504.10916", "pdf": "https://arxiv.org/pdf/2504.10916", "abs": "https://arxiv.org/abs/2504.10916", "authors": ["Zhenyu Yang", "Haiming Zhu", "Rihui Zhang", "Haipeng Zhang", "Jianliang Wang", "Chunhao Wang", "Minbin Chen", "Fang-Fang Yin"], "title": "Embedding Radiomics into Vision Transformers for Multimodal Medical Image Classification", "categories": ["physics.med-ph", "cs.CV"], "comment": "27 pages, 3 figures", "summary": "Background: Deep learning has significantly advanced medical image analysis,\nwith Vision Transformers (ViTs) offering a powerful alternative to\nconvolutional models by modeling long-range dependencies through\nself-attention. However, ViTs are inherently data-intensive and lack\ndomain-specific inductive biases, limiting their applicability in medical\nimaging. In contrast, radiomics provides interpretable, handcrafted descriptors\nof tissue heterogeneity but suffers from limited scalability and integration\ninto end-to-end learning frameworks. In this work, we propose the\nRadiomics-Embedded Vision Transformer (RE-ViT) that combines radiomic features\nwith data-driven visual embeddings within a ViT backbone.\n  Purpose: To develop a hybrid RE-ViT framework that integrates radiomics and\npatch-wise ViT embeddings through early fusion, enhancing robustness and\nperformance in medical image classification.\n  Methods: Following the standard ViT pipeline, images were divided into\npatches. For each patch, handcrafted radiomic features were extracted and fused\nwith linearly projected pixel embeddings. The fused representations were\nnormalized, positionally encoded, and passed to the ViT encoder. A learnable\n[CLS] token aggregated patch-level information for classification. We evaluated\nRE-ViT on three public datasets (including BUSI, ChestXray2017, and Retinal\nOCT) using accuracy, macro AUC, sensitivity, and specificity. RE-ViT was\nbenchmarked against CNN-based (VGG-16, ResNet) and hybrid (TransMed) models.\n  Results: RE-ViT achieved state-of-the-art results: on BUSI,\nAUC=0.950+/-0.011; on ChestXray2017, AUC=0.989+/-0.004; on Retinal OCT,\nAUC=0.986+/-0.001, which outperforms other comparison models.\n  Conclusions: The RE-ViT framework effectively integrates radiomics with ViT\narchitectures, demonstrating improved performance and generalizability across\nmultimodal medical image classification tasks."}
{"id": "2504.15806", "pdf": "https://arxiv.org/pdf/2504.15806", "abs": "https://arxiv.org/abs/2504.15806", "authors": ["Kai Luo", "Juan Tang", "Mingchao Cai", "Xiaoqing Zeng", "Manqi Xie", "Ming Yan"], "title": "DAE-KAN: A Kolmogorov-Arnold Network Model for High-Index Differential-Algebraic Equations", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Kolmogorov-Arnold Networks (KANs) have emerged as a promising alternative to\nMulti-layer Perceptrons (MLPs) due to their superior function-fitting abilities\nin data-driven modeling. In this paper, we propose a novel framework, DAE-KAN,\nfor solving high-index differential-algebraic equations (DAEs) by integrating\nKANs with Physics-Informed Neural Networks (PINNs). This framework not only\npreserves the ability of traditional PINNs to model complex systems governed by\nphysical laws but also enhances their performance by leveraging the\nfunction-fitting strengths of KANs. Numerical experiments demonstrate that for\nDAE systems ranging from index-1 to index-3, DAE-KAN reduces the absolute\nerrors of both differential and algebraic variables by 1 to 2 orders of\nmagnitude compared to traditional PINNs. To assess the effectiveness of this\napproach, we analyze the drift-off error and find that both PINNs and DAE-KAN\noutperform classical numerical methods in controlling this phenomenon. Our\nresults highlight the potential of neural network methods, particularly\nDAE-KAN, in solving high-index DAEs with substantial computational accuracy and\ngeneralization, offering a promising solution for challenging partial\ndifferential-algebraic equations."}
{"id": "2504.15876", "pdf": "https://arxiv.org/pdf/2504.15876", "abs": "https://arxiv.org/abs/2504.15876", "authors": ["Qizhen Wu", "Lei Chen", "Kexin Liu", "Jinhu Lü"], "title": "Bidirectional Task-Motion Planning Based on Hierarchical Reinforcement Learning for Strategic Confrontation", "categories": ["cs.RO", "cs.AI"], "comment": null, "summary": "In swarm robotics, confrontation scenarios, including strategic\nconfrontations, require efficient decision-making that integrates discrete\ncommands and continuous actions. Traditional task and motion planning methods\nseparate decision-making into two layers, but their unidirectional structure\nfails to capture the interdependence between these layers, limiting\nadaptability in dynamic environments. Here, we propose a novel bidirectional\napproach based on hierarchical reinforcement learning, enabling dynamic\ninteraction between the layers. This method effectively maps commands to task\nallocation and actions to path planning, while leveraging cross-training\ntechniques to enhance learning across the hierarchical framework. Furthermore,\nwe introduce a trajectory prediction model that bridges abstract task\nrepresentations with actionable planning goals. In our experiments, it achieves\nover 80% in confrontation win rate and under 0.01 seconds in decision time,\noutperforming existing approaches. Demonstrations through large-scale tests and\nreal-world robot experiments further emphasize the generalization capabilities\nand practical applicability of our method."}
{"id": "2504.14373", "pdf": "https://arxiv.org/pdf/2504.14373", "abs": "https://arxiv.org/abs/2504.14373", "authors": ["Chen Guo", "Zhuo Su", "Jian Wang", "Shuang Li", "Xu Chang", "Zhaohu Li", "Yang Zhao", "Guidong Wang", "Ruqi Huang"], "title": "SEGA: Drivable 3D Gaussian Head Avatar from a Single Image", "categories": ["cs.GR", "cs.CV"], "comment": null, "summary": "Creating photorealistic 3D head avatars from limited input has become\nincreasingly important for applications in virtual reality, telepresence, and\ndigital entertainment. While recent advances like neural rendering and 3D\nGaussian splatting have enabled high-quality digital human avatar creation and\nanimation, most methods rely on multiple images or multi-view inputs, limiting\ntheir practicality for real-world use. In this paper, we propose SEGA, a novel\napproach for Single-imagE-based 3D drivable Gaussian head Avatar creation that\ncombines generalized prior models with a new hierarchical UV-space Gaussian\nSplatting framework. SEGA seamlessly combines priors derived from large-scale\n2D datasets with 3D priors learned from multi-view, multi-expression, and\nmulti-ID data, achieving robust generalization to unseen identities while\nensuring 3D consistency across novel viewpoints and expressions. We further\npresent a hierarchical UV-space Gaussian Splatting framework that leverages\nFLAME-based structural priors and employs a dual-branch architecture to\ndisentangle dynamic and static facial components effectively. The dynamic\nbranch encodes expression-driven fine details, while the static branch focuses\non expression-invariant regions, enabling efficient parameter inference and\nprecomputation. This design maximizes the utility of limited 3D data and\nachieves real-time performance for animation and rendering. Additionally, SEGA\nperforms person-specific fine-tuning to further enhance the fidelity and\nrealism of the generated avatars. Experiments show our method outperforms\nstate-of-the-art approaches in generalization ability, identity preservation,\nand expression realism, advancing one-shot avatar creation for practical\napplications."}
{"id": "2504.16020", "pdf": "https://arxiv.org/pdf/2504.16020", "abs": "https://arxiv.org/abs/2504.16020", "authors": ["Soham Sane"], "title": "AlphaGrad: Non-Linear Gradient Normalization Optimizer", "categories": ["cs.LG", "cs.AI", "cs.NE", "stat.ML"], "comment": null, "summary": "We introduce AlphaGrad, a memory-efficient, conditionally stateless optimizer\naddressing the memory overhead and hyperparameter complexity of adaptive\nmethods like Adam. AlphaGrad enforces scale invariance via tensor-wise L2\ngradient normalization followed by a smooth hyperbolic tangent transformation,\n$g' = \\tanh(\\alpha \\cdot \\tilde{g})$, controlled by a single steepness\nparameter $\\alpha$. Our contributions include: (1) the AlphaGrad algorithm\nformulation; (2) a formal non-convex convergence analysis guaranteeing\nstationarity; (3) extensive empirical evaluation on diverse RL benchmarks (DQN,\nTD3, PPO). Compared to Adam, AlphaGrad demonstrates a highly context-dependent\nperformance profile. While exhibiting instability in off-policy DQN, it\nprovides enhanced training stability with competitive results in TD3 (requiring\ncareful $\\alpha$ tuning) and achieves substantially superior performance in\non-policy PPO. These results underscore the critical importance of empirical\n$\\alpha$ selection, revealing strong interactions between the optimizer's\ndynamics and the underlying RL algorithm. AlphaGrad presents a compelling\nalternative optimizer for memory-constrained scenarios and shows significant\npromise for on-policy learning regimes where its stability and efficiency\nadvantages can be particularly impactful."}
{"id": "2504.15305", "pdf": "https://arxiv.org/pdf/2504.15305", "abs": "https://arxiv.org/abs/2504.15305", "authors": ["Abhishek Tyagi", "Charu Gaur"], "title": "SLAM-Based Navigation and Fault Resilience in a Surveillance Quadcopter with Embedded Vision Systems", "categories": ["cs.RO", "cs.CV", "cs.SY", "eess.SY", "68T40, 68U10, 70Q05", "I.2.9; I.4.8; I.2.10; C.3"], "comment": "18 pages, 21 figures, 15 tables. Onboard processing using Raspberry\n  Pi 4 and Arduino Nano. Includes ORB-SLAM3-based navigation, LQR control,\n  rotor fault recovery, object detection, and PCA face recognition. Real-world\n  and simulation tests included. Designed for GPS-denied autonomous UAV\n  surveillance", "summary": "We present an autonomous aerial surveillance platform, Veg, designed as a\nfault-tolerant quadcopter system that integrates visual SLAM for\nGPS-independent navigation, advanced control architecture for dynamic\nstability, and embedded vision modules for real-time object and face\nrecognition. The platform features a cascaded control design with an LQR\ninner-loop and PD outer-loop trajectory control. It leverages ORB-SLAM3 for\n6-DoF localization and loop closure, and supports waypoint-based navigation\nthrough Dijkstra path planning over SLAM-derived maps. A real-time Failure\nDetection and Identification (FDI) system detects rotor faults and executes\nemergency landing through re-routing. The embedded vision system, based on a\nlightweight CNN and PCA, enables onboard object detection and face recognition\nwith high precision. The drone operates fully onboard using a Raspberry Pi 4\nand Arduino Nano, validated through simulations and real-world testing. This\nwork consolidates real-time localization, fault recovery, and embedded AI on a\nsingle platform suitable for constrained environments."}
{"id": "1609.08934", "pdf": "https://arxiv.org/pdf/1609.08934", "abs": "https://arxiv.org/abs/1609.08934", "authors": ["Ioannis Avramopoulos"], "title": "Multiplicative weights, equalizers, and P=PPAD", "categories": ["cs.GT", "cs.CC", "cs.LG"], "comment": "There is an error in Lemma 10", "summary": "We show that, by using multiplicative weights in a game-theoretic thought\nexperiment (and an important convexity result on the composition of\nmultiplicative weights with the relative entropy function), a symmetric\nbimatrix game (that is, a bimatrix matrix wherein the payoff matrix of each\nplayer is the transpose of the payoff matrix of the other) either has an\ninterior symmetric equilibrium or there is a pure strategy that is weakly\ndominated by some mixed strategy. Weakly dominated pure strategies can be\ndetected and eliminated in polynomial time by solving a linear program.\nFurthermore, interior symmetric equilibria are a special case of a more general\nnotion, namely, that of an \"equalizer,\" which can also be computed efficiently\nin polynomial time by solving a linear program. An elegant \"symmetrization\nmethod\" of bimatrix games [Jurg et al., 1992] and the well-known\nPPAD-completeness results on equilibrium computation in bimatrix games\n[Daskalakis et al., 2009, Chen et al., 2009] imply then the compelling P =\nPPAD."}
{"id": "2307.14530", "pdf": "https://arxiv.org/pdf/2307.14530", "abs": "https://arxiv.org/abs/2307.14530", "authors": ["Fedor Noskov", "Maxim Panov"], "title": "Optimal Noise Reduction in Dense Mixed-Membership Stochastic Block Models under Diverging Spiked Eigenvalues Condition", "categories": ["stat.ML", "cs.LG", "cs.SI"], "comment": null, "summary": "Community detection is one of the most critical problems in modern network\nscience. Its applications can be found in various fields, from protein modeling\nto social network analysis. Recently, many papers appeared studying the problem\nof overlapping community detection, where each node of a network may belong to\nseveral communities. In this work, we consider Mixed-Membership Stochastic\nBlock Model (MMSB) first proposed by Airoldi et al. MMSB provides quite a\ngeneral setting for modeling overlapping community structure in graphs. The\ncentral question of this paper is to reconstruct relations between communities\ngiven an observed network. We compare different approaches and establish the\nminimax lower bound on the estimation error. Then, we propose a new estimator\nthat matches this lower bound. Theoretical results are proved under fairly\ngeneral conditions on the considered model. Finally, we illustrate the theory\nin a series of experiments."}
{"id": "2312.03243", "pdf": "https://arxiv.org/pdf/2312.03243", "abs": "https://arxiv.org/abs/2312.03243", "authors": ["Jian Cheng Wong", "Chin Chun Ooi", "Abhishek Gupta", "Pao-Hsiung Chiu", "Joshua Shao Zheng Low", "My Ha Dao", "Yew-Soon Ong"], "title": "Evolutionary Optimization of Physics-Informed Neural Networks: Advancing Generalizability by the Baldwin Effect", "categories": ["cs.NE", "cs.CE", "cs.LG"], "comment": null, "summary": "Physics-informed neural networks (PINNs) are at the forefront of scientific\nmachine learning, making possible the creation of machine intelligence that is\ncognizant of physical laws and able to accurately simulate them. However,\ntoday's PINNs are often trained for a single physics task and require\ncomputationally expensive re-training for each new task, even for tasks from\nsimilar physics domains. To address this limitation, this paper proposes a\npioneering approach to advance the generalizability of PINNs through the\nframework of Baldwinian evolution. Drawing inspiration from the\nneurodevelopment of precocial species that have evolved to learn, predict and\nreact quickly to their environment, we envision PINNs that are pre-wired with\nconnection strengths inducing strong biases towards efficient learning of\nphysics. A novel two-stage stochastic programming formulation coupling\nevolutionary selection pressure (based on proficiency over a distribution of\nphysics tasks) with lifetime learning (to specialize on a sampled subset of\nthose tasks) is proposed to instantiate the Baldwin effect. The evolved\nBaldwinian-PINNs demonstrate fast and physics-compliant prediction capabilities\nacross a range of empirically challenging problem instances with more than an\norder of magnitude improvement in prediction accuracy at a fraction of the\ncomputation cost compared to state-of-the-art gradient-based meta-learning\nmethods. For example, when solving the diffusion-reaction equation, a 70x\nimprovement in accuracy was obtained while taking 700x less computational time.\nThis paper thus marks a leap forward in the meta-learning of PINNs as\ngeneralizable physics solvers. Sample codes are available at\nhttps://github.com/chiuph/Baldwinian-PINN."}
{"id": "2401.08637", "pdf": "https://arxiv.org/pdf/2401.08637", "abs": "https://arxiv.org/abs/2401.08637", "authors": ["Taesik Gong", "Si Young Jang", "Utku Günay Acer", "Fahim Kawsar", "Chulhong Min"], "title": "Synergy: Towards On-Body AI via Tiny AI Accelerator Collaboration on Wearables", "categories": ["cs.DC", "cs.LG"], "comment": "Accepted for publication in IEEE Transactions on Mobile Computing\n  (TMC)", "summary": "The advent of tiny artificial intelligence (AI) accelerators enables AI to\nrun at the extreme edge, offering reduced latency, lower power cost, and\nimproved privacy. When integrated into wearable devices, these accelerators\nopen exciting opportunities, allowing various AI apps to run directly on the\nbody. We present Synergy that provides AI apps with best-effort performance via\nsystem-driven holistic collaboration over AI accelerator-equipped wearables. To\nachieve this, Synergy provides device-agnostic programming interfaces to AI\napps, giving the system visibility and controllability over the app's resource\nuse. Then, Synergy maximizes the inference throughput of concurrent AI models\nby creating various execution plans for each app considering AI accelerator\navailability and intelligently selecting the best set of execution plans.\nSynergy further improves throughput by leveraging parallelization opportunities\nover multiple computation units. Our evaluations with 7 baselines and 8 models\ndemonstrate that, on average, Synergy achieves a 23.0 times improvement in\nthroughput, while reducing latency by 73.9% and power consumption by 15.8%,\ncompared to the baselines."}
{"id": "2406.10959", "pdf": "https://arxiv.org/pdf/2406.10959", "abs": "https://arxiv.org/abs/2406.10959", "authors": ["Jin Ma", "Gaozhan Wang", "Jianfeng Zhang"], "title": "Convergence Analysis for Entropy-Regularized Control Problems: A Probabilistic Approach", "categories": ["math.OC", "cs.LG", "93E35, 60H30, 35Q93"], "comment": "In this version, we have modified the title and improved the\n  convergence rate to a super-exponential one", "summary": "In this paper we investigate the convergence of the Policy Iteration\nAlgorithm (PIA) for a class of general continuous-time entropy-regularized\nstochastic control problems. In particular, instead of employing sophisticated\nPDE estimates for the iterative PDEs involved in the algorithm (see, e.g.,\nHuang-Wang-Zhou(2025)), we shall provide a simple proof from scratch for the\nconvergence of the PIA. Our approach builds on probabilistic representation\nformulae for solutions of PDEs and their derivatives. Moreover, in the finite\nhorizon model and in the infinite horizon model with large discount factor, the\nsimilar arguments lead to a super-exponential rate of convergence without tear.\nFinally, with some extra efforts we show that our approach can be extended to\nthe diffusion control case in the one dimensional setting, also with a\nsuper-exponential rate of convergence."}
{"id": "2408.12986", "pdf": "https://arxiv.org/pdf/2408.12986", "abs": "https://arxiv.org/abs/2408.12986", "authors": ["Niklas Risse", "Jing Liu", "Marcel Böhme"], "title": "Top Score on the Wrong Exam: On Benchmarking in Machine Learning for Vulnerability Detection", "categories": ["cs.CR", "cs.LG"], "comment": "Accepted at the 34th ACM SIGSOFT International Symposium on Software\n  Testing and Analysis (ISSTA 2025)", "summary": "According to our survey of machine learning for vulnerability detection\n(ML4VD), 9 in every 10 papers published in the past five years define ML4VD as\na function-level binary classification problem:\n  Given a function, does it contain a security flaw?\n  From our experience as security researchers, faced with deciding whether a\ngiven function makes the program vulnerable to attacks, we would often first\nwant to understand the context in which this function is called.\n  In this paper, we study how often this decision can really be made without\nfurther context and study both vulnerable and non-vulnerable functions in the\nmost popular ML4VD datasets. We call a function \"vulnerable\" if it was involved\nin a patch of an actual security flaw and confirmed to cause the program's\nvulnerability. It is \"non-vulnerable\" otherwise. We find that in almost all\ncases this decision cannot be made without further context. Vulnerable\nfunctions are often vulnerable only because a corresponding\nvulnerability-inducing calling context exists while non-vulnerable functions\nwould often be vulnerable if a corresponding context existed.\n  But why do ML4VD techniques achieve high scores even though there is\ndemonstrably not enough information in these samples? Spurious correlations: We\nfind that high scores can be achieved even when only word counts are available.\nThis shows that these datasets can be exploited to achieve high scores without\nactually detecting any security vulnerabilities.\n  We conclude that the prevailing problem statement of ML4VD is ill-defined and\ncall into question the internal validity of this growing body of work.\nConstructively, we call for more effective benchmarking methodologies to\nevaluate the true capabilities of ML4VD, propose alternative problem\nstatements, and examine broader implications for the evaluation of machine\nlearning and programming analysis research."}
{"id": "2409.17277", "pdf": "https://arxiv.org/pdf/2409.17277", "abs": "https://arxiv.org/abs/2409.17277", "authors": ["Tongfe Guo", "Taposh Banerjee", "Rui Liu", "Lili Su"], "title": "Building Real-time Awareness of Out-of-distribution in Trajectory Prediction for Autonomous Vehicles", "categories": ["cs.RO", "cs.LG"], "comment": null, "summary": "Accurate trajectory prediction is essential for the safe operation of\nautonomous vehicles in real-world environments. Even well-trained machine\nlearning models may produce unreliable predictions due to discrepancies between\ntraining data and real-world conditions encountered during inference. In\nparticular, the training dataset tends to overrepresent common scenes (e.g.,\nstraight lanes) while underrepresenting less frequent ones (e.g., traffic\ncircles). In addition, it often overlooks unpredictable real-world events such\nas sudden braking or falling objects. To ensure safety, it is critical to\ndetect in real-time when a model's predictions become unreliable. Leveraging\nthe intuition that in-distribution (ID) scenes exhibit error patterns similar\nto training data, while out-of-distribution (OOD) scenes do not, we introduce a\nprincipled, real-time approach for OOD detection by framing it as a\nchange-point detection problem. We address the challenging settings where the\nOOD scenes are deceptive, meaning that they are not easily detectable by human\nintuitions. Our lightweight solutions can handle the occurrence of OOD at any\ntime during trajectory prediction inference. Experimental results on multiple\nreal-world datasets using a benchmark trajectory prediction model demonstrate\nthe effectiveness of our methods."}
{"id": "2411.10959", "pdf": "https://arxiv.org/pdf/2411.10959", "abs": "https://arxiv.org/abs/2411.10959", "authors": ["Ashesh Rambachan", "Rahul Singh", "Davide Viviano"], "title": "Program Evaluation with Remotely Sensed Outcomes", "categories": ["econ.EM", "cs.LG", "math.ST", "stat.AP", "stat.ME", "stat.ML", "stat.TH"], "comment": null, "summary": "Economists often estimate treatment effects in experiments using remotely\nsensed variables (RSVs), e.g. satellite images or mobile phone activity, in\nplace of directly measured economic outcomes. A common practice is to use an\nobservational sample to train a predictor of the economic outcome from the RSV,\nand then to use its predictions as the outcomes in the experiment. We show that\nthis method is biased whenever the RSV is post-outcome, i.e. if variation in\nthe economic outcome causes variation in the RSV. In program evaluation,\nchanges in poverty or environmental quality cause changes in satellite images,\nbut not vice versa. As our main result, we nonparametrically identify the\ntreatment effect by formalizing the intuition that underlies common practice:\nthe conditional distribution of the RSV given the outcome and treatment is\nstable across the samples.Based on our identifying formula, we find that the\nefficient representation of RSVs for causal inference requires three\npredictions rather than one. Valid inference does not require any rate\nconditions on RSV predictions, justifying the use of complex deep learning\nalgorithms with unknown statistical properties. We re-analyze the effect of an\nanti-poverty program in India using satellite images."}
{"id": "2501.07294", "pdf": "https://arxiv.org/pdf/2501.07294", "abs": "https://arxiv.org/abs/2501.07294", "authors": ["Tri Kurniawan Wijaya", "Edoardo D'Amico", "Xinyang Shao"], "title": "Dataset-Agnostic Recommender Systems", "categories": ["cs.IR", "cs.LG"], "comment": null, "summary": "Recommender systems have become a cornerstone of personalized user\nexperiences, yet their development typically involves significant manual\nintervention, including dataset-specific feature engineering, hyperparameter\ntuning, and configuration. To this end, we introduce a novel paradigm:\nDataset-Agnostic Recommender Systems (DAReS) that aims to enable a single\ncodebase to autonomously adapt to various datasets without the need for\nfine-tuning, for a given recommender system task. Central to this approach is\nthe Dataset Description Language (DsDL), a structured format that provides\nmetadata about the dataset's features and labels, and allow the system to\nunderstand dataset's characteristics, allowing it to autonomously manage\nprocesses like feature selection, missing values imputation, noise removal, and\nhyperparameter optimization. By reducing the need for domain-specific expertise\nand manual adjustments, DAReS offers a more efficient and scalable solution for\nbuilding recommender systems across diverse application domains. It addresses\ncritical challenges in the field, such as reusability, reproducibility, and\naccessibility for non-expert users or entry-level researchers."}
{"id": "2501.12113", "pdf": "https://arxiv.org/pdf/2501.12113", "abs": "https://arxiv.org/abs/2501.12113", "authors": ["Yun-Peng Li", "Hans-Andrea Loeliger"], "title": "Dual NUP Representations and Min-Maximization in Factor Graphs", "categories": ["stat.ML", "cs.LG", "cs.SY", "eess.SP", "eess.SY"], "comment": null, "summary": "Normals with unknown parameters (NUP) can be used to convert nontrivial\nmodel-based estimation problems into iterations of linear least-squares or\nGaussian estimation problems. In this paper, we extend this approach by\naugmenting factor graphs with convex-dual variables and pertinent NUP\nrepresentations. In particular, in a state space setting, we propose a new\niterative forward-backward algorithm that is dual to a recently proposed\nbackward-forward algorithm."}
{"id": "2502.07630", "pdf": "https://arxiv.org/pdf/2502.07630", "abs": "https://arxiv.org/abs/2502.07630", "authors": ["Stephan Naunheim", "Luis Lopes de Paiva", "Vanessa Nadig", "Yannick Kuhl", "Stefan Gundacker", "Florian Mueller", "Volkmar Schulz"], "title": "Rethinking Timing Residuals: Advancing PET Detectors with Explicit TOF Corrections", "categories": ["physics.ins-det", "cs.LG"], "comment": null, "summary": "PET is a functional imaging method that visualizes metabolic processes. TOF\ninformation can be derived from coincident detector signals and incorporated\ninto image reconstruction to enhance the SNR. PET detectors are typically\nassessed by their CTR, but timing performance is degraded by various factors.\nResearch on timing calibration seeks to mitigate these degradations and restore\naccurate timing information. While many calibration methods use analytical\napproaches, machine learning techniques have recently gained attention due to\ntheir flexibility. We developed a residual physics-based calibration approach\nthat combines prior domain knowledge with the power of machine learning models.\nThis approach begins with an initial analytical calibration addressing\nfirst-order skews. The remaining deviations, regarded as residual effects, are\nused to train machine learning models to eliminate higher-order skews. The key\nadvantage is that the experimenter guides the learning process through the\ndefinition of timing residuals. In earlier studies, we developed models that\ndirectly predicted the expected time difference, which offered corrections only\nimplicitly (implicit correction models). In this study, we introduce a new\ndefinition for timing residuals, enabling us to train models that directly\npredict correction values (explicit correction models). The explicit correction\napproach significantly simplifies data acquisition, improves linearity, and\nenhances timing performance from $371 \\pm 6$ ps to $281 \\pm 5$ ps for\ncoincidences from 430 keV to 590 keV. Additionally, the new definition reduces\nmodel size, making it suitable for high-throughput applications like PET\nscanners. Experiments were conducted using two detector stacks composed of $4\n\\times 4$ LYSO:Ce,Ca crystals ($3.8\\times 3.8\\times 20$ mm$^{3}$) coupled to $4\n\\times 4$ Broadcom NUV-MT SiPMs and digitized with the TOFPET2 ASIC."}
{"id": "2502.12147", "pdf": "https://arxiv.org/pdf/2502.12147", "abs": "https://arxiv.org/abs/2502.12147", "authors": ["Xiang Fu", "Brandon M. Wood", "Luis Barroso-Luque", "Daniel S. Levine", "Meng Gao", "Misko Dzamba", "C. Lawrence Zitnick"], "title": "Learning Smooth and Expressive Interatomic Potentials for Physical Property Prediction", "categories": ["physics.comp-ph", "cs.LG"], "comment": "20 pages, 14 figures, 6 tables", "summary": "Machine learning interatomic potentials (MLIPs) have become increasingly\neffective at approximating quantum mechanical calculations at a fraction of the\ncomputational cost. However, lower errors on held out test sets do not always\ntranslate to improved results on downstream physical property prediction tasks.\nIn this paper, we propose testing MLIPs on their practical ability to conserve\nenergy during molecular dynamic simulations. If passed, improved correlations\nare found between test errors and their performance on physical property\nprediction tasks. We identify choices which may lead to models failing this\ntest, and use these observations to improve upon highly-expressive models. The\nresulting model, eSEN, provides state-of-the-art results on a range of physical\nproperty prediction tasks, including materials stability prediction, thermal\nconductivity prediction, and phonon calculations."}
{"id": "2504.00249", "pdf": "https://arxiv.org/pdf/2504.00249", "abs": "https://arxiv.org/abs/2504.00249", "authors": ["Rory Clements", "James Ellis", "Geoff Hassall", "Simon Horsley", "Gavin Tabor"], "title": "Plane-Wave Decomposition and Randomised Training; a Novel Path to Generalised PINNs for SHM", "categories": ["physics.comp-ph", "cs.LG"], "comment": "17 pages, 16 figures; corrected author listing metadata, added\n  references for section II, typos corrected, corrected conventional PINN\n  architecture and regenerated relevant results, improved styling of figures,\n  added further references", "summary": "In this paper, we introduce a formulation of Physics-Informed Neural Networks\n(PINNs), based on learning the form of the Fourier decomposition, and a\ntraining methodology based on a spread of randomly chosen boundary conditions.\nBy training in this way we produce a PINN that generalises; after training it\ncan be used to correctly predict the solution for an arbitrary set of boundary\nconditions and interpolate this solution between the samples that spanned the\ntraining domain. We demonstrate for a toy system of two coupled oscillators\nthat this gives the PINN formulation genuine predictive capability owing to an\neffective reduction of the training to evaluation times ratio due to this\ndecoupling of the solution from specific boundary conditions."}
{"id": "2504.00694", "pdf": "https://arxiv.org/pdf/2504.00694", "abs": "https://arxiv.org/abs/2504.00694", "authors": ["Yiling He", "Hongyu She", "Xingzhi Qian", "Xinran Zheng", "Zhuo Chen", "Zhan Qin", "Lorenzo Cavallaro"], "title": "On Benchmarking Code LLMs for Android Malware Analysis", "categories": ["cs.CR", "cs.LG"], "comment": "This paper has been accepted to the 34th ACM SIGSOFT ISSTA Companion\n  (LLMSC Workshop 2025)", "summary": "Large Language Models (LLMs) have demonstrated strong capabilities in various\ncode intelligence tasks. However, their effectiveness for Android malware\nanalysis remains underexplored. Decompiled Android malware code presents unique\nchallenges for analysis, due to the malicious logic being buried within a large\nnumber of functions and the frequent lack of meaningful function names. This\npaper presents CAMA, a benchmarking framework designed to systematically\nevaluate the effectiveness of Code LLMs in Android malware analysis. CAMA\nspecifies structured model outputs to support key malware analysis tasks,\nincluding malicious function identification and malware purpose summarization.\nBuilt on these, it integrates three domain-specific evaluation metrics\n(consistency, fidelity, and semantic relevance), enabling rigorous stability\nand effectiveness assessment and cross-model comparison. We construct a\nbenchmark dataset of 118 Android malware samples from 13 families collected in\nrecent years, encompassing over 7.5 million distinct functions, and use CAMA to\nevaluate four popular open-source Code LLMs. Our experiments provide insights\ninto how Code LLMs interpret decompiled code and quantify the sensitivity to\nfunction renaming, highlighting both their potential and current limitations in\nmalware analysis."}
{"id": "2504.02263", "pdf": "https://arxiv.org/pdf/2504.02263", "abs": "https://arxiv.org/abs/2504.02263", "authors": ["Ruidong Zhu", "Ziheng Jiang", "Chao Jin", "Peng Wu", "Cesar A. Stuardo", "Dongyang Wang", "Xinlei Zhang", "Huaping Zhou", "Haoran Wei", "Yang Cheng", "Jianzhe Xiao", "Xinyi Zhang", "Lingjun Liu", "Haibin Lin", "Li-Wen Chang", "Jianxi Ye", "Xiao Yu", "Xuanzhe Liu", "Xin Jin", "Xin Liu"], "title": "MegaScale-Infer: Serving Mixture-of-Experts at Scale with Disaggregated Expert Parallelism", "categories": ["cs.DC", "cs.LG"], "comment": null, "summary": "Mixture-of-Experts (MoE) showcases tremendous potential to scale large\nlanguage models (LLMs) with enhanced performance and reduced computational\ncomplexity. However, its sparsely activated architecture shifts feed-forward\nnetworks (FFNs) from being compute-intensive to memory-intensive during\ninference, leading to substantially lower GPU utilization and increased\noperational costs. We present MegaScale-Infer, an efficient and cost-effective\nsystem for serving large-scale MoE models. MegaScale-Infer disaggregates\nattention and FFN modules within each model layer, enabling independent\nscaling, tailored parallelism strategies, and heterogeneous deployment for both\nmodules. To fully exploit disaggregation in the presence of MoE's sparsity,\nMegaScale-Infer introduces ping-pong pipeline parallelism, which partitions a\nrequest batch into micro-batches and shuttles them between attention and FFNs\nfor inference. Combined with distinct model parallelism for each module,\nMegaScale-Infer effectively hides communication overhead and maximizes GPU\nutilization. To adapt to disaggregated attention and FFN modules and minimize\ndata transmission overhead (e.g., token dispatch), MegaScale-Infer provides a\nhigh-performance M2N communication library that eliminates unnecessary\nGPU-to-CPU data copies, group initialization overhead, and GPU synchronization.\nExperimental results indicate that MegaScale-Infer achieves up to 1.90x higher\nper-GPU throughput than state-of-the-art solutions."}
{"id": "2504.10707", "pdf": "https://arxiv.org/pdf/2504.10707", "abs": "https://arxiv.org/abs/2504.10707", "authors": ["Haoyu Ji", "Yalan Song", "Tadd Bindas", "Chaopeng Shen", "Yuan Yang", "Ming Pan", "Jiangtao Liu", "Farshid Rahmani", "Ather Abbas", "Hylke Beck", "Kathryn Lawson", "Yoshihide Wada"], "title": "Distinct hydrologic response patterns and trends worldwide revealed by physics-embedded learning", "categories": ["physics.geo-ph", "cs.LG"], "comment": null, "summary": "To track rapid changes within our water sector, Global Water Models (GWMs)\nneed to realistically represent hydrologic systems' response patterns - such as\nbaseflow fraction - but are hindered by their limited ability to learn from\ndata. Here we introduce a high-resolution physics-embedded big-data-trained\nmodel as a breakthrough in reliably capturing characteristic hydrologic\nresponse patterns ('signatures') and their shifts. By realistically\nrepresenting the long-term water balance, the model revealed widespread shifts\n- up to ~20% over 20 years - in fundamental green-blue-water partitioning and\nbaseflow ratios worldwide. Shifts in these response patterns, previously\nconsidered static, contributed to increasing flood risks in northern\nmid-latitudes, heightening water supply stresses in southern subtropical\nregions, and declining freshwater inputs to many European estuaries, all with\necological implications. With more accurate simulations at monthly and daily\nscales than current operational systems, this next-generation model resolves\nlarge, nonlinear seasonal runoff responses to rainfall ('elasticity') and\nstreamflow flashiness in semi-arid and arid regions. These metrics highlight\nregions with management challenges due to large water supply variability and\nhigh climate sensitivity, but also provide tools to forecast seasonal water\navailability. This capability newly enables global-scale models to deliver\nreliable and locally relevant insights for water management."}
{"id": "2504.14898", "pdf": "https://arxiv.org/pdf/2504.14898", "abs": "https://arxiv.org/abs/2504.14898", "authors": ["Bert de Vries", "Wouter Nuijten", "Thijs van de Laar", "Wouter Kouw", "Sepideh Adamiat", "Tim Nisslbeck", "Mykola Lukashchuk", "Hoang Minh Huu Nguyen", "Marco Hidalgo Araya", "Raphael Tresor", "Thijs Jenneskens", "Ivana Nikoloska", "Raaja Ganapathy Subramanian", "Bart van Erp", "Dmitry Bagaev", "Albert Podusenko"], "title": "Expected Free Energy-based Planning as Variational Inference", "categories": ["stat.ML", "cs.LG"], "comment": "18 pages", "summary": "We address the problem of planning under uncertainty, where an agent must\nchoose actions that not only achieve desired outcomes but also reduce\nuncertainty. Traditional methods often treat exploration and exploitation as\nseparate objectives, lacking a unified inferential foundation. Active\ninference, grounded in the Free Energy Principle, provides such a foundation by\nminimizing Expected Free Energy (EFE), a cost function that combines utility\nwith epistemic drives, such as ambiguity resolution and novelty seeking.\nHowever, the computational burden of EFE minimization had remained a\nsignificant obstacle to its scalability. In this paper, we show that EFE-based\nplanning arises naturally from minimizing a variational free energy functional\non a generative model augmented with preference and epistemic priors. This\nresult reinforces theoretical consistency with the Free Energy Principle by\ncasting planning under uncertainty itself as a form of variational inference.\nOur formulation yields policies that jointly support goal achievement and\ninformation gain, while incorporating a complexity term that accounts for\nbounded computational resources. This unifying framework connects and extends\nexisting methods, enabling scalable, resource-aware implementations of active\ninference agents."}
{"id": "2504.15327", "pdf": "https://arxiv.org/pdf/2504.15327", "abs": "https://arxiv.org/abs/2504.15327", "authors": ["Tianliang Yao", "Bo Lu", "Markus Kowarschik", "Yixuan Yuan", "Hubin Zhao", "Sebastien Ourselin", "Kaspar Althoefer", "Junbo Ge", "Peng Qi"], "title": "Advancing Embodied Intelligence in Robotic-Assisted Endovascular Procedures: A Systematic Review of AI Solutions", "categories": ["cs.RO", "cs.LG"], "comment": "41 pages, 7 figures", "summary": "Endovascular procedures have revolutionized the treatment of vascular\ndiseases thanks to minimally invasive solutions that significantly reduce\npatient recovery time and enhance clinical outcomes. However, the precision and\ndexterity required during these procedures poses considerable challenges for\ninterventionists. Robotic systems have emerged offering transformative\nsolutions, addressing issues such as operator fatigue, radiation exposure, and\nthe inherent limitations of human precision. The integration of Embodied\nIntelligence (EI) into these systems signifies a paradigm shift, enabling\nrobots to navigate complex vascular networks and adapt to dynamic physiological\nconditions. Data-driven approaches, advanced computer vision, medical image\nanalysis, and machine learning techniques, are at the forefront of this\nevolution. These methods augment procedural intelligence by facilitating\nreal-time vessel segmentation, device tracking, and anatomical landmark\ndetection. Reinforcement learning and imitation learning further refine\nnavigation strategies and replicate experts' techniques. This review\nsystematically examines the integration of EI principles into robotic\ntechnologies, in relation to endovascular procedures. We discuss recent\nadvancements in intelligent perception and data-driven control, and their\npractical applications in robot-assisted endovascular procedures. By critically\nevaluating current limitations and emerging opportunities, this review\nestablishes a framework for future developments, emphasizing the potential for\ngreater autonomy and improved clinical outcomes. Emerging trends and specific\nareas of research, such as federated learning for medical data sharing,\nexplainable AI for clinical decision support, and advanced human-robot\ncollaboration paradigms, are also explored, offering insights into the future\ndirection of this rapidly evolving field."}
